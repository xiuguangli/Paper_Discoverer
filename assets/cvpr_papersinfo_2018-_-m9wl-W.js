const e=JSON.parse(`[{"order":0,"title":"Embodied Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Das_Embodied_Question_Answering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Das_Embodied_Question_Answering_CVPR_2018_paper.html","abstract":"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\\"What color is the car?\\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question (\\"orange\\").  EmbodiedQA requires a range of AI skills -- language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments, evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning.","中文标题":"具身问答","摘要翻译":"我们提出了一个新的AI任务——具身问答（EmbodiedQA），在这个任务中，一个代理被随机放置在一个3D环境中的某个位置，并被问一个问题（“汽车是什么颜色的？”）。为了回答这个问题，代理必须首先智能地导航以探索环境，通过第一人称（自我中心）视觉收集必要的视觉信息，然后回答问题（“橙色”）。具身问答需要一系列AI技能——语言理解、视觉识别、主动感知、目标驱动导航、常识推理、长期记忆以及将语言基础化为行动。在这项工作中，我们开发了一个在House3D环境中的问题和答案数据集、评估指标，以及一个通过模仿和强化学习训练的分层模型。","领域":"具身智能/视觉问答/强化学习","问题":"如何在3D环境中通过智能导航和视觉信息收集来回答问题","动机":"探索和开发一个能够结合语言理解、视觉识别、主动感知等多种AI技能的具身问答系统","方法":"开发了一个在House3D环境中的问题和答案数据集、评估指标，以及一个通过模仿和强化学习训练的分层模型","关键词":["具身智能","视觉问答","强化学习"],"涉及的技术概念":"具身问答（EmbodiedQA）是一种结合了语言理解、视觉识别、主动感知、目标驱动导航、常识推理、长期记忆和将语言基础化为行动的AI任务。通过第一人称视觉在3D环境中智能导航和收集视觉信息来回答问题。"},{"order":1,"title":"Learning by Asking Questions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Misra_Learning_by_Asking_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Misra_Learning_by_Asking_CVPR_2018_paper.html","abstract":"We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA).  We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.","中文标题":"通过提问学习","摘要翻译":"我们引入了一个名为'通过提问学习'(LBA)的交互式学习框架，用于开发和测试智能视觉系统。我们在视觉问答(VQA)任务的背景下探索LBA。LBA与标准VQA训练的不同之处在于，在训练期间大多数问题并未被观察到，学习者必须提出它想要答案的问题。因此，LBA更接近自然学习，并且有可能比传统的VQA设置更高效地利用数据。我们提出了一个在CLEVR数据集上执行LBA的模型，并展示了它在从预言机交互学习时自动发现一个从易到难的课程。我们的LBA生成的数据始终匹配或优于CLEVR训练数据，并且样本效率更高。我们还展示了我们的模型提出的问题能够泛化到最先进的VQA模型和新的测试时间分布。","领域":"视觉问答/交互式学习/数据效率","问题":"如何在视觉问答任务中实现更自然和高效的学习","动机":"探索一种更接近自然学习且数据效率更高的视觉问答学习方法","方法":"引入'通过提问学习'(LBA)框架，提出一个在CLEVR数据集上执行LBA的模型，自动发现从易到难的课程","关键词":["视觉问答","交互式学习","数据效率","CLEVR数据集","课程学习"],"涉及的技术概念":{"LBA":"通过提问学习，一种交互式学习框架，允许学习者在训练期间提出问题","VQA":"视觉问答，一种结合视觉和语言理解的任务","CLEVR数据集":"一个用于视觉问答的合成数据集，包含复杂的视觉场景和问题","课程学习":"一种学习策略，从简单到复杂逐步学习，以提高学习效率和效果"}},{"order":2,"title":"Finding Tiny Faces in the Wild With Generative Adversarial Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Finding_Tiny_Faces_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bai_Finding_Tiny_Faces_CVPR_2018_paper.html","abstract":"Face detection techniques have been developed for decades, and one of remaining open challenges is detecting small faces in unconstrained conditions. The reason is that tiny faces are often lacking detailed information and blurring. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a blurry small one by adopting a generative adversarial network (GAN). Toward this end, the basic GAN formulation achieves it by super-resolving and refining sequentially (e.g. SR-GAN and cycle-GAN). However, we design a novel network to address the problem of super-resolving and refining jointly. We also introduce new training losses to guide the generator network to recover fine details and to promote the discriminator network to distinguish real vs. fake and face vs. non-face simultaneously. Extensive experiments on the challenging dataset WIDER FACE demonstrate the effectiveness of our proposed method in restoring a clear high-resolution face from a blurry small one, and show that the detection performance outperforms other state-of-the-art methods.","中文标题":"在野外使用生成对抗网络寻找微小面孔","摘要翻译":"人脸检测技术已经发展了几十年，其中一个尚未解决的挑战是在无约束条件下检测小面孔。原因是小面孔通常缺乏详细信息并且模糊。在本文中，我们提出了一种算法，通过采用生成对抗网络（GAN）直接从模糊的小面孔生成清晰的高分辨率面孔。为此，基本的GAN公式通过顺序超分辨率和细化（例如SR-GAN和cycle-GAN）实现这一点。然而，我们设计了一个新颖的网络来共同解决超分辨率和细化的问题。我们还引入了新的训练损失来指导生成器网络恢复精细细节，并促进鉴别器网络同时区分真实与伪造以及面孔与非面孔。在具有挑战性的数据集WIDER FACE上进行的大量实验证明了我们提出的方法在从模糊的小面孔恢复清晰高分辨率面孔方面的有效性，并显示检测性能优于其他最先进的方法。","领域":"人脸检测/超分辨率/生成对抗网络","问题":"在无约束条件下检测小面孔","动机":"小面孔通常缺乏详细信息并且模糊，难以检测","方法":"采用生成对抗网络（GAN）直接从模糊的小面孔生成清晰的高分辨率面孔，设计新颖的网络共同解决超分辨率和细化的问题，并引入新的训练损失","关键词":["人脸检测","超分辨率","生成对抗网络"],"涉及的技术概念":"生成对抗网络（GAN）是一种由两个网络组成的系统：生成器和鉴别器。生成器尝试创建看起来像真实数据的假数据，而鉴别器则尝试区分真实数据和假数据。通过这种方式，GAN可以生成高质量的数据。在本文中，GAN被用来从模糊的小面孔生成清晰的高分辨率面孔。"},{"order":3,"title":"Learning Face Age Progression: A Pyramid Architecture of GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Learning_Face_Age_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Learning_Face_Age_CVPR_2018_paper.html","abstract":"The two underlying requirements of face age progression, i.e. aging accuracy and identity permanence, are not well studied in the literature. In this paper, we present a novel generative adversarial network based approach. It separately models the constraints for the intrinsic subject-specific characteristics and the age-specific facial changes with respect to the elapsed time, ensuring that the generated faces present desired aging effects while simultaneously keeping personalized properties stable. Further, to generate more lifelike facial details, high-level age-specific features conveyed by the synthesized face are estimated by a pyramidal adversarial discriminator at multiple scales, which simulates the aging effects in a finer manner. The proposed method is applicable to diverse face samples in the presence of variations in pose, expression, makeup, etc., and remarkably vivid aging effects are achieved. Both visual fidelity and quantitative evaluations show that the approach advances the state-of-the-art.","中文标题":"学习人脸年龄进展：一种基于GAN的金字塔架构","摘要翻译":"人脸年龄进展的两个基本要求，即老化准确性和身份持久性，在文献中尚未得到充分研究。在本文中，我们提出了一种基于生成对抗网络的新方法。它分别对内在的特定主体特征和与经过时间相关的特定年龄面部变化的约束进行建模，确保生成的面部呈现出所需的老化效果，同时保持个性化属性的稳定。此外，为了生成更逼真的面部细节，通过金字塔对抗判别器在多个尺度上估计由合成面部传达的高级年龄特定特征，从而以更精细的方式模拟老化效果。所提出的方法适用于存在姿势、表情、化妆等变化的多样化面部样本，并实现了显著逼真的老化效果。视觉保真度和定量评估均表明，该方法推进了现有技术水平。","领域":"人脸老化/生成对抗网络/面部识别","问题":"解决人脸年龄进展中的老化准确性和身份持久性问题","动机":"现有文献中对于人脸年龄进展的两个基本要求——老化准确性和身份持久性——的研究不足，需要一种新方法来同时满足这两个要求。","方法":"提出了一种基于生成对抗网络的新方法，分别对特定主体的内在特征和与时间相关的年龄特定面部变化进行建模，并通过金字塔对抗判别器在多个尺度上估计高级年龄特定特征，以更精细的方式模拟老化效果。","关键词":["人脸老化","生成对抗网络","面部识别"],"涉及的技术概念":"生成对抗网络（GAN）用于模拟面部老化过程，金字塔对抗判别器用于在多个尺度上估计高级年龄特定特征，以实现更精细的老化效果模拟。"},{"order":4,"title":"PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_PairedCycleGAN_Asymmetric_Style_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_PairedCycleGAN_Asymmetric_Style_CVPR_2018_paper.html","abstract":"This paper introduces an automatic method for editing a portrait photo so that the subject appears to be wearing makeup in the style of another person in a reference photo. Our unsupervised learning approach relies on a new framework of cycle-consistent generative adversarial networks. Different from the image domain transfer problem, our style transfer problem involves two asymmetric functions: a forward function encodes example-based style transfer, whereas a backward function removes the style. We construct two coupled networks to implement these functions -- one that transfers makeup style and a second that can remove makeup -- such that the output of their successive application to an input photo will match the input. The learned style network can then quickly apply an arbitrary makeup style to an arbitrary photo. We demonstrate the effectiveness on a broad range of portraits and styles.","中文标题":"PairedCycleGAN：用于应用和移除化妆的非对称风格迁移","摘要翻译":"本文介绍了一种自动编辑肖像照片的方法，使得照片中的主体看起来像是穿着参考照片中另一个人的化妆风格。我们的无监督学习方法依赖于一种新的循环一致性生成对抗网络框架。与图像域转移问题不同，我们的风格转移问题涉及两个非对称函数：一个前向函数编码基于示例的风格转移，而一个后向函数移除风格。我们构建了两个耦合网络来实现这些功能——一个用于转移化妆风格，另一个可以移除化妆——这样它们对输入照片的连续应用的输出将与输入匹配。学习到的风格网络可以快速将任意化妆风格应用于任意照片。我们在广泛的肖像和风格上展示了其有效性。","领域":"风格迁移/图像编辑/生成对抗网络","问题":"如何自动编辑肖像照片以应用或移除特定化妆风格","动机":"实现无需监督即可自动将一种化妆风格应用到另一张照片上，并能移除化妆风格，以简化图像编辑过程","方法":"采用循环一致性生成对抗网络框架，构建两个耦合网络分别用于转移和移除化妆风格，确保连续应用后的输出与输入匹配","关键词":["风格迁移","图像编辑","生成对抗网络"],"涉及的技术概念":"循环一致性生成对抗网络（CycleGAN）是一种用于图像到图像转换的框架，能够在没有成对示例的情况下学习两个图像域之间的映射。本文通过构建两个耦合网络，一个用于转移化妆风格，另一个用于移除化妆，实现了非对称风格迁移。"},{"order":5,"title":"GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mueller_GANerated_Hands_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mueller_GANerated_Hands_for_CVPR_2018_paper.html","abstract":"We address the highly challenging problem of real-time 3D hand tracking based on a monocular RGB-only sequence. Our tracking method combines a convolutional neural network with a kinematic 3D hand model, such that it generalizes well to unseen data, is robust to occlusions and varying camera viewpoints, and leads to anatomically plausible as well as temporally smooth hand motions. For training our CNN we propose a novel approach for the synthetic generation of training data that is based on a geometrically consistent image-to-image translation network. To be more specific, we use a neural network that translates synthetic images to \\"real\\" images, such that the so-generated images follow the same statistical distribution as real-world hand images. For training this translation network we combine an adversarial loss and a cycle-consistency loss with a geometric consistency loss in order to preserve geometric properties (such as hand pose) during translation. We demonstrate that our hand tracking system outperforms the current state-of-the-art on challenging RGB-only footage.","中文标题":"GAN生成的手用于单目RGB实时3D手部追踪","摘要翻译":"我们解决了基于单目RGB序列的实时3D手部追踪这一极具挑战性的问题。我们的追踪方法结合了卷积神经网络和运动学3D手部模型，使其能够很好地泛化到未见过的数据，对遮挡和变化的摄像机视角具有鲁棒性，并产生解剖学上合理且时间上平滑的手部动作。为了训练我们的CNN，我们提出了一种基于几何一致的图像到图像转换网络的合成训练数据生成新方法。具体来说，我们使用一个神经网络将合成图像转换为“真实”图像，使得生成的图像遵循与现实世界手部图像相同的统计分布。为了训练这个转换网络，我们将对抗性损失和循环一致性损失与几何一致性损失结合起来，以在转换过程中保留几何属性（如手部姿势）。我们证明了我们的手部追踪系统在具有挑战性的仅RGB视频上优于当前的最新技术。","领域":"3D手部追踪/图像合成/几何一致性","问题":"实时3D手部追踪","动机":"提高基于单目RGB序列的实时3D手部追踪的准确性和鲁棒性","方法":"结合卷积神经网络和运动学3D手部模型，使用基于几何一致的图像到图像转换网络的合成训练数据生成方法","关键词":["3D手部追踪","图像合成","几何一致性"],"涉及的技术概念":"卷积神经网络（CNN）用于特征提取和模式识别，运动学3D手部模型用于模拟手部动作，图像到图像转换网络用于生成逼真的训练数据，对抗性损失、循环一致性损失和几何一致性损失用于训练转换网络以保持图像转换过程中的几何属性。"},{"order":6,"title":"Learning Pose Specific Representations by Predicting Different Views","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Poier_Learning_Pose_Specific_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Poier_Learning_Pose_Specific_CVPR_2018_paper.html","abstract":"The labeled data required to learn pose estimation for articulated objects is difficult to provide in the desired quantity, realism, density, and accuracy. To address this issue, we develop a method to learn representations, which are very specific for articulated poses, without the need for labeled training data. We exploit the observation that the object pose of a known object is predictive for the appearance in any known view. That is, given only the pose and shape parameters of a hand, the hand's appearance from any viewpoint can be approximated. To exploit this observation, we train a model that - given input from one view - estimates a latent representation, which is trained to be predictive for the appearance of the object when captured from another viewpoint. Thus, the only necessary supervision is the second view. The training process of this model reveals an implicit pose representation in the latent space. Importantly, at test time the pose representation can be inferred using only a single view. In qualitative and quantitative experiments we show that the learned representations capture detailed pose information. Moreover, when training the proposed method jointly with labeled and unlabeled data, it consistently surpasses the performance of its fully supervised counterpart, while reducing the amount of needed labeled samples by at least one order of magnitude.","中文标题":"通过预测不同视角学习姿态特定表示","摘要翻译":"学习关节物体姿态估计所需的标记数据难以在所需的数量、真实性、密度和准确性上提供。为了解决这个问题，我们开发了一种方法，可以在不需要标记训练数据的情况下学习非常特定于关节姿态的表示。我们利用了一个观察结果，即已知物体的姿态对于任何已知视角的外观都是可预测的。也就是说，仅给定手的姿态和形状参数，就可以近似地从任何视角看到手的外观。为了利用这一观察结果，我们训练了一个模型，该模型在给定一个视角的输入时，估计一个潜在表示，该表示被训练为预测物体从另一个视角捕获时的外观。因此，唯一必要的监督是第二个视角。该模型的训练过程揭示了潜在空间中的隐式姿态表示。重要的是，在测试时，仅使用单个视角就可以推断出姿态表示。在定性和定量实验中，我们展示了学习到的表示捕捉到了详细的姿态信息。此外，当将所提出的方法与标记和未标记数据联合训练时，它始终超越其完全监督的对应方法的性能，同时将所需的标记样本数量减少至少一个数量级。","领域":"姿态估计/关节物体/视角预测","问题":"关节物体姿态估计所需的标记数据难以在所需的数量、真实性、密度和准确性上提供","动机":"开发一种方法，可以在不需要标记训练数据的情况下学习非常特定于关节姿态的表示","方法":"训练一个模型，该模型在给定一个视角的输入时，估计一个潜在表示，该表示被训练为预测物体从另一个视角捕获时的外观","关键词":["姿态估计","关节物体","视角预测","潜在表示","隐式姿态表示"],"涉及的技术概念":"潜在表示是指模型在训练过程中学习到的一种隐式姿态表示，它能够预测物体从不同视角捕获时的外观。这种方法减少了对大量标记数据的依赖，通过利用物体姿态与外观之间的关系，实现了从单个视角推断姿态的能力。"},{"order":7,"title":"Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Weakly_and_Semi_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Weakly_and_Semi_CVPR_2018_paper.html","abstract":"Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object/animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code will be made publicly available.","中文标题":"通过姿态引导知识转移实现弱监督和半监督的人体部位解析","摘要翻译":"人体部位解析，或称人体语义部位分割，是许多计算机视觉任务的基础。在传统的语义分割方法中，提供了地面真实分割，并以端到端的方式训练全卷积网络（FCN）。尽管这些方法展示了令人印象深刻的结果，但它们的性能高度依赖于训练数据的数量和质量。在本文中，我们提出了一种新颖的方法，使用易于获得的人体关键点注释生成合成的人体部位分割数据。我们的关键思想是利用人体之间的解剖相似性，将一个人的解析结果转移到具有相似姿态的另一个人。使用这些估计结果作为额外的训练数据，我们的半监督模型在PASCAL-Person-Part数据集上比其强监督对应物高出6 mIOU，并且我们实现了最先进的人体解析结果。我们的方法是通用的，可以很容易地扩展到其他对象/动物解析任务，假设它们的解剖相似性可以通过关键点注释。提出的模型和附带的源代码将公开提供。","领域":"人体部位解析/语义分割/关键点检测","问题":"解决人体部位解析中训练数据数量和质量依赖性强的问题","动机":"提高人体部位解析的准确性和效率，减少对大量高质量标注数据的依赖","方法":"利用人体关键点注释生成合成的人体部位分割数据，通过解剖相似性转移解析结果，使用这些数据训练半监督模型","关键词":["人体部位解析","语义分割","关键点检测","半监督学习","解剖相似性"],"涉及的技术概念":{"人体部位解析":"指对人体图像进行分割，识别出各个部位的过程","语义分割":"一种图像分割技术，旨在为图像中的每个像素分配一个类别标签","关键点检测":"识别图像中特定点的位置，如人体关节","半监督学习":"一种机器学习方法，结合使用有标签和无标签的数据进行训练","解剖相似性":"指不同个体在解剖结构上的相似性，用于知识转移"}},{"order":8,"title":"Person Transfer GAN to Bridge Domain Gap for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Person_Transfer_GAN_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Person_Transfer_GAN_CVPR_2018_paper.html","abstract":"Although the performance of person Re-Identification (ReID) has been significantly boosted, many challenging issues in real scenarios have not been fully investigated, e.g., the complex scenes and lighting variations, viewpoint and pose changes, and the large number of identities in a camera network. To facilitate the research towards conquering those issues, this paper contributes a new dataset called MSMT17 with many important features, e.g., 1) the raw videos are taken by an 15-camera network deployed in both indoor and outdoor scenes, 2) the videos cover a long period of time and present complex lighting variations, and 3) it contains currently the largest number of annotated identities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe that, domain gap commonly exists between datasets, which essentially causes severe performance drop when training and testing on different datasets. This results in that available training data cannot be effectively leveraged for new testing domains. To relieve the expensive costs of annotating new training samples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to bridge the domain gap. Comprehensive experiments show that the domain gap could be substantially narrowed-down by the PTGAN.","中文标题":"人物转移生成对抗网络以弥合人物重识别中的领域差距","摘要翻译":"尽管人物重识别（ReID）的性能已显著提升，但在实际场景中的许多挑战性问题尚未得到充分研究，例如复杂场景和光照变化、视角和姿态变化，以及摄像头网络中大量的身份。为了促进克服这些问题的研究，本文贡献了一个名为MSMT17的新数据集，它具有许多重要特征，例如：1）原始视频是由一个部署在室内外场景的15摄像头网络拍摄的，2）视频覆盖了长时间并呈现复杂的光照变化，3）它包含了目前最大数量的注释身份，即4,101个身份和126,441个边界框。我们还观察到，数据集之间普遍存在领域差距，这本质上导致了在不同数据集上训练和测试时性能的严重下降。这使得可用的训练数据无法有效地用于新的测试领域。为了减轻注释新训练样本的昂贵成本，我们提出了一种人物转移生成对抗网络（PTGAN）来弥合领域差距。综合实验表明，PTGAN可以显著缩小领域差距。","领域":"人物重识别/生成对抗网络/领域适应","问题":"人物重识别中的领域差距问题","动机":"为了克服人物重识别在实际场景中的挑战性问题，如复杂场景和光照变化、视角和姿态变化，以及摄像头网络中大量的身份，并减轻注释新训练样本的昂贵成本。","方法":"提出了一种人物转移生成对抗网络（PTGAN）来弥合领域差距。","关键词":["人物重识别","生成对抗网络","领域适应"],"涉及的技术概念":"人物重识别（ReID）是指在不同的摄像头视角下识别同一人物的技术。生成对抗网络（GAN）是一种深度学习模型，通过生成器和判别器的对抗过程来生成数据。领域适应是指将在一个领域（数据集）上学习到的知识应用到另一个不同但相关的领域（数据集）上的技术。"},{"order":9,"title":"Cross-Modal Deep Variational Hand Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.html","abstract":"The human hand moves in complex and high-dimensional ways, making estimation of 3D hand pose configurations from images alone a challenging task. In this work we propose a method to learn a statistical hand model represented by a cross-modal trained latent space via a generative deep neural network. We derive an objective function from the variational lower bound of the VAE framework and jointly optimize the resulting cross-modal KL-divergence and the posterior reconstruction objective, naturally admitting a training regime that leads to a coherent latent space across multiple modalities such as RGB images, 2D keypoint detections or 3D hand configurations. Additionally, it grants a straightforward way of using semi-supervision. This latent space can be directly used to estimate 3D hand poses from RGB images, outperforming the state-of-the art in different settings. Furthermore, we show that our proposed method can be used without changes on depth images and performs comparably to specialized methods. Finally, the model is fully generative and can synthesize consistent pairs of hand configurations across modalities. We evaluate our method on both RGB and depth datasets and analyze the latent space qualitatively.","中文标题":"跨模态深度变分手部姿态估计","摘要翻译":"人类手部的运动复杂且高维，仅从图像中估计3D手部姿态配置是一项具有挑战性的任务。在本工作中，我们提出了一种方法，通过生成深度神经网络学习由跨模态训练的潜在空间表示的统计手部模型。我们从VAE框架的变分下界推导出目标函数，并联合优化由此产生的跨模态KL散度和后验重建目标，自然地允许一种训练机制，该机制导致跨多个模态（如RGB图像、2D关键点检测或3D手部配置）的连贯潜在空间。此外，它提供了一种直接使用半监督的方式。这个潜在空间可以直接用于从RGB图像估计3D手部姿态，在不同设置下优于现有技术。此外，我们展示了我们提出的方法可以在深度图像上无需更改地使用，并且与专门方法表现相当。最后，该模型完全生成，可以跨模态合成一致的手部配置对。我们在RGB和深度数据集上评估我们的方法，并定性分析潜在空间。","领域":"手部姿态估计/生成模型/跨模态学习","问题":"从图像中估计3D手部姿态配置","动机":"人类手部的运动复杂且高维，仅从图像中估计3D手部姿态配置具有挑战性","方法":"通过生成深度神经网络学习由跨模态训练的潜在空间表示的统计手部模型，从VAE框架的变分下界推导出目标函数，并联合优化跨模态KL散度和后验重建目标","关键词":["手部姿态估计","生成模型","跨模态学习","VAE框架","KL散度","半监督学习"],"涉及的技术概念":"VAE框架（变分自编码器框架）用于推导目标函数，KL散度用于衡量两个概率分布之间的差异，半监督学习允许使用部分标注数据进行训练，生成模型能够生成新的数据样本。"},{"order":10,"title":"Disentangled Person Image Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Disentangled_Person_Image_CVPR_2018_paper.html","abstract":"Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.","中文标题":"解耦的人物图像生成","摘要翻译":"生成新颖且真实的人物图像是一项具有挑战性的任务，因为不同的图像因素（如前景、背景和姿态信息）之间存在复杂的相互作用。在这项工作中，我们旨在基于一种新颖的两阶段重建管道生成此类图像，该管道学习上述图像因素的解耦表示，并同时生成新颖的人物图像。首先，提出了一种多分支重建网络，将这三个因素解耦并编码为嵌入特征，然后将这些特征组合以重新组合输入图像本身。其次，以对抗方式学习三个相应的映射函数，以便将高斯噪声映射到学习到的嵌入特征空间，分别针对每个因素。使用所提出的框架，我们可以操纵输入图像的前景、背景和姿态，还可以采样新的嵌入特征以生成此类目标操纵，从而提供对生成过程的更多控制。在Market-1501和Deepfashion数据集上的实验表明，我们的模型不仅生成具有新前景、背景和姿态的真实人物图像，而且还操纵生成的因素并插值中间状态。在Market-1501上的另一组实验表明，我们的模型也可以对人物重新识别任务有益。","领域":"图像生成/人物重新识别/对抗学习","问题":"生成具有新颖前景、背景和姿态的真实人物图像","动机":"探索如何通过解耦图像因素来生成新颖且真实的人物图像，并提供对生成过程的更多控制","方法":"提出了一种两阶段重建管道，包括多分支重建网络和对抗性映射函数，以解耦和编码图像因素，并生成新颖的人物图像","关键词":["图像生成","人物重新识别","对抗学习","解耦表示","嵌入特征"],"涉及的技术概念":"解耦表示指的是将图像中的不同因素（如前景、背景和姿态）分离并独立表示的技术。嵌入特征是指将图像因素编码为低维空间中的特征向量。对抗学习是一种训练模型的方法，通过让两个模型相互竞争来提高生成图像的质量。"},{"order":11,"title":"Super-FAN: Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses With GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bulat_Super-FAN_Integrated_Facial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bulat_Super-FAN_Integrated_Facial_CVPR_2018_paper.html","abstract":"This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images.","中文标题":"Super-FAN：集成面部标志定位和任意姿态下真实世界低分辨率面部图像的基于GAN的超分辨率","摘要翻译":"本文解决了两个具有挑战性的任务：提高低分辨率面部图像的质量和在此类低质量图像上准确定位面部标志。为此，我们做出了以下五个贡献：(a) 我们提出了Super-FAN：第一个同时解决这两个任务的端到端系统，即同时提高面部分辨率和检测面部标志。Super-FAN的新颖之处在于通过集成一个用于通过热图回归进行面部对齐的子网络并优化一种新颖的热图损失，将结构信息整合到基于GAN的超分辨率算法中。(b) 我们通过不仅在正面图像上（如先前的工作）报告良好的结果，而且在整个面部姿态范围内，并且不仅在合成低分辨率图像上（如先前的工作）而且在真实世界的图像上报告良好的结果，说明了联合训练这两个网络的好处。(c) 我们通过提出一种新的基于残差的架构，改进了面部超分辨率的最新技术。(d) 定量上，我们展示了在面部超分辨率和对齐方面对最新技术的显著改进。(e) 定性上，我们首次展示了在真实世界低分辨率图像上的良好结果。","领域":"面部超分辨率/面部标志定位/生成对抗网络","问题":"提高低分辨率面部图像的质量和在此类低质量图像上准确定位面部标志","动机":"解决在低分辨率面部图像上同时进行超分辨率和面部标志定位的挑战","方法":"提出了Super-FAN，一个端到端系统，通过集成一个用于面部对齐的子网络并优化一种新颖的热图损失，将结构信息整合到基于GAN的超分辨率算法中","关键词":["面部超分辨率","面部标志定位","生成对抗网络","热图回归","残差架构"],"涉及的技术概念":"Super-FAN系统结合了生成对抗网络（GANs）用于超分辨率，通过热图回归进行面部对齐，并采用了一种新的热图损失来优化。此外，提出了一种新的基于残差的架构来改进面部超分辨率的效果。"},{"order":12,"title":"Multistage Adversarial Losses for Pose-Based Human Image Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.html","abstract":"Human image synthesis has extensive practical applications e.g. person re-identification and data augmentation for human pose estimation. However, it is much more challenging than rigid object synthesis, e.g. cars and chairs, due to the variability of human posture. In this paper, we propose a pose-based human image synthesis method which can keep the human posture unchanged in novel viewpoints. Furthermore, we adopt multistage adversarial losses separately for the foreground and background generation, which fully exploits the multi-modal characteristics of generative loss to generate more realistic looking images. We perform extensive experiments on the Human3.6M dataset and verify the effectiveness of each stage of our method. The generated human images not only keep the same pose as the input image, but also have clear detailed foreground and background. The quantitative comparison results illustrate that our approach achieves much better results than several state-of-the-art methods.","中文标题":"基于姿态的多阶段对抗损失用于人体图像合成","摘要翻译":"人体图像合成在人员重识别和人体姿态估计的数据增强等方面具有广泛的实际应用。然而，由于人体姿态的多样性，它比刚性物体（如汽车和椅子）的合成更具挑战性。在本文中，我们提出了一种基于姿态的人体图像合成方法，该方法可以在新视角下保持人体姿态不变。此外，我们分别对前景和背景生成采用了多阶段对抗损失，充分利用生成损失的多模态特性来生成更逼真的图像。我们在Human3.6M数据集上进行了大量实验，并验证了我们方法每个阶段的有效性。生成的人体图像不仅保持了与输入图像相同的姿态，而且具有清晰的前景和背景细节。定量比较结果表明，我们的方法比几种最先进的方法取得了更好的结果。","领域":"人体图像合成/对抗网络/姿态估计","问题":"在新视角下保持人体姿态不变的人体图像合成","动机":"解决人体图像合成中由于人体姿态多样性带来的挑战，以及提高合成图像的逼真度","方法":"提出了一种基于姿态的人体图像合成方法，采用多阶段对抗损失分别处理前景和背景生成","关键词":["人体图像合成","对抗网络","姿态估计","多阶段对抗损失","Human3.6M数据集"],"涉及的技术概念":"多阶段对抗损失是一种在生成对抗网络（GANs）中使用的技术，通过在生成过程中引入多个阶段的对抗性损失来提高生成图像的质量和逼真度。Human3.6M数据集是一个广泛用于人体姿态估计和人体图像合成研究的大规模数据集，包含了丰富的3D人体姿态和动作数据。"},{"order":13,"title":"Rotation Averaging and Strong Duality","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.html","abstract":"In this paper we explore the role of duality principles within the problem of rotation averaging, a fundamental task in a wide range of computer vision applications. In its conventional form, rotation averaging is stated as a minimization over multiple rotation constraints. As these constraints are non-convex, this problem is generally considered challenging to solve globally. We show how to circumvent this difficulty through the use of Lagrangian duality. While such an approach is well-known it is normally not guaranteed to provide a tight relaxation. Based on spectral graph theory, we analytically prove that in many cases there is no duality gap unless the noise levels are severe. This allows us to obtain certifiably global solutions to a class of important non-convex problems in polynomial time.   We also propose an efficient, scalable algorithm that out-performs general purpose numerical solvers and is able to handle the large problem instances commonly occurring in structure from motion settings. The potential of this proposed method is demonstrated on a number of different problems, consisting of both synthetic and real-world data.","中文标题":"旋转平均与强对偶性","摘要翻译":"在本文中，我们探讨了对偶原理在旋转平均问题中的作用，这是广泛计算机视觉应用中的一项基本任务。旋转平均通常被表述为在多个旋转约束下的最小化问题。由于这些约束是非凸的，这个问题通常被认为难以全局解决。我们展示了如何通过使用拉格朗日对偶性来规避这一困难。虽然这种方法众所周知，但通常不保证能提供紧密的松弛。基于谱图理论，我们分析证明，在许多情况下，除非噪声水平严重，否则不存在对偶间隙。这使我们能够在多项式时间内获得一类重要非凸问题的可证明全局解。我们还提出了一种高效、可扩展的算法，该算法优于通用数值求解器，并且能够处理在运动结构设置中常见的大规模问题实例。通过合成和真实世界数据组成的多个不同问题，展示了所提出方法的潜力。","领域":"三维重建/运动结构/旋转平均","问题":"解决旋转平均问题中的非凸约束导致的全局解难以获得的问题","动机":"探索对偶原理在旋转平均问题中的应用，以克服非凸约束带来的挑战，实现全局解的获取","方法":"使用拉格朗日对偶性，基于谱图理论分析证明在许多情况下不存在对偶间隙，提出一种高效、可扩展的算法","关键词":["旋转平均","拉格朗日对偶性","谱图理论","全局解","多项式时间"],"涉及的技术概念":"拉格朗日对偶性是一种优化技术，用于将原始问题转化为对偶问题，以期找到原始问题的解。谱图理论是研究图的谱性质（即图的邻接矩阵或拉普拉斯矩阵的特征值和特征向量）的数学分支，用于分析图的结构和性质。旋转平均是计算机视觉中的一个基本问题，涉及从多个旋转测量中估计平均旋转。"},{"order":14,"title":"Hybrid Camera Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.html","abstract":"In this paper, we aim to solve the pose estimation problem of calibrated pinhole and generalized cameras w.r.t. a Structure-from-Motion (SfM) model by leveraging both 2D-3D correspondences as well as 2D-2D correspondences. Traditional approaches either focus on the use of 2D-3D matches, known as structure-based pose estimation or solely on 2D-2D matches (structure-less pose estimation). Absolute pose approaches are limited in their performance by the quality of the 3D point triangulations as well as the completeness of the 3D model. Relative pose approaches, on the other hand, while being more accurate, also tend to be far more computationally costly and often return dozens of possible solutions. This work aims to bridge the gap between these two paradigms. We propose a new RANSAC-based approach that automatically chooses the best type of solver to use at each iteration in a data-driven way. The solvers chosen by our RANSAC can range from pure structure-based or structure-less solvers, to any possible combination of hybrid solvers (i.e. using both types of matches) in between. A number of these new hybrid minimal solvers are also presented in this paper. Both synthetic and real data experiments show our approach to be as accurate as structure-less approaches, while staying close to the efficiency of structure-based methods.","中文标题":"混合相机姿态估计","摘要翻译":"在本文中，我们旨在通过利用2D-3D对应关系以及2D-2D对应关系，解决相对于结构从运动（SfM）模型的校准针孔和广义相机的姿态估计问题。传统方法要么专注于使用2D-3D匹配，称为基于结构的姿态估计，要么仅专注于2D-2D匹配（无结构姿态估计）。绝对姿态方法的性能受限于3D点三角测量的质量以及3D模型的完整性。另一方面，相对姿态方法虽然更准确，但也往往计算成本更高，并且经常返回数十种可能的解决方案。这项工作旨在弥合这两种范式之间的差距。我们提出了一种新的基于RANSAC的方法，该方法以数据驱动的方式自动选择每次迭代中使用的最佳求解器类型。我们的RANSAC选择的求解器范围可以从纯基于结构或无结构求解器，到任何可能的混合求解器组合（即使用两种类型的匹配）。本文还介绍了一些新的混合最小求解器。合成和真实数据实验表明，我们的方法既准确又接近基于结构方法的效率。","领域":"相机姿态估计/结构从运动/混合求解器","问题":"解决校准针孔和广义相机相对于结构从运动（SfM）模型的姿态估计问题","动机":"传统方法在性能上受限于3D点三角测量的质量和3D模型的完整性，相对姿态方法虽然准确但计算成本高，需要一种既能保持准确性又能提高效率的方法","方法":"提出了一种新的基于RANSAC的方法，自动选择每次迭代中使用的最佳求解器类型，包括纯基于结构、无结构或混合求解器","关键词":["相机姿态估计","结构从运动","混合求解器","RANSAC"],"涉及的技术概念":"2D-3D对应关系、2D-2D对应关系、结构从运动（SfM）、RANSAC、混合求解器"},{"order":15,"title":"A Certifiably Globally Optimal Solution to the Non-Minimal Relative Pose Problem","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Briales_A_Certifiably_Globally_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Briales_A_Certifiably_Globally_CVPR_2018_paper.html","abstract":"Finding the relative pose between two calibrated views ranks among the most fundamental geometric vision problems. It therefore appears as somewhat a surprise that a globally optimal solver that minimizes a properly defined energy over non-minimal correspondence sets and in the original space of relative transformations has yet to be discovered. This, notably, is the contribution of the present paper. We formulate the problem as a Quadratically Constrained Quadratic Program (QCQP), which can be converted into a Semidefinite Program (SDP) using Shor's convex relaxation. While a theoretical proof for the tightness of this relaxation remains open, we prove through exhaustive validation on both simulated and real experiments that our approach always finds and certifies (a-posteriori) the global optimum of the cost function.","中文标题":"非最小相对姿态问题的可证明全局最优解","摘要翻译":"寻找两个校准视图之间的相对姿态是最基本的几何视觉问题之一。因此，令人惊讶的是，迄今为止尚未发现一种全局最优求解器，它能在非最小对应集上最小化适当定义的能量，并在相对变换的原始空间中进行。这，值得注意的是，是本论文的贡献。我们将问题表述为二次约束二次规划（QCQP），可以使用Shor的凸松弛将其转换为半定规划（SDP）。虽然这种松弛的紧性理论证明仍然开放，但我们通过对模拟和真实实验的详尽验证证明，我们的方法总是能找到并（后验地）证明成本函数的全局最优解。","领域":"几何视觉/优化算法/相对姿态估计","问题":"寻找两个校准视图之间的相对姿态","动机":"尚未发现一种全局最优求解器，能在非最小对应集上最小化适当定义的能量，并在相对变换的原始空间中进行","方法":"将问题表述为二次约束二次规划（QCQP），并使用Shor的凸松弛将其转换为半定规划（SDP）","关键词":["几何视觉","优化算法","相对姿态估计"],"涉及的技术概念":{"相对姿态":"两个校准视图之间的相对位置和方向","二次约束二次规划（QCQP）":"一种优化问题，目标函数和约束都是二次的","半定规划（SDP）":"一种优化问题，涉及半定矩阵的约束","Shor的凸松弛":"一种将非凸优化问题转换为凸优化问题的方法"}},{"order":16,"title":"Single View Stereo Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Single_View_Stereo_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Single_View_Stereo_CVPR_2018_paper.html","abstract":"Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.","中文标题":"单视图立体匹配","摘要翻译":"之前的单目深度估计方法采用单一视图并直接回归预期结果。尽管最近的进展通过在训练期间应用几何启发的损失函数来实现，但推理过程并未明确施加任何几何约束。因此，这些模型完全依赖于数据的质量和学习的有效性来泛化。这要么导致次优结果，要么需要大量昂贵的真实标记数据来生成合理的结果。在本文中，我们首次展示了单目深度估计问题可以重新表述为两个子问题，即视图合成过程后跟立体匹配，具有两个引人入胜的特性，即i）在推理期间可以明确施加几何约束；ii）对标记深度数据的需求可以大大减轻。我们展示了整个管道仍然可以以端到端的方式进行训练，并且这种新的表述在提高性能方面起着关键作用。所得到的模型在仅使用少量真实训练数据的情况下，在具有挑战性的KITTI数据集中优于所有以前的单目深度估计方法以及立体块匹配方法。该模型也很好地泛化到其他单目深度估计基准。我们还讨论了使用立体方法解决单目深度估计的含义和优势。","领域":"深度估计/立体视觉/视图合成","问题":"单目深度估计的准确性和效率问题","动机":"减少对大量昂贵真实标记数据的依赖，提高深度估计的准确性和效率","方法":"将单目深度估计问题重新表述为视图合成和立体匹配两个子问题，并在推理期间明确施加几何约束","关键词":["深度估计","立体匹配","视图合成"],"涉及的技术概念":"单目深度估计指的是从单一图像中估计场景的深度信息。立体匹配是指从两个或多个视角的图像中寻找对应点以估计深度。视图合成是指从一个或多个视角的图像生成新的视角图像。几何约束是指在算法中利用几何原理来限制或指导深度估计的过程。"},{"order":17,"title":"Fight Ill-Posedness With Ill-Posedness: Single-Shot Variational Depth Super-Resolution From Shading","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Haefner_Fight_Ill-Posedness_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Haefner_Fight_Ill-Posedness_With_CVPR_2018_paper.html","abstract":"We put forward a principled variational approach for up-sampling a single depth map to the resolution of the companion color image provided by an RGB-D sensor. We combine heterogeneous depth and color data in order to jointly solve the ill-posed depth super-resolution and shape-from-shading problems. The low-frequency geometric information necessary to disambiguate shape-from-shading is extracted from the low-resolution depth measurements and, symmetrically, the high-resolution photometric clues in the RGB image provide the high-frequency information required to disambiguate depth super-resolution.","中文标题":"以病态解病态：基于单次变分的深度超分辨率从阴影中重建","摘要翻译":"我们提出了一种基于原则的变分方法，用于将单个深度图上采样到RGB-D传感器提供的伴生彩色图像的分辨率。我们结合了异质的深度和彩色数据，以共同解决病态的深度超分辨率和从阴影中重建形状的问题。从低分辨率深度测量中提取出必要的低频几何信息以消除从阴影中重建形状的歧义，同时，RGB图像中的高分辨率光度线索提供了消除深度超分辨率歧义所需的高频信息。","领域":"深度超分辨率/形状重建/光度立体视觉","问题":"解决深度图超分辨率和从阴影中重建形状的病态问题","动机":"为了提高深度图的分辨率并准确重建物体的形状，需要一种能够结合深度和彩色数据的方法来解决这两个病态问题。","方法":"采用变分方法，结合深度和彩色数据，共同解决深度超分辨率和从阴影中重建形状的问题。通过从低分辨率深度测量中提取低频几何信息，以及利用RGB图像中的高分辨率光度线索，来消除这两个问题的歧义。","关键词":["深度超分辨率","形状重建","光度立体视觉"],"涉及的技术概念":"变分方法是一种数学优化技术，用于寻找函数的最小值或最大值。深度超分辨率是指提高深度图的分辨率，使其与彩色图像的分辨率相匹配。从阴影中重建形状是一种通过分析物体表面的阴影来推断其三维形状的技术。光度立体视觉是一种利用不同光照条件下的图像来重建物体表面形状的技术。"},{"order":18,"title":"Deep Depth Completion of a Single RGB-D Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Depth_Completion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Depth_Completion_CVPR_2018_paper.html","abstract":"The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.","中文标题":"单一RGB-D图像的深度补全","摘要翻译":"我们工作的目标是补全RGB-D图像的深度通道。商品级深度相机通常无法感知闪亮、明亮、透明和远距离表面的深度。为了解决这个问题，我们训练了一个深度网络，该网络以RGB图像作为输入，并预测密集的表面法线和遮挡边界。然后，这些预测与RGB-D相机提供的原始深度观测相结合，以解决所有像素的深度问题，包括原始观测中缺失的那些。这种方法被选择而不是其他方法（例如，直接修复深度），是基于对一个新的深度补全基准数据集的广泛实验的结果，其中通过从多视图RGB-D扫描创建的表面重建的渲染来填充训练数据中的空洞。对不同网络输入、深度表示、损失函数、优化方法、修复方法和深度深度估计网络的实验表明，我们提出的方法提供了比这些替代方案更好的深度补全。","领域":"深度估计/三维重建/图像补全","问题":"商品级深度相机无法准确感知闪亮、明亮、透明和远距离表面的深度","动机":"为了解决RGB-D图像中深度信息缺失的问题，提高深度补全的准确性","方法":"训练一个深度网络，该网络以RGB图像作为输入，预测密集的表面法线和遮挡边界，然后与原始深度观测结合，解决所有像素的深度问题","关键词":["深度补全","RGB-D图像","表面法线","遮挡边界","深度网络"],"涉及的技术概念":"深度网络用于预测密集的表面法线和遮挡边界，这些预测与RGB-D相机提供的原始深度观测相结合，以解决所有像素的深度问题。这种方法通过从多视图RGB-D扫描创建的表面重建的渲染来填充训练数据中的空洞，从而提高了深度补全的准确性。"},{"order":19,"title":"Multi-View Harmonized Bilinear Network for 3D Object Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.html","abstract":"View-based methods have achieved considerable success in $3$D object recognition tasks.  Different from existing view-based methods pooling the view-wise features, we tackle this problem from the perspective of patches-to-patches similarity measurement. By exploiting the relationship between polynomial kernel and bilinear pooling, we obtain an effective $3$D object representation by aggregating local convolutional features through bilinear pooling. Meanwhile, we harmonize different components inherited in the pooled bilinear feature to obtain a more discriminative representation for a $3$D object. To achieve an end-to-end trainable framework, we incorporate the harmonized bilinear pooling  operation as a layer of a  network,  constituting the proposed Multi-view Harmonized Bilinear Network (MHBN). Systematic experiments conducted on two public benchmark datasets demonstrate the efficacy of the proposed methods in $3$D object recognition.","中文标题":"多视图协调双线性网络用于3D物体识别","摘要翻译":"基于视图的方法在3D物体识别任务中取得了相当大的成功。与现有的基于视图的方法不同，我们通过从局部到局部的相似性度量的角度来解决这个问题。通过利用多项式核和双线性池化之间的关系，我们通过双线性池化聚合局部卷积特征，获得了一个有效的3D物体表示。同时，我们协调了池化双线性特征中继承的不同组件，以获得更具区分性的3D物体表示。为了实现端到端的可训练框架，我们将协调的双线性池化操作作为网络的一层，构成了所提出的多视图协调双线性网络（MHBN）。在两个公共基准数据集上进行的系统实验证明了所提出方法在3D物体识别中的有效性。","领域":"3D物体识别/卷积神经网络/双线性池化","问题":"如何从多视图图像中有效地识别3D物体","动机":"现有的基于视图的方法在3D物体识别中虽然取得了一定成功，但通常通过视图特征池化来处理，缺乏对局部特征的深入利用和协调。","方法":"提出了一种多视图协调双线性网络（MHBN），通过双线性池化聚合局部卷积特征，并协调池化双线性特征中的不同组件，以实现更有效的3D物体识别。","关键词":["3D物体识别","双线性池化","多视图协调","卷积神经网络"],"涉及的技术概念":"双线性池化是一种用于聚合局部特征的技术，通过计算两个特征向量的外积来捕捉它们之间的相互作用。多项式核是一种用于计算特征之间相似性的函数，可以用于增强特征的表达能力。多视图协调双线性网络（MHBN）是一种端到端的深度学习框架，专门设计用于3D物体识别，通过协调不同视图的特征来提高识别的准确性。"},{"order":20,"title":"PPFNet: Global Context Aware Local Features for Robust 3D Point Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_PPFNet_Global_Context_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_PPFNet_Global_Context_CVPR_2018_paper.html","abstract":"We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel N-tuple loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.","中文标题":"PPFNet：全局上下文感知的局部特征用于鲁棒的三维点匹配","摘要翻译":"我们提出了PPFNet——点对特征网络，用于深度学习一个全局信息化的三维局部特征描述符，以在无组织的点云中找到对应关系。PPFNet在纯几何上学习局部描述符，并且高度意识到全局上下文，这是深度学习中的一个重要线索。我们的三维表示被计算为点对特征的集合，结合了局部区域内的点和法线。我们的排列不变网络设计灵感来源于PointNet，使PPFNet成为无顺序的。与体素化相反，我们的方法能够消耗原始点云以利用完全的稀疏性。PPFNet使用了一种新颖的N元组损失和架构，将全局信息自然地注入到局部描述符中。这表明上下文意识也提升了局部特征表示。我们网络的定性和定量评估表明，召回率提高，鲁棒性和不变性增强，以及在三维描述符提取性能上的重要一步。","领域":"三维点云处理/深度学习/特征提取","问题":"在无组织的点云中找到对应关系","动机":"提高三维点云中局部特征描述符的鲁棒性和不变性，以及提升三维描述符提取性能","方法":"提出PPFNet，一种基于点对特征的网络，通过深度学习全局信息化的三维局部特征描述符，利用原始点云的稀疏性，采用N-tuple损失和架构将全局信息注入局部描述符","关键词":["三维点云","局部特征描述符","全局上下文","点对特征","N-tuple损失"],"涉及的技术概念":{"点对特征":"一种用于描述三维点云中点之间关系的特征","排列不变网络设计":"一种网络设计，使得网络对输入点的顺序不敏感","N-tuple损失":"一种损失函数，用于训练网络以同时考虑多个样本之间的关系","体素化":"一种将三维空间分割成固定大小的小立方体（体素）的方法，用于处理三维数据"}},{"order":21,"title":"FoldingNet: Point Cloud Auto-Encoder via Deep Grid Deformation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_FoldingNet_Point_Cloud_CVPR_2018_paper.html","abstract":"Recent deep networks that directly handle points in a point set, e.g., PointNet, have been state-of-the-art for supervised learning tasks on point clouds such as classification and segmentation. In this work, a novel end-to-end deep auto-encoder is proposed to address unsupervised learning challenges on point clouds. On the encoder side, a graph-based enhancement is enforced to promote local structures on top of PointNet. Then, a novel folding-based decoder deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud, achieving low reconstruction errors even for objects with delicate structures. The proposed decoder only uses about 7% parameters of a decoder with fully-connected neural networks, yet leads to a more discriminative representation that achieves higher linear SVM classification accuracy than the benchmark. In addition, the proposed decoder structure is shown, in theory, to be a generic architecture that is able to reconstruct an arbitrary point cloud from a 2D grid. Our code is available at http://www.merl.com/research/license#FoldingNet","中文标题":"FoldingNet: 通过深度网格变形的点云自编码器","摘要翻译":"最近，直接处理点集中点的深度网络，例如PointNet，在点云的监督学习任务（如分类和分割）上已经达到了最先进的水平。在这项工作中，提出了一种新颖的端到端深度自编码器，以解决点云上的无监督学习挑战。在编码器方面，基于图的增强被强制执行，以在PointNet之上促进局部结构。然后，一种新颖的基于折叠的解码器将一个规范的2D网格变形到点云的底层3D对象表面上，即使对于具有精细结构的对象也能实现低重建误差。所提出的解码器仅使用全连接神经网络解码器约7%的参数，但却导致了一个更具区分性的表示，实现了比基准更高的线性SVM分类准确率。此外，所提出的解码器结构在理论上被证明是一种通用架构，能够从2D网格重建任意点云。我们的代码可在http://www.merl.com/research/license#FoldingNet获取。","领域":"点云处理/3D重建/无监督学习","问题":"解决点云上的无监督学习挑战","动机":"为了在点云的无监督学习任务中实现更高的准确率和更低的参数使用","方法":"提出了一种新颖的端到端深度自编码器，包括基于图的编码器增强和基于折叠的解码器，后者通过将2D网格变形到3D对象表面来实现低重建误差","关键词":["点云","自编码器","无监督学习","3D重建","网格变形"],"涉及的技术概念":{"PointNet":"一种直接处理点集中点的深度网络，用于点云的分类和分割任务","自编码器":"一种神经网络，用于无监督学习，通过编码和解码过程学习数据的有效表示","基于图的增强":"在编码器中使用图结构来增强局部结构的表示","基于折叠的解码器":"一种新颖的解码器，通过将2D网格变形到3D对象表面来实现点云的重建","线性SVM分类":"一种使用线性支持向量机进行分类的方法，用于评估解码器生成的表示的区分性"}},{"order":22,"title":"A Papier-Mâché Approach to Learning 3D Surface Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Groueix_A_Papier-Mache_Approach_CVPR_2018_paper.html","abstract":"We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues.  We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes,  and (ii) single-view reconstruction from a still image.  We also provide results showing its potentialfor other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.","中文标题":"一种学习3D表面生成的纸浆方法","摘要翻译":"我们介绍了一种学习生成3D形状表面的方法。我们的方法将3D形状表示为参数化表面元素的集合，与生成体素网格或点云的方法相比，自然地推断出形状的表面表示。除了其新颖性之外，我们的新形状生成框架AtlasNet具有显著优势，如提高的精度和泛化能力，以及生成任意分辨率形状而不会出现内存问题的可能性。我们在ShapeNet基准上展示了这些优势，并与强基线进行了比较，应用于两个应用：(i)形状的自动编码，和(ii)从静止图像的单视图重建。我们还提供了显示其在其他应用中的潜力的结果，如变形、参数化、超分辨率、匹配和共同分割。","领域":"3D形状生成/表面重建/形状编码","问题":"如何有效地生成3D形状的表面表示","动机":"探索一种新的3D形状表面生成方法，以提高精度和泛化能力，同时解决内存问题","方法":"采用参数化表面元素集合表示3D形状，自然推断表面表示","关键词":["3D形状生成","表面重建","形状编码","参数化表面","AtlasNet"],"涉及的技术概念":"AtlasNet框架通过将3D形状表示为参数化表面元素的集合，自然地推断出形状的表面表示，这种方法与传统的体素网格或点云生成方法不同，能够提高精度和泛化能力，并且可以生成任意分辨率的形状而不会遇到内存问题。"},{"order":23,"title":"LEGO: Learning Edge With Geometry All at Once by Watching Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_LEGO_Learning_Edge_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_LEGO_Learning_Edge_CVPR_2018_paper.html","abstract":"Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network is attracting significant attention. In this paper, we introduce a “3D as-smooth-as-possible (3D-ASAP)” prior inside the pipeline, which enables joint estimation of edges and 3D scene, yielding results with significant improvement in accuracy for fine detailed structures.      Specifically, we define the 3D-ASAP prior by requiring that any two points recovered in 3D from an image should lie on an existing planar surface if no other cues provided. We design an unsupervised framework that Learns Edges and Geometry (depth, normal) all at Once (LEGO).     The predicted edges are embedded into depth and surface normal smoothness terms, where pixels without edges in-between are constrained to satisfy the prior. In our framework, the predicted depths, normals and edges are forced to be consistent all the time.     We conduct experiments on KITTI to evaluate our estimated geometry and CityScapes to perform edge evaluation. We show that in all of the tasks, i.e. depth, normal and edge, our algorithm vastly outperforms other state-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.","中文标题":"LEGO：通过观看视频一次性学习边缘与几何","摘要翻译":"通过观看未标记的视频，利用深度卷积网络学习从单张图像估计3D几何的方法正引起广泛关注。在本文中，我们在流程中引入了“尽可能平滑的3D（3D-ASAP）”先验，这使得边缘和3D场景的联合估计成为可能，从而在精细结构的准确性上取得了显著提升。具体来说，我们通过要求在没有其他线索的情况下，从图像中恢复的3D中的任何两点都应位于现有的平面表面上，来定义3D-ASAP先验。我们设计了一个无监督框架，该框架一次性学习边缘和几何（深度、法线）（LEGO）。预测的边缘被嵌入到深度和表面法线平滑项中，其中没有边缘的像素被约束以满足先验。在我们的框架中，预测的深度、法线和边缘始终被强制保持一致。我们在KITTI上进行实验以评估我们估计的几何，并在CityScapes上进行边缘评估。我们展示了在所有任务中，即深度、法线和边缘，我们的算法大大优于其他最先进的（SOTA）算法，证明了我们方法的优势。","领域":"3D重建/边缘检测/无监督学习","问题":"从单张图像估计3D几何","动机":"提高从单张图像估计3D几何的准确性，特别是在精细结构上","方法":"引入3D-ASAP先验，设计一个无监督框架LEGO，联合估计边缘和3D场景，通过深度和表面法线平滑项嵌入预测的边缘","关键词":["3D重建","边缘检测","无监督学习"],"涉及的技术概念":"3D-ASAP先验是一种要求在没有其他线索的情况下，从图像中恢复的3D中的任何两点都应位于现有的平面表面上的先验。LEGO框架是一个无监督学习框架，旨在一次性学习边缘和几何（深度、法线），通过将预测的边缘嵌入到深度和表面法线平滑项中，确保预测的深度、法线和边缘的一致性。"},{"order":24,"title":"Five-Point Fundamental Matrix Estimation for Uncalibrated Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Barath_Five-Point_Fundamental_Matrix_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Barath_Five-Point_Fundamental_Matrix_CVPR_2018_paper.html","abstract":"We aim at estimating the fundamental matrix in two views from five correspondences of rotation invariant features obtained by e.g. the SIFT detector. The proposed minimal solver first estimates a homography from three correspondences assuming that they are co-planar and exploiting their rotational components. Then the fundamental matrix is obtained from the homography and two additional point pairs in general position. The proposed approach, combined with robust estimators like Graph-Cut RANSAC, is superior to other state-of-the-art algorithms both in terms of accuracy and number of iterations required. This is validated on synthesized data and 561 real image pairs. Moreover, the tests show that requiring three points on a plane is not too restrictive in urban environment and locally optimized robust estimators lead to accurate estimates even if the points are not entirely co-planar. As a potential application, we show that using the proposed method makes two-view multi-motion estimation more accurate.","中文标题":"未校准相机的五点基本矩阵估计","摘要翻译":"我们的目标是从通过SIFT检测器等获得的旋转不变特征的五个对应关系中估计两视图中的基本矩阵。提出的最小求解器首先假设三个对应点是共面的，并利用它们的旋转分量来估计单应性。然后，从单应性和另外两个一般位置的点对中获得基本矩阵。所提出的方法与Graph-Cut RANSAC等鲁棒估计器相结合，在准确性和所需迭代次数方面均优于其他最先进的算法。这在合成数据和561对真实图像对上得到了验证。此外，测试表明，在城市环境中要求三个点在一个平面上并不太限制，并且即使点不完全共面，局部优化的鲁棒估计器也能导致准确的估计。作为一个潜在的应用，我们展示了使用所提出的方法可以使两视图多运动估计更加准确。","领域":"三维重建/运动估计/特征匹配","问题":"从两视图中的五个对应关系中估计基本矩阵","动机":"提高两视图多运动估计的准确性","方法":"首先估计三个对应点的单应性，然后从单应性和另外两个点对中获得基本矩阵，结合Graph-Cut RANSAC等鲁棒估计器","关键词":["基本矩阵","单应性","SIFT检测器","Graph-Cut RANSAC","多运动估计"],"涉及的技术概念":"基本矩阵是描述两视图间几何关系的关键矩阵，单应性是指两个平面之间的映射关系，SIFT检测器用于检测图像中的关键点，Graph-Cut RANSAC是一种鲁棒估计方法，用于在存在噪声和异常值的情况下估计模型参数。"},{"order":25,"title":"PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.html","abstract":"We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors.  We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform on par or better than the state-of-the-art on these diverse datasets without any dataset-specific model tuning.","中文标题":"PointFusion: 用于3D边界框估计的深度传感器融合","摘要翻译":"我们提出了PointFusion，一种通用的3D物体检测方法，它利用了图像和3D点云信息。与现有的要么使用多阶段流水线要么持有传感器和数据集特定假设的方法不同，PointFusion在概念上简单且与应用无关。图像数据和原始点云数据分别由CNN和PointNet架构独立处理。然后，通过一种新颖的融合网络将结果输出结合起来，该网络使用输入的3D点作为空间锚点，预测多个3D框假设及其置信度。我们在两个独特的数据集上评估了PointFusion：KITTI数据集，其特点是使用激光雷达-相机设置捕捉的驾驶场景，以及SUN-RGBD数据集，它使用RGB-D相机捕捉室内环境。我们的模型是第一个能够在这些多样化的数据集上无需任何数据集特定的模型调整就能达到或超过最先进水平的模型。","领域":"3D物体检测/传感器融合/自动驾驶","问题":"如何有效地结合图像和3D点云信息进行3D物体检测","动机":"现有的3D物体检测方法要么复杂，要么依赖于特定的传感器和数据集假设，限制了其通用性和应用范围","方法":"使用CNN处理图像数据，PointNet处理点云数据，然后通过一种新颖的融合网络结合这两种数据，预测3D框假设及其置信度","关键词":["3D物体检测","传感器融合","自动驾驶"],"涉及的技术概念":"CNN（卷积神经网络）用于处理图像数据，PointNet用于处理3D点云数据，融合网络用于结合图像和点云数据以预测3D边界框。"},{"order":26,"title":"Scalable Dense Non-Rigid Structure-From-Motion: A Grassmannian Perspective","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kumar_Scalable_Dense_Non-Rigid_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kumar_Scalable_Dense_Non-Rigid_CVPR_2018_paper.html","abstract":"This paper addresses the task of dense non-rigid structure-from-motion (NRSfM) using multiple images. State-of-the-art methods to this problem are often hurdled by scalability, expensive computations, and noisy measurements. Further, recent methods to NRSfM usually either assume a small number of sparse feature points or ignore local non-linearities of shape deformations, and thus cannot reliably model complex non-rigid deformations. To address these issues, in this paper, we propose a new approach for dense NRSfM by modeling the problem on a Grassmann manifold. Specifically, we assume the complex non-rigid deformations lie on a union of local linear subspaces both spatially and temporally. This naturally allows for a compact representation of the complex non-rigid deformation over frames. We provide experimental results on several synthetic and real benchmark datasets. The procured results clearly demonstrate that our method, apart from being scalable and more accurate than state-of-the-art methods, is also more robust to noise and generalizes to highly non-linear deformations.","中文标题":"可扩展的密集非刚性结构从运动中恢复：格拉斯曼视角","摘要翻译":"本文探讨了使用多幅图像进行密集非刚性结构从运动中恢复（NRSfM）的任务。针对这一问题的现有最先进方法常常受到可扩展性、昂贵的计算成本和噪声测量的限制。此外，最近的NRSfM方法通常要么假设少量稀疏特征点，要么忽略形状变形的局部非线性，因此无法可靠地建模复杂的非刚性变形。为了解决这些问题，本文提出了一种新的密集NRSfM方法，通过在格拉斯曼流形上建模问题。具体来说，我们假设复杂的非刚性变形在空间和时间上都位于局部线性子空间的并集上。这自然允许对帧间复杂非刚性变形进行紧凑表示。我们在几个合成和真实基准数据集上提供了实验结果。获得的结果清楚地表明，我们的方法除了比现有最先进方法更具可扩展性和准确性外，还对噪声更具鲁棒性，并能推广到高度非线性的变形。","领域":"非刚性结构恢复/格拉斯曼流形/密集重建","问题":"解决密集非刚性结构从运动中恢复的可扩展性、计算成本和噪声问题","动机":"现有方法在可扩展性、计算成本和噪声处理方面存在限制，且无法可靠建模复杂非刚性变形","方法":"通过在格拉斯曼流形上建模，假设复杂非刚性变形位于局部线性子空间的并集上，实现对帧间复杂非刚性变形的紧凑表示","关键词":["非刚性结构恢复","格拉斯曼流形","密集重建","噪声鲁棒性","非线性变形"],"涉及的技术概念":"格拉斯曼流形是一种数学概念，用于描述高维空间中的子空间集合。在本文中，它被用来建模复杂的非刚性变形，通过假设这些变形位于局部线性子空间的并集上，从而实现对变形的紧凑表示。这种方法有助于提高算法的可扩展性、准确性和对噪声的鲁棒性。"},{"order":27,"title":"GVCNN: Group-View Convolutional Neural Networks for 3D Shape Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Feng_GVCNN_Group-View_Convolutional_CVPR_2018_paper.html","abstract":"3D shape recognition has attracted much attention recently. Its recent advances advocate the usage of deep features and achieve the state-of-the-art performance. However, existing deep features for 3D shape recognition are restricted to a view-to-shape setting, which learns the shape descriptor from the view-level feature directly. Despite the exciting progress on view-based 3D shape description, the intrinsic hierarchical correlation and discriminability among views have not been well exploited, which is important for 3D shape representation. To tackle this issue, in this paper, we propose a group-view convolutional neural network (GVCNN) framework for hierarchical correlation modeling towards discriminative 3D shape description. The proposed GVCNN framework is composed of a hierarchical view-group-shape architecture, i.e., from the view level, the group level and the shape level, which are organized using a grouping strategy. Concretely, we first use an expanded CNN to extract a view level descriptor. Then, a grouping module is introduced to estimate the content discrimination of each view, based on which all views can be splitted into different groups according to their discriminative level. A group level description can be further generated by pooling from view descriptors. Finally, all group level descriptors are combined into the shape level descriptor according to their discriminative weights. Experimental results and comparison with state-of-the-art methods show that our proposed GVCNN method can achieve a significant performance gain on both the 3D shape classification and retrieval tasks.","中文标题":"GVCNN：用于3D形状识别的组视图卷积神经网络","摘要翻译":"3D形状识别最近引起了广泛关注。其最新进展提倡使用深度特征，并实现了最先进的性能。然而，现有的3D形状识别深度特征仅限于视图到形状的设置，即直接从视图级特征学习形状描述符。尽管基于视图的3D形状描述取得了令人兴奋的进展，但视图之间的内在层次相关性和区分性尚未得到充分利用，这对于3D形状表示非常重要。为了解决这个问题，本文提出了一种组视图卷积神经网络（GVCNN）框架，用于层次相关性建模，以实现区分性的3D形状描述。提出的GVCNN框架由层次视图-组-形状架构组成，即从视图级别、组级别到形状级别，这些级别通过分组策略组织。具体来说，我们首先使用扩展的CNN提取视图级描述符。然后，引入分组模块来估计每个视图的内容区分度，基于此，所有视图可以根据其区分度水平被分成不同的组。通过从视图描述符中池化，可以进一步生成组级描述。最后，所有组级描述符根据其区分权重组合成形状级描述符。实验结果与最先进方法的比较表明，我们提出的GVCNN方法在3D形状分类和检索任务上都能实现显著的性能提升。","领域":"3D形状识别/卷积神经网络/深度学习","问题":"现有3D形状识别方法未能充分利用视图间的层次相关性和区分性","动机":"为了提升3D形状描述的区分性，充分利用视图间的层次相关性","方法":"提出了一种组视图卷积神经网络（GVCNN）框架，通过层次视图-组-形状架构，利用分组策略组织视图，提取视图级描述符，估计视图内容区分度，生成组级描述，并最终组合成形状级描述符","关键词":["3D形状识别","卷积神经网络","层次相关性建模","视图分组","形状描述符"],"涉及的技术概念":"GVCNN框架通过扩展的CNN提取视图级描述符，引入分组模块估计视图内容区分度，通过池化生成组级描述，并根据区分权重组合成形状级描述符，以实现区分性的3D形状描述。"},{"order":28,"title":"Depth and Transient Imaging With Compressive SPAD Array Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Depth_and_Transient_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Depth_and_Transient_CVPR_2018_paper.html","abstract":"Time-of-flight depth imaging and transient imaging are two imaging modalities that have recently received a lot of interest. Despite much research, existing hardware systems are limited either in terms of temporal resolution or are prohibitively expensive. Arrays of Single Photon Avalanche Diodes (SPADs) promise to fill this gap by providing higher temporal resolution at an affordable cost. Unfortunately SPAD arrays are to date only available in relatively small resolutions. In this work we aim to overcome the spatial resolution limit of SPAD arrays by employing a compressive sensing camera design. Using a DMD and custom optics, we achieve an image resolution of up to 800*400 on SPAD Arrays of resolution 64*32. Using our new data fitting model for the time histograms, we suppress the noise while abstracting the phase and amplitude information, so as to realize a temporal resolution of a few tens of picoseconds.","中文标题":"使用压缩SPAD阵列相机进行深度和瞬态成像","摘要翻译":"飞行时间深度成像和瞬态成像是最近受到广泛关注的两种成像模式。尽管已有大量研究，现有的硬件系统要么在时间分辨率上有限，要么成本过高。单光子雪崩二极管（SPAD）阵列有望以可承受的成本提供更高的时间分辨率来填补这一空白。不幸的是，迄今为止，SPAD阵列仅在相对较小的分辨率下可用。在这项工作中，我们旨在通过采用压缩感知相机设计来克服SPAD阵列的空间分辨率限制。使用DMD和定制光学元件，我们在64*32分辨率的SPAD阵列上实现了高达800*400的图像分辨率。使用我们新的时间直方图数据拟合模型，我们在提取相位和振幅信息的同时抑制了噪声，从而实现了数十皮秒的时间分辨率。","领域":"深度成像/瞬态成像/压缩感知","问题":"现有硬件系统在时间分辨率上的限制和高成本问题","动机":"通过使用SPAD阵列以可承受的成本提供更高的时间分辨率，克服现有硬件系统的限制","方法":"采用压缩感知相机设计，使用DMD和定制光学元件提高SPAD阵列的图像分辨率，并开发新的时间直方图数据拟合模型以抑制噪声并提取相位和振幅信息","关键词":["深度成像","瞬态成像","压缩感知","SPAD阵列","时间分辨率"],"涉及的技术概念":"飞行时间深度成像、瞬态成像、单光子雪崩二极管（SPAD）阵列、压缩感知、数字微镜器件（DMD）、时间直方图数据拟合模型"},{"order":29,"title":"GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_GeoNet_Geometric_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_GeoNet_Geometric_Neural_CVPR_2018_paper.html","abstract":"In this paper, we propose Geometric Neural Network (GeoNet) to jointly predict depth and surface normal maps from a single image. Building on top of two-stream CNNs, our GeoNet incorporates geometric relation between depth and surface normal via the new depth-to-normal and normal- to-depth networks. Depth-to-normal network exploits the least square solution of surface normal from depth and im- proves its quality with a residual module. Normal-to-depth network, contrarily, refines the depth map based on the con- straints from the surface normal through a kernel regression module, which has no parameter to learn. These two net- works enforce the underlying model to efficiently predict depth and surface normal for high consistency and corre- sponding accuracy. Our experiments on NYU v2 dataset verify that our GeoNet is able to predict geometrically con- sistent depth and normal maps. It achieves top performance on surface normal estimation and is on par with state-of-the- art depth estimation methods.","中文标题":"GeoNet: 用于联合深度和表面法线估计的几何神经网络","摘要翻译":"在本文中，我们提出了几何神经网络（GeoNet），用于从单一图像中联合预测深度和表面法线图。基于双流CNN的基础上，我们的GeoNet通过新的深度到法线和法线到深度网络，将深度和表面法线之间的几何关系结合起来。深度到法线网络利用最小二乘法从深度中求解表面法线，并通过残差模块提高其质量。相反，法线到深度网络基于表面法线的约束，通过一个无需学习参数的内核回归模块来细化深度图。这两个网络强制底层模型高效地预测深度和表面法线，以实现高度一致性和相应的准确性。我们在NYU v2数据集上的实验验证了我们的GeoNet能够预测几何上一致的深度和法线图。它在表面法线估计上达到了顶级性能，并与最先进的深度估计方法相当。","领域":"三维重建/几何学习/深度估计","问题":"从单一图像中联合预测深度和表面法线图","动机":"提高深度和表面法线预测的几何一致性和准确性","方法":"提出了几何神经网络（GeoNet），结合深度到法线和法线到深度网络，利用最小二乘法和内核回归模块来提高预测质量","关键词":["几何神经网络","深度估计","表面法线估计","双流CNN","内核回归"],"涉及的技术概念":"GeoNet是一种几何神经网络，用于从单一图像中联合预测深度和表面法线图。它基于双流CNN，通过深度到法线和法线到深度网络结合深度和表面法线之间的几何关系。深度到法线网络利用最小二乘法从深度中求解表面法线，并通过残差模块提高其质量。法线到深度网络基于表面法线的约束，通过一个无需学习参数的内核回归模块来细化深度图。"},{"order":30,"title":"Real-Time Seamless Single Shot 6D Object Pose Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tekin_Real-Time_Seamless_Single_CVPR_2018_paper.html","abstract":"We propose a single-shot approach for simultaneously detecting an object in an RGB image and predicting its 6D pose without requiring multiple stages or having to examine multiple hypotheses. Unlike a recently proposed single-shot technique for this task [Kehl et al. 2017] that only predicts an approximate 6D pose that must then be refined, ours is accurate enough not to require additional post-processing. As a  result, it is much faster - 50 fps on a Titan X (Pascal) GPU - and more suitable for real-time processing. The key component of our method is a new CNN architecture inspired by [Redmon et al. 2016, Redmon and Farhadi 2017] that directly predicts the 2D image locations of the projected vertices of the object's 3D bounding box. The object's 6D pose is then estimated using a PnP algorithm.   For single object and multiple object pose estimation on the LineMod and Occlusion datasets, our approach substantially outperforms other recent CNN-based approaches [Kehl et al. 2017, Rad and Lepetit 2017] when they are all used without post-processing. During post-processing, a pose refinement step can be used to boost the accuracy of these two methods, but at 10 fps or less, they are much slower than our method.","中文标题":"实时无缝单次6D物体姿态预测","摘要翻译":"我们提出了一种单次拍摄的方法，用于同时检测RGB图像中的物体并预测其6D姿态，而无需多个阶段或检查多个假设。与最近提出的仅预测近似6D姿态然后需要精炼的单次拍摄技术[Kehl et al. 2017]不同，我们的方法足够准确，不需要额外的后处理。因此，它更快——在Titan X (Pascal) GPU上达到50 fps——更适合实时处理。我们方法的关键组成部分是一个新的CNN架构，灵感来源于[Redmon et al. 2016, Redmon and Farhadi 2017]，它直接预测物体3D边界框投影顶点的2D图像位置。然后使用PnP算法估计物体的6D姿态。对于LineMod和Occlusion数据集上的单物体和多物体姿态估计，当所有这些方法都不使用后处理时，我们的方法显著优于其他最近的基于CNN的方法[Kehl et al. 2017, Rad and Lepetit 2017]。在后处理过程中，可以使用姿态精炼步骤来提高这两种方法的准确性，但在10 fps或更低的速度下，它们比我们的方法慢得多。","领域":"物体检测/姿态估计/实时处理","问题":"在RGB图像中同时检测物体并预测其6D姿态","动机":"提高6D物体姿态预测的准确性和速度，以适应实时处理的需求","方法":"提出了一种新的CNN架构，直接预测物体3D边界框投影顶点的2D图像位置，并使用PnP算法估计物体的6D姿态","关键词":["6D姿态预测","单次拍摄","实时处理"],"涉及的技术概念":"CNN架构用于直接预测物体3D边界框投影顶点的2D图像位置，PnP算法用于估计物体的6D姿态"},{"order":31,"title":"Factoring Shape, Pose, and Layout From the 2D Image of a 3D Scene","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulsiani_Factoring_Shape_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tulsiani_Factoring_Shape_Pose_CVPR_2018_paper.html","abstract":"The goal of this paper is to take a single 2D image of a scene and recover the 3D structure in terms of a small set of factors: a layout representing the enclosing surfaces as well as a set of objects represented in terms of shape and pose. We propose a convolutional neural network-based approach to predict this representation and benchmark it on a large dataset of indoor scenes. Our experiments evaluate a number of practical design questions, demonstrate that we can infer this representation, and quantitatively and qualitatively demonstrate its merits compared to alternate representations.","中文标题":"从3D场景的2D图像中分解形状、姿态和布局","摘要翻译":"本文的目标是从场景的单个2D图像中恢复3D结构，以一组小因素表示：代表封闭表面的布局以及以形状和姿态表示的一组对象。我们提出了一种基于卷积神经网络的方法来预测这种表示，并在一个大型室内场景数据集上进行了基准测试。我们的实验评估了许多实际设计问题，证明我们可以推断出这种表示，并通过定量和定性分析展示了其与替代表示相比的优势。","领域":"三维重建/室内场景理解/卷积神经网络","问题":"从单个2D图像中恢复3D场景的结构，包括布局、形状和姿态","动机":"为了从单个2D图像中更准确地理解和重建3D场景，需要一种能够分解和表示场景中关键因素（如布局、形状和姿态）的方法。","方法":"提出了一种基于卷积神经网络的方法，用于预测3D场景的表示，并在大型室内场景数据集上进行了基准测试。","关键词":["三维重建","室内场景理解","卷积神经网络"],"涉及的技术概念":"卷积神经网络（CNN）用于从2D图像中预测3D场景的表示，包括布局、形状和姿态。这种方法在室内场景数据集上进行了测试，以评估其有效性和优势。"},{"order":32,"title":"Monocular Relative Depth Perception With Web Stereo Data Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_Monocular_Relative_Depth_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_Monocular_Relative_Depth_CVPR_2018_paper.html","abstract":"In this paper we study the problem of monocular relative depth perception in the wild. We introduce a simple yet effective method to automatically generate dense relative depth annotations from web stereo images, and propose a new dataset that consists of diverse images as well as corresponding dense relative depth maps. Further, an improved ranking loss is introduced to deal with imbalanced ordinal relations, enforcing the network to focus on a set of hard pairs. Experimental results demonstrate that our proposed approach not only achieves state-of-the-art accuracy of relative depth perception in the wild, but also benefits other dense per-pixel prediction tasks, e.g., metric depth estimation and semantic segmentation.","中文标题":"单目相对深度感知与网络立体数据监督","摘要翻译":"本文研究了在自然环境中单目相对深度感知的问题。我们引入了一种简单而有效的方法，从网络立体图像中自动生成密集的相对深度注释，并提出了一个新的数据集，该数据集包含多样化的图像以及相应的密集相对深度图。此外，引入了一种改进的排序损失来处理不平衡的序数关系，强制网络关注一组困难对。实验结果表明，我们提出的方法不仅在自然环境中实现了最先进的相对深度感知精度，而且还有益于其他密集像素级预测任务，例如度量深度估计和语义分割。","领域":"深度感知/立体视觉/图像注释","问题":"在自然环境中实现单目相对深度感知","动机":"提高单目相对深度感知的精度，并生成密集的相对深度注释","方法":"从网络立体图像中自动生成密集的相对深度注释，引入改进的排序损失处理不平衡的序数关系","关键词":["单目视觉","相对深度","立体图像","密集注释","排序损失"],"涉及的技术概念":"单目相对深度感知是指在仅使用一个摄像头的情况下，估计场景中物体之间的相对深度。网络立体图像指的是通过互联网获取的立体图像对，这些图像对可以用来生成深度信息。密集相对深度注释是指为图像中的每个像素提供相对深度信息。改进的排序损失是一种用于处理不平衡数据的学习策略，特别是在处理序数关系时，它可以帮助网络更好地学习困难样本。"},{"order":33,"title":"Spline Error Weighting for Robust Visual-Inertial Fusion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ovren_Spline_Error_Weighting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ovren_Spline_Error_Weighting_CVPR_2018_paper.html","abstract":"In this paper we derive and test a probability-based weighting that can balance residuals of different types in spline fitting. In contrast to previous formulations, the proposed spline error weighting scheme also incorporates a prediction of the approximation error of the spline fit. We demonstrate the effectiveness of the prediction in a synthetic experiment, and apply it to visual-inertial fusion on rolling shutter cameras. This results in a method that can estimate 3D structure with metric scale on generic first-person videos. We also propose a quality measure for spline fitting, that can be used to  automatically select the knot spacing. Experiments verify that the obtained trajectory quality corresponds well with the requested quality. Finally, by linearly scaling the weights, we show that the proposed spline error weighting minimizes the estimation errors on real sequences, in terms of scale and end-point errors.","中文标题":"用于鲁棒视觉-惯性融合的样条误差加权","摘要翻译":"在本文中，我们推导并测试了一种基于概率的加权方法，该方法可以在样条拟合中平衡不同类型的残差。与之前的公式相比，所提出的样条误差加权方案还结合了样条拟合近似误差的预测。我们在合成实验中证明了预测的有效性，并将其应用于滚动快门相机的视觉-惯性融合。这导致了一种可以在通用第一人称视频上估计具有度量尺度的3D结构的方法。我们还提出了一个用于样条拟合的质量度量，可用于自动选择节点间距。实验验证了所获得的轨迹质量与所请求的质量很好地对应。最后，通过线性缩放权重，我们展示了所提出的样条误差加权在真实序列上最小化了估计误差，包括尺度和端点误差。","领域":"视觉-惯性融合/样条拟合/3D结构估计","问题":"如何在视觉-惯性融合中平衡不同类型的残差并最小化估计误差","动机":"提高视觉-惯性融合中样条拟合的准确性和鲁棒性，特别是在滚动快门相机和第一人称视频中的应用","方法":"提出了一种基于概率的样条误差加权方案，该方案结合了样条拟合近似误差的预测，并提出了一个用于自动选择节点间距的质量度量","关键词":["样条误差加权","视觉-惯性融合","滚动快门相机","3D结构估计","质量度量"],"涉及的技术概念":"样条拟合是一种数学方法，用于通过一系列控制点（节点）来近似或插值数据。视觉-惯性融合结合了视觉信息和惯性测量单元（IMU）数据，以提高定位和映射的准确性。滚动快门相机是一种图像传感器，其曝光方式导致图像中的运动物体出现扭曲。3D结构估计涉及从2D图像或视频中恢复三维场景的几何形状。质量度量用于评估样条拟合的准确性，以便自动调整节点间距，从而优化拟合结果。"},{"order":34,"title":"Single-Image Depth Estimation Based on Fourier Domain Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Single-Image_Depth_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Single-Image_Depth_Estimation_CVPR_2018_paper.html","abstract":"We propose a deep learning algorithm for single-image depth estimation based on the Fourier frequency domain analysis. First, we develop a convolutional neural network structure and propose a new loss function, called depth-balanced Euclidean loss, to train the network reliably for a wide range of depths. Then, we generate multiple depth map candidates by cropping input images with various cropping ratios. In general, a cropped image with a small ratio yields depth details more faithfully, while that with a large ratio provides the overall depth distribution more reliably. To take advantage of these complementary properties, we combine the multiple candidates in the frequency domain. Experimental results demonstrate that proposed algorithm provides the state-of-art performance. Furthermore, through the frequency domain analysis, we validate the efficacy of the proposed algorithm in most frequency bands.","中文标题":"基于傅里叶域分析的单图像深度估计","摘要翻译":"我们提出了一种基于傅里叶频域分析的深度学习算法，用于单图像深度估计。首先，我们开发了一种卷积神经网络结构，并提出了一种新的损失函数，称为深度平衡欧几里得损失，以可靠地训练网络以适应广泛的深度范围。然后，我们通过以各种裁剪比例裁剪输入图像来生成多个深度图候选。通常，裁剪比例小的图像能更忠实地呈现深度细节，而裁剪比例大的图像则能更可靠地提供整体深度分布。为了利用这些互补特性，我们在频域中结合了多个候选。实验结果表明，所提出的算法提供了最先进的性能。此外，通过频域分析，我们验证了所提出算法在大多数频带中的有效性。","领域":"深度估计/傅里叶分析/卷积神经网络","问题":"单图像深度估计","动机":"提高单图像深度估计的准确性和可靠性","方法":"开发了一种卷积神经网络结构，并提出了一种新的损失函数（深度平衡欧几里得损失），通过裁剪输入图像生成多个深度图候选，并在频域中结合这些候选","关键词":["深度估计","傅里叶分析","卷积神经网络","深度平衡欧几里得损失","频域分析"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。傅里叶分析是一种将信号从时间域转换到频率域的数学工具，用于分析信号的频率成分。深度平衡欧几里得损失是一种新的损失函数，旨在提高深度估计的准确性和可靠性。频域分析是指对信号在频率域中的特性进行分析，以提取有用的信息。"},{"order":35,"title":"Unsupervised Learning of Monocular Depth Estimation and Visual Odometry With Deep Feature Reconstruction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.html","abstract":"Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat.","中文标题":"无监督学习的单目深度估计与视觉里程计与深度特征重建","摘要翻译":"尽管基于学习的方法在单视图深度估计和视觉里程计中显示出有希望的结果，但大多数现有方法以监督方式处理这些任务。最近关于单视图深度估计的方法探索了通过最小化光度误差来在没有完全监督的情况下学习的可能性。在本文中，我们探索了使用立体序列来学习深度和视觉里程计。使用立体序列使得可以利用空间（左右对之间）和时间（前后）光度扭曲误差，并将场景深度和相机运动约束在一个共同的、现实世界的尺度上。在测试时，我们的框架能够从单目序列中估计单视图深度和双视图里程计。我们还展示了如何通过考虑深度特征的扭曲来改进标准光度扭曲损失。我们通过大量实验表明：（i）联合训练单视图深度和视觉里程计由于对深度施加了额外的约束而改善了深度预测，并在视觉里程计中取得了竞争性的结果；（ii）基于深度特征的扭曲损失在单视图深度估计和视觉里程计中都优于简单的光度扭曲损失。我们的方法在KITTI驾驶数据集上的两项任务中均优于现有的基于学习的方法。源代码可在https://github.com/Huangying-Zhan/Depth-VO-Feat获取。","领域":"单目深度估计/视觉里程计/深度特征重建","问题":"单视图深度估计和视觉里程计的无监督学习","动机":"探索在没有完全监督的情况下，通过使用立体序列来学习深度和视觉里程计，以提高深度预测的准确性和视觉里程计的性能","方法":"使用立体序列，利用空间和时间光度扭曲误差，将场景深度和相机运动约束在现实世界的尺度上，并通过考虑深度特征的扭曲来改进标准光度扭曲损失","关键词":["单目深度估计","视觉里程计","深度特征重建"],"涉及的技术概念":"立体序列、光度误差、深度特征扭曲、单视图深度估计、视觉里程计、光度扭曲损失"},{"order":36,"title":"Detect-and-Track: Efficient Pose Estimation in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Girdhar_Detect-and-Track_Efficient_Pose_CVPR_2018_paper.html","abstract":"This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2% on the validation and 51.8% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.","中文标题":"检测与跟踪：视频中的高效姿态估计","摘要翻译":"本文解决了在复杂的多人视频中估计和跟踪人体关键点的问题。我们提出了一种极其轻量级但高效的方法，该方法基于人体检测和视频理解的最新进展。我们的方法分两个阶段操作：在帧或短片段中进行关键点估计，然后进行轻量级跟踪以生成整个视频中链接的关键点预测。对于帧级姿态估计，我们尝试了Mask R-CNN，以及我们自己提出的该模型的3D扩展，该扩展利用小片段上的时间信息生成更稳健的帧预测。我们在新发布的多人视频姿态估计基准PoseTrack上进行了广泛的消融实验，以验证我们模型的各种设计选择。我们的方法在使用多目标跟踪准确度（MOTA）指标的验证集上达到了55.2%的准确度，在测试集上达到了51.8%的准确度，并在ICCV 2017 PoseTrack关键点跟踪挑战中实现了最先进的性能。","领域":"姿态估计/视频理解/人体检测","问题":"在复杂的多人视频中估计和跟踪人体关键点","动机":"提高视频中人体姿态估计的准确性和效率","方法":"两阶段方法：首先在帧或短片段中进行关键点估计，然后进行轻量级跟踪以生成整个视频中链接的关键点预测。使用了Mask R-CNN及其3D扩展模型。","关键词":["姿态估计","视频理解","人体检测","Mask R-CNN","3D扩展模型"],"涉及的技术概念":{"Mask R-CNN":"一种用于对象检测和分割的深度学习模型，能够同时预测对象的边界框和像素级分割掩码。","3D扩展模型":"对Mask R-CNN的扩展，利用时间信息（如视频中的帧序列）来提高姿态估计的准确性。","多目标跟踪准确度（MOTA）":"一种用于评估多目标跟踪系统性能的指标，考虑了检测准确度、误报率和身份切换等因素。","PoseTrack":"一个用于评估视频中多人姿态估计和跟踪性能的基准数据集。"}},{"order":37,"title":"Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Supervision-by-Registration_An_Unsupervised_CVPR_2018_paper.html","abstract":"In this paper, we present supervision-by-registration, an unsupervised approach to improve the precision of facial landmark detectors on both images and video. Our key observation is that the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. Interestingly, coherency of optical flow is a source of supervision that does not require manual labeling, and can be leveraged during detector training. For example, we can enforce in the training loss function that a detected landmark at frame t-1 followed by optical flow tracking from frame t-1 to frame t should coincide with the location of the detection at frame t. Essentially, supervision-by-registration augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos. End-to-end training with the registration loss is made possible by a differentiable Lucas-Kanade operation, which computes optical flow registration in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector. The output of our method is a more precise image-based facial landmark detector, which can be applied to single images or video. With supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities), and (2) significant reduction of jittering in video detections.","中文标题":"通过注册监督：一种提高面部标志检测器精度的无监督方法","摘要翻译":"在本文中，我们提出了通过注册监督的方法，这是一种无监督的方法，旨在提高面部标志检测器在图像和视频上的精度。我们的关键观察是，相邻帧中同一标志的检测应与注册（即光流）保持一致。有趣的是，光流的一致性是一种不需要手动标注的监督来源，可以在检测器训练过程中加以利用。例如，我们可以在训练损失函数中强制执行，即在帧t-1检测到的标志，通过从帧t-1到帧t的光流跟踪，应该与帧t的检测位置一致。本质上，通过注册监督，我们通过注册损失增强了训练损失函数，从而训练检测器不仅输出接近标注图像中的标注，而且与大量未标注视频上的注册保持一致。通过可微分的Lucas-Kanade操作，使得带有注册损失的端到端训练成为可能，该操作在前向传递中计算光流注册，并在反向传播中传播鼓励检测器中时间一致性的梯度。我们的方法输出的是一个更精确的基于图像的面部标志检测器，可以应用于单张图像或视频。通过注册监督，我们展示了（1）在图像（300W，ALFW）和视频（300VW，Youtube-Celebrities）上面部标志检测的改进，以及（2）视频检测中抖动的显著减少。","领域":"面部标志检测/光流/无监督学习","问题":"提高面部标志检测器在图像和视频上的精度","动机":"利用光流的一致性作为无监督的监督来源，以提高面部标志检测器的精度","方法":"通过注册监督增强训练损失函数，利用可微分的Lucas-Kanade操作进行端到端训练","关键词":["面部标志检测","光流","无监督学习","Lucas-Kanade操作"],"涉及的技术概念":"注册监督是一种无监督学习方法，通过光流的一致性来提高面部标志检测器的精度。Lucas-Kanade操作是一种计算光流的技术，用于在视频帧之间跟踪面部标志的位置。"},{"order":38,"title":"Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Diversity_Regularized_Spatiotemporal_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Diversity_Regularized_Spatiotemporal_CVPR_2018_paper.html","abstract":"Video-based person re-identification matches video clips of people across non-overlapping cameras. Most existing methods tackle this problem by encoding each video frame in its entirety and computing an aggregate representation across all frames. In practice, people are often partially occluded, which can corrupt the extracted features. Instead, we propose a new spatiotemporal attention model that automatically discovers a diverse set of distinctive body parts.  This allows useful information to be extracted from all frames without succumbing to occlusions and misalignments.  The network learns multiple spatial attention models and employs a diversity regularization term to ensure multiple models do not discover the same body part.  Features extracted from local image regions are organized by spatial attention model and are combined using temporal attention. As a result, the network learns latent representations of the face, torso and other body parts using the best available image patches from the entire video sequence. Extensive evaluations on three datasets show that our framework outperforms the state-of-the-art approaches by large margins on multiple metrics.","中文标题":"基于视频的行人重识别中的多样性正则化时空注意力模型","摘要翻译":"基于视频的行人重识别任务是在不重叠的摄像头之间匹配人物的视频片段。大多数现有方法通过整体编码每一帧视频并计算所有帧的聚合表示来解决这个问题。实际上，人物经常被部分遮挡，这可能会破坏提取的特征。相反，我们提出了一种新的时空注意力模型，该模型自动发现一组多样化的显著身体部位。这使得可以从所有帧中提取有用信息，而不会受到遮挡和错位的影响。网络学习多个空间注意力模型，并采用多样性正则化项来确保多个模型不会发现相同的身体部位。从局部图像区域提取的特征通过空间注意力模型组织，并使用时间注意力结合。因此，网络使用整个视频序列中最佳的图像块学习面部、躯干和其他身体部位的潜在表示。在三个数据集上的广泛评估表明，我们的框架在多个指标上大幅优于最先进的方法。","领域":"行人重识别/视频分析/注意力机制","问题":"解决视频中行人重识别时因遮挡和错位导致的特征提取问题","动机":"提高在存在遮挡和错位情况下视频行人重识别的准确性和鲁棒性","方法":"提出一种新的时空注意力模型，通过多样性正则化确保发现多样化的显著身体部位，并结合时间注意力从整个视频序列中提取最佳图像块","关键词":["行人重识别","时空注意力","多样性正则化"],"涉及的技术概念":"时空注意力模型用于自动发现视频中的显著身体部位，多样性正则化确保模型发现多样化的身体部位，时间注意力用于结合从整个视频序列中提取的最佳图像块，以学习身体部位的潜在表示。"},{"order":39,"title":"Style Aggregated Network for Facial Landmark Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Style_Aggregated_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Style_Aggregated_Network_CVPR_2018_paper.html","abstract":"Recent advances in facial landmark detection achieve success by learning discriminative features from rich deformation of face shapes and poses. Besides the variance of faces themselves, the intrinsic variance of image styles, e.g., grayscale vs. color images, light vs. dark, intense vs. dull, and so on, has constantly been overlooked. This issue becomes inevitable as increasing web images are collected from various sources for training neural networks. In this work, we propose a style-aggregated approach to deal with the large intrinsic variance of image styles for facial landmark detection. Our method transforms original face images to style-aggregated images by a generative adversarial module. The proposed scheme uses the style-aggregated image to maintain face images that are more robust to environmental changes. Then the original face images accompanying with style-aggregated ones play a duet to train a landmark detector which is complementary to each other. In this way, for each face, our method takes two images as input, i.e., one in its original style and the other in the aggregated style. In experiments, we observe that the large variance of image styles would degenerate the performance of facial landmark detectors. Moreover, we show the robustness of our method to the large variance of image styles by comparing to a variant of our approach, in which the generative adversarial module is removed, and no style-aggregated images are used. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on benchmark datasets AFLW and 300-W. Code is publicly available on GitHub: https://github.com/D-X-Y/SAN","中文标题":"风格聚合网络用于面部标志点检测","摘要翻译":"近年来，面部标志点检测通过从丰富的面部形状和姿态变形中学习判别特征取得了成功。除了面部本身的差异外，图像风格的内在差异，例如灰度与彩色图像、亮与暗、强烈与柔和等，一直被忽视。随着越来越多的网络图像从各种来源收集用于训练神经网络，这个问题变得不可避免。在这项工作中，我们提出了一种风格聚合的方法来处理图像风格的大内在差异，用于面部标志点检测。我们的方法通过生成对抗模块将原始面部图像转换为风格聚合图像。所提出的方案使用风格聚合图像来保持面部图像对环境变化更加鲁棒。然后，原始面部图像与风格聚合图像一起训练一个标志点检测器，两者相互补充。这样，对于每个面部，我们的方法将两个图像作为输入，即一个以其原始风格，另一个以聚合风格。在实验中，我们观察到图像风格的大差异会降低面部标志点检测器的性能。此外，我们通过比较我们的方法与一个变体（其中生成对抗模块被移除，且不使用风格聚合图像）来展示我们的方法对图像风格大差异的鲁棒性。我们的方法在基准数据集AFLW和300-W上与最先进的算法相比表现良好。代码已在GitHub上公开：https://github.com/D-X-Y/SAN","领域":"面部标志点检测/生成对抗网络/图像风格转换","问题":"处理图像风格的大内在差异对面部标志点检测性能的影响","动机":"随着网络图像来源的多样化，图像风格的差异对面部标志点检测的影响变得不可忽视，需要一种方法来提高检测器对图像风格变化的鲁棒性","方法":"提出了一种风格聚合的方法，通过生成对抗模块将原始面部图像转换为风格聚合图像，然后使用这两种图像训练一个互补的标志点检测器","关键词":["面部标志点检测","生成对抗网络","图像风格转换"],"涉及的技术概念":"生成对抗网络（GAN）用于图像风格转换，风格聚合图像用于提高面部标志点检测器对图像风格变化的鲁棒性"},{"order":40,"title":"Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Deep_Models_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Deep_Models_CVPR_2018_paper.html","abstract":"Face anti-spoofing is crucial  to prevent face recognition systems from a security breach. Previous deep learning approaches formulate face anti-spoofing as a binary classification problem. Many of them struggle to grasp adequate spoofing cues and generalize poorly. In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues. A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision. The estimated depth and rPPG are fused to distinguish live vs. spoof faces. Further, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations. Experiments show that our model achieves the state-of-the-art results on both intra- and cross-database testing.","中文标题":"学习用于人脸反欺骗的深度模型：二元或辅助监督","摘要翻译":"人脸反欺骗对于防止人脸识别系统遭受安全破坏至关重要。以往的深度学习方法将人脸反欺骗问题表述为一个二元分类问题。许多方法难以捕捉足够的欺骗线索，泛化能力差。在本文中，我们讨论了辅助监督在引导学习向区分性和可泛化线索发展的重要性。我们学习了一个CNN-RNN模型，通过像素级监督估计人脸深度，并通过序列级监督估计rPPG信号。估计的深度和rPPG被融合以区分真实人脸与欺骗人脸。此外，我们引入了一个新的人脸反欺骗数据库，涵盖了广泛的照明、主体和姿态变化。实验表明，我们的模型在数据库内和跨数据库测试中均达到了最先进的结果。","领域":"人脸识别/生物识别安全/深度学习应用","问题":"如何提高人脸反欺骗系统的准确性和泛化能力","动机":"现有的人脸反欺骗方法难以捕捉足够的欺骗线索，泛化能力差，需要更有效的方法来提高系统的安全性和可靠性","方法":"采用CNN-RNN模型，结合像素级和序列级监督，估计人脸深度和rPPG信号，融合这些信息以区分真实与欺骗人脸","关键词":["人脸反欺骗","CNN-RNN模型","rPPG信号","像素级监督","序列级监督"],"涉及的技术概念":"CNN-RNN模型结合了卷积神经网络和循环神经网络的特点，用于处理图像和序列数据；rPPG信号指的是远程光电容积描记信号，用于非接触式心率监测；像素级监督和序列级监督分别指在像素级别和序列级别对模型进行监督学习，以提高模型的准确性和泛化能力。"},{"order":41,"title":"Deep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Deep_Cost-Sensitive_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Deep_Cost-Sensitive_and_CVPR_2018_paper.html","abstract":"Facial age estimation from a face image is an important yet very challenging task in computer vision, since humans with different races and/or genders, exhibit quite different patterns in their facial aging processes. To deal with the influence of race and gender, previous methods perform age estimation within each population separately. In practice, however, it is often very difficult to collect and label sufficient data for each population. Therefore, it would be helpful to exploit an existing large labeled dataset of one (source) population to improve the age estimation performance on another (target) population with only a small labeled dataset available. In this work, we propose a Deep Cross-Population (DCP) age estimation model to achieve this goal. In particular, our DCP model develops a two-stage training strategy. First, a novel cost-sensitive multi-task loss function is designed to learn transferable aging features by training on the source population. Second, a novel order-preserving pair-wise loss function is designed to align the aging features of the two populations. By doing so, our DCP  model can transfer the knowledge encoded in the source population to the target population. Extensive experiments on the two of the largest benchmark datasets show that our DCP model outperforms several strong baseline methods and many state-of-the-art methods.","中文标题":"深度成本敏感和顺序保持特征学习用于跨群体年龄估计","摘要翻译":"从面部图像中估计年龄是计算机视觉中一个重要但非常具有挑战性的任务，因为不同种族和/或性别的人类在面部老化过程中表现出截然不同的模式。为了处理种族和性别的影响，以前的方法在每个群体内分别进行年龄估计。然而，在实践中，为每个群体收集和标记足够的数据往往非常困难。因此，利用一个（源）群体现有的带有大量标签的数据集来提高另一个（目标）群体（只有少量标签数据集可用）的年龄估计性能将非常有帮助。在这项工作中，我们提出了一个深度跨群体（DCP）年龄估计模型来实现这一目标。特别是，我们的DCP模型开发了一种两阶段训练策略。首先，设计了一种新颖的成本敏感多任务损失函数，通过在源群体上进行训练来学习可转移的老化特征。其次，设计了一种新颖的顺序保持成对损失函数来对齐两个群体的老化特征。通过这样做，我们的DCP模型可以将源群体中编码的知识转移到目标群体。在两个最大的基准数据集上的大量实验表明，我们的DCP模型优于几种强基线方法和许多最先进的方法。","领域":"面部年龄估计/跨群体学习/特征学习","问题":"跨群体面部年龄估计","动机":"由于不同种族和性别的人类在面部老化过程中表现出不同的模式，且为每个群体收集和标记足够的数据困难，因此需要一种方法利用一个群体的数据来提高另一个群体的年龄估计性能。","方法":"提出了一种深度跨群体（DCP）年龄估计模型，采用两阶段训练策略：首先设计成本敏感多任务损失函数学习可转移的老化特征，然后设计顺序保持成对损失函数对齐两个群体的老化特征。","关键词":["面部年龄估计","跨群体学习","特征学习","成本敏感学习","顺序保持"],"涉及的技术概念":"成本敏感多任务损失函数用于学习可转移的老化特征，顺序保持成对损失函数用于对齐不同群体的老化特征，实现跨群体知识转移。"},{"order":42,"title":"First-Person Hand Action Benchmark With RGB-D Videos and 3D Hand Pose Annotations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.html","abstract":"In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand  configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.","中文标题":"第一人称手部动作基准测试：包含RGB-D视频和3D手部姿势标注","摘要翻译":"在本研究中，我们探讨了使用3D手部姿势来识别与3D物体交互的第一人称动态手部动作。为此，我们收集了包含超过100K帧的45种日常手部动作类别的RGB-D视频序列，涉及26种不同物体在多种手部配置中的交互。为了获得手部姿势标注，我们使用了自有的动作捕捉系统，该系统通过6个磁性传感器和逆向运动学自动推断出手模型21个关节的3D位置。此外，我们还记录了6D物体姿势，并为一部分手-物体交互序列提供了3D物体模型。据我们所知，这是第一个使用3D手部姿势研究第一人称手部动作的基准测试。我们通过18种基线/最先进的方法对RGB-D和基于姿势的动作识别进行了广泛的实验评估。测量了使用外观特征、姿势及其组合的影响，并评估了不同的训练/测试协议。最后，我们评估了当手在自我中心视图中被物体严重遮挡时，3D手部姿势估计领域的准备情况及其对动作识别的影响。从结果中，我们看到了使用手部姿势作为动作识别线索相比其他数据模式的明显优势。我们的数据集和实验可能对3D手部姿势估计、6D物体姿势、机器人学以及动作识别社区感兴趣。","领域":"3D手部姿势估计/6D物体姿势/动作识别","问题":"识别与3D物体交互的第一人称动态手部动作","动机":"研究使用3D手部姿势来识别第一人称动态手部动作的有效性","方法":"收集RGB-D视频序列，使用动作捕捉系统自动推断手模型21个关节的3D位置，记录6D物体姿势，提供3D物体模型，进行广泛的实验评估","关键词":["3D手部姿势估计","6D物体姿势","动作识别"],"涉及的技术概念":"RGB-D视频序列、3D手部姿势、动作捕捉系统、逆向运动学、6D物体姿势、3D物体模型、动作识别"},{"order":43,"title":"A Pose-Sensitive Embedding for Person Re-Identification With Expanded Cross Neighborhood Re-Ranking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sarfraz_A_Pose-Sensitive_Embedding_CVPR_2018_paper.html","abstract":"Person re-identification is a challenging retrieval task that requires matching a person’s acquired image across non-overlapping camera views. In this paper we propose an effective approach that incorporates both the fine and coarse pose information of the person to learn a discrim- inative embedding. In contrast to the recent direction of explicitly modeling body parts or correcting for misalignment based on these, we show that a rather straightforward inclusion of acquired camera view and/or the detected joint locations into a convolutional neural network helps to learn a very effective representation. To increase retrieval performance, re-ranking techniques based on computed distances have recently gained much attention. We propose a new unsupervised and automatic re-ranking framework that achieves state-of-the-art re-ranking performance. We show that in contrast to the current state-of-the-art re-ranking methods our approach does not require to compute new rank lists for each image pair (e.g., based on reciprocal neighbors) and performs well by using simple direct rank list based comparison or even by just using the already computed euclidean distances between the images. We show that both our learned representation and our re-ranking method achieve state-of-the-art performance on a number of challenging surveillance image and video datasets.","中文标题":"用于行人重识别的姿态敏感嵌入与扩展交叉邻域重排序","摘要翻译":"行人重识别是一项具有挑战性的检索任务，需要在非重叠的摄像机视图中匹配一个人的获取图像。在本文中，我们提出了一种有效的方法，该方法结合了人的细粒度和粗粒度姿态信息来学习一个判别性嵌入。与最近明确建模身体部位或基于这些部位进行对齐校正的方向相比，我们表明，将获取的摄像机视图和/或检测到的关节位置直接纳入卷积神经网络有助于学习非常有效的表示。为了提高检索性能，基于计算距离的重排序技术最近受到了广泛关注。我们提出了一种新的无监督自动重排序框架，该框架实现了最先进的重排序性能。我们表明，与当前最先进的重排序方法相比，我们的方法不需要为每个图像对计算新的排名列表（例如，基于互惠邻居），并且通过使用简单的直接排名列表比较或仅使用已经计算的图像之间的欧几里得距离就能表现良好。我们展示了我们学习的表示和我们的重排序方法在许多具有挑战性的监控图像和视频数据集上都达到了最先进的性能。","领域":"行人重识别/姿态估计/图像检索","问题":"在非重叠摄像机视图中匹配行人图像","动机":"提高行人重识别的准确性和效率","方法":"结合细粒度和粗粒度姿态信息学习判别性嵌入，并提出新的无监督自动重排序框架","关键词":["行人重识别","姿态估计","图像检索","卷积神经网络","重排序"],"涉及的技术概念":"本文涉及的技术概念包括行人重识别、姿态估计、卷积神经网络（CNN）、图像检索、重排序技术、欧几里得距离计算等。"},{"order":44,"title":"Disentangling 3D Pose in a Dendritic CNN for Unconstrained 2D Face Alignment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kumar_Disentangling_3D_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kumar_Disentangling_3D_Pose_CVPR_2018_paper.html","abstract":"Heatmap regression has been used for landmark localization for quite a while now. Most of the methods use a very deep stack of bottleneck modules for heatmap classification stage, followed by heatmap regression to extract the keypoints. In this paper, we present a single dendritic CNN, termed as Pose Conditioned Dendritic Convolution Neural Network (PCD-CNN), where a classification network is followed by a second and modular classification network, trained in an end to end fashion to obtain accurate landmark points. Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches. Extensive experimentation shows that conditioning on pose reduces the localization error by making it agnostic to face pose. The proposed model can be extended to yield variable number of landmark points and hence broadening its applicability to other datasets. Instead of increasing depth or width of the network, we train the CNN efficiently with Mask-Softmax Loss and hard sample mining to achieve upto 15% reduction in error compared to state-of-the-art methods for extreme and medium pose face images from challenging datasets including AFLW, AFW, COFW and IBUG.","中文标题":"解缠3D姿态在树突状CNN中用于无约束2D面部对齐","摘要翻译":"热图回归用于地标定位已经有一段时间了。大多数方法在热图分类阶段使用非常深的瓶颈模块堆栈，然后通过热图回归提取关键点。在本文中，我们提出了一种单一的树突状CNN，称为姿态条件树突卷积神经网络（PCD-CNN），其中分类网络之后是第二个模块化分类网络，以端到端的方式进行训练，以获得准确的地标点。遵循贝叶斯公式，我们通过将地标估计条件化于姿态来显式解缠面部图像的3D姿态，使其不同于多任务方法。大量实验表明，通过使定位误差对脸部姿态不敏感，条件化于姿态减少了定位误差。所提出的模型可以扩展以产生可变数量的地标点，从而扩大其适用于其他数据集的范围。与增加网络的深度或宽度不同，我们通过使用Mask-Softmax损失和硬样本挖掘有效地训练CNN，与包括AFLW、AFW、COFW和IBUG在内的挑战性数据集中的极端和中等姿态面部图像的最先进方法相比，误差减少了多达15%。","领域":"面部对齐/3D姿态估计/地标定位","问题":"在无约束条件下准确进行2D面部对齐","动机":"提高面部地标定位的准确性，特别是在极端和中等姿态的面部图像上","方法":"提出了一种姿态条件树突卷积神经网络（PCD-CNN），通过将地标估计条件化于姿态来显式解缠面部图像的3D姿态，使用Mask-Softmax损失和硬样本挖掘进行训练","关键词":["面部对齐","3D姿态估计","地标定位","树突状CNN","Mask-Softmax损失","硬样本挖掘"],"涉及的技术概念":"热图回归用于地标定位，姿态条件树突卷积神经网络（PCD-CNN）通过条件化于姿态来解缠3D姿态，Mask-Softmax损失和硬样本挖掘用于训练CNN以提高定位准确性"},{"order":45,"title":"A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_A_Hierarchical_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_A_Hierarchical_Generative_CVPR_2018_paper.html","abstract":"In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthe- sis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye ge- ometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermedi- ate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model’s effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields.","中文标题":"一种用于眼睛图像合成和眼睛注视估计的分层生成模型","摘要翻译":"在本工作中，我们引入了一种分层生成模型（HGM），以实现逼真的前向眼睛图像合成以及有效的后向眼睛注视估计。提出的HGM包括一个分层生成形状模型（HGSM）和一个条件双向生成对抗网络（c-BiGAN）。HGSM编码眼睛几何知识并将眼睛注视与眼睛形状相关联，而c-BiGAN利用大数据并捕捉眼睛形状与眼睛外观之间的依赖关系。作为中间组件，眼睛形状连接了基于知识的模型（HGSM）和数据驱动模型（c-BiGAN），并实现了双向推理。通过自上而下的推理，HGM可以合成与给定眼睛注视一致的眼睛图像。通过自下而上的推理，HGM可以从给定的眼睛图像中有效地推断出眼睛注视。在基准数据集上的定性和定量评估证明了我们的模型在眼睛图像合成和眼睛注视估计方面的有效性。此外，所提出的模型不仅限于眼睛图像。它可以适应面部图像和任何形状-外观相关的领域。","领域":"生成模型/眼睛注视估计/图像合成","问题":"如何同时实现逼真的眼睛图像合成和有效的眼睛注视估计","动机":"为了在眼睛图像合成和眼睛注视估计之间建立有效的桥梁，利用生成模型的力量来提高这两项任务的性能","方法":"提出了一种分层生成模型（HGM），包括分层生成形状模型（HGSM）和条件双向生成对抗网络（c-BiGAN），通过双向推理实现眼睛图像合成和眼睛注视估计","关键词":["分层生成模型","眼睛注视估计","图像合成","条件双向生成对抗网络","分层生成形状模型"],"涉及的技术概念":{"分层生成模型（HGM）":"一种结合了分层生成形状模型（HGSM）和条件双向生成对抗网络（c-BiGAN）的模型，用于眼睛图像合成和眼睛注视估计","分层生成形状模型（HGSM）":"编码眼睛几何知识并将眼睛注视与眼睛形状相关联的模型","条件双向生成对抗网络（c-BiGAN）":"利用大数据并捕捉眼睛形状与眼睛外观之间依赖关系的生成对抗网络","双向推理":"通过自上而下和自下而上的推理方式，实现眼睛图像合成和眼睛注视估计的过程"}},{"order":46,"title":"MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html","abstract":"Human actions in videos are three-dimensional (3D) signals. Recent attempts use 3D convolutional neural networks (CNNs) to explore spatio-temporal information for human action recognition. Though promising, 3D CNNs have not achieved high performanceon on this task with respect to their well-established two-dimensional (2D) counterparts for visual recognition in still images. We argue that the high training complexity of spatio-temporal fusion and the huge memory cost of 3D convolution hinder current 3D CNNs, which stack 3D convolutions layer by layer, by outputting deeper feature maps that are crucial for high-level tasks. We thus propose a Mixed Convolutional Tube (MiCT) that integrates 2D CNNs with the 3D convolution module to generate deeper and more informative feature maps, while reducing training complexity in each round of spatio-temporal fusion. A new end-to-end trainable deep 3D network, MiCT-Net, is also proposed based on the MiCT to better explore spatio-temporal information in human actions. Evaluations on three well-known benchmark datasets (UCF101, Sport-1M and HMDB-51) show that the proposed MiCT-Net significantly outperforms the original 3D CNNs. Compared with state-of-the-art approaches for action recognition on UCF101 and HMDB51, our MiCT-Net yields the best performance.","中文标题":"MiCT: 用于人类动作识别的混合3D/2D卷积管","摘要翻译":"视频中的人类动作是三维（3D）信号。最近的尝试使用3D卷积神经网络（CNNs）来探索时空信息以进行人类动作识别。尽管前景看好，但3D CNNs在这一任务上相对于其在静止图像视觉识别中已确立的二维（2D）对应物尚未实现高性能。我们认为，时空融合的高训练复杂性和3D卷积的巨大内存成本阻碍了当前逐层堆叠3D卷积的3D CNNs，使其无法输出对高级任务至关重要的更深层次特征图。因此，我们提出了一种混合卷积管（MiCT），它将2D CNNs与3D卷积模块集成，以生成更深层次和信息更丰富的特征图，同时减少每轮时空融合的训练复杂性。基于MiCT，还提出了一种新的端到端可训练的深度3D网络MiCT-Net，以更好地探索人类动作中的时空信息。在三个知名基准数据集（UCF101、Sport-1M和HMDB-51）上的评估显示，提出的MiCT-Net显著优于原始的3D CNNs。与UCF101和HMDB51上动作识别的最新方法相比，我们的MiCT-Net表现最佳。","领域":"动作识别/视频分析/时空信息处理","问题":"3D卷积神经网络在人类动作识别任务中的性能未达到预期","动机":"提高3D卷积神经网络在人类动作识别任务中的性能，减少训练复杂性和内存成本","方法":"提出混合卷积管（MiCT）集成2D CNNs与3D卷积模块，生成更深层次特征图，减少训练复杂性，并基于MiCT提出新的端到端可训练的深度3D网络MiCT-Net","关键词":["动作识别","3D卷积神经网络","时空信息"],"涉及的技术概念":"3D卷积神经网络（3D CNNs）用于处理视频中的时空信息，2D卷积神经网络（2D CNNs）用于处理静止图像，混合卷积管（MiCT）结合了2D和3D卷积的优点，MiCT-Net是基于MiCT的深度3D网络，用于更好地探索人类动作中的时空信息。"},{"order":47,"title":"Learning to Estimate 3D Human Pose and Shape From a Single Color Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.html","abstract":"This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efficient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further refinement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.","中文标题":"学习从单一彩色图像估计3D人体姿态和形状","摘要翻译":"本工作解决了从单一彩色图像估计全身3D人体姿态和形状的问题。这是一个通常由基于迭代优化的解决方案主导的任务，而卷积网络（ConvNets）由于缺乏训练数据和其低分辨率的3D预测而受到影响。我们的工作旨在弥合这一差距，并提出了一种基于ConvNets的高效且有效的直接预测方法。我们方法的核心部分是在我们的端到端框架中整合了一个参数化的统计体型模型（SMPL）。这使我们能够获得非常详细的3D网格结果，同时只需要估计少量参数，使其适合直接网络预测。有趣的是，我们证明了这些参数可以仅从2D关键点和掩码中可靠地预测。这些是通用2D人体分析ConvNets的典型输出，使我们能够放宽对具有3D形状地面实况图像用于训练的大量需求。同时，通过保持可微分性，在训练时我们从估计的参数生成3D网格，并使用3D每顶点损失明确优化表面。最后，使用可微分渲染器将3D网格投影到图像上，这通过优化投影与2D注释（即2D关键点或掩码）的一致性，进一步细化网络。所提出的方法在此任务上优于之前的基线，并为从单一彩色图像直接预测3D形状提供了一个有吸引力的解决方案。","领域":"3D人体姿态估计/3D形状重建/深度学习","问题":"从单一彩色图像估计全身3D人体姿态和形状","动机":"解决卷积网络在缺乏训练数据和低分辨率3D预测方面的不足，提出一种高效且有效的直接预测方法","方法":"整合参数化的统计体型模型（SMPL）到端到端框架中，通过2D关键点和掩码预测参数，使用3D每顶点损失优化表面，并利用可微分渲染器进行投影优化","关键词":["3D人体姿态估计","3D形状重建","卷积网络","参数化统计体型模型","可微分渲染器"],"涉及的技术概念":{"卷积网络（ConvNets）":"一种深度学习模型，特别适用于处理图像数据。","参数化的统计体型模型（SMPL）":"一种用于表示和预测人体3D形状的统计模型。","3D每顶点损失":"一种用于优化3D网格表面准确性的损失函数。","可微分渲染器":"一种能够将3D模型投影到2D图像上，并保持可微分性的工具，用于进一步优化网络。"}},{"order":48,"title":"Glimpse Clouds: Human Activity Recognition From Unstructured Feature Points","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.html","abstract":"We propose a method for human activity recognition from RGB data that does not rely on any pose information during test time, and does not explicitly calculate pose information internally. Instead, a visual attention module learns to predict glimpse sequences in each frame. These glimpses correspond to interest points in the scene that are relevant to the classified activities. No spatial coherence is forced on the glimpse locations, which gives the attention module liberty to explore different points at each frame and better optimize the process of scrutinizing visual information. Tracking and sequentially integrating this kind of unstructured data is a challenge, which we address by sep- arating the set of glimpses from a set of recurrent tracking/recognition workers. These workers receive glimpses, jointly performing subsequent motion tracking and activity prediction. The glimpses are soft-assigned to the workers, optimizing coherence of the assignments in space, time and feature space using an external memory module. No hard decisions are taken, i.e. each glimpse point is assigned to all existing workers, albeit with different importance. Our methods outperform the state-of-the-art on the largest human activity recognition dataset available to-date, NTU RGB+D, and on the Northwestern-UCLA Multiview Action 3D Dataset.","中文标题":"一瞥云：从非结构化特征点进行人类活动识别","摘要翻译":"我们提出了一种从RGB数据进行人类活动识别的方法，该方法在测试时不依赖于任何姿势信息，也不在内部显式计算姿势信息。相反，一个视觉注意力模块学习预测每一帧中的一瞥序列。这些一瞥对应于场景中与分类活动相关的兴趣点。一瞥位置不强制空间一致性，这使得注意力模块可以自由地在每一帧探索不同的点，并更好地优化视觉信息的审查过程。跟踪和顺序整合这种非结构化数据是一个挑战，我们通过将一瞥集合与一组循环跟踪/识别工作者分离来解决这个问题。这些工作者接收一瞥，共同执行后续的运动跟踪和活动预测。一瞥被软分配给工作者，使用外部记忆模块优化空间、时间和特征空间中的分配一致性。没有做出硬性决定，即每个一瞥点都被分配给所有现有的工作者，尽管重要性不同。我们的方法在迄今为止最大的人类活动识别数据集NTU RGB+D和西北大学-加州大学洛杉矶分校多视图动作3D数据集上优于最先进的技术。","领域":"人类活动识别/视觉注意力机制/非结构化数据处理","问题":"如何在不依赖姿势信息的情况下从RGB数据中识别人类活动","动机":"减少对人类活动识别中姿势信息的依赖，提高识别效率和准确性","方法":"使用视觉注意力模块预测每一帧中的一瞥序列，通过分离一瞥集合与循环跟踪/识别工作者来优化视觉信息的审查过程","关键词":["人类活动识别","视觉注意力机制","非结构化数据处理"],"涉及的技术概念":"视觉注意力模块用于预测一瞥序列，这些一瞥对应于场景中与分类活动相关的兴趣点。通过分离一瞥集合与循环跟踪/识别工作者，并使用外部记忆模块优化分配一致性，实现了对非结构化数据的有效处理和人类活动的高效识别。"},{"order":49,"title":"Context-Aware Deep Feature Compression for High-Speed Visual Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.html","abstract":"We propose a new context-aware correlation filter based tracking framework to achieve both high computational speed and state-of-the-art performance among real-time trackers. The major contribution to the high computational speed lies in the proposed deep feature compression that is achieved by a context-aware scheme utilizing multiple expert auto-encoders; a context in our framework refers to the coarse category of the tracking target according to appearance patterns. In the pre-training phase, one expert auto-encoder is trained per category. In the tracking phase, the best expert auto-encoder is selected for a given target, and only this auto-encoder is used. To achieve high tracking performance with the compressed feature map, we introduce extrinsic denoising processes and a new orthogonality loss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware framework through a number of experiments, where our method achieves a comparable performance to state-of-the-art trackers which cannot run in real-time, while running at a significantly fast speed of over 100 fps.","中文标题":"上下文感知的深度特征压缩用于高速视觉跟踪","摘要翻译":"我们提出了一种新的基于上下文感知的相关滤波器跟踪框架，旨在实现高计算速度和实时跟踪器中的最先进性能。高计算速度的主要贡献在于提出的深度特征压缩，这是通过利用多个专家自动编码器的上下文感知方案实现的；在我们的框架中，上下文指的是根据外观模式对跟踪目标的粗略分类。在预训练阶段，每个类别训练一个专家自动编码器。在跟踪阶段，为给定目标选择最佳的专家自动编码器，并且仅使用这个自动编码器。为了在压缩的特征图上实现高跟踪性能，我们引入了外部去噪过程和新的正交性损失项，用于专家自动编码器的预训练和微调。我们通过一系列实验验证了所提出的上下文感知框架，在这些实验中，我们的方法实现了与无法实时运行的最先进跟踪器相当的性能，同时以超过100 fps的显著快速速度运行。","领域":"视觉跟踪/特征压缩/自动编码器","问题":"如何在保证高计算速度的同时，实现实时视觉跟踪的最先进性能","动机":"提高视觉跟踪的计算速度，同时保持或超越现有实时跟踪器的性能","方法":"提出了一种基于上下文感知的相关滤波器跟踪框架，利用多个专家自动编码器进行深度特征压缩，并引入外部去噪过程和新的正交性损失项来优化跟踪性能","关键词":["视觉跟踪","特征压缩","自动编码器","上下文感知","相关滤波器"],"涉及的技术概念":"上下文感知指的是根据跟踪目标的外观模式对其进行粗略分类；深度特征压缩通过多个专家自动编码器实现，每个类别训练一个自动编码器；外部去噪过程和正交性损失项用于优化自动编码器的预训练和微调，以提高跟踪性能。"},{"order":50,"title":"Correlation Tracking via Joint Discrimination and Reliability Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Correlation_Tracking_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Correlation_Tracking_via_CVPR_2018_paper.html","abstract":"For visual tracking, an ideal filter learned by the correlation filter (CF) method should take both discrimination and reliability information. However, existing attempts usually focus on the former one while pay less attention to reliability learning. This may make the learned filter be dominated by the unexpected salient regions on the feature map, thereby resulting in model degradation. To address this issue, we propose a novel CF-based optimization problem to jointly model the discrimination and reliability information. First, we treat the filter as the element-wise product of a base filter and a reliability term. The base filter is aimed to learn the discrimination information between the target and backgrounds, and the reliability term encourages the final filter to focus on more reliable regions. Second, we introduce a local response consistency regular term to emphasize equal contributions of different regions and avoid the tracker being dominated by unreliable regions. The proposed optimization problem can be solved using the alternating direction method and speeded up in the Fourier domain. We conduct extensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to evaluate the proposed tracker. Experimental results show that our tracker performs favorably against other state-of-the-art trackers.","中文标题":"通过联合判别和可靠性学习的相关性跟踪","摘要翻译":"对于视觉跟踪，通过相关滤波器（CF）方法学习的理想滤波器应该同时考虑判别信息和可靠性信息。然而，现有的尝试通常只关注前者，而对可靠性学习的关注较少。这可能导致学习到的滤波器被特征图上意外的显著区域所主导，从而导致模型退化。为了解决这个问题，我们提出了一种新的基于CF的优化问题，以联合建模判别和可靠性信息。首先，我们将滤波器视为基础滤波器和可靠性项的逐元素乘积。基础滤波器旨在学习目标和背景之间的判别信息，而可靠性项则鼓励最终滤波器关注更可靠的区域。其次，我们引入了一个局部响应一致性正则项，以强调不同区域的平等贡献，并避免跟踪器被不可靠区域所主导。所提出的优化问题可以使用交替方向法解决，并在傅里叶域中加速。我们在OTB-2013、OTB-2015和VOT-2016数据集上进行了广泛的实验，以评估所提出的跟踪器。实验结果表明，我们的跟踪器在与其他最先进的跟踪器相比时表现优异。","领域":"视觉跟踪/相关滤波器/优化问题","问题":"现有相关滤波器方法在视觉跟踪中过于关注判别信息，而忽视了可靠性信息，导致模型退化。","动机":"为了提高视觉跟踪的准确性和鲁棒性，需要同时考虑判别信息和可靠性信息。","方法":"提出了一种新的基于相关滤波器的优化问题，通过联合建模判别和可靠性信息，并引入局部响应一致性正则项来避免跟踪器被不可靠区域所主导。","关键词":["视觉跟踪","相关滤波器","优化问题","判别信息","可靠性信息"],"涉及的技术概念":"相关滤波器（CF）方法用于视觉跟踪，通过联合建模判别和可靠性信息来提高跟踪性能。基础滤波器学习目标和背景之间的判别信息，而可靠性项确保滤波器关注更可靠的区域。局部响应一致性正则项用于强调不同区域的平等贡献，避免跟踪器被不可靠区域所主导。优化问题通过交替方向法解决，并在傅里叶域中加速。"},{"order":51,"title":"PhaseNet for Video Frame Interpolation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Meyer_PhaseNet_for_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Meyer_PhaseNet_for_Video_CVPR_2018_paper.html","abstract":"Most approaches for video frame interpolation require accurate dense correspondences to synthesize an in-between frame. Therefore, they do not perform well in challenging scenarios with e.g. lighting changes or motion blur. Recent deep learning approaches that rely on kernels to represent motion can only alleviate these problems to some extent. In those cases, methods that use a per-pixel phase-based motion representation have been shown to work well. However, they are only applicable for a limited amount of motion. We propose a new approach, PhaseNet, that is designed to robustly handle challenging scenarios while also coping with larger motion. Our approach consists of a neural network decoder that directly estimates the phase decomposition of the intermediate frame. We show that this is superior to the hand-crafted heuristics previously used in phase-based methods and also compares favorably to recent deep learning based approaches for video frame interpolation on challenging datasets.","中文标题":"用于视频帧插值的PhaseNet","摘要翻译":"大多数视频帧插值方法需要精确的密集对应关系来合成中间帧。因此，它们在具有挑战性的场景中表现不佳，例如光照变化或运动模糊。最近依赖核表示运动的深度学习方法只能在一定程度上缓解这些问题。在这些情况下，使用基于每像素相位的运动表示的方法已被证明效果良好。然而，它们仅适用于有限量的运动。我们提出了一种新方法，PhaseNet，旨在稳健地处理具有挑战性的场景，同时也能应对更大的运动。我们的方法包括一个神经网络解码器，直接估计中间帧的相位分解。我们证明，这优于先前在基于相位的方法中使用的手工启发式方法，并且在具有挑战性的数据集上，与最近的基于深度学习的视频帧插值方法相比也具有优势。","领域":"视频处理/运动估计/神经网络","问题":"视频帧插值在光照变化或运动模糊等挑战性场景中的性能问题","动机":"提高视频帧插值在具有挑战性场景中的性能，特别是对于更大的运动","方法":"提出PhaseNet，一种神经网络解码器，直接估计中间帧的相位分解","关键词":["视频帧插值","相位分解","神经网络解码器"],"涉及的技术概念":"视频帧插值是一种技术，用于在视频序列中生成新的帧，以增加帧率或创建慢动作效果。相位分解是一种表示运动的方法，通过分析视频帧中每个像素的相位变化来估计运动。神经网络解码器是一种深度学习模型，用于从输入数据中解码出特定的信息或特征。"},{"order":52,"title":"The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bideau_The_Best_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bideau_The_Best_of_CVPR_2018_paper.html","abstract":"Traditional methods of motion segmentation use powerful geometric constraints to understand motion, but fail to leverage the semantics of high-level image understanding. Modern CNN methods of motion analysis, on the other hand, excel at identifying well-known structures, but may not precisely characterize well-known geometric constraints. In this work, we build a new statistical model of rigid motion flow based on classical perspective projection constraints. We then combine piecewise rigid motions into complex deformable and articulated objects, guided by semantic segmentation from CNNs and a second \`\`object-level\\" statistical model. This combination of classical geometric knowledge combined with the pattern recognition abilities of CNNs yields excellent performance on a wide range of motion segmentation benchmarks, from complex geometric scenes to camouflaged animals.","中文标题":"两全其美：结合CNN和几何约束进行层次运动分割","摘要翻译":"传统的运动分割方法使用强大的几何约束来理解运动，但未能利用高级图像理解的语义。另一方面，现代的CNN运动分析方法在识别已知结构方面表现出色，但可能无法精确描述已知的几何约束。在这项工作中，我们基于经典的透视投影约束构建了一个新的刚体运动流统计模型。然后，我们通过CNN的语义分割和第二个“对象级”统计模型的指导，将分段刚体运动组合成复杂的可变形和关节对象。这种结合了经典几何知识和CNN模式识别能力的方法，在从复杂几何场景到伪装动物的广泛运动分割基准上表现出色。","领域":"运动分割/语义分割/几何建模","问题":"如何有效结合几何约束和CNN的语义理解能力进行运动分割","动机":"传统方法缺乏对高级图像语义的理解，而现代CNN方法在几何约束的精确描述上存在不足，需要一种结合两者优势的方法来提高运动分割的性能。","方法":"构建基于透视投影约束的刚体运动流统计模型，并通过CNN的语义分割和对象级统计模型指导，将分段刚体运动组合成复杂的可变形和关节对象。","关键词":["运动分割","语义分割","几何建模"],"涉及的技术概念":{"几何约束":"用于理解运动的传统方法，基于透视投影约束。","CNN":"卷积神经网络，用于图像识别和语义分割。","刚体运动流":"描述刚体在空间中的运动，基于统计模型。","语义分割":"通过CNN实现的图像分割，识别图像中的语义信息。","对象级统计模型":"用于指导将分段刚体运动组合成复杂对象的模型。"}},{"order":53,"title":"Hyperparameter Optimization for Tracking With Continuous Deep Q-Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Hyperparameter_Optimization_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Hyperparameter_Optimization_for_CVPR_2018_paper.html","abstract":"Hyperparameters are numerical presets whose values are assigned prior to the commencement of the learning process. Selecting appropriate hyperparameters is critical for the accuracy of tracking algorithms, yet it is difficult to determine their optimal values, in particular, adaptive ones for each specific video sequence. Most hyperparameter optimization algorithms depend on searching a generic range and they are imposed blindly on all sequences. Here, we propose a novel hyperparameter optimization method that can find optimal hyperparameters for a given sequence using an action-prediction network leveraged on Continuous Deep Q-Learning. Since the common state-spaces for object tracking tasks are significantly more complex than the ones in traditional control problems, existing Continuous Deep Q-Learning algorithms cannot be directly applied. To overcome this challenge, we introduce an efficient heuristic to accelerate the convergence behavior. We evaluate our method on several tracking benchmarks and demonstrate its superior performance.","中文标题":"使用连续深度Q学习的跟踪超参数优化","摘要翻译":"超参数是在学习过程开始之前分配的数值预设。选择合适的超参数对于跟踪算法的准确性至关重要，但确定其最优值，特别是针对每个特定视频序列的自适应值，是非常困难的。大多数超参数优化算法依赖于搜索一个通用范围，并且它们被盲目地应用于所有序列。在这里，我们提出了一种新颖的超参数优化方法，该方法可以利用基于连续深度Q学习的动作预测网络为给定序列找到最优超参数。由于对象跟踪任务的常见状态空间比传统控制问题中的状态空间复杂得多，现有的连续深度Q学习算法不能直接应用。为了克服这一挑战，我们引入了一种有效的启发式方法来加速收敛行为。我们在几个跟踪基准上评估了我们的方法，并展示了其卓越的性能。","领域":"目标跟踪/强化学习/超参数优化","问题":"如何为每个特定视频序列找到最优的自适应超参数","动机":"选择合适的超参数对于跟踪算法的准确性至关重要，但确定其最优值，特别是针对每个特定视频序列的自适应值，是非常困难的。","方法":"提出了一种新颖的超参数优化方法，该方法可以利用基于连续深度Q学习的动作预测网络为给定序列找到最优超参数，并引入了一种有效的启发式方法来加速收敛行为。","关键词":["目标跟踪","强化学习","超参数优化"],"涉及的技术概念":{"超参数":"在学习过程开始之前分配的数值预设，对算法的性能有重要影响。","连续深度Q学习":"一种强化学习算法，用于在连续动作空间中寻找最优策略。","动作预测网络":"一种网络结构，用于预测在给定状态下采取的动作。","启发式方法":"一种加速算法收敛的策略，通过引入经验或规则来减少搜索空间。"}},{"order":54,"title":"Scale-Transferrable Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Scale-Transferrable_Object_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Scale-Transferrable_Object_Detection_CVPR_2018_paper.html","abstract":"Scale problem lies in the heart of object detection. In this work, we develop a novel Scale-Transferrable Detection Network (STDN) for detecting multi-scale objects in images. In contrast to previous methods that simply combine object predictions from multiple feature maps from different network depths, the proposed network is equipped with embedded super-resolution layers (named as scale-transfer layer/module in this work) to explicitly explore the inter-scale consistency nature across multiple detection scales. Scale-transfer module naturally fits the base network with little computational cost. This module is further integrated with a dense convolutional network (DenseNet) to yield a one-stage object detector. We evaluate our proposed architecture on PASCAL VOC 2007 and MS COCO benchmark tasks and STDN obtains significant improvements over the comparable state-of-the-art detection models.","中文标题":"尺度可转移的目标检测","摘要翻译":"尺度问题处于目标检测的核心。在这项工作中，我们开发了一种新颖的尺度可转移检测网络（STDN），用于检测图像中的多尺度对象。与之前简单地将来自不同网络深度的多个特征图的对象预测结合起来的方法不同，所提出的网络配备了嵌入式超分辨率层（在本工作中称为尺度转移层/模块），以明确探索多个检测尺度之间的尺度一致性性质。尺度转移模块自然地适应基础网络，计算成本低。该模块进一步与密集卷积网络（DenseNet）集成，以产生一个单阶段目标检测器。我们在PASCAL VOC 2007和MS COCO基准任务上评估了我们提出的架构，STDN在可比的最先进检测模型上获得了显著的改进。","领域":"目标检测/多尺度分析/超分辨率","问题":"解决目标检测中的多尺度问题","动机":"探索并利用多个检测尺度之间的尺度一致性性质，以提高目标检测的准确性和效率","方法":"开发了一种新颖的尺度可转移检测网络（STDN），该网络包含嵌入式超分辨率层（尺度转移层/模块），并与密集卷积网络（DenseNet）集成，形成一个单阶段目标检测器","关键词":["目标检测","多尺度分析","超分辨率","尺度转移","密集卷积网络"],"涉及的技术概念":"尺度可转移检测网络（STDN）是一种新颖的网络架构，旨在通过嵌入式超分辨率层（尺度转移层/模块）来探索和利用多个检测尺度之间的尺度一致性性质。这种方法与密集卷积网络（DenseNet）集成，形成一个单阶段目标检测器，旨在提高目标检测的准确性和效率。"},{"order":55,"title":"A Prior-Less Method for Multi-Face Tracking in Unconstrained Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_A_Prior-Less_Method_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_A_Prior-Less_Method_CVPR_2018_paper.html","abstract":"This paper presents a prior-less method for tracking and clustering an unknown number of human faces and maintaining their individual identities in unconstrained videos. The key challenge is to accurately track faces with partial occlusion and drastic appearance changes in multiple shots resulting from significant variations of makeup, facial expression, head pose and illumination. To address this challenge, we propose a new multi-face tracking and re-identification algorithm, which provides high accuracy in face association in the entire video with automatic cluster number generation, and is robust to outliers. We develop a co-occurrence model of multiple body parts to seamlessly create face tracklets, and recursively link tracklets to construct a graph for extracting clusters. A Gaussian Process model is introduced to compensate the deep feature insufficiency, and is further used to refine the linking results. The advantages of the proposed algorithm are demonstrated using a variety of challenging music videos and newly introduced body-worn camera videos. The proposed method obtains significant improvements over the state of the art [51], while relying less on handling video-specific prior information to achieve high performance.","中文标题":"无先验方法在无约束视频中的多面部追踪","摘要翻译":"本文提出了一种无先验方法，用于在无约束视频中追踪和聚类未知数量的人脸，并保持其个体身份。关键挑战在于准确追踪在多个镜头中由于化妆、面部表情、头部姿势和光照的显著变化而导致的部分遮挡和剧烈外观变化的人脸。为了解决这一挑战，我们提出了一种新的多面部追踪和重新识别算法，该算法在整个视频中提供高精度的人脸关联，并自动生成聚类数量，对异常值具有鲁棒性。我们开发了一种多身体部位的共现模型，以无缝创建面部轨迹，并递归地链接轨迹以构建用于提取聚类的图。引入了高斯过程模型以补偿深度特征的不足，并进一步用于精炼链接结果。所提出算法的优势通过使用各种具有挑战性的音乐视频和新引入的佩戴式摄像机视频得到展示。所提出的方法在依赖较少处理视频特定先验信息的情况下，实现了对现有技术[51]的显著改进，同时实现了高性能。","领域":"面部识别/视频分析/聚类算法","问题":"在无约束视频中准确追踪和聚类未知数量的人脸，并保持其个体身份","动机":"解决在多个镜头中由于化妆、面部表情、头部姿势和光照的显著变化而导致的部分遮挡和剧烈外观变化的人脸追踪问题","方法":"提出了一种新的多面部追踪和重新识别算法，包括开发多身体部位的共现模型以创建面部轨迹，递归地链接轨迹以构建图，引入高斯过程模型以补偿深度特征的不足并精炼链接结果","关键词":["面部追踪","聚类","重新识别"],"涉及的技术概念":"多面部追踪和重新识别算法、多身体部位的共现模型、高斯过程模型、深度特征、视频特定先验信息"},{"order":56,"title":"End-to-End Flow Correlation Tracking With Spatial-Temporal Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_End-to-End_Flow_Correlation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_End-to-End_Flow_Correlation_CVPR_2018_paper.html","abstract":"Discriminative correlation filters (DCF) with deep convolutional features have achieved favorable performance in recent tracking benchmarks. However, most of existing DCF trackers only consider appearance features of current frame, and hardly benefit from motion and inter-frame information. The lack of temporal information degrades the tracking performance during challenges such as partial occlusion and deformation. In this paper, we propose the FlowTrack, which focuses on making use of the rich flow information in consecutive frames to improve the feature representation and the tracking accuracy. The FlowTrack formulates individual components, including optical flow estimation, feature extraction, aggregation and correlation filters tracking as special layers in network. To the best of our knowledge, this is the first work to jointly train flow and tracking task in deep learning framework. Then the historical feature maps at predefined intervals are warped and aggregated with current ones by the guiding of flow. For adaptive aggregation, we propose a novel spatial-temporal attention mechanism. In experiments, the proposed method achieves leading performance on OTB2013, OTB2015, VOT2015 and VOT2016.","中文标题":"端到端流相关跟踪与时空注意力机制","摘要翻译":"近年来，结合深度卷积特征的判别相关滤波器（DCF）在跟踪基准测试中取得了良好的性能。然而，大多数现有的DCF跟踪器仅考虑当前帧的外观特征，很少从运动和帧间信息中受益。时间信息的缺乏在部分遮挡和变形等挑战中降低了跟踪性能。在本文中，我们提出了FlowTrack，它专注于利用连续帧中的丰富流信息来提高特征表示和跟踪准确性。FlowTrack将光流估计、特征提取、聚合和相关滤波器跟踪等各个组件制定为网络中的特殊层。据我们所知，这是首次在深度学习框架中联合训练流和跟踪任务的工作。然后，通过流的指导，将预定义间隔的历史特征图与当前特征图进行变形和聚合。为了自适应聚合，我们提出了一种新颖的时空注意力机制。在实验中，所提出的方法在OTB2013、OTB2015、VOT2015和VOT2016上取得了领先的性能。","领域":"目标跟踪/光流估计/深度学习","问题":"现有判别相关滤波器跟踪器在部分遮挡和变形等挑战中性能下降的问题","动机":"提高跟踪性能，特别是在部分遮挡和变形等挑战中","方法":"提出FlowTrack，利用连续帧中的流信息，通过光流估计、特征提取、聚合和相关滤波器跟踪等组件，以及一种新颖的时空注意力机制来提高特征表示和跟踪准确性","关键词":["目标跟踪","光流估计","时空注意力机制"],"涉及的技术概念":"判别相关滤波器（DCF）、深度卷积特征、光流估计、特征提取、特征聚合、时空注意力机制"},{"order":57,"title":"Deep Texture Manifold for Ground Terrain Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xue_Deep_Texture_Manifold_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xue_Deep_Texture_Manifold_CVPR_2018_paper.html","abstract":"We present a texture network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition. Recognition of ground terrain is an important task in establishing robot or vehicular control parameters, as well as for localization within an outdoor environment. The architecture of DEP integrates orderless texture details and local spatial information and the performance of DEP surpasses state-of-the-art methods for this task. The GTOS database (comprised of over 30,000 images of 40 classes of ground terrain in outdoor scenes) enables supervised recognition. For evaluation under realistic conditions, we use test images that are not from the existing GTOS dataset, but are instead from hand-held mobile phone videos of similar terrain. This new evaluation dataset, GTOS-mobile, consists of 81 videos of 31 classes of ground terrain such as grass, gravel, asphalt and sand. The resultant network shows excellent performance not only for GTOS-mobile, but also for more general databases (MINC and DTD). Leveraging the discriminant features learned from this network, we build a new texture manifold called DEP-manifold. We learn a parametric distribution in feature space in a fully supervised manner, which gives the distance relationship among classes and provides a means to implicitly represent ambiguous class boundaries. The source code and database are publicly available.","中文标题":"深度纹理流形用于地面地形识别","摘要翻译":"我们提出了一种称为深度编码池化网络（DEP）的纹理网络，用于地面地形识别任务。地面地形识别在建立机器人或车辆控制参数以及户外环境中的定位中是一个重要任务。DEP的架构整合了无序纹理细节和局部空间信息，其性能超越了该任务的最先进方法。GTOS数据库（包含超过30,000张户外场景中40类地面地形的图像）支持有监督的识别。为了在现实条件下进行评估，我们使用的测试图像不是来自现有的GTOS数据集，而是来自手持手机拍摄的类似地形视频。这个新的评估数据集，GTOS-mobile，由81个视频组成，涵盖了31类地面地形，如草地、碎石、沥青和沙子。结果网络不仅在GTOS-mobile上表现出色，而且在更一般的数据库（MINC和DTD）上也表现出色。利用从该网络学习到的判别特征，我们构建了一个新的纹理流形，称为DEP流形。我们在特征空间中以完全监督的方式学习参数分布，这给出了类别之间的距离关系，并提供了一种隐式表示模糊类别边界的方法。源代码和数据库是公开可用的。","领域":"地面地形识别/纹理分析/机器人控制","问题":"地面地形识别","动机":"建立机器人或车辆控制参数以及户外环境中的定位","方法":"提出深度编码池化网络（DEP），整合无序纹理细节和局部空间信息，构建DEP流形以学习参数分布","关键词":["地面地形识别","纹理网络","深度编码池化网络","DEP流形"],"涉及的技术概念":{"深度编码池化网络（DEP）":"一种用于地面地形识别的纹理网络，整合了无序纹理细节和局部空间信息","GTOS数据库":"包含超过30,000张户外场景中40类地面地形的图像，用于有监督的识别","GTOS-mobile":"一个新的评估数据集，由81个视频组成，涵盖了31类地面地形，用于在现实条件下评估网络性能","DEP流形":"利用从DEP网络学习到的判别特征构建的纹理流形，用于学习参数分布并隐式表示模糊类别边界"}},{"order":58,"title":"Learning Superpixels With Segmentation-Aware Affinity Loss","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tu_Learning_Superpixels_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tu_Learning_Superpixels_With_CVPR_2018_paper.html","abstract":"Superpixel segmentation has been widely used in many computer vision tasks. Existing superpixel algorithms are mainly based on hand-crafted features, which often fail to preserve weak object boundaries. In this work, we leverage deep neural networks to facilitate extracting superpixels from images. We show a simple integration of deep features with existing superpixel algorithms does not result in better performance as these features do not model segmentation. Instead, we propose a segmentation-aware affinity learning approach for superpixel segmentation. Specifically, we propose a new loss function that takes the segmentation error into account for affinity learning. We also develop the Pixel Affinity Net for affinity prediction. Extensive experimental results show that the proposed algorithm based on the learned segmentation-aware loss performs favorably against the state-of-the-art methods. We also demonstrate the use of the learned superpixels in numerous vision applications with consistent improvements.","中文标题":"学习具有分割感知亲和力损失的超级像素","摘要翻译":"超级像素分割已被广泛应用于许多计算机视觉任务中。现有的超级像素算法主要基于手工制作的特征，这些特征往往无法保留弱对象边界。在这项工作中，我们利用深度神经网络来促进从图像中提取超级像素。我们展示了将深度特征与现有超级像素算法简单集成并不会带来更好的性能，因为这些特征没有建模分割。相反，我们提出了一种分割感知的亲和力学习方法用于超级像素分割。具体来说，我们提出了一种新的损失函数，该函数考虑了分割误差以进行亲和力学习。我们还开发了用于亲和力预测的像素亲和力网络。大量的实验结果表明，基于学习到的分割感知损失的算法在性能上优于最先进的方法。我们还展示了学习到的超级像素在众多视觉应用中的使用，并取得了持续的改进。","领域":"图像分割/深度学习/计算机视觉","问题":"现有超级像素算法无法有效保留弱对象边界","动机":"提高超级像素分割的准确性和性能，特别是在保留弱对象边界方面","方法":"提出了一种分割感知的亲和力学习方法，包括新的损失函数和像素亲和力网络","关键词":["超级像素分割","分割感知","亲和力学习"],"涉及的技术概念":"深度神经网络用于提取超级像素，分割感知的亲和力学习方法，新的损失函数考虑了分割误差，像素亲和力网络用于亲和力预测"},{"order":59,"title":"Interactive Image Segmentation With Latent Diversity","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Interactive_Image_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Interactive_Image_Segmentation_CVPR_2018_paper.html","abstract":"Interactive image segmentation is characterized by multimodality. When the user clicks on a door, do they intend to select the door or the whole house? We present an end-to-end learning approach to interactive image segmentation that tackles this ambiguity. Our architecture couples two convolutional networks. The first is trained to synthesize a diverse set of plausible segmentations that conform to the user's input. The second is trained to select among these. By selecting a single solution, our approach retains compatibility with existing interactive segmentation interfaces. By synthesizing multiple diverse solutions before selecting one, the architecture is given the representational power to explore the multimodal solution space. We show that the proposed approach outperforms existing methods for interactive image segmentation, including prior work that applied convolutional networks to this problem, while being much faster.","中文标题":"具有潜在多样性的交互式图像分割","摘要翻译":"交互式图像分割的特点是多模态性。当用户点击一扇门时，他们是想选择门还是整个房子？我们提出了一种端到端的学习方法来解决这种模糊性。我们的架构结合了两个卷积网络。第一个网络被训练来合成一组符合用户输入的多样化可能分割。第二个网络被训练来从这些分割中选择一个。通过选择一个解决方案，我们的方法保持了与现有交互式分割界面的兼容性。通过在选择一个解决方案之前合成多个多样化的解决方案，该架构被赋予了探索多模态解决方案空间的表示能力。我们展示了所提出的方法在交互式图像分割方面优于现有方法，包括之前将卷积网络应用于此问题的工作，同时速度更快。","领域":"图像分割/卷积神经网络/多模态学习","问题":"解决交互式图像分割中的多模态性和用户意图模糊性问题","动机":"提高交互式图像分割的准确性和效率，同时保持与现有界面的兼容性","方法":"采用两个卷积网络，一个用于合成多样化的分割方案，另一个用于选择最合适的方案","关键词":["交互式图像分割","卷积神经网络","多模态学习"],"涉及的技术概念":"交互式图像分割是一种允许用户通过简单交互（如点击）来指导分割过程的技术。多模态性指的是对于同一用户输入，可能存在多种合理的分割结果。卷积神经网络是一种深度学习模型，特别适用于处理图像数据。端到端学习指的是直接从输入到输出进行学习，无需手动设计中间步骤。"},{"order":60,"title":"The Unreasonable Effectiveness of Deep Features as a Perceptual Metric","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_The_Unreasonable_Effectiveness_CVPR_2018_paper.html","abstract":"While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the  underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called \`\`perceptual losses\\"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.","中文标题":"深度特征作为感知度量的不合理有效性","摘要翻译":"尽管人类几乎可以毫不费力地快速评估两幅图像之间的感知相似性，但背后的过程被认为是非常复杂的。尽管如此，今天最广泛使用的感知度量，如PSNR和SSIM，是简单、浅层的函数，未能考虑到人类感知的许多细微差别。最近，深度学习社区发现，在ImageNet分类上训练的VGG网络的特征作为图像合成的训练损失非常有用。但这些所谓的“感知损失”有多感知？哪些元素对它们的成功至关重要？为了回答这些问题，我们引入了一个新的人类感知相似性判断数据集。我们系统地评估了不同架构和任务中的深度特征，并将它们与经典度量进行了比较。我们发现，深度特征在我们的数据集上大幅优于所有先前的度量。更令人惊讶的是，这一结果不仅限于ImageNet训练的VGG特征，而是适用于不同的深度架构和监督级别（有监督、自监督，甚至无监督）。我们的结果表明，感知相似性是深度视觉表示中共享的一个涌现属性。","领域":"图像合成/感知相似性/深度视觉表示","问题":"如何评估和比较不同深度特征在感知相似性度量上的有效性","动机":"现有的感知度量如PSNR和SSIM未能充分考虑到人类感知的复杂性，而深度特征在图像合成中的应用显示出潜力，需要系统评估其作为感知度量的有效性","方法":"引入新的人类感知相似性判断数据集，系统评估不同深度架构和任务中的深度特征，并与经典度量进行比较","关键词":["感知相似性","深度特征","图像合成"],"涉及的技术概念":"PSNR（峰值信噪比）和SSIM（结构相似性）是两种广泛使用的图像质量评估度量。VGG网络是一种深度卷积神经网络，常用于图像识别任务。ImageNet是一个大型视觉数据库，用于视觉对象识别软件研究。感知损失是指使用深度神经网络的特征作为损失函数，以指导图像合成等任务的训练过程。"},{"order":61,"title":"Local Descriptors Optimized for Average Precision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_Local_Descriptors_Optimized_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Local_Descriptors_Optimized_CVPR_2018_paper.html","abstract":"Extraction of local feature descriptors is a vital stage in the solution pipelines for numerous computer vision tasks. Learning-based approaches improve performance in certain tasks, but still cannot replace handcrafted features in general. In this paper, we improve the learning of local feature descriptors by optimizing the performance of descriptor matching, which is a common stage that follows descriptor extraction in local feature based pipelines, and can be formulated as nearest neighbor retrieval. Specifically, we directly optimize a ranking-based retrieval performance metric, Average Precision, using deep neural networks. This general-purpose solution can also be viewed as a listwise learning to rank approach, which is advantageous compared to recent local ranking approaches. On standard benchmarks, descriptors learned with our formulation achieve state-of-the-art results in patch verification, patch retrieval, and image matching.","中文标题":"针对平均精度优化的局部描述符","摘要翻译":"局部特征描述符的提取是众多计算机视觉任务解决方案流程中的一个关键阶段。基于学习的方法在某些任务中提高了性能，但在一般情况下仍无法替代手工特征。在本文中，我们通过优化描述符匹配的性能来改进局部特征描述符的学习，这是基于局部特征的流程中描述符提取后的一个常见阶段，并且可以表述为最近邻检索。具体来说，我们直接使用深度神经网络优化基于排名的检索性能指标——平均精度。这种通用解决方案也可以被视为一种列表式学习排序方法，与最近的局部排序方法相比具有优势。在标准基准测试中，使用我们的公式学习的描述符在补丁验证、补丁检索和图像匹配方面达到了最先进的结果。","领域":"特征提取/描述符匹配/图像匹配","问题":"如何提高局部特征描述符的学习效果，以优化描述符匹配的性能","动机":"尽管基于学习的方法在某些任务中提高了性能，但在一般情况下仍无法替代手工特征，因此需要改进局部特征描述符的学习方法","方法":"使用深度神经网络直接优化基于排名的检索性能指标——平均精度，采用列表式学习排序方法","关键词":["局部特征描述符","描述符匹配","平均精度","深度神经网络","列表式学习排序"],"涉及的技术概念":{"局部特征描述符":"用于描述图像局部区域特征的描述符，是计算机视觉任务中的关键组成部分","描述符匹配":"在计算机视觉中，用于比较和匹配不同图像中局部特征描述符的过程","平均精度":"一种评估检索系统性能的指标，特别是在信息检索和机器学习中，用于衡量检索结果的相关性","深度神经网络":"一种模仿人脑结构和功能的计算模型，用于处理和分析大量数据，特别是在图像和语音识别等领域","列表式学习排序":"一种机器学习方法，旨在优化整个检索列表的排序，而不是单个项目的排序"}},{"order":62,"title":"Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.html","abstract":"Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN and EnhanceNet.","中文标题":"通过深度空间特征变换恢复图像超分辨率中的真实纹理","摘要翻译":"尽管卷积神经网络（CNN）最近在单图像超分辨率（SR）重建中展示了高质量的重建能力，恢复自然和真实的纹理仍然是一个具有挑战性的问题。在本文中，我们展示了恢复忠实于语义类别的纹理是可能的。特别是，我们只需要根据语义分割概率图调节单个网络中少数中间层的特征。这是通过一种新颖的空间特征变换（SFT）层实现的，该层生成用于空间特征调制的仿射变换参数。SFT层可以与SR网络一起使用相同的损失函数进行端到端训练。在测试期间，它接受任意大小的输入图像，并根据类别先验条件仅通过一次前向传递生成高分辨率图像。我们的最终结果表明，与最先进的SRGAN和EnhanceNet相比，配备SFT的SR网络可以生成更真实和视觉上更令人愉悦的纹理。","领域":"图像超分辨率/纹理恢复/语义分割","问题":"恢复图像超分辨率中的自然和真实纹理","动机":"尽管CNN在图像超分辨率重建中取得了进展，但恢复真实纹理仍然是一个挑战，需要新的方法来解决。","方法":"提出了一种新颖的空间特征变换（SFT）层，通过生成仿射变换参数来调节中间层特征，从而实现端到端的训练，并在测试时根据语义分割概率图生成高分辨率图像。","关键词":["图像超分辨率","纹理恢复","语义分割","空间特征变换","仿射变换"],"涉及的技术概念":"卷积神经网络（CNN）用于图像超分辨率重建，空间特征变换（SFT）层用于生成仿射变换参数以调节特征，语义分割概率图用于条件生成高分辨率图像。"},{"order":63,"title":"Deep Extreme Cut: From Extreme Points to Object Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Maninis_Deep_Extreme_Cut_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Maninis_Deep_Extreme_Cut_CVPR_2018_paper.html","abstract":"This paper explores the use of extreme points in an object (left-most, right-most, top, bottom pixels) as input to obtain precise object segmentation for images and videos. We do so by adding an extra channel to the image in the input of a convolutional neural network (CNN), which contains a Gaussian centered in each of the extreme points. The CNN learns to transform this information into a segmentation of an object that matches those extreme points.  We demonstrate the usefulness of this approach for guided segmentation (grabcut-style), interactive segmentation, video object segmentation, and dense segmentation annotation. We show that we obtain the most precise results to date, also with less user input, in an extensive and varied selection of benchmarks and datasets. All our models and code are publicly available on http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr.","中文标题":"深度极端切割：从极端点到对象分割","摘要翻译":"本文探讨了使用对象的极端点（最左、最右、顶部、底部像素）作为输入，以获得图像和视频的精确对象分割。我们通过在卷积神经网络（CNN）的输入中为图像添加一个额外通道来实现这一点，该通道包含以每个极端点为中心的高斯分布。CNN学习将这些信息转换为与这些极端点匹配的对象分割。我们展示了这种方法在引导分割（grabcut风格）、交互式分割、视频对象分割和密集分割注释中的实用性。我们展示了在广泛且多样的基准和数据集选择中，我们以较少的用户输入获得了迄今为止最精确的结果。我们所有的模型和代码都可以在http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr上公开获取。","领域":"对象分割/视频处理/图像注释","问题":"如何从图像的极端点精确分割对象","动机":"探索使用极端点作为输入，以提高对象分割的精确度和减少用户输入的需求","方法":"在卷积神经网络的输入中添加包含极端点高斯分布的额外通道，使CNN学习将极端点信息转换为对象分割","关键词":["极端点","对象分割","卷积神经网络","高斯分布","引导分割","交互式分割","视频对象分割","密集分割注释"],"涉及的技术概念":{"极端点":"对象的最左、最右、顶部、底部像素","卷积神经网络（CNN）":"一种深度学习模型，用于处理图像数据","高斯分布":"一种概率分布，用于在图像中标记极端点的位置","引导分割":"一种分割技术，通过用户输入引导分割过程","交互式分割":"允许用户交互式地调整分割结果","视频对象分割":"在视频序列中分割出特定对象","密集分割注释":"为图像中的每个像素提供注释的分割技术"}},{"order":64,"title":"Learning to Parse Wireframes in Images of Man-Made Environments","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_to_Parse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Learning_to_Parse_CVPR_2018_paper.html","abstract":"In this paper, we propose a learning-based approach to the task of automatically extracting a \\"wireframe\\" representation for images of cluttered man-made environments. The wireframe contains all salient straight lines and their junctions of the scene that encode efficiently and accurately large-scale geometry and object shapes. To this end, we have built a very large new dataset of over 5,000 images with wireframes thoroughly labelled by humans. We have proposed two convolutional neural networks that are suitable for extracting junctions and lines with large spatial support, respectively. The networks trained on our dataset have achieved significantly better performance than state-of-the-art methods for junction detection and line segment detection, respectively. We have conducted extensive experiments to evaluate quantitatively and qualitatively the wireframes obtained by our method, and have convincingly shown that effectively and efficiently parsing wireframes for images of man-made environments is a feasible goal within reach. Such wireframes could benefit many important visual tasks such as feature correspondence, 3D reconstruction, vision-based mapping, localization, and navigation.","中文标题":"学习解析人造环境图像中的线框","摘要翻译":"在本文中，我们提出了一种基于学习的方法，用于自动提取杂乱人造环境图像的“线框”表示。该线框包含场景中所有显著的直线及其交汇点，这些直线和交汇点有效地编码了大尺度的几何形状和物体形状。为此，我们构建了一个非常大的新数据集，包含超过5,000张图像，这些图像的线框由人类彻底标记。我们提出了两个卷积神经网络，分别适用于提取具有大空间支持的交叉点和线条。在我们数据集上训练的网络分别在交叉点检测和线段检测方面取得了显著优于现有技术的性能。我们进行了广泛的实验，定量和定性地评估了我们方法获得的线框，并令人信服地表明，有效且高效地解析人造环境图像的线框是一个可实现的目标。这样的线框可以有益于许多重要的视觉任务，如特征对应、3D重建、基于视觉的映射、定位和导航。","领域":"几何解析/场景理解/视觉定位","问题":"自动提取杂乱人造环境图像的线框表示","动机":"为了有效地编码大尺度的几何形状和物体形状，以及支持多种视觉任务","方法":"提出了两个卷积神经网络，分别用于提取具有大空间支持的交叉点和线条，并在大规模数据集上进行训练","关键词":["线框解析","卷积神经网络","几何编码"],"涉及的技术概念":"线框表示：一种包含场景中所有显著直线及其交汇点的表示方法，用于编码大尺度的几何形状和物体形状。卷积神经网络：一种深度学习模型，特别适用于处理图像数据，能够自动提取图像特征。"},{"order":65,"title":"Occlusion-Aware Rolling Shutter Rectification of 3D Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vasu_Occlusion-Aware_Rolling_Shutter_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vasu_Occlusion-Aware_Rolling_Shutter_CVPR_2018_paper.html","abstract":"A vast majority of contemporary cameras employ rolling shutter (RS) mechanism to capture images. Due to the sequential mechanism, images acquired with a moving camera are subjected to rolling shutter effect which manifests as geometric distortions. In this work, we consider the specific scenario of a fast moving camera wherein the rolling shutter distortions not only are predominant but also become depth-dependent which in turn results in intra-frame occlusions. To this end, we develop a first-of-its-kind pipeline to recover the latent image of a 3D scene from a set of such RS distorted images. The proposed approach sequentially recovers both the camera motion and scene structure while accounting for RS and occlusion effects. Subsequently, we perform depth and occlusion-aware rectification of RS images to yield the desired latent image. Our experiments on synthetic and real image sequences reveal that the proposed approach achieves state-of-the-art results.","中文标题":"遮挡感知的3D场景滚动快门校正","摘要翻译":"绝大多数现代相机采用滚动快门（RS）机制来捕捉图像。由于这种顺序机制，使用移动相机拍摄的图像会受到滚动快门效应的影响，表现为几何失真。在这项工作中，我们考虑了一个快速移动相机的特定场景，其中滚动快门失真不仅占主导地位，而且变得依赖于深度，这反过来又导致了帧内遮挡。为此，我们开发了一种前所未有的管道，从一组这样的RS失真图像中恢复3D场景的潜在图像。所提出的方法依次恢复相机运动和场景结构，同时考虑RS和遮挡效应。随后，我们执行深度和遮挡感知的RS图像校正，以产生所需的潜在图像。我们在合成和真实图像序列上的实验表明，所提出的方法达到了最先进的结果。","领域":"3D重建/图像校正/遮挡处理","问题":"解决快速移动相机拍摄图像时，由于滚动快门效应导致的几何失真和深度依赖性遮挡问题","动机":"为了从滚动快门失真的图像中恢复出3D场景的潜在图像，需要一种能够同时处理相机运动、场景结构、滚动快门效应和遮挡效应的方法","方法":"开发了一种新的管道，依次恢复相机运动和场景结构，同时考虑滚动快门和遮挡效应，然后进行深度和遮挡感知的滚动快门图像校正","关键词":["滚动快门","3D重建","遮挡处理","图像校正"],"涉及的技术概念":"滚动快门效应（RS effect）是指由于相机采用滚动快门机制，在捕捉快速移动场景时，图像的不同部分在不同时间被捕捉，导致图像出现几何失真的现象。深度依赖性遮挡（depth-dependent occlusion）指的是在3D场景中，由于物体的深度不同，导致某些物体在图像中被其他物体遮挡的现象。"},{"order":66,"title":"Content-Sensitive Supervoxels via Uniform Tessellations on Video Manifolds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yi_Content-Sensitive_Supervoxels_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yi_Content-Sensitive_Supervoxels_via_CVPR_2018_paper.html","abstract":"Supervoxels are perceptually meaningful atomic regions in videos, obtained by grouping voxels that exhibit coherence in both appearance and motion. In this paper, we propose content-sensitive supervoxels (CSS), which are regularly-shaped 3D primitive volumes that possess the following characteristic: they are typically larger and longer in content-sparse regions (i.e., with homogeneous appearance and motion), and smaller and shorter in content-dense regions (i.e., with high variation of appearance and/or motion). To compute CSS, we map a video X to a 3-dimensional manifold M embedded in R^6, whose volume elements give a good measure of the content density in X. We propose an efficient Lloyd-like method with a splitting-merging scheme to compute a uniform tessellation on M, which induces the CSS in X. Theoretically our method has a good competitive ratio O(1). We also present a simple extension of CSS to stream CSS for processing long videos that cannot be loaded into main memory at once. We evaluate CSS, stream CSS and seven representative supervoxel methods on four video datasets. The results show that our method outperforms existing supervoxel methods.","中文标题":"通过视频流形上的均匀分割实现内容敏感的超级体素","摘要翻译":"超级体素是视频中感知上有意义的原子区域，通过将表现出外观和运动一致性的体素分组获得。在本文中，我们提出了内容敏感的超级体素（CSS），它们是规则形状的3D原始体积，具有以下特征：在内容稀疏的区域（即具有均匀外观和运动的区域）通常更大和更长，在内容密集的区域（即具有高变化的外观和/或运动的区域）更小和更短。为了计算CSS，我们将视频X映射到嵌入在R^6中的3维流形M，其体积元素很好地衡量了X中的内容密度。我们提出了一种有效的Lloyd-like方法，采用分割-合并方案来计算M上的均匀分割，从而在X中诱导出CSS。理论上，我们的方法具有良好的竞争比O(1)。我们还提出了CSS的一个简单扩展，即流CSS，用于处理无法一次性加载到主内存中的长视频。我们在四个视频数据集上评估了CSS、流CSS和七种代表性的超级体素方法。结果表明，我们的方法优于现有的超级体素方法。","领域":"视频分析/3D视觉/流形学习","问题":"如何在视频中有效地生成内容敏感的超级体素","动机":"为了在视频分析中更有效地处理内容稀疏和内容密集的区域，需要一种能够根据内容密度自动调整大小的超级体素生成方法。","方法":"提出了一种内容敏感的超级体素（CSS）生成方法，通过将视频映射到3维流形上，并采用Lloyd-like方法进行均匀分割，从而在视频中生成CSS。","关键词":["超级体素","内容敏感","均匀分割","视频流形","Lloyd-like方法"],"涉及的技术概念":{"超级体素":"视频中感知上有意义的原子区域，通过将表现出外观和运动一致性的体素分组获得。","内容敏感的超级体素（CSS）":"规则形状的3D原始体积，根据内容密度自动调整大小。","3维流形":"嵌入在R^6中的3维空间，用于衡量视频中的内容密度。","Lloyd-like方法":"一种用于均匀分割的算法，通过迭代优化分割结果。","流CSS":"CSS的扩展，用于处理无法一次性加载到主内存中的长视频。"}},{"order":67,"title":"Intrinsic Image Transformation via Scale Space Decomposition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Intrinsic_Image_Transformation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Intrinsic_Image_Transformation_CVPR_2018_paper.html","abstract":"We introduce a new network structure for decomposing an image into its intrinsic albedo and shading. We treat this as an image-to-image transformation problem and explore the scale space of the input and output. By expanding the output images (albedo and shading) into their Laplacian pyramid components, we develop a multi-channel network structure that learns the image-to-image transformation function in successive frequency bands in parallel, within each channel is a fully convolutional neural network with skip connections. This network structure is general and extensible, and has demonstrated excellent performance on the intrinsic image decomposition problem.  We evaluate the network on two benchmark datasets: the MPI-Sintel dataset and the MIT Intrinsic Images dataset. Both quantitative and qualitative results show our model delivers a clear progression over state-of-the-art.","中文标题":"通过尺度空间分解进行本征图像变换","摘要翻译":"我们引入了一种新的网络结构，用于将图像分解为其本征反射率和阴影。我们将此视为图像到图像的变换问题，并探索输入和输出的尺度空间。通过将输出图像（反射率和阴影）扩展为它们的拉普拉斯金字塔组件，我们开发了一种多通道网络结构，该结构在连续频带中并行学习图像到图像的变换函数，每个通道内是一个带有跳跃连接的全卷积神经网络。这种网络结构是通用且可扩展的，并在本征图像分解问题上展示了卓越的性能。我们在两个基准数据集上评估了网络：MPI-Sintel数据集和MIT本征图像数据集。定量和定性结果均显示我们的模型在现有技术基础上有了明显的进步。","领域":"图像分解/尺度空间分析/卷积神经网络","问题":"本征图像分解","动机":"探索图像到图像的变换问题，并解决本征图像分解的挑战","方法":"开发了一种多通道网络结构，通过将输出图像扩展为拉普拉斯金字塔组件，在连续频带中并行学习图像到图像的变换函数","关键词":["本征图像分解","尺度空间","拉普拉斯金字塔","全卷积神经网络","跳跃连接"],"涉及的技术概念":"本征图像分解指的是将图像分解为反射率和阴影两个本征成分的过程。尺度空间分析是一种在不同尺度上分析图像特征的方法。拉普拉斯金字塔是一种多分辨率分析技术，用于图像处理。全卷积神经网络是一种用于图像处理的深度学习模型，能够处理任意大小的输入图像。跳跃连接是一种网络结构设计，用于改善深层网络中的梯度流动。"},{"order":68,"title":"Learned Shape-Tailored Descriptors for Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Khan_Learned_Shape-Tailored_Descriptors_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Khan_Learned_Shape-Tailored_Descriptors_CVPR_2018_paper.html","abstract":"We address the problem of texture segmentation by grouping dense pixel-wise descriptors. We introduce and construct learned Shape-Tailored Descriptors that aggregate image statistics only within regions of interest to avoid mixing statistics of different textures, and that are invariant to complex nuisances (e.g., illumination, perspective and deformations). This is accomplished by training a neural network to discriminate base shape-tailored descriptors of oriented gradients at various scales. These descriptors are defined through partial differential equations to obtain data at various scales in arbitrarily shaped regions. We formulate and optimize a joint optimization problem in the segmentation and descriptors to discriminate these base descriptors using the learned metric, equivalent to grouping learned descriptors. We test the method on datasets to illustrate the effect of both the shape-tailored and learned properties of the descriptors. Experiments show that the descriptors learned on a small dataset of segmented images generalize well to unseen textures in other datasets, showing the generic nature of these descriptors. We show stateof- the-art results on texture segmentation benchmarks.","中文标题":"学习形状定制描述符用于分割","摘要翻译":"我们通过分组密集像素级描述符来解决纹理分割问题。我们引入并构建了学习形状定制描述符，这些描述符仅在感兴趣区域内聚合图像统计信息，以避免混合不同纹理的统计信息，并且对复杂的干扰（例如，光照、视角和变形）保持不变。这是通过训练神经网络来区分各种尺度的定向梯度的基础形状定制描述符来实现的。这些描述符通过偏微分方程定义，以在任意形状区域内获取各种尺度的数据。我们制定并优化了分割和描述符中的联合优化问题，以使用学习到的度量来区分这些基础描述符，相当于分组学习到的描述符。我们在数据集上测试了该方法，以说明描述符的形状定制和学习特性的效果。实验表明，在分割图像的小数据集上学习的描述符能够很好地推广到其他数据集中的未见纹理，显示了这些描述符的通用性。我们在纹理分割基准上展示了最先进的结果。","领域":"纹理分割/图像描述符/神经网络","问题":"纹理分割中的密集像素级描述符分组问题","动机":"避免混合不同纹理的统计信息，并对复杂的干扰保持不变","方法":"训练神经网络来区分各种尺度的定向梯度的基础形状定制描述符，并通过偏微分方程定义描述符，在任意形状区域内获取各种尺度的数据，优化分割和描述符中的联合优化问题","关键词":["纹理分割","形状定制描述符","神经网络"],"涉及的技术概念":"学习形状定制描述符通过训练神经网络来区分各种尺度的定向梯度的基础形状定制描述符，这些描述符通过偏微分方程定义，以在任意形状区域内获取各种尺度的数据。"},{"order":69,"title":"PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PAD-Net_Multi-Tasks_Guided_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_PAD-Net_Multi-Tasks_Guided_CVPR_2018_paper.html","abstract":"Depth estimation and scene parsing are two particularly important tasks in visual scene understanding. In this paper we tackle the problem of simultaneous depth estimation and scene parsing in a joint CNN. The task can be typically treated as a deep multi-task learning problem [42]. Different from previous methods directly optimizing multiple tasks given the input training data, this paper proposes a novel multi-task guided prediction-and-distillation network (PAD-Net), which first predicts a set of intermediate auxiliary tasks ranging from low level to high level, and then the predictions from these intermediate auxiliary tasks are utilized as multi-modal input via our proposed multi-modal distillation modules for the final tasks. During the joint learning, the intermediate tasks not only act as supervision for learning more robust deep representations but also pro- vide rich multi-modal information for improving the final tasks. Extensive experiments are conducted on two challenging datasets (i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing tasks, demonstrating the effectiveness of the proposed approach.","中文标题":"PAD-Net：多任务引导的预测与蒸馏网络用于同时进行深度估计和场景解析","摘要翻译":"深度估计和场景解析是视觉场景理解中两个特别重要的任务。在本文中，我们解决了在联合CNN中同时进行深度估计和场景解析的问题。该任务通常可以视为一个深度多任务学习问题[42]。与之前的方法直接优化给定输入训练数据的多个任务不同，本文提出了一种新颖的多任务引导预测与蒸馏网络（PAD-Net），该网络首先预测一组从低层次到高层次的中间辅助任务，然后通过我们提出的多模态蒸馏模块将这些中间辅助任务的预测作为多模态输入用于最终任务。在联合学习过程中，中间任务不仅作为学习更鲁棒深度表示的监督，而且为改进最终任务提供了丰富的多模态信息。在两个具有挑战性的数据集（即NYUD-v2和Cityscapes）上进行了广泛的实验，用于深度估计和场景解析任务，证明了所提出方法的有效性。","领域":"深度估计/场景解析/多任务学习","问题":"同时进行深度估计和场景解析","动机":"解决视觉场景理解中深度估计和场景解析两个重要任务，通过联合CNN和多任务学习提高任务性能","方法":"提出多任务引导预测与蒸馏网络（PAD-Net），通过预测中间辅助任务并利用这些任务的预测作为多模态输入来改进最终任务","关键词":["深度估计","场景解析","多任务学习","多模态蒸馏"],"涉及的技术概念":"CNN（卷积神经网络）、多任务学习、多模态蒸馏模块、深度估计、场景解析"},{"order":70,"title":"Multi-Image Semantic Matching by Mining Consistent Features","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Image_Semantic_Matching_CVPR_2018_paper.html","abstract":"This work proposes a multi-image matching method to estimate semantic correspondences across multiple images.  In contrast to the previous methods that optimize all pairwise correspondences, the proposed method identifies and matches only a sparse set of reliable features in the image collection. In this way, the proposed method is able to prune nonrepeatable features and also highly scalable to handle thousands of images. We additionally propose a low-rank constraint to ensure the geometric consistency of feature correspondences over the whole image collection. Besides the competitive performance on multi-graph matching and semantic flow benchmarks, we also demonstrate the applicability of the proposed method for reconstructing object-class models and discovering object-class landmarks from images without using any annotation.","中文标题":"通过挖掘一致性特征进行多图像语义匹配","摘要翻译":"本研究提出了一种多图像匹配方法，用于估计多幅图像之间的语义对应关系。与之前优化所有成对对应关系的方法不同，所提出的方法仅识别并匹配图像集合中的一组稀疏可靠特征。通过这种方式，所提出的方法能够修剪不可重复的特征，并且具有高度可扩展性，能够处理数千幅图像。我们还提出了一种低秩约束，以确保整个图像集合中特征对应关系的几何一致性。除了在多图匹配和语义流基准测试中的竞争性能外，我们还展示了所提出方法在重建对象类模型和从图像中发现对象类地标方面的适用性，而无需使用任何注释。","领域":"图像匹配/语义对应/对象模型重建","问题":"估计多幅图像之间的语义对应关系","动机":"提高多图像匹配的效率和准确性，通过识别和匹配稀疏可靠特征来修剪不可重复的特征，并确保特征对应关系的几何一致性","方法":"提出了一种多图像匹配方法，该方法通过识别和匹配图像集合中的一组稀疏可靠特征，并引入低秩约束来确保特征对应关系的几何一致性","关键词":["多图像匹配","语义对应","稀疏特征","低秩约束","对象模型重建"],"涉及的技术概念":"多图像匹配方法、稀疏可靠特征、低秩约束、几何一致性、对象类模型重建、对象类地标发现"},{"order":71,"title":"Density-Aware Single Image De-Raining Using a Multi-Stream Dense Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Density-Aware_Single_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Density-Aware_Single_Image_CVPR_2018_paper.html","abstract":"Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images. We present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method.","中文标题":"使用多流密集网络的密度感知单图像去雨","摘要翻译":"单图像雨条纹去除是一个极其具有挑战性的问题，因为图像中存在非均匀的雨密度。我们提出了一种新颖的密度感知多流密集连接卷积神经网络算法，称为DID-MDN，用于联合雨密度估计和去雨。所提出的方法使网络本身能够自动确定雨密度信息，然后根据估计的雨密度标签有效地去除相应的雨条纹。为了更好地表征不同尺度和形状的雨条纹，提出了一个多流密集连接的去雨网络，该网络有效地利用了来自不同尺度的特征。此外，创建了一个包含雨密度标签图像的新数据集，并用于训练所提出的密度感知网络。在合成和真实数据集上的大量实验表明，所提出的方法比最近的最先进方法有显著改进。此外，还进行了消融研究，以展示所提出方法中不同模块所获得的改进。","领域":"图像去雨/雨密度估计/密集连接网络","问题":"单图像雨条纹去除","动机":"由于图像中存在非均匀的雨密度，单图像雨条纹去除是一个极其具有挑战性的问题。","方法":"提出了一种密度感知多流密集连接卷积神经网络算法，用于联合雨密度估计和去雨。","关键词":["图像去雨","雨密度估计","密集连接网络","多流网络"],"涉及的技术概念":"DID-MDN算法是一种基于卷积神经网络的解决方案，它通过多流密集连接网络自动确定雨密度信息，并有效去除雨条纹。该方法利用了不同尺度的特征来更好地表征雨条纹，并通过新创建的数据集进行训练，以提高去雨效果。"},{"order":72,"title":"Joint Cuts and Matching of Partitions in One Graph","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Joint_Cuts_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Joint_Cuts_and_CVPR_2018_paper.html","abstract":"As two fundamental problems, graph cuts and graph matching have been intensively investigated over the decades, resulting in vast literature in these two topics respectively. However the way of jointly applying and solving graph cuts and matching receives few attention. In this paper, we first formalize the problem of simultaneously cutting a graph into two partitions i.e. graph cuts and establishing their correspondence i.e. graph matching. Then we develop an optimization algorithm by updating matching and cutting alternatively, provided with theoretical analysis. The efficacy of our algorithm is verified on both synthetic dataset and real-world images containing similar regions or structures.","中文标题":"图中分区的联合切割与匹配","摘要翻译":"作为两个基本问题，图切割和图匹配在过去几十年中被深入研究，分别在这两个主题上产生了大量文献。然而，联合应用和解决图切割与匹配的方法却很少受到关注。在本文中，我们首先将同时切割图成两个分区（即图切割）并建立它们之间的对应关系（即图匹配）的问题形式化。然后，我们开发了一种通过交替更新匹配和切割的优化算法，并提供了理论分析。我们的算法在包含相似区域或结构的合成数据集和真实世界图像上的有效性得到了验证。","领域":"图论/优化算法/图像分析","问题":"如何联合应用和解决图切割与匹配的问题","动机":"图切割和图匹配作为两个基本问题，虽然各自被深入研究，但联合应用和解决这两个问题的方法却很少受到关注。","方法":"开发了一种通过交替更新匹配和切割的优化算法，并提供了理论分析。","关键词":["图切割","图匹配","优化算法","图像分析"],"涉及的技术概念":"图切割指的是将图分割成两个或多个部分的过程，图匹配指的是在两个图之间建立对应关系的过程。本文提出的方法通过交替更新匹配和切割来联合解决这两个问题，并在合成数据集和真实世界图像上验证了算法的有效性。"},{"order":73,"title":"Progressive Attention Guided Recurrent Network for Salient Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Progressive_Attention_Guided_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Progressive_Attention_Guided_CVPR_2018_paper.html","abstract":"Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task. FCN-based methods directly apply multi-level convolutional features without distinction, which leads to sub-optimal results due to the distraction from redundant details. In this paper, we propose a novel attention guided network which selectively integrates multi-level contextual information in a progressive manner. Attentive features generated by our network can alleviate distraction of background thus achieve better performance. On the other hand, it is observed that most of existing algorithms conduct salient object detection by exploiting side-output features of the backbone feature extraction network. However, shallower layers of backbone network lack the ability to obtain global semantic information, which limits the effective feature learning. To address the problem, we introduce multi-path recurrent feedback to enhance our proposed progressive attention driven framework. Through multi-path recurrent connections, global semantic information from the top convolutional layer is transferred to shallower layers, which intrinsically refines the entire network. Experimental results on six benchmark datasets demonstrate that our algorithm performs favorably against the state-of-the-art approaches.","中文标题":"渐进式注意力引导的递归网络用于显著目标检测","摘要翻译":"有效的卷积特征在显著性估计中扮演着重要角色，但如何学习强大的特征用于显著性仍然是一个具有挑战性的任务。基于FCN的方法直接应用多级卷积特征而不加区分，这导致由于冗余细节的干扰而得到次优结果。在本文中，我们提出了一种新颖的注意力引导网络，它以渐进的方式选择性地整合多级上下文信息。我们的网络生成的注意力特征可以减轻背景的干扰，从而实现更好的性能。另一方面，观察到大多数现有算法通过利用骨干特征提取网络的侧输出特征来进行显著目标检测。然而，骨干网络的较浅层缺乏获取全局语义信息的能力，这限制了有效特征的学习。为了解决这个问题，我们引入了多路径递归反馈来增强我们提出的渐进式注意力驱动框架。通过多路径递归连接，来自顶层卷积层的全局语义信息被传递到较浅层，这本质上优化了整个网络。在六个基准数据集上的实验结果表明，我们的算法在性能上优于最先进的方法。","领域":"显著目标检测/注意力机制/递归网络","问题":"如何学习强大的特征用于显著性估计，以及如何有效地整合多级上下文信息以提高显著目标检测的性能","动机":"现有的基于FCN的方法直接应用多级卷积特征而不加区分，导致次优结果；骨干网络的较浅层缺乏获取全局语义信息的能力，限制了有效特征的学习","方法":"提出了一种新颖的注意力引导网络，以渐进的方式选择性地整合多级上下文信息，并引入多路径递归反馈来增强渐进式注意力驱动框架","关键词":["显著目标检测","注意力机制","递归网络","多级上下文信息","多路径递归反馈"],"涉及的技术概念":"FCN（全卷积网络）、注意力机制、递归网络、多级上下文信息、多路径递归反馈、全局语义信息"},{"order":74,"title":"Fast and Accurate Single Image Super-Resolution via Information Distillation Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_Fast_and_Accurate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Fast_and_Accurate_CVPR_2018_paper.html","abstract":"Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few number of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance.","中文标题":"通过信息蒸馏网络实现快速准确的单图像超分辨率","摘要翻译":"最近，深度卷积神经网络（CNNs）在单图像超分辨率方面取得了显著进展。然而，随着网络深度和宽度的增加，基于CNN的超分辨率方法在实践中面临着计算复杂性和内存消耗的挑战。为了解决上述问题，我们提出了一个深度但紧凑的卷积网络，直接从原始低分辨率图像重建高分辨率图像。通常，所提出的模型由三部分组成，分别是特征提取块、堆叠的信息蒸馏块和重建块。通过将增强单元与压缩单元结合到蒸馏块中，可以有效地提取局部长路径和短路径特征。具体来说，所提出的增强单元将两种不同类型的特征混合在一起，而压缩单元则为后续块蒸馏出更有用的信息。此外，由于每层滤波器数量相对较少以及使用组卷积，所提出的网络具有快速执行的优势。实验结果表明，所提出的方法优于最先进的方法，特别是在时间性能方面。","领域":"超分辨率/卷积神经网络/图像重建","问题":"解决单图像超分辨率中的计算复杂性和内存消耗问题","动机":"随着网络深度和宽度的增加，基于CNN的超分辨率方法在实践中面临计算复杂性和内存消耗的挑战，需要一种更高效的方法来实现快速准确的单图像超分辨率。","方法":"提出一个深度但紧凑的卷积网络，包括特征提取块、堆叠的信息蒸馏块和重建块，通过增强单元与压缩单元结合来有效提取特征，并利用较少的滤波器数量和组卷积实现快速执行。","关键词":["超分辨率","卷积神经网络","图像重建","信息蒸馏","组卷积"],"涉及的技术概念":{"深度卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征。","超分辨率":"一种技术，旨在从低分辨率图像中恢复出高分辨率图像。","信息蒸馏块":"网络中的一种结构，用于从输入数据中提取和蒸馏有用信息。","组卷积":"一种卷积操作，通过将输入通道分组来减少计算量和参数数量。","特征提取块":"网络的一部分，负责从输入图像中提取特征。","重建块":"网络的一部分，负责从提取的特征中重建高分辨率图像。"}},{"order":75,"title":"Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Hallucinated-IQA_No-Reference_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Hallucinated-IQA_No-Reference_Image_CVPR_2018_paper.html","abstract":"No-reference image quality assessment (NR-IQA) is a fundamental yet challenging task in low-level computer vision community. The difficulty is particularly pronounced for the limited information, for which the corresponding reference for comparison is typically absent. Although various feature extraction mechanisms have been leveraged from natural scene statistics to deep neural networks in previous methods, the performance bottleneck still exists.   In this work, we propose a hallucination-guided quality regression network to address the issue. We firstly generate a hallucinated reference constrained on the distorted image, to compensate the absence of the true reference. Then, we pair the information of hallucinated reference with the distorted image, and forward them to the regressor to learn the perceptual discrepancy with the guidance of an implicit ranking relationship within the generator, and therefore produce the precise quality prediction. To demonstrate the effectiveness of our approach, comprehensive experiments are evaluated on four popular image quality assessment benchmarks. Our method significantly outperforms all the previous state-of-the-art methods by large margins. The code and model are publicly available on the project page https://kwanyeelin.github.io/projects/HIQA/HIQA.html","中文标题":"幻觉引导的图像质量评估：通过对抗学习进行无参考图像质量评估","摘要翻译":"无参考图像质量评估（NR-IQA）是低级计算机视觉领域中的一个基础但具有挑战性的任务。对于信息有限的情况，这一难度尤为突出，因为通常缺乏相应的参考进行比较。尽管在之前的方法中已经利用了从自然场景统计到深度神经网络的各种特征提取机制，但性能瓶颈仍然存在。在这项工作中，我们提出了一种幻觉引导的质量回归网络来解决这个问题。我们首先在失真图像上生成一个受约束的幻觉参考，以补偿真实参考的缺失。然后，我们将幻觉参考的信息与失真图像配对，并将它们转发给回归器，以学习在生成器内隐式排名关系的指导下的感知差异，从而产生精确的质量预测。为了证明我们方法的有效性，我们在四个流行的图像质量评估基准上进行了全面的实验。我们的方法显著优于所有之前的最先进方法，差距很大。代码和模型已在项目页面https://kwanyeelin.github.io/projects/HIQA/HIQA.html上公开提供。","领域":"图像质量评估/对抗学习/深度学习","问题":"解决无参考图像质量评估中的性能瓶颈问题","动机":"由于缺乏真实参考图像，现有的无参考图像质量评估方法在信息有限的情况下表现不佳，需要一种新的方法来提高评估的准确性。","方法":"提出了一种幻觉引导的质量回归网络，通过生成幻觉参考图像并与失真图像配对，利用回归器学习感知差异，从而进行精确的质量预测。","关键词":["无参考图像质量评估","对抗学习","幻觉参考","质量回归网络"],"涉及的技术概念":"无参考图像质量评估（NR-IQA）是指在缺乏原始参考图像的情况下评估图像质量的技术。对抗学习是一种机器学习方法，通过让两个模型相互对抗来提高性能。幻觉参考图像是通过算法生成的，用于模拟或补偿缺失的真实参考图像。质量回归网络是一种深度学习模型，用于预测图像的质量分数。"},{"order":76,"title":"NAG: Network for Adversary Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mopuri_NAG_Network_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mopuri_NAG_Network_for_CVPR_2018_paper.html","abstract":"Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.","中文标题":"NAG: 对抗生成网络","摘要翻译":"对抗性扰动可能对部署机器学习系统构成严重威胁。最近的研究表明，存在图像无关的扰动，可以欺骗大多数自然图像上的分类器。现有方法提出了优化方法，通过解决带有不可察觉性约束的欺骗目标来制作扰动。然而，对于给定的分类器，它们一次只能生成一个扰动，这是对抗性扰动流形中的一个实例。此外，为了构建鲁棒模型，探索对抗性扰动的流形至关重要。在本文中，我们首次提出了一种生成方法来建模对抗性扰动的分布。所提出模型的架构受到GANs的启发，并使用欺骗和多样性目标进行训练。我们训练过的生成器网络试图捕捉给定分类器的对抗性扰动分布，并轻松生成各种此类扰动。我们的实验评估表明，由我们的模型制作的扰动（i）达到了最先进的欺骗率，（ii）表现出广泛的多样性，以及（iii）提供了出色的跨模型泛化能力。我们的工作可以被视为推断对抗性扰动复杂流形过程中的重要一步。","领域":"对抗性机器学习/生成对抗网络/模型鲁棒性","问题":"如何有效地生成多样化的对抗性扰动以增强模型的鲁棒性","动机":"探索对抗性扰动的流形对于构建鲁棒模型至关重要","方法":"提出了一种基于生成对抗网络（GANs）架构的生成方法，通过训练生成器网络来捕捉对抗性扰动的分布","关键词":["对抗性扰动","生成对抗网络","模型鲁棒性","欺骗率","多样性","跨模型泛化"],"涉及的技术概念":"对抗性扰动指的是对输入数据进行微小修改，以欺骗机器学习模型做出错误预测的扰动。生成对抗网络（GANs）是一种深度学习模型，由生成器和判别器组成，用于生成新的数据实例。模型鲁棒性指的是模型在面对输入数据的小变化时仍能保持其性能的能力。"},{"order":77,"title":"Dynamic-Structured Semantic Propagation Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Dynamic-Structured_Semantic_Propagation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Dynamic-Structured_Semantic_Propagation_CVPR_2018_paper.html","abstract":"Semantic concept hierarchy is yet under-explored for semantic segmentation due to the inefficiency and complicated optimization of incorporating structural inference into the dense prediction. This lack of modeling dependencies among concepts severely limits the generalization capability of segmentation models for open set large-scale vocabularies. Prior works thus must tune highly-specified models for each task due to the label discrepancy across datasets. In this paper, we propose a Dynamic-Structured Semantic Propagation Network (DSSPN) that builds a semantic neuron graph to explicitly incorporate the concept hierarchy into dynamic network construction, leading to an interpretable reasoning process. Each neuron for one super-class (eg food) represents the instantiated module for recognizing among fine-grained child concepts (eg editable fruit or pizza), and then its learned features flow into the child neurons (eg distinguishing between orange or apple) for hierarchical categorization in finer levels. A dense semantic-enhanced neural block propagates the learned knowledge of all ancestral neurons into each fine-grained child neuron for progressive feature evolving. During training, DSSPN performs the dynamic-structured neuron computational graph by only activating a sub-graph of neurons for each image. Another merit of such semantic explainable structure is the ability to learn a unified model concurrently on diverse datasets by selectively activating different neuron sub-graphs for each annotation at each step. Extensive experiments on four public semantic segmentation datasets (i.e. ADE20K, COCO-Stuff, Cityscape and Mapillary) demonstrate the superiority of DSSPN, and a universal segmentation model that is jointly trained on diverse datasets can surpass the common fine-tuning scheme for exploiting multi-domain knowledge.","中文标题":"动态结构化语义传播网络","摘要翻译":"由于将结构推理整合到密集预测中的低效性和复杂性，语义概念层次在语义分割中尚未得到充分探索。这种概念间依赖关系建模的缺乏严重限制了分割模型在开放集大规模词汇上的泛化能力。因此，由于跨数据集的标签差异，先前的工作必须为每个任务调整高度指定的模型。在本文中，我们提出了一种动态结构化语义传播网络（DSSPN），它构建了一个语义神经元图，以显式地将概念层次结构整合到动态网络构建中，从而产生一个可解释的推理过程。每个超类（如食物）的神经元代表识别细粒度子概念（如可食用的水果或披萨）的实例化模块，然后其学习到的特征流入子神经元（如区分橙子或苹果）以进行更细粒度的层次分类。一个密集的语义增强神经块将所有祖先神经元的学习知识传播到每个细粒度子神经元中，以实现渐进式特征进化。在训练期间，DSSPN通过仅为每个图像激活神经元的子图来执行动态结构化的神经元计算图。这种语义可解释结构的另一个优点是通过在每个步骤为每个注释选择性地激活不同的神经元子图，能够在不同数据集上同时学习统一模型。在四个公共语义分割数据集（即ADE20K、COCO-Stuff、Cityscape和Mapillary）上的大量实验证明了DSSPN的优越性，并且一个在多样化数据集上联合训练的通用分割模型可以超越利用多领域知识的常见微调方案。","领域":"语义分割/神经网络/特征学习","问题":"语义分割中概念层次结构的低效和复杂优化问题","动机":"提高分割模型在开放集大规模词汇上的泛化能力，解决跨数据集标签差异问题","方法":"提出动态结构化语义传播网络（DSSPN），构建语义神经元图，显式整合概念层次结构到动态网络构建中，实现可解释的推理过程","关键词":["语义分割","神经网络","特征学习","动态网络","语义神经元图"],"涉及的技术概念":"语义概念层次、密集预测、结构推理、动态网络构建、语义神经元图、渐进式特征进化、动态结构化的神经元计算图、语义可解释结构、统一模型、多领域知识"},{"order":78,"title":"Cross-Domain Self-Supervised Multi-Task Feature Learning Using Synthetic Imagery","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Cross-Domain_Self-Supervised_Multi-Task_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Cross-Domain_Self-Supervised_Multi-Task_CVPR_2018_paper.html","abstract":"In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multi-task learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classification and 2012 detection.","中文标题":"使用合成图像的跨域自监督多任务特征学习","摘要翻译":"在人类学习中，通常联合使用多种信息来源。然而，大多数现有的特征学习方法仅从单一任务中学习。在本文中，我们提出了一种新颖的多任务深度网络，以学习可推广的高级视觉表示。由于多任务学习需要对同一训练实例的多个属性进行注释，我们寻求使用合成图像来训练我们的网络。为了克服真实数据和合成数据之间的领域差异，我们采用了一种基于对抗学习的无监督特征空间领域适应方法。给定输入的合成RGB图像，我们的网络同时预测其表面法线、深度和实例轮廓，同时最小化真实数据和合成数据之间的特征空间领域差异。通过大量实验，我们证明了与单任务基线相比，我们的网络学习了更具可转移性的表示。我们学习的表示在PASCAL VOC 2007分类和2012检测上产生了最先进的迁移学习结果。","领域":"视觉表示学习/领域适应/多任务学习","问题":"如何从合成图像中学习可推广的高级视觉表示，并克服真实数据和合成数据之间的领域差异","动机":"现有的特征学习方法大多仅从单一任务中学习，而人类学习通常联合使用多种信息来源，因此需要一种能够从多任务中学习并克服领域差异的方法","方法":"提出了一种多任务深度网络，使用合成图像进行训练，并采用基于对抗学习的无监督特征空间领域适应方法来克服领域差异","关键词":["视觉表示学习","领域适应","多任务学习","对抗学习","合成图像"],"涉及的技术概念":"多任务深度网络、无监督特征空间领域适应、对抗学习、合成图像、表面法线预测、深度预测、实例轮廓预测"},{"order":79,"title":"A Two-Step Disentanglement Method","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.html","abstract":"We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/A-Two-Step-Disentanglement-Method.","中文标题":"两步解耦方法","摘要翻译":"我们解决了将生成给定数据的因素解耦为与标签相关和与标签无关的问题。我们的解决方案比之前的解决方案更简单，并采用了对抗训练。首先，通过与标签相关的数据部分通过训练分类器来提取。然后，提取另一部分，使其能够重建原始数据但不包含标签信息。新方法的实用性在视觉数据集以及金融数据上得到了展示。我们的代码可在https://github.com/naamahadad/A-Two-Step-Disentanglement-Method获取。","领域":"数据解耦/对抗训练/分类器训练","问题":"将生成数据的因素解耦为与标签相关和与标签无关的部分","动机":"为了简化数据解耦过程，并提高解耦的效率和效果","方法":"首先通过训练分类器提取与标签相关的数据部分，然后提取能够重建原始数据但不包含标签信息的另一部分","关键词":["数据解耦","对抗训练","分类器训练"],"涉及的技术概念":"对抗训练是一种通过让两个模型相互对抗来训练它们的方法，常用于生成对抗网络（GANs）中。分类器训练是指通过训练一个模型来识别或分类数据的过程。数据解耦是指将数据中的不同因素或特征分离出来，以便更好地理解和处理数据。"},{"order":80,"title":"Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Merget_Robust_Facial_Landmark_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Merget_Robust_Facial_Landmark_CVPR_2018_paper.html","abstract":"While fully-convolutional neural networks are very strong at modeling local features, they fail to aggregate global context due to their constrained receptive field. Modern methods typically address the lack of global context by introducing cascades, pooling, or by fitting a statistical model. In this work, we propose a new approach that introduces global context into a fully-convolutional neural network directly. The key concept is an implicit kernel convolution within the network. The kernel convolution blurs the output of a local-context subnet, which is then refined by a global-context subnet using dilated convolutions. The kernel convolution is crucial for the convergence of the network because it smoothens the gradients and reduces overfitting. In a postprocessing step, a simple PCA-based 2D shape model is fitted to the network output in order to filter outliers. Our experiments demonstrate the effectiveness of our approach, outperforming several state-of-the-art methods in facial landmark detection.","中文标题":"通过全卷积局部-全局上下文网络实现鲁棒的面部标志检测","摘要翻译":"虽然全卷积神经网络在建模局部特征方面非常强大，但由于其受限的感受野，它们无法聚合全局上下文。现代方法通常通过引入级联、池化或拟合统计模型来解决全局上下文的缺乏。在这项工作中，我们提出了一种新方法，直接将全局上下文引入全卷积神经网络。关键概念是网络内的隐式核卷积。核卷积模糊了局部上下文子网的输出，然后通过使用扩张卷积的全局上下文子网进行细化。核卷积对于网络的收敛至关重要，因为它平滑了梯度并减少了过拟合。在后处理步骤中，一个简单的基于PCA的2D形状模型被拟合到网络输出，以过滤异常值。我们的实验证明了我们方法的有效性，在面部标志检测方面优于几种最先进的方法。","领域":"面部标志检测/卷积神经网络/图像分析","问题":"全卷积神经网络在面部标志检测中缺乏全局上下文信息的问题","动机":"解决全卷积神经网络在面部标志检测中因受限的感受野而无法有效聚合全局上下文信息的问题","方法":"提出了一种新方法，通过在全卷积神经网络中引入隐式核卷积来直接引入全局上下文信息，并使用扩张卷积的全局上下文子网对局部上下文子网的输出进行细化","关键词":["面部标志检测","全卷积神经网络","全局上下文","核卷积","扩张卷积","PCA"],"涉及的技术概念":{"全卷积神经网络":"一种不包含全连接层的卷积神经网络，能够处理任意大小的输入图像","感受野":"在卷积神经网络中，某一层的一个输出单元所对应的输入图像的区域大小","核卷积":"一种在神经网络内部进行的卷积操作，用于模糊局部上下文子网的输出","扩张卷积":"一种卷积操作，通过在卷积核的元素之间插入零来增加感受野，而不增加参数数量","PCA":"主成分分析，一种统计方法，用于降低数据的维度，同时保留数据的主要特征"}},{"order":81,"title":"Decorrelated Batch Normalization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Decorrelated_Batch_Normalization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Decorrelated_Batch_Normalization_CVPR_2018_paper.html","abstract":"Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.","中文标题":"去相关批量归一化","摘要翻译":"批量归一化（BN）通过在小批量内对激活进行居中和缩放，能够加速深度模型的训练。在这项工作中，我们提出了去相关批量归一化（DBN），它不仅对激活进行居中和缩放，还对其进行白化处理。我们探索了多种白化技术，发现PCA白化会导致我们称之为随机轴交换的问题，这对学习是有害的。我们展示了ZCA白化不会遇到这个问题，允许成功的学习。DBN保留了BN的理想特性，并进一步提高了BN的优化效率和泛化能力。我们设计了全面的实验来展示DBN可以在多层感知器和卷积神经网络上提高BN的性能。此外，我们持续提高了残差网络在CIFAR-10、CIFAR-100和ImageNet上的准确性。","领域":"神经网络优化/深度学习/图像识别","问题":"批量归一化在处理激活时仅进行居中和缩放，未考虑去相关处理，可能影响模型的学习效率和泛化能力。","动机":"提高批量归一化的优化效率和模型的泛化能力，通过引入去相关处理来进一步加速深度模型的训练。","方法":"提出了去相关批量归一化（DBN），通过探索多种白化技术，特别是ZCA白化，来解决PCA白化导致的随机轴交换问题，从而提高模型的性能。","关键词":["批量归一化","去相关","白化技术","ZCA白化","PCA白化","随机轴交换"],"涉及的技术概念":"批量归一化（BN）是一种加速深度模型训练的技术，通过对小批量内的激活进行居中和缩放。去相关批量归一化（DBN）在此基础上增加了白化处理，特别是ZCA白化，以避免PCA白化导致的随机轴交换问题，从而提高模型的优化效率和泛化能力。"},{"order":82,"title":"Learning to Sketch With Shortcut Cycle Consistency","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Learning_to_Sketch_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Learning_to_Sketch_CVPR_2018_paper.html","abstract":"To see is to sketch -- free-hand sketching naturally builds ties between human and machine vision. In this paper, we present a novel approach for translating an object photo to a sketch, mimicking the human sketching process. This is an extremely challenging task because the photo and sketch domains differ significantly. Furthermore, human sketches exhibit various levels of sophistication and abstraction even when depicting the same object instance in a reference photo. This means that even if photo-sketch pairs are available, they only provide weak supervision signal to learn a translation model. Compared with existing supervised approaches that solve the problem of D(E(photo)) -> sketch, where E(·) and D(·) denote encoder and decoder respectively,  we take advantage of the inverse problem (e.g., D(E(sketch)) -> photo), and combine with the unsupervised learning tasks of within-domain reconstruction, all within a  multi-task learning framework. Compared with existing unsupervised approaches based on cycle consistency (i.e., D(E(D(E(photo)))) -> photo), we introduce a shortcut consistency enforced at the encoder bottleneck (e.g., D(E(photo)) -> photo) to exploit the additional self-supervision. Both qualitative and quantitative results show that the proposed model is superior to a number of state-of-the-art alternatives. We also show that  the synthetic sketches can be used to train a better fine-grained  sketch-based image retrieval (FG-SBIR) model, effectively alleviating the problem of sketch data scarcity.","中文标题":"学习通过快捷循环一致性进行素描","摘要翻译":"所见即所绘——手绘素描自然地在人类视觉与机器视觉之间建立了联系。在本文中，我们提出了一种新颖的方法，用于将物体照片转换为素描，模仿人类的素描过程。这是一项极具挑战性的任务，因为照片和素描领域差异显著。此外，即使描绘的是参考照片中的同一物体实例，人类素描也表现出不同程度的复杂性和抽象性。这意味着即使有照片-素描对可用，它们也只能为学习翻译模型提供弱监督信号。与现有的解决D(E(照片)) ->素描问题的监督方法相比，其中E(·)和D(·)分别表示编码器和解码器，我们利用逆问题（例如，D(E(素描)) ->照片），并结合域内重建的无监督学习任务，所有这些都在一个多任务学习框架内。与现有的基于循环一致性的无监督方法（即D(E(D(E(照片)))) ->照片）相比，我们在编码器瓶颈处引入了快捷一致性（例如，D(E(照片)) ->照片）以利用额外的自我监督。定性和定量结果均显示，所提出的模型优于许多最先进的替代方案。我们还展示了合成素描可用于训练更好的细粒度基于素描的图像检索（FG-SBIR）模型，有效缓解素描数据稀缺的问题。","领域":"图像生成/图像翻译/细粒度图像检索","问题":"将物体照片转换为素描，模仿人类的素描过程","动机":"照片和素描领域差异显著，且人类素描表现出不同程度的复杂性和抽象性，使得即使有照片-素描对可用，它们也只能为学习翻译模型提供弱监督信号","方法":"利用逆问题并结合域内重建的无监督学习任务，在编码器瓶颈处引入快捷一致性以利用额外的自我监督，所有操作都在一个多任务学习框架内","关键词":["图像生成","图像翻译","细粒度图像检索"],"涉及的技术概念":"D(E(照片)) ->素描，其中E(·)和D(·)分别表示编码器和解码器；逆问题D(E(素描)) ->照片；域内重建的无监督学习任务；多任务学习框架；基于循环一致性的无监督方法D(E(D(E(照片)))) ->照片；在编码器瓶颈处引入快捷一致性D(E(照片)) ->照片；细粒度基于素描的图像检索（FG-SBIR）模型"},{"order":83,"title":"Towards a Mathematical Understanding of the Difficulty in Learning With Feedforward Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Towards_a_Mathematical_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Towards_a_Mathematical_CVPR_2018_paper.html","abstract":"Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.","中文标题":"迈向对前馈神经网络学习困难性的数学理解","摘要翻译":"训练深度神经网络以解决机器学习问题是该领域的一大挑战，主要因为其相关的优化问题高度非凸。最近的发展表明，在某些情况下，许多训练算法不会受到不良局部最小值的影响，因此引发了大量努力以寻求对此类观察的数学解释。这项工作从平滑优化的角度提供了对这一挑战的另一种数学理解。通过假设对有限样本的精确学习，通过临界点分析确定了确保任何局部最小值也是全局最小值的充分条件。此外，作为近似牛顿算法的一种，广义高斯-牛顿（GGN）算法被严格重新审视，它在精确学习的条件下具有局部二次收敛到全局最小值的特性。","领域":"优化理论/神经网络/数学建模","问题":"深度神经网络训练中的优化问题，特别是非凸优化问题","动机":"寻求对深度神经网络训练算法在某些情况下不受不良局部最小值影响的数学解释","方法":"从平滑优化的角度提供数学理解，通过临界点分析确定确保局部最小值也是全局最小值的条件，并重新审视广义高斯-牛顿算法作为近似牛顿算法的性质","关键词":["优化理论","神经网络","数学建模","平滑优化","临界点分析","广义高斯-牛顿算法"],"涉及的技术概念":"深度神经网络、非凸优化、平滑优化、临界点分析、广义高斯-牛顿算法、局部二次收敛"},{"order":84,"title":"FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.html","abstract":"Face synthesis has achieved advanced development by using generative adversarial networks (GANs). Existing methods typically formulate GAN as a two-player game, where a discriminator distinguishes face images from the real and synthesized domains, while a generator reduces its discriminativeness by synthesizing a face of photo-realistic quality. Their competition converges when the discriminator is unable to differentiate these two domains.  Unlike two-player GANs, this work generates identity-preserving faces by proposing FaceID-GAN, which treats a classifier of face identity as the third player, competing with the generator by distinguishing the identities of the real and synthesized faces (see Fig.1). A stationary point is reached when the generator produces faces that have high quality as well as preserve identity. Instead of simply modeling the identity classifier as an additional discriminator, FaceID-GAN is formulated by satisfying information symmetry, which ensures that the real and synthesized images are projected into the same feature space. In other words, the identity classifier is used to extract identity features from both input (real) and output (synthesized) face images of the generator, substantially alleviating training difficulty of GAN. Extensive experiments show that FaceID-GAN is able to generate faces of arbitrary viewpoint while preserve identity, outperforming recent advanced approaches.","中文标题":"FaceID-GAN: 学习一个对称三玩家GAN用于身份保持的人脸合成","摘要翻译":"通过使用生成对抗网络（GANs），人脸合成已经取得了先进的发展。现有方法通常将GAN制定为一个双玩家游戏，其中一个判别器区分来自真实和合成领域的人脸图像，而一个生成器通过合成具有照片级真实感质量的人脸来减少其可区分性。当判别器无法区分这两个领域时，它们的竞争就会收敛。与双玩家GAN不同，这项工作通过提出FaceID-GAN生成保持身份的人脸，它将人脸身份分类器视为第三个玩家，通过区分真实和合成人脸的身份与生成器竞争（见图1）。当生成器产生既高质量又保持身份的人脸时，达到一个静止点。FaceID-GAN不是简单地将身份分类器建模为额外的判别器，而是通过满足信息对称性来制定，这确保了真实和合成图像被投影到相同的特征空间。换句话说，身份分类器用于从生成器的输入（真实）和输出（合成）人脸图像中提取身份特征，大大减轻了GAN的训练难度。大量实验表明，FaceID-GAN能够生成任意视角的人脸同时保持身份，优于最近的高级方法。","领域":"人脸合成/生成对抗网络/身份保持","问题":"如何在生成高质量人脸图像的同时保持身份信息","动机":"现有双玩家GAN在生成人脸图像时难以保持身份信息，需要一种新方法来同时保证图像质量和身份一致性","方法":"提出FaceID-GAN，引入身份分类器作为第三个玩家，通过信息对称性确保真实和合成图像在相同特征空间，从而生成既高质量又保持身份的人脸图像","关键词":["人脸合成","生成对抗网络","身份保持"],"涉及的技术概念":"生成对抗网络（GANs）是一种由生成器和判别器组成的框架，通过对抗过程学习生成数据。FaceID-GAN在此框架基础上引入了身份分类器作为第三个玩家，通过信息对称性确保生成的人脸图像在保持高质量的同时，也能保持身份信息。"},{"order":85,"title":"A Constrained Deep Neural Network for Ordinal Regression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_A_Constrained_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_A_Constrained_Deep_CVPR_2018_paper.html","abstract":"Ordinal regression is a supervised learning problem aiming to classify instances into ordinal categories. It is challenging to automatically extract high-level features for representing intraclass information and interclass ordinal relationship simultaneously. This paper proposes a constrained optimization formulation for the ordinal regression problem which minimizes the negative loglikelihood for multiple categories constrained by the order relationship between instances. Mathematically, it is equivalent to an unconstrained formulation with a pairwise regularizer. An implementation based on the CNN framework is proposed to solve the problem such that high-level features can be extracted automatically, and the optimal solution can be learned through the traditional back-propagation method. The proposed pairwise constraints make the algorithm work even on small datasets, and a proposed efficient implementation make it be scalable for large datasets. Experimental results on four real-world benchmarks demonstrate that the proposed algorithm outperforms the traditional deep learning approaches and other state-of-the-art approaches based on hand-crafted features.","中文标题":"用于序数回归的约束深度神经网络","摘要翻译":"序数回归是一种监督学习问题，旨在将实例分类为有序类别。同时自动提取高级特征以表示类内信息和类间序数关系是具有挑战性的。本文提出了一种序数回归问题的约束优化公式，该公式最小化了受实例间顺序关系约束的多个类别的负对数似然。在数学上，它等同于具有成对正则化器的无约束公式。提出了基于CNN框架的实现来解决该问题，以便可以自动提取高级特征，并且可以通过传统的反向传播方法学习最优解。所提出的成对约束使得算法即使在小型数据集上也能工作，并且提出的高效实现使其能够扩展到大型数据集。在四个真实世界基准上的实验结果表明，所提出的算法优于传统的深度学习方法和其他基于手工特征的最先进方法。","领域":"序数回归/特征提取/深度学习","问题":"如何同时自动提取高级特征以表示类内信息和类间序数关系","动机":"解决序数回归问题中自动提取高级特征的挑战","方法":"提出了一种约束优化公式，并基于CNN框架实现，通过成对约束和高效实现解决序数回归问题","关键词":["序数回归","特征提取","深度学习","约束优化","CNN框架"],"涉及的技术概念":{"序数回归":"一种监督学习问题，旨在将实例分类为有序类别","约束优化公式":"一种最小化受实例间顺序关系约束的多个类别的负对数似然的优化方法","CNN框架":"卷积神经网络框架，用于自动提取高级特征","成对正则化器":"一种用于约束优化公式的正则化方法，确保实例间的顺序关系","反向传播方法":"一种用于训练神经网络的传统方法，通过计算梯度来更新网络权重"}},{"order":86,"title":"Modulated Convolutional Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.html","abstract":"Despite great effectiveness of very deep and wide Convolutional Neural Networks (CNNs) in various computer vision tasks, the significant cost in terms of storage requirement of such networks impedes the deployment on computationally limited devices. In this paper, we propose new Modulated Convolutional Networks (MCNs) to improve the portability of  CNNs via binarized filters. In MCNs, we propose a new loss function which considers the filter loss, center loss and softmax loss in an end-to-end framework. We first introduce modulation filters (M-Filters) to recover the unbinarized filters, which leads to a  new architecture to calculate the network model.  The convolution operation is further approximated by considering intra-class compactness in the loss function. As a result, our MCNs can reduce the size of required storage space of convolutional filters  by a factor of 32, in contrast to the full-precision model, while achieving much better performances than state-of-the-art binarized models. Most importantly, MCNs achieve a comparable performance  to the  full-precision ResNets and Wide-ResNets. The code will be available publicly soon.","中文标题":"调制卷积网络","摘要翻译":"尽管非常深和宽的卷积神经网络（CNNs）在各种计算机视觉任务中表现出极大的有效性，但这类网络在存储需求方面的显著成本阻碍了其在计算能力有限的设备上的部署。在本文中，我们提出了新的调制卷积网络（MCNs），通过二值化滤波器来提高CNNs的可移植性。在MCNs中，我们提出了一个新的损失函数，该函数考虑了滤波器损失、中心损失和softmax损失，在一个端到端的框架内。我们首先引入了调制滤波器（M-Filters）来恢复未二值化的滤波器，这导致了一个新的架构来计算网络模型。通过考虑损失函数中的类内紧凑性，卷积操作得到了进一步的近似。因此，与全精度模型相比，我们的MCNs可以将卷积滤波器所需的存储空间大小减少32倍，同时实现了比最先进的二值化模型更好的性能。最重要的是，MCNs实现了与全精度ResNets和Wide-ResNets相当的性能。代码将很快公开。","领域":"卷积神经网络优化/模型压缩/二值化网络","问题":"减少卷积神经网络在存储需求方面的成本，以便在计算能力有限的设备上部署","动机":"提高卷积神经网络的可移植性，使其能够在存储和计算资源有限的设备上有效运行","方法":"提出调制卷积网络（MCNs），通过引入调制滤波器（M-Filters）和新的损失函数来恢复未二值化的滤波器，并近似卷积操作，以减少存储空间需求","关键词":["卷积神经网络","二值化滤波器","模型压缩"],"涉及的技术概念":"调制卷积网络（MCNs）是一种新型的卷积神经网络架构，通过使用二值化滤波器来减少模型的存储需求。MCNs引入了一种新的损失函数，该函数综合考虑了滤波器损失、中心损失和softmax损失，以实现端到端的优化。此外，MCNs通过调制滤波器（M-Filters）来恢复未二值化的滤波器，并采用了一种新的网络模型计算方法。通过考虑损失函数中的类内紧凑性，卷积操作得到了进一步的近似，从而在减少存储空间的同时，保持了与全精度模型相当的性能。"},{"order":87,"title":"Learning Steerable Filters for Rotation Equivariant CNNs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Weiler_Learning_Steerable_Filters_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Weiler_Learning_Steerable_Filters_CVPR_2018_paper.html","abstract":"In many machine learning tasks it is desirable that a model's prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He's weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.","中文标题":"学习可操纵滤波器以实现旋转等变卷积神经网络","摘要翻译":"在许多机器学习任务中，期望模型的预测在其输入变换下以等变的方式变换。卷积神经网络（CNNs）通过构造实现平移等变性；然而，对于其他变换，它们被迫学习适当的映射。在这项工作中，我们开发了可操纵滤波器卷积神经网络（SFCNNs），通过设计实现平移和旋转的联合等变性。所提出的架构采用可操纵滤波器，以高效计算多个方向的依赖方向的响应，而不会因滤波器旋转而遭受插值伪影。我们利用群卷积，保证等变映射。此外，我们将He的权重初始化方案推广到定义为原子滤波器系统线性组合的滤波器。数值实验显示，随着采样滤波器方向数量的增加，样本复杂性显著提高，并确认网络在方向上泛化学习到的模式。所提出的方法在旋转MNIST基准测试和ISBI 2012 2D EM分割挑战中达到了最先进的水平。","领域":"卷积神经网络/滤波器设计/图像分割","问题":"实现卷积神经网络在平移和旋转变换下的联合等变性","动机":"提高模型预测在输入变换下的等变性，特别是在旋转变换方面","方法":"开发可操纵滤波器卷积神经网络（SFCNNs），利用群卷积保证等变映射，并推广He的权重初始化方案到原子滤波器系统的线性组合","关键词":["可操纵滤波器","旋转等变性","群卷积","权重初始化","图像分割"],"涉及的技术概念":"卷积神经网络（CNNs）通过构造实现平移等变性，但对于其他变换如旋转，需要学习适当的映射。可操纵滤波器卷积神经网络（SFCNNs）通过设计实现平移和旋转的联合等变性，采用可操纵滤波器高效计算多个方向的依赖方向的响应，避免插值伪影。群卷积用于保证等变映射，He的权重初始化方案被推广到原子滤波器系统的线性组合。"},{"order":88,"title":"Efficient Interactive Annotation of Segmentation Datasets With Polygon-RNN++","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Acuna_Efficient_Interactive_Annotation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Acuna_Efficient_Interactive_Annotation_CVPR_2018_paper.html","abstract":"Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.","中文标题":"高效交互式分割数据集标注与Polygon-RNN++","摘要翻译":"手动标注带有对象掩码的数据集极其耗时。在这项工作中，我们遵循Polygon-RNN的理念，使用人机交互的方式交互式地生成对象的多边形标注。我们引入了几个重要的模型改进：1）设计了一个新的CNN编码器架构，2）展示了如何通过强化学习有效地训练模型，以及3）使用图神经网络显著提高了输出分辨率，使模型能够准确标注图像中的高分辨率对象。在Cityscapes数据集上的广泛评估显示，我们称之为Polygon-RNN++的模型在自动（平均IoU绝对提高10%，相对提高16%）和交互模式（标注者点击次数减少50%）下均显著优于原始模型。我们进一步分析了跨域场景，其中我们的模型在一个数据集上训练，并在不同领域的数据集上即插即用。结果表明，Polygon-RNN++展示了强大的泛化能力，相比现有的像素级方法取得了显著改进。通过简单的在线微调，我们进一步实现了新数据集标注时间的大幅减少，向实际使用的交互式标注工具迈进了一步。","领域":"图像分割/强化学习/图神经网络","问题":"手动标注带有对象掩码的数据集耗时且效率低下","动机":"提高数据集标注的效率和准确性，减少人工标注的时间和成本","方法":"设计新的CNN编码器架构，通过强化学习训练模型，使用图神经网络提高输出分辨率","关键词":["图像分割","强化学习","图神经网络","交互式标注","CNN编码器"],"涉及的技术概念":{"Polygon-RNN":"一种用于生成对象多边形标注的递归神经网络模型","CNN编码器":"卷积神经网络编码器，用于提取图像特征","强化学习":"一种机器学习方法，通过奖励机制来训练模型","图神经网络":"一种处理图结构数据的神经网络，用于提高模型输出分辨率","IoU":"交并比，用于评估图像分割准确性的指标"}},{"order":89,"title":"SplineCNN: Fast Geometric Deep Learning With Continuous B-Spline Kernels","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fey_SplineCNN_Fast_Geometric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fey_SplineCNN_Fast_Geometric_CVPR_2018_paper.html","abstract":"We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors.  For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub.","中文标题":"SplineCNN：使用连续B样条核的快速几何深度学习","摘要翻译":"我们提出了基于样条的卷积神经网络（SplineCNNs），这是一种针对不规则结构和几何输入（如图形或网格）的深度神经网络变体。我们的主要贡献是基于B样条的新型卷积算子，由于B样条基函数的局部支持特性，使得计算时间与核大小无关。因此，我们通过使用由固定数量的可训练权重参数化的连续核函数，获得了传统CNN卷积算子的泛化。与在谱域中过滤的相关方法相比，所提出的方法纯粹在空间域中聚合特征。此外，SplineCNN允许仅使用几何结构作为输入，而不是手工制作的特征描述符，进行深度架构的端到端训练。为了验证，我们将我们的方法应用于图像图分类、形状对应和图节点分类领域的任务，并显示它在显著更快的同时，优于或与最先进的方法相媲美，并具有领域独立性等有利特性。我们的源代码可在GitHub上获得。","领域":"几何深度学习/图神经网络/卷积神经网络","问题":"处理不规则结构和几何输入（如图形或网格）的深度学习问题","动机":"为了解决传统CNN在处理不规则结构和几何输入时的局限性，提出一种新的卷积算子，以提高计算效率和泛化能力","方法":"提出基于B样条的新型卷积算子，利用B样条基函数的局部支持特性，使得计算时间与核大小无关，并在空间域中聚合特征","关键词":["几何深度学习","图神经网络","卷积神经网络","B样条","空间域"],"涉及的技术概念":"B样条基函数的局部支持特性使得计算时间与核大小无关，通过使用连续核函数参数化，实现了传统CNN卷积算子的泛化。与在谱域中过滤的方法不同，该方法在空间域中聚合特征。SplineCNN支持仅使用几何结构作为输入的端到端训练，无需手工制作的特征描述符。"},{"order":90,"title":"GAGAN: Geometry-Aware Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kossaifi_GAGAN_Geometry-Aware_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kossaifi_GAGAN_Geometry-Aware_Generative_CVPR_2018_paper.html","abstract":"Deep generative models learned through adversarial training have become increasingly popular for their ability to generate naturalistic image textures. However, aside from their texture, the visual appearance of objects is significantly influenced by their shape geometry; information which is not taken into account by existing generative models. This paper introduces the Geometry-Aware Generative Adversarial Networks (GAGAN) for incorporating geometric information into the image generation process. Specifically, in GAGAN the generator samples latent variables from the probability space of a statistical shape model. By mapping the output of the generator to a canonical coordinate frame through a differentiable geometric transformation, we enforce the geometry of the objects and add an implicit connection from the prior to the generated object. Experimental results on face generation indicate that the GAGAN can generate realistic images of faces with arbitrary facial attributes such as facial expression, pose, and morphology, that are of better quality than current GAN-based methods. Our method can be used to augment any existing GAN architecture and improve the quality of the images generated.","中文标题":"GAGAN: 几何感知生成对抗网络","摘要翻译":"通过对抗训练学习的深度生成模型因其生成自然图像纹理的能力而变得越来越受欢迎。然而，除了纹理之外，物体的视觉外观还显著受到其形状几何的影响；现有生成模型并未考虑这一信息。本文介绍了几何感知生成对抗网络（GAGAN），用于将几何信息纳入图像生成过程。具体来说，在GAGAN中，生成器从统计形状模型的概率空间中采样潜在变量。通过可微分几何变换将生成器的输出映射到规范坐标系，我们强制物体的几何形状，并添加从先验到生成物体的隐式连接。面部生成的实验结果表明，GAGAN可以生成具有任意面部属性（如面部表情、姿态和形态）的逼真面部图像，其质量优于当前基于GAN的方法。我们的方法可用于增强任何现有的GAN架构，并提高生成图像的质量。","领域":"生成模型/几何处理/图像生成","问题":"现有生成模型未考虑物体形状几何对视觉外观的影响","动机":"提高生成图像的质量，特别是通过考虑物体的几何形状","方法":"引入几何感知生成对抗网络（GAGAN），通过从统计形状模型的概率空间中采样潜在变量，并使用可微分几何变换将生成器的输出映射到规范坐标系，强制物体的几何形状","关键词":["生成对抗网络","几何感知","图像生成"],"涉及的技术概念":"生成对抗网络（GAN）是一种通过对抗过程估计生成模型的框架，其中生成器尝试生成尽可能真实的图像，而判别器则尝试区分真实图像和生成图像。几何感知指的是在图像生成过程中考虑物体的几何形状。统计形状模型是一种用于描述物体形状变化的数学模型。可微分几何变换允许在图像生成过程中对几何形状进行精确控制。"},{"order":91,"title":"On the Robustness of Semantic Segmentation Models to Adversarial Attacks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Arnab_On_the_Robustness_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Arnab_On_the_Robustness_CVPR_2018_paper.html","abstract":"Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classification and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing. In this paper, we present what to our knowledge is the first rigorous evaluation of adversarial attacks on modern semantic segmentation models, using two large-scale datasets. We analyse the effect of different network architectures, model capacity and multiscale processing, and show that many observations made on the task of classification do not always transfer to this more complex task. Furthermore, we show how mean-field inference in deep structured models and multiscale processing naturally implement recently proposed adversarial defenses. Our observations will aid future efforts in understanding and defending against adversarial examples. Moreover, in the shorter term, we show which segmentation models should currently be preferred in safety-critical applications due to their inherent robustness.","中文标题":"关于语义分割模型对对抗攻击的鲁棒性","摘要翻译":"深度神经网络（DNNs）在大多数识别任务上表现出色，如图像分类和分割。然而，它们也被证明容易受到对抗样本的影响。这一现象最近引起了广泛关注，但尚未在多个大规模数据集和复杂任务（如语义分割）上进行广泛研究，这些任务通常需要更专业的网络，包括CRFs、扩张卷积、跳跃连接和多尺度处理等额外组件。在本文中，我们提出了据我们所知是第一个对现代语义分割模型进行对抗攻击的严格评估，使用了两个大规模数据集。我们分析了不同网络架构、模型容量和多尺度处理的影响，并展示了在分类任务上做出的许多观察并不总是适用于这一更复杂的任务。此外，我们展示了深度结构化模型中的均值场推断和多尺度处理如何自然地实现了最近提出的对抗防御。我们的观察将有助于未来在理解和防御对抗样本方面的努力。此外，在短期内，我们展示了由于它们固有的鲁棒性，在安全关键应用中目前应优先选择哪些分割模型。","领域":"语义分割/对抗攻击/模型鲁棒性","问题":"语义分割模型对对抗攻击的鲁棒性问题","动机":"研究深度神经网络在语义分割任务中对对抗样本的脆弱性，以及如何提高模型的鲁棒性","方法":"使用两个大规模数据集对现代语义分割模型进行对抗攻击的严格评估，分析不同网络架构、模型容量和多尺度处理的影响，并展示深度结构化模型中的均值场推断和多尺度处理如何实现对抗防御","关键词":["语义分割","对抗攻击","模型鲁棒性","深度神经网络","均值场推断","多尺度处理"],"涉及的技术概念":{"深度神经网络（DNNs）":"一种模拟人脑神经网络结构和功能的计算模型，用于处理复杂的模式识别任务。","对抗样本":"经过特殊设计的输入样本，能够导致机器学习模型产生错误的输出。","语义分割":"一种计算机视觉任务，旨在将图像分割成多个区域或对象，并为每个像素分配一个类别标签。","CRFs（条件随机场）":"一种用于结构化预测的统计建模方法，常用于图像分割等任务。","扩张卷积":"一种卷积操作，通过在卷积核中插入零来增加感受野，而不增加参数数量。","跳跃连接":"一种网络架构设计，允许信息从网络的一层直接传递到更深层，有助于解决梯度消失问题。","多尺度处理":"一种处理技术，通过在不同尺度上分析数据来提高模型的性能。","均值场推断":"一种近似推断方法，用于估计复杂概率分布，常用于深度结构化模型中。"}},{"order":92,"title":"Feedback-Prop: Convolutional Neural Network Inference Under Partial Evidence","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Feedback-Prop_Convolutional_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Feedback-Prop_Convolutional_Neural_CVPR_2018_paper.html","abstract":"We propose an inference procedure for deep convolutional neural networks (CNNs) when partial evidence is available. Our method consists of a general feedback-based propagation approach (feedback-prop) that boosts the prediction accuracy for an arbitrary set of unknown target labels when the values for a non-overlapping arbitrary set of target labels are known. We show that existing models trained in a multi-label or multi-task setting can readily take advantage of feedback-prop without any retraining or fine-tuning. Our feedback-prop inference procedure is general, simple, reliable, and works on different challenging visual recognition tasks. We present two variants of feedback-prop based on layer-wise and residual iterative updates. We experiment using several multi-task models and show that feedback-prop is effective in all of them. Our results unveil a previously unreported but interesting dynamic property of deep CNNs. We also present an associated technical approach that takes advantage of this property for inference under partial evidence in general visual recognition tasks.","中文标题":"反馈传播：部分证据下的卷积神经网络推理","摘要翻译":"我们提出了一种在部分证据可用时用于深度卷积神经网络（CNNs）的推理程序。我们的方法包括一种基于反馈的传播方法（feedback-prop），当一组不重叠的任意目标标签的值已知时，该方法可以提高对任意未知目标标签集的预测准确性。我们展示了在多标签或多任务设置下训练的现有模型可以无需任何重新训练或微调即可利用feedback-prop。我们的feedback-prop推理程序是通用的、简单的、可靠的，并且适用于不同的具有挑战性的视觉识别任务。我们提出了基于逐层和残差迭代更新的两种feedback-prop变体。我们使用几种多任务模型进行实验，并展示了feedback-prop在所有模型中的有效性。我们的结果揭示了深度CNNs之前未报告但有趣的动态特性。我们还提出了一种相关的技术方法，利用这一特性在一般视觉识别任务中进行部分证据下的推理。","领域":"视觉识别/多任务学习/卷积神经网络","问题":"在部分证据可用时提高卷积神经网络对未知目标标签的预测准确性","动机":"探索和利用深度卷积神经网络在部分证据下的推理能力，以提高预测准确性","方法":"提出了一种基于反馈的传播方法（feedback-prop），包括逐层和残差迭代更新两种变体","关键词":["反馈传播","卷积神经网络","多任务学习","视觉识别"],"涉及的技术概念":"深度卷积神经网络（CNNs）、反馈传播（feedback-prop）、多标签学习、多任务学习、视觉识别任务、逐层更新、残差迭代更新"},{"order":93,"title":"Super-Resolving Very Low-Resolution Face Images With Supplementary Attributes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Super-Resolving_Very_Low-Resolution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Super-Resolving_Very_Low-Resolution_CVPR_2018_paper.html","abstract":"Given a tiny face image, conventional face hallucination methods aim to super-resolve its high-resolution (HR) counterpart by learning a mapping from an exemplar dataset. Since a low-resolution (LR) input patch may correspond to many HR candidate patches, this ambiguity may lead to erroneous HR facial details and thus distorts final results, such as gender reversal.   An LR input contains low-frequency facial components of its HR version while its residual face image defined as the difference between the HR ground-truth and interpolated LR images contains the missing high-frequency facial details. We demonstrate that supplementing residual images or feature maps with facial attribute information can significantly reduce the ambiguity in face super-resolution.  To explore this idea, we develop an attribute-embedded upsampling network, which consists of an upsampling network and a discriminative network. The upsampling network is composed of an autoencoder with skip-connections, which incorporates facial attribute vectors into the residual features of LR inputs at the bottleneck of the autoencoder and deconvolutional layers used for upsampling. The discriminative network is designed to examine whether super-resolved faces contain the desired attributes or not and then its loss is used for updating the upsampling network. In this manner, we can super-resolve tiny unaligned (16$\\times$16 pixels) face images with a large upscaling factor of 8$\\times$ while reducing the uncertainty of one-to-many mappings significantly. By conducting extensive evaluations on a large-scale dataset, we demonstrate that our method achieves superior face hallucination results and outperforms the state-of-the-art.","中文标题":"利用补充属性超分辨率极低分辨率人脸图像","摘要翻译":"给定一个极小的人脸图像，传统的人脸幻觉方法旨在通过学习从示例数据集的映射来超分辨率其高分辨率（HR）对应图像。由于低分辨率（LR）输入块可能对应许多HR候选块，这种模糊性可能导致错误的HR面部细节，从而扭曲最终结果，如性别反转。LR输入包含其HR版本的低频面部组件，而其残差面部图像（定义为HR真实图像与插值LR图像之间的差异）包含缺失的高频面部细节。我们证明，通过补充残差图像或特征图与面部属性信息，可以显著减少面部超分辨率中的模糊性。为了探索这一想法，我们开发了一个属性嵌入的上采样网络，该网络由上采样网络和判别网络组成。上采样网络由带有跳跃连接的自动编码器组成，它将面部属性向量集成到自动编码器瓶颈处的LR输入的残差特征中，并用于上采样的反卷积层。判别网络旨在检查超分辨率面部是否包含所需属性，然后其损失用于更新上采样网络。通过这种方式，我们可以在显著减少一对多映射的不确定性的同时，以8倍的大比例因子超分辨率极小的未对齐（16×16像素）人脸图像。通过对大规模数据集进行广泛评估，我们证明了我们的方法实现了卓越的人脸幻觉结果，并优于现有技术。","领域":"面部超分辨率/图像上采样/面部属性分析","问题":"解决极低分辨率人脸图像在超分辨率过程中由于一对多映射导致的模糊性问题","动机":"提高极低分辨率人脸图像超分辨率的准确性，减少性别反转等错误","方法":"开发了一个属性嵌入的上采样网络，包括一个带有跳跃连接的自动编码器和一个判别网络，通过补充面部属性信息减少模糊性","关键词":["面部超分辨率","图像上采样","面部属性分析"],"涉及的技术概念":"自动编码器、跳跃连接、反卷积层、判别网络、面部属性向量、残差特征"},{"order":94,"title":"Frustum PointNets for 3D Object Detection From RGB-D Data","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html","abstract":"In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.","中文标题":"用于RGB-D数据3D目标检测的Frustum PointNets","摘要翻译":"在本研究中，我们研究了室内外场景中基于RGB-D数据的3D目标检测。虽然之前的方法主要关注图像或3D体素，往往掩盖了3D数据的自然模式和不变性，但我们通过弹出RGB-D扫描直接操作原始点云。然而，这种方法的一个关键挑战是如何在大规模场景的点云中高效定位物体（区域提议）。我们的方法不仅依赖于3D提议，还利用了成熟的2D目标检测器和先进的3D深度学习进行目标定位，实现了高效率以及对小物体的高召回率。得益于直接在原始点云中学习，我们的方法即使在强遮挡或点非常稀疏的情况下也能精确估计3D边界框。在KITTI和SUN RGB-D 3D检测基准上的评估显示，我们的方法以显著的优势超越了现有技术，同时具备实时能力。","领域":"3D目标检测/点云处理/深度学习","问题":"如何在大规模场景的点云中高效定位物体","动机":"之前的方法主要关注图像或3D体素，往往掩盖了3D数据的自然模式和不变性，直接操作原始点云可以更有效地进行3D目标检测","方法":"利用成熟的2D目标检测器和先进的3D深度学习进行目标定位，直接在原始点云中学习","关键词":["3D目标检测","点云处理","深度学习"],"涉及的技术概念":{"RGB-D数据":"包含颜色（RGB）和深度（D）信息的数据，常用于3D场景理解","点云":"一组在三维空间中定义的点，用于表示物体的表面","3D体素":"三维空间中的体积像素，用于表示3D物体的体积","2D目标检测器":"用于在二维图像中检测和定位目标的算法","3D深度学习":"应用于3D数据的深度学习技术，用于特征提取和目标检测","KITTI":"一个广泛使用的自动驾驶数据集，包含丰富的3D目标检测任务","SUN RGB-D":"一个包含室内场景RGB-D图像的数据集，用于3D场景理解和目标检测"}},{"order":95,"title":"W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_W2F_A_Weakly-Supervised_CVPR_2018_paper.html","abstract":"Weakly-supervised object detection has attracted much attention lately, since it does not require bounding box annotations for training. Although significant progress has also been made, there is still a large gap in performance between weakly-supervised and fully-supervised object detection. Recently, some works use pseudo ground-truths which are generated by a weakly-supervised detector to train a supervised detector. Such approaches incline to find the most representative parts of objects, and only seek one ground-truth box per class even though many same-class instances exist. To overcome these issues, we propose a weakly-supervised to fully-supervised framework, where a weakly-supervised detector is implemented using multiple instance learning. Then, we propose a pseudo ground-truth excavation (PGE) algorithm to find the pseudo ground-truth of each instance in the image. Moreover, the pseudo ground-truth adaptation (PGA) algorithm is designed to further refine the pseudo ground-truths from PGE. Finally, we use these pseudo ground-truths to train a fully-supervised detector. Extensive experiments on the challenging PASCAL VOC 2007 and 2012 benchmarks strongly demonstrate the effectiveness of our framework. We obtain 52.4% and 47.8% mAP on VOC2007 and VOC2012 respectively, a significant improvement over previous state-of-the-art methods.","中文标题":"W2F: 从弱监督到全监督的目标检测框架","摘要翻译":"弱监督目标检测最近引起了广泛关注，因为它不需要边界框注释进行训练。尽管已经取得了显著进展，但弱监督和全监督目标检测之间的性能仍有很大差距。最近，一些工作使用由弱监督检测器生成的伪真实值来训练监督检测器。这种方法倾向于找到对象的最具代表性部分，并且即使存在许多同类实例，也只寻找每个类别的一个真实值框。为了克服这些问题，我们提出了一个从弱监督到全监督的框架，其中使用多实例学习实现弱监督检测器。然后，我们提出了一种伪真实值挖掘（PGE）算法来找到图像中每个实例的伪真实值。此外，设计了伪真实值适应（PGA）算法以进一步从PGE中精炼伪真实值。最后，我们使用这些伪真实值来训练一个全监督检测器。在具有挑战性的PASCAL VOC 2007和2012基准上进行的大量实验强烈证明了我们框架的有效性。我们在VOC2007和VOC2012上分别获得了52.4%和47.8%的mAP，相比之前的最先进方法有显著改进。","领域":"目标检测/弱监督学习/全监督学习","问题":"弱监督目标检测与全监督目标检测之间的性能差距","动机":"减少对边界框注释的依赖，同时提高目标检测的性能","方法":"提出一个从弱监督到全监督的框架，包括使用多实例学习的弱监督检测器、伪真实值挖掘（PGE）算法和伪真实值适应（PGA）算法，最后使用伪真实值训练全监督检测器","关键词":["弱监督学习","全监督学习","目标检测","伪真实值挖掘","伪真实值适应"],"涉及的技术概念":"多实例学习是一种弱监督学习方法，它允许模型从一组实例中学习，而不需要每个实例的标签。伪真实值挖掘（PGE）算法用于从弱监督检测器中生成每个实例的伪真实值。伪真实值适应（PGA）算法用于进一步精炼这些伪真实值，以提高全监督检测器的训练效果。"},{"order":96,"title":"3D Object Detection With Latent Support Surfaces","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_3D_Object_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_3D_Object_Detection_CVPR_2018_paper.html","abstract":"We develop a 3D object detection algorithm that uses latent support surfaces to capture contextual relationships in indoor scenes. Existing 3D representations for RGB-D images capture the local shape and appearance of object categories, but have limited power to represent objects with different visual styles. The detection of small objects is also challenging because the search space is very large in 3D scenes. However, we observe that much of the shape variation within 3D object categories can be explained by the location of a latent support surface, and smaller objects are often supported by larger objects. Therefore, we explicitly use latent support surfaces to better represent the 3D appearance of large objects, and provide contextual cues to improve the detection of small objects. We evaluate our model with 19 object categories from the SUN RGB-D database, and demonstrate state-of-the-art performance.","中文标题":"使用潜在支撑表面的3D物体检测","摘要翻译":"我们开发了一种3D物体检测算法，该算法使用潜在支撑表面来捕捉室内场景中的上下文关系。现有的RGB-D图像的3D表示方法能够捕捉物体类别的局部形状和外观，但在表示具有不同视觉风格的物体时能力有限。小物体的检测也很有挑战性，因为在3D场景中搜索空间非常大。然而，我们观察到，3D物体类别内的许多形状变化可以通过潜在支撑表面的位置来解释，较小的物体通常由较大的物体支撑。因此，我们明确使用潜在支撑表面来更好地表示大物体的3D外观，并提供上下文线索以改善小物体的检测。我们使用SUN RGB-D数据库中的19个物体类别评估我们的模型，并展示了最先进的性能。","领域":"3D视觉/室内场景理解/物体检测","问题":"在3D场景中检测小物体和表示具有不同视觉风格的物体的能力有限","动机":"观察到3D物体类别内的形状变化可以通过潜在支撑表面的位置来解释，以及小物体通常由大物体支撑的现象","方法":"开发了一种使用潜在支撑表面的3D物体检测算法，以捕捉室内场景中的上下文关系，并改善小物体的检测","关键词":["3D物体检测","潜在支撑表面","室内场景理解"],"涉及的技术概念":"潜在支撑表面：在3D物体检测中，用于解释物体形状变化和提供上下文线索的概念，特别是在室内场景中，小物体通常由大物体支撑。SUN RGB-D数据库：一个包含RGB-D图像的数据库，用于3D场景理解和物体检测的研究。"},{"order":97,"title":"Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Towards_Faster_Training_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Towards_Faster_Training_CVPR_2018_paper.html","abstract":"Global covariance pooling in convolutional neural networks has achieved impressive improvement over the classical first-order pooling. Recent works have shown matrix square root normalization plays a central role in achieving state-of-the-art performance. However, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD), suffering from inefficient training due to limited support of EIG and SVD on GPU. Towards addressing this problem, we propose an iterative matrix square root normalization method for fast end-to-end training of global covariance pooling networks. At the core of our method is a meta-layer designed with loop-embedded directed graph structure. The meta-layer consists of three consecutive nonlinear structured layers, which perform pre-normalization, coupled matrix iteration and post-compensation, respectively. Our method is much faster than EIG or SVD based ones, since it involves only matrix multiplications, suitable for parallel implementation on GPU.  Moreover, the proposed network with ResNet architecture can converge in much less epochs, further accelerating network training. On large-scale ImageNet, we achieve competitive performance superior to existing counterparts. By finetuning our models pre-trained on ImageNet, we establish state-of-the-art results on three challenging fine-grained benchmarks. The source code and network models will be available at http://www.peihuali.org/iSQRT-COV.","中文标题":"通过迭代矩阵平方根归一化实现全局协方差池化网络的更快训练","摘要翻译":"卷积神经网络中的全局协方差池化相比经典的一阶池化取得了显著的改进。最近的研究表明，矩阵平方根归一化在实现最先进性能中起着核心作用。然而，现有方法严重依赖于特征分解（EIG）或奇异值分解（SVD），由于GPU对EIG和SVD的支持有限，导致训练效率低下。为了解决这个问题，我们提出了一种迭代矩阵平方根归一化方法，用于全局协方差池化网络的快速端到端训练。我们方法的核心是一个设计有循环嵌入有向图结构的元层。该元层由三个连续的非线性结构化层组成，分别执行预归一化、耦合矩阵迭代和后补偿。我们的方法比基于EIG或SVD的方法快得多，因为它只涉及矩阵乘法，适合在GPU上并行实现。此外，采用ResNet架构的所提出网络可以在更少的周期内收敛，进一步加速网络训练。在大规模ImageNet上，我们实现了优于现有方法的竞争性能。通过在ImageNet上预训练的模型进行微调，我们在三个具有挑战性的细粒度基准测试中建立了最先进的结果。源代码和网络模型将在http://www.peihuali.org/iSQRT-COV上提供。","领域":"卷积神经网络/全局协方差池化/矩阵平方根归一化","问题":"现有全局协方差池化网络训练效率低下，主要由于依赖于特征分解（EIG）或奇异值分解（SVD）的方法在GPU上的支持有限。","动机":"提高全局协方差池化网络的训练效率，减少训练时间，同时保持或提高模型性能。","方法":"提出了一种迭代矩阵平方根归一化方法，通过设计一个包含预归一化、耦合矩阵迭代和后补偿的元层，实现快速端到端训练。","关键词":["全局协方差池化","矩阵平方根归一化","卷积神经网络","GPU并行计算","ResNet架构"],"涉及的技术概念":"全局协方差池化是一种在卷积神经网络中用于捕捉特征间二阶统计信息的技术。矩阵平方根归一化是一种用于改善特征表示的方法，通过计算矩阵的平方根来实现。特征分解（EIG）和奇异值分解（SVD）是两种常用的矩阵分解方法，用于矩阵平方根的计算。GPU并行计算指的是利用图形处理单元进行并行数据处理，以加速计算过程。ResNet架构是一种深度卷积神经网络架构，通过引入残差连接来解决深层网络训练中的梯度消失问题。"},{"order":98,"title":"Recurrent Scene Parsing With Perspective Understanding in the Loop","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kong_Recurrent_Scene_Parsing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kong_Recurrent_Scene_Parsing_CVPR_2018_paper.html","abstract":"Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution. We propose a depth-aware gating module that adaptively selects the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby. The depth gating signal is provided by stereo disparity or estimated directly from monocular input. We integrate this depth-aware gating into a recurrent convolutional neural network to perform semantic segmentation. Our recurrent module iteratively efines the segmentation results, leveraging the depth and semantic predictions from the previous iterations.  Through extensive experiments on four popular large-scale datasets, we demonstrate this approach achieves competitive semantic segmentation performance with a model which is substantially more compact. We carry out extensive analysis of this architecture including variants that operate on monocular RGB but use depth as side-information during training, unsupervised gating as a generic attentional mechanism, and multi-resolution gating. We find that gated pooling for joint semantic segmentation and depth yields state-of-the-art results for quantitative monocular depth estimation.","中文标题":"循环场景解析与透视理解","摘要翻译":"在场景的透视图像中，物体可能以任意尺度出现，这对以固定分辨率处理图像的识别系统构成了挑战。我们提出了一个深度感知的门控模块，该模块根据物体尺度（与深度成反比）自适应地选择卷积网络架构中的池化场大小，以便为远处的物体保留小细节，同时为近处的物体使用更大的感受野。深度门控信号由立体视差提供或直接从单目输入估计。我们将这种深度感知门控集成到循环卷积神经网络中，以执行语义分割。我们的循环模块利用前几次迭代的深度和语义预测，迭代地定义分割结果。通过在四个流行的大规模数据集上的广泛实验，我们证明了这种方法在模型显著更紧凑的情况下实现了有竞争力的语义分割性能。我们对这种架构进行了广泛的分析，包括在训练期间使用深度作为辅助信息但在单目RGB上操作的变体、作为通用注意力机制的无监督门控以及多分辨率门控。我们发现，用于联合语义分割和深度的门控池化在定量单目深度估计方面取得了最先进的结果。","领域":"语义分割/深度估计/卷积神经网络","问题":"在透视图像中，物体以任意尺度出现，对固定分辨率处理图像的识别系统构成挑战。","动机":"为了解决透视图像中物体尺度变化对识别系统的影响，提出了一种深度感知的门控模块。","方法":"提出深度感知的门控模块，根据物体尺度自适应选择卷积网络中的池化场大小，并将其集成到循环卷积神经网络中进行语义分割。","关键词":["深度感知","门控模块","语义分割","深度估计","卷积神经网络"],"涉及的技术概念":"深度感知门控模块、卷积神经网络、语义分割、深度估计、循环卷积神经网络、立体视差、单目输入、池化场大小、感受野、无监督门控、多分辨率门控。"},{"order":99,"title":"Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Noh_Improving_Occlusion_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Noh_Improving_Occlusion_and_CVPR_2018_paper.html","abstract":"We propose methods of addressing two critical issues of pedestrian detection: (i) occlusion of target objects as false negative failure, and (ii) confusion with hard negative examples like vertical structures as false positive failure. Our solutions to these two problems are general and flexible enough to be applicable to any single-stage detection models. We implement our methods into four state-of-the-art single-stage models, including SqueezeDet+, YOLOv2, SSD, and DSSD. We empirically validate that our approach indeed improves the performance of those four models on Caltech pedestrian and CityPersons dataset. Moreover, in some heavy occlusion settings, our approach achieves the best reported performance. Specifically, our two solutions are as follows. For better occlusion handling, we update the output tensors of single-stage models so that they include the prediction of part confidence scores, from which we compute a final occlusion-aware detection score. For reducing confusion with hard negative examples, we introduce average grid classifiers as post-refinement classifiers, trainable in an end-to-end fashion with little memory and time overhead (e.g. increase of 1--5 MB in memory and 1--2 ms in inference time).","中文标题":"改进单阶段行人检测器的遮挡和困难负样本处理","摘要翻译":"我们提出了解决行人检测中两个关键问题的方法：(i) 目标对象的遮挡导致的假阴性失败，和(ii) 与如垂直结构等困难负样本的混淆导致的假阳性失败。我们对这两个问题的解决方案既通用又灵活，足以适用于任何单阶段检测模型。我们将我们的方法实现到四个最先进的单阶段模型中，包括SqueezeDet+、YOLOv2、SSD和DSSD。我们通过实验验证了我们的方法确实提高了这四个模型在Caltech行人和CityPersons数据集上的性能。此外，在一些严重遮挡的设置中，我们的方法达到了报告中的最佳性能。具体来说，我们的两个解决方案如下。为了更好地处理遮挡，我们更新了单阶段模型的输出张量，使其包括部分置信度分数的预测，从中我们计算出一个最终的遮挡感知检测分数。为了减少与困难负样本的混淆，我们引入了平均网格分类器作为后细化分类器，可以以端到端的方式进行训练，且内存和时间开销很小（例如，内存增加1--5 MB，推理时间增加1--2 ms）。","领域":"行人检测/目标检测/深度学习应用","问题":"解决行人检测中的遮挡问题和困难负样本的混淆问题","动机":"提高单阶段行人检测模型在遮挡和困难负样本情况下的检测性能","方法":"更新单阶段模型的输出张量以包括部分置信度分数的预测，并引入平均网格分类器作为后细化分类器","关键词":["行人检测","遮挡处理","困难负样本"],"涉及的技术概念":"单阶段检测模型、部分置信度分数、遮挡感知检测分数、平均网格分类器、端到端训练"},{"order":100,"title":"Learning to Act Properly: Predicting and Explaining Affordances From Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chuang_Learning_to_Act_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html","abstract":"We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent’s actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k, referred to as ADE-Affordance, which containing annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.","中文标题":"学会正确行动：从图像预测和解释可供性","摘要翻译":"我们解决了现实世界中多样化场景中的可供性推理问题。可供性将代理的行为与其在周围物体上采取行动时的效果联系起来。在我们的工作中，我们采用场景的自我中心视角，旨在推理既尊重物理世界又尊重社会施加的社会规范的动作-物体可供性。我们还旨在教导人工代理为什么在某些情况下不应采取某些行动，以及如果采取这些行动可能会发生什么。我们收集了一个基于ADE20k的新数据集，称为ADE-Affordance，其中包含支持这种丰富视觉推理的注释。我们提出了一个模型，该模型利用图神经网络传播场景中的上下文信息，以对每个对象进行详细的可供性推理。通过各种消融研究展示了我们的模型，指出了这一复杂任务中的成功和挑战。","领域":"可供性推理/场景理解/社会规范","问题":"解决现实世界中多样化场景中的可供性推理问题","动机":"教导人工代理理解动作-物体可供性，包括物理世界和社会规范，以及为什么在某些情况下不应采取某些行动","方法":"采用图神经网络传播场景中的上下文信息，对每个对象进行详细的可供性推理","关键词":["可供性推理","图神经网络","社会规范"],"涉及的技术概念":"可供性推理指的是理解代理（如机器人或人工代理）在特定环境下可以采取的行动及其效果。图神经网络是一种处理图结构数据的神经网络，能够有效地传播和聚合图中节点的信息。社会规范是指社会成员普遍接受的行为准则和期望。"},{"order":101,"title":"Pointwise Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hua_Pointwise_Convolutional_Neural_CVPR_2018_paper.html","abstract":"Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently. However, the capability of using point clouds with convolutional neural network has been so far not fully explored. In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud. Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task.","中文标题":"逐点卷积神经网络","摘要翻译":"近年来，使用重建点云和CAD模型等3D数据进行深度学习引起了广泛的研究兴趣。然而，迄今为止，使用点云与卷积神经网络的能力尚未得到充分探索。在本文中，我们提出了一种用于3D点云语义分割和物体识别的卷积神经网络。我们网络的核心是逐点卷积，这是一种可以在点云的每个点上应用的新卷积算子。我们的全卷积网络设计，尽管实现起来出奇地简单，但在语义分割和物体识别任务中都能产生具有竞争力的准确性。","领域":"3D点云处理/语义分割/物体识别","问题":"如何有效地使用3D点云数据进行语义分割和物体识别","动机":"探索和扩展卷积神经网络在处理3D点云数据方面的能力，以解决现有方法在语义分割和物体识别任务中的局限性","方法":"提出了一种新的卷积算子——逐点卷积，并设计了一个全卷积网络，该网络简单易实现，同时在语义分割和物体识别任务中表现出色","关键词":["3D点云","语义分割","物体识别","逐点卷积","全卷积网络"],"涉及的技术概念":{"3D点云":"由大量点组成的三维数据表示，每个点包含空间坐标信息，常用于表示物体的表面形状","语义分割":"一种图像分析技术，旨在将图像中的每个像素或点分配到特定的类别，以理解图像内容","物体识别":"识别图像或场景中特定物体的技术，通常涉及分类和定位","逐点卷积":"一种新的卷积操作，可以直接在点云的每个点上应用，用于提取局部特征","全卷积网络":"一种特殊的卷积神经网络，其所有层都是卷积层，适用于像素级或点级的预测任务"}},{"order":102,"title":"Image-Image Domain Adaptation With Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Image-Image_Domain_Adaptation_CVPR_2018_paper.html","abstract":"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a \`\`learning via translation'' framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.  Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.","中文标题":"保持自相似性和域间差异性的图像-图像域适应用于行人重识别","摘要翻译":"在一个领域训练的行人重识别（re-ID）模型往往难以很好地泛化到另一个领域。在我们的尝试中，我们提出了一个“通过翻译学习”的框架。在基线中，我们以无监督的方式将标记的图像从源域翻译到目标域。然后，我们通过监督方法使用翻译后的图像训练re-ID模型。然而，作为该框架的一个重要部分，无监督的图像-图像翻译在翻译过程中会遭受源域标签信息丢失的问题。我们的动机有两个方面。首先，对于每张图像，其ID标签中包含的判别性线索在翻译后应保持不变。其次，鉴于两个领域有完全不同的人，翻译后的图像应与任何目标ID都不相似。为此，我们提出保持两种类型的无监督相似性，1）翻译前后图像的自相似性，和2）翻译后的源图像与目标图像的域间差异性。这两种约束都在相似性保持生成对抗网络（SPGAN）中实现，该网络由Siamese网络和CycleGAN组成。通过域适应实验，我们展示了由SPGAN生成的图像更适合域适应，并在两个大规模数据集上产生了一致且有竞争力的re-ID准确率。","领域":"行人重识别/图像翻译/域适应","问题":"行人重识别模型在不同领域间的泛化能力差","动机":"保持翻译前后图像的判别性线索和确保翻译后的图像与目标域图像不相似","方法":"提出保持自相似性和域间差异性的无监督图像-图像翻译方法，通过相似性保持生成对抗网络（SPGAN）实现","关键词":["行人重识别","图像翻译","域适应"],"涉及的技术概念":{"行人重识别（re-ID）":"指在不同摄像头视角下识别同一行人的技术。","图像-图像翻译":"将图像从一个域转换到另一个域的过程，同时保持图像内容的一致性。","无监督学习":"一种机器学习方法，模型在没有标签的数据上学习。","生成对抗网络（GAN）":"由生成器和判别器组成的网络，通过对抗过程生成数据。","Siamese网络":"一种网络结构，用于比较两个输入的相似性。","CycleGAN":"一种用于图像到图像翻译的生成对抗网络，能够实现无配对数据的域转换。"}},{"order":103,"title":"A Generative Adversarial Approach for Zero-Shot Learning From Noisy Texts","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_A_Generative_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_A_Generative_Adversarial_CVPR_2018_paper.html","abstract":"Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g.Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.","中文标题":"基于生成对抗网络的零样本学习从噪声文本中的方法","摘要翻译":"大多数现有的零样本学习方法将问题视为视觉语义嵌入问题。鉴于生成对抗网络（GANs）在生成图像方面展示的能力，我们转而利用GANs从文本描述中想象未见过的类别，从而在没有看到任何示例的情况下识别新类别。具体来说，我们提出了一种简单但有效的生成模型，该模型以关于未见类别的噪声文本描述（例如维基百科文章）作为输入，并生成该类别的合成视觉特征。通过添加伪数据，零样本学习自然转化为传统的分类问题。此外，为了保持生成特征的类间区分性，提出了视觉枢轴正则化作为显式监督。与之前使用复杂工程正则化器的方法不同，我们的方法可以在没有额外正则化的情况下很好地抑制噪声。经验上，我们展示了我们的方法在基于文本的零样本学习的最大可用基准上始终优于现有技术。","领域":"零样本学习/生成对抗网络/文本到图像生成","问题":"解决从噪声文本描述中进行零样本学习的问题","动机":"利用生成对抗网络的能力从文本描述中生成未见类别的视觉特征，以识别新类别","方法":"提出一种生成模型，输入噪声文本描述并生成合成视觉特征，通过添加伪数据将零样本学习转化为传统分类问题，并提出视觉枢轴正则化以保持类间区分性","关键词":["零样本学习","生成对抗网络","文本到图像生成","视觉语义嵌入","噪声抑制"],"涉及的技术概念":"生成对抗网络（GANs）用于从文本描述生成图像，零样本学习通过生成未见类别的视觉特征来识别新类别，视觉枢轴正则化用于保持生成特征的类间区分性，噪声抑制技术用于处理输入文本中的噪声。"},{"order":104,"title":"Tensorize, Factorize and Regularize: Robust Visual Relationship Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hwang_Tensorize_Factorize_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hwang_Tensorize_Factorize_and_CVPR_2018_paper.html","abstract":"Visual relationships provide higher-level information of objects and their relations in an image – this enables a semantic understanding of the scene and helps downstream applications. Given a set of localized objects in some training data, visual relationship detection seeks to detect the most likely “relationship” between objects in a given image. While the specific objects may be well represented in training data, their relationships may still be infrequent. The empirical distribution obtained from seeing these relationships in a dataset does not model the underlying distribution well — a serious issue for most learning methods. In this work, we start from a simple multi-relational learning model, which in principle, offers a rich formalization for deriving a strong prior for learning visual relationships. While the inference problem for deriving the regularizer is challenging, our main technical contribution is to show how adapting recent results in numerical linear algebra lead to efficient algorithms for a factorization scheme that yields highly informative priors. The factorization provides sample size bounds for inference (under mild conditions) for the underlying [[object, predicate, object]] relationship learning task on its own and surprisingly outperforms (in some cases) existing methods even without utilizing visual features. Then, when integrated with an end to-end architecture for visual relationship detection leveraging image data, we substantially improve the state-of-the-art.","中文标题":"张量化、因子化与正则化：鲁棒的视觉关系学习","摘要翻译":"视觉关系提供了图像中对象及其关系的更高层次信息——这使得场景的语义理解成为可能，并有助于下游应用。给定一些训练数据中的一组定位对象，视觉关系检测旨在检测给定图像中对象之间最可能的“关系”。虽然特定对象在训练数据中可能得到了很好的表示，但它们的关系可能仍然不常见。从数据集中看到这些关系获得的经验分布并不能很好地模拟基础分布——这对大多数学习方法来说是一个严重的问题。在这项工作中，我们从一个简单的多关系学习模型开始，该模型原则上为学习视觉关系提供了丰富的形式化方法，以推导出强先验。虽然推导正则化器的推理问题具有挑战性，但我们的主要技术贡献是展示了如何适应数值线性代数的最新成果，从而为因子化方案带来高效算法，该方案产生了信息丰富的先验。因子化为基础[[对象，谓词，对象]]关系学习任务提供了推理的样本大小界限（在温和条件下），并且令人惊讶的是，即使不利用视觉特征，在某些情况下也优于现有方法。然后，当与利用图像数据进行视觉关系检测的端到端架构集成时，我们显著提高了最先进的技术。","领域":"视觉关系检测/多关系学习/数值线性代数","问题":"视觉关系检测中对象关系的不常见性和经验分布不能很好地模拟基础分布的问题","动机":"提高视觉关系检测的准确性和效率，通过提供信息丰富的先验来改善学习方法的性能","方法":"采用多关系学习模型，结合数值线性代数的最新成果，开发高效的因子化算法，以产生信息丰富的先验，并将其集成到端到端的视觉关系检测架构中","关键词":["视觉关系检测","多关系学习","数值线性代数","因子化算法","端到端架构"],"涉及的技术概念":"视觉关系检测涉及识别图像中对象之间的关系，多关系学习模型用于形式化这些关系，数值线性代数的应用使得因子化算法能够高效地产生信息丰富的先验，这些先验被集成到端到端的视觉关系检测架构中，以提高检测的准确性和效率。"},{"order":105,"title":"Transductive Unbiased Embedding for Zero-Shot Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Transductive_Unbiased_Embedding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Transductive_Unbiased_Embedding_CVPR_2018_paper.html","abstract":"Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem, in which instances of unseen (target) classes tend to be categorized as one of the seen (source) classes. So they yield poor performance after being deployed in the generalized ZSL settings. In this paper, we propose a straightforward yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate the bias problem. Our method follows the way of transductive learning, which assumes that both the labeled source images and unlabeled target images are available for training. In the semantic embedding space, the labeled source images are mapped to several fixed points specified by the source categories, and the unlabeled target images are forced to be mapped to other points specified by the target categories. Experiments conducted on AwA2, CUB and SUN datasets demonstrate that our method outperforms existing state-of-the-art approaches by a huge margin of 9.3~24.5% following generalized ZSL settings, and by a large margin of 0.2~16.2% following conventional ZSL settings.","中文标题":"转导无偏嵌入用于零样本学习","摘要翻译":"大多数现有的零样本学习（ZSL）方法存在强烈的偏差问题，其中未见（目标）类别的实例往往被分类为已见（源）类别之一。因此，在广义ZSL设置中部署后，它们的表现不佳。在本文中，我们提出了一种简单而有效的方法，称为准全监督学习（QFSL），以缓解偏差问题。我们的方法遵循转导学习的方式，假设标记的源图像和未标记的目标图像都可用于训练。在语义嵌入空间中，标记的源图像被映射到由源类别指定的几个固定点，而未标记的目标图像被迫映射到由目标类别指定的其他点。在AwA2、CUB和SUN数据集上进行的实验表明，我们的方法在广义ZSL设置下比现有最先进的方法高出9.3~24.5%，在常规ZSL设置下高出0.2~16.2%。","领域":"零样本学习/语义嵌入/转导学习","问题":"零样本学习中的偏差问题，即未见类别的实例倾向于被分类为已见类别之一。","动机":"解决现有零样本学习方法在广义ZSL设置中表现不佳的问题。","方法":"提出准全监督学习（QFSL）方法，通过转导学习的方式，在语义嵌入空间中映射源图像和目标图像到不同的固定点。","关键词":["零样本学习","语义嵌入","转导学习","准全监督学习"],"涉及的技术概念":{"零样本学习（ZSL）":"一种机器学习方法，旨在让模型能够识别训练时未见过的类别。","语义嵌入":"将数据映射到语义空间中的表示，以便于理解和处理。","转导学习":"一种学习方法，利用训练数据和测试数据之间的关系进行学习，而不仅仅是训练数据。","准全监督学习（QFSL）":"本文提出的方法，旨在通过利用标记和未标记数据来缓解零样本学习中的偏差问题。"}},{"order":106,"title":"Hierarchical Novelty Detection for Visual Object Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.html","abstract":"Deep neural networks have achieved impressive success in large-scale visual object recognition tasks with a predefined set of classes. However, recognizing objects of novel classes unseen during training still remains challenging. The problem of detecting such novel classes has been addressed in the literature, but most prior works have focused on providing simple binary or regressive decisions, e.g., the output would be \\"known,\\" \\"novel,\\" or corresponding confidence intervals. In this paper, we study more informative novelty detection schemes based on a hierarchical classification framework. For an object of a novel class, we aim for finding its closest super class in the hierarchical taxonomy of known classes. To this end, we propose two different approaches termed top-down and flatten methods, and their combination as well. The essential ingredients of our methods are confidence-calibrated classifiers, data relabeling, and the leave-one-out strategy for modeling novel classes under the hierarchical taxonomy. Furthermore, our method can generate a hierarchical embedding that leads to improved generalized zero-shot learning performance in combination with other commonly-used semantic embeddings.","中文标题":"层次化新颖性检测用于视觉对象识别","摘要翻译":"深度神经网络在具有预定义类别集的大规模视觉对象识别任务中取得了令人印象深刻的成功。然而，识别训练期间未见的新类别对象仍然具有挑战性。检测此类新类别的问题已在文献中得到解决，但大多数先前的工作都集中在提供简单的二元或回归决策上，例如，输出将是“已知”、“新颖”或相应的置信区间。在本文中，我们研究了基于层次分类框架的更信息丰富的新颖性检测方案。对于新类别对象，我们的目标是在已知类别的层次分类法中找到其最接近的超类。为此，我们提出了两种不同的方法，称为自上而下和平坦方法，以及它们的组合。我们方法的基本要素是置信度校准的分类器、数据重新标记以及在层次分类法下建模新类别的留一策略。此外，我们的方法可以生成层次嵌入，结合其他常用的语义嵌入，可以提高广义零样本学习性能。","领域":"新颖性检测/层次分类/零样本学习","问题":"识别训练期间未见的新类别对象","动机":"提高对新类别对象的识别能力，特别是在层次分类法中找到其最接近的超类","方法":"提出了两种不同的方法，称为自上而下和平坦方法，以及它们的组合，使用置信度校准的分类器、数据重新标记和在层次分类法下建模新类别的留一策略","关键词":["新颖性检测","层次分类","零样本学习"],"涉及的技术概念":"深度神经网络、置信度校准的分类器、数据重新标记、留一策略、层次嵌入、广义零样本学习"},{"order":107,"title":"Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Zero-Shot_Visual_Recognition_CVPR_2018_paper.html","abstract":"We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem — semantic loss — in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2%, 9.3%, 4.0%, and 3.6% in terms of harmonic mean values.","中文标题":"使用语义保持对抗嵌入网络的零样本视觉识别","摘要翻译":"我们提出了一种名为语义保持对抗嵌入网络（SP-AEN）的新框架，用于零样本视觉识别（ZSL），其中测试图像及其类别在训练期间都是未见过的。SP-AEN旨在解决基于嵌入的ZSL中固有的问题——语义丢失，在训练过程中，如果某些语义对于训练类别不具有区分性，则可能会被丢弃，但这些语义对于识别测试类别可能变得至关重要。具体来说，SP-AEN通过引入一个独立的视觉到语义空间嵌入器来防止语义丢失，该嵌入器将语义空间分解为两个子空间，用于两个可能冲突的目标：分类和重建。通过对这两个子空间的对抗学习，SP-AEN可以将语义从重建子空间转移到判别子空间，从而实现对未见类别的改进的零样本识别。与之前的工作相比，SP-AEN不仅可以提高分类性能，还可以生成逼真的图像，证明了语义保持的有效性。在四个流行的基准测试：CUB、AWA、SUN和aPY上，SP-AEN在调和平均值方面分别以12.2%、9.3%、4.0%和3.6%的绝对性能差异显著优于其他最先进的方法。","领域":"零样本学习/对抗学习/图像生成","问题":"解决零样本视觉识别中的语义丢失问题","动机":"在基于嵌入的零样本视觉识别中，非区分性语义在训练过程中可能被丢弃，但这些语义对于识别测试类别可能至关重要。","方法":"引入独立的视觉到语义空间嵌入器，将语义空间分解为分类和重建两个子空间，并通过对抗学习将语义从重建子空间转移到判别子空间。","关键词":["零样本学习","对抗学习","图像生成"],"涉及的技术概念":"零样本视觉识别（ZSL）是一种在训练期间未见过的类别上进行识别的方法。语义保持对抗嵌入网络（SP-AEN）是一种新框架，通过将语义空间分解为分类和重建两个子空间，并通过对抗学习来防止语义丢失，从而提高零样本识别的性能。"},{"order":108,"title":"Learning Rich Features for Image Manipulation Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Learning_Rich_Features_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Learning_Rich_Features_CVPR_2018_paper.html","abstract":"Image manipulation detection is different from traditional semantic object detection because it pays more attention to tampering artifacts than to image content, which suggests that richer features need to be learned. We propose a two-stream Faster R-CNN network and train it end-to- end to detect the tampered regions given a manipulated image. One of the two streams is an RGB stream whose purpose is to extract features from the RGB image input to find tampering artifacts like strong contrast difference, unnatural tampered boundaries, and so on. The other is a noise stream that leverages the noise features extracted from a steganalysis rich model filter layer to discover the noise inconsistency between authentic and tampered regions. We then fuse features from the two streams through a bilinear pooling layer to further incorporate spatial co-occurrence of these two modalities. Experiments on four standard image manipulation datasets demonstrate that our two-stream framework outperforms each individual stream, and also achieves state-of-the-art performance compared to alternative methods with robustness to resizing and compression.","中文标题":"学习丰富的特征用于图像篡改检测","摘要翻译":"图像篡改检测与传统语义对象检测不同，因为它更关注篡改痕迹而非图像内容，这表明需要学习更丰富的特征。我们提出了一个双流Faster R-CNN网络，并对其进行端到端训练，以检测给定篡改图像中的篡改区域。双流之一是一个RGB流，其目的是从RGB图像输入中提取特征，以发现如强烈对比差异、不自然的篡改边界等篡改痕迹。另一个是噪声流，它利用从隐写分析丰富模型过滤层提取的噪声特征，来发现真实区域和篡改区域之间的噪声不一致性。然后，我们通过双线性池化层融合这两个流的特征，以进一步结合这两种模态的空间共现性。在四个标准图像篡改数据集上的实验表明，我们的双流框架优于每个单独的流，并且与替代方法相比，在调整大小和压缩的鲁棒性方面也达到了最先进的性能。","领域":"图像篡改检测/隐写分析/特征融合","问题":"检测图像中的篡改区域","动机":"传统语义对象检测方法不足以有效检测图像篡改痕迹，需要学习更丰富的特征以提高检测准确性和鲁棒性。","方法":"提出一个双流Faster R-CNN网络，包括RGB流和噪声流，通过双线性池化层融合特征，以检测图像篡改区域。","关键词":["图像篡改检测","双流网络","特征融合","隐写分析"],"涉及的技术概念":{"Faster R-CNN":"一种用于对象检测的深度学习网络，能够同时进行区域提议和对象分类。","RGB流":"处理RGB图像输入，提取与篡改痕迹相关的特征。","噪声流":"利用隐写分析技术提取噪声特征，用于发现篡改区域。","双线性池化层":"一种特征融合技术，用于结合不同模态的特征，提高检测性能。"}},{"order":109,"title":"Human Semantic Parsing for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kalayeh_Human_Semantic_Parsing_CVPR_2018_paper.html","abstract":"Person re-identification is a challenging task mainly due to factors such as background clutter, pose, illumination and camera point of view variations. These elements hinder the process of extracting robust and discriminative representations, hence preventing different identities from being successfully distinguished. To improve the representation learning, usually local features from human body parts are extracted. However, the common practice for such a process has been based on bounding box part detection. In this paper, we propose to adopt human semantic parsing which, due to its pixel-level accuracy and capability of modeling arbitrary contours, is naturally a better alternative. Our proposed SPReID integrates human semantic parsing in person re-identification and not only considerably outperforms its counter baseline, but achieves state-of-the-art performance. We also show that, by employing a simple yet effective training strategy, standard popular deep convolutional architectures such as Inception-V3 and ResNet-152, with no modification, while operating solely on full image, can dramatically outperform current state-of-the-art. Our proposed methods improve state-of-the-art person re-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by ~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.","中文标题":"人类语义解析用于行人重识别","摘要翻译":"行人重识别是一项具有挑战性的任务，主要由于背景杂乱、姿态、光照和摄像机视角变化等因素。这些因素阻碍了提取鲁棒和区分性表示的过程，从而阻止了不同身份的成功区分。为了改进表示学习，通常从人体部位提取局部特征。然而，这种过程的常见做法是基于边界框部位检测。在本文中，我们提出采用人类语义解析，由于其像素级精度和建模任意轮廓的能力，自然是一个更好的替代方案。我们提出的SPReID将人类语义解析集成到行人重识别中，不仅显著优于其对比基线，而且实现了最先进的性能。我们还表明，通过采用简单而有效的训练策略，标准流行的深度卷积架构，如Inception-V3和ResNet-152，无需修改，仅在全图像上操作，就能显著超越当前的最先进技术。我们提出的方法在以下数据集上改进了最先进的行人重识别：Market-1501的mAP提高了约17%，rank-1提高了约6%；CUHK03的rank-1提高了约4%；DukeMTMC-reID的mAP提高了约24%，rank-1提高了约10%。","领域":"行人重识别/语义解析/表示学习","问题":"行人重识别中的表示学习问题","动机":"提高行人重识别的准确性和鲁棒性，通过改进表示学习方法","方法":"采用人类语义解析技术，结合深度卷积架构进行训练","关键词":["行人重识别","语义解析","表示学习","深度卷积架构"],"涉及的技术概念":"人类语义解析是一种像素级精度的技术，能够建模任意轮廓，用于改进行人重识别的表示学习。本文提出的SPReID方法集成了人类语义解析，通过简单而有效的训练策略，使用未修改的Inception-V3和ResNet-152架构，在全图像上操作，显著提高了行人重识别的性能。"},{"order":110,"title":"Stacked Latent Attention for Multimodal Reasoning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Stacked_Latent_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Stacked_Latent_Attention_CVPR_2018_paper.html","abstract":"Attention has shown to be a pivotal development in deep learning and has been used for a multitude of multimodal learning tasks such as visual question answering and image captioning. In this work, we pinpoint the potential limitations to the design of a traditional attention model. We identify that 1) current attention mechanisms discard the latent information from intermediate reasoning, losing the positional information already captured by the attention heatmaps and 2) stacked attention, a common way to improve spatial reasoning, may have suboptimal performance because of the vanishing gradient problem. We introduce a novel attention architecture to address these problems, in which all spatial configuration information contained in the intermediate reasoning process is retained in a pathway of convolutional layers. We show that this new attention leads to substantial improvements in multiple multimodal reasoning tasks, including achieving single model performance without using external knowledge comparable to the state-of-the-art on the VQA dataset, as well as clear gains for the image captioning task.","中文标题":"堆叠潜在注意力用于多模态推理","摘要翻译":"注意力机制已被证明是深度学习中的一项关键发展，并已被用于多种多模态学习任务，如视觉问答和图像描述。在这项工作中，我们指出了传统注意力模型设计的潜在局限性。我们发现1）当前的注意力机制丢弃了中间推理过程中的潜在信息，丢失了注意力热图已经捕捉到的位置信息；2）堆叠注意力，一种常见的提高空间推理能力的方法，可能由于梯度消失问题而表现不佳。我们引入了一种新颖的注意力架构来解决这些问题，其中中间推理过程中包含的所有空间配置信息都保留在卷积层的路径中。我们展示了这种新的注意力机制在多种多模态推理任务中带来了显著的改进，包括在不使用外部知识的情况下，在VQA数据集上实现了与最先进技术相媲美的单一模型性能，以及在图像描述任务中取得了明显的增益。","领域":"视觉问答/图像描述/空间推理","问题":"传统注意力模型设计中的潜在局限性，包括丢弃中间推理的潜在信息和堆叠注意力可能因梯度消失问题而表现不佳。","动机":"解决传统注意力模型在处理多模态学习任务时的局限性，提高空间推理能力和任务性能。","方法":"引入一种新颖的注意力架构，保留中间推理过程中的所有空间配置信息在卷积层的路径中。","关键词":["注意力机制","多模态学习","空间推理","视觉问答","图像描述"],"涉及的技术概念":"注意力机制是一种在深度学习中用于提高模型性能的技术，特别是在处理多模态数据（如视觉和文本）时。堆叠注意力是一种通过多层注意力机制来提高模型对空间信息的理解能力的方法。梯度消失问题是指在深度神经网络训练过程中，梯度在反向传播过程中逐渐变小，导致网络权重更新缓慢，影响模型性能。卷积层是深度学习中用于提取图像特征的一种网络层，通过卷积操作捕捉图像中的空间信息。"},{"order":111,"title":"R-FCN-3000 at 30fps: Decoupling Detection and Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_R-FCN-3000_at_30fps_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Singh_R-FCN-3000_at_30fps_CVPR_2018_paper.html","abstract":"We propose a modular approach towards large-scale real-time object detection by decoupling objectness detection and classification. We exploit the fact that many object classes are visually similar and share parts. Thus, a universal objectness detector can be learned for class-agnostic object detection followed by fine-grained classification using a (non)linear classifier. Our approach is a modification of the R-FCN architecture to learn shared filters for performing localization across different object classes. We trained a detector for 3000 object classes, called R-FCN-3000, that obtains an mAP of 34.9% on the ImageNet detection dataset. It outperforms  YOLO-9000 by 18% while processing 30 images per second. We also show that the objectness learned by R-FCN-3000 generalizes to novel classes  and the performance increases with the number of training object classes - supporting the hypothesis that it is possible to learn a universal objectness detector.","中文标题":"R-FCN-3000以30fps运行：解耦检测与分类","摘要翻译":"我们提出了一种模块化方法，通过解耦物体检测和分类来实现大规模实时物体检测。我们利用了许多物体类别在视觉上相似并共享部分的事实。因此，可以学习一个通用的物体检测器用于类别无关的物体检测，然后使用（非）线性分类器进行细粒度分类。我们的方法是对R-FCN架构的修改，以学习共享过滤器来执行不同物体类别的定位。我们训练了一个名为R-FCN-3000的检测器，用于3000个物体类别，在ImageNet检测数据集上获得了34.9%的mAP。它以每秒处理30张图像的速度，性能优于YOLO-9000 18%。我们还展示了R-FCN-3000学习的物体检测能力可以推广到新类别，并且性能随着训练物体类别数量的增加而提高——支持了学习通用物体检测器的假设。","领域":"物体检测/实时处理/大规模分类","问题":"大规模实时物体检测中的检测与分类耦合问题","动机":"提高大规模实时物体检测的效率和准确性，通过解耦检测与分类来优化性能","方法":"修改R-FCN架构，学习共享过滤器进行不同物体类别的定位，先进行类别无关的物体检测，再进行细粒度分类","关键词":["物体检测","实时处理","大规模分类","R-FCN","解耦"],"涉及的技术概念":"R-FCN架构是一种用于物体检测的深度学习模型，通过共享过滤器来定位不同类别的物体。mAP（mean Average Precision）是评估物体检测模型性能的指标，表示模型在所有类别上的平均精度。YOLO-9000是另一种实时物体检测模型，R-FCN-3000在性能上优于它。"},{"order":112,"title":"CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_CSRNet_Dilated_Convolutional_CVPR_2018_paper.html","abstract":"We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves  47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.","中文标题":"CSRNet: 用于理解高度拥挤场景的扩张卷积神经网络","摘要翻译":"我们提出了一种名为CSRNet的网络，用于拥挤场景识别，旨在提供一种数据驱动和深度学习的方法，能够理解高度拥挤的场景，并执行准确的计数估计以及呈现高质量的密度图。提出的CSRNet由两个主要组件组成：一个卷积神经网络（CNN）作为前端用于2D特征提取，以及一个扩张CNN作为后端，它使用扩张核来提供更大的感受野并替换池化操作。由于其纯卷积结构，CSRNet是一个易于训练的模型。我们在四个数据集（ShanghaiTech数据集、UCF_CC_50数据集、WorldEXPO'10数据集和UCSD数据集）上展示了CSRNet，并提供了最先进的性能。在ShanghaiTech Part_B数据集中，CSRNet实现了比之前最先进方法低47.3%的平均绝对误差（MAE）。我们扩展了目标应用，以计数其他对象，例如TRANCOS数据集中的车辆。结果显示，CSRNet显著提高了输出质量，比之前最先进的方法低15.4%的MAE。","领域":"拥挤场景识别/密度图估计/计数估计","问题":"如何在高度拥挤的场景中准确估计人数并生成高质量的密度图","动机":"为了提供一种有效的方法来理解和分析高度拥挤的场景，以及在这些场景中准确估计人数和生成高质量的密度图","方法":"提出了一种名为CSRNet的网络，该网络由两个主要组件组成：一个用于2D特征提取的卷积神经网络（CNN）前端和一个使用扩张核的扩张CNN后端，以提供更大的感受野并替换池化操作","关键词":["拥挤场景识别","密度图估计","计数估计"],"涉及的技术概念":"卷积神经网络（CNN）用于2D特征提取，扩张CNN用于提供更大的感受野并替换池化操作，平均绝对误差（MAE）用于评估模型性能"},{"order":113,"title":"Revisiting Knowledge Transfer for Training Object Class Detectors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Uijlings_Revisiting_Knowledge_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Uijlings_Revisiting_Knowledge_Transfer_CVPR_2018_paper.html","abstract":"We propose to revisit knowledge transfer for training object detectors on target classes from weakly supervised training images, helped by a set of source classes with bounding-box annotations. We present a unified knowledge transfer framework based on training a single neural network multi-class object detector over all source classes, organized in a semantic hierarchy. This generates proposals with scores at multiple levels in the hierarchy, which we use to explore knowledge transfer over a broad range of generality, ranging from class-specific (bycicle to motorbike) to class-generic (objectness to any class). Experiments on the 200 object classes in the ILSVRC 2013 detection dataset show that our technique (1) leads to much better performance on the target classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline which uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2) delivers target object detectors reaching 80% of the mAP of their fully supervised counterparts. (3) outperforms the best reported transfer learning results on this dataset (+41% CorLoc and +3% mAP over [18, 46], +16.2% mAP over [32]). Moreover, we also carry out several across-dataset knowledge transfer experiments [27, 24, 35] and find that (4) our technique outperforms the weakly supervised baseline in all dataset pairs by 1.5 × −1.9×, establishing its general applicability.","中文标题":"重新审视知识转移以训练目标类物体检测器","摘要翻译":"我们提出重新审视知识转移，以从弱监督训练图像中训练目标类物体检测器，借助一组带有边界框注释的源类。我们提出了一个统一的知识转移框架，基于在所有源类上训练一个单一神经网络多类物体检测器，这些源类组织在一个语义层次结构中。这会在层次结构的多个级别生成带有分数的提案，我们使用这些提案来探索从类特定（自行车到摩托车）到类通用（物体性到任何类）的广泛通用性范围内的知识转移。在ILSVRC 2013检测数据集上的200个对象类上的实验表明，我们的技术（1）在目标类上比使用手动设计的物体性的弱监督基线（50.5% CorLoc，25.4% mAP）表现更好（70.3% CorLoc，36.9% mAP）。（2）提供的目标物体检测器达到了其完全监督对应物的80%的mAP。（3）在该数据集上优于最佳报告的转移学习结果（CorLoc +41%和mAP +3%超过[18, 46]，mAP +16.2%超过[32]）。此外，我们还进行了几个跨数据集的知识转移实验[27, 24, 35]，发现（4）我们的技术在所有数据集对中均优于弱监督基线1.5×−1.9×，确立了其普遍适用性。","领域":"物体检测/知识转移/弱监督学习","问题":"如何有效地利用带有边界框注释的源类来训练目标类物体检测器","动机":"提高在弱监督条件下训练目标类物体检测器的性能","方法":"提出一个统一的知识转移框架，通过训练一个单一神经网络多类物体检测器，利用语义层次结构中的源类生成提案，探索从类特定到类通用的知识转移","关键词":["物体检测","知识转移","弱监督学习","语义层次结构","神经网络"],"涉及的技术概念":{"知识转移":"利用已有知识（如带有边界框注释的源类）来帮助学习新知识（如目标类物体检测器）的过程","弱监督学习":"一种机器学习方法，其中训练数据的标签信息不完全，例如只有图像级别的标签而没有具体的物体位置信息","语义层次结构":"一种组织类别的方式，其中类别按照从一般到具体的层次结构排列，有助于知识转移","神经网络":"一种模拟人脑结构和功能的计算模型，用于处理复杂的模式识别和预测任务"}},{"order":114,"title":"Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Deep_Sparse_Coding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Deep_Sparse_Coding_CVPR_2018_paper.html","abstract":"Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous in virtually all machine learning and computer vision challenges; however, advancements in CNNs have arguably reached an engineering saturation point where incremental novelty results in minor performance gains.  Although there is evidence that object classification has reached human levels on narrowly defined tasks, for general applications, the biological visual system is far superior to that of any computer.  Research reveals there are numerous missing components in feed-forward deep neural networks that are critical in mammalian vision.   The brain does not work solely in a feed-forward fashion, but rather all of the neurons are in competition with each other; neurons are integrating information in a bottom up and top down fashion and incorporating expectation and feedback in the modeling process.  Furthermore, our visual cortex is working in tandem with our parietal lobe, integrating sensory information from various modalities.   In our work, we sought to improve upon the standard feed-forward deep learning model by augmenting them with biologically inspired concepts of sparsity, top down feedback, and lateral inhibition.  We define our model as a sparse coding problem using hierarchical layers.  We solve the sparse coding problem with an additional top down feedback error driving the dynamics of the neural network.  While building and observing the behavior of our model, we were fascinated that multimodal, invariant neurons naturally emerged that mimicked, \\"Halle Berry neurons\\" found in the human brain.  These neurons trained in our sparse model learned to respond to high level concepts from multiple modalities, which is not the case with a standard feed-forward autoencoder.  Furthermore, our sparse representation of multimodal signals demonstrates qualitative and quantitative superiority to the standard feed-forward joint embedding in common vision and machine learning tasks.","中文标题":"深度稀疏编码用于不变多模态Halle Berry神经元","摘要翻译":"深度前馈卷积神经网络（CNNs）几乎在所有机器学习和计算机视觉挑战中变得无处不在；然而，CNNs的进步可以说已经达到了一个工程饱和点，即增量创新带来的性能提升微乎其微。尽管有证据表明，在狭义定义的任务上，物体分类已经达到了人类水平，但对于一般应用而言，生物视觉系统远远优于任何计算机。研究揭示了前馈深度神经网络中缺少许多在哺乳动物视觉中至关重要的组件。大脑不仅仅以前馈方式工作，而是所有神经元都在相互竞争；神经元以自下而上和自上而下的方式整合信息，并在建模过程中融入预期和反馈。此外，我们的视觉皮层与顶叶协同工作，整合来自各种模态的感觉信息。在我们的工作中，我们试图通过增加生物启发的稀疏性、自上而下的反馈和侧向抑制概念来改进标准的前馈深度学习模型。我们将我们的模型定义为使用层次层的稀疏编码问题。我们通过额外的自上而下反馈误差驱动神经网络的动态来解决稀疏编码问题。在构建和观察我们的模型行为时，我们着迷于多模态、不变的神经元自然出现，模仿了人类大脑中发现的“Halle Berry神经元”。在我们的稀疏模型中训练的神经元学会了响应来自多个模态的高级概念，这是标准前馈自动编码器所不具备的。此外，我们对多模态信号的稀疏表示在常见的视觉和机器学习任务中展示了定性和定量上的优越性，相较于标准的前馈联合嵌入。","领域":"神经科学/深度学习/多模态学习","问题":"改进标准前馈深度学习模型，使其更接近生物视觉系统的功能","动机":"生物视觉系统在一般应用上远优于任何计算机，而现有的前馈深度神经网络缺少许多在哺乳动物视觉中至关重要的组件","方法":"通过增加生物启发的稀疏性、自上而下的反馈和侧向抑制概念来改进标准的前馈深度学习模型，定义为使用层次层的稀疏编码问题，并通过额外的自上而下反馈误差驱动神经网络的动态","关键词":["稀疏编码","多模态学习","生物启发模型","Halle Berry神经元"],"涉及的技术概念":"稀疏编码是一种在信号处理中用于表示信号的技术，它假设信号可以用少量的非零系数表示。多模态学习涉及从多种类型的数据（如视觉和听觉）中学习。生物启发模型是指那些受到生物系统启发而设计的计算模型。Halle Berry神经元是指在人类大脑中发现的对特定概念（如名人Halle Berry）响应的神经元。"},{"order":115,"title":"On the Convergence of PatchMatch and Its Variants","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ehret_On_the_Convergence_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ehret_On_the_Convergence_CVPR_2018_paper.html","abstract":"Many problems in image/video processing and computer vision require the computation of a dense k-nearest neighbor field (k-NNF) between two images. For each patch in a query image, the k-NNF determines the positions of the k most similar patches in a database image. With the introduction of the PatchMatch algorithm, Barnes et al. demonstrated that this large search problem can be approximated efficiently by collaborative search methods that exploit the local coherency of image patches. After its introduction, several variants of the original PatchMatch algorithm have been proposed, some of them reducing the computational time by two orders of magnitude. In this work we propose a theoretical framework for the analysis of PatchMatch and its variants, and apply it to derive bounds on their covergence rate. We consider a generic PatchMatch algorithm from which most specific instances found in the literature can be derived as particular cases. We also derive more specific bounds for two of these particular cases: the original PatchMatch and Coherency Sensitive Hashing. The proposed bounds are validated by contrasting them to the convergence observed in practice.","中文标题":"关于PatchMatch及其变体的收敛性","摘要翻译":"在图像/视频处理和计算机视觉中的许多问题需要计算两幅图像之间的密集k最近邻域（k-NNF）。对于查询图像中的每个补丁，k-NNF确定数据库图像中k个最相似补丁的位置。随着PatchMatch算法的引入，Barnes等人展示了通过利用图像补丁的局部一致性进行协作搜索方法，可以有效地近似这个大型搜索问题。自其引入以来，已经提出了几种原始PatchMatch算法的变体，其中一些将计算时间减少了两个数量级。在这项工作中，我们提出了一个用于分析PatchMatch及其变体的理论框架，并应用它来推导它们的收敛速度界限。我们考虑了一个通用的PatchMatch算法，从中可以推导出文献中发现的大多数特定实例作为特殊情况。我们还为其中两个特殊情况推导了更具体的界限：原始PatchMatch和一致性敏感哈希。通过将它们与实践中观察到的收敛性进行对比，验证了所提出的界限。","领域":"图像处理/计算机视觉/算法优化","问题":"计算两幅图像之间的密集k最近邻域（k-NNF）","动机":"提高图像/视频处理和计算机视觉中k-NNF计算的效率","方法":"提出了一个理论框架来分析PatchMatch及其变体，并推导它们的收敛速度界限","关键词":["k-NNF","PatchMatch","一致性敏感哈希"],"涉及的技术概念":{"k-NNF":"密集k最近邻域，用于确定图像中每个补丁的k个最相似补丁的位置","PatchMatch":"一种用于高效近似k-NNF计算的算法","一致性敏感哈希":"PatchMatch算法的一个变体，用于提高计算效率"}},{"order":116,"title":"Rethinking the Faster R-CNN Architecture for Temporal Action Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Rethinking_the_Faster_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Rethinking_the_Faster_CVPR_2018_paper.html","abstract":"We propose TAL-Net, an improved approach to temporal action localization in video that is inspired by the Faster R-CNN object detection framework. TAL-Net addresses three key shortcomings of existing approaches: (1) we improve receptive field alignment using a multi-scale architecture that can accommodate extreme variation in action durations; (2) we better exploit the temporal context of actions for both proposal generation and action classification by appropriately extending receptive fields; and (3) we explicitly consider multi-stream feature fusion and demonstrate that fusing motion late is important. We achieve state-of-the-art performance for both action proposal and localization on THUMOS'14 detection benchmark and competitive performance on ActivityNet challenge.","中文标题":"重新思考用于时序动作定位的Faster R-CNN架构","摘要翻译":"我们提出了TAL-Net，这是一种受Faster R-CNN目标检测框架启发的改进视频时序动作定位方法。TAL-Net解决了现有方法的三个关键不足：(1) 我们通过使用能够适应动作持续时间极端变化的多尺度架构来改进感受野对齐；(2) 我们通过适当扩展感受野，更好地利用动作的时间上下文进行提案生成和动作分类；(3) 我们明确考虑了多流特征融合，并证明了后期融合运动信息的重要性。我们在THUMOS'14检测基准上实现了最先进的动作提案和定位性能，并在ActivityNet挑战中取得了竞争性的表现。","领域":"视频分析/动作识别/时序建模","问题":"现有方法在视频时序动作定位中的三个关键不足：感受野对齐、时间上下文利用和多流特征融合。","动机":"改进视频时序动作定位的准确性和效率，通过解决现有方法的不足。","方法":"提出TAL-Net，采用多尺度架构改进感受野对齐，扩展感受野以更好地利用时间上下文，以及考虑多流特征融合并证明后期融合运动信息的重要性。","关键词":["时序动作定位","多尺度架构","感受野扩展","多流特征融合"],"涉及的技术概念":"TAL-Net是一种改进的视频时序动作定位方法，它通过多尺度架构改进感受野对齐，扩展感受野以更好地利用时间上下文，以及考虑多流特征融合并证明后期融合运动信息的重要性。这些技术概念旨在提高动作提案和定位的准确性和效率。"},{"order":117,"title":"MoNet: Deep Motion Exploitation for Video Object Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xiao_MoNet_Deep_Motion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xiao_MoNet_Deep_Motion_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel MoNet model to deeply exploit motion cues for boosting video object segmentation performance from two aspects, i.e., frame representation learning and segmentation refinement. Concretely, MoNet exploits computed motion cue (i.e., optical flow) to reinforce the representation of the target frame by  aligning and integrating representations from its neighbors. The new representation provides valuable temporal contexts for segmentation and improves robustness to various common contaminating factors, e.g., motion blur, appearance variation and deformation of video objects. Moreover, MoNet exploits motion inconsistency and transforms such motion cue into foreground/background prior to eliminate distraction from confusing instances and noisy regions. By introducing a distance transform layer, MoNet can effectively separate motion-inconstant instances/regions and thoroughly refine segmentation results. Integrating the proposed two motion exploitation components with a standard segmentation network, MoNet provides new state-of-the-art performance on three competitive benchmark datasets.","中文标题":"MoNet: 深度运动利用用于视频对象分割","摘要翻译":"在本文中，我们提出了一种新颖的MoNet模型，从两个方面深入利用运动线索以提升视频对象分割性能，即帧表示学习和分割细化。具体来说，MoNet利用计算出的运动线索（即光流）通过对齐和整合来自其邻居的表示来加强目标帧的表示。这种新的表示为分割提供了宝贵的时间上下文，并提高了对视频对象的各种常见污染因素（如运动模糊、外观变化和变形）的鲁棒性。此外，MoNet利用运动不一致性并将这种运动线索转化为前景/背景先验，以消除来自混淆实例和噪声区域的干扰。通过引入距离变换层，MoNet能够有效地分离运动不一致的实例/区域，并彻底细化分割结果。将提出的两种运动利用组件与标准分割网络集成，MoNet在三个竞争性基准数据集上提供了新的最先进性能。","领域":"视频对象分割/光流分析/运动估计","问题":"提升视频对象分割性能","动机":"通过深入利用运动线索来加强视频对象分割的表示学习和分割细化，提高对常见污染因素的鲁棒性，并消除混淆实例和噪声区域的干扰。","方法":"利用光流加强目标帧的表示，通过运动不一致性转化为前景/背景先验，引入距离变换层分离运动不一致的实例/区域，与标准分割网络集成。","关键词":["视频对象分割","光流","运动估计"],"涉及的技术概念":"MoNet模型通过计算光流（运动线索）来加强视频帧的表示，利用运动不一致性来区分前景和背景，通过距离变换层细化分割结果，最终与标准分割网络集成，提升视频对象分割的性能。"},{"order":118,"title":"Video Representation Learning Using Discriminative Pooling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Representation_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Video_Representation_Learning_CVPR_2018_paper.html","abstract":"Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action---indeed, many are common across multiple actions---pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.","中文标题":"使用判别池化的视频表示学习","摘要翻译":"流行的视频动作识别深度模型为短视频片段生成独立的预测，然后通过启发式方法将这些预测池化，以分配完整视频片段的动作标签。由于并非所有帧都能表征底层动作——实际上，许多帧在多个动作中是共有的——对所有帧施加同等重要性的池化方案可能是不利的。为了解决这个问题，我们提出了判别池化，基于在所有短视频片段生成的深度特征中，至少有一个特征能够表征动作的概念。为此，我们学习了一个（非线性）超平面，将这个未知但具有判别性的特征与其他特征分开。在大间隔设置中应用多实例学习，我们使用这个分隔超平面的参数作为完整视频片段的描述符。由于这些参数与最大间隔框架中的支持向量直接相关，它们作为特征池化的鲁棒表示。我们制定了一个联合目标和一个有效的求解器，该求解器学习每个视频的这些超平面以及超平面上的相应动作分类器。我们的池化方案在深度框架内是端到端可训练的。我们报告了在三个基准数据集上的实验结果，这些数据集涵盖了各种挑战，并展示了在这些任务中的最先进性能。","领域":"视频动作识别/特征学习/多实例学习","问题":"视频动作识别中，如何有效地池化短视频片段的预测以分配完整视频片段的动作标签","动机":"由于并非所有视频帧都能表征底层动作，且许多帧在多个动作中共有，对所有帧施加同等重要性的池化方案可能不利于动作识别","方法":"提出判别池化方法，通过学习一个非线性超平面来分离具有判别性的特征，并利用多实例学习在大间隔设置中应用这些超平面参数作为视频片段的描述符","关键词":["视频动作识别","判别池化","多实例学习","大间隔学习","非线性超平面"],"涉及的技术概念":"深度特征、多实例学习、大间隔框架、支持向量、非线性超平面、端到端训练"},{"order":119,"title":"Recognizing Human Actions as the Evolution of Pose Estimation Maps","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Recognizing_Human_Actions_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html","abstract":"Most video-based action recognition approaches choose to extract features from the whole video to recognize actions. The cluttered background and non-action motions limit the performances of these methods, since they lack the explicit modeling of human body movements. With recent advances of human pose estimation, this work presents a novel method to recognize human action as the evolution of pose estimation maps. Instead of relying on the inaccurate human poses estimated from videos, we observe that pose estimation maps, the byproduct of pose estimation, preserve richer cues of human body to benefit action recognition. Specifically, the evolution of pose estimation maps can be decomposed as an evolution of heatmaps, e.g., probabilistic maps, and an evolution of estimated 2D human poses, which denote the changes of body shape and body pose, respectively. Considering the sparse property of heatmap, we develop spatial rank pooling to aggregate the evolution of heatmaps as a body shape evolution image. As body shape evolution image does not differentiate body parts, we design body guided sampling to aggregate the evolution of poses as a body pose evolution image. The complementary properties between both types of images are explored by deep convolutional neural networks to predict action label. Experiments on NTU RGB+D, UTD-MHAD and PennAction datasets verify the effectiveness of our method, which outperforms most state-of-the-art methods.","中文标题":"将人体动作识别为姿态估计图的演变","摘要翻译":"大多数基于视频的动作识别方法选择从整个视频中提取特征来识别动作。由于缺乏对人体运动的明确建模，杂乱的背景和非动作运动限制了这些方法的性能。随着人体姿态估计的最新进展，本文提出了一种新颖的方法，将人体动作识别为姿态估计图的演变。我们观察到，姿态估计图作为姿态估计的副产品，保留了更丰富的人体线索，有利于动作识别，而不是依赖于从视频中估计的不准确的人体姿态。具体来说，姿态估计图的演变可以分解为热图的演变（例如概率图）和估计的2D人体姿态的演变，分别表示身体形状和身体姿态的变化。考虑到热图的稀疏性，我们开发了空间秩池化来将热图的演变聚合为身体形状演变图像。由于身体形状演变图像不区分身体部位，我们设计了身体引导采样来将姿态的演变聚合为身体姿态演变图像。通过深度卷积神经网络探索这两种类型图像之间的互补属性，以预测动作标签。在NTU RGB+D、UTD-MHAD和PennAction数据集上的实验验证了我们方法的有效性，其性能优于大多数最先进的方法。","领域":"动作识别/姿态估计/视频分析","问题":"视频中人体动作识别受限于杂乱的背景和非动作运动，缺乏对人体运动的明确建模。","动机":"利用姿态估计图作为姿态估计的副产品，保留更丰富的人体线索，以提高动作识别的性能。","方法":"提出将人体动作识别为姿态估计图的演变，通过空间秩池化和身体引导采样分别聚合热图和姿态的演变，利用深度卷积神经网络探索两种类型图像之间的互补属性来预测动作标签。","关键词":["动作识别","姿态估计","视频分析","空间秩池化","身体引导采样"],"涉及的技术概念":"姿态估计图、热图（概率图）、2D人体姿态、空间秩池化、身体引导采样、深度卷积神经网络"},{"order":120,"title":"Video Person Re-Identification With Competitive Snippet-Similarity Aggregation and Co-Attentive Snippet Embedding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Video_Person_Re-Identification_CVPR_2018_paper.html","abstract":"In this paper, we address video-based person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding.  Our approach divides long person sequences into multiple short video snippets and aggregates the top-ranked snippet similarities for sequence-similarity estimation.  With this strategy, the intra-person visual variation of each sample could be minimized for similarity estimation, while the diverse appearance and temporal information are maintained. The snippet similarities are estimated by a deep neural network with a novel temporal co-attention for snippet embedding. The attention weights are obtained based on a query feature, which is learned from the whole probe snippet by an LSTM network, making the resulting embeddings less affected by noisy frames. The gallery snippet shares the same query feature with the probe snippet. Thus the embedding of gallery snippet can present more relevant features to compare with the probe snippet, yielding more accurate snippet similarity. Extensive ablation studies verify the effectiveness of competitive snippet-similarity aggregation as well as the temporal co-attentive embedding.  Our method significantly outperforms the current state-of-the-art approaches on multiple datasets.","中文标题":"视频行人重识别：基于竞争性片段相似性聚合与共注意力片段嵌入","摘要翻译":"本文提出了一种基于视频的行人重识别方法，该方法采用竞争性片段相似性聚合和共注意力片段嵌入。我们的方法将长行人序列分割成多个短视频片段，并聚合排名最高的片段相似性以估计序列相似性。通过这种策略，可以最小化每个样本的个体内视觉变化以进行相似性估计，同时保持多样化的外观和时间信息。片段相似性通过一个深度神经网络估计，该网络采用了一种新颖的时间共注意力机制进行片段嵌入。注意力权重基于查询特征获得，该查询特征通过LSTM网络从整个探测片段中学习，使得生成的嵌入受噪声帧的影响较小。画廊片段与探测片段共享相同的查询特征。因此，画廊片段的嵌入可以呈现更多相关特征与探测片段进行比较，从而产生更准确的片段相似性。大量的消融研究验证了竞争性片段相似性聚合以及时间共注意力嵌入的有效性。我们的方法在多个数据集上显著优于当前的最先进方法。","领域":"行人重识别/视频分析/深度学习","问题":"视频中行人重识别的准确性问题","动机":"提高视频行人重识别的准确性，通过最小化个体内视觉变化并保持多样化的外观和时间信息","方法":"将长行人序列分割成短视频片段，采用竞争性片段相似性聚合和共注意力片段嵌入，通过深度神经网络估计片段相似性","关键词":["行人重识别","视频片段","相似性聚合","共注意力机制","LSTM网络"],"涉及的技术概念":"竞争性片段相似性聚合是一种通过聚合排名最高的片段相似性来估计序列相似性的方法。共注意力片段嵌入是一种通过时间共注意力机制进行片段嵌入的方法，其中注意力权重基于查询特征获得，该查询特征通过LSTM网络从整个探测片段中学习。这种方法旨在最小化个体内视觉变化，同时保持多样化的外观和时间信息，以提高行人重识别的准确性。"},{"order":121,"title":"Mask-Guided Contrastive Attention Model for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.html","abstract":"Person Re-identification (ReID) is an important yet challenging task in computer vision. Due to the diverse background clutters, variations on viewpoints and body poses, it is far from solved. How to extract discriminative and robust features invariant to background clutters is the core problem. In this paper, we first introduce the binary segmentation masks to construct synthetic RGB-Mask pairs as inputs, then we design a mask-guided contrastive attention model (MGCAM) to learn features separately from the body and background regions. Moreover, we propose a novel region-level triplet loss to restrain the features learnt from different regions, i.e., pulling the features from the full image and body region close, whereas pushing the features from backgrounds away. We may be the first one to successfully introduce the binary mask into person ReID task and the first one to propose region-level contrastive learning. We evaluate the proposed method on three public datasets, including MARS, Market-1501 and CUHK03. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. Mask and code will be released upon request.","中文标题":"基于掩码引导的对比注意力模型用于行人重识别","摘要翻译":"行人重识别（ReID）是计算机视觉中一个重要但具有挑战性的任务。由于背景杂乱的多样性、视角和身体姿势的变化，这一问题远未解决。如何提取对背景杂乱不变的区分性和鲁棒性特征是核心问题。在本文中，我们首先引入二值分割掩码来构建合成的RGB-掩码对作为输入，然后设计了一个掩码引导的对比注意力模型（MGCAM）来分别从身体和背景区域学习特征。此外，我们提出了一种新颖的区域级三元组损失来约束从不同区域学习的特征，即将全图和身体区域的特征拉近，而将背景特征推远。我们可能是第一个成功将二值掩码引入行人ReID任务并首次提出区域级对比学习的人。我们在三个公共数据集上评估了所提出的方法，包括MARS、Market-1501和CUHK03。大量的实验结果表明，所提出的方法是有效的，并达到了最先进的结果。掩码和代码将根据请求发布。","领域":"行人重识别/特征提取/对比学习","问题":"如何提取对背景杂乱不变的区分性和鲁棒性特征","动机":"解决行人重识别中由于背景杂乱、视角和身体姿势变化导致的特征提取难题","方法":"引入二值分割掩码构建RGB-掩码对作为输入，设计掩码引导的对比注意力模型（MGCAM）分别从身体和背景区域学习特征，并提出区域级三元组损失来约束特征学习","关键词":["行人重识别","特征提取","对比学习","二值分割掩码","区域级三元组损失"],"涉及的技术概念":"二值分割掩码用于构建RGB-掩码对输入，掩码引导的对比注意力模型（MGCAM）用于分别从身体和背景区域学习特征，区域级三元组损失用于约束特征学习，使得全图和身体区域的特征拉近，背景特征推远。"},{"order":122,"title":"Blazingly Fast Video Object Segmentation With Pixel-Wise Metric Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Blazingly_Fast_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Blazingly_Fast_Video_CVPR_2018_paper.html","abstract":"This paper tackles the problem of video object segmentation,  given some user annotation which indicates the object of interest. The problem is formulated as pixel-wise retrieval in a learned embedding space: we embed pixels of the same object instance into the vicinity of each other, using a fully convolutional network trained by a modified triplet loss as the embedding model. Then the annotated pixels are set as reference and the rest of the pixels are classified using a nearest-neighbor approach. The proposed method supports different kinds of user input such as segmentation mask in the first frame (semi-supervised scenario), or a sparse set of clicked points (interactive scenario). In the semi-supervised scenario, we achieve results competitive with the state of the art but at a fraction of computation cost (275 milliseconds per frame). In the interactive scenario where the user is able to refine their input iteratively, the proposed method provides instant response to each input, and reaches comparable quality to competing methods with much less interaction.","中文标题":"通过像素级度量学习实现极速视频对象分割","摘要翻译":"本文解决了视频对象分割的问题，给定一些用户注释指示感兴趣的对象。该问题被表述为在学习的嵌入空间中进行像素级检索：我们使用通过修改的三元组损失训练的完全卷积网络作为嵌入模型，将同一对象实例的像素嵌入到彼此的附近。然后，将注释的像素设置为参考，并使用最近邻方法对剩余的像素进行分类。所提出的方法支持不同类型的用户输入，例如第一帧中的分割掩码（半监督场景），或一组稀疏的点击点（交互式场景）。在半监督场景中，我们实现了与现有技术水平相竞争的结果，但计算成本仅为一部分（每帧275毫秒）。在用户可以迭代地细化其输入的交互式场景中，所提出的方法对每个输入提供即时响应，并以更少的交互达到与竞争方法相当的质量。","领域":"视频对象分割/像素级度量学习/交互式分割","问题":"视频对象分割","动机":"提高视频对象分割的速度和效率，同时支持不同类型的用户输入","方法":"使用完全卷积网络和修改的三元组损失进行像素级度量学习，采用最近邻方法对像素进行分类","关键词":["视频对象分割","像素级度量学习","交互式分割"],"涉及的技术概念":"完全卷积网络、三元组损失、像素级检索、最近邻方法、半监督学习、交互式分割"},{"order":123,"title":"Learning to Compare: Relation Network for Few-Shot Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sung_Learning_to_Compare_CVPR_2018_paper.html","abstract":"We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.","中文标题":"学习比较：用于少样本学习的关系网络","摘要翻译":"我们提出了一个概念上简单、灵活且通用的少样本学习框架，其中分类器必须学会仅从每个新类的少量示例中识别新类。我们的方法称为关系网络（RN），是从头开始端到端训练的。在元学习过程中，它学会学习一个深度距离度量，以比较每个情节中的少量图像，每个情节都设计为模拟少样本设置。一旦训练完成，RN能够通过计算查询图像与每个新类的少量示例之间的关系分数来对新类别的图像进行分类，而无需进一步更新网络。除了在少样本学习上提供改进的性能外，我们的框架还容易扩展到零样本学习。在五个基准上的大量实验表明，我们的简单方法为这两个任务提供了一个统一且有效的解决方案。","领域":"少样本学习/元学习/图像分类","问题":"如何在仅提供少量示例的情况下，使分类器能够识别新类别","动机":"解决少样本学习中的挑战，即分类器需要从每个新类别的少量示例中学习识别新类别","方法":"提出了关系网络（RN），一个端到端训练的框架，通过元学习学习深度距离度量来比较少量图像，并能够通过计算查询图像与新类别示例之间的关系分数来分类新类别的图像","关键词":["少样本学习","关系网络","元学习","图像分类","零样本学习"],"涉及的技术概念":"关系网络（RN）是一种端到端训练的框架，用于少样本学习。它通过元学习学习一个深度距离度量，以比较少量图像，并能够通过计算查询图像与新类别示例之间的关系分数来分类新类别的图像。此外，该框架还可以扩展到零样本学习。"},{"order":124,"title":"COCO-Stuff: Thing and Stuff Classes in Context","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Caesar_COCO-Stuff_Thing_and_CVPR_2018_paper.html","abstract":"Semantic classes can be either things (objects with a well-defined shape, e.g. car, person) or stuff (amorphous background regions, e.g. grass, sky). While lots of classification and detection works focus on thing classes, less attention has been given to stuff classes. Nonetheless, stuff classes are important as they allow to explain important aspects of an image, including (1) scene type; (2) which thing classes are likely to be present and their location (through contextual reasoning); (3) physical attributes, material types and geometric properties of the scene. To understand stuff and things in context we introduce COCO-Stuff, which augments all 164K images of the COCO 2017 dataset with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff annotation protocol based on superpixels, which leverages the original thing annotations. We quantify the speed versus quality trade-off of our protocol and explore the relation between annotation time and boundary complexity. Furthermore, we use COCO-Stuff to analyze: (a) the importance of stuff and thing classes in terms of their surface cover and how frequently they are mentioned in image captions; (b) the spatial relations between stuff and things, highlighting the rich contextual relations that make our dataset unique; (c) the performance of a modern semantic segmentation method on stuff and thing classes, and whether stuff is easier to segment than things.","中文标题":"COCO-Stuff: 上下文中的物体与背景类别","摘要翻译":"语义类别可以是物体（具有明确形状的对象，例如汽车、人）或背景（无定形的背景区域，例如草地、天空）。虽然许多分类和检测工作集中在物体类别上，但对背景类别的关注较少。然而，背景类别很重要，因为它们可以解释图像的重要方面，包括（1）场景类型；（2）可能存在的物体类别及其位置（通过上下文推理）；（3）场景的物理属性、材料类型和几何属性。为了理解上下文中的背景和物体，我们引入了COCO-Stuff，它为COCO 2017数据集的所有164K图像增加了91个背景类别的像素级注释。我们引入了一种基于超像素的高效背景注释协议，该协议利用了原始的物体注释。我们量化了协议的速度与质量之间的权衡，并探讨了注释时间与边界复杂性之间的关系。此外，我们使用COCO-Stuff来分析：（a）背景和物体类别在表面覆盖方面的重要性以及它们在图像描述中被提及的频率；（b）背景和物体之间的空间关系，突出了使我们的数据集独特的丰富上下文关系；（c）现代语义分割方法在背景和物体类别上的表现，以及背景是否比物体更容易分割。","领域":"语义分割/图像理解/场景分析","问题":"背景类别在图像理解中的重要性及其与物体类别的上下文关系","动机":"尽管背景类别在解释图像的重要方面（如场景类型、物体可能的位置和场景的物理属性）中起着关键作用，但相比物体类别，它们受到的关注较少。","方法":"引入COCO-Stuff数据集，为COCO 2017数据集的所有图像增加91个背景类别的像素级注释，并基于超像素开发了一种高效的背景注释协议。","关键词":["语义分割","图像理解","场景分析","背景类别","物体类别","上下文关系"],"涉及的技术概念":"COCO-Stuff是一个扩展的COCO 2017数据集，增加了91个背景类别的像素级注释。使用基于超像素的注释协议来提高注释效率，同时探讨了注释速度与质量之间的权衡。此外，分析了背景和物体类别在图像理解中的重要性，以及它们之间的空间关系，评估了现代语义分割方法在背景和物体类别上的表现。"},{"order":125,"title":"Image Generation From Scene Graphs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Johnson_Image_Generation_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Johnson_Image_Generation_From_CVPR_2018_paper.html","abstract":"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on gen- erating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and rela- tionships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explic- itly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, com- putes a scene layout by predicting bounding boxes and seg- mentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to en- sure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, abla- tions, and user studies demonstrate our method’s ability to generate complex images with multiple objects.","中文标题":"从场景图生成图像","摘要翻译":"为了真正理解视觉世界，我们的模型不仅应该能够识别图像，还应该能够生成它们。为此，最近在从自然语言描述生成图像方面取得了令人兴奋的进展。这些方法在有限的领域（如鸟类或花卉的描述）上给出了惊人的结果，但在忠实再现包含许多对象和关系的复杂句子时却遇到了困难。为了克服这一限制，我们提出了一种从场景图生成图像的方法，使得能够明确地推理对象及其关系。我们的模型使用图卷积处理输入图，通过预测对象的边界框和分割掩码来计算场景布局，并使用级联细化网络将布局转换为图像。网络通过对抗一对鉴别器进行训练，以确保输出逼真。我们在Visual Genome和COCO-Stuff上验证了我们的方法，定性结果、消融实验和用户研究证明了我们方法生成包含多个对象的复杂图像的能力。","领域":"图像生成/场景理解/图卷积网络","问题":"如何从包含多个对象和复杂关系的场景图中生成逼真的图像","动机":"现有的从自然语言描述生成图像的方法在处理复杂句子时存在困难，需要一种能够明确推理对象及其关系的方法来生成更复杂的图像","方法":"提出了一种从场景图生成图像的方法，使用图卷积处理输入图，预测对象的边界框和分割掩码来计算场景布局，并通过级联细化网络将布局转换为图像，同时使用对抗训练确保输出逼真","关键词":["图像生成","场景图","图卷积网络","级联细化网络","对抗训练"],"涉及的技术概念":{"场景图":"一种表示图像中对象及其关系的图形结构","图卷积":"一种处理图形结构数据的卷积方法，能够捕捉节点之间的关系","级联细化网络":"一种逐步细化图像的网络结构，用于提高生成图像的质量","对抗训练":"一种训练方法，通过引入鉴别器网络来区分生成图像和真实图像，从而提高生成图像的真实感"}},{"order":126,"title":"Deep Cauchy Hashing for Hamming Space Retrieval","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Deep_Cauchy_Hashing_CVPR_2018_paper.html","abstract":"Due to its computation efficiency and retrieval quality, hashing has been widely applied to approximate nearest neighbor search for large-scale image retrieval, while deep hashing further improves the retrieval quality by end-to-end representation learning and hash coding. With compact hash codes, Hamming space retrieval enables the most efficient constant-time search that returns data points within a given Hamming radius to each query, by hash table lookups instead of linear scan. However, subject to the weak capability of concentrating relevant images to be within a small Hamming ball due to mis-specified loss functions, existing deep hashing methods may underperform for Hamming space retrieval. This work presents Deep Cauchy Hashing (DCH), a novel deep hashing model that generates compact and concentrated binary hash codes to enable efficient and effective Hamming space retrieval. The main idea is to design a pairwise cross-entropy loss based on Cauchy distribution, which penalizes significantly on similar image pairs with Hamming distance larger than the given Hamming radius threshold. Comprehensive experiments demonstrate that DCH can generate highly concentrated hash codes and yield state-of-the-art Hamming space retrieval performance on three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.","中文标题":"深度柯西哈希用于汉明空间检索","摘要翻译":"由于其计算效率和检索质量，哈希已被广泛应用于大规模图像检索的近似最近邻搜索，而深度哈希通过端到端的表示学习和哈希编码进一步提高了检索质量。通过紧凑的哈希码，汉明空间检索实现了最高效的恒定时间搜索，通过哈希表查找而非线性扫描，返回每个查询给定汉明半径内的数据点。然而，由于损失函数的不当指定，现有深度哈希方法在汉明空间检索中可能表现不佳，因为它们将相关图像集中在一个小汉明球内的能力较弱。本文提出了深度柯西哈希（DCH），一种新颖的深度哈希模型，它生成紧凑且集中的二进制哈希码，以实现高效和有效的汉明空间检索。其主要思想是基于柯西分布设计一个成对交叉熵损失，该损失对汉明距离大于给定汉明半径阈值的相似图像对进行显著惩罚。综合实验表明，DCH可以生成高度集中的哈希码，并在NUS-WIDE、CIFAR-10和MS-COCO三个数据集上实现了最先进的汉明空间检索性能。","领域":"图像检索/哈希学习/近似最近邻搜索","问题":"现有深度哈希方法在汉明空间检索中表现不佳，因为它们将相关图像集中在一个小汉明球内的能力较弱。","动机":"提高汉明空间检索的效率和效果，通过生成紧凑且集中的二进制哈希码。","方法":"设计一个基于柯西分布的成对交叉熵损失，该损失对汉明距离大于给定汉明半径阈值的相似图像对进行显著惩罚。","关键词":["深度哈希","汉明空间检索","柯西分布","成对交叉熵损失"],"涉及的技术概念":"深度哈希是一种通过端到端的表示学习和哈希编码来提高图像检索质量的技术。汉明空间检索是一种通过哈希表查找而非线性扫描，返回每个查询给定汉明半径内的数据点的检索方法。柯西分布是一种概率分布，用于设计成对交叉熵损失，以惩罚汉明距离大于给定汉明半径阈值的相似图像对。"},{"order":127,"title":"Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jayaraman_Learning_to_Look_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jayaraman_Learning_to_Look_CVPR_2018_paper.html","abstract":"It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge.  We address the problem of learning to look around: if an agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative visual observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training.  As a result, 1) the learned \\"look around\\" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments.","中文标题":"学会环顾四周：智能探索未知环境以应对未知任务","摘要翻译":"通常，我们隐含地假设能够访问智能捕获的输入（例如，来自人类摄影师的照片），然而，自主捕获良好的观察本身就是一项重大挑战。我们解决了学会环顾四周的问题：如果一个代理有能力自愿获取新视角以观察其环境，它如何学习有效的探索行为以获取信息丰富的视觉观察？我们提出了一个强化学习解决方案，其中代理因减少其对环境未观察部分的不确定性的行动而获得奖励。基于这一原则，我们开发了一种基于循环神经网络的方法，以执行全景自然场景和3D物体形状的主动完成。关键的是，学习到的策略并不与任何识别任务或训练期间看到的特定语义内容绑定。因此，1）学习到的“环顾四周”行为即使对于新任务在未见过的环境中也是相关的，2）训练数据的获取不涉及手动标记。通过在多样化的设置中进行测试，我们证明了我们的方法学习到了有用的通用策略，这些策略可以转移到新的未见过的任务和环境中。","领域":"强化学习/环境探索/视觉观察","问题":"如何让代理学习有效的探索行为以获取信息丰富的视觉观察","动机":"自主捕获良好的观察是一项重大挑战，需要开发能够智能探索未知环境的方法","方法":"提出了一种基于循环神经网络的强化学习解决方案，代理因减少其对环境未观察部分的不确定性的行动而获得奖励","关键词":["强化学习","环境探索","视觉观察","循环神经网络","主动完成"],"涉及的技术概念":"强化学习是一种机器学习方法，其中代理通过与环境交互来学习策略，以最大化某种累积奖励。循环神经网络（RNN）是一种适用于处理序列数据的神经网络，能够捕捉时间序列数据中的时间依赖性。主动完成指的是代理通过选择性地获取新信息来减少对未知部分的不确定性。"},{"order":128,"title":"Multi-Scale Location-Aware Kernel Representation for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Scale_Location-Aware_Kernel_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Scale_Location-Aware_Kernel_CVPR_2018_paper.html","abstract":"Although Faster R-CNN and its variants have shown promising performance in object detection, they only exploit simple first order representation of object proposals for final classification and regression. Recent classification methods demonstrate that the integration of high order statistics into deep convolutional neural networks can achieve impressive improvement, but their goal is to model whole images by discarding location information so that they cannot be directly adopted to object detection. In this paper, we make an attempt to exploit high-order statistics in object detection, aiming at generating more discriminative representations for proposals to enhance the performance of detectors. To this end, we propose a novel Multi-scale Location-aware Kernel Representation (MLKP) to capture high-order statistics of deep features in proposals. Our MLKP can be efficiently computed on a modified multi-scale feature map using a low-dimensional polynomial kernel approximation. Moreover, different from existing orderless global representations based on high-order statistics, our proposed MLKP is location retentive and sensitive so that it can be flexibly adopted to object detection. Through integrating into Faster R-CNN schema, the proposed MLKP achieves very competitive performance with state-of-the-art methods, and improves Faster R-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0 (AP at IOU=[0.5:0.05:0.95]) on PASCAL VOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at: https://github.com/Hwang64/MLKP.","中文标题":"多尺度位置感知核表示用于目标检测","摘要翻译":"尽管Faster R-CNN及其变体在目标检测中展示了有希望的性能，但它们仅利用目标提议的简单一阶表示进行最终分类和回归。最近的分类方法表明，将高阶统计量集成到深度卷积神经网络中可以取得显著的改进，但它们的目标是通过丢弃位置信息来建模整个图像，因此不能直接用于目标检测。在本文中，我们尝试在目标检测中利用高阶统计量，旨在为目标提议生成更具区分性的表示，以增强检测器的性能。为此，我们提出了一种新颖的多尺度位置感知核表示（MLKP）来捕捉提议中深度特征的高阶统计量。我们的MLKP可以在修改后的多尺度特征图上使用低维多项式核近似高效计算。此外，与现有的基于高阶统计量的无序全局表示不同，我们提出的MLKP保留了位置信息并且对位置敏感，因此可以灵活地用于目标检测。通过集成到Faster R-CNN框架中，所提出的MLKP在PASCAL VOC 2007、VOC 2012和MS COCO基准测试上分别比Faster R-CNN提高了4.9%（mAP）、4.7%（mAP）和5.0%（AP at IOU=[0.5:0.05:0.95]），与最先进的方法相比具有非常竞争力的性能。代码可在https://github.com/Hwang64/MLKP获取。","领域":"目标检测/深度学习/卷积神经网络","问题":"目标检测中目标提议的表示不够区分性","动机":"利用高阶统计量增强目标检测器的性能","方法":"提出多尺度位置感知核表示（MLKP）来捕捉提议中深度特征的高阶统计量","关键词":["目标检测","高阶统计量","多尺度特征","位置感知","核表示"],"涉及的技术概念":"MLKP是一种新颖的多尺度位置感知核表示方法，旨在捕捉目标提议中深度特征的高阶统计量。它通过低维多项式核近似在多尺度特征图上高效计算，保留了位置信息并且对位置敏感，从而可以灵活地用于目标检测。"},{"order":129,"title":"Clinical Skin Lesion Diagnosis Using Representations Inspired by Dermatologist Criteria","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Clinical_Skin_Lesion_CVPR_2018_paper.html","abstract":"The skin is the largest organ in human body. Around 30%-70% of individuals worldwide have skin related health problems, for whom effective and efficient diagnosis is necessary. Recently, computer aided diagnosis (CAD) systems have been successfully applied to the recognition of skin cancers in dermatoscopic images. However, little work has concentrated on the commonly encountered skin diseases in clinical images captured by easily-accessed cameras or mobile phones. Meanwhile, for a CAD system, the representations of skin lesions are required to be understandable for dermatologists so that the predictions are convincing. To address this problem, we present effective representations inspired by the accepted dermatological criteria for diagnosing clinical skin lesions. We demonstrate that the dermatological criteria are highly correlated with measurable visual components. Accordingly, we design six medical representations considering different criteria for the recognition of skin lesions, and construct a diagnosis system for clinical skin disease images. Experimental results show that the proposed medical representations can not only capture the manifestations of skin lesions effectively, and consistently with the dermatological criteria, but also improve the prediction performance with respect to the state-of-the-art methods based on uninterpretable features.","中文标题":"使用受皮肤科医生标准启发的表示法进行临床皮肤病变诊断","摘要翻译":"皮肤是人体最大的器官。全球约30%-70%的人有与皮肤相关的健康问题，对他们来说，有效且高效的诊断是必要的。最近，计算机辅助诊断（CAD）系统已成功应用于皮肤镜图像中皮肤癌的识别。然而，很少有工作集中在通过易于访问的相机或手机拍摄的临床图像中常见的皮肤病上。同时，对于CAD系统来说，皮肤病变的表示需要让皮肤科医生能够理解，以便预测结果具有说服力。为了解决这个问题，我们提出了受公认的皮肤科诊断标准启发的有效表示法。我们证明了皮肤科标准与可测量的视觉成分高度相关。因此，我们设计了六种考虑不同标准的医学表示法用于皮肤病变的识别，并构建了一个用于临床皮肤病图像的诊断系统。实验结果表明，所提出的医学表示法不仅能有效捕捉皮肤病变的表现，与皮肤科标准一致，而且相对于基于不可解释特征的最先进方法，还能提高预测性能。","领域":"皮肤病学/医学影像分析/计算机辅助诊断","问题":"如何有效识别和诊断通过普通相机或手机拍摄的临床图像中的常见皮肤病","动机":"提高对常见皮肤病的诊断效率和准确性，使计算机辅助诊断系统的预测结果对皮肤科医生具有说服力","方法":"设计六种考虑不同皮肤科诊断标准的医学表示法，并构建一个诊断系统","关键词":["皮肤病变","医学影像分析","计算机辅助诊断"],"涉及的技术概念":"本文涉及的技术概念包括计算机辅助诊断（CAD）系统、皮肤科诊断标准、医学表示法设计、以及通过普通相机或手机拍摄的临床图像处理。"},{"order":130,"title":"Compare and Contrast: Learning Prominent Visual Differences","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Compare_and_Contrast_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Compare_and_Contrast_CVPR_2018_paper.html","abstract":"Relative attribute models can compare images in terms of all detected properties or attributes, exhaustively predicting which image is fancier, more natural, and so on without any regard to ordering. However, when humans compare images, certain differences will naturally stick out and come to mind first. These most noticeable differences, or prominent differences, are likely to be described first. In addition, many differences, although present, may not be mentioned at all. In this work, we introduce and model prominent differences, a rich new functionality for comparing images. We collect instance-level annotations of most noticeable differences, and build a model trained on relative attribute features that predicts prominent differences for unseen pairs. We test our model on the challenging UT-Zap50K shoes and LFW-10 faces datasets, and outperform an array of baseline methods. We then demonstrate how our prominence model improves two vision tasks, image search and description generation, enabling more natural communication between people and vision systems.","中文标题":"比较与对比：学习显著的视觉差异","摘要翻译":"相对属性模型可以根据所有检测到的属性或特征来比较图像，详尽地预测哪张图像更花哨、更自然等，而不考虑顺序。然而，当人类比较图像时，某些差异自然会突出并首先想到。这些最显著的差异，或称为显著差异，很可能首先被描述。此外，许多差异虽然存在，但可能根本不会被提及。在这项工作中，我们引入并建模了显著差异，这是一种用于比较图像的丰富新功能。我们收集了最显著差异的实例级注释，并构建了一个基于相对属性特征的模型，该模型可以预测未见过的图像对的显著差异。我们在具有挑战性的UT-Zap50K鞋类和LFW-10人脸数据集上测试了我们的模型，并超越了一系列基线方法。然后，我们展示了我们的显著模型如何改进两个视觉任务，图像搜索和描述生成，使人与视觉系统之间的交流更加自然。","领域":"图像比较/视觉差异分析/图像描述生成","问题":"如何有效地比较图像并突出显示最显著的视觉差异","动机":"人类在比较图像时，会自然地注意到并首先描述最显著的差异，而现有的相对属性模型无法模拟这一过程","方法":"收集实例级的最显著差异注释，构建基于相对属性特征的模型来预测图像对的显著差异","关键词":["显著差异","图像比较","相对属性","图像搜索","描述生成"],"涉及的技术概念":"相对属性模型是一种能够根据图像的属性或特征来比较图像的技术，而显著差异指的是在比较图像时，人类首先注意到的差异。实例级注释指的是对特定实例（如图像）的详细描述或标记。"},{"order":131,"title":"Multi-Evidence Filtering and Fusion for Multi-Label Classification, Object Detection and Semantic Segmentation Based on Weakly Supervised Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Multi-Evidence_Filtering_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ge_Multi-Evidence_Filtering_and_CVPR_2018_paper.html","abstract":"Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In this paper, we propose a novel weakly supervised curriculum learning pipeline for multi-label object recognition, detection and semantic segmentation. In this pipeline, we first obtain intermediate object localization and pixel labeling results for the training images, and then use such results to train task-specific deep networks in a fully supervised manner. The entire process consists of four stages, including object localization in the training images, filtering and fusing object instances, pixel labeling for the training images, and task-specific network training. To obtain clean object instances in the training images, we propose a novel algorithm for filtering, fusing and classifying object instances collected from multiple solution mechanisms. In this algorithm, we incorporate both metric learning and density-based clustering to filter detected object instances. Experiments show that our weakly supervised pipeline achieves state-of-the-art results in multi-label image classification as well as weakly supervised object detection and very competitive results in weakly supervised semantic segmentation on MS-COCO, PASCAL VOC 2007 and PASCAL VOC 2012.","中文标题":"基于弱监督学习的多证据过滤与融合用于多标签分类、目标检测和语义分割","摘要翻译":"监督式目标检测和语义分割需要对象甚至像素级别的标注。当仅存在图像级别标签时，弱监督算法实现准确预测具有挑战性。顶级弱监督算法实现的准确性仍显著低于其全监督对应物。在本文中，我们提出了一种新颖的弱监督课程学习流程，用于多标签对象识别、检测和语义分割。在此流程中，我们首先获得训练图像的中间对象定位和像素标注结果，然后使用这些结果以全监督方式训练任务特定的深度网络。整个过程包括四个阶段，包括训练图像中的对象定位、过滤和融合对象实例、训练图像的像素标注以及任务特定网络训练。为了在训练图像中获得干净的对象实例，我们提出了一种新颖的算法，用于过滤、融合和分类从多种解决方案机制收集的对象实例。在此算法中，我们结合了度量学习和基于密度的聚类来过滤检测到的对象实例。实验表明，我们的弱监督流程在多标签图像分类以及弱监督目标检测方面实现了最先进的结果，在MS-COCO、PASCAL VOC 2007和PASCAL VOC 2012上的弱监督语义分割方面也取得了非常有竞争力的结果。","领域":"目标检测/语义分割/多标签分类","问题":"在仅存在图像级别标签的情况下，如何提高弱监督算法在目标检测和语义分割中的准确性","动机":"解决弱监督算法在目标检测和语义分割中准确性低于全监督算法的问题","方法":"提出了一种弱监督课程学习流程，包括对象定位、过滤和融合对象实例、像素标注以及任务特定网络训练四个阶段，并采用度量学习和基于密度的聚类来过滤检测到的对象实例","关键词":["弱监督学习","目标检测","语义分割","多标签分类","度量学习","密度聚类"],"涉及的技术概念":"弱监督学习是一种在训练数据不完全标注的情况下进行学习的方法。目标检测是指在图像中识别出特定对象的位置和类别。语义分割是指对图像中的每个像素进行分类，以识别出图像中的不同对象或区域。多标签分类是指一个样本可以同时属于多个类别。度量学习是一种学习数据点之间距离或相似度的方法。密度聚类是一种基于数据点密度进行聚类的方法。"},{"order":132,"title":"HashGAN: Deep Learning to Hash With Pair Conditional Wasserstein GAN","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_HashGAN_Deep_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_HashGAN_Deep_Learning_CVPR_2018_paper.html","abstract":"Deep learning to hash improves image retrieval performance by end-to-end representation learning and hash coding from training data with pairwise similarity information. Subject to the scarcity of similarity information that is often expensive to collect for many application domains, existing deep learning to hash methods may overfit the training data and result in substantial loss of retrieval quality. This paper presents HashGAN, a novel architecture for deep learning to hash, which learns compact binary hash codes from both real images and diverse images synthesized by generative models. The main idea is to augment the training data with nearly real images synthesized from a new Pair Conditional Wasserstein GAN (PC-WGAN) conditioned on the pairwise similarity information. Extensive experiments demonstrate that HashGAN can generate high-quality binary hash codes and yield state-of-the-art image retrieval performance on three benchmarks, NUS-WIDE, CIFAR-10, and MS-COCO.","中文标题":"HashGAN: 使用配对条件Wasserstein GAN进行深度哈希学习","摘要翻译":"深度哈希学习通过端到端的表示学习和从具有成对相似性信息的训练数据中进行哈希编码，提高了图像检索的性能。由于在许多应用领域中收集相似性信息的成本较高，现有的深度哈希学习方法可能会过度拟合训练数据，导致检索质量大幅下降。本文提出了HashGAN，一种新颖的深度哈希学习架构，它从真实图像和由生成模型合成的多样化图像中学习紧凑的二进制哈希码。其主要思想是通过从新的配对条件Wasserstein GAN（PC-WGAN）合成的几乎真实的图像来增强训练数据，该GAN以成对相似性信息为条件。大量实验证明，HashGAN能够生成高质量的二进制哈希码，并在NUS-WIDE、CIFAR-10和MS-COCO三个基准测试中实现了最先进的图像检索性能。","领域":"图像检索/生成对抗网络/哈希学习","问题":"解决在相似性信息稀缺的情况下，深度哈希学习方法可能过度拟合训练数据，导致检索质量下降的问题","动机":"由于在许多应用领域中收集相似性信息的成本较高，需要一种能够有效利用有限相似性信息进行深度哈希学习的方法","方法":"提出HashGAN架构，通过配对条件Wasserstein GAN（PC-WGAN）合成的几乎真实的图像来增强训练数据，从而学习紧凑的二进制哈希码","关键词":["图像检索","生成对抗网络","哈希学习"],"涉及的技术概念":"深度哈希学习是一种通过端到端的表示学习和哈希编码来提高图像检索性能的技术。生成对抗网络（GAN）是一种通过生成器和判别器的对抗过程来生成数据的技术。配对条件Wasserstein GAN（PC-WGAN）是一种特定类型的GAN，它以成对相似性信息为条件来生成图像。"},{"order":133,"title":"Min-Entropy Latent Model for Weakly Supervised Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Min-Entropy_Latent_Model_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Min-Entropy_Latent_Model_CVPR_2018_paper.html","abstract":"Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy is used as a metric to measure the randomness of object localization during learning, as well as serving as a model to learn object locations. It aims to principally reduce the variance of positive instances and alleviate the ambiguity of detectors. MELM is deployed as two sub-models, which respectively discovers and localizes objects by minimizing the global and local entropy. MELM is unified with feature learning and optimized with a recurrent learning algorithm, which progressively transfers the weak supervision to object locations. Experiments demonstrate that MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, against the state-of-the-art approaches.","中文标题":"最小熵潜在模型用于弱监督目标检测","摘要翻译":"弱监督目标检测是一项具有挑战性的任务，当提供图像类别监督时，同时需要学习目标位置和目标检测器。弱监督与学习目标之间的不一致性为目标位置引入了随机性，并为检测器带来了模糊性。本文提出了一种用于弱监督目标检测的最小熵潜在模型（MELM）。最小熵被用作衡量学习过程中目标定位随机性的度量，同时也作为学习目标位置的模型。它旨在从根本上减少正实例的方差并减轻检测器的模糊性。MELM被部署为两个子模型，分别通过最小化全局和局部熵来发现和定位目标。MELM与特征学习统一，并通过循环学习算法进行优化，逐步将弱监督转移到目标位置。实验表明，与最先进的方法相比，MELM显著提高了弱监督检测、弱监督定位和图像分类的性能。","领域":"目标检测/弱监督学习/图像分类","问题":"弱监督目标检测中的目标位置随机性和检测器模糊性问题","动机":"解决弱监督与学习目标之间的不一致性，减少目标定位的随机性和检测器的模糊性","方法":"提出最小熵潜在模型（MELM），通过最小化全局和局部熵来发现和定位目标，与特征学习统一并通过循环学习算法优化","关键词":["弱监督目标检测","最小熵潜在模型","目标定位","特征学习","循环学习算法"],"涉及的技术概念":"最小熵潜在模型（MELM）是一种用于弱监督目标检测的模型，通过最小化全局和局部熵来发现和定位目标。MELM与特征学习统一，并通过循环学习算法进行优化，逐步将弱监督转移到目标位置。"},{"order":134,"title":"MAttNet: Modular Attention Network for Referring Expression Comprehension","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_MAttNet_Modular_Attention_CVPR_2018_paper.html","abstract":"In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit,  we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components.  Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-the-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks.","中文标题":"MAttNet：模块化注意力网络用于指代表达理解","摘要翻译":"在本文中，我们解决了指代表达理解的问题：定位由自然语言表达描述的图像区域。虽然最近的工作将表达视为一个单一单元，但我们提出将其分解为与主体外观、位置和与其他对象的关系相关的三个模块化组件。这使我们能够灵活地适应包含不同类型信息的表达，在一个端到端的框架中。在我们的模型中，我们称之为模块化注意力网络（MAttNet），利用了两种类型的注意力：基于语言的注意力，它学习模块权重以及每个模块应关注的单词/短语注意力；和视觉注意力，它允许主体和关系模块关注相关的图像组件。模块权重动态地结合所有三个模块的分数以输出总体分数。实验表明，MAttNet在边界框级别和像素级别的理解任务上均大幅优于之前的最先进方法。","领域":"指代表达理解/自然语言处理/图像理解","问题":"如何准确地定位由自然语言表达描述的图像区域","动机":"为了更灵活地处理包含不同类型信息的自然语言表达，并提高指代表达理解的准确性","方法":"提出了一种模块化注意力网络（MAttNet），通过分解表达为三个模块化组件，并利用基于语言的注意力和视觉注意力来动态结合模块分数","关键词":["指代表达理解","模块化注意力网络","自然语言处理","图像理解"],"涉及的技术概念":"模块化注意力网络（MAttNet）是一种用于指代表达理解的模型，它通过将自然语言表达分解为与主体外观、位置和与其他对象的关系相关的三个模块化组件，并利用基于语言的注意力和视觉注意力来动态结合模块分数，从而实现对图像区域的精确定位。"},{"order":135,"title":"AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_AttnGAN_Fine-Grained_Text_CVPR_2018_paper.html","abstract":"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation.  With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different sub-regions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.","中文标题":"AttnGAN：基于注意力生成对抗网络的细粒度文本到图像生成","摘要翻译":"本文提出了一种注意力生成对抗网络（AttnGAN），该网络允许通过注意力驱动、多阶段细化的方式进行细粒度的文本到图像生成。通过一种新颖的注意力生成网络，AttnGAN能够通过关注自然语言描述中的相关词汇，在图像的不同子区域合成细粒度的细节。此外，还提出了一种深度注意力多模态相似性模型，用于计算细粒度的图像-文本匹配损失以训练生成器。所提出的AttnGAN显著优于之前的最新技术，在CUB数据集上将最佳报告的初始分数提高了14.14%，在更具挑战性的COCO数据集上提高了170.25%。还通过可视化AttnGAN的注意力层进行了详细分析。这是首次展示分层注意力GAN能够自动选择单词级别的条件以生成图像的不同部分。","领域":"生成对抗网络/图像生成/自然语言处理","问题":"细粒度文本到图像生成","动机":"提高文本到图像生成的细粒度和质量，特别是在关注自然语言描述中的相关词汇以合成图像细节方面。","方法":"提出了一种注意力生成对抗网络（AttnGAN），包括一个新颖的注意力生成网络和一个深度注意力多模态相似性模型，用于计算细粒度的图像-文本匹配损失。","关键词":["生成对抗网络","图像生成","自然语言处理","注意力机制","多模态相似性"],"涉及的技术概念":{"注意力生成对抗网络（AttnGAN）":"一种允许通过注意力驱动、多阶段细化的方式进行细粒度的文本到图像生成的网络。","深度注意力多模态相似性模型":"用于计算细粒度的图像-文本匹配损失以训练生成器的模型。","初始分数（Inception Score）":"用于评估生成图像质量的一种指标。","CUB数据集":"一个用于细粒度图像分类和文本到图像生成的数据集。","COCO数据集":"一个包含复杂场景和对象的图像数据集，用于图像识别、分割和文本到图像生成等任务。"}},{"order":136,"title":"Adversarial Complementary Learning for Weakly Supervised Object Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Adversarial_Complementary_Learning_CVPR_2018_paper.html","abstract":"In this work, we propose Adversarial Complementary Learning (ACoL) to automatically localize integral objects of semantic interest with weak supervision. We first mathematically prove that class localization maps can be obtained by directly selecting the class-specific feature maps of the last convolutional layer, which paves a simple way to identify object regions. We then present a simple network architecture including two parallel-classifiers for object localization. Specifically, we leverage one classification branch to dynamically localize some discriminative object regions during the forward pass. Although it is usually responsive to sparse parts of the target objects, this classifier can drive the counterpart classifier to discover new and complementary object regions by erasing its discovered regions from the feature maps. With such an adversarial learning, the two parallel-classifiers are forced to leverage complementary object regions for classification and can finally generate integral object localization together. The merits of ACoL are mainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically erasing enables the counterpart classifier to discover complementary object regions more effectively. We demonstrate the superiority of our ACoL approach in a variety of experiments. In particular, the Top-1 localization error rate on the ILSVRC dataset is 45.14%, which is the new state-of-the-art.","中文标题":"对抗性互补学习用于弱监督对象定位","摘要翻译":"在本工作中，我们提出了对抗性互补学习（ACoL），以弱监督的方式自动定位语义兴趣的完整对象。我们首先数学上证明了通过直接选择最后一个卷积层的类特定特征图可以获得类定位图，这为识别对象区域提供了一种简单的方法。然后，我们提出了一个简单的网络架构，包括两个并行分类器用于对象定位。具体来说，我们利用一个分类分支在前向传播过程中动态定位一些具有区分性的对象区域。尽管这个分类器通常对目标对象的稀疏部分有响应，但它可以通过从特征图中擦除其发现的区域来驱动另一个分类器发现新的和互补的对象区域。通过这种对抗性学习，两个并行分类器被迫利用互补的对象区域进行分类，并最终可以一起生成完整的对象定位。ACoL的优点主要有两点：1）它可以以端到端的方式进行训练；2）动态擦除使另一个分类器能够更有效地发现互补的对象区域。我们在各种实验中展示了我们的ACoL方法的优越性。特别是在ILSVRC数据集上，Top-1定位错误率为45.14%，这是新的最先进水平。","领域":"对象定位/弱监督学习/对抗性学习","问题":"在弱监督条件下自动定位语义兴趣的完整对象","动机":"提高对象定位的准确性和效率，特别是在弱监督条件下","方法":"提出了一种对抗性互补学习（ACoL）方法，通过两个并行分类器动态定位和擦除对象区域，以发现互补的对象区域","关键词":["对象定位","弱监督学习","对抗性学习","卷积神经网络","特征图"],"涉及的技术概念":"对抗性互补学习（ACoL）是一种结合了对抗性学习和互补学习的方法，用于在弱监督条件下提高对象定位的准确性。通过两个并行分类器，一个分类器在前向传播过程中动态定位对象区域，而另一个分类器则通过擦除已发现区域来发现新的和互补的对象区域。这种方法利用了卷积神经网络的最后一个卷积层的类特定特征图来生成类定位图，从而实现对象区域的识别。"},{"order":137,"title":"Conditional Generative Adversarial Network for Structured Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.html","abstract":"In recent years, deep neural nets have triumphed over many computer vision problems, including semantic segmentation, which is a critical task in emerging autonomous driving and medical image diagnostics applications. In general, training deep neural nets requires a humongous amount of labeled data, which is laborious and costly to collect and annotate. Recent advances in computer graphics shed light on utilizing photo-realistic synthetic data with computer generated annotations to train neural nets. Nevertheless, the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e., integrating GAN into the FCN framework to mitigate the gap between source and target domains. Specifically, we learn a conditional generator to transform features of synthetic images to real-image like features, and a discriminator to distinguish them. For each training batch, the conditional generator and the discriminator compete against each other so that the generator learns to produce real-image like features to fool the discriminator; afterwards, the FCN parameters are updated to accommodate the changes of GAN. In experiments, without using labels of real image data, our method significantly outperforms the baselines as well as state-of-the-art methods by 12% ∼ 20% mean IoU on the Cityscapes dataset.","中文标题":"条件生成对抗网络用于结构化领域适应","摘要翻译":"近年来，深度神经网络在许多计算机视觉问题上取得了胜利，包括语义分割，这是新兴的自动驾驶和医学图像诊断应用中的关键任务。通常，训练深度神经网络需要大量的标注数据，这些数据的收集和标注既费力又昂贵。计算机图形学的最新进展揭示了利用具有计算机生成注释的逼真合成数据来训练神经网络的可能性。然而，真实图像和合成图像之间的领域不匹配是利用生成的数据和标签的主要挑战。在本文中，我们提出了一种原则性的方法来进行语义分割的结构化领域适应，即将GAN集成到FCN框架中以减轻源域和目标域之间的差距。具体来说，我们学习一个条件生成器将合成图像的特征转换为类似真实图像的特征，以及一个鉴别器来区分它们。对于每个训练批次，条件生成器和鉴别器相互竞争，使得生成器学会生成类似真实图像的特征以欺骗鉴别器；之后，FCN参数被更新以适应GAN的变化。在实验中，不使用真实图像数据的标签，我们的方法在Cityscapes数据集上的平均IoU显著优于基线方法和最先进的方法，提高了12%到20%。","领域":"语义分割/领域适应/生成对抗网络","问题":"解决真实图像和合成图像之间的领域不匹配问题，以利用合成数据训练语义分割模型","动机":"减少对大量标注数据的依赖，利用合成数据训练深度神经网络","方法":"将条件生成对抗网络（GAN）集成到全卷积网络（FCN）框架中，通过条件生成器将合成图像的特征转换为类似真实图像的特征，并通过鉴别器进行区分，以此减轻源域和目标域之间的差距","关键词":["语义分割","领域适应","生成对抗网络","全卷积网络"],"涉及的技术概念":"条件生成对抗网络（GAN）用于生成类似真实图像的特征，全卷积网络（FCN）用于语义分割，通过GAN和FCN的结合实现结构化领域适应，以减轻源域和目标域之间的差距。"},{"order":138,"title":"GroupCap: Group-Based Image Captioning With Structured Relevance and Diversity Constraints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_GroupCap_Group-Based_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_GroupCap_Group-Based_Image_CVPR_2018_paper.html","abstract":"Most image captioning models focus on one-line (single image) captioning, where the correlations like relevance and diversity among group images (e.g., within the same album or event) are simply neglected, resulting in less accurate and diverse captions. Recent works mainly consider imposing the diversity during the online inference only, which neglect the correlation among visual structures in offline training. In this paper, we propose a novel group-based image captioning scheme (termed GroupCap), which jointly models the structured relevance and diversity among visual contents of group images towards an optimal collaborative captioning. In particular, we first propose a visual tree parser (VP-Tree) to construct the structured semantic correlations within individual images. Then, the relevance and diversity among images are well modeled by exploiting the correlations among their tree structures. Finally, such correlations are modeled as constraints and sent into the LSTM-based captioning generator. In offline optimization, we adopt an end-to-end formulation, which jointly trains the visual tree parser, the structured relevance and diversity constraints, as well as the LSTM based captioning model. To facilitate quantitative evaluation, we further release two group captioning datasets derived from the MS-COCO benchmark, serving as the first of their kind. Quantitative results show that the proposed GroupCap model outperforms the state-of-the-art and alternative approaches, which can generate much more accurate and discriminative captions under various evaluation metrics.","中文标题":"GroupCap：基于群组的图像字幕生成与结构化相关性和多样性约束","摘要翻译":"大多数图像字幕生成模型专注于单行（单张图像）字幕生成，其中群组图像（例如，同一相册或事件内）之间的相关性和多样性等关联性被简单地忽略，导致字幕的准确性和多样性较低。最近的工作主要考虑在在线推理期间施加多样性，这忽略了离线训练中视觉结构之间的相关性。在本文中，我们提出了一种新颖的基于群组的图像字幕生成方案（称为GroupCap），它联合建模了群组图像视觉内容之间的结构化相关性和多样性，以实现最优的协作字幕生成。特别是，我们首先提出了一个视觉树解析器（VP-Tree）来构建单个图像内的结构化语义相关性。然后，通过利用它们树结构之间的相关性，很好地建模了图像之间的相关性和多样性。最后，这些相关性被建模为约束并发送到基于LSTM的字幕生成器中。在离线优化中，我们采用端到端的公式，联合训练视觉树解析器、结构化相关性和多样性约束，以及基于LSTM的字幕模型。为了促进定量评估，我们进一步发布了两个源自MS-COCO基准的群组字幕数据集，作为此类数据集的首创。定量结果表明，所提出的GroupCap模型在各种评估指标下均优于最先进的和替代的方法，能够生成更准确和更具区分性的字幕。","领域":"图像字幕生成/视觉内容分析/结构化语义建模","问题":"解决群组图像字幕生成中相关性和多样性建模不足的问题","动机":"提高群组图像字幕的准确性和多样性，通过建模视觉内容之间的结构化相关性和多样性","方法":"提出GroupCap方案，包括视觉树解析器（VP-Tree）构建单个图像的结构化语义相关性，利用树结构之间的相关性建模图像间的相关性和多样性，并将这些相关性作为约束输入到基于LSTM的字幕生成器中，采用端到端的训练方法","关键词":["图像字幕生成","视觉树解析器","结构化相关性","多样性约束","LSTM"],"涉及的技术概念":"视觉树解析器（VP-Tree）用于构建图像内的结构化语义相关性；基于LSTM的字幕生成器用于生成字幕；端到端训练方法用于联合训练视觉树解析器、结构化相关性和多样性约束以及字幕模型。"},{"order":139,"title":"Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.html","abstract":"Weakly-supervised semantic segmentation under image tags supervision is a challenging task as it directly associates high-level semantic to low-level appearance. To bridge this gap, in this paper, we propose an iterative bottom-up and top-down framework which alternatively expands object regions and optimizes segmentation network. We start from initial localization produced by classification networks. While classification networks are only responsive to small and coarse discriminative object regions, we argue that, these regions contain significant common features about objects. So in the bottom-up step, we mine common object features from the initial localization and expand object regions with the mined features. To supplement non-discriminative regions, saliency maps are then considered under Bayesian framework to refine the object regions. Then in the top-down step, the refined object regions are used as supervision to train the segmentation network and to predict object masks. These object masks provide more accurate localization and contain more regions of object. Further, we take these object masks as initial localization and mine common object features from them. These processes are conducted iteratively to progressively produce fine object masks and optimize segmentation networks. Experimental results on Pascal VOC 2012 dataset demonstrate that the proposed method outperforms previous state-of-the-art methods by a large margin.","中文标题":"通过迭代挖掘共同对象特征的弱监督语义分割","摘要翻译":"在图像标签监督下的弱监督语义分割是一项具有挑战性的任务，因为它直接将高级语义与低级外观关联起来。为了弥合这一差距，本文提出了一种迭代的自下而上和自上而下的框架，该框架交替扩展对象区域并优化分割网络。我们从分类网络产生的初始定位开始。虽然分类网络仅对小而粗糙的区分性对象区域有响应，但我们认为这些区域包含关于对象的重要共同特征。因此，在自下而上的步骤中，我们从初始定位中挖掘共同对象特征，并用挖掘到的特征扩展对象区域。为了补充非区分性区域，随后在贝叶斯框架下考虑显著性图以细化对象区域。然后，在自上而下的步骤中，使用细化的对象区域作为监督来训练分割网络并预测对象掩码。这些对象掩码提供了更准确的定位并包含更多的对象区域。此外，我们将这些对象掩码作为初始定位，并从中挖掘共同对象特征。这些过程迭代进行，以逐步生成精细的对象掩码并优化分割网络。在Pascal VOC 2012数据集上的实验结果表明，所提出的方法大大优于之前的最先进方法。","领域":"语义分割/弱监督学习/图像分析","问题":"在图像标签监督下进行弱监督语义分割，直接关联高级语义与低级外观的挑战","动机":"为了弥合高级语义与低级外观之间的差距，提出一种新的框架以提高分割的准确性","方法":"提出一种迭代的自下而上和自上而下的框架，通过挖掘共同对象特征和优化分割网络来扩展和细化对象区域","关键词":["语义分割","弱监督学习","图像分析","共同对象特征","贝叶斯框架","显著性图"],"涉及的技术概念":{"弱监督语义分割":"一种在有限监督信息（如图像标签）下进行的语义分割方法","自下而上和自上而下框架":"一种结合从局部到全局（自下而上）和从全局到局部（自上而下）信息处理策略的框架","共同对象特征":"指在图像中不同对象间共享的特征，有助于识别和分割对象","贝叶斯框架":"一种基于贝叶斯定理的统计框架，用于处理不确定性和进行推断","显著性图":"一种表示图像中每个像素或区域重要性的图，用于辅助图像分析和理解"}},{"order":140,"title":"Bootstrapping the Performance of Webly Supervised Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Bootstrapping_the_Performance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Bootstrapping_the_Performance_CVPR_2018_paper.html","abstract":"Fully supervised methods for semantic segmentation require pixel-level class masks to train, the creation of which are expensive in terms of manual labour and time. In this work, we focus on weak supervision, developing a method for training a high-quality pixel-level classifier for semantic segmentation, using only image-level class labels as the provided ground-truth. Our method is formulated as a two-stage approach in which we first aim to create accurate pixel-level masks for the training images via a bootstrapping process, and then use these now-accurately segmented images as a proxy ground-truth in a more standard supervised setting. The key driver for our work is that in the target dataset we typically have reliable ground-truth image-level labels, while data crawled from the web may have unreliable labels, but can be filtered to comprise only easy images to segment, therefore having reliable boundaries. These two forms of information are complementary and we use this observation to build a novel bi-directional transfer learning. This framework transfers knowledge between two domains, target domain and web domain, bootstrapping the performance of weakly supervised semantic segmentation. Conducting experiments on the popular benchmark dataset PASCAL VOC 2012 based on both a VGG16 network and on ResNet50, we reach state-of-the-art performance with scores of 60.2% IoU and 63.9% IoU respectively.","中文标题":"提升网络监督语义分割性能的自举方法","摘要翻译":"完全监督的语义分割方法需要像素级的类别掩码进行训练，这些掩码的创建在人工和时间方面都非常昂贵。在这项工作中，我们专注于弱监督，开发了一种仅使用图像级类别标签作为提供的地面实况来训练高质量像素级分类器的方法。我们的方法被制定为一个两阶段方法，其中我们首先旨在通过自举过程为训练图像创建准确的像素级掩码，然后使用这些现在准确分割的图像作为更标准监督设置中的代理地面实况。我们工作的关键驱动因素是，在目标数据集中，我们通常有可靠的地面实况图像级标签，而从网络抓取的数据可能具有不可靠的标签，但可以过滤以仅包含易于分割的图像，因此具有可靠的边界。这两种形式的信息是互补的，我们利用这一观察结果构建了一种新颖的双向迁移学习。该框架在两个领域之间转移知识，目标领域和网络领域，自举弱监督语义分割的性能。在基于VGG16网络和ResNet50的流行基准数据集PASCAL VOC 2012上进行实验，我们分别达到了60.2% IoU和63.9% IoU的最先进性能。","领域":"语义分割/弱监督学习/迁移学习","问题":"如何在不依赖昂贵的像素级标注的情况下，训练出高质量的语义分割模型","动机":"减少语义分割模型训练过程中对昂贵像素级标注的依赖，利用图像级标签和网络数据提升模型性能","方法":"提出了一种两阶段的自举方法，首先通过自举过程为训练图像创建准确的像素级掩码，然后使用这些掩码作为代理地面实况进行标准监督训练，并构建了一种双向迁移学习框架来提升弱监督语义分割的性能","关键词":["语义分割","弱监督学习","迁移学习","自举方法","双向迁移学习"],"涉及的技术概念":"自举过程指的是通过迭代改进的方法逐步提升模型性能；双向迁移学习指的是在两个不同但相关的领域之间转移知识，以提升模型在目标领域的性能；IoU（交并比）是评估语义分割模型性能的常用指标，表示预测分割区域与真实分割区域的重叠程度。"},{"order":141,"title":"DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection Under Partial Occlusion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_DeepVoting_A_Robust_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_DeepVoting_A_Robust_CVPR_2018_paper.html","abstract":"In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner.  In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network,  e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues.","中文标题":"DeepVoting: 一种用于部分遮挡下语义部分检测的鲁棒且可解释的深度网络","摘要翻译":"在本文中，我们研究了在部分遮挡下检测物体语义部分（例如，汽车的轮子）的任务。我们提出，所有模型都应该在没有看到遮挡的情况下进行训练，同时能够将学到的知识转移到处理遮挡上。这种设置减轻了收集覆盖遮挡模式的指数级大数据集的难度，并且更为本质。在这种情况下，基于提议的深度网络，如RCNN系列，往往产生不满意的结果，因为提议提取和分类阶段都可能被不相关的遮挡物混淆。为了解决这个问题，[25]提出了一种投票机制，该机制结合了多个局部视觉线索来检测语义部分。即使由于遮挡而缺少一些视觉线索，语义部分仍然可以被检测到。然而，这种方法是由人工设计的，因此难以以端到端的方式进行优化。在本文中，我们提出了DeepVoting，它将[25]展示的鲁棒性融入到深度网络中，从而使整个流程可以联合优化。具体来说，它在深度网络的中间特征（例如，VGGNet的pool-4层）之后添加了两层。第一层提取局部视觉线索的证据，第二层通过利用视觉线索和语义部分之间的空间关系执行投票机制。我们还提出了一个改进版本DeepVoting+，通过学习对象外部的上下文来学习视觉线索。在实验中，DeepVoting在遮挡下的语义部分检测中显著优于包括Faster-RCNN在内的几种基线方法。此外，DeepVoting具有可解释性，因为检测结果可以通过查看投票线索进行诊断。","领域":"语义部分检测/遮挡处理/深度网络优化","问题":"在部分遮挡下检测物体的语义部分","动机":"减轻收集覆盖遮挡模式的指数级大数据集的难度，并提高检测的鲁棒性和可解释性","方法":"提出DeepVoting，一种将鲁棒性融入深度网络的方法，通过添加两层来提取局部视觉线索的证据并执行投票机制，以及提出改进版本DeepVoting+通过学习对象外部的上下文来学习视觉线索","关键词":["语义部分检测","遮挡处理","深度网络优化","投票机制","局部视觉线索"],"涉及的技术概念":"DeepVoting是一种深度网络，通过在网络的中间特征后添加两层来提取局部视觉线索的证据并执行投票机制，以提高在部分遮挡下检测物体语义部分的鲁棒性和可解释性。DeepVoting+是DeepVoting的改进版本，通过学习对象外部的上下文来学习视觉线索。"},{"order":142,"title":"Geometry-Aware Scene Text Detection With Instance Transformation Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Geometry-Aware_Scene_Text_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Geometry-Aware_Scene_Text_CVPR_2018_paper.html","abstract":"Localizing text in the wild is challenging in the situations of complicated geometric layout of the targets like random orientation and large aspect ratio. In this paper, we propose a geometry-aware modeling approach tailored for scene text representation with an end-to-end learning scheme. In our approach, a novel Instance Transformation Network (ITN) is presented to learn the geometry-aware representation encoding the unique geometric configurations of scene text instances with in-network transformation embedding, resulting in a robust and elegant framework to detect words or text lines at one pass. An end-to-end multi-task learning strategy with transformation regression, text/non-text classification and coordinate regression is adopted in the ITN. Experiments on the benchmark datasets demonstrate the effectiveness of the proposed approach in detecting scene text in various geometric configurations.","中文标题":"几何感知的场景文本检测与实例变换网络","摘要翻译":"在目标具有复杂几何布局（如随机方向和大纵横比）的情况下，定位野外文本具有挑战性。本文提出了一种几何感知建模方法，专门用于场景文本表示，采用端到端学习方案。在我们的方法中，提出了一种新颖的实例变换网络（ITN），通过学习编码场景文本实例独特几何配置的几何感知表示，并在网络内嵌入变换，从而形成一个强大而优雅的框架，以一次性检测单词或文本行。ITN采用了端到端的多任务学习策略，包括变换回归、文本/非文本分类和坐标回归。在基准数据集上的实验证明了所提出方法在检测各种几何配置的场景文本方面的有效性。","领域":"场景文本检测/几何建模/端到端学习","问题":"在复杂几何布局下定位野外文本","动机":"解决在目标具有复杂几何布局（如随机方向和大纵横比）的情况下，定位野外文本的挑战","方法":"提出了一种几何感知建模方法，采用端到端学习方案，并引入实例变换网络（ITN）来学习编码场景文本实例独特几何配置的几何感知表示","关键词":["场景文本检测","几何建模","端到端学习","实例变换网络"],"涉及的技术概念":{"几何感知建模":"一种专门用于场景文本表示的方法，考虑文本的几何布局","实例变换网络（ITN）":"一种新颖的网络结构，用于学习场景文本实例的几何感知表示","端到端学习":"一种学习策略，直接从输入到输出进行学习，无需手动设计特征","多任务学习":"一种学习策略，同时学习多个相关任务以提高模型的泛化能力"}},{"order":143,"title":"Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Optical_Flow_Guided_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html","abstract":"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatio-temporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0% and 74.2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https://github.com/kevin-ssy/Optical-Flow-Guided-Feature","中文标题":"光流引导特征：一种快速且鲁棒的视频动作识别运动表示","摘要翻译":"运动表示在视频中的人类动作识别中扮演着至关重要的角色。在本研究中，我们引入了一种新颖的紧凑运动表示，名为光流引导特征（OFF），它使网络能够通过一种快速且鲁棒的方法提取时间信息。OFF源自光流的定义，并且与光流正交。该推导还为使用两帧之间的差异提供了理论支持。通过直接计算深度特征图的像素级时空梯度，OFF可以嵌入到任何现有的基于CNN的视频动作识别框架中，只需轻微增加成本。它使CNN能够提取时空信息，特别是帧间的时间信息。这一简单但强大的想法通过实验结果得到了验证。仅通过RGB输入提供OFF的网络在UCF-101上达到了93.3%的竞争性准确率，这与通过双流（RGB和光流）获得的结果相当，但速度提高了15倍。实验结果还表明，OFF与其他运动模式（如光流）是互补的。当所提出的方法插入到最先进的视频动作识别框架中时，它在UCF-101和HMDB-51上的准确率分别为96.0%和74.2%。本项目的代码可在以下网址获取：https://github.com/kevin-ssy/Optical-Flow-Guided-Feature","领域":"视频动作识别/光流分析/时空信息提取","问题":"如何快速且鲁棒地提取视频中的运动信息以进行动作识别","动机":"为了提高视频动作识别的效率和准确性，需要一种能够快速提取时间信息的方法","方法":"引入光流引导特征（OFF），通过直接计算深度特征图的像素级时空梯度来提取时空信息","关键词":["视频动作识别","光流引导特征","时空信息提取"],"涉及的技术概念":"光流引导特征（OFF）是一种基于光流定义的运动表示方法，它通过计算深度特征图的像素级时空梯度来提取视频中的时空信息，特别是帧间的时间信息。这种方法可以嵌入到现有的基于CNN的视频动作识别框架中，以较低的成本提高动作识别的准确率和速度。"},{"order":144,"title":"Motion-Guided Cascaded Refinement Network for Video Object Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Motion-Guided_Cascaded_Refinement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Motion-Guided_Cascaded_Refinement_CVPR_2018_paper.html","abstract":"Deep CNNs have achieved superior performance in many tasks of computer vision and image understanding. However, it is still difficult to effectively apply deep CNNs to video object segmentation(VOS) since treating video frames as separate and static will lose the information hidden in motion. To tackle this problem, we propose a Motion-guided Cascaded Refinement Network for VOS. By assuming the object motion is normally different from the background motion, for a video frame we first apply an active contour model on optical flow to coarsely segment objects of interest. Then, the proposed Cascaded Refinement Network(CRN) takes the coarse segmentation as guidance to generate an accurate segmentation of full resolution. In this way, the motion information and the deep CNNs can well complement each other to accurately segment objects from video frames. Furthermore, in CRN we introduce a Single-channel Residual Attention Module to incorporate the coarse segmentation map as attention, making our network effective and efficient in both training and testing. We perform experiments on the popular benchmarks and the results show that our method achieves state-of-the-art performance at a much faster speed.","中文标题":"运动引导的级联优化网络用于视频对象分割","摘要翻译":"深度卷积神经网络（CNNs）在计算机视觉和图像理解的许多任务中已经取得了卓越的性能。然而，由于将视频帧视为独立和静态的会丢失隐藏在运动中的信息，因此将深度CNNs有效应用于视频对象分割（VOS）仍然具有挑战性。为了解决这个问题，我们提出了一种用于VOS的运动引导级联优化网络。通过假设对象运动通常与背景运动不同，我们首先在光流上应用活动轮廓模型来粗略分割感兴趣的对象。然后，提出的级联优化网络（CRN）以粗略分割为指导，生成全分辨率的精确分割。这样，运动信息和深度CNNs可以很好地互补，以准确地从视频帧中分割对象。此外，在CRN中，我们引入了一个单通道残差注意力模块，将粗略分割图作为注意力纳入，使我们的网络在训练和测试中都有效且高效。我们在流行的基准上进行了实验，结果表明我们的方法以更快的速度实现了最先进的性能。","领域":"视频对象分割/光流分析/注意力机制","问题":"如何有效利用视频中的运动信息进行对象分割","动机":"由于将视频帧视为独立和静态的会丢失隐藏在运动中的信息，因此需要一种方法来有效利用这些信息以提高视频对象分割的准确性。","方法":"提出了一种运动引导的级联优化网络，首先在光流上应用活动轮廓模型来粗略分割感兴趣的对象，然后使用级联优化网络以粗略分割为指导生成精确分割，并引入单通道残差注意力模块来提高网络的效率和效果。","关键词":["视频对象分割","光流分析","注意力机制","级联优化网络","活动轮廓模型"],"涉及的技术概念":"深度卷积神经网络（CNNs）、视频对象分割（VOS）、光流、活动轮廓模型、级联优化网络（CRN）、单通道残差注意力模块。"},{"order":145,"title":"A Memory Network Approach for Story-Based Temporal Summarization of 360° Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_A_Memory_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_A_Memory_Network_CVPR_2018_paper.html","abstract":"We address the problem of story-based temporal summarization of long 360° videos. We propose a novel memory network model named Past-Future Memory Network (PFMN), in which we first compute the scores of 81 normal field of view (NFOV) region proposals cropped from the input 360° video, and then recover a latent, collective summary using the network with two external memories that store the embeddings of previously selected subshots and future candidate subshots. Our major contributions are two-fold. First, our work is the first to address story-based temporal summarization of 360° videos. Second, our model is the first attempt to leverage memory networks for video summarization tasks. For evaluation, we perform three sets of experiments. First, we investigate the view selection capability of our model on the Pano2Vid dataset. Second, we evaluate the temporal summarization with a newly collected 360° video dataset. Finally, we experiment our model's performance in another domain, with image-based storytelling VIST dataset. We verify that our model achieves state-of-the-art performance on all the tasks.","中文标题":"基于记忆网络的故事性时间摘要方法用于360°视频","摘要翻译":"我们解决了长360°视频的故事性时间摘要问题。我们提出了一种新颖的记忆网络模型，名为过去-未来记忆网络（PFMN），在该模型中，我们首先计算从输入的360°视频中裁剪出的81个正常视野（NFOV）区域提议的分数，然后使用带有两个外部记忆的网络恢复一个潜在的、集体的摘要，这两个外部记忆存储了先前选择的子镜头和未来候选子镜头的嵌入。我们的主要贡献有两个方面。首先，我们的工作是第一个解决360°视频的故事性时间摘要问题的。其次，我们的模型是首次尝试利用记忆网络进行视频摘要任务。为了评估，我们进行了三组实验。首先，我们在Pano2Vid数据集上调查了我们模型的视图选择能力。其次，我们使用新收集的360°视频数据集评估了时间摘要。最后，我们在另一个领域，即基于图像的故事讲述VIST数据集上实验了我们模型的性能。我们验证了我们的模型在所有任务上都达到了最先进的性能。","领域":"视频摘要/360°视频处理/记忆网络","问题":"长360°视频的故事性时间摘要","动机":"解决360°视频的故事性时间摘要问题，并首次尝试利用记忆网络进行视频摘要任务","方法":"提出过去-未来记忆网络（PFMN），通过计算正常视野（NFOV）区域提议的分数，并使用带有两个外部记忆的网络恢复潜在的、集体的摘要","关键词":["视频摘要","360°视频","记忆网络"],"涉及的技术概念":"正常视野（NFOV）区域提议、过去-未来记忆网络（PFMN）、外部记忆、子镜头嵌入"},{"order":146,"title":"Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Cube_Padding_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Cube_Padding_for_CVPR_2018_paper.html","abstract":"Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) unsupervisedly trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, PC introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms all baseline methods in both speed and quality.","中文标题":"立方体填充用于360°视频中的弱监督显著性预测","摘要翻译":"在360°视频中自动显著性预测对于视点引导应用（例如，Facebook 360指南）至关重要。我们提出了一种时空网络，该网络（1）是无监督训练的，（2）专为360°视球量身定制。请注意，大多数现有方法的可扩展性较差，因为它们依赖于注释的显著性图进行训练。最重要的是，它们将360°球体转换为2D图像（例如，单个等距圆柱投影图像或多个单独的普通视场（NFoV）图像），这引入了失真和图像边界。相比之下，我们提出了一种简单而有效的立方体填充（CP）技术，如下所述。首先，我们使用透视投影将360°视图渲染到立方体的六个面上。因此，它引入了非常少的失真。然后，我们连接所有六个面，同时利用立方体上面之间的连接性进行卷积、池化、卷积LSTM层中的图像填充（即立方体填充）。通过这种方式，PC在适用于几乎所有卷积神经网络（CNN）结构的同时，不引入图像边界。为了评估我们的方法，我们提出了Wild-360，一个新的360°视频显著性数据集，包含具有显著性热图注释的挑战性视频。在实验中，我们的方法在速度和质量上都优于所有基线方法。","领域":"显著性预测/360°视频处理/无监督学习","问题":"解决360°视频中自动显著性预测的问题，特别是在视点引导应用中。","动机":"现有方法依赖于注释的显著性图进行训练，且将360°球体转换为2D图像时引入失真和图像边界，限制了方法的可扩展性和效果。","方法":"提出了一种时空网络，采用立方体填充（CP）技术，通过透视投影将360°视图渲染到立方体的六个面上，并利用立方体上面之间的连接性进行图像填充，以减少失真并避免图像边界问题。","关键词":["显著性预测","360°视频","无监督学习","立方体填充","时空网络"],"涉及的技术概念":{"显著性预测":"指在视频或图像中自动识别出人类视觉系统最感兴趣的区域。","360°视频":"指能够提供全方位视角的视频，观看者可以自由选择观看方向。","无监督学习":"一种机器学习方法，模型在没有标签的数据上进行训练。","立方体填充":"一种图像处理技术，通过在立方体的六个面上渲染360°视图，并利用面之间的连接性进行图像填充，以减少失真。","时空网络":"一种能够同时处理空间和时间信息的神经网络结构，适用于视频分析等任务。"}},{"order":147,"title":"Appearance-and-Relation Networks for Video Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Appearance-and-Relation_Networks_for_CVPR_2018_paper.html","abstract":"Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods.","中文标题":"用于视频分类的外观与关系网络","摘要翻译":"视频中的时空特征学习是计算机视觉中的一个基本问题。本文提出了一种新的架构，称为外观与关系网络（ARTNet），以端到端的方式学习视频表示。ARTNets通过堆叠多个通用构建块（称为SMART）来构建，其目标是以分离和明确的方式同时从RGB输入中建模外观和关系。具体来说，SMART块将时空学习模块解耦为用于空间建模的外观分支和用于时间建模的关系分支。外观分支基于每帧中像素或滤波器响应的线性组合实现，而关系分支则基于跨多帧的像素或滤波器响应之间的乘法交互设计。我们在三个动作识别基准上进行了实验：Kinetics、UCF101和HMDB51，证明SMART块在时空特征学习方面比3D卷积有明显改进。在相同的训练设置下，ARTNets在这三个数据集上实现了优于现有最先进方法的性能。","领域":"动作识别/视频理解/时空特征学习","问题":"视频中的时空特征学习","动机":"为了更有效地从视频中学习时空特征，提出了一种新的架构来同时建模视频的外观和关系。","方法":"提出了一种称为ARTNet的新架构，通过堆叠SMART块来构建，SMART块将时空学习模块解耦为外观分支和关系分支，分别用于空间建模和时间建模。","关键词":["动作识别","视频理解","时空特征学习"],"涉及的技术概念":"ARTNet是一种新的视频表示学习架构，通过SMART块同时建模视频的外观和关系。SMART块将时空学习模块解耦为外观分支和关系分支，外观分支基于每帧中像素或滤波器响应的线性组合实现，关系分支基于跨多帧的像素或滤波器响应之间的乘法交互设计。"},{"order":148,"title":"Excitation Backprop for RNNs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bargal_Excitation_Backprop_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bargal_Excitation_Backprop_for_CVPR_2018_paper.html","abstract":"Deep models are state-of-the-art or many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing/training for these tasks.","中文标题":"RNN的激励反向传播","摘要翻译":"深度模型在包括视频动作识别和视频字幕在内的许多视觉任务中处于领先地位。模型被训练来为视频中的活动进行字幕或分类，但对于用于做出这些决策的证据知之甚少。深度网络做出的决策在空间视觉内容中已经得到了研究，为图像的模型预测提供了更多的洞察。然而，对于时空视觉内容——视频的模型，这样的研究相对缺乏。在这项工作中，我们设计了一种公式，使用自上而下的显著性，在单次传递中同时在地点和时间上定位证据。我们使用模型的内部表示来可视化对深度模型的分类/字幕输出有贡献的时空线索。基于这些时空线索，我们能够定位视频中与特定动作或字幕短语相对应的片段，而无需为这些任务明确优化/训练。","领域":"视频动作识别/视频字幕/时空视觉分析","问题":"理解深度模型在视频动作识别和视频字幕任务中做出决策的证据","动机":"为了更深入地理解深度模型在处理时空视觉内容（视频）时的决策过程，以及如何在不明确优化/训练的情况下定位视频中的特定动作或字幕短语","方法":"设计了一种公式，使用自上而下的显著性在单次传递中同时在地点和时间上定位证据，并通过模型的内部表示可视化时空线索","关键词":["视频动作识别","视频字幕","时空视觉分析","显著性","模型内部表示"],"涉及的技术概念":"深度模型、视频动作识别、视频字幕、时空视觉内容、自上而下的显著性、模型内部表示、时空线索"},{"order":149,"title":"One-Shot Action Localization by Learning Sequence Matching Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_One-Shot_Action_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_One-Shot_Action_Localization_CVPR_2018_paper.html","abstract":"Learning based temporal action localization methods require vast amounts of training data. However, such large-scale video datasets, which are expected to capture the dynamics of every action category, are not only very expensive to acquire but are also not practical simply because there exists an uncountable number of action classes. This poses a critical restriction to the current methods when the training samples are few and rare (e.g. when the target action classes are not present in the current publicly available datasets). To address this challenge, we conceptualize a new example-based action detection problem where only a few examples are provided, and the goal is to find the occurrences of these examples in an untrimmed video sequence. Towards this objective, we introduce a novel one-shot action localization method that alleviates the need for large amounts of training samples. Our solution adopts the one-shot learning technique of Matching Network and utilizes correlations to mine and localize actions of previously unseen classes. We evaluate our one-shot action localization method on the THUMOS14 and ActivityNet datasets, of which we modified the configuration to fit our one-shot problem setup.","中文标题":"通过学习序列匹配网络实现一次性动作定位","摘要翻译":"基于学习的时间动作定位方法需要大量的训练数据。然而，这样的大规模视频数据集不仅获取成本非常高，而且由于动作类别数量庞大，实际上并不实用。这给当前方法带来了一个关键限制，即当训练样本稀少时（例如，当目标动作类别在当前公开可用的数据集中不存在时）。为了应对这一挑战，我们概念化了一个新的基于示例的动作检测问题，其中只提供了少量示例，目标是在未剪辑的视频序列中找到这些示例的出现。针对这一目标，我们引入了一种新颖的一次性动作定位方法，该方法减少了对大量训练样本的需求。我们的解决方案采用了匹配网络的一次性学习技术，并利用相关性来挖掘和定位以前未见类别的动作。我们在THUMOS14和ActivityNet数据集上评估了我们的一次性动作定位方法，并修改了配置以适应我们的一次性问题设置。","领域":"动作识别/视频分析/序列学习","问题":"在训练样本稀少的情况下，如何有效地定位视频中的动作","动机":"由于获取大规模视频数据集的成本高且不实用，特别是在目标动作类别在当前公开数据集中不存在时，需要一种减少对大量训练样本依赖的方法。","方法":"采用一次性学习技术，通过匹配网络和相关性挖掘来定位视频中以前未见类别的动作。","关键词":["一次性学习","动作定位","序列匹配"],"涉及的技术概念":"一次性学习是一种机器学习方法，旨在通过极少量的示例来学习新类别。匹配网络是一种用于一次性学习的神经网络架构，它通过比较输入示例与目标示例的相似性来进行分类或定位。相关性挖掘是指通过分析数据之间的相关性来发现有用信息的过程。"},{"order":150,"title":"Structure Preserving Video Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Structure_Preserving_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Structure_Preserving_Video_CVPR_2018_paper.html","abstract":"Despite recent emergence of adversarial based methods for video prediction, existing algorithms often produce unsatisfied results in image regions with rich structural information (i.e., object boundary) and detailed motion (i.e., articulated body movement). To this end, we present a structure preserving video prediction framework to explicitly address above issues and enhance video prediction quality. On one hand, our framework contains a two-stream generation architecture which deals with high frequency video content (i.e., detailed object or articulated motion structure) and low frequency video content (i.e., location or moving directions) in two separate streams. On the other hand, we propose a RNN structure for video prediction, which employs temporal-adaptive convolutional kernels to capture time-varying motion patterns as well as the tiny object within a scene. Extensive experiments on diverse scene, ranging from human motion to semantic layout prediction, demonstrate the effectiveness of the proposed video prediction approach.","中文标题":"结构保持的视频预测","摘要翻译":"尽管最近出现了基于对抗方法的视频预测，现有算法在具有丰富结构信息（即物体边界）和详细运动（即关节身体运动）的图像区域中往往产生不满意的结果。为此，我们提出了一个结构保持的视频预测框架，以明确解决上述问题并提高视频预测质量。一方面，我们的框架包含一个双流生成架构，分别在两个独立的流中处理高频视频内容（即详细物体或关节运动结构）和低频视频内容（即位置或移动方向）。另一方面，我们提出了一个用于视频预测的RNN结构，它采用时间自适应卷积核来捕捉时间变化的运动模式以及场景中的微小物体。从人体运动到语义布局预测的多样场景中的大量实验证明了所提出的视频预测方法的有效性。","领域":"视频预测/运动捕捉/场景理解","问题":"现有视频预测算法在具有丰富结构信息和详细运动的图像区域中产生不满意的结果","动机":"提高视频预测质量，特别是在处理具有丰富结构信息和详细运动的图像区域时","方法":"提出了一个结构保持的视频预测框架，包含双流生成架构处理高频和低频视频内容，以及采用时间自适应卷积核的RNN结构来捕捉时间变化的运动模式和微小物体","关键词":["视频预测","结构保持","双流生成架构","时间自适应卷积核","RNN"],"涉及的技术概念":"双流生成架构用于分别处理视频中的高频和低频内容，时间自适应卷积核用于捕捉视频中的时间变化运动模式，RNN（递归神经网络）用于视频预测，以捕捉场景中的微小物体和运动模式。"},{"order":151,"title":"Person Re-Identification With Cascaded Pairwise Convolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Person_Re-Identification_With_CVPR_2018_paper.html","abstract":"In this paper, a novel deep architecture named BraidNet is proposed for person re-identification. BraidNet has a specially designed WConv layer, and the cascaded WConv structure learns to extract the comparison features of two images, which are robust to misalignments and color differences across cameras. Furthermore, a Channel Scaling layer is designed to optimize the scaling factor of each input channel, which helps mitigate the zero gradient problem in the training phase. To solve the problem of imbalanced volume of negative and positive training samples, a Sample Rate Learning strategy is proposed to adaptively update the ratio between positive and negative samples in each batch. Experiments conducted on CUHK03-Detected, CUHK03-Labeled, CUHK01, Market-1501 and DukeMTMC-reID datasets demonstrate that our method achieves competitive performance when compared to state-of-the-art methods.","中文标题":"使用级联成对卷积进行行人重识别","摘要翻译":"本文提出了一种名为BraidNet的新型深度架构，用于行人重识别。BraidNet具有特别设计的WConv层，级联的WConv结构学习提取两幅图像的比较特征，这些特征对跨摄像机的错位和颜色差异具有鲁棒性。此外，设计了一个通道缩放层来优化每个输入通道的缩放因子，这有助于缓解训练阶段的零梯度问题。为了解决负样本和正样本训练样本量不平衡的问题，提出了一种样本率学习策略，以自适应地更新每批中正负样本的比例。在CUHK03-Detected、CUHK03-Labeled、CUHK01、Market-1501和DukeMTMC-reID数据集上进行的实验表明，与最先进的方法相比，我们的方法实现了竞争性的性能。","领域":"行人重识别/深度学习/卷积神经网络","问题":"行人重识别中的跨摄像机错位和颜色差异问题，以及训练样本不平衡问题","动机":"提高行人重识别的准确性和鲁棒性，特别是在面对跨摄像机的错位和颜色差异时，以及解决训练样本不平衡的问题","方法":"提出了一种名为BraidNet的新型深度架构，包括特别设计的WConv层和通道缩放层，以及样本率学习策略","关键词":["行人重识别","级联成对卷积","样本率学习"],"涉及的技术概念":{"BraidNet":"一种新型深度架构，用于行人重识别","WConv层":"特别设计的卷积层，用于提取两幅图像的比较特征","通道缩放层":"用于优化每个输入通道的缩放因子，缓解训练阶段的零梯度问题","样本率学习策略":"一种策略，用于自适应地更新每批中正负样本的比例，解决训练样本不平衡的问题"}},{"order":152,"title":"On the Importance of Label Quality for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zlateski_On_the_Importance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zlateski_On_the_Importance_CVPR_2018_paper.html","abstract":"Convolutional networks (ConvNets) have become the dominant approach to semantic image segmentation. Producing accurate, pixel--level labels required for this task is a tedious and time consuming process; however, producing approximate, coarse labels could take only a fraction of the time and effort.  We investigate the relationship between the quality of labels and the performance of ConvNets for semantic segmentation.  We create a very large synthetic dataset with perfectly labeled street view scenes.  From these perfect labels, we synthetically coarsen labels with different qualities and estimate human--hours required for producing them.  We perform a series of experiments by training ConvNets with a varying number of training images and label quality.  We found that the performance of ConvNets mostly depends on the time spent creating the training labels. That is, a larger coarsely--annotated dataset can yield the same performance as a smaller finely--annotated one.  Furthermore, fine--tuning coarsely pre--trained ConvNets with few finely-annotated labels can yield comparable or superior performance to training it with a large amount of finely-annotated labels alone, at a fraction of the labeling cost. We demonstrate that our result is also valid for different network architectures, and various object classes in an urban scene.","中文标题":"论标签质量对语义分割的重要性","摘要翻译":"卷积网络（ConvNets）已成为语义图像分割的主导方法。为这项任务生成准确的像素级标签是一个繁琐且耗时的过程；然而，生成近似的、粗糙的标签可能只需要一小部分时间和精力。我们研究了标签质量与卷积网络在语义分割中性能之间的关系。我们创建了一个非常大的合成数据集，其中包含完美标记的街景场景。从这些完美标签中，我们合成了不同质量的粗糙标签，并估计了生成它们所需的人工小时数。我们通过使用不同数量的训练图像和标签质量训练卷积网络进行了一系列实验。我们发现，卷积网络的性能主要取决于创建训练标签所花费的时间。也就是说，一个较大的粗略注释数据集可以产生与较小的精细注释数据集相同的性能。此外，用少量精细注释的标签对粗略预训练的卷积网络进行微调，可以产生与单独使用大量精细注释标签训练相当或更优的性能，而标签成本仅为一部分。我们证明了我们的结果对于不同的网络架构和城市场景中的各种对象类别也是有效的。","领域":"语义分割/卷积网络/数据集合成","问题":"标签质量对卷积网络在语义分割任务中性能的影响","动机":"探索在语义分割任务中，使用不同质量的标签对卷积网络性能的影响，以优化标签制作过程，减少时间和成本。","方法":"创建完美标记的街景场景合成数据集，通过合成不同质量的粗糙标签并估计其制作时间，进行实验以研究标签质量与卷积网络性能的关系。","关键词":["语义分割","卷积网络","标签质量","数据集合成","性能优化"],"涉及的技术概念":"卷积网络（ConvNets）是一种深度学习模型，广泛应用于图像识别和分割任务。语义分割是指将图像中的每个像素分配到对应的对象类别，是计算机视觉中的一个重要任务。数据集合成是指通过计算机生成或修改现有数据来创建新的数据集，用于训练和测试机器学习模型。"},{"order":153,"title":"Scalable and Effective Deep CCA via Soft Decorrelation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Scalable_and_Effective_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Scalable_and_Effective_CVPR_2018_paper.html","abstract":"Recently the widely used multi-view learning model, Canonical Correlation Analysis (CCA) has been generalised to the non-linear setting via deep neural networks. Existing deep CCA models typically first decorrelate the feature dimensions of each view before the different views are maximally correlated in a common latent space. This feature decorrelation is achieved by enforcing an exact decorrelation constraint; these models are thus computationally expensive due to the matrix inversion or SVD operations required for exact decorrelation at each training iteration. Furthermore, the decorrelation step is often separated from the gradient descent based optimisation, resulting in sub-optimal solutions. We propose a novel deep CCA model Soft CCA to overcome these problems. Specifically, exact decorrelation is replaced by soft decorrelation via a mini-batch based Stochastic Decorrelation Loss (SDL) to be optimised jointly with the other training objectives. Extensive experiments show that the proposed soft CCA is more effective and efficient than existing deep CCA models. In addition, our SDL loss can be applied to other deep models beyond multi-view learning, and obtains superior performance compared to existing decorrelation losses.","中文标题":"可扩展且有效的深度CCA通过软去相关","摘要翻译":"最近，广泛使用的多视图学习模型，典型相关分析（CCA）已经通过深度神经网络推广到非线性设置。现有的深度CCA模型通常首先在将不同视图在共同潜在空间中最大化相关之前，对每个视图的特征维度进行去相关。这种特征去相关是通过强制执行精确的去相关约束来实现的；因此，由于每次训练迭代都需要进行矩阵求逆或SVD操作以实现精确的去相关，这些模型在计算上是昂贵的。此外，去相关步骤通常与基于梯度下降的优化分离，导致次优解。我们提出了一种新颖的深度CCA模型Soft CCA来克服这些问题。具体来说，精确的去相关被通过基于小批量的随机去相关损失（SDL）的软去相关所取代，该损失与其他训练目标共同优化。大量实验表明，所提出的软CCA比现有的深度CCA模型更有效和高效。此外，我们的SDL损失可以应用于多视图学习之外的其他深度模型，并且与现有的去相关损失相比，获得了优越的性能。","领域":"多视图学习/深度神经网络/优化算法","问题":"现有深度CCA模型在计算上昂贵且优化不充分的问题","动机":"提高深度CCA模型的计算效率和优化效果","方法":"提出了一种新颖的深度CCA模型Soft CCA，通过引入基于小批量的随机去相关损失（SDL）实现软去相关，并与其他训练目标共同优化","关键词":["多视图学习","深度CCA","软去相关","随机去相关损失","优化算法"],"涉及的技术概念":{"典型相关分析（CCA）":"一种统计方法，用于分析两组变量之间的线性关系","深度神经网络":"一种模仿人脑结构和功能的计算模型，用于处理和分析复杂数据","矩阵求逆":"一种数学运算，用于找到给定矩阵的逆矩阵","SVD操作":"奇异值分解，一种矩阵分解方法，用于信号处理和统计学","梯度下降":"一种优化算法，用于最小化或最大化目标函数","小批量":"在深度学习中，指每次迭代中用于更新模型参数的一小部分数据"}},{"order":154,"title":"Duplex Generative Adversarial Network for Unsupervised Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Duplex_Generative_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Duplex_Generative_Adversarial_CVPR_2018_paper.html","abstract":"Domain adaptation attempts to transfer the knowledge obtained from the source domain to the target domain, i.e., the domain where the testing data are. The main challenge lies in the distribution discrepancy between source and target domain. Most existing works endeavor to learn domain invariant representation usually by minimizing a distribution distance, e.g., MMD and the discriminator in the recently proposed generative adversarial network (GAN). Following the similar idea of GAN, this work proposes a novel GAN architecture with duplex adversarial discriminators (referred to as DupGAN), which can achieve domain-invariant representation and domain transformation. Specifically, our proposed network consists of three parts, an encoder, a generator and two discriminators. The encoder embeds samples from both domains into the latent representation, and the generator decodes the latent representation to both source and target domains respectively conditioned on a domain code, i.e., achieves domain transformation. The generator is pitted against duplex discriminators, one for source domain and the other for target, to ensure the reality of domain transformation, the latent representation domain invariant and the category information of it preserved as well. Our proposed work achieves the state-of-the-art performance on unsupervised domain adaptation of digit classification and object recognition.","中文标题":"双工生成对抗网络用于无监督领域适应","摘要翻译":"领域适应试图将从源领域获得的知识转移到目标领域，即测试数据所在的领域。主要挑战在于源领域和目标领域之间的分布差异。大多数现有工作通常通过最小化分布距离（例如，MMD和最近提出的生成对抗网络（GAN）中的判别器）来学习领域不变表示。遵循GAN的类似思想，本工作提出了一种具有双工对抗判别器的新型GAN架构（称为DupGAN），它可以实现领域不变表示和领域转换。具体来说，我们提出的网络由三部分组成：编码器、生成器和两个判别器。编码器将来自两个领域的样本嵌入到潜在表示中，生成器根据领域代码将潜在表示分别解码为源领域和目标领域，即实现领域转换。生成器与双工判别器对抗，一个用于源领域，另一个用于目标领域，以确保领域转换的真实性、潜在表示的领域不变性以及其类别信息的保留。我们提出的工作在无监督领域适应的数字分类和对象识别方面达到了最先进的性能。","领域":"领域适应/生成对抗网络/无监督学习","问题":"解决源领域和目标领域之间的分布差异问题，实现领域不变表示和领域转换。","动机":"为了将从源领域获得的知识有效转移到目标领域，克服领域间分布差异的挑战。","方法":"提出了一种新型GAN架构（DupGAN），包括编码器、生成器和两个判别器，通过双工对抗判别器实现领域不变表示和领域转换。","关键词":["领域适应","生成对抗网络","无监督学习","领域转换","潜在表示"],"涉及的技术概念":"生成对抗网络（GAN）是一种通过对抗过程估计生成模型的框架，其中生成器尝试生成尽可能真实的数据，而判别器则尝试区分真实数据和生成数据。领域适应是一种技术，旨在将在一个领域（源领域）上训练得到的模型适应到另一个不同但相关的领域（目标领域）。无监督学习是一种机器学习方法，它不依赖于标记数据来训练模型。"},{"order":155,"title":"Edit Probability for Scene Text Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bai_Edit_Probability_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bai_Edit_Probability_for_CVPR_2018_paper.html","abstract":"We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance.","中文标题":"场景文本识别的编辑概率","摘要翻译":"我们在基于注意力的编码器-解码器框架下考虑场景文本识别问题，这是目前最先进的方法。现有方法通常采用逐帧最大似然损失来优化模型。当我们训练模型时，由于缺失或多余字符导致的地面真实字符串与注意力输出概率分布序列之间的不对齐，会混淆和误导训练过程，从而使训练成本高昂并降低识别准确性。为了解决这个问题，我们提出了一种称为编辑概率（EP）的新方法用于场景文本识别。EP尝试有效地估计从输入图像条件下的概率分布输出序列生成字符串的概率，同时考虑可能出现的缺失/多余字符。其优势在于训练过程可以专注于缺失、多余和未识别的字符，从而减轻甚至克服不对齐问题的影响。我们在包括IIIT-5K、街景文本和ICDAR数据集在内的标准基准上进行了广泛的实验。实验结果表明，EP可以显著提高场景文本识别的性能。","领域":"场景文本识别/自然语言处理/深度学习","问题":"解决场景文本识别中由于缺失或多余字符导致的训练过程中的不对齐问题","动机":"提高场景文本识别的准确性和训练效率","方法":"提出了一种称为编辑概率（EP）的新方法，通过估计从输入图像条件下的概率分布输出序列生成字符串的概率，专注于缺失、多余和未识别的字符","关键词":["场景文本识别","编辑概率","注意力机制"],"涉及的技术概念":"基于注意力的编码器-解码器框架、逐帧最大似然损失、编辑概率（EP）、IIIT-5K、街景文本、ICDAR数据集"},{"order":156,"title":"Global Versus Localized Generative Adversarial Nets","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Global_Versus_Localized_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Global_Versus_Localized_CVPR_2018_paper.html","abstract":"In this paper, we present a novel localized Generative Adversarial Net (GAN) to learn on the manifold of real data. Compared with the classic GAN that {em globally} parameterizes a manifold, the Localized GAN (LGAN) uses local coordinate charts to parameterize distinct local geometry of how data points can transform at different locations on the manifold. Specifically, around each point there exists a {em local} generator that can produce data following diverse patterns of transformations on the manifold.  The locality nature of LGAN enables local generators to adapt to and directly access the local geometry without need to invert the generator in a global GAN. Furthermore, it can prevent the manifold from being locally collapsed to a dimensionally deficient tangent subspace by imposing an orthonormality prior between tangents. This provides a geometric approach to alleviating mode collapse at least locally on the manifold by imposing independence between data transformations in different tangent directions. We will also demonstrate the LGAN can be applied to train a robust classifier that prefers locally consistent classification decisions on the manifold, and the resultant regularizer is closely related with the Laplace-Beltrami operator. Our experiments show that the proposed LGANs can not only produce diverse image transformations, but also deliver superior classification performances.","中文标题":"全局与局部生成对抗网络","摘要翻译":"在本文中，我们提出了一种新颖的局部生成对抗网络（GAN），以在真实数据的流形上学习。与经典GAN全局参数化流形相比，局部GAN（LGAN）使用局部坐标图来参数化流形上不同位置的数据点如何转换的独特局部几何。具体来说，围绕每个点存在一个局部生成器，可以生成遵循流形上不同转换模式的数据。LGAN的局部性质使得局部生成器能够适应并直接访问局部几何，而无需在全局GAN中反转生成器。此外，通过在切线之间施加正交性先验，它可以防止流形局部塌陷到维度不足的切子空间。这提供了一种几何方法，通过在不同切线方向上的数据转换之间施加独立性，至少可以在流形上局部缓解模式崩溃。我们还将展示LGAN可以用于训练一个鲁棒的分类器，该分类器倾向于在流形上做出局部一致的分类决策，并且所得的正则化器与拉普拉斯-贝尔特拉米算子密切相关。我们的实验表明，所提出的LGAN不仅可以产生多样化的图像转换，还可以提供卓越的分类性能。","领域":"生成对抗网络/流形学习/图像分类","问题":"解决全局生成对抗网络在处理流形上数据转换时的局部几何适应性问题","动机":"为了提高生成对抗网络在流形上数据转换的局部适应性和防止模式崩溃","方法":"提出局部生成对抗网络（LGAN），使用局部坐标图参数化流形上的局部几何，并在切线方向施加正交性先验","关键词":["局部生成对抗网络","流形学习","图像分类","模式崩溃","拉普拉斯-贝尔特拉米算子"],"涉及的技术概念":"局部生成对抗网络（LGAN）通过在流形上使用局部坐标图来参数化局部几何，使得局部生成器能够直接访问和适应局部几何，而无需全局反转生成器。通过在切线方向施加正交性先验，防止流形局部塌陷到维度不足的切子空间，从而缓解模式崩溃。此外，LGAN还可以用于训练一个鲁棒的分类器，该分类器倾向于在流形上做出局部一致的分类决策，所得的正则化器与拉普拉斯-贝尔特拉米算子密切相关。"},{"order":157,"title":"MoCoGAN: Decomposing Motion and Content for Video Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tulyakov_MoCoGAN_Decomposing_Motion_CVPR_2018_paper.html","abstract":"Visual signals in a video can be divided into content and motion. While content specifies which objects are in the video, motion describes their dynamics. Based on this prior, we propose the Motion and Content decomposed Generative Adversarial Network (MoCoGAN) framework for video generation. The proposed framework generates a video by mapping a sequence of random vectors to a sequence of video frames. Each random vector consists of a content part and a motion part. While the content part is  kept fixed, the motion part is realized as a stochastic process. To learn motion and content decomposition in an unsupervised manner, we introduce a novel adversarial learning scheme utilizing both image and video discriminators. Extensive experimental results on several challenging datasets with qualitative and quantitative comparison to the state-of-the-art approaches, verify effectiveness of the proposed framework. In addition, we show that MoCoGAN allows one to generate videos with same content but different motion as well as videos with different content and same motion. Our code is available at https://github.com/sergeytulyakov/mocogan.","中文标题":"MoCoGAN: 分解运动和内容以生成视频","摘要翻译":"视频中的视觉信号可以分为内容和运动。内容指定了视频中的对象，而运动描述了它们的动态。基于这一前提，我们提出了运动和内容分解的生成对抗网络（MoCoGAN）框架用于视频生成。所提出的框架通过将一系列随机向量映射到一系列视频帧来生成视频。每个随机向量由内容部分和运动部分组成。内容部分保持不变，而运动部分实现为一个随机过程。为了以无监督的方式学习运动和内容的分解，我们引入了一种新颖的对抗学习方案，利用图像和视频鉴别器。在几个具有挑战性的数据集上进行的大量实验结果，与最先进的方法进行定性和定量比较，验证了所提出框架的有效性。此外，我们展示了MoCoGAN允许生成具有相同内容但不同运动的视频，以及具有不同内容和相同运动的视频。我们的代码可在https://github.com/sergeytulyakov/mocogan获取。","领域":"视频生成/生成对抗网络/无监督学习","问题":"如何有效地生成具有特定内容和运动的视频","动机":"视频中的视觉信号可以分为内容和运动，为了生成具有特定内容和运动的视频，需要一种能够分解这两部分并分别控制的框架","方法":"提出了运动和内容分解的生成对抗网络（MoCoGAN）框架，通过将一系列随机向量映射到一系列视频帧来生成视频，其中内容部分保持不变，运动部分实现为一个随机过程，并引入了一种新颖的对抗学习方案，利用图像和视频鉴别器以无监督的方式学习运动和内容的分解","关键词":["视频生成","生成对抗网络","无监督学习","内容分解","运动分解"],"涉及的技术概念":"生成对抗网络（GAN）是一种深度学习模型，由生成器和鉴别器组成，通过对抗过程学习生成数据。无监督学习是一种机器学习方法，模型在没有标签的数据上学习数据的内在结构和分布。内容分解和运动分解是指将视频中的视觉信号分为静态的内容和动态的运动两部分，以便于分别控制和生成。"},{"order":158,"title":"Recurrent Residual Module for Fast Inference in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Recurrent_Residual_Module_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Recurrent_Residual_Module_CVPR_2018_paper.html","abstract":"Deep convolutional neural networks (CNNs) have made impressive progress in many video recognition tasks such as video pose estimation and video object detection. However, running CNN inference on video requires numerous computation and is usually slow. In this work, we propose a framework called Recurrent Residual Module (RRM) to accelerate the CNN inference for video recognition tasks. This framework has a novel design of using the similarity of the intermediate feature maps of two consecutive frames to largely reduce the redundant computation. One unique property of the proposed method compared to previous work is that feature maps of each frame are precisely computed. The experiments show that, while maintaining the similar recognition performance, our RRM yields averagely 2× acceleration on the commonly used CNNs such as AlexNet, ResNet, deep compression model (thus 8−12× faster than the original dense models on the efﬁcient inference engine), and impressively 9× acceleration on some binary networks such as XNOR-Nets (thus 500× faster than the original model). We further verify the effectiveness of the RRM on speeding CNNs for video pose estimation and video object detection.","中文标题":"用于视频快速推理的循环残差模块","摘要翻译":"深度卷积神经网络（CNNs）在许多视频识别任务中取得了显著进展，如视频姿态估计和视频目标检测。然而，在视频上运行CNN推理需要大量计算，通常速度较慢。在这项工作中，我们提出了一个名为循环残差模块（RRM）的框架，以加速视频识别任务的CNN推理。该框架采用了一种新颖的设计，利用两个连续帧的中间特征图的相似性来大幅减少冗余计算。与之前的工作相比，所提出方法的一个独特属性是每一帧的特征图都被精确计算。实验表明，在保持相似的识别性能的同时，我们的RRM在常用的CNNs上平均实现了2倍的加速，如AlexNet、ResNet、深度压缩模型（因此在高效推理引擎上比原始密集模型快8-12倍），在一些二进制网络如XNOR-Nets上实现了令人印象深刻的9倍加速（因此比原始模型快500倍）。我们进一步验证了RRM在加速CNNs用于视频姿态估计和视频目标检测方面的有效性。","领域":"视频识别/特征图优化/推理加速","问题":"视频识别任务中CNN推理速度慢的问题","动机":"提高视频识别任务中CNN推理的速度，减少计算冗余","方法":"提出循环残差模块（RRM）框架，利用连续帧中间特征图的相似性减少冗余计算","关键词":["视频识别","推理加速","特征图优化"],"涉及的技术概念":"深度卷积神经网络（CNNs）、循环残差模块（RRM）、视频姿态估计、视频目标检测、特征图、AlexNet、ResNet、深度压缩模型、XNOR-Nets"},{"order":159,"title":"Improving Landmark Localization With Semi-Supervised Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Honari_Improving_Landmark_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Honari_Improving_Landmark_Localization_CVPR_2018_paper.html","abstract":"We present two techniques to improve landmark localization in images from partially annotated datasets. Our primary goal is to leverage the common situation where precise landmark locations are only provided for a small data subset, but where class labels for classification or regression tasks related to the landmarks are more abundantly available.  First, we propose the framework of sequential multitasking and explore it here through an architecture for landmark localization where training with class labels acts as an auxiliary signal to guide the landmark localization on unlabeled data. A key aspect of our approach is that errors can be backpropagated through a complete landmark localization model. Second, we propose and explore an unsupervised learning technique for landmark localization based on having a model predict equivariant landmarks with respect to transformations applied to the image. We show that these techniques, improve landmark prediction considerably and can learn effective detectors even when only a small fraction of the dataset has landmark labels. We present results on two toy datasets and four real datasets, with hands and faces, and report new state-of-the-art on two datasets in the wild, e.g. with only 5% of labeled images we outperform previous state-of-the-art trained on the AFLW dataset.","中文标题":"利用半监督学习改进地标定位","摘要翻译":"我们提出了两种技术来改进从部分注释数据集中进行图像地标定位。我们的主要目标是利用常见情况，即精确的地标位置仅为一小部分数据子集提供，但与地标相关的分类或回归任务的类别标签更为丰富。首先，我们提出了顺序多任务框架，并通过一种地标定位架构进行探索，其中使用类别标签进行训练作为辅助信号，以指导未标记数据上的地标定位。我们方法的一个关键方面是，错误可以通过完整的地标定位模型进行反向传播。其次，我们提出并探索了一种基于模型预测相对于图像变换的等变地标的无监督学习技术。我们展示了这些技术显著改进了地标预测，并且即使只有一小部分数据集具有地标标签，也能学习到有效的检测器。我们在两个玩具数据集和四个真实数据集（包括手和脸）上展示了结果，并在两个野外数据集上报告了新的最先进水平，例如，仅使用5%的标记图像，我们在AFLW数据集上的表现优于之前的最先进水平。","领域":"地标定位/半监督学习/无监督学习","问题":"在部分注释的数据集中改进地标定位","动机":"利用丰富的类别标签信息来指导地标定位，尤其是在只有少量数据具有精确地标位置的情况下","方法":"提出了顺序多任务框架和无监督学习技术，通过类别标签训练作为辅助信号，以及预测相对于图像变换的等变地标","关键词":["地标定位","半监督学习","无监督学习","顺序多任务框架","等变地标"],"涉及的技术概念":"顺序多任务框架是一种利用类别标签作为辅助信号来指导地标定位的方法，而无监督学习技术则是通过模型预测相对于图像变换的等变地标来实现地标定位。这两种方法都旨在提高地标预测的准确性，尤其是在只有少量数据具有地标标签的情况下。"},{"order":160,"title":"Adversarial Data Programming: Using GANs to Relax the Bottleneck of Curated Labeled Data","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pal_Adversarial_Data_Programming_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pal_Adversarial_Data_Programming_CVPR_2018_paper.html","abstract":"Paucity of large curated hand labeled training data forms a major bottleneck in the deployment of machine learning models in computer vision and other fields. Recent work (Data Programming) has shown how distant supervision signals in the form of labeling functions can be used to obtain labels for given data in near-constant time. In this work, we present Adversarial Data Programming (ADP), which presents an adversarial methodology to generate data as well as a curated aggregated label, given a set of weak labeling functions. We validated our method on the MNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many state-of-the-art models. We conducted extensive experiments to study its usefulness, as well as showed how the proposed ADP framework can be used for transfer learning as well as multitask learning, where data from two domains are generated simultaneously using the framework along with the label information. Our future work will involve understanding the theoretical implications of this new framework from a game-theoretic perspective, as well as explore the performance of the method on more complex datasets.","中文标题":"对抗性数据编程：使用GANs缓解精选标记数据的瓶颈","摘要翻译":"在计算机视觉及其他领域，缺乏大量精选的手工标记训练数据是部署机器学习模型的主要瓶颈。最近的工作（数据编程）已经展示了如何以标记函数的形式使用远距离监督信号，在近乎恒定的时间内为给定数据获取标签。在这项工作中，我们提出了对抗性数据编程（ADP），它提出了一种对抗性方法，以生成数据以及精选的聚合标签，给定一组弱标记函数。我们在MNIST、Fashion MNIST、CIFAR 10和SVHN数据集上验证了我们的方法，并且它优于许多最先进的模型。我们进行了广泛的实验来研究其有用性，并展示了所提出的ADP框架如何用于迁移学习以及多任务学习，其中使用框架同时生成来自两个领域的数据以及标签信息。我们未来的工作将包括从博弈论的角度理解这一新框架的理论含义，以及探索该方法在更复杂数据集上的性能。","领域":"生成对抗网络/数据增强/迁移学习","问题":"解决缺乏大量精选手工标记训练数据的问题","动机":"缓解机器学习模型部署中因缺乏大量精选手工标记训练数据而遇到的瓶颈","方法":"提出对抗性数据编程（ADP），使用生成对抗网络（GANs）生成数据及精选的聚合标签，基于一组弱标记函数","关键词":["生成对抗网络","数据增强","迁移学习","多任务学习"],"涉及的技术概念":"对抗性数据编程（ADP）是一种结合生成对抗网络（GANs）和数据编程的方法，旨在通过生成数据和精选的聚合标签来缓解缺乏大量精选手工标记训练数据的问题。ADP框架支持迁移学习和多任务学习，能够同时生成来自两个领域的数据及标签信息。"},{"order":161,"title":"Stochastic Variational Inference With Gradient Linearization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Plotz_Stochastic_Variational_Inference_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Plotz_Stochastic_Variational_Inference_CVPR_2018_paper.html","abstract":"Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.","中文标题":"随机变分推断与梯度线性化","摘要翻译":"由于随机方法的应用，变分推断最近变得非常流行，这些方法为广泛的模型类别提供了实用工具。一个关键的好处是，随机变分推断避免了推导闭式变量更新的繁琐过程。相反，人们只需要推导对数后验的梯度，这通常要容易得多。然而，对于某些模型类别，使用标准梯度技术优化对数后验本身是困难的。一个这样的例子是随机场模型，其中基于梯度线性化的优化已经变得流行，因为它显著加快了收敛速度，并且可以避免陷入不良的局部最优。在本文中，我们提出了带有梯度线性化的随机变分推断（SVIGL）。它与标准随机变分推断同样方便——所需要的只是能量梯度的局部线性化。与使用传统梯度方法的随机变分推断相比，它的好处是收敛速度的明显提高，同时在KL散度方面产生可比甚至更好的变分近似。我们在三个应用中展示了SVIGL的好处：光流估计、泊松-高斯去噪和3D表面重建。","领域":"变分推断/随机场模型/梯度优化","问题":"优化对数后验的困难","动机":"提高收敛速度，避免陷入不良的局部最优","方法":"提出带有梯度线性化的随机变分推断（SVIGL）","关键词":["随机变分推断","梯度线性化","随机场模型","KL散度"],"涉及的技术概念":"变分推断是一种近似推断方法，用于在复杂模型中近似计算后验分布。随机变分推断通过引入随机性来简化这一过程，使得对于广泛的模型类别变得实用。梯度线性化是一种优化技术，通过线性化能量梯度来加速收敛并避免局部最优。KL散度（Kullback-Leibler divergence）是衡量两个概率分布差异的指标，常用于评估变分近似的质量。"},{"order":162,"title":"Multi-Label Zero-Shot Learning With Structured Knowledge Graphs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Multi-Label_Zero-Shot_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Multi-Label_Zero-Shot_Learning_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel deep learning architecture for multi-label zero-shot learning (ML-ZSL), which is able to predict multiple unseen class labels for each input instance. Inspired by the way humans utilize semantic knowledge between objects of interests, we propose a framework that incorporates knowledge graphs for describing the relationships between multiple labels. Our model learns an information propagation mechanism from the semantic label space, which can be applied to model the interdependencies between seen and unseen class labels. With such investigation of structured knowledge graphs for visual reasoning, we show that our model can be applied for solving multi-label classification and ML-ZSL tasks. Compared to state-of-the-art approaches, comparable or improved performances can be achieved by our method.","中文标题":"基于结构化知识图的多标签零样本学习","摘要翻译":"在本文中，我们提出了一种新颖的深度学习架构，用于多标签零样本学习（ML-ZSL），该架构能够为每个输入实例预测多个未见过的类别标签。受到人类利用感兴趣对象之间语义知识的方式的启发，我们提出了一个框架，该框架结合了知识图来描述多个标签之间的关系。我们的模型从语义标签空间学习信息传播机制，该机制可以应用于建模已见和未见类别标签之间的相互依赖性。通过对结构化知识图进行视觉推理的研究，我们展示了我们的模型可以应用于解决多标签分类和ML-ZSL任务。与最先进的方法相比，我们的方法可以实现相当或改进的性能。","领域":"多标签学习/零样本学习/知识图谱","问题":"解决多标签零样本学习中的未见类别标签预测问题","动机":"受到人类利用语义知识进行推理的启发，旨在通过结构化知识图提高多标签零样本学习的性能","方法":"提出了一种结合知识图的深度学习架构，通过学习语义标签空间的信息传播机制来建模已见和未见类别标签之间的相互依赖性","关键词":["多标签学习","零样本学习","知识图谱","信息传播机制","视觉推理"],"涉及的技术概念":"多标签零样本学习（ML-ZSL）是一种能够预测未见类别标签的学习方法。知识图用于描述标签之间的关系，信息传播机制是指模型从语义标签空间学习如何传播信息以建模标签间的相互依赖性。视觉推理是指利用视觉信息进行逻辑推理的过程。"},{"order":163,"title":"MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_MorphNet_Fast__CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_MorphNet_Fast__CVPR_2018_paper.html","abstract":"We present MorphNet, an approach to automate the design of neural  network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on  activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous  approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g.   the number of floating-point operations per inference), and capable of increasing the network’s performance. When applied to standard network architectures on a  wide variety of datasets, our  approach discovers novel structures  in each domain, obtaining higher  performance while respecting the resource constraint.","中文标题":"MorphNet：快速且简单的资源受限深度网络结构学习","摘要翻译":"我们提出了MorphNet，一种自动化设计神经网络结构的方法。MorphNet通过迭代地收缩和扩展网络，收缩是通过对激活的资源加权稀疏正则化器实现的，扩展是通过对所有层应用统一的乘法因子实现的。与之前的方法相比，我们的方法能够扩展到大型网络，适应特定的资源约束（例如每次推理的浮点操作数），并且能够提高网络的性能。当应用于各种数据集上的标准网络架构时，我们的方法在每个领域发现了新的结构，在尊重资源约束的同时获得了更高的性能。","领域":"神经网络优化/自动化机器学习/资源约束优化","问题":"自动化设计神经网络结构以适应特定的资源约束","动机":"提高神经网络在特定资源约束下的性能，同时自动化网络结构的设计过程","方法":"通过迭代地收缩和扩展网络，使用资源加权稀疏正则化器进行收缩，对所有层应用统一的乘法因子进行扩展","关键词":["神经网络优化","自动化机器学习","资源约束优化"],"涉及的技术概念":"MorphNet是一种自动化设计神经网络结构的方法，它通过迭代地收缩和扩展网络来适应特定的资源约束。收缩是通过对激活的资源加权稀疏正则化器实现的，这有助于减少网络的复杂性。扩展是通过对所有层应用统一的乘法因子实现的，这有助于增加网络的容量。这种方法能够提高网络在特定资源约束下的性能，并且能够自动化地发现新的网络结构。"},{"order":164,"title":"Deep Adversarial Subspace Clustering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Deep_Adversarial_Subspace_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Deep_Adversarial_Subspace_CVPR_2018_paper.html","abstract":"Most existing subspace clustering methods hinge on self-expression of handcrafted representations and are unaware of potential clustering errors. Thus they  perform unsatisfactorily on real data with complex underlying subspaces. To solve this issue, we propose a novel deep adversarial subspace clustering (DASC) model, which learns more favorable sample representations by deep learning for subspace clustering, and more importantly introduces adversarial learning to supervise sample representation learning and subspace clustering. Specifically, DASC consists of a subspace clustering generator and a quality-verifying discriminator, which learn against each other. The generator produces subspace estimation and sample clustering. The discriminator  evaluates  current clustering performance by inspecting whether the re-sampled data from estimated subspaces have consistent subspace properties, and  supervises the generator to progressively improve subspace clustering. Experimental results on the handwritten recognition, face and object clustering tasks demonstrate the advantages of DASC over shallow and few deep subspace clustering models. Moreover, to our best knowledge, this is the first successful application of GAN-alike model for unsupervised subspace clustering, which also paves the way for deep learning to solve other unsupervised learning problems.","中文标题":"深度对抗子空间聚类","摘要翻译":"大多数现有的子空间聚类方法依赖于手工制作表示的自表达，并且对潜在的聚类错误不知情。因此，它们在具有复杂底层子空间的真实数据上表现不佳。为了解决这个问题，我们提出了一种新颖的深度对抗子空间聚类（DASC）模型，该模型通过深度学习学习更有利的样本表示以进行子空间聚类，更重要的是引入了对抗学习来监督样本表示学习和子空间聚类。具体来说，DASC由一个子空间聚类生成器和一个质量验证判别器组成，它们相互学习。生成器产生子空间估计和样本聚类。判别器通过检查从估计子空间重新采样的数据是否具有一致的子空间属性来评估当前聚类性能，并监督生成器逐步改进子空间聚类。在手写识别、面部和对象聚类任务上的实验结果证明了DASC相对于浅层和少数深度子空间聚类模型的优势。此外，据我们所知，这是首次成功应用类似GAN的模型进行无监督子空间聚类，这也为深度学习解决其他无监督学习问题铺平了道路。","领域":"子空间聚类/对抗学习/无监督学习","问题":"现有子空间聚类方法在复杂底层子空间的真实数据上表现不佳","动机":"提高子空间聚类在复杂底层子空间真实数据上的表现","方法":"提出深度对抗子空间聚类（DASC）模型，通过深度学习学习样本表示，并引入对抗学习监督样本表示学习和子空间聚类","关键词":["子空间聚类","对抗学习","无监督学习"],"涉及的技术概念":"子空间聚类是一种将数据点分组到多个子空间中的技术，每个子空间都是原始数据空间的一个低维子集。对抗学习是一种通过让两个模型相互对抗来学习的技术，其中一个模型尝试生成数据，另一个模型尝试区分生成的数据和真实数据。无监督学习是一种不需要标签数据的学习方法，它试图从数据本身发现结构或模式。"},{"order":165,"title":"Towards Human-Machine Cooperation: Self-Supervised Sample Mining for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Towards_Human-Machine_Cooperation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Towards_Human-Machine_Cooperation_CVPR_2018_paper.html","abstract":"Though quite challenging, leveraging large-scale unlabeled or partially labeled images in a cost-effective way has increasingly attracted interests for its great importance to computer vision. To tackle this problem, many Active Learning (AL) methods have been developed. However, these methods mainly define their sample selection criteria within a single image context, leading to the suboptimal robustness and impractical solution for large-scale object detection. In this paper, aiming to remedy the drawbacks of existing AL methods, we present a principled Self-supervised Sample Mining (SSM) process accounting for the real challenges in object detection. Specifically, our SSM process concentrates on automatically discovering and pseudo-labeling reliable region proposals for enhancing the object detector via the introduced cross image validation, i.e., pasting these proposals into different labeled images to comprehensively measure their values under different image contexts. By resorting to the SSM process, we propose a new AL framework for gradually incorporating unlabeled or partially labeled data into the model learning while minimizing the annotating effort of users. Extensive experiments on two public benchmarks clearly demonstrate our proposed framework can achieve the comparable performance to the state-of-the-art methods with significantly fewer annotations.","中文标题":"迈向人机合作：用于目标检测的自监督样本挖掘","摘要翻译":"尽管相当具有挑战性，但以成本效益高的方式利用大规模未标记或部分标记的图像因其对计算机视觉的重要性而日益引起关注。为了解决这个问题，许多主动学习（AL）方法被开发出来。然而，这些方法主要在单个图像上下文中定义其样本选择标准，导致在大规模目标检测中的鲁棒性不足和不切实际的解决方案。在本文中，旨在弥补现有AL方法的不足，我们提出了一个原则性的自监督样本挖掘（SSM）过程，以应对目标检测中的实际挑战。具体来说，我们的SSM过程专注于自动发现和伪标记可靠区域提议，通过引入的跨图像验证来增强目标检测器，即，将这些提议粘贴到不同的标记图像中，以全面衡量它们在不同图像上下文中的价值。通过采用SSM过程，我们提出了一个新的AL框架，用于逐步将未标记或部分标记的数据纳入模型学习，同时最小化用户的注释工作。在两个公共基准上的大量实验清楚地表明，我们提出的框架可以在显著减少注释的情况下实现与最先进方法相当的性能。","领域":"目标检测/主动学习/自监督学习","问题":"如何在大规模未标记或部分标记的图像中有效地进行目标检测","动机":"弥补现有主动学习方法在单个图像上下文中定义样本选择标准的不足，提高大规模目标检测的鲁棒性和实用性","方法":"提出自监督样本挖掘（SSM）过程，通过跨图像验证自动发现和伪标记可靠区域提议，增强目标检测器","关键词":["目标检测","主动学习","自监督学习","跨图像验证","伪标记"],"涉及的技术概念":"主动学习（AL）是一种机器学习方法，它允许模型在训练过程中选择最有信息量的样本进行学习，以减少注释成本。自监督学习是一种无需人工标注的学习方法，通过设计预测任务来利用数据本身的结构信息进行学习。跨图像验证是一种技术，通过将图像中的区域提议粘贴到其他图像中来评估其在不同上下文中的表现。伪标记是指在没有真实标签的情况下，使用模型预测的标签作为训练数据的过程。"},{"order":166,"title":"Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Laude_Discrete-Continuous_ADMM_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Laude_Discrete-Continuous_ADMM_for_CVPR_2018_paper.html","abstract":"This paper introduces a novel algorithm for transductive inference in higher-order MRFs, where the unary energies are parameterized by a variable classifier. The considered task is posed as a joint optimization problem in the continuous classifier parameters and the discrete label variables. In contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of the objective function into discrete and continuous subproblems and a novel, efficient optimization method related to ADMM. This approach preserves integrality of the discrete label variables and guarantees global convergence to a critical point.  We demonstrate the advantages of our approach in several experiments including video object segmentation on the DAVIS data set and interactive image segmentation.","中文标题":"离散-连续ADMM用于高阶马尔可夫随机场的转导推理","摘要翻译":"本文介绍了一种用于高阶马尔可夫随机场（MRFs）转导推理的新算法，其中一元能量由可变分类器参数化。所考虑的任务被表述为连续分类器参数和离散标签变量的联合优化问题。与凸松弛等先前方法相比，我们提出了一种将目标函数分解为离散和连续子问题的有利解耦方法，以及一种与ADMM相关的新型高效优化方法。这种方法保留了离散标签变量的完整性，并保证全局收敛到临界点。我们在包括DAVIS数据集上的视频对象分割和交互式图像分割在内的多个实验中展示了我们方法的优势。","领域":"视频对象分割/交互式图像分割/优化算法","问题":"高阶马尔可夫随机场中的转导推理问题","动机":"解决现有方法在处理高阶马尔可夫随机场转导推理时遇到的挑战，如保持离散标签变量的完整性和保证全局收敛","方法":"提出了一种将目标函数分解为离散和连续子问题的解耦方法，并开发了一种与ADMM相关的新型高效优化方法","关键词":["转导推理","高阶马尔可夫随机场","ADMM","视频对象分割","交互式图像分割"],"涉及的技术概念":{"转导推理":"一种机器学习方法，旨在利用未标记数据来改进模型性能","高阶马尔可夫随机场":"一种用于建模复杂依赖关系的统计模型，能够处理多个变量之间的高阶交互","ADMM":"交替方向乘子法，一种用于解决优化问题的算法，特别适用于分解和并行计算","视频对象分割":"从视频序列中分离出特定对象的过程","交互式图像分割":"一种图像分割方法，允许用户通过交互方式指导分割过程"}},{"order":167,"title":"Robust Physical-World Attacks on Deep Learning Visual Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.html","abstract":"Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP 2 ), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP 2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.","中文标题":"深度学习视觉分类在现实世界中的鲁棒性攻击","摘要翻译":"最近的研究表明，最先进的深度神经网络（DNNs）对对抗性示例是脆弱的，这些示例是由于在输入中添加了小幅度的扰动而产生的。鉴于新兴的物理系统在安全关键的情况下使用DNNs，对抗性示例可能会误导这些系统并导致危险情况。因此，理解现实世界中的对抗性示例是开发鲁棒学习算法的重要一步。我们提出了一种通用的攻击算法，鲁棒物理扰动（RP 2），以在不同物理条件下生成鲁棒的视觉对抗性扰动。通过道路标志分类的真实案例，我们展示了使用RP 2生成的对抗性示例在各种环境条件下，包括视角，对标准架构的道路标志分类器在现实世界中实现了高目标误分类率。由于目前缺乏标准化的测试方法，我们提出了一种两阶段的评估方法，用于鲁棒物理对抗性示例，包括实验室和现场测试。使用这种方法，我们评估了物理对抗性操作对真实对象的有效性。仅使用黑白贴纸形式的扰动，我们攻击了一个真实的停车标志，在实验室设置中获得的图像中100%实现了目标误分类，在移动车辆上捕获的视频帧中（现场测试）对目标分类器的误分类率为84.8%。","领域":"对抗性机器学习/安全关键系统/视觉分类","问题":"深度神经网络在现实世界中对对抗性示例的脆弱性","动机":"开发鲁棒的学习算法以应对现实世界中的对抗性攻击","方法":"提出了一种通用的攻击算法，鲁棒物理扰动（RP 2），并采用两阶段评估方法进行测试","关键词":["对抗性示例","鲁棒物理扰动","道路标志分类"],"涉及的技术概念":"对抗性示例是指在输入数据中添加微小扰动以误导深度学习模型的输出。鲁棒物理扰动（RP 2）是一种算法，旨在生成能够在不同物理条件下保持有效的对抗性扰动。两阶段评估方法包括实验室测试和现场测试，用于评估物理对抗性操作的有效性。"},{"order":168,"title":"Generating a Fusion Image: One's Identity and Another's Shape","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Generating_a_Fusion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Generating_a_Fusion_CVPR_2018_paper.html","abstract":"Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset , Eye dataset (MPIIGaze and UnityEyes), and the Photo–Sketch–Cartoon dataset.","中文标题":"生成融合图像：一个人的身份和另一个人的形状","摘要翻译":"通过操作两张输入图像生成一张新颖的图像是生成对抗网络（GANs）研究中的一个有趣的研究问题。我们提出了一种新的基于GAN的网络，该网络生成一张融合图像，该图像具有输入图像x的身份和输入图像y的形状。我们的网络可以以无监督的方式同时在两个以上的图像数据集上进行训练。我们定义了一个身份损失LI来捕捉图像x的身份，以及一个形状损失LS来获取y的形状。此外，我们提出了一种新的训练方法，称为Min-Patch训练，以使生成器专注于图像的关键部分，而不是整个图像。我们在VGG Youtube Pose数据集、Eye数据集（MPIIGaze和UnityEyes）以及照片-素描-卡通数据集上展示了定性结果。","领域":"图像生成/身份识别/形状识别","问题":"如何生成一张融合图像，该图像具有一张图像的身份特征和另一张图像的形状特征","动机":"探索生成对抗网络在图像生成中的应用，特别是如何通过融合不同图像的特征来创造新颖的图像","方法":"提出了一种新的基于GAN的网络，定义了身份损失和形状损失来分别捕捉身份和形状特征，并引入了Min-Patch训练方法来提高生成器对图像关键部分的关注","关键词":["图像生成","身份识别","形状识别","生成对抗网络","无监督学习"],"涉及的技术概念":{"生成对抗网络（GANs）":"一种深度学习模型，通过生成器和判别器的对抗过程来生成数据","身份损失（LI）":"用于捕捉和保持输入图像身份特征的损失函数","形状损失（LS）":"用于捕捉和保持输入图像形状特征的损失函数","Min-Patch训练":"一种训练方法，使生成器专注于图像的关键部分，而不是整个图像"}},{"order":169,"title":"Learning to Promote Saliency Detectors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zeng_Learning_to_Promote_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zeng_Learning_to_Promote_CVPR_2018_paper.html","abstract":"The categories and appearance of salient objects vary from image to image, therefore, saliency detection is an image-specific task. Due to lack of large-scale saliency training data, using deep neural networks (DNNs) with pre-training is difficult to precisely capture the image-specific saliency cues. To solve this issue, we formulate a zero-shot learning problem to promote existing saliency detectors. Concretely, a DNN is trained as an embedding function to map pixels and the attributes of the salient/background regions of an image into the same metric space, in which an image-specific classifier is learned to classify the pixels. Since the image-specific task is performed by the classifier, the DNN embedding effectively plays the role of a general feature extractor. Compared with transferring the learning to a new recognition task using limited data, this formulation makes the DNN learn more effectively from small data. Extensive experiments on five data sets show that our method significantly improves accuracy of existing methods and compares favorably against state-of-the-art approaches.","中文标题":"学习提升显著性检测器","摘要翻译":"显著对象的类别和外观因图像而异，因此，显著性检测是一项图像特定的任务。由于缺乏大规模的显著性训练数据，使用预训练的深度神经网络（DNNs）难以精确捕捉图像特定的显著性线索。为了解决这个问题，我们提出了一个零样本学习问题来提升现有的显著性检测器。具体来说，一个DNN被训练为一个嵌入函数，将像素和图像中显著/背景区域的属性映射到同一度量空间，在该空间中学习一个图像特定的分类器来分类像素。由于图像特定的任务由分类器执行，DNN嵌入有效地扮演了通用特征提取器的角色。与使用有限数据将学习转移到新的识别任务相比，这种表述使DNN能够更有效地从小数据中学习。在五个数据集上的大量实验表明，我们的方法显著提高了现有方法的准确性，并与最先进的方法相比具有优势。","领域":"显著性检测/零样本学习/深度神经网络","问题":"缺乏大规模显著性训练数据，难以精确捕捉图像特定的显著性线索","动机":"提升现有显著性检测器的准确性","方法":"提出一个零样本学习问题，训练DNN作为嵌入函数，将像素和图像中显著/背景区域的属性映射到同一度量空间，学习图像特定的分类器来分类像素","关键词":["显著性检测","零样本学习","深度神经网络"],"涉及的技术概念":"深度神经网络（DNNs）被用作嵌入函数，将图像中的像素和显著/背景区域的属性映射到同一度量空间，以便学习一个图像特定的分类器。这种方法通过零样本学习问题来提升显著性检测器的性能，使得DNN能够更有效地从小数据中学习。"},{"order":170,"title":"Image Super-Resolution via Dual-State Recurrent Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Image_Super-Resolution_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Image_Super-Resolution_via_CVPR_2018_paper.html","abstract":"Advances in image super-resolution (SR) have recently benefited significantly from rapid developments in deep neural networks. Inspired by these recent discoveries, we note that many state-of-the-art deep SR architectures can be reformulated as a single-state recurrent neural network (RNN) with finite unfoldings. In this paper, we explore new structures for SR based on this compact RNN view, leading us to a dual-state design, the Dual-State Recurrent Network (DSRN). Compared to its single-state counterparts that op- erate at a fixed spatial resolution, DSRN exploits both low- resolution (LR) and high-resolution (HR) signals jointly. Recurrent signals are exchanged between these states in both directions (both LR to HR and HR to LR) via de- layed feedback. Extensive quantitative and qualitative eval- uations on benchmark datasets and on a recent challenge demonstrate that the proposed DSRN performs favorably against state-of-the-art algorithms in terms of both mem- ory consumption and predictive accuracy.","中文标题":"通过双状态循环网络实现图像超分辨率","摘要翻译":"图像超分辨率（SR）的进展最近从深度神经网络的快速发展中受益匪浅。受到这些最新发现的启发，我们注意到许多最先进的深度SR架构可以被重新表述为具有有限展开的单状态循环神经网络（RNN）。在本文中，我们基于这种紧凑的RNN视图探索了SR的新结构，引导我们设计出双状态设计，即双状态循环网络（DSRN）。与在固定空间分辨率下操作的单状态对应物相比，DSRN联合利用了低分辨率（LR）和高分辨率（HR）信号。通过延迟反馈，循环信号在这些状态之间双向交换（从LR到HR和从HR到LR）。在基准数据集和最近挑战上的广泛定量和定性评估表明，所提出的DSRN在内存消耗和预测准确性方面均优于最先进的算法。","领域":"图像超分辨率/循环神经网络/深度学习","问题":"提高图像超分辨率的性能","动机":"受到深度神经网络在图像超分辨率领域快速发展的启发，探索基于循环神经网络的新结构以提高性能","方法":"提出双状态循环网络（DSRN），通过联合利用低分辨率和高分辨率信号，并在这些状态之间双向交换循环信号","关键词":["图像超分辨率","循环神经网络","双状态设计"],"涉及的技术概念":"图像超分辨率（SR）是一种提高图像分辨率的技术，旨在从低分辨率图像中恢复出高分辨率图像。循环神经网络（RNN）是一种特殊的神经网络结构，能够处理序列数据，通过其内部状态捕获时间序列中的动态信息。双状态设计指的是在模型中同时考虑和处理两种不同状态（如低分辨率和高分辨率）的信息，通过双向交换循环信号来增强模型的性能。"},{"order":171,"title":"Deep Back-Projection Networks for Super-Resolution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Haris_Deep_Back-Projection_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Haris_Deep_Back-Projection_Networks_CVPR_2018_paper.html","abstract":"The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and down-sampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and down-sampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8x across multiple data sets.","中文标题":"深度反向投影网络用于超分辨率","摘要翻译":"最近提出的深度超分辨率网络的前馈架构学习了低分辨率输入的表示，以及从这些表示到高分辨率输出的非线性映射。然而，这种方法并未完全解决低分辨率和高分辨率图像之间的相互依赖性。我们提出了深度反向投影网络（DBPN），该网络利用迭代的上采样和下采样层，为每个阶段的投影误差提供错误反馈机制。我们构建了相互连接的上采样和下采样阶段，每个阶段代表不同类型的图像退化和高分辨率组件。我们展示了将这一想法扩展到允许跨上采样和下采样阶段的特征连接（密集DBPN）使我们能够进一步改进超分辨率重建，产生卓越的结果，特别是在多个数据集上为8倍等大比例因子建立了新的最先进结果。","领域":"超分辨率/图像重建/深度学习","问题":"解决低分辨率和高分辨率图像之间的相互依赖性","动机":"现有的深度超分辨率网络未能充分利用低分辨率和高分辨率图像之间的相互依赖性，限制了超分辨率重建的效果。","方法":"提出深度反向投影网络（DBPN），通过迭代的上采样和下采样层以及错误反馈机制，构建相互连接的上采样和下采样阶段，以代表不同类型的图像退化和高分辨率组件，并通过特征连接进一步改进超分辨率重建。","关键词":["超分辨率","图像重建","深度学习"],"涉及的技术概念":"深度反向投影网络（DBPN）是一种利用迭代上采样和下采样层以及错误反馈机制来改进超分辨率重建的深度学习架构。通过构建相互连接的上采样和下采样阶段，DBPN能够代表不同类型的图像退化和高分辨率组件，并通过特征连接进一步优化重建效果。"},{"order":172,"title":"Focus Manipulation Detection via Photometric Histogram Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Focus_Manipulation_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Focus_Manipulation_Detection_CVPR_2018_paper.html","abstract":"With the rise of misinformation spread via social media channels, enabled by the increasing automation and realism of image manipulation tools, image forensics is an increasingly relevant problem.  Classic image forensic methods leverage low-level cues such as metadata, sensor noise fingerprints, and others that are easily fooled when the image is re-encoded upon upload to facebook, etc.  This necessitates the use of higher-level physical and semantic cues that, once hard to estimate reliably in the wild, have become more effective due to the increasing power of computer vision.  In particular, we detect  manipulations introduced by artificial blurring of the image, which creates inconsistent photometric relationships between image intensity and various cues.  We achieve 98% accuracy on the most challenging cases in a new dataset of blur manipulations, where the blur is geometrically correct and consistent with the scene's physical arrangement.  Such manipulations are now easily generated, for instance, by smartphone cameras having hardware to measure depth, e.g. \`Portrait Mode' of the iPhone7Plus. We also demonstrate good performance on a challenge dataset evaluating a wider range of manipulations in imagery representing \`in the wild' conditions.","中文标题":"通过光度直方图分析进行焦点操纵检测","摘要翻译":"随着社交媒体渠道上错误信息传播的增加，以及图像处理工具自动化和真实感的提升，图像取证成为了一个日益重要的问题。经典的图像取证方法依赖于低级线索，如元数据、传感器噪声指纹等，这些在图像上传至Facebook等平台时重新编码后容易被欺骗。这需要使用更高级的物理和语义线索，这些线索在野外曾经难以可靠估计，但由于计算机视觉能力的增强，现在变得更加有效。特别是，我们检测由图像人工模糊引入的操纵，这在图像强度和各种线索之间创建了不一致的光度关系。我们在一个新的模糊操纵数据集上实现了98%的准确率，其中模糊在几何上是正确的，并且与场景的物理排列一致。这种操纵现在很容易生成，例如，通过具有测量深度硬件的智能手机相机，如iPhone7Plus的'肖像模式'。我们还在一个挑战数据集上展示了良好的性能，该数据集评估了代表'野外'条件下更广泛范围的图像操纵。","领域":"图像取证/光度分析/模糊检测","问题":"检测图像中的人工模糊操纵","动机":"随着图像处理工具自动化和真实感的提升，以及社交媒体上错误信息传播的增加，需要更有效的方法来检测图像操纵","方法":"利用高级物理和语义线索，特别是通过光度直方图分析检测人工模糊引入的操纵","关键词":["图像取证","光度分析","模糊检测","人工模糊","语义线索"],"涉及的技术概念":"光度直方图分析是一种技术，用于分析图像中光强度与各种线索之间的关系，以检测不一致性，从而识别图像操纵。人工模糊是一种图像处理技术，通过模糊图像的一部分来改变其视觉焦点，这种技术在现代智能手机相机中广泛应用，如iPhone7Plus的'肖像模式'。"},{"order":173,"title":"Compassionately Conservative Balanced Cuts for Image Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cahill_Compassionately_Conservative_Balanced_CVPR_2018_paper.html","abstract":"The Normalized Cut (NCut) objective function, widely used in data clustering and image segmentation, quantifies the cost of graph partitioning in a way that biases clusters or segments that are balanced towards having lower values than unbalanced partitionings. However, this bias is so strong that it avoids any singleton partitions, even when vertices are very weakly connected to the rest of the graph. Motivated by the Buehler-Hein family of balanced cut costs, we propose the family of Compassionately Conservative Balanced (CCB) Cut costs, which are indexed by a parameter that can be used to strike a compromise between the desire to avoid too many singleton partitions and the notion that all partitions should be balanced. We show that CCB-Cut minimization can be relaxed into an orthogonally constrained $ell_{\\tau}$-minimization problem that coincides with the problem of computing Piecewise Flat Embeddings (PFE) for one particular index value, and we present an algorithm for solving the relaxed problem by iteratively minimizing a sequence of reweighted Rayleigh quotients (IRRQ). Using images from the BSDS500 database, we show that image segmentation based on CCB-Cut minimization provides better accuracy with respect to ground truth and greater variability in region size than NCut-based image segmentation.","中文标题":"同情保守平衡切割用于图像分割","摘要翻译":"归一化切割（NCut）目标函数广泛用于数据聚类和图像分割，它量化了图分割的成本，使得偏向于平衡的聚类或分割比不平衡的分割具有更低的值。然而，这种偏向性非常强，以至于它避免了任何单例分割，即使顶点与图的其余部分连接非常弱。受Buehler-Hein家族平衡切割成本的启发，我们提出了同情保守平衡（CCB）切割成本家族，这些成本由一个参数索引，可以用来在避免太多单例分割和所有分割应该平衡的概念之间达成妥协。我们展示了CCB-Cut最小化可以放松为一个正交约束的$ell_{\\\\tau}$-最小化问题，该问题与计算特定索引值的分段平坦嵌入（PFE）问题一致，并且我们提出了一种通过迭代最小化一系列重新加权的Rayleigh商（IRRQ）来解决放松问题的算法。使用BSDS500数据库中的图像，我们展示了基于CCB-Cut最小化的图像分割在相对于地面实况的准确性和区域大小的变异性方面提供了比基于NCut的图像分割更好的结果。","领域":"图像分割/图分割/数据聚类","问题":"归一化切割（NCut）目标函数在图像分割中避免任何单例分割，即使顶点与图的其余部分连接非常弱","动机":"受Buehler-Hein家族平衡切割成本的启发，提出同情保守平衡（CCB）切割成本家族，以在避免太多单例分割和所有分割应该平衡的概念之间达成妥协","方法":"提出了一种通过迭代最小化一系列重新加权的Rayleigh商（IRRQ）来解决放松问题的算法","关键词":["归一化切割","图分割","数据聚类","分段平坦嵌入","Rayleigh商"],"涉及的技术概念":{"归一化切割（NCut）":"一种广泛用于数据聚类和图像分割的目标函数，量化图分割的成本","Buehler-Hein家族平衡切割成本":"一种平衡切割成本家族，用于图分割","同情保守平衡（CCB）切割成本":"一种新的平衡切割成本家族，旨在在避免太多单例分割和所有分割应该平衡的概念之间达成妥协","分段平坦嵌入（PFE）":"一种用于图分割的技术，与CCB-Cut最小化问题一致","迭代最小化重新加权的Rayleigh商（IRRQ）":"一种算法，用于解决CCB-Cut最小化问题"}},{"order":174,"title":"A High-Quality Denoising Dataset for Smartphone Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.html","abstract":"The last decade has seen an astronomical shift from imaging with DSLR and point-and-shoot cameras to imaging with smartphone cameras.  Due to the small aperture and sensor size, smartphone images have notably more noise than their DSLR counterparts.  While denoising for smartphone images is an active research area, the research community currently lacks a denoising image dataset representative of real noisy images from smartphone cameras with high-quality ground truth.  We address this issue in this paper with the following contributions.    We propose a systematic procedure for estimating ground truth for noisy images that can be used to benchmark denoising performance for smartphone cameras.  Using this procedure, we have captured a dataset, the Smartphone Image Denoising Dataset (SIDD), of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images.  We used this dataset to benchmark a number of denoising algorithms.  We show that CNN-based methods perform better when trained on our high-quality dataset than when trained using alternative strategies, such as low-ISO images used as a proxy for ground truth data.","中文标题":"智能手机相机高质量去噪数据集","摘要翻译":"过去十年，从使用DSLR和傻瓜相机成像到使用智能手机相机成像发生了天翻地覆的变化。由于小光圈和传感器尺寸，智能手机图像比DSLR图像有更多的噪声。虽然智能手机图像去噪是一个活跃的研究领域，但研究界目前缺乏一个代表智能手机相机真实噪声图像的高质量去噪图像数据集。我们在本文中通过以下贡献解决了这个问题。我们提出了一种系统的方法来估计噪声图像的真实值，该方法可用于基准测试智能手机相机的去噪性能。使用这种方法，我们捕获了一个数据集，即智能手机图像去噪数据集（SIDD），该数据集包含来自10个场景的约30,000张噪声图像，这些图像是在不同光照条件下使用五种代表性智能手机相机拍摄的，并生成了它们的真实值图像。我们使用这个数据集来基准测试多种去噪算法。我们展示了基于CNN的方法在我们的高质量数据集上训练时比使用其他策略（如使用低ISO图像作为真实值数据的代理）训练时表现更好。","领域":"图像去噪/智能手机成像/深度学习","问题":"智能手机相机图像去噪","动机":"研究界缺乏代表智能手机相机真实噪声图像的高质量去噪图像数据集","方法":"提出了一种系统的方法来估计噪声图像的真实值，并创建了一个包含约30,000张噪声图像的数据集，用于基准测试去噪算法","关键词":["图像去噪","智能手机成像","深度学习"],"涉及的技术概念":{"DSLR":"数码单反相机，一种使用单镜头反射技术的相机，通常提供比智能手机相机更高质量的图像。","智能手机相机":"集成在智能手机中的相机，由于其小光圈和传感器尺寸，通常会产生更多噪声的图像。","去噪":"减少或消除图像中噪声的过程，以提高图像质量。","CNN":"卷积神经网络，一种深度学习模型，特别适用于处理图像数据。","ISO":"国际标准化组织定义的感光度标准，低ISO值通常意味着较少的图像噪声。","SIDD":"智能手机图像去噪数据集，一个包含约30,000张噪声图像及其真实值图像的数据集，用于基准测试去噪算法。"}},{"order":175,"title":"Context-Aware Synthesis for Video Frame Interpolation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Niklaus_Context-Aware_Synthesis_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Niklaus_Context-Aware_Synthesis_for_CVPR_2018_paper.html","abstract":"Video frame interpolation algorithms typically estimate optical flow or its variations and then use it to guide the synthesis of an intermediate frame between two consecutive original frames. To handle challenges like occlusion, bidirectional flow between the two input frames is often estimated and used to warp and blend the input frames. However, how to effectively blend the two warped frames still remains a challenging problem. This paper presents a context-aware synthesis approach that warps not only the input frames but also their pixel-wise contextual information and uses them to interpolate a high-quality intermediate frame. Specifically, we first use a pre-trained neural network to extract per-pixel contextual information for input frames. We then employ a state-of-the-art optical flow algorithm to estimate bidirectional flow between them and pre-warp both input frames and their context maps. Finally, unlike common approaches that blend the pre-warped frames, our method feeds them and their context maps to a video frame synthesis neural network to produce the interpolated frame in a context-aware fashion. Our neural network is fully convolutional and is trained end to end. Our experiments show that our method can handle challenging scenarios such as occlusion and large motion and outperforms representative state-of-the-art approaches.","中文标题":"上下文感知的视频帧插值合成","摘要翻译":"视频帧插值算法通常估计光流或其变体，然后使用它来指导在两个连续原始帧之间合成中间帧。为了处理如遮挡等挑战，通常估计两个输入帧之间的双向光流，并使用它来扭曲和混合输入帧。然而，如何有效地混合两个扭曲的帧仍然是一个具有挑战性的问题。本文提出了一种上下文感知的合成方法，不仅扭曲输入帧，还扭曲它们的像素级上下文信息，并使用它们来插值高质量的中间帧。具体来说，我们首先使用预训练的神经网络提取输入帧的每个像素的上下文信息。然后，我们采用最先进的光流算法估计它们之间的双向光流，并预扭曲输入帧及其上下文图。最后，与常见的混合预扭曲帧的方法不同，我们的方法将它们及其上下文图输入到视频帧合成神经网络中，以上下文感知的方式生成插值帧。我们的神经网络是全卷积的，并且是端到端训练的。我们的实验表明，我们的方法能够处理遮挡和大运动等挑战性场景，并且优于代表性的最先进方法。","领域":"视频处理/光流估计/神经网络","问题":"如何有效地混合两个扭曲的帧以生成高质量的中间帧","动机":"解决视频帧插值中遮挡和大运动等挑战性问题","方法":"使用预训练的神经网络提取像素级上下文信息，采用最先进的光流算法估计双向光流并预扭曲输入帧及其上下文图，最后通过视频帧合成神经网络以上下文感知的方式生成插值帧","关键词":["视频帧插值","光流估计","上下文感知","神经网络"],"涉及的技术概念":"光流估计是一种用于估计图像序列中物体运动的技术，通过分析连续帧之间的像素变化来预测物体的移动方向和速度。上下文感知指的是在图像处理或视频处理中，考虑像素周围的环境信息来改善处理结果。全卷积神经网络是一种特殊的神经网络结构，它能够接受任意尺寸的输入图像，并输出相应尺寸的特征图，常用于图像分割等任务。"},{"order":176,"title":"Salient Object Detection Driven by Fixation Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Salient_Object_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Salient_Object_Detection_CVPR_2018_paper.html","abstract":"Research in visual saliency has been focused on two major types of models namely fixation prediction and salient object detection. The relationship between the two, however, has been less explored. In this paper, we propose to employ the former model type to identify and segment salient objects in scenes. We build a novel neural network called Attentive Saliency Network (ASNet) that learns to detect salient objects from fixation maps. The fixation map, derived at the upper network layers, captures a high-level understanding of the scene. Salient object detection is then viewed as fine-grained object-level saliency segmentation and is progressively optimized with the guidance of the fixation map in a top-down manner. ASNet is based on a hierarchy of convolutional LSTMs (convLSTMs) that offers an efficient recurrent mechanism for sequential refinement of the segmentation map. Several loss functions are introduced for boosting the performance of the ASNet. Extensive experimental evaluation shows that our proposed ASNet is capable of generating accurate segmentation maps with the help of the computed fixation map. Our work offers a deeper insight into the mechanisms of attention and narrows the gap between salient object detection and fixation prediction.","中文标题":"基于注视预测的显著目标检测","摘要翻译":"视觉显著性研究主要集中在两种主要类型的模型上，即注视预测和显著目标检测。然而，这两者之间的关系较少被探索。在本文中，我们提出利用前一种模型类型来识别和分割场景中的显著目标。我们构建了一个名为注意力显著性网络（ASNet）的新型神经网络，该网络学习从注视图中检测显著目标。在网络的较高层导出的注视图捕捉了场景的高级理解。显著目标检测被视为细粒度的目标级显著性分割，并在注视图的指导下以自上而下的方式逐步优化。ASNet基于卷积LSTM（convLSTMs）的层次结构，为分割图的顺序细化提供了有效的递归机制。引入了几个损失函数以提高ASNet的性能。广泛的实验评估表明，我们提出的ASNet能够在计算的注视图的帮助下生成准确的分割图。我们的工作提供了对注意力机制的更深入理解，并缩小了显著目标检测与注视预测之间的差距。","领域":"视觉显著性/显著目标检测/注视预测","问题":"显著目标检测与注视预测之间的关系未被充分探索","动机":"探索并利用注视预测模型来改进显著目标检测","方法":"构建了一个名为注意力显著性网络（ASNet）的新型神经网络，该网络基于卷积LSTM的层次结构，通过注视图指导显著目标检测","关键词":["视觉显著性","显著目标检测","注视预测","注意力显著性网络","卷积LSTM"],"涉及的技术概念":{"注视预测":"一种预测人类在观察图像时可能注视的区域的模型","显著目标检测":"识别和分割图像中最吸引注意力的目标","注意力显著性网络（ASNet）":"一种新型神经网络，用于从注视图中检测显著目标","卷积LSTM（convLSTMs）":"一种结合了卷积神经网络和长短期记忆网络的结构，用于处理序列数据"}},{"order":177,"title":"Enhancing the Spatial Resolution of Stereo Images Using a Parallax Prior","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jeon_Enhancing_the_Spatial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jeon_Enhancing_the_Spatial_CVPR_2018_paper.html","abstract":"We present a novel method that can enhance the spatial resolution of stereo images using a parallax prior. While traditional stereo imaging has focused on estimating depth from stereo images, our method utilizes stereo images to enhance spatial resolution instead of estimating disparity. The critical challenge for enhancing spatial resolution from stereo images: how to register corresponding pixels with subpixel accuracy. Since disparity in traditional stereo imaging is calculated per pixel, it is directly inappropriate for enhancing spatial resolution. We, therefore, learn a parallax prior from stereo image datasets by jointly training two-stage networks. The first network learns how to enhance the spatial resolution of stereo images in luminance, and the second network learns how to reconstruct a high-resolution color image from high-resolution luminance and chrominance of the input image. Our two-stage joint network enhances the spatial resolution of stereo images significantly more than single-image super-resolution methods. The proposed method is directly applicable to any stereo depth imaging methods, enabling us to enhance the spatial resolution of stereo images.","中文标题":"使用视差先验增强立体图像的空间分辨率","摘要翻译":"我们提出了一种新颖的方法，可以利用视差先验来增强立体图像的空间分辨率。虽然传统的立体成像技术主要集中于从立体图像中估计深度，但我们的方法利用立体图像来增强空间分辨率，而不是估计视差。从立体图像中增强空间分辨率的关键挑战在于：如何以亚像素精度注册对应的像素。由于传统立体成像中的视差是按像素计算的，因此直接用于增强空间分辨率是不合适的。因此，我们通过联合训练两阶段网络从立体图像数据集中学习视差先验。第一个网络学习如何增强立体图像的亮度空间分辨率，第二个网络学习如何从输入图像的高分辨率亮度和色度重建高分辨率彩色图像。我们的两阶段联合网络显著增强了立体图像的空间分辨率，超过了单图像超分辨率方法。所提出的方法直接适用于任何立体深度成像方法，使我们能够增强立体图像的空间分辨率。","领域":"立体视觉/超分辨率/图像重建","问题":"如何从立体图像中增强空间分辨率","动机":"传统立体成像技术主要集中于从立体图像中估计深度，而本研究的动机是利用立体图像来增强空间分辨率，而不是估计视差。","方法":"通过联合训练两阶段网络从立体图像数据集中学习视差先验，第一个网络增强立体图像的亮度空间分辨率，第二个网络从高分辨率亮度和色度重建高分辨率彩色图像。","关键词":["立体视觉","超分辨率","图像重建"],"涉及的技术概念":"视差先验、两阶段网络、亮度空间分辨率、色度、高分辨率彩色图像重建"},{"order":178,"title":"HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sironi_HATS_Histograms_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sironi_HATS_Histograms_of_CVPR_2018_paper.html","abstract":"Event-based cameras have recently drawn the attention of the Computer Vision community thanks to their advantages  in terms of high temporal resolution, low power consumption and high dynamic range, compared to traditional frame-based cameras.  These properties make event-based cameras an ideal choice for autonomous vehicles, robot navigation or UAV vision, among others.  However, the accuracy of event-based object classification algorithms,  which is of crucial importance for any reliable system working in real-world conditions,  is still far behind their frame-based counterparts. Two main reasons for this performance gap are:  1. The lack of effective low-level representations and architectures for event-based object classification and  2. The absence of large real-world event-based datasets. In this paper we address both problems.  First, we introduce a novel event-based feature representation together with a new machine learning architecture. Compared to previous approaches, we use local memory units to efficiently leverage past temporal information  and build a robust event-based representation.  Second, we release the first large real-world event-based dataset for object classification. We compare our method to the state-of-the-art with extensive experiments,  showing better classification performance and real-time computation.","中文标题":"HATS：基于平均时间表面直方图的鲁棒事件对象分类","摘要翻译":"基于事件的相机最近引起了计算机视觉社区的关注，这得益于它们在高时间分辨率、低功耗和高动态范围方面的优势，与传统的基于帧的相机相比。这些特性使得基于事件的相机成为自动驾驶车辆、机器人导航或无人机视觉等的理想选择。然而，基于事件的对象分类算法的准确性，对于任何在现实世界条件下工作的可靠系统来说都至关重要，仍然远远落后于基于帧的对应算法。这种性能差距的两个主要原因是：1. 缺乏有效的低级表示和架构用于基于事件的对象分类；2. 缺乏大型的现实世界基于事件的数据集。在本文中，我们解决了这两个问题。首先，我们引入了一种新颖的基于事件的特征表示以及一种新的机器学习架构。与之前的方法相比，我们使用本地记忆单元来有效利用过去的时间信息，并构建一个鲁棒的基于事件的表示。其次，我们发布了第一个大型现实世界基于事件的对象分类数据集。我们通过广泛的实验将我们的方法与最先进的技术进行比较，展示了更好的分类性能和实时计算能力。","领域":"事件相机/对象分类/实时计算","问题":"基于事件的对象分类算法的准确性问题","动机":"提高基于事件的对象分类算法的准确性，以支持自动驾驶车辆、机器人导航或无人机视觉等应用","方法":"引入新颖的基于事件的特征表示和新的机器学习架构，使用本地记忆单元有效利用过去的时间信息，并构建鲁棒的基于事件的表示；发布第一个大型现实世界基于事件的对象分类数据集","关键词":["事件相机","对象分类","实时计算"],"涉及的技术概念":"基于事件的相机：一种能够捕捉高时间分辨率、低功耗和高动态范围图像的相机技术。本地记忆单元：一种用于有效利用过去时间信息的技术，以构建鲁棒的基于事件的表示。"},{"order":179,"title":"A Bi-Directional Message Passing Model for Salient Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_A_Bi-Directional_Message_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_A_Bi-Directional_Message_CVPR_2018_paper.html","abstract":"Recent progress on salient object detection is beneficial from Fully Convolutional Neural Network (FCN). The saliency cues contained in multi-level convolutional features are complementary for detecting salient objects. How to integrate multi-level features becomes an open problem in saliency detection. In this paper, we propose a novel bi-directional message passing model to integrate multi-level features for salient object detection. At first, we adopt a Multi-scale Context-aware Feature Extraction Module (MCFEM) for multi-level feature maps to capture rich context information. Then a bi-directional structure is designed to pass messages between multi-level features, and a gate function is exploited to control the message passing rate. We use the features after message passing, which simultaneously encode semantic information and spatial details, to predict saliency maps. Finally, the predicted results are efficiently combined to generate the final saliency map. Quantitative and qualitative experiments on five benchmark datasets demonstrate that our proposed model performs favorably against the state-of-the-art methods under different evaluation metrics.","中文标题":"双向消息传递模型用于显著目标检测","摘要翻译":"显著目标检测的最新进展得益于全卷积神经网络（FCN）。多级卷积特征中包含的显著性线索对于检测显著目标是互补的。如何整合多级特征成为显著性检测中的一个开放问题。在本文中，我们提出了一种新颖的双向消息传递模型，用于整合多级特征以进行显著目标检测。首先，我们采用多尺度上下文感知特征提取模块（MCFEM）来捕获丰富的上下文信息。然后设计了一个双向结构来在多级特征之间传递消息，并利用门函数来控制消息传递速率。我们使用消息传递后的特征，这些特征同时编码了语义信息和空间细节，来预测显著性图。最后，预测结果被有效地组合以生成最终的显著性图。在五个基准数据集上的定量和定性实验表明，我们提出的模型在不同评估指标下均优于最先进的方法。","领域":"显著目标检测/特征整合/上下文信息捕获","问题":"如何有效整合多级特征以进行显著目标检测","动机":"多级卷积特征中的显著性线索对于检测显著目标是互补的，但如何整合这些特征是一个开放问题","方法":"提出了一种双向消息传递模型，包括多尺度上下文感知特征提取模块（MCFEM）和双向结构，利用门函数控制消息传递速率，最终生成显著性图","关键词":["显著目标检测","特征整合","上下文信息捕获","双向消息传递","多尺度特征提取"],"涉及的技术概念":{"全卷积神经网络（FCN）":"一种用于图像分割的深度学习模型，能够处理任意大小的输入图像并输出相应大小的分割图。","多级卷积特征":"指在卷积神经网络的不同层次提取的特征，这些特征包含了从低级到高级的不同抽象级别的信息。","多尺度上下文感知特征提取模块（MCFEM）":"一种用于捕获图像中多尺度上下文信息的模块，有助于提高显著目标检测的准确性。","双向消息传递":"一种在模型的不同部分之间传递信息的方法，旨在整合来自不同层次的特征信息。","门函数":"用于控制信息流动的机制，可以调节消息传递的速率，确保信息的有效整合。","显著性图":"一种表示图像中哪些区域最吸引人注意力的图，通常用于显著目标检测任务中。"}},{"order":180,"title":"Matching Pixels Using Co-Occurrence Statistics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kat_Matching_Pixels_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kat_Matching_Pixels_Using_CVPR_2018_paper.html","abstract":"We propose a new error measure for matching pixels that is based on co-occurrence statistics. The measure relies on a co-occurrence matrix that counts the number of times pairs of pixel values co-occur within a window. The error incurred by matching a pair of pixels is inverse proportional to the probability that their values co-occur together, and not their color difference. This measure also works with features other than color, e.g. deep features. We show that this improves the state-of-the-art performance of template matching on standard benchmarks.  We then propose an embedding scheme that maps the input image to an embedded image such that the Euclidean distance between pixel values in the embedded space resembles the co-occurrence statistics in the original space. This lets us run existing vision algorithms on the embedded images and enjoy the power of co-occurrence statistics for free. We demonstrate this on two algorithms, the Lucas-Kanade image registration and the Kernelized Correlation Filter (KCF) tracker. Experiments show that performance of each algorithm improves by about 10%.","中文标题":"使用共现统计匹配像素","摘要翻译":"我们提出了一种新的基于共现统计的像素匹配误差度量。该度量依赖于一个共现矩阵，该矩阵计算在窗口内像素值对共现的次数。匹配一对像素所产生的误差与它们的值共现的概率成反比，而不是它们的颜色差异。这种度量也适用于颜色以外的特征，例如深度特征。我们展示了这提高了标准基准上模板匹配的最新性能。然后，我们提出了一种嵌入方案，将输入图像映射到嵌入图像，使得嵌入空间中像素值之间的欧几里得距离类似于原始空间中的共现统计。这使我们能够在嵌入图像上运行现有的视觉算法，并免费享受共现统计的力量。我们在两种算法上展示了这一点，Lucas-Kanade图像配准和核化相关滤波器（KCF）跟踪器。实验表明，每种算法的性能提高了约10%。","领域":"图像配准/目标跟踪/特征匹配","问题":"提高像素匹配的准确性和效率","动机":"为了改进现有视觉算法在像素匹配上的性能，利用共现统计提供更准确的匹配度量","方法":"提出基于共现统计的误差度量方法和一种将输入图像映射到嵌入图像的方案，以利用共现统计改进现有视觉算法","关键词":["共现统计","像素匹配","嵌入图像","Lucas-Kanade图像配准","核化相关滤波器"],"涉及的技术概念":"共现矩阵用于计算像素值对在窗口内共现的次数，嵌入方案通过映射输入图像到嵌入图像，使得嵌入空间中的欧几里得距离反映原始空间中的共现统计，从而改进视觉算法的性能。"},{"order":181,"title":"SeedNet: Automatic Seed Generation With Deep Reinforcement Learning for Robust Interactive Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.html","abstract":"In this paper, we propose an automatic seed generation technique with deep reinforcement learning to solve the interactive segmentation problem. One of the main issues of the interactive segmentation problem is robust and consistent object extraction with less human effort. Most of the existing algorithms highly depend on the distribution of inputs, which differs from one user to another and hence need sequential user interactions to achieve adequate performance. In our system, when a user first specifies a point on the desired object and a point in the background, a sequence of artificial user input is automatically generated for precisely segmenting the desired object. The proposed system allows the user to reduce the number of input significantly. This problem is difficult to cast as a supervised learning problem because it is not possible to define globally optimal user input at some stage of the interactive segmentation task. Hence, we formulate automatic seed generation problem as Markov Decision Process (MDP) and then optimize it by reinforcement learning with Deep Q-Network (DQN). We train our network on the MSRA10K dataset and show that the network achieves notable performance improvement from inaccurate initial segmentation on both seen and unseen datasets.","中文标题":"SeedNet: 使用深度强化学习进行自动种子生成以实现鲁棒的交互式分割","摘要翻译":"在本文中，我们提出了一种使用深度强化学习的自动种子生成技术，以解决交互式分割问题。交互式分割问题的主要问题之一是以较少的人力实现鲁棒且一致的对象提取。大多数现有算法高度依赖于输入的分布，这因用户而异，因此需要顺序的用户交互以达到足够的性能。在我们的系统中，当用户首先在所需对象上指定一个点并在背景中指定一个点时，会自动生成一系列人工用户输入以精确分割所需对象。所提出的系统允许用户显著减少输入次数。由于在交互式分割任务的某个阶段无法定义全局最优的用户输入，因此将此问题视为监督学习问题较为困难。因此，我们将自动种子生成问题表述为马尔可夫决策过程（MDP），然后通过深度Q网络（DQN）的强化学习进行优化。我们在MSRA10K数据集上训练我们的网络，并展示该网络在已见和未见数据集上从不准确的初始分割中实现了显著的性能提升。","领域":"交互式分割/强化学习/图像分割","问题":"实现鲁棒且一致的对象提取，减少用户交互次数","动机":"现有算法高度依赖于输入的分布，需要顺序的用户交互以达到足够的性能","方法":"将自动种子生成问题表述为马尔可夫决策过程（MDP），并通过深度Q网络（DQN）的强化学习进行优化","关键词":["自动种子生成","深度强化学习","交互式分割","马尔可夫决策过程","深度Q网络"],"涉及的技术概念":{"深度强化学习":"一种结合了深度学习和强化学习的方法，用于解决复杂的决策问题","马尔可夫决策过程（MDP）":"一种数学框架，用于建模决策问题，其中结果部分随机，部分受决策者控制","深度Q网络（DQN）":"一种使用深度神经网络来近似Q学习中的Q函数的算法，用于解决高维状态空间的问题","交互式分割":"一种图像分割方法，允许用户通过交互来指导分割过程，以提高分割的准确性"}},{"order":182,"title":"Jerk-Aware Video Acceleration Magnification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Takeda_Jerk-Aware_Video_Acceleration_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Takeda_Jerk-Aware_Video_Acceleration_CVPR_2018_paper.html","abstract":"Video magnification reveals subtle changes invisible to the naked eye, but such tiny yet meaningful changes are often hidden under large motions: small deformation of the muscles in doing sports, or tiny vibrations of strings in ukulele playing. For magnifying subtle changes under large motions, video acceleration magnification method has recently been proposed. This method magnifies subtle acceleration changes and ignores slow large motions. However, quick large motions severely distort this method. In this paper, we present a novel use of jerk to make the acceleration method robust to quick large motions. Jerk has been used to assess smoothness of time series data in the neuroscience and mechanical engineering fields. On the basis of our observation that subtle changes are smoother than quick large motions at temporal scale, we used jerk-based smoothness to design a jerk-aware filter that passes subtle changes only under quick large motions. By applying our filter to the acceleration method, we obtain impressive magnification results better than those obtained with state-of-the-art.","中文标题":"抖动感知的视频加速放大","摘要翻译":"视频放大技术揭示了肉眼无法察觉的细微变化，但这些微小却有意义的变化往往被大动作所掩盖：如运动中肌肉的微小变形，或尤克里里琴弦的微小振动。为了放大在大动作下的细微变化，最近提出了视频加速放大方法。该方法放大了细微的加速度变化并忽略了缓慢的大动作。然而，快速的大动作严重扭曲了这种方法。在本文中，我们提出了一种新颖的使用抖动的方法，使加速度方法对快速大动作具有鲁棒性。抖动已被用于评估神经科学和机械工程领域中时间序列数据的平滑度。基于我们观察到的细微变化在时间尺度上比快速大动作更平滑，我们使用基于抖动的平滑度设计了一个抖动感知滤波器，该滤波器仅在快速大动作下传递细微变化。通过将我们的滤波器应用于加速度方法，我们获得了比现有技术更好的放大效果。","领域":"视频处理/运动分析/信号处理","问题":"在快速大动作下放大视频中的细微变化","动机":"现有的视频加速放大方法在处理快速大动作时会出现严重扭曲，需要一种新的方法来提高其鲁棒性。","方法":"提出了一种使用抖动来设计抖动感知滤波器的方法，该滤波器能够在快速大动作下仅传递细微变化，从而提高视频加速放大方法的鲁棒性。","关键词":["视频放大","抖动感知","加速度放大"],"涉及的技术概念":"抖动（Jerk）是指加速度的变化率，用于评估时间序列数据的平滑度。在本文中，抖动被用来设计一个滤波器，该滤波器能够区分并传递细微变化，同时忽略快速大动作，从而提高视频加速放大方法的效果。"},{"order":183,"title":"Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Defense_Against_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Defense_Against_Adversarial_CVPR_2018_paper.html","abstract":"Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin. footnote{Code: url{https://github.com/lfz/Guided-Denoise}.}","中文标题":"使用高级表示引导去噪器防御对抗攻击","摘要翻译":"神经网络容易受到对抗样本的攻击，这对它们在安全敏感系统中的应用构成了威胁。我们提出了高级表示引导去噪器（HGD）作为图像分类的防御手段。标准的去噪器受到误差放大效应的影响，其中小的残留对抗噪声逐渐被放大并导致错误分类。HGD通过使用定义为由干净图像和去噪图像激活的目标模型输出之间的差异的损失函数来克服这个问题。与在大图像上是最先进的防御方法的集成对抗训练相比，HGD有三个优势。首先，使用HGD作为防御，目标模型对白盒或黑盒对抗攻击更加鲁棒。其次，HGD可以在图像的小子集上训练，并且很好地泛化到其他图像和未见过的类别。第三，HGD可以转移到防御除了引导它的模型之外的其他模型。在NIPS对抗攻击防御竞赛中，我们的HGD解决方案赢得了第一名，并且以很大的优势超过了其他模型。脚注{代码：url{https://github.com/lfz/Guided-Denoise}.}","领域":"对抗样本防御/图像分类/模型鲁棒性","问题":"神经网络对对抗样本的脆弱性","动机":"提高神经网络在安全敏感系统中的鲁棒性","方法":"提出高级表示引导去噪器（HGD），通过定义损失函数来克服误差放大效应","关键词":["对抗样本","图像分类","模型鲁棒性","去噪器"],"涉及的技术概念":{"对抗样本":"对神经网络进行微小修改的输入，旨在误导模型产生错误的输出","误差放大效应":"在去噪过程中，小的残留对抗噪声被逐渐放大，导致错误分类的现象","高级表示引导去噪器（HGD）":"一种新的去噪方法，通过比较干净图像和去噪图像激活的目标模型输出来定义损失函数，以提高模型的鲁棒性","集成对抗训练":"一种防御对抗攻击的方法，通过训练模型来识别和抵抗多种对抗样本"}},{"order":184,"title":"Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.html","abstract":"Understanding shadows from a single image consists of two types of task in previous studies, containing shadow detection and shadow removal. In this paper, we present a multi-task perspective, which is not embraced by any existing work, to jointly learn both detection and removal in an end-to-end fashion that aims at enjoying the mutually improved benefits from each other. Our framework is based on a novel STacked Conditional Generative Adversarial Network (ST-CGAN), which is composed of two stacked CGANs, each with a generator and a discriminator. Specifically, a shadow image is fed into the first generator which produces a shadow detection mask. That shadow image, concatenated with its predicted mask, goes through the second generator in order to recover its shadow-free image consequently. In addition, the two corresponding discriminators are very likely to model higher level relationships and global scene characteristics for the detected shadow region and reconstruction via removing shadows, respectively. More importantly, for multi-task learning, our design of stacked paradigm provides a novel view which is notably different from the commonly used one as the multi-branch version. To fully evaluate the performance of our proposed framework, we construct the first large-scale benchmark with 1870 image triplets (shadow image, shadow mask image, and shadow-free image) under 135 scenes. Extensive experimental results consistently show the advantages of ST-CGAN over several representative state-of-the-art methods on two large-scale publicly available datasets and our newly released one.","中文标题":"堆叠条件生成对抗网络用于联合学习阴影检测和阴影去除","摘要翻译":"在以往的研究中，从单张图像理解阴影包含两种任务，即阴影检测和阴影去除。本文提出了一种多任务视角，这是现有工作中未曾采纳的，旨在以端到端的方式联合学习检测和去除，从而享受彼此带来的相互提升的好处。我们的框架基于一种新颖的堆叠条件生成对抗网络（ST-CGAN），该网络由两个堆叠的CGAN组成，每个CGAN包含一个生成器和一个判别器。具体来说，阴影图像被输入到第一个生成器中，该生成器产生阴影检测掩码。该阴影图像与其预测的掩码连接后，通过第二个生成器以恢复其无阴影图像。此外，两个相应的判别器很可能分别为检测到的阴影区域和通过去除阴影的重建建模更高级别的关系和全局场景特征。更重要的是，对于多任务学习，我们设计的堆叠范式提供了一个新颖的视角，这与常用的多分支版本显著不同。为了全面评估我们提出的框架的性能，我们构建了第一个大规模基准，包含135个场景下的1870个图像三元组（阴影图像、阴影掩码图像和无阴影图像）。广泛的实验结果一致表明，ST-CGAN在两个大规模公开可用数据集和我们新发布的数据集上优于几种代表性的最先进方法。","领域":"阴影检测/阴影去除/生成对抗网络","问题":"如何从单张图像中联合学习阴影检测和阴影去除","动机":"现有工作未采纳多任务视角联合学习阴影检测和去除，旨在通过端到端的方式享受相互提升的好处","方法":"提出了一种基于堆叠条件生成对抗网络（ST-CGAN）的框架，该网络由两个堆叠的CGAN组成，每个CGAN包含一个生成器和一个判别器，通过堆叠范式实现多任务学习","关键词":["阴影检测","阴影去除","生成对抗网络","多任务学习"],"涉及的技术概念":"堆叠条件生成对抗网络（ST-CGAN）是一种由两个堆叠的CGAN组成的网络结构，每个CGAN包含一个生成器和一个判别器，用于从单张图像中联合学习阴影检测和阴影去除。通过这种结构，可以实现端到端的多任务学习，从而在阴影检测和去除任务之间实现相互提升。"},{"order":185,"title":"Image Correction via Deep Reciprocating HDR Transformation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Image_Correction_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Image_Correction_via_CVPR_2018_paper.html","abstract":"Image correction aims to adjust an input image into a visually pleasing one with the detail in the under/over exposed regions recovered. However, existing image correction methods are mainly based on image pixel operations, and attempting to recover the lost detail from these under/over exposed regions is challenging. We, therefore, revisit the image formation procedure and notice that detail is contained in the high dynamic range (HDR) light intensities(which can be perceived by human eyes) but is lost during the nonlinear imaging process by of the camera in the low dynamic range (LDR) domain. Inspired by this observation, we formulate the image correction problem as the Deep Reciprocating HDR Transformation (DRHT) process and propose a novel approach to first reconstruct the lost detail in the HDR domain and then transfer them back to the LDR image as the output image with the recovered detail preserved. To this end, we propose an end-to-end DRHT model, which contains two CNNs, one for HDR detail reconstruction and the other for LDR detail correction. Experiments on the standard benchmarks demonstrate the effectiveness of the proposed method, compared with state-of-the-art image correction methods.","中文标题":"通过深度往复高动态范围变换进行图像校正","摘要翻译":"图像校正旨在将输入图像调整为一个视觉上令人愉悦的图像，并恢复曝光不足/过度区域的细节。然而，现有的图像校正方法主要基于图像像素操作，试图从这些曝光不足/过度区域恢复丢失的细节是具有挑战性的。因此，我们重新审视了图像形成过程，并注意到细节包含在高动态范围（HDR）光强度中（人眼可以感知），但在相机的非线性成像过程中在低动态范围（LDR）域中丢失。受此观察启发，我们将图像校正问题表述为深度往复高动态范围变换（DRHT）过程，并提出了一种新颖的方法，首先在HDR域中重建丢失的细节，然后将它们转移回LDR图像作为输出图像，保留恢复的细节。为此，我们提出了一个端到端的DRHT模型，该模型包含两个CNN，一个用于HDR细节重建，另一个用于LDR细节校正。在标准基准上的实验证明了所提出方法的有效性，与最先进的图像校正方法相比。","领域":"高动态范围成像/图像校正/深度学习","问题":"从曝光不足/过度区域恢复丢失的细节","动机":"现有的图像校正方法主要基于图像像素操作，难以从曝光不足/过度区域恢复丢失的细节","方法":"提出深度往复高动态范围变换（DRHT）过程，使用两个CNN分别进行HDR细节重建和LDR细节校正","关键词":["高动态范围成像","图像校正","深度学习"],"涉及的技术概念":"高动态范围（HDR）光强度、低动态范围（LDR）域、深度往复高动态范围变换（DRHT）、卷积神经网络（CNN）"},{"order":186,"title":"PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Prashnani_PieAPP_Perceptual_Image-Error_CVPR_2018_paper.html","abstract":"The ability to estimate the perceptual error between images is an important problem in computer vision with many applications. Although it has been studied extensively, however, no method currently exists that can robustly predict visual differences like humans. Some previous approaches used hand-coded models, but they fail to model the complexity of the human visual system. Others used machine learning to train models on human-labeled datasets, but creating large, high-quality datasets is difficult because people are unable to assign consistent error labels to distorted images. In this paper, we present a new learning-based method that is the first to predict perceptual image error like human observers. Since it is much easier for people to compare two given images and identify the one more similar to a reference than to assign quality scores to each, we propose a new, large-scale dataset labeled with the probability that humans will prefer one image over another. We then train a deep-learning model using a novel, pairwise-learning framework to predict the preference of one distorted image over the other. Our key observation is that our trained network can then be used separately with only one distorted image and a reference to predict its perceptual error, without ever being trained on explicit human perceptual-error labels. The perceptual error estimated by our new metric, PieAPP, is well-correlated with human opinion. Furthermore, it significantly outperforms existing algorithms, beating the state-of-the-art by almost 3x on our test set in terms of binary error rate, while also generalizing to new kinds of distortions, unlike previous learning-based methods.","中文标题":"PieAPP: 通过成对偏好进行感知图像误差评估","摘要翻译":"估计图像之间的感知误差的能力是计算机视觉中的一个重要问题，具有许多应用。尽管已经进行了广泛的研究，但目前还没有一种方法能够像人类一样稳健地预测视觉差异。一些先前的方法使用了手工编码的模型，但它们未能模拟人类视觉系统的复杂性。其他方法使用机器学习在人类标记的数据集上训练模型，但由于人们无法为失真图像分配一致的误差标签，创建大规模、高质量的数据集是困难的。在本文中，我们提出了一种新的基于学习的方法，这是第一种能够像人类观察者一样预测感知图像误差的方法。由于人们比较两幅给定图像并识别出与参考图像更相似的那一幅比给每幅图像分配质量分数要容易得多，我们提出了一个新的、大规模的数据集，该数据集标记了人类更喜欢一幅图像而不是另一幅的概率。然后，我们使用一种新颖的成对学习框架训练深度学习模型，以预测一幅失真图像相对于另一幅的偏好。我们的关键观察是，我们训练的网络可以单独使用，仅需一幅失真图像和参考图像来预测其感知误差，而无需在明确的人类感知误差标签上进行训练。我们的新度量PieAPP估计的感知误差与人类意见高度相关。此外，它在我们的测试集上显著优于现有算法，在二进制误差率方面几乎比现有技术高出3倍，同时也能够推广到新类型的失真，这与之前基于学习的方法不同。","领域":"图像质量评估/感知误差预测/深度学习","问题":"如何准确预测图像之间的感知误差","动机":"现有方法无法像人类一样稳健地预测视觉差异，且创建高质量的人类标记数据集困难","方法":"提出了一种新的基于学习的方法，通过成对学习框架训练深度学习模型，预测人类对失真图像的偏好，进而估计感知误差","关键词":["图像质量评估","感知误差预测","深度学习","成对学习"],"涉及的技术概念":"感知误差预测是指估计图像之间的视觉差异，类似于人类视觉系统的判断。成对学习框架是一种训练深度学习模型的方法，通过比较两幅图像的相似性来预测人类偏好，而不是直接分配质量分数。PieAPP是一种新的度量标准，用于估计图像的感知误差，与人类意见高度相关。"},{"order":187,"title":"Normalized Cut Loss for Weakly-Supervised CNN Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_Normalized_Cut_Loss_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Normalized_Cut_Loss_CVPR_2018_paper.html","abstract":"Most recent semantic segmentation methods train deep convolutional neural networks with fully annotated masks requiring pixel-accuracy for good quality training. Common weakly-supervised approaches generate full masks from partial input (e.g. scribbles or seeds) using standard interactive segmentation methods as preprocessing. But, errors in such masks result in poorer training since standard loss functions (e.g. cross-entropy) do not distinguish seeds from potentially mislabeled other pixels. Inspired by the general ideas in semi-supervised learning, we address these problems via a new principled loss function evaluating network output with criteria standard in \`\`shallow'' segmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of our loss evaluates only seeds where labels are known while normalized cut softly evaluates consistency of all pixels.  We focus on normalized cut loss where dense Gaussian kernel is efficiently implemented in linear time by fast Bilateral filtering. Our normalized cut loss approach to segmentation brings the quality of weakly-supervised training significantly closer to fully supervised methods.","中文标题":"弱监督CNN分割的归一化割损失","摘要翻译":"最近的语义分割方法训练深度卷积神经网络时，需要完全注释的掩码以达到高质量的训练。常见的弱监督方法使用标准的交互式分割方法作为预处理，从部分输入（例如涂鸦或种子）生成完整的掩码。但是，由于标准损失函数（例如交叉熵）不区分种子和可能错误标记的其他像素，这样的掩码中的错误会导致训练质量较差。受到半监督学习中的一般思想的启发，我们通过一个新的原则性损失函数来解决这些问题，该函数使用“浅层”分割中的标准标准（例如归一化割）来评估网络输出。与之前的工作不同，我们的损失的交叉熵部分仅评估已知标签的种子，而归一化割则软评估所有像素的一致性。我们专注于归一化割损失，其中密集的高斯核通过快速双边滤波在线性时间内高效实现。我们的归一化割损失分割方法使弱监督训练的质量显著接近完全监督方法。","领域":"语义分割/弱监督学习/卷积神经网络","问题":"弱监督语义分割中，由于标准损失函数不区分种子和可能错误标记的其他像素，导致训练质量较差。","动机":"受到半监督学习中的一般思想的启发，解决弱监督语义分割中由于标准损失函数不区分种子和可能错误标记的其他像素导致的训练质量较差的问题。","方法":"提出一个新的原则性损失函数，该函数使用归一化割来评估网络输出，其中交叉熵部分仅评估已知标签的种子，而归一化割则软评估所有像素的一致性。","关键词":["语义分割","弱监督学习","归一化割","卷积神经网络","损失函数"],"涉及的技术概念":{"归一化割":"一种用于图像分割的技术，通过评估像素间的一致性来分割图像。","交叉熵":"一种常用的损失函数，用于评估分类模型的性能。","快速双边滤波":"一种图像处理技术，用于在保持边缘的同时平滑图像。","弱监督学习":"一种机器学习方法，使用不完全或部分注释的数据进行训练。"}},{"order":188,"title":"ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ISTA-Net_Interpretable_Optimization-Inspired_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ISTA-Net_Interpretable_Optimization-Inspired_CVPR_2018_paper.html","abstract":"With the aim of developing a fast yet accurate algorithm for compressive sensing (CS) reconstruction of natural images, we combine in this paper the merits of two existing categories of CS methods: the structure insights of traditional optimization-based methods and the performance/speed of recent network-based ones. Specifically, we propose a novel structured deep network, dubbed ISTA-Net,  which is inspired by the Iterative Shrinkage-Thresholding Algorithm (ISTA) for optimizing a general L1 norm CS reconstruction model. To cast ISTA into deep network form, we develop an effective strategy to solve the proximal mapping associated with the sparsity-inducing regularizer using nonlinear transforms. All the parameters in ISTA-Net (e.g. nonlinear transforms, shrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than being hand-crafted. Moreover, considering that the residuals of natural images are more compressible, an enhanced version of ISTA-Net in the residual domain, dubbed ISTA-Net+, is derived to further improve CS reconstruction. Extensive CS experiments demonstrate that the proposed ISTA-Nets outperform existing state-of-the-art optimization-based and network-based CS methods by large margins, while maintaining fast computational speed.","中文标题":"ISTA-Net：用于图像压缩感知的可解释优化启发深度网络","摘要翻译":"为了开发一种快速而准确的算法用于自然图像的压缩感知（CS）重建，本文结合了现有两类CS方法的优点：传统基于优化方法的结构洞察力和最近基于网络方法的性能/速度。具体来说，我们提出了一种新颖的结构化深度网络，称为ISTA-Net，它受到迭代收缩阈值算法（ISTA）的启发，用于优化一般的L1范数CS重建模型。为了将ISTA转化为深度网络形式，我们开发了一种有效策略，使用非线性变换来解决与稀疏诱导正则化器相关的近端映射。ISTA-Net中的所有参数（例如非线性变换、收缩阈值、步长等）都是端到端学习的，而不是手工设计的。此外，考虑到自然图像的残差更易于压缩，我们推导出了在残差域中的增强版ISTA-Net，称为ISTA-Net+，以进一步提高CS重建。大量的CS实验表明，所提出的ISTA-Nets在保持快速计算速度的同时，大幅优于现有的基于优化和基于网络的最先进CS方法。","领域":"图像压缩感知/深度学习/优化算法","问题":"开发一种快速而准确的算法用于自然图像的压缩感知重建","动机":"结合传统基于优化方法的结构洞察力和基于网络方法的性能/速度，以提高压缩感知重建的效率和准确性","方法":"提出了一种新颖的结构化深度网络ISTA-Net，受到迭代收缩阈值算法（ISTA）的启发，用于优化一般的L1范数CS重建模型，并开发了一种有效策略使用非线性变换解决近端映射问题","关键词":["图像压缩感知","深度学习","优化算法"],"涉及的技术概念":"压缩感知（CS）重建、迭代收缩阈值算法（ISTA）、L1范数优化、非线性变换、近端映射、残差域、端到端学习"},{"order":189,"title":"Fast End-to-End Trainable Guided Filter","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Fast_End-to-End_Trainable_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Fast_End-to-End_Trainable_CVPR_2018_paper.html","abstract":"Image processing and pixel-wise dense prediction have been advanced by harnessing the capabilities of deep learning. One central issue of deep learning is the limited capacity to handle joint upsampling. We present a deep learning building block for joint upsampling, namely guided filtering layer. This layer aims at efficiently generating the high-resolution output given the corresponding low-resolution one and a high-resolution guidance map. The proposed layer is composed of a guided filter, which is reformulated as a fully differentiable block. To this end, we show that a guided filter can be expressed as a group of spatial varying linear transformation matrices. This layer could be integrated with the convolutional neural networks (CNNs) and jointly optimized through end-to-end training. To further take advantage of end-to-end training, we plug in a trainable transformation function that generates task-specific guidance maps. By integrating the CNNs and the proposed layer, we form deep guided filtering networks. The proposed networks are evaluated on five advanced image processing tasks. Experiments on MIT-Adobe FiveK Dataset demonstrate that the proposed approach runs 10-100 times faster and achieves the state-of-the-art performance. We also show that the proposed guided filtering layer helps to improve the performance of multiple pixel-wise dense prediction tasks.","中文标题":"快速端到端可训练的引导滤波器","摘要翻译":"通过利用深度学习的能力，图像处理和像素级密集预测得到了推进。深度学习的一个核心问题是处理联合上采样的能力有限。我们提出了一个用于联合上采样的深度学习构建块，即引导滤波层。该层旨在高效地生成高分辨率输出，给定相应的低分辨率输入和高分辨率引导图。所提出的层由引导滤波器组成，该滤波器被重新表述为一个完全可微分的块。为此，我们展示了引导滤波器可以表示为一组空间变化的线性变换矩阵。该层可以与卷积神经网络（CNNs）集成，并通过端到端训练共同优化。为了进一步利用端到端训练的优势，我们插入了一个可训练的变换函数，该函数生成任务特定的引导图。通过集成CNNs和所提出的层，我们形成了深度引导滤波网络。所提出的网络在五个高级图像处理任务上进行了评估。在MIT-Adobe FiveK数据集上的实验表明，所提出的方法运行速度快10-100倍，并达到了最先进的性能。我们还展示了所提出的引导滤波层有助于提高多个像素级密集预测任务的性能。","领域":"图像上采样/像素级预测/深度学习应用","问题":"深度学习在处理联合上采样方面的能力有限","动机":"提高图像处理和像素级密集预测任务的效率和性能","方法":"提出了一个可端到端训练的引导滤波层，该层可以与卷积神经网络集成，并通过端到端训练共同优化","关键词":["引导滤波","联合上采样","端到端训练","卷积神经网络","像素级密集预测"],"涉及的技术概念":{"引导滤波":"一种用于图像处理的滤波技术，可以根据引导图来调整滤波效果","联合上采样":"一种图像处理技术，旨在从低分辨率图像生成高分辨率图像","端到端训练":"一种训练深度学习模型的方法，允许模型从输入到输出端到端地学习","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","像素级密集预测":"一种图像处理任务，旨在为图像的每个像素生成预测结果"}},{"order":190,"title":"Disentangling Structure and Aesthetics for Style-Aware Image Completion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gilbert_Disentangling_Structure_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gilbert_Disentangling_Structure_and_CVPR_2018_paper.html","abstract":"Content-aware image completion or in-painting is a fundamental tool for the correction of defects or removal of objects in images.  We propose a non-parametric in-painting algorithm that enforces both structural and aesthetic (style) consistency within the resulting image.  Our contributions are two-fold: 1) we explicitly disentangle image structure and style during patch search and selection to ensure a visually consistent look and feel within the target image; 2) we perform adaptive stylization of patches to conform the aesthetics of selected patches to the target image, so harmonising the integration of selected patches into the final composition.  We show that explicit consideration of visual style during in-painting delivers excellent qualitative and quantitative results across the varied image styles and content, over the Places2 photographic dataset and a challenging new in-painting dataset of artwork derived from BAM!","中文标题":"解构结构与美学以实现风格感知的图像补全","摘要翻译":"内容感知的图像补全或修复是用于校正图像缺陷或移除图像中对象的基本工具。我们提出了一种非参数的修复算法，该算法在结果图像中强制执行结构和美学（风格）一致性。我们的贡献有两个方面：1）在补丁搜索和选择过程中，我们明确解构图像的结构和风格，以确保目标图像内的视觉一致性和感觉；2）我们对补丁进行自适应风格化，以使所选补丁的美学符合目标图像，从而协调所选补丁与最终构图的整合。我们展示了在修复过程中明确考虑视觉风格可以在不同的图像风格和内容上提供出色的定性和定量结果，这些结果基于Places2摄影数据集和来自BAM!的具有挑战性的新修复艺术作品数据集。","领域":"图像修复/风格迁移/视觉一致性","问题":"如何在图像修复过程中同时保持结构和美学的一致性","动机":"为了在图像修复过程中不仅保持结构的一致性，还要确保修复后的图像在视觉风格上与原始图像保持一致，从而提高修复结果的自然度和美观度。","方法":"提出了一种非参数的修复算法，通过明确解构图像的结构和风格，在补丁搜索和选择过程中确保视觉一致性，并对补丁进行自适应风格化以符合目标图像的美学。","关键词":["图像修复","风格迁移","视觉一致性"],"涉及的技术概念":"非参数修复算法、图像结构和风格解构、补丁搜索和选择、自适应风格化、视觉一致性"},{"order":191,"title":"Learning a Discriminative Feature Network for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_a_Discriminative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Learning_a_Discriminative_CVPR_2018_paper.html","abstract":"Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.","中文标题":"学习用于语义分割的判别特征网络","摘要翻译":"大多数现有的语义分割方法仍然面临两个方面的挑战：类内不一致性和类间不明确性。为了解决这两个问题，我们提出了一个判别特征网络（DFN），它包含两个子网络：平滑网络和边界网络。具体来说，为了处理类内不一致性问题，我们特别设计了一个带有通道注意力块和全局平均池化的平滑网络，以选择更具判别性的特征。此外，我们提出了一个边界网络，通过深度语义边界监督使边界两侧的特征可区分。基于我们提出的DFN，我们在PASCAL VOC 2012数据集上达到了86.2%的平均IOU，在Cityscapes数据集上达到了80.3%的平均IOU，实现了最先进的性能。","领域":"语义分割/图像理解/特征学习","问题":"解决语义分割中的类内不一致性和类间不明确性问题","动机":"提高语义分割的准确性和效率，通过设计更有效的网络结构来处理现有方法中的挑战","方法":"提出了一个包含平滑网络和边界网络的判别特征网络（DFN），其中平滑网络通过通道注意力块和全局平均池化选择更具判别性的特征，边界网络通过深度语义边界监督使边界两侧的特征可区分","关键词":["语义分割","判别特征网络","通道注意力块","全局平均池化","深度语义边界监督"],"涉及的技术概念":{"语义分割":"一种图像分割技术，旨在将图像分割成多个区域或对象，并为每个区域或对象分配一个语义标签。","判别特征网络（DFN）":"一种网络结构，旨在通过特定的子网络设计来解决语义分割中的特定问题。","通道注意力块":"一种机制，用于增强网络对重要特征的关注，通过调整通道的权重来实现。","全局平均池化":"一种池化操作，用于减少特征图的空间维度，同时保留重要的特征信息。","深度语义边界监督":"一种监督学习方法，用于通过深度网络提供的语义信息来指导边界特征的区分。"}},{"order":192,"title":"Kernelized Subspace Pooling for Deep Local Descriptors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Kernelized_Subspace_Pooling_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Kernelized_Subspace_Pooling_CVPR_2018_paper.html","abstract":"Representing local image patches in an invariant and discriminative manner is an active research topic in computer vision. It has recently been demonstrated that local feature learning based on deep Convolutional Neural Network (CNN) can significantly improve the matching performance. Previous works on learning such descriptors have focused on developing various loss functions, regularizations and data mining strategies to learn discriminative CNN representations. Such methods, however, have little analysis on how to increase geometric invariance of their generated descriptors. In this paper, we propose a descriptor that has both highly invariant and discriminative power. The abilities come from a novel pooling method, dubbed Subspace Pooling (SP) which is invariant to a range of geometric deformations. To further increase the discriminative power of our descriptor, we propose a simple distance kernel integrated to the marginal triplet loss that helps to focus on hard examples in CNN training. Finally, we show that by combining SP with the projection distance metric, the generated feature descriptor is equivalent to that of the Bilinear CNN model, but outperforms the latter with much lower memory and computation consumptions. The proposed method is simple, easy to understand and achieves good performance. Experimental results on several patch matching benchmarks show that our method outperforms the state-of-the-arts significantly.","中文标题":"深度局部描述符的核化子空间池化","摘要翻译":"以不变且具有区分性的方式表示局部图像块是计算机视觉中的一个活跃研究课题。最近已经证明，基于深度卷积神经网络（CNN）的局部特征学习可以显著提高匹配性能。先前关于学习此类描述符的工作主要集中在开发各种损失函数、正则化和数据挖掘策略，以学习具有区分性的CNN表示。然而，这些方法很少分析如何增加其生成描述符的几何不变性。在本文中，我们提出了一种既具有高度不变性又具有区分能力的描述符。这种能力来自于一种新颖的池化方法，称为子空间池化（SP），它对一系列几何变形具有不变性。为了进一步提高我们描述符的区分能力，我们提出了一种简单的距离核，集成到边际三元组损失中，有助于在CNN训练中专注于困难样本。最后，我们展示了通过将SP与投影距离度量相结合，生成的特征描述符与双线性CNN模型的描述符等效，但在内存和计算消耗方面优于后者。所提出的方法简单、易于理解，并且实现了良好的性能。在几个图像块匹配基准上的实验结果表明，我们的方法显著优于现有技术。","领域":"局部特征描述/图像匹配/卷积神经网络","问题":"如何提高局部图像块描述符的几何不变性和区分能力","动机":"现有的局部特征学习方法在提高描述符的几何不变性方面分析不足，需要一种新的方法来同时提高描述符的不变性和区分能力。","方法":"提出了一种新颖的子空间池化（SP）方法，该方法对几何变形具有不变性，并通过集成一个简单的距离核到边际三元组损失中来提高描述符的区分能力。","关键词":["局部特征描述","图像匹配","卷积神经网络","子空间池化","几何不变性"],"涉及的技术概念":"子空间池化（SP）是一种新颖的池化方法，旨在提高局部图像块描述符的几何不变性。边际三元组损失是一种用于训练卷积神经网络的损失函数，通过集成距离核来专注于困难样本，从而提高描述符的区分能力。双线性CNN模型是一种用于图像特征提取的模型，本文提出的方法在性能上优于该模型，同时减少了内存和计算资源的消耗。"},{"order":193,"title":"pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_pOSE_Pseudo_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_pOSE_Pseudo_Object_CVPR_2018_paper.html","abstract":"Bundle adjustment is a nonlinear refinement method for camera poses and 3D structure requiring sufficiently good initialization. In recent years, it was experimentally observed that useful minima can be reached even from arbitrary initialization for affine bundle adjustment problems (and fixed-rank matrix factorization instances in general). The key success factor lies in the use of the variable projection (VarPro) method, which is known to have a wide basin of convergence for such problems. In this paper, we propose the Pseudo Object Space Error (pOSE), which is an objective with cameras represented as a hybrid between the affine and projective models. This formulation allows us to obtain 3D reconstructions that are close to the true projective reconstructions while retaining a bilinear problem structure suitable for the VarPro method. Experimental results show that using pOSE has a high success rate to yield faithful 3D reconstructions from random initializations, taking one step towards initialization-free structure from motion.","中文标题":"pOSE：用于无需初始化的束调整的伪对象空间误差","摘要翻译":"束调整是一种用于相机姿态和3D结构的非线性优化方法，需要足够好的初始化。近年来，实验观察到，即使从任意初始化开始，对于仿射束调整问题（以及一般的固定秩矩阵分解实例）也可以达到有用的最小值。成功的关键在于使用了变量投影（VarPro）方法，该方法已知对此类问题具有广泛的收敛域。在本文中，我们提出了伪对象空间误差（pOSE），这是一种将相机表示为仿射和投影模型之间混合的目标。这种表述使我们能够获得接近真实投影重建的3D重建，同时保留适合VarPro方法的双线性问题结构。实验结果表明，使用pOSE从随机初始化开始产生忠实的3D重建的成功率很高，朝着无需初始化的运动结构迈出了一步。","领域":"3D重建/运动结构/仿射束调整","问题":"解决束调整方法中需要良好初始化的问题","动机":"探索在无需良好初始化的条件下，通过改进目标函数实现有效的3D重建","方法":"提出伪对象空间误差（pOSE）作为新的目标函数，结合仿射和投影模型的优点，利用变量投影（VarPro）方法进行优化","关键词":["3D重建","运动结构","仿射束调整","变量投影"],"涉及的技术概念":"束调整是一种优化相机姿态和3D结构的非线性方法；变量投影（VarPro）方法是一种优化技术，具有广泛的收敛域；伪对象空间误差（pOSE）是一种新的目标函数，结合了仿射和投影模型的优点，用于3D重建。"},{"order":194,"title":"Deformable Shape Completion With Graph Convolutional Autoencoders","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Litany_Deformable_Shape_Completion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Litany_Deformable_Shape_Completion_CVPR_2018_paper.html","abstract":"The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learningbased method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality.","中文标题":"可变形形状补全与图卷积自编码器","摘要翻译":"价格合理且便携的深度传感器的可用性使得扫描物体和人比以往任何时候都更简单。然而，处理遮挡和缺失部分仍然是一个重大挑战。从单个或多个部分扫描重建（可能非刚性移动的）3D物体的问题近年来受到了越来越多的关注。在这项工作中，我们提出了一种新颖的基于学习的方法来完成部分形状。与大多数现有方法不同，我们的方法专注于可以经历非刚性变形的物体。我们方法的核心是一个带有图卷积操作的变分自编码器，它学习了一个用于完整现实形状的潜在空间。在推理时，我们优化以找到这个潜在空间中的表示，使生成的形状最适合已知的部分输入。完成的形状在未知部分展现出逼真的外观。我们展示了在完成人体和面部网格的合成和真实扫描方面有希望的结果，这些网格展示了不同的关节和部分性风格。","领域":"3D重建/形状补全/图卷积网络","问题":"从单个或多个部分扫描重建可能非刚性移动的3D物体","动机":"处理遮挡和缺失部分，完成部分形状的重建","方法":"提出了一种基于学习的变分自编码器方法，使用图卷积操作学习完整现实形状的潜在空间，并在推理时优化以找到最适合已知部分输入的潜在空间表示","关键词":["3D重建","形状补全","图卷积网络","变分自编码器"],"涉及的技术概念":"图卷积操作是一种在图结构数据上进行的卷积操作，能够捕捉节点之间的关系。变分自编码器是一种生成模型，通过学习数据的潜在表示来生成新的数据样本。"},{"order":195,"title":"Learning From Millions of 3D Scans for Large-Scale 3D Face Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gilani_Learning_From_Millions_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gilani_Learning_From_Millions_CVPR_2018_paper.html","abstract":"Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.","中文标题":"从数百万次3D扫描中学习用于大规模3D人脸识别","摘要翻译":"在数百万张面部图像上训练的深度网络被认为在人脸识别方面接近人类水平。然而，开放世界的人脸识别仍然是一个挑战。尽管3D人脸识别相比2D有其固有的优势，但由于缺乏大规模的训练和测试数据集，它并未从深度学习的最新发展中受益。由于现有3D人脸数据集的小规模画廊，识别准确率已经饱和。与2D照片不同，3D面部扫描无法从网络上获取，这导致了深度3D人脸识别网络和数据集发展的瓶颈。在此背景下，我们提出了一种方法，用于生成大量标记的3D人脸身份及其多个实例用于训练，并提出了一种协议，用于合并最具挑战性的现有3D数据集进行测试。我们还提出了第一个专门为3D人脸识别设计的深度CNN模型，并在100K身份的3.1百万次3D面部扫描上进行了训练。我们的测试数据集包括1,853个身份，每个身份在画廊中有一个3D扫描，另外还有31K扫描作为探针，这比现有数据集大几个数量级。在没有对此数据集进行微调的情况下，我们的网络已经比最先进的人脸识别技术高出10%以上。我们在画廊集上微调我们的网络，以执行端到端的大规模3D人脸识别，这进一步提高了准确性。最后，我们展示了我们的方法在开放世界人脸识别问题上的有效性。","领域":"3D人脸识别/深度学习/大规模数据集","问题":"开放世界3D人脸识别的挑战","动机":"由于缺乏大规模的训练和测试数据集，3D人脸识别未能从深度学习的最新发展中受益","方法":"提出了一种生成大量标记3D人脸身份及其多个实例的方法，并设计了第一个专门为3D人脸识别设计的深度CNN模型","关键词":["3D人脸识别","深度学习","大规模数据集"],"涉及的技术概念":"深度CNN模型、3D面部扫描、开放世界人脸识别、大规模数据集、端到端学习"},{"order":196,"title":"CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.html","abstract":"Despite significant research in the area, reconstruction of multiple dynamic rigid objects (eg. vehicles) observed from   wide-baseline, uncalibrated and unsynchronized cameras, remains hard. On one hand, feature tracking works well within each view but is hard to correspond across multiple cameras with limited overlap in fields of view or due to occlusions. On the other hand, advances in deep learning have resulted in strong detectors that work across different viewpoints but are still not precise enough for triangulation-based reconstruction. In this work, we develop a framework to fuse both the single-view feature tracks and multi-view detected part locations to significantly improve the detection, localization and reconstruction of moving vehicles, even in the presence of strong occlusions. We demonstrate our framework at a busy traffic intersection by reconstructing over 62 vehicles passing within a 3-minute window. We evaluate the different components within our framework and compare to alternate  approaches such as reconstruction using tracking-by-detection.","中文标题":"CarFusion: 结合点跟踪和部件检测进行车辆动态3D重建","摘要翻译":"尽管在该领域进行了大量研究，但从宽基线、未校准和未同步的摄像机观察到的多个动态刚性物体（例如车辆）的重建仍然很困难。一方面，特征跟踪在单个视图中效果良好，但由于视野重叠有限或遮挡，很难在多个摄像机之间对应。另一方面，深度学习的进展导致了强大的检测器，这些检测器在不同的视角下工作，但对于基于三角测量的重建来说仍然不够精确。在这项工作中，我们开发了一个框架，融合了单视图特征跟踪和多视图检测到的部件位置，以显著提高移动车辆的检测、定位和重建，即使在存在强烈遮挡的情况下也是如此。我们通过在一个繁忙的交通路口重建在3分钟窗口内通过的62辆以上的车辆来展示我们的框架。我们评估了我们框架中的不同组件，并与使用检测跟踪的重建等替代方法进行了比较。","领域":"动态3D重建/车辆检测/多视图融合","问题":"从宽基线、未校准和未同步的摄像机观察到的多个动态刚性物体的重建","动机":"提高在存在强烈遮挡情况下的移动车辆的检测、定位和重建","方法":"融合单视图特征跟踪和多视图检测到的部件位置","关键词":["动态3D重建","车辆检测","多视图融合"],"涉及的技术概念":"特征跟踪、深度学习检测器、三角测量重建、检测跟踪"},{"order":197,"title":"Deep Material-Aware Cross-Spectral Stereo Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhi_Deep_Material-Aware_Cross-Spectral_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhi_Deep_Material-Aware_Cross-Spectral_CVPR_2018_paper.html","abstract":"Cross-spectral imaging provides strong benefits for recognition and detection tasks. Often, multiple cameras are used for cross-spectral imaging, thus requiring image alignment, or disparity estimation in a stereo setting. Increasingly, multi-camera cross-spectral systems are embedded in active RGBD devices (e.g. RGB-NIR cameras in Kinect and iPhone X). Hence, stereo matching also provides an opportunity to obtain depth without an active projector source. However, matching images from different spectral bands is challenging because of large appearance variations. We develop a novel deep learning framework to simultaneously transform images across spectral bands and estimate disparity. A material-aware loss function is incorporated within the disparity prediction network to handle regions with unreliable matching such as light sources, glass windshields and glossy surfaces. No depth supervision is required by our method. To evaluate our method, we used a vehicle-mounted RGB-NIR stereo system to collect 13.7 hours of video data across a range of areas in and around a city. Experiments show that our method achieves strong performance and reaches real-time speed.","中文标题":"深度材料感知跨光谱立体匹配","摘要翻译":"跨光谱成像为识别和检测任务提供了强大的优势。通常，使用多个相机进行跨光谱成像，因此需要图像对齐或在立体设置中进行视差估计。越来越多的多相机跨光谱系统被嵌入到主动RGBD设备中（例如Kinect和iPhone X中的RGB-NIR相机）。因此，立体匹配也提供了无需主动投影源即可获得深度的机会。然而，由于外观变化大，匹配来自不同光谱带的图像具有挑战性。我们开发了一种新颖的深度学习框架，可以同时跨光谱带转换图像并估计视差。在视差预测网络中加入了材料感知损失函数，以处理光源、玻璃挡风玻璃和光泽表面等匹配不可靠的区域。我们的方法不需要深度监督。为了评估我们的方法，我们使用车载RGB-NIR立体系统收集了城市内外多个区域的13.7小时视频数据。实验表明，我们的方法实现了强大的性能并达到了实时速度。","领域":"跨光谱成像/立体视觉/深度估计","问题":"跨光谱带图像匹配和深度估计","动机":"解决跨光谱成像中由于外观变化大导致的图像匹配难题，以及无需主动投影源即可获得深度的需求","方法":"开发了一种深度学习框架，同时进行跨光谱带图像转换和视差估计，并引入材料感知损失函数处理匹配不可靠的区域","关键词":["跨光谱成像","立体匹配","深度估计","材料感知损失函数"],"涉及的技术概念":"跨光谱成像涉及使用不同光谱带的图像进行识别和检测；立体视觉涉及从多个视角的图像中估计深度或视差；深度估计是指从图像中推断出场景的深度信息；材料感知损失函数是一种用于处理图像中特定材料区域（如光源、玻璃挡风玻璃和光泽表面）匹配问题的损失函数。"},{"order":198,"title":"Augmenting Crowd-Sourced 3D Reconstructions Using Semantic Detections","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Price_Augmenting_Crowd-Sourced_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Price_Augmenting_Crowd-Sourced_3D_CVPR_2018_paper.html","abstract":"Image-based 3D reconstruction for Internet photo collections has become a robust technology to produce impressive virtual representations of real-world scenes. However, several fundamental challenges remain for Structure-from-Motion (SfM) pipelines, namely: the placement and reconstruction of transient objects only observed in single views, estimating the absolute scale of the scene, and (suprisingly often) recovering ground surfaces in the scene. We propose a method to jointly address these remaining open problems of SfM. In particular, we focus on detecting people in individual images and accurately placing them into an existing 3D model. As part of this placement, our method also estimates the absolute scale of the scene from object semantics, which in this case constitutes the height distribution of the population. Further, we obtain a smooth approximation of the ground surface and recover the gravity vector of the scene directly from the individual person detections. We demonstrate the results of our approach on a number of unordered Internet photo collections, and we quantitatively evaluate the obtained absolute scene scales.","中文标题":"利用语义检测增强众包3D重建","摘要翻译":"基于图像的互联网照片集合的3D重建已成为一种强大的技术，能够产生令人印象深刻的现实世界场景的虚拟表示。然而，对于从运动中恢复结构（SfM）的流程，仍然存在几个基本挑战，即：仅在单一视图中观察到的瞬态物体的放置和重建，估计场景的绝对比例，以及（令人惊讶地经常）恢复场景中的地面。我们提出了一种方法，共同解决SfM的这些剩余开放问题。特别是，我们专注于在单个图像中检测人，并准确地将他们放置到现有的3D模型中。作为这种放置的一部分，我们的方法还从对象语义中估计场景的绝对比例，在这种情况下，这构成了人口的高度分布。此外，我们从单个人员检测中直接获得地面的平滑近似，并恢复场景的重力向量。我们在多个无序的互联网照片集合上展示了我们方法的结果，并对获得的绝对场景比例进行了定量评估。","领域":"3D重建/语义检测/场景理解","问题":"解决从运动中恢复结构（SfM）流程中的瞬态物体放置和重建、场景绝对比例估计及地面恢复问题","动机":"提高3D重建技术的准确性和实用性，特别是在处理无序互联网照片集合时","方法":"通过检测单个图像中的人并准确放置到现有3D模型中，从对象语义估计场景绝对比例，并从单个人员检测中恢复地面和重力向量","关键词":["3D重建","语义检测","场景理解","绝对比例估计","地面恢复"],"涉及的技术概念":"从运动中恢复结构（SfM）是一种从2D图像序列中恢复3D结构的技术。语义检测指的是通过图像识别技术识别图像中的对象及其属性。绝对比例估计是指确定场景中物体的大小和距离的真实比例。地面恢复是指从图像中识别并重建场景的地面。重力向量是指场景中重力方向的三维表示。"},{"order":199,"title":"Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Richter_Matryoshka_Networks_Predicting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Richter_Matryoshka_Networks_Predicting_CVPR_2018_paper.html","abstract":"In this paper, we develop novel, efficient 2D encodings for 3D geometry, which enable reconstructing full 3D shapes from a single image at high resolution. The key idea is to pose 3D shape reconstruction as a 2D prediction problem. To that end, we first develop a simple baseline network that predicts entire voxel tubes at each pixel of a reference view. By leveraging well-proven architectures for 2D pixel-prediction tasks, we attain state-of-the-art results, clearly outperforming purely voxel-based approaches. We scale this baseline to higher resolutions by proposing a memory-efficient shape encoding, which recursively decomposes a 3D shape into nested shape layers, similar to the pieces of a Matryoshka doll. This allows reconstructing highly detailed shapes with complex topology, as demonstrated in extensive experiments; we clearly outperform previous octree-based approaches despite having a much simpler architecture using standard network components. Our Matryoshka networks further enable reconstructing shapes from IDs or shape similarity, as well as shape sampling.","中文标题":"嵌套形状层网络：通过嵌套形状层预测3D几何","摘要翻译":"在本文中，我们开发了新颖、高效的3D几何2D编码，这使得从单一图像高分辨率重建完整3D形状成为可能。关键思想是将3D形状重建作为一个2D预测问题提出。为此，我们首先开发了一个简单的基线网络，该网络在参考视图的每个像素处预测整个体素管。通过利用在2D像素预测任务中经过验证的架构，我们达到了最先进的结果，明显优于纯粹的体素基础方法。我们通过提出一种内存高效的形状编码，将这个基线扩展到更高分辨率，该编码递归地将3D形状分解为嵌套的形状层，类似于俄罗斯套娃的碎片。这允许重建具有复杂拓扑的高度详细形状，正如在广泛的实验中所展示的；尽管使用标准网络组件具有更简单的架构，我们明显优于以前的基于八叉树的方法。我们的嵌套形状层网络进一步使得从ID或形状相似性重建形状以及形状采样成为可能。","领域":"3D重建/形状编码/图像处理","问题":"从单一图像高分辨率重建完整3D形状","动机":"提高3D形状重建的效率和分辨率，超越现有方法","方法":"开发了一种内存高效的形状编码，递归地将3D形状分解为嵌套的形状层，并利用2D像素预测任务的架构","关键词":["3D重建","形状编码","高分辨率"],"涉及的技术概念":"2D编码、3D几何、体素管、嵌套形状层、俄罗斯套娃、八叉树、形状采样"},{"order":200,"title":"Triplet-Center Loss for Multi-View 3D Object Retrieval","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_Triplet-Center_Loss_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Triplet-Center_Loss_for_CVPR_2018_paper.html","abstract":"Most existing 3D object recognition algorithms  focus on leveraging the strong discriminative power of deep learning models with softmax loss for the classification of 3D data,  while learning discriminative features with deep metric learning for 3D object retrieval  is more or less neglected. In the paper,  we  study variants of deep metric learning losses for  3D object retrieval, which did not receive enough attention from this area. First , two kinds of representative losses, triplet loss and center loss,  are introduced which could  learn more discriminative features than traditional classification loss. Then we propose a novel loss named triplet-center loss, which can further enhance the discriminative power of the features. The proposed triplet-center loss learns a center for each class and requires that the distances between samples and centers from the same class are closer than those from different classes. Extensive experimental results on two popular 3D object retrieval benchmarks and two widely-adopted sketch-based 3D shape retrieval benchmarks consistently demonstrate the effectiveness of our proposed loss, and significant improvements have been achieved compared to the state-of-the-arts.","中文标题":"用于多视图3D物体检索的三元组中心损失","摘要翻译":"大多数现有的3D物体识别算法侧重于利用深度学习模型的强大判别能力与softmax损失进行3D数据的分类，而利用深度度量学习学习判别特征进行3D物体检索则或多或少被忽视。在本文中，我们研究了用于3D物体检索的深度度量学习损失的变体，这一领域并未得到足够的关注。首先，介绍了两种代表性的损失，三元组损失和中心损失，它们可以比传统的分类损失学习到更具判别性的特征。然后，我们提出了一种名为三元组中心损失的新损失，它可以进一步增强特征的判别能力。提出的三元组中心损失为每个类别学习一个中心，并要求来自同一类别的样本与中心之间的距离比来自不同类别的更近。在两个流行的3D物体检索基准和两个广泛采用的基于草图的3D形状检索基准上的大量实验结果一致证明了我们提出的损失的有效性，并且与最先进的技术相比，取得了显著的改进。","领域":"3D物体检索/深度度量学习/特征学习","问题":"3D物体检索中深度度量学习损失的应用不足","动机":"提高3D物体检索中特征的判别能力","方法":"提出了一种新的损失函数——三元组中心损失，通过为每个类别学习一个中心，并要求来自同一类别的样本与中心之间的距离比来自不同类别的更近，来增强特征的判别能力","关键词":["3D物体检索","深度度量学习","三元组中心损失","特征学习"],"涉及的技术概念":{"三元组损失":"一种深度度量学习损失，通过比较正样本对和负样本对之间的距离来学习特征","中心损失":"一种深度度量学习损失，通过最小化每个样本与其类别中心之间的距离来学习特征","三元组中心损失":"本文提出的新损失函数，结合了三元组损失和中心损失的特点，旨在进一步增强特征的判别能力"}},{"order":201,"title":"Learning 3D Shape Completion From Laser Scan Data With Weak Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Stutz_Learning_3D_Shape_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Stutz_Learning_3D_Shape_CVPR_2018_paper.html","abstract":"3D shape completion from partial point clouds is a fundamental problem in computer vision and computer graphics. Recent approaches can be characterized as either data-driven or learning-based. Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations. Learning-based approaches, in contrast, avoid the expensive optimization step and instead directly predict the complete shape from the incomplete observations using deep neural networks. However, full supervision is required which is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. Tackling 3D shape completion of cars on ShapeNet and KITTI, we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and a state-of-the-art data-driven approach while being significantly faster. On ModelNet, we additionally show that the approach is able to generalize to other object categories as well.","中文标题":"从激光扫描数据中学习3D形状补全：弱监督方法","摘要翻译":"从部分点云中完成3D形状补全是计算机视觉和计算机图形学中的一个基本问题。最近的方法可以分为数据驱动或基于学习的方法。数据驱动方法依赖于形状模型，其参数被优化以拟合观察结果。相比之下，基于学习的方法避免了昂贵的优化步骤，而是直接使用深度神经网络从不完整的观察中预测完整的形状。然而，这需要完全监督，而这在实践中往往不可用。在这项工作中，我们提出了一种弱监督的基于学习的方法来进行3D形状补全，该方法既不需要缓慢的优化，也不需要直接监督。虽然我们也在合成数据上学习形状先验，但我们使用深度神经网络来分摊，即学习，最大似然拟合，从而在不牺牲准确性的情况下实现高效的形状补全。通过在ShapeNet和KITTI上处理汽车的3D形状补全，我们证明了所提出的分摊最大似然方法能够与完全监督的基线和最先进的数据驱动方法竞争，同时显著更快。在ModelNet上，我们还展示了该方法能够泛化到其他对象类别。","领域":"3D形状补全/激光扫描数据处理/弱监督学习","问题":"从部分点云中完成3D形状补全","动机":"现有的3D形状补全方法需要完全监督，这在实际应用中往往不可用，因此需要一种不需要完全监督的方法。","方法":"提出了一种弱监督的基于学习的方法，使用深度神经网络分摊最大似然拟合，实现高效的3D形状补全。","关键词":["3D形状补全","弱监督学习","深度神经网络","最大似然拟合"],"涉及的技术概念":"3D形状补全是指从部分点云数据中预测完整的3D形状。弱监督学习是一种机器学习方法，它不需要完全标注的数据。深度神经网络是一种模拟人脑结构和功能的计算模型，用于处理和分析大量数据。最大似然拟合是一种统计方法，用于估计模型参数，使得观察到的数据在该模型下的概率最大。"},{"order":202,"title":"End-to-End Learning of Keypoint Detector and Descriptor for Pose Invariant 3D Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Georgakis_End-to-End_Learning_of_CVPR_2018_paper.html","abstract":"Finding correspondences between images or 3D scans is at the heart of many computer vision and image retrieval applications and is often enabled by matching local keypoint descriptors. Various learning approaches have been applied in the past to different stages of the matching pipeline, considering detection, description, or metric learning objectives. These objectives were typically addressed separately and most previous work has focused on image data.  This paper proposes an end-to-end learning framework for keypoint detection and its representation (descriptor) for 3D depth maps or 3D scans, where the two can be jointly optimized towards task-specific objectives without a need for separate annotations. We employ a Siamese architecture augmented by a sampling layer and a novel score loss function which in turn affects the selection of region proposals. The positive and negative examples are obtained automatically by sampling corresponding region proposals based on their consistency with known 3D pose labels. Matching experiments with depth data on multiple benchmark datasets demonstrate the efficacy of the proposed approach, showing significant improvements over state-of-the-art methods.","中文标题":"端到端学习关键点检测器和描述符以实现姿态不变的3D匹配","摘要翻译":"在图像或3D扫描之间找到对应关系是许多计算机视觉和图像检索应用的核心，通常通过匹配局部关键点描述符来实现。过去，各种学习方法已被应用于匹配流程的不同阶段，考虑到检测、描述或度量学习目标。这些目标通常是单独解决的，大多数先前的工作都集中在图像数据上。本文提出了一种端到端的学习框架，用于3D深度图或3D扫描的关键点检测及其表示（描述符），其中两者可以针对特定任务目标联合优化，而无需单独的注释。我们采用了一种由采样层和一种新颖的得分损失函数增强的Siamese架构，这反过来影响了区域提议的选择。正负样本通过基于与已知3D姿态标签的一致性采样相应的区域提议自动获得。在多个基准数据集上的深度数据匹配实验证明了所提出方法的有效性，显示出相对于最先进方法的显著改进。","领域":"3D视觉/关键点检测/描述符学习","问题":"如何在3D深度图或3D扫描中实现姿态不变的关键点检测和描述符学习","动机":"为了在3D视觉应用中提高关键点检测和描述符匹配的准确性和效率，需要一种能够联合优化检测和描述符学习的方法","方法":"提出了一种端到端的学习框架，采用Siamese架构，通过采样层和一种新颖的得分损失函数来增强，自动获取正负样本，并在多个基准数据集上进行深度数据匹配实验","关键词":["3D视觉","关键点检测","描述符学习","Siamese架构","得分损失函数"],"涉及的技术概念":"Siamese架构是一种用于比较两个输入的神经网络架构，常用于学习相似性度量。得分损失函数是一种用于优化模型预测与真实标签之间差异的函数，这里特指用于影响区域提议选择的新颖损失函数。3D姿态标签指的是3D对象在空间中的位置和方向信息。"},{"order":203,"title":"ICE-BA: Incremental, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.html","abstract":"Modern visual-inertial SLAM (VI-SLAM) achieves higher accuracy and robustness than pure visual SLAM, thanks to the complementariness of visual features and inertial measurements. However, jointly using visual and inertial measurements to optimize SLAM objective functions is a problem of high computational complexity. In many VI-SLAM applications, the conventional optimization solvers can only use a very limited number of recent measurements for real time pose estimation, at the cost of suboptimal localization accuracy. In this work, we renovate the numerical solver for VI-SLAM. Compared to conventional solvers, our proposal provides an exact solution with significantly higher computational efficiency. Our solver allows us to use remarkably larger number of measurements to achieve higher accuracy and robustness. Furthermore, our method resolves the global consistency problem that is unaddressed by many state-of-the-art SLAM systems: to guarantee the minimization of re-projection function and inertial constraint function during loop closure. Experiments demonstrate our novel formulation renders lower localization error and more than 10x speedup compared to alternatives. We release the source code of our implementation to benefit the community.","中文标题":"ICE-BA: 视觉-惯性SLAM的增量、一致和高效捆绑调整","摘要翻译":"现代视觉-惯性SLAM（VI-SLAM）由于视觉特征和惯性测量的互补性，比纯视觉SLAM实现了更高的精度和鲁棒性。然而，联合使用视觉和惯性测量来优化SLAM目标函数是一个高计算复杂度的问题。在许多VI-SLAM应用中，传统的优化求解器只能使用非常有限的最近测量值进行实时姿态估计，以牺牲次优的定位精度为代价。在这项工作中，我们革新了VI-SLAM的数值求解器。与传统的求解器相比，我们的提议提供了显著更高计算效率的精确解。我们的求解器允许我们使用显著更多的测量值来实现更高的精度和鲁棒性。此外，我们的方法解决了许多最先进的SLAM系统未解决的全局一致性问题：在闭环期间保证重投影函数和惯性约束函数的最小化。实验表明，与替代方案相比，我们的新公式实现了更低的定位误差和超过10倍的速度提升。我们发布了我们实现的源代码，以造福社区。","领域":"视觉-惯性SLAM/数值优化/实时定位","问题":"高计算复杂度的视觉-惯性SLAM目标函数优化问题","动机":"提高视觉-惯性SLAM的精度和鲁棒性，同时解决全局一致性问题","方法":"革新VI-SLAM的数值求解器，提供计算效率更高的精确解，允许使用更多测量值，解决全局一致性问题","关键词":["视觉-惯性SLAM","数值优化","实时定位","全局一致性","计算效率"],"涉及的技术概念":{"视觉-惯性SLAM":"结合视觉和惯性测量进行同时定位与地图构建的技术","数值优化":"通过数学方法寻找最优解的过程","实时定位":"在动态环境中实时确定位置的技术","全局一致性":"在SLAM系统中，确保整个地图和轨迹的一致性和准确性","计算效率":"算法执行过程中资源使用的效率，包括时间和空间"}},{"order":204,"title":"GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.html","abstract":"We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.","中文标题":"GeoNet：无监督学习密集深度、光流和相机姿态","摘要翻译":"我们提出了GeoNet，一个联合无监督学习框架，用于从视频中估计单目深度、光流和自我运动。这三个组件通过3D场景几何的本质耦合，由我们的框架以端到端的方式联合学习。具体来说，几何关系在单个模块的预测上被提取，然后作为图像重建损失结合起来，分别对静态和动态场景部分进行推理。此外，我们提出了一种自适应几何一致性损失，以增加对异常值和非朗伯区域的鲁棒性，这有效地解决了遮挡和纹理模糊问题。在KITTI驾驶数据集上的实验表明，我们的方案在所有三个任务中都达到了最先进的结果，表现优于之前的无监督方法，并与有监督方法相当。","领域":"深度估计/光流估计/自我运动估计","问题":"从视频中无监督地估计单目深度、光流和自我运动","动机":"通过联合学习框架，利用3D场景几何的本质，提高深度、光流和自我运动估计的准确性和鲁棒性","方法":"提出GeoNet框架，通过端到端的方式联合学习深度、光流和自我运动估计，利用几何关系提取和图像重建损失，以及自适应几何一致性损失来提高鲁棒性","关键词":["无监督学习","深度估计","光流估计","自我运动估计","3D场景几何","图像重建损失","自适应几何一致性损失"],"涉及的技术概念":"GeoNet是一个无监督学习框架，用于从视频中估计单目深度、光流和自我运动。它通过3D场景几何的本质耦合这三个组件，并以端到端的方式联合学习。框架提取几何关系作为图像重建损失，并分别推理静态和动态场景部分。此外，引入自适应几何一致性损失以提高对异常值和非朗伯区域的鲁棒性，有效解决遮挡和纹理模糊问题。"},{"order":205,"title":"Radially-Distorted Conjugate Translations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pritts_Radially-Distorted_Conjugate_Translations_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pritts_Radially-Distorted_Conjugate_Translations_CVPR_2018_paper.html","abstract":"This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Gr{\\\\\\"o}bner-basis method are stable, small and fast. The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation.","中文标题":"径向失真共轭平移","摘要翻译":"本文介绍了首个联合解决从共面重复模式中获取仿射校正和径向镜头失真的最小解算器。即使使用中等失真镜头的图像，使用针孔相机模型进行平面校正也是不准确或无效的。所提出的解算器将镜头失真纳入相机模型，并将精确校正扩展到广角图像，这在消费级相机中现已常见。这些解算器源自成像场景平面的共轭平移所诱导的约束，这些约束与径向镜头失真的分割模型相结合。使用理想饱和度的隐藏变量技巧重新表述约束，使得由Gröbner基方法生成的解算器稳定、小巧且快速。所提出的解算器用于基于RANSAC的估计器中。从一个共轭平移的仿射协变特征或两个独立平移的相似协变特征中恢复校正和镜头失真。实验证实，RANSAC以极少的迭代次数准确估计了校正和径向失真。所提出的解算器在仿射校正和径向失真估计方面与最先进技术进行了评估。","领域":"相机校正/图像失真校正/广角图像处理","问题":"解决从共面重复模式中获取仿射校正和径向镜头失真的问题","动机":"即使使用中等失真镜头的图像，使用针孔相机模型进行平面校正也是不准确或无效的，需要一种新的方法来提高校正的准确性和有效性","方法":"将镜头失真纳入相机模型，使用共轭平移的约束与径向镜头失真的分割模型相结合，采用隐藏变量技巧重新表述约束，使用Gröbner基方法生成解算器，并在RANSAC估计器中应用","关键词":["相机校正","图像失真校正","广角图像处理"],"涉及的技术概念":"共轭平移、径向镜头失真、仿射校正、针孔相机模型、Gröbner基方法、RANSAC估计器、隐藏变量技巧、理想饱和度"},{"order":206,"title":"Deep Ordinal Regression Network for Monocular Depth Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.html","abstract":"Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed prob- lem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multi- layer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, i.e., KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin.","中文标题":"用于单目深度估计的深度序数回归网络","摘要翻译":"单目深度估计在理解3D场景几何中扮演着关键角色，是一个不适定问题。最近的方法通过探索来自深度卷积神经网络（DCNNs）的图像级信息和层次特征，取得了显著改进。这些方法将深度估计建模为回归问题，并通过最小化均方误差来训练回归网络，这导致收敛速度慢和局部解不理想。此外，现有的深度估计网络采用重复的空间池化操作，导致不理想的低分辨率特征图。为了获得高分辨率的深度图，需要跳跃连接或多层反卷积网络，这使网络训练复杂化并消耗更多计算。为了消除或至少大幅减少这些问题，我们引入了一种间距增加的离散化（SID）策略来离散化深度，并将深度网络学习重新定义为序数回归问题。通过使用序数回归损失训练网络，我们的方法在同步中实现了更高的准确性和更快的收敛。此外，我们采用了一种多尺度网络结构，避免了不必要的空间池化，并并行捕获多尺度信息。所提出的深度序数回归网络（DORN）在三个具有挑战性的基准测试中，即KITTI [16]、Make3D [49]和NYU Depth v2 [41]，取得了最先进的结果，并大幅超越了现有方法。","领域":"单目深度估计/3D场景理解/深度卷积神经网络","问题":"单目深度估计中的收敛速度慢和局部解不理想问题","动机":"提高单目深度估计的准确性和收敛速度，同时减少计算消耗","方法":"引入间距增加的离散化（SID）策略，将深度网络学习重新定义为序数回归问题，并采用多尺度网络结构","关键词":["单目深度估计","序数回归","多尺度网络"],"涉及的技术概念":{"单目深度估计":"从单一图像中估计场景的深度信息","序数回归":"一种回归方法，用于处理有序类别的问题","多尺度网络":"一种网络结构，能够同时处理不同尺度的信息","间距增加的离散化（SID）":"一种策略，用于将连续的深度值离散化为有序的类别"}},{"order":207,"title":"Analytical Modeling of Vanishing Points and Curves in Catadioptric Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Miraldo_Analytical_Modeling_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Miraldo_Analytical_Modeling_of_CVPR_2018_paper.html","abstract":"Vanishing points and vanishing lines are classical geometrical concepts in perspective cameras that have a lineage dating back to 3 centuries. A vanishing point is a point on the image space where parallel lines in 3D space appear to converge, whereas a vanishing line passes through 2 or more vanishing points. While such concepts are simple and intuitive in perspective cameras, their counterparts in catadioptric cameras (obtained using mirrors and lenses) are more involved. For example, lines in the 3D space map to higher degree curves in catadioptric cameras. The projection of a set of 3D parallel lines converges on a single point in perspective images, whereas they converge to more than one point in catadioptric cameras. To the best of our knowledge, we are not aware of any systematic development of analytical models for vanishing points and vanishing curves in different types of catadioptric cameras. In this paper, we derive parametric equations for vanishing points and vanishing curves using the calibration parameters, mirror shape coefficients, and direction vectors of parallel lines in 3D space. We show compelling experimental results on vanishing point estimation and absolute pose estimation for a wide variety of catadioptric cameras in both simulations and real experiments.","中文标题":"反射相机中消失点和消失曲线的解析建模","摘要翻译":"消失点和消失线是透视相机中的经典几何概念，其历史可以追溯到三个世纪前。消失点是图像空间中的一个点，在3D空间中的平行线似乎在此点汇聚，而消失线则穿过两个或更多的消失点。虽然在透视相机中这些概念简单直观，但在反射相机（使用镜子和透镜获得）中它们的对应物则更为复杂。例如，3D空间中的线在反射相机中映射为更高阶的曲线。一组3D平行线在透视图像中的投影汇聚于一个点，而在反射相机中它们则汇聚于多个点。据我们所知，我们不知道有任何关于不同类型反射相机中消失点和消失曲线的解析模型的系统开发。在本文中，我们使用校准参数、镜子形状系数和3D空间中平行线的方向向量推导出消失点和消失曲线的参数方程。我们在模拟和实际实验中展示了关于消失点估计和绝对姿态估计的引人注目的实验结果，适用于各种反射相机。","领域":"几何视觉/相机校准/3D重建","问题":"反射相机中消失点和消失曲线的解析建模","动机":"解决反射相机中消失点和消失曲线的复杂性问题，填补解析模型开发的空白","方法":"使用校准参数、镜子形状系数和3D空间中平行线的方向向量推导出消失点和消失曲线的参数方程","关键词":["消失点","消失曲线","反射相机","解析建模","相机校准"],"涉及的技术概念":"消失点和消失线是透视相机中的经典几何概念，反射相机通过使用镜子和透镜捕捉图像，其几何特性与透视相机不同，导致3D空间中的平行线在反射相机中映射为更高阶的曲线，并可能汇聚于多个点。本文通过推导参数方程，解决了反射相机中消失点和消失曲线的解析建模问题。"},{"order":208,"title":"Learning Depth From Monocular Videos Using Direct Methods","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Depth_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_Depth_From_CVPR_2018_paper.html","abstract":"The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.","中文标题":"使用直接方法从单目视频中学习深度","摘要翻译":"利用CNN的最新进展从单张图像预测深度的能力越来越受到视觉社区的关注。无监督学习策略特别吸引人，因为它们可以在学习过程中利用更大和更多样化的单目视频数据集，而无需地面真实深度或立体信息。在以前的工作中，必须确定单独的姿势和深度CNN预测器，以便它们的联合输出最小化光度误差。受到直接视觉里程计（DVO）最新进展的启发，我们认为可以在没有姿势CNN预测器的情况下学习深度CNN预测器。此外，我们通过实验证明，结合DVO的可微分实现以及一种新颖的深度归一化策略，显著提高了使用单目视频进行训练的最先进技术的性能。","领域":"深度估计/单目视觉/视觉里程计","问题":"如何从单目视频中无监督地学习深度信息","动机":"利用无监督学习策略从单目视频中预测深度，无需地面真实深度或立体信息，可以处理更大和更多样化的数据集","方法":"提出了一种无需姿势CNN预测器即可学习深度CNN预测器的方法，并采用直接视觉里程计（DVO）的可微分实现和深度归一化策略来提高性能","关键词":["深度估计","单目视觉","视觉里程计","无监督学习","CNN"],"涉及的技术概念":"CNN（卷积神经网络）用于深度预测，直接视觉里程计（DVO）用于改进深度预测的准确性，深度归一化策略用于提高模型性能"},{"order":209,"title":"Salience Guided Depth Calibration for Perceptually Optimized Compressive Light Field 3D Display","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Salience_Guided_Depth_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Salience_Guided_Depth_CVPR_2018_paper.html","abstract":"Multi-layer light field displays are a type of computational three-dimensional (3D) display which has recently gained increasing interest for its holographic-like effect and natural compatibility with 2D displays. However, the major shortcoming, depth limitation, still cannot be overcome in the traditional light field modeling and reconstruction based on multi-layer liquid crystal displays (LCDs). Considering this disadvantage, our paper incorporates a salience guided depth optimization over a limited display range to calibrate the displayed depth and present the maximum area of salience region for multi-layer light field display. Different from previously reported cascaded light field displays that use the fixed initialization plane as the depth center of display content, our method automatically calibrates the depth initialization based on the salience results derived from the proposed contrast enhanced salience detection method. Experiments demonstrate that the proposed method provides a promising advantage in visual perception for the compressive light field displays from both software simulation and prototype demonstration.","中文标题":"显著性引导的深度校准用于感知优化的压缩光场3D显示","摘要翻译":"多层光场显示器是一种计算三维（3D）显示器，最近因其类似全息的效果和与2D显示器的自然兼容性而受到越来越多的关注。然而，基于多层液晶显示器（LCDs）的传统光场建模和重建仍然无法克服其主要缺点，即深度限制。考虑到这一缺点，我们的论文在有限的显示范围内引入了显著性引导的深度优化，以校准显示的深度并呈现多层光场显示的最大显著性区域。与之前报道的使用固定初始化平面作为显示内容深度中心的级联光场显示器不同，我们的方法基于提出的对比度增强显著性检测方法得出的显著性结果自动校准深度初始化。实验表明，从软件模拟和原型演示来看，所提出的方法为压缩光场显示器在视觉感知方面提供了显著的优势。","领域":"光场显示/三维显示/视觉感知优化","问题":"传统多层液晶显示器在光场建模和重建中的深度限制问题","动机":"克服传统光场显示器在深度表现上的限制，提升视觉感知效果","方法":"引入显著性引导的深度优化，自动校准深度初始化","关键词":["光场显示","三维显示","视觉感知优化","深度校准","显著性检测"],"涉及的技术概念":"多层光场显示器、计算三维显示、多层液晶显示器（LCDs）、显著性引导的深度优化、对比度增强显著性检测方法、压缩光场显示"},{"order":210,"title":"MegaDepth: Learning Single-View Depth Prediction From Internet Photos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_MegaDepth_Learning_Single-View_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_MegaDepth_Learning_Single-View_CVPR_2018_paper.html","abstract":"Single-view depth prediction is a fundamental problem in computer vision. Recently, deep learning methods have led to significant progress, but such methods are limited by the available training data. Current datasets based on 3D sensors have key limitations, including indoor-only images (NYU), small numbers of training examples (Make3D), and sparse sampling (KITTI). We propose to use multi-view Internet photo collections, a virtually unlimited data source, to generate training data via modern structure-from-motion and multi-view stereo (MVS) methods, and present a large depth dataset called MegaDepth based on this idea. Data derived from MVS comes with its own challenges, including noise and unreconstructable objects. We address these challenges with new data cleaning methods, as well as automatically augmenting our data with ordinal depth relations generated using semantic segmentation. We validate the use of large amounts of Internet data by showing that models trained on MegaDepth exhibit strong generalization—not only to novel scenes, but also to other diverse datasets including Make3D, KITTI, and DIW, even when no images from those datasets are seen during training.","中文标题":"MegaDepth: 从互联网照片学习单视图深度预测","摘要翻译":"单视图深度预测是计算机视觉中的一个基本问题。最近，深度学习方法取得了显著进展，但这些方法受到可用训练数据的限制。当前基于3D传感器的数据集存在关键限制，包括仅限于室内图像（NYU）、训练样本数量少（Make3D）和稀疏采样（KITTI）。我们提出使用多视图互联网照片集合，这是一个几乎无限的数据源，通过现代的结构从运动和多视图立体（MVS）方法生成训练数据，并基于这一想法提出了一个名为MegaDepth的大型深度数据集。从MVS导出的数据有其自身的挑战，包括噪声和无法重建的对象。我们通过新的数据清理方法以及使用语义分割生成的顺序深度关系自动增强我们的数据来解决这些挑战。我们通过展示在MegaDepth上训练的模型表现出强大的泛化能力——不仅对新场景，而且对其他多样化的数据集包括Make3D、KITTI和DIW，即使在这些数据集的图像在训练期间未被看到的情况下，验证了大量互联网数据的使用。","领域":"深度估计/三维重建/语义分割","问题":"单视图深度预测的训练数据限制","动机":"解决现有数据集在室内图像、训练样本数量和稀疏采样方面的限制，利用互联网照片集合生成更丰富的训练数据","方法":"使用多视图互联网照片集合，通过结构从运动和多视图立体方法生成训练数据，并采用新的数据清理方法和语义分割生成的顺序深度关系自动增强数据","关键词":["深度估计","三维重建","语义分割"],"涉及的技术概念":"单视图深度预测是指从单一视角的图像中预测场景的深度信息。结构从运动（Structure-from-Motion, SfM）是一种从一系列图像中重建三维场景结构的技术。多视图立体（Multi-View Stereo, MVS）是一种利用多张图像来重建三维场景的技术。语义分割是一种将图像分割成多个区域或对象的技术，每个区域或对象对应于特定的语义类别。"},{"order":211,"title":"LayoutNet: Reconstructing the 3D Room Layout From a Single RGB Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zou_LayoutNet_Reconstructing_the_CVPR_2018_paper.html","abstract":"We propose an algorithm to predict room layout from a single image that generalizes across panoramas and perspective images, cuboid layouts and more general layouts (e.g. \\"L\\"-shape room). Our method operates directly on the panoramic image, rather than decomposing into perspective images as do recent works.  Our network architecture is similar to that of RoomNet, but we show improvements due to aligning the image based on vanishing points, predicting multiple layout elements (corners, boundaries, size and translation), and fitting a constrained Manhattan layout to the resulting predictions. Our method compares well in speed and accuracy to other existing work on panoramas, achieves among the best accuracy for perspective images, and can handle both cuboid-shaped and more general Manhattan layouts.","中文标题":"LayoutNet: 从单一RGB图像重建3D房间布局","摘要翻译":"我们提出了一种算法，用于从单一图像预测房间布局，该算法能够泛化应用于全景图和透视图、立方体布局以及更一般的布局（例如“L”形房间）。我们的方法直接在全景图像上操作，而不是像最近的工作那样将其分解为透视图。我们的网络架构与RoomNet类似，但我们展示了由于基于消失点对齐图像、预测多个布局元素（角落、边界、大小和平移）以及对结果预测拟合受限的曼哈顿布局而带来的改进。我们的方法在全景图的速度和准确性方面与其他现有工作相比表现良好，在透视图方面达到了最佳准确性之一，并且能够处理立方体形状和更一般的曼哈顿布局。","领域":"三维重建/室内场景理解/全景图像处理","问题":"从单一RGB图像中准确预测和重建3D房间布局","动机":"为了克服现有方法在处理不同类型图像（如全景图和透视图）和不同房间布局（如立方体布局和更复杂的“L”形房间布局）时的局限性，提出一种更通用、更准确的房间布局预测算法。","方法":"直接在全景图像上操作，通过基于消失点对齐图像、预测多个布局元素（包括角落、边界、大小和平移），并拟合一个受限的曼哈顿布局到预测结果中，从而改进房间布局的预测准确性。","关键词":["三维重建","室内场景理解","全景图像处理","曼哈顿布局","消失点"],"涉及的技术概念":"全景图像处理涉及将全景图直接用于房间布局预测，而不是先将其分解为透视图。曼哈顿布局是一种假设房间的墙壁与三个正交方向之一对齐的布局模型。消失点是指图像中平行线在透视投影下看起来会聚的点，用于对齐图像和预测房间布局。"},{"order":212,"title":"CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Batsos_CBMV_A_Coalesced_CVPR_2018_paper.html","abstract":"Recently, there has been a paradigm shift in stereo matching with learning-based methods achieving the best results on all popular benchmarks. The success of these methods is due to the availability of training data with ground truth; training learning-based systems on these datasets has allowed them to surpass the accuracy of conventional approaches based on heuristics and assumptions. Many of these assumptions, however, had been validated extensively and hold for the majority of possible inputs. In this paper, we generate a matching volume leveraging both data with ground truth and conventional wisdom. We accomplish this by coalescing diverse evidence from a bidirectional matching process via random forest classifiers. We show that the resulting matching volume estimation method achieves similar accuracy to purely data-driven alternatives on benchmarks and that it  generalizes to unseen data much better. In fact, the results we submitted to the KITTI benchmarks were generated using a classifier trained on the Middlebury dataset.","中文标题":"CBMV: 一种用于视差估计的融合双向匹配体积","摘要翻译":"最近，立体匹配领域出现了范式转变，基于学习的方法在所有流行基准测试中取得了最佳结果。这些方法的成功归功于带有真实标签的训练数据的可用性；在这些数据集上训练基于学习的系统使它们超越了基于启发式和假设的传统方法的准确性。然而，这些假设中的许多已经被广泛验证，并且适用于大多数可能的输入。在本文中，我们利用带有真实标签的数据和传统智慧生成一个匹配体积。我们通过随机森林分类器融合来自双向匹配过程的不同证据来实现这一点。我们展示了所得到的匹配体积估计方法在基准测试中达到了与纯数据驱动替代方案相似的准确性，并且它在未见数据上的泛化能力更强。事实上，我们提交给KITTI基准测试的结果是使用在Middlebury数据集上训练的分类器生成的。","领域":"立体匹配/视差估计/随机森林","问题":"提高立体匹配的准确性和泛化能力","动机":"结合传统方法和基于学习的方法的优势，以提高立体匹配的准确性和泛化能力","方法":"通过随机森林分类器融合双向匹配过程中的不同证据，生成匹配体积","关键词":["立体匹配","视差估计","随机森林"],"涉及的技术概念":{"立体匹配":"一种用于从两个或多个视角的图像中估计深度信息的技术","视差估计":"通过比较两个视角的图像中对应点的位置差异来估计深度","随机森林":"一种集成学习方法，通过构建多个决策树来进行分类或回归"}},{"order":213,"title":"Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pang_Zoom_and_Learn_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pang_Zoom_and_Learn_CVPR_2018_paper.html","abstract":"Despite the recent success of stereo matching with convolutional neural networks (CNNs), it remains arduous to generalize a pre-trained deep stereo model to a novel domain. A major difficulty is to collect accurate ground-truth disparities for stereo pairs in the target domain. In this work, we propose a self-adaptation approach for CNN training, utilizing both synthetic training data (with ground-truth disparities) and stereo pairs in the new domain (without ground-truths). Our method is driven by two empirical observations. By feeding real stereo pairs of different domains to stereo models pre-trained with synthetic data, we see that: i) a pre-trained model does not generalize well to the new domain, producing artifacts at boundaries and ill-posed regions; however, ii) feeding an up-sampled stereo pair leads to a disparity map with extra details. To avoid i) while exploiting ii), we formulate an iterative optimization problem with graph Laplacian regularization. At each iteration, the CNN adapts itself better to the new domain: we let the CNN learn its own higher-resolution output; at the meanwhile, a graph Laplacian regularization is imposed to discriminatively keep the desired edges while smoothing out the artifacts. We demonstrate the effectiveness of our method in two domains: daily scenes collected by smartphone cameras, and street views captured in a driving car.","中文标题":"缩放与学习：将深度立体匹配推广到新领域","摘要翻译":"尽管最近使用卷积神经网络（CNNs）在立体匹配方面取得了成功，但将预训练的深度立体模型推广到新领域仍然困难重重。一个主要困难是收集目标领域中立体对的准确地面真实视差。在这项工作中，我们提出了一种自适应的CNN训练方法，利用合成训练数据（带有地面真实视差）和新领域中的立体对（没有地面真实视差）。我们的方法受到两个经验观察的驱动。通过将不同领域的真实立体对输入到用合成数据预训练的立体模型中，我们发现：i）预训练模型不能很好地推广到新领域，在边界和不适定区域产生伪影；然而，ii）输入上采样的立体对会导致带有额外细节的视差图。为了避免i）同时利用ii），我们提出了一个带有图拉普拉斯正则化的迭代优化问题。在每次迭代中，CNN更好地适应新领域：我们让CNN学习其自己的高分辨率输出；同时，施加图拉普拉斯正则化以区分性地保留所需的边缘，同时平滑伪影。我们在两个领域展示了我们方法的有效性：智能手机相机收集的日常场景和驾驶汽车中捕获的街景。","领域":"立体视觉/自适应学习/图像处理","问题":"如何将预训练的深度立体模型推广到新领域","动机":"收集目标领域中立体对的准确地面真实视差困难","方法":"提出了一种自适应的CNN训练方法，利用合成训练数据和新领域中的立体对，通过迭代优化问题与图拉普拉斯正则化来适应新领域","关键词":["立体匹配","卷积神经网络","自适应学习","图拉普拉斯正则化"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据。","立体匹配":"通过比较两个或多个视角的图像来确定物体在三维空间中的位置。","地面真实视差":"在立体视觉中，指两个视角之间物体位置的准确差异。","图拉普拉斯正则化":"一种用于图像处理的技术，通过在优化问题中引入图拉普拉斯矩阵来保持图像的平滑性和边缘。","自适应学习":"一种机器学习方法，模型能够根据新数据调整其参数以改善性能。"}},{"order":214,"title":"Exploring Disentangled Feature Representation Beyond Face Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.html","abstract":"This paper proposes learning disentangled but complementary face features with a minimal supervision by face identification. Specifically, we construct an identity Distilling and Dispelling Auto-Encoder (D^2AE) framework that adversarially learns the identity-distilled features for identity verification and the identity-dispelled features to fool the verification system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity verification performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.","中文标题":"探索超越人脸识别的解耦特征表示","摘要翻译":"本文提出了一种通过人脸识别以最小监督学习解耦但互补的人脸特征的方法。具体而言，我们构建了一个身份蒸馏和驱散自动编码器（D^2AE）框架，该框架通过对抗学习身份蒸馏特征用于身份验证，以及身份驱散特征以欺骗验证系统。得益于双流线索的设计，学习到的解耦特征不仅代表身份或属性，而且代表完整的输入图像。综合评估进一步证明，所提出的特征不仅在LFW上保持了最先进的身份验证性能，而且在CelebA和LFWA上获得了可比较的人脸属性识别判别力。此外，所提出的系统能够基于各种身份和属性以无监督的方式语义控制人脸生成/编辑。","领域":"人脸识别/特征学习/图像生成","问题":"如何在最小监督下学习解耦但互补的人脸特征","动机":"提高人脸识别系统的性能，同时探索人脸特征在图像生成和编辑中的应用","方法":"构建身份蒸馏和驱散自动编码器（D^2AE）框架，通过对抗学习身份蒸馏特征和身份驱散特征","关键词":["人脸识别","特征学习","图像生成","自动编码器","对抗学习"],"涉及的技术概念":"身份蒸馏和驱散自动编码器（D^2AE）是一种通过对抗学习来同时学习用于身份验证的身份蒸馏特征和用于欺骗验证系统的身份驱散特征的框架。这种方法旨在通过最小监督学习解耦但互补的人脸特征，以提高人脸识别系统的性能，并探索这些特征在图像生成和编辑中的应用。"},{"order":215,"title":"Learning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Learning_Facial_Action_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Learning_Facial_Action_CVPR_2018_paper.html","abstract":"We present a scalable weakly supervised clustering approach to learn facial action units (AUs) from large, freely available web images. Unlike most existing methods (e.g., CNNs) that rely on fully annotated data, our method exploits web images with inaccurate annotations. Specifically, we derive a weakly-supervised spectral algorithm that learns an embedding space to couple image appearance and semantics. The algorithm has efficient gradient update, and scales up to large quantities of images with a stochastic extension. With the learned embedding space, we adopt rank-order clustering to identify groups of visually and semantically similar images, and re-annotate these groups for training AU classifiers. Evaluation on the 1 millon EmotioNet dataset demonstrates the effectiveness of our approach: (1) our learned annotations reach on average 91.3% agreement with human annotations on 7 common AUs, (2) classifiers trained with re-annotated images perform comparably to, sometimes even better than, its supervised CNN-based counterpart, and (3) our method offers intuitive outlier/noise pruning instead of forcing one annotation to every image. Code is available.","中文标题":"从网络图像中学习面部动作单元的可扩展弱监督聚类方法","摘要翻译":"我们提出了一种可扩展的弱监督聚类方法，用于从大量免费可用的网络图像中学习面部动作单元（AUs）。与大多数现有方法（例如，卷积神经网络）依赖完全注释的数据不同，我们的方法利用带有不准确注释的网络图像。具体来说，我们推导出一种弱监督的光谱算法，该算法学习一个嵌入空间来耦合图像外观和语义。该算法具有高效的梯度更新，并且通过随机扩展可以扩展到大量图像。利用学习到的嵌入空间，我们采用秩次聚类来识别视觉和语义上相似的图像组，并重新注释这些组以训练AU分类器。在100万张EmotioNet数据集上的评估证明了我们方法的有效性：（1）我们学习的注释在7个常见AUs上与人类注释的平均一致性达到91.3%，（2）使用重新注释的图像训练的分类器表现与基于监督的卷积神经网络相当，有时甚至更好，（3）我们的方法提供了直观的异常值/噪声修剪，而不是强制为每张图像分配一个注释。代码已提供。","领域":"面部表情分析/弱监督学习/图像聚类","问题":"如何从带有不准确注释的大量网络图像中有效学习面部动作单元","动机":"现有方法依赖完全注释的数据，而网络图像通常带有不准确的注释，这为学习面部动作单元提供了新的挑战和机会","方法":"提出了一种弱监督的光谱算法，学习一个嵌入空间来耦合图像外观和语义，并采用秩次聚类来识别和重新注释视觉和语义上相似的图像组","关键词":["面部动作单元","弱监督学习","图像聚类","嵌入空间","秩次聚类"],"涉及的技术概念":"面部动作单元（AUs）是面部表情的基本组成部分，弱监督学习是一种利用不完全或噪声标签进行学习的方法，嵌入空间是将高维数据映射到低维空间以保留数据间关系的技术，秩次聚类是一种基于数据点之间相对顺序的聚类方法。"},{"order":216,"title":"Human Pose Estimation With Parsing Induced Learner","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nie_Human_Pose_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nie_Human_Pose_Estimation_CVPR_2018_paper.html","abstract":"Human pose estimation still faces various difficulties in challenging scenarios. Human parsing, as a closely related task, can provide valuable cues for better pose estimation, which however has not been fully exploited. In this paper, we propose a novel Parsing Induced Learner to exploit parsing information to effectively assist pose estimation by learning to fast adapt the base pose estimation model. The proposed Parsing Induced Learner is composed of a parsing encoder and a pose model parameter adapter, which together learn to predict dynamic parameters of the pose model to extract complementary useful features for more accurate pose estimation. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part show that the proposed  Parsing Induced Learner can improve performance of both single- and multi-person pose estimation to new state-of-the-art. Cross-dataset experiments also show that the proposed Parsing Induced Learner from LIP dataset can accelerate learning of a human pose estimation model on MPII benchmark in addition to achieving outperforming performance.","中文标题":"基于解析诱导学习器的人体姿态估计","摘要翻译":"在具有挑战性的场景中，人体姿态估计仍然面临各种困难。人体解析作为一个密切相关的任务，可以为更好的姿态估计提供有价值的线索，然而这一点尚未被充分利用。在本文中，我们提出了一种新颖的解析诱导学习器，通过快速适应基础姿态估计模型来有效利用解析信息以辅助姿态估计。所提出的解析诱导学习器由解析编码器和姿态模型参数适配器组成，它们共同学习预测姿态模型的动态参数，以提取互补的有用特征，从而实现更准确的姿态估计。在LIP和扩展的PASCAL-Person-Part基准上的综合实验表明，所提出的解析诱导学习器可以将单人和多人姿态估计的性能提升到新的最先进水平。跨数据集实验还表明，来自LIP数据集的解析诱导学习器除了实现卓越的性能外，还可以加速MPII基准上人体姿态估计模型的学习。","领域":"人体姿态估计/人体解析/深度学习","问题":"在具有挑战性的场景中提高人体姿态估计的准确性和效率","动机":"利用人体解析提供的有价值线索来辅助人体姿态估计，以克服现有方法在挑战性场景中的困难","方法":"提出了一种解析诱导学习器，包括解析编码器和姿态模型参数适配器，通过学习预测姿态模型的动态参数来提取互补的有用特征","关键词":["人体姿态估计","人体解析","解析诱导学习器","动态参数预测","特征提取"],"涉及的技术概念":"解析诱导学习器是一种新颖的方法，它通过解析编码器和姿态模型参数适配器来学习预测姿态模型的动态参数，从而提取出对姿态估计有用的互补特征。这种方法旨在通过快速适应基础姿态估计模型来有效利用解析信息，以提高姿态估计的准确性和效率。"},{"order":217,"title":"Multi-Level Factorisation Net for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Multi-Level_Factorisation_Net_CVPR_2018_paper.html","abstract":"Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.","中文标题":"多层次因子分解网络用于行人重识别","摘要翻译":"有效的行人重识别（Re-ID）关键在于在高和低语义层次上建模行人外观的区分性和视角不变性因素。最近开发的深度Re-ID模型要么学习一个整体的单一语义层次特征表示，要么需要对这些因素进行繁琐的人工注释作为属性。我们提出了多层次因子分解网络（MLFN），这是一种新颖的网络架构，它无需手动注释即可将人的视觉外观分解为多个语义层次的潜在区分性因素。MLFN由多个堆叠的块组成。每个块包含多个因子模块，用于在特定层次上建模潜在因素，以及因子选择模块，这些模块动态选择因子模块来解释每个输入图像的内容。因子选择模块的输出还提供了一个紧凑的潜在因子描述符，这与传统的深度学习特征互补。MLFN在三个Re-ID数据集上实现了最先进的结果，并在通用对象分类CIFAR-100数据集上取得了令人信服的结果。","领域":"行人重识别/深度学习/图像分类","问题":"如何在无需手动注释的情况下，有效地建模行人外观的区分性和视角不变性因素","动机":"解决现有深度Re-ID模型需要繁琐人工注释或仅能学习单一语义层次特征表示的问题","方法":"提出多层次因子分解网络（MLFN），通过多个堆叠的块，每个块包含多个因子模块和因子选择模块，动态选择因子模块来解释输入图像内容，并生成紧凑的潜在因子描述符","关键词":["行人重识别","多层次因子分解","潜在因子描述符"],"涉及的技术概念":"MLFN是一种网络架构，通过因子模块和因子选择模块在多个语义层次上分解行人视觉外观，生成潜在区分性因素，无需手动注释。这种方法不仅提高了行人重识别的准确性，还能应用于通用对象分类任务。"},{"order":218,"title":"Attention-Aware Compositional Network for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Attention-Aware_Compositional_Network_CVPR_2018_paper.html","abstract":"Person re-identification (ReID) is to identify pedestrians observed from different camera views based on visual appearance. It is a challenging task due to large pose variations, complex background clutters and severe occlusions. Recently, human pose estimation by predicting joint locations was largely improved in accuracy. It is reasonable to use pose estimation results for handling pose variations and background clutters, and such attempts have obtained great improvement in ReID performance. However, we argue that the pose information was not well utilized and hasn’t yet been fully exploited for person ReID. In this work, we introduce a novel framework called Attention-Aware Compositional Network (AACN) for person ReID. AACN consists of two main components: Pose-guided Part Attention (PPA) and Attention-aware Feature Composition (AFC). PPA is learned and applied to mask out undesirable background features in pedestrian feature maps. Furthermore, pose-guided visibility scores are estimated for body parts to deal with part occlusion in the proposed AFC module. Extensive experiments with ablation analysis show the effectiveness of our method, and state-of-the-art results are achieved on several public datasets, including Market-1501, CUHK03, CUHK01, SenseReID, CUHK03-NP and DukeMTMC-reID.","中文标题":"注意力感知组合网络用于行人重识别","摘要翻译":"行人重识别（ReID）是基于视觉外观识别从不同摄像头视角观察到的行人。由于姿态变化大、背景复杂和严重遮挡，这是一项具有挑战性的任务。最近，通过预测关节位置进行的人体姿态估计在准确性上有了很大提高。使用姿态估计结果来处理姿态变化和背景杂乱是合理的，这样的尝试已经在ReID性能上取得了很大改进。然而，我们认为姿态信息并未得到充分利用，尚未在行人ReID中充分发挥作用。在这项工作中，我们引入了一种名为注意力感知组合网络（AACN）的新框架用于行人ReID。AACN由两个主要组件组成：姿态引导的部分注意力（PPA）和注意力感知的特征组合（AFC）。PPA被学习并应用于屏蔽行人特征图中不希望的背景特征。此外，在提出的AFC模块中，估计了身体部分的姿态引导可见性分数以处理部分遮挡。通过消融分析的大量实验证明了我们方法的有效性，并在多个公共数据集上取得了最先进的结果，包括Market-1501、CUHK03、CUHK01、SenseReID、CUHK03-NP和DukeMTMC-reID。","领域":"行人重识别/姿态估计/特征提取","问题":"行人重识别中的姿态变化、背景杂乱和严重遮挡问题","动机":"利用姿态估计结果改进行人重识别性能，但现有方法未能充分利用姿态信息","方法":"提出了一种名为注意力感知组合网络（AACN）的新框架，包括姿态引导的部分注意力（PPA）和注意力感知的特征组合（AFC）两个主要组件，用于屏蔽不希望的背景特征和处理部分遮挡","关键词":["行人重识别","姿态估计","特征提取"],"涉及的技术概念":"姿态引导的部分注意力（PPA）用于屏蔽行人特征图中的背景特征，注意力感知的特征组合（AFC）用于估计身体部分的姿态引导可见性分数以处理部分遮挡"},{"order":219,"title":"Look at Boundary: A Boundary-Aware Face Alignment Algorithm","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Look_at_Boundary_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Look_at_Boundary_CVPR_2018_paper.html","abstract":"We present a novel boundary-aware face alignment algorithm by utilising boundary lines as the geometric structure of a human face to help facial landmark localisation. Unlike the conventional heatmap based method and regression based method, our approach derives face landmarks from boundary lines which remove the ambiguities in the landmark definition. Three questions are explored and answered by this work: 1. Why using boundary? 2. How to use boundary? 3. What is the relationship between boundary estimation and landmarks localisation? Our boundary-aware face alignment algorithm achieves 3.49% mean error on 300-W Fullset, which outperforms state-of-the-art methods by a large margin. Our method can also easily integrate information from other datasets. By utilising boundary information of 300-W dataset, our method achieves 3.92% mean error with 0.39% failure rate on COFW dataset, and 1.25% mean error on AFLW-Full dataset. Moreover, we propose a new dataset WFLW to unify training and testing across different factors, including poses, expressions, illuminations, makeups, occlusions, and blurriness. Dataset and model are publicly available at https://wywu.github.io/projects/LAB/LAB.html","中文标题":"看边界：一种边界感知的人脸对齐算法","摘要翻译":"我们提出了一种新颖的边界感知人脸对齐算法，通过利用边界线作为人脸的几何结构来帮助面部地标定位。与传统的基于热图的方法和基于回归的方法不同，我们的方法从边界线中推导出面部地标，这消除了地标定义中的歧义。这项工作探索并回答了三个问题：1. 为什么使用边界？2. 如何使用边界？3. 边界估计与地标定位之间的关系是什么？我们的边界感知人脸对齐算法在300-W Fullset上实现了3.49%的平均误差，大大超过了最先进的方法。我们的方法还可以轻松整合来自其他数据集的信息。通过利用300-W数据集的边界信息，我们的方法在COFW数据集上实现了3.92%的平均误差和0.39%的失败率，在AFLW-Full数据集上实现了1.25%的平均误差。此外，我们提出了一个新的数据集WFLW，以统一不同因素（包括姿势、表情、光照、化妆、遮挡和模糊）的训练和测试。数据集和模型可在https://wywu.github.io/projects/LAB/LAB.html公开获取。","领域":"人脸对齐/面部地标定位/边界估计","问题":"如何更准确地定位面部地标","动机":"消除面部地标定义中的歧义，提高定位的准确性","方法":"利用边界线作为人脸的几何结构来推导面部地标","关键词":["人脸对齐","面部地标定位","边界估计"],"涉及的技术概念":"边界感知算法通过利用边界线作为人脸的几何结构来帮助面部地标定位，这种方法与传统的基于热图的方法和基于回归的方法不同，它从边界线中推导出面部地标，从而消除了地标定义中的歧义。此外，该研究还提出了一个新的数据集WFLW，用于统一不同因素（如姿势、表情、光照、化妆、遮挡和模糊）的训练和测试。"},{"order":220,"title":"Demo2Vec: Reasoning Object Affordances From Online Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.html","abstract":"Watching expert demonstrations is an important way for humans and robots to reason about affordances of unseen objects. In this paper, we consider the problem of reasoning object affordances through the feature embedding of demonstration videos. We design the Demo2Vec model which learns to extract embedded vectors of demonstration videos and predicts the interaction region and the action label on a target image of the same object. We introduce the Online Product Review dataset for Affordance (OPRA) by collecting and labeling  diverse YouTube product review videos. Our Demo2Vec model outperforms various recurrent neural network baselines on the collected dataset.","中文标题":"Demo2Vec: 从在线视频中推理物体的可供性","摘要翻译":"观看专家演示是人类和机器人推理未见物体可供性的重要方式。在本文中，我们考虑通过演示视频的特征嵌入来推理物体的可供性的问题。我们设计了Demo2Vec模型，该模型学习提取演示视频的嵌入向量，并预测同一物体目标图像上的交互区域和动作标签。我们通过收集和标注多样化的YouTube产品评论视频，引入了可供性在线产品评论数据集（OPRA）。我们的Demo2Vec模型在收集的数据集上优于各种循环神经网络基线。","领域":"物体可供性推理/视频分析/特征嵌入","问题":"如何通过演示视频的特征嵌入来推理物体的可供性","动机":"为了帮助人类和机器人更好地理解和推理未见物体的可供性，通过观看专家演示视频来学习","方法":"设计了Demo2Vec模型，该模型学习提取演示视频的嵌入向量，并预测同一物体目标图像上的交互区域和动作标签","关键词":["物体可供性","视频分析","特征嵌入","交互区域预测","动作标签预测"],"涉及的技术概念":"Demo2Vec模型是一种通过学习演示视频的嵌入向量来推理物体可供性的方法，它能够预测目标图像上的交互区域和动作标签。OPRA数据集是通过收集和标注YouTube产品评论视频创建的，用于训练和评估Demo2Vec模型。"},{"order":221,"title":"Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes - The Importance of Multiple Scene Constraints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.html","abstract":"Human sensing has greatly benefited from recent advances in deep learning, parametric human modeling, and large scale 2d and 3d datasets. However, existing 3d models make strong assumptions about the scene, considering either a single person per image, full views of the person, a simple background or many cameras. In this paper, we leverage state-of-the-art deep multi-task neural networks and parametric human and scene modeling, towards a fully automatic monocular visual sensing system for multiple interacting people, which (i) infers the 2d and 3d pose and shape of multiple people from a single image, relying on detailed semantic representations at both model and image level, to guide a combined optimization with feedforward and feedback components, (ii) automatically integrates scene constraints including ground plane support and simultaneous volume occupancy by multiple people, and (iii) extends the single image model to video by optimally solving the temporal person assignment problem and imposing coherent temporal pose and motion reconstructions while preserving image alignment fidelity. We perform experiments on both single and multi-person datasets, and systematically evaluate each component of the model, showing improved performance and extensive multiple human sensing capability. We also apply our method to images with multiple people, severe occlusions and diverse backgrounds captured in challenging natural scenes, and obtain results of good perceptual quality.","中文标题":"自然场景中多人单目3D姿态与形状估计 - 多重场景约束的重要性","摘要翻译":"人类感知技术最近得益于深度学习的进步、参数化人体建模以及大规模的2D和3D数据集。然而，现有的3D模型对场景做出了强烈的假设，考虑的是每张图像中只有一个人、人的完整视角、简单背景或多个摄像头。在本文中，我们利用最先进的深度多任务神经网络和参数化人体及场景建模，朝着一个完全自动化的单目视觉感知系统发展，该系统能够（i）从单张图像中推断出多个人的2D和3D姿态与形状，依赖于模型和图像层面的详细语义表示，以指导结合前馈和反馈组件的联合优化，（ii）自动整合包括地面支撑和多人同时体积占用在内的场景约束，以及（iii）通过最优解决时间人物分配问题并将连贯的时间姿态和运动重建强加于视频中，同时保持图像对齐的保真度，将单图像模型扩展到视频。我们在单人和多人数据集上进行了实验，并系统地评估了模型的每个组件，展示了改进的性能和广泛的多人类感知能力。我们还将我们的方法应用于在具有挑战性的自然场景中捕获的包含多人、严重遮挡和多样化背景的图像，并获得了良好感知质量的结果。","领域":"人体姿态估计/场景理解/视频分析","问题":"在复杂自然场景中从单张图像估计多个人的3D姿态和形状","动机":"现有3D模型对场景的假设过于简单，无法处理复杂自然场景中的多人、遮挡和多样化背景","方法":"利用深度多任务神经网络和参数化人体及场景建模，开发一个完全自动化的单目视觉感知系统，该系统能够从单张图像中推断出多个人的2D和3D姿态与形状，并自动整合场景约束，同时将模型扩展到视频处理","关键词":["3D姿态估计","人体建模","场景约束"],"涉及的技术概念":"深度多任务神经网络用于处理复杂的视觉感知任务，参数化人体建模用于精确表示人体姿态和形状，场景约束包括地面支撑和多人同时体积占用，用于提高模型的准确性和鲁棒性。"},{"order":222,"title":"3D Human Sensing, Action and Emotion Recognition in Robot Assisted Therapy of Children With Autism","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.html","abstract":"We introduce new, fine-grained action and emotion recognition tasks defined on non-staged videos, recorded during robot-assisted therapy sessions of children with autism. The tasks present several challenges: a large dataset with long videos, a large number of highly variable actions, children that are only partially visible, have different ages and may show unpredictable behaviour, as well as  non-standard camera viewpoints. We investigate how state-of-the-art 3d human pose reconstruction methods perform on the newly introduced tasks and propose extensions to adapt them to deal with these challenges. We also analyze multiple approaches in action and emotion recognition from 3d human pose data, establish several baselines, and discuss results and their implications in the context of child-robot interaction.","中文标题":"机器人辅助治疗自闭症儿童中的3D人体感知、动作与情感识别","摘要翻译":"我们介绍了新的、细粒度的动作和情感识别任务，这些任务定义在非舞台视频上，这些视频是在机器人辅助治疗自闭症儿童的会话期间录制的。这些任务提出了几个挑战：一个包含长视频的大型数据集，大量高度可变的动作，部分可见的儿童，不同年龄的儿童可能表现出不可预测的行为，以及非标准的摄像机视角。我们研究了最先进的3D人体姿态重建方法在新引入的任务上的表现，并提出了扩展以适应这些挑战。我们还分析了从3D人体姿态数据进行动作和情感识别的多种方法，建立了几个基线，并讨论了结果及其在儿童-机器人交互背景下的意义。","领域":"自闭症治疗/机器人辅助治疗/3D人体姿态分析","问题":"在机器人辅助治疗自闭症儿童的会话中，从非舞台视频中进行细粒度的动作和情感识别","动机":"为了改善自闭症儿童的机器人辅助治疗效果，需要准确识别儿童的动作和情感，这需要克服视频数据中的多种挑战","方法":"研究最先进的3D人体姿态重建方法在新任务上的表现，并提出扩展以适应挑战；分析从3D人体姿态数据进行动作和情感识别的多种方法，并建立基线","关键词":["自闭症治疗","机器人辅助治疗","3D人体姿态分析","动作识别","情感识别"],"涉及的技术概念":"3D人体姿态重建方法用于处理长视频、高度可变的动作、部分可见的儿童、不同年龄的儿童可能表现出不可预测的行为以及非标准的摄像机视角等挑战；从3D人体姿态数据进行动作和情感识别的多种方法分析"},{"order":223,"title":"Facial Expression Recognition by De-Expression Residue Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Facial_Expression_Recognition_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Facial_Expression_Recognition_CVPR_2018_paper.html","abstract":"A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by extracting information of the expressive component through a de-expression learning procedure, called De-expression Residue Learning (DeRL). First, a generative model is trained by cGAN. This model generates the corresponding neutral face image for any input face image. We call this procedure de-expression because the expressive information is filtered out by the generative model; however, the expressive information is still recorded in the intermediate layers. Given the neutral face image, unlike previous works using pixel-level or feature-level difference for facial expression classification, our new method learns the deposition (or residue) that remains in the intermediate layers of the generative model. Such a residue is essential as it contains the expressive component deposited in the generative model from any input facial expression images. Seven public facial expression databases are employed in our experiments. With two databases (BU-4DFE and BP4D-spontaneous) for pre-training, the DeRL method has been evaluated on five databases, CK+, Oulu-CASIA, MMI, BU- 3DFE, and BP4D+. The experimental results demonstrate the superior performance of the proposed method.","中文标题":"通过去表情残差学习进行面部表情识别","摘要翻译":"面部表情是一个人表情成分和中性成分的组合。在本文中，我们提出通过去表情学习过程提取表情成分的信息来识别面部表情，这种方法被称为去表情残差学习（DeRL）。首先，通过cGAN训练一个生成模型。该模型为任何输入的面部图像生成相应的中性面部图像。我们称这个过程为去表情，因为生成模型过滤掉了表情信息；然而，表情信息仍然记录在中间层中。给定中性面部图像，与之前使用像素级或特征级差异进行面部表情分类的工作不同，我们的新方法学习生成模型中间层中保留的沉积（或残差）。这种残差是必不可少的，因为它包含了从任何输入面部表情图像沉积在生成模型中的表情成分。我们的实验中使用了七个公共面部表情数据库。使用两个数据库（BU-4DFE和BP4D-spontaneous）进行预训练，DeRL方法在五个数据库上进行了评估，包括CK+、Oulu-CASIA、MMI、BU-3DFE和BP4D+。实验结果证明了所提出方法的优越性能。","领域":"面部表情识别/生成对抗网络/图像生成","问题":"如何有效地从面部图像中提取和识别表情成分","动机":"为了更准确地识别面部表情，需要从面部图像中分离出表情成分和中性成分，以便专注于表情成分的分析","方法":"使用生成对抗网络（cGAN）训练生成模型，生成输入面部图像的中性版本，然后从生成模型的中间层学习表情成分的残差","关键词":["面部表情识别","生成对抗网络","图像生成"],"涉及的技术概念":"cGAN（条件生成对抗网络）是一种生成模型，能够根据条件输入生成特定类型的图像。在本研究中，cGAN被用来生成中性面部图像，从而过滤掉表情信息。去表情残差学习（DeRL）是一种新方法，它通过分析生成模型中间层的残差来识别面部表情，这些残差包含了从输入面部表情图像中沉积的表情成分。"},{"order":224,"title":"A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking Interacting Objects","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_A_Causal_And-Or_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_A_Causal_And-Or_CVPR_2018_paper.html","abstract":"Tracking humans that are interacting with the other subjects or environment remains unsolved in visual tracking, because the visibility of the human of interests in videos is unknown and might vary over time. In particular, it is still difficult for state-of-the-art human trackers to recover complete human trajectories in crowded scenes with frequent human interactions. In this work, we consider the visibility status of a subject as a fluent variable, whose change is mostly attributed to the subject's interaction with the surrounding, e.g., crossing behind another object, entering a building, or getting into a vehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the causal-effect relations between an object's visibility fluent and its activities, and develop a probabilistic graph model to jointly reason the visibility fluent change (e.g., from visible to invisible) and track humans in videos. We formulate this joint task as an iterative search of a feasible causal graph structure that enables fast search algorithm, e.g., dynamic programming method. We apply the proposed method on challenging video sequences to evaluate its capabilities of estimating visibility fluent changes of subjects and tracking subjects of interests over time. Results with comparisons demonstrate that our method outperforms the alternative trackers and can recover complete trajectories of humans in complicated scenarios with frequent human interactions.","中文标题":"用于跟踪交互对象中可见性流畅推理的因果与或图模型","摘要翻译":"在视觉跟踪中，跟踪与其他主体或环境交互的人类仍然是一个未解决的问题，因为视频中感兴趣的人的可见性是未知的，并且可能随时间变化。特别是，对于最先进的人类跟踪器来说，在拥挤的场景中恢复完整的人类轨迹仍然很困难，这些场景中人类互动频繁。在这项工作中，我们将主体的可见性状态视为一个流畅变量，其变化主要归因于主体与周围环境的交互，例如，穿过另一个物体后面，进入建筑物，或进入车辆等。我们引入了一个因果与或图（C-AOG）来表示对象的可见性流畅与其活动之间的因果关系，并开发了一个概率图模型来共同推理可见性流畅变化（例如，从可见到不可见）并在视频中跟踪人类。我们将这个联合任务表述为对可行因果图结构的迭代搜索，这使得快速搜索算法成为可能，例如动态规划方法。我们在具有挑战性的视频序列上应用了所提出的方法，以评估其估计主体可见性流畅变化和随时间跟踪感兴趣主体的能力。比较结果表明，我们的方法优于其他跟踪器，并且可以在人类互动频繁的复杂场景中恢复完整的人类轨迹。","领域":"视觉跟踪/因果推理/概率图模型","问题":"在拥挤且人类互动频繁的场景中，恢复完整的人类轨迹","动机":"解决在视觉跟踪中，由于人类与环境的交互导致可见性变化，难以准确跟踪的问题","方法":"引入因果与或图（C-AOG）表示对象的可见性流畅与其活动之间的因果关系，并开发概率图模型共同推理可见性流畅变化和跟踪人类","关键词":["视觉跟踪","因果推理","概率图模型","人类轨迹恢复"],"涉及的技术概念":{"因果与或图（C-AOG）":"一种用于表示因果关系和逻辑关系的图模型，特别适用于处理对象的可见性流畅与其活动之间的关系。","概率图模型":"一种利用图结构表示随机变量之间关系的统计模型，用于推理和预测。","动态规划方法":"一种解决多阶段决策过程最优化问题的数学方法，适用于快速搜索算法。"}},{"order":225,"title":"Weakly Supervised Facial Action Unit Recognition Through Adversarial Training","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Weakly_Supervised_Facial_CVPR_2018_paper.html","abstract":"Current works on facial action unit (AU) recognition typically require fully AU-annotated facial images for supervised AU classifier training. AU annotation is a time-consuming, expensive, and error-prone process. While AUs are hard to annotate, facial expression is relatively easy to label. Furthermore, there exist strong probabilistic dependencies between expressions and AUs as well as dependencies among AUs. Such dependencies are referred to as domain knowledge. In this paper, we propose a novel AU recognition method that learns AU classifiers from domain knowledge and expression-annotated facial images through adversarial training. Specifically, we first generate pseudo AU labels according to the probabilistic dependencies between expressions and AUs as well as correlations among AUs summarized from domain knowledge. Then we propose a weakly supervised AU recognition method via an adversarial process, in which we simultaneously train two models: a recognition model R, which learns AU classifiers, and a discrimination model D, which estimates the probability that AU labels generated from domain knowledge rather than the recognized AU labels from R. The training procedure for R maximizes the probability of D making a mistake. By leveraging the adversarial mechanism, the distribution of recognized AUs is closed to AU prior distribution from domain knowledge. Furthermore, the proposed weakly supervised AU recognition can be extended to semi-supervised learning scenarios with partially AU-annotated images. Experimental results on three benchmark databases demonstrate that the proposed method successfully leverages the summarized domain knowledge to weakly supervised AU classifier learning through an adversarial process, and thus achieves state-of-the-art performance.","中文标题":"通过对抗训练实现弱监督的面部动作单元识别","摘要翻译":"目前关于面部动作单元（AU）识别的研究通常需要完全标注AU的面部图像来进行监督AU分类器的训练。AU标注是一个耗时、昂贵且容易出错的过程。虽然AU难以标注，但面部表情相对容易标记。此外，表情与AU之间以及AU之间存在强烈的概率依赖关系。这些依赖关系被称为领域知识。在本文中，我们提出了一种新颖的AU识别方法，该方法通过对抗训练从领域知识和表情标注的面部图像中学习AU分类器。具体来说，我们首先根据表情与AU之间的概率依赖关系以及从领域知识总结出的AU之间的相关性生成伪AU标签。然后，我们提出了一种通过对抗过程实现的弱监督AU识别方法，在该方法中，我们同时训练两个模型：一个识别模型R，它学习AU分类器；一个判别模型D，它估计从领域知识生成的AU标签而不是从R识别的AU标签的概率。R的训练过程最大化D犯错误的概率。通过利用对抗机制，识别的AU的分布接近于来自领域知识的AU先验分布。此外，所提出的弱监督AU识别可以扩展到具有部分AU标注图像的半监督学习场景。在三个基准数据库上的实验结果表明，所提出的方法成功地利用总结的领域知识通过对抗过程进行弱监督AU分类器学习，从而实现了最先进的性能。","领域":"面部表情分析/对抗学习/弱监督学习","问题":"面部动作单元（AU）识别需要大量标注数据，但AU标注过程耗时、昂贵且容易出错。","动机":"利用面部表情与AU之间的概率依赖关系以及AU之间的相关性，通过对抗训练从表情标注的面部图像中学习AU分类器，以减少对完全AU标注数据的依赖。","方法":"提出了一种通过对抗过程实现的弱监督AU识别方法，包括生成伪AU标签和同时训练识别模型R和判别模型D，以最大化D犯错误的概率，从而使识别的AU分布接近于领域知识的AU先验分布。","关键词":["面部动作单元识别","对抗训练","弱监督学习","面部表情分析"],"涉及的技术概念":"面部动作单元（AU）识别、对抗训练、弱监督学习、面部表情分析、伪标签生成、识别模型、判别模型、领域知识、半监督学习。"},{"order":226,"title":"Non-Linear Temporal Subspace Representations for Activity Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cherian_Non-Linear_Temporal_Subspace_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cherian_Non-Linear_Temporal_Subspace_CVPR_2018_paper.html","abstract":"Representations that can compactly and effectively capture the temporal evolution of semantic content are important to computer vision and machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in a reproducing kernel Hilbert space, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective. We then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results.","中文标题":"非线性时间子空间表示用于活动识别","摘要翻译":"能够紧凑且有效地捕捉语义内容时间演变的表示对于处理多变量时间序列数据的计算机视觉和机器学习算法至关重要。我们以人类动作识别任务为动机，研究了这样的表示。在这里，每个数据实例通过多变量特征（例如通过深度CNN）进行编码，其中动作动态通过它们在时间上的变化来表征。由于这些特征通常是非线性的，我们提出了一种新的池化方法，即核化秩池化，它将给定序列紧凑地表示为再生核希尔伯特空间中超平面参数的预图像，数据在其上的投影捕捉了它们的时间顺序。我们进一步发展了这一思想，并表明这种池化方案可以被视为一个顺序约束的核化PCA目标。然后，我们提出使用核化低秩特征子空间的参数作为序列的表示。我们将我们的公式化为广义Grassmann流形上的优化问题，然后使用黎曼优化技术高效地解决它。我们在几个动作识别数据集上使用多种特征模态进行了实验，并展示了最先进的结果。","领域":"动作识别/时间序列分析/特征表示","问题":"如何紧凑且有效地捕捉多变量时间序列数据中语义内容的时间演变","动机":"为了改进人类动作识别任务中的表示方法","方法":"提出了一种新的池化方法，核化秩池化，将序列表示为核化低秩特征子空间的参数，并在广义Grassmann流形上进行优化","关键词":["动作识别","时间序列分析","特征表示","核化秩池化","广义Grassmann流形"],"涉及的技术概念":{"核化秩池化":"一种新的池化方法，用于紧凑地表示序列","再生核希尔伯特空间":"一种数学空间，用于核化秩池化中的超平面参数表示","顺序约束的核化PCA":"一种目标函数，用于捕捉数据的时间顺序","广义Grassmann流形":"一种数学结构，用于优化核化低秩特征子空间的参数","黎曼优化技术":"一种优化方法，用于在广义Grassmann流形上高效地解决问题"}},{"order":227,"title":"Towards Pose Invariant Face Recognition in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.html","abstract":"Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a \\"learning to learn\\" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.","中文标题":"面向野外环境下的姿态不变人脸识别","摘要翻译":"姿态变化是人脸识别中的一个关键挑战。与当前姿态不变人脸识别技术相比，这些技术要么直接提取姿态不变特征进行识别，要么在特征提取之前先将侧面人脸图像归一化为正面姿态，我们认为更理想的做法是同时执行这两项任务，使它们能够相互受益。为此，我们提出了一种野外环境下的人脸识别姿态不变模型（PIM），具有三个显著的新颖之处。首先，PIM是一种新颖且统一的深度架构，包含一个面部正面化子网（FFN）和一个判别学习子网（DLN），它们是从端到端联合学习的。其次，FFN是一个精心设计的双路径生成对抗网络（GAN），它同时感知全局结构和局部细节，结合了无监督的跨域对抗训练和“学习学习”策略，以实现高保真和身份保持的正面视图合成。第三，DLN是一个通用的卷积神经网络（CNN），用于人脸识别，采用我们强制的交叉熵优化策略，以学习判别性但泛化的特征表示。在受控和野外基准上的定性和定量实验证明了所提出模型相对于现有技术的优越性。","领域":"人脸识别/生成对抗网络/卷积神经网络","问题":"解决人脸识别中姿态变化带来的挑战","动机":"提高野外环境下人脸识别的准确性和鲁棒性","方法":"提出了一种姿态不变模型（PIM），包含面部正面化子网（FFN）和判别学习子网（DLN），通过联合学习实现高保真和身份保持的正面视图合成，以及学习判别性但泛化的特征表示","关键词":["姿态不变","人脸识别","生成对抗网络","卷积神经网络","面部正面化"],"涉及的技术概念":"姿态不变模型（PIM）是一种深度架构，包含面部正面化子网（FFN）和判别学习子网（DLN）。FFN采用双路径生成对抗网络（GAN）进行高保真和身份保持的正面视图合成，DLN则是一个通用的卷积神经网络（CNN），用于学习判别性但泛化的特征表示。"},{"order":228,"title":"Unifying Identification and Context Learning for Person Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Unifying_Identification_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Unifying_Identification_and_CVPR_2018_paper.html","abstract":"Despite the great success of face recognition techniques, recognizing persons under unconstrained settings remains challenging. Issues like profile views, unfavorable lighting, and occlusions can cause substantial difficulties. Previous works have attempted to tackle this problem by exploiting the context, e.g. clothes and social relations. While showing promising improvement, they are usually limited in two important aspects, relying on simple heuristics to combine different cues and separating the construction of context from people identities. In this work, we aim to move beyond such limitations and propose a new framework to leverage context for person recognition. In particular, we propose a Region Attention Network, which is learned to adaptively combine visual cues with instance-dependent weights. We also develop a unified formulation, where the social contexts are learned along with the reasoning of people identities. These models substantially improve the robustness when working with the complex contextual relations in unconstrained environments. On two large datasets, PIPA and Cast In Movies (CIM), a new dataset proposed in this work, our method consistently achieves state-of-the-art performance under multiple evaluation policies.","中文标题":"统一身份识别与上下文学习用于人物识别","摘要翻译":"尽管人脸识别技术取得了巨大成功，但在无约束环境下识别人物仍然具有挑战性。侧面视角、不利的光照条件和遮挡等问题可能导致重大困难。以前的工作尝试通过利用上下文（例如衣物和社会关系）来解决这个问题。虽然显示出有希望的改进，但它们通常在两个重要方面受到限制：依赖简单的启发式方法来结合不同的线索，并将上下文的构建与人物身份分离。在这项工作中，我们旨在超越这些限制，并提出一个新的框架来利用上下文进行人物识别。特别是，我们提出了一个区域注意力网络，该网络被学习以自适应地结合视觉线索与实例依赖的权重。我们还开发了一个统一的公式，其中社会上下文与人物身份的推理一起学习。这些模型在处理无约束环境中的复杂上下文关系时大大提高了鲁棒性。在两个大型数据集PIPA和Cast In Movies（CIM）上，我们的方法在多种评估策略下始终实现了最先进的性能。","领域":"人物识别/上下文学习/社会关系分析","问题":"在无约束环境下识别人物","动机":"解决在不利条件下（如侧面视角、不利光照和遮挡）识别人物的问题，并超越现有方法在结合不同线索和上下文构建方面的限制","方法":"提出了一个区域注意力网络来自适应地结合视觉线索与实例依赖的权重，并开发了一个统一的公式来同时学习社会上下文和人物身份的推理","关键词":["人物识别","上下文学习","区域注意力网络","社会关系分析"],"涉及的技术概念":"区域注意力网络是一种用于自适应结合视觉线索的深度学习模型，实例依赖的权重指的是根据每个实例（即每个人物）的不同特征动态调整的权重。统一的公式指的是一个能够同时处理社会上下文和人物身份推理的数学模型。"},{"order":229,"title":"Jointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_Jointly_Optimize_Data_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_Jointly_Optimize_Data_CVPR_2018_paper.html","abstract":"Random data augmentation is a critical technique to avoid overfitting in training deep models. Yet, data augmentation and network training are often two isolated processes in most settings, yielding to a suboptimal training. Why not jointly optimize the two? We propose adversarial data augmentation to address this limitation. The key idea is to design a generator (e.g. an augmentation network) that competes against a discriminator (e.g. a target network) by generating hard examples online. The generator explores weaknesses of the discriminator, while the discriminator learns from hard augmentations to achieve better performance. A reward/penalty strategy is also proposed for efficient joint training. We investigate human pose estimation and carry out comprehensive ablation studies to validate our method. The results prove that our method can effectively improve state-of-the-art models without additional data effort.","中文标题":"联合优化数据增强与网络训练：人体姿态估计中的对抗性数据增强","摘要翻译":"随机数据增强是避免深度模型训练过拟合的关键技术。然而，在大多数设置中，数据增强和网络训练往往是两个孤立的过程，导致训练效果不佳。为什么不联合优化这两个过程呢？我们提出了对抗性数据增强来解决这一限制。关键思想是设计一个生成器（例如，一个增强网络），通过与判别器（例如，目标网络）竞争，在线生成难以处理的例子。生成器探索判别器的弱点，而判别器则从难以处理的增强中学习，以实现更好的性能。我们还提出了一种奖励/惩罚策略，以实现高效的联合训练。我们研究了人体姿态估计，并进行了全面的消融研究以验证我们的方法。结果证明，我们的方法可以有效地改进最先进的模型，而无需额外的数据努力。","领域":"人体姿态估计/对抗性学习/数据增强","问题":"数据增强和网络训练过程孤立，导致训练效果不佳","动机":"联合优化数据增强和网络训练过程，以提高模型性能","方法":"提出对抗性数据增强方法，设计生成器与判别器竞争，生成难以处理的例子，并采用奖励/惩罚策略进行高效联合训练","关键词":["对抗性数据增强","人体姿态估计","联合训练"],"涉及的技术概念":"对抗性数据增强是一种通过设计生成器与判别器竞争的方法，生成器探索判别器的弱点，而判别器则从难以处理的增强中学习，以提高模型性能。奖励/惩罚策略用于实现高效的联合训练。"},{"order":230,"title":"Wing Loss for Robust Facial Landmark Localisation With Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Feng_Wing_Loss_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Feng_Wing_Loss_for_CVPR_2018_paper.html","abstract":"We present a new loss function, namely Wing loss, for robust facial landmark localisation with Convolutional Neural Networks (CNNs). We first compare and analyse different loss functions including L2, L1 and smooth L1. The analysis of these loss functions suggests that, for the training of a CNN-based localisation model, more attention should be paid to small and medium range errors. To this end, we design a piece-wise loss function. The new loss amplifies the impact of errors from the interval (-w, w) by switching from L1 loss to a modified logarithm function.  To address the problem of under-representation of samples with large out-of-plane head rotations in the training set, we propose a simple but effective boosting strategy, referred to as pose-based data balancing. In particular, we deal with the data imbalance problem by duplicating the minority training samples and perturbing them by injecting random image rotation, bounding box translation and other data augmentation approaches. Last, the proposed approach is extended to create a two-stage framework for robust facial landmark localisation. The experimental results obtained on AFLW and 300W demonstrate the merits of the Wing loss function, and prove the superiority of the proposed method over the state-of-the-art approaches.","中文标题":"用于卷积神经网络鲁棒面部地标定位的Wing损失函数","摘要翻译":"我们提出了一种新的损失函数，即Wing损失，用于卷积神经网络（CNNs）的鲁棒面部地标定位。我们首先比较并分析了不同的损失函数，包括L2、L1和平滑L1。对这些损失函数的分析表明，对于基于CNN的定位模型的训练，应更多地关注小到中等范围的误差。为此，我们设计了一种分段损失函数。这种新的损失函数通过在区间（-w, w）内从L1损失切换到修改后的对数函数来放大误差的影响。为了解决训练集中大角度头部旋转样本不足的问题，我们提出了一种简单但有效的增强策略，称为基于姿态的数据平衡。特别是，我们通过复制少数训练样本并通过注入随机图像旋转、边界框平移和其他数据增强方法来处理数据不平衡问题。最后，所提出的方法被扩展以创建一个用于鲁棒面部地标定位的两阶段框架。在AFLW和300W上获得的实验结果证明了Wing损失函数的优点，并证明了所提出方法相对于最先进方法的优越性。","领域":"面部地标定位/卷积神经网络/损失函数设计","问题":"面部地标定位中的误差处理和数据不平衡问题","动机":"提高卷积神经网络在面部地标定位任务中的鲁棒性和准确性","方法":"设计了一种新的分段损失函数（Wing损失），并提出了基于姿态的数据平衡策略来处理数据不平衡问题","关键词":["面部地标定位","损失函数","数据平衡"],"涉及的技术概念":{"Wing损失":"一种新的损失函数，通过在特定区间内使用修改后的对数函数来放大误差的影响，以提高模型对小到中等范围误差的敏感性","基于姿态的数据平衡":"一种通过复制少数样本并应用数据增强技术来处理训练集中样本不平衡问题的策略","两阶段框架":"一种扩展的方法，用于进一步提高面部地标定位的鲁棒性"}},{"order":231,"title":"Multiple Granularity Group Interaction Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yao_Multiple_Granularity_Group_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yao_Multiple_Granularity_Group_CVPR_2018_paper.html","abstract":"Most human activity analysis works (i.e., recognition or　prediction) only focus on a single granularity, i.e., either　modelling global motion based on the coarse level movement such as human trajectories or　forecasting future detailed action based on body parts’ movement such as skeleton motion. In contrast, in this work, we propose a multi-granularity interaction prediction network which integrates　both global motion and detailed local action. Built on a bi- directional LSTM network, the　proposed method possesses　between granularities links which encourage feature sharing as well as cross-feature consistency between both global　and local granularity (e.g., trajectory or local action), and in turn predict long-term global location and local dynamics of each individual. We validate our method on several　public datasets with promising performance.","中文标题":"多粒度群体互动预测","摘要翻译":"大多数人类活动分析工作（即识别或预测）仅关注单一粒度，即要么基于粗粒度运动（如人类轨迹）建模全局运动，要么基于身体部位运动（如骨架运动）预测未来详细动作。相比之下，在本工作中，我们提出了一个多粒度互动预测网络，该网络整合了全局运动和详细的局部动作。基于双向LSTM网络构建，所提出的方法具有粒度间的链接，这些链接鼓励特征共享以及全局和局部粒度（如轨迹或局部动作）之间的跨特征一致性，进而预测每个个体的长期全局位置和局部动态。我们在几个公开数据集上验证了我们的方法，并取得了令人满意的性能。","领域":"人类活动分析/运动预测/特征共享","问题":"如何同时预测人类活动的全局运动和局部详细动作","动机":"现有的人类活动分析工作大多只关注单一粒度的运动预测，缺乏对全局和局部运动同时预测的能力","方法":"提出一个基于双向LSTM网络的多粒度互动预测网络，通过粒度间的链接实现特征共享和跨特征一致性，以预测长期全局位置和局部动态","关键词":["多粒度","互动预测","LSTM网络","特征共享","跨特征一致性"],"涉及的技术概念":{"多粒度":"指在分析或预测时考虑不同层次或尺度的信息，如全局运动和局部动作","互动预测":"预测个体或群体之间的互动行为","LSTM网络":"长短期记忆网络，一种特殊的循环神经网络，适用于处理和预测时间序列数据","特征共享":"在不同任务或不同粒度间共享特征，以提高模型的泛化能力和效率","跨特征一致性":"确保不同粒度或不同来源的特征在预测过程中保持一致性和协调性"}},{"order":232,"title":"Social GAN: Socially Acceptable Trajectories With Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gupta_Social_GAN_Socially_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gupta_Social_GAN_Socially_CVPR_2018_paper.html","abstract":"Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several  datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.","中文标题":"社交GAN：使用生成对抗网络生成社会可接受的轨迹","摘要翻译":"理解人类运动行为对于自主移动平台（如自动驾驶汽车和社交机器人）在人类中心环境中导航至关重要。这具有挑战性，因为人类运动本质上是多模态的：给定人类运动路径的历史，人们未来有许多社会可接受的移动方式。我们通过结合序列预测和生成对抗网络的工具来解决这个问题：一个循环序列到序列模型观察运动历史并预测未来行为，使用一种新颖的池化机制来聚合跨人群的信息。我们通过对抗训练一个循环鉴别器来预测社会可接受的未来，并使用一种新颖的多样性损失来鼓励多样化的预测。通过在几个数据集上的实验，我们证明了我们的方法在准确性、多样性、避免碰撞和计算复杂性方面优于之前的工作。","领域":"自动驾驶/社交机器人/人类行为预测","问题":"预测人类在未来的社会可接受的移动轨迹","动机":"为了自主移动平台（如自动驾驶汽车和社交机器人）在人类中心环境中更有效地导航，需要准确预测人类未来的移动行为。","方法":"结合序列预测和生成对抗网络，使用循环序列到序列模型观察运动历史并预测未来行为，采用新颖的池化机制聚合信息，并通过对抗训练和多样性损失来预测社会可接受的未来。","关键词":["人类行为预测","生成对抗网络","序列预测","自动驾驶","社交机器人"],"涉及的技术概念":"生成对抗网络（GANs）用于生成数据，循环序列到序列模型用于处理时间序列数据，池化机制用于信息聚合，对抗训练用于提高预测的社会可接受性，多样性损失用于增加预测的多样性。"},{"order":233,"title":"Deep Group-Shuffling Random Walk for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.html","abstract":"Person re-identification aims at finding a person of interest in an image gallery by comparing the probe image of this person with all the gallery images. It is generally treated as a retrieval problem, where the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. However, most existing methods only consider P2G affinities but ignore the affinities between all the gallery images (G2G affinity). Some frameworks incorporated G2G affinities into the testing process, which is not end-to-end trainable for deep neural networks. In this paper, we propose a novel group-shuffling random walk network for fully utilizing the affinity information between gallery images in both the training and testing processes. The proposed approach aims at end-to-end refining the P2G affinities based on G2G affinity information with a simple yet effective matrix operation, which can be integrated into deep neural networks. Feature grouping and group shuffle are also proposed to apply rich supervisions for learning better person features. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by large margins, which demonstrate the effectiveness of our approach.","中文标题":"深度组洗牌随机游走用于行人重识别","摘要翻译":"行人重识别旨在通过比较感兴趣人物的探针图像与所有图库图像，在图库中找到该人物。它通常被视为一个检索问题，其中探针图像与图库图像之间的亲和力（P2G亲和力）用于对检索到的图库图像进行排名。然而，大多数现有方法仅考虑P2G亲和力，而忽略了所有图库图像之间的亲和力（G2G亲和力）。一些框架将G2G亲和力纳入测试过程，这对于深度神经网络来说不是端到端可训练的。在本文中，我们提出了一种新颖的组洗牌随机游走网络，以充分利用训练和测试过程中图库图像之间的亲和力信息。所提出的方法旨在通过简单而有效的矩阵操作，基于G2G亲和力信息端到端地精炼P2G亲和力，这可以集成到深度神经网络中。还提出了特征分组和组洗牌，以应用丰富的监督来学习更好的人物特征。所提出的方法在Market-1501、CUHK03和DukeMTMC数据集上大幅优于最先进的方法，这证明了我们方法的有效性。","领域":"行人重识别/图像检索/深度学习","问题":"如何有效利用图库图像之间的亲和力信息来提升行人重识别的性能","动机":"现有方法大多仅考虑探针图像与图库图像之间的亲和力，忽略了图库图像之间的亲和力，这限制了行人重识别的性能","方法":"提出了一种新颖的组洗牌随机游走网络，通过简单而有效的矩阵操作，基于G2G亲和力信息端到端地精炼P2G亲和力，并集成到深度神经网络中","关键词":["行人重识别","图像检索","深度学习","随机游走","矩阵操作"],"涉及的技术概念":"P2G亲和力（探针图像与图库图像之间的亲和力）、G2G亲和力（图库图像之间的亲和力）、组洗牌随机游走网络、矩阵操作、特征分组、组洗牌"},{"order":234,"title":"Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Transferable_Joint_Attribute-Identity_CVPR_2018_paper.html","abstract":"Most existing person re-identification (re-id) methods require supervised model learning from a separate large set of pairwise labelled training data for every single camera pair. This significantly limits their scalability and usability in real-world large scale deployments with the need for performing re-id across many camera views. To address this scalability problem, we develop a novel deep learning method for transferring the labelled information of an existing dataset to a new unseen (unlabelled) target domain for person re-id without any supervised learning in the target domain. Specifically, we introduce an Transferable Joint Attribute-Identity Deep Learning (TJ-AIDL) for simultaneously learning an attribute-semantic and identitydiscriminative feature representation space transferrable to any new (unseen) target domain for re-id tasks without the need for collecting new labelled training data from the target domain (i.e. unsupervised learning in the target domain). Extensive comparative evaluations validate the superiority of this new TJ-AIDL model for unsupervised person re-id over a wide range of state-of- the-art methods on four challenging benchmarks including VIPeR, PRID, Market-1501, and DukeMTMC-ReID.","中文标题":"可转移的联合属性-身份深度学习用于无监督行人重识别","摘要翻译":"大多数现有的行人重识别（re-id）方法需要从每个相机对的大量成对标记训练数据中进行监督模型学习。这显著限制了它们在现实世界大规模部署中的可扩展性和可用性，因为需要在许多相机视图之间执行重识别。为了解决这一可扩展性问题，我们开发了一种新的深度学习方法，用于将现有数据集的标记信息转移到新的未见（未标记）目标域，以进行行人重识别，而无需在目标域中进行任何监督学习。具体来说，我们引入了一种可转移的联合属性-身份深度学习（TJ-AIDL），用于同时学习一个属性语义和身份区分特征表示空间，该空间可转移到任何新的（未见）目标域，用于重识别任务，而无需从目标域收集新的标记训练数据（即在目标域中进行无监督学习）。广泛的比较评估验证了这种新的TJ-AIDL模型在无监督行人重识别方面的优越性，超过了包括VIPeR、PRID、Market-1501和DukeMTMC-ReID在内的四个具有挑战性的基准测试中的一系列最先进方法。","领域":"行人重识别/特征学习/无监督学习","问题":"解决行人重识别在现实世界大规模部署中的可扩展性问题","动机":"减少对大量成对标记训练数据的依赖，提高行人重识别方法的可扩展性和实用性","方法":"开发了一种新的深度学习方法TJ-AIDL，用于将现有数据集的标记信息转移到新的未见目标域，无需在目标域中进行监督学习","关键词":["行人重识别","无监督学习","特征表示","深度学习"],"涉及的技术概念":"TJ-AIDL（可转移的联合属性-身份深度学习）是一种深度学习方法，旨在同时学习属性语义和身份区分特征表示空间，以便在没有目标域监督学习的情况下，将学习到的特征表示转移到新的未见目标域，用于行人重识别任务。"},{"order":235,"title":"Harmonious Attention Network for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Harmonious_Attention_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Harmonious_Attention_Network_CVPR_2018_paper.html","abstract":"Existing person re-identiﬁcation (re-id) methods either assume the availability of well-aligned person bounding box images as model input or rely on constrained attention selection mechanisms to calibrate misaligned images. They are therefore sub-optimal for re-id matching in arbitrarily aligned person images potentially with large human pose variations and unconstrained auto-detection errors. In this work, we show the advantages of jointly learning attention selection and feature representation in a Convolutional Neural Network (CNN) by maximising the complementary information of different levels of visual attention subject to re-id discriminative learning constraints. Speciﬁcally, we formulate a novel Harmonious Attention CNN (HA-CNN) model for joint learning of soft pixel attention and hard regional attention along with simultaneous optimisation of feature representations, dedicated to optimise person re-id in uncontrolled (misaligned) images. Extensive comparative evaluations validate the superiority of this new HACNN model for person re-id over a wide variety of state-of-the-art methods on three large-scale benchmarks including CUHK03, Market-1501, and DukeMTMC-ReID.","中文标题":"和谐注意力网络用于行人重识别","摘要翻译":"现有的行人重识别（re-id）方法要么假设模型输入为良好对齐的行人边界框图像，要么依赖于受限的注意力选择机制来校准未对齐的图像。因此，这些方法在处理具有大姿态变化和不受约束的自动检测错误的任意对齐行人图像时，对于行人重识别匹配来说并不是最优的。在这项工作中，我们展示了在卷积神经网络（CNN）中通过最大化不同层次视觉注意力的互补信息，在重识别判别学习约束下联合学习注意力选择和特征表示的优势。具体来说，我们提出了一种新颖的和谐注意力CNN（HA-CNN）模型，用于软像素注意力和硬区域注意力的联合学习，同时优化特征表示，专门用于优化未控制（未对齐）图像中的行人重识别。广泛的比较评估验证了这种新的HA-CNN模型在包括CUHK03、Market-1501和DukeMTMC-ReID在内的三个大规模基准测试上，对于行人重识别优于各种最先进方法的优越性。","领域":"行人重识别/注意力机制/卷积神经网络","问题":"处理具有大姿态变化和不受约束的自动检测错误的任意对齐行人图像的行人重识别问题","动机":"现有方法在处理未对齐图像时表现不佳，需要一种能够联合学习注意力选择和特征表示的方法来提高行人重识别的准确性","方法":"提出了一种新颖的和谐注意力CNN（HA-CNN）模型，用于软像素注意力和硬区域注意力的联合学习，同时优化特征表示","关键词":["行人重识别","注意力机制","卷积神经网络"],"涉及的技术概念":"卷积神经网络（CNN）用于特征提取和表示学习，软像素注意力和硬区域注意力用于处理图像中的关键区域，以优化行人重识别的准确性。"},{"order":236,"title":"Real-Time Rotation-Invariant Face Detection With Progressive Calibration Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Real-Time_Rotation-Invariant_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Real-Time_Rotation-Invariant_Face_CVPR_2018_paper.html","abstract":"Rotation-invariant face detection, i.e. detecting faces with arbitrary rotation-in-plane (RIP) angles, is widely required in unconstrained applications but still remains as a challenging task, due to the large variations of face appearances. Most existing methods compromise with speed or accuracy to handle the large RIP variations. To address this problem more efficiently, we propose Progressive Calibration Networks (PCN) to perform rotation-invariant face detection in a coarse-to-fine manner. PCN consists of three stages, each of which not only distinguishes the faces from non-faces, but also calibrates the RIP orientation of each face candidate to upright progressively. By dividing the calibration process into several progressive steps and only predicting coarse orientations in early stages, PCN can achieve precise and fast calibration. By performing binary classification of face vs. non-face with gradually decreasing RIP ranges, PCN can accurately detect faces with full $360^{circ}$ RIP angles. Such designs lead to a real-time rotation-invariant face detector. The experiments on multi-oriented FDDB and a challenging subset of WIDER FACE containing rotated faces in the wild show that our PCN achieves quite promising performance.","中文标题":"实时旋转不变人脸检测与渐进校准网络","摘要翻译":"旋转不变人脸检测，即检测具有任意平面内旋转（RIP）角度的人脸，在无约束应用中广泛需求，但由于人脸外观的大幅变化，这仍然是一个具有挑战性的任务。大多数现有方法在处理大的RIP变化时，都会在速度或准确性上做出妥协。为了更有效地解决这个问题，我们提出了渐进校准网络（PCN），以从粗到细的方式执行旋转不变人脸检测。PCN由三个阶段组成，每个阶段不仅区分人脸与非人脸，还逐步校准每个人脸候选的RIP方向至直立。通过将校准过程分为几个渐进步骤，并在早期阶段仅预测粗略方向，PCN可以实现精确且快速的校准。通过执行逐渐减少RIP范围的人脸与非人脸的二元分类，PCN可以准确检测具有完整360度RIP角度的人脸。这样的设计导致了一个实时旋转不变人脸检测器。在多方向FDDB和包含野外旋转人脸的WIDER FACE挑战子集上的实验表明，我们的PCN实现了非常有前途的性能。","领域":"人脸检测/旋转不变性/实时处理","问题":"在无约束环境中检测具有任意平面内旋转角度的人脸","动机":"由于人脸外观的大幅变化，旋转不变人脸检测在无约束应用中是一个具有挑战性的任务，需要一种既能保持高准确性又能实现实时处理的方法。","方法":"提出渐进校准网络（PCN），通过三个阶段从粗到细地执行旋转不变人脸检测，每个阶段不仅区分人脸与非人脸，还逐步校准每个人脸候选的RIP方向至直立。","关键词":["人脸检测","旋转不变性","实时处理","渐进校准网络"],"涉及的技术概念":"渐进校准网络（PCN）是一种用于旋转不变人脸检测的方法，通过将校准过程分为几个渐进步骤，并在早期阶段仅预测粗略方向，实现精确且快速的校准。此外，通过执行逐渐减少RIP范围的人脸与非人脸的二元分类，PCN可以准确检测具有完整360度RIP角度的人脸。"},{"order":237,"title":"Deep Regression Forests for Age Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Regression_Forests_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Regression_Forests_CVPR_2018_paper.html","abstract":"Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is inhomogeneous, due to the large variation in facial appearance across different persons of the same age and the non-stationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with inhomogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by fixing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by fixing the split nodes, the leaf nodes are optimized by iterating a step-size free update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.","中文标题":"深度回归森林用于年龄估计","摘要翻译":"从面部图像估计年龄通常被视为一个非线性回归问题。这个问题的主要挑战是面部特征空间相对于年龄是不均匀的，这是由于相同年龄不同人的面部外观变化大以及老化模式的非平稳特性。在本文中，我们提出了深度回归森林（DRFs），一个端到端模型，用于年龄估计。DRFs将分裂节点连接到卷积神经网络（CNN）的全连接层，并通过在分裂节点联合学习输入依赖的数据分区和在叶节点学习数据抽象来处理不均匀数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化；然后，通过固定分裂节点，叶节点通过迭代从变分边界导出的无步长更新规则进行优化。我们在三个标准的年龄估计基准上验证了所提出的DRFs，并在所有这些基准上取得了最先进的结果。","领域":"年龄估计/面部识别/回归分析","问题":"面部图像年龄估计中的非线性回归问题","动机":"解决面部特征空间相对于年龄的不均匀性，由于相同年龄不同人的面部外观变化大以及老化模式的非平稳特性","方法":"提出深度回归森林（DRFs），一个端到端模型，通过在分裂节点联合学习输入依赖的数据分区和在叶节点学习数据抽象来处理不均匀数据，采用交替策略优化模型参数","关键词":["年龄估计","面部识别","回归分析","深度回归森林","卷积神经网络"],"涉及的技术概念":"深度回归森林（DRFs）是一种端到端模型，结合了卷积神经网络（CNN）和回归森林的概念，用于处理面部图像年龄估计中的非线性回归问题。通过交替策略优化模型参数，包括通过反向传播优化分裂节点和CNN参数，以及通过迭代无步长更新规则优化叶节点。"},{"order":238,"title":"Weakly-Supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Weakly-Supervised_Deep_Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Weakly-Supervised_Deep_Convolutional_CVPR_2018_paper.html","abstract":"Facial action unit (AU) intensity estimation plays an important role in affective computing and human-computer interaction. Recent works have introduced deep neural networks for AU intensity estimation, but they require a large amount of intensity annotations. AU annotation needs strong domain expertise and it is expensive to construct a large database to learn deep models. We propose a novel knowledge-based semi-supervised deep convolutional neural network for AU intensity estimation with extremely limited AU annotations. Only the intensity annotations of peak and valley frames in training sequences are needed. To provide additional supervision for model learning, we exploit naturally existing constraints on AUs, including relative appearance similarity, temporal intensity ordering, facial symmetry, and contrastive appearance difference.  Experimental evaluations are performed on two public benchmark databases. With around 2% of intensity annotations in FERA 2015 and around 1% in DISFA for training, our method can achieve comparable or even better performance than the state-of-the-art methods which use 100% of intensity annotations in the training set.","中文标题":"弱监督深度卷积神经网络学习用于面部动作单元强度估计","摘要翻译":"面部动作单元（AU）强度估计在情感计算和人机交互中扮演着重要角色。最近的工作已经引入了深度神经网络用于AU强度估计，但它们需要大量的强度注释。AU注释需要强大的领域专业知识，并且构建一个大型数据库来学习深度模型是昂贵的。我们提出了一种新颖的基于知识的半监督深度卷积神经网络，用于在极其有限的AU注释下进行AU强度估计。仅需要训练序列中峰值和谷值帧的强度注释。为了为模型学习提供额外的监督，我们利用了AU上自然存在的约束，包括相对外观相似性、时间强度排序、面部对称性和对比外观差异。在两个公共基准数据库上进行了实验评估。在FERA 2015中使用约2%的强度注释和在DISFA中使用约1%的强度注释进行训练，我们的方法可以实现与使用训练集中100%强度注释的最先进方法相当甚至更好的性能。","领域":"情感计算/人机交互/面部表情分析","问题":"面部动作单元（AU）强度估计需要大量注释，但注释成本高且需要专业知识","动机":"减少对面部动作单元强度估计所需注释的依赖，降低注释成本","方法":"提出了一种基于知识的半监督深度卷积神经网络，利用自然存在的约束进行模型学习","关键词":["面部动作单元","强度估计","半监督学习","深度卷积神经网络","情感计算","人机交互"],"涉及的技术概念":"面部动作单元（AU）强度估计、深度卷积神经网络、半监督学习、情感计算、人机交互、相对外观相似性、时间强度排序、面部对称性、对比外观差异"},{"order":239,"title":"Memory Based Online Learning of Deep Representations From Video Streams","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pernici_Memory_Based_Online_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pernici_Memory_Based_Online_CVPR_2018_paper.html","abstract":"We present a novel online unsupervised method for face identity learning from video streams. The method exploits deep face descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbour and a forgetting strategy that detect redundant descriptors and discard them appropriately while time progresses. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information. Code will be publicly available.","中文标题":"基于记忆的在线学习从视频流中深度表示","摘要翻译":"我们提出了一种新颖的在线无监督方法，用于从视频流中学习人脸身份。该方法利用深度人脸描述符以及基于记忆的学习机制，该机制利用了视觉数据的时间连贯性。具体来说，我们引入了一种基于反向最近邻的判别性描述符匹配解决方案和一种遗忘策略，该策略检测冗余描述符并随着时间的推移适当地丢弃它们。研究表明，所提出的学习过程是渐近稳定的，并且可以有效地用于相关应用，如从无约束视频流中进行多个人脸识别和跟踪。实验结果表明，在多人脸跟踪任务中，所提出的方法取得了与利用未来信息的离线方法相当的结果，并在人脸识别方面表现更好。代码将公开提供。","领域":"人脸识别/视频分析/在线学习","问题":"从无约束视频流中在线学习人脸身份","动机":"为了有效地从视频流中学习人脸身份，利用视觉数据的时间连贯性，提高人脸识别和跟踪的准确性和效率。","方法":"采用深度人脸描述符和基于记忆的学习机制，引入基于反向最近邻的判别性描述符匹配解决方案和遗忘策略。","关键词":["人脸识别","视频流","在线学习","深度描述符","记忆机制","反向最近邻","遗忘策略"],"涉及的技术概念":"深度人脸描述符：用于提取人脸特征的深度学习方法。基于记忆的学习机制：利用历史数据来改进当前学习过程的方法。反向最近邻：一种用于匹配和识别相似描述符的技术。遗忘策略：一种用于管理和优化存储空间，通过检测和删除冗余数据来提高系统效率的方法。"},{"order":240,"title":"Efficient and Deep Person Re-Identification Using Multi-Level Similarity","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Guo_Efficient_and_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Guo_Efficient_and_Deep_CVPR_2018_paper.html","abstract":"Person Re-Identification (ReID) requires comparing two images of person captured under different conditions. Existing work based on neural networks often computes the similarity of feature maps from one single convolutional layer. In this work, we propose an efficient, end-to-end fully convolutional Siamese network that computes the similarities at multiple levels. We demonstrate that multi-level similarity can improve the accuracy considerably using low-complexity network structures in ReID problem. Specifically, first, we use several convolutional layers to extract the features of two input images. Then, we propose Convolution Similarity Network to compute the similarity score maps for the inputs. We use spatial transformer networks (STNs) to determine spatial attention. We propose to apply efficient depth-wise convolution to compute the similarity. The proposed Convolution Similarity Networks can be inserted into different convolutional layers to extract visual similarities at different levels. Furthermore, we use an improved ranking loss to further improve the performance. Our work is the first to propose to compute visual similarities at low, middle and high levels for ReID. With extensive experiments and analysis, we demonstrate that our system, compact yet effective, can achieve competitive results with much smaller model size and computational complexity.","中文标题":"使用多层次相似性的高效深度行人重识别","摘要翻译":"行人重识别（ReID）需要比较在不同条件下捕捉到的两个人的图像。现有的基于神经网络的工作通常从单一卷积层计算特征图的相似性。在这项工作中，我们提出了一种高效的、端到端的全卷积Siamese网络，它在多个层次上计算相似性。我们证明了使用低复杂度网络结构的多层次相似性可以显著提高ReID问题的准确性。具体来说，首先，我们使用几个卷积层来提取两个输入图像的特征。然后，我们提出了卷积相似性网络来计算输入的相似性得分图。我们使用空间变换网络（STNs）来确定空间注意力。我们提出应用高效的深度卷积来计算相似性。所提出的卷积相似性网络可以插入到不同的卷积层中，以在不同层次上提取视觉相似性。此外，我们使用改进的排名损失来进一步提高性能。我们的工作是第一个提出在低、中、高三个层次上计算视觉相似性以用于ReID的。通过广泛的实验和分析，我们证明了我们的系统，虽然紧凑但有效，可以在模型大小和计算复杂度小得多的情况下实现竞争性的结果。","领域":"行人重识别/卷积神经网络/相似性计算","问题":"提高行人重识别的准确性和效率","动机":"现有的行人重识别方法通常只从单一卷积层计算特征图的相似性，这限制了识别的准确性和效率。","方法":"提出了一种高效的、端到端的全卷积Siamese网络，通过多层次计算相似性，使用空间变换网络确定空间注意力，并应用深度卷积计算相似性，以及使用改进的排名损失提高性能。","关键词":["行人重识别","卷积神经网络","相似性计算","空间变换网络","深度卷积"],"涉及的技术概念":{"Siamese网络":"一种特殊的神经网络架构，用于比较两个输入的相似性。","卷积相似性网络":"一种网络结构，用于计算输入图像的相似性得分图。","空间变换网络（STNs）":"一种网络结构，用于确定图像的空间注意力，即图像中哪些部分对于识别任务更为重要。","深度卷积":"一种卷积操作，它在减少参数数量的同时保持或提高模型的性能。","排名损失":"一种损失函数，用于优化模型以使得正确的匹配对在相似性排名中位于不正确的匹配对之上。"}},{"order":241,"title":"Multi-Level Fusion Based 3D Object Detection From Monocular Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Multi-Level_Fusion_Based_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Multi-Level_Fusion_Based_CVPR_2018_paper.html","abstract":"In this paper, we present an end-to-end deep learning based framework for 3D object detection from a single monocular image. A deep convolutional neural network is introduced for simultaneous 2D and 3D object detection. First, 2D region proposals are generated through a region proposal network. Then the shared features are learned within the proposals to predict the class probability, 2D bounding box, orientation, dimension, and 3D location. We adopt a stand-alone module to predict the disparity and extract features from the computed point cloud. Thus features from the original image and the point cloud will be fused in different levels for accurate 3D localization. The estimated disparity is also used for front view feature encoding to enhance the input image,regarded as an input-fusionprocess. The proposed algorithm can directly output both 2D and 3D object detection results in an end-to-end fashion with only a single RGB image as the input. The experimental results on the challenging KITTI benchmark demonstrate that our algorithm signiﬁcantly outperforms the state-of-the-art methods with only monocular images.","中文标题":"基于多级融合的单目图像三维物体检测","摘要翻译":"本文提出了一种基于端到端深度学习的框架，用于从单张单目图像中进行三维物体检测。引入了一种深度卷积神经网络，用于同时进行二维和三维物体检测。首先，通过区域提议网络生成二维区域提议。然后，在提议中学习共享特征，以预测类别概率、二维边界框、方向、尺寸和三维位置。我们采用了一个独立模块来预测视差并从计算的点云中提取特征。因此，原始图像和点云的特征将在不同级别进行融合，以实现精确的三维定位。估计的视差也用于前视图特征编码，以增强输入图像，这被视为输入融合过程。所提出的算法可以直接输出二维和三维物体检测结果，仅需单张RGB图像作为输入。在具有挑战性的KITTI基准测试上的实验结果表明，我们的算法仅使用单目图像就显著优于最先进的方法。","领域":"三维物体检测/单目视觉/深度学习","问题":"从单张单目图像中准确检测三维物体","动机":"提高单目图像三维物体检测的准确性和效率","方法":"采用深度卷积神经网络进行二维和三维物体检测，通过区域提议网络生成二维区域提议，学习共享特征预测物体属性，独立模块预测视差并提取点云特征，多级融合原始图像和点云特征，使用视差进行前视图特征编码增强输入图像","关键词":["三维物体检测","单目视觉","深度学习","特征融合","视差预测"],"涉及的技术概念":"深度卷积神经网络用于同时进行二维和三维物体检测，区域提议网络生成二维区域提议，共享特征学习预测物体属性，独立模块预测视差并提取点云特征，多级融合原始图像和点云特征，视差用于前视图特征编码增强输入图像"},{"order":242,"title":"A Perceptual Measure for Deep Single Image Camera Calibration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hold-Geoffroy_A_Perceptual_Measure_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hold-Geoffroy_A_Perceptual_Measure_CVPR_2018_paper.html","abstract":"Most current single image camera calibration methods rely on specific image features or user input, and cannot be applied to natural images captured in uncontrolled settings. We propose inferring directly camera calibration parameters from a single image using a deep convolutional neural network. This network is trained using automatically generated samples from a large-scale panorama dataset, and considerably outperforms other methods, including recent deep learning-based approaches, in terms of standard L2 error. However, we argue that in many cases it is more important to consider how humans perceive errors in camera estimation. To this end, we conduct a large-scale human perception study where we ask users to judge the realism of 3D objects composited with and without ground truth camera calibration. Based on this study, we develop a new perceptual measure for camera calibration, and demonstrate that our deep calibration network outperforms other methods on this measure. Finally, we demonstrate the use of our calibration network for a number of applications including virtual object insertion, image retrieval and compositing.","中文标题":"一种用于深度单图像相机校准的感知度量","摘要翻译":"目前大多数单图像相机校准方法依赖于特定的图像特征或用户输入，无法应用于在不受控制的环境中捕获的自然图像。我们提出使用深度卷积神经网络直接从单张图像推断相机校准参数。该网络使用从大规模全景数据集中自动生成的样本进行训练，在标准L2误差方面显著优于其他方法，包括最近的基于深度学习的方法。然而，我们认为在许多情况下，考虑人类如何感知相机估计中的错误更为重要。为此，我们进行了一项大规模的人类感知研究，要求用户判断使用和不使用地面真实相机校准合成的3D物体的真实感。基于这项研究，我们开发了一种新的相机校准感知度量，并证明我们的深度校准网络在这种度量上优于其他方法。最后，我们展示了我们的校准网络在虚拟物体插入、图像检索和合成等多种应用中的使用。","领域":"相机校准/3D视觉/感知度量","问题":"如何在不受控制的环境中从单张自然图像中准确推断相机校准参数","动机":"现有方法依赖于特定图像特征或用户输入，无法广泛应用于自然图像，且缺乏对人类感知误差的考虑","方法":"使用深度卷积神经网络直接从单张图像推断相机校准参数，并通过大规模人类感知研究开发新的感知度量","关键词":["相机校准","深度卷积神经网络","感知度量","3D视觉"],"涉及的技术概念":"深度卷积神经网络用于从单张图像直接推断相机校准参数；大规模人类感知研究用于开发新的相机校准感知度量；应用包括虚拟物体插入、图像检索和合成"},{"order":243,"title":"Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xiong_Learning_to_Generate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xiong_Learning_to_Generate_CVPR_2018_paper.html","abstract":"Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to $128\\times 128$ resolution for 32 frames. Quantitative and qualitative experiment results have demonstrated the superiority of our model over the state-of-the-art models.","中文标题":"学习使用多阶段动态生成对抗网络生成延时视频","摘要翻译":"在户外拍摄一张照片，我们能否预测即时的未来，例如天空中的云将如何移动？我们通过提出一种基于生成对抗网络（GAN）的两阶段方法来解决这个问题，以生成高分辨率的逼真延时视频。给定第一帧，我们的模型学习生成长期的未来帧。第一阶段为每一帧生成具有真实内容的视频。第二阶段通过强制生成的视频在运动动态上更接近真实视频，来细化第一阶段生成的视频。为了进一步鼓励最终生成的视频中有生动的运动，我们使用Gram矩阵来更精确地建模运动。我们建立了一个大规模的延时数据集，并在这个新数据集上测试了我们的方法。使用我们的模型，我们能够生成分辨率高达128×128的32帧逼真视频。定量和定性的实验结果证明了我们的模型优于最先进的模型。","领域":"视频生成/延时摄影/动态建模","问题":"如何从单张户外照片预测并生成逼真的未来帧序列，即延时视频","动机":"探索从静态图像预测动态未来场景的可能性，特别是在自然现象如云层运动的场景中","方法":"采用两阶段生成对抗网络（GAN）方法，首先生成各帧内容，再通过运动动态细化视频，使用Gram矩阵精确建模运动","关键词":["视频生成","延时摄影","动态建模"],"涉及的技术概念":{"生成对抗网络（GAN）":"一种通过生成器和判别器相互竞争的框架，用于生成逼真的数据","Gram矩阵":"用于捕捉和建模图像或视频中的风格和动态特征，特别是在运动精确建模中","两阶段方法":"首先生成基础内容，然后通过细化阶段提升质量，特别是在视频生成中用于分别处理内容和动态"}},{"order":244,"title":"Document Enhancement Using Visibility Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kligler_Document_Enhancement_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kligler_Document_Enhancement_Using_CVPR_2018_paper.html","abstract":"This paper re-visits classical problems in document enhancement. Rather than proposing a new algorithm for a specific problem, we introduce a novel general approach. The key idea is to modify any state- of-the-art algorithm, by providing it with new information (input), improving its own results. Interestingly, this information is based on a solution to a seemingly unrelated problem of visibility detection in R3. We show that a simple representation of an image as a 3D point cloud, gives visibility detection on this cloud a new interpretation. What does it mean for a point to be visible? Although this question has been widely studied within computer vision, it has always been assumed that the point set is a sampling of a real scene. We show that the answer to this question in our context reveals unique and useful information about the image. We demonstrate the benefit of this idea for document binarization and for unshadowing.","中文标题":"使用可见性检测进行文档增强","摘要翻译":"本文重新审视了文档增强中的经典问题。我们并未针对特定问题提出新算法，而是引入了一种新颖的通用方法。关键思想是通过提供新信息（输入）来修改任何最先进的算法，从而改进其自身的结果。有趣的是，这些信息基于一个看似不相关的R3中可见性检测问题的解决方案。我们展示了将图像简单表示为3D点云，使得该云上的可见性检测有了新的解释。一个点可见意味着什么？尽管这个问题在计算机视觉领域已被广泛研究，但总是假设点集是真实场景的采样。我们展示了在我们的背景下，这个问题的答案揭示了关于图像的独特且有用的信息。我们展示了这一想法在文档二值化和去阴影方面的益处。","领域":"文档增强/可见性检测/3D点云","问题":"文档增强中的经典问题","动机":"通过提供新信息改进现有算法，探索可见性检测在文档增强中的应用","方法":"引入一种新颖的通用方法，通过提供新信息（基于R3中可见性检测问题的解决方案）来修改任何最先进的算法","关键词":["文档增强","可见性检测","3D点云","文档二值化","去阴影"],"涉及的技术概念":"本文涉及的技术概念包括文档增强、可见性检测、3D点云表示、文档二值化和去阴影。通过将图像表示为3D点云，并利用可见性检测的新解释，本文探索了这些技术在改进文档处理算法中的应用。"},{"order":245,"title":"A Weighted Sparse Sampling and Smoothing Frame Transition Approach for Semantic Fast-Forward First-Person Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Silva_A_Weighted_Sparse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Silva_A_Weighted_Sparse_CVPR_2018_paper.html","abstract":"Thanks to the advances in the technology of low-cost digital cameras and the popularity of the self-recording culture, the amount of visual data on the Internet is going to the opposite side of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched in a computer folder or website. In this work, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem, which combined with a smoothing frame transition method accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. The experiments show that our method is able to fast-forward videos to retain as much relevant information and smoothness as the state-of-the-art techniques in less time. We also present a new 80-hour multimodal (RGB-D, IMU, and GPS) dataset of first-person videos with annotations for recorder profile, frame scene, activities, interaction, and attention.","中文标题":"一种用于语义快速前进第一人称视频的加权稀疏采样和平滑帧过渡方法","摘要翻译":"由于低成本数码相机技术的进步和自我录制文化的普及，互联网上的视觉数据量与用户可用时间和耐心成反比增长。因此，大多数上传的视频注定会被遗忘，无人观看，存放在电脑文件夹或网站上。在这项工作中，我们解决了创建平滑快速前进视频而不丢失相关内容的问题。我们提出了一种新的自适应帧选择方法，将其表述为加权最小重建问题，结合平滑帧过渡方法，加速第一人称视频，强调相关片段并避免视觉不连续性。实验表明，我们的方法能够在更短的时间内快速前进视频，保留尽可能多的相关信息和平滑度，与最先进的技术相当。我们还提出了一个新的80小时多模态（RGB-D、IMU和GPS）第一人称视频数据集，带有记录者档案、帧场景、活动、互动和注意力的注释。","领域":"视频处理/第一人称视角/数据压缩","问题":"创建平滑快速前进视频而不丢失相关内容","动机":"解决因视觉数据量增长与用户可用时间和耐心成反比导致的大多数上传视频被遗忘和无人观看的问题","方法":"提出一种新的自适应帧选择方法，将其表述为加权最小重建问题，结合平滑帧过渡方法，加速第一人称视频，强调相关片段并避免视觉不连续性","关键词":["视频处理","第一人称视角","数据压缩","平滑帧过渡","自适应帧选择"],"涉及的技术概念":"加权最小重建问题是一种优化问题，旨在通过最小化重建误差来选择最重要的帧。平滑帧过渡方法用于在视频快速前进时保持视觉连续性。RGB-D、IMU和GPS是多模态数据采集技术，分别代表彩色深度图像、惯性测量单元和全球定位系统，用于丰富第一人称视频的数据集。"},{"order":246,"title":"Context Contrasted Feature and Gated Multi-Scale Aggregation for Scene Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Context_Contrasted_Feature_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ding_Context_Contrasted_Feature_CVPR_2018_paper.html","abstract":"Scene segmentation is a challenging task as it need label every pixel in the image. It is crucial to exploit discriminative context and aggregate multi-scale features to achieve better segmentation. In this paper, we first propose a novel context contrasted local feature that not only leverages the informative context but also spotlights the local information in contrast to the context. The proposed context contrasted local feature greatly improves the parsing performance, especially for inconspicuous objects and background stuff. Furthermore, we propose a scheme of gated sum to selectively aggregate multi-scale features for each spatial position. The gates in this scheme control the information flow of different scale features. Their values are generated from the testing image by the proposed network learnt from the training data so that they are adaptive not only to the training data, but also to the specific testing image. Without bells and whistles, the proposed approach achieves the state-of-the-arts consistently on the three popular scene segmentation datasets, Pascal Context, SUN-RGBD and COCO Stuff.","中文标题":"场景分割中的上下文对比特征与门控多尺度聚合","摘要翻译":"场景分割是一项具有挑战性的任务，因为它需要为图像中的每个像素打上标签。利用区分性上下文并聚合多尺度特征以实现更好的分割至关重要。在本文中，我们首先提出了一种新颖的上下文对比局部特征，该特征不仅利用了信息丰富的上下文，而且通过与上下文对比来突出局部信息。所提出的上下文对比局部特征大大提高了解析性能，特别是对于不显眼的物体和背景物品。此外，我们提出了一种门控求和的方案，以选择性地聚合每个空间位置的多尺度特征。该方案中的门控制不同尺度特征的信息流。它们的值由从训练数据中学习到的网络从测试图像中生成，因此它们不仅适应于训练数据，而且适应于特定的测试图像。无需任何花哨的技巧，所提出的方法在三个流行的场景分割数据集Pascal Context、SUN-RGBD和COCO Stuff上一致地达到了最先进的水平。","领域":"场景理解/语义分割/特征提取","问题":"如何有效地利用上下文信息和多尺度特征进行场景分割","动机":"提高场景分割的准确性，特别是对于不显眼的物体和背景物品","方法":"提出上下文对比局部特征和门控多尺度特征聚合方案","关键词":["场景分割","上下文对比","门控多尺度聚合"],"涉及的技术概念":"上下文对比局部特征通过对比上下文信息来突出局部特征，门控多尺度聚合通过门控机制选择性地聚合不同尺度的特征，以提高场景分割的准确性。"},{"order":247,"title":"Deep Layer Aggregation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.html","abstract":"Visual recognition requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improves inference of what and where. Architectural efforts are exploring many dimensions for network backbones, designing deeper or wider architectures, but how to best aggregate layers and blocks across a network deserves further attention. Although skip connections have been incorporated to combine layers, these connections have been \`\`shallow''  themselves, and only fuse by simple, one-step operations. We augment standard architectures with deeper aggregation to better fuse information across layers. Our deep layer aggregation structures iteratively and hierarchically merge the feature hierarchy to make networks with better accuracy and fewer parameters. Experiments across architectures and tasks show that deep layer aggregation improves recognition and resolution compared to existing branching and merging schemes.","中文标题":"深层聚合","摘要翻译":"视觉识别需要跨越从低到高的层次、从小到大的尺度、从细到粗的分辨率的丰富表示。即使在卷积网络的特征深度中，单独的一层是不够的：复合和聚合这些表示可以改善对内容和位置的推断。架构设计正在探索网络骨干的多个维度，设计更深或更宽的架构，但如何最好地聚合网络中的层和块值得进一步关注。尽管已经引入了跳跃连接来结合层，但这些连接本身是“浅”的，并且仅通过简单的一步操作进行融合。我们通过更深的聚合来增强标准架构，以更好地跨层融合信息。我们的深层聚合结构迭代和分层地合并特征层次，以构建具有更好准确性和更少参数的网络。跨架构和任务的实验表明，与现有的分支和合并方案相比，深层聚合提高了识别和分辨率。","领域":"卷积神经网络/特征表示/网络架构设计","问题":"如何有效地聚合网络中的层和块以改善视觉识别的准确性和分辨率","动机":"现有的跳跃连接方法在结合层时过于简单，无法充分利用网络中各层的信息，需要更有效的方法来融合跨层的信息","方法":"通过引入深层聚合结构，迭代和分层地合并特征层次，以构建具有更好准确性和更少参数的网络","关键词":["卷积神经网络","特征表示","网络架构设计","深层聚合","视觉识别"],"涉及的技术概念":"深层聚合结构是一种通过迭代和分层方式合并特征层次的技术，旨在提高卷积神经网络在视觉识别任务中的准确性和分辨率。这种方法通过增强标准架构，使得网络能够更有效地融合跨层的信息，从而在保持或减少参数数量的同时，提高识别性能。"},{"order":248,"title":"Convolutional Neural Networks With Alternately Updated Clique","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Convolutional_Neural_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Convolutional_Neural_Networks_CVPR_2018_paper.html","abstract":"Improving information flow in deep networks helps to ease the training difficulties and utilize parameters more efficiently. Here we propose a new convolutional neural network architecture with alternately updated clique (CliqueNet). In contrast to prior networks, there are both forward and backward connections between any two layers in the same block. The layers are constructed as a loop and are updated alternately. The CliqueNet has some unique properties. For each layer, it is both the input and output of any other layer in the same block, so that the information flow among layers is maximized. During propagation, the newly updated layers are concatenated to re-update previously updated layer, and parameters are reused for multiple times. This recurrent feedback structure is able to bring higher level visual information back to refine low-level filters and achieve spatial attention. We analyze the features generated at different stages and observe that using refined features leads to a better result. We adopt a multi-scale feature strategy that effectively avoids the progressive growth of parameters. Experiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet show that our proposed models achieve the state-of-the-art performance with fewer parameters.","中文标题":"交替更新团卷积神经网络","摘要翻译":"改进深度网络中的信息流动有助于缓解训练难度并更有效地利用参数。在此，我们提出了一种新的卷积神经网络架构——交替更新团（CliqueNet）。与之前的网络不同，同一块中的任何两层之间都有前向和后向连接。这些层被构建为一个循环，并交替更新。CliqueNet具有一些独特的属性。对于每一层来说，它既是同一块中任何其他层的输入也是输出，从而最大化层间的信息流动。在传播过程中，新更新的层被连接起来以重新更新之前更新的层，并且参数被多次重用。这种循环反馈结构能够将更高层次的视觉信息带回以细化低级过滤器并实现空间注意力。我们分析了不同阶段生成的特征，并观察到使用细化特征可以带来更好的结果。我们采用了一种多尺度特征策略，有效避免了参数的逐步增长。在包括CIFAR-10、CIFAR-100、SVHN和ImageNet在内的图像识别数据集上的实验表明，我们提出的模型以更少的参数实现了最先进的性能。","领域":"卷积神经网络/图像识别/深度学习","问题":"如何改进深度网络中的信息流动以缓解训练难度并更有效地利用参数","动机":"为了缓解深度网络训练中的困难并提高参数利用效率","方法":"提出了一种新的卷积神经网络架构——交替更新团（CliqueNet），通过在同一块中的任何两层之间建立前向和后向连接，构建循环并交替更新层，以最大化层间的信息流动","关键词":["卷积神经网络","信息流动","参数重用","多尺度特征","图像识别"],"涉及的技术概念":"交替更新团（CliqueNet）是一种新的卷积神经网络架构，通过在同一块中的任何两层之间建立前向和后向连接，构建循环并交替更新层，以最大化层间的信息流动。这种结构允许参数被多次重用，并能够将更高层次的视觉信息带回以细化低级过滤器并实现空间注意力。此外，采用多尺度特征策略有效避免了参数的逐步增长。"},{"order":249,"title":"Practical Block-Wise Neural Network Architecture Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhong_Practical_Block-Wise_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhong_Practical_Block-Wise_Neural_CVPR_2018_paper.html","abstract":"Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy.The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.","中文标题":"实用的块级神经网络架构生成","摘要翻译":"卷积神经网络在计算机视觉领域取得了显著的成就。然而，大多数可用的网络架构都是手工设计的，通常需要专业知识和精心设计。在本文中，我们提供了一个称为BlockQNN的块级网络生成管道，它使用Q-Learning范式与epsilon-greedy探索策略自动构建高性能网络。最优网络块由学习代理构建，该代理被顺序训练以选择组件层。我们将这些块堆叠起来构建整个自动生成的网络。为了加速生成过程，我们还提出了一个分布式异步框架和早期停止策略。块级生成带来了独特的优势：（1）与手工设计的最先进网络相比，它在图像分类上表现出竞争性的结果，此外，BlockQNN生成的最佳网络在CIFAR-10上达到了3.54%的top-1错误率，击败了所有现有的自动生成网络。（2）同时，它大大减少了设计网络时的搜索空间，仅用32个GPU花费了3天时间，（3）此外，它具有很强的泛化能力，在CIFAR上构建的网络在更大规模的ImageNet数据集上也表现良好。","领域":"神经网络架构搜索/自动化机器学习/强化学习","问题":"自动生成高性能的卷积神经网络架构","动机":"减少手工设计网络架构的需求，降低设计复杂性和时间成本","方法":"使用Q-Learning范式与epsilon-greedy探索策略自动构建网络块，并通过堆叠这些块来构建整个网络。采用分布式异步框架和早期停止策略加速生成过程。","关键词":["神经网络架构搜索","自动化机器学习","强化学习","Q-Learning","epsilon-greedy探索策略","分布式异步框架","早期停止策略"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据。","Q-Learning":"一种无模型的强化学习算法，用于学习动作价值函数。","epsilon-greedy探索策略":"一种在探索和利用之间进行权衡的策略，以一定的概率选择随机动作（探索），否则选择当前认为最优的动作（利用）。","分布式异步框架":"一种计算框架，允许多个计算任务在不同的计算节点上并行执行，以提高计算效率。","早期停止策略":"一种防止过拟合的技术，通过在验证集上的性能不再提升时提前停止训练。"}},{"order":250,"title":"xUnit: Learning a Spatial Activation Function for Efficient Image Restoration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kligvasser_xUnit_Learning_a_CVPR_2018_paper.html","abstract":"In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance.","中文标题":"xUnit: 学习一种空间激活函数以实现高效的图像恢复","摘要翻译":"近年来，深度神经网络（DNNs）在许多低级视觉任务中取得了前所未有的性能。然而，最先进的结果通常是由非常深的网络实现的，这些网络可以达到数十层，拥有数千万个参数。为了使DNNs在资源有限的平台上可实施，有必要削弱性能与效率之间的权衡。在本文中，我们提出了一种新的激活单元，特别适用于图像恢复问题。与广泛使用的逐像素激活单元（如ReLUs和sigmoids）相比，我们的单元实现了一种具有空间连接的可学习非线性函数。这使得网络能够捕捉到更复杂的特征，从而在达到相同性能的情况下，显著减少所需的层数。我们通过对去噪、去雨和超分辨率等已经被认为非常小的最先进网络进行实验，展示了我们单元的有效性。通过我们的方法，我们能够进一步将这些模型减少近50%，而不会导致任何性能下降。","领域":"图像恢复/去噪/超分辨率","问题":"如何在资源有限的平台上实现高效的图像恢复","动机":"为了削弱深度神经网络在图像恢复任务中性能与效率之间的权衡","方法":"提出了一种新的激活单元，该单元实现了一种具有空间连接的可学习非线性函数，以减少网络层数同时保持性能","关键词":["图像恢复","去噪","超分辨率","去雨"],"涉及的技术概念":"深度神经网络（DNNs）、激活单元、空间连接、非线性函数、去噪、去雨、超分辨率"},{"order":251,"title":"Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Crafting_a_Toolchain_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Crafting_a_Toolchain_CVPR_2018_paper.html","abstract":"We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.","中文标题":"通过深度强化学习构建图像修复工具链","摘要翻译":"我们研究了一种通过强化学习进行图像修复的新方法。与现有研究大多训练一个大型网络来完成特定任务不同，我们准备了一个工具箱，其中包含不同复杂度的小规模卷积网络，专门用于不同的任务。我们的方法RL-Restore随后学习一种策略，从工具箱中选择适当的工具，逐步恢复损坏图像的质量。我们制定了一个逐步奖励函数，该函数与每一步图像修复的好坏成正比，以学习动作策略。我们还设计了一种联合学习方案，以训练代理和工具，以更好地处理不确定性。与传统的由人类设计的网络相比，RL-Restore能够以更参数高效的方式使用动态形成的工具链来修复具有复杂和未知失真的图像。","领域":"图像修复/强化学习/卷积神经网络","问题":"如何有效地恢复损坏图像的质量","动机":"现有方法通常训练单一大型网络处理特定任务，缺乏灵活性和效率","方法":"准备一个包含不同复杂度小规模卷积网络的工具箱，通过强化学习策略选择适当工具逐步恢复图像质量，并设计逐步奖励函数和联合学习方案","关键词":["图像修复","强化学习","卷积神经网络","工具链","逐步奖励函数","联合学习"],"涉及的技术概念":"强化学习（Reinforcement Learning, RL）是一种让智能体（agent）通过与环境交互来学习策略的方法，目的是最大化某种累积奖励。卷积神经网络（Convolutional Neural Networks, CNNs）是一种深度学习模型，特别适合处理图像数据。图像修复是指从损坏或降质的图像中恢复出高质量图像的过程。工具链（Toolchain）在这里指的是一系列专门设计用于不同任务的工具或模型的集合。逐步奖励函数（Step-wise Reward Function）是一种根据每一步行动的效果来给予奖励的机制，用于指导智能体学习最优策略。联合学习方案（Joint Learning Scheme）是指同时训练多个模型或组件，以提高整体性能的方法。"},{"order":252,"title":"Deformation Aware Image Compression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shaham_Deformation_Aware_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shaham_Deformation_Aware_Image_CVPR_2018_paper.html","abstract":"Lossy compression algorithms aim to compactly encode images in a way which enables to restore them with minimal error. We show that a key limitation of existing algorithms is that they rely on error measures that are extremely sensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder to invest many bits in describing the exact geometry of every fine detail in the image, which is obviously wasteful, because the human visual system is indifferent to small local translations. Motivated by this observation, we propose a deformation-insensitive error measure that can be easily incorporated into any existing compression scheme. As we show, optimal compression under our criterion involves slightly deforming the input image such that it becomes more \\"compressible\\". Surprisingly, while these small deformations are barely noticeable, they enable the CODEC to preserve details that are otherwise completely lost. Our technique uses the CODEC as a \\"black box\\", thus allowing simple integration with arbitrary compression methods. Extensive experiments, including user studies, confirm that our approach significantly improves the visual quality of many CODECs. These include JPEG, JPEG~2000, WebP, BPG, and a recent deep-net method.","中文标题":"变形感知图像压缩","摘要翻译":"有损压缩算法旨在以紧凑的方式编码图像，以便能够以最小的误差恢复它们。我们展示了现有算法的一个关键限制是它们依赖于对几何变形极其敏感的错误度量（例如SSD，SSIM）。这些迫使编码器投入许多比特来描述图像中每个细节的精确几何形状，这显然是浪费的，因为人类视觉系统对小局部平移不敏感。基于这一观察，我们提出了一种对变形不敏感的错误度量，可以轻松地融入任何现有的压缩方案中。正如我们所展示的，在我们的标准下进行的最优压缩涉及对输入图像进行轻微变形，使其变得更容易“压缩”。令人惊讶的是，虽然这些小变形几乎不可察觉，但它们使编解码器能够保留否则会完全丢失的细节。我们的技术将编解码器视为“黑箱”，从而允许与任意压缩方法的简单集成。包括用户研究在内的广泛实验证实，我们的方法显著提高了许多编解码器的视觉质量。这些包括JPEG，JPEG 2000，WebP，BPG和最近的深度网络方法。","领域":"图像压缩/视觉质量评估/编解码器优化","问题":"现有图像压缩算法对几何变形敏感，导致编码效率低下","动机":"人类视觉系统对小局部平移不敏感，现有算法在描述图像细节的精确几何形状上浪费比特","方法":"提出一种对变形不敏感的错误度量，通过轻微变形输入图像使其更易压缩，同时保留细节","关键词":["图像压缩","变形不敏感","编解码器优化"],"涉及的技术概念":"SSD（平方差和）、SSIM（结构相似性）、JPEG、JPEG 2000、WebP、BPG、深度网络方法"},{"order":253,"title":"Distributable Consistent Multi-Object Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Distributable_Consistent_Multi-Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Distributable_Consistent_Multi-Object_CVPR_2018_paper.html","abstract":"In this paper we propose an optimization-based framework to multiple object matching. The framework takes maps computed between pairs of objects as input, and outputs maps that are consistent among all pairs of objects. The central idea of our approach is to divide the input object collection into overlapping sub-collections and enforce map consistency among each sub-collection. This leads to a distributed formulation, which is scalable to large-scale datasets. We also present an equivalence condition between this decoupled scheme and the original scheme. Experiments on both synthetic and real-world datasets show that our framework is competitive against state-of-the-art multi-object matching techniques.","中文标题":"可分配的一致性多对象匹配","摘要翻译":"在本文中，我们提出了一个基于优化的多对象匹配框架。该框架以对象对之间计算的地图作为输入，并输出在所有对象对之间一致的地图。我们方法的核心思想是将输入的对象集合划分为重叠的子集，并在每个子集中强制地图一致性。这导致了一个分布式公式，该公式可扩展到大规模数据集。我们还提出了这种解耦方案与原始方案之间的等价条件。在合成和真实世界数据集上的实验表明，我们的框架与最先进的多对象匹配技术相比具有竞争力。","领域":"多对象匹配/分布式计算/优化算法","问题":"解决多对象匹配中的一致性问题","动机":"为了提高多对象匹配的一致性和可扩展性，特别是在处理大规模数据集时","方法":"将输入的对象集合划分为重叠的子集，并在每个子集中强制地图一致性，采用分布式公式以提高可扩展性","关键词":["多对象匹配","分布式计算","优化算法"],"涉及的技术概念":"多对象匹配指的是在多个对象之间找到一致的对应关系；分布式计算是一种计算方法，它通过将问题分解为可以在多个计算节点上并行处理的小任务来提高效率；优化算法是寻找最优解或近似最优解的算法，通常用于解决复杂的数学问题。"},{"order":254,"title":"Residual Dense Network for Image Super-Resolution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Residual_Dense_Network_CVPR_2018_paper.html","abstract":"In this paper, we propose dense feature fusion (DFF) for image super-resolution (SR). As the same content in different natural images often have various scales and angles of view, jointly leaning hierarchical features is essential for image SR. On the other hand, very deep convolutional neural network (CNN) has recently achieved great success for image SR and offered hierarchical features as well. However, most of deep CNN based SR models neglect to jointly make full use of the hierarchical features. In addition, dense connected layers would allow the network to be deeper, efficient to train, and more powerful. To embrace these observations, in our proposed DFF model, we fully exploit all the meaningful convolutional features in local and global manners. Specifically, we use dense connected convolutional layers to extract abundant local features. We use local feature fusion to adaptively learn more efficient features from preceding and current local features. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Extensive experiments on benchmark datasets show that our DFF achieves favorable performance against state-of-the-art methods quantitatively and visually.","中文标题":"用于图像超分辨率的残差密集网络","摘要翻译":"在本文中，我们提出了用于图像超分辨率（SR）的密集特征融合（DFF）。由于不同自然图像中的相同内容往往具有不同的尺度和视角，联合学习层次特征对于图像SR至关重要。另一方面，非常深的卷积神经网络（CNN）最近在图像SR方面取得了巨大成功，并提供了层次特征。然而，大多数基于深度CNN的SR模型忽视了联合充分利用层次特征。此外，密集连接的层将使网络更深，训练更高效，更强大。为了拥抱这些观察结果，在我们提出的DFF模型中，我们充分利用了所有有意义的卷积特征，以局部和全局的方式。具体来说，我们使用密集连接的卷积层来提取丰富的局部特征。我们使用局部特征融合从前面的和当前的局部特征中自适应地学习更有效的特征。在完全获得密集的局部特征后，我们使用全局特征融合以整体的方式联合和自适应地学习全局层次特征。在基准数据集上的大量实验表明，我们的DFF在定量和视觉上都优于最先进的方法。","领域":"图像超分辨率/卷积神经网络/特征融合","问题":"如何更有效地利用卷积神经网络中的层次特征进行图像超分辨率","动机":"现有的深度卷积神经网络模型在图像超分辨率方面虽然取得了成功，但大多忽视了联合充分利用层次特征的重要性，且密集连接的层可以使网络更深、训练更高效、更强大。","方法":"提出了密集特征融合（DFF）模型，通过密集连接的卷积层提取丰富的局部特征，并使用局部特征融合和全局特征融合方法，从前面的和当前的局部特征中自适应地学习更有效的特征，以及以整体的方式联合和自适应地学习全局层次特征。","关键词":["图像超分辨率","卷积神经网络","特征融合"],"涉及的技术概念":"密集特征融合（DFF）是一种用于图像超分辨率的技术，通过密集连接的卷积层提取局部特征，并通过局部和全局特征融合方法，从这些特征中学习更有效的层次特征。这种方法旨在充分利用卷积神经网络中的层次特征，以提高图像超分辨率的性能。"},{"order":255,"title":"Attentive Generative Adversarial Network for Raindrop Removal From a Single Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qian_Attentive_Generative_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qian_Attentive_Generative_Adversarial_CVPR_2018_paper.html","abstract":"Raindrops adhered to a glass window or camera lens can severely hamper the visibility of a background scene and degrade an image considerably. In this paper, we address the problem by visually removing raindrops, and thus transforming a raindrop degraded image into a clean one. The problem is intractable, since first the regions occluded by raindrops are not given. Second, the information about the background scene of the occluded regions is completely lost for most part. To resolve the problem, we apply an attentive generative network using adversarial training. Our main idea is to inject visual attention into both the generative and discriminative networks. During the training, our visual attention  learns about raindrop regions and their surroundings. Hence, by injecting this information, the generative network will pay more attention to the raindrop regions and the surrounding structures, and the discriminative network will be able to assess the local consistency of the restored regions. This injection of visual attention to both generative and discriminative networks is the main contribution of this paper. Our experiments show the effectiveness of our approach, which outperforms the state of the art methods quantitatively and qualitatively.","中文标题":"注意力生成对抗网络用于单幅图像雨滴去除","摘要翻译":"附着在玻璃窗或相机镜头上的雨滴会严重阻碍背景场景的可见性，并显著降低图像质量。在本文中，我们通过视觉上移除雨滴来解决这个问题，从而将受雨滴影响的图像转换为干净的图像。这个问题是难以解决的，因为首先，被雨滴遮挡的区域是未知的。其次，关于被遮挡区域背景场景的信息大部分完全丢失。为了解决这个问题，我们应用了一个使用对抗训练的注意力生成网络。我们的主要想法是将视觉注意力注入到生成网络和判别网络中。在训练过程中，我们的视觉注意力学习雨滴区域及其周围环境。因此，通过注入这些信息，生成网络将更加关注雨滴区域和周围结构，判别网络将能够评估恢复区域的局部一致性。这种将视觉注意力注入到生成和判别网络中的方法是本文的主要贡献。我们的实验显示了我们的方法的有效性，在数量和质量上都优于现有技术。","领域":"图像修复/生成对抗网络/视觉注意力机制","问题":"单幅图像中雨滴的视觉去除","动机":"雨滴附着在玻璃窗或相机镜头上会严重影响背景场景的可见性和图像质量，需要一种有效的方法来去除雨滴，恢复图像质量。","方法":"应用注意力生成对抗网络，通过将视觉注意力注入到生成网络和判别网络中，使生成网络更加关注雨滴区域和周围结构，判别网络评估恢复区域的局部一致性。","关键词":["图像修复","生成对抗网络","视觉注意力机制"],"涉及的技术概念":"本文涉及的技术概念包括生成对抗网络（GANs）、视觉注意力机制、图像修复技术。生成对抗网络用于生成高质量的图像，视觉注意力机制帮助网络集中注意力于图像的关键区域，如图像中的雨滴区域及其周围环境，图像修复技术用于从受影响的图像中恢复出干净的图像。"},{"order":256,"title":"FSRNet: End-to-End Learning Face Super-Resolution With Facial Priors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.html","abstract":"Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to superresolve very low-resolution (LR) face images without wellaligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.","中文标题":"FSRNet: 端到端学习利用面部先验进行人脸超分辨率","摘要翻译":"人脸超分辨率（SR）是一个特定领域的超分辨率问题。利用面部先验知识可以更好地超分辨率人脸图像。我们提出了一种新颖的深度端到端可训练的人脸超分辨率网络（FSRNet），该网络利用几何先验，即面部地标热图和解析图，来超分辨率非常低分辨率（LR）的人脸图像，而无需良好对齐的要求。具体来说，我们首先构建一个粗糙的SR网络来恢复粗糙的高分辨率（HR）图像。然后，粗糙的HR图像被发送到两个分支：一个精细的SR编码器和一个先验信息估计网络，它们分别提取图像特征并估计地标热图/解析图。图像特征和先验信息都被发送到精细的SR解码器以恢复HR图像。为了生成逼真的人脸，我们还提出了人脸超分辨率生成对抗网络（FSRGAN），将对抗性损失纳入FSRNet。此外，我们引入了两个相关任务，人脸对齐和解析，作为人脸SR的新评估指标，解决了经典指标与视觉感知不一致的问题。大量实验表明，FSRNet和FSRGAN在非常低分辨率的人脸SR方面显著优于现有技术，无论是定量还是定性。","领域":"人脸超分辨率/生成对抗网络/图像恢复","问题":"解决非常低分辨率人脸图像的超分辨率问题","动机":"利用面部先验知识提高人脸图像超分辨率的效果","方法":"提出FSRNet网络，利用几何先验（面部地标热图和解析图）进行端到端训练，并引入FSRGAN以生成更逼真的人脸图像","关键词":["人脸超分辨率","生成对抗网络","图像恢复"],"涉及的技术概念":"FSRNet是一种深度端到端可训练的网络，利用面部地标热图和解析图作为几何先验，通过粗糙SR网络和精细SR编码器及先验信息估计网络来恢复高分辨率图像。FSRGAN通过引入对抗性损失来生成更逼真的人脸图像。此外，人脸对齐和解析任务被用作新的评估指标，以解决经典指标与视觉感知不一致的问题。"},{"order":257,"title":"Burst Denoising With Kernel Prediction Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mildenhall_Burst_Denoising_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mildenhall_Burst_Denoising_With_CVPR_2018_paper.html","abstract":"We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data.","中文标题":"使用核预测网络进行突发去噪","摘要翻译":"我们提出了一种技术，用于联合去噪从手持相机拍摄的图像突发。特别是，我们提出了一种卷积神经网络架构，用于预测可以对齐和去噪帧的空间变化核，一种基于现实噪声形成模型的合成数据生成方法，以及一种通过退火损失函数引导的优化，以避免不希望的局部最小值。我们的模型在真实和合成数据上，在广泛的噪声水平范围内，匹配或超越了最先进的技术。","领域":"图像去噪/卷积神经网络/合成数据生成","问题":"解决手持相机拍摄的图像突发中的去噪问题","动机":"提高图像突发去噪的效果，特别是在不同噪声水平下","方法":"提出了一种卷积神经网络架构用于预测空间变化核，采用基于现实噪声形成模型的合成数据生成方法，以及通过退火损失函数引导的优化","关键词":["图像去噪","卷积神经网络","合成数据生成","空间变化核","退火损失函数"],"涉及的技术概念":{"卷积神经网络架构":"用于预测可以对齐和去噪帧的空间变化核的神经网络架构","合成数据生成":"基于现实噪声形成模型生成用于训练和测试的合成数据","退火损失函数":"一种优化方法，通过逐渐降低损失函数的温度来避免陷入局部最小值"}},{"order":258,"title":"Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qu_Unsupervised_Sparse_Dirichlet-Net_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qu_Unsupervised_Sparse_Dirichlet-Net_CVPR_2018_paper.html","abstract":"In many computer vision applications, obtaining images of high resolution in both the spatial and spectral domains are equally important. However, due to hardware limitations, one can only expect to acquire images of high resolution in either the spatial or spectral domains. This paper focuses on hyperspectral image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low spatial resolution (LR) but high spectral resolution is fused with a multispectral image (MSI) with high spatial resolution (HR) but low spectral resolution to obtain HR HSI. Existing deep learning-based solutions are all supervised that would need a large training set and the availability of HR HSI, which is unrealistic. Here, we make the first attempt to solving the HSI-SR problem using an unsupervised encoder-decoder architecture that carries the following uniquenesses. First, it is composed of two encoder-decoder networks, coupled through a shared decoder, in order to preserve the rich spectral information from the HSI network. Second, the network encourages the representations from both modalities to follow a sparse Dirichlet distribution which naturally incorporates the two physical constraints of HSI and MSI. Third, the angular difference between representations are minimized in order to reduce the spectral distortion. We refer to the proposed architecture as unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results demonstrate the superior performance of uSDN as compared to the state-of-the-art.","中文标题":"无监督稀疏狄利克雷网络用于高光谱图像超分辨率","摘要翻译":"在许多计算机视觉应用中，获取在空间和光谱域都具有高分辨率的图像同样重要。然而，由于硬件限制，人们只能期望获取在空间或光谱域中具有高分辨率的图像。本文专注于高光谱图像超分辨率（HSI-SR），其中具有低空间分辨率（LR）但高光谱分辨率的高光谱图像（HSI）与具有高空间分辨率（HR）但低光谱分辨率的多光谱图像（MSI）融合，以获得HR HSI。现有的基于深度学习的解决方案都是监督式的，需要大量的训练集和HR HSI的可用性，这是不现实的。在这里，我们首次尝试使用无监督的编码器-解码器架构来解决HSI-SR问题，该架构具有以下独特性。首先，它由两个编码器-解码器网络组成，通过共享的解码器耦合，以保留来自HSI网络的丰富光谱信息。其次，网络鼓励两种模态的表示遵循稀疏狄利克雷分布，这自然包含了HSI和MSI的两个物理约束。第三，表示之间的角度差异被最小化，以减少光谱失真。我们将提出的架构称为无监督稀疏狄利克雷网络，或uSDN。大量的实验结果表明，与最先进的技术相比，uSDN具有优越的性能。","领域":"高光谱图像处理/图像超分辨率/无监督学习","问题":"高光谱图像超分辨率","动机":"由于硬件限制，无法同时获取在空间和光谱域都具有高分辨率的图像，需要一种方法将低空间分辨率的高光谱图像与高空间分辨率的多光谱图像融合，以获得高分辨率的高光谱图像。","方法":"采用无监督的编码器-解码器架构，该架构由两个编码器-解码器网络组成，通过共享的解码器耦合，以保留来自HSI网络的丰富光谱信息，并鼓励两种模态的表示遵循稀疏狄利克雷分布，同时最小化表示之间的角度差异以减少光谱失真。","关键词":["高光谱图像","超分辨率","无监督学习","稀疏狄利克雷分布"],"涉及的技术概念":"高光谱图像（HSI）是指包含丰富光谱信息的图像，通常用于遥感等领域。超分辨率是指从低分辨率图像中恢复高分辨率图像的技术。无监督学习是一种机器学习方法，它不需要标注数据来训练模型。稀疏狄利克雷分布是一种概率分布，用于模型中的稀疏性约束。"},{"order":259,"title":"Dynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Dynamic_Scene_Deblurring_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Dynamic_Scene_Deblurring_CVPR_2018_paper.html","abstract":"Due to the spatially variant blur caused by camera shake and object motions under different scene depths, deblurring images captured from dynamic scenes is challenging. Although recent works based on deep neural networks have shown great progress on this problem, their models are usually large and computationally expensive. In this paper, we propose a novel spatially variant neural network to address the problem. The proposed network is composed of three deep convolutional neural networks (CNNs) and a recurrent neural network (RNN). RNN is used as a deconvolution operator performed on feature maps extracted from the input image by one of the CNNs. Another CNN is used to learn the weights for the RNN at every location. As a result, the RNN is spatially variant and could implicitly model the deblurring process with spatially variant kernels. The third CNN is used to reconstruct the final deblurred feature maps into restored image. The whole network is end-to-end trainable. Our analysis shows that the proposed network has a large receptive field even with a small model size. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of accuracy, speed, and model size.","中文标题":"使用空间变异循环神经网络进行动态场景去模糊","摘要翻译":"由于相机抖动和不同场景深度下的物体运动引起的空间变异模糊，从动态场景中捕获的图像去模糊具有挑战性。尽管最近基于深度神经网络的工作在这个问题上取得了很大进展，但它们的模型通常很大且计算成本高。在本文中，我们提出了一种新颖的空间变异神经网络来解决这个问题。所提出的网络由三个深度卷积神经网络（CNNs）和一个循环神经网络（RNN）组成。RNN被用作对由其中一个CNN从输入图像中提取的特征图执行的反卷积操作。另一个CNN用于在每个位置学习RNN的权重。因此，RNN是空间变异的，并且可以隐式地模拟具有空间变异核的去模糊过程。第三个CNN用于将最终的去模糊特征图重建为恢复的图像。整个网络是端到端可训练的。我们的分析表明，即使模型尺寸较小，所提出的网络也具有较大的感受野。在公共数据集上的定量和定性评估表明，所提出的方法在准确性、速度和模型大小方面优于最先进的算法。","领域":"图像去模糊/神经网络/动态场景分析","问题":"动态场景中由于相机抖动和物体运动引起的空间变异模糊问题","动机":"解决现有深度神经网络模型在动态场景去模糊中模型大、计算成本高的问题","方法":"提出了一种由三个深度卷积神经网络和一个循环神经网络组成的空间变异神经网络，其中循环神经网络用于执行反卷积操作，另一个卷积神经网络用于学习循环神经网络的权重，第三个卷积神经网络用于重建最终的去模糊图像","关键词":["图像去模糊","空间变异","循环神经网络","卷积神经网络"],"涉及的技术概念":"空间变异模糊、深度卷积神经网络（CNNs）、循环神经网络（RNN）、反卷积操作、感受野、端到端训练"},{"order":260,"title":"SPLATNet: Sparse Lattice Networks for Point Cloud Processing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.html","abstract":"We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Naively applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.","中文标题":"SPLATNet: 用于点云处理的稀疏格子网络","摘要翻译":"我们提出了一种用于处理点云的网络架构，该架构直接操作于表示为高维格子中稀疏样本集合的点集。随着格子大小的增加，直接在这种格子上应用卷积在内存和计算成本方面都表现不佳。相反，我们的网络使用稀疏双边卷积层作为构建块。这些层通过使用索引结构仅在格子的占用部分应用卷积来保持效率，并允许灵活指定格子结构，从而实现分层和空间感知的特征学习，以及联合2D-3D推理。基于点和基于图像的表示都可以轻松地融入具有此类层的网络中，并且可以以端到端的方式训练生成的模型。我们在3D分割任务上展示了结果，其中我们的方法优于现有的最先进技术。","领域":"点云处理/3D分割/稀疏卷积","问题":"处理高维格子中稀疏样本集合的点云数据时，直接应用卷积在内存和计算成本方面表现不佳的问题","动机":"为了提高处理点云数据的效率和灵活性，特别是在3D分割任务中，需要一种能够有效处理高维稀疏数据并支持分层和空间感知特征学习的网络架构","方法":"使用稀疏双边卷积层作为网络构建块，通过索引结构仅在格子的占用部分应用卷积，保持效率并允许灵活指定格子结构，支持分层和空间感知的特征学习以及联合2D-3D推理","关键词":["点云处理","3D分割","稀疏卷积","双边卷积层","高维格子"],"涉及的技术概念":"稀疏双边卷积层是一种特殊的卷积层，它通过使用索引结构仅在格子的占用部分应用卷积，从而在处理高维稀疏数据时保持效率。这种方法允许网络灵活地指定格子结构，支持分层和空间感知的特征学习，以及联合2D-3D推理。"},{"order":261,"title":"Surface Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kostrikov_Surface_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kostrikov_Surface_Networks_CVPR_2018_paper.html","abstract":"We study data-driven representations for three-dimensional triangle meshes, which are one of the prevalent objects used to represent 3D geometry. Recent works have developed models that exploit the intrinsic geometry of manifolds and graphs, namely the Graph Neural Networks (GNNs) and its spectral variants, which learn from the local metric tensor via the Laplacian operator.   Despite offering excellent sample complexity and built-in invariances, intrinsic geometry alone is invariant to isometric deformations, making it unsuitable for  many applications. To overcome this limitation, we propose several upgrades to GNNs to leverage extrinsic differential geometry properties of three-dimensional surfaces, increasing its modeling power. In particular, we propose to exploit the Dirac operator, whose spectrum detects principal curvature directions --- this is in stark contrast with the classical Laplace   operator, which directly measures mean curvature. We coin the resulting models emph{Surface Networks (SN)}.  We prove that these models define shape representations that are stable to deformation and to discretization, and we demonstrate the efficiency and versatility of SNs on   two challenging tasks: temporal prediction of mesh deformations under non-linear dynamics and generative models using a variational autoencoder framework with encoders/decoders given by SNs.","中文标题":"表面网络","摘要翻译":"我们研究了用于三维三角形网格的数据驱动表示，这是用于表示3D几何的普遍对象之一。最近的工作开发了利用流形和图的内在几何的模型，即图神经网络（GNNs）及其光谱变体，它们通过拉普拉斯算子从局部度量张量中学习。尽管提供了优秀的样本复杂性和内置的不变性，但内在几何本身对等距变形是不变的，这使得它不适合许多应用。为了克服这一限制，我们提出了几种GNNs的升级，以利用三维表面的外在微分几何特性，增加其建模能力。特别是，我们提出利用狄拉克算子，其光谱检测主曲率方向——这与直接测量平均曲率的经典拉普拉斯算子形成鲜明对比。我们将这些模型命名为“表面网络（SN）”。我们证明了这些模型定义了对变形和离散化稳定的形状表示，并在两个具有挑战性的任务上展示了SNs的效率和多功能性：在非线性动力学下的网格变形的时间预测和使用由SNs提供的编码器/解码器的变分自编码器框架的生成模型。","领域":"三维几何处理/图神经网络/微分几何","问题":"如何利用外在微分几何特性增强图神经网络的建模能力","动机":"内在几何对等距变形不变，限制了其在许多应用中的适用性","方法":"提出利用狄拉克算子检测主曲率方向，增强图神经网络的建模能力","关键词":["三维三角形网格","图神经网络","狄拉克算子","主曲率方向","变分自编码器"],"涉及的技术概念":"三维三角形网格是表示3D几何的普遍对象。图神经网络（GNNs）及其光谱变体通过拉普拉斯算子从局部度量张量中学习。狄拉克算子的光谱检测主曲率方向，与直接测量平均曲率的经典拉普拉斯算子形成对比。表面网络（SNs）利用外在微分几何特性，增强了对变形和离散化的稳定性，适用于非线性动力学下的网格变形时间预测和变分自编码器框架的生成模型。"},{"order":262,"title":"Self-Supervised Multi-Level Face Model Learning for Monocular Reconstruction at Over 250 Hz","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tewari_Self-Supervised_Multi-Level_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tewari_Self-Supervised_Multi-Level_Face_CVPR_2018_paper.html","abstract":"The reconstruction of dense 3D models of face geometry and appearance from a single image is highly challenging and ill-posed. To constrain the problem, many approaches rely on strong priors, such as parametric face models learned from limited 3D scan data. However, prior models restrict generalization of the true diversity in facial geometry, skin reflectance and illumination. To alleviate this problem, we present the first approach that jointly learns 1) a regressor for face shape, expression, reflectance and illumination on the basis of 2) a concurrently learned parametric face model. Our multi-level face model combines the advantage of 3D Morphable Models for regularization with the out-of-space generalization of a learned corrective space. We train end-to-end on in-the-wild images without dense annotations by fusing a convolutional encoder with a differentiable expert-designed renderer and a self-supervised training loss, both defined at multiple detail levels. Our approach compares favorably to the state-of-the-art in terms of reconstruction quality, better generalizes to real world faces, and runs at over 250Hz.","中文标题":"自监督多层次人脸模型学习用于单目重建超过250 Hz","摘要翻译":"从单张图像重建密集的3D人脸几何和外观模型极具挑战性且问题定义不明确。为了约束问题，许多方法依赖于强先验，例如从有限的3D扫描数据中学习到的参数化人脸模型。然而，先验模型限制了面部几何、皮肤反射率和照明的真实多样性的泛化。为了缓解这个问题，我们提出了第一种方法，该方法联合学习1)基于2)同时学习的参数化人脸模型的面部形状、表情、反射率和照明的回归器。我们的多层次人脸模型结合了3D可变形模型的正则化优势与学习到的校正空间的泛化能力。我们通过在多个细节级别上定义的卷积编码器与可微分的专家设计渲染器和自监督训练损失的融合，在无需密集注释的野外图像上进行端到端训练。我们的方法在重建质量方面优于现有技术，更好地泛化到现实世界的人脸，并且运行速度超过250Hz。","领域":"人脸重建/3D建模/自监督学习","问题":"从单张图像重建密集的3D人脸几何和外观模型","动机":"先验模型限制了面部几何、皮肤反射率和照明的真实多样性的泛化","方法":"联合学习面部形状、表情、反射率和照明的回归器，基于同时学习的参数化人脸模型，结合3D可变形模型的正则化优势与学习到的校正空间的泛化能力，通过卷积编码器与可微分的专家设计渲染器和自监督训练损失的融合进行端到端训练","关键词":["人脸重建","3D建模","自监督学习","参数化模型","卷积编码器","可微分渲染"],"涉及的技术概念":"3D可变形模型（3D Morphable Models）是一种用于表示和重建3D人脸形状的统计模型。自监督学习是一种不需要大量标注数据的学习方法，通过模型自身生成监督信号来训练。卷积编码器是一种深度学习模型，用于从图像中提取特征。可微分渲染器允许通过渲染过程反向传播梯度，使得渲染过程可以集成到深度学习模型中。"},{"order":263,"title":"CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.html","abstract":"The representation of geometry in real-time 3D perception systems continues to be a critical research issue. Dense maps capture complete surface shape and can be augmented with semantic labels, but their high dimensionality makes them computationally costly to store and process, and unsuitable for rigorous probabilistic inference. Sparse feature-based representations avoid these problems, but capture only partial scene information and are mainly useful for localisation only.  We present a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code consisting of a small number of parameters. We are inspired by work both on learned depth from images, and auto-encoders. Our approach is suitable for use in a keyframe-based monocular dense SLAM system: While each keyframe with a code can produce a depth map, the code can be optimised efficiently jointly with pose variables and together with the codes of overlapping keyframes to attain global consistency. Conditioning the depth map on the image allows the code to only represent aspects of the local geometry which cannot directly be predicted from the image. We explain how to learn our code representation, and demonstrate its advantageous properties in monocular SLAM.","中文标题":"CodeSLAM — 学习一种紧凑、可优化的表示用于密集视觉SLAM","摘要翻译":"实时3D感知系统中的几何表示仍然是一个关键的研究问题。密集地图捕捉完整的表面形状，并可以增加语义标签，但它们的高维度使得存储和处理计算成本高，不适合严格的概率推理。基于稀疏特征的表示避免了这些问题，但只捕捉部分场景信息，主要用于定位。我们提出了一种新的紧凑但密集的场景几何表示，它基于来自单张图像的强度数据，并由少量参数组成的代码生成。我们受到从图像学习深度和自动编码器工作的启发。我们的方法适用于基于关键帧的单目密集SLAM系统：每个带有代码的关键帧可以生成深度图，代码可以与姿态变量以及重叠关键帧的代码一起高效优化，以达到全局一致性。将深度图基于图像条件允许代码仅表示无法直接从图像预测的局部几何方面。我们解释了如何学习我们的代码表示，并展示了其在单目SLAM中的优势特性。","领域":"SLAM/三维重建/深度估计","问题":"如何在实时3D感知系统中有效地表示和处理密集几何信息","动机":"密集地图的高维度和稀疏特征表示的局限性促使研究一种新的紧凑但密集的场景几何表示方法","方法":"提出一种基于单张图像强度数据的紧凑场景几何表示方法，通过少量参数组成的代码生成，适用于基于关键帧的单目密集SLAM系统，并实现代码与姿态变量及重叠关键帧代码的联合优化","关键词":["SLAM","三维重建","深度估计","自动编码器","关键帧"],"涉及的技术概念":"密集地图、稀疏特征表示、单目SLAM系统、深度图、自动编码器、关键帧、全局一致性优化"},{"order":264,"title":"SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SGPN_Similarity_Group_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_SGPN_Similarity_Group_CVPR_2018_paper.html","abstract":"We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive deep learning framework for 3D object instance segmentation on point clouds. SGPN uses a single network to  predict point grouping proposals and a corresponding semantic class for each proposal, from which we can directly extract instance segmentation results. Important to the effectiveness of SGPN is its novel representation of 3D instance segmentation results in the form of a similarity matrix that indicates the similarity between each pair of points in embedded feature space, thus producing an accurate grouping proposal for each point.  To the best of our knowledge, SGPN is the first framework to learn 3D instance-aware semantic segmentation on point clouds. Experimental results on various 3D scenes show the effectiveness of our method on 3D instance segmentation, and we also evaluate the capability of SGPN to improve 3D object detection and semantic segmentation results. We also demonstrate its flexibility by seamlessly incorporating 2D CNN features into the framework to boost performance.","中文标题":"SGPN：用于3D点云实例分割的相似性组提议网络","摘要翻译":"我们介绍了相似性组提议网络（SGPN），这是一个简单直观的深度学习框架，用于点云上的3D对象实例分割。SGPN使用单一网络来预测点分组提议和每个提议的相应语义类别，从中我们可以直接提取实例分割结果。SGPN有效性的关键在于其新颖的3D实例分割结果表示形式，即相似性矩阵，该矩阵指示了嵌入特征空间中每对点之间的相似性，从而为每个点产生准确的分组提议。据我们所知，SGPN是第一个学习点云上3D实例感知语义分割的框架。在各种3D场景上的实验结果表明了我们的方法在3D实例分割上的有效性，我们还评估了SGPN提高3D对象检测和语义分割结果的能力。我们还通过无缝地将2D CNN特征整合到框架中以提升性能，展示了其灵活性。","领域":"3D视觉/点云处理/实例分割","问题":"3D点云上的对象实例分割","动机":"提高3D点云上对象实例分割的准确性和效率","方法":"使用单一网络预测点分组提议和每个提议的语义类别，通过相似性矩阵表示3D实例分割结果","关键词":["3D点云","实例分割","相似性矩阵","深度学习","2D CNN特征"],"涉及的技术概念":"相似性组提议网络（SGPN）是一种深度学习框架，专门用于处理3D点云数据，通过预测点分组提议和语义类别来实现实例分割。相似性矩阵是该框架的核心，用于表示点云中每对点之间的相似性，从而实现准确的分组。此外，SGPN框架能够整合2D CNN特征，以进一步提升性能。"},{"order":265,"title":"PlaneNet: Piece-Wise Planar Reconstruction From a Single RGB Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper.html","abstract":"This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image pixel-wise depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation,  and has been a difficult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depth maps for training and testing from ScanNet, a large-scale indoor capture database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the first end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image.","中文标题":"PlaneNet: 从单张RGB图像进行分段平面重建","摘要翻译":"本文提出了一种深度神经网络（DNN），用于从单张RGB图像进行分段平面深度图重建。虽然DNN在单图像像素级深度预测方面取得了显著进展，但分段平面深度图重建需要结构化的几何表示，即使对于DNN来说也是一项难以掌握的任务。所提出的端到端DNN学习直接从单张RGB图像推断一组平面参数和相应的平面分割掩码。我们从ScanNet，一个大规模室内捕捉数据库中，生成了超过50,000个分段平面深度图用于训练和测试。我们的定性和定量评估表明，所提出的方法在平面分割和深度估计准确性方面均优于基线方法。据我们所知，本文首次提出了从单张RGB图像进行分段平面重建的端到端神经架构。","领域":"三维重建/深度估计/平面分割","问题":"从单张RGB图像进行分段平面深度图重建","动机":"尽管深度神经网络在单图像像素级深度预测方面取得了进展，但分段平面深度图重建需要更结构化的几何表示，这是一项挑战。","方法":"提出了一种端到端的深度神经网络，直接从单张RGB图像推断平面参数和平面分割掩码。","关键词":["三维重建","深度估计","平面分割"],"涉及的技术概念":"深度神经网络（DNN）、分段平面深度图重建、平面参数、平面分割掩码、ScanNet数据库、端到端神经架构"},{"order":266,"title":"Deep Parametric Continuous Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.html","abstract":"Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.","中文标题":"深度参数连续卷积神经网络","摘要翻译":"标准的卷积神经网络假设输入是网格结构的，并利用离散卷积作为其基本构建块。这限制了它们在许多现实世界应用中的适用性。在本文中，我们提出了参数连续卷积，这是一种新的可学习操作符，它可以在非网格结构的数据上操作。关键思想是利用参数化的核函数，这些函数跨越整个连续向量空间。这种泛化使我们能够在任意数据结构上学习，只要它们的支持关系是可计算的。我们的实验显示，在室内外场景的点云分割和驾驶场景的激光雷达运动估计方面，相比现有技术有显著改进。","领域":"点云处理/激光雷达数据处理/连续卷积","问题":"标准卷积神经网络在处理非网格结构数据时的局限性","动机":"扩展卷积神经网络的适用性，使其能够处理更广泛的现实世界应用","方法":"提出了一种新的可学习操作符——参数连续卷积，利用参数化的核函数跨越整个连续向量空间，以支持任意数据结构的学习","关键词":["参数连续卷积","非网格结构数据","点云分割","激光雷达运动估计"],"涉及的技术概念":"参数连续卷积是一种新的卷积操作符，它通过参数化的核函数在连续向量空间上操作，使得卷积神经网络能够处理非网格结构的数据。这种方法的关键在于利用参数化的核函数来跨越整个连续向量空间，从而实现对任意数据结构的学习，只要这些数据的支持关系是可计算的。"},{"order":267,"title":"FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Verma_FeaStNet_Feature-Steered_Graph_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Verma_FeaStNet_Feature-Steered_Graph_CVPR_2018_paper.html","abstract":"Convolutional neural networks (CNNs) have massively impacted visual  recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly  improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.","中文标题":"FeaStNet: 用于3D形状分析的特征导向图卷积","摘要翻译":"卷积神经网络（CNNs）在2D图像的视觉识别方面产生了巨大影响，并且在最先进的方法中无处不在。然而，CNNs并不容易扩展到非规则网格表示的数据，如3D形状网格或其他图结构数据，传统的局部卷积算子不能直接应用于这些数据。为了解决这个问题，我们提出了一种新颖的图卷积算子，以建立滤波器权重与具有任意连接性的图邻域之间的对应关系。我们方法的关键新颖之处在于，这些对应关系是从网络学习的特征动态计算的，而不是像以前的工作那样依赖于图上预定义的静态坐标。我们获得了优异的实验结果，显著改进了以前的最先进的形状对应结果。这表明我们的方法可以从原始输入坐标中学习有效的形状表示，而不依赖于形状描述符。","领域":"3D形状分析/图卷积网络/特征学习","问题":"如何将卷积神经网络应用于非规则网格表示的数据，如3D形状网格或其他图结构数据","动机":"卷积神经网络在2D图像识别中取得了巨大成功，但在处理3D形状等非规则网格数据时存在困难，需要新的方法来解决这一问题","方法":"提出了一种新颖的图卷积算子，通过动态计算网络学习到的特征来建立滤波器权重与图邻域之间的对应关系，而不是依赖于预定义的静态坐标","关键词":["图卷积","3D形状分析","特征学习"],"涉及的技术概念":"卷积神经网络（CNNs）是一种深度学习模型，主要用于处理图像数据。图卷积是一种扩展卷积操作到图结构数据的方法，允许模型处理如社交网络、分子结构等非欧几里得数据。3D形状分析涉及对三维物体的几何形状进行识别、分类或分割等任务。特征学习是指模型自动从数据中学习有用的表示或特征，以提高任务的性能。"},{"order":268,"title":"Image Collection Pop-Up: 3D Reconstruction and Clustering of Rigid and Non-Rigid Categories","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Agudo_Image_Collection_Pop-Up_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Agudo_Image_Collection_Pop-Up_CVPR_2018_paper.html","abstract":"This paper introduces an approach to simultaneously estimate 3D shape, camera pose, and object and type of deformation clustering, from partial 2D annotations in a multi-instance collection of images. Furthermore, we can indistinctly process rigid and non-rigid categories. This advances existing work, which only addresses the problem for one single object or, if multiple objects are considered, they are assumed to be clustered a priori. To handle this broader version of the problem, we model object deformation using a formulation based on multiple unions of subspaces, able to span from small rigid motion to complex deformations. The parameters of this model are learned via Augmented Lagrange Multipliers, in a completely unsupervised manner that does not require any training data at all. Extensive validation is provided in a wide variety of synthetic and real scenarios, including rigid and non-rigid categories with small and large deformations. In all cases our approach outperforms state-of-the-art in terms of 3D reconstruction accuracy, while also providing clustering results that allow segmenting the images into object instances and their associated type of deformation (or action the object is performing).","中文标题":"图像集合弹出：刚性和非刚性类别的3D重建与聚类","摘要翻译":"本文介绍了一种方法，能够从多实例图像集合中的部分2D注释同时估计3D形状、相机姿态以及对象和变形类型的聚类。此外，我们能够不加区分地处理刚性和非刚性类别。这推进了现有工作，现有工作仅针对单一对象解决问题，或者如果考虑多个对象，则假定它们已事先聚类。为了处理这个更广泛的问题版本，我们使用基于多个子空间联合的公式来建模对象变形，该公式能够涵盖从小幅刚性运动到复杂变形的范围。该模型的参数通过增广拉格朗日乘数法学习，完全无监督，不需要任何训练数据。在包括具有小和大变形的刚性和非刚性类别的各种合成和真实场景中提供了广泛的验证。在所有情况下，我们的方法在3D重建准确性方面优于最先进的技术，同时还提供了聚类结果，允许将图像分割为对象实例及其相关的变形类型（或对象正在执行的动作）。","领域":"3D重建/变形分析/图像分割","问题":"从多实例图像集合中的部分2D注释同时估计3D形状、相机姿态以及对象和变形类型的聚类","动机":"推进现有工作，现有工作仅针对单一对象解决问题，或者如果考虑多个对象，则假定它们已事先聚类","方法":"使用基于多个子空间联合的公式来建模对象变形，该模型的参数通过增广拉格朗日乘数法学习，完全无监督，不需要任何训练数据","关键词":["3D重建","变形分析","图像分割"],"涉及的技术概念":"增广拉格朗日乘数法是一种用于解决约束优化问题的数值方法，通过引入拉格朗日乘数来将约束条件融入目标函数中，从而将约束优化问题转化为无约束优化问题进行求解。"},{"order":269,"title":"Geometry-Aware Learning of Maps for Camera Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Brahmbhatt_Geometry-Aware_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Brahmbhatt_Geometry-Aware_Learning_of_CVPR_2018_paper.html","abstract":"Maps are a key component in image-based camera localization and visual SLAM systems: they are used to establish geometric constraints between images, correct drift in relative pose estimation, and relocalize cameras after lost tracking. The exact definitions of maps, however, are often application-specific and hand-crafted for different scenarios (e.g. 3D landmarks, lines, planes, bags of visual words). We propose to represent maps as a deep neural net called MapNet, which enables learning a data-driven map representation. Unlike prior work on learning maps, MapNet exploits cheap and ubiquitous sensory inputs like visual odometry and GPS in addition to images and fuses them together for camera localization. Geometric constraints expressed by these inputs, which have traditionally been used in bundle adjustment or pose-graph optimization, are formulated as loss terms in MapNet training and also used during inference. In addition to directly improving localization accuracy, this allows us to update the MapNet (i.e., maps) in a self-supervised manner using additional unlabeled video sequences from the scene. We also propose a novel parameterization for camera rotation which is better suited for deep-learning based camera pose regression. Experimental results on both the indoor 7-Scenes and the outdoor Oxford RobotCar datasets show significant improvement over prior work. The MapNet project webpage is https://goo.gl/mRB3Au.","中文标题":"几何感知的地图学习用于相机定位","摘要翻译":"地图是基于图像的相机定位和视觉SLAM系统中的关键组件：它们用于建立图像之间的几何约束，校正相对姿态估计中的漂移，并在丢失跟踪后重新定位相机。然而，地图的确切定义通常是特定于应用程序的，并且为不同场景手工定制（例如3D地标、线条、平面、视觉词袋）。我们提出将地图表示为一个称为MapNet的深度神经网络，这使得能够学习数据驱动的地图表示。与之前关于学习地图的工作不同，MapNet除了图像外，还利用了廉价且普遍存在的感官输入，如视觉里程计和GPS，并将它们融合在一起用于相机定位。这些输入表达的几何约束，传统上用于束调整或姿态图优化，被表述为MapNet训练中的损失项，并在推理过程中使用。除了直接提高定位精度外，这还允许我们使用场景中的额外未标记视频序列以自监督的方式更新MapNet（即地图）。我们还提出了一种新的相机旋转参数化方法，更适合于基于深度学习的相机姿态回归。在室内7-Scenes和室外Oxford RobotCar数据集上的实验结果显示，与之前的工作相比有显著改进。MapNet项目网页是https://goo.gl/mRB3Au。","领域":"相机定位/视觉SLAM/深度学习","问题":"如何有效地表示和利用地图信息以提高相机定位的准确性和鲁棒性","动机":"传统地图定义特定于应用且手工定制，缺乏通用性和灵活性，需要一种数据驱动的方法来学习地图表示","方法":"提出MapNet，一个深度神经网络，用于学习数据驱动的地图表示，融合视觉里程计、GPS和图像信息，将几何约束作为训练损失项，并采用自监督方式更新地图","关键词":["相机定位","视觉SLAM","深度学习","地图学习","自监督学习"],"涉及的技术概念":{"MapNet":"一个深度神经网络，用于学习数据驱动的地图表示","视觉里程计":"一种通过分析连续图像帧来估计相机运动的技术","GPS":"全球定位系统，用于提供地理位置信息","束调整":"一种优化技术，用于同时调整多个相机姿态和3D点位置，以最小化重投影误差","姿态图优化":"一种优化技术，用于调整相机姿态图，以最小化姿态之间的误差","自监督学习":"一种学习方法，模型通过未标记的数据自我学习，无需外部标注"}},{"order":270,"title":"Recurrent Slice Networks for 3D Segmentation of Point Clouds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Recurrent_Slice_Networks_CVPR_2018_paper.html","abstract":"Point clouds are an efficient data format for 3D data. However, existing 3D segmentation methods for point clouds either do not model local dependencies or require added computations. This work presents a novel 3D segmentation framework, RSNet, to efficiently model local structures in point clouds. The key component of the RSNet is a lightweight local dependency module. It is a combination of a novel slice pooling layer, Recurrent Neural Network (RNN) layers, and a slice unpooling layer. The slice pooling layer is designed to project features of unordered points onto an ordered sequence of feature vectors so that traditional end-to-end learning algorithms (RNNs) can be applied. The performance of RSNet is validated by comprehensive experiments on the S3DIS, ScanNet, and ShapeNet datasets. In its simplest form, RSNets surpass all previous state-of-the-art methods on these benchmarks. And comparisons against previous state-of-the-art methods demonstrate the efficiency of RSNets.","中文标题":"循环切片网络用于点云的3D分割","摘要翻译":"点云是3D数据的一种高效格式。然而，现有的点云3D分割方法要么没有建模局部依赖关系，要么需要额外的计算。这项工作提出了一种新颖的3D分割框架RSNet，以高效地建模点云中的局部结构。RSNet的关键组件是一个轻量级的局部依赖模块。它是新颖的切片池化层、循环神经网络（RNN）层和切片反池化层的组合。切片池化层旨在将无序点的特征投影到有序的特征向量序列上，以便可以应用传统的端到端学习算法（RNN）。通过在S3DIS、ScanNet和ShapeNet数据集上的综合实验验证了RSNet的性能。在其最简单的形式中，RSNet在这些基准测试中超越了所有之前的最先进方法。与之前最先进方法的比较展示了RSNet的效率。","领域":"3D点云分割/循环神经网络/局部依赖建模","问题":"现有的点云3D分割方法未能有效建模局部依赖关系或需要额外计算","动机":"为了提高点云3D分割的效率和准确性，需要一种能够有效建模局部结构的新方法","方法":"提出了一种新颖的3D分割框架RSNet，包括一个轻量级的局部依赖模块，该模块结合了切片池化层、RNN层和切片反池化层","关键词":["3D点云分割","循环神经网络","局部依赖建模"],"涉及的技术概念":{"点云":"一种用于表示3D空间中的点的集合的数据格式","3D分割":"将3D数据分割成多个部分或对象的过程","局部依赖模块":"用于建模点云中局部点之间依赖关系的模块","切片池化层":"一种将无序点特征投影到有序特征向量序列上的技术","循环神经网络（RNN）":"一种能够处理序列数据的神经网络","切片反池化层":"与切片池化层相反的过程，用于从有序特征向量序列恢复原始点特征"}},{"order":271,"title":"Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future Goals","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yuan_Depth-Based_3D_Hand_CVPR_2018_paper.html","abstract":"In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.","中文标题":"基于深度的3D手部姿态估计：从当前成就到未来目标","摘要翻译":"在本文中，我们努力回答两个问题：从深度图像进行3D手部姿态估计的当前状态是什么？以及，接下来需要解决的挑战是什么？继成功的百万手挑战（HIM2017）之后，我们调查了在三个任务上的前10种最先进的方法：单帧3D姿态估计、3D手部跟踪和物体交互期间的手部姿态估计。我们分析了不同CNN结构在手形、关节可见性、视角和关节分布方面的性能。我们的发现包括：（1）在[70, 120]度的视角范围内，孤立的3D手部姿态估计实现了低平均误差（10毫米），但对于极端视角远未解决；（2）3D体积表示优于2D CNN，更好地捕捉深度数据的空间结构；（3）判别方法对于未见手形的泛化能力仍然较差；（4）虽然关节遮挡对大多数方法构成挑战，但结构约束的显式建模可以显著缩小可见关节和遮挡关节误差之间的差距。","领域":"3D视觉/姿态估计/手部跟踪","问题":"从深度图像进行3D手部姿态估计的当前状态和未来挑战","动机":"了解3D手部姿态估计的现状，并确定未来需要解决的挑战","方法":"分析在单帧3D姿态估计、3D手部跟踪和物体交互期间的手部姿态估计任务上的前10种最先进方法，比较不同CNN结构的性能","关键词":["3D手部姿态估计","深度图像","CNN结构","手部跟踪","物体交互"],"涉及的技术概念":"3D手部姿态估计涉及从深度图像中估计手部的3D姿态，使用CNN结构分析手形、关节可见性、视角和关节分布。3D体积表示用于更好地捕捉深度数据的空间结构，而判别方法在泛化到未见手形方面表现不佳。结构约束的显式建模用于减少关节遮挡带来的误差。"},{"order":272,"title":"SobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-Rigid Motion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Slavcheva_SobolevFusion_3D_Reconstruction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Slavcheva_SobolevFusion_3D_Reconstruction_CVPR_2018_paper.html","abstract":"We present a system that builds 3D models of non-rigidly moving surfaces from scratch in real time using a single RGB-D stream. Our solution is based on the variational level set method, thus it copes with arbitrary geometry, including topological changes. It warps a given truncated signed distance field (TSDF) to a target TSDF via gradient flow. Unlike previous approaches that define the gradient using an L2 inner product, our method relies on gradient flow in Sobolev space. Its favourable regularity properties allow for a more straightforward energy formulation that is faster to compute and that achieves higher geometric detail, mitigating the over-smoothing effects introduced by other regularization schemes. In addition, the coarse-to-fine evolution behaviour of the flow is able to handle larger motions, making few frames sufficient for a high-fidelity reconstruction. Last but not least, our pipeline determines voxel correspondences between partial shapes by matching signatures in a low-dimensional embedding of their Laplacian eigenfunctions, and is thus able to reliably colour the output model. A variety of quantitative and qualitative evaluations demonstrate the advantages of our technique.","中文标题":"SobolevFusion: 自由非刚性运动场景的三维重建","摘要翻译":"我们提出了一个系统，该系统能够实时从单个RGB-D流中从头开始构建非刚性移动表面的三维模型。我们的解决方案基于变分水平集方法，因此能够处理任意几何形状，包括拓扑变化。它通过梯度流将给定的截断有符号距离场（TSDF）扭曲到目标TSDF。与之前使用L2内积定义梯度的方法不同，我们的方法依赖于Sobolev空间中的梯度流。其良好的正则性特性允许更直接的能量公式，计算速度更快，并且能够实现更高的几何细节，减轻了其他正则化方案引入的过度平滑效应。此外，流的从粗到细的演化行为能够处理更大的运动，使得仅需少量帧即可实现高保真重建。最后但同样重要的是，我们的流程通过在拉普拉斯特征函数的低维嵌入中匹配签名来确定部分形状之间的体素对应关系，从而能够可靠地为输出模型着色。各种定量和定性评估展示了我们技术的优势。","领域":"三维重建/非刚性运动/实时处理","问题":"实时构建非刚性移动表面的三维模型","动机":"为了更有效地处理非刚性移动表面的三维重建，减少计算时间并提高几何细节，同时处理更大的运动。","方法":"基于变分水平集方法，使用Sobolev空间中的梯度流进行TSDF扭曲，通过拉普拉斯特征函数的低维嵌入匹配签名确定体素对应关系。","关键词":["三维重建","非刚性运动","实时处理","变分水平集方法","Sobolev空间","梯度流","拉普拉斯特征函数"],"涉及的技术概念":{"RGB-D流":"一种结合了RGB彩色图像和深度信息的视频流，用于捕捉场景的三维信息。","变分水平集方法":"一种用于图像分割和三维重建的数学方法，通过最小化能量函数来演化曲线或曲面。","截断有符号距离场（TSDF）":"一种表示三维形状的方法，通过计算每个点到最近表面的有符号距离，并在一定范围内截断。","Sobolev空间":"一种函数空间，其中的函数及其导数具有特定的可积性，常用于偏微分方程和变分问题中。","拉普拉斯特征函数":"拉普拉斯算子的特征函数，用于描述形状的几何和拓扑特性，常用于形状分析和匹配。"}},{"order":273,"title":"AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.html","abstract":"Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.","中文标题":"AdaDepth: 无监督内容一致适应用于深度估计","摘要翻译":"监督深度学习方法在单目深度估计任务中显示出有希望的结果；但获取地面实况成本高昂，且容易受到噪声和不准确性的影响。虽然合成数据集已被用来规避上述问题，但由于固有的领域转移，所得模型不能很好地泛化到自然场景。最近的对抗性领域适应方法在缓解源域和目标域之间的差异方面表现良好。但这些方法大多局限于分类设置，并且不能很好地扩展到全卷积架构。在这项工作中，我们提出了AdaDepth - 一种用于单目深度估计的像素级回归任务的无监督领域适应策略。所提出的方法通过a)对抗性学习和b)对适应的目标表示施加明确的内容一致性，避免了上述限制。我们的无监督方法在深度估计任务中与其他已建立的方法竞争，并在半监督设置中实现了最先进的结果。","领域":"深度估计/领域适应/对抗性学习","问题":"单目深度估计中的领域适应问题","动机":"由于获取真实深度数据的成本高且易受噪声影响，以及合成数据与自然场景之间的领域转移问题，需要一种有效的无监督领域适应方法来提高深度估计的准确性和泛化能力。","方法":"提出了一种名为AdaDepth的无监督领域适应策略，通过对抗性学习和在适应的目标表示上施加内容一致性来解决单目深度估计中的领域适应问题。","关键词":["深度估计","领域适应","对抗性学习","无监督学习","内容一致性"],"涉及的技术概念":"单目深度估计是指从单一图像中估计场景的深度信息。领域适应是一种技术，旨在将在一个领域（源域）上训练的模型适应到另一个不同但相关的领域（目标域）。对抗性学习是一种通过引入对抗性过程来改进模型性能的方法，常用于生成对抗网络（GANs）中。无监督学习是指在没有标签数据的情况下训练模型。内容一致性是指在领域适应过程中保持目标表示与源表示在内容上的一致性。"},{"order":274,"title":"Learning to Find Good Correspondences","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yi_Learning_to_Find_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yi_Learning_to_Find_CVPR_2018_paper.html","abstract":"We develop a deep architecture to learn to find good correspondences for wide-baseline stereo. Given a set of putative sparse matches and the camera intrinsics, we train our network in an end-to-end fashion to label the correspondences as inliers or outliers, while simultaneously using them to recover the relative pose, as encoded by the essential matrix. Our architecture is based on a multi-layer perceptron operating on pixel coordinates rather than directly on the image, and is thus simple and small. We introduce a novel normalization technique, called Context Normalization, which allows us to process each data point separately while embedding global information in it, and also makes the network invariant to the order of the correspondences. Our experiments on multiple challenging datasets demonstrate that our method is able to drastically improve the state of the art with little training data.","中文标题":"学习寻找良好对应关系","摘要翻译":"我们开发了一种深度架构，用于学习为宽基线立体视觉寻找良好的对应关系。给定一组假定的稀疏匹配和相机内参，我们以端到端的方式训练我们的网络，以将对应关系标记为内点或外点，同时使用它们来恢复由本质矩阵编码的相对姿态。我们的架构基于一个多层感知器，它操作的是像素坐标而不是直接操作图像，因此既简单又小巧。我们引入了一种称为上下文归一化的新技术，它允许我们分别处理每个数据点，同时在其中嵌入全局信息，并且使网络对对应关系的顺序保持不变。我们在多个具有挑战性的数据集上的实验表明，我们的方法能够以极少的训练数据大幅提高现有技术水平。","领域":"立体视觉/三维重建/相机姿态估计","问题":"在宽基线立体视觉中寻找良好的对应关系","动机":"提高在宽基线立体视觉中对应关系的准确性和效率，以恢复更精确的相对姿态","方法":"开发了一种基于多层感知器的深度架构，引入上下文归一化技术，以端到端的方式训练网络来标记对应关系为内点或外点，并恢复相对姿态","关键词":["立体视觉","对应关系","上下文归一化","多层感知器","相对姿态"],"涉及的技术概念":{"宽基线立体视觉":"一种从不同视角拍摄的图像中恢复三维场景信息的技术","稀疏匹配":"在图像中找到的少量对应点","相机内参":"描述相机内部特性的参数，如焦距、主点等","端到端训练":"一种训练方法，直接从输入到输出进行训练，无需手动设计中间步骤","内点/外点":"在模型拟合中，内点是符合模型的点，外点是不符合模型的点","本质矩阵":"描述两个相机之间相对姿态的矩阵","多层感知器":"一种前馈人工神经网络模型","上下文归一化":"一种新的归一化技术，允许单独处理每个数据点同时嵌入全局信息"}},{"order":275,"title":"OATM: Occlusion Aware Template Matching by Consensus Set Maximization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Korman_OATM_Occlusion_Aware_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Korman_OATM_Occlusion_Aware_CVPR_2018_paper.html","abstract":"We present a novel approach to template matching that is efficient, can handle partial occlusions, and comes with provable performance guarantees. A key component of the method is a reduction that transforms the problem of searching a nearest neighbor among $N$ high-dimensional vectors, to searching neighbors among two sets of order $sqrt{N}$ vectors, which can be found efficiently using range search techniques. This allows for a quadratic improvement in search complexity, and makes the method scalable in handling large search spaces. The second contribution is a hashing scheme based on consensus set maximization, which allows us to handle occlusions. The resulting scheme can be seen as a randomized hypothesize-and-test algorithm, which is equipped with guarantees regarding the number of iterations required for obtaining an optimal solution with high probability. The predicted matching rates are validated empirically and the algorithm shows a significant improvement over the state-of-the-art in both speed and robustness to occlusions.","中文标题":"OATM：通过共识集最大化实现遮挡感知的模板匹配","摘要翻译":"我们提出了一种新颖的模板匹配方法，该方法高效、能够处理部分遮挡，并具有可证明的性能保证。该方法的一个关键组成部分是将搜索$N$个高维向量中的最近邻问题，转化为搜索两组数量为$sqrt{N}$的向量中的邻居问题，这可以通过范围搜索技术高效完成。这使得搜索复杂度有了二次改进，并使方法在处理大搜索空间时具有可扩展性。第二个贡献是基于共识集最大化的哈希方案，这使我们能够处理遮挡。最终方案可以被视为一种随机化的假设-测试算法，它配备了关于以高概率获得最优解所需迭代次数的保证。预测的匹配率通过实验验证，该算法在速度和遮挡鲁棒性方面均显示出对现有技术的显著改进。","领域":"模板匹配/遮挡处理/搜索算法","问题":"高效处理部分遮挡的模板匹配问题","动机":"提高模板匹配在处理部分遮挡时的效率和鲁棒性","方法":"通过将高维向量搜索问题转化为更小规模的搜索问题，并采用基于共识集最大化的哈希方案来处理遮挡","关键词":["模板匹配","遮挡处理","搜索算法","共识集最大化","哈希方案"],"涉及的技术概念":{"模板匹配":"一种在图像中寻找与给定模板最相似区域的技术","遮挡处理":"处理图像中部分区域被遮挡的情况","搜索算法":"用于在数据集中查找特定项的算法","共识集最大化":"一种通过最大化共识集来处理遮挡的方法","哈希方案":"一种将数据映射到固定大小值的技术，用于快速查找"}},{"order":276,"title":"Deep Learning of Graph Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Deep_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Deep_Learning_of_CVPR_2018_paper.html","abstract":"The problem of graph matching under node and pair-wise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.","中文标题":"图匹配的深度学习","摘要翻译":"在节点和成对约束下的图匹配问题在组合优化、机器学习或计算机视觉等领域中是基础性的，其中表示节点之间的关系及其邻域结构是至关重要的。我们提出了一个端到端模型，使得学习图匹配过程的所有参数成为可能，包括一元和成对节点邻域，表示为深度特征提取层次结构。挑战在于模型的不同矩阵计算层的公式化，使得从损失函数通过解决匹配问题的组合优化层和特征提取层次结构的完整管道中梯度的连续、有效传播成为可能。我们在具有挑战性的数据集（如PASCAL VOC关键点、Sintel和CUB）上的计算机视觉实验和消融研究表明，端到端精炼的匹配模型优于基于为其他问题训练的特征层次结构的对应模型。","领域":"图匹配/组合优化/特征提取","问题":"在节点和成对约束下的图匹配问题","动机":"表示节点之间的关系及其邻域结构对于组合优化、机器学习或计算机视觉等领域至关重要","方法":"提出了一个端到端模型，学习图匹配过程的所有参数，包括一元和成对节点邻域，表示为深度特征提取层次结构","关键词":["图匹配","组合优化","特征提取"],"涉及的技术概念":"端到端模型、深度特征提取层次结构、矩阵计算层、梯度传播、组合优化层、损失函数"},{"order":277,"title":"Unsupervised Discovery of Object Landmarks as Structural Representations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.html","abstract":"Deep neural networks can model images with rich latent representations, but they cannot naturally conceptualize structures of object categories in a human-perceptible way. This paper addresses the problem of learning object structures in an image modeling process without supervision. We propose an autoencoding formulation to discover landmarks as explicit structural representations. The encoding module outputs landmark coordinates, whose validity is ensured by constraints that reflect the necessary properties for landmarks. The decoding module takes the landmarks as a part of the learnable input representations in an end-to-end differentiable framework. Our discovered landmarks are semantically meaningful and more predictive of manually annotated landmarks than those discovered by previous methods. The coordinates of our landmarks are also complementary features to pretrained deep-neuralnetwork representations in recognizing visual attributes. In addition, the proposed method naturally creates an unsupervised, perceptible interface to manipulate object shapes and decode images with controllable structures.","中文标题":"无监督发现对象地标作为结构表示","摘要翻译":"深度神经网络能够用丰富的潜在表示来建模图像，但它们不能以人类可感知的方式自然地概念化对象类别的结构。本文解决了在图像建模过程中无监督学习对象结构的问题。我们提出了一种自动编码公式来发现地标作为明确的结构表示。编码模块输出地标坐标，其有效性通过反映地标必要属性的约束来确保。解码模块将地标作为可学习输入表示的一部分，在一个端到端可微分的框架中。我们发现的地标在语义上有意义，并且比之前方法发现的地标更能预测手动注释的地标。我们的地标坐标也是识别视觉属性时预训练深度神经网络表示的补充特征。此外，所提出的方法自然地创建了一个无监督的、可感知的界面来操纵对象形状和解码具有可控结构的图像。","领域":"图像建模/对象结构学习/视觉属性识别","问题":"在图像建模过程中无监督学习对象结构的问题","动机":"深度神经网络不能以人类可感知的方式自然地概念化对象类别的结构","方法":"提出了一种自动编码公式来发现地标作为明确的结构表示，编码模块输出地标坐标，解码模块将地标作为可学习输入表示的一部分","关键词":["无监督学习","对象结构","地标发现","图像建模","视觉属性识别"],"涉及的技术概念":"自动编码公式、地标坐标、端到端可微分框架、预训练深度神经网络表示"},{"order":278,"title":"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html","abstract":"The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based visual recognition models call for efficient on-device inference schemes. We propose a quantization scheme along with a co-designed training procedure allowing inference to be carried out using integer-only arithmetic while preserving an end-to-end model accuracy that is close to floating-point inference. Inference using integer-only arithmetic performs better than floating-point arithmetic on typical ARM CPUs and can be implemented on integer-arithmetic-only hardware such as mobile accelerators (e.g. Qualcomm Hexagon). By quantizing both activations and weights as 8-bit integers, we obtain a close to 4x memory footprint reduction compared to 32-bit floating-point representations. Even on MobileNets, a model family known for runtime efficiency, our quantization approach results in an improved tradeoff between latency and accuracy on popular ARM CPUs for ImageNet classification and COCO detection.","中文标题":"量化和训练神经网络以实现仅整数算术的高效推理","摘要翻译":"智能移动设备的日益普及以及基于深度学习的视觉识别模型的计算成本高昂，要求高效的设备上推理方案。我们提出了一种量化方案以及共同设计的训练过程，允许使用仅整数算术进行推理，同时保持接近浮点推理的端到端模型准确性。在典型的ARM CPU上，使用仅整数算术的推理比浮点算术表现更好，并且可以在仅支持整数算术的硬件上实现，如移动加速器（例如Qualcomm Hexagon）。通过将激活和权重量化为8位整数，我们获得了与32位浮点表示相比接近4倍的内存占用减少。即使在以运行时效率著称的MobileNets模型家族上，我们的量化方法在流行的ARM CPU上为ImageNet分类和COCO检测提供了延迟和准确性之间的改进权衡。","领域":"神经网络量化/移动设备推理/模型压缩","问题":"如何在移动设备上实现高效的深度学习模型推理","动机":"智能移动设备的普及和深度学习模型的高计算成本要求开发高效的设备上推理方案","方法":"提出了一种量化方案和共同设计的训练过程，允许使用仅整数算术进行推理，同时保持接近浮点推理的模型准确性","关键词":["神经网络量化","移动设备推理","模型压缩"],"涉及的技术概念":{"量化方案":"将神经网络的激活和权重从浮点数转换为整数，以减少内存占用和加速计算","仅整数算术":"一种计算方式，仅使用整数进行算术运算，适用于不支持浮点运算的硬件","ARM CPUs":"一种广泛用于移动设备的处理器架构","MobileNets":"一种专为移动和嵌入式视觉应用设计的轻量级深度学习模型家族","ImageNet分类":"一种广泛使用的图像分类任务，用于评估计算机视觉模型的性能","COCO检测":"一种对象检测任务，用于评估模型在复杂场景中识别和定位对象的能力"}},{"order":279,"title":"Lean Multiclass Crowdsourcing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_Lean_Multiclass_Crowdsourcing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Van_Horn_Lean_Multiclass_Crowdsourcing_CVPR_2018_paper.html","abstract":"We introduce a method for efficiently crowdsourcing multiclass annotations in challenging, real world image datasets. Our method is designed to minimize the number of human annotations that are necessary to achieve a desired level of confidence on class labels. It is based on combining models of worker behavior with computer vision. Our method is general: it can handle a large number of classes, worker labels that come from a taxonomy rather than a flat list, and can model the dependence of labels when workers can see a history of previous annotations. Our method may be used as a drop-in replacement for the majority vote algorithms used in online crowdsourcing services that aggregate multiple human annotations into a final consolidated label. In experiments conducted on two real-life applications we find that our method can reduce the number of required annotations by as much as a factor of 5.4 and can reduce the residual annotation error by up to 90% when compared with majority voting. Furthermore, the online risk estimates of the models may be used to sort the annotated collection and minimize subsequent expert review effort.","中文标题":"精益多类众包","摘要翻译":"我们介绍了一种方法，用于在具有挑战性的现实世界图像数据集中高效地进行多类注释众包。我们的方法旨在最小化达到所需类别标签置信度水平所需的人类注释数量。它基于将工人行为模型与计算机视觉相结合。我们的方法是通用的：它可以处理大量类别，工人标签来自分类法而非平面列表，并且可以模拟当工人可以看到先前注释历史时标签的依赖性。我们的方法可以用作在线众包服务中用于将多个人类注释聚合成最终合并标签的多数投票算法的直接替代品。在两个现实生活应用的实验中，我们发现我们的方法可以将所需的注释数量减少多达5.4倍，并且与多数投票相比，可以将残余注释错误减少多达90%。此外，模型的在线风险估计可用于对注释集合进行排序，并最小化后续专家评审的工作量。","领域":"众包注释/图像分类/行为模型","问题":"在现实世界图像数据集中高效地进行多类注释众包","动机":"最小化达到所需类别标签置信度水平所需的人类注释数量","方法":"结合工人行为模型与计算机视觉，处理大量类别和来自分类法的工人标签，模拟标签依赖性","关键词":["众包注释","图像分类","行为模型"],"涉及的技术概念":"工人行为模型、计算机视觉、多类注释、分类法、标签依赖性、多数投票算法、在线风险估计"},{"order":280,"title":"Partial Transfer Learning With Selective Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Partial_Transfer_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Partial_Transfer_Learning_CVPR_2018_paper.html","abstract":"Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing large-scale domains to unknown small-scale domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets.","中文标题":"选择性对抗网络的部分迁移学习","摘要翻译":"对抗学习已成功嵌入深度网络中以学习可迁移特征，从而减少源域和目标域之间的分布差异。现有的域对抗网络假设域间标签空间完全共享。在大数据背景下，将分类和表示模型从现有的大规模域迁移到未知的小规模域有强烈的动机。本文介绍了部分迁移学习，它放宽了共享标签空间的假设，使得目标标签空间仅是源标签空间的子空间。以往的方法通常将整个源域与目标域匹配，这在部分迁移问题中容易导致负迁移。我们提出了选择性对抗网络（SAN），它通过选择异常源类来同时避免负迁移，并通过在共享标签空间中最大化匹配数据分布来促进正迁移。实验证明，我们的模型在多个基准数据集上的部分迁移学习任务中超过了最先进的结果。","领域":"迁移学习/对抗学习/深度学习","问题":"解决在源域和目标域标签空间不完全共享情况下的迁移学习问题","动机":"在大数据背景下，将分类和表示模型从现有的大规模域迁移到未知的小规模域的需求","方法":"提出选择性对抗网络（SAN），通过选择异常源类避免负迁移，并在共享标签空间中最大化匹配数据分布以促进正迁移","关键词":["迁移学习","对抗学习","选择性对抗网络","负迁移","正迁移"],"涉及的技术概念":{"对抗学习":"一种通过对抗过程训练模型的方法，旨在提高模型的泛化能力","迁移学习":"一种机器学习方法，旨在将从一个任务学到的知识迁移到另一个相关任务上","选择性对抗网络（SAN）":"一种新型网络结构，通过选择性地处理源域数据来避免负迁移，并促进正迁移","负迁移":"在迁移学习中，由于源域和目标域之间的差异过大，导致迁移效果变差的现象","正迁移":"在迁移学习中，源域和目标域之间的知识迁移有效提高了目标域任务性能的现象"}},{"order":281,"title":"Self-Supervised Feature Learning by Learning to Spot Artifacts","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jenni_Self-Supervised_Feature_Learning_CVPR_2018_paper.html","abstract":"We introduce a novel self-supervised learning method based on adversarial training. Our objective is to train a discriminator network to distinguish real images from images with synthetic artifacts, and then to extract features from its intermediate layers that can be transferred to other data domains and tasks.  To generate images with artifacts, we pre-train a high-capacity autoencoder and then we use a damage and repair strategy: First, we freeze the autoencoder and damage the output of the encoder by randomly dropping its entries. Second, we augment the decoder with a repair network, and train it in an adversarial manner against the discriminator. The repair network helps generate more realistic images by inpainting the dropped feature entries. To make the discriminator focus on the artifacts, we also make it predict what entries in the feature were dropped. We demonstrate experimentally that features learned by creating and spotting artifacts achieve state of the art performance in several benchmarks.","中文标题":"通过学会发现伪影进行自监督特征学习","摘要翻译":"我们介绍了一种基于对抗训练的新型自监督学习方法。我们的目标是训练一个判别器网络，以区分真实图像和带有合成伪影的图像，然后从其中间层提取特征，这些特征可以转移到其他数据域和任务中。为了生成带有伪影的图像，我们预训练了一个高容量的自动编码器，然后使用一种损坏和修复策略：首先，我们冻结自动编码器并通过随机丢弃其条目来损坏编码器的输出。其次，我们通过添加一个修复网络来增强解码器，并以对抗的方式训练它对抗判别器。修复网络通过填补丢弃的特征条目来帮助生成更真实的图像。为了使判别器专注于伪影，我们还使其预测特征中哪些条目被丢弃。我们通过实验证明，通过创建和发现伪影学习的特征在多个基准测试中达到了最先进的性能。","领域":"自监督学习/对抗训练/特征提取","问题":"如何在没有大量标注数据的情况下，有效地学习可转移的特征","动机":"减少对大量标注数据的依赖，提高特征学习的效率和效果","方法":"使用对抗训练策略，通过损坏和修复自动编码器的输出，训练判别器网络区分真实图像和带有合成伪影的图像，并从中提取特征","关键词":["自监督学习","对抗训练","特征提取","自动编码器","修复网络"],"涉及的技术概念":{"自监督学习":"一种不需要大量标注数据的学习方法，通过设计任务让模型从未标注数据中学习有用的特征","对抗训练":"一种训练策略，通过让两个网络（如生成器和判别器）相互对抗来提高模型的性能","特征提取":"从数据中提取有用信息的过程，这些信息可以用于后续的任务，如分类或识别","自动编码器":"一种神经网络，用于学习数据的有效编码，通常包括编码器和解码器两部分","修复网络":"一种网络结构，用于修复或填补数据中的缺失或损坏部分"}},{"order":282,"title":"LDMNet: Low Dimensional Manifold Regularized Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_LDMNet_Low_Dimensional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_LDMNet_Low_Dimensional_CVPR_2018_paper.html","abstract":"Deep neural networks have proved very successful on archetypal tasks for which large training sets are available, but when the training data are scarce, their performance suffers from overfitting. Many existing methods of reducing overfitting are data-independent. Data-dependent regularizations are mostly motivated by the observation that data of interest lie close to a manifold, which is typically hard to parametrize explicitly. These methods usually only focus on the geometry of the input data, and do not necessarily encourage the networks to produce geometrically meaningful features. To resolve this, we propose the Low-Dimensional- Manifold-regularized neural Network (LDMNet), which incorporates a feature regularization method that focuses on the geometry of both the input data and the output features. In LDMNet, we regularize the network by encouraging the combination of the input data and the output features to sample a collection of low dimensional manifolds, which are searched efficiently without explicit parametrization. To achieve this, we directly use the manifold dimension as a regularization term in a variational functional. The resulting Euler-Lagrange equation is a Laplace-Beltrami equation over a point cloud, which is solved by the point integral method without increasing the computational complexity. In the experiments, we show that LDMNet significantly outperforms widely-used regularizers. Moreover, LDMNet can extract common features of an object imaged via different modalities, which is very useful in real-world applications such as cross-spectral face recognition.","中文标题":"LDMNet: 低维流形正则化神经网络","摘要翻译":"深度神经网络在典型任务上已证明非常成功，这些任务有大量训练集可用，但当训练数据稀缺时，其性能会因过拟合而受到影响。许多现有的减少过拟合的方法是数据无关的。数据依赖的正则化大多是基于观察到的数据靠近一个流形，而这个流形通常难以显式参数化。这些方法通常只关注输入数据的几何形状，并不一定鼓励网络产生几何上有意义的特征。为了解决这个问题，我们提出了低维流形正则化神经网络（LDMNet），它采用了一种特征正则化方法，专注于输入数据和输出特征的几何形状。在LDMNet中，我们通过鼓励输入数据和输出特征的组合采样一组低维流形来正则化网络，这些流形无需显式参数化即可高效搜索。为了实现这一点，我们直接将流形维度作为变分函数中的正则化项。由此产生的欧拉-拉格朗日方程是在点云上的拉普拉斯-贝尔特拉米方程，通过点积分法解决，而不会增加计算复杂度。在实验中，我们展示了LDMNet显著优于广泛使用的正则化器。此外，LDMNet可以提取通过不同模态成像的对象的共同特征，这在跨光谱人脸识别等实际应用中非常有用。","领域":"流形学习/正则化方法/特征提取","问题":"深度神经网络在训练数据稀缺时的过拟合问题","动机":"观察到数据靠近一个难以显式参数化的流形，现有方法只关注输入数据的几何形状，不鼓励网络产生几何上有意义的特征","方法":"提出低维流形正则化神经网络（LDMNet），通过鼓励输入数据和输出特征的组合采样一组低维流形来正则化网络，直接将流形维度作为变分函数中的正则化项","关键词":["流形学习","正则化方法","特征提取","过拟合","拉普拉斯-贝尔特拉米方程"],"涉及的技术概念":"低维流形正则化神经网络（LDMNet）是一种新的正则化方法，它通过将流形维度作为变分函数中的正则化项，来解决深度神经网络在训练数据稀缺时的过拟合问题。这种方法不仅关注输入数据的几何形状，还鼓励网络产生几何上有意义的输出特征。LDMNet通过点积分法解决拉普拉斯-贝尔特拉米方程，而不会增加计算复杂度。"},{"order":283,"title":"CondenseNet: An Efficient DenseNet Using Learned Group Convolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_CondenseNet_An_Efficient_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_CondenseNet_An_Efficient_CVPR_2018_paper.html","abstract":"Deep neural networks are increasingly used on mobile devices, where computational resources are limited. In this paper we develop CondenseNet, a novel network architecture with unprecedented efficiency. It combines dense connectivity with a novel module called learned group convolution. The dense connectivity facilitates feature re-use in the network, whereas learned group convolutions remove connections between layers for which this feature re-use is superfluous. At test time, our model can be implemented using standard group convolutions, allowing for efficient computation in practice. Our experiments show that CondenseNets are far more efficient than state-of-the-art compact convolutional networks such as MobileNets and ShuffleNets.","中文标题":"CondenseNet: 使用学习到的分组卷积的高效DenseNet","摘要翻译":"深度神经网络越来越多地用于计算资源有限的移动设备上。在本文中，我们开发了CondenseNet，一种具有前所未有的效率的新型网络架构。它结合了密集连接和一个称为学习到的分组卷积的新模块。密集连接促进了网络中特征的重复使用，而学习到的分组卷积则移除了那些特征重复使用是多余的层间连接。在测试时，我们的模型可以使用标准的分组卷积来实现，从而在实践中实现高效计算。我们的实验表明，CondenseNets比最先进的紧凑卷积网络（如MobileNets和ShuffleNets）要高效得多。","领域":"卷积神经网络/移动计算/网络优化","问题":"在计算资源有限的移动设备上实现高效的深度神经网络","动机":"为了在移动设备上更有效地使用深度神经网络，需要开发一种既保持网络性能又减少计算资源消耗的网络架构。","方法":"结合密集连接和学习到的分组卷积，通过移除不必要的层间连接来提高网络效率。","关键词":["密集连接","分组卷积","网络效率"],"涉及的技术概念":"密集连接是指在网络中每一层都直接连接到所有后续层，以促进特征的重复使用。学习到的分组卷积是一种新型的卷积方式，它通过学习过程自动决定哪些连接是必要的，从而移除不必要的连接，减少计算量。标准分组卷积是一种在测试时实现高效计算的技术，它通过将输入通道分成若干组，每组独立进行卷积操作，以减少计算复杂度。"},{"order":284,"title":"Learning Deep Descriptors With Scale-Aware Triplet Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Keller_Learning_Deep_Descriptors_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Keller_Learning_Deep_Descriptors_CVPR_2018_paper.html","abstract":"Research on learning suitable feature descriptors for Computer Vision has recently shifted to deep learning where the biggest challenge lies with the formulation of appropriate loss functions, especially since the descriptors to be learned are not known at training time. While approaches such as Siamese and triplet losses have been applied with success, it is still not well understood what makes a good loss function. In this spirit, this work demonstrates that many commonly used losses suffer from a range of problems. Based on this analysis, we introduce mixed-context losses and scale-aware sampling, two methods that when combined enable networks to learn consistently scaled descriptors for the first time.","中文标题":"学习具有尺度感知的三重网络深度描述符","摘要翻译":"最近，关于学习适合计算机视觉的特征描述符的研究已经转向深度学习，其中最大的挑战在于制定适当的损失函数，尤其是在训练时不知道要学习的描述符的情况下。虽然诸如Siamese和triplet损失的方法已经成功应用，但什么构成一个好的损失函数仍然不太清楚。本着这种精神，这项工作展示了许多常用的损失函数存在一系列问题。基于这一分析，我们引入了混合上下文损失和尺度感知采样，这两种方法结合使用时首次使网络能够学习到一致缩放的描述符。","领域":"特征描述符学习/损失函数优化/尺度感知学习","问题":"如何制定适当的损失函数以学习适合计算机视觉的特征描述符","动机":"解决在训练时不知道要学习的描述符的情况下，如何制定适当的损失函数的问题","方法":"引入混合上下文损失和尺度感知采样，结合使用以学习一致缩放的描述符","关键词":["特征描述符","损失函数","尺度感知"],"涉及的技术概念":{"Siamese损失":"一种用于比较两个输入是否相似的损失函数","triplet损失":"一种用于学习特征表示，使得相同类别的样本距离更近，不同类别的样本距离更远的损失函数","混合上下文损失":"结合多种上下文信息来优化损失函数的方法","尺度感知采样":"一种考虑不同尺度特征的采样方法，以学习到一致缩放的描述符"}},{"order":285,"title":"Decoupled Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Decoupled_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Decoupled_Networks_CVPR_2018_paper.html","abstract":"Inner product-based convolution has been a central component of convolutional neural networks (CNNs) and the key to learning visual representations. Inspired by the observation that CNN-learned features are naturally decoupled with the norm of features corresponding to the intra-class variation and the angle corresponding to the semantic difference, we propose a generic decoupled learning framework which models the intra-class variation and semantic difference independently. Specifically, we first reparametrize the inner product to a decoupled form and then generalize it to the decoupled convolution operator which serves as the building block of our decoupled networks. We present several effective instances of the decoupled convolution operator. Each decoupled operator is well motivated and has an intuitive geometric interpretation. Based on these decoupled operators, we further propose to directly learn the operator from data. Extensive experiments show that such decoupled reparameterization renders significant performance gain with easier convergence and stronger robustness.","中文标题":"解耦网络","摘要翻译":"基于内积的卷积一直是卷积神经网络（CNNs）的核心组成部分，也是学习视觉表示的关键。受到CNN学习到的特征自然解耦的启发，其中特征的范数对应于类内变化，而角度对应于语义差异，我们提出了一个通用的解耦学习框架，该框架独立地建模类内变化和语义差异。具体来说，我们首先将内积重新参数化为解耦形式，然后将其推广到解耦卷积算子，作为我们解耦网络的构建块。我们提出了几种有效的解耦卷积算子实例。每个解耦算子都有充分的动机，并具有直观的几何解释。基于这些解耦算子，我们进一步提出直接从数据中学习算子。大量实验表明，这种解耦重新参数化带来了显著的性能提升，具有更容易的收敛性和更强的鲁棒性。","领域":"卷积神经网络/特征学习/解耦学习","问题":"如何有效地解耦卷积神经网络中的类内变化和语义差异","动机":"受到CNN学习到的特征自然解耦的启发，旨在独立地建模类内变化和语义差异","方法":"提出一个通用的解耦学习框架，包括将内积重新参数化为解耦形式，推广到解耦卷积算子，并直接从数据中学习算子","关键词":["解耦学习","卷积神经网络","特征学习","类内变化","语义差异"],"涉及的技术概念":"内积、卷积神经网络（CNNs）、解耦学习、类内变化、语义差异、解耦卷积算子、几何解释、数据学习、性能提升、收敛性、鲁棒性"},{"order":286,"title":"Deep Adversarial Metric Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.html","abstract":"Learning an effective distance metric between image pairs plays an important role in visual analysis, where the training procedure largely relies on hard negative samples. However, hard negatives in the training set usually account for the tiny minority, which may fail to fully describe the distribution of negative samples close to the margin. In this paper, we propose a deep adversarial metric learning (DAML) framework to generate synthetic hard negatives from the observed negative samples, which is widely applicable to supervised deep metric learning methods. Different from existing metric learning approaches which simply ignore numerous easy negatives, the proposed DAML exploits them to generate potential hard negatives adversary to the learned metric as complements. We simultaneously train the hard negative generator and feature embedding in an adversarial manner, so that more precise distance metrics can be learned with adequate and targeted synthetic hard negatives. Extensive experimental results on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that DAML effectively boosts the performance of existing deep metric learning approaches through adversarial learning.","中文标题":"深度对抗度量学习","摘要翻译":"学习图像对之间的有效距离度量在视觉分析中扮演着重要角色，其中训练过程很大程度上依赖于难以区分的负样本。然而，训练集中的难以区分的负样本通常只占极少数，这可能无法充分描述接近边界的负样本分布。在本文中，我们提出了一个深度对抗度量学习（DAML）框架，从观察到的负样本中生成合成的难以区分的负样本，这广泛适用于监督深度度量学习方法。与现有的度量学习方法不同，这些方法简单地忽略了大量容易区分的负样本，而提出的DAML利用它们生成潜在的难以区分的负样本，作为学习度量的补充。我们以对抗的方式同时训练难以区分的负样本生成器和特征嵌入，以便通过充足且有目标的合成难以区分的负样本学习到更精确的距离度量。在包括CUB-200-2011、Cars196和Stanford Online Products在内的三个基准数据集上的广泛实验结果表明，DAML通过对抗学习有效提升了现有深度度量学习方法的性能。","领域":"度量学习/对抗学习/图像分析","问题":"难以区分的负样本在训练集中占少数，无法充分描述接近边界的负样本分布","动机":"提升深度度量学习方法的性能，通过生成合成的难以区分的负样本来补充训练数据","方法":"提出深度对抗度量学习（DAML）框架，以对抗的方式同时训练难以区分的负样本生成器和特征嵌入，生成合成的难以区分的负样本","关键词":["度量学习","对抗学习","图像分析"],"涉及的技术概念":"深度对抗度量学习（DAML）框架，难以区分的负样本生成器，特征嵌入，对抗学习，监督深度度量学习方法"},{"order":287,"title":"PU-Net: Point Cloud Upsampling Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_PU-Net_Point_Cloud_CVPR_2018_paper.html","abstract":"Learning and analyzing 3D point clouds with deep networks is challenging due to the sparseness and irregularity of the data. In this paper, we present a data-driven point cloud upsampling technique. The key idea is to learn multi-level features per point and expand the point set via a multi-branch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. We conduct various experiments using synthesis and scan data to evaluate our method and demonstrate its superiority over some baseline methods and an optimization-based method.  Results show that our upsampled points have better uniformity and are located closer to the underlying surfaces.","中文标题":"PU-Net: 点云上采样网络","摘要翻译":"由于数据的稀疏性和不规则性，使用深度网络学习和分析3D点云具有挑战性。在本文中，我们提出了一种数据驱动的点云上采样技术。关键思想是学习每个点的多层次特征，并通过多分支卷积单元在特征空间中隐式扩展点集。然后，扩展的特征被分割成多个特征，这些特征随后被重建为上采样点集。我们的网络在补丁级别应用，具有联合损失函数，鼓励上采样点保持在底层表面上并具有均匀分布。我们使用合成和扫描数据进行了各种实验，以评估我们的方法，并展示了其相对于一些基线方法和基于优化的方法的优越性。结果表明，我们的上采样点具有更好的均匀性，并且更接近底层表面。","领域":"3D点云处理/深度学习/计算机图形学","问题":"解决3D点云数据的稀疏性和不规则性问题，实现高质量的点云上采样","动机":"由于3D点云数据的稀疏性和不规则性，直接使用深度网络进行学习和分析存在挑战，因此需要一种有效的方法来提高点云数据的质量和密度","方法":"提出了一种数据驱动的点云上采样技术，通过学习每个点的多层次特征，并通过多分支卷积单元在特征空间中隐式扩展点集，然后重建为上采样点集","关键词":["点云上采样","多分支卷积","特征空间扩展"],"涉及的技术概念":"3D点云、上采样技术、多分支卷积单元、特征空间扩展、联合损失函数、点云均匀性"},{"order":288,"title":"Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Atapour-Abarghouei_Real-Time_Monocular_Depth_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Atapour-Abarghouei_Real-Time_Monocular_Depth_CVPR_2018_paper.html","abstract":"Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques.","中文标题":"使用合成数据通过图像风格转移进行领域适应的实时单目深度估计","摘要翻译":"近年来，使用基于学习的方法进行单目深度估计变得很有前景。然而，大多数单目深度估计器要么需要依赖大量的地面真实深度数据，这些数据极其昂贵且难以获得，要么使用次级监督信号预测视差作为中间步骤，导致模糊和其他伪影。使用像素级完美的合成数据训练深度估计模型可以解决大多数这些问题，但引入了领域偏差的问题。这是指无法将训练在合成数据上的模型应用于现实世界场景。随着图像风格转移及其与领域适应（最大均值差异）的联系的进展，我们利用风格转移和对抗训练，基于大量合成环境数据的训练，从单个现实世界彩色图像预测像素级完美的深度。实验结果表明，与当代最先进的技术相比，我们的方法具有有效性。","领域":"单目深度估计/图像风格转移/领域适应","问题":"解决单目深度估计中依赖大量真实深度数据或预测视差导致的模糊和伪影问题，以及合成数据训练模型在现实世界应用中的领域偏差问题。","动机":"减少对昂贵且难以获得的地面真实深度数据的依赖，同时提高单目深度估计模型在现实世界场景中的应用能力。","方法":"利用图像风格转移和对抗训练，基于大量合成环境数据的训练，从单个现实世界彩色图像预测像素级完美的深度。","关键词":["单目深度估计","图像风格转移","领域适应","对抗训练","合成数据"],"涉及的技术概念":"单目深度估计是指从单个图像中估计场景的深度信息。图像风格转移是一种将一幅图像的风格应用到另一幅图像上的技术。领域适应是指将在一个领域（如合成数据）上训练的模型适应到另一个领域（如现实世界数据）上的过程。对抗训练是一种训练模型的方法，通过引入一个对抗网络来提高模型的泛化能力。"},{"order":289,"title":"Learning for Disparity Estimation Through Feature Constancy","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Learning_for_Disparity_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Learning_for_Disparity_CVPR_2018_paper.html","abstract":"Stereo matching algorithms usually consist of four steps, including matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement. Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution. In this paper, we propose a network architecture to incorporate all steps of stereo matching. The network consists of three parts. The first part calculates the multi-scale shared features. The second part performs matching cost calculation, matching cost aggregation and disparity calculation to estimate the initial disparity using shared features. The initial disparity and the shared features are used to calculate the feature constancy that measures correctness of the correspondence between two input images. The initial disparity and the feature constancy are then fed to a sub-network to refine the initial disparity. The proposed method has been evaluated on the Scene Flow and KITTI datasets. It achieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015 benchmarks while maintaining a very fast running time. Source code is available at http://github.com/leonzfa/iResNet.","中文标题":"通过特征恒定性学习进行视差估计","摘要翻译":"立体匹配算法通常包括四个步骤：匹配成本计算、匹配成本聚合、视差计算和视差优化。现有的基于CNN的方法仅采用CNN解决这四个步骤中的部分问题，或使用不同的网络处理不同的步骤，这使得它们难以获得整体最优解。在本文中，我们提出了一种网络架构，以整合立体匹配的所有步骤。该网络由三部分组成。第一部分计算多尺度共享特征。第二部分使用共享特征进行匹配成本计算、匹配成本聚合和视差计算，以估计初始视差。初始视差和共享特征用于计算特征恒定性，该恒定性衡量两个输入图像之间对应关系的正确性。然后，初始视差和特征恒定性被输入到一个子网络中，以优化初始视差。所提出的方法已在Scene Flow和KITTI数据集上进行了评估。它在KITTI 2012和KITTI 2015基准测试中实现了最先进的性能，同时保持了非常快的运行时间。源代码可在http://github.com/leonzfa/iResNet获取。","领域":"立体视觉/视差估计/特征匹配","问题":"现有基于CNN的立体匹配方法难以整合所有步骤以获得整体最优解","动机":"提出一种能够整合立体匹配所有步骤的网络架构，以实现更优的视差估计","方法":"提出了一种包含三部分的网络架构，通过计算多尺度共享特征、利用共享特征进行匹配成本计算、聚合和视差计算，以及通过特征恒定性优化初始视差","关键词":["立体匹配","视差估计","特征恒定性","CNN","多尺度特征"],"涉及的技术概念":"立体匹配算法通常包括匹配成本计算、匹配成本聚合、视差计算和视差优化四个步骤。本文提出的网络架构通过计算多尺度共享特征，并利用这些特征进行匹配成本计算、聚合和视差计算，最后通过特征恒定性优化初始视差。特征恒定性用于衡量两个输入图像之间对应关系的正确性。"},{"order":290,"title":"DeepMVS: Learning Multi-View Stereopsis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.html","abstract":"We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.","中文标题":"DeepMVS: 学习多视角立体视觉","摘要翻译":"我们提出了DeepMVS，一个用于多视角立体重建的深度卷积神经网络（ConvNet）。以任意数量的姿态图像作为输入，我们首先生成一组平面扫描体积，并使用提出的DeepMVS网络预测高质量的视差图。实现这些结果的关键贡献包括（1）在照片级真实感的合成数据集上进行监督预训练，（2）一种有效的方法来聚合一组无序图像中的信息，以及（3）整合来自预训练VGG-19网络的多层特征激活。我们使用ETH3D基准验证了DeepMVS的有效性。我们的结果表明，DeepMVS与最先进的传统MVS算法和其他基于ConvNet的方法相比具有优势，特别是在近无纹理区域和细结构方面。","领域":"立体视觉/三维重建/卷积神经网络","问题":"多视角立体视觉重建中的高质量视差图预测","动机":"提高多视角立体视觉重建的精度，特别是在处理近无纹理区域和细结构时","方法":"采用深度卷积神经网络（ConvNet），包括在合成数据集上的监督预训练、无序图像信息聚合方法以及预训练VGG-19网络的多层特征激活整合","关键词":["立体视觉","三维重建","卷积神经网络","视差图","VGG-19"],"涉及的技术概念":"DeepMVS是一个深度卷积神经网络，用于从多视角图像中预测高质量的视差图。它通过监督预训练、无序图像信息聚合和预训练VGG-19网络的多层特征激活整合来实现这一目标。"},{"order":291,"title":"Self-Calibrating Polarising Radiometric Calibration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Teo_Self-Calibrating_Polarising_Radiometric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Teo_Self-Calibrating_Polarising_Radiometric_CVPR_2018_paper.html","abstract":"We present a self-calibrating polarising radiometric calibration method. From a set of images taken from a single viewpoint under different unknown polarising angles, we recover the inverse camera response function and the polarising angles relative to the first angle. The problem is solved in an integrated manner, recovering both of the unknowns simultaneously. The method exploits the fact that the intensity of polarised light should vary sinusoidally as the polarising filter is rotated, provided that the response is linear. It offers the first solution to demonstrate the possibility of radiometric calibration through polarisation. We evaluate the accuracy of our proposed method using synthetic data and real world objects captured using different cameras. The self-calibrated results were found to be comparable with those from multiple exposure sequence.","中文标题":"自校准偏振辐射校准","摘要翻译":"我们提出了一种自校准偏振辐射校准方法。从一组在不同未知偏振角度下从单一视角拍摄的图像中，我们恢复了逆相机响应函数和相对于第一个角度的偏振角度。该问题以集成方式解决，同时恢复了这两个未知数。该方法利用了偏振光强度应随偏振滤光片旋转而正弦变化的事实，前提是响应是线性的。它提供了第一个解决方案，展示了通过偏振进行辐射校准的可能性。我们使用合成数据和不同相机捕获的真实世界对象评估了我们提出方法的准确性。自校准结果被发现与多曝光序列的结果相当。","领域":"辐射校准/偏振成像/相机响应函数","问题":"从单一视角下不同未知偏振角度拍摄的图像中恢复逆相机响应函数和偏振角度","动机":"展示通过偏振进行辐射校准的可能性","方法":"利用偏振光强度随偏振滤光片旋转而正弦变化的特性，以集成方式同时恢复逆相机响应函数和偏振角度","关键词":["自校准","偏振辐射校准","逆相机响应函数","偏振角度"],"涉及的技术概念":"偏振光强度随偏振滤光片旋转而正弦变化，逆相机响应函数，偏振角度"},{"order":292,"title":"Coding Kendall's Shape Trajectories for 3D Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tanfous_Coding_Kendalls_Shape_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tanfous_Coding_Kendalls_Shape_CVPR_2018_paper.html","abstract":"Suitable shape representations as well as their temporal evolution, termed trajectories, often lie to non-linear manifolds. This puts an additional constraint (i.e., non-linearity) in using conventional machine learning techniques for the purpose of classification, event detection, prediction, etc. This paper accommodates the well-known Sparse Coding and Dictionary Learning to the Kendall's shape space and illustrates effective coding of 3D skeletal sequences for action recognition. Grounding on the Riemannian geometry of the shape space, an intrinsic sparse coding and dictionary learning formulation is proposed for static skeletal shapes to overcome the inherent non-linearity of the manifold. As a main result, initial trajectories give rise to sparse code functions with suitable computational properties, including sparsity and vector space representation. To achieve action recognition, two different classification schemes were adopted. A bi-directional LSTM is directly performed on sparse code functions, while a linear SVM is applied after representing sparse code functions  using Fourier temporal pyramid. Experiments conducted on three publicly available datasets show the superiority of the proposed approach compared to existing Riemannian representations and its competitiveness with respect to other recently-proposed approaches. When the benefits of invariance are maintained from the Kendall's shape representation, our approach not only overcomes the problem of non-linearity but also yields to discriminative sparse code functions.","中文标题":"编码Kendall形状轨迹用于3D动作识别","摘要翻译":"合适的形状表示及其时间演化，称为轨迹，通常位于非线性流形上。这在使用传统机器学习技术进行分类、事件检测、预测等方面增加了额外的约束（即非线性）。本文适应了著名的稀疏编码和字典学习到Kendall的形状空间，并说明了用于动作识别的3D骨骼序列的有效编码。基于形状空间的黎曼几何，提出了一个内在的稀疏编码和字典学习公式，用于静态骨骼形状，以克服流形的固有非线性。作为一个主要结果，初始轨迹产生了具有合适计算特性的稀疏编码函数，包括稀疏性和向量空间表示。为了实现动作识别，采用了两种不同的分类方案。双向LSTM直接在稀疏编码函数上执行，而线性SVM在表示稀疏编码函数使用傅里叶时间金字塔后应用。在三个公开可用的数据集上进行的实验显示了所提出方法相对于现有黎曼表示的优越性及其与其他最近提出的方法的竞争力。当从Kendall的形状表示中保持不变性的好处时，我们的方法不仅克服了非线性问题，而且产生了具有区分性的稀疏编码函数。","领域":"3D动作识别/稀疏编码/黎曼几何","问题":"解决3D骨骼序列动作识别中的非线性流形问题","动机":"传统机器学习技术在处理位于非线性流形上的形状表示及其时间演化时存在局限性","方法":"采用稀疏编码和字典学习适应Kendall的形状空间，提出内在的稀疏编码和字典学习公式，使用双向LSTM和线性SVM进行分类","关键词":["3D动作识别","稀疏编码","黎曼几何","Kendall形状空间","非线性流形"],"涉及的技术概念":{"稀疏编码":"一种用于信号处理的技术，旨在用尽可能少的非零系数表示信号","字典学习":"一种机器学习技术，用于从数据中学习一个字典，以便用字典中的元素稀疏表示数据","黎曼几何":"研究黎曼流形的几何性质，适用于处理非线性空间的问题","Kendall形状空间":"用于描述形状的数学空间，考虑了形状的几何特性","双向LSTM":"一种递归神经网络，能够处理序列数据并考虑序列的前后信息","线性SVM":"支持向量机的一种，用于分类问题，通过寻找最优超平面来区分不同类别的数据","傅里叶时间金字塔":"一种用于时间序列分析的技术，通过傅里叶变换在不同时间尺度上分析序列"}},{"order":293,"title":"Efficient, Sparse Representation of Manifold Distance Matrices for Classical Scaling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Turek_Efficient_Sparse_Representation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Turek_Efficient_Sparse_Representation_CVPR_2018_paper.html","abstract":"Geodesic distance matrices can reveal shape properties that are largely invariant to non-rigid deformations, and thus are often used to analyze and represent 3-D shapes. However, these matrices grow quadratically with the number of points. Thus for large point sets it is common to use a low-rank approximation to the distance matrix, which fits in memory and can be efficiently analyzed using methods such as multidimensional scaling (MDS). In this paper we present a novel sparse method for efficiently representing geodesic distance matrices using biharmonic interpolation. This method exploits knowledge of the data manifold to learn a sparse interpolation operator that approximates distances using a subset of points. We show that our method is 2x faster and uses 20x less memory than current leading methods for solving MDS on large point sets, with similar quality. This enables analyses of large point sets that were previously infeasible.","中文标题":"高效、稀疏的流形距离矩阵表示用于经典缩放","摘要翻译":"测地距离矩阵可以揭示对非刚性变形基本不变的形状属性，因此常用于分析和表示3D形状。然而，这些矩阵随着点数的增加而呈二次增长。因此，对于大点集，通常使用距离矩阵的低秩近似，这样可以适应内存，并且可以使用多维缩放（MDS）等方法进行有效分析。在本文中，我们提出了一种新颖的稀疏方法，利用双调和插值高效表示测地距离矩阵。该方法利用数据流形的知识，学习一个稀疏插值算子，使用点的子集来近似距离。我们展示了我们的方法比当前解决大点集MDS的领先方法快2倍，内存使用少20倍，且质量相似。这使得分析以前不可行的大点集成为可能。","领域":"3D形状分析/测地距离/多维缩放","问题":"处理大点集测地距离矩阵的高效表示和分析","动机":"测地距离矩阵随点数增加而急剧增长，导致大点集分析在内存和计算上不可行","方法":"提出一种利用双调和插值的稀疏方法，通过数据流形知识学习稀疏插值算子，使用点的子集近似距离","关键词":["测地距离矩阵","双调和插值","多维缩放","稀疏表示"],"涉及的技术概念":"测地距离矩阵用于揭示形状属性，对非刚性变形基本不变；多维缩放（MDS）是一种用于分析距离矩阵的方法；双调和插值是一种利用数据流形知识进行稀疏表示的技术。"},{"order":294,"title":"Motion Segmentation by Exploiting Complementary Geometric Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Motion_Segmentation_by_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Motion_Segmentation_by_CVPR_2018_paper.html","abstract":"Many real-world sequences cannot be conveniently categorized as general or degenerate; in such cases, imposing a false dichotomy in using the fundamental matrix or homography model for motion segmentation would lead to difficulty. Even when we are confronted with a general scene-motion, the fundamental matrix approach as a model for motion segmentation still suffers from several defects, which we discuss in this paper. The full potential of the fundamental matrix approach could only be realized if we judiciously harness information from the simpler homography model. From these considerations, we propose a multi-view spectral clustering framework that synergistically combines multiple models together. We show that the performance can be substantially improved in this way. We perform extensive testing on existing motion segmentation datasets, achieving state-of-the-art performance on all of them; we also put forth a more realistic and challenging dataset adapted from the KITTI benchmark, containing real-world effects such as strong perspectives and strong forward translations not seen in the traditional datasets.","中文标题":"通过利用互补几何模型进行运动分割","摘要翻译":"许多现实世界的序列不能方便地归类为一般或退化；在这种情况下，在运动分割中使用基本矩阵或单应性模型强加一个错误的二分法会导致困难。即使我们面对的是一个一般的场景运动，基本矩阵方法作为运动分割的模型仍然存在一些缺陷，我们在本文中讨论了这些缺陷。只有我们明智地利用来自更简单的单应性模型的信息，基本矩阵方法的全部潜力才能实现。基于这些考虑，我们提出了一个多视图谱聚类框架，该框架协同地将多个模型结合在一起。我们展示了通过这种方式可以显著提高性能。我们在现有的运动分割数据集上进行了广泛的测试，在所有数据集上都达到了最先进的性能；我们还提出了一个更现实和更具挑战性的数据集，该数据集改编自KITTI基准，包含了传统数据集中未见过的强烈透视和强烈前向平移等现实世界效果。","领域":"运动分割/几何模型/谱聚类","问题":"解决在运动分割中因错误使用基本矩阵或单应性模型导致的困难","动机":"基本矩阵方法在运动分割中存在缺陷，需要结合单应性模型的信息以提高性能","方法":"提出一个多视图谱聚类框架，协同结合多个模型","关键词":["运动分割","几何模型","谱聚类","基本矩阵","单应性模型"],"涉及的技术概念":{"基本矩阵":"用于描述两幅图像之间对应点的几何关系，是计算机视觉中用于运动分割的一个重要概念。","单应性模型":"描述了两个平面之间的映射关系，常用于图像配准和运动分割中。","谱聚类":"一种基于图论的聚类方法，通过利用数据的谱（即特征值）来进行聚类，适用于处理复杂的非凸形状数据。","KITTI基准":"一个广泛使用的自动驾驶数据集，包含丰富的真实世界场景数据，用于评估计算机视觉算法的性能。"}},{"order":295,"title":"Estimation of Camera Locations in Highly Corrupted Scenarios: All About That Base, No Shape Trouble","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Estimation_of_Camera_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Estimation_of_Camera_CVPR_2018_paper.html","abstract":"We propose a strategy for improving camera location estimation in structure from motion. Our setting assumes highly corrupted pairwise directions (i.e., normalized relative location vectors), so there is a clear room for improving current state-of-the-art solutions for this problem. Our strategy identifies severely corrupted pairwise directions by using a geometric consistency condition. It then selects a cleaner set of pairwise directions as a preprocessing step for common solvers. We theoretically guarantee the successful performance of a basic version of our strategy under a synthetic corruption model. Numerical results on artificial and real data demonstrate the significant improvement obtained by our strategy.","中文标题":"高度损坏场景下的相机位置估计：全关于基础，无形状困扰","摘要翻译":"我们提出了一种策略，用于改进从运动中恢复结构的相机位置估计。我们的设置假设成对方向（即归一化的相对位置向量）高度损坏，因此有明确的空间来改进当前针对此问题的最先进解决方案。我们的策略通过使用几何一致性条件来识别严重损坏的成对方向。然后，它选择一组更干净的成对方向作为常见求解器的预处理步骤。我们在合成损坏模型下理论上保证了我们策略基本版本的成功性能。在人工和真实数据上的数值结果证明了我们的策略所获得的显著改进。","领域":"三维重建/相机定位/几何处理","问题":"在高度损坏的成对方向数据中准确估计相机位置","动机":"改进当前从运动中恢复结构的相机位置估计方法，特别是在成对方向数据高度损坏的情况下","方法":"使用几何一致性条件识别并选择更干净的成对方向数据作为预处理步骤","关键词":["相机位置估计","几何一致性","预处理"],"涉及的技术概念":"几何一致性条件用于识别损坏的成对方向数据，预处理步骤选择更干净的数据以提高相机位置估计的准确性。"},{"order":296,"title":"4D Human Body Correspondences From Panoramic Depth Maps","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_4D_Human_Body_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_4D_Human_Body_CVPR_2018_paper.html","abstract":"The availability of affordable 3D full body reconstruction systems has given rise to free-viewpoint video (FVV) of human avatars. Most existing solutions produce temporally uncorrelated point clouds or meshes with unknown point/vertex correspondences. Individually compressing each frame is ineffective and still yields to ultra-large data sizes. We present an end-to-end deep learning scheme to establish dense shape correspondences and subsequently compress the data. Our approach uses sparse set of \\"panoramic\\" depth maps or PDMs, each emulating an inward-viewing concentric mosaics (CM). We then develop a learning-based technique to learn pixel-wise feature descriptors on PDMs. The results are fed into an autoencoder-based network for compression. Comprehensive experiments demonstrate our solution is robust and effective on both public and our newly captured datasets.","中文标题":"从全景深度图中获取4D人体对应关系","摘要翻译":"经济实惠的3D全身重建系统的可用性已经促成了人类化身的自由视点视频（FVV）的兴起。大多数现有解决方案产生时间上不相关的点云或具有未知点/顶点对应关系的网格。单独压缩每一帧是无效的，并且仍然会导致超大数据量。我们提出了一种端到端的深度学习方案，以建立密集的形状对应关系并随后压缩数据。我们的方法使用稀疏的“全景”深度图或PDM集合，每个PDM模拟一个向内观看的同心镶嵌（CM）。然后，我们开发了一种基于学习的技术，以在PDM上学习像素级特征描述符。结果被输入到一个基于自动编码器的网络中进行压缩。全面的实验证明，我们的解决方案在公共数据集和我们新捕获的数据集上都是稳健和有效的。","领域":"3D重建/自由视点视频/数据压缩","问题":"解决3D全身重建系统中时间上不相关的点云或网格的对应关系问题，以及数据压缩问题","动机":"由于现有解决方案在处理3D全身重建时产生的时间上不相关的点云或网格，以及单独压缩每一帧的无效性，导致需要一种有效的方法来建立密集的形状对应关系并压缩数据","方法":"使用稀疏的全景深度图（PDM）集合，开发基于学习的技术在PDM上学习像素级特征描述符，并将结果输入到基于自动编码器的网络中进行压缩","关键词":["3D重建","自由视点视频","数据压缩","全景深度图","自动编码器"],"涉及的技术概念":"3D全身重建系统、自由视点视频（FVV）、点云、网格、全景深度图（PDM）、同心镶嵌（CM）、像素级特征描述符、自动编码器"},{"order":297,"title":"Reconstructing Thin Structures of Manifold Surfaces by Integrating Spatial Curves","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Reconstructing_Thin_Structures_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Reconstructing_Thin_Structures_CVPR_2018_paper.html","abstract":"The manifold surface reconstruction in multi-view stereo often fails in retaining thin structures due to incomplete and noisy reconstructed point clouds. In this paper, we address this problem by leveraging spatial curves. The curve representation in nature is advantageous in modeling thin and elongated structures, implying topology and connectivity information of the underlying geometry, which exactly compensates the weakness of scattered point clouds. We present a novel surface reconstruction method using both curves and point clouds. First, we propose a 3D curve reconstruction algorithm based on the initialize-optimize-expand strategy. Then, tetrahedra are constructed from points and curves, where the volumes of thin structures are robustly preserved by the Curve-conformed Delaunay Refinement. Finally, the mesh surface is extracted from tetrahedra by a graph optimization. The method has been intensively evaluated on both synthetic and real-world datasets, showing significant improvements over state-of-the-art methods.","中文标题":"通过整合空间曲线重建流形表面的薄结构","摘要翻译":"在多视角立体视觉中，流形表面重建往往由于重建点云的不完整和噪声而无法保留薄结构。本文通过利用空间曲线来解决这一问题。曲线表示本质上在建模薄而长的结构方面具有优势，暗示了底层几何的拓扑和连接信息，这正好弥补了散乱点云的弱点。我们提出了一种使用曲线和点云的新颖表面重建方法。首先，我们提出了一种基于初始化-优化-扩展策略的3D曲线重建算法。然后，从点和曲线构建四面体，其中通过曲线符合的Delaunay细化稳健地保留了薄结构的体积。最后，通过图优化从四面体中提取网格表面。该方法已在合成和真实世界数据集上进行了深入评估，显示出相对于最先进方法的显著改进。","领域":"三维重建/几何处理/计算几何","问题":"在多视角立体视觉中保留和重建流形表面的薄结构","动机":"由于重建点云的不完整和噪声，现有的流形表面重建方法难以保留薄结构，这促使我们探索利用空间曲线来改进重建效果。","方法":"提出了一种结合曲线和点云的表面重建方法，包括基于初始化-优化-扩展策略的3D曲线重建算法，使用曲线符合的Delaunay细化构建四面体，并通过图优化提取网格表面。","关键词":["三维重建","几何处理","计算几何"],"涉及的技术概念":"空间曲线、3D曲线重建算法、初始化-优化-扩展策略、曲线符合的Delaunay细化、图优化"},{"order":298,"title":"Multi-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tulsiani_Multi-View_Consistency_as_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tulsiani_Multi-View_Consistency_as_CVPR_2018_paper.html","abstract":"We present a framework for learning single-view shape and pose prediction without using direct supervision for either. Our approach allows leveraging multi-view observations from unknown poses as supervisory signal during training. Our proposed training setup enforces geometric consistency between the independently predicted shape and pose from two views of the same instance. We consequently learn to predict shape in an emergent canonical (view-agnostic) frame along with a corresponding pose predictor.  We show empirical and qualitative results using the ShapeNet dataset and observe encouragingly competitive performance to previous techniques which rely on stronger forms of supervision. We also demonstrate the applicability of our framework in a realistic setting which is beyond the scope of existing techniques: using a training dataset comprised of online product images where the underlying shape and pose are unknown.","中文标题":"多视图一致性作为学习形状和姿态预测的监督信号","摘要翻译":"我们提出了一个框架，用于学习单视图形状和姿态预测，而不使用直接的监督。我们的方法允许在训练期间利用来自未知姿态的多视图观察作为监督信号。我们提出的训练设置强制实施从同一实例的两个视图中独立预测的形状和姿态之间的几何一致性。因此，我们学会了在一个新兴的规范（视图无关）框架中预测形状，以及相应的姿态预测器。我们使用ShapeNet数据集展示了经验和定性结果，并观察到与依赖更强形式监督的先前技术相比，具有令人鼓舞的竞争性能。我们还展示了我们的框架在一个现实设置中的适用性，这超出了现有技术的范围：使用由在线产品图像组成的训练数据集，其中基础形状和姿态是未知的。","领域":"三维重建/姿态估计/自监督学习","问题":"如何在没有直接监督的情况下学习单视图形状和姿态预测","动机":"利用多视图观察作为监督信号，以在没有直接监督的情况下学习形状和姿态预测","方法":"提出了一种训练设置，强制实施从同一实例的两个视图中独立预测的形状和姿态之间的几何一致性，从而在规范框架中预测形状和姿态","关键词":["三维重建","姿态估计","自监督学习","几何一致性","ShapeNet"],"涉及的技术概念":"多视图一致性、几何一致性、自监督学习、ShapeNet数据集、在线产品图像"},{"order":299,"title":"Probabilistic Plant Modeling via Multi-View Image-to-Image Translation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Isokane_Probabilistic_Plant_Modeling_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Isokane_Probabilistic_Plant_Modeling_CVPR_2018_paper.html","abstract":"This paper describes a method for inferring three-dimensional (3D) plant branch structures that are hidden under leaves from multi-view observations. Unlike previous geometric approaches that heavily rely on the visibility of the branches or use parametric branching models, our method makes statistical inferences of branch structures in a probabilistic framework. By inferring the probability of branch existence using a Bayesian extension of image-to-image translation applied to each of multi-view images, our method generates a probabilistic plant 3D model, which represents the 3D branching pattern that cannot be directly observed. Experiments demonstrate the usefulness of the proposed approach in generating convincing branch structures in comparison to prior approaches.","中文标题":"通过多视图图像到图像翻译的概率植物建模","摘要翻译":"本文描述了一种从多视图观察中推断被叶子隐藏的三维（3D）植物分支结构的方法。与之前严重依赖分支可见性或使用参数化分支模型的几何方法不同，我们的方法在概率框架内对分支结构进行统计推断。通过将图像到图像翻译的贝叶斯扩展应用于每个多视图图像来推断分支存在的概率，我们的方法生成了一个概率植物3D模型，该模型代表了无法直接观察到的3D分支模式。实验证明了所提出方法在生成令人信服的分支结构方面与先前方法相比的有用性。","领域":"三维重建/植物建模/图像处理","问题":"从多视图图像中推断被叶子隐藏的三维植物分支结构","动机":"解决传统方法在分支不可见或依赖参数化模型时的局限性，提供一种更准确的植物分支结构推断方法","方法":"使用图像到图像翻译的贝叶斯扩展对多视图图像进行分支存在概率的统计推断，生成概率植物3D模型","关键词":["三维重建","植物建模","图像到图像翻译","贝叶斯推断"],"涉及的技术概念":"图像到图像翻译是一种将一种图像转换为另一种图像的技术，贝叶斯推断是一种统计推断方法，用于在给定证据或数据的情况下更新假设的概率。"},{"order":300,"title":"Deep Marching Cubes: Learning Explicit Surface Representations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Deep_Marching_Cubes_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Deep_Marching_Cubes_CVPR_2018_paper.html","abstract":"Existing learning based solutions to 3D surface prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., TSDF) from which 3D surface meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface prediction. We first demonstrate that the marching cubes algorithm is not differentiable and propose an alternative differentiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demonstrate that the model allows for predicting sub-voxel accurate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.","中文标题":"深度行进立方体：学习显式表面表示","摘要翻译":"现有的基于学习的3D表面预测解决方案无法进行端到端训练，因为它们操作于中间表示（例如，TSDF），从中必须在后处理步骤中提取3D表面网格（例如，通过行进立方体算法）。在本文中，我们研究了端到端3D表面预测的问题。我们首先证明了行进立方体算法是不可微分的，并提出了一个替代的可微分公式，我们将其作为最终层插入到3D卷积神经网络中。我们进一步提出了一组损失函数，允许我们的模型通过稀疏点监督进行训练。我们的实验表明，该模型允许预测任意拓扑的亚体素精确3D形状。此外，即使在稀疏和不完整的地面真实情况下，它也能学会完成形状并将对象的内部与外部分离。我们在从3D点云推断形状的任务上研究了我们的方法的好处。我们的模型是灵活的，可以与各种形状编码器和形状推断技术结合使用。","领域":"3D重建/几何处理/深度学习","问题":"现有的3D表面预测解决方案无法进行端到端训练，因为它们依赖于中间表示和后处理步骤来提取3D表面网格。","动机":"研究动机是为了实现端到端的3D表面预测，避免依赖于中间表示和后处理步骤，从而提高预测的准确性和效率。","方法":"提出了一种可微分的行进立方体算法替代方案，并将其作为最终层插入到3D卷积神经网络中，同时提出了一组损失函数用于通过稀疏点监督训练模型。","关键词":["3D表面预测","行进立方体算法","可微分公式","3D卷积神经网络","稀疏点监督"],"涉及的技术概念":"本文涉及的技术概念包括3D表面预测、行进立方体算法、可微分公式、3D卷积神经网络、稀疏点监督、端到端训练、亚体素精确3D形状预测、形状完成和分离、3D点云形状推断。"},{"order":301,"title":"Tags2Parts: Discovering Semantic Regions From Shape Tags","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Muralikrishnan_Tags2Parts_Discovering_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Muralikrishnan_Tags2Parts_Discovering_Semantic_CVPR_2018_paper.html","abstract":"We propose a novel method for discovering shape regions that strongly correlate with user-prescribed tags. For example, given a collection of chairs tagged as either \\"has armrest\\" or \\"lacks armrest\\", our system correctly highlights the armrest regions as the main distinctive parts between the two chair types.  To obtain point-wise predictions from shape-wise tags we develop a novel neural network architecture that is trained with tag classification loss, but is designed to rely on segmentation to predict the tag. Our network is inspired by U-Net, but we replicate shallow U structures several times with new skip connections and pooling layers, and call the resulting architecture \\"WU-Net\\".  We test our method on segmentation benchmarks and show that even with weak supervision of whole shape tags, our method can infer meaningful semantic regions, without ever observing shape segmentations. Further, once trained, the model can process shapes for which the tag is entirely unknown. As a bonus, our architecture is directly operational under full supervision and performs strongly on standard benchmarks. We validate our method through experiments with many variant architectures and prior baselines, and demonstrate several applications.","中文标题":"Tags2Parts：从形状标签中发现语义区域","摘要翻译":"我们提出了一种新颖的方法，用于发现与用户指定标签强烈相关的形状区域。例如，给定一组被标记为“有扶手”或“无扶手”的椅子，我们的系统能够正确地将扶手区域突出显示为两种椅子类型之间的主要区别部分。为了从形状标签中获得点级预测，我们开发了一种新颖的神经网络架构，该架构通过标签分类损失进行训练，但设计上依赖于分割来预测标签。我们的网络受到U-Net的启发，但我们通过新的跳跃连接和池化层多次复制浅层U结构，并将结果架构称为“WU-Net”。我们在分割基准上测试了我们的方法，并表明即使在整个形状标签的弱监督下，我们的方法也能推断出有意义的语义区域，而无需观察形状分割。此外，一旦训练完成，模型可以处理标签完全未知的形状。作为额外的好处，我们的架构在完全监督下直接操作，并在标准基准上表现强劲。我们通过许多变体架构和先前基线的实验验证了我们的方法，并展示了几个应用。","领域":"语义分割/形状分析/神经网络架构","问题":"如何从用户指定的形状标签中发现与之强烈相关的语义区域","动机":"为了在没有形状分割的情况下，仅通过形状标签推断出有意义的语义区域","方法":"开发了一种新颖的神经网络架构WU-Net，通过标签分类损失进行训练，但设计上依赖于分割来预测标签","关键词":["语义分割","形状分析","神经网络架构"],"涉及的技术概念":"U-Net是一种用于生物医学图像分割的卷积神经网络架构，通过跳跃连接将编码器和解码器连接起来，以捕捉上下文信息。WU-Net是在U-Net的基础上，通过复制浅层U结构并添加新的跳跃连接和池化层来增强网络性能的变体。"},{"order":302,"title":"Uncalibrated Photometric Stereo Under Natural Illumination","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mo_Uncalibrated_Photometric_Stereo_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mo_Uncalibrated_Photometric_Stereo_CVPR_2018_paper.html","abstract":"This paper presents a photometric stereo method that works with unknown natural illuminations without any calibration object. To solve this challenging problem, we propose the use of an equivalent directional lighting model for small surface patches consisting of slowly varying normals, and solve each patch up to an arbitrary rotation ambiguity. Our method connects the resulting patches and unifies the local ambiguities to a global rotation one through angular distance propagation defined over the whole surface. After applying the integrability constraint, our final solution contains only a binary ambiguity, which could be easily removed. Experiments using both synthetic and real-world datasets show our method provides even comparable results to calibrated methods","中文标题":"自然光照下未校准的光度立体视觉","摘要翻译":"本文提出了一种光度立体视觉方法，该方法能够在未知自然光照条件下工作，无需任何校准对象。为了解决这一具有挑战性的问题，我们提出了使用等效方向性光照模型来处理由缓慢变化的法线组成的小表面块，并解决每个块至任意旋转模糊的问题。我们的方法通过在整个表面上定义的角距离传播连接结果块，并将局部模糊统一为全局旋转模糊。在应用可积性约束后，我们的最终解决方案仅包含二元模糊，这可以很容易地去除。使用合成和真实世界数据集的实验表明，我们的方法提供了与校准方法相当的结果。","领域":"光度立体视觉/三维重建/自然光照处理","问题":"在未知自然光照条件下进行光度立体视觉分析，无需校准对象","动机":"解决在自然光照条件下进行光度立体视觉分析时遇到的挑战，特别是无需校准对象的情况下","方法":"提出使用等效方向性光照模型处理小表面块，通过角距离传播统一局部模糊为全局旋转模糊，并应用可积性约束以简化解决方案","关键词":["光度立体视觉","自然光照","三维重建"],"涉及的技术概念":{"光度立体视觉":"一种通过分析物体在不同光照条件下的图像来重建其三维形状的技术","自然光照":"指来自自然光源（如太阳）的光照条件，与人工光源相对","等效方向性光照模型":"一种模拟光照效果以简化复杂光照条件分析的模型","角距离传播":"一种用于统一局部旋转模糊为全局旋转模糊的技术","可积性约束":"在三维重建中用于确保表面法线一致性的数学约束"}},{"order":303,"title":"Robust Depth Estimation From Auto Bracketed Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Im_Robust_Depth_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Im_Robust_Depth_Estimation_CVPR_2018_paper.html","abstract":"As demand for advanced photographic applications on hand-held devices grows, these electronics require the capture of high quality depth. However, under low-light conditions, most devices still suffer from low imaging quality and inaccurate depth acquisition. To address the problem, we present a robust depth estimation method from a short burst shot with varied intensity (i.e., Auto Bracketing) or strong noise (i.e., High ISO). We introduce a geometric transformation between flow and depth tailored for burst images, enabling our learning-based multi-view stereo matching to be performed effectively. We then describe our depth estimation pipeline that incorporates the geometric transformation into our residual-flow network. It allows our framework to produce an accurate depth map even with a bracketed image sequence. We demonstrate that our method outperforms state-of-the-art methods for various datasets captured by a smartphone and a DSLR camera. Moreover, we show that the estimated depth is applicable for image quality enhancement and photographic editing.","中文标题":"从自动包围曝光图像中进行鲁棒的深度估计","摘要翻译":"随着手持设备上高级摄影应用需求的增长，这些电子设备需要捕捉高质量的深度信息。然而，在低光条件下，大多数设备仍然面临成像质量低和深度获取不准确的问题。为了解决这个问题，我们提出了一种从具有不同强度（即自动包围曝光）或强噪声（即高ISO）的短爆发拍摄中进行鲁棒深度估计的方法。我们引入了一种针对爆发图像量身定制的流与深度之间的几何变换，使我们基于学习的多视图立体匹配能够有效执行。然后，我们描述了将几何变换整合到我们的残差流网络中的深度估计流程。这使得我们的框架即使在包围曝光图像序列中也能产生准确的深度图。我们证明了我们的方法在由智能手机和DSLR相机捕捉的各种数据集上优于最先进的方法。此外，我们展示了估计的深度适用于图像质量增强和摄影编辑。","领域":"深度估计/图像增强/摄影编辑","问题":"在低光条件下，手持设备捕捉高质量深度信息的问题","动机":"解决低光条件下手持设备成像质量低和深度获取不准确的问题","方法":"提出了一种从自动包围曝光图像中进行鲁棒深度估计的方法，包括引入几何变换和基于学习的多视图立体匹配，以及将几何变换整合到残差流网络中的深度估计流程","关键词":["深度估计","自动包围曝光","多视图立体匹配","残差流网络","图像质量增强","摄影编辑"],"涉及的技术概念":"自动包围曝光（Auto Bracketing）是一种摄影技术，通过拍摄一系列不同曝光设置的照片来捕捉更广泛的亮度范围。高ISO（High ISO）指的是相机传感器的灵敏度设置，高ISO可以在低光条件下捕捉更亮的图像，但可能会引入更多噪声。几何变换在这里指的是在图像处理中，通过数学变换来调整图像的空间关系，以便更好地进行深度估计。多视图立体匹配（Multi-view Stereo Matching）是一种从多个视角的图像中估计深度信息的技术。残差流网络（Residual-flow Network）是一种深度学习模型，用于处理图像序列中的运动信息，以估计深度。"},{"order":304,"title":"Free Supervision From Video Games","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Krahenbuhl_Free_Supervision_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Krahenbuhl_Free_Supervision_From_CVPR_2018_paper.html","abstract":"Deep networks are extremely hungry for data. They devour hundreds of thousands of labeled images to learn robust and semantically meaningful feature representations. Current networks are so data hungry that collecting labeled data has become as important as designing the networks themselves. Unfortunately, manual data collection is both expensive and time consuming. We present an alternative, and show how ground truth labels for many vision tasks are easily extracted from video games in real time as we play them. We interface the popular Microsoft DirectX rendering API, and inject specialized rendering code into the game as it is running. This code produces ground truth labels for instance segmentation, semantic labeling, depth estimation, optical flow, intrinsic image decomposition, and instance tracking. Instead of labeling images, a researcher now simply plays video games all day long. Our method is general and works on a wide range of video games. We collected a dataset of 220k training images, and 60k test images across 3 video games, and evaluate state of the art optical flow, depth estimation and intrinsic image decomposition algorithms. Our video game data is visually closer to real world images, than other synthetic dataset.","中文标题":"从视频游戏中获取免费监督","摘要翻译":"深度网络对数据的需求极为巨大。它们需要吞噬数十万张标注图像以学习鲁棒且语义上有意义的特征表示。当前的网络对数据的需求如此之大，以至于收集标注数据变得与设计网络本身同等重要。不幸的是，手动数据收集既昂贵又耗时。我们提出了一种替代方案，展示了如何在我们玩游戏时实时从视频游戏中轻松提取许多视觉任务的真实标签。我们与流行的Microsoft DirectX渲染API接口，并在游戏运行时注入专门的渲染代码。这段代码为实例分割、语义标注、深度估计、光流、内在图像分解和实例跟踪生成真实标签。现在，研究人员不再需要标注图像，而是整天玩视频游戏。我们的方法是通用的，适用于广泛的视频游戏。我们收集了一个包含22万张训练图像和6万张测试图像的数据集，涵盖了3款视频游戏，并评估了最先进的光流、深度估计和内在图像分解算法。我们的视频游戏数据在视觉上比其他合成数据集更接近真实世界的图像。","领域":"实例分割/深度估计/光流","问题":"深度网络对大量标注数据的需求","动机":"手动数据收集既昂贵又耗时，需要一种更高效的数据收集方法","方法":"通过接口Microsoft DirectX渲染API，在游戏运行时注入专门的渲染代码，从视频游戏中提取真实标签","关键词":["实例分割","深度估计","光流","内在图像分解","实例跟踪"],"涉及的技术概念":"Microsoft DirectX渲染API、实例分割、语义标注、深度估计、光流、内在图像分解、实例跟踪"},{"order":305,"title":"Planar Shape Detection at Structural Scales","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Planar_Shape_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fang_Planar_Shape_Detection_CVPR_2018_paper.html","abstract":"Interpreting 3D data such as point clouds or surface meshes depends heavily on the scale of observation. Yet, existing algorithms for shape detection rely on trial-and-error parameter tunings to output configurations representative of a structural scale. We present a framework to automatically extract a set of representations that capture the shape and structure of man-made objects at different key abstraction levels. A shape-collapsing process first generates a fine-to-coarse sequence of shape representations by exploiting local planarity. This sequence is then analyzed to identify significant geometric variations between successive representations through a supervised energy minimization. Our framework is flexible enough to learn how to detect both existing structural formalisms such as the CityGML Levels Of Details, and expert-specified levels of abstraction. Experiments on different input data and classes of man-made objects, as well as comparisons with existing shape detection methods, illustrate the strengths of our approach in terms of efficiency and flexibility.","中文标题":"结构尺度下的平面形状检测","摘要翻译":"解释3D数据（如点云或表面网格）在很大程度上取决于观察的尺度。然而，现有的形状检测算法依赖于试错参数调整来输出代表结构尺度的配置。我们提出了一个框架，自动提取一组表示，以捕捉人造物体在不同关键抽象层次上的形状和结构。首先，通过利用局部平面性，形状坍塌过程生成从细到粗的形状表示序列。然后，通过监督能量最小化分析该序列，以识别连续表示之间的显著几何变化。我们的框架足够灵活，可以学习如何检测现有的结构形式（如CityGML细节层次）和专家指定的抽象层次。对不同输入数据和人造物体类别的实验，以及与现有形状检测方法的比较，展示了我们方法在效率和灵活性方面的优势。","领域":"3D形状分析/几何处理/结构检测","问题":"自动提取和识别不同抽象层次下人造物体的形状和结构","动机":"现有形状检测算法依赖试错参数调整，难以自动识别代表结构尺度的配置","方法":"提出一个框架，通过形状坍塌过程生成从细到粗的形状表示序列，并通过监督能量最小化分析该序列，识别显著几何变化","关键词":["3D形状分析","几何处理","结构检测","形状坍塌","监督能量最小化"],"涉及的技术概念":"点云、表面网格、局部平面性、形状坍塌、监督能量最小化、CityGML细节层次"},{"order":306,"title":"Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Pix3D_Dataset_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Pix3D_Dataset_and_CVPR_2018_paper.html","abstract":"We study 3D shape modeling from a single image and make contributions to it in three aspects. First, we present Pix3D, a large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications in shape-related tasks including reconstruction, retrieval, viewpoint estimation, etc. Building such a large-scale dataset, however, is highly challenging; existing datasets either contain only synthetic data, or lack precise alignment between 2D images and 3D shapes, or only have a small number of images. Second, we calibrate the evaluation criteria for 3D shape reconstruction through behavioral studies, and use them to objectively and systematically benchmark cutting-edge reconstruction algorithms on Pix3D. Third, we design a novel model that simultaneously performs 3D reconstruction and pose estimation; our multi-task learning approach achieves state-of-the-art performance on both tasks.","中文标题":"Pix3D: 单图像3D形状建模的数据集与方法","摘要翻译":"我们研究了从单张图像进行3D形状建模，并在三个方面做出了贡献。首先，我们提出了Pix3D，这是一个包含多样化图像-形状对的大规模基准，具有像素级的2D-3D对齐。Pix3D在形状相关的任务中有广泛的应用，包括重建、检索、视角估计等。然而，构建如此大规模的数据集极具挑战性；现有的数据集要么仅包含合成数据，要么缺乏2D图像和3D形状之间的精确对齐，要么只有少量图像。其次，我们通过行为研究校准了3D形状重建的评估标准，并使用它们在Pix3D上客观系统地基准测试了尖端重建算法。第三，我们设计了一个新颖的模型，该模型同时执行3D重建和姿态估计；我们的多任务学习方法在这两项任务上都达到了最先进的性能。","领域":"3D重建/姿态估计/多任务学习","问题":"从单张图像进行3D形状建模的挑战，包括缺乏大规模、精确对齐的图像-形状对数据集，以及现有3D重建算法的评估标准不统一。","动机":"为了解决现有数据集在规模、对齐精度和多样性方面的不足，以及提供统一的评估标准来客观系统地评估3D重建算法。","方法":"提出了Pix3D数据集，通过行为研究校准3D形状重建的评估标准，并设计了一个同时执行3D重建和姿态估计的新模型，采用多任务学习方法。","关键词":["3D重建","姿态估计","多任务学习","数据集构建","评估标准"],"涉及的技术概念":"Pix3D数据集是一个包含多样化图像-形状对的大规模基准，具有像素级的2D-3D对齐。多任务学习方法用于同时执行3D重建和姿态估计，以达到最先进的性能。"},{"order":307,"title":"Camera Pose Estimation With Unknown Principal Point","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Larsson_Camera_Pose_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Larsson_Camera_Pose_Estimation_CVPR_2018_paper.html","abstract":"To estimate the 6-DoF extrinsic pose of a pinhole camera with partially unknown intrinsic parameters is a critical sub-problem in structure-from-motion and camera localization. In most of existing camera pose estimation solvers, the principal point is assumed to be in the image center. Unfortunately, this assumption is not always true, especially for asymmetrically cropped images. In this paper, we develop the first exactly minimal solver for the case of unknown principal point and focal length by using four and a half point correspondences (P4.5Pfuv). We also present an extremely fast solver for the case of unknown aspect ratio (P5Pfuva). The new solvers outperform the previous state-of-the-art in terms of stability and speed. Finally, we explore the extremely challenging case of both unknown principal point and radial distortion, and develop the first practical non-minimal solver by using seven point correspondences (P7Pfruv). Experimental results on both simulated data and real Internet images demonstrate the usefulness of our new solvers.","中文标题":"未知主点的相机姿态估计","摘要翻译":"估计具有部分未知内参的针孔相机的6自由度外参姿态是结构从运动和相机定位中的一个关键子问题。在大多数现有的相机姿态估计求解器中，主点被假定位于图像中心。不幸的是，这一假设并不总是成立，特别是对于非对称裁剪的图像。在本文中，我们通过使用四个半点的对应关系（P4.5Pfuv），开发了第一个针对未知主点和焦距情况的精确最小求解器。我们还提出了一个针对未知纵横比情况的极快求解器（P5Pfuva）。新的求解器在稳定性和速度方面优于之前的最先进技术。最后，我们探索了同时未知主点和径向畸变的极具挑战性的情况，并通过使用七个点的对应关系（P7Pfruv）开发了第一个实用的非最小求解器。在模拟数据和真实互联网图像上的实验结果证明了我们新求解器的实用性。","领域":"相机定位/结构从运动/几何视觉","问题":"在相机内参部分未知的情况下，准确估计相机的6自由度外参姿态","动机":"现有相机姿态估计求解器通常假设主点位于图像中心，这一假设在非对称裁剪图像中不成立，需要开发新的求解器以处理未知主点的情况","方法":"开发了针对未知主点和焦距的精确最小求解器（P4.5Pfuv），针对未知纵横比的极快求解器（P5Pfuva），以及针对同时未知主点和径向畸变的非最小求解器（P7Pfruv）","关键词":["相机姿态估计","主点","焦距","纵横比","径向畸变"],"涉及的技术概念":"6-DoF（六自由度）外参姿态估计，针孔相机模型，结构从运动（Structure-from-Motion），相机定位，主点（Principal Point），焦距（Focal Length），纵横比（Aspect Ratio），径向畸变（Radial Distortion），最小求解器（Minimal Solver），非最小求解器（Non-minimal Solver）"},{"order":308,"title":"Inverse Composition Discriminative Optimization for Point Cloud Registration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.html","abstract":"Rigid Point Cloud Registration (PCReg) refers to the problem of finding the rigid transformation between two sets of point clouds. This problem is particularly important due to the advances in new 3D sensing hardware, and it is challenging because neither the correspondence nor the transformation parameters are known. Traditional local PCReg methods (e.g., ICP) rely on local optimization algorithms, which can get trapped in bad local minima in the presence of noise, outliers, bad initializations, etc. To alleviate these issues, this paper proposes Inverse Composition Discriminative Optimization (ICDO), an extension of Discriminative Optimization (DO), which learns a sequence of update steps from synthetic training data that search the parameter space for an improved solution. Unlike DO, ICDO is object-independent and generalizes even to unseen shapes. We evaluated ICDO on both synthetic and real data, and show that ICDO can match the speed and outperform the accuracy of state-of-the-art PCReg algorithms.","中文标题":"点云配准的逆合成判别优化","摘要翻译":"刚性点云配准（PCReg）指的是寻找两组点云之间的刚性变换的问题。由于新型3D传感硬件的进步，这个问题尤为重要，并且具有挑战性，因为既不知道对应关系也不知道变换参数。传统的局部PCReg方法（例如，ICP）依赖于局部优化算法，这些算法在存在噪声、异常值、不良初始化等情况下可能会陷入不良的局部最小值。为了缓解这些问题，本文提出了逆合成判别优化（ICDO），这是判别优化（DO）的扩展，它从合成训练数据中学习一系列更新步骤，这些步骤在参数空间中搜索改进的解决方案。与DO不同，ICDO是对象无关的，甚至可以推广到未见过的形状。我们在合成数据和真实数据上评估了ICDO，并表明ICDO可以匹配最先进的PCReg算法的速度，并在准确性上超越它们。","领域":"点云配准/3D传感/优化算法","问题":"解决在存在噪声、异常值、不良初始化等情况下，传统局部点云配准方法可能陷入不良局部最小值的问题","动机":"由于新型3D传感硬件的进步，点云配准问题变得尤为重要，但传统方法在处理噪声、异常值等问题时存在局限性","方法":"提出逆合成判别优化（ICDO），一种从合成训练数据中学习更新步骤以在参数空间中搜索改进解决方案的方法","关键词":["点云配准","逆合成判别优化","3D传感"],"涉及的技术概念":"刚性点云配准（PCReg）指的是寻找两组点云之间的刚性变换的问题。逆合成判别优化（ICDO）是一种扩展的判别优化（DO）方法，通过学习一系列更新步骤来改进解决方案，这种方法不依赖于特定对象，能够推广到未见过的形状。"},{"order":309,"title":"SurfConv: Bridging 3D and 2D Convolution for RGBD Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chu_SurfConv_Bridging_3D_CVPR_2018_paper.html","abstract":"The last few years have seen approaches trying to combine the increasing popularity of depth sensors and the success of the convolutional neural networks. Using depth as additional channel alongside the RGB input has the scale variance problem present in image convolution based approaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor. Instead, we propose SurfConv, which “slides” compact 2D filters along the visible 3D surface. SurfConv is formulated as a simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D4) scheme. We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets. Our method achieves state-of-the-art performance while using less than 30% parameters used by the 3D convolution based approaches.","中文标题":"SurfConv: 桥接3D和2D卷积用于RGBD图像","摘要翻译":"过去几年中，有方法尝试结合深度传感器的日益普及和卷积神经网络的成功。将深度作为RGB输入之外的额外通道使用，存在基于图像卷积的方法中的尺度变化问题。另一方面，3D卷积在主要由传感器可见的表面组成的大部分未占用的3D空间上浪费了大量内存。相反，我们提出了SurfConv，它沿着可见的3D表面“滑动”紧凑的2D滤波器。SurfConv通过一种新的数据驱动深度离散化（D4）方案，被表述为一种简单的深度感知多尺度2D卷积。我们在室内和室外的3D语义分割数据集上展示了我们方法的有效性。我们的方法在使用少于基于3D卷积的方法30%的参数的情况下，达到了最先进的性能。","领域":"3D语义分割/深度感知处理/多尺度卷积","问题":"解决在RGBD图像处理中，3D卷积内存消耗大和2D卷积尺度变化的问题","动机":"结合深度传感器和卷积神经网络的优势，提高3D语义分割的效率和准确性","方法":"提出SurfConv方法，通过数据驱动深度离散化（D4）方案，实现深度感知的多尺度2D卷积","关键词":["3D语义分割","深度感知","多尺度卷积"],"涉及的技术概念":"SurfConv是一种新型的卷积方法，它通过在可见的3D表面上滑动2D滤波器来减少内存消耗，同时通过数据驱动深度离散化（D4）方案来处理深度信息，从而实现高效的3D语义分割。"},{"order":310,"title":"A Fast Resection-Intersection Method for the Known Rotation Problem","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_A_Fast_Resection-Intersection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_A_Fast_Resection-Intersection_CVPR_2018_paper.html","abstract":"The known rotation problem refers to a special case of structure-from-motion where the absolute orientations of the cameras are known. When formulated as a minimax (l_infty) problem on reprojection errors, the problem is an instance of pseudo-convex programming. Though theoretically tractable, solving the known rotation problem on large-scale data (1,000’s of views, 10,000’s scene points) using existing methods can be very time-consuming. In this paper, we devise a fast algorithm for the known rotation problem. Our approach alternates between pose estimation and triangulation (i.e., resection-intersection) to break the problem into multiple simpler instances of pseudo-convex programming. The key to the vastly superior performance of our method lies in using a novel minimum enclosing ball (MEB) technique for the calculation of updating steps, which obviates the need for convex optimisation routines and greatly reduces memory footprint. We demonstrate the practicality of our method on large-scale problem instances which easily overwhelm current state-of-the-art algorithms (demo program available in supplementary).","中文标题":"已知旋转问题的快速重投影-交会方法","摘要翻译":"已知旋转问题指的是在运动结构恢复中的一个特殊情况，其中摄像机的绝对方向是已知的。当将其表述为关于重投影误差的最小最大（l_infty）问题时，该问题属于伪凸规划的实例。尽管理论上可处理，但使用现有方法在大规模数据（数千个视角，数万个场景点）上解决已知旋转问题可能非常耗时。在本文中，我们设计了一种针对已知旋转问题的快速算法。我们的方法在姿态估计和三角测量（即重投影-交会）之间交替进行，将问题分解为多个更简单的伪凸规划实例。我们方法性能显著优越的关键在于使用了一种新颖的最小包围球（MEB）技术来计算更新步骤，这消除了对凸优化例程的需求，并大大减少了内存占用。我们在大规模问题实例上展示了我们方法的实用性，这些问题实例很容易压倒当前最先进的算法（补充材料中提供了演示程序）。","领域":"运动结构恢复/伪凸规划/最小包围球技术","问题":"在大规模数据上解决已知旋转问题的效率问题","动机":"现有方法在处理大规模数据的已知旋转问题时效率低下，需要一种更快速的算法","方法":"交替进行姿态估计和三角测量，使用最小包围球技术计算更新步骤","关键词":["运动结构恢复","伪凸规划","最小包围球技术"],"涉及的技术概念":"已知旋转问题是指在运动结构恢复中，摄像机的绝对方向已知的情况。本文通过将问题表述为关于重投影误差的最小最大问题，属于伪凸规划的实例。为了解决大规模数据上的效率问题，本文提出了一种快速算法，该方法在姿态估计和三角测量之间交替进行，并使用最小包围球技术来计算更新步骤，从而避免了凸优化例程的需求，并减少了内存占用。"},{"order":311,"title":"3D Pose Estimation and 3D Model Retrieval for Objects in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Grabner_3D_Pose_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Grabner_3D_Pose_Estimation_CVPR_2018_paper.html","abstract":"We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.","中文标题":"野外物体的3D姿态估计与3D模型检索","摘要翻译":"我们提出了一种可扩展、高效且准确的方法，用于检索野外物体的3D模型。我们的贡献是双重的。首先，我们提出了一种针对物体类别的3D姿态估计方法，该方法在Pascal3D+上显著优于现有技术。其次，我们使用估计的姿态作为先验，检索能够准确表示RGB图像中物体几何形状的3D模型。为此，我们从3D模型中根据预测的姿态渲染深度图像，并使用基于CNN的多视角度量学习方法将RGB图像的学习图像描述符与渲染的深度图像的描述符进行匹配。通过这种方式，我们首次报告了在Pascal3D+上进行3D模型检索的定量结果，我们的方法在验证图像中平均有50%的情况下选择了与人类注释者相同的模型。此外，我们展示了我们的方法，该方法仅在Pascal3D+上训练，能够从ShapeNet中检索出丰富且准确的3D模型，给定野外物体的RGB图像。","领域":"3D视觉/物体识别/几何建模","问题":"如何在野外环境中准确估计物体的3D姿态并检索相应的3D模型","动机":"提高在复杂环境中对物体3D姿态估计的准确性，并有效检索出能够准确表示物体几何形状的3D模型","方法":"首先提出一种3D姿态估计方法，然后使用估计的姿态作为先验，通过渲染深度图像并匹配图像描述符来检索3D模型","关键词":["3D姿态估计","3D模型检索","多视角度量学习"],"涉及的技术概念":"3D姿态估计是指在三维空间中确定物体的位置和方向。3D模型检索是指从数据库中检索出与给定图像或描述相匹配的三维模型。多视角度量学习是一种通过比较不同视角下的图像特征来学习相似度度量的方法。"},{"order":312,"title":"Structure From Recurrent Motion: From Rigidity to Recurrency","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Structure_From_Recurrent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Structure_From_Recurrent_CVPR_2018_paper.html","abstract":"This   paper   proposes   a   new   method   for   Non-rigidstructure-from-motion   (NRSfM).   Departing   significantlyfrom  the  traditional  idea  of  using  linear  low-order  shapemodel for NRSfM, our method exploits the property of shaperecurrence (i.e. many dynamic shapes tend to repeat them-selves in time).  We show that recurrency is in fact agen-eralized  rigidity.   Based  on  this,  we  show  how  to  reduceNRSfM problems to rigid ones, provided that the recurrencecondition  is  satisfied.   Given  such  a  reduction,  standardrigid-SFM techniques can be applied directly (without anychange) to reconstruct the non-rigid dynamic shape. To im-plement this idea as a practical approach,  this paper de-velops efficient and reliable algorithm for automatic recur-rence detection,  as well as new method for camera viewsclustering via rigidity-check. Experiments on both syntheticsequences and real data demonstrate the effectiveness of theproposed method. Since the method provides novel perspec-tive to look at Structure-from-Motion, we hope it will inspireother new researches in the field.","中文标题":"从重复运动中恢复结构：从刚性到重复性","摘要翻译":"本文提出了一种新的非刚性结构恢复运动（NRSfM）方法。与传统使用线性低阶形状模型进行NRSfM的方法显著不同，我们的方法利用了形状重复性的特性（即许多动态形状倾向于在时间上重复自己）。我们展示了重复性实际上是刚性的一种广义形式。基于这一点，我们展示了如何在满足重复性条件的情况下，将NRSfM问题简化为刚性结构恢复问题。给定这样的简化，标准的刚性结构恢复运动技术可以直接（无需任何改变）应用于重建非刚性动态形状。为了实现这一想法作为一种实用方法，本文开发了高效可靠的自动重复性检测算法，以及通过刚性检查进行相机视图聚类的新方法。在合成序列和真实数据上的实验证明了所提出方法的有效性。由于该方法提供了看待结构恢复运动的新视角，我们希望它能激发该领域的其他新研究。","领域":"三维重建/动态形状分析/相机视图聚类","问题":"非刚性结构恢复运动（NRSfM）问题","动机":"探索形状重复性作为解决非刚性结构恢复运动问题的新视角","方法":"利用形状重复性将NRSfM问题简化为刚性结构恢复问题，开发自动重复性检测算法和相机视图聚类新方法","关键词":["非刚性结构恢复运动","形状重复性","刚性结构恢复","相机视图聚类"],"涉及的技术概念":"非刚性结构恢复运动（NRSfM）是一种从视频序列中恢复非刚性物体三维形状的技术。形状重复性指的是动态形状在时间上的重复特性。刚性结构恢复运动（SFM）是一种从视频序列中恢复刚性物体三维形状的技术。自动重复性检测算法用于识别形状重复性的模式。相机视图聚类通过刚性检查将相似的相机视图分组，以便更有效地处理非刚性动态形状的恢复。"},{"order":313,"title":"Learning Patch Reconstructability for Accelerating Multi-View Stereo","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Poms_Learning_Patch_Reconstructability_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Poms_Learning_Patch_Reconstructability_CVPR_2018_paper.html","abstract":"We present an approach to accelerate multi-view stereo (MVS) by prioritizing computation on image patches that are likely to produce accurate 3D surface reconstructions. Our key insight is that the accuracy of the surface reconstruction from a given image patch can be predicted significantly faster than performing the actual stereo matching. The intuition is that non-specular, fronto-parallel, in-focus patches are more likely to produce accurate surface reconstructions than highly specular, slanted, blurry patches --- and that these properties can be reliably predicted from the image itself. By prioritizing stereo matching on a subset of patches that are highly reconstructable and also cover the 3D surface, we are able to accelerate MVS with minimal reduction in accuracy and completeness. To predict the reconstructability score of an image patch from a single view, we train an image-to-reconstructability neural network: the I2RNet. This reconstructability score enables us to efficiently identify image patches that are likely to provide the most accurate surface estimates before performing stereo matching. We demonstrate that the I2RNet, when trained on the ScanNet dataset, generalizes to the DTU and Tanks and Temples MVS datasets. By using our I2RNet with an existing MVS implementation, we show that our method can achieve more than a 30x speed-up over the baseline with only an minimal loss in completeness.","中文标题":"学习图像块可重建性以加速多视图立体视觉","摘要翻译":"我们提出了一种通过优先计算可能产生准确3D表面重建的图像块来加速多视图立体视觉（MVS）的方法。我们的关键见解是，从给定图像块进行表面重建的准确性可以比实际执行立体匹配更快地预测。直觉是非镜面、正面平行、对焦的图像块比高度镜面、倾斜、模糊的图像块更可能产生准确的表面重建——这些属性可以从图像本身可靠地预测。通过优先对高度可重建且覆盖3D表面的图像块子集进行立体匹配，我们能够在准确性和完整性最小减少的情况下加速MVS。为了从单一视图预测图像块的可重建性分数，我们训练了一个图像到可重建性神经网络：I2RNet。这个可重建性分数使我们能够在执行立体匹配之前有效地识别可能提供最准确表面估计的图像块。我们证明，当在ScanNet数据集上训练时，I2RNet能够泛化到DTU和Tanks and Temples MVS数据集。通过将我们的I2RNet与现有的MVS实现结合使用，我们展示了我们的方法可以在完整性仅最小损失的情况下实现超过30倍的加速。","领域":"3D重建/立体视觉/神经网络","问题":"加速多视图立体视觉（MVS）的计算过程","动机":"通过预测图像块的可重建性来优先计算，以加速MVS同时保持重建的准确性和完整性","方法":"训练一个图像到可重建性神经网络（I2RNet）来预测图像块的可重建性分数，并优先对高度可重建的图像块进行立体匹配","关键词":["3D重建","立体视觉","神经网络"],"涉及的技术概念":"多视图立体视觉（MVS）、图像块可重建性、立体匹配、图像到可重建性神经网络（I2RNet）、ScanNet数据集、DTU数据集、Tanks and Temples MVS数据集"},{"order":314,"title":"Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Progressively_Complementarity-Aware_Fusion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Progressively_Complementarity-Aware_Fusion_CVPR_2018_paper.html","abstract":"How to incorporate cross-modal complementarity sufficiently is the cornerstone question for RGB-D salient object detection. Previous works mainly address this issue by simply concatenating multi-modal features or combining unimodal predictions. In this paper, we answer this question from two perspectives: (1) We argue that if the complementary part can be modelled more explicitly, the cross-modal complement is likely to be better captured. To this end, we design a novel complementarity-aware fusion (CA-Fuse) module when adopting the Convolutional Neural Network (CNN). By introducing cross-modal residual functions and complementarity-aware supervisions in each CA-Fuse module, the problem of learning complementary information from the paired modality is explicitly posed as asymptotically approximating the residual function. (2) Exploring the complement across all the levels. By cascading the CA-Fuse module and adding level-wise supervision from deep to shallow densely, the cross-level complement can be selected and combined progressively. The proposed RGB-D fusion network disambiguates both cross-modal and cross-level fusion processes and enables more sufficient fusion results. The experiments on public datasets show the effectiveness of the proposed CA-Fuse module and the RGB-D salient object detection network.","中文标题":"渐进互补感知融合网络用于RGB-D显著目标检测","摘要翻译":"如何充分整合跨模态互补性是RGB-D显著目标检测的基石问题。以往的工作主要通过简单地连接多模态特征或结合单模态预测来解决这个问题。在本文中，我们从两个角度回答这个问题：（1）我们认为，如果能更明确地建模互补部分，跨模态互补性可能会被更好地捕捉。为此，我们在采用卷积神经网络（CNN）时设计了一种新颖的互补感知融合（CA-Fuse）模块。通过在每个CA-Fuse模块中引入跨模态残差函数和互补感知监督，从配对模态中学习互补信息的问题被明确地提出为渐进逼近残差函数。（2）探索所有层次的互补性。通过级联CA-Fuse模块并从深到浅密集地添加层次监督，跨层次互补性可以被逐步选择和结合。提出的RGB-D融合网络消除了跨模态和跨层次融合过程的歧义，并实现了更充分的融合结果。在公共数据集上的实验显示了所提出的CA-Fuse模块和RGB-D显著目标检测网络的有效性。","领域":"显著目标检测/跨模态学习/卷积神经网络","问题":"如何充分整合跨模态互补性以改进RGB-D显著目标检测","动机":"为了更有效地捕捉跨模态互补性，提高RGB-D显著目标检测的性能","方法":"设计了一种新颖的互补感知融合（CA-Fuse）模块，通过引入跨模态残差函数和互补感知监督，以及通过级联CA-Fuse模块和添加层次监督来探索所有层次的互补性","关键词":["显著目标检测","跨模态学习","卷积神经网络","互补感知融合","RGB-D融合"],"涉及的技术概念":"跨模态互补性指的是不同模态（如RGB和深度）之间能够互相补充的信息。互补感知融合（CA-Fuse）模块是一种设计用于捕捉这种互补性的模块，通过引入跨模态残差函数和互补感知监督来实现。卷积神经网络（CNN）是一种深度学习模型，广泛用于图像识别和处理任务。RGB-D显著目标检测是指使用RGB图像和深度信息来检测图像中最显著的目标。"},{"order":315,"title":"Pixels, Voxels, and Views: A Study of Shape Representations for Single View 3D Object Shape Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_Pixels_Voxels_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_Pixels_Voxels_and_CVPR_2018_paper.html","abstract":"The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input.  By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewer-centered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our findings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also find that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame significantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.","中文标题":"像素、体素与视角：单视图3D物体形状预测的形状表示研究","摘要翻译":"本文的目标是比较基于表面和基于体积的3D物体形状表示，以及以观察者为中心和以物体为中心的参考框架，用于单视图3D形状预测。我们提出了一种新算法，用于从多个视角预测深度图，输入为单个深度或RGB图像。通过修改网络和模型评估方式，我们可以直接比较体素与表面、以观察者为中心与以物体为中心的表示对于熟悉与不熟悉物体的优劣，这些预测基于RGB或深度图像。在我们的发现中，我们展示了对于新类别物体，基于表面的方法优于体素表示，并产生更高分辨率的输出。我们还发现，使用以观察者为中心的坐标对于新物体是有利的，而以物体为中心的表示对于更熟悉的物体更好。有趣的是，坐标框架显著影响了学习的形状表示，以物体为中心的表示更重视隐式识别物体类别，而以观察者为中心的表示产生的形状表示较少依赖于类别识别。","领域":"3D形状预测/深度图预测/物体识别","问题":"比较不同3D物体形状表示和参考框架在单视图3D形状预测中的效果","动机":"探索和比较基于表面和体积的3D物体形状表示，以及不同参考框架对单视图3D形状预测的影响，以提高预测的准确性和分辨率","方法":"提出一种新算法，通过修改网络和模型评估方式，从多个视角预测深度图，输入为单个深度或RGB图像","关键词":["3D形状预测","深度图预测","物体识别","表面表示","体素表示","观察者中心","物体中心"],"涉及的技术概念":"本文涉及的技术概念包括3D物体形状的表示方法（基于表面和基于体积）、参考框架（以观察者为中心和以物体为中心）、深度图预测算法、网络修改和模型评估方式。"},{"order":316,"title":"Learning Dual Convolutional Neural Networks for Low-Level Vision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Learning_Dual_Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Learning_Dual_Convolutional_CVPR_2018_paper.html","abstract":"In this paper, we propose a general dual convolutional neural network (DualCNN) for low-level vision problems, e.g., super-resolution, edge-preserving filtering, deraining and dehazing. These problems usually involve the estimation of two components of the target signals: structures and details. Motivated by this, our proposed DualCNN consists of two parallel branches, which respectively recovers the structures and details in an end-to-end manner. The recovered structures and details can generate the target signals according to the formation model for each particular application. The DualCNN is a flexible framework for low-level vision tasks and can be easily incorporated with existing CNNs. Experimental results show that the DualCNN can be effectively applied to numerous low-level vision tasks with favorable performance against the state-of-the-art methods.","中文标题":"学习用于低级视觉的双卷积神经网络","摘要翻译":"在本文中，我们提出了一种通用的双卷积神经网络（DualCNN）用于低级视觉问题，例如超分辨率、边缘保留滤波、去雨和去雾。这些问题通常涉及目标信号的两个组成部分的估计：结构和细节。受此启发，我们提出的DualCNN由两个并行分支组成，分别以端到端的方式恢复结构和细节。恢复的结构和细节可以根据每个特定应用的形成模型生成目标信号。DualCNN是一个灵活的框架，适用于低级视觉任务，并且可以轻松地与现有的CNN结合。实验结果表明，DualCNN可以有效地应用于众多低级视觉任务，与最先进的方法相比具有优越的性能。","领域":"超分辨率/边缘保留滤波/去雨和去雾","问题":"低级视觉问题中的结构和细节估计","动机":"低级视觉问题通常需要同时估计目标信号的结构和细节，这促使我们开发一种能够并行恢复这两个组成部分的网络。","方法":"提出了一种双卷积神经网络（DualCNN），该网络由两个并行分支组成，分别用于恢复目标信号的结构和细节，并以端到端的方式工作。","关键词":["双卷积神经网络","低级视觉","超分辨率","边缘保留滤波","去雨","去雾"],"涉及的技术概念":"双卷积神经网络（DualCNN）是一种用于低级视觉任务的网络架构，它通过两个并行分支分别恢复图像的结构和细节。这种网络架构的灵活性使其能够与现有的卷积神经网络（CNN）结合，以解决多种低级视觉问题，如超分辨率、边缘保留滤波、去雨和去雾。"},{"order":317,"title":"Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Defocus_Blur_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Defocus_Blur_Detection_CVPR_2018_paper.html","abstract":"Defocus blur detection (DBD) is the separation of infocus and out-of-focus regions in an image. This process has been paid considerable attention because of its remarkable potential applications. Accurate differentiation of homogeneous regions and detection of low-contrast focal regions, as well as suppression of background clutter, are challenges associated with DBD. To address these issues, we propose a multi-stream bottom-top-bottom fully convolutional network (BTBNet), which is the first attempt to develop an end-to-end deep network for DBD. First, we develop a fully convolutional BTBNet to integrate low-level cues and high-level semantic information. Then, considering that the degree of defocus blur is sensitive to scales, we propose multi-stream BTBNets that handle input images with different scales to improve the performance of DBD. Finally, we design a fusion and recursive reconstruction network to recursively refine the preceding blur detection maps. To promote further study and evaluation of the DBD models, we construct a new database of 500 challenging images and their pixel-wise defocus blur annotations. Experimental results on the existing and our new datasets demonstrate that the proposed method achieves significantly better performance than other state-of-the-art algorithms.","中文标题":"通过多流自下而上-自上而下全卷积网络进行离焦模糊检测","摘要翻译":"离焦模糊检测（DBD）是区分图像中聚焦和离焦区域的过程。由于其显著的应用潜力，这一过程受到了广泛关注。准确区分同质区域和检测低对比度聚焦区域，以及抑制背景杂波，是与DBD相关的挑战。为了解决这些问题，我们提出了一种多流自下而上-自上而下全卷积网络（BTBNet），这是首次尝试开发用于DBD的端到端深度网络。首先，我们开发了一个全卷积BTBNet，以整合低级线索和高级语义信息。然后，考虑到离焦模糊的程度对尺度敏感，我们提出了多流BTBNets，处理不同尺度的输入图像，以提高DBD的性能。最后，我们设计了一个融合和递归重建网络，以递归地细化前面的模糊检测图。为了促进DBD模型的进一步研究和评估，我们构建了一个包含500张挑战性图像及其像素级离焦模糊注释的新数据库。在现有和我们的新数据集上的实验结果表明，所提出的方法比其他最先进的算法实现了显著更好的性能。","领域":"图像质量评估/图像分割/图像增强","问题":"准确区分图像中的聚焦和离焦区域，检测低对比度聚焦区域，以及抑制背景杂波","动机":"离焦模糊检测具有显著的应用潜力，但准确区分同质区域和检测低对比度聚焦区域，以及抑制背景杂波是挑战","方法":"提出了一种多流自下而上-自上而下全卷积网络（BTBNet），整合低级线索和高级语义信息，处理不同尺度的输入图像，设计融合和递归重建网络递归地细化模糊检测图","关键词":["离焦模糊检测","全卷积网络","图像分割"],"涉及的技术概念":"离焦模糊检测（DBD）是指区分图像中聚焦和离焦区域的过程。全卷积网络（Fully Convolutional Network, FCN）是一种用于图像分割的深度学习模型，能够处理任意大小的输入图像并输出相应大小的分割图。多流网络指的是处理输入图像的不同尺度，以提高检测性能。融合和递归重建网络用于递归地细化模糊检测图，提高检测的准确性。"},{"order":318,"title":"PiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_PiCANet_Learning_Pixel-Wise_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_PiCANet_Learning_Pixel-Wise_CVPR_2018_paper.html","abstract":"Contexts play an important role in the saliency detection task. However, given a context region, not all contextual information is helpful for the final task. In this paper, we propose a novel pixel-wise contextual attention network, i.e., the PiCANet, to learn to selectively attend to informative context locations for each pixel. Specifically, for each pixel, it can generate an attention map in which each attention weight corresponds to the contextual relevance at each context location. An attended contextual feature can then be constructed by selectively aggregating the contextual information. We formulate the proposed PiCANet in both global and local forms to attend to global and local contexts, respectively. Both models are fully differentiable and can be embedded into CNNs for joint training. We also incorporate the proposed models with the U-Net architecture to detect salient objects. Extensive experiments show that the proposed PiCANets can consistently improve saliency detection performance. The global and local PiCANets facilitate learning global contrast and homogeneousness, respectively. As a result, our saliency model can detect salient objects more accurately and uniformly, thus performing favorably against the state-of-the-art methods.","中文标题":"PiCANet: 学习像素级上下文注意力用于显著性检测","摘要翻译":"上下文在显著性检测任务中扮演着重要角色。然而，给定一个上下文区域，并非所有的上下文信息都对最终任务有帮助。在本文中，我们提出了一种新颖的像素级上下文注意力网络，即PiCANet，以学习为每个像素选择性地关注信息丰富的上下文位置。具体来说，对于每个像素，它可以生成一个注意力图，其中每个注意力权重对应于每个上下文位置的上下文相关性。然后，通过选择性地聚合上下文信息，可以构建一个被关注的上下文特征。我们将提出的PiCANet以全局和局部形式进行公式化，分别关注全局和局部上下文。这两种模型都是完全可微的，并且可以嵌入到CNNs中进行联合训练。我们还将提出的模型与U-Net架构结合，以检测显著对象。大量实验表明，提出的PiCANets可以持续提高显著性检测性能。全局和局部PiCANets分别有助于学习全局对比度和同质性。因此，我们的显著性模型可以更准确和均匀地检测显著对象，从而在与最先进方法的比较中表现优异。","领域":"显著性检测/注意力机制/卷积神经网络","问题":"如何选择性地利用上下文信息以提高显著性检测的准确性","动机":"在显著性检测任务中，有效地利用上下文信息可以提高检测的准确性，但并非所有上下文信息都是有益的，因此需要一种方法来选择性地关注对任务有帮助的上下文信息。","方法":"提出了一种像素级上下文注意力网络（PiCANet），通过为每个像素生成注意力图来选择性地关注信息丰富的上下文位置，并构建被关注的上下文特征。该网络以全局和局部形式存在，可以嵌入到CNNs中进行联合训练。","关键词":["显著性检测","注意力机制","卷积神经网络","U-Net架构"],"涉及的技术概念":"PiCANet是一种新颖的像素级上下文注意力网络，旨在通过为每个像素生成注意力图来选择性地关注信息丰富的上下文位置。这种方法可以嵌入到卷积神经网络（CNNs）中，与U-Net架构结合使用，以提高显著性检测的准确性。全局和局部PiCANets分别关注全局对比度和同质性，从而更准确和均匀地检测显著对象。"},{"order":319,"title":"Curve Reconstruction via the Global Statistics of Natural Curves","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Barnea_Curve_Reconstruction_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Barnea_Curve_Reconstruction_via_CVPR_2018_paper.html","abstract":"Reconstructing the missing parts of a curve has been the subject of much computational research, with applications in image inpainting, object synthesis, etc. Different approaches for solving that problem are typically based on processes that seek visually pleasing or perceptually plausible completions. In this work we focus on reconstructing the underlying physically likely shape by  utilizing the global statistics of natural curves. More specifically, we develop a reconstruction model that seeks the mean physical curve for a given inducer configuration. This simple model is both straightforward to compute and it is receptive to diverse additional information, but it requires enough samples for all curve configurations, a practical requirement that limits its effective utilization. To address this practical issue we explore and exploit statistical geometrical properties of natural curves, and in particular, we show that in many cases the mean curve is scale invariant and often times it is extensible. This, in turn, allows to boost the number of examples and thus the robustness of the statistics and its applicability. The reconstruction results are not only more physically plausible but they also lead to important insights on the reconstruction problem, including an elegant explanation why certain inducer configurations are more likely to yield consistent perceptual completions than others.","中文标题":"通过自然曲线的全局统计进行曲线重建","摘要翻译":"重建曲线的缺失部分一直是计算研究的主题，应用于图像修复、物体合成等领域。解决该问题的不同方法通常基于寻求视觉上令人愉悦或感知上合理的完成过程。在这项工作中，我们专注于通过利用自然曲线的全局统计来重建潜在的物理可能形状。更具体地说，我们开发了一个重建模型，该模型寻求给定诱导配置的平均物理曲线。这个简单的模型既易于计算，又能接受各种附加信息，但它需要足够多的样本来覆盖所有曲线配置，这一实际需求限制了其有效利用。为了解决这一实际问题，我们探索并利用了自然曲线的统计几何特性，特别是我们展示了在许多情况下，平均曲线是尺度不变的，并且通常是可扩展的。这反过来又增加了示例的数量，从而增强了统计的鲁棒性和适用性。重建结果不仅在物理上更为合理，而且还对重建问题提供了重要的见解，包括为什么某些诱导配置比其他配置更有可能产生一致的感知完成的优雅解释。","领域":"图像修复/物体合成/曲线重建","问题":"重建曲线的缺失部分","动机":"通过利用自然曲线的全局统计来重建潜在的物理可能形状，以解决图像修复和物体合成中的问题","方法":"开发了一个重建模型，该模型寻求给定诱导配置的平均物理曲线，并探索和利用了自然曲线的统计几何特性","关键词":["曲线重建","图像修复","物体合成"],"涉及的技术概念":"全局统计、自然曲线、物理可能形状、诱导配置、尺度不变性、可扩展性"},{"order":320,"title":"What Do Deep Networks Like to See?","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Palacio_What_Do_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Palacio_What_Do_Deep_CVPR_2018_paper.html","abstract":"We propose a novel way to measure and understand convolutional neural networks by quantifying the amount of input signal they let in. To do this, an autoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with fixed parameters. We compared the reconstructed samples from AEs that were fine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and Inception~v3) and found substantial differences. The AE learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. Measuring the changes in accuracy when the signal of one classifier is used by a second one, a relation of total order emerges. This order depends directly on each classifier's input signal but it does not correlate with classification accuracy or network size. Further evidence of this phenomenon is provided by measuring the normalized mutual information between original images and auto-encoded reconstructions from different fine-tuned AEs. These findings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their results. We present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses.","中文标题":"深度网络喜欢看什么？","摘要翻译":"我们提出了一种新颖的方法来量化和理解卷积神经网络，通过量化它们允许输入的信号量。为此，我们使用来自预训练分类器的梯度对自动编码器（AE）进行了微调，该分类器的参数是固定的。我们比较了在一组图像分类器（AlexNet、VGG16、ResNet-50和Inception~v3）上微调的AE重建样本，并发现了显著差异。AE根据反向传播梯度中编码的信息学习保留输入空间的哪些方面以及忽略哪些方面。当使用一个分类器的信号由第二个分类器使用时，测量准确性的变化，出现了一个全序关系。这个顺序直接取决于每个分类器的输入信号，但它与分类准确性或网络大小无关。通过测量原始图像和来自不同微调AE的自动编码重建之间的归一化互信息，提供了这一现象的进一步证据。这些发现为神经网络理解领域开辟了新天地，为推理、调试和解释其结果开辟了新途径。我们提出了文献中的四个具体例子，现在可以用模型使用的输入信号来解释这些观察结果。","领域":"卷积神经网络/自动编码器/图像分类","问题":"如何量化和理解卷积神经网络允许输入的信号量","动机":"探索卷积神经网络如何处理和选择输入信号，以更好地理解、调试和解释神经网络的结果","方法":"使用预训练分类器的梯度对自动编码器进行微调，比较不同图像分类器上微调的AE重建样本，测量分类器间信号使用的准确性变化和归一化互信息","关键词":["卷积神经网络","自动编码器","图像分类","信号量化","网络理解"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","自动编码器":"一种神经网络，用于学习数据的有效编码，通过压缩输入数据到潜在空间表示，然后重建输入数据","图像分类":"将图像分配到预定义类别的任务","信号量化":"测量和量化神经网络处理输入信号的方式","归一化互信息":"一种衡量两个变量之间相互依赖性的方法，用于评估原始图像和重建图像之间的相似性"}},{"order":321,"title":"“Zero-Shot” Super-Resolution Using Deep Internal Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shocher_Zero-Shot_Super-Resolution_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shocher_Zero-Shot_Super-Resolution_Using_CVPR_2018_paper.html","abstract":"Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce \`\`Zero-Shot'' SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA  CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.","中文标题":"使用深度内部学习的“零样本”超分辨率","摘要翻译":"深度学习在过去几年中引领了超分辨率（SR）性能的戏剧性飞跃。然而，由于是监督学习，这些SR方法受限于特定的训练数据，其中从高分辨率（HR）图像获取低分辨率（LR）图像的过程是预先确定的（例如，双三次下采样），没有任何干扰伪影（例如，传感器噪声、图像压缩、非理想PSF等）。然而，真实的LR图像很少遵守这些限制，导致最先进（SotA）方法的SR结果不佳。在本文中，我们介绍了“零样本”SR，它利用了深度学习的力量，但不依赖于先前的训练。我们利用单张图像内部信息的重复性，在测试时仅从输入图像本身提取的示例上训练一个小型的图像特定CNN。因此，它可以适应每张图像的不同设置。这使得可以对真实的老照片、噪声图像、生物数据以及其他采集过程未知或非理想的图像进行SR。在此类图像上，我们的方法优于基于CNN的SotA SR方法以及之前的无监督SR方法。据我们所知，这是第一个基于CNN的无监督SR方法。","领域":"超分辨率/图像复原/深度学习","问题":"解决真实低分辨率图像超分辨率效果不佳的问题","动机":"现有的超分辨率方法依赖于特定的训练数据和预先确定的低分辨率图像获取过程，无法很好地处理真实世界中的低分辨率图像","方法":"利用单张图像内部信息的重复性，在测试时仅从输入图像本身提取的示例上训练一个小型的图像特定CNN，实现无监督的超分辨率","关键词":["超分辨率","无监督学习","卷积神经网络"],"涉及的技术概念":"超分辨率（SR）是指从低分辨率（LR）图像恢复高分辨率（HR）图像的技术。深度学习是一种机器学习方法，通过构建多层的神经网络来学习数据的深层次特征。卷积神经网络（CNN）是一种深度学习模型，特别适合处理图像数据。无监督学习是一种机器学习方法，它不依赖于标注数据，而是从数据本身学习特征和结构。"},{"order":322,"title":"Detect Globally, Refine Locally: A Novel Approach to Saliency Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Detect_Globally_Refine_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Detect_Globally_Refine_CVPR_2018_paper.html","abstract":"Effective integration of contextual information is crucial for salient object detection. To achieve this, most existing methods based on 'skip' architecture mainly focus on how to integrate hierarchical features of Convolutional Neural Networks (CNNs). They simply apply concatenation or element-wise operation to incorporate high-level semantic cues and low-level detailed information. However, this can degrade the quality of predictions because cluttered and noisy information can also be passed through. To address this problem, we proposes a global Recurrent Localization Network (RLN) which exploits contextual information by the weighted response map in order to localize salient objects more accurately. % and emphasize more on useful ones. Particularly, a recurrent module is employed to progressively refine the inner structure of the CNN over multiple time steps. Moreover, to effectively recover object boundaries, we propose a local Boundary Refinement Network (BRN) to adaptively learn the local contextual information for each spatial position. The learned propagation coefficients can be used to optimally capture relations between each pixel and its neighbors. Experiments on five challenging datasets show that our approach performs favorably against all existing methods in terms of the popular evaluation metrics.","中文标题":"全局检测，局部优化：一种新颖的显著性检测方法","摘要翻译":"有效整合上下文信息对于显著物体检测至关重要。为了实现这一点，大多数基于'跳跃'架构的现有方法主要关注如何整合卷积神经网络（CNNs）的层次特征。它们简单地应用连接或元素级操作来融合高级语义线索和低级详细信息。然而，这可能会降低预测的质量，因为杂乱和嘈杂的信息也可能被传递。为了解决这个问题，我们提出了一个全局循环定位网络（RLN），它通过加权响应图利用上下文信息，以更准确地定位显著物体。特别是，采用了一个循环模块来逐步优化CNN的内部结构，经过多个时间步骤。此外，为了有效恢复物体边界，我们提出了一个局部边界优化网络（BRN），以自适应地学习每个空间位置的局部上下文信息。学习到的传播系数可以用来最佳地捕捉每个像素与其邻居之间的关系。在五个具有挑战性的数据集上的实验表明，我们的方法在流行的评估指标方面优于所有现有方法。","领域":"显著性检测/卷积神经网络/上下文信息整合","问题":"现有方法在整合上下文信息时，可能会传递杂乱和嘈杂的信息，从而降低显著物体检测的质量。","动机":"为了提高显著物体检测的准确性，需要一种能够更有效整合上下文信息的方法。","方法":"提出了一个全局循环定位网络（RLN）和一个局部边界优化网络（BRN），分别用于利用加权响应图定位显著物体和自适应学习局部上下文信息以恢复物体边界。","关键词":["显著性检测","卷积神经网络","上下文信息整合","循环定位网络","边界优化网络"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据。","跳跃架构":"一种网络架构，通过在不同层次之间建立直接连接来整合多层次特征。","加权响应图":"一种技术，用于根据重要性对特征图进行加权，以突出显示关键信息。","循环模块":"一种网络组件，通过多次迭代逐步优化网络内部结构。","传播系数":"用于描述像素与其邻居之间关系的参数，有助于捕捉局部上下文信息。"}},{"order":323,"title":"Beyond the Pixel-Wise Loss for Topology-Aware Delineation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mosinska_Beyond_the_Pixel-Wise_CVPR_2018_paper.html","abstract":"Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological importance of prediction errors. Instead, we propose a new loss term that is aware of the higher-order topological features of the linear structures. We also introduce a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step while keeping the number of parameters and the complexity of the model constant.  When combined with the standard pixel-wise loss, both our new loss term and iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained only with the binary cross-entropy. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images.","中文标题":"超越像素级损失：拓扑感知的轮廓描绘","摘要翻译":"曲线结构的描绘是计算机视觉中的一个重要问题，具有多种实际应用。随着深度学习的兴起，许多当前的自动描绘方法都集中在寻找更强大的深度架构上，但继续使用习惯的像素级损失，如二元交叉熵。在本文中，我们声称仅使用像素级损失不适合这个问题，因为它们无法反映预测错误的拓扑重要性。相反，我们提出了一个新的损失项，它能够意识到线性结构的高阶拓扑特征。我们还引入了一个细化管道，该管道在之前的描绘上迭代应用相同的模型，以在每一步细化预测，同时保持参数数量和模型复杂性不变。当与标准的像素级损失结合使用时，我们的新损失项和迭代细化都提高了预测描绘的质量，在某些情况下，与仅使用二元交叉熵训练的相同分类器相比，准确率几乎翻倍。我们展示了我们的方法在从显微镜到航空图像的广泛数据上优于最先进的方法。","领域":"曲线结构描绘/深度学习/拓扑分析","问题":"如何提高曲线结构自动描绘的准确性和质量","动机":"现有的自动描绘方法主要依赖于像素级损失，如二元交叉熵，这些损失无法充分反映预测错误的拓扑重要性，从而限制了描绘的准确性。","方法":"提出了一种新的损失项，该损失项能够考虑到线性结构的高阶拓扑特征，并引入了一个迭代细化管道，以在保持模型参数和复杂性不变的情况下，逐步提高预测的准确性。","关键词":["曲线结构描绘","拓扑分析","深度学习"],"涉及的技术概念":"像素级损失（如二元交叉熵）在描绘曲线结构时存在局限性，因为它们无法反映预测错误的拓扑重要性。本文提出的新损失项和迭代细化管道能够提高描绘的准确性，特别是在考虑线性结构的拓扑特征方面。"},{"order":324,"title":"KIPPI: KInetic Polygonal Partitioning of Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bauchet_KIPPI_KInetic_Polygonal_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bauchet_KIPPI_KInetic_Polygonal_CVPR_2018_paper.html","abstract":"Recent works showed that floating polygons can be an interesting alternative to traditional superpixels, especially for analyzing scenes with strong geometric signatures, as man-made environments. Existing algorithms produce homogeneously-sized polygons that fail to capture thin geometric structures and over-partition large uniform areas. We propose a kinetic approach that brings more flexibility on polygon shape and size. The key idea consists in progressively extending pre-detected line-segments until they meet each other. Our experiments demonstrate that output partitions both contain less polygons and better capture geometric structures than those delivered by existing methods. We also show the applicative potential of the method when used as preprocessing in object contouring.","中文标题":"KIPPI: 图像的动力学多边形分割","摘要翻译":"最近的研究表明，浮动多边形可以成为传统超像素的一个有趣替代品，特别是在分析具有强烈几何特征的场景时，如人造环境。现有的算法生成的均匀大小的多边形无法捕捉到细小的几何结构，并且对大面积的均匀区域进行了过度分割。我们提出了一种动力学方法，该方法在多边形的形状和大小上提供了更大的灵活性。关键思想是逐步扩展预先检测到的线段，直到它们相互接触。我们的实验表明，与现有方法相比，输出的分割既包含更少的多边形，又能更好地捕捉几何结构。我们还展示了该方法在对象轮廓提取中作为预处理的潜在应用价值。","领域":"图像分割/几何处理/场景分析","问题":"现有算法生成的均匀大小多边形无法有效捕捉细小的几何结构，并对大面积均匀区域进行过度分割。","动机":"为了提高对具有强烈几何特征场景的分析能力，特别是人造环境中的场景。","方法":"提出了一种动力学方法，通过逐步扩展预先检测到的线段，直到它们相互接触，从而在多边形的形状和大小上提供更大的灵活性。","关键词":["图像分割","几何处理","场景分析"],"涉及的技术概念":"浮动多边形、超像素、几何结构、动力学方法、线段扩展、对象轮廓提取"},{"order":325,"title":"Image Blind Denoising With Generative Adversarial Network Based Noise Modeling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Image_Blind_Denoising_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Image_Blind_Denoising_CVPR_2018_paper.html","abstract":"In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.","中文标题":"基于生成对抗网络噪声建模的图像盲去噪","摘要翻译":"在本文中，我们考虑了一个典型的图像盲去噪问题，即从噪声图像中去除未知噪声。众所周知，基于判别学习的方法，如DnCNN，可以实现最先进的去噪效果，但由于缺乏配对的训练数据，它们不适用于此问题。为了解决这一障碍，我们提出了一个新颖的两步框架。首先，训练一个生成对抗网络（GAN）来估计输入噪声图像的噪声分布并生成噪声样本。其次，利用从第一步采样的噪声块构建一个配对的训练数据集，该数据集随后用于训练一个深度卷积神经网络（CNN）进行去噪。大量的实验已经完成，以证明我们的方法在图像盲去噪中的优越性。","领域":"图像去噪/生成对抗网络/卷积神经网络","问题":"图像盲去噪问题，即从噪声图像中去除未知噪声","动机":"由于缺乏配对的训练数据，现有的基于判别学习的方法不适用于图像盲去噪问题","方法":"提出一个两步框架，首先使用生成对抗网络（GAN）估计噪声分布并生成噪声样本，然后利用这些噪声样本构建配对的训练数据集，用于训练深度卷积神经网络（CNN）进行去噪","关键词":["图像去噪","生成对抗网络","卷积神经网络"],"涉及的技术概念":"生成对抗网络（GAN）用于估计噪声分布和生成噪声样本，深度卷积神经网络（CNN）用于图像去噪"},{"order":326,"title":"Multi-Scale Weighted Nuclear Norm Image Restoration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yair_Multi-Scale_Weighted_Nuclear_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yair_Multi-Scale_Weighted_Nuclear_CVPR_2018_paper.html","abstract":"A prominent property of natural images is that groups of similar patches within them tend to lie on low-dimensional subspaces. This property has been previously used for image denoising, with particularly notable success via weighted nuclear norm minimization (WNNM). In this paper, we extend the WNNM method into a general image restoration algorithm, capable of handling arbitrary degradations (e.g. blur, missing pixels, etc.). Our approach is based on a novel regularization term which simultaneously penalizes for high weighted nuclear norm values of all the patch groups in the image. Our regularizer is isolated from the data-term, thus enabling convenient treatment of arbitrary degradations. Furthermore, it exploits the fractal property of natural images, by accounting for patch similarities also across different scales of the image. We propose a variable splitting method for solving the resulting optimization problem. This leads to an algorithm that is quite different from \`plug-and-play' techniques, which solve image-restoration problems using a sequence of denoising steps. As we verify through extensive experiments, our algorithm achieves state of the art results in deblurring and inpainting, outperforming even the recent deep net based methods.","中文标题":"多尺度加权核范数图像恢复","摘要翻译":"自然图像的一个显著特性是，其中相似的图像块组往往位于低维子空间上。这一特性先前已被用于图像去噪，特别是通过加权核范数最小化（WNNM）取得了显著成功。在本文中，我们将WNNM方法扩展为一种通用的图像恢复算法，能够处理任意退化（例如模糊、缺失像素等）。我们的方法基于一种新的正则化项，该正则化项同时惩罚图像中所有图像块组的高加权核范数值。我们的正则化器与数据项分离，从而便于处理任意退化。此外，它通过考虑图像不同尺度上的图像块相似性，利用了自然图像的分形特性。我们提出了一种变量分裂方法来解决由此产生的优化问题。这导致了一种与“即插即用”技术截然不同的算法，后者通过一系列去噪步骤解决图像恢复问题。正如我们通过大量实验所验证的那样，我们的算法在去模糊和修复方面达到了最先进的结果，甚至超越了最近基于深度网络的方法。","领域":"图像恢复/去噪/图像修复","问题":"处理图像中的任意退化问题，如模糊和缺失像素","动机":"利用自然图像中相似图像块组位于低维子空间的特性，扩展加权核范数最小化方法以处理更广泛的图像恢复问题","方法":"提出一种新的正则化项，同时惩罚图像中所有图像块组的高加权核范数值，并利用变量分裂方法解决优化问题","关键词":["加权核范数最小化","图像恢复","去噪","图像修复","正则化","变量分裂"],"涉及的技术概念":"加权核范数最小化（WNNM）是一种用于图像去噪的技术，通过最小化图像块组的加权核范数来实现。本文扩展了WNNM方法，提出了一种新的正则化项，用于处理图像恢复中的任意退化问题。此外，本文还利用了自然图像的分形特性，通过考虑不同尺度上的图像块相似性来改进图像恢复效果。变量分裂方法是一种优化技术，用于解决复杂的优化问题，本文采用此方法来解决图像恢复问题。"},{"order":327,"title":"MoNet: Moments Embedding Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gou_MoNet_Moments_Embedding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gou_MoNet_Moments_Embedding_CVPR_2018_paper.html","abstract":"Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve  performance in  multiple  vision tasks. Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion. However, a serious drawback of this family of pooling layers is their dimensionality explosion. Approximate pooling methods with compact properties have been explored towards resolving this weakness. Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize  unstable higher order information.  However, combining  compact pooling with matrix normalization and other order information has not been explored until now. In this paper, we unify bilinear pooling  and the global Gaussian embedding layers through the empirical moment matrix. In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods. Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet. Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with  encoded features with 96% less dimensions.","中文标题":"MoNet: 矩嵌入网络","摘要翻译":"最近，双线性池化被提出作为一种特征编码层，可以用于深度网络的卷积层之后，以提高在多个视觉任务中的性能。与传统的全局平均池化或全连接层不同，双线性池化以平移不变的方式收集二阶信息。然而，这类池化层的一个严重缺点是它们的维度爆炸。为了解决这一弱点，已经探索了具有紧凑特性的近似池化方法。此外，最近的结果表明，通过添加一阶信息并应用矩阵归一化来规范不稳定的高阶信息，可以实现显著的性能提升。然而，直到现在，将紧凑池化与矩阵归一化和其他阶信息结合起来还没有被探索。在本文中，我们通过经验矩矩阵统一了双线性池化和全局高斯嵌入层。此外，我们提出了一种新的子矩阵平方根层，它可以直接用于归一化卷积层的输出，并通过现成的紧凑池化方法缓解维度问题。我们在三个广泛使用的细粒度分类数据集上的实验表明，我们提出的架构MoNet可以实现与最先进的G2DeNet相似或更好的性能。此外，当与紧凑池化技术结合时，MoNet在特征维度减少96%的情况下，获得了与编码特征相当的性能。","领域":"细粒度分类/特征编码/池化技术","问题":"解决双线性池化层维度爆炸的问题","动机":"提高在多个视觉任务中的性能，同时解决双线性池化层维度爆炸的问题","方法":"通过经验矩矩阵统一双线性池化和全局高斯嵌入层，并提出新的子矩阵平方根层用于归一化卷积层的输出","关键词":["双线性池化","紧凑池化","矩阵归一化"],"涉及的技术概念":"双线性池化是一种特征编码方法，用于收集二阶信息；紧凑池化方法旨在减少特征维度；矩阵归一化用于规范不稳定的高阶信息；子矩阵平方根层是一种新的技术，用于直接归一化卷积层的输出。"},{"order":328,"title":"Active Fixation Control to Predict Saccade Sequences","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wloka_Active_Fixation_Control_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wloka_Active_Fixation_Control_CVPR_2018_paper.html","abstract":"Visual attention is a field with a considerable history, with eye movement control and prediction forming an important subfield. Fixation modeling in the past decades has been largely dominated computationally by a number of highly influential bottom-up saliency models, such as the Itti-Koch-Niebur model. The accuracy of such models has dramatically increased recently due to deep learning. However, on static images the emphasis of these models has largely been based on non-ordered prediction of fixations through a saliency map. Very few implemented models can generate temporally ordered human-like sequences of saccades beyond an initial fixation point. Towards addressing these shortcomings we present STAR-FC, a novel multi-saccade generator based on the integration of central high-level and object-based saliency and peripheral lower-level feature-based saliency. We have evaluated our model using the CAT2000 database, successfully predicting human patterns of fixation with equivalent accuracy and quality compared to what can be achieved by using one human sequence to predict another.","中文标题":"主动注视控制以预测扫视序列","摘要翻译":"视觉注意是一个有着相当历史的领域，其中眼动控制和预测构成了一个重要的子领域。过去几十年中，注视建模在计算上主要由一些极具影响力的自下而上的显著性模型主导，如Itti-Koch-Niebur模型。由于深度学习，这些模型的准确性最近显著提高。然而，在静态图像上，这些模型的重点主要是通过显著性图进行非顺序的注视预测。很少有实现的模型能够生成超越初始注视点的时间顺序的人类扫视序列。为了解决这些不足，我们提出了STAR-FC，这是一种基于中央高级和基于对象的显著性与外围低级基于特征的显著性整合的新型多扫视生成器。我们使用CAT2000数据库评估了我们的模型，成功地预测了人类的注视模式，其准确性和质量与使用一个人类序列预测另一个序列所能达到的水平相当。","领域":"视觉注意/眼动控制/显著性建模","问题":"如何生成时间顺序的人类扫视序列","动机":"解决现有模型在静态图像上主要进行非顺序注视预测的不足","方法":"提出了一种基于中央高级和基于对象的显著性与外围低级基于特征的显著性整合的新型多扫视生成器STAR-FC","关键词":["视觉注意","眼动控制","显著性建模","扫视序列","深度学习"],"涉及的技术概念":"显著性模型（如Itti-Koch-Niebur模型）、深度学习、多扫视生成器（STAR-FC）、CAT2000数据库"},{"order":329,"title":"Densely Connected Pyramid Dehazing Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Densely_Connected_Pyramid_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Densely_Connected_Pyramid_CVPR_2018_paper.html","abstract":"We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Network (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end-to-end learning is achieved by directly embedding the atmospheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense network that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi-level pyramid pooling module for estimating the transmission map. This network is optimized using a newly introduced edge-preserving loss function. To further incor- We propose a new end-to-end single image dehazing method, called Densely Connected Pyramid Dehazing Net- work (DCPDN), which can jointly learn the transmission map, atmospheric light and dehazing all together. The end- to-end learning is achieved by directly embedding the atmo- spheric scattering model into the network, thereby ensuring that the proposed method strictly follows the physics-driven scattering model for dehazing. Inspired by the dense net- work that can maximize the information flow along features from different levels, we propose a new edge-preserving densely connected encoder-decoder structure with multi- level pyramid pooling module for estimating the transmis- sion map. This network is optimized using a newly in- troduced edge-preserving loss function. To further incor- porate the mutual structural information between the esti- mated transmission map and the dehazed result, we pro- pose a joint-discriminator based on generative adversar- ial network framework to decide whether the correspond- ing dehazed image and the estimated transmission map are real or fake. An ablation study is conducted to demon- strate the effectiveness of each module evaluated at both estimated transmission map and dehazed result. Exten- sive experiments demonstrate that the proposed method achieves significant improvements over the state-of-the- art methods. Code and dataset is made available at: https://github.com/hezhangsprinter/DCPDN","中文标题":"密集连接金字塔去雾网络","摘要翻译":"我们提出了一种新的端到端单图像去雾方法，称为密集连接金字塔去雾网络（DCPDN），该方法可以联合学习传输图、大气光和去雾。端到端学习是通过直接将大气散射模型嵌入网络实现的，从而确保所提出的方法严格遵循物理驱动的散射模型进行去雾。受到密集网络可以最大化不同层次特征信息流的启发，我们提出了一种新的边缘保持密集连接编码器-解码器结构，带有多层次金字塔池化模块，用于估计传输图。该网络使用新引入的边缘保持损失函数进行优化。为了进一步整合估计的传输图和去雾结果之间的相互结构信息，我们提出了基于生成对抗网络框架的联合判别器，以判断相应的去雾图像和估计的传输图是真实的还是伪造的。进行了消融研究，以展示每个模块在估计传输图和去雾结果上的有效性。大量实验表明，所提出的方法在现有技术方法上取得了显著的改进。代码和数据集可在以下网址获取：https://github.com/hezhangsprinter/DCPDN","领域":"图像去雾/大气散射模型/生成对抗网络","问题":"单图像去雾","动机":"提高去雾效果，严格遵循物理驱动的散射模型","方法":"提出了一种新的端到端单图像去雾方法，称为密集连接金字塔去雾网络（DCPDN），通过直接将大气散射模型嵌入网络，使用边缘保持密集连接编码器-解码器结构和多层次金字塔池化模块估计传输图，并引入边缘保持损失函数进行优化，以及基于生成对抗网络框架的联合判别器整合传输图和去雾结果之间的相互结构信息","关键词":["图像去雾","大气散射模型","生成对抗网络","边缘保持","密集连接","金字塔池化"],"涉及的技术概念":{"密集连接金字塔去雾网络（DCPDN）":"一种新的端到端单图像去雾方法，能够联合学习传输图、大气光和去雾","大气散射模型":"用于描述光在大气中传播时受到散射影响的物理模型","边缘保持密集连接编码器-解码器结构":"一种网络结构，旨在保持图像边缘的同时，通过密集连接最大化不同层次特征的信息流","多层次金字塔池化模块":"用于估计传输图的模块，通过多层次池化操作捕捉不同尺度的特征","边缘保持损失函数":"一种新引入的损失函数，用于优化网络，以保持图像的边缘信息","生成对抗网络框架的联合判别器":"基于生成对抗网络的框架，用于判断去雾图像和估计的传输图的真实性，整合两者之间的相互结构信息"}},{"order":330,"title":"Universal Denoising Networks : A Novel CNN Architecture for Image Denoising","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lefkimmiatis_Universal_Denoising_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lefkimmiatis_Universal_Denoising_Networks_CVPR_2018_paper.html","abstract":"We design a novel network architecture for learning discriminative image models that are employed to efficiently tackle the problem of grayscale and color image denoising. Based on the proposed architecture, we introduce two different variants. The first network involves convolutional layers as a core component, while the second one relies instead on non-local filtering layers and thus it is able to exploit the inherent non-local self-similarity property of natural images. As opposed to most of the existing deep network approaches, which require the training of a specific model for each considered noise level, the proposed models are able to handle a wide range of noise levels using a single set of learned parameters, while they are very robust when the noise degrading the latent image does not match the statistics of the noise used during training. The latter argument is supported by results that we report on publicly available images corrupted by unknown noise and which we compare against solutions obtained by competing methods.  At the same time the introduced networks achieve excellent results under additive white Gaussian noise (AWGN), which are comparable to those of the current state-of-the-art network, while they depend on a more shallow architecture with the number of trained parameters being one order of magnitude smaller. These properties make the proposed networks ideal candidates to serve as sub-solvers on restoration methods that deal with general inverse imaging problems such as deblurring, demosaicking, superresolution, etc.","中文标题":"通用去噪网络：一种用于图像去噪的新型CNN架构","摘要翻译":"我们设计了一种新颖的网络架构，用于学习区分性图像模型，这些模型被用来有效地解决灰度和彩色图像去噪的问题。基于所提出的架构，我们引入了两种不同的变体。第一种网络以卷积层为核心组件，而第二种则依赖于非局部滤波层，因此能够利用自然图像固有的非局部自相似性特性。与大多数现有的深度网络方法不同，这些方法需要对每个考虑的噪声水平训练一个特定的模型，而所提出的模型能够使用一组学习到的参数处理广泛的噪声水平，同时当降噪潜在图像的噪声与训练期间使用的噪声统计不匹配时，它们非常稳健。后一个论点得到了我们在公开可用的被未知噪声破坏的图像上报告的结果的支持，并且我们将这些结果与通过竞争方法获得的解决方案进行了比较。同时，引入的网络在加性白高斯噪声（AWGN）下取得了优异的结果，这些结果与当前最先进的网络相当，同时它们依赖于更浅的架构，训练参数的数量少了一个数量级。这些特性使得所提出的网络成为处理一般逆成像问题（如去模糊、去马赛克、超分辨率等）的恢复方法的理想子解算器。","领域":"图像去噪/卷积神经网络/非局部滤波","问题":"解决灰度和彩色图像去噪的问题","动机":"设计一种能够处理广泛噪声水平且对噪声统计不匹配具有鲁棒性的网络架构","方法":"提出两种网络变体，一种基于卷积层，另一种基于非局部滤波层，利用自然图像的非局部自相似性特性","关键词":["图像去噪","卷积神经网络","非局部滤波"],"涉及的技术概念":"卷积层、非局部滤波层、加性白高斯噪声（AWGN）、去模糊、去马赛克、超分辨率"},{"order":331,"title":"Learning Convolutional Networks for Content-Weighted Image Compression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Convolutional_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Convolutional_Networks_CVPR_2018_paper.html","abstract":"Lossy image compression is generally formulated as a joint rate-distortion optimization problem to learn encoder, quantizer, and decoder. Due to the  non-differentiable quantizer and discrete entropy estimation, it is very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that: (i) the bit rate of the different parts of the image is adapted to local content, and (ii) the content-aware bit rate is allocated under the guidance of a content-weighted importance map. The sum of the importance map can thus serve as a continuous alternative of discrete entropy estimation to control compression rate. The binarizer is adopted to quantize the output of encoder and a proxy function is introduced for approximating binary operation in backward propagation to make it differentiable. The encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner. And a convolutional entropy encoder is further presented for lossless compression of importance map and binary codes. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.","中文标题":"学习卷积网络用于内容加权图像压缩","摘要翻译":"有损图像压缩通常被表述为一个联合率失真优化问题，以学习编码器、量化器和解码器。由于不可微分的量化器和离散的熵估计，开发一个基于卷积网络（CNN）的图像压缩系统非常具有挑战性。在本文中，受到图像中局部信息内容空间变化的启发，我们建议：（i）图像不同部分的比特率应适应局部内容，（ii）内容感知的比特率应在内容加权重要性图的指导下分配。因此，重要性图的总和可以作为离散熵估计的连续替代来控制压缩率。采用二值化器来量化编码器的输出，并引入代理函数来近似反向传播中的二进制操作，使其可微分。编码器、解码器、二值化器和重要性图可以以端到端的方式联合优化。并且进一步提出了卷积熵编码器，用于重要性图和二进制代码的无损压缩。在低比特率图像压缩中，实验表明，我们的系统在结构相似性（SSIM）指数上显著优于JPEG和JPEG 2000，并且可以产生边缘锐利、纹理丰富、伪影更少的视觉结果。","领域":"图像压缩/卷积网络/熵编码","问题":"开发一个基于卷积网络（CNN）的图像压缩系统，解决由于不可微分的量化器和离散的熵估计带来的挑战。","动机":"受到图像中局部信息内容空间变化的启发，提出适应局部内容的比特率分配和内容加权重要性图指导下的比特率分配。","方法":"采用二值化器量化编码器输出，引入代理函数近似反向传播中的二进制操作，使系统可微分；编码器、解码器、二值化器和重要性图以端到端方式联合优化；提出卷积熵编码器用于重要性图和二进制代码的无损压缩。","关键词":["图像压缩","卷积网络","熵编码","二值化器","重要性图"],"涉及的技术概念":"卷积网络（CNN）用于图像压缩，通过内容加权重要性图控制压缩率，采用二值化器进行量化，引入代理函数使系统可微分，卷积熵编码器用于无损压缩。"},{"order":332,"title":"Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jo_Deep_Video_Super-Resolution_CVPR_2018_paper.html","abstract":"Video super-resolution (VSR) has become even more important recently to provide high resolution (HR) contents for ultra high definition displays. While many deep learning based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and compensation. We introduce a fundamentally different framework for VSR in this paper. We propose a novel end-to-end deep neural network that generates dynamic upsampling filters and a residual image, which are computed depending on the local spatio-temporal neighborhood of each pixel to avoid explicit motion compensation. With our approach, an HR image is reconstructed directly from the input image using the dynamic upsampling filters, and the fine details are added through the computed residual. Our network with the help of a new data augmentation technique can generate much sharper HR videos with temporal consistency, compared with the previous methods. We also provide analysis of our network through extensive experiments to show how the network deals with motions implicitly.","中文标题":"使用动态上采样滤波器无需显式运动补偿的深度视频超分辨率网络","摘要翻译":"视频超分辨率（VSR）最近变得尤为重要，以提供超高清晰度显示器所需的高分辨率（HR）内容。虽然已经提出了许多基于深度学习的VSR方法，但大多数方法严重依赖于运动估计和补偿的准确性。本文介绍了一种根本不同的VSR框架。我们提出了一种新颖的端到端深度神经网络，该网络生成动态上采样滤波器和残差图像，这些是根据每个像素的局部时空邻域计算的，以避免显式运动补偿。通过我们的方法，使用动态上采样滤波器直接从输入图像重建HR图像，并通过计算的残差添加细节。我们的网络借助新的数据增强技术，可以生成比之前方法更清晰、时间一致的HR视频。我们还通过大量实验提供了对我们网络的分析，以展示网络如何隐式处理运动。","领域":"视频超分辨率/深度学习/图像重建","问题":"视频超分辨率中依赖显式运动估计和补偿的问题","动机":"提供一种不依赖显式运动补偿的视频超分辨率方法，以提高高分辨率视频的生成质量和时间一致性","方法":"提出一种端到端的深度神经网络，该网络生成动态上采样滤波器和残差图像，通过局部时空邻域计算避免显式运动补偿","关键词":["视频超分辨率","动态上采样滤波器","残差图像","时间一致性","数据增强"],"涉及的技术概念":"动态上采样滤波器：根据每个像素的局部时空邻域动态生成的滤波器，用于直接从输入图像重建高分辨率图像。残差图像：通过计算得到的图像，用于添加细节到重建的高分辨率图像中。数据增强技术：一种技术，用于通过增加训练数据的多样性来提高模型的泛化能力和性能。"},{"order":333,"title":"Erase or Fill? Deep Joint Recurrent Rain Removal and Reconstruction in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Erase_or_Fill_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Erase_or_Fill_CVPR_2018_paper.html","abstract":"In this paper, we address the problem of video rain removal by constructing deep recurrent convolutional networks. We visit the rain removal case by considering rain occlusion regions, i.e. light transmittance of rain streaks is low. Different from additive rain streaks, in such rain occlusion regions, the details of background images are completely lost. Therefore, we propose a hybrid rain model to depict both rain streaks and occlusions. With the wealth of temporal redundancy, we build a Joint Recurrent Rain Removal and Reconstruction Network (J4R-Net) that seamlessly integrates rain degradation classification, spatial texture appearances based rain removal and temporal coherence based background details reconstruction. The rain degradation classification provides a binary map that reveals whether a location degraded by linear additive streaks or occlusions. With this side information, the gate of the recurrent unit learns to make a trade-off between rain streak removal and background details reconstruction. Extensive experiments on a series of synthetic and real videos with rain streaks verify the superiority of the proposed method over previous state-of-the-art methods.","中文标题":"擦除还是填充？视频中深度联合循环去雨与重建","摘要翻译":"在本文中，我们通过构建深度循环卷积网络来解决视频去雨问题。我们通过考虑雨遮挡区域来探讨去雨情况，即雨条纹的光透射率较低。与加性雨条纹不同，在这种雨遮挡区域中，背景图像的细节完全丢失。因此，我们提出了一种混合雨模型来描述雨条纹和遮挡。利用丰富的时间冗余，我们构建了一个联合循环去雨与重建网络（J4R-Net），该网络无缝集成了雨退化分类、基于空间纹理外观的去雨和基于时间一致性的背景细节重建。雨退化分类提供了一个二元图，揭示了位置是否被线性加性条纹或遮挡退化。有了这些辅助信息，循环单元的门学会了在去除雨条纹和重建背景细节之间做出权衡。在一系列带有雨条纹的合成和真实视频上的大量实验验证了所提出方法相对于之前最先进方法的优越性。","领域":"视频去雨/背景重建/循环神经网络","问题":"视频中雨条纹的去除和背景细节的重建","动机":"在雨遮挡区域，背景图像的细节完全丢失，需要一种方法来同时去除雨条纹并重建背景细节。","方法":"提出了一种混合雨模型来描述雨条纹和遮挡，并构建了一个联合循环去雨与重建网络（J4R-Net），该网络集成了雨退化分类、基于空间纹理外观的去雨和基于时间一致性的背景细节重建。","关键词":["视频去雨","背景重建","循环神经网络"],"涉及的技术概念":"深度循环卷积网络、混合雨模型、雨退化分类、空间纹理外观、时间一致性、联合循环去雨与重建网络（J4R-Net）"},{"order":334,"title":"Flow Guided Recurrent Neural Encoder for Video Salient Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Flow_Guided_Recurrent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Flow_Guided_Recurrent_CVPR_2018_paper.html","abstract":"Image saliency detection has recently witnessed significant progress due to deep convolutional neural networks. However, extending state-of-the-art saliency detectors from image to video is challenging. The performance of salient object detection suffers from object or camera motion and the dramatic change of the appearance contrast in videos. In this paper, we present flow guided recurrent neural encoder(FGRNE), an accurate and end-to-end learning framework for video salient object detection. It works by enhancing the temporal coherence of the per-frame feature by exploiting both motion information in terms of optical flow and sequential feature evolution encoding in terms of LSTM networks. It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection. Intensive experimental results verify the effectiveness of each part of FGRNE and confirm that our proposed method significantly outperforms state-of-the-art methods on the public benchmarks of DAVIS and FBMS.","中文标题":"流引导的循环神经编码器用于视频显著目标检测","摘要翻译":"由于深度卷积神经网络的发展，图像显著性检测最近取得了显著进展。然而，将最先进的显著性检测器从图像扩展到视频是具有挑战性的。显著目标检测的性能受到对象或相机运动以及视频中外观对比度剧烈变化的影响。在本文中，我们提出了流引导的循环神经编码器(FGRNE)，这是一个准确且端到端的学习框架，用于视频显著目标检测。它通过利用光流形式的运动信息和LSTM网络形式的序列特征演化编码来增强每帧特征的时间一致性。它可以被视为一个通用框架，用于将任何基于FCN的静态显著性检测器扩展到视频显著目标检测。大量的实验结果验证了FGRNE每个部分的有效性，并证实了我们提出的方法在DAVIS和FBMS公共基准上显著优于最先进的方法。","领域":"视频显著性检测/光流分析/循环神经网络","问题":"视频显著目标检测中的对象或相机运动以及外观对比度剧烈变化问题","动机":"解决将图像显著性检测扩展到视频时遇到的挑战，提高视频显著目标检测的性能","方法":"提出流引导的循环神经编码器(FGRNE)，通过利用光流和LSTM网络增强每帧特征的时间一致性","关键词":["视频显著性检测","光流","循环神经网络","LSTM","FCN"],"涉及的技术概念":"光流是一种用于估计图像序列中物体运动的技术，LSTM（长短期记忆网络）是一种特殊的循环神经网络，能够学习长期依赖信息，FCN（全卷积网络）是一种用于图像分割的深度学习模型。"},{"order":335,"title":"Gated Fusion Network for Single Image Dehazing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Gated_Fusion_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Gated_Fusion_Network_CVPR_2018_paper.html","abstract":"In this paper, we propose an efficient algorithm to directly restore a clear image from a hazy input. The proposed algorithm hinges on an end-to-end trainable neural network that consists of an encoder and a decoder. The encoder is exploited to capture the context of the derived input images, while the decoder is employed to estimate the contribution of each input to the final dehazed result using the learned representations attributed to the encoder. The constructed network adopts a novel fusion-based strategy which derives three inputs from an original hazy image by applying White Balance (WB), Contrast Enhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence maps based on the appearance differences between these different inputs to blend the information of the derived inputs and preserve the regions with pleasant visibility. The final dehazed image is yielded by gating the important features of the derived inputs. To train the network, we introduce a multi-scale based approach so that the halo artifacts can be avoided. Extensive experimental results on both synthetic and real-world images demonstrate that the proposed algorithm performs favorably against the state-of-the-art algorithms.","中文标题":"用于单幅图像去雾的门控融合网络","摘要翻译":"本文提出了一种有效的算法，直接从有雾的输入图像中恢复出清晰的图像。该算法依赖于一个端到端可训练的神经网络，该网络由编码器和解码器组成。编码器用于捕捉派生输入图像的上下文，而解码器则利用编码器学习到的表示来估计每个输入对最终去雾结果的贡献。构建的网络采用了一种新颖的基于融合的策略，通过应用白平衡（WB）、对比度增强（CE）和伽马校正（GC）从原始有雾图像中派生出三个输入。我们基于这些不同输入之间的外观差异计算像素级置信度图，以混合派生输入的信息并保留具有良好可见性的区域。最终的去雾图像是通过门控派生输入的重要特征来生成的。为了训练网络，我们引入了一种基于多尺度的方法，以避免光晕伪影。在合成图像和真实世界图像上的大量实验结果表明，所提出的算法在性能上优于最先进的算法。","领域":"图像去雾/神经网络/图像增强","问题":"单幅图像去雾","动机":"直接从有雾的输入图像中恢复出清晰的图像，提高图像质量","方法":"采用端到端可训练的神经网络，包括编码器和解码器，以及基于融合的策略，通过白平衡、对比度增强和伽马校正从原始有雾图像中派生出三个输入，计算像素级置信度图，最终通过门控派生输入的重要特征生成去雾图像","关键词":["图像去雾","神经网络","图像增强","白平衡","对比度增强","伽马校正"],"涉及的技术概念":"端到端可训练的神经网络、编码器、解码器、白平衡（WB）、对比度增强（CE）、伽马校正（GC）、像素级置信度图、多尺度方法"},{"order":336,"title":"Learning a Single Convolutional Super-Resolution Network for Multiple Degradations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_a_Single_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_a_Single_CVPR_2018_paper.html","abstract":"Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to non-blindly deal with multiple degradations. To address these issues, we propose a general framework with dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.","中文标题":"学习一个适用于多种退化的单一卷积超分辨率网络","摘要翻译":"近年来，深度卷积神经网络（CNNs）在单图像超分辨率（SISR）领域取得了前所未有的成功。然而，现有的基于CNN的SISR方法大多假设低分辨率（LR）图像是从高分辨率（HR）图像通过双三次下采样得到的，因此当真实退化不符合这一假设时，不可避免地会导致性能不佳。此外，它们在学习单一模型以非盲目方式处理多种退化方面缺乏可扩展性。为了解决这些问题，我们提出了一个具有维度拉伸策略的通用框架，使得单一卷积超分辨率网络能够将SISR退化过程的两个关键因素，即模糊核和噪声水平，作为输入。因此，超分辨率器可以处理多种甚至空间变化的退化，这显著提高了实用性。在合成和真实LR图像上的大量实验结果表明，所提出的卷积超分辨率网络不仅能够在多种退化上产生有利的结果，而且计算效率高，为实际SISR应用提供了一个高效且可扩展的解决方案。","领域":"超分辨率/图像恢复/卷积神经网络","问题":"现有基于CNN的SISR方法在处理不符合双三次下采样假设的真实退化时性能不佳，且缺乏处理多种退化的可扩展性。","动机":"提高单图像超分辨率（SISR）方法在处理多种退化时的性能和可扩展性。","方法":"提出了一个具有维度拉伸策略的通用框架，使单一卷积超分辨率网络能够将模糊核和噪声水平作为输入，从而处理多种甚至空间变化的退化。","关键词":["超分辨率","图像恢复","卷积神经网络","维度拉伸策略","模糊核","噪声水平"],"涉及的技术概念":"深度卷积神经网络（CNNs）用于单图像超分辨率（SISR），维度拉伸策略用于处理多种退化，包括模糊核和噪声水平。"},{"order":337,"title":"Non-Blind Deblurring: Handling Kernel Uncertainty With CNNs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vasu_Non-Blind_Deblurring_Handling_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vasu_Non-Blind_Deblurring_Handling_CVPR_2018_paper.html","abstract":"Blind motion deblurring methods are primarily responsible for recovering an accurate estimate of the blur kernel. Non-blind deblurring (NBD) methods, on the other hand, attempt to faithfully restore the original image, given the blur estimate. However, NBD is quite susceptible to errors in blur kernel. In this work, we present a convolutional neural network-based approach to handle kernel uncertainty in non-blind motion deblurring. We provide multiple latent image estimates corresponding to different prior strengths obtained from a given blurry observation in order to exploit the complementarity of these inputs for improved learning. To generalize the performance to tackle arbitrary kernel noise, we train our network with a large number of real and synthetic noisy blur kernels. Our network mitigates the effects of kernel noise so as to yield detail-preserving and artifact-free restoration. Our quantitative and qualitative evaluations on benchmark datasets demonstrate that the proposed method delivers state-of-the-art results. To further underscore the benefits that can be achieved from our network, we propose two adaptations of our method to improve kernel estimates, and image deblurring quality, respectively.","中文标题":"非盲去模糊：使用卷积神经网络处理核不确定性","摘要翻译":"盲运动去模糊方法主要负责恢复模糊核的准确估计。而非盲去模糊（NBD）方法则试图在给定模糊估计的情况下，忠实地恢复原始图像。然而，NBD对模糊核中的错误非常敏感。在这项工作中，我们提出了一种基于卷积神经网络的方法来处理非盲运动去模糊中的核不确定性。我们提供了多个潜在图像估计，这些估计对应于从给定的模糊观察中获得的不同先验强度，以利用这些输入的互补性来改进学习。为了使性能泛化以应对任意核噪声，我们使用大量真实和合成的噪声模糊核来训练我们的网络。我们的网络减轻了核噪声的影响，从而产生细节保留和无伪影的恢复。我们在基准数据集上的定量和定性评估表明，所提出的方法提供了最先进的结果。为了进一步强调我们的网络可以实现的好处，我们提出了我们方法的两种适应，分别用于改进核估计和图像去模糊质量。","领域":"图像恢复/去模糊/卷积神经网络","问题":"非盲去模糊方法对模糊核中的错误非常敏感，导致恢复的图像质量下降。","动机":"提高非盲去模糊方法对模糊核不确定性的鲁棒性，以产生更高质量的图像恢复结果。","方法":"提出了一种基于卷积神经网络的方法，通过提供多个潜在图像估计来利用不同先验强度的互补性，并使用大量真实和合成的噪声模糊核进行训练，以减轻核噪声的影响。","关键词":["非盲去模糊","卷积神经网络","核不确定性","图像恢复"],"涉及的技术概念":"卷积神经网络（CNN）用于处理图像去模糊中的核不确定性，通过训练网络来减轻核噪声的影响，实现细节保留和无伪影的图像恢复。"},{"order":338,"title":"Boundary Flow: A Siamese Network That Predicts Boundary Motion Without Training on Motion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lei_Boundary_Flow_A_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lei_Boundary_Flow_A_CVPR_2018_paper.html","abstract":"Using deep learning, this paper addresses the problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize objects' spatial extents, and the flow indicates objects' motions and interactions. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet-based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPM-Flow but on the augmented input with our boundary-flow matches, and achieve significant performance improvement on the Sintel benchmark.","中文标题":"边界流：一种无需运动训练的连体网络预测边界运动","摘要翻译":"本文利用深度学习解决了视频中联合对象边界检测和边界运动估计的问题，我们称之为边界流估计。边界流是一个重要的中级视觉线索，因为边界表征了对象的空间范围，而流则指示了对象的运动和交互。然而，大多数先前关于运动估计的工作都集中在密集对象运动或可能不一定位于边界上的特征点上。对于边界流估计，我们指定了一种新的全卷积连体网络（FCSN），它联合估计两个连续帧中的对象级边界。通过相同的FCSN和一个新的、非常规的反卷积方法预测两帧中的边界对应关系。最后，通过基于边缘的滤波改进了边界流估计。评估在三个任务上进行：视频中的边界检测、边界流估计和光流估计。在边界检测方面，我们在VSB100基准数据集上实现了最先进的性能。在边界流估计方面，我们在Sintel训练数据集上展示了首次结果。对于光流估计，我们运行了最近的方法CPM-Flow，但在增加了我们的边界流匹配的输入上，并在Sintel基准上实现了显著的性能改进。","领域":"视频分析/运动估计/边界检测","问题":"视频中联合对象边界检测和边界运动估计","动机":"边界流作为中级视觉线索，对理解对象的空间范围、运动和交互至关重要","方法":"使用全卷积连体网络（FCSN）联合估计两个连续帧中的对象级边界，并通过新的反卷积方法预测边界对应关系，最后通过基于边缘的滤波改进边界流估计","关键词":["边界流估计","全卷积连体网络","反卷积方法","边缘滤波"],"涉及的技术概念":"全卷积连体网络（FCSN）是一种深度学习模型，用于处理图像或视频中的特定任务，如边界检测和运动估计。反卷积方法是一种用于从低分辨率特征图中恢复高分辨率信息的技术。边缘滤波是一种图像处理技术，用于增强或提取图像中的边缘信息。"},{"order":339,"title":"Learning to See in the Dark","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Learning_to_See_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Learning_to_See_CVPR_2018_paper.html","abstract":"Imaging in low light is challenging due to low photon count and low SNR. Short-exposure images suffer from noise, while long exposure can lead to blurry images and is often impractical. A variety of denoising, deblurring, and enhancement techniques have been proposed, but their effectiveness is limited in extreme conditions, such as video-rate imaging at night. To support the development of learning-based pipelines for low-light image processing, we introduce a dataset of raw short-exposure low-light images, with corresponding long-exposure reference images. Using the presented dataset, we develop a pipeline for processing low-light images, based on end-to-end training of a fully-convolutional network. The network operates directly on raw sensor data and replaces much of the traditional image processing pipeline, which tends to perform poorly on such data. We report promising results on the new dataset, analyze factors that affect performance, and highlight opportunities for future work.","中文标题":"学习在黑暗中看见","摘要翻译":"在低光条件下成像具有挑战性，因为光子数量少且信噪比低。短曝光图像会受到噪声的影响，而长曝光则可能导致图像模糊，并且通常不切实际。已经提出了多种去噪、去模糊和增强技术，但它们在极端条件下的效果有限，例如夜间的视频速率成像。为了支持基于学习的低光图像处理流程的开发，我们引入了一个包含原始短曝光低光图像及其对应的长曝光参考图像的数据集。利用所提供的数据集，我们开发了一个基于端到端训练的全卷积网络的低光图像处理流程。该网络直接在原始传感器数据上操作，并取代了大部分传统的图像处理流程，这些流程在此类数据上往往表现不佳。我们在新数据集上报告了有希望的结果，分析了影响性能的因素，并强调了未来工作的机会。","领域":"低光成像/图像去噪/图像增强","问题":"解决在低光条件下成像的挑战，包括噪声和图像模糊问题","动机":"为了开发更有效的低光图像处理技术，特别是在极端条件下，如夜间的视频速率成像","方法":"引入一个包含原始短曝光低光图像及其对应的长曝光参考图像的数据集，并开发一个基于端到端训练的全卷积网络的低光图像处理流程","关键词":["低光成像","图像去噪","图像增强"],"涉及的技术概念":"全卷积网络、端到端训练、原始传感器数据处理、图像去噪、图像增强"},{"order":340,"title":"BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_BPGrad_Towards_Global_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_BPGrad_Towards_Global_CVPR_2018_paper.html","abstract":"Understanding the global optimality in deep learning (DL) has been attracting more and more attention recently. Conventional DL solvers, however, have not been developed intentionally to seek for such global optimality. In this paper we propose a novel approximation algorithm, {em BPGrad}, towards optimizing deep models globally via branch and pruning. Our BPGrad is based on the assumption of Lipschitz continuity in DL, and as a result it can adaptively determine the step size for current gradient given the history of previous updates, wherein theoretically no smaller steps can achieve the global optimality. We prove that by repeating such branch-and-pruning procedure, we can locate the global optimality within finite iterations. Empirically an efficient solver based on BPGrad for DL is proposed as well, and it outperforms conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the tasks of object recognition, detection, and segmentation.","中文标题":"BPGrad：通过分支与剪枝实现深度学习的全局最优","摘要翻译":"理解深度学习（DL）中的全局最优性最近吸引了越来越多的关注。然而，传统的DL求解器并未有意开发以寻求这种全局最优性。在本文中，我们提出了一种新颖的近似算法，{em BPGrad}，通过分支与剪枝来全局优化深度模型。我们的BPGrad基于DL中的Lipschitz连续性假设，因此它可以根据先前更新的历史自适应地确定当前梯度的步长，理论上没有更小的步长可以实现全局最优性。我们证明，通过重复这种分支与剪枝过程，我们可以在有限次迭代内定位全局最优性。经验上，基于BPGrad的DL高效求解器也被提出，并且在对象识别、检测和分割任务中优于传统的DL求解器，如Adagrad、Adadelta、RMSProp和Adam。","领域":"优化算法/深度学习理论/计算机视觉","问题":"深度学习模型在训练过程中难以达到全局最优解","动机":"探索并实现深度学习模型训练过程中的全局最优性，以提高模型性能","方法":"提出了一种基于Lipschitz连续性假设的BPGrad算法，通过分支与剪枝过程自适应确定梯度步长，以定位全局最优解","关键词":["全局最优性","Lipschitz连续性","分支与剪枝","梯度步长","对象识别","检测","分割"],"涉及的技术概念":"Lipschitz连续性是指在数学中，一个函数的变化率有一个上限，这个概念在深度学习中用于保证算法的稳定性和收敛性。BPGrad算法利用这一性质，通过历史更新信息自适应地调整梯度步长，以寻找全局最优解。"},{"order":341,"title":"Perturbative Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Juefei-Xu_Perturbative_Neural_Networks_CVPR_2018_paper.html","abstract":"Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs. We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.","中文标题":"扰动神经网络","摘要翻译":"卷积神经网络在计算机视觉系统中得到了广泛的应用，涵盖了多种视觉识别任务。这一进展很大程度上得益于卷积神经网络架构和学习算法的进步，尽管卷积层的基本前提保持不变。在本文中，我们旨在重新审视作为最先进视觉识别模型核心的卷积层。我们引入了一个非常简单但有效的模块，称为扰动层，作为卷积层的替代。扰动层摒弃了传统意义上的卷积，而是将其响应计算为非线性激活的加性噪声扰动输入的加权线性组合。我们通过分析和实证证明，这种扰动层可以有效地替代标准的卷积层。实证上，使用扰动层（称为扰动神经网络，PNNs）代替卷积层的深度神经网络，在一系列视觉数据集（MNIST、CIFAR-10、PASCAL VOC和ImageNet）上表现与标准CNN相当，且参数更少。","领域":"神经网络架构/视觉识别/深度学习","问题":"传统卷积层在视觉识别任务中的局限性","动机":"探索卷积层的替代方案，以提高视觉识别模型的效率和性能","方法":"引入扰动层作为卷积层的替代，通过非线性激活的加性噪声扰动输入的加权线性组合计算响应","关键词":["扰动层","卷积神经网络","视觉识别"],"涉及的技术概念":"扰动层是一种新的神经网络层，它通过非线性激活的加性噪声扰动输入的加权线性组合来计算响应，而不是传统的卷积操作。这种方法旨在减少参数数量，同时保持或提高模型在视觉识别任务上的性能。"},{"order":342,"title":"Unsupervised Correlation Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hoshen_Unsupervised_Correlation_Analysis_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hoshen_Unsupervised_Correlation_Analysis_CVPR_2018_paper.html","abstract":"Linking between two data sources is a basic building block in numerous computer vision problems. In this paper, we set to answer a fundamental cognitive question: are prior correspondences necessary for linking between different domains?     One of the most popular methods for linking between domains is Canonical Correlation Analysis (CCA). All current CCA algorithms require correspondences between the views. We introduce a new method Unsupervised Correlation Analysis (UCA), which requires no prior correspondences between the two domains. The correlation maximization term in CCA is replaced by a combination of a reconstruction term (similar to autoencoders), full cycle loss, orthogonality and multiple domain confusion terms. Due to lack of supervision, the optimization leads to multiple alternative solutions with similar scores and we therefore introduce a consensus-based mechanism that is often able to recover the desired solution. Remarkably, this suffices in order to link remote domains such as text and images. We also present results on well accepted CCA benchmarks, showing that performance far exceeds other unsupervised baselines, and approaches supervised performance in some cases.","中文标题":"无监督相关分析","摘要翻译":"在两个数据源之间建立联系是众多计算机视觉问题中的基本构建块。在本文中，我们旨在回答一个基本的认知问题：在不同领域之间建立联系是否需要先前的对应关系？连接领域之间最流行的方法之一是典型相关分析（CCA）。所有当前的CCA算法都需要视图之间的对应关系。我们引入了一种新方法——无监督相关分析（UCA），它不需要两个领域之间的先前对应关系。CCA中的相关最大化项被替换为重建项（类似于自动编码器）、完整周期损失、正交性和多领域混淆项的组合。由于缺乏监督，优化导致多个具有相似分数的替代解决方案，因此我们引入了一种基于共识的机制，通常能够恢复所需的解决方案。值得注意的是，这足以连接文本和图像等远程领域。我们还在广泛接受的CCA基准上展示了结果，表明性能远远超过其他无监督基线，并在某些情况下接近监督性能。","领域":"数据关联/领域适应/无监督学习","问题":"在不同领域之间建立联系是否需要先前的对应关系","动机":"探索在没有先前对应关系的情况下，如何有效地在不同领域之间建立联系","方法":"引入无监督相关分析（UCA），通过重建项、完整周期损失、正交性和多领域混淆项的组合来替代CCA中的相关最大化项，并采用基于共识的机制来恢复所需的解决方案","关键词":["无监督学习","领域适应","数据关联"],"涉及的技术概念":"典型相关分析（CCA）是一种用于分析两组变量之间关系的统计方法。无监督相关分析（UCA）是一种新方法，它不需要两个领域之间的先前对应关系，通过重建项、完整周期损失、正交性和多领域混淆项的组合来替代CCA中的相关最大化项。基于共识的机制用于在缺乏监督的情况下恢复所需的解决方案。"},{"order":343,"title":"A Biresolution Spectral Framework for Product Quantization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mukherjee_A_Biresolution_Spectral_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mukherjee_A_Biresolution_Spectral_CVPR_2018_paper.html","abstract":"Product quantization (PQ) (and its variants) has been effec- tively used to encode high-dimensional data into compact codes for many problems in vision. In principle, PQ decomposes the given data into a number of lower-dimensional subspaces where the quantization proceeds independently for each subspace. While the original PQ approach does not explicitly optimize for these subspaces, later proposals have argued that the performance tends to benefit significantly if such subspaces are chosen in an optimal manner. Despite such consensus, existing approaches in the literature diverge in terms of which specific properties of these subspaces are desirable and how one should proceed to solve/optimize them. Nonetheless, despite the empirical support, there is less clarity regarding the theoretical properties that underlie these experimental benefits for quantization problems in general. In this paper, we study the quantization problem in the setting where subspaces are orthogonal and show that this problem is intricately related to a specific type of spectral decomposition of the data. This insight not only opens the door to a rich body of work in spectral analysis, but also leads to distinct computational benefits. Our resultant biresolution spectral formulation captures both the subspace projection error as well as the quantization error within the same framework. After a reformulation, the core steps of our algorithm involve a simple eigen decomposition step, which can be solved efficiently. We show that our method performs very favorably against a number of state of the art methods on standard data sets.","中文标题":"双分辨率光谱框架用于产品量化","摘要翻译":"产品量化（PQ）及其变体已被有效地用于将高维数据编码为紧凑代码，以解决视觉中的许多问题。原则上，PQ将给定数据分解为多个低维子空间，其中量化在每个子空间中独立进行。虽然原始的PQ方法没有明确优化这些子空间，但后来的提议认为，如果以最优方式选择这些子空间，性能往往会显著受益。尽管存在这样的共识，但现有文献中的方法在哪些特定属性是这些子空间所期望的以及如何解决/优化它们方面存在分歧。然而，尽管有实证支持，但对于量化问题的一般实验效益背后的理论属性，清晰度较低。在本文中，我们研究了子空间正交的设置下的量化问题，并表明这个问题与数据的特定类型的光谱分解密切相关。这一见解不仅打开了光谱分析中丰富的工作的大门，而且还带来了独特的计算效益。我们所得的双分辨率光谱公式在同一框架内捕捉了子空间投影误差和量化误差。经过重新表述后，我们算法的核心步骤涉及一个简单的特征分解步骤，可以高效解决。我们展示了我们的方法在标准数据集上对许多最先进的方法表现非常有利。","领域":"高维数据编码/量化技术/光谱分析","问题":"如何优化产品量化中的子空间选择以提高性能","动机":"探索量化问题中子空间选择的理论基础，以提高量化性能","方法":"提出了一种双分辨率光谱框架，该框架同时考虑子空间投影误差和量化误差，并通过特征分解步骤高效解决问题","关键词":["产品量化","子空间优化","光谱分解"],"涉及的技术概念":"产品量化（PQ）是一种将高维数据编码为紧凑代码的技术，通过将数据分解为多个低维子空间并在每个子空间内独立进行量化。光谱分解是一种数学技术，用于将数据分解为不同的频率成分，以便于分析和处理。特征分解是线性代数中的一个过程，用于将矩阵分解为由其特征向量和特征值组成的集合，这对于理解数据的结构和动态非常重要。"},{"order":344,"title":"Domain Adaptive Faster R-CNN for Object Detection in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.html","abstract":"Object detection typically assumes that training and test data are drawn from an identical distribution, which, however, does not always hold in practice. Such a distribution mismatch will lead to a significant performance drop. In this work, we aim to improve the cross-domain robustness of object detection. We tackle the domain shift on two levels: 1) the image-level shift, such as image style, illumination, etc, and 2) the instance-level shift, such as object appearance, size, etc. We build our approach based on the recent state-of-the-art Faster R-CNN model, and design two domain adaptation components, on image level and instance level, to reduce the domain discrepancy. The two domain adaptation components are based on H-divergence theory, and are implemented by learning a domain classifier in adversarial training manner. The domain classifiers on different levels are further reinforced with a consistency regularization to learn a domain-invariant region proposal network (RPN) in the Faster R-CNN model. We evaluate our newly proposed approach using multiple datasets including Cityscapes, KITTI, SIM10K, etc. The results demonstrate the effectiveness of our proposed approach for robust object detection in various domain shift scenarios.","中文标题":"面向野外目标检测的域自适应Faster R-CNN","摘要翻译":"目标检测通常假设训练和测试数据来自相同的分布，然而，这在实际中并不总是成立。这种分布不匹配会导致性能显著下降。在这项工作中，我们旨在提高目标检测的跨域鲁棒性。我们在两个层面上解决域偏移问题：1）图像级偏移，如图像风格、光照等，和2）实例级偏移，如物体外观、大小等。我们的方法基于最新的Faster R-CNN模型，并设计了两个域适应组件，分别在图像级和实例级，以减少域差异。这两个域适应组件基于H-散度理论，并通过以对抗训练方式学习域分类器来实现。不同级别的域分类器通过一致性正则化进一步加强，以在Faster R-CNN模型中学习域不变区域提议网络（RPN）。我们使用包括Cityscapes、KITTI、SIM10K等多个数据集评估了我们新提出的方法。结果表明，我们提出的方法在各种域偏移场景下对鲁棒目标检测的有效性。","领域":"目标检测/域适应/对抗训练","问题":"解决目标检测中训练和测试数据分布不一致导致的性能下降问题","动机":"提高目标检测模型在不同域之间的鲁棒性，以应对实际应用中常见的域偏移问题","方法":"基于Faster R-CNN模型，设计图像级和实例级两个域适应组件，通过对抗训练学习域分类器，并利用一致性正则化加强域不变区域提议网络的学习","关键词":["目标检测","域适应","对抗训练","Faster R-CNN","H-散度理论","一致性正则化"],"涉及的技术概念":{"Faster R-CNN":"一种先进的目标检测模型，能够同时进行区域提议和目标分类","H-散度理论":"一种衡量两个概率分布差异的理论，用于域适应中减少源域和目标域之间的差异","对抗训练":"一种训练方法，通过让模型在对抗样本上训练来提高其鲁棒性","一致性正则化":"一种正则化技术，用于确保模型在不同条件下（如不同域）的输出一致性"}},{"order":345,"title":"Low-Shot Learning With Large-Scale Diffusion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Douze_Low-Shot_Learning_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Douze_Low-Shot_Learning_With_CVPR_2018_paper.html","abstract":"This paper considers the problem of inferring image labels from images when only a few annotated examples are available at training time. This setup is often referred to as low-shot learning, where a standard approach is to re-train the  last few layers of a convolutional neural network learned on separate classes for which training examples are abundant.  We consider a semi-supervised setting based on a large collection of images to support label propagation.  This is  possible by leveraging the recent advances on large-scale similarity graph construction.   We show that despite its conceptual simplicity, scaling label propagation up to hundred millions of images leads to state of the art accuracy in the low-shot learning regime.","中文标题":"大规模扩散下的少样本学习","摘要翻译":"本文探讨了在训练时只有少量标注样本可用的情况下，从图像中推断图像标签的问题。这种设置通常被称为少样本学习，其中一种标准方法是重新训练在训练样本丰富的单独类别上学习的卷积神经网络的最后几层。我们考虑了一种基于大量图像集合的半监督设置，以支持标签传播。这可以通过利用最近在大规模相似图构建方面的进展来实现。我们展示了尽管其概念简单，但将标签传播扩展到数亿张图像，在少样本学习领域达到了最先进的准确性。","领域":"少样本学习/图像标签推断/半监督学习","问题":"在训练时只有少量标注样本可用的情况下，从图像中推断图像标签","动机":"解决在训练样本有限的情况下，如何有效地推断图像标签的问题","方法":"利用大规模图像集合进行半监督学习，通过大规模相似图构建技术实现标签传播","关键词":["少样本学习","图像标签推断","半监督学习","大规模相似图构建","标签传播"],"涉及的技术概念":"卷积神经网络（CNN）的重新训练、大规模相似图构建、标签传播技术"},{"order":346,"title":"Joint Pose and Expression Modeling for Facial Expression Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Joint_Pose_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Joint_Pose_and_CVPR_2018_paper.html","abstract":"Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.","中文标题":"联合姿态和表情建模用于面部表情识别","摘要翻译":"面部表情识别（FER）由于在任意姿态下的不同表情而成为一个具有挑战性的任务。大多数传统方法要么对非正面面部图像进行正面化处理，要么为每个姿态学习单独的分类器。与现有方法不同，在本文中，我们提出了一种端到端的深度学习模型，通过联合利用不同的姿态和表情来进行同时的面部图像合成和姿态不变的面部表情识别。所提出的模型基于生成对抗网络（GAN），并具有几个优点。首先，生成器的编码器-解码器结构可以学习面部图像的生成性和判别性身份表示。其次，身份表示通过表情和姿态代码明确地从表情和姿态变化中解耦。第三，我们的模型可以自动生成在任意姿态下具有不同表情的面部图像，以扩大和丰富FER的训练集。在受控和野外数据集上的定量和定性评估表明，所提出的算法在性能上优于最先进的方法。","领域":"面部表情识别/生成对抗网络/图像合成","问题":"解决在任意姿态下进行面部表情识别的挑战","动机":"传统方法在处理非正面面部图像时存在局限性，需要一种能够同时处理姿态和表情变化的方法","方法":"提出了一种基于生成对抗网络（GAN）的端到端深度学习模型，通过联合利用不同的姿态和表情来进行面部图像合成和姿态不变的面部表情识别","关键词":["面部表情识别","生成对抗网络","图像合成"],"涉及的技术概念":"生成对抗网络（GAN）是一种深度学习模型，由生成器和判别器组成，用于生成新的数据样本。在本研究中，GAN被用来生成具有不同表情和姿态的面部图像，以增强面部表情识别的训练集。编码器-解码器结构是GAN中生成器的一部分，用于学习面部图像的生成性和判别性身份表示。表情和姿态代码用于从身份表示中解耦表情和姿态变化，从而实现姿态不变的面部表情识别。"},{"order":347,"title":"Lightweight Probabilistic Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gast_Lightweight_Probabilistic_Deep_CVPR_2018_paper.html","abstract":"Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.","中文标题":"轻量级概率深度网络","摘要翻译":"尽管神经网络的概率处理方法已有很长的历史，但它们在实践中并未得到广泛应用。对于简单网络，采样方法通常已经太慢。计算机视觉中典型CNN架构的输入大小和深度只会加剧这个问题。因此，尽管不确定性可能提供关于预测可靠性和网络内部工作的重要信息，但在实践中，神经网络中的不确定性在很大程度上被忽视了。在本文中，我们介绍了两种使概率深度网络的监督学习变得实用的轻量级方法：首先，我们提出了用于分类和回归的概率输出层，这些层只需要对现有网络进行最小的更改。其次，我们采用假设密度过滤，并展示了激活不确定性可以通过整个网络以实用的方式传播，同样只需要进行少量更改。这两种概率网络都保留了确定性对应物的预测能力，但产生的不确定性与它们的预测引起的经验误差有很好的相关性。此外，对对抗样本的鲁棒性显著提高。","领域":"概率深度学习/对抗样本防御/网络不确定性估计","问题":"如何在保持预测能力的同时，有效地在深度网络中引入和处理不确定性","动机":"尽管神经网络中的不确定性可能提供关于预测可靠性和网络内部工作的重要信息，但在实践中，这一领域的研究和应用相对较少，主要因为现有方法在计算上过于昂贵，尤其是在处理大规模输入和深层网络时。","方法":"提出了两种轻量级方法：一是引入概率输出层，仅需对现有网络进行最小更改；二是采用假设密度过滤，展示激活不确定性可以通过整个网络以实用的方式传播。","关键词":["概率深度学习","对抗样本防御","网络不确定性估计"],"涉及的技术概念":"概率输出层：一种用于分类和回归的层，能够输出预测的概率分布。假设密度过滤：一种技术，用于在网络中传播激活不确定性，以估计预测的不确定性。"},{"order":348,"title":"Adversarially Learned One-Class Classifier for Novelty Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.html","abstract":"Novelty detection is the process of identifying the observation(s) that differ in some respect from the training observations (the target class). In reality, the novelty class is often absent during training, poorly sampled or not well defined. Therefore, one-class classifiers can efficiently model such problems. However, due to the unavailability of data from the novelty class, training an end-to-end deep network is a cumbersome task. In this paper, inspired by the success of generative adversarial networks for training deep models in unsupervised and semi-supervised settings, we propose an end-to-end architecture for one-class classification. Our architecture is composed of two deep networks, each of which trained by competing with each other while collaborating to understand the underlying concept in the target class, and then classify the testing samples. One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples. The proposed framework applies to different related applications of anomaly and outlier detection in images and videos. The results on MNIST and Caltech-256 image datasets, along with the challenging UCSD Ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods.","中文标题":"对抗性学习的一类分类器用于新颖性检测","摘要翻译":"新颖性检测是识别与训练观察（目标类）在某些方面不同的观察结果的过程。实际上，新颖类在训练期间通常不存在、采样不足或定义不明确。因此，一类分类器可以有效地模拟这类问题。然而，由于新颖类数据的不可用性，训练一个端到端的深度网络是一项繁琐的任务。在本文中，受到生成对抗网络在无监督和半监督设置下训练深度模型成功的启发，我们提出了一种用于一类分类的端到端架构。我们的架构由两个深度网络组成，每个网络通过相互竞争来训练，同时协作理解目标类中的基本概念，然后对测试样本进行分类。一个网络作为新颖性检测器工作，而另一个通过增强内点样本和扭曲离群点来支持它。直觉是增强的内点和扭曲的离群点的可分离性比决定原始样本要好得多。所提出的框架适用于图像和视频中异常和离群点检测的不同相关应用。在MNIST和Caltech-256图像数据集上的结果，以及用于视频异常检测的挑战性UCSD Ped2数据集上的结果表明，我们提出的方法有效地学习了目标类，并且优于基线和最先进的方法。","领域":"新颖性检测/异常检测/离群点检测","问题":"在缺乏新颖类数据的情况下，如何有效地训练一个端到端的深度网络进行一类分类","动机":"由于新颖类在训练期间通常不存在、采样不足或定义不明确，需要一种有效的方法来模拟这类问题","方法":"提出了一种由两个深度网络组成的端到端架构，一个网络作为新颖性检测器，另一个通过增强内点样本和扭曲离群点来支持它","关键词":["新颖性检测","异常检测","离群点检测"],"涉及的技术概念":"生成对抗网络（GANs）用于在无监督和半监督设置下训练深度模型，一类分类器用于模拟缺乏新颖类数据的问题，端到端架构由两个深度网络组成，一个用于检测新颖性，另一个通过增强内点样本和扭曲离群点来支持检测过程。"},{"order":349,"title":"Defense Against Universal Adversarial Perturbations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Akhtar_Defense_Against_Universal_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Akhtar_Defense_Against_Universal_CVPR_2018_paper.html","abstract":"Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to \`any' image  can fool a state-of-the-art network classifier to change its prediction about the  image label. These \`Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as \`pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the  network classifiers against  unseen adversarial perturbations in the real-world scenarios  with up to 96.4% success rate. The PRN also generalizes well in the sense that training for one targeted  network defends another network with a comparable success rate.","中文标题":"防御通用对抗性扰动","摘要翻译":"深度学习的最新进展表明，存在与图像无关的几乎不可察觉的扰动，当这些扰动应用于\`任何\`图像时，可以欺骗最先进的网络分类器改变其对图像标签的预测。这些\`通用对抗性扰动\`对深度学习在实践中的成功构成了严重威胁。我们提出了第一个专门用于有效防御网络免受此类扰动的框架。我们的方法学习了一个扰动校正网络（PRN）作为目标模型的\`预输入\`层，这样目标模型无需修改。PRN是从真实和合成的与图像无关的扰动中学习的，其中还提出了一种计算后者的有效方法。一个扰动检测器在PRN的输入输出差异的离散余弦变换上单独训练。查询图像首先通过PRN，并由检测器验证。如果检测到扰动，则使用PRN的输出而不是实际图像进行标签预测。严格的评估表明，我们的框架可以在现实世界场景中以高达96.4%的成功率防御网络分类器免受未见过的对抗性扰动的影响。PRN在训练一个目标网络防御另一个网络方面也表现出良好的泛化能力，成功率相当。","领域":"对抗性防御/网络安全/图像分类","问题":"防御深度学习模型免受通用对抗性扰动的影响","动机":"通用对抗性扰动对深度学习模型的实际应用构成了严重威胁，需要有效的防御机制","方法":"提出了一个扰动校正网络（PRN）作为预输入层，以及一个基于离散余弦变换的扰动检测器，用于检测和校正对抗性扰动","关键词":["对抗性防御","网络安全","图像分类"],"涉及的技术概念":"通用对抗性扰动、扰动校正网络（PRN）、离散余弦变换、对抗性防御机制"},{"order":350,"title":"Disentangling Factors of Variation by Mixing Them","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Disentangling_Factors_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Disentangling_Factors_of_CVPR_2018_paper.html","abstract":"We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets.","中文标题":"通过混合解耦变异因素","摘要翻译":"我们提出了一种学习图像表示的方法，该表示由解耦的变异因素组成，而不利用任何手动标记或数据领域知识。变异因素对应于可以在图像集合中一致识别的图像属性，例如物体的姿态或颜色。我们的解耦表示由特征块的连接组成，每个块代表一个变异因素。它支持诸如通过简单地混合和分离特征块将属性从一个图像转移到另一个图像的应用，以及通过考虑用户指定的特征块子集基于一个或多个属性进行分类或检索。我们使用具有两个新颖训练目标的自动编码器架构学习我们的表示，无需任何标记或数据领域知识：首先，我们提出了一个不变性目标，以鼓励每个属性的编码和每个块的解码分别对其他属性和块的变化保持不变；其次，我们包括一个分类目标，确保每个块对应于表示图像中一致可识别的属性，从而避免某些块被完全忽略的退化特征映射。我们在MNIST、Sprites和CelebA数据集上展示了我们方法的有效性。","领域":"图像表示学习/自动编码器/特征解耦","问题":"如何在没有手动标记或数据领域知识的情况下学习由解耦的变异因素组成的图像表示","动机":"为了支持图像属性的转移和基于属性的分类或检索，需要一种能够解耦图像中变异因素的方法","方法":"使用具有不变性和分类目标的自动编码器架构，通过混合和分离特征块来学习解耦的图像表示","关键词":["图像表示","特征解耦","自动编码器","不变性目标","分类目标"],"涉及的技术概念":{"解耦的变异因素":"指图像中可以一致识别的属性，如物体的姿态或颜色","自动编码器":"一种神经网络架构，用于学习数据的有效表示","不变性目标":"训练目标之一，旨在使编码和解码过程对特定变化保持不变","分类目标":"训练目标之一，确保每个特征块对应于图像中一致可识别的属性"}},{"order":351,"title":"Deformable GANs for Pose-Based Human Image Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Siarohin_Deformable_GANs_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Siarohin_Deformable_GANs_for_CVPR_2018_paper.html","abstract":"In this paper we address the problem of generating person images   conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in  the  generator  of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test  our approach using  photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two  benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.","中文标题":"基于姿态的人体图像生成的可变形GANs","摘要翻译":"在本文中，我们解决了基于给定姿态生成人物图像的问题。具体来说，给定一个人物的图像和一个目标姿态，我们合成了该人物在新姿态下的新图像。为了处理由姿态差异引起的像素到像素的错位，我们在生成对抗网络的生成器中引入了可变形跳跃连接。此外，提出了一种最近邻损失，而不是常见的L1和L2损失，以便将生成图像的细节与目标图像匹配。我们使用不同姿态的人物照片测试了我们的方法，并将我们的方法与这一领域的先前工作进行了比较，在两个基准测试中展示了最先进的结果。我们的方法可以应用于更广泛的可变形对象生成领域，前提是可以通过关键点检测器提取关节对象的姿态。","领域":"人体姿态估计/图像生成/生成对抗网络","问题":"基于给定姿态生成人物图像","动机":"解决由姿态差异引起的像素到像素的错位问题，提高生成图像的质量","方法":"在生成对抗网络的生成器中引入可变形跳跃连接，并提出最近邻损失以匹配生成图像的细节与目标图像","关键词":["可变形跳跃连接","最近邻损失","姿态估计","图像生成","生成对抗网络"],"涉及的技术概念":"可变形跳跃连接是一种在生成对抗网络中处理像素错位问题的技术，通过允许网络在生成过程中动态调整连接方式来适应不同的姿态变化。最近邻损失是一种用于图像生成的损失函数，它通过比较生成图像和目标图像的最近邻像素来优化生成细节，而不是使用传统的L1或L2损失。"},{"order":352,"title":"Hierarchical Recurrent Attention Networks for Structured Online Maps","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Homayounfar_Hierarchical_Recurrent_Attention_CVPR_2018_paper.html","abstract":"In this paper, we tackle the problem of online road network extraction from sparse 3D point clouds. Our method is inspired by how an annotator builds a lane graph, by first identifying how many lanes there are and then drawing each one in turn.  We develop a hierarchical recurrent network that attends to initial regions of a lane boundary and traces them out completely by outputting a structured polyline. We also propose a novel differentiable loss function that measures the deviation of the edges of the ground truth polylines and their predictions. This is more suitable than distances on vertices, as  there exists many ways to draw equivalent polylines. We demonstrate the effectiveness of our method on a 90 km stretch of highway, and show that we can recover the right topology 92% of the time.","中文标题":"层次循环注意力网络用于结构化在线地图","摘要翻译":"在本文中，我们解决了从稀疏3D点云中在线提取道路网络的问题。我们的方法受到注释者如何构建车道图的启发，首先识别有多少条车道，然后依次绘制每条车道。我们开发了一个层次循环网络，该网络关注车道边界的初始区域，并通过输出结构化的折线完全追踪它们。我们还提出了一种新颖的可微分损失函数，该函数测量地面真实折线的边缘与其预测之间的偏差。这比顶点上的距离更合适，因为存在许多绘制等效折线的方法。我们在90公里的高速公路上展示了我们方法的有效性，并显示我们能够92%的时间恢复正确的拓扑结构。","领域":"自动驾驶/地理信息系统/三维重建","问题":"从稀疏3D点云中在线提取道路网络","动机":"受到注释者构建车道图的方式启发，旨在更有效地从稀疏3D点云中提取道路网络","方法":"开发了一个层次循环网络，关注车道边界的初始区域并通过输出结构化的折线完全追踪它们，并提出了一种新颖的可微分损失函数来测量地面真实折线的边缘与其预测之间的偏差","关键词":["自动驾驶","地理信息系统","三维重建"],"涉及的技术概念":"层次循环网络、可微分损失函数、结构化折线、3D点云、车道图"},{"order":353,"title":"Sliced Wasserstein Distance for Learning Gaussian Mixture Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kolouri_Sliced_Wasserstein_Distance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kolouri_Sliced_Wasserstein_Distance_CVPR_2018_paper.html","abstract":"Gaussian mixture models (GMM) are powerful parametric tools with many applications in machine learning and computer vision. Expectation maximization (EM) is the most popular algorithm for estimating the GMM parameters. However, EM  guarantees only convergence to a stationary point of the log-likelihood function, which could be arbitrarily worse than the optimal solution. Inspired by the relationship between the negative log-likelihood function and the Kullback-Leibler (KL) divergence, we propose an alternative formulation for estimating the GMM parameters using the sliced Wasserstein distance, which gives rise to a new algorithm. Specifically, we propose minimizing the sliced-Wasserstein distance between the mixture model and the data distribution with respect to the GMM parameters. In contrast to the KL-divergence, the energy landscape for the sliced-Wasserstein distance is more well-behaved and therefore more suitable for a stochastic gradient descent scheme to obtain the optimal GMM parameters. We show that our formulation results in parameter estimates that are more robust to random initializations and demonstrate that it can estimate high-dimensional data distributions more faithfully than the EM algorithm.","中文标题":"切片Wasserstein距离用于学习高斯混合模型","摘要翻译":"高斯混合模型（GMM）是机器学习与计算机视觉中应用广泛的有力参数工具。期望最大化（EM）算法是估计GMM参数最流行的算法。然而，EM仅保证收敛到对数似然函数的驻点，这可能比最优解差很多。受负对数似然函数与Kullback-Leibler（KL）散度之间关系的启发，我们提出了一种使用切片Wasserstein距离估计GMM参数的替代公式，从而产生了一种新算法。具体来说，我们提出最小化混合模型与数据分布之间的切片Wasserstein距离，相对于GMM参数。与KL散度相比，切片Wasserstein距离的能量景观更为良好，因此更适合于通过随机梯度下降方案获得最优GMM参数。我们展示了我们的公式得到的参数估计对随机初始化更为鲁棒，并证明它比EM算法能更忠实地估计高维数据分布。","领域":"概率模型/优化算法/高维数据分析","问题":"如何更有效地估计高斯混合模型的参数","动机":"期望最大化算法仅能保证收敛到对数似然函数的驻点，可能远离最优解，需要一种更鲁棒且能更准确估计高维数据分布的方法","方法":"提出使用切片Wasserstein距离作为替代公式，通过最小化混合模型与数据分布之间的切片Wasserstein距离来估计GMM参数，采用随机梯度下降方案优化","关键词":["高斯混合模型","切片Wasserstein距离","随机梯度下降","参数估计"],"涉及的技术概念":{"高斯混合模型":"一种概率模型，用于表示由多个高斯分布组成的混合分布","期望最大化算法":"一种迭代算法，用于寻找统计模型中参数的最大似然估计","Kullback-Leibler散度":"衡量两个概率分布之间差异的指标","切片Wasserstein距离":"一种衡量两个概率分布之间距离的度量，通过考虑所有一维投影来计算","随机梯度下降":"一种优化算法，用于最小化目标函数，通过迭代地调整参数"}},{"order":354,"title":"Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Aligning_Infinite-Dimensional_Covariance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Aligning_Infinite-Dimensional_Covariance_CVPR_2018_paper.html","abstract":"Domain shift, which occurs when there is a mismatch between the distributions of training (source) and testing (target) datasets, usually results in poor performance of the trained model on the target domain. Existing algorithms typically solve this issue by reducing the distribution discrepancy in the input spaces. However, for kernel-based learning machines, performance highly depends on the statistical properties of data in reproducing kernel Hilbert spaces (RKHS). Motivated by these considerations, we propose a novel strategy for matching distributions in RKHS, which is done by aligning the RKHS covariance matrices (descriptors) across domains. This strategy is a generalization of the correlation alignment problem in Euclidean spaces to (potentially) infinite-dimensional feature spaces. In this paper, we provide two alignment approaches, for both of which we obtain closed-form expressions via kernel matrices. Furthermore, our approaches are scalable to large datasets since they can naturally handle out-of-sample instances. We conduct extensive experiments (248 domain adaptation tasks) to evaluate our approaches. Experiment results show that our approaches outperform other state-of-the-art methods in both accuracy and computationally efficiency.","中文标题":"在再生核希尔伯特空间中对齐无限维协方差矩阵以实现领域适应","摘要翻译":"领域偏移，即训练（源）和测试（目标）数据集分布不匹配时发生的情况，通常会导致训练模型在目标领域上的性能不佳。现有算法通常通过减少输入空间中的分布差异来解决这一问题。然而，对于基于核的学习机器，性能高度依赖于再生核希尔伯特空间（RKHS）中数据的统计特性。基于这些考虑，我们提出了一种新的策略，用于在RKHS中匹配分布，这是通过跨领域对齐RKHS协方差矩阵（描述符）来实现的。这一策略是将欧几里得空间中的相关对齐问题推广到（可能）无限维特征空间。在本文中，我们提供了两种对齐方法，对于这两种方法，我们都通过核矩阵获得了闭式表达式。此外，我们的方法可以自然地处理样本外实例，因此可以扩展到大型数据集。我们进行了广泛的实验（248个领域适应任务）来评估我们的方法。实验结果表明，我们的方法在准确性和计算效率方面均优于其他最先进的方法。","领域":"领域适应/核方法/统计学习","问题":"解决领域适应中由于训练和测试数据集分布不匹配导致的模型性能下降问题","动机":"基于核的学习机器性能高度依赖于再生核希尔伯特空间（RKHS）中数据的统计特性，因此需要一种新的策略来在RKHS中匹配分布","方法":"提出了一种通过跨领域对齐RKHS协方差矩阵（描述符）来匹配分布的新策略，并提供了两种对齐方法，通过核矩阵获得闭式表达式，且方法可扩展到大型数据集","关键词":["领域适应","核方法","统计学习","协方差矩阵","再生核希尔伯特空间"],"涉及的技术概念":{"领域偏移":"训练（源）和测试（目标）数据集分布不匹配时发生的情况","再生核希尔伯特空间（RKHS）":"一种特殊的希尔伯特空间，用于核方法中，能够处理高维甚至无限维的特征空间","协方差矩阵":"描述数据集中变量之间线性关系的矩阵，用于统计分析和机器学习中","核矩阵":"在核方法中，用于表示数据点之间相似性的矩阵，是核函数计算的结果"}},{"order":355,"title":"CLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kozerawski_CLEAR_Cumulative_LEARning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kozerawski_CLEAR_Cumulative_LEARning_CVPR_2018_paper.html","abstract":"This work addresses the novel problem of one-shot one-class classification. The goal is to estimate a classification decision boundary for a novel class based on a single image example. Our method exploits transfer learning to model the transformation from a representation of the input, extracted by a Convolutional Neural Network, to a classification decision boundary. We use a deep neural network to learn this transformation from a large labelled dataset of images and their associated class decision boundaries generated from ImageNet, and then apply the learned decision boundary to classify subsequent query images. We tested our approach on several benchmark datasets and significantly outperformed the baseline methods.","中文标题":"CLEAR：一次性单类图像识别的累积学习","摘要翻译":"本工作解决了一次性单类分类的新问题。目标是根据单个图像示例估计新类的分类决策边界。我们的方法利用迁移学习来建模从卷积神经网络提取的输入表示到分类决策边界的转换。我们使用深度神经网络从ImageNet生成的大量标记图像及其相关类决策边界中学习这种转换，然后将学习到的决策边界应用于分类后续查询图像。我们在几个基准数据集上测试了我们的方法，并显著优于基线方法。","领域":"图像分类/迁移学习/卷积神经网络","问题":"一次性单类分类问题，即基于单个图像示例估计新类的分类决策边界","动机":"解决在仅有一个示例图像的情况下，如何为新类估计分类决策边界的问题","方法":"利用迁移学习，通过深度神经网络从大量标记图像及其相关类决策边界中学习转换，然后将学习到的决策边界应用于分类后续查询图像","关键词":["一次性学习","单类分类","迁移学习","卷积神经网络","决策边界"],"涉及的技术概念":"卷积神经网络（CNN）用于提取输入图像的表示；迁移学习用于从大量数据中学习转换；深度神经网络用于学习从图像表示到分类决策边界的转换；ImageNet数据集用于生成类决策边界。"},{"order":356,"title":"Local and Global Optimization Techniques in Graph-Based Clustering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ikami_Local_and_Global_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ikami_Local_and_Global_CVPR_2018_paper.html","abstract":"The goal of graph-based clustering is to divide a dataset into disjoint subsets with members similar to each other from an affinity (similarity) matrix between data. The most popular method of solving graph-based clustering is spectral clustering. However, spectral clustering has drawbacks. Spectral clustering can only be applied to macro-average-based cost functions, which tend to generate undesirable small clusters. This study first introduces a novel cost function based on micro-average. We propose a local optimization method, which is widely applicable to graph-based clustering cost functions. We also propose an initial-guess-free algorithm to avoid its initialization dependency. Moreover, we present two global optimization techniques. The experimental results exhibit significant clustering performances from our proposed methods, including 100% clustering accuracy in the COIL-20 dataset.","中文标题":"基于图的聚类中的局部和全局优化技术","摘要翻译":"基于图的聚类的目标是将数据集划分为不相交的子集，这些子集的成员之间根据数据之间的亲和力（相似性）矩阵相似。解决基于图的聚类最流行的方法是谱聚类。然而，谱聚类有其缺点。谱聚类只能应用于基于宏观平均的成本函数，这往往会产生不理想的小簇。本研究首先介绍了一种基于微观平均的新成本函数。我们提出了一种局部优化方法，该方法广泛适用于基于图的聚类成本函数。我们还提出了一种无需初始猜测的算法，以避免其初始化依赖性。此外，我们提出了两种全局优化技术。实验结果表明，我们提出的方法在聚类性能上表现出色，包括在COIL-20数据集上实现了100%的聚类准确率。","领域":"图聚类/优化技术/数据挖掘","问题":"解决谱聚类在基于宏观平均的成本函数上产生不理想小簇的问题","动机":"提高基于图的聚类的准确性和效率，避免初始化依赖性","方法":"提出基于微观平均的新成本函数，局部优化方法，无需初始猜测的算法，以及两种全局优化技术","关键词":["图聚类","优化技术","数据挖掘","谱聚类","成本函数"],"涉及的技术概念":"基于图的聚类是一种将数据集划分为不相交子集的技术，这些子集的成员之间根据数据之间的亲和力（相似性）矩阵相似。谱聚类是解决基于图的聚类问题的一种流行方法，但它只能应用于基于宏观平均的成本函数，这往往会产生不理想的小簇。本研究提出了一种基于微观平均的新成本函数，以及局部优化方法和无需初始猜测的算法，旨在提高聚类的准确性和效率。此外，还提出了两种全局优化技术，以进一步优化聚类结果。"},{"order":357,"title":"Multi-Task Learning by Maximizing Statistical Dependence","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mejjati_Multi-Task_Learning_by_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mejjati_Multi-Task_Learning_by_CVPR_2018_paper.html","abstract":"We present a new multi-task learning (MTL) approach that can be applied to multiple heterogeneous task estimators. Our motivation is that the best task estimator could change depending on the task itself. For example, we may have a deep neural network for the first task and a Gaussian process for the second task. Classical MTL approaches cannot handle this case, as they require the same model or even the same parameter types for all tasks. We tackle this by considering task-specific estimators as random variables. Then, the task relationships are discovered by measuring the statistical dependence between each pair of random variables. By doing so, our model is independent of the parametric nature of each task, and is even agnostic to the existence of such parametric formulation. We compare our algorithm with existing MTL approaches on challenging real world ranking and regression datasets, and show that our approach achieves comparable or better performance without knowing the parametric form.","中文标题":"通过最大化统计依赖性的多任务学习","摘要翻译":"我们提出了一种新的多任务学习（MTL）方法，该方法可以应用于多个异构任务估计器。我们的动机是，最佳任务估计器可能会根据任务本身而变化。例如，我们可能为第一个任务使用深度神经网络，为第二个任务使用高斯过程。传统的MTL方法无法处理这种情况，因为它们要求所有任务使用相同的模型甚至相同的参数类型。我们通过将任务特定的估计器视为随机变量来解决这个问题。然后，通过测量每对随机变量之间的统计依赖性来发现任务关系。通过这样做，我们的模型独立于每个任务的参数性质，甚至对这样的参数公式的存在是不可知的。我们在具有挑战性的现实世界排名和回归数据集上将我们的算法与现有的MTL方法进行比较，并表明我们的方法在不知道参数形式的情况下实现了可比或更好的性能。","领域":"多任务学习/统计学习/回归分析","问题":"处理多任务学习中异构任务估计器的问题","动机":"最佳任务估计器可能会根据任务本身而变化，传统MTL方法无法处理异构任务估计器","方法":"将任务特定的估计器视为随机变量，通过测量每对随机变量之间的统计依赖性来发现任务关系","关键词":["多任务学习","统计依赖性","异构任务估计器"],"涉及的技术概念":"多任务学习（MTL）是一种机器学习方法，旨在同时学习多个相关任务以提高学习效率和预测性能。统计依赖性是指两个随机变量之间存在某种统计上的关联。异构任务估计器指的是用于不同任务的估计器可能具有不同的模型或参数类型。"},{"order":358,"title":"Robust Classification With Convolutional Prototype Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Robust_Classification_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Robust_Classification_With_CVPR_2018_paper.html","abstract":"Convolutional neural networks (CNNs) have been widely used for image classification. Despite its high accuracies, CNN has been shown to be easily fooled by some adversarial examples, indicating that CNN is not robust enough for pattern classification. In this paper, we argue that the lack of robustness for CNN is caused by the softmax layer, which is a totally discriminative model and based on the assumption of closed world (i.e., with a fixed number of categories). To improve the robustness, we propose a novel learning framework called convolutional prototype learning (CPL). The advantage of using prototypes is that it can well handle the open world recognition problem and therefore improve the robustness. Under the framework of CPL, we design multiple classification criteria to train the network. Moreover, a prototype loss (PL) is proposed as a regularization to improve the intra-class compactness of the feature representation, which can be viewed as a generative model based on the Gaussian assumption of different classes. Experiments on several datasets demonstrate that CPL can achieve comparable or even better results than traditional CNN, and from the robustness perspective, CPL shows great advantages for both the rejection and incremental category learning tasks.","中文标题":"使用卷积原型学习的鲁棒分类","摘要翻译":"卷积神经网络（CNNs）已被广泛用于图像分类。尽管其准确率高，但CNN已被证明容易被一些对抗性示例所欺骗，这表明CNN对于模式分类来说不够鲁棒。在本文中，我们认为CNN缺乏鲁棒性的原因是由于softmax层，这是一个完全判别模型，并基于封闭世界（即具有固定数量的类别）的假设。为了提高鲁棒性，我们提出了一种称为卷积原型学习（CPL）的新学习框架。使用原型的优势在于它能够很好地处理开放世界识别问题，从而提高鲁棒性。在CPL框架下，我们设计了多种分类标准来训练网络。此外，提出了一种原型损失（PL）作为正则化，以提高特征表示的类内紧凑性，这可以视为基于不同类别高斯假设的生成模型。在多个数据集上的实验表明，CPL可以实现与传统CNN相当甚至更好的结果，从鲁棒性的角度来看，CPL在拒绝和增量类别学习任务中显示出巨大的优势。","领域":"图像分类/对抗性示例/开放世界识别","问题":"提高卷积神经网络在图像分类中的鲁棒性","动机":"CNN在处理对抗性示例时表现出鲁棒性不足，主要原因是softmax层的设计基于封闭世界假设，限制了其在开放世界识别中的应用。","方法":"提出卷积原型学习（CPL）框架，通过设计多种分类标准和引入原型损失（PL）作为正则化，提高特征表示的类内紧凑性，从而增强模型的鲁棒性。","关键词":["卷积神经网络","对抗性示例","开放世界识别","原型学习","鲁棒性"],"涉及的技术概念":"卷积神经网络（CNNs）是一种深度学习模型，广泛用于图像分类任务。对抗性示例是指经过特殊设计的输入，能够欺骗模型产生错误的输出。开放世界识别是指模型能够识别训练数据中未出现的新类别。原型学习是一种通过定义类别原型来进行分类的方法，有助于处理开放世界识别问题。原型损失（PL）是一种正则化方法，用于提高特征表示的类内紧凑性，基于高斯假设的生成模型。"},{"order":359,"title":"Generative Modeling Using the Sliced Wasserstein Distance","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Deshpande_Generative_Modeling_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Deshpande_Generative_Modeling_Using_CVPR_2018_paper.html","abstract":"Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our ap- proach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good mea- sure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.","中文标题":"使用切片Wasserstein距离的生成建模","摘要翻译":"生成对抗网络（GANs）在从给定样本中建模分布方面非常成功，即使在高维情况下也是如此。然而，它们的公式也被认为是难以优化的，并且往往不稳定。虽然这对于早期的GAN公式尤其如此，但通过使用Wasserstein距离而不是Jenson-Shannon散度，已经有了显著的基于经验和理论基础的进展来提高稳定性。在这里，我们考虑了一种基于随机投影的生成建模的替代公式，其最简单的形式导致单一目标而不是鞍点公式。通过用判别器增强这种方法，我们提高了其准确性。我们发现，与改进的Wasserstein GAN相比，我们的方法显著更稳定。此外，与传统的GAN损失不同，我们方法中制定的损失是分布之间实际距离的良好度量，并且我们首次能够展示GAN训练的相同估计。","领域":"生成模型/优化算法/稳定性分析","问题":"生成对抗网络（GANs）在优化和稳定性方面的问题","动机":"提高生成对抗网络（GANs）的稳定性和优化效率","方法":"采用基于随机投影的生成建模替代公式，并通过增加判别器来提高准确性","关键词":["生成对抗网络","Wasserstein距离","随机投影","稳定性","优化"],"涉及的技术概念":"生成对抗网络（GANs）是一种用于生成模型的深度学习技术，通过对抗过程来估计数据分布。Wasserstein距离是一种衡量两个概率分布之间差异的度量，相比Jenson-Shannon散度，它在优化生成模型时提供了更好的稳定性和收敛性。随机投影是一种降维技术，用于在高维数据中捕捉重要特征，从而简化模型的复杂性。"},{"order":360,"title":"Learning Time/Memory-Efficient Deep Architectures With Budgeted Super Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Veniat_Learning_TimeMemory-Efficient_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Veniat_Learning_TimeMemory-Efficient_Deep_CVPR_2018_paper.html","abstract":"We propose to focus on the problem of discovering neural network architectures efficient in terms of both prediction quality and cost. For instance, our approach is able to solve the following tasks: learn a neural network able to predict well in less than 100 milliseconds or learn an efficient model that fits in a 50 Mb memory. Our contribution is a novel family of models called Budgeted Super Networks (BSN). They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost, while making no assumption on the nature of this cost. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost and a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and Convolutional Neural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost.","中文标题":"学习具有预算超级网络的时间和内存高效深度架构","摘要翻译":"我们提出关注于发现既在预测质量又在成本方面高效的神经网络架构的问题。例如，我们的方法能够解决以下任务：学习一个能够在不到100毫秒内做出良好预测的神经网络，或学习一个适合50 Mb内存的高效模型。我们的贡献是一个名为预算超级网络（BSN）的新模型家族。它们是通过应用梯度下降技术在一个预算学习目标函数上学习的，该函数集成了最大授权成本，同时不对这种成本的性质做出假设。我们展示了一系列关于计算机视觉问题的实验，并分析了我们的技术在处理三种不同成本方面的能力：计算成本、内存消耗成本和分布式计算成本。我们特别展示了我们的模型能够发现比ResNet和卷积神经网络织物架构在CIFAR-10和CIFAR-100上具有更好准确性的神经网络架构，且成本更低。","领域":"神经网络架构搜索/高效深度学习/资源受限学习","问题":"如何在保证预测质量的同时，发现时间和内存成本高效的神经网络架构","动机":"为了在资源受限的环境下实现高效的深度学习模型，需要开发能够在有限的时间和内存资源内做出高质量预测的神经网络架构","方法":"提出了一种名为预算超级网络（BSN）的新模型家族，通过应用梯度下降技术在一个预算学习目标函数上学习，该函数集成了最大授权成本","关键词":["神经网络架构搜索","高效深度学习","资源受限学习"],"涉及的技术概念":"预算超级网络（BSN）是一种新型的神经网络架构，旨在在保证预测质量的同时，优化时间和内存成本。通过梯度下降技术在预算学习目标函数上学习，该函数允许指定最大授权成本，从而在不假设成本性质的情况下，实现资源的高效利用。"},{"order":361,"title":"Cross-View Image Synthesis Using Conditional GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Regmi_Cross-View_Image_Synthesis_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Regmi_Cross-View_Image_Synthesis_CVPR_2018_paper.html","abstract":"Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called  Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64×64 and 256×256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.","中文标题":"使用条件GAN进行跨视角图像合成","摘要翻译":"学习生成自然场景一直是计算机视觉中的一个挑战性任务。当生成过程基于视角截然不同的图像时，这一任务变得更加艰巨。这主要是因为跨视角理解、对应和转换外观及语义信息并非易事。在本文中，我们尝试解决跨视角图像合成这一新问题，即从空中视角到街景视角的转换，反之亦然，使用条件生成对抗网络（cGAN）。提出了两种新架构，称为Crossview Fork（X-Fork）和Crossview Sequential（X-Seq），以生成分辨率为64×64和256×256像素的场景。X-Fork架构具有一个判别器和一个生成器。生成器在目标视角下同时生成图像及其语义分割。X-Seq架构利用两个cGAN。第一个生成目标图像，随后将其输入第二个cGAN以生成相应的语义分割图。第二个cGAN的反馈帮助第一个cGAN生成更清晰的图像。我们提出的两种架构都学会了生成自然图像及其语义分割图。所提出的方法表明，与仅考虑场景视觉外观的传统图像到图像转换方法相比，它们能够更好地捕捉和保持源视角和目标视角中物体的真实语义。广泛的定性和定量评估支持了我们框架的有效性，与两种最先进的方法相比，在生成视角截然不同的自然场景方面。","领域":"图像合成/语义分割/生成对抗网络","问题":"跨视角图像合成，特别是从空中视角到街景视角的转换及其反向转换","动机":"解决跨视角图像合成中的挑战，即理解、对应和转换不同视角下的外观及语义信息","方法":"提出了两种新架构Crossview Fork（X-Fork）和Crossview Sequential（X-Seq），使用条件生成对抗网络（cGAN）进行跨视角图像合成","关键词":["跨视角图像合成","条件生成对抗网络","语义分割"],"涉及的技术概念":"条件生成对抗网络（cGAN）是一种深度学习模型，用于生成数据，其生成过程基于某些条件输入。在本研究中，cGAN被用于跨视角图像合成，即根据一个视角的图像生成另一个视角的图像。语义分割是指将图像分割成多个区域，每个区域对应一个特定的语义类别，如道路、建筑物等。"},{"order":362,"title":"Sparse, Smart Contours to Represent and Edit Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dekel_Sparse_Smart_Contours_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dekel_Sparse_Smart_Contours_CVPR_2018_paper.html","abstract":"We study the problem of reconstructing an image from information stored at contour locations. We show that high-quality reconstructions with high fidelity to the source image can be obtained from sparse input, e.g., comprising less than 6% of image pixels. This is a significant improvement over existing contour-based reconstruction methods that require much denser input to capture subtle texture information and to ensure image quality. Our model, based on generative adversarial networks, synthesizes texture and details in regions where no input information is provided.  The semantic  knowledge  encoded  into  our  model  and  the  sparsity of the input allows to use contours as an intuitive interface for semantically-aware image manipulation: local edits in contour domain translate to long-range and coherent changes in pixel space. We can perform complex structural changes such as changing facial expression by simple edits of contours.  Our experiments demonstrate that humans as well as a face recognition system mostly cannot distinguish between our reconstructions and the source images.","中文标题":"稀疏、智能轮廓用于表示和编辑图像","摘要翻译":"我们研究了从轮廓位置存储的信息重建图像的问题。我们展示了可以从稀疏输入中获得高质量的重建，例如，包含少于6%的图像像素。这是对现有基于轮廓的重建方法的显著改进，现有方法需要更密集的输入来捕捉细微的纹理信息并确保图像质量。我们的模型基于生成对抗网络，在没有提供输入信息的区域合成纹理和细节。编码到我们模型中的语义知识和输入的稀疏性允许使用轮廓作为语义感知图像操作的直观界面：轮廓域中的局部编辑转化为像素空间中的长距离和连贯变化。我们可以通过简单的轮廓编辑执行复杂的结构变化，例如改变面部表情。我们的实验表明，人类以及面部识别系统大多无法区分我们的重建和源图像。","领域":"图像重建/生成对抗网络/语义图像编辑","问题":"如何从稀疏的轮廓信息中高质量地重建图像","动机":"改进现有基于轮廓的重建方法，减少所需输入密度，同时保持或提高图像质量","方法":"基于生成对抗网络的模型，利用编码的语义知识和输入稀疏性，在无输入信息的区域合成纹理和细节","关键词":["图像重建","生成对抗网络","语义图像编辑","稀疏轮廓","图像质量"],"涉及的技术概念":"生成对抗网络（GANs）用于图像重建和细节合成，语义知识编码用于实现语义感知的图像操作，稀疏轮廓作为输入信息用于高质量图像重建。"},{"order":363,"title":"Anticipating Traffic Accidents With Adaptive Loss and Large-Scale Incident DB","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Suzuki_Anticipating_Traffic_Accidents_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Suzuki_Anticipating_Traffic_Accidents_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel approach for traffic accident anticipation through (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a large-scale self-annotated incident database. The proposed AdaLEA allows us to gradually learn an earlier anticipation as training progresses. The loss function adaptively assigns penalty weights depending on how early the model can anticipate a traffic accident at each epoch. Additionally, a new Near-miss Incident DataBase (NIDB) that contains an enormous number of traffic near-miss incidents in which the four classes of cyclist, pedestrian, vehicle, and background class are labeled is discussed. The NIDB provides joint estimations of traffic incident anticipation and risk-factor categorization. In our experimental results, we found our proposal achieved the highest scores for anticipation (99.1% mean average precision (mAP) and 4.81 sec  anticipation of the average time-to-collision (ATTC), values which are +6.6% better and 2.36 sec faster than previous work) and joint estimation (62.1% (mAP) and 3.65 sec anticipation (ATTC), values which are +4.3% better and 0.70 sec faster than previous work).","中文标题":"利用自适应损失和大规模事故数据库预测交通事故","摘要翻译":"在本文中，我们提出了一种新颖的方法来预测交通事故，该方法通过（i）早期预测的自适应损失（AdaLEA）和（ii）一个大规模自注释的事故数据库来实现。提出的AdaLEA使我们能够在训练过程中逐步学习更早的预测。损失函数根据模型在每个时期能够多早预测到交通事故来自适应地分配惩罚权重。此外，还讨论了一个新的近失事故数据库（NIDB），该数据库包含了大量交通近失事故，其中骑行者、行人、车辆和背景类别被标记。NIDB提供了交通事故预测和风险因素分类的联合估计。在我们的实验结果中，我们发现我们的提案在预测（99.1%的平均精度（mAP）和4.81秒的平均碰撞时间（ATTC）预测，这些值比之前的工作好6.6%和快2.36秒）和联合估计（62.1%（mAP）和3.65秒预测（ATTC），这些值比之前的工作好4.3%和快0.70秒）方面取得了最高分。","领域":"交通预测/事故分析/风险因素分类","问题":"如何更早且准确地预测交通事故","动机":"提高交通事故预测的准确性和时效性，以减少事故发生的风险","方法":"采用自适应损失函数（AdaLEA）和大规模自注释的事故数据库（NIDB）来逐步学习更早的预测，并通过自适应地分配惩罚权重来提高预测的准确性","关键词":["交通预测","事故分析","风险因素分类"],"涉及的技术概念":"自适应损失函数（AdaLEA）允许模型在训练过程中逐步学习更早的预测，通过自适应地分配惩罚权重来提高预测的准确性。大规模自注释的事故数据库（NIDB）包含了大量交通近失事故，提供了交通事故预测和风险因素分类的联合估计。"},{"order":364,"title":"A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Birdal_A_Minimalist_Approach_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Birdal_A_Minimalist_Approach_CVPR_2018_paper.html","abstract":"This paper proposes a segmentation-free, automatic and efficient procedure to detect general geometric quadric forms in point clouds, where clutter and occlusions are inevitable. Our everyday world is dominated by man-made objects which are designed using 3D primitives (such as planes, cones, spheres, cylinders, etc.). These objects are also omnipresent in industrial environments. This gives rise to the possibility of abstracting 3D scenes through primitives, thereby positions these geometric forms as an integral part of perception and high level 3D scene understanding.  As opposed to state-of-the-art, where a tailored algorithm treats each primitive type separately, we propose to encapsulate all types in a single robust detection procedure. At the center of our approach lies a closed form 3D quadric fit, operating in both primal & dual spaces and requiring as low as 4 oriented-points. Around this fit, we design a novel, local null-space voting strategy to reduce the 4-point case to 3. Voting is coupled with the famous RANSAC and makes our algorithm orders of magnitude faster than its conventional counterparts. This is the first method capable of performing a generic cross-type multi-object primitive detection in difficult scenes. Results on synthetic and real datasets support the validity of our method.","中文标题":"点云中二次曲面类型无关检测的极简方法","摘要翻译":"本文提出了一种无需分割、自动且高效的程序，用于在点云中检测一般的几何二次曲面形式，其中杂波和遮挡是不可避免的。我们的日常世界由使用3D基本体（如平面、圆锥、球体、圆柱体等）设计的人造物体主导。这些物体在工业环境中也无所不在。这提出了通过基本体抽象3D场景的可能性，从而将这些几何形式定位为感知和高级3D场景理解的组成部分。与现有技术中每种基本体类型分别处理的定制算法不同，我们提出将所有类型封装在一个鲁棒的检测程序中。我们方法的核心是一个封闭形式的3D二次曲面拟合，它在原始空间和对偶空间中操作，并且只需要4个定向点。围绕这个拟合，我们设计了一种新颖的局部零空间投票策略，将4点情况减少到3点。投票与著名的RANSAC结合，使我们的算法比传统算法快几个数量级。这是第一个能够在困难场景中执行通用跨类型多对象基本体检测的方法。合成和真实数据集的结果支持了我们方法的有效性。","领域":"三维重建/几何处理/点云分析","问题":"在存在杂波和遮挡的点云中检测一般几何二次曲面形式","动机":"人造物体设计广泛使用3D基本体，这些物体在工业环境中普遍存在，通过基本体抽象3D场景有助于感知和高级3D场景理解","方法":"提出了一种封闭形式的3D二次曲面拟合方法，结合局部零空间投票策略和RANSAC，实现高效检测","关键词":["二次曲面检测","点云分析","RANSAC"],"涉及的技术概念":"3D二次曲面拟合、局部零空间投票策略、RANSAC算法"},{"order":365,"title":"Facelet-Bank for Fast Portrait Manipulation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Facelet-Bank_for_Fast_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Facelet-Bank_for_Fast_CVPR_2018_paper.html","abstract":"Digital face manipulation has become a popular and fascinating way to touch images with the prevalence of smart phones and social networks. With a wide variety of user preferences, facial expressions, and accessories, a general and flexible model is necessary to accommodate different types of facial editing. In this paper, we propose a model to achieve this goal based on an end-to-end convolutional neural network that supports fast inference, edit-effect control, and quick partial-model update. In addition, this model learns from unpaired image sets with different attributes. Experimental results show that our framework can handle a wide range of expressions, accessories, and makeup effects. It produces high-resolution and high-quality results in fast speed.","中文标题":"用于快速肖像操作的Facelet-Bank","摘要翻译":"随着智能手机和社交网络的普及，数字面部操作已成为一种流行且迷人的图像处理方式。面对用户偏好、面部表情和配饰的多样性，需要一个通用且灵活的模型来适应不同类型的面部编辑。在本文中，我们提出了一个基于端到端卷积神经网络的模型来实现这一目标，该模型支持快速推理、编辑效果控制和快速部分模型更新。此外，该模型从具有不同属性的未配对图像集中学习。实验结果表明，我们的框架能够处理广泛的表情、配饰和化妆效果。它以快速的速度生成高分辨率和高质量的结果。","领域":"面部编辑/卷积神经网络/图像生成","问题":"如何实现一个通用且灵活的模型以适应不同类型的面部编辑","动机":"随着智能手机和社交网络的普及，数字面部操作变得流行，需要一种方法来满足用户对面部编辑的多样化需求","方法":"提出一个基于端到端卷积神经网络的模型，支持快速推理、编辑效果控制和快速部分模型更新，并从具有不同属性的未配对图像集中学习","关键词":["面部编辑","卷积神经网络","图像生成","快速推理","编辑效果控制"],"涉及的技术概念":{"端到端卷积神经网络":"一种深度学习模型，能够直接从输入到输出进行学习，无需手动设计特征","快速推理":"模型能够快速处理输入数据并生成输出结果","编辑效果控制":"用户能够控制编辑操作的效果，以达到预期的面部编辑结果","快速部分模型更新":"模型能够快速更新其部分参数，以适应新的编辑需求或数据","未配对图像集":"一组没有明确对应关系的图像，模型需要从中学习不同属性之间的关系"}},{"order":366,"title":"Visual to Sound: Generating Natural Sound for Videos in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Visual_to_Sound_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Visual_to_Sound_CVPR_2018_paper.html","abstract":"As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.","中文标题":"视觉到声音：为野外视频生成自然声音","摘要翻译":"作为五种传统人类感官（视觉、听觉、味觉、嗅觉和触觉）中的两种，视觉和声音是人类理解世界的基本来源。在自然事件中，这两种模态经常相互关联，共同影响人类的感知。在本文中，我们提出了给定视觉输入生成声音的任务。这种能力可以帮助实现虚拟现实中的应用（自动为虚拟场景生成声音）或为视觉障碍者提供图像或视频的额外可访问性。作为这一方向的第一步，我们应用基于学习的方法，根据输入的视频帧生成原始波形样本。我们在包含各种声音（如环境声音和来自人/动物的声音）的视频数据集上评估我们的模型。我们的实验表明，生成的声音相当真实，并且与视觉输入具有良好的时间同步性。","领域":"虚拟现实/辅助技术/声音合成","问题":"如何根据视觉输入生成自然声音","动机":"增强虚拟现实体验和辅助视觉障碍者理解视觉内容","方法":"应用基于学习的方法，根据输入的视频帧生成原始波形样本","关键词":["虚拟现实","辅助技术","声音合成"],"涉及的技术概念":"基于学习的方法指的是使用机器学习技术，特别是深度学习模型，来分析和理解视频帧内容，并据此生成相应的声音波形。这种方法涉及到对视频和声音数据的处理、特征提取、模型训练和评估等一系列技术步骤。"},{"order":367,"title":"3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.html","abstract":"We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-specific shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.","中文标题":"3D-RCNN：通过渲染与比较实现实例级3D物体重建","摘要翻译":"我们提出了一个快速的逆向图形框架，用于实例级的3D场景理解。我们训练了一个深度卷积网络，该网络学习将图像区域映射到图像中所有物体实例的完整3D形状和姿态。我们的方法生成了一个紧凑的3D场景表示，可以轻松用于自动驾驶等应用。许多传统的2D视觉输出，如实例分割和深度图，可以通过简单地渲染我们的输出3D场景模型获得。我们通过从CAD模型集合中学习低维形状空间来利用类别特定的形状先验。我们提出了形状和姿态的新颖表示，旨在实现更好的3D等变性和泛化。为了利用以2D注释形式存在的丰富监督信号，如分割，我们提出了一种可微分的渲染与比较损失，允许通过2D监督学习3D形状和姿态。我们在具有挑战性的真实世界数据集Pascal3D+和KITTI上评估了我们的方法，在这些数据集上我们取得了最先进的结果。","领域":"3D重建/自动驾驶/逆向图形","问题":"实例级3D场景理解","动机":"为了实现对图像中所有物体实例的完整3D形状和姿态的准确理解，以及生成紧凑的3D场景表示，以便于自动驾驶等应用。","方法":"训练一个深度卷积网络来映射图像区域到3D形状和姿态，利用类别特定的形状先验，提出新颖的形状和姿态表示，以及可微分的渲染与比较损失。","关键词":["3D重建","自动驾驶","逆向图形","深度卷积网络","形状先验","渲染与比较损失"],"涉及的技术概念":"深度卷积网络用于映射图像区域到3D形状和姿态；类别特定的形状先验通过从CAD模型集合中学习低维形状空间来利用；新颖的形状和姿态表示旨在实现更好的3D等变性和泛化；可微分的渲染与比较损失允许通过2D监督学习3D形状和姿态。"},{"order":368,"title":"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Fast_and_Furious_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Fast_and_Furious_CVPR_2018_paper.html","abstract":"In this paper we propose a novel  deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting  given data captured by a 3D sensor. By jointly reasoning about these tasks, our  holistic approach is  more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which  is very efficient in terms of both  memory and computation. Our experiments on a new very large scale dataset captured  in several north american cities,   show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all  tasks in as little as 30 ms.","中文标题":"快速而狂怒：使用单一卷积网络实现实时端到端3D检测、跟踪与运动预测","摘要翻译":"在本文中，我们提出了一种新颖的深度神经网络，该网络能够联合推理由3D传感器捕获的数据，进行3D检测、跟踪和运动预测。通过联合推理这些任务，我们的整体方法对于遮挡以及远距离的稀疏数据更加鲁棒。我们的方法在3D世界的鸟瞰图表示上跨空间和时间执行3D卷积，这在内存和计算方面都非常高效。我们在几个北美城市捕获的一个新的非常大规模的数据集上的实验表明，我们可以大幅超越现有技术。重要的是，通过共享计算，我们可以在短短30毫秒内完成所有任务。","领域":"自动驾驶/3D视觉/实时系统","问题":"实时端到端3D检测、跟踪与运动预测","动机":"提高对于遮挡和远距离稀疏数据的鲁棒性，同时实现高效的内存和计算使用","方法":"提出一种深度神经网络，通过联合推理3D检测、跟踪和运动预测任务，并在3D世界的鸟瞰图表示上执行3D卷积","关键词":["3D检测","跟踪","运动预测","实时系统","3D卷积"],"涉及的技术概念":"3D卷积是一种在三维空间上进行的卷积操作，用于处理3D数据，如3D图像或视频。鸟瞰图表示是一种从上方视角展示3D场景的方法，常用于自动驾驶和机器人导航中，以便更好地理解和预测环境中的动态。"},{"order":369,"title":"An Analysis of Scale Invariance in Object Detection ­ SNIP","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_An_Analysis_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Singh_An_Analysis_of_CVPR_2018_paper.html","abstract":"An analysis of different techniques for recognizing and detecting objects under extreme scale variation is presented. Scale specific and scale invariant design of detectors are compared by training them with different configurations of input data. By evaluating the performance of different network architectures for classifying small objects on ImageNet, we show that CNNs are not robust to changes in scale. Based on this analysis, we propose to train and test detectors on the same scales of an image-pyramid. Since small and large objects are difficult to recognize at smaller and larger scales respectively, we present a novel training scheme called Scale Normalization for Image Pyramids (SNIP) which selectively back-propagates the gradients of object instances of different sizes as a function of the image scale. On the COCO dataset, our single model performance is 45.7% and an ensemble of 3 networks obtains an mAP of 48.3%. We use off-the-shelf ImageNet-1000 pre-trained models and only train with bounding box supervision. Our submission won the Best Student Entry in the COCO 2017 challenge. Code will be made available at url{http://bit.ly/2yXVg4c}.","中文标题":"目标检测中尺度不变性分析——SNIP","摘要翻译":"本文分析了在极端尺度变化下识别和检测物体的不同技术。通过使用不同配置的输入数据训练，比较了尺度特定和尺度不变的检测器设计。通过评估不同网络架构在ImageNet上对小物体分类的性能，我们展示了卷积神经网络（CNNs）对尺度变化的鲁棒性不足。基于这一分析，我们提出在图像金字塔的相同尺度上训练和测试检测器。由于小物体和大物体分别在较小和较大尺度上难以识别，我们提出了一种新颖的训练方案，称为图像金字塔的尺度归一化（SNIP），它根据图像尺度选择性地反向传播不同大小物体实例的梯度。在COCO数据集上，我们的单一模型性能为45.7%，三个网络的集成获得了48.3%的mAP。我们使用了现成的ImageNet-1000预训练模型，并且仅使用边界框监督进行训练。我们的提交赢得了COCO 2017挑战赛的最佳学生作品。代码将在url{http://bit.ly/2yXVg4c}提供。","领域":"目标检测/尺度不变性/图像金字塔","问题":"解决在极端尺度变化下识别和检测物体的挑战","动机":"卷积神经网络对尺度变化的鲁棒性不足，需要一种方法来提高在不同尺度下检测物体的性能","方法":"提出了一种新颖的训练方案，称为图像金字塔的尺度归一化（SNIP），它根据图像尺度选择性地反向传播不同大小物体实例的梯度","关键词":["目标检测","尺度不变性","图像金字塔","卷积神经网络","尺度归一化"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据","ImageNet":"一个大型视觉数据库，用于视觉对象识别软件研究","COCO数据集":"一个广泛使用的图像识别、分割和字幕数据集","图像金字塔":"一种多尺度表示方法，通过在不同尺度上分析图像来检测不同大小的物体","尺度归一化（SNIP）":"一种训练方案，通过选择性地反向传播不同大小物体实例的梯度来提高检测器对尺度变化的鲁棒性"}},{"order":370,"title":"Relation Networks for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Relation_Networks_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Relation_Networks_for_CVPR_2018_paper.html","abstract":"Although it is well believed for years that modeling relations between objects would help object recognition,  there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances \\textbf{individually}, without exploiting their relations during learning.  This work proposes an object relation module. It processes a set of objects \\textbf{simultaneously} through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the \\textbf{first fully end-to-end object detector}.","中文标题":"关系网络用于目标检测","摘要翻译":"尽管多年来人们普遍认为建模对象之间的关系有助于对象识别，但在深度学习时代，这一想法尚未得到证实。所有最先进的目标检测系统仍然依赖于单独识别对象实例，而没有在学习过程中利用它们的关系。这项工作提出了一个对象关系模块。它通过对象的外观特征和几何形状之间的交互同时处理一组对象，从而允许建模它们的关系。它是轻量级的且原地操作。它不需要额外的监督，并且易于嵌入现有网络中。它在现代目标检测流程中提高对象识别和重复去除步骤方面显示出有效性。它验证了在基于CNN的检测中建模对象关系的有效性。它催生了第一个完全端到端的目标检测器。","领域":"目标检测/深度学习/卷积神经网络","问题":"现有目标检测系统未能有效利用对象间的关系进行识别","动机":"验证建模对象间关系在深度学习时代对目标检测的有效性","方法":"提出一个轻量级的对象关系模块，通过对象的外观特征和几何形状之间的交互同时处理一组对象，以建模它们的关系","关键词":["目标检测","对象关系","端到端学习"],"涉及的技术概念":{"对象关系模块":"一个轻量级的模块，通过对象的外观特征和几何形状之间的交互同时处理一组对象，以建模它们的关系","CNN":"卷积神经网络，一种深度学习模型，特别适用于处理图像数据","端到端学习":"一种学习方法，直接从输入到输出进行学习，无需手动设计中间步骤或特征"}},{"order":371,"title":"Zero-Shot Sketch-Image Hashing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Zero-Shot_Sketch-Image_Hashing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Zero-Shot_Sketch-Image_Hashing_CVPR_2018_paper.html","abstract":"Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. Providing training and test data subjected to a fixed set of pre-defined categories, the cutting-edge SBIR and cross-modal hashing works obtain acceptable retrieval performance. However, most of the existing methods fail when the categories of query sketches have never been seen during training.  In this paper, the above problem is briefed as a novel but realistic zero-shot SBIR hashing task. We elaborate the challenges of this special task and accordingly propose a zero-shot sketch-image hashing (ZSIH) model. An end-to-end three-network architecture is built, two of which are treated as the binary encoders. The third network mitigates the sketch-image heterogeneity and enhances the semantic relations among data by utilizing the Kronecker fusion layer and graph convolution, respectively. As an important part of ZSIH, we formulate a generative hashing scheme in reconstructing semantic knowledge representations for zero-shot retrieval. To the best of our knowledge, ZSIH is the first zero-shot hashing work suitable for SBIR and cross-modal search. Comprehensive experiments are conducted on two extended datasets, i.e., Sketchy and TU-Berlin with a novel zero-shot train-test split. The proposed model remarkably outperforms related works.","中文标题":"零样本草图-图像哈希","摘要翻译":"最近的研究表明，通过跨模态二进制表示学习方法可以有效地解决大规模基于草图的图像检索（SBIR）问题，其中汉明距离匹配显著加快了相似性搜索的过程。提供受限于一组预定义类别的训练和测试数据，最先进的SBIR和跨模态哈希工作获得了可接受的检索性能。然而，当查询草图的类别在训练期间从未见过时，大多数现有方法都会失败。在本文中，上述问题被简要描述为一个新颖但现实的零样本SBIR哈希任务。我们详细阐述了这一特殊任务的挑战，并相应地提出了一个零样本草图-图像哈希（ZSIH）模型。构建了一个端到端的三网络架构，其中两个被视为二进制编码器。第三个网络通过分别利用Kronecker融合层和图卷积来缓解草图-图像异质性并增强数据之间的语义关系。作为ZSIH的重要组成部分，我们制定了一个生成哈希方案，用于重建零样本检索的语义知识表示。据我们所知，ZSIH是第一个适用于SBIR和跨模态搜索的零样本哈希工作。在两个扩展数据集上进行了全面的实验，即Sketchy和TU-Berlin，采用了一种新颖的零样本训练-测试分割。所提出的模型显著优于相关工作。","领域":"跨模态检索/零样本学习/哈希学习","问题":"解决在训练期间未见过的类别查询草图的跨模态检索问题","动机":"现有方法在处理未见类别查询时表现不佳，需要一种新的方法来解决这一问题","方法":"提出了一种零样本草图-图像哈希（ZSIH）模型，采用端到端的三网络架构，包括两个二进制编码器和一个用于缓解草图-图像异质性并增强语义关系的网络，以及一个生成哈希方案用于重建语义知识表示","关键词":["跨模态检索","零样本学习","哈希学习","草图-图像检索","语义知识表示"],"涉及的技术概念":{"跨模态二进制表示学习":"一种学习方法，旨在通过二进制表示来桥接不同模态（如文本和图像）之间的差异，以便进行有效的检索。","汉明距离匹配":"一种用于比较两个二进制字符串相似度的方法，通过计算它们之间不同位的数量来实现。","Kronecker融合层":"一种用于融合不同特征或模态的技术，通过Kronecker乘积来实现特征之间的交互。","图卷积":"一种在图结构数据上进行的卷积操作，用于捕捉节点之间的关系和结构信息。","生成哈希方案":"一种用于生成哈希码的方法，旨在通过重建语义知识表示来支持零样本检索。"}},{"order":372,"title":"VizWiz Grand Challenge: Answering Visual Questions From Blind People","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.html","abstract":"The study of algorithms to automatically answer visual questions currently is motivated by visual question answering (VQA) datasets constructed in artificial VQA settings.  We propose VizWiz, the first goal-oriented VQA dataset arising from a natural VQA setting.  VizWiz consists of 31,000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.  VizWiz differs from the many existing VQA datasets because (1) images are captured by blind photographers and so are often poor quality, (2) questions are spoken and so are more conversational, and (3) often visual questions cannot be answered.  Evaluation of modern algorithms for answering visual questions and deciding if a visual question is answerable reveals that VizWiz is a challenging dataset.  We introduce this dataset to encourage a larger community to develop more generalized algorithms that can assist blind people.","中文标题":"VizWiz大挑战：回答盲人提出的视觉问题","摘要翻译":"目前，自动回答视觉问题的算法研究受到在人工视觉问答（VQA）设置下构建的VQA数据集的推动。我们提出了VizWiz，这是第一个源自自然VQA设置的目标导向VQA数据集。VizWiz包含31,000个来自盲人的视觉问题，每个盲人使用手机拍摄照片并记录了一个关于该照片的口头问题，每个视觉问题还附有10个众包答案。VizWiz与许多现有的VQA数据集不同，因为（1）图像是由盲人摄影师拍摄的，因此质量通常较差，（2）问题是口头的，因此更具对话性，（3）许多视觉问题无法回答。对现代算法回答视觉问题和判断视觉问题是否可回答的评估表明，VizWiz是一个具有挑战性的数据集。我们引入这个数据集，以鼓励更大的社区开发更通用的算法，以帮助盲人。","领域":"视觉问答/辅助技术/众包","问题":"如何自动回答由盲人提出的视觉问题","动机":"推动开发能够帮助盲人解决视觉问题的更通用算法","方法":"构建并引入VizWiz数据集，这是一个源自自然VQA设置的目标导向VQA数据集，包含由盲人拍摄的图像和口头问题，以及每个问题的众包答案","关键词":["视觉问答","辅助技术","众包"],"涉及的技术概念":"视觉问答（VQA）数据集，众包答案，图像质量，口头问题，算法评估"},{"order":373,"title":"Divide and Grow: Capturing Huge Diversity in Crowd Images With Incrementally Growing CNN","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sam_Divide_and_Grow_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sam_Divide_and_Grow_CVPR_2018_paper.html","abstract":"Automated counting of people in crowd images is a challenging task. The major difficulty stems from the large diversity in the way people appear in crowds. In fact, features available for crowd discrimination largely depend on the crowd density to the extent that people are only seen as blobs in a highly dense scene. We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes. Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images. In order to adapt with the huge diversity, we create two child regressors which are exact copies of the base CNN. A differential training procedure divides the dataset into two clusters and fine-tunes the child networks on their respective specialties. Consequently, without any hand-crafted criteria for forming specialties, the child regressors become experts on certain types of crowds. The child networks are again split recursively, creating two experts at every division. This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents. The leaf nodes are taken as the final experts and a classifier network is then trained to predict the correct specialty for a given test image patch. The proposed model achieves higher count accuracy on major crowd datasets. Further, we analyse the characteristics of specialties mined automatically by our method.","中文标题":"分而治之：使用增量增长的CNN捕捉人群图像中的巨大多样性","摘要翻译":"自动计数人群图像中的人数是一项具有挑战性的任务。主要困难源于人群中人们出现方式的巨大多样性。事实上，可用于人群区分的特征在很大程度上取决于人群密度，以至于在高度密集的场景中，人们只能被视为斑点。我们通过一个可以逐步增加其容量以应对人群场景中广泛变化的增长的CNN来解决这个问题。我们的模型从一个基础的CNN密度回归器开始，该回归器在所有类型的人群图像上等效训练。为了适应巨大的多样性，我们创建了两个子回归器，它们是基础CNN的精确副本。一个差异化的训练程序将数据集分为两个集群，并在各自的专业领域上微调子网络。因此，没有任何手工制定的形成专业标准，子回归器成为某些类型人群的专家。子网络再次递归地分裂，每次分裂创建两个专家。这种分层训练导致了一个CNN树，其中子回归器比它们的任何父级都更精细。叶节点被视为最终专家，然后训练一个分类器网络来预测给定测试图像块的正确专业。所提出的模型在主要人群数据集上实现了更高的计数准确性。此外，我们分析了通过我们的方法自动挖掘的专业特征。","领域":"人群计数/卷积神经网络/图像分析","问题":"自动计数人群图像中的人数，处理人群图像中人们出现方式的巨大多样性","动机":"解决由于人群密度变化大导致的人群图像中人物特征难以区分的问题","方法":"使用一个可以逐步增加容量的增长的CNN，通过创建和递归分裂子回归器来适应人群图像的多样性，最终形成一个CNN树结构，其中叶节点作为最终专家，并通过分类器网络预测测试图像块的正确专业","关键词":["人群计数","卷积神经网络","图像分析","密度回归","专业特征"],"涉及的技术概念":"CNN（卷积神经网络）用于图像分析，特别是人群计数；密度回归器用于估计人群密度；递归分裂和微调技术用于创建专家网络；分类器网络用于预测图像块的专业。"},{"order":374,"title":"Structured Set Matching Networks for One-Shot Part Labeling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Structured_Set_Matching_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_Structured_Set_Matching_CVPR_2018_paper.html","abstract":"Diagrams often depict complex phenomena and serve as a good test bed for visual and textual reasoning. However, understanding diagrams using natural image understanding approaches requires large training datasets of diagrams, which are very hard to obtain. Instead, this can be addressed as a matching problem either between labeled diagrams, images or both. This problem is very challenging since the absence of significant color and texture renders local cues ambiguous and requires global reasoning. We consider the problem of one-shot part labeling: labeling multiple parts of an object in a target image given only a single source image of that category. For this set-to-set matching problem, we introduce the Structured Set Matching Network (SSMN), a structured prediction model that incorporates convolutional neural networks. The SSMN is trained using global normalization to maximize local match scores between corresponding elements and a global consistency score among all matched elements, while also enforcing a matching constraint between the two sets. The SSMN significantly outperforms several strong baselines on three label transfer scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200 categories; image-to-image, evaluated on a dataset built on top of the Pascal Part Dataset; and image-to-diagram, evaluated on transferring labels across these datasets.","中文标题":"结构化集合匹配网络用于一次性部分标注","摘要翻译":"图表经常描绘复杂的现象，并作为视觉和文本推理的良好测试平台。然而，使用自然图像理解方法理解图表需要大量的图表训练数据集，这些数据集很难获得。相反，这可以被视为一个匹配问题，无论是在标记的图表、图像之间，还是两者之间。这个问题非常具有挑战性，因为缺乏显著的颜色和纹理使得局部线索变得模糊，需要全局推理。我们考虑一次性部分标注的问题：在仅给定该类别的单个源图像的情况下，标注目标图像中对象的多个部分。对于这个集合到集合的匹配问题，我们引入了结构化集合匹配网络（SSMN），这是一个结合了卷积神经网络的结构化预测模型。SSMN使用全局归一化进行训练，以最大化对应元素之间的局部匹配分数和所有匹配元素之间的全局一致性分数，同时也在两个集合之间强制执行匹配约束。SSMN在三种标签转移场景中显著优于几个强基线：图表到图表，在一个包含200多个类别的新图表数据集上评估；图像到图像，在基于Pascal Part Dataset构建的数据集上评估；以及图像到图表，在跨这些数据集转移标签时评估。","领域":"图表理解/标签转移/集合匹配","问题":"在仅给定单个源图像的情况下，标注目标图像中对象的多个部分","动机":"理解图表需要大量的图表训练数据集，这些数据集很难获得，因此需要一种新的方法来解决这个问题","方法":"引入了结构化集合匹配网络（SSMN），这是一个结合了卷积神经网络的结构化预测模型，使用全局归一化进行训练，以最大化对应元素之间的局部匹配分数和所有匹配元素之间的全局一致性分数，同时也在两个集合之间强制执行匹配约束","关键词":["图表理解","标签转移","集合匹配","卷积神经网络","全局归一化"],"涉及的技术概念":{"结构化集合匹配网络（SSMN）":"一个结合了卷积神经网络的结构化预测模型，用于解决集合到集合的匹配问题","全局归一化":"一种训练方法，用于最大化对应元素之间的局部匹配分数和所有匹配元素之间的全局一致性分数","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据"}},{"order":375,"title":"Self-Supervised Learning of Geometrically Stable Features Through Probabilistic Introspection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Novotny_Self-Supervised_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Novotny_Self-Supervised_Learning_of_CVPR_2018_paper.html","abstract":"Self-supervision can dramatically cut back the amount of manually-labelled data required to train deep neural networks. While self-supervision has usually been considered for tasks such as image classification, in this paper we aim at extending it to geometry-oriented tasks such as semantic matching and part detection. We do so by building on several recent ideas in unsupervised landmark detection. Our approach learns dense distinctive visual descriptors from an unlabeled dataset of images using synthetic image transformations. It does so by means of a robust probabilistic formulation that can introspectively determine which image regions are likely to result in stable image matching. We show empirically that a network pre-trained in this manner requires significantly less supervision to learn semantic object parts compared to numerous pre-training alternatives. We also show that the pre-trained representation is excellent for semantic object matching.","中文标题":"通过概率内省自监督学习几何稳定特征","摘要翻译":"自监督学习可以大幅减少训练深度神经网络所需的手动标记数据量。虽然自监督学习通常被考虑用于图像分类等任务，但在本文中，我们旨在将其扩展到几何导向的任务，如语义匹配和部件检测。我们通过基于无监督地标检测的几项最新想法来实现这一点。我们的方法使用合成图像变换从无标签的图像数据集中学习密集的独特视觉描述符。它通过一种鲁棒的概率公式来实现这一点，该公式可以内省地确定哪些图像区域可能产生稳定的图像匹配。我们通过实验证明，与许多预训练替代方案相比，以这种方式预训练的网络学习语义对象部件所需的监督显著减少。我们还展示了预训练表示在语义对象匹配方面表现出色。","领域":"语义匹配/部件检测/无监督学习","问题":"减少训练深度神经网络所需的手动标记数据量，并扩展到几何导向的任务","动机":"自监督学习通常用于图像分类等任务，但本文旨在将其扩展到几何导向的任务，如语义匹配和部件检测","方法":"使用合成图像变换从无标签的图像数据集中学习密集的独特视觉描述符，通过一种鲁棒的概率公式内省地确定哪些图像区域可能产生稳定的图像匹配","关键词":["自监督学习","几何稳定特征","概率内省","语义匹配","部件检测"],"涉及的技术概念":"自监督学习是一种减少手动标记数据需求的方法，通过使用未标记的数据进行训练。几何稳定特征指的是在图像变换下保持稳定的特征，这对于几何导向的任务如语义匹配和部件检测非常重要。概率内省是一种方法，用于确定图像中哪些区域在变换后仍能保持匹配，这对于提高匹配的稳定性和准确性至关重要。"},{"order":376,"title":"Link and Code: Fast Indexing With Graphs and Compact Regression Codes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Douze_Link_and_Code_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Douze_Link_and_Code_CVPR_2018_paper.html","abstract":"Similarity search approaches based on graph walks have recently attained outstanding speed-accuracy trade-offs, taking aside the memory requirements. In this paper, we revisit these approaches by considering, additionally, the memory constraint required to index billions of images on a single server. This leads us to propose a method based both on graph traversal and compact representations. We encode the indexed vectors using quantization and exploit the graph structure to refine the similarity estimation.   In essence, our method takes the best of these two worlds: the search strategy is based on nested graphs, thereby providing high precision with a relatively small set of comparisons. At the same time it offers a significant memory compression. As a result, our approach outperforms the state of the art on operating points considering 64--128 bytes per vector, as demonstrated by our results on two billion-scale public benchmarks.","中文标题":"链接与编码：使用图和紧凑回归码的快速索引","摘要翻译":"基于图游走的相似性搜索方法最近在速度-准确性权衡方面取得了显著成就，尽管忽略了内存需求。在本文中，我们通过额外考虑在单个服务器上索引数十亿图像所需的内存约束，重新审视了这些方法。这促使我们提出了一种基于图遍历和紧凑表示的方法。我们使用量化对索引向量进行编码，并利用图结构来优化相似性估计。本质上，我们的方法结合了这两个领域的优点：搜索策略基于嵌套图，从而在相对较小的比较集合中提供高精度。同时，它提供了显著的内存压缩。因此，我们的方法在考虑每个向量64-128字节的操作点上优于现有技术，正如我们在两个十亿级公共基准上的结果所证明的那样。","领域":"图像检索/大规模数据处理/相似性搜索","问题":"在单个服务器上索引数十亿图像时，如何在保证高精度的同时实现显著的内存压缩","动机":"现有的基于图游走的相似性搜索方法在速度-准确性权衡方面取得了显著成就，但忽略了内存需求，这对于大规模图像索引是一个重要挑战","方法":"提出了一种结合图遍历和紧凑表示的方法，使用量化对索引向量进行编码，并利用图结构优化相似性估计","关键词":["图游走","量化","内存压缩","相似性搜索","大规模图像索引"],"涉及的技术概念":"图游走是一种基于图的相似性搜索方法，通过遍历图中的节点来找到与查询最相似的项。量化是一种数据压缩技术，通过减少表示数据所需的位数来降低存储需求。内存压缩是指通过算法或数据结构优化来减少存储数据所需的内存空间。相似性搜索是指在大量数据中快速找到与给定查询最相似的项。大规模图像索引是指对数十亿级别的图像数据进行组织和管理，以便快速检索。"},{"order":377,"title":"Textbook Question Answering Under Instructor Guidance With Memory Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Textbook_Question_Answering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Textbook_Question_Answering_CVPR_2018_paper.html","abstract":"Textbook Question Answering (TQA) is a task to choose the most proper answers by reading a multi-modal context of abundant essays and images. TQA serves as a favorable test bed for visual and textual reasoning. However, most of the current methods are incapable of reasoning over the long contexts and images. To address this issue, we propose a novel approach of Instructor Guidance with Memory Networks (IGMN) which conducts the TQA task by finding contradictions between the candidate answers and their corresponding context. We build the Contradiction Entity-Relationship Graph (CERG) to extend the passage-level multi-modal contradictions to an essay level. The machine thus performs as an instructor to extract the essay-level contradictions as the Guidance. Afterwards, we exploit the memory networks to capture the information in the Guidance, and use the attention mechanisms to jointly reason over the global features of the multi-modal input. Extensive experiments demonstrate that our method outperforms the state-of-the-arts on the TQA dataset. The source code is available at https://github.com/freerailway/igmn.","中文标题":"在教师指导下使用记忆网络进行教科书问答","摘要翻译":"教科书问答（TQA）是一项通过阅读包含丰富文章和图像的多模态上下文来选择最合适答案的任务。TQA作为视觉和文本推理的有利测试平台。然而，当前大多数方法无法在长上下文和图像上进行推理。为了解决这个问题，我们提出了一种新颖的教师指导与记忆网络（IGMN）方法，该方法通过寻找候选答案与其对应上下文之间的矛盾来进行TQA任务。我们构建了矛盾实体关系图（CERG）以将段落级别的多模态矛盾扩展到文章级别。机器因此扮演教师的角色，提取文章级别的矛盾作为指导。随后，我们利用记忆网络捕捉指导中的信息，并使用注意力机制共同推理多模态输入的全局特征。大量实验证明，我们的方法在TQA数据集上优于现有技术。源代码可在https://github.com/freerailway/igmn获取。","领域":"自然语言处理/多模态学习/问答系统","问题":"在长上下文和图像上进行有效的视觉和文本推理","动机":"当前方法在长上下文和图像上的推理能力有限，需要一种新的方法来提高教科书问答任务的性能","方法":"提出教师指导与记忆网络（IGMN）方法，通过构建矛盾实体关系图（CERG）扩展多模态矛盾到文章级别，并利用记忆网络和注意力机制进行全局特征推理","关键词":["教科书问答","记忆网络","多模态学习","注意力机制","矛盾实体关系图"],"涉及的技术概念":"教师指导与记忆网络（IGMN）是一种结合记忆网络和注意力机制的方法，用于处理多模态输入（如文本和图像）的全局特征推理。矛盾实体关系图（CERG）是一种将段落级别的多模态矛盾扩展到文章级别的技术，用于提高问答系统的性能。"},{"order":378,"title":"Unsupervised Deep Generative Adversarial Hashing Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dizaji_Unsupervised_Deep_Generative_CVPR_2018_paper.html","abstract":"Unsupervised deep hash functions have not shown satisfactory improvements against the shallow alternatives, and usually, require supervised pretraining to avoid getting stuck in bad local minima. In this paper, we propose a deep unsupervised hashing function, called HashGAN, which outperforms unsupervised hashing models with significant margins without any supervised pretraining. HashGAN consists of three networks, a generator, a discriminator and an encoder. By sharing the parameters of the encoder and discriminator, we benefit from the adversarial loss as a data dependent regularization in training our deep hash function. Moreover, a novel loss function is introduced for hashing real images, resulting in minimum entropy, uniform frequency, consistent and independent hash bits. Furthermore, we train the generator conditioning on random binary inputs and also use these binary variables in a triplet ranking loss for improving hash codes. In our experiments, HashGAN outperforms the previous unsupervised hash functions in image retrieval and achieves the state-of-the-art performance in image clustering. We also provide an ablation study, showing the contribution of each component in our loss function.","中文标题":"无监督深度生成对抗哈希网络","摘要翻译":"无监督深度哈希函数相较于浅层替代方案尚未显示出令人满意的改进，并且通常需要监督预训练以避免陷入不良的局部最小值。在本文中，我们提出了一种称为HashGAN的深度无监督哈希函数，它在没有任何监督预训练的情况下，显著优于无监督哈希模型。HashGAN由三个网络组成：生成器、判别器和编码器。通过共享编码器和判别器的参数，我们利用对抗性损失作为数据依赖的正则化来训练我们的深度哈希函数。此外，引入了一种新的损失函数用于哈希真实图像，从而实现最小熵、均匀频率、一致且独立的哈希位。此外，我们训练生成器以随机二进制输入为条件，并在三元组排序损失中使用这些二进制变量以改进哈希码。在我们的实验中，HashGAN在图像检索中优于之前的无监督哈希函数，并在图像聚类中达到了最先进的性能。我们还提供了消融研究，展示了我们损失函数中每个组件的贡献。","领域":"图像检索/图像聚类/哈希学习","问题":"无监督深度哈希函数在性能上未能显著超越浅层哈希方法，且通常需要监督预训练以避免陷入不良的局部最小值。","动机":"提出一种无需监督预训练的深度无监督哈希函数，以显著提升无监督哈希模型的性能。","方法":"提出HashGAN，一种由生成器、判别器和编码器组成的深度无监督哈希函数，通过共享编码器和判别器的参数利用对抗性损失作为数据依赖的正则化，并引入新的损失函数以实现最小熵、均匀频率、一致且独立的哈希位。","关键词":["无监督学习","生成对抗网络","哈希学习","图像检索","图像聚类"],"涉及的技术概念":"对抗性损失用于数据依赖的正则化，新的损失函数用于实现最小熵、均匀频率、一致且独立的哈希位，三元组排序损失用于改进哈希码。"},{"order":379,"title":"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html","abstract":"A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers.  It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering.  Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.","中文标题":"视觉与语言导航：在真实环境中解释视觉基础导航指令","摘要翻译":"能够执行自然语言指令的机器人自《杰森一家》卡通系列想象由一群细心的机器人助手调解的休闲生活以来，一直是一个梦想。这个梦想仍然遥不可及。然而，视觉和语言方法的最新进展在密切相关的领域取得了令人难以置信的进步。这一点很重要，因为机器人根据其所见解释自然语言导航指令是在执行类似于视觉问答的视觉和语言过程。这两个任务都可以解释为视觉基础的序列到序列翻译问题，许多相同的方法都适用。为了促进和鼓励视觉和语言方法应用于解释视觉基础导航指令的问题，我们提出了Matterport3D模拟器——一个基于真实图像的大规模强化学习环境。使用这个模拟器，未来可以支持一系列体现视觉和语言任务，我们提供了第一个在真实建筑中进行视觉基础自然语言导航的基准数据集——房间到房间（R2R）数据集。","领域":"视觉与语言导航/强化学习/自然语言处理","问题":"解释视觉基础的自然语言导航指令","动机":"实现能够执行自然语言指令的机器人，尽管这一目标仍然具有挑战性，但视觉和语言方法的最新进展为此提供了可能性。","方法":"提出了Matterport3D模拟器，这是一个基于真实图像的大规模强化学习环境，并提供了第一个在真实建筑中进行视觉基础自然语言导航的基准数据集——房间到房间（R2R）数据集。","关键词":["视觉与语言导航","强化学习","自然语言处理","视觉问答","序列到序列翻译"],"涉及的技术概念":"视觉与语言导航涉及机器人根据视觉信息解释和执行自然语言指令的过程，类似于视觉问答任务。这些任务可以被视为视觉基础的序列到序列翻译问题，其中许多相同的方法都适用。Matterport3D模拟器是一个基于真实图像的大规模强化学习环境，旨在支持体现视觉和语言任务，如视觉基础的自然语言导航。"},{"order":380,"title":"DenseASPP for Semantic Segmentation in Street Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.html","abstract":"Semantic image segmentation is a basic street scene understanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded. To remedy this problem, atrous convolutioncite{Deeplabv1} was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)cite{Deeplabv2} was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapescite{Cityscapes} and achieve state-of-the-art performance.","中文标题":"用于街景语义分割的DenseASPP","摘要翻译":"语义图像分割是自动驾驶中一个基本的街景理解任务，其中高分辨率图像中的每个像素被分类为一组语义标签。与其他场景不同，自动驾驶场景中的物体表现出非常大的尺度变化，这对高级特征表示提出了巨大挑战，因为必须正确编码多尺度信息。为了解决这个问题，引入了空洞卷积（atrous convolution）来生成具有更大感受野的特征而不牺牲空间分辨率。基于空洞卷积，提出了空洞空间金字塔池化（Atrous Spatial Pyramid Pooling, ASPP），通过使用不同的扩张率将多个空洞卷积特征连接成最终的特征表示。尽管ASPP能够生成多尺度特征，但我们认为在自动驾驶场景中，尺度轴上的特征分辨率不够密集。为此，我们提出了密集连接的空洞空间金字塔池化（Densely connected Atrous Spatial Pyramid Pooling, DenseASPP），它以密集的方式连接一组空洞卷积层，从而生成不仅覆盖更大尺度范围，而且在该尺度范围内密集覆盖的多尺度特征，而不会显著增加模型大小。我们在街景基准Cityscapes上评估了DenseASPP，并实现了最先进的性能。","领域":"自动驾驶/语义分割/街景理解","问题":"自动驾驶场景中物体尺度变化大，导致高级特征表示和多尺度信息编码的挑战","动机":"提高自动驾驶场景中语义分割的准确性和效率，特别是在处理大尺度变化物体时","方法":"提出密集连接的空洞空间金字塔池化（DenseASPP），通过密集连接一组空洞卷积层来生成覆盖更大且更密集尺度范围的多尺度特征","关键词":["语义分割","空洞卷积","多尺度特征"],"涉及的技术概念":"空洞卷积（atrous convolution）是一种在不牺牲空间分辨率的情况下增加感受野的技术。空洞空间金字塔池化（ASPP）通过使用不同的扩张率将多个空洞卷积特征连接成最终的特征表示。密集连接的空洞空间金字塔池化（DenseASPP）是在ASPP的基础上，通过密集连接一组空洞卷积层来生成覆盖更大且更密集尺度范围的多尺度特征。"},{"order":381,"title":"Efficient Optimization for Rank-Based Loss Functions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mohapatra_Efficient_Optimization_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mohapatra_Efficient_Optimization_for_CVPR_2018_paper.html","abstract":"The accuracy of information retrieval systems is often measured using complex loss functions such as the average precision (AP) or the normalized discounted cumulative gain (NDCG). Given a set of positive and negative samples, the parameters of a retrieval system can be estimated by minimizing these loss functions. However, the non-differentiability and non-decomposability of these loss functions does not allow for simple gradient based optimization algorithms. This issue is generally circumvented by either optimizing a structured hinge-loss upper bound to the loss function or by using asymptotic methods like the direct-loss minimization framework. Yet, the high computational complexity of loss-augmented inference, which is necessary for both the frameworks, prohibits its use in large training data sets. To alleviate this deficiency, we present a novel quicksort flavored algorithm for a large class of non-decomposable loss functions. We provide a complete characterization of the loss functions that are amenable to our algorithm, and show that it includes both AP and NDCG based loss functions. Furthermore, we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically. We demonstrate the effectiveness of our approach in the context of optimizing the structured hinge loss upper bound of AP and NDCG loss for learning models for a variety of vision tasks. We show that our approach provides significantly better results than simpler decomposable loss functions, while requiring a comparable training time.","中文标题":"基于排序损失函数的高效优化","摘要翻译":"信息检索系统的准确性通常使用复杂的损失函数来衡量，例如平均精度（AP）或归一化折扣累积增益（NDCG）。给定一组正样本和负样本，可以通过最小化这些损失函数来估计检索系统的参数。然而，这些损失函数的不可微性和不可分解性不允许使用简单的基于梯度的优化算法。这个问题通常通过优化损失函数的结构化铰链损失上界或使用直接损失最小化框架等渐近方法来规避。然而，对于这两种框架都必需的损失增强推理的高计算复杂性，阻碍了其在大规模训练数据集上的使用。为了缓解这一缺陷，我们提出了一种新颖的快速排序风格算法，适用于一大类不可分解的损失函数。我们提供了适用于我们算法的损失函数的完整特征描述，并表明它包括基于AP和NDCG的损失函数。此外，我们证明了没有基于比较的算法可以在计算复杂性上渐近地改进我们的方法。我们在优化AP和NDCG损失的结构化铰链损失上界的背景下展示了我们方法的有效性，用于学习各种视觉任务的模型。我们展示了我们的方法比简单的可分解损失函数提供了显著更好的结果，同时需要相当的训练时间。","领域":"信息检索/视觉任务/损失函数优化","问题":"解决在信息检索系统中使用复杂损失函数（如AP和NDCG）进行参数估计时，由于损失函数的不可微性和不可分解性导致的优化难题","动机":"为了克服复杂损失函数在优化过程中遇到的高计算复杂性问题，提出一种新的算法以提高优化效率和效果","方法":"提出了一种新颖的快速排序风格算法，适用于一大类不可分解的损失函数，并提供了适用于该算法的损失函数的完整特征描述","关键词":["信息检索","损失函数","优化算法","视觉任务"],"涉及的技术概念":"平均精度（AP）、归一化折扣累积增益（NDCG）、结构化铰链损失上界、直接损失最小化框架、损失增强推理、快速排序风格算法"},{"order":382,"title":"Wasserstein Introspective Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_Wasserstein_Introspective_Neural_CVPR_2018_paper.html","abstract":"We present Wasserstein introspective neural networks (WINN) that are both a generator and a discriminator within a single model. WINN provides a significant improvement over the recent introspective neural networks (INN) method by enhancing INN's generative modeling capability. WINN has three interesting properties: (1) A mathematical connection between the formulation of the INN algorithm and that of Wasserstein generative adversarial networks (WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN results in a large enhancement to INN, achieving compelling results even with a single classifier --- e.g., providing nearly a 20 times reduction in model size over INN for unsupervised generative modeling. (3) When applied to supervised classification, WINN also gives rise to improved robustness against adversarial examples in terms of the error reduction. In the experiments, we report encouraging results on unsupervised learning problems including texture, face, and object modeling, as well as a supervised classification task against adversarial attacks.","中文标题":"Wasserstein自省神经网络","摘要翻译":"我们提出了Wasserstein自省神经网络（WINN），这是一种在单一模型中同时作为生成器和判别器的模型。WINN通过增强INN的生成建模能力，显著改进了最近的自省神经网络（INN）方法。WINN具有三个有趣的特性：（1）建立了INN算法与Wasserstein生成对抗网络（WGAN）公式之间的数学联系。（2）将Wasserstein距离明确引入INN，极大地增强了INN，即使使用单一分类器也能取得令人信服的结果——例如，在无监督生成建模方面，与INN相比，模型大小减少了近20倍。（3）当应用于监督分类时，WINN还在减少错误方面提高了对抗样本的鲁棒性。在实验中，我们报告了在包括纹理、面部和对象建模的无监督学习问题以及对抗攻击的监督分类任务上的鼓舞人心的结果。","领域":"生成模型/对抗网络/鲁棒性","问题":"增强自省神经网络的生成建模能力和对抗样本的鲁棒性","动机":"改进自省神经网络（INN）方法，通过引入Wasserstein距离来增强其生成建模能力，并提高对抗样本的鲁棒性","方法":"提出Wasserstein自省神经网络（WINN），在单一模型中同时作为生成器和判别器，明确采用Wasserstein距离来增强INN","关键词":["Wasserstein距离","自省神经网络","生成对抗网络","对抗样本","鲁棒性"],"涉及的技术概念":"Wasserstein自省神经网络（WINN）是一种结合了生成器和判别器的模型，通过引入Wasserstein距离来增强自省神经网络（INN）的生成建模能力，并提高对抗样本的鲁棒性。这种方法在无监督生成建模和监督分类任务中都取得了显著的改进。"},{"order":383,"title":"Taskonomy: Disentangling Task Transfer Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zamir_Taskonomy_Disentangling_Task_CVPR_2018_paper.html","abstract":"Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable uses; it is the concept underlying transfer learning and, for example, can provide a principled way for reusing supervision among related tasks, finding what tasks transfer well to an arbitrary target task, or solving many tasks in one system without piling up the complexity.      This paper proposes a fully computational approach for finding the structure of the space of visual tasks. This is done via a sampled dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer dependencies in a latent space. The product can be viewed as a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. the nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 while keeping the performance nearly the same. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.","中文标题":"任务学：解缠任务迁移学习","摘要翻译":"视觉任务之间是否存在关系，还是它们彼此无关？例如，拥有表面法线是否可以简化图像深度的估计？直觉上，这些问题的答案是肯定的，暗示着视觉任务之间存在某种结构。了解这种结构有显著的用途；它是迁移学习概念的基础，例如，可以提供一种原则性的方法来在相关任务之间重用监督，找到哪些任务可以很好地迁移到任意目标任务，或者在一个系统中解决许多任务而不增加复杂性。本文提出了一种完全计算的方法来寻找视觉任务空间的结构。这是通过一个包含二十六个2D、2.5D、3D和语义任务的采样字典，并在潜在空间中建模它们的（一阶和高阶）迁移依赖关系来实现的。该产品可以被视为任务迁移学习的计算分类图。我们研究了这种结构的后果，例如出现的非平凡关系，并利用它们来减少对标记数据的需求。例如，我们展示了解决一组10个任务所需的标记数据点的总数可以减少大约2/3，同时保持性能几乎不变。用户可以使用提供的二进制整数规划求解器，该求解器利用分类学来找到适合自己用例的有效监督策略。","领域":"视觉任务迁移/监督学习/数据效率","问题":"视觉任务之间的关系及其在迁移学习中的应用","动机":"探索视觉任务之间的结构关系，以提高迁移学习的效率和效果","方法":"通过采样字典和潜在空间建模，研究视觉任务之间的迁移依赖关系，并利用这些关系减少对标记数据的需求","关键词":["视觉任务迁移","监督学习","数据效率"],"涉及的技术概念":"本文涉及的技术概念包括视觉任务迁移、监督学习、数据效率、潜在空间建模、二进制整数规划求解器等。"},{"order":384,"title":"Maximum Classifier Discrepancy for Unsupervised Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.html","abstract":"In this work, we present a method for unsupervised domain adaptation.  Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics.  To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries.  We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy.  Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at url{https://github.com/mil-tokyo/MCD_DA}","中文标题":"最大分类器差异用于无监督领域适应","摘要翻译":"在本工作中，我们提出了一种无监督领域适应的方法。许多对抗学习方法训练领域分类器网络以区分特征为源或目标，并训练特征生成器网络以模仿判别器。这些方法存在两个问题。首先，领域分类器仅尝试将特征区分为源或目标，因此不考虑类别之间的任务特定决策边界。因此，训练后的生成器可能在类别边界附近生成模糊特征。其次，这些方法旨在完全匹配不同领域之间的特征分布，这由于每个领域的特性而变得困难。为了解决这些问题，我们引入了一种新方法，该方法尝试通过利用任务特定的决策边界来对齐源和目标的分布。我们提出最大化两个分类器输出之间的差异，以检测远离源支持的目标样本。特征生成器学习生成靠近支持的目标特征，以最小化差异。我们的方法在几个图像分类和语义分割数据集上优于其他方法。代码可在url{https://github.com/mil-tokyo/MCD_DA}获取。","领域":"领域适应/图像分类/语义分割","问题":"无监督领域适应中特征分布对齐和类别边界模糊的问题","动机":"解决现有对抗学习方法在无监督领域适应中忽视任务特定决策边界和难以完全匹配特征分布的问题","方法":"提出一种新方法，通过最大化两个分类器输出之间的差异来对齐源和目标分布，并训练特征生成器生成靠近支持的目标特征以最小化差异","关键词":["领域适应","图像分类","语义分割","对抗学习","特征生成器"],"涉及的技术概念":"无监督领域适应、对抗学习、特征生成器、领域分类器、任务特定决策边界、特征分布对齐"},{"order":385,"title":"Unsupervised Feature Learning via Non-Parametric Instance Discrimination","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html","abstract":"Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsu- pervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.","中文标题":"通过非参数实例区分进行无监督特征学习","摘要翻译":"在带有注释类别标签的数据上训练的神经网络分类器也可以捕捉类别之间的明显视觉相似性，而无需被指示这样做。我们研究这一观察是否可以扩展到传统的监督学习领域之外：我们是否可以通过仅仅要求特征能够区分个体实例而不是类别，来学习一个捕捉实例之间明显相似性的良好特征表示？我们将这一直觉表述为实例级别的非参数分类问题，并使用噪声对比估计来解决由大量实例类别带来的计算挑战。我们的实验结果表明，在无监督学习设置下，我们的方法在ImageNet分类上大幅超越了现有技术。我们的方法在随着更多训练数据和更好的网络架构而持续提高测试性能方面也表现突出。通过微调学习到的特征，我们进一步在半监督学习和对象检测任务中获得了竞争性的结果。我们的非参数模型非常紧凑：每张图像128个特征，我们的方法仅需600MB存储空间即可存储一百万张图像，实现了运行时的快速最近邻检索。","领域":"特征学习/无监督学习/图像分类","问题":"如何在无监督学习环境中学习捕捉实例间相似性的特征表示","动机":"探索是否可以通过要求特征能够区分个体实例而不是类别，来学习一个捕捉实例之间明显相似性的良好特征表示","方法":"将问题表述为实例级别的非参数分类问题，并使用噪声对比估计来解决计算挑战","关键词":["非参数实例区分","无监督特征学习","噪声对比估计","ImageNet分类","半监督学习","对象检测"],"涉及的技术概念":{"非参数实例区分":"一种不依赖于预设参数模型来区分不同实例的方法","噪声对比估计":"一种用于估计概率分布的技术，通过对比噪声样本和真实样本来学习模型参数","ImageNet分类":"使用ImageNet数据集进行的图像分类任务，ImageNet是一个大规模视觉识别挑战数据集","半监督学习":"一种结合了少量标注数据和大量未标注数据进行学习的方法","对象检测":"在图像中识别和定位特定对象的技术"}},{"order":386,"title":"Multi-Task Adversarial Network for Disentangled Feature Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.html","abstract":"We address the problem of image feature learning for the applications where multiple factors exist in the image generation process and only some factors are of our interest. We present a novel multi-task adversarial network based on an encoder-discriminator-generator architecture. The encoder extracts a disentangled feature representation for the factors of interest. The discriminators classify each of the factors as individual tasks. The encoder and the discriminators are trained cooperatively on factors of interest, but in an adversarial way on factors of distraction. The generator provides further regularization on the learned feature by reconstructing images with shared factors as the input image. We design a new optimization scheme to stabilize the adversarial optimization process when multiple distributions need to be aligned. The experiments on face recognition and font recognition tasks show that our method outperforms the state-of-the-art methods in terms of both recognizing the factors of interest and generalization to images with unseen variations.","中文标题":"多任务对抗网络用于解耦特征学习","摘要翻译":"我们解决了在图像生成过程中存在多个因素，但只有部分因素是我们感兴趣的应用中的图像特征学习问题。我们提出了一种基于编码器-判别器-生成器架构的新型多任务对抗网络。编码器提取感兴趣因素的解耦特征表示。判别器将每个因素分类为独立任务。编码器和判别器在感兴趣因素上合作训练，但在干扰因素上以对抗方式训练。生成器通过重建与输入图像共享因素的图像，对学习到的特征提供进一步的规范化。我们设计了一种新的优化方案，以在需要对齐多个分布时稳定对抗优化过程。在人脸识别和字体识别任务上的实验表明，我们的方法在识别感兴趣因素和对未见变化的图像泛化方面优于最先进的方法。","领域":"人脸识别/字体识别/特征学习","问题":"在图像生成过程中存在多个因素，但只有部分因素是我们感兴趣的应用中的图像特征学习问题","动机":"解决在图像生成过程中存在多个因素，但只有部分因素是我们感兴趣的应用中的图像特征学习问题","方法":"提出了一种基于编码器-判别器-生成器架构的新型多任务对抗网络，编码器提取感兴趣因素的解耦特征表示，判别器将每个因素分类为独立任务，编码器和判别器在感兴趣因素上合作训练，但在干扰因素上以对抗方式训练，生成器通过重建与输入图像共享因素的图像，对学习到的特征提供进一步的规范化，设计了一种新的优化方案，以在需要对齐多个分布时稳定对抗优化过程","关键词":["多任务学习","对抗网络","特征解耦","图像生成","人脸识别","字体识别"],"涉及的技术概念":"多任务对抗网络、编码器-判别器-生成器架构、特征解耦、对抗训练、图像重建、优化方案"},{"order":387,"title":"Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.html","abstract":"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.","中文标题":"从合成数据中学习：解决语义分割中的领域转移问题","摘要翻译":"视觉领域适应是计算机视觉中一个极其重要的问题。以往的方法表明，即使是深度神经网络也难以跨领域转移学习到信息丰富的表示。对于获取手工标注数据极其困难和繁琐的任务，这个问题更为严重。在这项工作中，我们专注于使分割网络在合成和真实领域之间学习的表示适应。与以往使用简单对抗目标或超像素信息来辅助过程的方法不同，我们提出了一种基于生成对抗网络（GANs）的方法，使嵌入在学习到的特征空间中更接近。为了展示我们方法的通用性和可扩展性，我们展示了我们可以在两个具有挑战性的合成到真实领域适应场景中实现最先进的结果。额外的探索性实验表明，我们的方法：（1）能够泛化到未见过的领域，并且（2）导致源和目标分布的对齐得到改善。","领域":"语义分割/领域适应/生成对抗网络","问题":"解决语义分割任务中合成数据与真实数据之间的领域转移问题","动机":"由于获取手工标注数据极其困难和繁琐，需要一种方法能够有效地使分割网络在合成和真实领域之间学习的表示适应，以解决领域转移问题","方法":"提出了一种基于生成对抗网络（GANs）的方法，通过使嵌入在学习到的特征空间中更接近，来适应分割网络在合成和真实领域之间学习的表示","关键词":["语义分割","领域适应","生成对抗网络"],"涉及的技术概念":"领域适应是指在机器学习中，模型在一个领域（源领域）上训练后，能够适应到另一个不同但相关的领域（目标领域）上的能力。生成对抗网络（GANs）是一种深度学习模型，由两部分组成：生成器和判别器，通过对抗过程来生成数据。"},{"order":388,"title":"Empirical Study of the Topology and Geometry of Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fawzi_Empirical_Study_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fawzi_Empirical_Study_of_CVPR_2018_paper.html","abstract":"The goal of this paper is to analyze the geometric properties of deep neural network image classifiers in the input space. We specifically study the topology of classification regions created by deep networks, as well as their associated decision boundary. Through a systematic empirical study, we show that state-of-the-art deep nets learn connected classification regions, and that the decision boundary in the vicinity of datapoints is flat along most directions. We further draw an essential connection between two seemingly unrelated properties of deep networks: their sensitivity to additive perturbations of the inputs, and the curvature of their decision boundary. The directions where the decision boundary is curved in fact characterize the directions to which the classifier is the most vulnerable. We finally leverage a fundamental asymmetry in the curvature of the decision boundary of deep nets, and propose a method to discriminate between original images, and images perturbed with small adversarial examples. We show the effectiveness of this purely geometric approach for detecting small adversarial perturbations in images, and for recovering the labels of perturbed images.","中文标题":"深度网络拓扑与几何特性的实证研究","摘要翻译":"本文的目标是分析深度神经网络图像分类器在输入空间中的几何特性。我们特别研究了由深度网络创建的分类区域的拓扑结构，以及它们相关的决策边界。通过系统的实证研究，我们展示了最先进的深度网络学习到的连接分类区域，以及数据点附近决策边界在大多数方向上是平坦的。我们进一步揭示了深度网络两个看似无关的特性之间的本质联系：它们对输入加性扰动的敏感性，以及它们决策边界的曲率。决策边界弯曲的方向实际上表征了分类器最脆弱的方向。最后，我们利用深度网络决策边界曲率的基本不对称性，提出了一种方法来区分原始图像和用小对抗样本扰动的图像。我们展示了这种纯几何方法在检测图像中的小对抗扰动以及恢复扰动图像标签方面的有效性。","领域":"对抗样本检测/图像分类/网络拓扑","问题":"分析深度神经网络图像分类器在输入空间中的几何特性，特别是分类区域的拓扑结构和决策边界","动机":"理解深度网络如何形成分类区域及其决策边界，以及这些特性如何影响网络对输入扰动的敏感性和对抗样本的检测","方法":"通过系统的实证研究分析深度网络的几何特性，提出利用决策边界曲率的不对称性来检测对抗样本的方法","关键词":["对抗样本检测","图像分类","网络拓扑"],"涉及的技术概念":"深度神经网络的几何特性、分类区域的拓扑结构、决策边界、对抗样本、输入加性扰动、决策边界曲率"},{"order":389,"title":"Boosting Domain Adaptation by Discovering Latent Domains","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.html","abstract":"Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, i.e. latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture: (i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin.","中文标题":"通过发现潜在领域提升领域适应","摘要翻译":"当前基于深度架构的领域适应（DA）方法假设源样本来自单一分布。然而，实际上大多数数据集可以被视为多个领域的混合物。在这些情况下，利用单源DA方法学习目标分类器可能会导致次优，如果不是差的结果。此外，在许多应用中，手动为所有源数据点提供领域标签是困难的，即应自动发现潜在领域。本文介绍了一种新颖的卷积神经网络（CNN）架构，它（i）自动发现视觉数据集中的潜在领域，并且（ii）利用这些信息来学习鲁棒的目标分类器。我们的方法基于引入两个主要组件，这两个组件可以嵌入到任何现有的CNN架构中：（i）一个侧分支，自动计算源样本到潜在领域的分配，以及（ii）新颖的层，利用领域成员信息适当地将CNN内部特征表示的分布对齐到参考分布。我们在公开可用的数据集上测试了我们的方法，显示它大大优于最先进的多源DA方法。","领域":"领域适应/卷积神经网络/视觉数据集","问题":"在领域适应中，源样本通常来自多个分布，而现有方法假设源样本来自单一分布，导致学习目标分类器时效果不佳。","动机":"为了解决现有领域适应方法在处理多分布源样本时的不足，以及自动发现潜在领域的需求。","方法":"引入一种新颖的卷积神经网络架构，包括自动计算源样本到潜在领域分配的侧分支和利用领域成员信息对齐CNN内部特征表示分布的新颖层。","关键词":["领域适应","卷积神经网络","潜在领域","视觉数据集","目标分类器"],"涉及的技术概念":"领域适应（Domain Adaptation, DA）是指在源领域和目标领域分布不同的情况下，利用源领域的数据来提高目标领域模型性能的技术。卷积神经网络（Convolutional Neural Network, CNN）是一种深度学习模型，特别适用于处理图像数据。潜在领域指的是数据集中未被明确标注但存在的不同分布或类别。"},{"order":390,"title":"Shape From Shading Through Shape Evolution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Shape_From_Shading_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Shape_From_Shading_CVPR_2018_paper.html","abstract":"In this paper, we address the shape-from-shading problem by training deep networks with synthetic images. Unlike conventional approaches that combine deep learning and synthetic imagery, we propose an approach that does not need any external shape dataset to render synthetic images. Our approach consists of two synergistic processes: the evolution of complex shapes from simple primitives, and the training of a deep network for shape-from-shading. The evolution generates better shapes guided by the network training, while the training improves by using the evolved shapes. We show that our approach achieves state-of-the-art performance on a shape-from-shading benchmark.","中文标题":"通过形状演化从阴影中恢复形状","摘要翻译":"在本文中，我们通过使用合成图像训练深度网络来解决从阴影中恢复形状的问题。与将深度学习和合成图像结合的传统方法不同，我们提出了一种不需要任何外部形状数据集来渲染合成图像的方法。我们的方法包括两个协同过程：从简单原语演化出复杂形状，以及训练一个用于从阴影中恢复形状的深度网络。演化过程在网络训练的指导下生成更好的形状，而训练过程则通过使用演化出的形状得到改进。我们展示了我们的方法在从阴影中恢复形状的基准测试中达到了最先进的性能。","领域":"三维重建/深度学习/计算机图形学","问题":"从阴影中恢复形状的问题","动机":"解决传统方法需要外部形状数据集来渲染合成图像的限制","方法":"提出了一种不需要外部形状数据集的方法，通过两个协同过程：形状演化和深度网络训练","关键词":["三维重建","深度学习","计算机图形学"],"涉及的技术概念":"本文涉及的技术概念包括从阴影中恢复形状（Shape From Shading）、深度网络训练、合成图像生成、形状演化过程。"},{"order":391,"title":"Weakly Supervised Instance Segmentation Using Class Peak Response","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Weakly_Supervised_Instance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Weakly_Supervised_Instance_CVPR_2018_paper.html","abstract":"Weakly supervised instance segmentation with image-level labels, instead of expensive pixel-level masks, remains unexplored. In this paper, we tackle this challenging problem by exploiting class peak responses to enable a classification network for instance mask extraction. With image labels supervision only, CNN classifiers in a fully convolutional manner can produce class response maps, which specify classification confidence at each image location. We observed that local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside each instance. Motivated by this, we first design a process to stimulate peaks to emerge from a class response map. The emerged peaks are then back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. We refer to the above maps generated from class peak responses as Peak Response Maps (PRMs). PRMs provide a fine-detailed instance-level representation, which allows instance masks to be extracted even with some off-the-shelf methods. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task. Extensive experiments show that our method also boosts weakly supervised pointwise localization as well as semantic segmentation performance, and reports state-of-the-art results on popular benchmarks, including PASCAL VOC 2012 and MS COCO.","中文标题":"使用类峰值响应的弱监督实例分割","摘要翻译":"使用图像级标签而非昂贵的像素级掩码进行弱监督实例分割仍然是一个未被充分探索的领域。在本文中，我们通过利用类峰值响应来解决这一挑战性问题，使得分类网络能够进行实例掩码提取。仅通过图像标签监督，以全卷积方式工作的CNN分类器可以生成类响应图，这些图指定了每个图像位置的分类置信度。我们观察到，类响应图中的局部最大值，即峰值，通常对应于每个实例内部的强视觉线索。受此启发，我们首先设计了一个过程来刺激峰值从类响应图中出现。然后，这些出现的峰值被反向传播并有效地映射到每个对象实例的高信息量区域，如实例边界。我们将上述从类峰值响应生成的图称为峰值响应图（PRMs）。PRMs提供了精细的实例级表示，这使得即使使用一些现成的方法也能提取实例掩码。据我们所知，我们首次报告了具有挑战性的图像级监督实例分割任务的结果。大量实验表明，我们的方法还提高了弱监督点定位以及语义分割的性能，并在包括PASCAL VOC 2012和MS COCO在内的流行基准上报告了最先进的结果。","领域":"实例分割/弱监督学习/图像分析","问题":"如何在仅使用图像级标签的情况下进行实例分割","动机":"减少对昂贵像素级掩码的依赖，探索使用图像级标签进行实例分割的可能性","方法":"利用类峰值响应刺激分类网络生成实例掩码，通过峰值响应图（PRMs）提供实例级表示","关键词":["实例分割","弱监督学习","图像分析","峰值响应图","CNN分类器"],"涉及的技术概念":{"弱监督学习":"一种机器学习方法，其中训练数据仅包含部分标签信息，而不是详细的标注。","实例分割":"计算机视觉中的一项任务，旨在识别图像中的每个对象实例并为每个实例分配一个像素级掩码。","类峰值响应":"在类响应图中，局部最大值（峰值）通常对应于图像中对象实例的强视觉线索。","峰值响应图（PRMs）":"从类峰值响应生成的图，提供了精细的实例级表示，有助于实例掩码的提取。","CNN分类器":"卷积神经网络分类器，用于图像分类任务，能够生成类响应图。"}},{"order":392,"title":"Collaborative and Adversarial Network for Unsupervised Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.html","abstract":"In this paper, we propose a new unsupervised domain adaptation approach called Collaborative and Adversarial Network (CAN) through domain-collaborative and domain-adversarial training of neural networks. We use several domain classifiers on multiple CNN feature extraction layers/blocks, in which each domain classifier is connected to the hidden representations from one block and one loss function is defined based on the hidden presentation and the domain labels (e.g., source and target). We design a new loss function by integrating the losses from all blocks in order to learn informative representations from lower layers through collaborative learning and learn uninformative representations from higher layers through adversarial learning. We further extend our CAN method as Incremental CAN (iCAN), in which we iteratively select a set of pseudo-labelled target samples based on the image classifier and the last domain classifier from the previous training epoch and re-train our CAN model using the enlarged training set.  Comprehensive experiments on two benchmark datasets Office and ImageCLEF-DA clearly demonstrate the effectiveness of our newly proposed approaches CAN and iCAN for unsupervised domain adaptation.","中文标题":"协作与对抗网络用于无监督领域适应","摘要翻译":"本文提出了一种新的无监督领域适应方法，称为协作与对抗网络（CAN），通过神经网络的领域协作和领域对抗训练。我们在多个CNN特征提取层/块上使用多个领域分类器，其中每个领域分类器连接到一个块的隐藏表示，并基于隐藏表示和领域标签（例如，源和目标）定义一个损失函数。我们通过整合所有块的损失设计了一个新的损失函数，以通过协作学习从较低层学习信息丰富的表示，并通过对抗学习从较高层学习无信息的表示。我们进一步将我们的CAN方法扩展为增量CAN（iCAN），在其中我们基于图像分类器和前一个训练周期的最后一个领域分类器迭代选择一组伪标记的目标样本，并使用扩大的训练集重新训练我们的CAN模型。在两个基准数据集Office和ImageCLEF-DA上的综合实验清楚地证明了我们新提出的CAN和iCAN方法在无监督领域适应中的有效性。","领域":"无监督学习/领域适应/神经网络","问题":"解决无监督领域适应问题，即在没有目标领域标签的情况下，将源领域的知识迁移到目标领域。","动机":"为了在没有目标领域标签的情况下，有效地将源领域的知识迁移到目标领域，提高模型在目标领域的性能。","方法":"提出协作与对抗网络（CAN），通过领域协作和领域对抗训练神经网络，设计新的损失函数整合所有块的损失，以学习信息丰富和无信息的表示，并扩展为增量CAN（iCAN）方法，通过迭代选择伪标记的目标样本并重新训练模型。","关键词":["无监督学习","领域适应","神经网络","协作学习","对抗学习"],"涉及的技术概念":"协作与对抗网络（CAN）是一种无监督领域适应方法，通过神经网络的领域协作和领域对抗训练，设计新的损失函数整合所有块的损失，以学习信息丰富和无信息的表示。增量CAN（iCAN）是CAN的扩展，通过迭代选择伪标记的目标样本并重新训练模型，以提高模型在目标领域的性能。"},{"order":393,"title":"Environment Upgrade Reinforcement Learning for Non-Differentiable Multi-Stage Pipelines","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Environment_Upgrade_Reinforcement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Environment_Upgrade_Reinforcement_CVPR_2018_paper.html","abstract":"Recent advances in multi-stage algorithms have shown great promise, but two important problems still remain. First of all, at inference time, information can't feed back from downstream to upstream. Second, at training time, end-to-end training is not possible if the overall pipeline involves non-differentiable functions, and so different stages can't be jointly optimized. In this paper, we propose a novel environment upgrade reinforcement learning framework to solve the feedback and joint optimization problems. Our framework re-links the downstream stage to the upstream stage by a reinforcement learning agent. While training the agent to improve final performance by refining the upstream stage's output, we also upgrade the downstream stage (environment) according to the agent's policy. In this way, agent policy and environment are jointly optimized. We propose a training algorithm for this framework to address the different training demands of agent and environment. Experiments on instance segmentation and human pose estimation demonstrate the effectiveness of the proposed framework.","中文标题":"环境升级强化学习用于不可微分多阶段管道","摘要翻译":"多阶段算法的最新进展显示出巨大的潜力，但仍存在两个重要问题。首先，在推理时，信息无法从下游反馈到上游。其次，在训练时，如果整个管道涉及不可微分函数，则无法进行端到端训练，因此不同阶段无法联合优化。在本文中，我们提出了一种新颖的环境升级强化学习框架来解决反馈和联合优化问题。我们的框架通过强化学习代理重新连接下游阶段到上游阶段。在训练代理通过改进上游阶段的输出来提高最终性能的同时，我们还根据代理的策略升级下游阶段（环境）。这样，代理策略和环境被联合优化。我们为这个框架提出了一种训练算法，以解决代理和环境的不同训练需求。在实例分割和人体姿态估计上的实验证明了所提出框架的有效性。","领域":"实例分割/人体姿态估计/强化学习","问题":"解决多阶段算法中信息无法从下游反馈到上游以及涉及不可微分函数时无法进行端到端训练的问题","动机":"为了克服多阶段算法在推理和训练时的限制，实现信息的有效反馈和不同阶段的联合优化","方法":"提出了一种环境升级强化学习框架，通过强化学习代理重新连接下游阶段到上游阶段，并联合优化代理策略和环境","关键词":["实例分割","人体姿态估计","强化学习","环境升级","联合优化"],"涉及的技术概念":"多阶段算法、不可微分函数、端到端训练、强化学习代理、环境升级、联合优化"},{"order":394,"title":"Teaching Categories to Human Learners With Visual Explanations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Aodha_Teaching_Categories_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Aodha_Teaching_Categories_to_CVPR_2018_paper.html","abstract":"We study the problem of computer-assisted teaching with explanations.   Conventional approaches for machine teaching typically only provide feedback at the instance level e.g., the category or label of the instance.   However, it is intuitive that clear explanations from a knowledgeable teacher can significantly improve a student's ability to learn a new concept.  To address these existing limitations, we propose a teaching framework that provides interpretable explanations as feedback and models how the learner incorporates this additional information.   In the case of images, we show that we can automatically generate explanations that highlight the parts of the image that are responsible for the class label. Experiments on human learners illustrate that, on average, participants achieve better test set performance on challenging categorization tasks when taught with our interpretable approach compared to existing methods.","中文标题":"通过视觉解释向人类学习者教授类别","摘要翻译":"我们研究了带有解释的计算机辅助教学问题。传统的机器教学方法通常只在实例级别提供反馈，例如实例的类别或标签。然而，直观上，来自知识渊博的教师的清晰解释可以显著提高学生学习新概念的能力。为了解决这些现有的限制，我们提出了一个教学框架，该框架提供可解释的解释作为反馈，并模拟学习者如何整合这些额外信息。在图像的情况下，我们展示了可以自动生成解释，突出显示图像中负责类别标签的部分。对人类学习者的实验表明，平均而言，与现有方法相比，使用我们的可解释方法教学的参与者在具有挑战性的分类任务上取得了更好的测试集表现。","领域":"教育技术/人工智能辅助学习/视觉解释","问题":"如何通过提供可解释的视觉反馈来提高人类学习者在分类任务上的表现","动机":"传统的机器教学方法缺乏提供解释的能力，而清晰的教学解释可以显著提高学习效率","方法":"提出一个教学框架，该框架提供可解释的解释作为反馈，并模拟学习者如何整合这些额外信息，特别是在图像分类任务中自动生成解释","关键词":["计算机辅助教学","视觉解释","分类任务"],"涉及的技术概念":"机器教学、可解释的反馈、图像分类、自动生成解释"},{"order":395,"title":"Density Adaptive Point Set Registration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lawin_Density_Adaptive_Point_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lawin_Density_Adaptive_Point_CVPR_2018_paper.html","abstract":"Probabilistic methods for point set registration have demonstrated competitive results in recent years. These techniques estimate a probability distribution model of the point clouds. While such a representation has shown promise, it is highly sensitive to variations in the density of 3D points. This fundamental problem is primarily caused by changes in the sensor location across point sets. We revisit the foundations of the probabilistic registration paradigm. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework. Our density-adaptive registration successfully handles severe density variations commonly encountered in terrestrial Lidar applications. We perform extensive experiments on several challenging real-world Lidar datasets. The results demonstrate that our approach outperforms state-of-the-art probabilistic methods for multi-view registration, without the need of re-sampling.","中文标题":"密度自适应点集配准","摘要翻译":"近年来，概率方法在点集配准方面展示了竞争性的结果。这些技术估计了点云的概率分布模型。虽然这种表示方法显示出了前景，但它对3D点密度变化非常敏感。这个基本问题主要是由点集间传感器位置的变化引起的。我们重新审视了概率配准范式的基础。与之前的工作相反，我们将场景的底层结构建模为潜在概率分布，从而诱导出对点集密度变化的不变性。场景的概率模型和配准参数都是通过在一个基于期望最大化的框架中最小化Kullback-Leibler散度来推断的。我们的密度自适应配准成功地处理了地面激光雷达应用中常见的严重密度变化。我们在几个具有挑战性的真实世界激光雷达数据集上进行了广泛的实验。结果表明，我们的方法在不需要重新采样的情况下，优于多视图配准的最先进概率方法。","领域":"激光雷达数据处理/3D点云配准/概率模型","问题":"处理3D点云配准中对点集密度变化敏感的问题","动机":"解决由于传感器位置变化导致的点集密度变化对配准结果的影响","方法":"将场景的底层结构建模为潜在概率分布，通过最小化Kullback-Leibler散度在期望最大化框架中推断场景的概率模型和配准参数","关键词":["3D点云配准","概率模型","密度自适应","激光雷达"],"涉及的技术概念":"Kullback-Leibler散度用于衡量两个概率分布之间的差异，期望最大化是一种迭代优化算法，用于在统计模型中寻找参数的最大似然估计。"},{"order":396,"title":"Left-Right Comparative Recurrent Model for Stereo Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jie_Left-Right_Comparative_Recurrent_CVPR_2018_paper.html","abstract":"Leveraging the disparity information from both  left and right views is crucial for stereo disparity estimation. Left-right consistency check is an effective way to enhance the disparity estimation by referring to the information from the opposite view. However, the conventional left-right consistency check is an isolated post-processing step and heavily hand-crafted. This paper proposes a novel left-right comparative recurrent model to perform left-right consistency checking jointly with   disparity estimation. At each recurrent step, the model produces disparity results for both views, and then performs online left-right comparison to identify the mismatched regions which may probably contain erroneously labeled pixels. A soft attention mechanism is introduced, which employs the learned error maps for better guiding the model to selectively focus on refining the unreliable regions at the next recurrent step. In this way, the generated disparity maps are progressively improved by the proposed recurrent model. Extensive evaluations on  KITTI 2015, Scene Flow and Middlebury benchmarks validate the effectiveness of our model,  demonstrating that state-of-the-art stereo disparity estimation results can be achieved by this new model.","中文标题":"左右对比循环模型用于立体匹配","摘要翻译":"利用左右视图的视差信息对于立体视差估计至关重要。左右一致性检查是通过参考相反视图的信息来增强视差估计的有效方法。然而，传统的左右一致性检查是一个孤立的后期处理步骤，且高度手工制作。本文提出了一种新颖的左右对比循环模型，以联合执行左右一致性检查和视差估计。在每个循环步骤中，模型为两个视图生成视差结果，然后执行在线左右比较以识别可能包含错误标记像素的不匹配区域。引入了一种软注意力机制，该机制利用学习到的误差图更好地指导模型在下一个循环步骤中选择性地关注于精炼不可靠区域。通过这种方式，生成的视差图由所提出的循环模型逐步改进。在KITTI 2015、Scene Flow和Middlebury基准上的广泛评估验证了我们模型的有效性，证明了这种新模型可以实现最先进的立体视差估计结果。","领域":"立体视觉/视差估计/注意力机制","问题":"传统的左右一致性检查是一个孤立的后期处理步骤，且高度手工制作，无法有效联合执行左右一致性检查和视差估计。","动机":"为了提高立体视差估计的准确性和效率，需要一种能够联合执行左右一致性检查和视差估计的模型。","方法":"提出了一种新颖的左右对比循环模型，该模型在每个循环步骤中为两个视图生成视差结果，并执行在线左右比较以识别不匹配区域。引入软注意力机制，利用学习到的误差图指导模型在下一个循环步骤中选择性地关注于精炼不可靠区域。","关键词":["立体视觉","视差估计","注意力机制"],"涉及的技术概念":"左右一致性检查是一种通过参考相反视图的信息来增强视差估计的方法。软注意力机制是一种利用学习到的误差图来指导模型选择性地关注于精炼不可靠区域的机制。"},{"order":397,"title":"Im2Pano3D: Extrapolating 360° Structure and Semantics Beyond the Field of View","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Im2Pano3D_Extrapolating_360deg_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Im2Pano3D_Extrapolating_360deg_CVPR_2018_paper.html","abstract":"We present Im2Pano3D, a convolutional neural network that generates a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360 panoramic view of an indoor scene when given only a partial observation ( <=50%) in the form of an RGB-D image. To make this possible, Im2Pano3D leverages strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To ease the prediction of 3D structure, we propose to parameterize 3D surfaces with their plane equations and train the model to predict these parameters directly. To provide meaningful training supervision, we make use of multiple loss functions that consider both pixel level accuracy and global context consistency. Experiments demonstrate that Im2Pano3D is able to predict the semantics and 3D structure of the unobserved scene with more than 56% pixel accuracy and less than 0.52m average distance error, which is significantly better than alternative approaches.","中文标题":"Im2Pano3D：从视野外推360°结构和语义","摘要翻译":"我们提出了Im2Pano3D，这是一个卷积神经网络，当仅给出部分观察（<=50%）的RGB-D图像时，它能够生成室内场景的全360°全景视图的3D结构的密集预测和语义标签的概率分布。为了实现这一点，Im2Pano3D利用了从大规模合成和真实世界室内场景中学习到的强上下文先验。为了简化3D结构的预测，我们提出用平面方程参数化3D表面，并训练模型直接预测这些参数。为了提供有意义的训练监督，我们使用了多个损失函数，这些函数考虑了像素级精度和全局上下文一致性。实验表明，Im2Pano3D能够以超过56%的像素精度和小于0.52米的平均距离误差预测未观察场景的语义和3D结构，这显著优于其他方法。","领域":"室内场景理解/3D重建/语义分割","问题":"从部分RGB-D图像预测室内场景的全360°全景视图的3D结构和语义标签","动机":"为了从有限的观察中推断出完整的室内场景的3D结构和语义信息，以支持更广泛的应用，如虚拟现实和机器人导航。","方法":"利用卷积神经网络Im2Pano3D，通过从大规模合成和真实世界室内场景中学习到的强上下文先验，参数化3D表面为平面方程，并使用多个损失函数进行训练监督。","关键词":["卷积神经网络","3D结构预测","语义标签","全景视图","RGB-D图像"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据。","3D结构预测":"预测场景中物体的三维形状和布局。","语义标签":"为图像中的每个像素分配一个类别标签，如“桌子”、“椅子”等。","全景视图":"一种能够捕捉360°视角的图像或视频。","RGB-D图像":"包含颜色（RGB）和深度（D）信息的图像，用于提供场景的视觉和几何信息。","平面方程":"用于描述3D空间中平面的数学方程。","损失函数":"用于评估模型预测与真实值之间差异的函数，是训练深度学习模型的关键部分。"}},{"order":398,"title":"Polarimetric Dense Monocular SLAM","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Polarimetric_Dense_Monocular_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Polarimetric_Dense_Monocular_CVPR_2018_paper.html","abstract":"This paper presents a novel polarimetric dense monocular SLAM (PDMS) algorithm based on a polarization camera. The algorithm exploits both photometric and polarimetric light information to produce more accurate and complete geometry. The polarimetric information allows us to recover the azimuth angle of surface normals from each video frame to facilitate dense reconstruction, especially at textureless or specular regions. There are two challenges in our approach: 1) surface azimuth angles from the polarization camera are very noisy; and 2) we need a near real-time solution for SLAM. Previous successful methods on polarimetric multi-view stereo are offline and require manually pre-segmented object masks to suppress the effects of erroneous angle information along boundaries. Our fully automatic approach efficiently iterates azimuth-based depth propagations, two-view depth consistency check, and depth optimization to produce a depthmap in real-time, where all the algorithmic steps are carefully designed to enable a GPU implementation. To our knowledge, this paper is the first to propose a photometric and polarimetric method for dense SLAM. We have qualitatively and quantitatively evaluated our algorithm against a few of competing methods, demonstrating the superior performance on various indoor and outdoor scenes.","中文标题":"偏振密集单目SLAM","摘要翻译":"本文提出了一种基于偏振相机的新型偏振密集单目SLAM（PDMS）算法。该算法利用光度和偏振光信息来生成更准确和完整的几何形状。偏振信息使我们能够从每个视频帧中恢复表面法线的方位角，以促进密集重建，特别是在无纹理或镜面区域。我们的方法面临两个挑战：1）来自偏振相机的表面方位角非常嘈杂；2）我们需要一个近实时的SLAM解决方案。以前在偏振多视图立体视觉方面成功的方案是离线的，并且需要手动预分割对象掩码以抑制沿边界的错误角度信息的影响。我们的全自动方法有效地迭代基于方位的深度传播、双视图深度一致性检查和深度优化，以实时生成深度图，其中所有算法步骤都经过精心设计，以实现GPU实现。据我们所知，本文是第一个提出用于密集SLAM的光度和偏振方法的研究。我们已经对我们的算法与几种竞争方法进行了定性和定量评估，展示了在各种室内和室外场景中的优越性能。","领域":"SLAM/三维重建/偏振成像","问题":"在无纹理或镜面区域实现准确和完整的密集三维重建","动机":"解决传统方法在处理无纹理或镜面区域时的局限性，以及实现实时SLAM的需求","方法":"利用偏振相机捕获的光度和偏振信息，通过迭代基于方位的深度传播、双视图深度一致性检查和深度优化，实时生成深度图","关键词":["偏振相机","密集重建","实时SLAM"],"涉及的技术概念":"偏振密集单目SLAM（PDMS）算法利用偏振相机捕获的光度和偏振信息，通过特定的算法步骤（如基于方位的深度传播、双视图深度一致性检查和深度优化）实现实时密集三维重建。这种方法特别适用于处理无纹理或镜面区域，解决了传统方法在这些区域的局限性。"},{"order":399,"title":"A Unifying Contrast Maximization Framework for Event Cameras, With Applications to Motion, Depth, and Optical Flow Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gallego_A_Unifying_Contrast_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gallego_A_Unifying_Contrast_CVPR_2018_paper.html","abstract":"We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.","中文标题":"事件相机的统一对比度最大化框架及其在运动、深度和光流估计中的应用","摘要翻译":"我们提出了一个统一的框架，用于解决事件相机在计算机视觉中的几个问题：运动、深度和光流估计。我们框架的主要思想是通过最大化一个目标函数：变形事件图像的对比度，来找到与事件数据最佳对齐的图像平面上的点轨迹。我们的方法隐式地处理事件之间的数据关联，因此不依赖于场景的额外外观信息。除了准确恢复问题的运动参数外，我们的框架还生成了具有高动态范围的运动校正边缘图像，这些图像可用于进一步的场景分析。所提出的方法不仅简单，而且据我们所知，它是第一个能够成功应用于事件相机如此多样化的重要视觉任务的方法。","领域":"事件相机/运动估计/光流估计","问题":"解决事件相机在运动、深度和光流估计中的问题","动机":"开发一个统一的框架，以解决事件相机在多个计算机视觉任务中的应用问题，提高运动参数恢复的准确性和生成高质量的运动校正边缘图像","方法":"通过最大化变形事件图像的对比度，找到与事件数据最佳对齐的图像平面上的点轨迹，隐式处理事件之间的数据关联","关键词":["事件相机","运动估计","光流估计","深度估计","对比度最大化"],"涉及的技术概念":"事件相机是一种能够捕捉场景中亮度变化的相机，与传统相机不同，它只在检测到亮度变化时生成数据。对比度最大化是一种优化技术，用于提高图像中感兴趣区域的对比度，以便更好地进行分析。运动估计、深度估计和光流估计是计算机视觉中的基本任务，分别涉及从图像序列中估计物体的运动、场景的深度信息以及图像中每个像素的运动矢量。"},{"order":400,"title":"Modeling Facial Geometry Using Compositional VAEs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bagautdinov_Modeling_Facial_Geometry_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bagautdinov_Modeling_Facial_Geometry_CVPR_2018_paper.html","abstract":"We propose a method for learning non-linear face geometry  representations using deep generative models.  Our model is a variational autoencoder with multiple levels of  hidden variables where lower layers capture global geometry  and higher ones encode more local deformations.  Based on that, we propose a new parameterization of facial  geometry that naturally decomposes the structure of the human face  into a set of semantically meaningful levels of detail.  This parameterization enables us to do model fitting while  capturing varying level of detail under different types of geometrical  constraints.","中文标题":"使用组合变分自编码器建模面部几何","摘要翻译":"我们提出了一种使用深度生成模型学习非线性面部几何表示的方法。我们的模型是一个具有多层次隐藏变量的变分自编码器，其中较低层捕捉全局几何，较高层编码更局部的变形。基于此，我们提出了一种新的面部几何参数化方法，自然地将人脸结构分解为一组具有语义意义的细节层次。这种参数化使我们能够在捕捉不同类型几何约束下的不同细节层次的同时进行模型拟合。","领域":"面部几何建模/生成模型/变分自编码器","问题":"如何有效地建模和参数化非线性面部几何","动机":"为了更准确地捕捉和表示人脸的几何结构，包括全局和局部的细节，以便于在不同几何约束下进行模型拟合。","方法":"采用多层次隐藏变量的变分自编码器，通过较低层捕捉全局几何，较高层编码局部变形，提出新的面部几何参数化方法。","关键词":["面部几何建模","变分自编码器","生成模型"],"涉及的技术概念":"变分自编码器（VAE）是一种生成模型，能够学习数据的潜在表示。多层次隐藏变量指的是模型中有多个层次的潜在变量，每个层次负责捕捉数据的不同级别的特征。参数化是指将复杂的数据结构转换为一系列参数的过程，以便于进一步的处理和分析。"},{"order":401,"title":"Tangent Convolutions for Dense Prediction in 3D","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tatarchenko_Tangent_Convolutions_for_CVPR_2018_paper.html","abstract":"We present an approach to semantic scene analysis using deep convolutional networks. Our approach is based on tangent convolutions - a new construction for convolutional networks on 3D data. In contrast to volumetric approaches, our method operates directly on surface geometry. Crucially, the construction is applicable to unstructured point clouds and other noisy real-world data. We show that tangent convolutions can be evaluated efficiently on large-scale point clouds with millions of points. Using tangent convolutions, we design a deep fully-convolutional network for semantic segmentation of 3D point clouds, and apply it to challenging real-world datasets of indoor and outdoor 3D environments. Experimental results show that the presented approach outperforms other recent deep network constructions in detailed analysis of large 3D scenes.","中文标题":"用于3D密集预测的切面卷积","摘要翻译":"我们提出了一种使用深度卷积网络进行语义场景分析的方法。我们的方法基于切面卷积——一种在3D数据上的卷积网络新构建。与体积方法相比，我们的方法直接操作于表面几何。关键的是，这种构建适用于非结构化点云和其他噪声现实世界数据。我们展示了切面卷积可以在包含数百万点的大规模点云上高效评估。使用切面卷积，我们设计了一个深度全卷积网络用于3D点云的语义分割，并将其应用于室内和室外3D环境的挑战性现实世界数据集。实验结果表明，所提出的方法在大型3D场景的详细分析中优于其他最近的深度网络构建。","领域":"3D场景分析/语义分割/点云处理","问题":"如何在3D数据上进行有效的语义场景分析","动机":"为了在非结构化点云和噪声现实世界数据上实现更高效的3D场景分析","方法":"提出了一种新的切面卷积方法，并设计了一个深度全卷积网络用于3D点云的语义分割","关键词":["切面卷积","3D点云","语义分割"],"涉及的技术概念":"切面卷积是一种在3D数据上的卷积网络新构建，直接操作于表面几何，适用于非结构化点云和噪声现实世界数据。深度全卷积网络是一种用于3D点云语义分割的网络结构。"},{"order":402,"title":"RayNet: Learning Volumetric 3D Reconstruction With Ray Potentials","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Paschalidou_RayNet_Learning_Volumetric_CVPR_2018_paper.html","abstract":"In this paper, we consider the problem of reconstructing a dense 3D model using images captured from different views. Recent methods based on convolutional neural networks (CNN) allow learning the entire task from data. However, they do not incorporate the physics of image formation such as perspective geometry and occlusion. Instead, classical approaches based on Markov Random Fields (MRF) with ray-potentials explicitly model these physical processes, but they cannot cope with large surface appearance variations across different viewpoints. In this paper, we propose RayNet, which combines the strengths of both frameworks. RayNet integrates a CNN that learns view-invariant feature representations with an MRF that explicitly encodes the physics of perspective projection and occlusion. We train RayNet end-to-end using empirical risk minimization. We thoroughly evaluate our approach on challenging real-world datasets and demonstrate its benefits over a piece-wise trained baseline, hand-crafted models as well as other learning-based approaches.","中文标题":"RayNet: 使用射线潜力学习体积3D重建","摘要翻译":"在本文中，我们考虑了使用从不同视角捕获的图像重建密集3D模型的问题。基于卷积神经网络（CNN）的最新方法允许从数据中学习整个任务。然而，它们没有结合图像形成的物理原理，如透视几何和遮挡。相反，基于马尔可夫随机场（MRF）与射线潜力的经典方法明确地模拟了这些物理过程，但它们无法应对不同视角之间大的表面外观变化。在本文中，我们提出了RayNet，它结合了两种框架的优势。RayNet集成了一个学习视角不变特征表示的CNN和一个明确编码透视投影和遮挡物理的MRF。我们使用经验风险最小化对RayNet进行端到端训练。我们在具有挑战性的真实世界数据集上彻底评估了我们的方法，并展示了其优于分段训练的基线、手工制作的模型以及其他基于学习的方法的优势。","领域":"3D重建/深度学习/计算机视觉","问题":"重建密集3D模型","动机":"结合卷积神经网络和马尔可夫随机场的优势，以更好地模拟图像形成的物理原理，如透视几何和遮挡，同时应对不同视角之间大的表面外观变化。","方法":"提出RayNet，集成学习视角不变特征表示的CNN和明确编码透视投影和遮挡物理的MRF，使用经验风险最小化进行端到端训练。","关键词":["3D重建","卷积神经网络","马尔可夫随机场","射线潜力","视角不变特征"],"涉及的技术概念":{"卷积神经网络（CNN）":"一种深度学习模型，特别适用于处理图像数据，能够从数据中学习特征表示。","马尔可夫随机场（MRF）":"一种统计模型，用于模拟具有相互依赖关系的随机变量集合，常用于图像处理和计算机视觉中。","射线潜力":"在3D重建中，用于模拟光线从不同视角投射到物体表面并反射回相机的物理过程。","经验风险最小化":"一种机器学习中的优化策略，旨在最小化模型在训练数据上的预测误差。"}},{"order":403,"title":"Neural 3D Mesh Renderer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kato_Neural_3D_Mesh_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kato_Neural_3D_Mesh_CVPR_2018_paper.html","abstract":"For modeling the 3D world behind 2D images, which 3D representation is most appropriate? A polygon mesh is a promising candidate for its compactness and geometric properties. However, it is not straightforward to model a polygon mesh from 2D images using neural networks because the conversion from a mesh to an image, or rendering, involves a discrete operation called rasterization, which prevents back-propagation. Therefore, in this work, we propose an approximate gradient for rasterization that enables the integration of rendering into neural networks. Using this renderer, we perform single-image 3D mesh reconstruction with silhouette image supervision and our system outperforms the existing voxel-based approach. Additionally, we perform gradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and 3D DeepDream, with 2D supervision for the first time. These applications demonstrate the potential of the integration of a mesh renderer into neural networks and the effectiveness of our proposed renderer.","中文标题":"神经3D网格渲染器","摘要翻译":"为了建模2D图像背后的3D世界，哪种3D表示最合适？多边形网格因其紧凑性和几何特性而成为一个有希望的候选者。然而，使用神经网络从2D图像建模多边形网格并不直接，因为从网格到图像的转换，或称为渲染，涉及一个称为光栅化的离散操作，这阻止了反向传播。因此，在这项工作中，我们提出了一个光栅化的近似梯度，使得渲染能够被整合到神经网络中。使用这个渲染器，我们在轮廓图像监督下执行单图像3D网格重建，并且我们的系统优于现有的基于体素的方法。此外，我们首次在2D监督下执行基于梯度的3D网格编辑操作，如2D到3D风格转移和3D DeepDream。这些应用展示了将网格渲染器整合到神经网络中的潜力以及我们提出的渲染器的有效性。","领域":"3D重建/渲染技术/神经网络应用","问题":"如何从2D图像中建模3D多边形网格","动机":"多边形网格因其紧凑性和几何特性成为建模3D世界的合适选择，但直接从2D图像建模多边形网格存在技术障碍","方法":"提出了一个光栅化的近似梯度，使得渲染能够被整合到神经网络中，从而实现了单图像3D网格重建和基于梯度的3D网格编辑操作","关键词":["3D重建","渲染技术","神经网络应用"],"涉及的技术概念":"多边形网格、光栅化、反向传播、单图像3D网格重建、基于梯度的3D网格编辑操作、2D到3D风格转移、3D DeepDream"},{"order":404,"title":"Structured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Structured_Attention_Guided_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Structured_Attention_Guided_CVPR_2018_paper.html","abstract":"Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.","中文标题":"结构化注意力引导的卷积神经场用于单目深度估计","摘要翻译":"最近的研究表明，将条件随机场（CRFs）模型集成到深度架构中有利于提高像素级预测任务的效果。沿着这一研究方向，本文提出了一种新颖的单目深度估计方法。与之前的工作类似，我们的方法采用连续CRF来融合从前端卷积神经网络（CNN）不同层提取的多尺度信息。与过去的工作不同，我们的方法得益于一个结构化注意力模型，该模型自动调节在不同尺度之间传递的相应特征的信息量。重要的是，所提出的注意力模型无缝集成到CRF中，允许整个架构的端到端训练。我们广泛的实验评估证明了所提出方法的有效性，该方法在KITTI基准测试中与之前的方法竞争，并在NYU Depth V2数据集上超越了现有技术。","领域":"单目深度估计/结构化注意力模型/条件随机场","问题":"提高单目深度估计的准确性和效率","动机":"探索如何更有效地融合多尺度信息以提高像素级预测任务的性能","方法":"采用结构化注意力模型自动调节不同尺度特征间的信息传递，并将其无缝集成到条件随机场中，实现端到端训练","关键词":["单目深度估计","结构化注意力模型","条件随机场","多尺度信息融合"],"涉及的技术概念":{"条件随机场（CRFs）":"一种统计建模方法，常用于序列数据的标注和预测任务，能够考虑上下文信息。","卷积神经网络（CNN）":"一种深度学习模型，特别适用于处理图像数据，能够自动提取图像特征。","结构化注意力模型":"一种机制，用于自动调节不同特征或信息源之间的注意力分配，以提高模型的性能。","端到端训练":"一种训练方法，使得整个模型从输入到输出可以直接通过梯度下降等优化算法进行训练，无需手动设计中间步骤。"}},{"order":405,"title":"Automatic 3D Indoor Scene Modeling From Single Panorama","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Automatic_3D_Indoor_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Automatic_3D_Indoor_CVPR_2018_paper.html","abstract":"We describe a system that automatically extracts 3D geometry of an indoor scene from a single 2D panorama. Our system recovers the spatial layout by finding the floor, walls, and ceiling; it also recovers shapes of typical indoor objects such as furniture. Using sampled perspective sub-views, we extract geometric cues (lines, vanishing points, orientation map, and surface normals) and semantic cues (saliency and object detection information). These cues are used for ground plane estimation and occlusion reasoning. The global spatial layout is inferred through a constraint graph on line segments and planar superpixels. The recovered layout is then used to guide shape estimation of the remaining objects using their normal information. Experiments on synthetic and real datasets show that our approach is state-of-the-art in both accuracy and efficiency. Our system can handle cluttered scenes with complex geometry that are challenging to existing techniques.","中文标题":"从单张全景图自动生成室内场景3D模型","摘要翻译":"我们描述了一个系统，该系统能够从单张2D全景图中自动提取室内场景的3D几何结构。我们的系统通过找到地板、墙壁和天花板来恢复空间布局；它还能恢复典型室内物体（如家具）的形状。通过采样的透视子视图，我们提取几何线索（线条、消失点、方向图和表面法线）和语义线索（显著性和物体检测信息）。这些线索用于地面平面估计和遮挡推理。通过线段和平面超像素的约束图推断全局空间布局。然后，利用恢复的布局通过物体的法线信息指导剩余物体的形状估计。在合成和真实数据集上的实验表明，我们的方法在准确性和效率上都是最先进的。我们的系统能够处理现有技术难以应对的具有复杂几何结构的杂乱场景。","领域":"3D重建/室内场景理解/几何处理","问题":"从单张2D全景图自动提取室内场景的3D几何结构","动机":"解决现有技术在处理具有复杂几何结构的杂乱场景时的挑战","方法":"通过采样的透视子视图提取几何和语义线索，利用这些线索进行地面平面估计和遮挡推理，通过线段和平面超像素的约束图推断全局空间布局，并利用恢复的布局指导剩余物体的形状估计","关键词":["3D重建","室内场景理解","几何处理","全景图","几何线索","语义线索","地面平面估计","遮挡推理","约束图","形状估计"],"涉及的技术概念":{"几何线索":"包括线条、消失点、方向图和表面法线，用于帮助理解场景的几何结构","语义线索":"包括显著性和物体检测信息，用于识别场景中的重要物体和区域","地面平面估计":"通过几何和语义线索估计场景中的地面平面","遮挡推理":"通过分析场景中的遮挡关系来推断物体的位置和形状","约束图":"通过线段和平面超像素构建的图，用于推断全局空间布局","形状估计":"利用物体的法线信息估计其形状"}},{"order":406,"title":"Extreme 3D Face Reconstruction: Seeing Through Occlusions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_Extreme_3D_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_Extreme_3D_Face_CVPR_2018_paper.html","abstract":"Existing single view, 3D face reconstruction methods can produce beautifully detailed 3D results, but typically only for near frontal, unobstructed viewpoints. We describe a system designed to provide detailed 3D reconstructions of faces viewed under extreme conditions, out of plane rotations, and occlusions. Motivated by the concept of bump mapping, we propose a layered approach which decouples estimation of a global shape from its mid-level details (e.g., wrinkles). We estimate a coarse 3D face shape which acts as a foundation and then separately layer this foundation with details represented by a bump map. We show how a deep convolutional encoder-decoder can be used to estimate such bump maps. We further show how this approach naturally extends to generate plausible details for occluded facial regions. We test our approach and its components extensively, quantitatively demonstrating the invariance of our estimated facial details. We further provide numerous qualitative examples showing that our method produces detailed 3D face shapes in viewing conditions where existing state of the art often break down.","中文标题":"极端3D面部重建：透视遮挡","摘要翻译":"现有的单视角3D面部重建方法可以产生非常详细的3D结果，但通常仅适用于接近正面、无遮挡的视角。我们描述了一个系统，旨在提供在极端条件下、平面外旋转和遮挡下观察的面部的详细3D重建。受凹凸贴图概念的启发，我们提出了一种分层方法，该方法将全局形状的估计与其中级细节（例如皱纹）解耦。我们估计一个粗糙的3D面部形状作为基础，然后分别用由凹凸贴图表示的细节层覆盖这个基础。我们展示了如何使用深度卷积编码器-解码器来估计这样的凹凸贴图。我们进一步展示了这种方法如何自然地扩展到为遮挡的面部区域生成合理的细节。我们广泛测试了我们的方法及其组件，定量证明了我们估计的面部细节的不变性。我们进一步提供了许多定性示例，显示我们的方法在现有最先进技术经常失效的观察条件下产生详细的3D面部形状。","领域":"3D重建/面部识别/深度学习","问题":"在极端条件、平面外旋转和遮挡下进行详细3D面部重建","动机":"现有方法在处理非正面、有遮挡的面部图像时效果不佳，需要一种能够在这些条件下也能产生详细3D面部重建的方法","方法":"提出一种分层方法，首先估计一个粗糙的3D面部形状作为基础，然后使用深度卷积编码器-解码器估计凹凸贴图来添加细节，特别是对于遮挡区域","关键词":["3D面部重建","遮挡处理","凹凸贴图"],"涉及的技术概念":"凹凸贴图是一种用于模拟表面细节的技术，通过在表面上应用高度图来改变光照效果，从而在不增加几何复杂度的情况下增加视觉细节。深度卷积编码器-解码器是一种深度学习架构，用于从输入图像中提取特征并生成输出图像，常用于图像到图像的转换任务。"},{"order":407,"title":"Beyond Grobner Bases: Basis Selection for Minimal Solvers","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Larsson_Beyond_Grobner_Bases_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Larsson_Beyond_Grobner_Bases_CVPR_2018_paper.html","abstract":"Many computer vision applications require robust estimation of the underlying geometry, in terms of camera motion and 3D structure of the scene. These robust methods often rely on running minimal solvers in a RANSAC framework. In this paper we show how we can make polynomial solvers based on the action matrix method faster, by careful selection of the monomial bases. These monomial bases have traditionally been based on a Grobner basis for the polynomial ideal. Here we describe how we can enumerate all such bases in an efficient way. We also show that going beyond Grobner bases leads to more efficient solvers in many cases. We present a novel basis sampling scheme that we evaluate on a number of problems.","中文标题":"超越Grobner基：最小求解器的基选择","摘要翻译":"许多计算机视觉应用需要从相机运动和场景的3D结构方面对基础几何进行鲁棒估计。这些鲁棒方法通常依赖于在RANSAC框架中运行最小求解器。在本文中，我们展示了如何通过仔细选择单项式基来使基于动作矩阵方法的多项式求解器更快。这些单项式基传统上基于多项式理想的Grobner基。在这里，我们描述了如何以高效的方式枚举所有这些基。我们还展示了在许多情况下，超越Grobner基可以带来更高效的求解器。我们提出了一种新的基采样方案，并在多个问题上进行了评估。","领域":"几何估计/多项式求解/基选择","问题":"提高基于动作矩阵方法的多项式求解器的效率","动机":"为了在计算机视觉应用中更快速、更鲁棒地估计相机运动和场景的3D结构","方法":"通过仔细选择单项式基来优化多项式求解器，并提出一种新的基采样方案","关键词":["几何估计","多项式求解","基选择","RANSAC框架","动作矩阵方法"],"涉及的技术概念":{"Grobner基":"用于多项式理想的基，传统上用于构建多项式求解器的单项式基","RANSAC框架":"一种鲁棒估计方法，用于从包含大量异常值的数据中估计数学模型参数","动作矩阵方法":"一种用于多项式求解的方法，通过构建动作矩阵来找到多项式的根","单项式基":"多项式求解器中用于表示多项式的基，选择合适的基可以加快求解速度"}},{"order":408,"title":"Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape From Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zuffi_Lions_and_Tigers_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zuffi_Lions_and_Tigers_CVPR_2018_paper.html","abstract":"Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames.","中文标题":"狮子、老虎和熊：从图像中捕捉非刚性、三维、关节形状","摘要翻译":"动物在自然界中广泛存在，对其形状和运动的分析在许多领域和行业中都很重要。然而，建模三维动物形状很困难，因为用于捕捉人类形状的三维扫描方法不适用于野生动物或自然环境。因此，我们提出了一种仅从图像中捕捉动物详细三维形状的方法。动物的关节和可变形的特性使得这个问题极具挑战性，特别是在具有移动和未校准摄像头的无约束环境中。为了实现这一点，我们使用了一个强大的关节动物形状先验模型，并将其拟合到图像数据中。然后，我们在一个标准参考姿势中变形动物形状，使其在关节化并投影到多个图像中时与图像证据匹配。我们的方法提取的三维形状细节比之前的方法多得多，并且能够仅使用几帧视频帧来建模新物种，包括一种已灭绝动物的形状。此外，投影的三维形状足够准确，可以从多帧中提取出逼真的纹理图。","领域":"三维重建/动物形状分析/纹理映射","问题":"从图像中捕捉非刚性、三维、关节动物形状","动机":"由于现有的三维扫描方法不适用于野生动物或自然环境，因此需要一种新的方法来捕捉动物的详细三维形状。","方法":"使用一个强大的关节动物形状先验模型，并将其拟合到图像数据中，然后在标准参考姿势中变形动物形状，使其在关节化并投影到多个图像中时与图像证据匹配。","关键词":["三维重建","动物形状分析","纹理映射"],"涉及的技术概念":"三维扫描方法、关节动物形状先验模型、图像数据拟合、标准参考姿势、图像证据匹配、纹理图提取"},{"order":409,"title":"Deep Cocktail Network: Multi-Source Unsupervised Domain Adaptation With Category Shift","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Deep_Cocktail_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Deep_Cocktail_Network_CVPR_2018_paper.html","abstract":"Most existing unsupervised domain adaptation (UDA) methods are based upon the assumption that source labeled data come from an identical underlying distribution. Whereas in practical scenario, labeled instances are typically collected from diverse sources. Moreover, those sources may not completely share their categories, which further brings a category shift challenge to multi-source (unsupervised) domain adaptation (MDA). In this paper, we propose a deep cocktail network (DCTN), to battle the domain and category shifts among multiple sources. Motivated by the theoretical results in cite{mansour2009domain}, the target distribution can be represented as the weighted combination of source distributions, and, the training of MDA via DCTN is then performed as two alternating steps: i) It deploys multi-way adversarial learning to minimize the discrepancy between the target and each of the multiple source domains, which also obtains the source-specific perplexity scores to denote the possibilities that a target sample belongs to different source domains. ii) The multi-source category classifiers are integrated with the perplexity scores to classify target sample, and the pseudo-labeled target samples together with source samples are utilized to update the multi-source category classifier and the representation module. We evaluate DCTN in three domain adaptation benchmarks, which clearly demonstrate the superiority of our framework.","中文标题":"深度鸡尾酒网络：具有类别偏移的多源无监督域适应","摘要翻译":"大多数现有的无监督域适应（UDA）方法基于源标记数据来自同一基础分布的假设。然而，在实际场景中，标记实例通常是从不同来源收集的。此外，这些来源可能不完全共享它们的类别，这进一步给多源（无监督）域适应（MDA）带来了类别偏移的挑战。在本文中，我们提出了一个深度鸡尾酒网络（DCTN），以应对多个源之间的域和类别偏移。受到cite{mansour2009domain}中理论结果的启发，目标分布可以表示为源分布的加权组合，然后通过DCTN进行MDA训练，分为两个交替步骤：i）它部署多路对抗学习以最小化目标与每个多源域之间的差异，这也获得了源特定的困惑度分数，以表示目标样本属于不同源域的可能性。ii）多源类别分类器与困惑度分数集成以分类目标样本，并且伪标记的目标样本与源样本一起用于更新多源类别分类器和表示模块。我们在三个域适应基准上评估了DCTN，这清楚地证明了我们框架的优越性。","领域":"域适应/多源学习/对抗学习","问题":"解决多源无监督域适应中的域和类别偏移问题","动机":"实际场景中标记实例通常来自不同来源，且这些来源可能不完全共享类别，这给多源无监督域适应带来了挑战","方法":"提出深度鸡尾酒网络（DCTN），通过多路对抗学习最小化目标与每个多源域之间的差异，并利用困惑度分数和多源类别分类器分类目标样本，更新分类器和表示模块","关键词":["域适应","多源学习","对抗学习","类别偏移"],"涉及的技术概念":"无监督域适应（UDA）、多源无监督域适应（MDA）、深度鸡尾酒网络（DCTN）、多路对抗学习、困惑度分数、多源类别分类器"},{"order":410,"title":"DOTA: A Large-Scale Dataset for Object Detection in Aerial Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xia_DOTA_A_Large-Scale_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_DOTA_A_Large-Scale_CVPR_2018_paper.html","abstract":"Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA).  To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000-by-4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188,282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.","中文标题":"DOTA: 用于航空图像中目标检测的大规模数据集","摘要翻译":"目标检测是计算机视觉中一个重要且具有挑战性的问题。尽管过去十年在自然场景中的目标检测取得了重大进展，但这些成功在航空图像中的应用进展缓慢，这不仅因为地球表面物体实例在尺度、方向和形状上的巨大变化，还因为航空场景中物体的良好注释数据集的稀缺。为了推进地球视觉（也称为地球观测和遥感）中的目标检测研究，我们引入了一个用于航空图像中目标检测的大规模数据集（DOTA）。为此，我们从不同的传感器和平台收集了2806张航空图像。每张图像的大小约为4000×4000像素，包含展示各种尺度、方向和形状的物体。这些DOTA图像随后由航空图像解释专家使用15个常见物体类别进行注释。完全注释的DOTA图像包含188,282个实例，每个实例都由一个任意的（8自由度）四边形标记。为了建立地球视觉中目标检测的基线，我们在DOTA上评估了最先进的目标检测算法。实验表明，DOTA很好地代表了真实的地球视觉应用，并且非常具有挑战性。","领域":"地球视觉/遥感/航空图像分析","问题":"航空图像中目标检测的挑战性问题","动机":"推进地球视觉中的目标检测研究，解决航空图像中目标检测的挑战","方法":"收集并注释大规模航空图像数据集，评估最先进的目标检测算法","关键词":["目标检测","航空图像","地球视觉","遥感","数据集"],"涉及的技术概念":{"目标检测":"识别图像中特定目标的技术","航空图像":"从空中拍摄的地球表面图像","地球视觉":"涉及地球观测和遥感的技术领域","遥感":"从远处获取地球表面信息的技术","数据集":"用于训练和测试算法的数据集合","注释":"对图像中的目标进行标记和分类的过程"}},{"order":411,"title":"Finding Beans in Burgers: Deep Semantic-Visual Embedding With Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Engilberge_Finding_Beans_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Engilberge_Finding_Beans_in_CVPR_2018_paper.html","abstract":"Several works have proposed to learn a two-path neural network that maps images and texts, respectively, to a same shared Euclidean space where geometry captures useful semantic relationships. Such a multi-modal embedding can be trained and used for various tasks, notably image captioning. In the present work, we introduce a new architecture of this type, with a visual path that leverages recent space-aware pooling mechanisms. Combined with a textual path which is jointly trained from scratch, our semantic-visual embedding offers a versatile model. Once trained under the supervision of captioned images, it yields new state-of-the-art performance on cross-modal retrieval. It also allows the localization of new concepts from the embedding space into any input image, delivering state-of-the-art result on the visual grounding of phrases.","中文标题":"在汉堡中寻找豆子：具有定位功能的深度语义-视觉嵌入","摘要翻译":"已有多个工作提出学习一个双路径神经网络，分别将图像和文本映射到同一个共享的欧几里得空间，其中几何结构捕捉有用的语义关系。这种多模态嵌入可以训练并用于各种任务，特别是图像描述。在本工作中，我们引入了一种新类型的架构，其视觉路径利用了最近的空间感知池化机制。与从头开始联合训练的文本路径相结合，我们的语义-视觉嵌入提供了一个多功能模型。一旦在有标题图像的监督下训练，它在跨模态检索上实现了新的最先进性能。它还允许将嵌入空间中的新概念定位到任何输入图像中，在短语的视觉定位上提供了最先进的结果。","领域":"跨模态检索/视觉定位/图像描述","问题":"如何有效地将图像和文本映射到同一共享空间，以捕捉语义关系并实现跨模态检索和视觉定位","动机":"为了提升跨模态检索和视觉定位的性能，需要一种能够同时处理图像和文本的多模态嵌入方法","方法":"引入了一种新类型的双路径神经网络架构，视觉路径利用空间感知池化机制，与文本路径联合训练，实现语义-视觉嵌入","关键词":["跨模态检索","视觉定位","图像描述"],"涉及的技术概念":"双路径神经网络、欧几里得空间、空间感知池化机制、语义-视觉嵌入、跨模态检索、视觉定位"},{"order":412,"title":"Feature Super-Resolution: Make Machine See More Clearly","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tan_Feature_Super-Resolution_Make_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tan_Feature_Super-Resolution_Make_CVPR_2018_paper.html","abstract":"Identifying small size images or small objects is a notoriously challenging problem, as discriminative representations are difficult to learn from the limited information contained in them with poor-quality appearance and unclear object structure. Existing research works usually increase the resolution of low-resolution image in the pixel space in order to provide better visual quality for human viewing. However, the improved performance of such methods is usually limited or even trivial in the case of very small image size (we will show it in this paper explicitly).  In this paper, different from image super-resolution (ISR), we propose a novel super-resolution technique called feature super-resolution (FSR), which aims at enhancing the discriminatory power of small size image in order to provide high recognition precision for machine. To achieve this goal, we propose a new Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model that transforms the raw poor features of small size images to highly discriminative ones by performing super-resolution in the feature space. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. By training the G and the D networks in an alternative manner, we encourage the G network to discover the latent distribution correlations between small size and large size images and then use G to improve the representations of small images. Extensive experiment results on Oxford5K, Paris, Holidays, and Flick100k datasets demonstrate that the proposed FSR approach can effectively enhance the discriminatory ability of features. Even when the resolution of query images is reduced greatly, e.g., 1/64 original size, the query feature enhanced by our FSR approach achieves surprisingly high retrieval performance at different image resolutions and increases the retrieval precision by 25% compared to the raw query feature.","中文标题":"特征超分辨率：让机器看得更清楚","摘要翻译":"识别小尺寸图像或小物体是一个众所周知的挑战性问题，因为从它们包含的有限信息中学习到具有区分性的表示是非常困难的，这些信息通常质量差且物体结构不清晰。现有的研究工作通常通过增加低分辨率图像在像素空间的分辨率来提供更好的视觉质量供人类观看。然而，在图像尺寸非常小的情况下，这些方法的性能提升通常是有限的，甚至是微不足道的（我们将在本文中明确展示这一点）。在本文中，与图像超分辨率（ISR）不同，我们提出了一种新的超分辨率技术，称为特征超分辨率（FSR），旨在增强小尺寸图像的区分能力，以提供高识别精度给机器。为了实现这一目标，我们提出了一种新的特征超分辨率生成对抗网络（FSR-GAN）模型，通过在特征空间执行超分辨率，将小尺寸图像的原始劣质特征转换为高度区分性的特征。我们的FSR-GAN由两个子网络组成：一个特征生成器网络G和一个特征判别器网络D。通过交替训练G和D网络，我们鼓励G网络发现小尺寸和大尺寸图像之间的潜在分布相关性，然后使用G来改善小图像的表示。在Oxford5K、Paris、Holidays和Flick100k数据集上的大量实验结果表明，所提出的FSR方法可以有效地增强特征的区分能力。即使查询图像的分辨率大大降低，例如原始尺寸的1/64，通过我们的FSR方法增强的查询特征在不同图像分辨率下实现了令人惊讶的高检索性能，并将检索精度提高了25%与原始查询特征相比。","领域":"图像识别/特征提取/图像检索","问题":"小尺寸图像或小物体的识别问题","动机":"提高小尺寸图像的识别精度，增强特征的区分能力","方法":"提出特征超分辨率生成对抗网络（FSR-GAN）模型，通过在特征空间执行超分辨率，将小尺寸图像的原始劣质特征转换为高度区分性的特征","关键词":["特征超分辨率","生成对抗网络","图像识别","特征提取","图像检索"],"涉及的技术概念":"特征超分辨率（FSR）是一种新的超分辨率技术，旨在增强小尺寸图像的区分能力。FSR-GAN模型通过在特征空间执行超分辨率，将小尺寸图像的原始劣质特征转换为高度区分性的特征。该模型由特征生成器网络G和特征判别器网络D组成，通过交替训练这两个网络，G网络能够发现小尺寸和大尺寸图像之间的潜在分布相关性，并利用这些相关性来改善小图像的表示。"},{"order":413,"title":"ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/LaLonde_ClusterNet_Detecting_Small_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/LaLonde_ClusterNet_Detecting_Small_CVPR_2018_paper.html","abstract":"Object detection in wide area motion imagery (WAMI) has drawn the attention of the computer vision research community for a number of years. WAMI proposes a number of unique challenges including extremely small object sizes, both sparse and densely-packed objects, and extremely large search spaces (large video frames). Nearly all state-of-the-art methods in WAMI object detection report that appearance-based classifiers fail in this challenging data and instead rely almost entirely on motion information in the form of background subtraction or frame-differencing. In this work, we experimentally verify the failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a heatmap-based fully convolutional neural network (CNN), and propose a novel two-stage spatio-temporal CNN which effectively and efficiently combines both appearance and motion information to significantly surpass the state-of-the-art in WAMI object detection. To reduce the large search space, the first stage (ClusterNet) takes in a set of extremely large video frames, combines the motion and appearance information within the convolutional architecture, and proposes regions of objects of interest (ROOBI). These ROOBI can contain from one to clusters of several hundred objects due to the large video frame size and varying object density in WAMI. The second stage (FoveaNet) then estimates the centroid location of all objects in that given ROOBI simultaneously via heatmap estimation. The proposed method exceeds state-of-the-art results on the WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped objects, as well as being the first proposed method in wide area motion imagery to detect completely stationary objects.","中文标题":"ClusterNet: 通过利用时空信息在大场景中检测小物体","摘要翻译":"在广域运动图像（WAMI）中的物体检测已经吸引了计算机视觉研究社区的多年关注。WAMI提出了一些独特的挑战，包括极小的物体尺寸、稀疏和密集堆积的物体以及极大的搜索空间（大视频帧）。几乎所有在WAMI物体检测中的最先进方法都报告称，基于外观的分类器在这种挑战性数据中失败，而是几乎完全依赖于背景减除或帧差形式的运动信息。在这项工作中，我们通过实验验证了基于外观的分类器在WAMI中的失败，如Faster R-CNN和基于热图的完全卷积神经网络（CNN），并提出了一种新颖的两阶段时空CNN，它有效地结合了外观和运动信息，显著超越了WAMI物体检测的最先进水平。为了减少大搜索空间，第一阶段（ClusterNet）接收一组极大的视频帧，结合卷积架构中的运动和外观信息，并提出感兴趣物体的区域（ROOBI）。由于WAMI中大视频帧尺寸和物体密度的变化，这些ROOBI可以包含从一到数百个物体的集群。第二阶段（FoveaNet）然后通过热图估计同时估计给定ROOBI中所有物体的质心位置。所提出的方法在WPAFB 2009数据集上对移动物体的检测结果超过了最先进水平5-16%，对静止物体的检测结果提高了近50%，并且是广域运动图像中第一个检测完全静止物体的方法。","领域":"物体检测/时空信息处理/卷积神经网络","问题":"在广域运动图像中检测极小的物体","动机":"解决基于外观的分类器在WAMI物体检测中的失败问题，以及处理大搜索空间和物体密度变化的挑战","方法":"提出了一种新颖的两阶段时空CNN，第一阶段（ClusterNet）结合运动和外观信息提出感兴趣物体的区域，第二阶段（FoveaNet）通过热图估计物体的质心位置","关键词":["物体检测","时空信息","卷积神经网络","热图估计","广域运动图像"],"涉及的技术概念":"Faster R-CNN是一种基于区域的卷积神经网络，用于物体检测；热图是一种用于表示物体位置概率分布的图像；背景减除和帧差是两种常用的运动检测技术，用于从视频序列中提取运动信息。"},{"order":414,"title":"MaskLab: Instance Segmentation by Refining Object Detection With Semantic and Direction Features","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_MaskLab_Instance_Segmentation_CVPR_2018_paper.html","abstract":"In this work, we tackle the problem of instance segmentation, the task of simultaneously solving object detection and semantic segmentation. Towards this goal, we present a model, called MaskLab, which produces three outputs: box detection, semantic segmentation, and direction prediction. Building on top of the Faster-RCNN object detector, the predicted boxes provide accurate localization of object instances. Within each region of interest, MaskLab performs foreground/background segmentation by combining semantic and direction prediction. Semantic segmentation assists the model in distinguishing between objects of different semantic classes including background, while the direction prediction, estimating each pixel's direction towards its corresponding center, allows separating instances of the same semantic class. Moreover, we explore the effect of incorporating recent successful methods from both segmentation and detection (eg, atrous convolution and hypercolumn). Our proposed model is evaluated on the COCO instance segmentation benchmark and shows comparable performance with other state-of-art models.","中文标题":"MaskLab：通过语义和方向特征精炼目标检测的实例分割","摘要翻译":"在这项工作中，我们解决了实例分割的问题，即同时解决目标检测和语义分割的任务。为此，我们提出了一个名为MaskLab的模型，该模型产生三个输出：框检测、语义分割和方向预测。基于Faster-RCNN目标检测器，预测的框提供了对象实例的准确定位。在每个感兴趣区域内，MaskLab通过结合语义和方向预测进行前景/背景分割。语义分割帮助模型区分包括背景在内的不同语义类别的对象，而方向预测估计每个像素向其对应中心的方向，允许分离相同语义类别的实例。此外，我们探讨了结合最近成功的分割和检测方法（例如，空洞卷积和超列）的效果。我们提出的模型在COCO实例分割基准上进行了评估，并显示出与其他最先进模型相当的性能。","领域":"实例分割/目标检测/语义分割","问题":"同时解决目标检测和语义分割的实例分割问题","动机":"提高实例分割的准确性，通过结合语义分割和方向预测来区分不同语义类别的对象和分离相同语义类别的实例","方法":"基于Faster-RCNN目标检测器，结合语义分割和方向预测进行前景/背景分割，并探讨了结合空洞卷积和超列等最新方法的效果","关键词":["实例分割","目标检测","语义分割","方向预测","空洞卷积","超列"],"涉及的技术概念":"Faster-RCNN是一种用于目标检测的深度学习模型，能够提供对象实例的准确定位。语义分割是指将图像中的每个像素分配给一个语义类别，如背景或特定对象。方向预测是指估计图像中每个像素向其对应对象中心的方向，有助于分离相同类别的不同实例。空洞卷积是一种在卷积神经网络中增加感受野的技术，而超列是指将不同层的特征图结合起来以增强模型性能的方法。"},{"order":415,"title":"Hashing as Tie-Aware Learning to Rank","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_Hashing_as_Tie-Aware_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Hashing_as_Tie-Aware_CVPR_2018_paper.html","abstract":"Hashing, or learning binary embeddings of data, is frequently used in nearest neighbor retrieval. In this paper, we develop learning to rank formulations for hashing, aimed at directly optimizing ranking-based evaluation metrics such as Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We first observe that the integer-valued Hamming distance often leads to tied rankings, and propose to use tie-aware versions of AP and NDCG to evaluate hashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive their continuous relaxations, and perform gradient-based optimization with deep neural networks. Our results establish the new state-of-the-art for image retrieval by Hamming ranking in common benchmarks.","中文标题":"哈希作为感知平局的排序学习","摘要翻译":"哈希，或学习数据的二进制嵌入，经常用于最近邻检索。在本文中，我们开发了用于哈希的排序学习公式，旨在直接优化基于排序的评估指标，如平均精度（AP）和归一化折扣累积增益（NDCG）。我们首先观察到整数值的汉明距离经常导致平局排名，并建议使用感知平局版本的AP和NDCG来评估检索的哈希。然后，为了优化感知平局的排序指标，我们推导了它们的连续松弛，并使用深度神经网络进行基于梯度的优化。我们的结果在常见基准测试中确立了通过汉明排名进行图像检索的新最先进技术。","领域":"图像检索/信息检索/排序学习","问题":"优化基于排序的评估指标，如平均精度（AP）和归一化折扣累积增益（NDCG）","动机":"整数值的汉明距离经常导致平局排名，需要一种方法来评估和优化这种情况下的检索效果","方法":"使用感知平局版本的AP和NDCG来评估检索的哈希，并推导它们的连续松弛，使用深度神经网络进行基于梯度的优化","关键词":["哈希","排序学习","图像检索","汉明距离","平均精度","归一化折扣累积增益"],"涉及的技术概念":{"哈希":"一种将数据转换为二进制嵌入的技术，用于最近邻检索","排序学习":"一种旨在直接优化基于排序的评估指标的学习方法","汉明距离":"用于衡量两个二进制字符串之间的差异，是哈希检索中常用的距离度量","平均精度（AP）":"一种评估检索系统性能的指标，考虑了检索结果中相关项的顺序","归一化折扣累积增益（NDCG）":"一种评估排序质量的指标，考虑了排序列表中每个位置的相关性"}},{"order":416,"title":"Classification-Driven Dynamic Image Enhancement","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sharma_Classification-Driven_Dynamic_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sharma_Classification-Driven_Dynamic_Image_CVPR_2018_paper.html","abstract":"Convolutional neural networks rely on image texture and structure to serve as discriminative features to classify the image content.  Image enhancement techniques can be used as preprocessing steps to help improve the overall image quality and in turn improve the overall effectiveness of a CNN. Existing image enhancement methods, however, are designed to improve the perceptual quality of an image for a human observer.  In this paper, we are interested in learning CNNs that can emulate image enhancement and restoration, but with the overall goal to improve image classification and not necessarily human perception.  To this end, we present a unified CNN architecture that uses a range of enhancement filters that can enhance  image-specific details via end-to-end dynamic filter learning. We demonstrate the effectiveness of this strategy on four challenging benchmark  datasets for fine-grained, object, scene and texture classification: CUB-200-2011, PASCAL-VOC2007,  MIT-Indoor, and DTD. Experiments using our proposed enhancement  shows promising results on all the datasets. In addition, our approach is capable of improving the performance of all generic CNN architectures.","中文标题":"分类驱动的动态图像增强","摘要翻译":"卷积神经网络依赖图像的纹理和结构作为区分特征来分类图像内容。图像增强技术可以作为预处理步骤，帮助提高整体图像质量，从而提高CNN的整体效果。然而，现有的图像增强方法旨在提高图像对人类观察者的感知质量。在本文中，我们感兴趣的是学习能够模拟图像增强和恢复的CNN，但总体目标是提高图像分类，而不一定是人类感知。为此，我们提出了一种统一的CNN架构，该架构使用一系列增强滤波器，通过端到端的动态滤波器学习来增强图像特定的细节。我们在四个具有挑战性的基准数据集上展示了这种策略的有效性，用于细粒度、物体、场景和纹理分类：CUB-200-2011、PASCAL-VOC2007、MIT-Indoor和DTD。使用我们提出的增强方法进行的实验在所有数据集上都显示出有希望的结果。此外，我们的方法能够提高所有通用CNN架构的性能。","领域":"图像分类/图像增强/卷积神经网络","问题":"如何通过图像增强技术提高卷积神经网络的图像分类效果","动机":"现有的图像增强方法主要关注提高图像对人类观察者的感知质量，而本研究旨在通过图像增强提高图像分类的效果","方法":"提出了一种统一的CNN架构，使用一系列增强滤波器，通过端到端的动态滤波器学习来增强图像特定的细节","关键词":["图像分类","图像增强","卷积神经网络","动态滤波器学习"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。图像增强技术是指通过一系列算法改善图像质量的过程，以便更好地进行后续处理或分析。端到端学习是一种机器学习方法，其中模型直接从输入到输出进行学习，无需手动设计中间步骤。动态滤波器学习指的是在训练过程中自动学习和调整滤波器参数，以适应特定的任务需求。"},{"order":417,"title":"Knowledge Aided Consistency for Weakly Supervised Phrase Grounding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Knowledge_Aided_Consistency_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Knowledge_Aided_Consistency_CVPR_2018_paper.html","abstract":"Given a natural language query, a phrase grounding system aims to localize mentioned objects in an image. In weakly supervised scenario, mapping between image regions (i.e., proposals) and language is not available in the training set. Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals. However, the optimization is solely guided by the reconstruction loss from the language modality, and ignores rich visual information contained in proposals and useful cues from external knowledge. In this paper, we explore the consistency contained in both visual and language modalities, and leverage complementary external knowledge to facilitate weakly supervised grounding. We propose a novel Knowledge Aided Consistency Network (KAC Net) which is optimized by reconstructing input query and proposal's information. To leverage complementary knowledge contained in the visual features, we introduce a Knowledge Based Pooling (KBP) gate to focus on query-related proposals. Experiments show that KAC Net provides a significant improvement on two popular datasets.","中文标题":"知识辅助一致性用于弱监督短语定位","摘要翻译":"给定一个自然语言查询，短语定位系统旨在定位图像中提到的对象。在弱监督场景下，训练集中没有图像区域（即提案）和语言之间的映射。以前的方法通过学习从预测的提案中重建输入查询中包含的语言信息来训练定位系统，以解决这一不足。然而，优化仅由语言模态的重建损失指导，忽略了提案中包含的丰富视觉信息和外部知识的有用线索。在本文中，我们探索了视觉和语言模态中包含的一致性，并利用互补的外部知识来促进弱监督定位。我们提出了一种新颖的知识辅助一致性网络（KAC Net），通过重建输入查询和提案的信息进行优化。为了利用视觉特征中包含的互补知识，我们引入了一个基于知识的池化（KBP）门来关注与查询相关的提案。实验表明，KAC Net在两个流行数据集上提供了显著的改进。","领域":"视觉问答/图像理解/自然语言处理","问题":"在弱监督场景下，如何有效地定位图像中提到的对象","动机":"解决弱监督场景下图像区域和语言之间映射不可用的问题，以及优化过程中仅依赖语言模态重建损失而忽略视觉信息和外部知识的问题","方法":"提出知识辅助一致性网络（KAC Net），通过重建输入查询和提案的信息进行优化，并引入基于知识的池化（KBP）门来关注与查询相关的提案","关键词":["弱监督学习","短语定位","知识辅助","视觉问答","图像理解"],"涉及的技术概念":{"弱监督学习":"在训练数据不完全标注的情况下进行学习","短语定位":"根据自然语言描述定位图像中的对象","知识辅助":"利用外部知识来增强模型的理解和定位能力","视觉问答":"结合视觉和语言信息回答关于图像的问题","图像理解":"从图像中提取和理解信息"}},{"order":418,"title":"Who Let the Dogs Out? Modeling Dog Behavior From Visual Data","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ehsani_Who_Let_the_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ehsani_Who_Let_the_CVPR_2018_paper.html","abstract":"We introduce the task of directly modeling a visually intelligent agent. Computer vision typically focuses on solving various subtasks related to visual intelligence. We depart from this standard approach to computer vision; instead we directly model a visually intelligent agent. Our model takes visual information as input and directly predicts the actions of the agent. Toward this end we introduce DECADE, a large-scale dataset of ego-centric videos from a dog's perspective as well as her corresponding movements. Using this data we model how the dog acts and how the dog plans her movements. We show under a variety of metrics that given just visual input we can successfully model this intelligent agent in many situations. Moreover, the representation learned by our model encodes distinct information compared to representations trained on image classification, and our learned representation can generalize to other domains. In particular, we show strong results on the task of walkable surface estimation by using this dog modeling task as representation learning.","中文标题":"谁让狗出去了？从视觉数据建模狗的行为","摘要翻译":"我们引入了直接建模视觉智能代理的任务。计算机视觉通常专注于解决与视觉智能相关的各种子任务。我们偏离了这种标准的计算机视觉方法；相反，我们直接建模一个视觉智能代理。我们的模型以视觉信息作为输入，直接预测代理的行为。为此，我们引入了DECADE，一个从狗的视角拍摄的大规模自我中心视频数据集以及她相应的动作。利用这些数据，我们建模了狗如何行动以及如何规划她的动作。我们展示了在各种指标下，仅凭视觉输入，我们可以在许多情况下成功建模这个智能代理。此外，我们的模型学习到的表示与在图像分类上训练的表示相比编码了不同的信息，并且我们学习到的表示可以推广到其他领域。特别是，我们通过使用这个狗建模任务作为表示学习，在可行走表面估计任务上展示了强大的结果。","领域":"视觉智能代理建模/表示学习/可行走表面估计","问题":"如何直接从视觉数据建模视觉智能代理的行为","动机":"探索直接从视觉信息预测智能代理行为的可能性，以及这种表示学习方法的泛化能力","方法":"引入DECADE数据集，使用狗的视角的自我中心视频和相应动作数据，建模狗的行为和动作规划，并评估模型在多种情况下的表现","关键词":["视觉智能代理","表示学习","可行走表面估计"],"涉及的技术概念":"DECADE数据集：一个从狗的视角拍摄的大规模自我中心视频数据集，包含狗的动作数据；表示学习：通过学习数据的表示来发现数据中的有用信息或结构；可行走表面估计：估计哪些表面是适合行走的，这在机器人导航和辅助技术中非常重要。"},{"order":419,"title":"Pseudo Mask Augmented Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.html","abstract":"In this work, we present a novel and effective framework to facilitate object detection with the instance-level segmentation information that is only supervised by bounding box annotation. Starting from the joint object detection and instance segmentation network, we propose to recursively estimate the pseudo ground-truth object masks from the instance-level object segmentation network training, and then enhance the detection network with top-down segmentation feedbacks. The pseudo ground truth mask and network parameters are optimized alternatively to mutually benefit each other. To obtain the promising pseudo masks in each iteration, we embed a graphical inference that incorporates the low-level image appearance consistency and the bounding box annotations to refine the segmentation masks predicted by the segmentation network. Our approach progressively improves the object detection performance by incorporating the detailed pixel-wise information learned from the weakly-supervised segmentation network. Extensive evaluation on the detection task in PASCAL VOC 2007 and 2012 verifies that the proposed approach is effective.","中文标题":"伪掩码增强的目标检测","摘要翻译":"在本工作中，我们提出了一个新颖且有效的框架，以促进仅通过边界框注释监督的实例级分割信息进行目标检测。从联合目标检测和实例分割网络出发，我们提出从实例级目标分割网络训练中递归估计伪真实目标掩码，然后通过自上而下的分割反馈增强检测网络。伪真实掩码和网络参数交替优化，以相互受益。为了在每次迭代中获得有前景的伪掩码，我们嵌入了一个图形推理，该推理结合了低级图像外观一致性和边界框注释，以细化由分割网络预测的分割掩码。我们的方法通过结合从弱监督分割网络学习到的详细像素级信息，逐步提高了目标检测性能。在PASCAL VOC 2007和2012的检测任务上的广泛评估验证了所提出方法的有效性。","领域":"目标检测/实例分割/弱监督学习","问题":"如何利用仅通过边界框注释监督的实例级分割信息来提高目标检测的性能","动机":"现有的目标检测方法大多依赖于精确的像素级注释，这在实际应用中往往难以获得。因此，探索仅通过边界框注释监督的实例级分割信息来提高目标检测性能的方法具有重要意义。","方法":"提出了一种递归估计伪真实目标掩码的方法，并通过自上而下的分割反馈增强检测网络。伪真实掩码和网络参数交替优化，以相互受益。此外，嵌入了一个图形推理，结合低级图像外观一致性和边界框注释，以细化分割掩码。","关键词":["伪掩码","目标检测","实例分割","弱监督学习","图形推理"],"涉及的技术概念":{"伪真实目标掩码":"通过递归估计从实例级目标分割网络训练中获得的掩码，用于增强检测网络。","自上而下的分割反馈":"一种通过分割网络的输出反馈来增强检测网络性能的方法。","图形推理":"一种结合低级图像外观一致性和边界框注释的技术，用于细化分割掩码。","弱监督学习":"一种仅通过边界框注释监督的学习方法，用于训练分割网络。"}},{"order":420,"title":"Dual Skipping Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Dual_Skipping_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Dual_Skipping_Networks_CVPR_2018_paper.html","abstract":"Inspired by the recent neuroscience studies on the left-right asymmetry of the human brain in processing low and high spatial frequency information, this paper introduces a dual skipping network which carries out coarse-to-fine object categorization. Such a network has two branches to simultaneously deal with both coarse and fine-grained classification tasks. Specifically, we propose a layer-skipping mechanism that learns a gating network to predict which layers to skip in the testing stage. This layer-skipping mechanism endows the network with good flexibility and capability in practice. Evaluations are conducted on several widely used coarse-to-fine object categorization benchmarks, and promising results are achieved by our proposed network model.","中文标题":"双跳跃网络","摘要翻译":"受到最近神经科学研究关于人类大脑在处理低和高空间频率信息时左右不对称性的启发，本文介绍了一种双跳跃网络，该网络执行从粗到细的对象分类。这种网络有两个分支，同时处理粗分类和细粒度分类任务。具体来说，我们提出了一种层跳跃机制，该机制学习一个门控网络来预测在测试阶段跳过哪些层。这种层跳跃机制赋予了网络良好的灵活性和实际能力。在几个广泛使用的从粗到细对象分类基准上进行了评估，我们提出的网络模型取得了有希望的结果。","领域":"对象分类/神经网络/深度学习","问题":"如何有效地执行从粗到细的对象分类任务","动机":"受到人类大脑在处理不同空间频率信息时左右不对称性的启发，旨在提高对象分类的灵活性和效率","方法":"提出了一种双跳跃网络，包含两个分支分别处理粗分类和细粒度分类任务，并引入层跳跃机制来预测测试阶段跳过的层","关键词":["对象分类","神经网络","层跳跃机制","门控网络"],"涉及的技术概念":"双跳跃网络是一种神经网络架构，包含两个分支分别处理不同粒度的分类任务。层跳跃机制通过学习一个门控网络来动态决定在测试阶段跳过哪些层，从而提高网络的灵活性和效率。"},{"order":421,"title":"Memory Matching Networks for One-Shot Image Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Memory_Matching_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Memory_Matching_Networks_CVPR_2018_paper.html","abstract":"In this paper, we introduce the new ideas of augmenting Convolutional Neural Networks (CNNs) with Memory and learning to learn the network parameters for the unlabelled images on the fly in one-shot learning. Specifically, we present Memory Matching Networks (MM-Net) --- a novel deep architecture that explores the training procedure, following the philosophy that training and test conditions must match. Technically, MM-Net writes the features of a set of labelled images (support set) into memory and reads from memory when performing inference to holistically leverage the knowledge in the set. Meanwhile, a Contextual Learner employs the memory slots in a sequential manner to predict the parameters of CNNs for unlabelled images. The whole architecture is trained by once showing only a few examples per class and switching the learning from minibatch to minibatch, which is tailored for one-shot learning when presented with a few examples of new categories at test time. Unlike the conventional one-shot learning approaches, our MM-Net could output one unified model irrespective of the number of shots and categories. Extensive experiments are conducted on two public datasets, i.e., Omniglot and emph{mini}ImageNet, and superior results are reported when compared to state-of-the-art approaches. More remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95% to 99.28% and from 49.21% to 53.37% on emph{mini}ImageNet.","中文标题":"记忆匹配网络用于一次性图像识别","摘要翻译":"在本文中，我们介绍了通过增强卷积神经网络（CNNs）与记忆的新思想，并在一次性学习中动态学习未标记图像的网络参数。具体来说，我们提出了记忆匹配网络（MM-Net）——一种新颖的深度架构，它探索了训练过程，遵循训练和测试条件必须匹配的理念。技术上，MM-Net将一组标记图像（支持集）的特征写入记忆，并在执行推理时从记忆中读取，以全面利用集合中的知识。同时，上下文学习者以顺序方式使用记忆槽来预测未标记图像的CNNs参数。整个架构通过每类仅展示几个示例并在小批量之间切换学习来训练，这是为在测试时呈现新类别的几个示例而量身定制的一次性学习。与传统的单次学习方法不同，我们的MM-Net可以输出一个统一的模型，而不论射击次数和类别数量如何。在两个公共数据集上进行了广泛的实验，即Omniglot和miniImageNet，并报告了与最先进方法相比的优越结果。更值得注意的是，我们的MM-Net将Omniglot上的一次性准确率从98.95%提高到99.28%，在miniImageNet上从49.21%提高到53.37%。","领域":"一次性学习/图像识别/卷积神经网络","问题":"解决在一次性学习场景下，如何有效利用少量示例进行图像识别的问题","动机":"探索在训练和测试条件必须匹配的理念下，如何通过增强卷积神经网络与记忆来提高一次性图像识别的准确率","方法":"提出记忆匹配网络（MM-Net），通过将标记图像的特征写入记忆并在推理时读取，以及使用上下文学习者预测未标记图像的CNNs参数，来实现一次性学习","关键词":["一次性学习","图像识别","卷积神经网络","记忆匹配网络"],"涉及的技术概念":"卷积神经网络（CNNs）、记忆匹配网络（MM-Net）、一次性学习、上下文学习者、支持集、Omniglot数据集、miniImageNet数据集"},{"order":422,"title":"IQA: Visual Question Answering in Interactive Environments","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gordon_IQA_Visual_Question_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gordon_IQA_Visual_Question_CVPR_2018_paper.html","abstract":"We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, like: “Are there any apples in the fridge?” The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on IQA owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset built upon AI2-THOR [35], a simulated photo-realistic environment of configurable indoor scenes with interactive objects. IQUAD V1 has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on IQUAD V1. For sample questions and results, please view our video: https://youtu.be/pXd3C-1jr98.","中文标题":"IQA：交互环境中的视觉问答","摘要翻译":"我们介绍了交互式问答（IQA），这是一项需要自主代理与动态视觉环境交互以回答问题任务。IQA向代理展示一个场景和一个问题，例如：“冰箱里有苹果吗？”代理必须导航场景，获取场景元素的视觉理解，与对象交互（例如打开冰箱）并根据问题规划一系列动作。由于状态空间大且多样，使用单一控制器的流行强化学习方法在IQA上表现不佳。我们提出了分层交互记忆网络（HIMN），由一组分解的控制器组成，允许系统在多个时间抽象层次上操作。为了评估HIMN，我们引入了IQUAD V1，这是一个基于AI2-THOR [35]的新数据集，AI2-THOR是一个可配置室内场景与交互对象的模拟照片级真实环境。IQUAD V1包含75,000个问题，每个问题都与一个独特的场景配置配对。我们的实验表明，我们提出的模型在IQUAD V1上优于流行的基于单一控制器的方法。有关示例问题和结果，请查看我们的视频：https://youtu.be/pXd3C-1jr98。","领域":"视觉问答/强化学习/交互式学习","问题":"在动态视觉环境中，自主代理如何有效地回答需要交互的问题","动机":"解决现有单一控制器强化学习方法在处理大且多样状态空间时的不足","方法":"提出了分层交互记忆网络（HIMN），通过一组分解的控制器在多个时间抽象层次上操作","关键词":["视觉问答","强化学习","交互式学习","分层交互记忆网络","IQUAD V1"],"涉及的技术概念":{"交互式问答（IQA）":"一种需要自主代理与动态视觉环境交互以回答问题的任务","分层交互记忆网络（HIMN）":"一种由分解的控制器组成的网络，允许系统在多个时间抽象层次上操作","IQUAD V1":"基于AI2-THOR的新数据集，包含75,000个与独特场景配置配对的问题","AI2-THOR":"一个模拟照片级真实环境，包含可配置的室内场景与交互对象"}},{"order":423,"title":"Pose Transferrable Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Pose_Transferrable_Person_CVPR_2018_paper.html","abstract":"Person re-identification (ReID) is an important task in the field of intelligent security. A key challenge is how to capture human pose variations, while existing benchmarks (i.e., Market1501, DukeMTMC-reID, CUHK03, etc.) do NOT provide sufficient pose coverage to train a robust ReID system.  To address this issue, we propose a pose-transferrable person ReID framework which utilizes pose-transferred sample augmentations (i.e., with ID supervision) to enhance ReID model training. On one hand, novel training samples with rich pose variations are generated via transferring pose instances from MARS dataset, and they are added into the target dataset to facilitate robust training. On the other hand, in addition to the conventional discriminator of GAN (i.e., to distinguish between REAL/FAKE samples), we propose a novel guider sub-network which encourages the generated sample (i.e., with novel pose) towards better satisfying the ReID loss (i.e., cross-entropy ReID loss, triplet ReID loss). In the meantime, an alternative optimization procedure is proposed to train the proposed Generator-Guider-Discriminator network. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 show that our method achieves great performance improvement, and outperforms most state-of-the-art methods without elaborate designing the ReID model.","中文标题":"姿态可转移的人员重识别","摘要翻译":"人员重识别（ReID）是智能安全领域的一项重要任务。一个关键挑战是如何捕捉人体姿态变化，而现有的基准测试（如Market1501、DukeMTMC-reID、CUHK03等）并未提供足够的姿态覆盖来训练一个鲁棒的ReID系统。为了解决这个问题，我们提出了一个姿态可转移的人员重识别框架，该框架利用姿态转移样本增强（即带有ID监督）来增强ReID模型的训练。一方面，通过从MARS数据集中转移姿态实例生成具有丰富姿态变化的新训练样本，并将它们添加到目标数据集中以促进鲁棒训练。另一方面，除了GAN的常规判别器（即区分真实/假样本）外，我们提出了一个新的引导子网络，该网络鼓励生成的样本（即具有新姿态）更好地满足ReID损失（即交叉熵ReID损失、三元组ReID损失）。同时，提出了一个替代优化程序来训练所提出的生成器-引导器-判别器网络。在Market-1501、DukeMTMC-reID和CUHK03上的实验结果表明，我们的方法实现了显著的性能提升，并且在不需要精心设计ReID模型的情况下，优于大多数最先进的方法。","领域":"智能安全/人员重识别/姿态估计","问题":"现有的人员重识别基准测试未能提供足够的姿态覆盖，难以训练出鲁棒的ReID系统。","动机":"为了解决现有基准测试中姿态覆盖不足的问题，提高人员重识别系统的鲁棒性。","方法":"提出了一种姿态可转移的人员重识别框架，通过姿态转移样本增强和引入引导子网络来增强模型训练。","关键词":["姿态转移","样本增强","引导子网络","交叉熵损失","三元组损失"],"涉及的技术概念":{"姿态转移":"从MARS数据集中转移姿态实例生成新训练样本。","样本增强":"通过添加具有丰富姿态变化的新训练样本来增强模型训练。","引导子网络":"一种新的网络结构，用于鼓励生成的样本更好地满足ReID损失。","交叉熵损失":"一种用于分类任务的损失函数，用于衡量模型预测的概率分布与真实分布之间的差异。","三元组损失":"一种用于度量学习的损失函数，通过比较锚点、正样本和负样本之间的距离来优化模型。"}},{"order":424,"title":"Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cui_Large_Scale_Fine-Grained_CVPR_2018_paper.html","abstract":"Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain-specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.","中文标题":"大规模细粒度分类与领域特定迁移学习","摘要翻译":"通过微调从大规模数据集（例如，ImageNet）学习到的知识，为领域特定的细粒度视觉分类（FGVC）任务（例如，识别鸟类物种或汽车品牌和型号）提供了有效的解决方案。在这种情况下，数据标注通常需要专门的领域知识，因此难以扩展。在这项工作中，我们首先解决了大规模FGVC中的一个问题。我们的方法在2017年iNaturalist大规模物种分类挑战中获得了第一名。我们方法成功的关键在于使用更高图像分辨率的训练方案，并处理训练数据的长尾分布。接下来，我们研究了通过微调从大规模数据集到小规模、领域特定的FGVC数据集的迁移学习。我们提出了一种通过地球移动距离估计领域相似性的方法，并证明了根据这一度量，迁移学习在源领域与目标领域相似的情况下受益。我们提出的迁移学习方法优于ImageNet预训练，并在多个常用的FGVC数据集上取得了最先进的结果。","领域":"细粒度视觉分类/迁移学习/图像识别","问题":"解决大规模细粒度视觉分类任务中的数据标注难题和领域特定迁移学习的效果优化问题","动机":"由于数据标注需要专门的领域知识，难以扩展，因此需要一种有效的方法来处理大规模细粒度视觉分类任务，并优化领域特定迁移学习的效果","方法":"采用更高图像分辨率的训练方案处理训练数据的长尾分布，提出通过地球移动距离估计领域相似性的方法，优化迁移学习效果","关键词":["细粒度视觉分类","迁移学习","图像分辨率","长尾分布","地球移动距离"],"涉及的技术概念":"细粒度视觉分类（FGVC）指的是在非常相似的类别之间进行分类，如不同种类的鸟类或不同品牌和型号的汽车。迁移学习是一种机器学习方法，它利用从一个任务中学到的知识来帮助解决另一个相关任务。地球移动距离（Earth Mover's Distance）是一种衡量两个概率分布之间差异的方法，用于估计领域相似性。"},{"order":425,"title":"Data Distillation: Towards Omni-Supervised Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Radosavovic_Data_Distillation_Towards_CVPR_2018_paper.html","abstract":"We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.","中文标题":"数据蒸馏：迈向全监督学习","摘要翻译":"我们研究了全监督学习，这是一种半监督学习的特殊形式，在这种形式中，学习者利用所有可用的标记数据加上互联网规模的无标记数据源。全监督学习的性能下限是现有标记数据集的性能，提供了超越最先进的全监督方法的潜力。为了利用全监督设置，我们提出了数据蒸馏，这是一种方法，它通过使用单一模型对无标记数据的多个变换进行预测集成，以自动生成新的训练注释。我们认为，视觉识别模型最近已经足够准确，现在可以将关于自训练的经典思想应用于具有挑战性的现实世界数据。我们的实验结果表明，在人体关键点检测和一般物体检测的情况下，使用数据蒸馏训练的最先进模型超越了仅使用COCO数据集标记数据的性能。","领域":"视觉识别/自训练/数据增强","问题":"如何有效利用大量未标记数据提升视觉识别模型的性能","动机":"探索超越现有全监督方法性能的可能性，通过利用所有可用的标记数据和互联网规模的无标记数据源","方法":"提出数据蒸馏方法，通过集成单一模型对无标记数据的多个变换的预测，自动生成新的训练注释","关键词":["数据蒸馏","全监督学习","自训练","视觉识别","数据增强"],"涉及的技术概念":"数据蒸馏是一种通过集成模型对无标记数据的多个变换的预测来自动生成训练注释的方法。全监督学习是一种半监督学习的特殊形式，旨在利用所有可用的标记数据和互联网规模的无标记数据源来提升模型性能。自训练是一种利用模型自身预测结果作为训练数据的方法，适用于视觉识别等任务。数据增强是通过对数据进行变换来增加训练数据的多样性，以提高模型的泛化能力。"},{"order":426,"title":"Object Referring in Videos With Language and Human Gaze","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vasudevan_Object_Referring_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vasudevan_Object_Referring_in_CVPR_2018_paper.html","abstract":"We investigate the problem of object referring (OR) i.e. to localize a target object in a visual scene coming with a language description. Humans perceive the world more as continued video snippets than as static images, and describe objects not only by their appearance, but also by their spatio-temporal context and motion features. Humans also gaze at the object when they issue a referring expression. Existing works for OR mostly focus on static images only, which fall short in providing many such cues. This paper addresses OR in videos with language and human gaze. To that end, we present a new video dataset for OR, with 30,000 objects over 5,000 stereo video sequences annotated for their descriptions and gaze. We further propose a novel network model for OR in videos, by integrating appearance, motion, gaze, and spatio-temporal context into one network. Experimental results show that our method effectively utilizes motion cues, human gaze, and  spatio-temporal context. Our method outperforms previous OR methods.","中文标题":"视频中的对象引用：结合语言与人类注视","摘要翻译":"我们研究了对象引用（OR）问题，即在带有语言描述的视觉场景中定位目标对象。人类感知世界更多是作为连续的视频片段而非静态图像，并且描述对象不仅通过它们的外观，还通过它们的时空上下文和运动特征。当人类发出引用表达时，他们也会注视该对象。现有的OR工作大多仅关注静态图像，这在提供许多此类线索方面存在不足。本文解决了结合语言和人类注视的视频中的OR问题。为此，我们提出了一个新的OR视频数据集，包含5000个立体视频序列中的30000个对象，这些对象被注释了描述和注视。我们进一步提出了一种新颖的视频OR网络模型，通过将外观、运动、注视和时空上下文集成到一个网络中。实验结果表明，我们的方法有效地利用了运动线索、人类注视和时空上下文。我们的方法优于之前的OR方法。","领域":"视频理解/对象定位/人类注视分析","问题":"在带有语言描述的视觉场景中定位目标对象","动机":"人类感知世界更多是作为连续的视频片段而非静态图像，并且描述对象不仅通过它们的外观，还通过它们的时空上下文和运动特征。现有的OR工作大多仅关注静态图像，这在提供许多此类线索方面存在不足。","方法":"提出了一种新颖的视频OR网络模型，通过将外观、运动、注视和时空上下文集成到一个网络中。","关键词":["对象引用","视频理解","人类注视","时空上下文","运动特征"],"涉及的技术概念":"对象引用（OR）是指在视觉场景中根据语言描述定位目标对象。本文通过结合外观、运动、注视和时空上下文，提出了一种新的网络模型来解决视频中的OR问题。"},{"order":427,"title":"Feature Selective Networks for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhai_Feature_Selective_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhai_Feature_Selective_Networks_CVPR_2018_paper.html","abstract":"Objects for detection usually have distinct characteristics in different sub-regions and different aspect ratios. However, in prevalent two-stage object detection methods, Region-of-Interest (RoI) features are extracted by RoI pooling with little emphasis on these translation-variant feature components. We present feature selective networks to reform the feature representations of RoIs by exploiting their disparities among sub-regions and aspect ratios. Our network produces the sub-region attention bank and aspect ratio attention bank for the whole image. The RoI-based sub-region attention map and aspect ratio attention map are selectively pooled from the banks, and then used to refine the original RoI features for RoI classification. Equipped with a light-weight detection subnetwork, our network gets a consistent boost in detection performance based on general ConvNet backbones (ResNet-101, GoogLeNet and VGG-16). Without bells and whistles, our detectors equipped with ResNet-101 achieve more than 3% mAP improvement compared to counterparts on PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO datasets.","中文标题":"用于目标检测的特征选择网络","摘要翻译":"检测对象通常在不同的子区域和不同的宽高比中具有明显的特征。然而，在流行的两阶段目标检测方法中，感兴趣区域（RoI）特征是通过RoI池化提取的，很少强调这些平移不变的特征组件。我们提出了特征选择网络，通过利用子区域和宽高比之间的差异来改革RoI的特征表示。我们的网络为整个图像生成子区域注意力库和宽高比注意力库。基于RoI的子区域注意力图和宽高比注意力图从库中选择性地池化，然后用于细化原始RoI特征以进行RoI分类。配备轻量级检测子网络，我们的网络在基于通用ConvNet骨干（ResNet-101、GoogLeNet和VGG-16）的基础上，检测性能得到了持续提升。无需花哨的技巧，我们的检测器配备ResNet-101，在PASCAL VOC 2007、PASCAL VOC 2012和MS COCO数据集上相比同类产品实现了超过3%的mAP提升。","领域":"目标检测/特征选择/注意力机制","问题":"在流行的两阶段目标检测方法中，RoI特征提取过程中对平移不变特征组件的重视不足","动机":"通过利用子区域和宽高比之间的差异来改革RoI的特征表示，以提高目标检测的性能","方法":"提出特征选择网络，生成子区域注意力库和宽高比注意力库，选择性地池化注意力图以细化原始RoI特征","关键词":["目标检测","特征选择","注意力机制","RoI池化","ConvNet"],"涉及的技术概念":{"RoI池化":"一种用于从卷积神经网络中提取固定大小特征图的技术，常用于目标检测任务中。","注意力机制":"一种使模型能够专注于输入数据的特定部分的技术，以提高模型的性能。","ConvNet":"卷积神经网络，一种深度学习模型，特别适用于处理图像数据。","mAP":"平均精度均值，是评估目标检测模型性能的常用指标。"}},{"order":428,"title":"Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_a_Discriminative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_a_Discriminative_CVPR_2018_paper.html","abstract":"Compared to earlier multistage frameworks using CNN features, recent end-to-end deep approaches for fine-grained recognition essentially enhance the mid-level learning capability of CNNs. Previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated feature encoding method to capture higher order feature statistics. We show that mid-level representation learning can be enhanced within the CNN framework, by learning a bank of convolutional filters that capture class-specific discriminative patches without extra part or bounding box annotations. Such a filter bank is well structured, properly initialized and discriminatively learned through a novel asymmetric multi-stream architecture with convolutional filter supervision and a non-random layer initialization. Experimental results show that our approach achieves state-of-the-art on three publicly available fine-grained recognition datasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and visualizations are further provided to understand our approach.","中文标题":"在CNN中学习判别性滤波器组用于细粒度识别","摘要翻译":"与早期使用CNN特征的多阶段框架相比，最近的端到端深度学习方法在细粒度识别方面本质上增强了CNN的中层学习能力。之前的方法通过引入辅助网络将定位信息注入主分类网络，或采用复杂的特征编码方法来捕捉高阶特征统计量来实现这一点。我们展示了在CNN框架内，通过学习一组卷积滤波器来捕捉类别特定的判别性补丁，而无需额外的部分或边界框注释，可以增强中层表示学习。这样的滤波器组结构良好，通过具有卷积滤波器监督和非随机层初始化的新型非对称多流架构进行适当初始化和判别性学习。实验结果表明，我们的方法在三个公开可用的细粒度识别数据集（CUB-200-2011、斯坦福汽车和FGVC-Aircraft）上达到了最先进的水平。进一步提供了消融研究和可视化以理解我们的方法。","领域":"细粒度识别/卷积神经网络/中层表示学习","问题":"增强卷积神经网络在细粒度识别中的中层学习能力","动机":"通过增强CNN的中层表示学习能力，提高细粒度识别的准确性和效率，而无需额外的部分或边界框注释","方法":"在CNN框架内学习一组卷积滤波器，捕捉类别特定的判别性补丁，通过新型非对称多流架构进行适当初始化和判别性学习","关键词":["细粒度识别","卷积神经网络","中层表示学习","非对称多流架构","卷积滤波器监督"],"涉及的技术概念":{"细粒度识别":"指在非常相似的类别之间进行识别，如不同种类的鸟类或汽车模型","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","中层表示学习":"指在深度学习模型中，学习能够捕捉到图像中特定部分或特征的表示","非对称多流架构":"一种网络架构设计，通过多个并行的处理流来增强模型的学习能力","卷积滤波器监督":"在训练过程中，直接对卷积滤波器的输出进行监督，以提高其判别能力"}},{"order":429,"title":"Grounding Referring Expressions in Images by Variational Context","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Grounding_Referring_Expressions_CVPR_2018_paper.html","abstract":"We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., \`\`largest elephant standing behind baby elephant''.  This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context --- visual attributes (e.g., \`\`largest'', \`\`baby'') and relationships (e.g., \`\`behind'') that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Our model exploits the reciprocal relation between the referent and context, i.e., either of them influences the estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. We also extend the model to the unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings. The code is available at url{https://github.com/yuleiniu/vc/","中文标题":"通过变分上下文在图像中定位引用表达","摘要翻译":"我们专注于在图像中定位（即定位或链接）引用表达，例如“站在小象后面的大象”。这是一个普遍但具有挑战性的视觉-语言任务，因为它不仅需要对象的定位，还需要对上下文的多模态理解——视觉属性（例如“最大的”，“小”）和关系（例如“后面”）有助于将引用对象与其他对象区分开来，尤其是同一类别的对象。由于建模与多个图像区域相关的上下文涉及的指数复杂性，现有工作通过多实例学习将此任务过度简化为成对区域建模。在本文中，我们提出了一种称为变分上下文的变分贝叶斯方法，以解决引用表达定位中的复杂上下文建模问题。我们的模型利用了引用对象和上下文之间的相互关系，即它们中的任何一个都会影响另一个的后验分布的估计，从而可以大大减少上下文的搜索空间。我们还将模型扩展到无监督设置，其中没有引用对象的注释。在各种基准上的大量实验表明，在监督和无监督设置中，与最先进的方法相比，我们的方法都有持续的改进。代码可在url{https://github.com/yuleiniu/vc/}获取。","领域":"视觉-语言理解/上下文建模/变分贝叶斯方法","问题":"在图像中准确理解和定位引用表达，尤其是在需要区分同一类别对象时。","动机":"解决现有方法在建模与多个图像区域相关的上下文时的过度简化问题，提高引用表达定位的准确性。","方法":"提出了一种变分贝叶斯方法，称为变分上下文，通过利用引用对象和上下文之间的相互关系来减少上下文的搜索空间，并扩展到无监督设置。","关键词":["引用表达定位","上下文建模","变分贝叶斯方法"],"涉及的技术概念":"变分贝叶斯方法是一种统计方法，用于估计复杂概率模型的后验分布。在本文中，它被用来建模引用表达和图像上下文之间的复杂关系，通过利用它们之间的相互关系来减少搜索空间，从而提高定位的准确性。"},{"order":430,"title":"Dynamic Graph Generation Network: Generating Relational Knowledge From Diagrams","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Dynamic_Graph_Generation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Dynamic_Graph_Generation_CVPR_2018_paper.html","abstract":"In this work, we introduce a new algorithm for analyzing a diagram, which contains visual and textual information in an abstract and integrated way. Whereas diagrams contain richer information compared with individual image-based or language-based data, proper solutions for automatically understanding them have not been proposed due to their innate characteristics of multi-modality and arbitrariness of layouts. To tackle this problem, we propose a unified diagram-parsing network for generating knowledge from diagrams based on an object detector and a recurrent neural network designed for a graphical structure. Specifically, we propose a dynamic graph-generation network that is based on dynamic memory and graph theory. We explore the dynamics of information in a diagram with activation of gates in gated recurrent unit (GRU) cells. On publicly available diagram datasets, our model demonstrates a state-of-the-art result that outperforms other baselines. Moreover, further experiments on question answering shows potentials of the proposed method for various applications.","中文标题":"动态图生成网络：从图表生成关系知识","摘要翻译":"在本工作中，我们引入了一种新的算法来分析图表，这些图表以抽象和集成的方式包含视觉和文本信息。尽管图表与基于图像或语言的个别数据相比包含更丰富的信息，但由于其多模态和布局任意性的内在特性，尚未提出自动理解它们的适当解决方案。为了解决这个问题，我们提出了一个统一的图表解析网络，用于基于对象检测器和为图形结构设计的循环神经网络从图表生成知识。具体来说，我们提出了一个基于动态内存和图论的动态图生成网络。我们通过门控循环单元（GRU）细胞中的门激活来探索图表中的信息动态。在公开可用的图表数据集上，我们的模型展示了超越其他基线的最先进结果。此外，关于问答的进一步实验显示了所提出方法在各种应用中的潜力。","领域":"图表理解/知识图谱/问答系统","问题":"自动理解包含丰富视觉和文本信息的图表","动机":"由于图表的多模态和布局任意性，缺乏有效的自动理解解决方案","方法":"提出一个统一的图表解析网络，结合对象检测器和循环神经网络，以及基于动态内存和图论的动态图生成网络","关键词":["图表理解","知识图谱","问答系统","动态图生成","门控循环单元"],"涉及的技术概念":"对象检测器用于识别图表中的元素，循环神经网络（特别是门控循环单元，GRU）用于处理序列数据，动态内存用于存储和更新信息，图论用于构建和理解图表中的关系。"},{"order":431,"title":"A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Roveri_A_Network_Architecture_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Roveri_A_Network_Architecture_CVPR_2018_paper.html","abstract":"We propose a novel neural network architecture for point cloud classification. Our key idea is to automatically transform the 3D unordered input data into a set of useful 2D depth images, and classify them by exploiting well performing image classification CNNs. We present new differentiable module designs to generate depth images from a point cloud. These modules can be combined with any network architecture for processing point clouds. We utilize them in combination with state-of-the-art classification networks, and get results competitive with the state of the art in point cloud classification. Furthermore, our architecture automatically produces informative images representing the input point cloud, which could be used for further applications such as point cloud visualization.","中文标题":"通过自动深度图像生成进行点云分类的网络架构","摘要翻译":"我们提出了一种新颖的神经网络架构用于点云分类。我们的核心思想是自动将3D无序输入数据转换为一组有用的2D深度图像，并通过利用性能良好的图像分类CNN进行分类。我们提出了新的可微分模块设计，用于从点云生成深度图像。这些模块可以与任何处理点云的网络架构结合使用。我们将它们与最先进的分类网络结合使用，并获得了与点云分类领域最先进技术相竞争的结果。此外，我们的架构自动生成代表输入点云的信息丰富的图像，这些图像可用于进一步的应用，如点云可视化。","领域":"点云处理/深度图像生成/图像分类","问题":"点云数据的分类问题","动机":"提高点云分类的准确性和效率，同时生成可用于进一步应用的信息丰富的图像","方法":"提出了一种新的神经网络架构，通过自动将3D点云数据转换为2D深度图像，并利用现有的高性能图像分类CNN进行分类","关键词":["点云分类","深度图像生成","神经网络架构"],"涉及的技术概念":"点云数据是一种3D无序数据，通常用于表示物体的表面。深度图像是从特定视角捕捉的2D图像，其中每个像素的值代表从该视角到物体表面的距离。CNN（卷积神经网络）是一种深度学习模型，特别适用于处理图像数据。"},{"order":432,"title":"Towards Dense Object Tracking in a 2D Honeybee Hive","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bozek_Towards_Dense_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bozek_Towards_Dense_Object_CVPR_2018_paper.html","abstract":"From human crowds to cells in a tissue, the detection and efficient tracking of multiple objects in dense configurations is an important and unsolved problem.  In the past, limitations of image analysis have restricted studies of dense groups to tracking one individual, a set of marked individuals, or to coarse-grained group-level dynamics, all of which yield incomplete information.  Here, we combine the power of convolutional neural networks (CNNs) with the model environment of a honeybee hive to develop an automated method for the recognition of all individuals in a dense group based on raw image data.   In the proposed solution, we create new, adapted individual labeling and use segmentation architecture U-Net with a specific loss function to predict both object location and orientation. We additionally leverage time series image data to exploit both structural and temporal regularities in the the tracked objects in a recurrent manner. This allowed us to achieve near human-level performance on real-world image data while dramatically reducing original network size to 6% of the initial parameters. Given the novel application of CNNs in this study, we generate extensive problem-specific image data in which labeled examples are produced through a custom interface with Amazon Mechanical Turk. This dataset contains over 375,000 labeled bee instances moving across 720 video frames with 2 fps sampling and represents an extensive resource for development and testing of dense object recognition and tracking methods. With our method we correctly detect 96% of individuals with a location error of ~7% of a typical body dimension, and orientation error of 12 degrees, approximating the variability in labeling by human raters with ~9% body dimension variation in position and 8 degrees orientation variation.  Our study represents an important step towards efficient image-based dense object tracking by allowing for the accurate determination of object location and orientation across time-series image data efficiently within one network architecture.","中文标题":"迈向二维蜜蜂蜂巢中的密集目标追踪","摘要翻译":"从人类群体到组织中的细胞，在密集配置中检测和有效追踪多个目标是一个重要且未解决的问题。过去，图像分析的局限性限制了密集群体研究只能追踪一个个体、一组标记个体或粗粒度的群体动态，这些都提供了不完整的信息。在这里，我们结合卷积神经网络（CNNs）的力量和蜜蜂蜂巢的模型环境，开发了一种基于原始图像数据的自动方法，用于识别密集群体中的所有个体。在所提出的解决方案中，我们创建了新的、适应性强的个体标签，并使用分割架构U-Net和特定的损失函数来预测目标的位置和方向。我们还利用时间序列图像数据，以递归方式利用被追踪对象的结构和时间规律性。这使得我们能够在真实世界的图像数据上实现接近人类水平的性能，同时将原始网络大小大幅减少到初始参数的6%。鉴于CNNs在本研究中的新颖应用，我们生成了大量特定问题的图像数据，其中标记的示例是通过与Amazon Mechanical Turk的自定义界面生成的。该数据集包含超过375,000个标记的蜜蜂实例，跨越720个视频帧，采样率为2 fps，代表了开发和测试密集目标识别和追踪方法的广泛资源。使用我们的方法，我们正确检测到96%的个体，位置误差约为典型身体尺寸的7%，方向误差为12度，接近人类评分者在位置上的9%身体尺寸变化和8度方向变化的标记变异性。我们的研究代表了向高效基于图像的密集目标追踪迈出的重要一步，通过在一个网络架构内有效地确定时间序列图像数据中的目标位置和方向。","领域":"目标追踪/图像分割/时间序列分析","问题":"在密集配置中检测和有效追踪多个目标","动机":"解决过去图像分析在密集群体研究中只能追踪一个个体、一组标记个体或粗粒度的群体动态，提供不完整信息的问题","方法":"结合卷积神经网络（CNNs）和蜜蜂蜂巢的模型环境，开发自动方法识别密集群体中的所有个体，使用分割架构U-Net和特定损失函数预测目标位置和方向，利用时间序列图像数据递归利用被追踪对象的结构和时间规律性","关键词":["目标追踪","图像分割","时间序列分析"],"涉及的技术概念":"卷积神经网络（CNNs）用于图像识别和分割，U-Net架构用于图像分割，特定的损失函数用于预测目标的位置和方向，时间序列图像数据用于利用被追踪对象的结构和时间规律性，Amazon Mechanical Turk用于生成标记的示例"},{"order":433,"title":"Long-Term On-Board Prediction of People in Traffic Scenes Under Uncertainty","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bhattacharyya_Long-Term_On-Board_Prediction_CVPR_2018_paper.html","abstract":"Progress towards advanced systems for assisted and autonomous driving is leveraging  recent advances in recognition and segmentation methods. Yet, we are still facing challenges in bringing reliable driving to inner cities, as those are composed of highly dynamic scenes observed from a moving platform at considerable speeds. Anticipation becomes a key element in order to react timely and prevent accidents.      In this paper we argue that it is necessary to predict at least 1 second and we thus propose a new model that jointly predicts ego motion and people trajectories over such large time horizons. We pay particular attention to modeling the uncertainty of our estimates arising from the non-deterministic nature of natural traffic scenes.     Our experimental results show that it is indeed possible to predict people trajectories at the desired time horizons and that our uncertainty estimates are informative of the prediction error. We also show that both sequence modeling of trajectories as well as our novel method of long term odometry prediction are essential for best performance.","中文标题":"不确定性下交通场景中长期车载行人预测","摘要翻译":"辅助和自动驾驶系统的进步正在利用识别和分割方法的最新进展。然而，在将可靠驾驶引入内城方面，我们仍面临挑战，因为这些场景由高度动态的场景组成，从移动平台上以相当高的速度观察。为了及时反应并预防事故，预测成为一个关键因素。在本文中，我们认为至少需要预测1秒钟，因此我们提出了一个新模型，该模型联合预测自我运动和行人在如此长时间范围内的轨迹。我们特别关注于建模由于自然交通场景的非确定性性质而产生的估计不确定性。我们的实验结果表明，确实可以在所需的时间范围内预测行人轨迹，并且我们的不确定性估计对预测误差具有信息性。我们还表明，轨迹的序列建模以及我们新颖的长期里程计预测方法对于最佳性能至关重要。","领域":"自动驾驶/行人轨迹预测/不确定性建模","问题":"在高速移动的平台上预测交通场景中行人的长期轨迹","动机":"为了在高度动态的内城交通场景中实现可靠驾驶，及时反应并预防事故","方法":"提出一个新模型，联合预测自我运动和行人在长时间范围内的轨迹，并特别关注于建模估计的不确定性","关键词":["自动驾驶","行人轨迹预测","不确定性建模"],"涉及的技术概念":"识别和分割方法的最新进展、自我运动预测、行人轨迹预测、不确定性建模、序列建模、长期里程计预测"},{"order":434,"title":"Single-Shot Refinement Neural Network for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.html","abstract":"For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression accuracy and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multi-task loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet.","中文标题":"单次优化神经网络用于目标检测","摘要翻译":"对于目标检测，两阶段方法（例如，Faster R-CNN）已经达到了最高的准确率，而一阶段方法（例如，SSD）则具有高效率的优势。为了继承两者的优点同时克服它们的缺点，本文提出了一种新颖的基于单次检测的检测器，称为RefineDet，它实现了比两阶段方法更好的准确率，并保持了一阶段方法的可比效率。RefineDet由两个相互连接的模块组成，即锚点优化模块和目标检测模块。具体来说，前者旨在（1）过滤掉负锚点以减少分类器的搜索空间，以及（2）粗略调整锚点的位置和大小，为后续的回归器提供更好的初始化。后者模块将优化后的锚点作为前者的输入，以进一步提高回归准确率并预测多类别标签。同时，我们设计了一个转移连接块，将锚点优化模块中的特征转移到目标检测模块中，以预测对象的位置、大小和类别标签。多任务损失函数使我们能够以端到端的方式训练整个网络。在PASCAL VOC 2007、PASCAL VOC 2012和MS COCO上的大量实验表明，RefineDet以高效率实现了最先进的检测准确率。代码可在https://github.com/sfzhang15/RefineDet获取。","领域":"目标检测/神经网络/深度学习","问题":"如何在保持高效率的同时提高目标检测的准确率","动机":"继承两阶段方法的高准确率和一阶段方法的高效率，同时克服它们的缺点","方法":"提出了一种新颖的基于单次检测的检测器RefineDet，包括锚点优化模块和目标检测模块，以及转移连接块和多任务损失函数","关键词":["目标检测","神经网络","深度学习"],"涉及的技术概念":"两阶段方法（如Faster R-CNN）和一阶段方法（如SSD）在目标检测中的应用，锚点优化模块用于减少分类器的搜索空间和调整锚点位置大小，目标检测模块用于提高回归准确率和预测多类别标签，转移连接块用于特征转移，多任务损失函数用于端到端训练"},{"order":435,"title":"Video Captioning via Hierarchical Reinforcement Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Video_Captioning_via_CVPR_2018_paper.html","abstract":"Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore,  our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.","中文标题":"通过分层强化学习进行视频字幕生成","摘要翻译":"视频字幕生成是自动生成视频中动作的文本描述的任务。尽管之前的工作（例如序列到序列模型）在抽象短视频的粗略描述方面显示出有希望的结果，但对于包含多个细粒度动作的视频进行详细描述的字幕生成仍然非常具有挑战性。本文旨在通过提出一种新颖的分层强化学习框架来解决这一挑战，其中高层管理器模块学习设计子目标，而低层工作者模块识别原始动作以实现子目标。通过这种在不同层次上加强视频字幕生成的组合框架，我们的方法在新引入的大规模细粒度视频字幕数据集上显著优于所有基线方法。此外，我们的非集成模型已经在广泛使用的MSR-VTT数据集上达到了最先进的结果。","领域":"视频字幕生成/强化学习/细粒度动作识别","问题":"解决视频中包含多个细粒度动作的详细描述字幕生成的挑战","动机":"尽管已有方法在短视频的粗略描述上取得了一定成果，但对于包含多个细粒度动作的视频进行详细描述的字幕生成仍然是一个挑战","方法":"提出了一种新颖的分层强化学习框架，包括高层管理器模块设计子目标和低层工作者模块识别原始动作以实现子目标","关键词":["视频字幕生成","分层强化学习","细粒度动作识别"],"涉及的技术概念":"分层强化学习框架由高层管理器模块和低层工作者模块组成，高层管理器负责设计子目标，低层工作者负责识别并执行原始动作以实现这些子目标。这种方法通过在不同层次上加强视频字幕生成，有效解决了细粒度动作识别和描述的挑战。"},{"order":436,"title":"Tips and Tricks for Visual Question Answering: Learnings From the 2017 Challenge","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Teney_Tips_and_Tricks_CVPR_2018_paper.html","abstract":"This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.","中文标题":"视觉问答的技巧与窍门：2017年挑战赛的启示","摘要翻译":"本文介绍了一种在2017年视觉问答挑战赛中获得第一名的先进视觉问答模型。视觉问答由于其多模态特性、明确的评估协议和潜在的实际应用，对人工智能研究具有重要意义。深度神经网络在视觉问答中的表现非常依赖于架构和超参数的选择。为了进一步推动该领域的研究，我们详细描述了我们高性能但相对简单的模型。通过对代表超过3000 GPU小时的架构和超参数的大量探索，我们确定了导致其成功的技巧和窍门，即：sigmoid输出、软训练目标、自下而上注意力的图像特征、门控tanh激活、使用GloVe和Google Images初始化的输出嵌入、大批量训练数据和训练数据的智能洗牌。我们提供了它们对性能影响的详细分析，以帮助他人做出适当的选择。","领域":"视觉问答/深度学习/神经网络","问题":"提高视觉问答任务的性能","动机":"视觉问答由于其多模态特性和潜在的实际应用，对人工智能研究具有重要意义","方法":"通过大量探索架构和超参数，确定了导致模型成功的技巧和窍门，包括sigmoid输出、软训练目标、自下而上注意力的图像特征等","关键词":["视觉问答","深度学习","神经网络"],"涉及的技术概念":{"sigmoid输出":"一种激活函数，用于将输入映射到0到1之间的输出","软训练目标":"在训练过程中使用非硬性标签，以提高模型的泛化能力","自下而上注意力的图像特征":"一种从图像中提取特征的方法，通过模拟人类视觉注意机制来关注图像的重要部分","门控tanh激活":"一种激活函数，通过门控机制控制信息的流动","GloVe":"一种用于获取词向量的算法","Google Images":"谷歌图像，用于获取图像数据","大批量训练数据":"在训练过程中使用大量数据，以提高模型的训练效率和性能","智能洗牌":"在训练过程中对数据进行智能化的随机排序，以提高模型的泛化能力"}},{"order":437,"title":"Learning to Segment Every Thing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_to_Segment_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Learning_to_Segment_CVPR_2018_paper.html","abstract":"Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.","中文标题":"学习分割一切","摘要翻译":"大多数对象实例分割方法要求所有训练样本都带有分割掩码标签。这一要求使得标注新类别变得昂贵，并将实例分割模型限制在约100个标注良好的类别上。本文的目标是提出一种新的部分监督训练范式，以及一种新颖的权重转移函数，使得能够在大量类别上训练实例分割模型，这些类别都有边界框标注，但只有一小部分有掩码标注。这些贡献使我们能够使用Visual Genome数据集中的边界框标注和COCO数据集中80个类别的掩码标注来训练Mask R-CNN，以检测和分割3000个视觉概念。我们在COCO数据集上的控制研究中评估了我们的方法。这项工作是朝着具有广泛视觉世界理解的实例分割模型迈出的第一步。","领域":"实例分割/视觉理解/数据标注","问题":"如何在没有大量掩码标注的情况下训练实例分割模型","动机":"减少实例分割模型训练中对大量掩码标注的依赖，降低标注成本，扩展模型可识别的类别范围","方法":"提出一种部分监督训练范式和新颖的权重转移函数，利用边界框标注和少量掩码标注训练实例分割模型","关键词":["实例分割","部分监督学习","权重转移","Mask R-CNN","Visual Genome","COCO"],"涉及的技术概念":{"实例分割":"一种计算机视觉任务，旨在识别图像中的每个对象实例，并为每个实例提供像素级的分割掩码。","部分监督学习":"一种训练范式，其中只有部分数据被完全标注，其余数据只有部分标注或没有标注。","权重转移":"一种技术，通过从有标注数据中学习到的知识（权重）来帮助训练只有部分标注或没有标注的数据。","Mask R-CNN":"一种流行的实例分割模型，能够同时进行对象检测和像素级分割。","Visual Genome":"一个包含大量视觉概念和它们之间关系的图像数据集。","COCO":"一个广泛使用的图像识别、分割和标注数据集，包含80个类别的对象。"}},{"order":438,"title":"Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Self-Supervised_Adversarial_Hashing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Self-Supervised_Adversarial_Hashing_CVPR_2018_paper.html","abstract":"Thanks to the success of deep learning, cross-modal retrieval has made significant progress recently. However, there still remains a crucial bottleneck: how to bridge the modality gap to further enhance the retrieval accuracy. In this paper, we propose a self-supervised adversarial hashing (SSAH) approach, which lies among the early attempts to incorporate adversarial learning into cross-modal hashing in a self-supervised fashion. The primary contribution of this work is that two adversarial networks are leveraged to maximize the semantic correlation and consistency of the representations between different modalities. In addition, we harness a self-supervised semantic network to discover high-level semantic information in the form of multi-label annotations. Such information guides the feature learning process and preserves the modality relationships in both the common semantic space and the Hamming space. Extensive experiments carried out on three benchmark datasets validate that the proposed SSAH surpasses the state-of-the-art methods.","中文标题":"自监督对抗哈希网络用于跨模态检索","摘要翻译":"得益于深度学习的成功，跨模态检索最近取得了显著进展。然而，仍然存在一个关键瓶颈：如何弥合模态差距以进一步提高检索准确性。在本文中，我们提出了一种自监督对抗哈希（SSAH）方法，这是将对抗学习以自监督方式融入跨模态哈希的早期尝试之一。这项工作的主要贡献是利用两个对抗网络来最大化不同模态之间表示的语义相关性和一致性。此外，我们利用自监督语义网络以多标签注释的形式发现高级语义信息。这些信息指导特征学习过程，并在共同语义空间和汉明空间中保留模态关系。在三个基准数据集上进行的广泛实验验证了所提出的SSAH超越了最先进的方法。","领域":"跨模态检索/对抗学习/自监督学习","问题":"如何弥合模态差距以进一步提高跨模态检索的准确性","动机":"尽管深度学习在跨模态检索方面取得了进展，但如何有效弥合不同模态之间的差距以提高检索准确性仍是一个挑战。","方法":"提出了一种自监督对抗哈希（SSAH）方法，利用两个对抗网络来最大化不同模态之间表示的语义相关性和一致性，并通过自监督语义网络发现高级语义信息以指导特征学习过程。","关键词":["跨模态检索","对抗学习","自监督学习","语义相关性","汉明空间"],"涉及的技术概念":"自监督对抗哈希（SSAH）方法结合了对抗学习和自监督学习的概念，通过对抗网络增强不同模态间的语义相关性和一致性，同时利用自监督语义网络提取高级语义信息，以指导特征学习并保持模态关系。"},{"order":439,"title":"Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhuang_Parallel_Attention_A_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Parallel_Attention_A_CVPR_2018_paper.html","abstract":"Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention.To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!.","中文标题":"并行注意力：通过对话和查询实现视觉对象发现的统一框架","摘要翻译":"根据预定义的固定类别标签集识别对象在计算机视觉领域已经得到了充分研究。然而，在许多实际应用中，可能感兴趣的主题事先并不知道，或者不容易界定。在许多这类情况下，自然语言对话是指定感兴趣主题的自然方式，实现这一能力的任务（即，指代表达理解）最近引起了关注。为此，我们提出了一个统一框架，即并行注意力网络（PLAN），用于发现图像中通过可变长度的自然语言表达描述所指向的对象，从短语查询到多轮对话。PLAN网络具有两种注意力机制，将表达的部分与全局视觉内容以及直接与候选对象相关联。此外，这些注意力机制是循环的，使得指代过程可视化和可解释。来自这两个来源的注意力信息被结合起来推理所指对象。这两种注意力机制可以并行训练，我们发现组合系统在几个基准数据集上优于现有技术，这些数据集具有不同长度的语言输入，如RefCOCO、RefCOCO+和GuessWhat?!。","领域":"指代表达理解/自然语言处理/视觉问答","问题":"如何在图像中发现通过自然语言表达描述所指向的对象","动机":"解决在实际应用中，可能感兴趣的主题事先并不知道或不容易界定的问题，通过自然语言对话指定感兴趣主题","方法":"提出并行注意力网络（PLAN），该网络具有两种注意力机制，将表达的部分与全局视觉内容以及直接与候选对象相关联，并通过循环注意力机制使指代过程可视化和可解释","关键词":["指代表达理解","自然语言处理","视觉问答"],"涉及的技术概念":"并行注意力网络（PLAN）是一种统一框架，用于通过自然语言表达描述发现图像中的对象。它包括两种注意力机制，一种将表达的部分与全局视觉内容相关联，另一种直接与候选对象相关联。这些机制是循环的，使得指代过程可视化和可解释。通过结合来自这两个来源的注意力信息，可以推理出所指对象。"},{"order":440,"title":"Zigzag Learning for Weakly Supervised Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Zigzag_Learning_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Zigzag_Learning_for_CVPR_2018_paper.html","abstract":"This paper addresses weakly supervised object detection with only image-level supervision at training stage. Previous approaches train detection models with entire images all at once, making the models prone to being trapped in sub-optimums due to the introduced false positive examples. Unlike them, we propose a zigzag learning strategy to simultaneously discover reliable object instances and prevent the model from overfitting initial seeds. Towards this goal, we first develop a criterion named mean Energy Accumulation Scores (mEAS) to automatically measure and rank localization difficulty of an image containing the target object, and accordingly learn the detector progressively by feeding examples with increasing difficulty. In this way, the model can be well prepared by training on easy examples for learning from more difficult ones and thus gain a stronger detection ability more efficiently. Furthermore, we introduce a novel masking regularization strategy over the high level convolutional feature maps to avoid overfitting initial samples. These two modules formulate a zigzag learning process, where progressive learning endeavors to discover reliable object instances, and masking regularization increases the difficulty of finding object instances properly. We achieve 47.6% mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.","中文标题":"Z字形学习用于弱监督目标检测","摘要翻译":"本文解决了在训练阶段仅使用图像级监督的弱监督目标检测问题。以往的方法一次性使用整个图像训练检测模型，这使得模型容易因为引入的假阳性样本而陷入次优解。与这些方法不同，我们提出了一种Z字形学习策略，以同时发现可靠的目标实例并防止模型过拟合初始种子。为此，我们首先开发了一个名为平均能量累积分数（mEAS）的标准，以自动测量和排序包含目标对象的图像的定位难度，并相应地通过提供难度逐渐增加的样本来逐步学习检测器。通过这种方式，模型可以通过在简单样本上训练来为学习更难的样本做好准备，从而更有效地获得更强的检测能力。此外，我们在高级卷积特征图上引入了一种新颖的掩码正则化策略，以避免过拟合初始样本。这两个模块构成了一个Z字形学习过程，其中渐进学习努力发现可靠的目标实例，而掩码正则化则增加了正确找到目标实例的难度。我们在PASCAL VOC 2007上实现了47.6%的mAP，大幅超越了现有技术。","领域":"目标检测/弱监督学习/卷积神经网络","问题":"解决在仅使用图像级监督的情况下进行弱监督目标检测的问题","动机":"防止模型因假阳性样本而陷入次优解，并提高检测能力","方法":"提出Z字形学习策略，包括开发mEAS标准进行图像难度排序和逐步学习，以及引入掩码正则化策略避免过拟合","关键词":["弱监督学习","目标检测","卷积神经网络","正则化策略"],"涉及的技术概念":"平均能量累积分数（mEAS）用于自动测量和排序图像的定位难度，掩码正则化策略用于避免模型过拟合初始样本。"},{"order":441,"title":"Attentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Attentive_Fashion_Grammar_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Attentive_Fashion_Grammar_CVPR_2018_paper.html","abstract":"This paper proposes a knowledge-guided fashion network to solve the problem of visual fashion analysis, e.g., fashion landmark localization and clothing category classification. The suggested fashion model is leveraged with high-level human knowledge in this domain. We propose two important fashion grammars: (i) dependency grammar capturing kinematics-like relation, and (ii) symmetry grammar accounting for the bilateral symmetry of clothes. We introduce Bidirectional Convolutional Recurrent Neural Networks (BCRNNs) for efficiently approaching message passing over grammar topologies, and producing regularized landmark layouts. For enhancing clothing category classification, our fashion network is encoded with two novel attention mechanisms, i.e., landmark-aware attention and category-driven attention. The former enforces our network to focus on the functional parts of clothes, and learns domain-knowledge centered representations, leading to a supervised attention mechanism. The latter is goal-driven, which directly enhances task-related features and can be learned in an implicit, top-down manner. Experimental results on large-scale fashion datasets demonstrate the superior performance of our fashion grammar network.","中文标题":"注意力时尚语法网络用于时尚地标检测和服装类别分类","摘要翻译":"本文提出了一种知识引导的时尚网络，以解决视觉时尚分析的问题，例如时尚地标定位和服装类别分类。所建议的时尚模型利用了该领域的高级人类知识。我们提出了两种重要的时尚语法：（i）捕捉类似运动学关系的依赖语法，和（ii）考虑服装双边对称性的对称语法。我们引入了双向卷积循环神经网络（BCRNNs），以有效地处理语法拓扑上的消息传递，并生成规范化的地标布局。为了增强服装类别分类，我们的时尚网络编码了两种新颖的注意力机制，即地标感知注意力和类别驱动注意力。前者强制我们的网络专注于服装的功能部分，并学习以领域知识为中心的表示，从而形成一种监督注意力机制。后者是目标驱动的，直接增强与任务相关的特征，并且可以以隐式的、自上而下的方式学习。在大规模时尚数据集上的实验结果表明，我们的时尚语法网络具有优越的性能。","领域":"时尚分析/服装分类/地标检测","问题":"视觉时尚分析中的时尚地标定位和服装类别分类","动机":"利用高级人类知识解决视觉时尚分析中的问题，提高时尚地标定位和服装类别分类的准确性和效率","方法":"提出了依赖语法和对称语法两种时尚语法，引入了双向卷积循环神经网络（BCRNNs）处理语法拓扑上的消息传递，并采用地标感知注意力和类别驱动注意力两种注意力机制增强服装类别分类","关键词":["时尚语法","地标检测","服装分类","注意力机制","双向卷积循环神经网络"],"涉及的技术概念":"依赖语法捕捉类似运动学的关系，对称语法考虑服装的双边对称性；双向卷积循环神经网络（BCRNNs）用于有效处理语法拓扑上的消息传递；地标感知注意力和类别驱动注意力是两种新颖的注意力机制，分别专注于服装的功能部分和直接增强与任务相关的特征。"},{"order":442,"title":"Generalized Zero-Shot Learning via Synthesized Examples","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Verma_Generalized_Zero-Shot_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Verma_Generalized_Zero-Shot_Learning_CVPR_2018_paper.html","abstract":"We present a generative framework for generalized zero-shot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic emph{conditional} decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zero-shot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning.","中文标题":"通过合成示例的广义零样本学习","摘要翻译":"我们提出了一个用于广义零样本学习的生成框架，其中训练和测试类不一定是不相交的。基于变分自编码器的架构，包括一个概率编码器和一个概率条件解码器，我们的模型可以从已见/未见类生成新的示例，给定它们各自的类属性。这些示例随后可以用于训练任何现成的分类模型。我们的编码器-解码器架构的一个关键方面是一个反馈驱动机制，其中一个判别器（一个多元回归器）学习将生成的示例映射到相应的类属性向量，从而改进生成器。我们的模型能够生成并利用未见类的示例来训练分类模型，这自然有助于减轻在广义零样本学习设置中对预测已见类的偏见。通过一系列全面的实验，我们展示了我们的模型在几个基准数据集上，无论是标准还是广义零样本学习，都优于几种最先进的方法。","领域":"零样本学习/生成模型/分类模型","问题":"解决广义零样本学习中的类偏见问题","动机":"减轻在广义零样本学习设置中对预测已见类的偏见","方法":"基于变分自编码器的架构，包括概率编码器和概率条件解码器，以及反馈驱动机制","关键词":["零样本学习","生成模型","分类模型","变分自编码器","反馈驱动机制"],"涉及的技术概念":{"广义零样本学习":"一种学习范式，其中训练和测试类不一定是不相交的，旨在从已见类学习以识别未见类。","变分自编码器":"一种生成模型，通过学习数据的潜在表示来生成新的数据样本。","概率编码器":"变分自编码器的一部分，用于将输入数据映射到潜在空间的概率分布。","概率条件解码器":"变分自编码器的一部分，根据潜在表示和条件信息生成数据样本。","反馈驱动机制":"一种机制，通过判别器将生成的示例映射回类属性向量，以改进生成器的性能。","多元回归器":"一种用于预测多个连续变量的统计模型。"}},{"order":443,"title":"Partially Shared Multi-Task Convolutional Neural Network With Local Constraint for Face Attribute Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Partially_Shared_Multi-Task_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Partially_Shared_Multi-Task_CVPR_2018_paper.html","abstract":"In this paper, we study the face attribute learning problem by considering the identity information and attribute relationships simultaneously. In particular, we first introduce a Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), in which four Task Specific Networks (TSNets) and one Shared Network (SNet) are connected by Partially Shared (PS) structures to learn better shared and task specific representations. To utilize identity information to further boost the performance, we introduce a local learning constraint which minimizes the difference between the representations of each sample and its local geometric neighbours with the same identity. Consequently, we present a local constraint regularized multi-task network, called Partially Shared Multi-task Convolutional Neural Network with Local Constraint (PS-MCNN-LC), where PS structure and local constraint are integrated together to help the framework learn better attribute representations. The experimental results on CelebA and LFWA demonstrate the promise of the proposed methods.","中文标题":"局部约束的部分共享多任务卷积神经网络用于人脸属性学习","摘要翻译":"在本文中，我们通过同时考虑身份信息和属性关系来研究人脸属性学习问题。特别是，我们首先引入了一种部分共享的多任务卷积神经网络（PS-MCNN），其中四个任务特定网络（TSNets）和一个共享网络（SNet）通过部分共享（PS）结构连接，以学习更好的共享和任务特定表示。为了利用身份信息进一步提高性能，我们引入了一种局部学习约束，该约束最小化每个样本与其具有相同身份的局部几何邻居表示之间的差异。因此，我们提出了一种局部约束正则化的多任务网络，称为带有局部约束的部分共享多任务卷积神经网络（PS-MCNN-LC），其中PS结构和局部约束被集成在一起，以帮助框架学习更好的属性表示。在CelebA和LFWA上的实验结果证明了所提出方法的潜力。","领域":"人脸识别/多任务学习/卷积神经网络","问题":"如何同时考虑身份信息和属性关系来提升人脸属性学习的性能","动机":"为了更有效地学习人脸属性，需要同时考虑身份信息和属性之间的关系，以及如何利用这些信息来提升学习性能。","方法":"提出了一种部分共享的多任务卷积神经网络（PS-MCNN），并引入局部学习约束来最小化相同身份样本与其局部几何邻居表示之间的差异，从而形成局部约束正则化的多任务网络（PS-MCNN-LC）。","关键词":["人脸属性学习","多任务学习","卷积神经网络","局部约束"],"涉及的技术概念":"部分共享的多任务卷积神经网络（PS-MCNN）是一种网络结构，它通过部分共享结构连接任务特定网络和共享网络，以学习共享和任务特定的表示。局部学习约束是一种正则化方法，用于最小化相同身份样本与其局部几何邻居表示之间的差异，以提高属性学习的性能。"},{"order":444,"title":"SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Faraone_SYQ_Learning_Symmetric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Faraone_SYQ_Learning_Symmetric_CVPR_2018_paper.html","abstract":"Inference for state-of-the-art deep neural networks is computationally expensive, making them difficult to deploy on constrained hardware environments. An efficient way to reduce this complexity is to quantize the weight parameters and/or activations during training by approximating their distributions with a limited entry codebook. For very low-precisions, such as binary or ternary networks with 1-8-bit activations, the information loss from quantization leads to significant accuracy degradation due to large gradient mismatches between the forward and backward functions. In this paper, we introduce a quantization method to reduce this loss by learning a symmetric codebook for particular weight subgroups. These subgroups are determined based on their locality in the weight matrix, such that the hardware simplicity of the low-precision representations is preserved. Empirically, we show that symmetric quantization can substantially improve accuracy for networks with extremely low-precision weights and activations. We also demonstrate that this representation imposes minimal or no hardware implications to more coarse-grained approaches. Source code is available at https://www.github.com/julianfaraone/SYQ.","中文标题":"SYQ: 学习对称量化以实现高效的深度神经网络","摘要翻译":"对于最先进的深度神经网络来说，推理计算成本高昂，这使得它们难以在受限的硬件环境中部署。减少这种复杂性的一种有效方法是在训练期间通过用有限的条目码本近似其分布来量化权重参数和/或激活。对于非常低的精度，如具有1-8位激活的二进制或三元网络，由于前向和后向函数之间的大梯度不匹配，量化导致的信息丢失会导致显著的精度下降。在本文中，我们引入了一种量化方法，通过学习特定权重子组的对称码本来减少这种损失。这些子组是根据它们在权重矩阵中的局部性确定的，从而保留了低精度表示的硬件简单性。我们通过实验证明，对称量化可以显著提高具有极低精度权重和激活的网络的准确性。我们还证明了这种表示对更粗粒度的方法施加了最小或没有硬件影响。源代码可在https://www.github.com/julianfaraone/SYQ获取。","领域":"神经网络优化/量化技术/硬件加速","问题":"在受限硬件环境中部署深度神经网络时，由于高计算成本导致的推理效率低下问题","动机":"为了在保持硬件简单性的同时，减少深度神经网络在低精度量化时的信息丢失，提高网络准确性","方法":"引入一种量化方法，通过学习特定权重子组的对称码本来减少信息丢失，同时保留低精度表示的硬件简单性","关键词":["量化","深度神经网络","硬件加速","对称码本","低精度"],"涉及的技术概念":"量化是一种减少深度神经网络计算复杂性的技术，通过近似权重参数和激活的分布来减少所需的位数。对称码本是一种特定的量化方法，旨在通过保持权重子组的对称性来减少信息丢失。低精度表示指的是使用较少的位数（如1-8位）来表示网络参数，以降低计算和存储需求。"},{"order":445,"title":"DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching Problems","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bernard_DS_Tighter_Lifting-Free_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bernard_DS_Tighter_Lifting-Free_CVPR_2018_paper.html","abstract":"In this work we study convex relaxations of quadratic optimisation problems over permutation matrices. While existing semidefinite programming approaches can achieve remarkably tight relaxations, they have the strong disadvantage that they lift the original n^2-dimensional variable to an n^4-dimensional variable, which limits their practical applicability. In contrast, here we present a lifting-free convex relaxation that is provably at least as tight as existing (lifting-free) convex relaxations. We demonstrate experimentally that our approach is superior to existing convex and non-convex methods for various problems, including image arrangement and multi-graph matching.","中文标题":"DS*: 针对二次匹配问题的更紧密的无提升凸松弛","摘要翻译":"在本研究中，我们研究了关于置换矩阵的二次优化问题的凸松弛。虽然现有的半定规划方法可以实现非常紧密的松弛，但它们有一个显著的缺点，即它们将原始的n^2维变量提升到n^4维变量，这限制了它们的实际应用性。相比之下，我们在这里提出了一种无提升的凸松弛方法，该方法被证明至少与现有的（无提升）凸松弛方法一样紧密。我们通过实验证明，我们的方法在包括图像排列和多图匹配在内的各种问题上优于现有的凸和非凸方法。","领域":"优化算法/图匹配/图像处理","问题":"解决二次优化问题在置换矩阵上的凸松弛问题","动机":"现有的半定规划方法虽然能实现紧密的松弛，但由于将变量从n^2维提升到n^4维，限制了实际应用性","方法":"提出了一种无提升的凸松弛方法，该方法至少与现有的无提升凸松弛方法一样紧密","关键词":["凸松弛","二次优化","置换矩阵","半定规划","图匹配"],"涉及的技术概念":{"凸松弛":"一种优化技术，通过放宽问题的约束条件来简化问题，使其更容易求解","二次优化":"目标函数为二次函数的优化问题","置换矩阵":"一种特殊的矩阵，每行和每列只有一个1，其余为0，用于表示排列或匹配","半定规划":"一种优化方法，用于求解目标函数和约束条件均为线性矩阵不等式的优化问题","图匹配":"在图论中，寻找两个图之间顶点或边的对应关系的问题"}},{"order":446,"title":"Deep Mutual Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.html","abstract":"Model distillation is an effective and widely used technique to transfer knowledge from a teacher to a student network. The typical application is to transfer from a powerful large network or ensemble to a small network, in order to meet the low-memory or fast execution requirements. In this paper, we present a deep mutual learning (DML) strategy.  Different from the one-way transfer between a static pre-defined teacher and a student in model distillation, with DML, an ensemble of students learn collaboratively and teach each other throughout the training process. Our experiments show that a variety of network architectures benefit from  mutual learning and achieve compelling results on both category and instance recognition tasks.  Surprisingly, it is revealed that no prior powerful teacher network is necessary -- mutual learning of a collection of simple student networks works, and moreover outperforms distillation from a more powerful yet static teacher.","中文标题":"深度互学习","摘要翻译":"模型蒸馏是一种有效且广泛使用的技术，用于将知识从教师网络转移到学生网络。典型的应用是从一个强大的大型网络或集成网络转移到一个小型网络，以满足低内存或快速执行的要求。在本文中，我们提出了一种深度互学习（DML）策略。与模型蒸馏中静态预定义教师和学生之间的单向转移不同，DML允许一组学生在整个训练过程中协作学习并相互教学。我们的实验表明，多种网络架构都能从互学习中受益，并在类别和实例识别任务上取得了令人信服的结果。令人惊讶的是，研究表明，不需要预先存在的强大教师网络——一组简单学生网络的互学习是有效的，而且比从更强大但静态的教师网络进行蒸馏表现更好。","领域":"知识蒸馏/网络架构优化/协作学习","问题":"如何在没有强大教师网络的情况下，通过学生网络之间的协作学习提高模型性能","动机":"探索一种新的学习策略，使得学生网络能够在没有强大教师网络的情况下，通过相互学习提高性能，同时满足低内存或快速执行的需求","方法":"提出深度互学习（DML）策略，允许一组学生在训练过程中协作学习并相互教学","关键词":["知识蒸馏","网络架构优化","协作学习"],"涉及的技术概念":"模型蒸馏是一种技术，用于将知识从教师网络转移到学生网络，通常用于将大型网络的知识转移到小型网络。深度互学习（DML）是一种新的策略，它允许一组学生在训练过程中协作学习并相互教学，而不是依赖于一个静态的教师网络。"},{"order":447,"title":"Coupled End-to-End Transfer Learning With Generalized Fisher Information","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Coupled_End-to-End_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Coupled_End-to-End_Transfer_CVPR_2018_paper.html","abstract":"In transfer learning, one seeks to transfer related information from source tasks with sufficient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framework, which mainly consists of two convolutional neural networks (source and target) that connect to a shared decoder. A novel loss function, the coupled loss, is used for CETL training. From a theoretical perspective, we demonstrate the rationale of the coupled loss by establishing a learning bound for CETL. Moreover, we introduce the generalized Fisher information to improve multi-task optimization in CETL. From a practical aspect, CETL provides a unified and highly flexible solution for various learning tasks such as domain adaption and knowledge distillation. Empirical result shows the superior performance of CETL on cross-domain and cross-task image classification.","中文标题":"耦合端到端迁移学习与广义费舍尔信息","摘要翻译":"在迁移学习中，人们试图从数据充足的源任务中转移相关信息，以帮助数据有限的目标任务的学习。本文提出了一种新颖的耦合端到端迁移学习（CETL）框架，主要由两个卷积神经网络（源和目标）组成，它们连接到一个共享的解码器。CETL训练使用了一种新颖的损失函数，即耦合损失。从理论角度，我们通过为CETL建立学习界限来证明耦合损失的合理性。此外，我们引入了广义费舍尔信息以改进CETL中的多任务优化。从实践角度来看，CETL为各种学习任务（如领域适应和知识蒸馏）提供了一个统一且高度灵活的解决方案。实证结果显示，CETL在跨领域和跨任务图像分类上表现出优越的性能。","领域":"迁移学习/图像分类/多任务学习","问题":"如何在数据有限的目标任务中有效利用源任务的信息进行学习","动机":"解决目标任务数据有限的问题，通过迁移学习提高学习效率和效果","方法":"提出耦合端到端迁移学习框架，使用耦合损失函数和广义费舍尔信息进行多任务优化","关键词":["迁移学习","图像分类","多任务学习","耦合损失","广义费舍尔信息"],"涉及的技术概念":{"耦合端到端迁移学习（CETL）":"一种新颖的迁移学习框架，通过两个卷积神经网络连接到一个共享解码器，使用耦合损失函数进行训练","耦合损失":"CETL框架中使用的损失函数，通过建立学习界限证明其合理性","广义费舍尔信息":"用于改进CETL中的多任务优化，提高学习效率和效果"}},{"order":448,"title":"Residual Parameter Transfer for Deep Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rozantsev_Residual_Parameter_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rozantsev_Residual_Parameter_Transfer_CVPR_2018_paper.html","abstract":"The goal of Deep Domain Adaptation is to make it possible to use Deep Nets trained in one domain where there is enough annotated training data in another where there is little or none. Most current approaches have focused on learning feature representations that are invariant to the changes that occur when going from one domain to the other, which means using the same network parameters in both domains. While some recent algorithms explicitly model the changes by adapting the network parameters, they either severely restrict the possible domain changes, or significantly increase the number of model parameters.  By contrast, we introduce a network architecture that includes auxiliary residual networks, which we train to predict the parameters in the domain with little annotated data from those in the other one. This architecture enables us to flexibly preserve the similarities between domains where they exist and model the differences when necessary. We demonstrate that our approach yields higher accuracy than state-of-the-art methods without undue complexity.","中文标题":"深度域适应的残差参数传递","摘要翻译":"深度域适应的目标是使得在一个有足够标注训练数据的领域训练的深度网络能够在另一个几乎没有或没有标注数据的领域中使用。当前大多数方法集中于学习对从一个领域到另一个领域变化不变的特征表示，这意味着在两个领域中使用相同的网络参数。虽然一些最近的算法通过调整网络参数来显式地建模这些变化，但它们要么严重限制了可能的领域变化，要么显著增加了模型参数的数量。相比之下，我们引入了一种包含辅助残差网络的网络架构，我们训练这些网络从有大量标注数据的领域预测几乎没有标注数据的领域的参数。这种架构使我们能够灵活地保持领域间存在的相似性，并在必要时建模差异。我们证明了我们的方法在不增加不必要复杂性的情况下，比最先进的方法具有更高的准确性。","领域":"深度域适应/神经网络/参数传递","问题":"如何在几乎没有标注数据的领域中有效使用在另一个有足够标注数据的领域训练的深度网络","动机":"解决现有方法在跨领域应用时要么限制领域变化范围，要么显著增加模型参数数量的问题","方法":"引入包含辅助残差网络的网络架构，通过训练这些网络从有大量标注数据的领域预测几乎没有标注数据的领域的参数，以灵活保持领域间的相似性和建模差异","关键词":["深度域适应","残差网络","参数传递"],"涉及的技术概念":"深度域适应是指在有标注数据充足的领域训练的深度网络模型能够适应到标注数据稀缺的领域。残差网络是一种通过引入跳跃连接来避免深层网络训练过程中梯度消失问题的网络架构。参数传递指的是在不同领域间传递网络参数，以利用源领域的知识来改善目标领域的模型性能。"},{"order":449,"title":"High-Order Tensor Regularization With Application to Attribute Ranking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_High-Order_Tensor_Regularization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_High-Order_Tensor_Regularization_CVPR_2018_paper.html","abstract":"When learning functions on manifolds, we can improve performance by regularizing with respect to the intrinsic manifold geometry rather than the ambient space. However, when regularizing tensor learning, calculating the derivatives along this intrinsic geometry is not possible, and so existing approaches are limited to regularizing in Euclidean space. Our new method for intrinsically regularizing and learning tensors on Riemannian manifolds introduces a surrogate object to encapsulate the geometric characteristic of the tensor. Regularizing this instead allows us to learn non-symmetric and high-order tensors. We apply our approach to the relative attributes problem, and we demonstrate that explicitly regularizing high-order relationships between pairs of data points improves performance.","中文标题":"高阶张量正则化及其在属性排序中的应用","摘要翻译":"在学习流形上的函数时，通过相对于内在流形几何而非周围空间进行正则化，我们可以提高性能。然而，在正则化张量学习时，沿着这种内在几何计算导数是不可能的，因此现有方法仅限于在欧几里得空间中进行正则化。我们在黎曼流形上引入了一种新的方法来内在正则化和学习张量，该方法引入了一个替代对象来封装张量的几何特性。正则化这个替代对象使我们能够学习非对称和高阶张量。我们将我们的方法应用于相对属性问题，并证明了显式正则化数据点对之间的高阶关系可以提高性能。","领域":"张量学习/黎曼几何/属性排序","问题":"在流形上学习张量时，如何有效地进行内在几何的正则化","动机":"提高在流形上学习函数的性能，通过引入内在几何的正则化方法","方法":"引入替代对象来封装张量的几何特性，并在黎曼流形上进行正则化和学习","关键词":["张量学习","黎曼几何","属性排序","高阶张量","正则化"],"涉及的技术概念":"摘要中提到的技术概念包括流形上的函数学习、内在几何正则化、黎曼流形、张量学习、非对称和高阶张量、以及相对属性问题。这些概念涉及到在特定的几何空间（如黎曼流形）中进行数据分析和学习的方法，以及如何通过正则化技术来提高学习性能。"},{"order":450,"title":"Learning to Localize Sound Source in Visual Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Senocak_Learning_to_Localize_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Senocak_Learning_to_Localize_CVPR_2018_paper.html","abstract":"Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We show that even with a few supervision, i.e., semi-supervised setup, false conclusion is able to be corrected effectively.","中文标题":"学习在视觉场景中定位声源","摘要翻译":"在我们的日常生活中，视觉事件通常伴随着声音。我们提出了一个问题：机器能否像人类一样，通过观察声音和视觉场景对来学习视觉场景与声音之间的对应关系，并定位声源？在本文中，我们提出了一种新颖的无监督算法来解决在视觉场景中定位声源的问题。开发了一种处理每种模态的双流网络结构，并采用注意力机制进行声源定位。此外，尽管我们的网络是在无监督学习框架内制定的，但它可以通过简单的修改扩展到监督和半监督学习设置的统一架构。同时，开发了一个新的声源数据集用于性能评估。我们的实证评估表明，无监督方法在某些情况下最终会得出错误结论。我们展示了即使只有少量的监督，即半监督设置，也能有效地纠正错误结论。","领域":"声源定位/视觉场景理解/无监督学习","问题":"在视觉场景中定位声源","动机":"探索机器是否能够通过观察声音和视觉场景对来学习视觉场景与声音之间的对应关系，并定位声源，类似于人类的能力。","方法":"提出了一种新颖的无监督算法，采用双流网络结构处理每种模态，并利用注意力机制进行声源定位。该算法可以扩展到监督和半监督学习设置的统一架构。","关键词":["声源定位","视觉场景理解","无监督学习","注意力机制","半监督学习"],"涉及的技术概念":"双流网络结构用于处理视觉和声音两种模态，注意力机制用于提高声源定位的准确性，无监督学习框架允许算法在没有标注数据的情况下学习，同时该框架可以扩展到监督和半监督学习设置。"},{"order":451,"title":"Dynamic Few-Shot Visual Learning Without Forgetting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html","abstract":"The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on \\"unseen\\" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick where we also achieve state-of-the-art results.","中文标题":"动态少样本视觉学习而不遗忘","摘要翻译":"人类的视觉系统具有显著的能力，能够轻松地从仅有的几个例子中学习新概念。在机器学习视觉系统中模仿这种行为是一个有趣且极具挑战性的研究问题，对现实世界的视觉应用有许多实际优势。在此背景下，我们工作的目标是设计一个少样本视觉学习系统，在测试时能够有效地从仅有的几个训练数据中学习新类别，同时不会忘记它最初训练的类别（这里称为基础类别）。为了实现这一目标，我们提出了（a）通过基于注意力的少样本分类权重生成器扩展对象识别系统，以及（b）将ConvNet模型的分类器重新设计为特征表示和分类权重向量之间的余弦相似度函数。后者不仅统一了对新类别和基础类别的识别，还导致了在“未见”类别上更好的特征表示泛化。我们在Mini-ImageNet上广泛评估了我们的方法，成功提高了少样本识别的最先进水平（即，在1-shot和5-shot设置下分别达到了56.20%和73.00%），同时我们没有牺牲基础类别的任何准确性，这是大多数先前方法所缺乏的特性。最后，我们将我们的方法应用于最近引入的Bharath和Girshick的少样本基准测试中，同样取得了最先进的结果。","领域":"少样本学习/注意力机制/卷积神经网络","问题":"如何在少样本视觉学习中有效学习新类别而不遗忘基础类别","动机":"模仿人类视觉系统轻松学习新概念的能力，解决机器学习视觉系统中的少样本学习问题","方法":"扩展对象识别系统以包含基于注意力的少样本分类权重生成器，并重新设计ConvNet模型的分类器为特征表示和分类权重向量之间的余弦相似度函数","关键词":["少样本学习","注意力机制","卷积神经网络","余弦相似度"],"涉及的技术概念":"少样本学习指的是从非常有限的样本中学习新类别的能力；注意力机制是一种使模型能够专注于输入数据中重要部分的技术；卷积神经网络（ConvNet）是一种深度学习模型，特别适用于处理图像数据；余弦相似度是一种衡量两个向量方向相似度的方法，用于重新设计分类器以统一识别新类别和基础类别。"},{"order":452,"title":"Two-Step Quantization for Low-Bit Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Two-Step_Quantization_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Two-Step_Quantization_for_CVPR_2018_paper.html","abstract":"Every bit matters in the hardware design of quantized neural networks. However, extremely-low-bit representation usually causes large accuracy drop. Thus, how to train extremely-low-bit neural networks with high accuracy is of central importance. Most existing network quantization approaches learn transformations (low-bit weights) as well as encodings (low-bit activations) simultaneously. This tight coupling makes the optimization problem difficult, and thus prevents the network from learning optimal representations. In this paper, we propose a simple yet effective Two-Step Quantization (TSQ) framework, by decomposing the network quantization problem into two steps: code learning and transformation function learning based on the learned codes. For the first step, we propose the sparse quantization method for code learning. The second step can be formulated as a non-linear least square regression problem with low-bit constraints, which can be solved efficiently in an iterative manner. Extensive experiments on CIFAR-10 and ILSVRC-12 datasets demonstrate that the proposed TSQ is effective and outperforms the state-of-the-art by a large margin. Especially, for 2-bit activation and ternary weight quantization of AlexNet, the accuracy of our TSQ drops only about 0.5 points compared with the full-precision counterpart, outperforming current state-of-the-art by more than 5 points.","中文标题":"两步量化用于低位神经网络","摘要翻译":"在量化神经网络的硬件设计中，每一位都至关重要。然而，极低位表示通常会导致准确率大幅下降。因此，如何训练出高准确率的极低位神经网络至关重要。大多数现有的网络量化方法同时学习变换（低位权重）和编码（低位激活）。这种紧密耦合使得优化问题变得困难，从而阻止网络学习到最优表示。在本文中，我们提出了一个简单而有效的两步量化（TSQ）框架，通过将网络量化问题分解为两个步骤：基于学习到的编码的代码学习和变换函数学习。对于第一步，我们提出了稀疏量化方法用于代码学习。第二步可以表述为一个带有低位约束的非线性最小二乘回归问题，可以以迭代方式高效解决。在CIFAR-10和ILSVRC-12数据集上的大量实验证明，所提出的TSQ是有效的，并且大幅超越了现有技术。特别是，对于AlexNet的2位激活和三值权重量化，我们的TSQ与全精度版本相比，准确率仅下降约0.5个百分点，比当前最先进的技术高出5个百分点以上。","领域":"神经网络量化/硬件加速/模型压缩","问题":"如何训练出高准确率的极低位神经网络","动机":"极低位表示通常会导致准确率大幅下降，需要一种方法来训练出高准确率的极低位神经网络","方法":"提出了一个两步量化（TSQ）框架，通过将网络量化问题分解为代码学习和变换函数学习两个步骤，采用稀疏量化方法进行代码学习，并将变换函数学习表述为一个带有低位约束的非线性最小二乘回归问题","关键词":["神经网络量化","硬件加速","模型压缩"],"涉及的技术概念":"量化神经网络、低位权重、低位激活、稀疏量化、非线性最小二乘回归、CIFAR-10、ILSVRC-12、AlexNet"},{"order":453,"title":"Improved Lossy Image Compression With Priming and Spatially Adaptive Bit Rates for Recurrent Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Johnston_Improved_Lossy_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Johnston_Improved_Lossy_Image_CVPR_2018_paper.html","abstract":"We propose a method for lossy image compression based on recurrent, convolutional neural networks that outper- forms BPG (4:2:0), WebP, JPEG2000, and JPEG as mea- sured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result us- ing a single model. First, we modify the recurrent architec- ture to improve spatial diffusion, which allows the network to more effectively capture and propagate image informa- tion through the network’s hidden state. Second, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited num- ber of bits to encode visually complex image regions. Fi- nally, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to sev- eral metrics. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well as recently published methods based on deep neural networks.","中文标题":"改进的基于循环网络的有损图像压缩与启动和空间自适应比特率","摘要翻译":"我们提出了一种基于循环卷积神经网络的有损图像压缩方法，该方法在MS-SSIM测量下优于BPG（4:2:0）、WebP、JPEG2000和JPEG。我们引入了三项改进，这些改进使得使用单一模型达到了最先进的结果。首先，我们修改了循环架构以改善空间扩散，这使得网络能够更有效地通过网络的隐藏状态捕获和传播图像信息。其次，除了无损熵编码外，我们还使用了一种空间自适应比特分配算法，以更有效地使用有限的比特数来编码视觉上复杂的图像区域。最后，我们展示了通过SSIM加权的逐像素损失训练可以提高重建质量，根据多项指标进行评估。我们在Kodak和Tecnick图像集上评估了我们的方法，并与标准编解码器以及最近发布的基于深度神经网络的方法进行了比较。","领域":"图像压缩/循环神经网络/卷积神经网络","问题":"提高有损图像压缩的效率和质量","动机":"为了超越现有的图像压缩标准（如BPG、WebP、JPEG2000和JPEG），并提高重建图像的质量","方法":"修改循环架构以改善空间扩散，使用空间自适应比特分配算法，以及通过SSIM加权的逐像素损失进行训练","关键词":["图像压缩","循环神经网络","卷积神经网络","空间自适应比特分配","SSIM"],"涉及的技术概念":"循环卷积神经网络（用于图像压缩），空间扩散（改善图像信息在网络中的传播），空间自适应比特分配算法（优化比特使用），SSIM（结构相似性指标，用于评估图像质量）"},{"order":454,"title":"Conditional Probability Models for Deep Image Compression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.html","abstract":"Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.","中文标题":"深度图像压缩的条件概率模型","摘要翻译":"最近，作为图像自动编码器训练的深度神经网络已成为推进图像压缩技术前沿的一个有前景的方向。学习这种网络的关键挑战有两个：处理量化问题，以及控制潜在图像表示的重建误差（失真）和熵（率）之间的权衡。在本文中，我们专注于后一个挑战，并提出了一种新技术来导航图像压缩自动编码器的率失真权衡。主要思想是通过使用上下文模型直接建模潜在表示的熵：一个3D-CNN，它学习自动编码器潜在分布的条件概率模型。在训练过程中，自动编码器利用上下文模型来估计其表示的熵，同时更新上下文模型以学习潜在表示中符号之间的依赖关系。我们的实验表明，当以MS-SSIM衡量时，这种方法基于简单的卷积自动编码器产生了最先进的图像压缩系统。","领域":"图像压缩/自动编码器/熵编码","问题":"控制图像压缩中重建误差和熵之间的权衡","动机":"推进图像压缩技术的前沿，解决量化问题和率失真权衡","方法":"使用3D-CNN作为上下文模型直接建模潜在表示的熵，以导航率失真权衡","关键词":["图像压缩","自动编码器","熵编码","3D-CNN","率失真权衡"],"涉及的技术概念":"深度神经网络、图像自动编码器、量化、重建误差（失真）、熵（率）、潜在图像表示、上下文模型、3D-CNN、条件概率模型、MS-SSIM"},{"order":455,"title":"Deep Diffeomorphic Transformer Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Detlefsen_Deep_Diffeomorphic_Transformer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Detlefsen_Deep_Diffeomorphic_Transformer_CVPR_2018_paper.html","abstract":"Spatial Transformer layers allow neural networks, at least in principle, to be invariant to large spatial transformations in image data. The model has, however, seen limited uptake as most practical implementations support only transformations that are too restricted, e.g. affine or homographic maps, and/or destructive maps, such as thin plate splines. We investigate the use of ﬂexible diffeomorphic image transformations within such networks and demonstrate that significant performance gains can be attained over currently-used models. The learned transformations are found to be both simple and intuitive, thereby providing insights into individual problem domains. With the proposed framework, a standard convolutional neural network matches state-of-the-art results on face veriﬁcation with only two extra lines of simple TensorFlow code.","中文标题":"深度微分同胚变换网络","摘要翻译":"空间变换层允许神经网络，至少在原则上，对图像数据中的大空间变换保持不变。然而，该模型的采用有限，因为大多数实际实现仅支持过于受限的变换，例如仿射或同形映射，和/或破坏性映射，如薄板样条。我们研究了在这种网络中使用灵活的微分同胚图像变换，并证明与当前使用的模型相比，可以显著提高性能。学习到的变换既简单又直观，从而为个别问题领域提供了见解。通过提出的框架，一个标准的卷积神经网络仅用两行简单的TensorFlow代码就能在面部验证上达到最先进的结果。","领域":"面部识别/图像变换/神经网络","问题":"提高神经网络对图像数据中空间变换的适应性和性能","动机":"现有空间变换层支持的变换过于受限，限制了神经网络的性能和适用性","方法":"研究并应用灵活的微分同胚图像变换于神经网络中","关键词":["空间变换","微分同胚","卷积神经网络"],"涉及的技术概念":"空间变换层允许神经网络对图像数据中的大空间变换保持不变，但实际应用中支持的变换类型有限。本文研究了使用灵活的微分同胚图像变换，以提高神经网络的性能和适应性。通过这种方法，学习到的变换既简单又直观，有助于理解个别问题领域。提出的框架使得标准卷积神经网络在面部验证任务上仅需添加两行简单的TensorFlow代码即可达到最先进的结果。"},{"order":456,"title":"The Lovász-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Berman_The_LovaSz-Softmax_Loss_CVPR_2018_paper.html","abstract":"The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lovász extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.","中文标题":"Lovász-Softmax损失：神经网络中交并比度量优化的可行替代方案","摘要翻译":"Jaccard指数，也称为交并比得分，因其感知质量、尺度不变性（这对小物体赋予了适当的相关性）以及与每像素损失相比对假阴性的适当计数，常用于图像分割结果的评估。我们提出了一种在语义图像分割的上下文中，基于子模损失的凸Lovász扩展，直接优化神经网络中平均交并比损失的方法。与传统的交叉熵损失相比，该损失在Jaccard指数度量上表现更好。我们展示了每张图像优化Jaccard指数与在整个数据集上优化Jaccard指数之间的定量和定性差异。我们在语义分割流程中评估了我们方法的影响，并展示了在使用最先进的深度学习分割架构时，Pascal VOC和Cityscapes数据集上的交并比分割得分显著提高。","领域":"语义分割/图像分割/深度学习","问题":"优化神经网络中的交并比（IoU）损失，以提高图像分割的准确性","动机":"由于Jaccard指数（交并比）在评估图像分割结果时具有感知质量、尺度不变性和对假阴性的适当计数等优点，因此需要一种方法直接优化这一指标，以提高分割性能","方法":"基于子模损失的凸Lovász扩展，提出了一种直接优化神经网络中平均交并比损失的方法","关键词":["语义分割","交并比","Lovász扩展"],"涉及的技术概念":"Jaccard指数（交并比）用于评估图像分割结果，具有感知质量、尺度不变性和对假阴性的适当计数等优点。提出的方法基于子模损失的凸Lovász扩展，直接优化神经网络中的平均交并比损失，以提高分割性能。"},{"order":457,"title":"Generative Adversarial Perturbations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.html","abstract":"In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.","中文标题":"生成对抗扰动","摘要翻译":"在本文中，我们提出了新颖的生成模型，用于创建对抗样本，这些样本是略微扰动的图像，类似于自然图像，但恶意制作以欺骗预训练模型。我们提出了可训练的深度神经网络，用于将图像转换为对抗扰动。我们提出的模型可以生成图像无关和图像依赖的扰动，用于目标和非目标攻击。我们还证明了类似的架构可以在欺骗分类和语义分割模型方面取得令人印象深刻的结果，从而消除了为每个任务手工制作攻击方法的需要。通过在具有挑战性的高分辨率数据集（如ImageNet和Cityscapes）上进行广泛的实验，我们展示了我们的扰动在小扰动范数下实现了高欺骗率。此外，我们的攻击在推理时比当前的迭代方法快得多。","领域":"对抗样本生成/图像分类/语义分割","问题":"如何有效地生成对抗样本以欺骗预训练模型","动机":"探索更高效、更通用的对抗样本生成方法，以评估和提高深度学习模型的鲁棒性","方法":"提出了一种基于深度神经网络的生成模型，能够生成图像无关和图像依赖的对抗扰动，适用于目标和非目标攻击","关键词":["对抗样本","深度神经网络","图像分类","语义分割","高分辨率数据集"],"涉及的技术概念":"对抗样本是指通过添加微小扰动到原始图像上生成的样本，这些样本能够欺骗深度学习模型，使其做出错误的预测。本文提出的方法利用深度神经网络生成这些对抗扰动，旨在提高生成效率和攻击成功率，同时减少扰动的大小。通过在高分辨率数据集上的实验，证明了该方法的有效性和高效性。"},{"order":458,"title":"Learning Strict Identity Mappings in Deep Residual Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_Strict_Identity_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Learning_Strict_Identity_CVPR_2018_paper.html","abstract":"A family of super deep networks, referred to as residual networks or ResNet~cite{he2016deep}, achieved record-beating performance in various visual tasks such as image recognition, object detection, and semantic segmentation. The ability to train very deep networks naturally pushed the researchers to use enormous resources to achieve the best performance. Consequently, in many applications super deep residual networks were employed for just a marginal improvement in performance. In this paper, we propose $epsilon$-ResNet that allows us to automatically discard redundant layers, which produces responses that are smaller than a threshold $epsilon$, without any loss in performance. The $epsilon$-ResNet architecture can be achieved using a few additional rectified linear units in the original ResNet. Our method does not use any additional variables nor numerous trials like other hyper-parameter optimization techniques. The layer selection is achieved using a single training process and the evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. In some instances, we achieve about 80% reduction in the number of parameters.","中文标题":"学习深度残差网络中的严格恒等映射","摘要翻译":"一系列被称为残差网络或ResNet的超深网络，在各种视觉任务中取得了破纪录的性能，如图像识别、物体检测和语义分割。训练非常深网络的能力自然推动了研究人员使用巨大的资源以达到最佳性能。因此，在许多应用中，超深残差网络仅用于性能的边际提升。在本文中，我们提出了$epsilon$-ResNet，它允许我们自动丢弃响应小于阈值$epsilon$的冗余层，而不会损失性能。$epsilon$-ResNet架构可以通过在原始ResNet中使用一些额外的修正线性单元来实现。我们的方法不使用任何额外的变量，也不像其他超参数优化技术那样进行多次试验。层选择通过单一训练过程实现，并在CIFAR-10、CIFAR-100、SVHN和ImageNet数据集上进行评估。在某些情况下，我们实现了约80%的参数减少。","领域":"神经网络优化/模型压缩/自动化机器学习","问题":"如何在不损失性能的情况下自动丢弃深度残差网络中的冗余层","动机":"为了减少训练超深残差网络所需的巨大资源，同时保持或提升网络性能","方法":"提出$epsilon$-ResNet架构，通过在原始ResNet中添加少量修正线性单元，自动丢弃响应小于阈值$epsilon$的冗余层","关键词":["残差网络","模型压缩","自动化层选择"],"涉及的技术概念":"残差网络（ResNet）是一种深度神经网络架构，通过引入“跳跃连接”来解决深层网络训练中的梯度消失问题。$epsilon$-ResNet是对ResNet的改进，旨在自动识别并移除网络中不重要的层，从而减少模型的复杂性和计算资源的需求，而不牺牲性能。"},{"order":459,"title":"Geometric Robustness of Deep Networks: Analysis and Improvement","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanbak_Geometric_Robustness_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kanbak_Geometric_Robustness_of_CVPR_2018_paper.html","abstract":"Deep convolutional neural networks have been shown to be vulnerable to arbitrary geometric transformations. However, there is no systematic method to measure the invariance properties of deep networks to such transformations. We propose ManiFool as a simple yet scalable algorithm to measure the invariance of deep networks. In particular, our algorithm measures the robustness of deep networks to geometric transformations in a worst-case regime as they can be problematic for sensitive applications. Our extensive experimental results show that ManiFool can be used to measure the invariance of fairly complex networks on high dimensional datasets and these values can be used for analyzing the reasons for it. Furthermore, we build on ManiFool to propose a new adversarial training scheme and we show its effectiveness on improving the invariance properties of deep neural networks.","中文标题":"深度网络的几何鲁棒性：分析与改进","摘要翻译":"深度卷积神经网络已被证明对任意几何变换具有脆弱性。然而，目前尚无系统的方法来衡量深度网络对此类变换的不变性。我们提出了ManiFool作为一种简单且可扩展的算法，用于测量深度网络的不变性。特别是，我们的算法在最坏情况下测量深度网络对几何变换的鲁棒性，因为这些变换对于敏感应用可能是有问题的。我们的大量实验结果表明，ManiFool可用于测量相当复杂的网络在高维数据集上的不变性，并且这些值可用于分析其原因。此外，我们在ManiFool的基础上提出了一种新的对抗训练方案，并展示了其在提高深度神经网络不变性方面的有效性。","领域":"几何变换/对抗训练/网络鲁棒性","问题":"深度卷积神经网络对几何变换的脆弱性","动机":"缺乏系统的方法来衡量深度网络对几何变换的不变性","方法":"提出ManiFool算法测量深度网络的不变性，并基于此提出新的对抗训练方案","关键词":["几何变换","对抗训练","网络鲁棒性","不变性测量"],"涉及的技术概念":{"深度卷积神经网络":"一种深度学习模型，特别适用于处理图像数据。","几何变换":"对图像进行旋转、缩放、平移等操作，改变图像的几何属性。","对抗训练":"一种训练方法，通过引入对抗样本来提高模型的鲁棒性。","ManiFool算法":"一种用于测量深度网络对几何变换不变性的算法。","网络鲁棒性":"指网络在面对输入数据的小变化时，输出结果的稳定性。"}},{"order":460,"title":"View Extrapolation of Human Body From a Single Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_View_Extrapolation_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_View_Extrapolation_of_CVPR_2018_paper.html","abstract":"We study how to synthesize novel views of human body from a single image. Though recent deep learning based methods work well for rigid objects, they often fail on objects with large articulation, like human bodies. The core step of existing methods is to fit a map from the observable views to novel views by CNNs; however, the rich articulation modes of human body make it rather challenging for CNNs to memorize and interpolate the data well. To address the problem, we propose a novel deep learning based pipeline that explicitly estimates and leverages the geometry of the underlying human body. Our new pipeline is a composition of a shape estimation network and an image generation network, and at the interface a perspective transformation is applied to generate a forward flow for pixel value transportation. Our design is able to factor out the space of data variation and makes learning at each step much easier. Empirically, we show that the performance for pose-varying objects can be improved dramatically. Our method can also be applied on real data captured by 3D sensors, and the flow generated by our methods can be used for generating high quality results in higher resolution.","中文标题":"从单张图像进行人体视图外推","摘要翻译":"我们研究了如何从单张图像合成人体的新视图。尽管最近基于深度学习的方法在刚性物体上表现良好，但它们往往在具有大关节的物体（如人体）上失败。现有方法的核心步骤是通过卷积神经网络（CNNs）从可观察视图拟合到新视图的映射；然而，人体丰富的关节模式使得CNNs难以很好地记忆和插值数据。为了解决这个问题，我们提出了一种新的基于深度学习的流程，该流程明确估计并利用基础人体的几何形状。我们的新流程由形状估计网络和图像生成网络组成，在接口处应用透视变换以生成用于像素值传输的前向流。我们的设计能够分解数据变化的空间，并使每一步的学习变得更加容易。经验上，我们展示了对于姿态变化物体的性能可以显著提高。我们的方法也可以应用于由3D传感器捕获的真实数据，并且我们的方法生成的流可以用于生成更高分辨率的高质量结果。","领域":"人体姿态估计/视图合成/图像生成","问题":"从单张图像合成人体的新视图","动机":"现有基于深度学习的方法在处理具有大关节的物体（如人体）时表现不佳，需要一种新的方法来提高视图合成的性能。","方法":"提出了一种新的基于深度学习的流程，包括形状估计网络和图像生成网络，通过透视变换生成前向流用于像素值传输。","关键词":["视图合成","人体姿态估计","图像生成"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，用于从图像中提取特征。","透视变换":"一种图像处理技术，用于模拟不同视角下的图像变换。","前向流":"在图像处理中，指用于描述像素从一个图像到另一个图像移动的向量场。","3D传感器":"用于捕获物体三维形状和位置的设备。"}},{"order":461,"title":"Geometry Aware Constrained Optimization Techniques for Deep Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Geometry_Aware_Constrained_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Geometry_Aware_Constrained_CVPR_2018_paper.html","abstract":"In this paper, we generalize the Stochastic Gradient Descent (SGD) and RMSProp algorithms to the setting of Riemannian optimization. SGD is a popular method for large scale optimization. In particular, it is widely used to train the weights of Deep Neural Networks. However, gradients computed using standard SGD can have large variance, which is detrimental for the convergence rate of the algorithm. Other methods such as RMSProp and ADAM address this issue. Nevertheless, these methods cannot be directly applied to constrained optimization problems. In this paper, we extend some popular optimization algorithm to the Riemannian (constrained) setting. We substantiate our proposed extensions with a range of relevant problems in machine learning such as incremental Principal Component Analysis, computating the Riemannian centroids of SPD matrices, and Deep Metric Learning. We achieve competitive results against the state of the art for fine-grained  object recognition datasets.","中文标题":"几何感知的深度学习约束优化技术","摘要翻译":"在本文中，我们将随机梯度下降（SGD）和RMSProp算法推广到黎曼优化的设置中。SGD是大规模优化的一种流行方法。特别是，它被广泛用于训练深度神经网络的权重。然而，使用标准SGD计算的梯度可能具有较大的方差，这对算法的收敛速度是有害的。其他方法如RMSProp和ADAM解决了这个问题。然而，这些方法不能直接应用于约束优化问题。在本文中，我们将一些流行的优化算法扩展到黎曼（约束）设置。我们通过一系列机器学习中的相关问题，如增量主成分分析、计算SPD矩阵的黎曼中心以及深度度量学习，来证实我们提出的扩展。我们在细粒度对象识别数据集上取得了与现有技术相竞争的结果。","领域":"优化算法/黎曼几何/深度度量学习","问题":"解决在黎曼几何设置下的约束优化问题","动机":"标准SGD计算的梯度可能具有较大的方差，影响算法收敛速度，且现有优化方法不能直接应用于约束优化问题","方法":"将SGD和RMSProp算法推广到黎曼优化的设置中，通过增量主成分分析、计算SPD矩阵的黎曼中心以及深度度量学习等问题来证实扩展的有效性","关键词":["随机梯度下降","RMSProp","黎曼优化","深度度量学习","SPD矩阵"],"涉及的技术概念":"随机梯度下降（SGD）是一种用于大规模优化的算法，特别适用于训练深度神经网络的权重。RMSProp和ADAM是解决SGD梯度方差大问题的优化算法。黎曼优化是指在黎曼几何空间中的优化问题，适用于处理约束优化问题。增量主成分分析是一种数据降维技术。SPD矩阵指的是对称正定矩阵，其黎曼中心是这些矩阵在黎曼几何空间中的平均位置。深度度量学习是一种通过深度学习技术来学习数据之间距离度量的方法。"},{"order":462,"title":"PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Uy_PointNetVLAD_Deep_Point_CVPR_2018_paper.html","abstract":"Unlike its image based counterpart, point cloud based retrieval for place recognition has remained as an unexplored and unsolved problem. This is largely due to the difficulty in extracting local feature descriptors from a point cloud that can  subsequently be encoded into a global descriptor for the retrieval task. In this paper, we propose the PointNetVLAD where we leverage on the recent success of deep networks to solve point cloud based retrieval for place recognition. Specifically, our PointNetVLAD is a combination/modification of the existing PointNet and NetVLAD, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. Furthermore, we propose the \\"lazy triplet and quadruplet\\" loss functions that can achieve more discriminative and generalizable global descriptors to tackle the retrieval task. We create benchmark datasets for point cloud based retrieval for place recognition, and the experimental results on these datasets show the feasibility of our PointNetVLAD.","中文标题":"PointNetVLAD: 基于深度点云的大规模地点识别检索","摘要翻译":"与基于图像的对应物不同，基于点云的地点识别检索仍然是一个未探索和未解决的问题。这主要是由于从点云中提取局部特征描述符并将其编码为用于检索任务的全局描述符的困难。在本文中，我们提出了PointNetVLAD，我们利用深度网络的最新成功来解决基于点云的地点识别检索。具体来说，我们的PointNetVLAD是现有PointNet和NetVLAD的组合/修改，它允许端到端的训练和推理，以从给定的3D点云中提取全局描述符。此外，我们提出了“懒惰三元组和四元组”损失函数，可以实现更具区分性和泛化性的全局描述符，以应对检索任务。我们为基于点云的地点识别检索创建了基准数据集，这些数据集上的实验结果表明了我们的PointNetVLAD的可行性。","领域":"三维视觉/地点识别/点云处理","问题":"从点云中提取局部特征描述符并将其编码为全局描述符用于地点识别检索","动机":"解决基于点云的地点识别检索这一未探索和未解决的问题","方法":"结合/修改现有的PointNet和NetVLAD，提出PointNetVLAD进行端到端训练和推理，以及提出“懒惰三元组和四元组”损失函数","关键词":["点云","地点识别","全局描述符","损失函数"],"涉及的技术概念":"PointNet是一种用于处理点云数据的深度网络架构，NetVLAD是一种用于图像检索的深度网络架构，通过结合这两种技术，PointNetVLAD能够从3D点云中提取全局描述符用于地点识别检索。"},{"order":463,"title":"An Efficient and Provable Approach for Mixture Proportion Estimation Using Linear Independence Assumption","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_An_Efficient_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_An_Efficient_and_CVPR_2018_paper.html","abstract":"In this paper, we study the mixture proportion estimation (MPE) problem in a new setting: given samples from the mixture and the component distributions, we identify the proportions of the components in the mixture distribution. To address this problem, we make use of a linear independence assumption, i.e., the component distributions are independent from each other, which is much weaker than assumptions exploited in the previous MPE methods. Based on this assumption, we propose a method (1) that uniquely identifies the mixture proportions, (2) whose output provably converges to the optimal solution, and (3) that is computationally efficient. We show the superiority of the proposed method over the state-of-the-art methods in two applications including learning with label noise and semi-supervised learning on both synthetic and real-world datasets.","中文标题":"使用线性独立性假设进行混合比例估计的高效且可证明的方法","摘要翻译":"本文中，我们研究了一个新设置下的混合比例估计（MPE）问题：给定来自混合分布和组件分布的样本，我们识别混合分布中组件的比例。为了解决这个问题，我们利用了一个线性独立性假设，即组件分布彼此独立，这比之前MPE方法中利用的假设要弱得多。基于这一假设，我们提出了一种方法（1）唯一地识别混合比例，（2）其输出可证明地收敛到最优解，以及（3）计算效率高。我们展示了所提出方法在包括标签噪声学习和半监督学习在内的两个应用中的优越性，这些应用在合成和真实世界的数据集上进行了测试。","领域":"统计学习/数据挖掘/算法优化","问题":"混合比例估计问题","动机":"为了在给定混合分布和组件分布样本的情况下，识别混合分布中组件的比例，提出了一种基于线性独立性假设的新方法。","方法":"提出了一种基于线性独立性假设的方法，该方法能够唯一地识别混合比例，其输出可证明地收敛到最优解，并且计算效率高。","关键词":["混合比例估计","线性独立性假设","标签噪声学习","半监督学习"],"涉及的技术概念":"混合比例估计（MPE）问题涉及从混合分布和组件分布的样本中识别组件比例。线性独立性假设指的是组件分布彼此独立，这为估计混合比例提供了一个较弱的假设基础。提出的方法不仅能够唯一地识别混合比例，而且其输出可证明地收敛到最优解，同时保持计算效率。"},{"order":464,"title":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.html","abstract":"Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms  the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative  representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.","中文标题":"VoxelNet：基于点云的3D物体检测的端到端学习","摘要翻译":"在3D点云中准确检测物体是许多应用中的核心问题，例如自主导航、家政机器人和增强/虚拟现实。为了将高度稀疏的LiDAR点云与区域提议网络（RPN）接口，大多数现有工作都集中在手工制作的特征表示上，例如鸟瞰图投影。在这项工作中，我们消除了对3D点云手动特征工程的需求，并提出了VoxelNet，这是一种通用的3D检测网络，将特征提取和边界框预测统一到一个单一阶段，端到端可训练的深度网络。具体来说，VoxelNet将点云划分为等间距的3D体素，并通过新引入的体素特征编码（VFE）层将每个体素内的一组点转换为统一的特征表示。通过这种方式，点云被编码为描述性的体积表示，然后连接到RPN以生成检测。在KITTI汽车检测基准上的实验表明，VoxelNet大幅超越了基于LiDAR的最先进的3D检测方法。此外，我们的网络学习了具有各种几何形状的物体的有效判别表示，在仅基于LiDAR的行人和骑自行车者的3D检测中取得了令人鼓舞的结果。","领域":"3D物体检测/点云处理/自主导航","问题":"在3D点云中准确检测物体","动机":"消除对3D点云手动特征工程的需求，提高检测的准确性和效率","方法":"提出VoxelNet，一种将特征提取和边界框预测统一到一个单一阶段，端到端可训练的深度网络，通过体素特征编码（VFE）层将点云转换为统一的特征表示","关键词":["3D物体检测","点云处理","自主导航","体素特征编码","LiDAR"],"涉及的技术概念":"VoxelNet是一种深度网络，用于3D物体检测，通过体素特征编码（VFE）层将点云转换为统一的特征表示，然后连接到区域提议网络（RPN）以生成检测。这种方法消除了对手工特征工程的需求，提高了检测的准确性和效率。"},{"order":465,"title":"Image to Image Translation for Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Murez_Image_to_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Murez_Image_to_Image_CVPR_2018_paper.html","abstract":"We propose a general framework for unsupervised domain adaptation, which allows deep neural networks trained on a source domain to be tested on a different target domain without requiring any training annotations in the target domain. This is achieved by adding extra networks and losses that help regularize the features extracted by the backbone encoder network. To this end we propose the novel use of the recently proposed unpaired image-to-image translation framework to constrain the features extracted by the encoder network. Specifically, we require that the features extracted are able to reconstruct the images in both domains. In addition we require that the distribution of features extracted from images in the two domains are indistinguishable. Many recent works can be seen as specific cases of our general framework. We apply our method for domain adaptation between MNIST, USPS, and SVHN datasets, and Amazon, Webcam and DSLR Office datasets in classification tasks, and also between GTA5 and Cityscapes datasets for a segmentation task. We demonstrate state of the art performance on each of these datasets.","中文标题":"图像到图像翻译用于领域适应","摘要翻译":"我们提出了一个无监督领域适应的通用框架，该框架允许在源领域上训练的深度神经网络在不同的目标领域上进行测试，而无需目标领域中的任何训练注释。这是通过添加额外的网络和损失来实现的，这些网络和损失有助于正则化由骨干编码器网络提取的特征。为此，我们提出了最近提出的未配对图像到图像翻译框架的新用途，以约束编码器网络提取的特征。具体来说，我们要求提取的特征能够重建两个领域中的图像。此外，我们要求从两个领域的图像中提取的特征分布是不可区分的。许多最近的工作可以被视为我们通用框架的特定情况。我们将我们的方法应用于MNIST、USPS和SVHN数据集之间，以及Amazon、Webcam和DSLR Office数据集在分类任务中的领域适应，以及GTA5和Cityscapes数据集在分割任务中的领域适应。我们在每个数据集上展示了最先进的性能。","领域":"领域适应/图像翻译/特征正则化","问题":"解决深度神经网络在不同领域间的适应性问题，无需目标领域的训练注释。","动机":"为了使得在源领域上训练的深度神经网络能够在不同的目标领域上有效工作，而无需额外的标注数据。","方法":"通过添加额外的网络和损失来正则化编码器网络提取的特征，并利用未配对图像到图像翻译框架来约束这些特征，确保它们能够重建两个领域中的图像，并且两个领域的特征分布不可区分。","关键词":["领域适应","图像翻译","特征正则化","无监督学习"],"涉及的技术概念":"无监督领域适应、深度神经网络、图像到图像翻译、特征正则化、编码器网络、特征分布"},{"order":466,"title":"MobileNetV2: Inverted Residuals and Linear Bottlenecks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.html","abstract":"In this paper we describe a new mobile architecture, mbox{MobileNetV2}, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call mbox{SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of mbox{DeepLabv3} which we call Mobile mbox{DeepLabv3}.   is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity.  Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design.  Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which  provides a convenient framework for further analysis. We measure our performance on mbox{ImageNet}~cite{Russakovsky:2015:ILS:2846547.2846559} classification, COCO object detection cite{COCO}, VOC image segmentation cite{PASCAL}. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.","中文标题":"MobileNetV2: 倒置残差和线性瓶颈","摘要翻译":"在本文中，我们描述了一种新的移动架构，mbox{MobileNetV2}，它在多个任务和基准测试上提高了移动模型的最新性能，以及跨越一系列不同模型大小。我们还描述了在我们称为mbox{SSDLite}的新框架中应用这些移动模型进行目标检测的有效方法。此外，我们展示了如何通过我们称为Mobile mbox{DeepLabv3}的简化形式构建移动语义分割模型。该架构基于倒置残差结构，其中快捷连接位于薄瓶颈层之间。中间扩展层使用轻量级深度卷积来过滤特征作为非线性的来源。此外，我们发现为了保持表示能力，在窄层中移除非线性是很重要的。我们证明了这一点可以提高性能，并提供了导致这种设计的直觉。最后，我们的方法允许将输入/输出域与转换的表达能力解耦，这为进一步分析提供了一个方便的框架。我们在mbox{ImageNet}分类、COCO目标检测和VOC图像分割上测量了我们的性能。我们评估了准确性、通过乘法加法(MAdd)测量的操作数量、实际延迟以及参数数量之间的权衡。","领域":"目标检测/语义分割/图像分类","问题":"提高移动模型在多个任务和基准测试上的性能","动机":"为了在移动设备上实现更高效的深度学习模型，提高模型在多个任务上的性能","方法":"采用倒置残差结构和线性瓶颈，使用轻量级深度卷积过滤特征，移除窄层中的非线性以保持表示能力","关键词":["倒置残差","线性瓶颈","深度卷积","目标检测","语义分割","图像分类"],"涉及的技术概念":"倒置残差结构是一种网络架构，其中快捷连接位于薄瓶颈层之间，旨在提高模型的效率和性能。线性瓶颈指的是在网络的某些部分使用线性变换而非非线性变换，以保持或增强模型的表示能力。深度卷积是一种卷积操作，它在每个输入通道上独立进行卷积，用于减少计算量和参数数量。"},{"order":467,"title":"Im2Struct: Recovering 3D Shape Structure From a Single RGB Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Niu_Im2Struct_Recovering_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Im2Struct_Recovering_3D_CVPR_2018_paper.html","abstract":"We propose to recover 3D shape structures from single RGB images, where structure refers to shape parts represented by cuboids and part relations encompassing connectivity and symmetry. Given a single 2D image with an object depicted, our goal is automatically recover a cuboid structure of the object parts as well as their mutual relations. We develop a convolutional-recursive auto-encoder comprised of structure parsing of a 2D image followed by structure recovering of a cuboid hierarchy. The encoder is achieved by a multi-scale convolutional network trained with the task of shape contour estimation, thereby learning to discern object structures in various forms and scales. The decoder fuses the features of the structure parsing network and the original image, and recursively decodes a hierarchy of cuboids. Since the decoder network is learned to recover part relations including connectivity and symmetry explicitly, the plausibility and generality of part structure recovery can be ensured. The two networks are jointly trained using the training data of contour-mask and cuboid-structure pairs. Such pairs are generated by rendering stock 3D CAD models coming with part segmentation. Our method achieves unprecedentedly faithful and detailed recovery of diverse 3D part structures from single-view 2D images. We demonstrate two applications of our method including structure-guided completion of 3D volumes reconstructed from single-view images and structure-aware interactive editing of 2D images.","中文标题":"Im2Struct: 从单张RGB图像恢复3D形状结构","摘要翻译":"我们提出从单张RGB图像中恢复3D形状结构，其中结构指的是由长方体表示的形状部分以及包含连接性和对称性的部分关系。给定一张描绘对象的2D图像，我们的目标是自动恢复对象部分的长方体结构及其相互关系。我们开发了一个卷积递归自动编码器，包括2D图像的结构解析和长方体层次结构的结构恢复。编码器通过一个多尺度卷积网络实现，该网络通过形状轮廓估计任务进行训练，从而学会识别各种形式和尺度的对象结构。解码器融合了结构解析网络的特征和原始图像，并递归解码长方体层次结构。由于解码器网络被学习以显式恢复包括连接性和对称性的部分关系，因此可以确保部分结构恢复的合理性和通用性。这两个网络通过轮廓掩码和长方体结构对的训练数据联合训练。这些对通过渲染带有部分分割的库存3D CAD模型生成。我们的方法实现了前所未有的忠实和详细的从单视角2D图像恢复多样3D部分结构。我们展示了我们方法的两个应用，包括从单视角图像重建的3D体积的结构引导完成和2D图像的结构感知交互编辑。","领域":"3D重建/图像解析/自动编码器","问题":"从单张RGB图像中恢复3D形状结构","动机":"为了自动恢复对象部分的长方体结构及其相互关系，以支持3D体积的结构引导完成和2D图像的结构感知交互编辑","方法":"开发了一个卷积递归自动编码器，包括2D图像的结构解析和长方体层次结构的结构恢复，通过多尺度卷积网络和递归解码长方体层次结构实现","关键词":["3D重建","图像解析","自动编码器","卷积网络","递归解码"],"涉及的技术概念":"卷积递归自动编码器是一种结合卷积神经网络和递归神经网络的模型，用于从2D图像中解析并恢复3D结构。多尺度卷积网络能够识别不同尺度的对象结构，而递归解码器则用于逐步构建和细化3D模型的结构。通过联合训练，模型能够学习到如何从单视角图像中恢复出详细且合理的3D部分结构。"},{"order":468,"title":"Trust Your Model: Light Field Depth Estimation With Inline Occlusion Handling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Schilling_Trust_Your_Model_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Schilling_Trust_Your_Model_CVPR_2018_paper.html","abstract":"We address the problem of depth estimation from light-field images. Our main contribution is a new way to handle occlusions which improves general accuracy and quality of object borders. In contrast to all prior work we work with a model which directly incorporates both depth and occlusion, using a local optimization scheme based on the PatchMatch algorithm. The key benefit of this joint approach is that we utilize all available data, and not erroneously discard valuable information in pre-processing steps. We see the benefit of our approach not only at improved object boundaries, but also at smooth surface reconstruction, where we outperform even methods which focus on good surface regularization. We have evaluated our method on a public light-field dataset, where we achieve state-of-the-art results in nine out of twelve error metrics, with a close tie for the remaining three.","中文标题":"信任你的模型：带有内联遮挡处理的光场深度估计","摘要翻译":"我们解决了从光场图像中进行深度估计的问题。我们的主要贡献是提出了一种新的处理遮挡的方法，该方法提高了物体边界的整体准确性和质量。与之前的所有工作不同，我们使用了一个直接结合深度和遮挡的模型，采用基于PatchMatch算法的局部优化方案。这种联合方法的关键优势在于我们利用了所有可用数据，而不是在预处理步骤中错误地丢弃有价值的信息。我们不仅看到了在改进物体边界方面的好处，而且在平滑表面重建方面也优于那些专注于良好表面正则化的方法。我们已经在公开的光场数据集上评估了我们的方法，在十二个误差指标中的九个上取得了最先进的结果，其余三个指标也接近领先。","领域":"光场成像/深度估计/遮挡处理","问题":"从光场图像中进行深度估计，并处理遮挡问题以提高物体边界的准确性和质量","动机":"提高深度估计的准确性，特别是在物体边界和平滑表面重建方面，通过一种新的处理遮挡的方法","方法":"采用了一个直接结合深度和遮挡的模型，使用基于PatchMatch算法的局部优化方案，以利用所有可用数据并避免在预处理步骤中丢弃有价值的信息","关键词":["光场成像","深度估计","遮挡处理","PatchMatch算法","局部优化"],"涉及的技术概念":"光场成像是一种捕捉光场信息的技术，可以用于深度估计。深度估计是从图像中估计场景中每个点到相机的距离。遮挡处理是指在深度估计过程中处理被遮挡物体的问题，以提高估计的准确性。PatchMatch算法是一种用于快速找到图像块之间匹配的算法，常用于图像编辑和计算机视觉任务中。局部优化是指在特定区域内寻找最优解的过程，以提高算法的效率和准确性。"},{"order":469,"title":"Baseline Desensitizing in Translation Averaging","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhuang_Baseline_Desensitizing_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Baseline_Desensitizing_in_CVPR_2018_paper.html","abstract":"Many existing translation averaging algorithms are either sensitive to disparate camera baselines and have to rely on extensive preprocessing to improve the observed Epipolar Geometry graph, or if they are robust against disparate camera baselines, require complicated optimization to minimize the highly nonlinear angular error objective. In this paper, we carefully design a simple yet effective bilinear objective function, introducing a variable to perform the requisite normalization. The objective function enjoys the baseline-insensitive property of the angular error and yet is amenable to simple and efficient optimization by block coordinate descent, with good empirical performance. A rotation-assisted Iterative Reweighted Least Squares scheme is further put forth to help deal with outliers. We also contribute towards a better understanding of the behavior of two recent convex algorithms, LUD and Shapefit/kick, clarifying the underlying subtle difference that leads to the performance gap. Finally, we demonstrate that our algorithm achieves overall superior accuracies in benchmark dataset compared to state-of-the-art methods, and is also several times faster.","中文标题":"平移平均中的基线去敏感化","摘要翻译":"许多现有的平移平均算法要么对不同的相机基线敏感，必须依赖大量的预处理来改善观察到的极线几何图，要么如果它们对不同的相机基线具有鲁棒性，则需要复杂的优化来最小化高度非线性的角度误差目标。在本文中，我们精心设计了一个简单而有效的双线性目标函数，引入了一个变量来执行必要的归一化。该目标函数具有角度误差的基线不敏感特性，并且可以通过块坐标下降法进行简单高效的优化，具有良好的实证性能。进一步提出了一个旋转辅助的迭代重加权最小二乘方案，以帮助处理异常值。我们还为更好地理解两种最近的凸算法LUD和Shapefit/kick的行为做出了贡献，阐明了导致性能差距的潜在微妙差异。最后，我们证明了我们的算法在基准数据集上实现了总体上优于最先进方法的准确度，并且速度也快了几倍。","领域":"三维重建/相机标定/几何优化","问题":"解决平移平均算法对相机基线敏感或需要复杂优化的问题","动机":"提高平移平均算法对不同相机基线的鲁棒性，简化优化过程，提高算法效率和准确度","方法":"设计了一个基线不敏感的双线性目标函数，采用块坐标下降法进行优化，并提出了旋转辅助的迭代重加权最小二乘方案处理异常值","关键词":["平移平均","相机基线","角度误差","块坐标下降法","迭代重加权最小二乘"],"涉及的技术概念":"平移平均算法用于从多个相机视角估计场景的三维结构，其中相机基线指的是两个相机中心之间的距离。极线几何图描述了不同视角下图像点之间的几何关系。角度误差是衡量估计的平移方向与真实方向之间差异的指标。块坐标下降法是一种优化技术，通过交替优化目标函数的各个块来简化问题。迭代重加权最小二乘是一种处理异常值的统计方法，通过迭代调整权重来减少异常值对结果的影响。"},{"order":470,"title":"Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Mining_Point_Cloud_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Mining_Point_Cloud_CVPR_2018_paper.html","abstract":"Unlike on images, semantic learning on 3D point clouds using a deep network is challenging due to the naturally unordered data structure. Among existing works, PointNet has achieved promising results by directly learning on point sets. However, it does not take full advantage of a point's local neighborhood that contains fine-grained structural information which turns out to be helpful towards better semantic learning. In this regard, we present two new operations to improve PointNet with a more efficient exploitation of local structures. The first one focuses on local 3D geometric structures. In analogy to a convolution kernel for images, we define a point-set kernel as a set of learnable 3D points that jointly respond to a set of neighboring data points according to their geometric affinities measured by kernel correlation, adapted from a similar technique for point cloud registration. The second one exploits local high-dimensional feature structures by recursive feature aggregation on a nearest-neighbor-graph computed from 3D positions. Experiments show that our network can efficiently capture local information and robustly achieve better performances on major datasets. Our code is available at http://www.merl.com/research/license#KCNet","中文标题":"通过核相关和图池化挖掘点云局部结构","摘要翻译":"与图像不同，使用深度网络对3D点云进行语义学习具有挑战性，因为其数据结构自然无序。在现有工作中，PointNet通过直接在点集上学习取得了有希望的结果。然而，它没有充分利用包含细粒度结构信息的点的局部邻域，这些信息对于更好的语义学习是有帮助的。鉴于此，我们提出了两种新的操作，以更有效地利用局部结构来改进PointNet。第一个操作专注于局部3D几何结构。与图像的卷积核类似，我们定义了一个点集核作为一组可学习的3D点，这些点根据通过核相关测量的几何亲和力共同响应一组邻近的数据点，这是从点云注册的类似技术中改编而来的。第二个操作通过在从3D位置计算出的最近邻图上递归特征聚合来利用局部高维特征结构。实验表明，我们的网络能够有效地捕捉局部信息，并在主要数据集上稳健地实现更好的性能。我们的代码可在http://www.merl.com/research/license#KCNet获取。","领域":"3D点云处理/几何深度学习/特征学习","问题":"如何更有效地利用3D点云的局部结构信息进行语义学习","动机":"现有的PointNet方法没有充分利用点的局部邻域信息，这些信息对于提高语义学习的效果是有帮助的","方法":"提出了两种新的操作：一是定义点集核来捕捉局部3D几何结构，二是通过最近邻图上的递归特征聚合来利用局部高维特征结构","关键词":["3D点云","局部结构","核相关","图池化","特征聚合"],"涉及的技术概念":{"PointNet":"一种直接在点集上进行学习的深度网络","点集核":"一组可学习的3D点，用于捕捉局部3D几何结构","核相关":"用于测量点之间几何亲和力的技术","最近邻图":"基于3D位置计算的图，用于特征聚合","递归特征聚合":"在图上递归地聚合特征以利用局部高维特征结构"}},{"order":471,"title":"Large-Scale Point Cloud Semantic Segmentation With Superpoint Graphs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.html","abstract":"We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).","中文标题":"大规模点云语义分割与超点图","摘要翻译":"我们提出了一种新颖的基于深度学习的框架，以解决包含数百万点的大规模点云语义分割的挑战。我们认为，通过一种称为超点图（SPG）的结构，可以有效地捕捉3D点云的组织，这种结构源自将扫描场景分割成几何上同质的元素。SPGs提供了对象部分之间上下文关系的紧凑而丰富的表示，然后由图卷积网络利用。我们的框架在分割室外LiDAR扫描（对于Semantic3D测试集分别提高了+11.9和+8.8 mIoU点）以及室内扫描（对于S3DIS数据集提高了+12.4 mIoU点）方面设定了新的技术水平。","领域":"点云处理/语义分割/图卷积网络","问题":"大规模点云的语义分割","动机":"解决包含数百万点的大规模点云语义分割的挑战","方法":"提出了一种基于超点图（SPG）和图卷积网络的深度学习框架","关键词":["点云处理","语义分割","图卷积网络","超点图"],"涉及的技术概念":{"超点图（SPG）":"一种结构，用于捕捉3D点云的组织，源自将扫描场景分割成几何上同质的元素","图卷积网络":"一种深度学习模型，用于处理图结构数据，这里用于利用SPGs提供的上下文关系","mIoU":"平均交并比，用于评估语义分割模型的性能"}},{"order":472,"title":"Very Large-Scale Global SfM by Distributed Motion Averaging","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Very_Large-Scale_Global_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Very_Large-Scale_Global_CVPR_2018_paper.html","abstract":"Global Structure-from-Motion (SfM) techniques have demonstrated superior efficiency and accuracy than the conventional incremental approach in many recent studies. This work proposes a divide-and-conquer framework to solve very large global SfM at the scale of millions of images. Specifically, we first divide all images into multiple partitions that preserve strong data association for well posed and parallel local motion averaging. Then, we solve a global motion averaging that determines cameras at partition boundaries and a similarity transformation per partition to register all cameras in a single coordinate frame. Finally, local and global motion averaging are iterated until convergence. Since local camera poses are fixed during the global motion average, we can avoid caching the whole reconstruction in memory at once. This distributed framework significantly enhances the efficiency and robustness of large-scale motion averaging.","中文标题":"通过分布式运动平均实现超大规模全局结构从运动","摘要翻译":"全局结构从运动（SfM）技术在许多最近的研究中展示了比传统增量方法更高的效率和准确性。这项工作提出了一个分而治之的框架，以解决百万级图像的非常大规模的全局SfM问题。具体来说，我们首先将所有图像划分为多个分区，这些分区保留了强大的数据关联，以便进行良好定位和并行的局部运动平均。然后，我们解决一个全局运动平均问题，确定分区边界的相机和每个分区的相似变换，以将所有相机注册到单一坐标系中。最后，局部和全局运动平均迭代进行，直到收敛。由于在全局运动平均过程中局部相机姿态是固定的，我们可以避免一次性将整个重建缓存到内存中。这个分布式框架显著提高了大规模运动平均的效率和鲁棒性。","领域":"三维重建/运动估计/分布式计算","问题":"解决百万级图像的全局结构从运动问题","动机":"提高大规模运动平均的效率和鲁棒性","方法":"提出一个分而治之的框架，包括图像分区、局部和全局运动平均迭代，以及避免一次性缓存整个重建到内存中","关键词":["全局结构从运动","分布式计算","运动平均","三维重建"],"涉及的技术概念":{"全局结构从运动（SfM）":"一种从二维图像序列中恢复三维场景结构的技术","分布式计算":"将计算任务分散到多个计算节点上执行，以提高效率和可扩展性","运动平均":"通过平均多个运动估计来优化相机姿态和场景结构的过程","三维重建":"从二维图像中恢复三维场景的过程"}},{"order":473,"title":"ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dai_ScanComplete_Large-Scale_Scene_CVPR_2018_paper.html","abstract":"We introduce ScanComplete, a novel data-driven approach for taking an incomplete 3D scan of a scene as input and predicting a complete 3D model along with per-voxel semantic labels. The key contribution of our method is its ability to handle large scenes with varying spatial extent, managing the cubic growth in data size as scene size increases. To this end, we devise a fully-convolutional generative 3D CNN model whose filter kernels are invariant to the overall scene size. The model can be trained on scene subvolumes but deployed on arbitrarily large scenes at test time. In addition, we propose a coarse-to-fine inference strategy in order to produce high-resolution output while also leveraging large input context sizes. In an extensive series of experiments, we carefully evaluate different model design choices, considering both deterministic and probabilistic models for completion and semantic inference. Our results show that we outperform other methods not only in the size of the environments handled and processing efficiency, but also with regard to completion quality and semantic segmentation performance by a significant margin.","中文标题":"ScanComplete: 大规模场景完成与3D扫描的语义分割","摘要翻译":"我们介绍了ScanComplete，一种新颖的数据驱动方法，它以场景的不完整3D扫描作为输入，并预测一个完整的3D模型以及每个体素的语义标签。我们方法的关键贡献在于其能够处理具有不同空间范围的大场景，管理随着场景大小增加而立方增长的数据量。为此，我们设计了一个全卷积生成3D CNN模型，其滤波器核对于整体场景大小是不变的。该模型可以在场景子体积上进行训练，但在测试时可以部署在任意大的场景上。此外，我们提出了一种从粗到细的推理策略，以产生高分辨率输出，同时利用大输入上下文大小。在一系列广泛的实验中，我们仔细评估了不同的模型设计选择，考虑了用于完成和语义推理的确定性和概率性模型。我们的结果表明，我们不仅在处理的环境大小和处理效率上优于其他方法，而且在完成质量和语义分割性能上也显著领先。","领域":"3D重建/语义分割/场景理解","问题":"处理不完整的3D扫描以预测完整的3D模型和每个体素的语义标签","动机":"为了能够处理具有不同空间范围的大场景，并管理随着场景大小增加而立方增长的数据量","方法":"设计了一个全卷积生成3D CNN模型，采用从粗到细的推理策略，以产生高分辨率输出并利用大输入上下文大小","关键词":["3D重建","语义分割","场景理解","全卷积网络","3D CNN"],"涉及的技术概念":"全卷积生成3D CNN模型是一种深度学习模型，用于处理3D数据，能够生成新的3D内容。从粗到细的推理策略是一种逐步细化输出的方法，以提高最终结果的精度和质量。"},{"order":474,"title":"Solving the Perspective-2-Point Problem for Flying-Camera Photo Composition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lan_Solving_the_Perspective-2-Point_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lan_Solving_the_Perspective-2-Point_CVPR_2018_paper.html","abstract":"Drone-mounted flying cameras will revolutionize photo-taking. The user, instead of holding a camera in hand and manually searching for a  viewpoint,   will interact directly with image contents in the viewfinder through simple gestures, and the flying camera will achieve the desired viewpoint  through the autonomous flying capability of the drone. This work studies the underlying viewpoint search problem for composing a photo with two objects of interest, a common situation in photo-taking. We model it as a Perspective-2-Point (P2P) problem, which is under-constrained to determine the six degrees-of-freedom camera pose uniquely. By incorporating the user's composition requirements and minimizing the camera's flying distance, we form a constrained nonlinear optimization problem and solve it in closed form. Experiments on synthetic data sets and on a real flying camera system indicate promising results.","中文标题":"解决飞行相机照片构图中的透视-2-点问题","摘要翻译":"搭载无人机的飞行相机将彻底改变拍照方式。用户无需手持相机手动寻找拍摄角度，而是通过简单的手势直接与取景器中的图像内容互动，飞行相机将利用无人机的自主飞行能力达到理想的拍摄角度。本研究探讨了在拍摄包含两个感兴趣对象的照片时的基本视角搜索问题，这是拍照中的常见情况。我们将其建模为透视-2-点（P2P）问题，该问题在确定六自由度相机姿态时是欠约束的。通过结合用户的构图要求并最小化相机的飞行距离，我们形成了一个约束非线性优化问题，并以闭式解的形式解决了它。在合成数据集和真实飞行相机系统上的实验表明了有希望的结果。","领域":"无人机摄影/自主飞行/图像构图","问题":"在拍摄包含两个感兴趣对象的照片时，如何自动确定最佳的相机拍摄角度","动机":"利用无人机的自主飞行能力，简化用户拍照过程，通过简单手势直接与图像内容互动，达到理想的拍摄角度","方法":"将问题建模为透视-2-点（P2P）问题，通过结合用户的构图要求并最小化相机的飞行距离，形成一个约束非线性优化问题，并以闭式解的形式解决","关键词":["无人机摄影","自主飞行","图像构图"],"涉及的技术概念":"透视-2-点（P2P）问题：在确定六自由度相机姿态时是欠约束的问题；约束非线性优化问题：通过结合特定约束条件，寻找最优解的问题；闭式解：可以直接通过公式计算得到的解，无需迭代或近似。"},{"order":475,"title":"Reflection Removal for Large-Scale 3D Point Clouds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yun_Reflection_Removal_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yun_Reflection_Removal_for_CVPR_2018_paper.html","abstract":"Large-scale 3D point clouds (LS3DPCs) captured by terrestrial LiDAR scanners often exhibit reflection artifacts by glasses, which degrade the performance of related computer vision techniques. In this paper, we propose an efficient reflection removal algorithm for LS3DPCs. We first partition the unit sphere into local surface patches which are then classified into the ordinary patches and the glass patches according to the number of echo pulses from emitted laser pulses. Then we estimate the glass region of dominant reflection artifacts by measuring the reliability. We also detect and remove the virtual points using the conditions of the reflection symmetry and the geometric similarity. We test the performance of the proposed algorithm on LS3DPCs capturing real-world outdoor scenes, and show that the proposed algorithm estimates valid glass regions faithfully and removes the virtual points caused by reflection artifacts successfully.","中文标题":"大规模3D点云的反射去除","摘要翻译":"由地面LiDAR扫描仪捕获的大规模3D点云（LS3DPCs）经常表现出由玻璃引起的反射伪影，这会降低相关计算机视觉技术的性能。在本文中，我们提出了一种针对LS3DPCs的高效反射去除算法。我们首先将单位球体划分为局部表面补丁，然后根据发射的激光脉冲的回波脉冲数量将这些补丁分类为普通补丁和玻璃补丁。接着，我们通过测量可靠性来估计主要反射伪影的玻璃区域。我们还利用反射对称性和几何相似性的条件来检测并去除虚拟点。我们在捕获真实世界户外场景的LS3DPCs上测试了所提出算法的性能，结果表明所提出的算法能够忠实地估计有效的玻璃区域，并成功去除由反射伪影引起的虚拟点。","领域":"3D重建/激光雷达数据处理/反射去除","问题":"大规模3D点云中由玻璃引起的反射伪影问题","动机":"反射伪影会降低计算机视觉技术的性能，因此需要一种有效的方法来去除这些伪影，以提高3D点云的质量和应用效果。","方法":"首先将单位球体划分为局部表面补丁，并根据回波脉冲数量分类为普通补丁和玻璃补丁；然后通过测量可靠性估计主要反射伪影的玻璃区域；最后利用反射对称性和几何相似性条件检测并去除虚拟点。","关键词":["3D点云","反射去除","激光雷达","反射伪影","虚拟点检测"],"涉及的技术概念":"单位球体划分、局部表面补丁分类、回波脉冲数量、反射对称性、几何相似性、虚拟点检测"},{"order":476,"title":"Attentional ShapeContextNet for Point Cloud Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Attentional_ShapeContextNet_for_CVPR_2018_paper.html","abstract":"We tackle the problem of point cloud recognition. Unlike previous approaches where a point cloud is either converted into a volume/image or represented independently in a permutation-invariant set, we develop a new representation by adopting the concept of shape context as the building block in our network design. The resulting model, called ShapeContextNet, consists of a hierarchy with modules not relying on a fixed grid while still enjoying properties similar to those in convolutional neural networks --- being able to capture and propagate the object part information. In addition, we find inspiration from self-attention based models to include a simple yet effective contextual modeling mechanism --- making the contextual region selection, the feature aggregation, and the feature transformation process fully automatic. ShapeContextNet is an end-to-end model that can be applied to the general point cloud classification and segmentation problems. We observe competitive results on a number of benchmark datasets.","中文标题":"注意力形状上下文网络用于点云识别","摘要翻译":"我们解决了点云识别的问题。与之前的方法不同，之前的方法要么将点云转换为体积/图像，要么以排列不变的集合独立表示，我们通过采用形状上下文的概念作为网络设计中的构建块，开发了一种新的表示方法。由此产生的模型称为ShapeContextNet，它由一个层次结构组成，其中的模块不依赖于固定网格，同时仍然享有类似于卷积神经网络中的属性——能够捕获和传播对象部分信息。此外，我们从基于自注意力的模型中获得灵感，包括一个简单但有效的上下文建模机制——使上下文区域选择、特征聚合和特征转换过程完全自动化。ShapeContextNet是一个端到端模型，可以应用于一般的点云分类和分割问题。我们在多个基准数据集上观察到了竞争性的结果。","领域":"点云处理/三维视觉/自注意力机制","问题":"点云识别","动机":"开发一种新的点云表示方法，以克服现有方法在表示点云时的局限性，并提高点云分类和分割的性能。","方法":"采用形状上下文的概念作为网络设计的构建块，构建了一个不依赖于固定网格的层次结构模型，并引入了自注意力机制来实现上下文建模的自动化。","关键词":["点云识别","形状上下文","自注意力机制"],"涉及的技术概念":"形状上下文是一种用于描述点云中点的局部几何形状的方法，自注意力机制是一种能够自动关注输入数据中重要部分的机制，用于提高模型的上下文建模能力。"},{"order":477,"title":"Geometry-Aware Deep Network for Single-Image Novel View Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Geometry-Aware_Deep_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Geometry-Aware_Deep_Network_CVPR_2018_paper.html","abstract":"This paper tackles the problem of novel view synthesis from a single image. In particular, we target real-world scenes with rich geometric structure, a challenging task due to the large appearance variations of such scenes and the lack of simple 3D models to represent them. Modern, learning-based approaches mostly focus on appearance to synthesize novel views and thus tend to generate predictions that are inconsistent with the underlying scene structure. By contrast, in this paper, we propose to exploit the 3D geometry of the scene to synthesize a novel view. Specifically, we approximate a real-world scene by a fixed number of planes, and learn to predict a set of homographies and their corresponding region masks to transform the input image into a novel view. To this end, we develop a new region-aware geometric transform network that performs these multiple tasks in a common framework. Our results on the outdoor KITTI and the indoor ScanNet datasets demonstrate the effectiveness of our network to generate high-quality synthetic views that respect the scene geometry, thus outperforming the state-of-the-art methods.","中文标题":"几何感知深度网络用于单图像新视角合成","摘要翻译":"本文解决了从单张图像进行新视角合成的问题。特别是，我们针对具有丰富几何结构的真实世界场景，这是一个具有挑战性的任务，因为这类场景的外观变化大且缺乏简单的3D模型来表示它们。现代基于学习的方法大多专注于外观以合成新视角，因此往往生成与底层场景结构不一致的预测。相比之下，在本文中，我们提出利用场景的3D几何来合成新视角。具体来说，我们通过固定数量的平面来近似真实世界场景，并学习预测一组单应性及其对应的区域掩码，以将输入图像转换为新视角。为此，我们开发了一个新的区域感知几何变换网络，该网络在一个共同的框架内执行这些多重任务。我们在户外KITTI和室内ScanNet数据集上的结果表明，我们的网络在生成尊重场景几何的高质量合成视图方面有效，从而超越了最先进的方法。","领域":"3D重建/视角合成/几何学习","问题":"从单张图像进行新视角合成，特别是在具有丰富几何结构的真实世界场景中。","动机":"解决现有方法在合成新视角时往往生成与底层场景结构不一致的预测的问题。","方法":"提出利用场景的3D几何来合成新视角，通过固定数量的平面近似真实世界场景，并学习预测一组单应性及其对应的区域掩码，以将输入图像转换为新视角。开发了一个新的区域感知几何变换网络来执行这些任务。","关键词":["3D重建","视角合成","几何学习"],"涉及的技术概念":"3D几何、单应性、区域掩码、区域感知几何变换网络、KITTI数据集、ScanNet数据集"},{"order":478,"title":"InverseFaceNet: Deep Monocular Inverse Face Rendering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_InverseFaceNet_Deep_Monocular_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_InverseFaceNet_Deep_Monocular_CVPR_2018_paper.html","abstract":"We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. By estimating all parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible in real time. Most previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created training corpus. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy.We further propose a self-supervised bootstrapping process in the network training loop, which iteratively updates the synthetic training corpus to better reflect the distribution of real-world imagery. We demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches.","中文标题":"InverseFaceNet: 深度单目逆向面部渲染","摘要翻译":"我们介绍了InverseFaceNet，一个深度卷积逆向渲染框架，用于从单一输入图像中联合估计面部姿态、形状、表情、反射和光照。通过仅从单一图像估计所有参数，使得在单一面部图像上进行高级编辑，如外观编辑和重新光照，变得实时可行。大多数之前基于学习的面部重建方法没有联合恢复所有维度，或者在视觉质量方面受到严重限制。相比之下，我们提出使用深度神经网络恢复高质量的面部姿态、形状、表情、反射和光照，该网络使用大量合成的训练语料库进行训练。我们的方法基于一种新颖的损失函数，该函数直接在参数空间中测量模型空间的相似性，并显著提高了重建精度。我们进一步提出了网络训练循环中的自监督引导过程，该过程迭代更新合成训练语料库，以更好地反映真实世界图像的分布。我们证明了这种策略优于完全合成训练的网络。最后，我们展示了高质量的重建，并将我们的方法与几种最先进的方法进行了比较。","领域":"面部重建/逆向渲染/深度学习","问题":"从单一图像中联合估计面部姿态、形状、表情、反射和光照","动机":"实现单一面部图像的高级编辑，如外观编辑和重新光照，并提高重建精度","方法":"使用深度神经网络和一种新颖的损失函数，以及自监督引导过程迭代更新合成训练语料库","关键词":["面部重建","逆向渲染","深度学习","自监督学习"],"涉及的技术概念":"深度卷积逆向渲染框架、面部姿态、形状、表情、反射和光照的联合估计、新颖的损失函数、自监督引导过程、合成训练语料库的迭代更新"},{"order":479,"title":"Sparse Photometric 3D Face Reconstruction Guided by Morphable Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Sparse_Photometric_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Sparse_Photometric_3D_CVPR_2018_paper.html","abstract":"We present a novel 3D face reconstruction technique that leverages sparse photometric stereo (PS) and latest advances on face registration / modeling from a single image. We observe that 3D morphable faces approach provides a reasonable geometry proxy for light position calibration. Specifically, we develop a robust optimization technique that can calibrate per-pixel lighting direction and illumination at a very high precision without assuming uniform surface albedos. Next, we apply semantic segmentation on input images and the geometry proxy to refine hairy vs. bare skin regions using tailored filter. Experiments on synthetic and real data show that by using a very small set of images, our technique is able to reconstruct fine geometric details such as wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing movie quality productions.","中文标题":"基于可变形模型引导的稀疏光度三维人脸重建","摘要翻译":"我们提出了一种新颖的三维人脸重建技术，该技术利用了稀疏光度立体（PS）和从单张图像进行人脸注册/建模的最新进展。我们观察到，三维可变形人脸方法为光位置校准提供了合理的几何代理。具体来说，我们开发了一种鲁棒的优化技术，可以在不假设表面反照率均匀的情况下，以非常高的精度校准每个像素的光照方向和照明。接下来，我们对输入图像和几何代理应用语义分割，使用定制的过滤器细化毛发与裸露皮肤区域。在合成和真实数据上的实验表明，通过使用非常小的图像集，我们的技术能够重建精细的几何细节，如皱纹、眉毛、粉刺、毛孔等，与电影质量产品相当，有时甚至超越。","领域":"三维重建/人脸建模/光度立体","问题":"如何从少量图像中重建出高质量的三维人脸模型，包括精细的几何细节","动机":"现有的三维人脸重建技术在处理精细几何细节时存在限制，需要一种能够从少量图像中重建出高质量三维人脸模型的方法","方法":"利用稀疏光度立体技术和三维可变形人脸模型进行光位置校准，开发鲁棒的优化技术校准每个像素的光照方向和照明，应用语义分割和定制过滤器细化毛发与裸露皮肤区域","关键词":["三维重建","人脸建模","光度立体","语义分割","优化技术"],"涉及的技术概念":"稀疏光度立体（PS）是一种从少量图像中恢复三维形状的技术。三维可变形人脸模型是一种能够通过参数调整来模拟不同人脸形状和表情的模型。语义分割是一种将图像分割成多个区域或对象的技术，每个区域或对象对应于特定的语义类别。"},{"order":480,"title":"Texture Mapping for 3D Reconstruction With RGB-D Sensor","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Texture_Mapping_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fu_Texture_Mapping_for_CVPR_2018_paper.html","abstract":"Acquiring realistic texture details for 3D models is important in 3D reconstruction. However, the existence of geometric errors, caused by noisy RGB-D sensor data, always makes the color images cannot be accurately aligned onto reconstructed 3D models. In this paper, we propose a global-to-local correction strategy to obtain more desired texture mapping results. Our algorithm first adaptively selects an optimal image for each face of the 3D model, which can effectively remove blurring and ghost artifacts produced by multiple image blending. We then adopt a non-rigid global-to-local correction step to reduce the seaming effect between textures. This can effectively compensate for the texture and the geometric misalignment caused by camera pose drift and geometric errors. We evaluate the proposed algorithm in a range of complex scenes and demonstrate its effective performance in generating seamless high fidelity textures for 3D models.","中文标题":"使用RGB-D传感器进行3D重建的纹理映射","摘要翻译":"获取3D模型的真实纹理细节在3D重建中非常重要。然而，由于RGB-D传感器数据的噪声引起的几何误差，总是使得彩色图像无法准确地对齐到重建的3D模型上。在本文中，我们提出了一种从全局到局部的校正策略，以获得更理想的纹理映射结果。我们的算法首先自适应地为3D模型的每个面选择最佳图像，这可以有效去除由多图像混合产生的模糊和鬼影伪影。然后，我们采用了一种非刚性的从全局到局部的校正步骤，以减少纹理之间的接缝效应。这可以有效地补偿由相机姿态漂移和几何误差引起的纹理和几何不对齐。我们在一系列复杂场景中评估了所提出的算法，并展示了其在生成无缝高保真纹理方面的有效性能。","领域":"3D重建/纹理映射/几何校正","问题":"解决由于RGB-D传感器数据噪声引起的几何误差，导致彩色图像无法准确对齐到重建的3D模型上的问题","动机":"提高3D模型纹理映射的准确性和质量，以生成更真实的无缝高保真纹理","方法":"提出了一种从全局到局部的校正策略，包括自适应选择最佳图像和非刚性全局到局部校正步骤","关键词":["3D重建","纹理映射","几何校正"],"涉及的技术概念":"RGB-D传感器数据噪声、几何误差、彩色图像对齐、自适应图像选择、非刚性校正、相机姿态漂移、几何不对齐、无缝高保真纹理"},{"order":481,"title":"Learning Less Is More - 6D Camera Localization via 3D Surface Regression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Brachmann_Learning_Less_Is_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Brachmann_Learning_Less_Is_CVPR_2018_paper.html","abstract":"Popular research areas like autonomous driving and augmented reality have renewed the interest in image-based camera localization. In this work, we address the task of predicting the 6D camera pose from a single RGB image in a given 3D environment. With the advent of neural networks, previous works have either learned the entire camera localization process, or multiple components of a camera localization pipeline. Our key contribution is to demonstrate and explain that learning a single component of this pipeline is sufficient. This component is a fully convolutional neural network for densely regressing so-called scene coordinates, defining the correspondence between the input image and the 3D scene space. The neural network is prepended to a new end-to-end trainable pipeline. Our system is efficient, highly accurate, robust in training, and exhibits outstanding generalization capabilities. It exceeds state-of-the-art consistently on indoor and outdoor datasets. Interestingly, our approach surpasses existing techniques even without utilizing a 3D model of the scene during training, since the network is able to discover 3D scene geometry automatically, solely from single-view constraints.","中文标题":"学习少即是多 - 通过3D表面回归实现6D相机定位","摘要翻译":"自动驾驶和增强现实等热门研究领域重新激发了基于图像的相机定位的兴趣。在这项工作中，我们解决了从单个RGB图像预测给定3D环境中的6D相机姿态的任务。随着神经网络的出现，以前的工作要么学习了整个相机定位过程，要么学习了相机定位管线的多个组件。我们的关键贡献是证明并解释学习该管线的单个组件就足够了。这个组件是一个全卷积神经网络，用于密集回归所谓的场景坐标，定义输入图像与3D场景空间之间的对应关系。该神经网络被前置到一个新的端到端可训练管线中。我们的系统高效、高度准确、训练稳健，并展现出卓越的泛化能力。它在室内和室外数据集上始终超过最先进的技术。有趣的是，我们的方法甚至在没有利用场景的3D模型进行训练的情况下也超越了现有技术，因为网络能够仅从单视图约束自动发现3D场景几何。","领域":"自动驾驶/增强现实/3D重建","问题":"从单个RGB图像预测给定3D环境中的6D相机姿态","动机":"证明并解释学习相机定位管线的单个组件就足够","方法":"使用全卷积神经网络密集回归场景坐标，定义输入图像与3D场景空间之间的对应关系，并将其前置到一个新的端到端可训练管线中","关键词":["6D相机定位","3D表面回归","全卷积神经网络","场景坐标","端到端训练"],"涉及的技术概念":{"6D相机定位":"指的是在三维空间中确定相机的位置和方向，包括三个平移自由度和三个旋转自由度。","3D表面回归":"通过算法从二维图像中推断出三维场景的表面形状和结构。","全卷积神经网络":"一种特殊的卷积神经网络，能够处理任意大小的输入图像，并输出相应大小的特征图。","场景坐标":"定义输入图像与3D场景空间之间对应关系的坐标系统。","端到端训练":"一种训练方法，其中整个系统从输入到输出作为一个整体进行训练，而不是单独训练各个组件。"}},{"order":482,"title":"Feature Mapping for Learning Fast and Accurate 3D Pose Inference From Synthetic Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rad_Feature_Mapping_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rad_Feature_Mapping_for_CVPR_2018_paper.html","abstract":"We propose a simple and efficient method for exploiting synthetic images when training a Deep Network to predict a 3D pose from an image. The ability of using synthetic images for training a Deep Network is extremely valuable as it is easy to create a virtually infinite training set made of such images, while capturing and annotating real images can be very cumbersome. However, synthetic images do not resemble real images exactly, and using them for training can result in suboptimal performance. It was recently shown that for exemplar-based approaches, it is possible to learn a mapping from the exemplar representations of real images to the exemplar representations of synthetic images. In this paper, we show that this approach is more general, and that a network can also be applied after the mapping to infer a 3D pose: At run-time, given a real image of the target object, we first compute the features for the image, map them to the feature space of synthetic images, and finally use the resulting features as input to another network which predicts the 3D pose. Since this network can be trained very effectively by using synthetic images, it performs very well in practice, and inference is faster and more accurate than with an exemplar-based approach. We demonstrate our approach on the LINEMOD dataset for 3D object pose estimation from color images, and the NYU dataset for 3D hand pose estimation from depth maps. We show that it allows us to outperform the state-of-the-art on both datasets.","中文标题":"特征映射用于从合成图像中学习快速准确的3D姿态推断","摘要翻译":"我们提出了一种简单而有效的方法，用于在训练深度网络从图像预测3D姿态时利用合成图像。使用合成图像训练深度网络的能力极为宝贵，因为可以轻松创建由这些图像组成的几乎无限的训练集，而捕获和注释真实图像可能非常繁琐。然而，合成图像并不完全类似于真实图像，使用它们进行训练可能导致性能不佳。最近有研究表明，对于基于示例的方法，可以学习从真实图像的示例表示到合成图像的示例表示的映射。在本文中，我们展示了这种方法更为通用，并且可以在映射后应用网络来推断3D姿态：在运行时，给定目标对象的真实图像，我们首先计算图像的特征，将它们映射到合成图像的特征空间，最后使用生成的特征作为输入到另一个预测3D姿态的网络。由于这个网络可以通过使用合成图像非常有效地进行训练，它在实践中表现非常好，推断比基于示例的方法更快、更准确。我们在LINEMOD数据集上展示了我们的方法，用于从彩色图像进行3D物体姿态估计，以及在NYU数据集上用于从深度图进行3D手部姿态估计。我们展示了它使我们能够在两个数据集上超越最先进的技术。","领域":"3D姿态估计/合成图像训练/深度网络","问题":"如何有效利用合成图像训练深度网络以预测3D姿态","动机":"合成图像易于创建且数量无限，而真实图像的捕获和注释过程繁琐，但合成图像与真实图像存在差异，直接使用可能导致性能不佳。","方法":"提出了一种方法，通过学习从真实图像到合成图像的特征映射，然后使用映射后的特征作为输入到另一个网络来预测3D姿态。","关键词":["3D姿态估计","合成图像","特征映射","深度网络"],"涉及的技术概念":"本文涉及的技术概念包括3D姿态估计、合成图像的使用、特征映射、深度网络训练和推断。通过特征映射，将真实图像的特征转换到合成图像的特征空间，然后利用这些特征进行3D姿态的预测，这种方法提高了推断的速度和准确性。"},{"order":483,"title":"Indoor RGB-D Compass From a Single Line and Plane","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Indoor_RGB-D_Compass_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Indoor_RGB-D_Compass_CVPR_2018_paper.html","abstract":"We propose a novel approach to estimate the three degrees of freedom (DoF) drift-free rotational motion of an RGB-D camera from only a single line and plane in the Manhattan world (MW). Previous approaches exploit the surface normal vectors and vanishing points to achieve accurate 3-DoF rotation estimation. However, they require multiple orthogonal planes or many consistent lines to be visible throughout the entire rotation estimation process; otherwise, these approaches fail. To overcome these limitations, we present a new method that estimates absolute camera orientation from only a single line and a single plane in RANSAC, which corresponds to the theoretical minimal sampling for 3-DoF rotation estimation. Once we find an initial rotation estimate, we refine the camera orientation by minimizing the average orthogonal distance from the endpoints of the lines parallel to the MW axes. We demonstrate the effectiveness of the proposed algorithm through an extensive evaluation on a variety of RGB-D datasets and compare with other state-of-the-art methods.","中文标题":"室内RGB-D指南针：从单一线和平面","摘要翻译":"我们提出了一种新颖的方法，用于从曼哈顿世界（MW）中的仅一条线和平面估计RGB-D相机的三自由度（DoF）无漂移旋转运动。以前的方法利用表面法向量和消失点来实现精确的三自由度旋转估计。然而，它们需要在整个旋转估计过程中可见多个正交平面或许多一致的线；否则，这些方法将失败。为了克服这些限制，我们提出了一种新方法，该方法仅从RANSAC中的一条线和单个平面估计绝对相机方向，这对应于三自由度旋转估计的理论最小采样。一旦我们找到初始旋转估计，我们通过最小化与MW轴平行的线端点的平均正交距离来细化相机方向。我们通过对各种RGB-D数据集的广泛评估，并与其他最先进的方法进行比较，证明了所提出算法的有效性。","领域":"三维重建/相机定位/视觉导航","问题":"从曼哈顿世界中的仅一条线和平面估计RGB-D相机的三自由度无漂移旋转运动","动机":"克服现有方法需要多个正交平面或许多一致线的限制，实现更灵活和鲁棒的相机旋转估计","方法":"提出一种新方法，仅从RANSAC中的一条线和单个平面估计绝对相机方向，并通过最小化与MW轴平行的线端点的平均正交距离来细化相机方向","关键词":["三自由度旋转估计","曼哈顿世界","RANSAC"],"涉及的技术概念":"三自由度（DoF）旋转估计指的是在三维空间中，物体绕三个相互垂直的轴（通常是X、Y、Z轴）旋转的能力。曼哈顿世界（MW）是一种假设，认为环境主要由与三个正交方向对齐的平面和线组成。RANSAC（随机抽样一致）是一种用于从包含大量异常值的数据集中估计数学模型参数的迭代方法。"},{"order":484,"title":"Geometry-Aware Network for Non-Rigid Shape Prediction From a Single View","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pumarola_Geometry-Aware_Network_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pumarola_Geometry-Aware_Network_for_CVPR_2018_paper.html","abstract":"We propose a method for predicting the 3D shape of a deformable surface from a single view. By contrast with previous approaches, we do not need a pre-registered template of the surface, and our method is robust to the lack of texture and partial occlusions. At the core of our approach is a geometry-aware deep architecture that tackles the problem as usually done in analytic solutions: first perform 2D detection of the mesh and then estimate a 3D shape that is geometrically consistent with the image. We train this architecture in an end-to-end manner using a large dataset of synthetic renderings of shapes under different levels of deformation, material properties, textures and lighting conditions. We evaluate our approach on a test split of this dataset and available real benchmarks, consistently improving state-of-the-art solutions with a significantly lower computational time.","中文标题":"几何感知网络用于单视图非刚性形状预测","摘要翻译":"我们提出了一种从单一视图预测可变形表面3D形状的方法。与之前的方法相比，我们不需要预先注册的表面模板，并且我们的方法对缺乏纹理和部分遮挡具有鲁棒性。我们方法的核心是一个几何感知的深度架构，该架构以通常用于解析解决方案的方式处理问题：首先进行网格的2D检测，然后估计与图像几何一致的3D形状。我们使用一个包含不同变形级别、材料属性、纹理和光照条件下形状合成渲染的大型数据集，以端到端的方式训练这个架构。我们在这个数据集的测试分割和可用的真实基准上评估我们的方法，一致地改进了最先进的解决方案，同时显著降低了计算时间。","领域":"3D重建/几何处理/深度学习","问题":"从单一视图预测可变形表面的3D形状","动机":"解决现有方法需要预先注册的表面模板以及对缺乏纹理和部分遮挡敏感的问题","方法":"提出一个几何感知的深度架构，首先进行2D网格检测，然后估计与图像几何一致的3D形状，并使用大型合成数据集进行端到端训练","关键词":["3D形状预测","几何感知","端到端训练"],"涉及的技术概念":"几何感知的深度架构、2D网格检测、3D形状估计、合成数据集、端到端训练"},{"order":485,"title":"Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sadeghi_Sim2Real_Viewpoint_Invariant_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sadeghi_Sim2Real_Viewpoint_Invariant_CVPR_2018_paper.html","abstract":"Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we propose learning viewpoint invariant visual servoing skills in a robot manipulation task. We train a deep recurrent controller that can automatically determine which actions move the end-effector of a robotic arm to a desired object. This problem is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing approach uses its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to previous visual servoing methods, which assume known dynamics or require a calibration phase. We learn our recurrent controller using simulated data, synthetic demonstrations and reinforcement learning. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: href{https://www.youtube.com/watch?v=oLgM2Bnb7fo}{https://www.youtube.com/watch?v=oLgM2Bnb7fo}","中文标题":"通过循环控制实现视角不变的Sim2Real视觉伺服","摘要翻译":"人类在从广泛视角控制肢体和工具方面表现出色。在机器人学中，这种能力被称为视觉伺服：主要使用视觉反馈将工具或端点移动到期望位置。在本文中，我们提出在机器人操作任务中学习视角不变的视觉伺服技能。我们训练了一个深度循环控制器，可以自动确定哪些动作将机器人手臂的末端执行器移动到期望的物体。这个问题本质上是模糊的：在视角严重变化的情况下，可能无法通过单一的前馈操作确定动作。相反，我们的视觉伺服方法利用其过去动作的记忆来理解从当前视角看动作如何影响机器人运动，纠正错误并逐渐接近目标。这种能力与之前的视觉伺服方法形成鲜明对比，后者假设已知动态或需要校准阶段。我们使用模拟数据、合成演示和强化学习来学习我们的循环控制器。然后，我们描述了如何通过将感知与控制分离并仅适应视觉层，将所得模型转移到现实世界的机器人上。适应后的模型可以在现实世界的Kuka IIWA机器人手臂上从未见过的视角伺服到新物体。有关补充视频，请参见：href{https://www.youtube.com/watch?v=oLgM2Bnb7fo}{https://www.youtube.com/watch?v=oLgM2Bnb7fo}","领域":"机器人视觉伺服/深度学习/强化学习","问题":"在视角严重变化的情况下，如何实现机器人手臂的视觉伺服控制","动机":"提高机器人在不同视角下控制末端执行器到达目标物体的能力","方法":"训练一个深度循环控制器，利用过去动作的记忆来理解动作如何影响机器人运动，并通过模拟数据、合成演示和强化学习进行学习","关键词":["视觉伺服","循环控制","机器人操作","视角不变","强化学习"],"涉及的技术概念":"视觉伺服是指使用视觉反馈来控制机器人末端执行器的位置。循环控制器是一种能够处理序列数据的神经网络，能够记住过去的信息来影响当前的决策。强化学习是一种通过奖励机制来训练模型做出决策的机器学习方法。"},{"order":486,"title":"DocUNet: Document Image Unwarping via a Stacked U-Net","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_DocUNet_Document_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DocUNet_Document_Image_CVPR_2018_paper.html","abstract":"Capturing document images is a common way for digitizing and recording physical documents due to the ubiquitousness of mobile cameras. To make text recognition easier, it is often desirable to digitally flatten a document image when the physical document sheet is folded or curved. In this paper, we develop the first learning-based method to achieve this goal. We propose a stacked U-Net with intermediate supervision to directly predict the forward mapping from a distorted image to its rectified version. Because large-scale real-world data with ground truth deformation is difficult to obtain, we create a synthetic dataset with approximately 100 thousand images by warping non-distorted document images. The network is trained on this dataset with various data augmentations to improve its generalization ability. We further create a comprehensive benchmark that covers various real-world conditions. We evaluate the proposed model quantitatively and qualitatively on the proposed benchmark, and compare it with previous non-learning-based methods.","中文标题":"DocUNet: 通过堆叠U-Net进行文档图像展开","摘要翻译":"由于移动摄像头的普及，捕捉文档图像已成为数字化和记录物理文档的常用方法。为了使文本识别更容易，当物理文档纸张折叠或弯曲时，通常希望数字上展平文档图像。在本文中，我们开发了第一个基于学习的方法来实现这一目标。我们提出了一个带有中间监督的堆叠U-Net，直接预测从扭曲图像到其校正版本的前向映射。由于难以获得具有真实变形的大规模真实世界数据，我们通过扭曲非扭曲文档图像创建了一个包含大约10万张图像的合成数据集。网络在这个数据集上进行了训练，并采用了各种数据增强技术以提高其泛化能力。我们进一步创建了一个涵盖各种真实世界条件的综合基准。我们在提出的基准上定量和定性地评估了所提出的模型，并将其与之前的非基于学习的方法进行了比较。","领域":"文档图像处理/图像校正/深度学习应用","问题":"如何有效地对折叠或弯曲的文档图像进行数字展平，以便于文本识别","动机":"由于移动摄像头的普及，捕捉文档图像已成为数字化和记录物理文档的常用方法，但折叠或弯曲的文档图像会影响文本识别的准确性，因此需要一种有效的方法来展平这些图像","方法":"提出了一个带有中间监督的堆叠U-Net，直接预测从扭曲图像到其校正版本的前向映射，并通过创建合成数据集和采用数据增强技术来提高模型的泛化能力","关键词":["文档图像展开","U-Net","图像校正"],"涉及的技术概念":"堆叠U-Net是一种深度学习架构，用于图像处理任务，通过堆叠多个U-Net层来提高模型的性能。中间监督是指在网络训练过程中，对网络的中间层进行监督，以帮助网络更好地学习特征。数据增强技术用于增加训练数据的多样性，提高模型的泛化能力。"},{"order":487,"title":"Analysis of Hand Segmentation in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Urooj_Analysis_of_Hand_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Urooj_Analysis_of_Hand_CVPR_2018_paper.html","abstract":"A large number of works in egocentric vision have concentrated on action and object recognition. Detection and segmentation of hands in first-person videos, however, has less been explored. For many applications in this domain, it is necessary to accurately segment not only hands of the camera wearer but also the hands of others with whom he is interacting. Here, we take an in-depth look at the hand segmentation problem. In the quest for robust hand segmentation methods, we evaluated the performance of the state of the art semantic segmentation methods, off the shelf and fine-tuned, on existing datasets. We fine-tune RefineNet, a leading semantic segmentation method, for hand segmentation and find that it does much better than the best contenders. Existing hand segmentation datasets are collected in the laboratory settings. To overcome this limitation, we contribute by collecting two new datasets: a) EgoYouTubeHands including egocentric videos containing hands in the wild, and b) HandOverFace to analyze the performance of our models in presence of similar appearance occlusions. We further explore whether conditional random fields can help refine generated hand segmentations. To demonstrate the benefit of accurate hand maps, we train a CNN for hand-based activity recognition and achieve higher accuracy when a CNN was trained using hand maps produced by the fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for fine-grained action recognition and show that an accuracy of 58.6% can be achieved by just looking at a single hand pose which is much better than the chance level (12.5%).","中文标题":"野外手部分割分析","摘要翻译":"在自我中心视觉领域，大量工作集中在动作和物体识别上。然而，第一人称视频中的手部检测和分割研究较少。对于该领域的许多应用，不仅需要准确分割佩戴相机者的手部，还需要分割与之互动的其他人的手部。本文深入探讨了手部分割问题。在寻求鲁棒的手部分割方法的过程中，我们评估了现有数据集上最先进的语义分割方法（包括现成的和微调的）的性能。我们对领先的语义分割方法RefineNet进行微调，用于手部分割，并发现其性能远超其他竞争者。现有的手部分割数据集是在实验室环境中收集的。为了克服这一限制，我们贡献了两个新数据集：a) EgoYouTubeHands，包含野外包含手部的自我中心视频，和b) HandOverFace，用于分析我们的模型在存在相似外观遮挡时的性能。我们进一步探讨了条件随机场是否可以帮助优化生成的手部分割。为了展示准确手部地图的好处，我们训练了一个用于基于手部的活动识别的CNN，并发现当使用微调后的RefineNet生成的手部地图训练CNN时，准确率更高。最后，我们为EgoHands数据集的一个子集注释了细粒度动作识别，并展示了仅通过观察单个手部姿势即可达到58.6%的准确率，这远高于机会水平（12.5%）。","领域":"自我中心视觉/手部分割/活动识别","问题":"第一人称视频中的手部检测和分割","动机":"为了在自我中心视觉领域的应用中准确分割手部，包括佩戴相机者及与之互动的其他人的手部","方法":"评估并微调最先进的语义分割方法RefineNet用于手部分割，收集新的数据集EgoYouTubeHands和HandOverFace，探索条件随机场优化手部分割，训练CNN用于基于手部的活动识别","关键词":["手部分割","自我中心视觉","活动识别","条件随机场","CNN"],"涉及的技术概念":{"RefineNet":"一种领先的语义分割方法，本文对其进行了微调以用于手部分割","条件随机场":"一种统计建模方法，用于优化生成的手部分割","CNN":"卷积神经网络，本文用于基于手部的活动识别"}},{"order":488,"title":"RoadTracer: Automatic Extraction of Road Networks From Aerial Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bastani_RoadTracer_Automatic_Extraction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bastani_RoadTracer_Automatic_Extraction_CVPR_2018_paper.html","abstract":"Mapping road networks is currently both expensive and labor-intensive. High-resolution aerial imagery provides a promising avenue to automatically infer a road network. Prior work uses convolutional neural networks (CNNs) to detect which pixels belong to a road (segmentation), and then uses complex post-processing heuristics to infer graph connectivity. We show that these segmentation methods have high error rates because noisy CNN outputs are difficult to correct. We propose RoadTracer, a new method to automatically construct accurate road network maps from aerial images. RoadTracer uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN. We compare our approach with a segmentation method on fifteen cities, and find that at a 5% error rate, RoadTracer correctly captures 45% more junctions across these cities.","中文标题":"RoadTracer: 从航拍图像自动提取道路网络","摘要翻译":"目前，绘制道路网络既昂贵又劳动密集。高分辨率航拍图像为自动推断道路网络提供了一个有希望的途径。先前的工作使用卷积神经网络（CNNs）来检测哪些像素属于道路（分割），然后使用复杂的后处理启发式方法来推断图的连通性。我们展示了这些分割方法由于CNN输出的噪声难以纠正，因此具有高错误率。我们提出了RoadTracer，一种从航拍图像自动构建准确道路网络地图的新方法。RoadTracer使用由CNN基于的决策函数引导的迭代搜索过程，直接从CNN的输出中推导出道路网络图。我们将我们的方法与十五个城市的分割方法进行了比较，发现在5%的错误率下，RoadTracer在这些城市中正确捕捉了45%以上的交叉口。","领域":"地理信息系统/遥感/自动化地图绘制","问题":"自动从航拍图像中提取准确的道路网络","动机":"传统方法昂贵且劳动密集，且现有基于CNN的分割方法错误率高","方法":"提出RoadTracer方法，使用CNN引导的迭代搜索过程直接从CNN输出中推导道路网络图","关键词":["道路网络","航拍图像","卷积神经网络","迭代搜索","地图绘制"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，用于图像识别和分割任务。","分割方法":"一种图像处理技术，用于识别图像中属于特定类别的像素。","迭代搜索过程":"一种逐步优化的搜索方法，用于在复杂数据中寻找最优解。","图连通性":"在图论中，指图中节点之间是否存在路径相连。"}},{"order":489,"title":"Alternating-Stereo VINS: Observability Analysis and Performance Evaluation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Paul_Alternating-Stereo_VINS_Observability_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Paul_Alternating-Stereo_VINS_Observability_CVPR_2018_paper.html","abstract":"One approach to improve the accuracy and robustness of vision-aided inertial navigation systems (VINS) that employ low-cost inertial sensors, is to obtain scale information from stereoscopic vision. Processing images from two cameras, however, is computationally expensive and increases latency. To address this limitation, in this work, a novel two-camera alternating-stereo VINS is presented. Specifically, the proposed system triggers the left-right cameras in an alternating fashion, estimates the poses corresponding to the left camera only, and introduces a linear interpolation model for processing the alternating right camera measurements.  Although not a regular stereo system, the alternating visual observations when employing the proposed interpolation scheme, still provide scale information, as shown by analyzing the observability properties of the vision-only corresponding system. Finally, the performance gain, of the proposed algorithm over its monocular and stereo counterparts is assessed using various datasets.","中文标题":"交替立体视觉惯性导航系统：可观测性分析与性能评估","摘要翻译":"提高采用低成本惯性传感器的视觉辅助惯性导航系统（VINS）精度和鲁棒性的一种方法是从立体视觉中获取尺度信息。然而，处理来自两个摄像头的图像计算成本高且增加了延迟。为了解决这一限制，本文提出了一种新颖的双摄像头交替立体视觉惯性导航系统。具体来说，所提出的系统以交替方式触发左右摄像头，仅估计左摄像头对应的姿态，并引入线性插值模型来处理交替右摄像头的测量值。尽管不是一个常规的立体系统，但采用所提出的插值方案时的交替视觉观察仍然提供了尺度信息，正如通过分析仅视觉对应系统的可观测性属性所展示的那样。最后，使用各种数据集评估了所提出算法相对于其单目和立体对应物的性能增益。","领域":"视觉惯性导航系统/立体视觉/尺度估计","问题":"提高低成本惯性传感器的视觉辅助惯性导航系统的精度和鲁棒性","动机":"处理来自两个摄像头的图像计算成本高且增加了延迟，需要一种更高效的方法来获取尺度信息","方法":"提出了一种新颖的双摄像头交替立体视觉惯性导航系统，通过交替触发左右摄像头并引入线性插值模型来处理交替右摄像头的测量值","关键词":["视觉惯性导航系统","立体视觉","尺度估计","线性插值模型","可观测性分析"],"涉及的技术概念":"视觉辅助惯性导航系统（VINS）是一种结合视觉信息和惯性传感器数据进行导航的系统。立体视觉通过两个摄像头获取深度信息，从而提供尺度信息。线性插值模型用于处理交替摄像头的测量值，以减少计算成本和延迟。可观测性分析用于评估系统在仅使用视觉信息时的性能。"},{"order":490,"title":"Soccer on Your Tabletop","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rematas_Soccer_on_Your_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rematas_Soccer_on_Your_CVPR_2018_paper.html","abstract":"We present a system that transforms a monocular video of a soccer game into a moving 3D reconstruction, in which the players and field can be rendered interactively with a 3D viewer or through an Augmented Reality device.  At the heart of our paper is an approach to estimate the depth map of each player, using a CNN that is trained on 3D player data extracted from soccer video games.  We compare with state of the art body pose and depth estimation techniques, and show results on both synthetic ground truth benchmarks, and real YouTube soccer footage.","中文标题":"桌面上的足球","摘要翻译":"我们提出了一个系统，该系统将足球比赛的单目视频转换为移动的3D重建，其中球员和场地可以通过3D查看器或增强现实设备进行交互式渲染。我们论文的核心是一种估计每个球员深度图的方法，使用了一个在从足球视频游戏中提取的3D球员数据上训练的CNN。我们与最先进的身体姿态和深度估计技术进行了比较，并在合成的地面真实基准和真实的YouTube足球片段上展示了结果。","领域":"3D重建/增强现实/深度估计","问题":"将足球比赛的单目视频转换为移动的3D重建","动机":"为了能够通过3D查看器或增强现实设备交互式地渲染足球比赛中的球员和场地","方法":"使用在从足球视频游戏中提取的3D球员数据上训练的CNN来估计每个球员的深度图，并与最先进的身体姿态和深度估计技术进行比较","关键词":["3D重建","增强现实","深度估计","CNN"],"涉及的技术概念":"CNN（卷积神经网络）用于深度估计，3D重建技术用于将2D视频转换为3D模型，增强现实技术用于交互式渲染。"},{"order":491,"title":"EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth From Light Field Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_EPINET_A_Fully-Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_EPINET_A_Fully-Convolutional_CVPR_2018_paper.html","abstract":"Light field cameras capture both the spatial and the angular properties of light rays in space. Due to its property, one can compute the depth from light fields in uncontrolled lighting environments, which is a big advantage over active sensing devices. Depth computed from light fields can be used for many applications including 3D modelling and refocusing. However, light field images from hand-held cameras have very narrow baselines with noise, making the depth estimation difficult. Many approaches have been proposed to overcome these limitations for the light field depth estimation, but there is a clear trade-off between the accuracy and the speed in these methods. In this paper, we introduce a fast and accurate light field depth estimation method based on a fully-convolutional neural network. Our network is designed by considering the light field geometry and we also overcome the lack of training data by proposing light field specific data augmentation methods. We achieved the top rank in the HCI 4D Light Field Benchmark on most metrics, and we also demonstrate the effectiveness of the proposed method on real-world light-field images.","中文标题":"EPINET: 使用极线几何从光场图像中获取深度的全卷积神经网络","摘要翻译":"光场相机捕捉空间中光线的空间和角度属性。由于其特性，可以在不受控制的光照环境中从光场计算深度，这比主动传感设备具有很大优势。从光场计算的深度可用于许多应用，包括3D建模和重新聚焦。然而，手持相机拍摄的光场图像基线非常窄且带有噪声，使得深度估计变得困难。已经提出了许多方法来克服光场深度估计的这些限制，但这些方法在准确性和速度之间存在明显的权衡。在本文中，我们介绍了一种基于全卷积神经网络的快速准确的光场深度估计方法。我们的网络设计考虑了光场几何，并通过提出光场特定的数据增强方法来克服训练数据的不足。我们在HCI 4D光场基准测试的大多数指标上取得了最高排名，并且我们还展示了所提出方法在真实世界光场图像上的有效性。","领域":"光场成像/深度估计/3D建模","问题":"从手持相机拍摄的带有噪声的窄基线光场图像中准确快速地估计深度","动机":"克服现有光场深度估计方法在准确性和速度之间的权衡，以及训练数据的不足","方法":"基于全卷积神经网络的光场深度估计方法，考虑光场几何，并提出光场特定的数据增强方法","关键词":["光场成像","深度估计","3D建模","全卷积神经网络","数据增强"],"涉及的技术概念":"光场相机捕捉光线的空间和角度属性，全卷积神经网络用于深度估计，光场几何考虑，数据增强方法用于克服训练数据的不足"},{"order":492,"title":"A Hybrid l1-l0 Layer Decomposition Model for Tone Mapping","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_A_Hybrid_l1-l0_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_A_Hybrid_l1-l0_CVPR_2018_paper.html","abstract":"Tone mapping aims to reproduce a standard dynamic range image from a high dynamic range image with visual information preserved. State-of-the-art tone mapping algorithms mostly decompose an image into a base layer and a detail layer, and process them accordingly. These methods may have problems of halo artifacts and over-enhancement, due to the lack of proper priors imposed on the two layers. In this paper, we propose a hybrid L1-L0 decomposition model to address these problems. Specifically, an L1 sparsity term is imposed on the base layer to model its piecewise smoothness property. An L0 sparsity term is imposed on the detail layer as a structural prior, which leads to piecewise constant effect. We further propose a multiscale tone mapping scheme based on our layer decomposition model. Experiments show that our tone mapping algorithm achieves visually compelling results with little halo artifacts, outperforming the state-of-the-art tone mapping algorithms in both subjective and objective evaluations.","中文标题":"一种用于色调映射的混合L1-L0层分解模型","摘要翻译":"色调映射旨在从高动态范围图像中再现标准动态范围图像，同时保留视觉信息。最先进的色调映射算法大多将图像分解为基础层和细节层，并相应地进行处理。由于缺乏对这两层的适当先验，这些方法可能会出现光晕伪影和过度增强的问题。在本文中，我们提出了一种混合L1-L0分解模型来解决这些问题。具体来说，我们在基础层上施加L1稀疏项以模拟其分段平滑特性。在细节层上施加L0稀疏项作为结构先验，这导致了分段恒定效果。我们进一步提出了一种基于我们层分解模型的多尺度色调映射方案。实验表明，我们的色调映射算法在视觉上取得了引人注目的结果，几乎没有光晕伪影，在主观和客观评估中都优于最先进的色调映射算法。","领域":"图像处理/计算机视觉/图形学","问题":"解决色调映射中的光晕伪影和过度增强问题","动机":"现有的色调映射算法由于缺乏对基础层和细节层的适当先验，导致光晕伪影和过度增强的问题","方法":"提出一种混合L1-L0分解模型，在基础层上施加L1稀疏项以模拟其分段平滑特性，在细节层上施加L0稀疏项作为结构先验，并进一步提出一种多尺度色调映射方案","关键词":["色调映射","L1-L0分解","多尺度处理"],"涉及的技术概念":"L1稀疏项用于模拟基础层的分段平滑特性，L0稀疏项作为细节层的结构先验，多尺度色调映射方案"},{"order":493,"title":"Deeply Learned Filter Response Functions for Hyperspectral Reconstruction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nie_Deeply_Learned_Filter_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nie_Deeply_Learned_Filter_CVPR_2018_paper.html","abstract":"Hyperspectral reconstruction from RGB imaging has recently achieved significant progress via sparse coding and deep learning. However, a largely ignored fact is that existing RGB cameras are tuned to mimic human  richromatic perception, thus their spectral responses are not necessarily optimal for hyperspectral reconstruction. In this paper, rather than use RGB spectral responses, we simultaneously learn optimized camera spectral response functions (to be implemented in hardware) and a mapping for spectral reconstruction by using an end-to-end network. Our core idea is that since camera spectral filters act in effect like the convolution layer, their response functions could be optimized by training standard neural networks. We propose two types of designed filters: a three-chip setup without spatial mosaicing and a single-chip setup with a Bayer-style 2x2 filter array. Numerical simulations verify the advantages of deeply learned spectral responses compared to existing RGB cameras. More interestingly, by considering physical restrictions in the design process, we are able to realize the deeply learned spectral response functions by using modern film filter production technologies, and thus construct data-inspired multispectral cameras for snapshot hyperspectral imaging.","中文标题":"深度学习的滤波器响应函数用于高光谱重建","摘要翻译":"从RGB成像中进行高光谱重建最近通过稀疏编码和深度学习取得了显著进展。然而，一个很大程度上被忽视的事实是，现有的RGB相机被调谐以模仿人类的三色感知，因此它们的光谱响应不一定对高光谱重建是最优的。在本文中，我们不是使用RGB光谱响应，而是通过使用端到端网络同时学习优化的相机光谱响应函数（将在硬件中实现）和光谱重建的映射。我们的核心思想是，由于相机光谱滤波器实际上像卷积层一样工作，它们的响应函数可以通过训练标准神经网络来优化。我们提出了两种设计滤波器：一种是没有空间镶嵌的三芯片设置，另一种是带有拜耳风格2x2滤波器阵列的单芯片设置。数值模拟验证了深度学习的谱响应与现有RGB相机相比的优势。更有趣的是，通过在设计过程中考虑物理限制，我们能够利用现代薄膜滤波器生产技术实现深度学习的谱响应函数，从而构建数据启发的多光谱相机用于快照高光谱成像。","领域":"高光谱成像/光谱重建/滤波器设计","问题":"现有RGB相机的光谱响应对于高光谱重建不是最优的","动机":"优化相机光谱响应函数以提高高光谱重建的质量","方法":"使用端到端网络同时学习优化的相机光谱响应函数和光谱重建的映射","关键词":["高光谱重建","光谱响应函数","端到端网络","滤波器设计","多光谱相机"],"涉及的技术概念":"本文涉及的技术概念包括高光谱重建、光谱响应函数、端到端网络、滤波器设计（包括三芯片设置和单芯片设置）、拜耳风格滤波器阵列、数值模拟、薄膜滤波器生产技术、数据启发的多光谱相机和快照高光谱成像。"},{"order":494,"title":"CRRN: Multi-Scale Guided Concurrent Reflection Removal Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_CRRN_Multi-Scale_Guided_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_CRRN_Multi-Scale_Guided_CVPR_2018_paper.html","abstract":"Removing the undesired reflections from images taken through the glass is of broad application to various computer vision tasks. Non-learning based methods utilize different handcrafted priors such as the separable sparse gradients caused by different levels of blurs, which often fail due to their limited description capability to the properties of real-world reflections. In this paper, we propose the Concurrent Reflection Removal Network (CRRN) to tackle this problem in a unified framework. Our network integrates image appearance information and multi-scale gradient information with human perception inspired loss function, and is trained on a new dataset with 3250 reflection images taken under diverse real-world scenes. Extensive experiments on a public benchmark dataset show that the proposed method performs favorably against state-of-the-art methods.","中文标题":"CRRN：多尺度引导的并发反射去除网络","摘要翻译":"从通过玻璃拍摄的图像中去除不需要的反射对于各种计算机视觉任务具有广泛的应用。非基于学习的方法利用不同的手工先验，如由不同程度的模糊引起的可分离稀疏梯度，这些方法由于对现实世界反射属性的描述能力有限而经常失败。在本文中，我们提出了并发反射去除网络（CRRN）来在一个统一的框架中解决这个问题。我们的网络整合了图像外观信息、多尺度梯度信息以及受人类感知启发的损失函数，并在一个包含3250张在不同现实世界场景下拍摄的反射图像的新数据集上进行了训练。在公共基准数据集上的大量实验表明，所提出的方法在性能上优于最先进的方法。","领域":"图像去反射/深度学习/计算机视觉","问题":"去除通过玻璃拍摄的图像中的不需要的反射","动机":"非基于学习的方法由于对现实世界反射属性的描述能力有限而经常失败，需要一种更有效的方法来解决这个问题","方法":"提出了并发反射去除网络（CRRN），整合图像外观信息、多尺度梯度信息以及受人类感知启发的损失函数，并在新数据集上进行训练","关键词":["图像去反射","深度学习","多尺度梯度"],"涉及的技术概念":"并发反射去除网络（CRRN）是一种深度学习模型，用于从通过玻璃拍摄的图像中去除反射。该网络通过整合图像的外观信息、多尺度梯度信息以及受人类感知启发的损失函数来提高去反射的效果。此外，该网络在一个包含3250张在不同现实世界场景下拍摄的反射图像的新数据集上进行了训练，以增强其泛化能力和实用性。"},{"order":495,"title":"Single Image Reflection Separation With Perceptual Losses","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single_Image_Reflection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single_Image_Reflection_CVPR_2018_paper.html","abstract":"We present an approach to separating reflection from a single image. The approach uses a fully convolutional network trained end-to-end with losses that exploit low-level and high-level image information. Our loss function includes two perceptual losses: a feature loss from a visual perception network, and an adversarial loss that encodes characteristics of images in the transmission layers. We also propose a novel exclusion loss that enforces pixel-level layer separation. We create a dataset of real-world images with reflection and corresponding ground-truth transmission layers for quantitative evaluation and model training. We validate our method through comprehensive quantitative experiments and show that our approach outperforms state-of-the-art reflection removal methods in PSNR, SSIM, and perceptual user study. We also extend our method to two other image enhancement tasks to demonstrate the generality of our approach.","中文标题":"使用感知损失的单图像反射分离","摘要翻译":"我们提出了一种从单张图像中分离反射的方法。该方法使用了一个全卷积网络，该网络通过利用低层次和高层次图像信息的损失进行端到端训练。我们的损失函数包括两个感知损失：一个来自视觉感知网络的特征损失，以及一个编码传输层图像特征的对抗损失。我们还提出了一种新颖的排除损失，以加强像素级的层分离。我们创建了一个包含反射和相应真实传输层的真实世界图像数据集，用于定量评估和模型训练。我们通过全面的定量实验验证了我们的方法，并显示我们的方法在PSNR、SSIM和感知用户研究中优于最先进的反射去除方法。我们还将我们的方法扩展到其他两个图像增强任务，以展示我们方法的通用性。","领域":"图像反射分离/图像增强/感知损失","问题":"从单张图像中分离反射","动机":"提高单张图像中反射分离的准确性和效率","方法":"使用全卷积网络和包括特征损失、对抗损失及新颖的排除损失在内的感知损失进行端到端训练","关键词":["反射分离","感知损失","图像增强"],"涉及的技术概念":{"全卷积网络":"一种用于图像处理的深度学习网络架构，能够处理任意大小的输入图像。","感知损失":"一种损失函数，旨在使生成的图像在视觉上与目标图像相似，通常通过预训练的神经网络提取特征来计算。","对抗损失":"一种损失函数，用于生成对抗网络（GANs）中，旨在使生成的图像难以与真实图像区分。","排除损失":"一种新颖的损失函数，用于加强像素级的层分离，确保反射和传输层的有效分离。","PSNR":"峰值信噪比，一种衡量图像质量的指标。","SSIM":"结构相似性指数，一种衡量两幅图像相似度的指标。"}},{"order":496,"title":"A Robust Method for Strong Rolling Shutter Effects Correction Using Lines With Automatic Feature Selection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lao_A_Robust_Method_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lao_A_Robust_Method_CVPR_2018_paper.html","abstract":"We present a robust method which compensates RS distortions in a single image using a set of image curves, basing on the knowledge that they correspond to 3D straight lines. Unlike in existing work, no a priori knowledge about the line directions (e.g. Manhattan World assumption) is required. We first formulate a parametric equation for the projection of a 3D straight line viewed by a moving rolling shutter camera under a uniform motion model. Then we propose a method which efficiently estimates ego angular velocity separately from pose parameters, using at least 4 image curves. Moreover, we propose for the first time a RANSAC-like strategy to select image curves which really correspond to 3D straight lines and reject those corresponding to actual curves in 3D world. A comparative experimental study with both synthetic and real data from famous benchmarks shows that the proposed method outperforms all the existing techniques from the state-of-the-art.","中文标题":"使用自动特征选择的线条校正强滚动快门效应的鲁棒方法","摘要翻译":"我们提出了一种鲁棒的方法，该方法利用一组图像曲线来补偿单张图像中的滚动快门（RS）失真，基于这些曲线对应于3D直线的知识。与现有工作不同，不需要关于线条方向的先验知识（例如曼哈顿世界假设）。我们首先为在均匀运动模型下由移动的滚动快门相机观察到的3D直线的投影制定了一个参数方程。然后，我们提出了一种方法，该方法使用至少4条图像曲线，有效地从姿态参数中单独估计自我角速度。此外，我们首次提出了一种类似于RANSAC的策略，以选择真正对应于3D直线的图像曲线，并拒绝那些对应于3D世界中实际曲线的曲线。一项使用来自著名基准的合成和真实数据的比较实验研究表明，所提出的方法优于所有现有的最先进技术。","领域":"滚动快门效应校正/3D重建/图像处理","问题":"解决单张图像中由于滚动快门效应引起的失真问题","动机":"现有的滚动快门效应校正方法需要关于线条方向的先验知识，限制了其应用范围。本研究旨在开发一种无需此类先验知识的鲁棒校正方法。","方法":"首先制定了一个参数方程来描述3D直线在滚动快门相机下的投影，然后提出了一种方法从姿态参数中估计自我角速度，并采用RANSAC-like策略选择真正对应于3D直线的图像曲线。","关键词":["滚动快门效应","3D直线","自我角速度","RANSAC策略"],"涉及的技术概念":"滚动快门效应是指在快速移动的相机中，由于图像逐行曝光而导致的图像失真。3D直线指的是在三维空间中的直线。自我角速度是指相机自身旋转的速度。RANSAC策略是一种用于从包含大量异常值的数据中估计数学模型参数的迭代方法。"},{"order":497,"title":"Time-Resolved Light Transport Decomposition for Thermal Photometric Stereo","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tanaka_Time-Resolved_Light_Transport_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tanaka_Time-Resolved_Light_Transport_CVPR_2018_paper.html","abstract":"We present a novel time-resolved light transport decomposition method using thermal imaging. Because the speed of heat propagation is much slower than the speed of light propagation, transient transport of far infrared light can be observed at a video frame rate. A key observation is that the thermal image looks similar to the visible light image in an appropriately controlled environment. This implies that conventional computer vision techniques can be straightforwardly applied to the thermal image. We show that the diffuse component in the thermal image can be separated and, therefore, the surface normals of objects can be estimated by the Lambertian photometric stereo. The effectiveness of our method is evaluated by conducting real-world experiments, and its applicability to black body, transparent, and translucent objects is shown.","中文标题":"时间分辨光传输分解用于热光度立体","摘要翻译":"我们提出了一种使用热成像的新型时间分辨光传输分解方法。由于热传播速度远低于光传播速度，因此可以在视频帧率下观察到远红外光的瞬态传输。一个关键的观察是，在适当控制的环境中，热图像看起来与可见光图像相似。这意味着传统的计算机视觉技术可以直接应用于热图像。我们展示了热图像中的漫反射分量可以被分离，因此可以通过朗伯光度立体法估计物体的表面法线。通过进行真实世界的实验评估了我们方法的有效性，并展示了其对黑体、透明和半透明物体的适用性。","领域":"热成像/光度立体/远红外成像","问题":"如何在热成像中分离漫反射分量并估计物体的表面法线","动机":"由于热传播速度远低于光传播速度，可以在视频帧率下观察到远红外光的瞬态传输，这为应用传统计算机视觉技术于热图像提供了可能性。","方法":"提出了一种时间分辨光传输分解方法，利用热成像技术分离热图像中的漫反射分量，并通过朗伯光度立体法估计物体的表面法线。","关键词":["热成像","光度立体","远红外成像","时间分辨","表面法线估计"],"涉及的技术概念":"时间分辨光传输分解是一种技术，它允许在视频帧率下观察和分析远红外光的瞬态传输。朗伯光度立体法是一种基于朗伯反射模型的技术，用于从多个光源下的图像中估计物体的表面法线。"},{"order":498,"title":"Efficient Diverse Ensemble for Discriminative Co-Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.html","abstract":"Ensemble discriminative tracking utilizes a committee of classifiers, to label data samples, which are in turn, used for retraining the tracker to localize the target using the collective knowledge of the committee. Committee members could vary in their features, memory update schemes, or training data, however, it is inevitable to have committee members that excessively agree because of large overlaps in their version space. To remove this redundancy and have an effective ensemble learning, it is critical for the committee to include consistent hypotheses that differ from one-another, covering the version space with minimum overlaps. In this study, we propose an online ensemble tracker that directly generates a diverse committee by generating an efficient set of artificial training. The artificial data is sampled from the empirical distribution of the samples taken from both target and background, whereas the process is governed by query-by-committee to shrink the overlap between classifiers. The experimental results demonstrate that the proposed scheme outperforms conventional ensemble trackers on public benchmarks.","中文标题":"高效多样集成用于判别协同跟踪","摘要翻译":"集成判别跟踪利用一组分类器来标记数据样本，这些样本随后用于重新训练跟踪器，以利用委员会的集体知识来定位目标。委员会成员可能在特征、记忆更新方案或训练数据上有所不同，然而，由于版本空间的大幅重叠，不可避免地会有委员会成员过度一致。为了消除这种冗余并实现有效的集成学习，委员会必须包含彼此不同的一致假设，以最小的重叠覆盖版本空间。在本研究中，我们提出了一种在线集成跟踪器，它通过生成一组高效的人工训练数据直接生成一个多样化的委员会。人工数据是从目标和背景样本的经验分布中采样的，而该过程由委员会查询控制，以减少分类器之间的重叠。实验结果表明，所提出的方案在公共基准上优于传统的集成跟踪器。","领域":"目标跟踪/集成学习/在线学习","问题":"如何消除集成跟踪中委员会成员之间的冗余，提高跟踪的准确性和效率","动机":"为了提高集成跟踪器的性能，需要减少委员会成员之间的重叠，增加多样性，从而更有效地覆盖版本空间","方法":"提出了一种在线集成跟踪器，通过生成高效的人工训练数据来直接生成一个多样化的委员会，利用委员会查询减少分类器之间的重叠","关键词":["目标跟踪","集成学习","在线学习","委员会查询","人工训练数据"],"涉及的技术概念":{"集成判别跟踪":"利用一组分类器来标记数据样本，用于重新训练跟踪器","委员会成员":"集成学习中的分类器，可能在特征、记忆更新方案或训练数据上有所不同","版本空间":"分类器可能产生的所有假设的集合","在线集成跟踪器":"一种能够在线生成多样化委员会的跟踪器","人工训练数据":"从目标和背景样本的经验分布中采样生成的数据，用于训练委员会成员","委员会查询":"一种减少分类器之间重叠的方法，通过控制人工数据的生成过程来实现"}},{"order":499,"title":"Rolling Shutter and Radial Distortion Are Features for High Frame Rate Multi-Camera Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bapat_Rolling_Shutter_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bapat_Rolling_Shutter_and_CVPR_2018_paper.html","abstract":"Traditionally, camera-based tracking approaches have treated rolling shutter and radial distortion as imaging artifacts that have to be overcome and corrected for in order to apply standard camera models and scene reconstruction methods. In this paper, we introduce a novel multi-camera tracking approach that for the first time jointly leverages the information introduced by rolling shutter and radial distortion as a feature to achieve superior performance with respect to high-frequency camera pose estimation. In particular, our system is capable of attaining high tracking rates that were previously unachievable. Our approach explicitly leverages rolling shutter capture and radial distortion to process individual rows, rather than entire image frames, for accurate camera motion estimation. We estimate a per-row 6 DoF pose of a rolling shutter camera by tracking multiple points on a radially distorted row whose rays span a curved surface in 3D space. Although tracking systems for rolling shutter cameras exist, we are the first to leverage radial distortion to measure a per-row pose -- enabling us to use less than half the number of cameras required by the previous state of the art. We validate our system on both synthetic and real imagery.","中文标题":"滚动快门和径向畸变作为高帧率多相机跟踪的特征","摘要翻译":"传统上，基于相机的跟踪方法将滚动快门和径向畸变视为必须克服和校正的成像伪影，以便应用标准相机模型和场景重建方法。在本文中，我们介绍了一种新颖的多相机跟踪方法，首次联合利用滚动快门和径向畸变引入的信息作为特征，以实现相对于高频相机姿态估计的卓越性能。特别是，我们的系统能够达到以前无法实现的高跟踪率。我们的方法明确利用滚动快门捕捉和径向畸变来处理单个行，而不是整个图像帧，以实现准确的相机运动估计。我们通过跟踪径向畸变行上的多个点来估计滚动快门相机的每行6自由度姿态，这些点的光线在3D空间中跨越一个曲面。尽管存在针对滚动快门相机的跟踪系统，但我们是第一个利用径向畸变来测量每行姿态的系统——使我们能够使用少于先前最先进技术所需相机数量的一半。我们在合成和真实图像上验证了我们的系统。","领域":"相机姿态估计/多相机跟踪/高频跟踪","问题":"如何在高帧率多相机跟踪中有效利用滚动快门和径向畸变","动机":"传统方法将滚动快门和径向畸变视为需要克服的成像伪影，限制了跟踪性能的提升","方法":"联合利用滚动快门和径向畸变作为特征，处理单个行而非整个图像帧，以估计每行6自由度姿态","关键词":["滚动快门","径向畸变","多相机跟踪","高频跟踪","相机姿态估计"],"涉及的技术概念":"滚动快门是一种相机捕捉技术，逐行扫描图像，导致快速移动的物体出现畸变。径向畸变是由于相机镜头的光学特性导致的图像边缘弯曲现象。6自由度（6 DoF）指的是物体在三维空间中的位置（3个自由度）和方向（3个自由度）。"},{"order":500,"title":"A Twofold Siamese Network for Real-Time Object Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_A_Twofold_Siamese_CVPR_2018_paper.html","abstract":"Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similarity learning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.","中文标题":"双重孪生网络用于实时目标跟踪","摘要翻译":"观察到在图像分类任务中学习的语义特征和在相似性匹配任务中学习的外观特征相互补充，我们构建了一个名为SA-Siam的双重孪生网络，用于实时目标跟踪。SA-Siam由语义分支和外观分支组成。每个分支都是一个相似性学习的孪生网络。SA-Siam中的一个重要设计选择是分别训练这两个分支，以保持两种类型特征的异质性。此外，我们为语义分支提出了一个通道注意力机制。根据目标位置周围的通道激活计算通道权重。虽然从SiamFC继承的架构使我们的跟踪器能够以超实时速度运行，但双重设计和注意力机制显著提高了跟踪性能。所提出的SA-Siam在OTB-2013/50/100基准测试中以较大优势优于所有其他实时跟踪器。","领域":"目标跟踪/语义特征提取/注意力机制","问题":"实时目标跟踪中的特征互补性问题","动机":"利用语义特征和外观特征的互补性提高实时目标跟踪的性能","方法":"构建双重孪生网络SA-Siam，分别训练语义分支和外观分支，并在语义分支中引入通道注意力机制","关键词":["孪生网络","实时目标跟踪","通道注意力机制"],"涉及的技术概念":{"孪生网络":"一种用于相似性学习的网络结构，通常由两个相同的子网络组成，用于比较两个输入的相似度。","实时目标跟踪":"在视频序列中实时跟踪目标对象的位置和状态。","通道注意力机制":"一种机制，通过计算通道权重来增强网络对重要特征的关注，从而提高模型性能。"}},{"order":501,"title":"Multi-Cue Correlation Filters for Robust Visual Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Multi-Cue_Correlation_Filters_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Multi-Cue_Correlation_Filters_CVPR_2018_paper.html","abstract":"In recent years, many tracking algorithms achieve impressive performance via fusing multiple types of features, however, most of them fail to fully explore the context among the adopted multiple features and the strength of them. In this paper, we propose an efficient multi-cue analysis framework for robust visual tracking. By combining different types of features, our approach constructs multiple experts through Discriminative Correlation Filter (DCF) and each of them tracks the target independently. With the proposed robustness evaluation strategy, the suitable expert is selected for tracking in each frame. Furthermore, the divergence of multiple experts reveals the reliability of the current tracking, which is quantified to update the experts adaptively to keep them from corruption.  Through the proposed multi-cue analysis, our tracker with standard DCF and deep features achieves outstanding results on several challenging benchmarks: OTB-2013, OTB-2015, Temple-Color and VOT 2016. On the other hand, when evaluated with only simple hand-crafted features, our method demonstrates comparable performance amongst complex non-realtime trackers, but exhibits much better efficiency, with a speed of 45 FPS on a CPU.","中文标题":"多线索相关滤波器用于鲁棒视觉跟踪","摘要翻译":"近年来，许多跟踪算法通过融合多种类型的特征取得了令人印象深刻的性能，然而，大多数算法未能充分利用所采用的多特征之间的上下文及其强度。在本文中，我们提出了一种高效的多线索分析框架，用于鲁棒视觉跟踪。通过结合不同类型的特征，我们的方法通过判别相关滤波器（DCF）构建多个专家，每个专家独立跟踪目标。通过提出的鲁棒性评估策略，为每一帧选择适合的专家进行跟踪。此外，多个专家的分歧揭示了当前跟踪的可靠性，该可靠性被量化以自适应地更新专家，防止其被破坏。通过提出的多线索分析，我们的跟踪器在标准DCF和深度特征的基础上，在多个具有挑战性的基准测试中取得了出色的结果：OTB-2013、OTB-2015、Temple-Color和VOT 2016。另一方面，当仅使用简单的手工特征进行评估时，我们的方法在复杂的非实时跟踪器中表现出可比的性能，但显示出更好的效率，在CPU上的速度为45 FPS。","领域":"视觉跟踪/特征融合/鲁棒性评估","问题":"如何有效融合多种类型的特征以提高视觉跟踪的鲁棒性和效率","动机":"现有大多数跟踪算法未能充分利用多特征之间的上下文及其强度，导致跟踪性能受限","方法":"提出了一种多线索分析框架，通过判别相关滤波器（DCF）构建多个专家，每个专家独立跟踪目标，并通过鲁棒性评估策略选择适合的专家进行跟踪，同时利用多个专家的分歧自适应更新专家","关键词":["视觉跟踪","特征融合","鲁棒性评估","判别相关滤波器"],"涉及的技术概念":"判别相关滤波器（DCF）是一种用于视觉跟踪的技术，通过构建滤波器来区分目标和背景。多线索分析框架指的是结合不同类型的特征来提高跟踪的准确性和鲁棒性。鲁棒性评估策略用于评估和选择最适合当前帧的跟踪专家。"},{"order":502,"title":"Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_Attentions_Residual_CVPR_2018_paper.html","abstract":"Offline training for object tracking has recently shown great potentials in balancing tracking accuracy and speed. However, it is still difficult to adapt an offline trained model to a target tracked online. This work presents a Residual Attentional Siamese Network (RASNet) for high performance object tracking. The RASNet model reformulates the correlation filter within a Siamese tracking framework, and introduces different kinds of the attention mechanisms to adapt the model without updating the model online. In particular, by exploiting the offline trained general attention, the target adapted residual attention, and the channel favored feature attention, the RASNet not only mitigates the over-fitting problem in deep network training, but also enhances its discriminative capacity and adaptability due to the separation of representation learning and discriminator learning. The proposed deep architecture is trained from end to end and takes full advantage of the rich spatial temporal information to achieve robust visual tracking. Experimental results on two latest benchmarks, OTB-2015 and VOT2017, show that the RASNet tracker has the state-of-the-art tracking accuracy while runs at more than 80 frames per second.","中文标题":"学习注意力：用于高性能在线视觉跟踪的残差注意力孪生网络","摘要翻译":"对象跟踪的离线训练最近在平衡跟踪精度和速度方面显示出巨大潜力。然而，将离线训练的模型适应于在线跟踪的目标仍然很困难。这项工作提出了一种用于高性能对象跟踪的残差注意力孪生网络（RASNet）。RASNet模型在孪生跟踪框架内重新制定了相关滤波器，并引入了不同类型的注意力机制，以在不更新模型的情况下适应模型。特别是，通过利用离线训练的通用注意力、目标适应的残差注意力和通道偏好的特征注意力，RASNet不仅缓解了深度网络训练中的过拟合问题，而且由于表示学习和判别器学习的分离，增强了其判别能力和适应性。所提出的深度架构是端到端训练的，并充分利用了丰富的时空信息来实现鲁棒的视觉跟踪。在OTB-2015和VOT2017两个最新基准上的实验结果表明，RASNet跟踪器具有最先进的跟踪精度，同时运行速度超过每秒80帧。","领域":"视觉跟踪/注意力机制/深度学习","问题":"如何在不更新模型的情况下，将离线训练的模型适应于在线跟踪的目标","动机":"提高对象跟踪的精度和速度，同时解决深度网络训练中的过拟合问题","方法":"提出了一种残差注意力孪生网络（RASNet），通过引入不同类型的注意力机制，如离线训练的通用注意力、目标适应的残差注意力和通道偏好的特征注意力，来适应模型","关键词":["视觉跟踪","注意力机制","深度学习"],"涉及的技术概念":"残差注意力孪生网络（RASNet）是一种深度架构，它通过重新制定相关滤波器和引入不同类型的注意力机制来适应模型，而无需在线更新模型。这种方法利用了离线训练的通用注意力、目标适应的残差注意力和通道偏好的特征注意力，以增强模型的判别能力和适应性。"},{"order":503,"title":"SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_SINT_Robust_Visual_CVPR_2018_paper.html","abstract":"Existing visual trackers are easily disturbed by occlusion,blurandlargedeformation. Inthechallengesofocclusion, motion blur and large object deformation, the performance of existing visual trackers may be limited due to the followingissues: i)Adoptingthedensesamplingstrategyto generate positive examples will make them less diverse; ii) Thetrainingdatawithdifferentchallengingfactorsarelimited, even though through collecting large training dataset. Collecting even larger training dataset is the most intuitive paradigm, but it may still can not cover all situations and the positive samples are still monotonous. In this paper, we propose to generate hard positive samples via adversarial learning for visual tracking. Speciﬁcally speaking, we assume the target objects all lie on a manifold, hence, we introduce the positive samples generation network (PSGN) to sampling massive diverse training data through traversing over the constructed target object manifold. The generated diverse target object images can enrich the training dataset and enhance the robustness of visual trackers. To make the tracker more robust to occlusion, we adopt the hard positive transformation network (HPTN) which can generate hard samples for tracking algorithm to recognize. We train this network with deep reinforcement learning to automaticallyoccludethetargetobjectwithanegativepatch. Based on the generated hard positive samples, we train a Siamese network for visual tracking and our experiments validate the effectiveness of the introduced algorithm.","中文标题":"SINT++：通过对抗性正例生成实现鲁棒视觉跟踪","摘要翻译":"现有的视觉跟踪器容易受到遮挡、模糊和大变形的干扰。在面对遮挡、运动模糊和大物体变形的挑战时，现有视觉跟踪器的性能可能受到以下问题的限制：i) 采用密集采样策略生成正例会使其多样性降低；ii) 尽管通过收集大量训练数据集，具有不同挑战因素的训练数据仍然有限。收集更大的训练数据集是最直观的范式，但它可能仍然无法覆盖所有情况，且正样本仍然单调。在本文中，我们提出通过对抗学习生成困难正样本用于视觉跟踪。具体来说，我们假设目标对象都位于一个流形上，因此，我们引入了正样本生成网络（PSGN），通过在构建的目标对象流形上遍历来采样大量多样的训练数据。生成的多样目标对象图像可以丰富训练数据集并增强视觉跟踪器的鲁棒性。为了使跟踪器对遮挡更加鲁棒，我们采用了困难正变换网络（HPTN），它可以生成困难样本供跟踪算法识别。我们使用深度强化学习训练这个网络，以自动用负片遮挡目标对象。基于生成的困难正样本，我们训练了一个用于视觉跟踪的Siamese网络，我们的实验验证了引入算法的有效性。","领域":"视觉跟踪/对抗学习/深度强化学习","问题":"视觉跟踪器在面对遮挡、模糊和大变形时的性能限制","动机":"提高视觉跟踪器在遮挡、运动模糊和大物体变形等挑战下的鲁棒性和多样性","方法":"通过对抗学习生成困难正样本，使用正样本生成网络（PSGN）和困难正变换网络（HPTN）增强训练数据的多样性和鲁棒性，并训练Siamese网络进行视觉跟踪","关键词":["视觉跟踪","对抗学习","深度强化学习","Siamese网络"],"涉及的技术概念":"正样本生成网络（PSGN）通过在目标对象流形上遍历来采样多样训练数据；困难正变换网络（HPTN）使用深度强化学习自动生成困难样本，以增强跟踪器对遮挡的鲁棒性；Siamese网络用于视觉跟踪，通过生成的困难正样本进行训练。"},{"order":504,"title":"High-Speed Tracking With Multi-Kernel Correlation Filters","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_High-Speed_Tracking_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_High-Speed_Tracking_With_CVPR_2018_paper.html","abstract":"Correlation filter (CF) based trackers are currently ranked top in terms of their performances. Nevertheless, only some of them, such as KCF [henriques12&15] and MKCF[tang&Feng15}, are able to exploit the powerful discriminability of non-linear kernels. Although MKCF achieves more powerful discriminability than KCF through introducing multi-kernel learning (MKL) into KCF, its improvement over KCF is quite limited and its computational burden increases significantly in comparison with KCF. In this paper, we will introduce the MKL into KCF in a different way than MKCF. We reformulate the MKL version of CF objective function with its upper bound, alleviating the negative mutual interference of different kernels significantly. Our novel MKCF tracker, MKCFup, outperforms KCF and MKCF with large margins and can still work at very high fps. Extensive experiments on public data sets show that our method is superior to state-of-the-art algorithms for target objects of small move at very high speed.","中文标题":"使用多核相关滤波器进行高速跟踪","摘要翻译":"基于相关滤波器（CF）的跟踪器目前在性能方面排名最高。然而，只有其中一些，如KCF [henriques12&15] 和 MKCF[tang&Feng15}，能够利用非线性核的强大辨别能力。尽管MKCF通过将多核学习（MKL）引入KCF中实现了比KCF更强大的辨别能力，但相比KCF，其改进相当有限，并且计算负担显著增加。在本文中，我们将以不同于MKCF的方式将MKL引入KCF。我们重新制定了CF目标函数的MKL版本，并设定了其上界，显著减轻了不同核之间的负面相互干扰。我们新颖的MKCF跟踪器，MKCFup，大幅超越了KCF和MKCF，并且仍然可以在非常高的帧率下工作。在公共数据集上的大量实验表明，我们的方法在高速移动的小目标物体上优于最先进的算法。","领域":"目标跟踪/视频分析/实时系统","问题":"提高基于相关滤波器的跟踪器在高速移动小目标上的性能和效率","动机":"现有的多核相关滤波器（MKCF）虽然提高了辨别能力，但改进有限且计算负担增加，需要一种新的方法来提高性能和效率","方法":"通过重新制定多核学习（MKL）版本的相关滤波器（CF）目标函数，并设定其上界，减轻不同核之间的负面相互干扰，开发出新的MKCF跟踪器MKCFup","关键词":["目标跟踪","多核学习","相关滤波器","实时系统"],"涉及的技术概念":{"相关滤波器（CF）":"一种用于目标跟踪的技术，通过计算目标与候选区域之间的相关性来定位目标","多核学习（MKL）":"一种机器学习方法，通过组合多个核函数来提高模型的辨别能力","帧率（fps）":"视频中每秒显示的帧数，用于衡量视频的流畅度"}},{"order":505,"title":"Occlusion Aware Unsupervised Learning of Optical Flow","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Occlusion_Aware_Unsupervised_CVPR_2018_paper.html","abstract":"It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper- vised learning. However, the performance of the unsuper- vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper- vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Espe- cially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.","中文标题":"遮挡感知的无监督光流学习","摘要翻译":"最近研究表明，卷积神经网络可以通过无监督学习进行光流估计。然而，与有监督学习相比，无监督方法的性能仍有较大差距。遮挡和大运动是限制当前无监督光流学习方法的主要因素。在这项工作中，我们引入了一种新方法，该方法明确地建模了遮挡，并提出了一种新的扭曲方式，以促进大运动的学习。我们的方法在Flying Chairs、MPI-Sintel和KITTI基准数据集上显示出有希望的结果。特别是在KITTI数据集上，存在大量未标记样本，我们的无监督方法优于使用有监督学习训练的对应方法。","领域":"光流估计/无监督学习/卷积神经网络","问题":"解决无监督光流学习中的遮挡和大运动问题","动机":"提高无监督光流学习方法的性能，特别是在处理遮挡和大运动方面","方法":"引入一种新方法，明确建模遮挡，并提出一种新的扭曲方式以促进大运动的学习","关键词":["光流估计","无监督学习","卷积神经网络","遮挡","大运动"],"涉及的技术概念":"卷积神经网络（CNN）用于光流估计，无监督学习方法，遮挡建模，大运动处理，Flying Chairs、MPI-Sintel和KITTI数据集"},{"order":506,"title":"Revisiting Video Saliency: A Large-Scale Benchmark and a New Model","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Revisiting_Video_Saliency_CVPR_2018_paper.html","abstract":"In this work, we contribute to video saliency research in two ways. First, we introduce a new benchmark for predicting human eye movements during dynamic scene free-viewing, which is long-time urged in this field. Our dataset, named DHF1K~(Dynamic Human Fixation), consists of 1K high-quality, elaborately selected video sequences spanning a large range of scenes, motions, object types and background complexity. Existing video saliency datasets lack variety and generality of common dynamic scenes and fall short in covering challenging situations in unconstrained environments. In contrast, DHF1K~makes a significant leap in terms of scalability, diversity and difficulty, and is expected to boost video saliency modeling. Second, we propose a novel video saliency model that augments the CNN-LSTM network architecture with an attention mechanism to enable fast, end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. We thoroughly examine the performance of our model, with respect to state-of-the-art saliency models, on three large-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that our model outperforms other competitors.","中文标题":"重新审视视频显著性：一个大规模基准和新模型","摘要翻译":"在这项工作中，我们以两种方式对视频显著性研究做出了贡献。首先，我们引入了一个新的基准，用于预测动态场景自由观看期间的人类眼动，这是该领域长期以来的需求。我们的数据集名为DHF1K（动态人类注视），由1K高质量、精心挑选的视频序列组成，涵盖了广泛的场景、运动、对象类型和背景复杂性。现有的视频显著性数据集缺乏常见动态场景的多样性和普遍性，并且在覆盖无约束环境中的挑战性情况方面存在不足。相比之下，DHF1K在可扩展性、多样性和难度方面实现了显著飞跃，预计将推动视频显著性建模的发展。其次，我们提出了一种新颖的视频显著性模型，该模型通过引入注意力机制增强了CNN-LSTM网络架构，以实现快速、端到端的显著性学习。注意力机制显式编码静态显著性信息，从而使LSTM能够专注于学习跨连续帧的更灵活的时间显著性表示。这样的设计充分利用了现有的大规模静态注视数据集，避免了过拟合，并显著提高了训练效率和测试性能。我们在三个大规模数据集（即DHF1K、Hollywood2、UCF体育）上，与最先进的显著性模型相比，彻底检查了我们模型的性能。在包含400K帧的超过1.2K测试视频上的实验结果表明，我们的模型优于其他竞争对手。","领域":"视频显著性/眼动预测/动态场景分析","问题":"现有视频显著性数据集缺乏多样性和普遍性，无法覆盖无约束环境中的挑战性情况","动机":"推动视频显著性建模的发展，提高预测人类眼动的准确性","方法":"引入新的基准数据集DHF1K，并提出一种新颖的视频显著性模型，该模型通过引入注意力机制增强了CNN-LSTM网络架构","关键词":["视频显著性","眼动预测","动态场景分析","注意力机制","CNN-LSTM"],"涉及的技术概念":"DHF1K数据集是一个包含1K高质量视频序列的数据集，用于预测动态场景自由观看期间的人类眼动。提出的视频显著性模型通过引入注意力机制增强了CNN-LSTM网络架构，以实现快速、端到端的显著性学习。注意力机制显式编码静态显著性信息，使LSTM能够专注于学习跨连续帧的更灵活的时间显著性表示。"},{"order":507,"title":"Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.html","abstract":"Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Compared with SRDCF, STRCF with hand-crafted features provides a 5× speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF with deep features also performs favorably against state-of-the-art trackers and achieves an AUC score of 68.3% on OTB-2015.","中文标题":"学习时空正则化相关滤波器用于视觉跟踪","摘要翻译":"判别相关滤波器（DCF）在视觉跟踪中效率高，但受到不希望的边界效应的影响。空间正则化DCF（SRDCF）通过在DCF系数上施加空间惩罚来解决这个问题，这不可避免地以增加复杂性为代价提高了跟踪性能。为了解决在线更新问题，SRDCF在多个训练图像上制定其模型，进一步增加了提高效率的难度。在这项工作中，通过引入时间正则化到SRDCF中，我们提出了时空正则化相关滤波器（STRCF）。STRCF公式不仅可以作为具有多个训练样本的SRDCF的合理近似，而且在大外观变化的情况下提供比SRDCF更稳健的外观模型。此外，它可以通过交替方向乘子法（ADMM）高效解决。通过结合时间和空间正则化，我们的STRCF可以处理边界效应而不会在效率上有太大损失，并在准确性和速度方面优于SRDCF。与SRDCF相比，使用手工特征的STRCF提供了5倍的速度提升，并在OTB-2015和Temple-Color上分别实现了5.4%和3.6%的AUC分数增益。此外，使用深度特征的STRCF也表现出色，与最先进的跟踪器相比，在OTB-2015上实现了68.3%的AUC分数。","领域":"视觉跟踪/相关滤波器/正则化方法","问题":"解决视觉跟踪中判别相关滤波器（DCF）的边界效应问题","动机":"提高视觉跟踪的准确性和速度，同时处理边界效应和在线更新问题","方法":"引入时间正则化到空间正则化DCF（SRDCF）中，提出时空正则化相关滤波器（STRCF），并通过交替方向乘子法（ADMM）高效解决","关键词":["视觉跟踪","相关滤波器","正则化方法","交替方向乘子法"],"涉及的技术概念":{"判别相关滤波器（DCF）":"一种用于视觉跟踪的高效方法，但存在边界效应问题。","空间正则化DCF（SRDCF）":"通过在DCF系数上施加空间惩罚来解决边界效应问题，但增加了复杂性。","时空正则化相关滤波器（STRCF）":"结合时间和空间正则化的方法，旨在提高视觉跟踪的准确性和速度。","交替方向乘子法（ADMM）":"一种用于高效解决优化问题的方法，用于实现STRCF。","AUC分数":"用于评估跟踪器性能的指标，表示跟踪准确性的面积下曲线。"}},{"order":508,"title":"Multimodal Visual Concept Learning With Weakly Supervised Techniques","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bouritsas_Multimodal_Visual_Concept_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bouritsas_Multimodal_Visual_Concept_CVPR_2018_paper.html","abstract":"Despite the availability of a huge amount of video data accompanied by descriptive texts, it is not always easy to exploit the information contained in natural language in order to automatically recognize video concepts. Towards this goal, in this paper we use textual cues as means of supervision, introducing two weakly supervised techniques that extend the Multiple Instance Learning (MIL) framework: the Fuzzy Sets Multiple Instance Learning (FSMIL) and the Probabilistic Labels Multiple Instance Learning (PLMIL). The former encodes the spatio-temporal imprecision of the linguistic descriptions with Fuzzy Sets, while the latter models different interpretations of each description’s semantics with Probabilistic Labels, both formulated through a convex optimization algorithm. In addition, we provide a novel technique to extract weak labels in the presence of complex semantics, that consists of semantic similarity computations. We evaluate our methods on two distinct problems, namely face and action recognition, in the challenging and realistic setting of movies accompanied by their screenplays, contained in the COGNIMUSE database. We show that, on both tasks, our method considerably outperforms a state-of-the-art weakly supervised approach, as well as other baselines.","中文标题":"多模态视觉概念学习与弱监督技术","摘要翻译":"尽管有大量伴随描述性文本的视频数据可用，但利用自然语言中的信息自动识别视频概念并不总是容易的。为了实现这一目标，本文使用文本线索作为监督手段，引入了两种扩展多实例学习（MIL）框架的弱监督技术：模糊集多实例学习（FSMIL）和概率标签多实例学习（PLMIL）。前者通过模糊集编码语言描述的时空不精确性，而后者通过概率标签建模每个描述语义的不同解释，两者都通过凸优化算法进行公式化。此外，我们提供了一种在复杂语义存在下提取弱标签的新技术，该技术包括语义相似度计算。我们在COGNIMUSE数据库中包含的电影及其剧本的挑战性和现实性设置下，评估了我们的方法在两个不同问题上的表现，即面部识别和动作识别。我们展示了在这两个任务上，我们的方法显著优于最先进的弱监督方法以及其他基线方法。","领域":"视频理解, 面部识别, 动作识别","问题":"如何利用自然语言中的信息自动识别视频概念","动机":"尽管有大量伴随描述性文本的视频数据可用，但利用自然语言中的信息自动识别视频概念并不总是容易的","方法":"引入了两种扩展多实例学习（MIL）框架的弱监督技术：模糊集多实例学习（FSMIL）和概率标签多实例学习（PLMIL），并通过凸优化算法进行公式化。此外，提供了一种在复杂语义存在下提取弱标签的新技术，该技术包括语义相似度计算","关键词":["多实例学习","模糊集","概率标签","语义相似度","凸优化算法"],"涉及的技术概念":"多实例学习（MIL）是一种机器学习方法，用于处理每个训练样本由多个实例组成的情况。模糊集是一种处理不确定性和不精确性的数学工具。概率标签用于建模每个描述语义的不同解释。语义相似度计算是一种评估两个语义表达之间相似程度的技术。凸优化算法是一种寻找凸函数最小值的数学优化方法。"},{"order":509,"title":"Efficient Large-Scale Approximate Nearest Neighbor Search on OpenCL FPGA","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Efficient_Large-Scale_Approximate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Efficient_Large-Scale_Approximate_CVPR_2018_paper.html","abstract":"We present a new method for Product Quantization (PQ) based approximated nearest neighbor search (ANN) in high dimensional spaces. Specifically, we first propose a quantization scheme for the codebook of coarse quantizer, product quantizer, and rotation matrix, to reduce the cost of accessing these codebooks. Our approach also combines a highly parallel k-selection method, which can be fused with the distance calculation to reduce the memory overhead. We implement the proposed method on Intel HARPv2 platform using OpenCL-FPGA.  The proposed method significantly outperforms state-of-the-art methods on CPU and GPU for high dimensional nearest neighbor queries on  billion-scale datasets in terms of query time and accuracy regardless of the batch size. To our best knowledge, this is the first work to demonstrate FPGA performance superior to CPU and GPU on high-dimensional, large-scale ANN datasets.","中文标题":"基于OpenCL FPGA的高效大规模近似最近邻搜索","摘要翻译":"我们提出了一种新的基于产品量化（PQ）的高维空间近似最近邻搜索（ANN）方法。具体来说，我们首先提出了一种量化方案，用于粗量化器、产品量化器和旋转矩阵的码本，以减少访问这些码本的成本。我们的方法还结合了一种高度并行的k选择方法，该方法可以与距离计算融合，以减少内存开销。我们在Intel HARPv2平台上使用OpenCL-FPGA实现了所提出的方法。所提出的方法在十亿级数据集上的高维最近邻查询中，在查询时间和准确性方面显著优于CPU和GPU上的最先进方法，无论批量大小如何。据我们所知，这是首次在高维、大规模ANN数据集上展示FPGA性能优于CPU和GPU的工作。","领域":"高维数据处理/近似最近邻搜索/硬件加速","问题":"高维空间中大规模近似最近邻搜索的效率问题","动机":"提高高维空间中大规模近似最近邻搜索的查询效率和准确性，同时减少内存开销","方法":"提出了一种新的量化方案，结合高度并行的k选择方法，并在Intel HARPv2平台上使用OpenCL-FPGA实现","关键词":["产品量化","近似最近邻搜索","高维数据处理","硬件加速"],"涉及的技术概念":{"产品量化（PQ）":"一种用于高维数据压缩和近似最近邻搜索的技术，通过将高维空间分解为多个低维子空间并分别量化来减少存储和计算成本。","近似最近邻搜索（ANN）":"在高维数据集中寻找与查询点最接近的数据点的近似方法，旨在提高搜索效率。","OpenCL-FPGA":"使用OpenCL编程模型在FPGA（现场可编程门阵列）上实现并行计算，以加速特定计算任务。","Intel HARPv2平台":"一种结合了CPU和FPGA的混合计算平台，旨在提供高性能计算能力。"}},{"order":510,"title":"Learning a Complete Image Indexing Pipeline","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jain_Learning_a_Complete_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jain_Learning_a_Complete_CVPR_2018_paper.html","abstract":"To work at scale, a complete image indexing system comprises two components: An inverted file index to restrict the actual search to only a subset that should contain most of the items relevant to the query; An approximate distance computation mechanism to rapidly scan these lists. While supervised deep learning has recently enabled improvements to the latter, the former continues to be based on unsupervised clustering in the literature. In this work, we propose a first system that learns both components within a unifying neural framework of structured binary encoding.","中文标题":"学习完整的图像索引管道","摘要翻译":"为了在大规模下工作，一个完整的图像索引系统包括两个组件：一个倒排文件索引，用于将实际搜索限制在应该包含与查询最相关的大部分项目的子集上；一个近似距离计算机制，用于快速扫描这些列表。虽然监督深度学习最近使得后者得到了改进，但前者在文献中仍然基于无监督聚类。在这项工作中，我们提出了第一个系统，该系统在一个统一的结构化二进制编码神经框架内学习这两个组件。","领域":"图像检索/深度学习/信息检索","问题":"如何在大规模图像数据中有效地进行索引和搜索","动机":"提高图像检索系统的效率和准确性，通过深度学习技术改进现有的图像索引方法","方法":"提出了一种新的系统，该系统在一个统一的结构化二进制编码神经框架内学习倒排文件索引和近似距离计算机制","关键词":["图像检索","深度学习","信息检索","结构化二进制编码"],"涉及的技术概念":"倒排文件索引是一种数据结构，用于存储从关键词到文档的映射，以便快速检索。近似距离计算机制用于在大量数据中快速找到与查询最接近的项。结构化二进制编码是一种将数据转换为二进制形式的技术，以便于存储和快速检索。"},{"order":511,"title":"Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mascharka_Transparency_by_Design_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mascharka_Transparency_by_Design_CVPR_2018_paper.html","abstract":"Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives’ outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.","中文标题":"设计透明性：缩小视觉推理中性能与可解释性之间的差距","摘要翻译":"视觉问答需要对图像进行高阶推理，这是机器系统遵循复杂指令所需的基本能力。最近，模块化网络已被证明是执行视觉推理任务的有效框架。虽然模块化网络最初设计时具有一定的模型透明度，但它们在复杂视觉推理基准上的表现却不足。当前的最先进方法没有提供理解推理过程的有效机制。在本文中，我们缩小了可解释模型与最先进视觉推理方法之间的性能差距。我们提出了一组视觉推理原语，当这些原语组合时，表现为一个能够以明确可解释的方式执行复杂推理任务的模型。这些原语输出的保真度和可解释性使得诊断结果模型的优缺点具有无与伦比的能力。关键的是，我们展示了这些原语具有很高的性能，在CLEVR数据集上达到了99.1%的最先进准确率。我们还展示了我们的模型在提供少量包含新对象属性的数据时，能够有效地学习广义表示。使用CoGenT泛化任务，我们展示了比当前最先进技术超过20个百分点的改进。","领域":"视觉问答/模块化网络/模型可解释性","问题":"提高视觉推理任务中模型的可解释性同时不牺牲性能","动机":"当前最先进的视觉推理方法缺乏有效的机制来理解推理过程，而模块化网络虽然在设计上具有一定的透明度，但在复杂视觉推理任务上的表现不足。","方法":"提出一组视觉推理原语，这些原语组合后能够以明确可解释的方式执行复杂推理任务，同时保持高保真度和可解释性。","关键词":["视觉问答","模块化网络","模型可解释性","视觉推理原语","CLEVR数据集","CoGenT泛化任务"],"涉及的技术概念":{"视觉问答":"一种需要机器系统对图像进行高阶推理以回答问题的任务。","模块化网络":"一种设计用于执行视觉推理任务的网络框架，具有一定的模型透明度。","模型可解释性":"指模型的决策过程能够被人类理解和解释的程度。","视觉推理原语":"一组基本的视觉推理操作，可以组合起来执行复杂的推理任务。","CLEVR数据集":"一个用于评估视觉推理能力的合成数据集。","CoGenT泛化任务":"一种用于评估模型在遇到新对象属性时泛化能力的任务。"}},{"order":512,"title":"Fooling Vision and Language Models Despite Localization and Attention Mechanism","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Fooling_Vision_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Fooling_Vision_and_CVPR_2018_paper.html","abstract":"Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., >90%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.","中文标题":"欺骗视觉和语言模型尽管有定位和注意力机制","摘要翻译":"已知对抗性攻击在分类器上能够成功，但更复杂的视觉系统是否易受攻击一直是一个开放性问题。在本文中，我们研究了视觉和语言模型的对抗性示例，这些模型结合了自然语言理解和复杂结构，如注意力、定位和模块化架构。特别是，我们调查了对密集字幕模型和两个视觉问答（VQA）模型的攻击。我们的评估显示，我们可以为这些模型生成高成功率（即>90%）的对抗性示例。我们的工作为理解具有语言组件的视觉系统的对抗性攻击提供了新的视角，并表明注意力、边界框定位和组合内部结构易受对抗性攻击。这些观察将为未来构建有效防御的工作提供信息。","领域":"视觉问答/密集字幕/对抗性攻击","问题":"视觉和语言模型是否易受对抗性攻击","动机":"探索更复杂的视觉系统是否易受对抗性攻击，特别是那些结合了自然语言理解和复杂结构的模型","方法":"研究了对密集字幕模型和两个视觉问答（VQA）模型的对抗性攻击，评估了攻击的成功率","关键词":["对抗性攻击","视觉问答","密集字幕"],"涉及的技术概念":"对抗性攻击指的是通过精心设计的输入来欺骗机器学习模型，使其做出错误的预测或分类。视觉问答（VQA）是一种结合了视觉和语言理解的技术，旨在回答关于图像内容的问题。密集字幕是一种技术，旨在为图像中的每个区域生成详细的描述。注意力机制是一种使模型能够专注于输入数据中重要部分的技术。边界框定位是一种在图像中定位对象的技术。组合内部结构指的是模型内部的结构设计，使其能够处理复杂的输入和任务。"},{"order":513,"title":"Categorizing Concepts With Basic Level for Vision-to-Language","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Categorizing_Concepts_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Categorizing_Concepts_With_CVPR_2018_paper.html","abstract":"Vision-to-language tasks require a unified semantic understanding of visual content. However, the information contained in image/video is essentially ambiguous on two perspectives manifested on the diverse understanding among different persons and the various understanding grains even for the same person. Inspired by the basic level in early cognition, a Basic Concept (BaC) category is proposed in this work that contains both consensus and proper level of visual content to help neural network tackle the above problems. Specifically, a salient concept category is firstly generated by intersecting the labels of ImageNet and the vocabulary of MSCOCO dataset. Then, according to the observation from human early cognition that children make fewer mistakes on the basic level, the salient category is further refined by clustering concepts with a defined confusion degree which measures the difficulty for convolutional neural network to distinguish class pairs. Finally, a pre-trained model based on GoogLeNet is produced with the proposed BaC category of 1,372 concept classes. To verify the effectiveness of the proposed categorizing method for vision-to-language tasks, two kinds of experiments are performed including image captioning and visual question answering with the benchmark datasets of MSCOCO, Flickr30k and COCO-QA. The experimental results demonstrate that the representations derived from the cognition-inspired BaC category promote representation learning of neural networks on vision-to-language tasks, and a performance improvement is gained without modifying standard models.","中文标题":"使用基础级别对视觉到语言任务中的概念进行分类","摘要翻译":"视觉到语言任务需要对视觉内容有统一的语义理解。然而，图像/视频中包含的信息本质上在两个角度上是模糊的，这体现在不同人之间的理解差异以及即使对于同一个人也有不同的理解粒度上。受到早期认知中基础级别的启发，本文提出了一个基础概念（BaC）类别，它包含了共识和适当级别的视觉内容，以帮助神经网络解决上述问题。具体来说，首先通过交叉ImageNet的标签和MSCOCO数据集的词汇生成一个显著概念类别。然后，根据人类早期认知中儿童在基础级别上犯错较少的观察，通过聚类概念并定义一个混淆度来进一步精炼显著类别，该混淆度衡量了卷积神经网络区分类别对的难度。最后，基于GoogLeNet的预训练模型被生成，其中包含了1,372个概念类别的BaC类别。为了验证所提出的分类方法在视觉到语言任务中的有效性，进行了两种实验，包括使用MSCOCO、Flickr30k和COCO-QA基准数据集的图像描述和视觉问答。实验结果表明，源自认知启发的BaC类别的表示促进了神经网络在视觉到语言任务上的表示学习，并且在不修改标准模型的情况下获得了性能提升。","领域":"视觉到语言转换/图像描述/视觉问答","问题":"解决视觉到语言任务中视觉内容理解的模糊性问题","动机":"受到人类早期认知中基础级别的启发，提出基础概念（BaC）类别以帮助神经网络更好地理解和处理视觉内容","方法":"通过交叉ImageNet和MSCOCO数据集的标签生成显著概念类别，然后根据人类早期认知的观察精炼这些类别，最后基于GoogLeNet生成预训练模型","关键词":["基础概念类别","视觉到语言任务","卷积神经网络","混淆度","表示学习"],"涉及的技术概念":"基础概念（BaC）类别是通过交叉ImageNet和MSCOCO数据集的标签生成的，旨在提供一个共识和适当级别的视觉内容理解。混淆度是一个衡量卷积神经网络区分类别对难度的指标。GoogLeNet是一种深度卷积神经网络架构，用于图像识别和分类任务。"},{"order":514,"title":"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html","abstract":"A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.","中文标题":"不要只是假设；观察并回答：克服视觉问答中的先验","摘要翻译":"许多研究发现，当今的视觉问答（VQA）模型在很大程度上受到训练数据中的表面相关性驱动，缺乏足够的图像基础。为了鼓励开发更注重后者的模型，我们提出了一个新的VQA设置，其中对于每种问题类型，训练集和测试集具有不同的答案先验分布。具体来说，我们提出了VQA v1和VQA v2数据集的新分割，我们称之为在变化先验下的视觉问答（分别为VQA-CP v1和VQA-CP v2）。首先，我们评估了几种现有的VQA模型在这种新设置下的表现，并显示它们的性能相比原始VQA设置显著下降。其次，我们提出了一种新颖的基于视觉的问答模型（GVQA），该模型在架构中包含归纳偏差和限制，专门设计用于防止模型主要通过依赖训练数据中的先验来“作弊”。具体来说，GVQA明确地将图像中视觉概念的识别与给定问题的可能答案空间的识别分离开来，使模型能够更稳健地泛化到不同的答案分布。GVQA是基于现有的VQA模型——堆叠注意力网络（SAN）构建的。我们的实验表明，GVQA在VQA-CP v1和VQA-CP v2数据集上显著优于SAN。有趣的是，它在某些情况下也优于更强大的VQA模型，如多模态紧凑双线性池化（MCB）。当在原始VQA v1和VQA v2数据集上训练和评估时，GVQA提供了与SAN互补的优势。最后，GVQA比现有的VQA模型更透明和可解释。","领域":"视觉问答/模型泛化/数据集分割","问题":"解决视觉问答模型过度依赖训练数据中的先验分布，缺乏足够的图像基础的问题","动机":"鼓励开发更注重图像基础的视觉问答模型，提高模型在不同答案分布下的泛化能力","方法":"提出新的VQA设置和数据集分割（VQA-CP v1和VQA-CP v2），并开发一种新颖的基于视觉的问答模型（GVQA），该模型通过架构中的归纳偏差和限制，明确分离视觉概念识别和答案空间识别，以提高模型的泛化能力和透明度","关键词":["视觉问答","模型泛化","数据集分割","归纳偏差","透明度"],"涉及的技术概念":"视觉问答（VQA）、先验分布、归纳偏差、堆叠注意力网络（SAN）、多模态紧凑双线性池化（MCB）"},{"order":515,"title":"Learning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ahn_Learning_Pixel-Level_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ahn_Learning_Pixel-Level_Semantic_CVPR_2018_paper.html","abstract":"The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.","中文标题":"学习像素级语义亲和力与图像级监督用于弱监督语义分割","摘要翻译":"分割标签的缺乏是野外语义分割的主要障碍之一。为了缓解这个问题，我们提出了一个新颖的框架，该框架在给定图像级类别标签的情况下生成图像的分割标签。在这种弱监督设置下，已知训练模型会分割局部判别部分而不是整个对象区域。我们的解决方案是将这种局部响应传播到属于同一语义实体的附近区域。为此，我们提出了一个称为AffinityNet的深度神经网络（DNN），它预测相邻图像坐标对之间的语义亲和力。然后通过随机游走与AffinityNet预测的亲和力实现语义传播。更重要的是，用于训练AffinityNet的监督由初始判别部分分割提供，这种分割作为分割注释是不完整的，但对于学习小图像区域内的语义亲和力是足够的。因此，整个框架仅依赖于图像级类别标签，不需要任何额外数据或注释。在PASCAL VOC 2012数据集上，使用我们的方法生成的分割标签学习的DNN优于之前使用相同级别监督训练的模型，甚至与那些依赖更强监督的模型竞争。","领域":"语义分割/弱监督学习/图像理解","问题":"在缺乏详细分割标签的情况下，如何有效地进行语义分割","动机":"解决野外语义分割中分割标签缺乏的问题，通过弱监督学习提高分割的准确性和效率","方法":"提出AffinityNet深度神经网络预测相邻图像坐标对之间的语义亲和力，并通过随机游走实现语义传播","关键词":["语义分割","弱监督学习","图像理解","AffinityNet","随机游走"],"涉及的技术概念":"AffinityNet是一个深度神经网络，用于预测图像中相邻坐标对之间的语义亲和力。随机游走是一种基于图的算法，用于根据预测的亲和力在图像中传播语义信息。这种方法允许在仅使用图像级类别标签的情况下，生成有效的分割标签，从而在弱监督设置下实现语义分割。"},{"order":516,"title":"From Lifestyle Vlogs to Everyday Interactions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fouhey_From_Lifestyle_Vlogs_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fouhey_From_Lifestyle_Vlogs_CVPR_2018_paper.html","abstract":"A major stumbling block to progress in understanding basic human interactions, such as getting out of bed or opening a refrigerator, is lack of good training data. Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it.  We use Internet Lifestyle Vlogs as the source of surprisingly large and diverse interaction data.   We show that by collecting the data first, we are able to achieve greater scale and far greater diversity in terms of actions and actors. Additionally, our data exposes biases built into common explicitly gathered data. We make sense of our data by analyzing the central component of interaction -- hands. We benchmark two tasks: identifying semantic object contact at the video level and non-semantic contact state at the frame level. We additionally demonstrate future prediction of hands.","中文标题":"从生活方式的视频博客到日常互动","摘要翻译":"在理解基本人类互动（如起床或打开冰箱）方面取得进展的一个主要障碍是缺乏良好的训练数据。过去的大多数努力都是明确地收集这些数据：从一系列动作标签开始，然后查询搜索引擎以获取每个标签标记的视频。在这项工作中，我们反其道而行之，进行隐式搜索：我们从大量富含互动的视频数据开始，然后对其进行注释和分析。我们使用互联网生活方式视频博客作为来源，这些博客提供了出人意料的大量和多样化的互动数据。我们展示了通过首先收集数据，我们能够在动作和参与者方面实现更大的规模和更大的多样性。此外，我们的数据揭示了常见明确收集数据中固有的偏见。我们通过分析互动的核心组成部分——手，来理解我们的数据。我们对两个任务进行了基准测试：在视频级别识别语义对象接触和在帧级别识别非语义接触状态。我们还展示了手的未来预测。","领域":"人类行为分析/视频分析/互动识别","问题":"缺乏良好的训练数据以理解基本人类互动","动机":"通过收集和分析大量富含互动的视频数据，以克服理解基本人类互动的障碍","方法":"使用互联网生活方式视频博客作为数据源，进行隐式搜索、注释和分析，特别是通过分析手的互动","关键词":["人类行为分析","视频分析","互动识别"],"涉及的技术概念":"隐式搜索、视频数据注释、语义对象接触识别、非语义接触状态识别、手的未来预测"},{"order":517,"title":"Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Inoue_Cross-Domain_Weakly-Supervised_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Inoue_Cross-Domain_Weakly-Supervised_Object_CVPR_2018_paper.html","abstract":"Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.","中文标题":"跨领域弱监督对象检测通过渐进式领域适应","摘要翻译":"我们能否在没有实例级注释的情况下检测各种图像领域中的常见对象？在本文中，我们提出了一个框架，用于解决这一问题的跨领域弱监督对象检测新任务。对于本文，我们可以访问源领域（例如，自然图像）中带有实例级注释的图像和目标领域（例如，水彩画）中带有图像级注释的图像。此外，目标领域中要检测的类别是源领域中所有或部分类别。从一个在源领域上预训练的完全监督对象检测器开始，我们提出了一种两步渐进式领域适应技术，通过在两种类型的人工和自动生成的样本上微调检测器。我们在新收集的包含三个图像领域的数据集上测试了我们的方法，并在平均精度（mAP）方面比表现最佳的基线提高了大约5到20个百分点。","领域":"对象检测/领域适应/弱监督学习","问题":"在没有实例级注释的情况下检测各种图像领域中的常见对象","动机":"解决跨领域弱监督对象检测的问题，即在目标领域没有实例级注释的情况下检测对象","方法":"提出了一种两步渐进式领域适应技术，通过在两种类型的人工和自动生成的样本上微调检测器","关键词":["对象检测","领域适应","弱监督学习"],"涉及的技术概念":"实例级注释、图像级注释、源领域、目标领域、完全监督对象检测器、渐进式领域适应技术、平均精度（mAP）"},{"order":518,"title":"RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews From Unsupervised Viewpoints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanezaki_RotationNet_Joint_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kanezaki_RotationNet_Joint_Object_CVPR_2018_paper.html","abstract":"We propose a Convolutional Neural Network (CNN)-based model \`\`RotationNet,'' which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet is designed to use only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves the state-of-the-art performance on an object pose estimation dataset.","中文标题":"RotationNet：使用无监督视角的多视图进行联合对象分类和姿态估计","摘要翻译":"我们提出了一个基于卷积神经网络（CNN）的模型“RotationNet”，该模型以对象的多视图图像作为输入，并联合估计其姿态和对象类别。与之前使用已知视角标签进行训练的方法不同，我们的方法将视角标签视为潜在变量，这些变量在训练过程中使用未对齐的对象数据集以无监督的方式学习。RotationNet设计为仅使用部分多视图图像进行推理，这一特性使其在实际场景中非常有用，因为在这些场景中通常只能获得部分视图。此外，我们的姿态对齐策略使得可以获得跨类别共享的视图特定特征表示，这对于保持对象分类和姿态估计的高准确性非常重要。RotationNet的有效性通过在10类和40类ModelNet数据集上的3D对象分类任务中优于最先进方法的性能得到证明。我们还展示了即使在没有已知姿态的情况下训练，RotationNet也能在对象姿态估计数据集上达到最先进的性能。","领域":"3D对象分类/姿态估计/无监督学习","问题":"联合估计对象的类别和姿态","动机":"解决在实际场景中仅能获得部分视图的情况下，如何准确进行对象分类和姿态估计的问题","方法":"提出了一种基于CNN的模型RotationNet，该模型以多视图图像作为输入，通过无监督学习视角标签，设计为仅使用部分多视图图像进行推理，并采用姿态对齐策略获得跨类别共享的视图特定特征表示","关键词":["3D对象分类","姿态估计","无监督学习","多视图图像","卷积神经网络"],"涉及的技术概念":{"卷积神经网络（CNN）":"一种深度学习模型，特别适用于处理图像数据","多视图图像":"从不同角度拍摄的同一对象的多个图像","无监督学习":"一种机器学习方法，模型在没有标签的数据上学习数据的内在结构和分布","姿态估计":"估计对象在空间中的位置和方向","对象分类":"将对象分配到预定义的类别中"}},{"order":519,"title":"An End-to-End TextSpotter With Explicit Alignment and Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_An_End-to-End_TextSpotter_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_An_End-to-End_TextSpotter_CVPR_2018_paper.html","abstract":"Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Jointly training two tasks is non-trivial due to significant differences in learning difficulties and convergence rates. In this work, we present a conceptually simple yet efficient framework that simultaneously processes the two tasks in a united framework. Our main contributions are three-fold: (1) we propose a novel textalignment layer that allows it to precisely compute convolutional features of a text instance in arbitrary orientation, which is the key to boost the performance; (2) a character attention mechanism is introduced by using character spatial information as explicit supervision, leading to large improvements in recognition; (3) two technologies, together with a new RNN branch for word recognition, are integrated seamlessly into a single model which is end-to-end trainable. This allows the two tasks to work collaboratively by sharing convolutional features, which is critical to identify challenging text instances. Our model obtains impressive results in end-to-end recognition on the ICDAR 2015, significantly advancing the most recent results, with improvements of F-measure from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong, weak and generic lexicon respectively. Thanks to joint training, our method can also serve as a good detector by achieving a new state-of-the-art detection performance on related benchmarks. Code is available at https://github. com/tonghe90/textspotter.","中文标题":"一种具有显式对齐和注意力的端到端文本识别器","摘要翻译":"自然图像中的文本检测和识别长期以来被视为两个独立的任务，需要顺序处理。由于学习难度和收敛速度的显著差异，联合训练这两个任务并非易事。在这项工作中，我们提出了一个概念上简单但高效的框架，该框架在一个统一的框架中同时处理这两个任务。我们的主要贡献有三点：（1）我们提出了一种新颖的文本对齐层，使其能够精确计算任意方向文本实例的卷积特征，这是提升性能的关键；（2）通过使用字符空间信息作为显式监督，引入了字符注意力机制，从而在识别方面取得了显著改进；（3）这两种技术，加上用于单词识别的新RNN分支，被无缝集成到一个单一模型中，该模型可端到端训练。这使得两个任务能够通过共享卷积特征协同工作，这对于识别具有挑战性的文本实例至关重要。我们的模型在ICDAR 2015的端到端识别中取得了令人印象深刻的结果，显著推进了最新成果，通过使用强、弱和通用词典，F-measure从（0.54, 0.51, 0.47）提高到（0.82, 0.77, 0.63）。得益于联合训练，我们的方法也可以作为一个优秀的检测器，在相关基准上实现了新的最先进的检测性能。代码可在https://github.com/tonghe90/textspotter获取。","领域":"文本检测/文本识别/端到端学习","问题":"自然图像中的文本检测和识别任务分离处理，导致联合训练困难","动机":"解决文本检测和识别任务分离处理的问题，通过联合训练提高性能","方法":"提出了一种新颖的文本对齐层和字符注意力机制，结合新的RNN分支，集成到一个可端到端训练的单一模型中","关键词":["文本对齐层","字符注意力机制","RNN分支","端到端训练"],"涉及的技术概念":{"文本对齐层":"用于精确计算任意方向文本实例的卷积特征","字符注意力机制":"通过使用字符空间信息作为显式监督，提高识别性能","RNN分支":"用于单词识别，与文本对齐层和字符注意力机制无缝集成","端到端训练":"允许文本检测和识别任务通过共享卷积特征协同工作"}},{"order":520,"title":"WILDTRACK: A Multi-Camera HD Dataset for Dense Unscripted Pedestrian Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chavdarova_WILDTRACK_A_Multi-Camera_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chavdarova_WILDTRACK_A_Multi-Camera_CVPR_2018_paper.html","abstract":"People detection methods are highly sensitive to occlusions between pedestrians, which are extremely frequent in many situations where cameras have to be mounted at a limited height. The reduction of camera prices allows for the generalization of static multi-camera set-ups. Using joint visual information from multiple synchronized cameras gives the opportunity to improve detection performance.  In this paper, we present a new large-scale and high-resolution dataset. It has been captured with seven static cameras in a public open area, and unscripted dense groups of pedestrians standing and walking. Together with the camera frames, we provide an accurate joint (extrinsic and intrinsic) calibration, as well as 7 series of 400 annotated frames for detection at a rate of 2 frames per second. This results in over 40,000 bounding boxes delimiting every person present in the area of interest, for a total of more than 300 individuals.   We provide a series of benchmark results using baseline algorithms published over the recent months for multi-view detection with deep neural networks, and trajectory estimation using a non-Markovian model.","中文标题":"WILDTRACK: 用于密集无脚本行人检测的多摄像头高清数据集","摘要翻译":"行人检测方法对行人之间的遮挡非常敏感，这种情况在摄像头必须安装在有限高度的许多情况下极为常见。摄像头价格的降低使得静态多摄像头设置的普及成为可能。利用来自多个同步摄像头的联合视觉信息，可以提高检测性能。在本文中，我们提出了一个新的大规模高分辨率数据集。该数据集是在一个公共开放区域使用七个静态摄像头捕获的，包括站立和行走的无脚本密集行人群体。除了摄像头帧外，我们还提供了精确的联合（外参和内参）校准，以及7个系列的400个标注帧用于检测，每秒2帧。这导致了超过40,000个边界框，界定了感兴趣区域内的每一个人，总共超过300个个体。我们提供了一系列基准测试结果，使用了最近几个月发布的用于多视角检测的深度神经网络基线算法，以及使用非马尔可夫模型的轨迹估计。","领域":"行人检测/多视角视觉/轨迹估计","问题":"解决在有限高度安装的摄像头下，行人检测方法对行人之间遮挡敏感的问题","动机":"通过利用多个同步摄像头的联合视觉信息，提高行人检测的性能","方法":"提出了一个新的高分辨率数据集，使用七个静态摄像头捕获无脚本密集行人群体，并提供精确的联合校准和标注帧。使用深度神经网络基线算法进行多视角检测，以及使用非马尔可夫模型进行轨迹估计","关键词":["行人检测","多视角视觉","轨迹估计","深度神经网络","非马尔可夫模型"],"涉及的技术概念":{"多视角视觉":"利用多个同步摄像头的联合视觉信息来提高检测性能","深度神经网络":"用于多视角检测的基线算法","非马尔可夫模型":"用于轨迹估计的模型"}},{"order":521,"title":"Direct Shape Regression Networks for End-to-End Face Alignment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Miao_Direct_Shape_Regression_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Miao_Direct_Shape_Regression_CVPR_2018_paper.html","abstract":"Face alignment has been extensively studied in computer vision community due to its fundamental role in facial analysis, but it remains an unsolved problem. The major challenges lie in the highly nonlinear relationship between face images and associated facial shapes, which is coupled by underlying correlation of landmarks. Existing methods mainly rely on cascaded regression, suffering from intrinsic shortcomings, e.g., strong dependency on initialization and failure to exploit landmark correlations. In this paper, we propose the direct shape regression network (DSRN) for end-to-end face alignment by jointly handling the aforementioned challenges in a unified framework. Specifically, by deploying doubly convolutional layer and by using the Fourier feature pooling layer proposed in this paper, DSRN efficiently constructs strong representations to disentangle highly nonlinear relationships between images and shapes; by incorporating a linear layer of low-rank learning, DSRN effectively encodes correlations of landmarks to improve performance. DSRN leverages the strengths of kernels for nonlinear feature extraction and neural networks for structured prediction, and provides the first end-to-end learning architecture for direct face alignment. Its effectiveness and generality are validated by extensive experiments on five benchmark datasets, including AFLW, 300W, CelebA, MAFL, and 300VW. All empirical results demonstrate that DSRN consistently produces high performance and in most cases surpasses state-of-the-art.","中文标题":"直接形状回归网络用于端到端面部对齐","摘要翻译":"面部对齐因其在面部分析中的基础作用而在计算机视觉社区中被广泛研究，但它仍然是一个未解决的问题。主要挑战在于面部图像与相关面部形状之间的高度非线性关系，这种关系由地标之间的潜在相关性所耦合。现有方法主要依赖于级联回归，存在固有的缺点，例如对初始化的强烈依赖和未能利用地标相关性。在本文中，我们提出了直接形状回归网络（DSRN），通过在一个统一的框架中共同处理上述挑战来实现端到端的面部对齐。具体来说，通过部署双重卷积层并使用本文提出的傅里叶特征池化层，DSRN有效地构建了强大的表示来解开图像和形状之间的高度非线性关系；通过结合低秩学习的线性层，DSRN有效地编码了地标的相关性以提高性能。DSRN利用核在非线性特征提取中的优势和神经网络在结构化预测中的优势，并提供了第一个用于直接面部对齐的端到端学习架构。其有效性和通用性通过在五个基准数据集（包括AFLW、300W、CelebA、MAFL和300VW）上的广泛实验得到验证。所有实证结果表明，DSRN始终产生高性能，并且在大多数情况下超越了最先进的技术。","领域":"面部对齐/深度学习/特征提取","问题":"解决面部图像与面部形状之间的高度非线性关系及地标相关性编码的问题","动机":"现有面部对齐方法存在对初始化强烈依赖和未能利用地标相关性的缺点，需要一种新的方法来解决这些问题","方法":"提出直接形状回归网络（DSRN），通过双重卷积层和傅里叶特征池化层构建强大表示，结合低秩学习的线性层编码地标相关性","关键词":["面部对齐","直接形状回归网络","傅里叶特征池化","低秩学习"],"涉及的技术概念":"双重卷积层用于构建图像和形状之间的非线性关系表示，傅里叶特征池化层用于特征提取，低秩学习的线性层用于编码地标相关性，DSRN结合了核在非线性特征提取和神经网络在结构化预测中的优势，提供了一个端到端的学习架构用于直接面部对齐。"},{"order":522,"title":"Natural and Effective Obfuscation by Head Inpainting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Natural_and_Effective_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Natural_and_Effective_CVPR_2018_paper.html","abstract":"As more and more personal photos are shared online, being able to obfuscate identities in such photos is becoming a necessity for privacy protection. People have largely resorted to blacking out or blurring head regions, but they result in poor user experience while being surprisingly ineffective against state of the art person recognizers[17]. In this work, we propose a novel head inpainting obfuscation technique. Generating a realistic head inpainting in social media photos is challenging because subjects appear in diverse activities and head orientations. We thus split the task into two sub-tasks: (1) facial landmark generation from image context (e.g. body pose) for seamless hypothesis of sensible head pose, and (2) facial landmark conditioned head inpainting. We verify that our inpainting method generates realistic person images, while achieving superior obfuscation performance against automatic person recognizers.","中文标题":"自然且有效的头部修复遮挡技术","摘要翻译":"随着越来越多的个人照片被分享到网上，能够对这些照片中的身份进行遮挡已成为隐私保护的必要手段。人们大多采用涂黑或模糊头部区域的方法，但这些方法不仅用户体验差，而且对于最先进的人物识别器来说效果出奇地差[17]。在这项工作中，我们提出了一种新颖的头部修复遮挡技术。在社交媒体照片中生成逼真的头部修复具有挑战性，因为主体出现在各种活动和头部方向中。因此，我们将任务分为两个子任务：（1）从图像上下文（如身体姿势）生成面部标志，以实现合理头部姿势的无缝假设，以及（2）基于面部标志的头部修复。我们验证了我们的修复方法能够生成逼真的人物图像，同时在对抗自动人物识别器方面实现了卓越的遮挡性能。","领域":"隐私保护/图像修复/人物识别","问题":"如何在社交媒体照片中有效地遮挡个人身份信息，以保护隐私","动机":"现有的身份遮挡方法（如涂黑或模糊）不仅用户体验差，而且对于先进的人物识别器效果不佳","方法":"提出了一种头部修复遮挡技术，通过将任务分为面部标志生成和基于面部标志的头部修复两个子任务，生成逼真的人物图像并实现有效的身份遮挡","关键词":["隐私保护","图像修复","人物识别"],"涉及的技术概念":"面部标志生成：从图像上下文（如身体姿势）生成面部标志，用于假设合理的头部姿势。头部修复：基于生成的面部标志进行头部区域的修复，以生成逼真的人物图像。"},{"order":523,"title":"3D Semantic Trajectory Reconstruction From 3D Pixel Continuum","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yoon_3D_Semantic_Trajectory_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yoon_3D_Semantic_Trajectory_CVPR_2018_paper.html","abstract":"This paper presents a method to reconstruct dense semantic trajectory stream of human interactions in 3D from synchronized multiple videos. The interactions inherently introduce self-occlusion and illumination/appearance/shape changes, resulting in highly fragmented trajectory reconstruction with noisy and coarse semantic labels. Our conjecture is that among many views, there exists a set of views that can confidently recognize the visual semantic label of a 3D trajectory. We introduce a new representation called 3D semantic map---a probability distribution over the semantic labels per trajectory. We construct the 3D semantic map by reasoning about visibility and 2D recognition confidence based on view-pooling, i.e., finding the view that best represents the semantics of the trajectory. Using the 3D semantic map, we precisely infer all trajectory labels jointly by considering the affinity between long range trajectories via estimating their local rigid transformations. This inference quantitatively outperforms the baseline approaches in terms of predictive validity, representation robustness, and affinity effectiveness. We demonstrate that our algorithm can robustly compute the semantic labels of a large scale trajectory set involving real-world human interactions with object, scenes, and people.","中文标题":"从3D像素连续体重建3D语义轨迹","摘要翻译":"本文提出了一种方法，用于从同步的多个视频中重建人类交互的密集语义轨迹流。这些交互本质上引入了自遮挡和光照/外观/形状变化，导致轨迹重建高度碎片化，带有噪声和粗糙的语义标签。我们的猜想是，在众多视角中，存在一组视角可以自信地识别3D轨迹的视觉语义标签。我们引入了一种新的表示方法，称为3D语义图——每个轨迹的语义标签上的概率分布。我们通过基于视图池的可见性和2D识别置信度推理来构建3D语义图，即找到最能代表轨迹语义的视图。使用3D语义图，我们通过估计局部刚性变换来考虑长距离轨迹之间的亲和力，从而精确地联合推断所有轨迹标签。这种推断在预测有效性、表示鲁棒性和亲和力有效性方面定量地优于基线方法。我们证明了我们的算法能够稳健地计算涉及现实世界人类与物体、场景和人的交互的大规模轨迹集的语义标签。","领域":"3D重建/语义理解/轨迹分析","问题":"从同步的多个视频中重建人类交互的密集语义轨迹流，解决自遮挡和光照/外观/形状变化导致的轨迹重建高度碎片化问题。","动机":"为了精确地重建和识别3D轨迹的语义标签，提高轨迹重建的预测有效性、表示鲁棒性和亲和力有效性。","方法":"引入3D语义图表示方法，通过基于视图池的可见性和2D识别置信度推理构建3D语义图，并考虑长距离轨迹之间的亲和力来精确推断所有轨迹标签。","关键词":["3D重建","语义理解","轨迹分析","视图池","局部刚性变换"],"涉及的技术概念":"3D语义图是一种新的表示方法，用于表示每个轨迹的语义标签上的概率分布。视图池是一种技术，用于从多个视角中选择最能代表轨迹语义的视图。局部刚性变换用于估计长距离轨迹之间的亲和力，以精确推断轨迹标签。"},{"order":524,"title":"Optimizing Filter Size in Convolutional Neural Networks for Facial Action Unit Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Optimizing_Filter_Size_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Optimizing_Filter_Size_CVPR_2018_paper.html","abstract":"Recognizing facial action units (AUs) during spontaneous facial displays is a challenging problem. Most recently, Convolutional Neural Networks (CNNs) have shown promise for facial AU recognition, where predefined and fixed convolution filter sizes are employed. In order to achieve the best performance, the optimal filter size is often empirically found by conducting extensive experimental validation. Such a training process suffers from expensive training cost, especially as the network becomes deeper.  This paper proposes a novel Optimized Filter Size CNN (OFS-CNN), where the filter sizes and weights of all convolutional layers are learned simultaneously from the training data along with learning convolution filters. Specifically, the filter size is defined as a continuous variable, which is optimized by minimizing the training loss. Experimental results on two AU-coded spontaneous databases have shown that the proposed OFS-CNN is capable of estimating optimal filter size for varying image resolution and outperforms traditional CNNs with the best filter size obtained by exhaustive search. The OFS-CNN also beats the CNN using multiple filter sizes and more importantly, is much more efficient during testing with the proposed forward-backward propagation algorithm.","中文标题":"优化卷积神经网络中的滤波器尺寸以进行面部动作单元识别","摘要翻译":"在自发表情展示中识别面部动作单元（AUs）是一个具有挑战性的问题。最近，卷积神经网络（CNNs）在面部AU识别方面显示出了潜力，其中采用了预定义和固定的卷积滤波器尺寸。为了达到最佳性能，通常通过进行广泛的实验验证来经验性地找到最佳滤波器尺寸。这样的训练过程遭受了昂贵的训练成本，尤其是随着网络变得更深。本文提出了一种新颖的优化滤波器尺寸CNN（OFS-CNN），其中所有卷积层的滤波器尺寸和权重与学习卷积滤波器同时从训练数据中学习。具体来说，滤波器尺寸被定义为一个连续变量，通过最小化训练损失来优化。在两个AU编码的自发数据库上的实验结果表明，所提出的OFS-CNN能够为不同的图像分辨率估计最佳滤波器尺寸，并且优于通过穷举搜索获得最佳滤波器尺寸的传统CNNs。OFS-CNN还击败了使用多个滤波器尺寸的CNN，更重要的是，在测试时使用提出的前向-后向传播算法要高效得多。","领域":"面部表情识别/卷积神经网络/滤波器优化","问题":"如何优化卷积神经网络中的滤波器尺寸以提高面部动作单元识别的性能","动机":"传统的卷积神经网络在面部动作单元识别中采用固定尺寸的滤波器，需要通过大量实验来寻找最佳滤波器尺寸，这一过程成本高昂，尤其是在深层网络中。","方法":"提出了一种新颖的优化滤波器尺寸CNN（OFS-CNN），其中滤波器尺寸和权重同时从训练数据中学习，滤波器尺寸作为连续变量通过最小化训练损失来优化。","关键词":["面部动作单元识别","卷积神经网络","滤波器优化"],"涉及的技术概念":"卷积神经网络（CNNs）是一种深度学习模型，特别适用于处理图像数据。在面部动作单元识别中，CNNs通过卷积层提取面部特征，其中卷积层的滤波器尺寸对模型性能有重要影响。本文提出的OFS-CNN通过将滤波器尺寸作为连续变量并优化其值，以提高识别性能并减少训练成本。"},{"order":525,"title":"V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.html","abstract":"Most of the existing deep learning-based methods for 3D hand and human pose estimation from a single depth map are based on a common framework that takes a 2D depth map and directly regresses the 3D coordinates of keypoints, such as hand or human body joints, via 2D convolutional neural networks (CNNs). The first weakness of this approach is the presence of perspective distortion in the 2D depth map. While the depth map is intrinsically 3D data, many previous methods treat depth maps as 2D images that can distort the shape of the actual object through projection from 3D to 2D space. This compels the network to perform perspective distortion-invariant estimation. The second weakness of the conventional approach is that directly regressing 3D coordinates from a 2D image is a highly non-linear mapping, which causes difficulty in the learning procedure. To overcome these weaknesses, we firstly cast the 3D hand and human pose estimation problem from a single depth map into a voxel-to-voxel prediction that uses a 3D voxelized grid and estimates the per-voxel likelihood for each keypoint. We design our model as a 3D CNN that provides accurate estimates while running in real-time. Our system outperforms previous methods in almost all publicly available 3D hand and human pose estimation datasets and placed first in the HANDS 2017 frame-based 3D hand pose estimation challenge. The code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE.","中文标题":"V2V-PoseNet：从单一深度图进行精确3D手部和人体姿态估计的体素到体素预测网络","摘要翻译":"大多数现有的基于深度学习的从单一深度图进行3D手部和人体姿态估计的方法都基于一个共同的框架，该框架采用2D深度图并通过2D卷积神经网络（CNNs）直接回归关键点的3D坐标，如手部或人体关节。这种方法的第一弱点是2D深度图中存在透视失真。虽然深度图本质上是3D数据，但许多先前的方法将深度图视为2D图像，这可以通过从3D到2D空间的投影扭曲实际物体的形状。这迫使网络执行透视失真不变的估计。传统方法的第二弱点是从2D图像直接回归3D坐标是一个高度非线性的映射，这导致学习过程中的困难。为了克服这些弱点，我们首先将从单一深度图进行3D手部和人体姿态估计的问题转化为使用3D体素化网格的体素到体素预测，并估计每个关键点的每体素可能性。我们将我们的模型设计为一个3D CNN，它在实时运行时提供准确的估计。我们的系统在几乎所有公开可用的3D手部和人体姿态估计数据集中都优于以前的方法，并在HANDS 2017基于帧的3D手部姿态估计挑战中排名第一。代码可在https://github.com/mks0601/V2V-PoseNet_RELEASE获取。","领域":"3D姿态估计/深度学习/计算机视觉","问题":"从单一深度图进行3D手部和人体姿态估计时存在的透视失真和高度非线性映射问题","动机":"克服现有方法在处理2D深度图时存在的透视失真和直接回归3D坐标的困难","方法":"将3D手部和人体姿态估计问题转化为体素到体素预测，使用3D体素化网格和3D CNN进行估计","关键词":["3D姿态估计","体素化","3D CNN"],"涉及的技术概念":"2D深度图、3D坐标回归、透视失真、体素化网格、3D卷积神经网络（CNN）"},{"order":526,"title":"Ring Loss: Convex Feature Normalization for Face Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zheng_Ring_Loss_Convex_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zheng_Ring_Loss_Convex_CVPR_2018_paper.html","abstract":"We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.","中文标题":"环损失：用于人脸识别的凸特征归一化","摘要翻译":"我们提出并展示了环损失，这是一种简单而优雅的特征归一化方法，旨在增强如Softmax等标准损失函数的深度网络。我们认为，深度特征归一化是监督分类问题中的一个重要方面，在这些问题中，我们要求模型在多类问题中同样好地表示每个类别。通过硬归一化操作直接进行特征归一化的方法会导致非凸公式。相反，环损失应用软归一化，逐渐学习将范数约束到缩放的单位圆上，同时保持凸性，从而产生更稳健的特征。我们将环损失应用于大规模人脸识别问题，并在LFW、IJB-A Janus的挑战性协议、Janus CS3（IJB-A Janus的超集）、Celebrity Frontal-Profile（CFP）和包含100万干扰项的MegaFace上展示了结果。环损失优于强大的基线，在IJB-A Janus上匹配了最先进的性能，并在具有挑战性的Janus CS3上优于所有其他结果，从而实现了最先进的性能。在处理极低分辨率人脸匹配方面，我们也优于强大的基线。","领域":"人脸识别/特征归一化/监督学习","问题":"如何在多类分类问题中实现深度特征的有效归一化，以提升模型的分类性能","动机":"为了在多类分类问题中，使模型能够同样好地表示每个类别，需要一种有效的特征归一化方法","方法":"提出环损失，一种软归一化方法，通过逐渐学习将特征范数约束到缩放的单位圆上，同时保持优化问题的凸性","关键词":["环损失","特征归一化","人脸识别","监督分类","软归一化"],"涉及的技术概念":"环损失是一种特征归一化技术，旨在通过软归一化方法增强深度网络的特征表示能力，特别是在人脸识别任务中。它通过将特征范数约束到缩放的单位圆上，同时保持优化问题的凸性，从而产生更稳健的特征。这种方法被应用于多个大规模人脸识别数据集，包括LFW、IJB-A Janus、Janus CS3、Celebrity Frontal-Profile（CFP）和MegaFace，展示了其在提升模型性能方面的有效性。"},{"order":527,"title":"Adversarially Occluded Samples for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.html","abstract":"Person re-identification (ReID) is the task of retrieving particular persons across different cameras. Despite its great progress in recent years, it is still confronted with challenges like pose variation, occlusion, and similar appearance among different persons. The large gap between training and testing performance with existing models implies the insufficiency of generalization. Considering this fact, we propose to augment the variation of training data by introducing Adversarially Occluded Samples. These special samples are both a) meaningful in that they resemble real-scene occlusions, and b) effective in that they are tough for the original model and thus provide the momentum to jump out of local optimum. We mine these samples based on a trained ReID model and with the help of network visualization techniques. Extensive experiments show that the proposed samples help the model discover new discriminative clues on the body and generalize much better at test time. Our strategy makes significant improvement over strong baselines on three large-scale ReID datasets, Market1501, CUHK03 and DukeMTMC-reID.","中文标题":"对抗性遮挡样本用于行人重识别","摘要翻译":"行人重识别（ReID）是在不同摄像头间检索特定行人的任务。尽管近年来取得了巨大进展，但仍面临姿态变化、遮挡和不同行人之间外观相似等挑战。现有模型在训练和测试性能之间存在较大差距，暗示了泛化能力的不足。考虑到这一事实，我们提出通过引入对抗性遮挡样本来增加训练数据的变化。这些特殊样本既a)有意义，因为它们类似于真实场景中的遮挡，又b)有效，因为它们对原始模型来说很困难，从而提供了跳出局部最优的动力。我们基于训练好的ReID模型并借助网络可视化技术挖掘这些样本。大量实验表明，所提出的样本帮助模型发现了身体上的新判别线索，并在测试时表现出更好的泛化能力。我们的策略在三个大规模ReID数据集Market1501、CUHK03和DukeMTMC-reID上对强基线做出了显著改进。","领域":"行人重识别/对抗性学习/数据增强","问题":"解决行人重识别中由于姿态变化、遮挡和外观相似导致的泛化能力不足问题","动机":"提高行人重识别模型在真实场景中的泛化能力","方法":"引入对抗性遮挡样本增加训练数据的变化，基于训练好的ReID模型和网络可视化技术挖掘这些样本","关键词":["行人重识别","对抗性学习","数据增强","网络可视化"],"涉及的技术概念":"对抗性遮挡样本是通过对抗性学习生成的，旨在模拟真实场景中的遮挡情况，以提高模型的泛化能力。网络可视化技术用于帮助挖掘这些样本，使得模型能够发现新的判别线索。"},{"order":528,"title":"Classifier Learning With Prior Probabilities for Facial Action Unit Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Classifier_Learning_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Classifier_Learning_With_CVPR_2018_paper.html","abstract":"Facial action units (AUs) play an important role in human emotion understanding. One big challenge for data-driven AU recognition approaches is the lack of enough AU annotations, since AU annotation requires strong domain expertise. To alleviate this issue, we propose a knowledge-driven method for jointly learning multiple AU classifiers without any AU annotation by leveraging prior probabilities on AUs, including expression-independent and expression-dependent AU probabilities. These prior probabilities are drawn from facial anatomy and emotion studies, and are independent of datasets. We incorporate the prior probabilities on AUs as the constraints into the objective function of multiple AU classifiers, and develop an efficient learning algorithm to solve the formulated problem. Experimental results on five benchmark expression databases demonstrate the effectiveness of the proposed method, especially its generalization ability, and the power of the prior probabilities.","中文标题":"基于先验概率的面部动作单元识别分类器学习","摘要翻译":"面部动作单元（AUs）在人类情感理解中扮演着重要角色。数据驱动的AU识别方法面临的一大挑战是缺乏足够的AU注释，因为AU注释需要强大的领域专业知识。为了缓解这一问题，我们提出了一种知识驱动的方法，通过利用AU的先验概率（包括表情无关和表情相关的AU概率），无需任何AU注释即可联合学习多个AU分类器。这些先验概率来源于面部解剖学和情感研究，并且与数据集无关。我们将AU的先验概率作为约束条件纳入多个AU分类器的目标函数中，并开发了一种有效的学习算法来解决所提出的问题。在五个基准表情数据库上的实验结果证明了所提出方法的有效性，特别是其泛化能力，以及先验概率的力量。","领域":"情感计算/面部表情分析/先验知识利用","问题":"缺乏足够的面部动作单元注释","动机":"提高面部动作单元识别的准确性和泛化能力","方法":"利用面部动作单元的先验概率作为约束条件，联合学习多个AU分类器","关键词":["面部动作单元","先验概率","情感理解","知识驱动方法","泛化能力"],"涉及的技术概念":{"面部动作单元（AUs）":"面部表情的基本组成部分，用于描述面部肌肉的运动。","先验概率":"在没有任何数据注释的情况下，基于面部解剖学和情感研究得出的概率，用于指导分类器的学习。","知识驱动方法":"利用领域知识（如面部解剖学和情感研究）来指导机器学习模型的训练，而不是完全依赖数据驱动的方法。"}},{"order":529,"title":"4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_4DFAB_A_Large_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_4DFAB_A_Large_CVPR_2018_paper.html","abstract":"The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contain recordings of 180 subjects captured in four different sessions spanned over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database in various applications. The database will be made publicly available for research purposes.","中文标题":"4DFAB: 用于面部表情分析和生物识别应用的大规模4D数据库","摘要翻译":"当前在许多计算机视觉应用（包括自动面部分析）中见证的进展，如果没有在收集和注释大规模视觉数据库方面的巨大努力，是不可能实现的。为此，我们提出了4DFAB，一个新的动态高分辨率3D面部大规模数据库（超过1,800,000个3D网格）。4DFAB包含在五年期间四个不同会话中捕获的180名受试者的记录。它包含显示自发和摆拍面部行为的受试者的4D视频。该数据库可用于面部和面部表情识别，以及行为生物识别。它还可以用于学习非常强大的混合形状，以参数化面部行为。在本文中，我们进行了几项实验，并展示了该数据库在各种应用中的有用性。该数据库将公开供研究使用。","领域":"面部表情分析/生物识别/3D面部建模","问题":"缺乏大规模、高质量的动态3D面部数据库，限制了面部表情分析和生物识别技术的发展","动机":"为了推动自动面部分析等计算机视觉应用的发展，需要大规模、高质量的动态3D面部数据库","方法":"提出了4DFAB数据库，包含超过1,800,000个3D网格，记录了180名受试者在五年期间四个不同会话中的面部行为，包括自发和摆拍的面部表情","关键词":["面部表情分析","生物识别","3D面部建模","动态3D面部数据库"],"涉及的技术概念":"4D视频指的是包含时间维度的3D视频，能够捕捉面部表情的动态变化；混合形状（blendshapes）是一种用于参数化面部行为的技术，通过调整一系列基础形状的权重来生成不同的面部表情。"},{"order":530,"title":"Seeing Small Faces From Robust Anchor's Perspective","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Seeing_Small_Faces_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Seeing_Small_Faces_CVPR_2018_paper.html","abstract":"This paper introduces a novel anchor design principle to support anchor-based face detection for superior scale-invariant performance, especially on tiny faces. To achieve this, we explicitly address the problem that anchor-based detectors drop performance drastically on faces with tiny sizes, e.g. less than 16x16 pixels. In this paper, we investigate why this is the case. We discover that current anchor design cannot guarantee high overlaps between tiny faces and anchor boxes, which increases the difficulty of training. The new Expected Max Overlapping (EMO) score is proposed which can theoretically explain the low overlapping issue and inspire several effective strategies of new anchor design leading to higher face overlaps, including anchor stride reduction with new network architectures, extra shifted anchors, and stochastic face shifting. Comprehensive experiments show that our proposed method significantly outperforms the baseline anchor-based detector, while consistently achieving state-of-the-art results on challenging face detection datasets with competitive runtime speed.","中文标题":"从稳健锚点的角度看小脸检测","摘要翻译":"本文介绍了一种新颖的锚点设计原则，以支持基于锚点的人脸检测，实现卓越的尺度不变性能，特别是在小脸检测上。为此，我们明确解决了基于锚点的检测器在极小尺寸（例如小于16x16像素）的人脸上性能急剧下降的问题。在本文中，我们探讨了为什么会这样。我们发现，当前的锚点设计不能保证小脸与锚点框之间的高重叠率，这增加了训练的难度。提出了新的期望最大重叠（EMO）评分，它可以从理论上解释低重叠问题，并启发了几种有效的新锚点设计策略，包括通过新网络架构减少锚点步长、额外的偏移锚点和随机人脸偏移。综合实验表明，我们提出的方法显著优于基线锚点检测器，同时在具有挑战性的人脸检测数据集上始终达到最先进的结果，并具有竞争力的运行速度。","领域":"人脸检测/小目标检测/锚点设计","问题":"解决基于锚点的检测器在极小尺寸人脸上性能急剧下降的问题","动机":"当前锚点设计不能保证小脸与锚点框之间的高重叠率，增加了训练难度","方法":"提出期望最大重叠（EMO）评分，通过新网络架构减少锚点步长、额外的偏移锚点和随机人脸偏移等策略提高人脸重叠率","关键词":["人脸检测","小目标检测","锚点设计","期望最大重叠"],"涉及的技术概念":"锚点设计原则、期望最大重叠（EMO）评分、锚点步长、网络架构、偏移锚点、随机人脸偏移"},{"order":531,"title":"2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html","abstract":"Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.","中文标题":"使用多任务深度学习进行2D/3D姿态估计和动作识别","摘要翻译":"动作识别和人体姿态估计密切相关，但在文献中这两个问题通常被视为独立任务。在本研究中，我们提出了一个多任务框架，用于从静态图像中联合进行2D和3D姿态估计以及从视频序列中进行人体动作识别。我们展示了一个单一架构可以有效地解决这两个问题，并且仍然能够达到最先进的结果。此外，我们证明了端到端的优化比分离学习能显著提高准确性。所提出的架构可以无缝地同时使用来自不同类别的数据进行训练。在四个数据集（MPII、Human3.6M、Penn Action和NTU）上报告的结果证明了我们的方法在目标任务上的有效性。","领域":"姿态估计/动作识别/多任务学习","问题":"如何有效地联合解决2D/3D姿态估计和动作识别问题","动机":"动作识别和人体姿态估计密切相关，但通常被视为独立任务，本研究旨在探索一个多任务框架以联合解决这两个问题","方法":"提出一个多任务框架，使用单一架构同时进行2D/3D姿态估计和动作识别，并采用端到端优化方法","关键词":["姿态估计","动作识别","多任务学习","端到端优化"],"涉及的技术概念":"多任务学习是一种机器学习方法，旨在同时解决多个相关任务，通过共享表示来提高学习效率和性能。端到端优化指的是从输入到输出的整个流程中，所有参数都通过梯度下降等优化方法进行统一调整，以提高模型的整体性能。"},{"order":532,"title":"Dense 3D Regression for Hand Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Dense_3D_Regression_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Dense_3D_Regression_CVPR_2018_paper.html","abstract":"We present a simple and effective method for 3D hand pose estimation from a single depth frame. As opposed to previous state-of-arts based on holistic 3D regression, our method works on dense pixel-wise estimation. This is achieved by careful design choices in pose parameterization, which leverages both 2D and 3D properties of depth map. Specifically, we decompose the pose parameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat maps and unit 3D direction vector fields. The 2D/3D joint heat maps and 3D joint offsets are estimated via multi-task network cascades, which is trained end-to-end. The pixel-wise estimations can be directly translated into a vote casting scheme. A variant of mean shift is then used to aggregate local votes and explicitly handles the global 3D estimation in consensus with pixel-wise 2D and 3D estimations. Our method is efficient and highly accurate. On MSRA and NYU hand dataset, our method outperforms all previous state-of-arts by a large margin. On ICVL hand dataset, our method achieves similar accuracy compared to the state-of-art which is nearly saturated and outperforms other state-of-arts. Code will be made available.","中文标题":"密集3D回归用于手部姿态估计","摘要翻译":"我们提出了一种简单有效的方法，用于从单一深度帧进行3D手部姿态估计。与之前基于整体3D回归的最新技术不同，我们的方法在密集像素级估计上工作。这是通过在姿态参数化中的精心设计选择实现的，该选择利用了深度图的2D和3D属性。具体来说，我们将姿态参数分解为一组每像素估计，即2D热图、3D热图和单位3D方向向量场。2D/3D关节热图和3D关节偏移通过多任务网络级联估计，该网络是端到端训练的。像素级估计可以直接转换为投票方案。然后使用均值漂移的变体来聚合局部投票，并明确处理与像素级2D和3D估计一致的全局3D估计。我们的方法既高效又高度准确。在MSRA和NYU手部数据集上，我们的方法大幅超越了所有之前的最新技术。在ICVL手部数据集上，我们的方法达到了与几乎饱和的最新技术相似的准确性，并超越了其他最先进的技术。代码将公开提供。","领域":"3D姿态估计/手部姿态估计/深度图处理","问题":"从单一深度帧进行3D手部姿态估计","动机":"提高3D手部姿态估计的准确性和效率","方法":"通过精心设计的姿态参数化，将姿态参数分解为每像素估计，包括2D热图、3D热图和单位3D方向向量场，使用多任务网络级联进行端到端训练，并通过均值漂移的变体聚合局部投票以处理全局3D估计","关键词":["3D姿态估计","手部姿态估计","深度图处理","多任务网络级联","均值漂移"],"涉及的技术概念":"2D热图、3D热图、单位3D方向向量场、多任务网络级联、均值漂移"},{"order":533,"title":"Camera Style Adaptation for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.html","abstract":"Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art.","中文标题":"人物重识别中的相机风格适应","摘要翻译":"作为一个跨相机检索任务，人物重识别遭受由不同相机引起的图像风格变化的困扰。现有技术通过隐式学习相机不变描述子子空间来解决这个问题。在本文中，我们通过引入相机风格（CamStyle）适应来明确考虑这一挑战。CamStyle可以作为一种数据增强方法，平滑相机风格差异。具体来说，使用CycleGAN，标记的训练图像可以被风格转移到每个相机，并与原始训练样本一起形成增强的训练集。这种方法在增加数据多样性以防止过拟合的同时，也引入了相当程度的噪声。为了减轻噪声的影响，采用了标签平滑正则化（LSR）。我们的方法的原始版本（没有LSR）在少数相机系统中表现相当好，这些系统经常发生过拟合。使用LSR，我们在所有系统中都展示了持续的改进，无论过拟合的程度如何。我们还报告了与现有技术相比具有竞争力的准确性。","领域":"人物重识别/数据增强/风格迁移","问题":"解决人物重识别中由于不同相机引起的图像风格变化问题","动机":"通过显式考虑相机风格差异，提高人物重识别的准确性和鲁棒性","方法":"引入相机风格（CamStyle）适应，使用CycleGAN进行风格转移，结合标签平滑正则化（LSR）减轻噪声影响","关键词":["人物重识别","数据增强","风格迁移","标签平滑正则化"],"涉及的技术概念":"CycleGAN用于图像风格转移，标签平滑正则化（LSR）用于减轻数据增强过程中引入的噪声影响。"},{"order":534,"title":"PoseTrack: A Benchmark for Human Pose Estimation and Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Andriluka_PoseTrack_A_Benchmark_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Andriluka_PoseTrack_A_Benchmark_CVPR_2018_paper.html","abstract":"Existing systems for video-based pose estimation and tracking struggle to perform well on realistic videos with multiple people and often fail to output body-pose trajectories consistent over time. To address this shortcoming this paper introduces PoseTrack which is a new large-scale benchmark for video-based human pose estimation and articulated tracking. Our new benchmark encompasses three tasks focusing on i) single-frame multi-person pose estimation, ii) multi-person pose estimation in videos, and iii) multi-person articulated tracking. To establish the benchmark, we collect, annotate and release a new dataset that features videos with multiple people labeled with person tracks and articulated pose. A public centralized evaluation server is provided to allow the research community to evaluate on a held-out test set. Furthermore, we conduct an extensive experimental study on recent approaches to articulated pose tracking and provide analysis of the strengths and weaknesses of the state of the art. We envision that the proposed benchmark will stimulate productive research both by providing a large and representative training dataset as well as providing a platform to objectively evaluate and compare the proposed methods. The benchmark is freely accessible at https://posetrack.net/.","中文标题":"PoseTrack: 人体姿态估计与跟踪的基准","摘要翻译":"现有的基于视频的姿态估计和跟踪系统在处理包含多个人的真实视频时表现不佳，常常无法输出随时间一致的身体姿态轨迹。为了解决这一不足，本文介绍了PoseTrack，这是一个新的基于视频的人体姿态估计和关节跟踪的大规模基准。我们的新基准包括三个任务，专注于i)单帧多人姿态估计，ii)视频中的多人姿态估计，以及iii)多人关节跟踪。为了建立这个基准，我们收集、注释并发布了一个新的数据集，该数据集包含带有人员轨迹和关节姿态标签的多人视频。提供了一个公共的集中评估服务器，以允许研究社区在保留的测试集上进行评估。此外，我们对最近的关节姿态跟踪方法进行了广泛的实验研究，并提供了对现有技术优缺点的分析。我们预计，所提出的基准将通过提供大型且具有代表性的训练数据集以及提供一个客观评估和比较所提出方法的平台，来刺激富有成效的研究。该基准可在https://posetrack.net/免费访问。","领域":"人体姿态估计/视频分析/关节跟踪","问题":"现有系统在处理包含多个人的真实视频时，姿态估计和跟踪表现不佳，无法输出随时间一致的身体姿态轨迹。","动机":"为了解决现有系统在处理多人视频时的不足，提供一个大规模基准来促进人体姿态估计和关节跟踪的研究。","方法":"引入PoseTrack基准，包括三个任务：单帧多人姿态估计、视频中的多人姿态估计和多人关节跟踪。收集、注释并发布一个新的数据集，提供一个公共的集中评估服务器，并对现有方法进行广泛的实验研究。","关键词":["人体姿态估计","关节跟踪","视频分析"],"涉及的技术概念":"PoseTrack基准包括单帧多人姿态估计、视频中的多人姿态估计和多人关节跟踪三个任务。通过收集和注释一个新的数据集，并提供一个公共的集中评估服务器，来促进这一领域的研究。"},{"order":535,"title":"Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Exploit_the_Unknown_CVPR_2018_paper.html","abstract":"We focus on the one-shot learning for video-based person re-Identification (re-ID). Unlabeled tracklets for the person re-ID tasks can be easily obtained by pre-processing, such as pedestrian detection and tracking. In this paper, we propose an approach to exploiting unlabeled tracklets by gradually but steadily improving the discriminative capability of the Convolutional Neural Network (CNN) feature representation via stepwise learning. We first initialize a CNN model using one labeled tracklet for each identity. Then we update the CNN model by the following two steps iteratively: 1. sample a few candidates with most reliable pseudo labels from unlabeled tracklets; 2. update the CNN model according to the selected data. Instead of the static sampling strategy applied in existing works, we propose a progressive sampling method to increase the number of the selected pseudo-labeled candidates step by step. We systematically investigate the way how we should select pseudo-labeled tracklets into the training set to make the best use of them. Notably, the rank-1 accuracy of our method outperforms the state-of-the-art method by 21.46 points (absolute, i.e., 62.67% vs. 41.21%) on the MARS dataset, and 16.53 points on the DukeMTMC-VideoReID dataset.","中文标题":"逐步探索未知：通过逐步学习实现基于视频的一次性行人重识别","摘要翻译":"我们专注于基于视频的行人重识别（re-ID）的一次性学习。通过预处理，如行人检测和跟踪，可以轻松获得用于行人重识别任务的未标记轨迹。在本文中，我们提出了一种方法，通过逐步但稳定地提高卷积神经网络（CNN）特征表示的区分能力，利用未标记的轨迹进行逐步学习。我们首先使用每个身份的一个标记轨迹初始化CNN模型。然后，我们通过以下两个步骤迭代更新CNN模型：1. 从未标记的轨迹中采样一些具有最可靠伪标签的候选者；2. 根据选定的数据更新CNN模型。与现有工作中应用的静态采样策略不同，我们提出了一种渐进式采样方法，以逐步增加选定的伪标签候选者的数量。我们系统地研究了如何选择伪标签轨迹进入训练集以充分利用它们。值得注意的是，我们的方法在MARS数据集上的rank-1准确率比最先进的方法高出21.46个百分点（绝对值，即62.67%对41.21%），在DukeMTMC-VideoReID数据集上高出16.53个百分点。","领域":"行人重识别/视频分析/深度学习","问题":"如何有效利用未标记的视频轨迹进行一次性行人重识别","动机":"提高基于视频的行人重识别任务中未标记轨迹的利用效率，以增强模型的区分能力","方法":"提出了一种渐进式采样方法，通过逐步增加选定的伪标签候选者数量来更新CNN模型","关键词":["行人重识别","视频分析","一次性学习","渐进式采样","卷积神经网络"],"涉及的技术概念":"本文涉及的技术概念包括一次性学习、行人重识别、卷积神经网络（CNN）、伪标签、渐进式采样方法。一次性学习指的是在只有少量标记数据的情况下进行学习；行人重识别是指在不同的摄像头视角下识别同一行人；卷积神经网络是一种深度学习模型，用于提取图像特征；伪标签是指对未标记数据自动生成的标签；渐进式采样方法是一种逐步增加训练数据量的策略。"},{"order":536,"title":"Pose-Robust Face Recognition via Deep Residual Equivariant Mapping","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Pose-Robust_Face_Recognition_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Pose-Robust_Face_Recognition_CVPR_2018_paper.html","abstract":"Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead.","中文标题":"通过深度残差等变映射实现姿态鲁棒的人脸识别","摘要翻译":"得益于深度学习的出现，人脸识别取得了非凡的成功。然而，与正面脸相比，许多当代人脸识别模型在处理侧面脸时表现相对较差。一个关键原因是正面和侧面训练脸的数量高度不平衡——正面训练样本远多于侧面样本。此外，本质上很难学习到对大幅姿态变化几何不变性的深度表示。在本研究中，我们假设正面脸和侧面脸之间存在一种固有映射，因此，它们之间的差异可以通过等变映射在深度表示空间中得到桥接。为了利用这种映射，我们提出了一种新颖的深度残差等变映射（DREAM）块，它能够自适应地向输入的深度表示添加残差，以将侧面脸的表示转换为简化识别的规范姿态。DREAM块持续增强了包括ResNet模型在内的许多强大深度网络的侧面脸识别性能，而无需刻意增加侧面脸的训练数据。该块易于使用，轻量级，并且可以以可忽略的计算开销实现。","领域":"人脸识别/深度学习/姿态估计","问题":"处理侧面脸识别性能较差的问题","动机":"正面和侧面训练脸数量不平衡，以及学习对大幅姿态变化几何不变性的深度表示的困难","方法":"提出深度残差等变映射（DREAM）块，通过自适应添加残差将侧面脸的表示转换为规范姿态","关键词":["人脸识别","姿态估计","深度学习"],"涉及的技术概念":"深度残差等变映射（DREAM）块是一种能够自适应地向输入的深度表示添加残差的技术，用于将侧面脸的表示转换为简化识别的规范姿态。这种方法旨在解决正面和侧面训练脸数量不平衡的问题，以及学习对大幅姿态变化几何不变性的深度表示的困难。"},{"order":537,"title":"DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_DecideNet_Counting_Varying_CVPR_2018_paper.html","abstract":"In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.","中文标题":"DecideNet：通过注意力引导的检测和密度估计来计数不同密度的人群","摘要翻译":"在现实世界的人群计数应用中，人群密度在空间和时间领域变化很大。基于检测的计数方法将在低密度场景中准确估计人群，而在拥挤区域的可靠性则下降。另一方面，基于回归的方法捕捉了拥挤区域的一般密度信息。由于不知道每个人的位置，它往往会在低密度区域高估人数。因此，仅使用其中任何一种方法都不足以处理各种密度变化的场景。为了解决这个问题，提出了一种名为DecideNet（检测和密度估计网络）的新型端到端人群计数框架。它可以根据图像上不同位置的实际密度条件自适应地决定适当的计数模式。DecideNet首先通过分别生成基于检测和回归的密度图来估计人群密度。为了捕捉密度的不可避免的变化，它引入了一个注意力模块，旨在自适应地评估两种类型估计的可靠性。最终的人群计数是在注意力模块的指导下，从两种密度图中采用合适的估计得到的。实验结果表明，我们的方法在三个具有挑战性的人群计数数据集上实现了最先进的性能。","领域":"人群计数/密度估计/注意力机制","问题":"处理不同密度场景下的人群计数问题","动机":"现有的人群计数方法在处理不同密度场景时存在局限性，需要一种能够自适应选择合适计数模式的方法","方法":"提出了一种名为DecideNet的端到端人群计数框架，通过生成基于检测和回归的密度图，并引入注意力模块自适应评估两种估计的可靠性，最终在注意力模块的指导下采用合适的估计得到人群计数","关键词":["人群计数","密度估计","注意力机制"],"涉及的技术概念":"DecideNet框架结合了基于检测和回归的密度估计方法，通过引入注意力模块来评估和选择最合适的密度估计，从而实现对不同密度场景下人群的准确计数。"},{"order":538,"title":"LSTM Pose Machines","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_LSTM_Pose_Machines_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_LSTM_Pose_Machines_CVPR_2018_paper.html","abstract":"We observed that recent state-of-the-art results on single image human pose estimation were achieved by multi-stage Convolution Neural Networks (CNN). Notwithstanding the superior performance on static images, the application of these models on videos is not only computationally intensive, it also suffers from performance degeneration and flicking. Such suboptimal results are mainly attributed to the inability of imposing sequential geometric consistency, handling severe image quality degradation (e.g. motion blur and occlusion) as well as the inability of capturing the temporal correlation among video frames. In this paper, we proposed a novel recurrent network to tackle these problems. We showed that if we were to impose the weight sharing scheme to the multi-stage CNN, it could be re-written as a Recurrent Neural Network (RNN). This property decouples the relationship among multiple network stages and results in significantly faster speed in invoking the network for videos. It also enables the adoption of Long Short-Term Memory (LSTM) units between video frames. We found such memory augmented RNN is very effective in imposing geometric consistency among frames. It also well handles input quality degradation in videos while successfully stabilizes the sequential outputs. The experiments showed that our approach significantly outperformed current state-of-the-art methods on two large-scale video pose estimation benchmarks. We also explored the memory cells inside the LSTM and provided insights on why such mechanism would benefit the prediction for video-based pose estimations.","中文标题":"LSTM姿态机","摘要翻译":"我们观察到，最近在单张图像人体姿态估计方面取得的最先进成果是由多阶段卷积神经网络（CNN）实现的。尽管在静态图像上表现出色，但这些模型在视频上的应用不仅计算密集，而且还存在性能退化和闪烁的问题。这种次优结果主要归因于无法施加序列几何一致性、处理严重的图像质量退化（如运动模糊和遮挡）以及无法捕捉视频帧之间的时间相关性。在本文中，我们提出了一种新颖的循环网络来解决这些问题。我们展示了如果我们将权重共享方案应用于多阶段CNN，它可以被重写为循环神经网络（RNN）。这一特性解耦了多个网络阶段之间的关系，并显著加快了在视频中调用网络的速度。它还允许在视频帧之间采用长短期记忆（LSTM）单元。我们发现这种记忆增强的RNN在施加帧间几何一致性方面非常有效。它还能很好地处理视频中的输入质量退化，同时成功地稳定了序列输出。实验表明，我们的方法在两个大规模视频姿态估计基准上显著优于当前的最先进方法。我们还探索了LSTM内部的记忆单元，并提供了关于这种机制为何有利于基于视频的姿态估计预测的见解。","领域":"视频姿态估计/循环神经网络/长短期记忆网络","问题":"视频中人体姿态估计的性能退化和闪烁问题","动机":"解决多阶段CNN在视频应用中的计算密集、性能退化和闪烁问题，以及无法施加序列几何一致性和处理图像质量退化的问题","方法":"提出了一种新颖的循环网络，通过权重共享方案将多阶段CNN重写为RNN，并在视频帧之间采用LSTM单元，以施加几何一致性、处理输入质量退化并稳定序列输出","关键词":["视频姿态估计","循环神经网络","长短期记忆网络"],"涉及的技术概念":"多阶段卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆（LSTM）单元、权重共享方案、序列几何一致性、图像质量退化（如运动模糊和遮挡）、视频帧之间的时间相关性"},{"order":539,"title":"Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Disentangling_Features_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Disentangling_Features_in_CVPR_2018_paper.html","abstract":"This paper proposes an encoder-decoder network to disentangle shape features during 3D face shape reconstruction from single 2D images, such that the tasks of learning discriminative shape features for face recognition and reconstructing accurate 3D face shapes can be done simultaneously. Unlike existing 3D face reconstruction methods, our proposed method directly regresses dense 3D face shapes from single 2D images, and tackles identity and residual (i.e., non-identity) components in 3D face shapes explicitly and separately based on a composite 3D face shape model with latent representations. We devise a training process for the proposed network with a joint loss measuring both face identification error and 3D face shape reconstruction error. We develop a multi image 3D morphable model (3DMM) fitting method for multiple 2D images of a subject to construct training data. Comprehensive experiments have been done on MICC, BU3DFE, LFW and YTF databases. The results show that our method expands the capacity of 3DMM for capturing discriminative shape features and facial detail, and thus outperforms existing methods both in 3D face reconstruction accuracy and in face recognition accuracy.","中文标题":"解缠3D面部形状特征以实现联合面部重建与识别","摘要翻译":"本文提出了一种编码器-解码器网络，用于从单张2D图像重建3D面部形状时解缠形状特征，使得学习用于面部识别的判别性形状特征和重建准确的3D面部形状的任务可以同时进行。与现有的3D面部重建方法不同，我们提出的方法直接从单张2D图像回归密集的3D面部形状，并基于具有潜在表示的复合3D面部形状模型，明确且分别处理3D面部形状中的身份和残差（即非身份）组件。我们为所提出的网络设计了一个训练过程，该过程使用联合损失来衡量面部识别误差和3D面部形状重建误差。我们开发了一种多图像3D可变形模型（3DMM）拟合方法，用于构建训练数据。在MICC、BU3DFE、LFW和YTF数据库上进行了全面的实验。结果表明，我们的方法扩展了3DMM捕捉判别性形状特征和面部细节的能力，因此在3D面部重建准确性和面部识别准确性方面均优于现有方法。","领域":"3D面部重建/面部识别/3D可变形模型","问题":"如何同时实现从单张2D图像准确重建3D面部形状和学习用于面部识别的判别性形状特征","动机":"现有的3D面部重建方法无法同时高效地进行面部识别和3D面部形状重建，需要一种新的方法来解决这一问题","方法":"提出了一种编码器-解码器网络，通过解缠形状特征，直接从单张2D图像回归密集的3D面部形状，并设计了一个训练过程，使用联合损失来衡量面部识别误差和3D面部形状重建误差","关键词":["3D面部重建","面部识别","3D可变形模型"],"涉及的技术概念":"编码器-解码器网络用于解缠形状特征，直接从单张2D图像回归密集的3D面部形状；复合3D面部形状模型用于处理身份和残差组件；联合损失用于衡量面部识别误差和3D面部形状重建误差；多图像3D可变形模型（3DMM）拟合方法用于构建训练数据"},{"order":540,"title":"Convolutional Sequence to Sequence Model for Human Dynamics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Convolutional_Sequence_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Convolutional_Sequence_to_CVPR_2018_paper.html","abstract":"Human motion modeling is a classic problem in com- puter vision and graphics. Challenges in modeling human motion include high dimensional prediction as well as extremely complicated dynamics.We present a novel approach to human motion modeling based on convolutional neural networks (CNN). The hierarchical structure of CNN makes it capable of capturing both spatial and temporal correlations effectively. In our proposed approach, a convolutional long-term encoder is used to encode the whole given motion sequence into a long-term hidden variable, which is used with a decoder to predict the remainder of the sequence. The decoder itself also has an encoder-decoder structure, in which the short-term encoder encodes a shorter sequence to a short-term hidden variable, and the spatial decoder maps the long and short-term hidden variable to motion predictions. By using such a model, we are able to capture both invariant and dynamic information of human motion, which results in more accurate predictions. Experiments show that our algorithm outperforms the state-of-the-art methods on the Human3.6M and CMU Motion Capture datasets. Our code is available at the project website","中文标题":"卷积序列到序列模型用于人体动力学","摘要翻译":"人体运动建模是计算机视觉和图形学中的一个经典问题。建模人体运动的挑战包括高维预测以及极其复杂的动力学。我们提出了一种基于卷积神经网络（CNN）的人体运动建模新方法。CNN的层次结构使其能够有效地捕捉空间和时间相关性。在我们提出的方法中，使用卷积长期编码器将整个给定的运动序列编码为长期隐藏变量，该变量与解码器一起用于预测序列的剩余部分。解码器本身也具有编码器-解码器结构，其中短期编码器将较短的序列编码为短期隐藏变量，空间解码器将长期和短期隐藏变量映射到运动预测。通过使用这样的模型，我们能够捕捉人体运动的不变信息和动态信息，从而实现更准确的预测。实验表明，我们的算法在Human3.6M和CMU Motion Capture数据集上优于最先进的方法。我们的代码可在项目网站上获得。","领域":"人体运动建模/卷积神经网络/运动预测","问题":"高维预测和复杂动力学的人体运动建模","动机":"解决人体运动建模中的高维预测和复杂动力学问题，提高预测准确性","方法":"使用卷积神经网络（CNN）的层次结构捕捉空间和时间相关性，通过卷积长期编码器和解码器结构预测运动序列","关键词":["人体运动建模","卷积神经网络","运动预测"],"涉及的技术概念":"卷积神经网络（CNN）用于捕捉空间和时间相关性，卷积长期编码器用于编码整个运动序列，解码器结构用于预测运动序列的剩余部分"},{"order":541,"title":"Gesture Recognition: Focus on the Hands","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Narayana_Gesture_Recognition_Focus_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Narayana_Gesture_Recognition_Focus_CVPR_2018_paper.html","abstract":"Gestures are a common form of human communication and important for human computer interfaces (HCI). Recent approaches to gesture recognition use deep learning methods, including multi-channel methods. We show that when spatial channels are focused on the hands, gesture recognition improves significantly, particularly when the channels are fused using a sparse network. Using this technique, we improve performance on the ChaLearn IsoGD dataset from a previous best of 67.71% to 82.07%, and on the NVIDIA dataset from 83.8% to 91.28%.","中文标题":"手势识别：聚焦于手部","摘要翻译":"手势是人类交流的一种常见形式，对于人机界面（HCI）非常重要。最近的手势识别方法使用深度学习方法，包括多通道方法。我们展示了当空间通道聚焦于手部时，手势识别显著提高，特别是当通道使用稀疏网络融合时。使用这种技术，我们在ChaLearn IsoGD数据集上的表现从之前的67.71%提高到82.07%，在NVIDIA数据集上从83.8%提高到91.28%。","领域":"手势识别/人机交互/深度学习","问题":"提高手势识别的准确率","动机":"手势作为人类交流的一种形式，对于改善人机界面至关重要","方法":"使用深度学习方法，特别是聚焦于手部的多通道方法，并通过稀疏网络融合这些通道","关键词":["手势识别","人机交互","深度学习","多通道方法","稀疏网络"],"涉及的技术概念":"多通道方法指的是使用多个数据通道（如颜色、深度等）来捕捉手势信息。稀疏网络是一种网络结构，它通过减少网络中的连接数量来提高效率和性能。"},{"order":542,"title":"Crowd Counting via Adversarial Cross-Scale Consistency Pursuit","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Crowd_Counting_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Crowd_Counting_via_CVPR_2018_paper.html","abstract":"Crowd counting or density estimation is a challenging task in computer vision due to large scale variations, perspective distortions and serious occlusions, etc. Existing methods generally suffers from two issues: 1) the model averaging effects in multi-scale CNNs induced by the widely adopted L2 regression loss; and 2) inconsistent estimation across different scaled inputs. To explicitly address these issues, we propose a novel crowd counting (density estimation) framework called Adversarial Cross-Scale Consistency Pursuit (ACSCP). On one hand, a U-net structural network is designed to generate density map from input patch, and an adversarial loss is employed to shrink the solution onto a realistic subspace, thus attenuating the blurry effects of density map estimation. On the other hand, we design a novel scale-consistency regularizer which enforces that the sum up of the crowd counts from local patches (i.e., small scale) is coherent with the overall count of their region union (i.e., large scale). The above losses are integrated via a joint training scheme, so as to help boost density estimation performance by further exploring the collaboration between both objectives. Extensive experiments on four benchmarks have well demonstrated the effectiveness of the proposed innovations as well as the superior performance over prior art.","中文标题":"通过对抗性跨尺度一致性追求进行人群计数","摘要翻译":"人群计数或密度估计是计算机视觉中的一个挑战性任务，由于尺度变化大、透视变形和严重遮挡等原因。现有方法普遍存在两个问题：1）由于广泛采用的L2回归损失导致的多尺度CNN中的模型平均效应；2）不同尺度输入之间的估计不一致。为了明确解决这些问题，我们提出了一种新颖的人群计数（密度估计）框架，称为对抗性跨尺度一致性追求（ACSCP）。一方面，设计了一个U-net结构网络从输入补丁生成密度图，并采用对抗性损失将解缩小到现实子空间，从而减弱密度图估计的模糊效果。另一方面，我们设计了一种新颖的尺度一致性正则化器，它强制局部补丁（即小尺度）的人群计数总和与其区域联合（即大尺度）的总体计数一致。上述损失通过联合训练方案集成，以通过进一步探索两个目标之间的协作来帮助提高密度估计性能。在四个基准上的大量实验充分证明了所提出创新的有效性以及相对于现有技术的优越性能。","领域":"人群计数/密度估计/对抗性学习","问题":"解决多尺度CNN中的模型平均效应和不同尺度输入之间的估计不一致问题","动机":"提高人群计数或密度估计的准确性和一致性","方法":"设计了一个U-net结构网络生成密度图，并采用对抗性损失和尺度一致性正则化器来增强估计的准确性和一致性","关键词":["人群计数","密度估计","对抗性学习","U-net","尺度一致性"],"涉及的技术概念":"L2回归损失、多尺度CNN、对抗性损失、U-net结构网络、尺度一致性正则化器"},{"order":543,"title":"3D Human Pose Estimation in the Wild by Adversarial Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_3D_Human_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_3D_Human_Pose_CVPR_2018_paper.html","abstract":"Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difficult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of defining hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efficacy of our adversarial learning framework with the new geometric descriptor have been demonstrated through extensive experiments on two widely used public benchmarks. Our approach significantly improves the performance compared with previous state-of-the-art approaches.","中文标题":"通过对抗学习在野外进行3D人体姿态估计","摘要翻译":"最近，由于强大的深度卷积神经网络（DCNNs），从单眼图像中进行3D人体姿态估计取得了显著进展。尽管在受控实验室环境中收集的大规模数据集上取得了成功，但在野外图像中获取3D姿态注释仍然困难。因此，野外3D人体姿态估计仍然是一个挑战。在本文中，我们提出了一个对抗学习框架，该框架将从完全注释的数据集中学习到的3D人体姿态结构提炼到仅有2D姿态注释的野外图像中。我们没有定义硬编码规则来约束姿态估计结果，而是设计了一个新颖的多源判别器来区分预测的3D姿态与真实姿态，这有助于强制姿态估计器生成符合人体测量学的有效姿态，即使在野外图像中也是如此。我们还观察到，为判别器精心设计的信息源对于提升性能至关重要。因此，我们设计了一个几何描述符，它计算身体关节之间的成对相对位置和距离，作为判别器的新信息源。我们通过在两个广泛使用的公共基准上进行的大量实验证明了我们的对抗学习框架与新几何描述符的有效性。与之前的最先进方法相比，我们的方法显著提高了性能。","领域":"3D人体姿态估计/对抗学习/几何描述符","问题":"在野外图像中获取3D姿态注释的困难","动机":"解决在野外环境中进行3D人体姿态估计的挑战","方法":"提出一个对抗学习框架，使用多源判别器和几何描述符来提升3D姿态估计的性能","关键词":["3D人体姿态估计","对抗学习","几何描述符"],"涉及的技术概念":"深度卷积神经网络（DCNNs）用于从单眼图像中进行3D人体姿态估计；对抗学习框架用于提炼3D姿态结构；多源判别器用于区分预测的3D姿态与真实姿态；几何描述符用于计算身体关节之间的成对相对位置和距离。"},{"order":544,"title":"CosFace: Large Margin Cosine Loss for Deep Face Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_CosFace_Large_Margin_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html","abstract":"Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.","中文标题":"CosFace: 用于深度人脸识别的大间隔余弦损失","摘要翻译":"由于深度卷积神经网络（CNNs）的进步，人脸识别取得了非凡的进展。人脸识别的核心任务，包括人脸验证和识别，涉及人脸特征的区分。然而，深度CNNs的传统softmax损失通常缺乏区分能力。为了解决这个问题，最近提出了几种损失函数，如中心损失、大间隔softmax损失和角度softmax损失。所有这些改进的损失函数都共享同一个理念：最大化类间差异和最小化类内差异。在本文中，我们提出了一种新的损失函数，即大间隔余弦损失（LMCL），以从不同的角度实现这一理念。更具体地说，我们通过L2归一化特征和权重向量来消除径向变化，将softmax损失重新表述为余弦损失，在此基础上引入余弦间隔项以进一步最大化角度空间中的决策间隔。结果，通过归一化和余弦决策间隔最大化，实现了最小类内差异和最大类间差异。我们将使用LMCL训练的模型称为CosFace。在MegaFace Challenge、Youtube Faces（YTF）和Labeled Face in the Wild（LFW）等最受欢迎的公共领域人脸识别数据集上进行了广泛的实验评估。我们在这些基准测试中实现了最先进的性能，这证实了我们提出的方法的有效性。","领域":"人脸识别/深度学习/损失函数优化","问题":"传统softmax损失在人脸识别任务中缺乏区分能力","动机":"提高人脸识别中特征的区分能力，通过最大化类间差异和最小化类内差异","方法":"提出大间隔余弦损失（LMCL），通过L2归一化特征和权重向量，并引入余弦间隔项来最大化决策间隔","关键词":["人脸识别","损失函数","特征区分"],"涉及的技术概念":{"深度卷积神经网络（CNNs）":"一种深度学习模型，用于处理图像数据，特别是在人脸识别任务中。","softmax损失":"一种常用的损失函数，用于分类任务中，但传统形式在人脸识别中可能缺乏足够的区分能力。","L2归一化":"一种特征归一化方法，通过将特征向量的L2范数归一化为1，来消除径向变化。","余弦间隔项":"在损失函数中引入的一个项，用于在角度空间中进一步最大化决策间隔，以提高特征的区分能力。"}},{"order":545,"title":"Encoding Crowd Interaction With Deep Neural Network for Pedestrian Trajectory Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Encoding_Crowd_Interaction_CVPR_2018_paper.html","abstract":"Pedestrian trajectory prediction is a challenging task because of the complex nature of humans. In this paper, we tackle the problem within a deep learning framework by considering motion information of each pedestrian and its interaction with the crowd. Specifically, motivated by the residual learning in deep learning, we propose to predict displacement between neighboring frames for each pedestrian sequentially. To predict such displacement, we design a crowd interaction deep neural network (CIDNN) which considers the different importance of different pedestrians for the displacement prediction of a target pedestrian. Specifically, we use an LSTM to model motion information for all pedestrians and use a multi-layer perceptron to map the location of each pedestrian to a high dimensional feature space where the inner product between features is used as a measurement for the spatial affinity between two pedestrians. Then we weight the motion features of all pedestrians based on their spatial affinity to the target pedestrian for location displacement prediction. Extensive experiments on publicly available datasets validate the effectiveness of our method for trajectory prediction.","中文标题":"使用深度神经网络编码人群交互以进行行人轨迹预测","摘要翻译":"行人轨迹预测是一项具有挑战性的任务，因为人类行为的复杂性。在本文中，我们通过考虑每个行人的运动信息及其与人群的交互，在深度学习框架内解决了这个问题。具体来说，受到深度学习中残差学习的启发，我们提出顺序预测每个行人在相邻帧之间的位移。为了预测这种位移，我们设计了一个考虑不同行人对目标行人位移预测不同重要性的深度神经网络（CIDNN）。具体来说，我们使用LSTM来建模所有行人的运动信息，并使用多层感知器将每个行人的位置映射到高维特征空间，其中特征之间的内积被用作两个行人之间空间亲和力的度量。然后，我们根据所有行人与目标行人的空间亲和力来加权他们的运动特征，以进行位置位移预测。在公开可用的数据集上进行的大量实验验证了我们方法在轨迹预测中的有效性。","领域":"行人轨迹预测/深度学习/空间亲和力建模","问题":"行人轨迹预测","动机":"考虑到人类行为的复杂性和行人之间的交互，需要一种有效的方法来预测行人轨迹。","方法":"提出了一种深度神经网络（CIDNN），该网络通过LSTM建模行人运动信息，并使用多层感知器将行人位置映射到高维特征空间，通过特征内积衡量空间亲和力，进而加权运动特征以预测位移。","关键词":["行人轨迹预测","深度神经网络","LSTM","多层感知器","空间亲和力"],"涉及的技术概念":"LSTM（长短期记忆网络）用于建模时间序列数据，如行人运动信息；多层感知器用于将数据映射到高维空间；空间亲和力通过特征内积衡量，用于评估行人之间的相互影响。"},{"order":546,"title":"Mean-Variance Loss for Deep Age Estimation From a Face","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.html","abstract":"Age estimation has broad application prospects of many fields, such as video surveillance, social networking, and human-computer interaction. However, many of the published age estimation approaches simply treat the age estimation as an exact age regression problem, and thus did not leverage a distribution's robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Specifically, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed mean-variance loss and softmax loss are embedded jointly into Convolutional Neural Networks (CNNs) for age estimation, and the network weights are optimized via stochastic gradient descent (SGD) in an end-to-end learning way. Experimental results on a number of challenging face aging databases (FG-NET, MORPH Album II, and CLAP2016) show that the proposed approach outperforms the state-of-the-art methods by a large margin using a single model.","中文标题":"基于均值-方差损失的深度年龄估计","摘要翻译":"年龄估计在视频监控、社交网络和人机交互等多个领域具有广泛的应用前景。然而，许多已发布的年龄估计方法仅将年龄估计视为一个精确的年龄回归问题，因此没有利用分布表示标签（如年龄）的模糊性的鲁棒性。在本文中，我们提出了一种新的损失函数，称为均值-方差损失，用于通过分布学习进行鲁棒的年龄估计。具体来说，均值-方差损失由均值损失和方差损失组成，均值损失惩罚估计年龄分布的均值与真实年龄之间的差异，方差损失惩罚估计年龄分布的方差以确保分布的集中性。提出的均值-方差损失和softmax损失联合嵌入卷积神经网络（CNNs）中进行年龄估计，并通过随机梯度下降（SGD）以端到端学习的方式优化网络权重。在多个具有挑战性的人脸老化数据库（FG-NET、MORPH Album II和CLAP2016）上的实验结果表明，所提出的方法在使用单一模型的情况下，大幅超越了最先进的方法。","领域":"人脸识别/年龄估计/深度学习","问题":"如何提高年龄估计的准确性和鲁棒性","动机":"现有的年龄估计方法未能充分利用年龄标签的模糊性，导致估计结果不够准确和鲁棒","方法":"提出了一种新的均值-方差损失函数，结合softmax损失，通过卷积神经网络进行年龄估计，并采用随机梯度下降法优化网络权重","关键词":["年龄估计","均值-方差损失","卷积神经网络","随机梯度下降"],"涉及的技术概念":"均值-方差损失是一种新的损失函数，用于通过分布学习进行年龄估计，它包括均值损失和方差损失两部分，旨在提高年龄估计的准确性和鲁棒性。卷积神经网络（CNNs）是一种深度学习模型，用于处理图像数据。随机梯度下降（SGD）是一种优化算法，用于最小化损失函数，通过迭代更新模型参数来优化模型性能。"},{"order":547,"title":"Probabilistic Joint Face-Skull Modelling for Facial Reconstruction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Madsen_Probabilistic_Joint_Face-Skull_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Madsen_Probabilistic_Joint_Face-Skull_CVPR_2018_paper.html","abstract":"We present a novel method for co-registration of two independent statistical shape models. We solve the problem of aligning a face model to a skull model with stochastic optimization based on Markov Chain Monte Carlo (MCMC). We create a probabilistic joint face-skull model and show how to obtain a distribution of plausible face shapes given a skull shape. Due to environmental and genetic factors, there exists a distribution of possible face shapes arising from the same skull. We pose facial reconstruction as a conditional distribution of plausible face shapes given a skull shape. Because it is very difficult to obtain the distribution directly from MRI or CT data, we create a dataset of artificial face-skull pairs. To do this, we propose to combine three data sources of independent origin to model the joint face-skull distribution: a face shape model, a skull shape model and tissue depth marker information. For a given skull, we compute the posterior distribution of faces matching the tissue depth distribution with Metropolis-Hastings. We estimate the joint face-skull distribution from samples of the posterior. To find faces matching to an unknown skull, we estimate the probability of the face under the joint face-skull model. To our knowledge, we are the first to provide a whole distribution of plausible faces arising from a skull instead of only a single reconstruction. We show how the face-skull model can be used to rank a face dataset and on average successfully identify the correct match in top 30%. The face ranking even works when obtaining the face shapes from 2D images. We furthermore show how the face-skull model can be useful to estimate the skull position in an MR-image.","中文标题":"概率联合面部-颅骨建模用于面部重建","摘要翻译":"我们提出了一种新颖的方法，用于两个独立统计形状模型的共同配准。我们解决了基于马尔可夫链蒙特卡罗（MCMC）的随机优化将面部模型与颅骨模型对齐的问题。我们创建了一个概率联合面部-颅骨模型，并展示了如何根据颅骨形状获得可能的面部形状分布。由于环境和遗传因素，同一颅骨可能产生多种面部形状的分布。我们将面部重建视为给定颅骨形状的可能面部形状的条件分布。由于直接从MRI或CT数据中获取分布非常困难，我们创建了一个人工面部-颅骨对的数据集。为此，我们提出结合三个独立来源的数据来建模联合面部-颅骨分布：面部形状模型、颅骨形状模型和组织深度标记信息。对于给定的颅骨，我们使用Metropolis-Hastings算法计算匹配组织深度分布的面部的后验分布。我们从后验样本中估计联合面部-颅骨分布。为了找到与未知颅骨匹配的面部，我们估计面部在联合面部-颅骨模型下的概率。据我们所知，我们是第一个提供从颅骨产生的可能面部形状的整个分布，而不仅仅是单一重建的。我们展示了如何使用面部-颅骨模型对面部数据集进行排名，并平均成功识别出前30%的正确匹配。即使从2D图像中获取面部形状，面部排名仍然有效。我们还展示了面部-颅骨模型如何有助于估计MR图像中的颅骨位置。","领域":"面部重建/统计形状模型/医学图像分析","问题":"解决面部模型与颅骨模型对齐的问题，以及如何根据颅骨形状获得可能的面部形状分布","动机":"由于环境和遗传因素，同一颅骨可能产生多种面部形状的分布，直接从MRI或CT数据中获取分布非常困难","方法":"使用马尔可夫链蒙特卡罗（MCMC）的随机优化方法，结合面部形状模型、颅骨形状模型和组织深度标记信息，创建概率联合面部-颅骨模型，并通过Metropolis-Hastings算法计算匹配组织深度分布的面部的后验分布","关键词":["面部重建","统计形状模型","医学图像分析","马尔可夫链蒙特卡罗","Metropolis-Hastings算法"],"涉及的技术概念":"马尔可夫链蒙特卡罗（MCMC）是一种随机优化方法，用于解决复杂的优化问题。Metropolis-Hastings算法是MCMC的一种，用于从复杂的概率分布中采样。统计形状模型用于描述和分析形状的变异性。组织深度标记信息提供了面部软组织与颅骨之间的关系，对于面部重建至关重要。"},{"order":548,"title":"Learning Latent Super-Events to Detect Multiple Activities in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Piergiovanni_Learning_Latent_Super-Events_CVPR_2018_paper.html","abstract":"In this paper, we introduce the concept of learning latent super-events from activity videos, and present how it benefits activity detection in continuous videos. We define a super-event as a set of multiple events occurring together in videos with a particular temporal organization; it is the opposite concept of sub-events. Real-world videos contain multiple activities and are rarely segmented (e.g., surveillance videos), and learning latent super-events allows the model to capture how the events are temporally related in videos. We design emph{temporal structure filters} that enable the model to focus on particular sub-intervals of the videos, and use them together with a soft attention mechanism to learn representations of latent super-events. Super-event representations are combined with per-frame or per-segment CNNs to provide frame-level annotations. Our approach is designed to be fully differentiable, enabling end-to-end learning of latent super-event representations jointly with the activity detector using them. Our experiments with multiple public video datasets confirm that the proposed concept of latent super-event learning significantly benefits activity detection, advancing the state-of-the-arts.","中文标题":"学习潜在超级事件以检测视频中的多个活动","摘要翻译":"在本文中，我们介绍了从活动视频中学习潜在超级事件的概念，并展示了它如何有利于连续视频中的活动检测。我们将超级事件定义为视频中具有特定时间组织的多个事件的集合；它是子事件的对立概念。现实世界的视频包含多个活动，并且很少被分割（例如，监控视频），学习潜在超级事件使模型能够捕捉视频中事件的时间关系。我们设计了强调时间结构过滤器，使模型能够专注于视频的特定子间隔，并将它们与软注意力机制一起使用，以学习潜在超级事件的表示。超级事件表示与每帧或每段CNN结合，以提供帧级注释。我们的方法设计为完全可微分，使得潜在超级事件表示与使用它们的活动检测器能够进行端到端学习。我们在多个公共视频数据集上的实验证实，所提出的潜在超级事件学习概念显著有利于活动检测，推动了该领域的最新进展。","领域":"视频分析/活动识别/时间序列分析","问题":"如何在连续视频中有效检测多个活动","动机":"现实世界的视频包含多个活动且很少被分割，需要一种方法来捕捉视频中事件的时间关系","方法":"设计时间结构过滤器与软注意力机制结合，学习潜在超级事件的表示，并与每帧或每段CNN结合提供帧级注释","关键词":["潜在超级事件","活动检测","时间结构过滤器","软注意力机制","帧级注释"],"涉及的技术概念":"潜在超级事件指的是视频中具有特定时间组织的多个事件的集合。时间结构过滤器使模型能够专注于视频的特定子间隔。软注意力机制用于学习潜在超级事件的表示。这些表示与每帧或每段CNN结合，以提供帧级注释。"},{"order":549,"title":"Temporal Hallucinating for Action Recognition With Few Still Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Temporal_Hallucinating_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Temporal_Hallucinating_for_CVPR_2018_paper.html","abstract":"Action recognition in still images has been recently promoted by deep learning. However, the success of these deep models heavily depends on huge amount of training images for various action categories, which may not be available in practice. Alternatively, humans can classify new action categories after seeing few images, since we may not only compare appearance similarities between images on hand, but also attempt to recall importance motion cues from relevant action videos in our memory. To mimic this capacity, we propose a novel Hybrid Video Memory (HVM) machine, which can hallucinate temporal features of still images from video memory, in order to boost action recognition with few still images. First, we design a temporal memory module consisting of temporal hallucinating and predicting. Temporal hallucinating can generate temporal features of still images in an unsupervised manner. Hence, it can be flexibly used in realistic scenarios, where image and video categories may not be consistent. Temporal predicting can effectively infer action categories for query image, by integrating temporal features of training images and videos within a domain-adaptation manner. Second, we design a spatial memory module for spatial predicting. As spatial and temporal features are complementary to represent different actions, we apply spatial-temporal prediction fusion to further boost performance. Finally, we design a video selection module to select strongly-relevant videos as memory. In this case, we can balance the number of images and videos to reduce prediction bias as well as preserve computation efficiency. To show the effectiveness, we conduct extensive experiments on three challenging data sets, where our HVM outperforms a number of recent approaches by temporal hallucinating from video memory.","中文标题":"利用少量静态图像进行动作识别的时间幻觉","摘要翻译":"深度学习最近推动了静态图像中的动作识别。然而，这些深度模型的成功在很大程度上依赖于大量各种动作类别的训练图像，这在实际中可能不可用。相反，人类在看过少量图像后可以分类新的动作类别，因为我们不仅可能比较手头图像之间的外观相似性，还可能尝试从记忆中的相关动作视频中回忆重要的运动线索。为了模仿这种能力，我们提出了一种新颖的混合视频记忆（HVM）机器，它可以从视频记忆中幻觉出静态图像的时间特征，以利用少量静态图像提升动作识别。首先，我们设计了一个时间记忆模块，包括时间幻觉和预测。时间幻觉可以以无监督的方式生成静态图像的时间特征。因此，它可以灵活地用于现实场景，其中图像和视频类别可能不一致。时间预测可以通过在领域适应方式下整合训练图像和视频的时间特征，有效地推断查询图像的动作类别。其次，我们设计了一个空间记忆模块用于空间预测。由于空间和时间特征在表示不同动作时是互补的，我们应用时空预测融合以进一步提升性能。最后，我们设计了一个视频选择模块来选择强相关的视频作为记忆。在这种情况下，我们可以平衡图像和视频的数量，以减少预测偏差并保持计算效率。为了展示有效性，我们在三个具有挑战性的数据集上进行了广泛的实验，我们的HVM通过从视频记忆中幻觉时间特征，优于许多最近的方法。","领域":"动作识别/视频分析/时间序列分析","问题":"在仅有少量静态图像的情况下进行有效的动作识别","动机":"模仿人类在看过少量图像后能够分类新动作类别的能力，解决深度学习模型对大量训练图像的依赖问题","方法":"提出混合视频记忆（HVM）机器，包括时间记忆模块（时间幻觉和预测）、空间记忆模块和视频选择模块，通过时空预测融合提升动作识别性能","关键词":["动作识别","时间幻觉","视频记忆","时空预测融合","领域适应"],"涉及的技术概念":"时间幻觉指的是从静态图像中生成时间特征的过程，模仿人类从记忆中回忆动作视频的能力。混合视频记忆（HVM）机器是一种结合了时间记忆模块、空间记忆模块和视频选择模块的系统，旨在通过从视频记忆中提取时间特征来增强动作识别的能力。时空预测融合是指将空间和时间特征结合起来，以提高动作识别的准确性。领域适应是一种技术，用于调整模型以适应不同但相关的数据分布，这里用于整合训练图像和视频的时间特征。"},{"order":550,"title":"Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html","abstract":"In this paper, we propose a deep progressive reinforcement learning (DPRL) method for action recognition in skeleton-based videos, which aims to distil the most informative frames and discard ambiguous frames in sequences for recognizing actions. Since the choices of selecting representative frames are multitudinous for each video, we model the frame selection as a progressive process through deep reinforcement learning, during which we progressively adjust the chosen frames by taking two important factors into account: (1) the quality of the selected frames and (2) the relationship between the selected frames to the whole video. Moreover, considering the topology of human body inherently lies in a graph-based structure, where the vertices and edges represent the hinged joints and rigid bones respectively, we employ the graph-based convolutional neural network to capture the dependency between the joints for action recognition. Our approach achieves very competitive performance on three widely used benchmarks.","中文标题":"基于骨架的动作识别的深度渐进强化学习","摘要翻译":"本文提出了一种用于基于骨架视频中动作识别的深度渐进强化学习（DPRL）方法，旨在提炼出最具信息量的帧并丢弃序列中的模糊帧以识别动作。由于每个视频中选择代表性帧的选择众多，我们通过深度强化学习将帧选择建模为一个渐进过程，在此过程中，我们通过考虑两个重要因素逐步调整所选帧：（1）所选帧的质量和（2）所选帧与整个视频之间的关系。此外，考虑到人体拓扑结构本质上基于图结构，其中顶点和边分别代表铰接关节和刚性骨骼，我们采用基于图的卷积神经网络来捕捉关节之间的依赖关系以进行动作识别。我们的方法在三个广泛使用的基准测试中实现了非常有竞争力的性能。","领域":"动作识别/强化学习/图卷积网络","问题":"在基于骨架的视频中识别动作时，如何选择最具信息量的帧并丢弃模糊帧","动机":"为了提高动作识别的准确性和效率，需要一种方法能够从视频序列中提炼出最具信息量的帧，同时考虑帧的质量和它们与整个视频的关系","方法":"采用深度渐进强化学习方法，通过考虑所选帧的质量和它们与整个视频的关系，逐步调整所选帧，并使用基于图的卷积神经网络捕捉关节之间的依赖关系","关键词":["动作识别","强化学习","图卷积网络","骨架视频","帧选择"],"涉及的技术概念":"深度渐进强化学习（DPRL）是一种结合深度学习和强化学习的方法，用于从视频序列中选择最具信息量的帧。图卷积网络（GCN）是一种处理图结构数据的神经网络，能够捕捉图中节点（如人体关节）之间的依赖关系。"},{"order":551,"title":"Gaze Prediction in Dynamic 360° Immersive Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Gaze_Prediction_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Gaze_Prediction_in_CVPR_2018_paper.html","abstract":"This paper explores gaze prediction in dynamic $360^circ$ immersive videos, emph{i.e.}, based on the history scan path and VR contents, we predict where a viewer will look at an upcoming time. To tackle this problem, we first present the large-scale eye-tracking in dynamic VR scene dataset. Our dataset contains 208 $360^circ$ videos captured in dynamic scenes, and each video is viewed by at least 31 subjects. Our analysis shows that gaze prediction depends on its history scan path and image contents. In terms of the image contents, those salient objects easily attract viewers' attention. On the one hand, the saliency is related to both appearance and motion of the objects. Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: the sub-image patch centered at current gaze point, the sub-image corresponding to the Field of View (FoV), and the panorama image. Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction. Meanwhile, we also use a Long-Short-Term-Memory (LSTM) to encode the history scan path. Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time. Extensive experiments validate the effectiveness of our method for gaze prediction in dynamic VR scenes.","中文标题":"动态360°沉浸式视频中的视线预测","摘要翻译":"本文探讨了动态360°沉浸式视频中的视线预测问题，即基于历史扫描路径和VR内容，我们预测观众在接下来的时间会看向哪里。为了解决这个问题，我们首先提出了一个大规模的动态VR场景中的眼动追踪数据集。我们的数据集包含208个在动态场景中捕获的360°视频，每个视频至少由31名受试者观看。我们的分析表明，视线预测依赖于其历史扫描路径和图像内容。就图像内容而言，那些显著的对象容易吸引观众的注意力。一方面，显著性既与对象的外观有关，也与对象的运动有关。考虑到在不同尺度下测量的显著性不同，我们提出在不同空间尺度上计算显著性图：以当前视线点为中心的图像子块、对应于视野（FoV）的图像子块以及全景图像。然后，我们将显著性图和相应的图像输入卷积神经网络（CNN）进行特征提取。同时，我们还使用长短期记忆（LSTM）来编码历史扫描路径。然后，我们将CNN特征和LSTM特征结合起来，用于预测当前时间视线点与即将到来的时间视线点之间的视线位移。大量实验验证了我们的方法在动态VR场景中视线预测的有效性。","领域":"虚拟现实/眼动追踪/显著性检测","问题":"动态360°沉浸式视频中的视线预测","动机":"为了预测观众在动态360°沉浸式视频中接下来的时间会看向哪里，基于历史扫描路径和VR内容进行研究。","方法":"首先提出一个大规模的动态VR场景中的眼动追踪数据集，然后通过在不同空间尺度上计算显著性图，并将显著性图和相应的图像输入卷积神经网络（CNN）进行特征提取，同时使用长短期记忆（LSTM）来编码历史扫描路径，最后结合CNN特征和LSTM特征进行视线位移预测。","关键词":["视线预测","360°视频","显著性检测","卷积神经网络","长短期记忆"],"涉及的技术概念":"显著性图（Saliency Maps）：用于表示图像中哪些区域更可能吸引人眼注意力的图。卷积神经网络（CNN）：一种深度学习模型，特别适用于处理图像数据。长短期记忆（LSTM）：一种特殊的递归神经网络（RNN），能够学习长期依赖信息。"},{"order":552,"title":"When Will You Do What? - Anticipating Temporal Occurrences of Activities","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Abu_Farha_When_Will_You_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Abu_Farha_When_Will_You_CVPR_2018_paper.html","abstract":"Analyzing human actions in videos has gained increased attention recently. While most works focus on classifying and labeling observed video frames or anticipating the very recent future, making long-term predictions over more than just a few seconds is a task with many practical applications that has not yet been addressed. In this paper, we propose two methods to predict a considerably large amount of future actions and their durations. Both, a CNN and an RNN are trained to learn future video labels based on previously seen content. We show that our methods generate accurate predictions of the future even for long videos with a huge amount of different actions and can even deal with noisy or erroneous input information.","中文标题":"你何时会做什么？——预测活动的时间发生","摘要翻译":"近年来，分析视频中的人类行为受到了越来越多的关注。虽然大多数工作集中在分类和标记观察到的视频帧或预测非常近期的未来，但进行超过几秒钟的长期预测是一个具有许多实际应用的任务，这一任务尚未得到解决。在本文中，我们提出了两种方法来预测大量未来行为及其持续时间。我们训练了一个CNN和一个RNN，以基于先前看到的内容学习未来的视频标签。我们展示了我们的方法即使对于包含大量不同行为的长时间视频也能生成准确的未来预测，并且甚至能够处理噪声或错误的输入信息。","领域":"行为预测/视频分析/时间序列预测","问题":"长期预测视频中的人类行为及其持续时间","动机":"解决现有方法无法进行超过几秒钟的长期预测的问题，以满足实际应用需求","方法":"提出两种方法，分别使用CNN和RNN基于先前内容学习未来视频标签，以预测未来行为和持续时间","关键词":["行为预测","视频分析","时间序列预测"],"涉及的技术概念":"CNN（卷积神经网络）和RNN（循环神经网络）是两种深度学习模型，分别用于处理空间信息和时间序列信息。在本研究中，CNN用于从视频帧中提取特征，而RNN用于处理这些特征以预测未来的行为和持续时间。"},{"order":553,"title":"Fusing Crowd Density Maps and Visual Object Trackers for People Tracking in Crowd Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_Fusing_Crowd_Density_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_Fusing_Crowd_Density_CVPR_2018_paper.html","abstract":"While people tracking has been greatly improved over the recent years, crowd scenes remain particularly challenging for people tracking due to heavy occlusions, high crowd density, and significant appearance variation. To address these challenges, we first design a Sparse Kernelized Correlation Filter (S-KCF) to suppress target response variations caused by occlusions and illumination changes, and spurious responses due to similar distractor objects. We then propose a people tracking framework that fuses the S-KCF response map with an estimated crowd density map using a convolutional neural network (CNN), yielding a refined response map. To train the fusion CNN, we propose a two-stage strategy to gradually optimize the parameters. The first stage is to train a preliminary model in batch mode with image patches selected around the targets, and the second stage is to fine-tune the preliminary model using the real frame-by-frame tracking process. Our density fusion framework can significantly improves people tracking in crowd scenes, and can also be combined with other trackers to improve the tracking performance. We validate our framework on two crowd video datasets: UCSD and PETS2009.","中文标题":"融合人群密度图和视觉对象跟踪器用于人群场景中的人员跟踪","摘要翻译":"尽管近年来人员跟踪技术有了很大的改进，但由于严重的遮挡、高人群密度和显著的外观变化，人群场景中的人员跟踪仍然特别具有挑战性。为了应对这些挑战，我们首先设计了一个稀疏核化相关滤波器（S-KCF），以抑制由遮挡和光照变化引起的目标响应变化，以及由于相似干扰物引起的虚假响应。然后，我们提出了一个人员跟踪框架，该框架通过卷积神经网络（CNN）将S-KCF响应图与估计的人群密度图融合，产生一个精细化的响应图。为了训练融合CNN，我们提出了一个两阶段策略来逐步优化参数。第一阶段是使用围绕目标选择的图像块在批处理模式下训练初步模型，第二阶段是使用真实的逐帧跟踪过程对初步模型进行微调。我们的密度融合框架可以显著提高人群场景中的人员跟踪，并且可以与其他跟踪器结合使用以提高跟踪性能。我们在两个人群视频数据集上验证了我们的框架：UCSD和PETS2009。","领域":"人群分析/视频监控/行为识别","问题":"在人群场景中准确跟踪人员","动机":"解决人群场景中由于遮挡、高密度和外观变化导致的人员跟踪难题","方法":"设计稀疏核化相关滤波器（S-KCF）抑制目标响应变化和虚假响应，提出融合S-KCF响应图与人群密度图的人员跟踪框架，采用两阶段策略训练融合CNN","关键词":["人群密度图","视觉对象跟踪","卷积神经网络","稀疏核化相关滤波器","人员跟踪"],"涉及的技术概念":"稀疏核化相关滤波器（S-KCF）用于抑制目标响应变化和虚假响应，卷积神经网络（CNN）用于融合S-KCF响应图与人群密度图，两阶段训练策略用于优化融合CNN的参数。"},{"order":554,"title":"Dual Attention Matching Network for Context-Aware Feature Sequence Based Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Dual_Attention_Matching_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Si_Dual_Attention_Matching_CVPR_2018_paper.html","abstract":"Typical person re-identification (ReID) methods usually describe each pedestrian with a single feature vector and match them in a task-specific metric space. However, the methods based on a single feature vector are not sufficient enough to overcome visual ambiguity, which frequently occurs in real scenario. In this paper, we propose a novel end-to-end trainable framework, called Dual ATtention Matching network (DuATM), to learn context-aware feature sequences and perform attentive sequence comparison simultaneously. The core component of our DuATM framework is a dual attention mechanism, in which both intra-sequence and inter-sequence attention strategies are used for feature refinement and feature-pair alignment, respectively. Thus, detailed visual cues contained in the intermediate feature sequences can be automatically exploited and properly compared. We train the proposed DuATM network as a siamese network via a triplet loss assisted with a de-correlation loss and a cross-entropy loss. We conduct extensive experiments on both image and video based ReID benchmark datasets. Experimental results demonstrate the significant advantages of our approach compared to the state-of-the-art methods.","中文标题":"双注意力匹配网络用于基于上下文感知特征序列的人员再识别","摘要翻译":"典型的人员再识别（ReID）方法通常用单一特征向量描述每个行人，并在特定任务的度量空间中进行匹配。然而，基于单一特征向量的方法不足以克服实际场景中经常出现的视觉模糊性。在本文中，我们提出了一种新颖的端到端可训练框架，称为双注意力匹配网络（DuATM），以学习上下文感知的特征序列并同时执行注意力序列比较。我们的DuATM框架的核心组件是一个双注意力机制，其中序列内和序列间注意力策略分别用于特征细化和特征对对齐。因此，可以自动利用并适当比较中间特征序列中包含的详细视觉线索。我们通过三重损失辅助的去相关损失和交叉熵损失，将提出的DuATM网络作为连体网络进行训练。我们在基于图像和视频的ReID基准数据集上进行了广泛的实验。实验结果表明，与最先进的方法相比，我们的方法具有显著优势。","领域":"人员再识别/注意力机制/特征序列","问题":"克服实际场景中人员再识别的视觉模糊性问题","动机":"基于单一特征向量的人员再识别方法在实际场景中经常遇到视觉模糊性问题，需要更有效的方法来提高识别准确率","方法":"提出了一种端到端可训练的双注意力匹配网络（DuATM），通过双注意力机制学习上下文感知的特征序列并执行注意力序列比较","关键词":["人员再识别","注意力机制","特征序列","视觉模糊性","端到端学习"],"涉及的技术概念":{"双注意力机制":"一种同时使用序列内和序列间注意力策略的机制，用于特征细化和特征对对齐","连体网络":"一种网络结构，通过共享权重来学习输入对之间的相似性","三重损失":"一种用于训练连体网络的损失函数，旨在使相同类别的样本之间的距离小于不同类别样本之间的距离","去相关损失":"一种损失函数，用于减少特征之间的相关性，以提高特征的判别能力","交叉熵损失":"一种常用的分类损失函数，用于衡量模型预测的概率分布与真实分布之间的差异"}},{"order":555,"title":"Easy Identification From Better Constraints: Multi-Shot Person Re-Identification From Reference Constraints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Easy_Identification_From_CVPR_2018_paper.html","abstract":"Multi-shot person re-identification (MsP-RID) utilizes multiple images from the same person to facilitate identification. Considering the fact that motion information may not be discriminative nor reliable enough for MsP-RID, this paper is focused on handling the large variations in the visual appearances through learning discriminative visual metrics for identification. Existing metric learning-based methods usually exploit pair-wise or triple-wise similarity constraints, that generally demands intensive optimization in metric learning, or leads to degraded performances by using sub-optimal solutions. In addition, as the training data are significantly imbalanced, the learning can be largely dominated by the negative pairs and thus produces unstable and non-discriminative results. In this paper, we propose a novel type of similarity constraint. It assigns the sample points to a set of \\textbf{reference points} to produce a linear number of \\textbf{reference constraints}. Several optimal transport-based schemes for reference constraint generation are proposed and studied. Based on those constraints, by utilizing a typical regressive metric learning model, the closed-form solution of the learned metric can be easily obtained. Extensive experiments and comparative studies on several public MsP-RID benchmarks have validated the effectiveness of our method and its significant superiority over the state-of-the-art MsP-RID methods in terms of both identification accuracy and running speed.","中文标题":"从更好的约束中轻松识别：基于参考约束的多镜头行人重识别","摘要翻译":"多镜头行人重识别（MsP-RID）利用同一个人的多张图像来促进识别。考虑到运动信息可能不足以区分或不够可靠用于MsP-RID，本文专注于通过学习区分性视觉度量来处理视觉外观的大变化。现有的基于度量学习的方法通常利用成对或三重的相似性约束，这通常需要在度量学习中进行密集优化，或者通过使用次优解决方案导致性能下降。此外，由于训练数据显著不平衡，学习可能主要由负对主导，从而产生不稳定且不具区分性的结果。在本文中，我们提出了一种新型的相似性约束。它将样本点分配给一组参考点，以产生线性数量的参考约束。提出并研究了几种基于最优传输的参考约束生成方案。基于这些约束，通过利用典型的回归度量学习模型，可以轻松获得学习度量的闭式解。在多个公共MsP-RID基准上的广泛实验和比较研究验证了我们方法的有效性及其在识别准确性和运行速度方面相对于最先进的MsP-RID方法的显著优势。","领域":"行人重识别/度量学习/最优传输","问题":"处理多镜头行人重识别中视觉外观的大变化","动机":"运动信息可能不足以区分或不够可靠用于多镜头行人重识别，需要通过学习区分性视觉度量来处理视觉外观的大变化","方法":"提出了一种新型的相似性约束，将样本点分配给一组参考点以产生线性数量的参考约束，并利用典型的回归度量学习模型获得学习度量的闭式解","关键词":["行人重识别","度量学习","最优传输"],"涉及的技术概念":"多镜头行人重识别（MsP-RID）利用同一个人的多张图像来促进识别。本文提出了一种新型的相似性约束，通过将样本点分配给一组参考点来产生线性数量的参考约束，并利用最优传输方案生成这些约束。通过典型的回归度量学习模型，可以轻松获得学习度量的闭式解。"},{"order":556,"title":"Crowd Counting With Deep Negative Correlation Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shi_Crowd_Counting_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shi_Crowd_Counting_With_CVPR_2018_paper.html","abstract":"Deep convolutional networks (ConvNets) have achieved unprecedented performances on many computer vision tasks. However, their adaptations to crowd counting on single images are still in their infancy and suffer from severe over-fitting. Here we propose a new learning strategy to produce generalizable features by way of deep negative correlation learning (NCL). More specifically, we deeply learn a pool of decorrelated regressors with sound generalization capabilities through managing their intrinsic diversities. Our proposed method, named decorrelated ConvNet (D-ConvNet), is end-to-end-trainable and independent of the backbone fully-convolutional network architectures.  Extensive experiments on very deep VGGNet as well as our customized network structure indicate the superiority of D-ConvNet when compared with several state-of-the-art methods. Our implementation will be released at https://github.com/shizenglin/Deep-NCL","中文标题":"使用深度负相关学习进行人群计数","摘要翻译":"深度卷积网络（ConvNets）在许多计算机视觉任务上取得了前所未有的性能。然而，它们在单张图像上的人群计数适应仍处于起步阶段，并且遭受严重的过拟合问题。在这里，我们提出了一种新的学习策略，通过深度负相关学习（NCL）来产生具有泛化能力的特征。更具体地说，我们通过管理其内在多样性，深入学习了一组具有良好泛化能力的去相关回归器。我们提出的方法，名为去相关卷积网络（D-ConvNet），是端到端可训练的，并且独立于骨干全卷积网络架构。在非常深的VGGNet以及我们定制的网络结构上进行的大量实验表明，与几种最先进的方法相比，D-ConvNet具有优越性。我们的实现将在https://github.com/shizenglin/Deep-NCL发布。","领域":"人群计数/深度学习/卷积神经网络","问题":"解决单张图像上的人群计数问题，特别是过拟合问题","动机":"提高深度卷积网络在人群计数任务上的泛化能力，减少过拟合","方法":"提出了一种新的学习策略，即深度负相关学习（NCL），通过管理回归器的内在多样性来学习一组去相关回归器，从而产生具有良好泛化能力的特征","关键词":["人群计数","深度负相关学习","去相关卷积网络","泛化能力","过拟合"],"涉及的技术概念":"深度卷积网络（ConvNets）是一种在计算机视觉任务中表现出色的深度学习模型。深度负相关学习（NCL）是一种新的学习策略，旨在通过管理模型的内在多样性来提高其泛化能力。去相关卷积网络（D-ConvNet）是基于NCL提出的方法，旨在解决人群计数中的过拟合问题。"},{"order":557,"title":"Human Appearance Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Human_Appearance_Transfer_CVPR_2018_paper.html","abstract":"We propose an automatic person-to-person appearance transfer model based on explicit parametric 3d human representations and learned, constrained deep translation network architectures for photographic image synthesis. Given a single source image and a single target image, each corresponding to different human subjects, wearing different clothing and in different poses, our goal is to photo-realistically transfer the appearance from the source image onto the target image while preserving the target shape and clothing segmentation layout. Our solution to this new problem is formulated in terms of a computational pipeline that combines (1) 3d human pose and body shape estimation from monocular images, (2) identifying 3d surface colors elements (mesh triangles) visible in both images, that can be transferred directly using barycentric procedures, and (3) predicting surface appearance missing in the first image but visible in the second one using deep learning-based image synthesis techniques. Our model achieves promising results as supported by a perceptual user study where the participants rated around 65% of our results as good, very good or perfect, as well in automated tests (Inception scores and a Faster-RCNN human detector responding very similarly to real and model generated images). We further show how the proposed architecture can be profiled to automatically generate images of a person dressed with different clothing transferred from a person in another image, opening paths for applications in entertainment and photo-editing (e.g. embodying and posing as friends or famous actors), the fashion industry, or affordable online shopping of clothing.","中文标题":"人类外观转移","摘要翻译":"我们提出了一种基于显式参数化3D人体表示和学习的、受限的深度翻译网络架构的自动人对外观转移模型，用于摄影图像合成。给定一个源图像和一个目标图像，每个图像对应不同的人类主体，穿着不同的服装并以不同的姿势，我们的目标是逼真地将源图像的外观转移到目标图像上，同时保留目标形状和服装分割布局。我们对这个新问题的解决方案被表述为一个计算管道，该管道结合了（1）从单目图像中估计3D人体姿势和体型，（2）识别在两幅图像中都可见的3D表面颜色元素（网格三角形），这些元素可以直接使用重心程序进行转移，以及（3）使用基于深度学习的图像合成技术预测在第一幅图像中缺失但在第二幅图像中可见的表面外观。我们的模型取得了有希望的结果，这得到了感知用户研究的支持，在该研究中，参与者将大约65%的结果评为好、非常好或完美，以及在自动化测试中（Inception分数和Faster-RCNN人类检测器对真实和模型生成的图像反应非常相似）。我们进一步展示了如何对提出的架构进行分析，以自动生成一个人穿着从另一幅图像中转移过来的不同服装的图像，为娱乐和照片编辑（例如，体现和摆姿势作为朋友或著名演员）、时尚行业或经济实惠的在线服装购物开辟了应用路径。","领域":"图像合成/3D人体建模/深度学习","问题":"如何逼真地将一个人的外观转移到另一个人的图像上，同时保留目标形状和服装分割布局","动机":"开发一种自动化的方法，用于在娱乐、时尚和在线购物等领域中实现逼真的人类外观转移","方法":"结合3D人体姿势和体型估计、3D表面颜色元素识别和基于深度学习的图像合成技术","关键词":["图像合成","3D人体建模","深度学习"],"涉及的技术概念":"3D人体姿势和体型估计、3D表面颜色元素识别、基于深度学习的图像合成技术、重心程序、Inception分数、Faster-RCNN人类检测器"},{"order":558,"title":"Domain Generalization With Adversarial Feature Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Domain_Generalization_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Domain_Generalization_With_CVPR_2018_paper.html","abstract":"In this paper, we tackle the problem of domain generalization: how to learn a generalized feature representation for an “unseen” target domain by taking the advantage of multiple seen source-domain data. We present a novel framework based on adversarial autoencoders to learn a generalized latent feature representation across domains for domain generalization. To be specific, we extend adversarial autoencoders by imposing the Maximum Mean Discrepancy (MMD) measure to align the distributions among different domains, and matching the aligned distribution to an arbitrary prior distribution via adversarial feature learning. In this way, the learned feature representation is supposed to be universal to the seen source domains because of the MMD regularization, and is expected to generalize well on the target domain because of the introduction of the prior distribution. We proposed an algorithm to jointly train different components of our proposed framework. Extensive experiments on various vision tasks demonstrate that our proposed framework can learn better generalized features for the unseen target domain compared with state of-the-art domain generalization methods.","中文标题":"通过对抗性特征学习实现领域泛化","摘要翻译":"在本文中，我们解决了领域泛化的问题：如何通过利用多个已知源领域数据的优势，为“未见”的目标领域学习一个泛化的特征表示。我们提出了一个基于对抗性自动编码器的新框架，以跨领域学习泛化的潜在特征表示，用于领域泛化。具体来说，我们通过施加最大均值差异（MMD）度量来扩展对抗性自动编码器，以对齐不同领域之间的分布，并通过对抗性特征学习将对齐的分布匹配到任意先验分布。这样，由于MMD正则化，学习到的特征表示对于已知源领域应该是通用的，并且由于引入了先验分布，预计在目标领域上也能很好地泛化。我们提出了一种算法来联合训练我们提出的框架的不同组件。在各种视觉任务上的大量实验表明，与最先进的领域泛化方法相比，我们提出的框架可以为未见的目标领域学习到更好的泛化特征。","领域":"领域泛化/对抗性学习/特征表示","问题":"如何为未见的目标领域学习一个泛化的特征表示","动机":"利用多个已知源领域数据的优势，提高模型在未见目标领域的泛化能力","方法":"基于对抗性自动编码器的新框架，通过最大均值差异（MMD）度量对齐不同领域之间的分布，并通过对抗性特征学习将对齐的分布匹配到任意先验分布","关键词":["领域泛化","对抗性学习","特征表示","最大均值差异","自动编码器"],"涉及的技术概念":{"领域泛化":"指模型在训练时使用多个源领域的数据，目的是提高模型在未见过的目标领域上的性能。","对抗性学习":"一种机器学习方法，通过引入对抗性过程来提高模型的泛化能力和鲁棒性。","特征表示":"指数据在模型中的表示方式，良好的特征表示可以提高模型的性能。","最大均值差异（MMD）":"一种用于衡量两个分布之间差异的统计方法，常用于领域适应和领域泛化任务中。","自动编码器":"一种神经网络，用于学习数据的有效编码，通常用于降维和特征学习。"}},{"order":559,"title":"Pyramid Stereo Matching Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.html","abstract":"Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in ill-posed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.","中文标题":"金字塔立体匹配网络","摘要翻译":"最近的研究表明，从一对立体图像中进行深度估计可以被表述为一个监督学习任务，通过卷积神经网络（CNNs）来解决。然而，当前的架构依赖于基于补丁的Siamese网络，缺乏利用上下文信息在不适定区域寻找对应关系的手段。为了解决这个问题，我们提出了PSMNet，一个金字塔立体匹配网络，由两个主要模块组成：空间金字塔池化和3D CNN。空间金字塔池化模块通过在不同尺度和位置聚合上下文信息来形成成本体积，从而利用全局上下文信息的能力。3D CNN通过堆叠多个沙漏网络并结合中间监督来学习正则化成本体积。所提出的方法在几个基准数据集上进行了评估。我们的方法在2018年3月18日之前在KITTI 2012和2015排行榜上排名第一。PSMNet的代码可在https://github.com/JiaRenChang/PSMNet获取。","领域":"立体视觉/深度估计/卷积神经网络","问题":"解决在不适定区域利用上下文信息进行立体匹配的问题","动机":"当前基于补丁的Siamese网络架构在利用上下文信息进行立体匹配方面存在不足","方法":"提出了PSMNet，包含空间金字塔池化和3D CNN两个主要模块，通过聚合不同尺度和位置的上下文信息形成成本体积，并利用堆叠的沙漏网络进行正则化","关键词":["立体匹配","深度估计","卷积神经网络","空间金字塔池化","3D CNN"],"涉及的技术概念":"卷积神经网络（CNNs）用于深度估计，空间金字塔池化用于聚合全局上下文信息，3D CNN用于正则化成本体积，沙漏网络用于中间监督"},{"order":560,"title":"Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.html","abstract":"Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle’s steering angle. To make the best out of this sensor–algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset (≈1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras","中文标题":"基于事件的视觉与深度学习在自动驾驶汽车转向预测中的结合","摘要翻译":"事件相机是一种受生物启发的视觉传感器，它自然地捕捉场景的动态，过滤掉冗余信息。本文提出了一种深度神经网络方法，该方法在具有挑战性的运动估计任务中释放了事件相机的潜力：预测车辆的转向角度。为了充分利用这种传感器-算法组合，我们将最先进的卷积架构适应于事件传感器的输出，并在一个公开的大规模事件相机数据集（约1000公里）上广泛评估了我们方法的性能。我们提供了定性和定量的解释，说明为什么事件相机即使在传统相机失败的情况下（例如具有挑战性的照明条件和快速运动）也能实现稳健的转向预测。最后，我们展示了从传统视觉到基于事件的视觉的迁移学习的优势，并表明我们的方法优于基于标准相机的最先进算法。","领域":"自动驾驶/事件相机/运动估计","问题":"预测自动驾驶汽车的转向角度","动机":"探索事件相机在自动驾驶汽车转向预测中的应用，特别是在传统相机难以应对的挑战性条件下","方法":"采用深度神经网络方法，将最先进的卷积架构适应于事件传感器的输出，并在大规模事件相机数据集上进行性能评估","关键词":["事件相机","自动驾驶","转向预测","深度神经网络","运动估计"],"涉及的技术概念":"事件相机是一种能够捕捉场景动态并过滤冗余信息的视觉传感器。本文通过深度神经网络方法，将卷积架构适应于事件传感器的输出，用于预测自动驾驶汽车的转向角度。研究还探讨了在挑战性照明条件和快速运动等传统相机难以应对的情况下，事件相机如何实现稳健的转向预测。此外，本文还展示了从传统视觉到基于事件的视觉的迁移学习的优势。"},{"order":561,"title":"Learning Answer Embeddings for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.html","abstract":"We propose a novel probabilistic model for visual question answering (Visual QA). The key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers. The learning objective is to learn the best parameterization of those embeddings such that the correct answer has higher likelihood among all possible answers. In contrast to several existing approaches of treating Visual QA as multi-way classification, the proposed approach takes the semantic relationships (as characterized by the embeddings) among answers into consideration, instead of viewing them as independent ordinal numbers. Thus, the learned embedded function can  be used to embed unseen answers (in the training dataset). These properties make the approach particularly appealing for transfer learning for open-ended Visual QA, where the source dataset on which the model is learned has limited overlapping with the target dataset in the space of answers. We have also developed large-scale optimization techniques for applying the model to datasets with a large number of answers, where the challenge is to properly normalize the proposed probabilistic models. We validate our approach on several Visual QA datasets and investigate its utility for transferring models across datasets. The empirical results have shown that the approach  performs well not only on in-domain learning but also on transfer learning.","中文标题":"学习视觉问答的答案嵌入","摘要翻译":"我们提出了一种新颖的概率模型用于视觉问答（Visual QA）。关键思想是推断两组嵌入：一组用于图像和问题的联合表示，另一组用于答案。学习目标是学习这些嵌入的最佳参数化，使得在所有可能的答案中，正确答案具有更高的可能性。与将Visual QA视为多路分类的几种现有方法相比，所提出的方法考虑了答案之间的语义关系（由嵌入表征），而不是将它们视为独立的序数。因此，学习到的嵌入函数可以用于嵌入未见过的答案（在训练数据集中）。这些特性使得该方法特别适用于开放式的Visual QA的迁移学习，其中模型学习的源数据集在答案空间上与目标数据集的重叠有限。我们还开发了大规模优化技术，以便将模型应用于具有大量答案的数据集，其中的挑战是正确规范化所提出的概率模型。我们在几个Visual QA数据集上验证了我们的方法，并研究了其在跨数据集迁移模型中的效用。实证结果表明，该方法不仅在域内学习上表现良好，而且在迁移学习上也表现出色。","领域":"视觉问答/迁移学习/嵌入学习","问题":"如何在视觉问答中有效地嵌入答案，以考虑答案之间的语义关系，并支持迁移学习。","动机":"现有的视觉问答方法通常将问题视为多路分类，忽略了答案之间的语义关系，这限制了模型在开放式问答和迁移学习中的应用。","方法":"提出了一种新颖的概率模型，通过推断图像和问题的联合嵌入以及答案的嵌入，学习这些嵌入的最佳参数化，以考虑答案之间的语义关系，并开发了大规模优化技术以处理大量答案的数据集。","关键词":["视觉问答","嵌入学习","迁移学习","概率模型","语义关系"],"涉及的技术概念":"嵌入（Embeddings）是指将数据（如图像、问题、答案）转换为低维空间中的向量表示，以便于计算语义相似度。概率模型（Probabilistic Model）是一种统计模型，用于描述变量之间的概率关系。迁移学习（Transfer Learning）是指将一个领域（源领域）学到的知识应用到另一个领域（目标领域）的过程。"},{"order":562,"title":"Good View Hunting: Learning Photo Composition From Dense View Pairs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Good_View_Hunting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Good_View_Hunting_CVPR_2018_paper.html","abstract":"Finding views with good photo composition is a challenging task for machine learning methods.  A key difficulty is the lack of well annotated large scale datasets. Most existing datasets only provide a limited number of annotations for good views, while ignoring the comparative nature of view selection. In this work, we present the first large scale Comparative Photo Composition dataset, which contains over one million comparative view pairs annotated using a cost-effective crowdsourcing workflow. We show that these comparative view annotations are essential for training a robust neural network model for composition. In addition, we propose a novel knowledge transfer framework to train a fast view proposal network, which runs at 75+ FPS and achieves state-of-the-art performance in image cropping and thumbnail generation tasks on three benchmark datasets. The superiority of our method is also demonstrated in a user study on a challenging experiment, where our method significantly outperforms the baseline methods in producing diversified well-composed views.","中文标题":"好景捕捉：从密集视图对中学习照片构图","摘要翻译":"寻找具有良好照片构图的视图对于机器学习方法来说是一个具有挑战性的任务。一个关键困难是缺乏良好注释的大规模数据集。大多数现有数据集仅提供有限数量的良好视图注释，而忽略了视图选择的比较性质。在这项工作中，我们提出了第一个大规模的比较照片构图数据集，该数据集包含超过一百万对使用成本效益高的众包工作流程注释的比较视图对。我们展示了这些比较视图注释对于训练一个稳健的神经网络模型进行构图是必不可少的。此外，我们提出了一个新颖的知识转移框架来训练一个快速的视图提议网络，该网络以75+ FPS的速度运行，并在三个基准数据集上的图像裁剪和缩略图生成任务中实现了最先进的性能。我们的方法的优越性也在一个具有挑战性的实验的用户研究中得到了证明，其中我们的方法在生成多样化的良好构图视图方面显著优于基线方法。","领域":"照片构图/图像裁剪/缩略图生成","问题":"缺乏良好注释的大规模数据集来训练机器学习模型以识别和生成良好构图的照片","动机":"解决现有数据集在视图选择比较性质上的不足，以及提高图像裁剪和缩略图生成任务的性能","方法":"提出了一个大规模的比较照片构图数据集，并开发了一个新颖的知识转移框架来训练一个快速的视图提议网络","关键词":["照片构图","图像裁剪","缩略图生成","比较视图对","知识转移框架","视图提议网络"],"涉及的技术概念":"比较照片构图数据集、知识转移框架、视图提议网络、图像裁剪、缩略图生成"},{"order":563,"title":"CleanNet: Transfer Learning for Scalable Image Classifier Training With Label Noise","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_CleanNet_Transfer_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lee_CleanNet_Transfer_Learning_CVPR_2018_paper.html","abstract":"In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5% compared to current weakly supervised methods. It also achieves 47% of the performance gain of verifying all images with only 3.2% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.","中文标题":"CleanNet: 用于可扩展图像分类器训练的标签噪声转移学习","摘要翻译":"在本文中，我们研究了在标签噪声存在的情况下学习图像分类模型的问题。依赖人工监督的现有方法通常不可扩展，因为手动识别正确或错误的标签是耗时的，而不依赖人工监督的方法虽然可扩展但效果较差。为了减少标签噪声清理所需的人工监督量，我们引入了CleanNet，一个联合神经嵌入网络，它只需要手动验证一小部分类别，以提供可以转移到其他类别的标签噪声知识。我们进一步将CleanNet和传统的卷积神经网络分类器集成到一个框架中，用于图像分类学习。我们在几个大规模数据集上展示了所提出算法在标签噪声检测任务和噪声数据上的图像分类任务中的有效性。实验结果表明，与当前弱监督方法相比，CleanNet可以在没有人工监督的情况下，将保留类别的标签噪声检测错误率降低41.5%。在图像分类任务中，仅验证3.2%的图像就实现了47%的性能提升。源代码和数据集将在kuanghuei.github.io/CleanNetProject上提供。","领域":"图像分类/标签噪声处理/转移学习","问题":"在标签噪声存在的情况下学习图像分类模型","动机":"减少标签噪声清理所需的人工监督量，提高图像分类模型的学习效率和效果","方法":"引入CleanNet，一个联合神经嵌入网络，仅需手动验证一小部分类别，以提供可以转移到其他类别的标签噪声知识，并将其与传统的卷积神经网络分类器集成到一个框架中","关键词":["图像分类","标签噪声","转移学习","神经嵌入网络"],"涉及的技术概念":"CleanNet是一种联合神经嵌入网络，用于减少标签噪声清理所需的人工监督量。它通过仅手动验证一小部分类别来提供标签噪声知识，这些知识可以转移到其他类别。此外，CleanNet与传统的卷积神经网络分类器集成，用于图像分类学习。"},{"order":564,"title":"Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Independently_Recurrent_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Independently_Recurrent_Neural_CVPR_2018_paper.html","abstract":"Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.","中文标题":"独立循环神经网络（IndRNN）：构建更长更深的RNN","摘要翻译":"循环神经网络（RNNs）已被广泛用于处理序列数据。然而，由于众所周知的梯度消失和爆炸问题，RNNs通常难以训练，并且难以学习长期模式。长短期记忆（LSTM）和门控循环单元（GRU）被开发来解决这些问题，但使用双曲正切和sigmoid激活函数会导致梯度随层数衰减。因此，构建一个可有效训练的深度网络具有挑战性。此外，RNN层中的所有神经元都纠缠在一起，它们的行为难以解释。为了解决这些问题，本文提出了一种新型的RNN，称为独立循环神经网络（IndRNN），其中同一层中的神经元彼此独立，并且它们跨层连接。我们已经证明，IndRNN可以很容易地调节以防止梯度爆炸和消失问题，同时允许网络学习长期依赖关系。此外，IndRNN可以与未饱和的激活函数（如relu（整流线性单元））一起工作，并且仍然可以稳健地训练。多个IndRNN可以堆叠以构建比现有RNNs更深的网络。实验结果表明，所提出的IndRNN能够处理非常长的序列（超过5000个时间步长），可以用于构建非常深的网络（实验中使用了21层），并且仍然可以稳健地训练。与传统的RNN和LSTM相比，使用IndRNNs在各种任务上取得了更好的性能。","领域":"序列数据处理/神经网络架构/深度学习优化","问题":"解决循环神经网络（RNNs）在训练过程中遇到的梯度消失和爆炸问题，以及难以学习长期模式的问题","动机":"为了构建一个能够有效训练、学习长期依赖关系并且易于解释的深度循环神经网络","方法":"提出了一种新型的独立循环神经网络（IndRNN），其中同一层中的神经元彼此独立，并且跨层连接，使用非饱和激活函数如relu，并展示了如何通过堆叠多个IndRNN来构建更深的网络","关键词":["独立循环神经网络","梯度消失","梯度爆炸","长期依赖","非饱和激活函数"],"涉及的技术概念":{"循环神经网络（RNNs）":"一种用于处理序列数据的神经网络，能够利用其内部状态（记忆）来处理输入序列","梯度消失和爆炸问题":"在训练深层神经网络时，梯度可能会变得非常小（消失）或非常大（爆炸），导致训练困难","长短期记忆（LSTM）":"一种特殊的RNN，能够学习长期依赖关系，通过引入门控机制来控制信息的流动","门控循环单元（GRU）":"另一种RNN变体，类似于LSTM，但结构更简单，通过门控机制来调节信息流","独立循环神经网络（IndRNN）":"本文提出的新型RNN，其中同一层中的神经元彼此独立，解决了传统RNN中的一些问题","非饱和激活函数":"如relu（整流线性单元），这类激活函数不会导致梯度随层数衰减，有助于训练深层网络"}},{"order":565,"title":"Mix and Match Networks: Encoder-Decoder Alignment for Zero-Pair Image Translation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Mix_and_Match_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Mix_and_Match_CVPR_2018_paper.html","abstract":"We address the problem of image translation between domains or modalities for which no direct paired data is available (i.e. zero-pair translation). We propose mix and match networks, based on multiple encoders and decoders aligned in such a way that other encoder-decoder pairs can be composed at test time to perform unseen image translation tasks between domains or modalities for which explicit paired samples were not seen during training. We study the impact of autoencoders, side information and losses in improving the alignment and transferability of trained pairwise translation models to unseen translations. We show our approach is scalable and can perform colorization and style transfer between unseen combinations of domains. We evaluate our system in a challenging cross-modal setting where semantic segmentation is estimated from depth images, without explicit access to any depth-semantic segmentation training pairs. Our model outperforms baselines based on pix2pix and CycleGAN models.","中文标题":"混合与匹配网络：用于零对图像翻译的编码器-解码器对齐","摘要翻译":"我们解决了在没有直接配对数据可用的情况下（即零对翻译）进行领域或模态间图像翻译的问题。我们提出了基于多个编码器和解码器的混合与匹配网络，这些编码器和解码器以这样一种方式对齐，即在测试时可以组合其他编码器-解码器对，以执行在训练期间未见过的领域或模态间的图像翻译任务。我们研究了自编码器、辅助信息和损失在提高训练好的成对翻译模型对齐和可转移性到未见翻译任务中的影响。我们展示了我们的方法是可扩展的，并且可以在未见过的领域组合之间执行着色和风格转移。我们在一个具有挑战性的跨模态设置中评估了我们的系统，其中从深度图像估计语义分割，而无需显式访问任何深度-语义分割训练对。我们的模型优于基于pix2pix和CycleGAN模型的基线。","领域":"图像翻译/跨模态学习/语义分割","问题":"在没有直接配对数据的情况下进行领域或模态间的图像翻译","动机":"探索在没有显式配对样本的情况下，如何有效地进行图像翻译，特别是在跨模态和领域间进行翻译","方法":"提出了一种基于多个编码器和解码器的混合与匹配网络，通过自编码器、辅助信息和特定损失函数来提高模型的对齐和可转移性","关键词":["图像翻译","跨模态学习","语义分割","自编码器","风格转移"],"涉及的技术概念":"自编码器用于提高模型的对齐能力；辅助信息和特定损失函数用于增强模型的可转移性；混合与匹配网络允许在测试时组合不同的编码器-解码器对，以执行未见过的翻译任务"},{"order":566,"title":"Structured Uncertainty Prediction Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.html","abstract":"This paper is the first work to propose a network to predict a structured uncertainty distribution for a synthesized image. Previous approaches have been mostly limited to predicting diagonal covariance matrices. Our novel model learns to predict a full Gaussian covariance matrix for each reconstruction, which permits efficient sampling and likelihood evaluation.  We demonstrate that our model can accurately reconstruct ground truth correlated residual distributions for synthetic datasets and generate plausible high frequency samples for real face images.  We also illustrate the use of these predicted covariances for structure preserving image denoising.","中文标题":"结构化不确定性预测网络","摘要翻译":"本文是首次提出一种网络来预测合成图像的结构化不确定性分布的工作。以往的方法大多局限于预测对角协方差矩阵。我们的新模型学会为每次重建预测一个完整的高斯协方差矩阵，这允许有效的采样和似然评估。我们证明了我们的模型能够准确重建合成数据集的地面真实相关残差分布，并为真实人脸图像生成合理的高频样本。我们还展示了这些预测的协方差在结构保持图像去噪中的应用。","领域":"图像合成/图像去噪/高斯模型","问题":"预测合成图像的结构化不确定性分布","动机":"以往的方法大多局限于预测对角协方差矩阵，无法准确重建地面真实相关残差分布和生成合理的高频样本。","方法":"提出一种新模型，学会为每次重建预测一个完整的高斯协方差矩阵，允许有效的采样和似然评估。","关键词":["结构化不确定性","高斯协方差矩阵","图像去噪"],"涉及的技术概念":"结构化不确定性分布指的是图像合成过程中对不确定性进行建模，以预测图像的不确定性。高斯协方差矩阵是一种统计工具，用于描述多维随机变量之间的线性关系。图像去噪是指从噪声图像中恢复原始图像的过程，结构保持图像去噪则是在去噪过程中尽量保持图像的原有结构信息。"},{"order":567,"title":"Between-Class Learning for Image Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tokozume_Between-Class_Learning_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tokozume_Between-Class_Learning_for_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning). We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.","中文标题":"类间学习用于图像分类","摘要翻译":"在本文中，我们提出了一种新颖的图像分类学习方法，称为类间学习（BC学习）。我们通过以随机比例混合属于不同类别的两张图像来生成类间图像。然后，我们将混合图像输入模型，并训练模型以输出混合比例。BC学习有能力对特征分布的形状施加约束，从而提高了泛化能力。BC学习最初是为可以数字混合的声音开发的方法。混合两个图像数据似乎没有意义；然而，我们认为，由于卷积神经网络有将输入数据视为波形的一面，对声音有效的方法也必然对图像有效。首先，我们提出了一种使用内部分割的简单混合方法，这意外地显著提高了性能。其次，我们提出了一种将图像视为波形的混合方法，这进一步提高了性能。结果，我们在ImageNet-1K和CIFAR-10上分别实现了19.4%和2.26%的top-1错误率。","领域":"图像分类/卷积神经网络/特征学习","问题":"提高图像分类模型的泛化能力","动机":"探索通过混合不同类别的图像来训练模型，以提高模型的泛化能力","方法":"提出类间学习（BC学习），通过混合不同类别的图像并训练模型预测混合比例，以及将图像视为波形进行混合的方法","关键词":["图像分类","类间学习","卷积神经网络","特征分布","泛化能力"],"涉及的技术概念":"类间学习（BC学习）是一种通过混合不同类别的图像来训练图像分类模型的方法，旨在通过约束特征分布的形状来提高模型的泛化能力。该方法借鉴了声音处理中的技术，认为卷积神经网络处理图像数据的方式与处理波形数据相似，因此适用于图像分类。提出的两种混合方法包括使用内部分割的简单混合方法和将图像视为波形的混合方法，均显著提高了模型性能。"},{"order":568,"title":"Adversarial Feature Augmentation for Unsupervised Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Volpi_Adversarial_Feature_Augmentation_CVPR_2018_paper.html","abstract":"Recent works showed that Generative Adversarial Networks (GANs) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples. In particular, it was shown that a GAN objective function can be used to learn target features indistinguishable from the source ones. In this work, we extend this framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation. While data augmentation in the image space is a well established technique in deep learning, feature augmentation has not yet received the same level of attention. We accomplish it by means of a feature generator trained by playing the GAN minimax game against source features. Results show that both enforcing domain-invariance and performing feature augmentation lead to superior or comparable performance to state-of-the-art results in several unsupervised domain adaptation benchmarks.","中文标题":"对抗性特征增强用于无监督领域适应","摘要翻译":"最近的研究表明，生成对抗网络（GANs）可以成功地应用于无监督领域适应，其中，给定一个标记的源数据集和一个未标记的目标数据集，目标是训练强大的分类器以处理目标样本。特别是，研究表明，GAN的目标函数可以用来学习与源特征无法区分的特征。在这项工作中，我们通过（i）强制学习的特征提取器具有领域不变性，以及（ii）通过在特征空间中进行数据增强来训练它，即执行特征增强，来扩展这一框架。虽然图像空间中的数据增强是深度学习中的一项成熟技术，但特征增强尚未受到同等的关注。我们通过一个特征生成器来实现这一点，该生成器通过对抗源特征来训练GAN的极小极大游戏。结果表明，在几个无监督领域适应基准测试中，强制领域不变性和执行特征增强都导致了优于或与最先进结果相当的性能。","领域":"无监督学习/领域适应/特征学习","问题":"如何在无监督领域适应中训练强大的分类器以处理目标样本","动机":"提高无监督领域适应中分类器的性能，通过特征增强和领域不变性来增强模型的泛化能力","方法":"通过生成对抗网络（GANs）进行特征增强，强制特征提取器具有领域不变性，并在特征空间中进行数据增强","关键词":["无监督学习","领域适应","特征增强","生成对抗网络"],"涉及的技术概念":"生成对抗网络（GANs）用于学习与源特征无法区分的特征，特征增强通过在特征空间中进行数据增强来实现，领域不变性通过强制特征提取器来实现"},{"order":569,"title":"Generative Image Inpainting With Contextual Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Generative_Image_Inpainting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Generative_Image_Inpainting_CVPR_2018_paper.html","abstract":"Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.","中文标题":"基于上下文注意力的生成图像修复","摘要翻译":"最近，基于深度学习的方法在修复图像中大面积缺失区域的挑战性任务中显示出有希望的结果。这些方法可以生成视觉上合理的图像结构和纹理，但经常会产生与周围区域不一致的扭曲结构或模糊纹理。这主要是由于卷积神经网络在显式借用或复制远处空间位置信息方面的无效性。另一方面，传统的纹理和块合成方法在需要从周围区域借用纹理时特别适用。受这些观察的启发，我们提出了一种新的基于深度生成模型的方法，该方法不仅可以合成新的图像结构，还可以在网络训练期间显式利用周围图像特征作为参考，以做出更好的预测。该模型是一个前馈的、完全卷积的神经网络，可以在测试时处理具有多个孔洞、位置任意且大小可变的图像。在包括人脸（CelebA、CelebA-HQ）、纹理（DTD）和自然图像（ImageNet、Places2）在内的多个数据集上的实验表明，我们提出的方法生成的修复结果质量高于现有方法。代码、演示和模型可在https://github.com/JiahuiYu/generative_inpainting获取。","领域":"图像修复/生成模型/卷积神经网络","问题":"修复图像中大面积缺失区域时产生的扭曲结构或模糊纹理问题","动机":"解决卷积神经网络在显式借用或复制远处空间位置信息方面的无效性，以及传统纹理和块合成方法在借用周围区域纹理时的适用性","方法":"提出了一种新的基于深度生成模型的方法，该方法不仅可以合成新的图像结构，还可以在网络训练期间显式利用周围图像特征作为参考，以做出更好的预测","关键词":["图像修复","生成模型","卷积神经网络","上下文注意力"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。生成模型是指能够生成新数据的模型，如图像修复中的新图像结构。上下文注意力机制是一种技术，允许模型在处理图像时显式地关注和利用周围区域的信息，以提高修复质量。"},{"order":570,"title":"CSGNet: Neural Shape Parser for Constructive Solid Geometry","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sharma_CSGNet_Neural_Shape_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sharma_CSGNet_Neural_Shape_CVPR_2018_paper.html","abstract":"We present a neural architecture that takes as input a 2D or 3D shape and outputs a program that generates the shape. The instructions in our program are based on constructive solid geometry principles, i.e., a set of boolean operations on shape primitives defined recursively. Bottom-up techniques for this shape parsing task rely on primitive detection and are inherently slow since the search space over possible primitive combinations is large. In contrast, our model uses a recurrent neural network that parses the input shape  in a top-down manner, which is significantly faster and yields a compact and easy-to-interpret sequence of modeling instructions. Our model is also more effective as a shape detector compared to existing state-of-the-art detection techniques. We finally demonstrate that our network can be trained on novel datasets without ground-truth program annotations through policy gradient techniques.","中文标题":"CSGNet: 用于构造实体几何的神经形状解析器","摘要翻译":"我们提出了一种神经架构，该架构以2D或3D形状作为输入，并输出生成该形状的程序。我们程序中的指令基于构造实体几何原理，即对递归定义的形状基元进行一组布尔操作。对于这种形状解析任务，自下而上的技术依赖于基元检测，并且由于可能的基元组合的搜索空间很大，因此本质上很慢。相比之下，我们的模型使用递归神经网络以自上而下的方式解析输入形状，这种方式显著更快，并产生一个紧凑且易于解释的建模指令序列。与现有的最先进的检测技术相比，我们的模型作为形状检测器也更有效。最后，我们证明了我们的网络可以通过策略梯度技术在没有地面真实程序注释的新数据集上进行训练。","领域":"几何建模/形状解析/神经网络","问题":"如何高效解析2D或3D形状并生成相应的构造实体几何程序","动机":"现有的自下而上的形状解析技术效率低下，因为它们依赖于基元检测，且搜索空间大，导致解析速度慢","方法":"采用递归神经网络以自上而下的方式解析输入形状，生成紧凑且易于解释的建模指令序列","关键词":["构造实体几何","形状解析","递归神经网络","策略梯度"],"涉及的技术概念":"构造实体几何（CSG）是一种通过布尔操作（如并集、交集和差集）组合简单形状基元来构建复杂形状的技术。递归神经网络（RNN）是一种能够处理序列数据的神经网络，适用于解析任务。策略梯度是一种强化学习技术，用于在没有明确标签的情况下训练模型。"},{"order":571,"title":"Conditional Image-to-Image Translation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Conditional_Image-to-Image_Translation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Conditional_Image-to-Image_Translation_CVPR_2018_paper.html","abstract":"Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs) and dual learning. However, existing models lack the ability to control the translated results in the target domain and their results usually lack of diversity in the sense that a fixed image usually leads to (almost) deterministic translation result. In this paper, we study a new problem, conditional image-to-image translation, which is to translate an image from the source domain to the target domain conditioned on a given image in the target domain. It requires that the generated image should inherit some domain-specific features of the conditional image from the target domain.  Therefore, changing the conditional image in the target domain will lead to diverse translation results for a fixed input image from the source domain, and therefore the conditional input image helps to control the translation results. We tackle this problem with unpaired data based on GANs and dual learning. We twist two conditional translation models (one translation from A domain to B domain, and the other one from B domain to A domain) together for inputs combination and reconstruction while preserving domain independent features. We carry out experiments on men's faces from-to women's faces translation and edges to shoes and bags translations. The results demonstrate the effectiveness of our proposed method.","中文标题":"条件图像到图像翻译","摘要翻译":"图像到图像翻译任务已经广泛研究了生成对抗网络（GANs）和双重学习。然而，现有模型缺乏在目标域中控制翻译结果的能力，并且它们的结果通常缺乏多样性，即固定图像通常导致（几乎）确定性的翻译结果。在本文中，我们研究了一个新问题，条件图像到图像翻译，即将图像从源域翻译到目标域，条件是目标域中的给定图像。它要求生成的图像应继承目标域中条件图像的一些域特定特征。因此，改变目标域中的条件图像将导致源域中固定输入图像的多样化翻译结果，因此条件输入图像有助于控制翻译结果。我们基于GANs和双重学习处理这个问题，使用未配对数据。我们将两个条件翻译模型（一个从A域到B域的翻译，另一个从B域到A域的翻译）扭曲在一起，用于输入组合和重建，同时保留域独立特征。我们在男性面孔到女性面孔的翻译以及边缘到鞋子和包的翻译上进行了实验。结果证明了我们提出的方法的有效性。","领域":"图像翻译/生成对抗网络/双重学习","问题":"现有图像到图像翻译模型缺乏在目标域中控制翻译结果的能力，并且结果缺乏多样性。","动机":"研究如何通过条件输入图像控制图像到图像翻译的结果，以增加翻译结果的多样性。","方法":"基于生成对抗网络（GANs）和双重学习，使用未配对数据，将两个条件翻译模型扭曲在一起，用于输入组合和重建，同时保留域独立特征。","关键词":["图像翻译","生成对抗网络","双重学习","条件翻译"],"涉及的技术概念":"生成对抗网络（GANs）是一种深度学习模型，由生成器和判别器组成，用于生成新的数据实例。双重学习是一种学习策略，通过两个相互依赖的任务来提高学习效率。条件图像到图像翻译是指在翻译过程中，使用目标域中的给定图像作为条件，以控制翻译结果。"},{"order":572,"title":"Continuous Relaxation of MAP Inference: A Nonconvex Perspective","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Le-Huu_Continuous_Relaxation_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Le-Huu_Continuous_Relaxation_of_CVPR_2018_paper.html","abstract":"In this paper, we study a nonconvex continuous relaxation of MAP inference in discrete Markov random fields (MRFs). We show that for arbitrary MRFs, this relaxation is tight, and a discrete stationary point of it can be easily reached by a simple block coordinate descent algorithm. In addition, we study the resolution of this relaxation using popular gradient methods, and further propose a more effective solution using a multilinear decomposition framework based on the alternating direction method of multipliers (ADMM). Experiments on many real-world problems demonstrate that the proposed ADMM significantly outperforms other nonconvex relaxation based methods, and compares favorably with state of the art MRF optimization algorithms in different settings.","中文标题":"MAP推断的连续松弛：非凸视角","摘要翻译":"在本文中，我们研究了离散马尔可夫随机场（MRFs）中MAP推断的非凸连续松弛。我们展示了对于任意MRFs，这种松弛是紧的，并且通过简单的块坐标下降算法可以轻松达到其离散静止点。此外，我们研究了使用流行的梯度方法解决这种松弛，并进一步提出了一种基于交替方向乘子法（ADMM）的多线性分解框架的更有效解决方案。在许多实际问题的实验中，所提出的ADMM显著优于其他基于非凸松弛的方法，并在不同设置中与最先进的MRF优化算法相比表现良好。","领域":"马尔可夫随机场/优化算法/非凸优化","问题":"解决离散马尔可夫随机场中MAP推断的非凸连续松弛问题","动机":"探索更有效的非凸连续松弛方法，以提高MAP推断的准确性和效率","方法":"采用块坐标下降算法和基于交替方向乘子法（ADMM）的多线性分解框架","关键词":["MAP推断","非凸优化","马尔可夫随机场","交替方向乘子法","块坐标下降"],"涉及的技术概念":{"MAP推断":"最大后验概率推断，用于在给定观测数据的情况下估计最可能的未观测变量状态","非凸优化":"处理目标函数或约束条件为非凸的优化问题","马尔可夫随机场":"一种用于建模随机变量之间依赖关系的图模型","交替方向乘子法":"一种用于解决可分解优化问题的迭代算法","块坐标下降":"一种优化算法，通过交替固定其他变量来优化一个变量或一组变量"}},{"order":573,"title":"Feature Generating Networks for Zero-Shot Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_Feature_Generating_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_Feature_Generating_Networks_CVPR_2018_paper.html","abstract":"Suffering from the extreme training data imbalance between seen and unseen classes,  most of existing state-of-the-art approaches fail to achieve satisfactory results for the challenging generalized zero-shot learning task. To circumvent the need for labeled examples of unseen classes, we propose a novel generative adversarial network(GAN) that synthesizes CNN features conditioned on class-level semantic information, offering a shortcut directly from a semantic descriptor of a class to a class-conditional feature distribution. Our proposed approach, pairing a Wasserstein GAN with a classification loss, is able to generate sufficiently discriminative CNN features to train softmax classifiers or any multimodal embedding method. Our experimental results demonstrate a significant boost in accuracy over the state of the art on five challenging datasets -- CUB, FLO, SUN, AWA and ImageNet -- in both the zero-shot learning and generalized zero-shot learning settings.","中文标题":"特征生成网络用于零样本学习","摘要翻译":"由于可见类与未见类之间极端的训练数据不平衡，大多数现有的最先进方法在挑战性的广义零样本学习任务中未能取得令人满意的结果。为了规避对未见类标记示例的需求，我们提出了一种新颖的生成对抗网络（GAN），该网络根据类级语义信息合成CNN特征，提供了一条从类的语义描述符直接到类条件特征分布的捷径。我们提出的方法将Wasserstein GAN与分类损失配对，能够生成足够区分的CNN特征来训练softmax分类器或任何多模态嵌入方法。我们的实验结果表明，在五个具有挑战性的数据集——CUB、FLO、SUN、AWA和ImageNet——上，无论是在零样本学习还是广义零样本学习设置中，准确率都有了显著提升。","领域":"零样本学习/生成对抗网络/特征生成","问题":"解决广义零样本学习任务中由于训练数据不平衡导致的性能不佳问题","动机":"为了规避对未见类标记示例的需求，提高零样本学习的准确率","方法":"提出了一种新颖的生成对抗网络（GAN），该网络根据类级语义信息合成CNN特征，并将Wasserstein GAN与分类损失配对","关键词":["零样本学习","生成对抗网络","特征生成","Wasserstein GAN","分类损失"],"涉及的技术概念":"生成对抗网络（GAN）是一种深度学习模型，通过对抗过程估计生成模型。Wasserstein GAN是GAN的一种变体，旨在解决传统GAN训练不稳定和模式崩溃的问题。CNN特征指的是通过卷积神经网络提取的特征，这些特征对于图像识别等任务非常重要。类级语义信息指的是描述类别的语义属性或描述符，用于指导特征生成。"},{"order":574,"title":"Joint Optimization Framework for Learning With Noisy Labels","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tanaka_Joint_Optimization_Framework_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tanaka_Joint_Optimization_Framework_CVPR_2018_paper.html","abstract":"Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.","中文标题":"联合优化框架用于带噪声标签的学习","摘要翻译":"在大规模数据集上训练的深度神经网络（DNNs）在图像分类方面表现出显著的性能。然而，许多从网站收集的大规模数据集往往包含不准确的标签，这些标签被称为噪声标签。在这样的噪声标签数据集上训练会导致性能下降，因为DNNs容易过拟合到噪声标签。为了克服这个问题，我们提出了一个联合优化框架，用于学习DNN参数和估计真实标签。我们的框架可以通过交替更新网络参数和标签来在训练过程中纠正标签。我们在噪声CIFAR-10数据集和Clothing1M数据集上进行了实验。结果表明，我们的方法显著优于其他最先进的方法。","领域":"图像分类/噪声标签处理/深度学习优化","问题":"解决在含有噪声标签的数据集上训练深度神经网络导致的性能下降问题","动机":"由于从网站收集的大规模数据集往往包含不准确的噪声标签，直接在这些数据集上训练深度神经网络会导致模型过拟合到噪声标签，从而影响模型的性能。","方法":"提出一个联合优化框架，通过交替更新网络参数和标签来纠正训练过程中的噪声标签，从而估计出真实标签。","关键词":["噪声标签","联合优化","图像分类","深度神经网络","标签纠正"],"涉及的技术概念":"深度神经网络（DNNs）：一种模拟人脑神经网络结构和功能的计算模型，用于处理复杂的模式识别和分类任务。噪声标签：指数据集中不准确或错误的标签，可能由于数据收集过程中的错误或标注错误导致。联合优化框架：一种同时优化多个目标或参数的框架，在这里指同时优化网络参数和标签估计。CIFAR-10和Clothing1M数据集：两个常用的图像分类数据集，其中CIFAR-10包含10类共60000张32x32的彩色图像，Clothing1M是一个大规模服装图像数据集。"},{"order":575,"title":"Convolutional Image Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.html","abstract":"Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the LSTM baseline, while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.","中文标题":"卷积图像描述","摘要翻译":"图像描述是一项重要的任务，适用于虚拟助手、编辑工具、图像索引和支持残疾人。近年来，在使用由长短期记忆（LSTM）单元驱动的递归神经网络进行图像描述方面取得了显著进展。尽管缓解了梯度消失问题，并且尽管LSTM单元具有记忆依赖性的引人注目的能力，但LSTM单元复杂且本质上在时间上是顺序的。为了解决这个问题，最近的工作显示了卷积网络在机器翻译和条件图像生成中的优势。受到它们成功的启发，在本文中，我们开发了一种卷积图像描述技术。我们在具有挑战性的MSCOCO数据集上展示了其有效性，并展示了与LSTM基线相当的性能，同时每个参数的训练时间更快。我们还进行了详细的分析，提供了支持卷积语言生成方法的引人注目的理由。","领域":"图像描述/自然语言处理/卷积神经网络","问题":"解决图像描述任务中LSTM单元复杂性和顺序性问题","动机":"LSTM单元虽然有效，但复杂且顺序性强，限制了图像描述任务的效率和性能","方法":"开发了一种卷积图像描述技术，利用卷积网络的优势进行图像描述","关键词":["图像描述","卷积神经网络","LSTM","MSCOCO数据集"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征","LSTM":"长短期记忆网络，一种特殊的递归神经网络，能够学习长期依赖信息","MSCOCO数据集":"一个广泛使用的图像描述和对象识别数据集，包含超过20万张标注图像"}},{"order":576,"title":"AON: Towards Arbitrarily-Oriented Text Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_AON_Towards_Arbitrarily-Oriented_CVPR_2018_paper.html","abstract":"Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring research of several decades on optical character recognition (OCR), recognizing texts from natural images is still a challenging task. This is because scene texts are often in irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed in the literature. Existing methods on text recognition mainly work with regular (horizontal and frontal) texts and cannot be trivially generalized to handle irregular texts. In this paper, we develop the arbitrary orientation network (AON) to directly capture the deep features of irregular texts, which are combined into an attention-based decoder to generate character sequence. The whole network can be trained end-to-end by using only images and word-level annotations. Extensive experiments on various benchmarks, including the CUTE80, SVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed AON-based method achieves the-state-of-the-art performance in irregular datasets, and is comparable to major existing methods in regular datasets.","中文标题":"AON：面向任意方向文本识别","摘要翻译":"从自然图像中识别文本是计算机视觉中的一个热门研究课题，因为它有各种应用。尽管光学字符识别（OCR）经过几十年的持续研究，从自然图像中识别文本仍然是一个具有挑战性的任务。这是因为场景文本通常以不规则（例如弯曲、任意方向或严重扭曲）的排列方式出现，这在文献中尚未得到很好的解决。现有的文本识别方法主要适用于规则（水平和正面）文本，不能轻易推广到处理不规则文本。在本文中，我们开发了任意方向网络（AON）来直接捕捉不规则文本的深层特征，这些特征被结合到基于注意力的解码器中以生成字符序列。整个网络可以仅使用图像和单词级注释进行端到端训练。在各种基准测试上的广泛实验，包括CUTE80、SVT-Perspective、IIIT5k、SVT和ICDAR数据集，表明所提出的基于AON的方法在不规则数据集中实现了最先进的性能，并且在规则数据集中与主要现有方法相当。","领域":"文本识别/场景文本理解/光学字符识别","问题":"解决自然图像中不规则排列文本的识别问题","动机":"由于场景文本通常以不规则（如弯曲、任意方向或严重扭曲）的排列方式出现，现有的文本识别方法主要适用于规则文本，不能轻易推广到处理不规则文本，因此需要开发新的方法来直接捕捉不规则文本的深层特征。","方法":"开发了任意方向网络（AON）来直接捕捉不规则文本的深层特征，这些特征被结合到基于注意力的解码器中以生成字符序列。整个网络可以仅使用图像和单词级注释进行端到端训练。","关键词":["文本识别","场景文本理解","光学字符识别","不规则文本","任意方向网络"],"涉及的技术概念":{"任意方向网络（AON）":"一种直接捕捉不规则文本深层特征的网络结构。","基于注意力的解码器":"用于生成字符序列的解码器，它结合了AON捕捉的深层特征。","端到端训练":"整个网络可以仅使用图像和单词级注释进行训练，无需复杂的预处理或后处理步骤。"}},{"order":577,"title":"Wrapped Gaussian Process Regression on Riemannian Manifolds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mallasto_Wrapped_Gaussian_Process_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mallasto_Wrapped_Gaussian_Process_CVPR_2018_paper.html","abstract":"Gaussian process (GP) regression is a powerful tool in non-parametric regression providing uncertainty estimates. However, it is limited to data in vector spaces. In fields such as shape analysis and diffusion tensor imaging, the data often lies on a manifold, making GP regression non- viable, as the resulting predictive distribution does not live in the correct geometric space. We tackle the problem by defining wrapped Gaussian processes (WGPs) on Rieman- nian manifolds, using the probabilistic setting to general- ize GP regression to the context of manifold-valued targets. The method is validated empirically on diffusion weighted imaging (DWI) data, directional data on the sphere and in the Kendall shape space, endorsing WGP regression as an efficient and flexible tool for manifold-valued regression.","中文标题":"黎曼流形上的包裹高斯过程回归","摘要翻译":"高斯过程（GP）回归是非参数回归中提供不确定性估计的强大工具。然而，它仅限于向量空间中的数据。在形状分析和扩散张量成像等领域，数据通常位于流形上，这使得GP回归不可行，因为由此产生的预测分布不在正确的几何空间中。我们通过在黎曼流形上定义包裹高斯过程（WGPs）来解决这个问题，使用概率设置将GP回归推广到流形值目标的上下文中。该方法在扩散加权成像（DWI）数据、球面上的方向数据以及Kendall形状空间中的数据上进行了实证验证，证实了WGP回归作为流形值回归的高效灵活工具。","领域":"形状分析/扩散张量成像/流形学习","问题":"高斯过程回归在处理位于流形上的数据时不可行","动机":"为了将高斯过程回归推广到流形值目标的上下文中，以处理形状分析和扩散张量成像等领域的数据","方法":"在黎曼流形上定义包裹高斯过程（WGPs），使用概率设置将GP回归推广到流形值目标的上下文中","关键词":["高斯过程回归","黎曼流形","包裹高斯过程","扩散加权成像","Kendall形状空间"],"涉及的技术概念":{"高斯过程回归":"一种非参数回归方法，提供不确定性估计","黎曼流形":"一种具有黎曼度量的流形，用于研究几何和拓扑性质","包裹高斯过程":"在黎曼流形上定义的高斯过程，用于处理流形值数据","扩散加权成像":"一种磁共振成像技术，用于测量水分子在组织中的扩散","Kendall形状空间":"用于形状分析的一种数学空间，其中形状被视为点"}},{"order":578,"title":"Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gan_Geometry_Guided_Convolutional_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gan_Geometry_Guided_Convolutional_CVPR_2018_paper.html","abstract":"It is often laborious and costly to manually annotate videos for training high-quality video recognition models, so there has been some work and interest in exploring alternative, cheap, and yet often noisy and indirect, training signals for learning the video representations. However, these signals are still coarse, supplying supervision at the whole video frame level, and subtle, sometimes enforcing the learning agent to solve problems that are even hard for humans. In this paper, we instead explore geometry, a grand new type of auxiliary supervision for the self-supervised learning of video representations. In particular, we extract pixel-wise geometry information as flow fields and disparity maps from synthetic imagery and real 3D movies. Although the geometry and high-level semantics are seemingly distant topics, surprisingly, we find that the convolutional neural networks pre-trained by the geometry cues can be effectively adapted to semantic video understanding tasks. In addition, we also find that a progressive training strategy can foster a better neural network for the video recognition task than blindly pooling the distinct sources of geometry cues together.  Extensive results on video dynamic scene recognition and action recognition tasks show that our geometry guided networks significantly outperform the competing methods that are trained with other types of labeling-free supervision signals.","中文标题":"几何引导的卷积神经网络用于自监督视频表示学习","摘要翻译":"手动标注视频以训练高质量的视频识别模型通常既费力又昂贵，因此有一些工作和兴趣探索替代的、廉价的、但通常嘈杂和间接的训练信号来学习视频表示。然而，这些信号仍然粗糙，提供的是整个视频帧级别的监督，且微妙，有时迫使学习代理解决甚至对人类来说都困难的问题。在本文中，我们转而探索几何，这是一种用于自监督学习视频表示的全新类型的辅助监督。特别是，我们从合成图像和真实3D电影中提取像素级几何信息作为流场和视差图。尽管几何和高层语义看似是遥远的话题，但令人惊讶的是，我们发现通过几何线索预训练的卷积神经网络可以有效地适应语义视频理解任务。此外，我们还发现，渐进式训练策略比盲目地将不同的几何线索源汇集在一起更能培养出更好的视频识别任务的神经网络。在视频动态场景识别和动作识别任务上的广泛结果表明，我们的几何引导网络显著优于使用其他类型的无标签监督信号训练的竞争方法。","领域":"视频理解/自监督学习/几何信息处理","问题":"如何利用几何信息作为辅助监督信号来提高视频表示学习的质量","动机":"减少对昂贵且费力的手动标注视频的依赖，探索廉价且有效的训练信号","方法":"从合成图像和真实3D电影中提取像素级几何信息作为流场和视差图，用于预训练卷积神经网络，并采用渐进式训练策略","关键词":["视频表示学习","自监督学习","几何信息","卷积神经网络","渐进式训练"],"涉及的技术概念":"几何信息（流场和视差图）作为辅助监督信号，用于预训练卷积神经网络，并通过渐进式训练策略提高视频识别任务的性能。"},{"order":579,"title":"DiverseNet: When One Right Answer Is Not Enough","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Firman_DiverseNet_When_One_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Firman_DiverseNet_When_One_CVPR_2018_paper.html","abstract":"Many structured prediction tasks in machine vision have a collection of acceptable answers, instead of one definitive ground truth answer. Segmentation of images, for example, is subject to human labeling bias. Similarly, there are multiple possible pixel values that could plausibly complete occluded image regions. State-of-the art supervised learning methods are typically optimized to make a single test-time prediction for each query, failing to find other modes in the output space. Existing methods that allow for sampling often sacrifice speed or accuracy.  We introduce a simple method for training a neural network, which enables diverse structured predictions to be made for each test-time query. For a single input, we learn to predict a range of possible answers. We compare favorably to methods that seek diversity through an ensemble of networks. Such stochastic multiple choice learning faces mode collapse, where one or more ensemble members fail to receive any training signal. Our best performing solution can be deployed for various tasks, and just involves small modifications to the existing single-mode architecture, loss function, and training regime. We demonstrate that our method results in quantitative improvements across three challenging tasks: 2D image completion, 3D volume estimation, and flow prediction.","中文标题":"DiverseNet: 当一个正确答案不够时","摘要翻译":"在机器视觉中的许多结构化预测任务中，存在一系列可接受的答案，而不是一个确定性的真实答案。例如，图像分割受到人类标注偏见的影响。同样，有多个可能的像素值可以合理地完成被遮挡的图像区域。最先进的监督学习方法通常被优化为对每个查询做出单一测试时间预测，未能找到输出空间中的其他模式。现有的允许采样的方法往往牺牲速度或准确性。我们介绍了一种简单的训练神经网络的方法，使得可以对每个测试时间查询做出多样化的结构化预测。对于单个输入，我们学会预测一系列可能的答案。我们与通过网络集合寻求多样性的方法相比具有优势。这种随机多选学习面临模式崩溃的问题，其中一个或多个集合成员未能接收到任何训练信号。我们表现最佳的解决方案可以部署用于各种任务，并且仅涉及对现有单模式架构、损失函数和训练制度的小幅修改。我们证明了我们的方法在三个具有挑战性的任务中带来了定量改进：2D图像完成、3D体积估计和流预测。","领域":"图像分割/图像完成/3D重建","问题":"解决在结构化预测任务中，单一预测无法覆盖所有可接受答案的问题","动机":"现有的监督学习方法通常只能做出单一预测，无法找到输出空间中的其他模式，且现有允许采样的方法往往牺牲速度或准确性","方法":"介绍了一种简单的训练神经网络的方法，使得可以对每个测试时间查询做出多样化的结构化预测，通过小幅修改现有单模式架构、损失函数和训练制度来实现","关键词":["结构化预测","图像分割","图像完成","3D体积估计","流预测"],"涉及的技术概念":"结构化预测任务指的是在机器视觉中，对于每个输入，预测一个结构化的输出（如分割图、完成图等）。模式崩溃是指在多选学习中，某些模式（或答案）被忽略，导致多样性减少。"},{"order":580,"title":"Deep Face Detector Adaptation Without Negative Transfer or Catastrophic Forgetting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jamal_Deep_Face_Detector_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jamal_Deep_Face_Detector_CVPR_2018_paper.html","abstract":"Arguably, no single face detector fits all real-life scenarios. It is often desirable to have some built-in schemes for a face detector to automatically adapt, e.g., to a particular user's photo album (the target domain). We propose a novel face detector adaptation approach that works as long as there are representative images of the target domain no matter they are labeled or not and, more importantly, without the need of accessing the training data of the source domain. Our approach explicitly accounts for the notorious negative transfer caveat in domain adaptation thanks to a residual loss by design. Moreover, it does not incur catastrophic interference with the knowledge learned from the source domain and, therefore, the adapted face detectors maintain about the same performance as the old detectors in the original source domain. As such, our adaption approach to face detectors is analogous to the popular interpolation techniques for language models; it may opens a new direction for progressively training the face detectors domain by domain. We report extensive experimental results to verify our approach on two massively benchmarked face detectors.","中文标题":"无需负迁移或灾难性遗忘的深度人脸检测器适应","摘要翻译":"可以说，没有单一的人脸检测器适合所有现实生活场景。通常希望人脸检测器具有一些内置方案，以便自动适应，例如，适应特定用户的相册（目标域）。我们提出了一种新颖的人脸检测器适应方法，只要目标域有代表性图像，无论是否标记，更重要的是，无需访问源域的训练数据，该方法即可工作。我们的方法通过设计中的残差损失，明确考虑了领域适应中臭名昭著的负迁移问题。此外，它不会对从源域学到的知识造成灾难性干扰，因此，适应后的人脸检测器在原始源域中保持与旧检测器大致相同的性能。因此，我们的人脸检测器适应方法类似于语言模型的流行插值技术；它可能为逐步训练人脸检测器开辟了新的方向。我们报告了广泛的实验结果，以验证我们在两个大规模基准测试的人脸检测器上的方法。","领域":"人脸检测/领域适应/残差学习","问题":"如何使单一的人脸检测器适应不同的现实生活场景，同时避免负迁移和灾难性遗忘的问题","动机":"现实生活场景多样，单一的人脸检测器难以适应所有场景，需要一种方法使其能够自动适应特定场景，同时保持原有性能","方法":"提出了一种新颖的人脸检测器适应方法，通过设计中的残差损失来避免负迁移，同时不造成灾难性遗忘，使得适应后的人脸检测器在原始源域中保持与旧检测器大致相同的性能","关键词":["人脸检测","领域适应","残差学习","负迁移","灾难性遗忘"],"涉及的技术概念":"负迁移指的是在领域适应过程中，源域的知识对目标域的学习产生负面影响的现象。灾难性遗忘指的是在学习新知识时，模型忘记了之前学到的知识。残差损失是一种用于减少模型预测与真实值之间差异的技术，通过设计中的残差损失，可以有效地避免负迁移问题。"},{"order":581,"title":"Analyzing Filters Toward Efficient ConvNet","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kobayashi_Analyzing_Filters_Toward_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kobayashi_Analyzing_Filters_Toward_CVPR_2018_paper.html","abstract":"Deep convolutional neural network (ConvNet) is a promising approach for high-performance image classification. The behavior of ConvNet is analyzed mainly based on the neuron activations, such as by visualizing them. In this paper, in contrast to the activations, we focus on filters which are main components of ConvNets. Through analyzing two types of filters at convolution and fully-connected (FC) layers, respectively, on various pre-trained ConvNets, we present the methods to efficiently reformulate the filters, contributing to improving both memory size and classification performance of the ConvNets. They render the filter bases formulated in a parameter-free form as well as the efficient representation for the FC layer. The experimental results on image classification show that the methods are favorably applied to improve various ConvNets, including ResNet, trained on ImageNet with exhibiting high transferability on the other datasets.","中文标题":"分析滤波器以实现高效的卷积网络","摘要翻译":"深度卷积神经网络（ConvNet）是实现高性能图像分类的一种有前景的方法。ConvNet的行为主要基于神经元激活进行分析，例如通过可视化它们。在本文中，与激活不同，我们专注于作为ConvNets主要组成部分的滤波器。通过分析各种预训练ConvNets在卷积层和全连接（FC）层上的两种滤波器，我们提出了有效重构滤波器的方法，有助于改善ConvNets的内存大小和分类性能。它们使滤波器基以无参数形式重构，并为FC层提供了有效的表示。图像分类的实验结果表明，这些方法可以有利地应用于改进各种ConvNets，包括在ImageNet上训练的ResNet，并在其他数据集上表现出高可转移性。","领域":"卷积神经网络优化/图像分类/网络压缩","问题":"如何通过分析卷积神经网络中的滤波器来提高网络的效率和性能","动机":"探索卷积神经网络中滤波器的特性，以找到提高网络内存效率和分类性能的方法","方法":"分析卷积层和全连接层的滤波器，提出无参数形式的滤波器基重构方法和全连接层的高效表示方法","关键词":["滤波器分析","网络优化","图像分类"],"涉及的技术概念":"深度卷积神经网络（ConvNet）、神经元激活、滤波器、卷积层、全连接层（FC层）、无参数形式、网络重构、内存效率、分类性能、ResNet、ImageNet、可转移性"},{"order":582,"title":"Regularizing Deep Networks by Modeling and Predicting Label Structure","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mostajabi_Regularizing_Deep_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mostajabi_Regularizing_Deep_Networks_CVPR_2018_paper.html","abstract":"We construct custom regularization functions for use in supervised training of deep neural networks.  Our technique is applicable when the ground-truth labels themselves exhibit internal structure; we derive a regularizer by learning an autoencoder over the set of annotations.  Training thereby becomes a two-phase procedure.  The first phase models labels with an autoencoder.  The second phase trains the actual network of interest by attaching an auxiliary branch that must predict output via a hidden layer of the autoencoder.  After training, we discard this auxiliary branch.  We experiment in the context of semantic segmentation, demonstrating this regularization strategy leads to consistent accuracy boosts over baselines, both when training from scratch, or in combination with ImageNet pretraining.  Gains are also consistent over different choices of convolutional network architecture.  As our regularizer is discarded after training, our method has zero cost at test time; the performance improvements are essentially free.  We are simply able to learn better network weights by building an abstract model of the label space, and then training the network to understand this abstraction alongside the original task.","中文标题":"通过建模和预测标签结构来正则化深度网络","摘要翻译":"我们构建了自定义的正则化函数，用于深度神经网络的监督训练。我们的技术适用于当真实标签本身展现出内部结构时；我们通过学习注释集上的自编码器来推导出正则化器。因此，训练成为一个两阶段的过程。第一阶段使用自编码器对标签进行建模。第二阶段通过附加一个辅助分支来训练实际感兴趣的网络，该分支必须通过自编码器的隐藏层预测输出。训练后，我们丢弃这个辅助分支。我们在语义分割的背景下进行了实验，证明了这种正则化策略在从头开始训练或与ImageNet预训练结合时，都能带来相对于基线的持续准确率提升。在不同的卷积网络架构选择上，增益也是一致的。由于我们的正则化器在训练后被丢弃，我们的方法在测试时成本为零；性能提升基本上是免费的。我们能够通过构建标签空间的抽象模型，然后训练网络以理解这个抽象以及原始任务，从而学习到更好的网络权重。","领域":"语义分割/自编码器/正则化","问题":"如何在深度神经网络的监督训练中有效利用标签的内部结构进行正则化","动机":"利用标签的内部结构来提升深度神经网络在语义分割任务中的性能","方法":"通过构建标签空间的自编码器模型，并在训练过程中附加一个辅助分支来预测自编码器隐藏层的输出，从而实现正则化","关键词":["语义分割","自编码器","正则化","监督训练","深度神经网络"],"涉及的技术概念":"自编码器是一种无监督学习算法，用于学习数据的有效编码。在本研究中，自编码器被用来建模标签的内部结构，并通过附加的辅助分支在训练过程中实现正则化，从而提升深度神经网络在语义分割任务中的性能。"},{"order":583,"title":"In-Place Activated BatchNorm for Memory-Optimized Training of DNNs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bulo_In-Place_Activated_BatchNorm_CVPR_2018_paper.html","abstract":"In this work we present In-Place Activated Batch Normalization (InPlace-ABN) -- a novel approach to drastically reduce the training memory footprint of modern deep neural networks in a computationally efficient way. Our solution substitutes the conventionally used succession of BatchNorm + Activation layers with a single plugin layer, hence avoiding invasive framework surgery while providing straightforward applicability for existing deep learning frameworks. We obtain memory savings of up to 50% by dropping intermediate results and by recovering required information during the backward pass through the inversion of stored forward results, with only minor increase (0.8-2%) in computation time. Also, we demonstrate how frequently used checkpointing approaches can be made computationally as efficient as InPlace-ABN. In our experiments on image classification, we demonstrate on-par results on ImageNet-1k with state-of-the-art approaches. On the memory-demanding task of semantic segmentation, we report competitive results for COCO-Stuff and set new state-of-the-art results for Cityscapes and Mapillary Vistas. Code can be found at https://github.com/mapillary/inplace_abn.","中文标题":"用于深度神经网络内存优化训练的就地激活批量归一化","摘要翻译":"在本工作中，我们提出了就地激活批量归一化（InPlace-ABN）——一种新颖的方法，以计算效率高的方式大幅减少现代深度神经网络的训练内存占用。我们的解决方案用单个插件层替代了传统上使用的批量归一化+激活层序列，从而避免了侵入性的框架修改，同时为现有的深度学习框架提供了直接的应用性。通过丢弃中间结果并通过反转存储的前向结果在反向传播过程中恢复所需信息，我们实现了高达50%的内存节省，计算时间仅略有增加（0.8-2%）。此外，我们还展示了如何使常用的检查点方法在计算上像InPlace-ABN一样高效。在我们的图像分类实验中，我们展示了与ImageNet-1k上最先进方法相当的结果。在内存需求高的语义分割任务中，我们报告了COCO-Stuff的竞争性结果，并为Cityscapes和Mapillary Vistas设定了新的最先进结果。代码可在https://github.com/mapillary/inplace_abn找到。","领域":"深度神经网络训练优化/语义分割/图像分类","问题":"减少深度神经网络训练过程中的内存占用","动机":"为了在计算效率高的前提下，大幅减少现代深度神经网络的训练内存占用","方法":"提出就地激活批量归一化（InPlace-ABN），通过用单个插件层替代传统的批量归一化+激活层序列，丢弃中间结果并通过反转存储的前向结果在反向传播过程中恢复所需信息","关键词":["内存优化","批量归一化","语义分割","图像分类"],"涉及的技术概念":"就地激活批量归一化（InPlace-ABN）是一种减少深度神经网络训练内存占用的技术，通过替代传统的批量归一化+激活层序列为单个插件层，丢弃中间结果并通过反转存储的前向结果在反向传播过程中恢复所需信息，从而实现内存节省。"},{"order":584,"title":"DVQA: Understanding Data Visualizations via Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.html","abstract":"Bar charts are an effective way to convey numeric information, but today's algorithms cannot parse them. Existing methods fail when faced with even minor variations in appearance. Here, we present DVQA, a dataset that tests many aspects of bar chart understanding in a question answering framework. Unlike visual question answering (VQA), DVQA requires processing words and answers that are unique to a particular bar chart. State-of-the-art VQA algorithms perform poorly on DVQA, and we propose two strong baselines that perform considerably better. Our work will enable algorithms to automatically extract numeric and semantic information from vast quantities of bar charts found in scientific publications, Internet articles, business reports, and many other areas.","中文标题":"DVQA：通过问答理解数据可视化","摘要翻译":"条形图是传达数字信息的有效方式，但现今的算法无法解析它们。现有的方法在面对外观上的微小变化时也会失败。在这里，我们介绍了DVQA，一个在问答框架下测试条形图理解多个方面的数据集。与视觉问答（VQA）不同，DVQA需要处理特定条形图特有的文字和答案。最先进的VQA算法在DVQA上表现不佳，我们提出了两个表现显著更好的强基线。我们的工作将使算法能够自动从科学出版物、网络文章、商业报告等众多领域的大量条形图中提取数字和语义信息。","领域":"数据可视化/问答系统/自然语言处理","问题":"现有算法无法有效解析条形图，尤其是在面对外观上的微小变化时。","动机":"使算法能够自动从大量条形图中提取数字和语义信息，以应用于科学出版物、网络文章、商业报告等领域。","方法":"提出了DVQA数据集，并在问答框架下测试条形图理解的多个方面，同时提出了两个表现显著优于现有VQA算法的基线方法。","关键词":["条形图","问答系统","数据可视化"],"涉及的技术概念":{"DVQA":"一个专门用于测试条形图理解的数据集，要求处理特定条形图特有的文字和答案。","VQA":"视觉问答，一种结合视觉和语言理解的技术，用于回答关于图像内容的问题。","基线方法":"在研究中作为比较基准的初步或简单方法，用于评估新方法的性能。"}},{"order":585,"title":"DA-GAN: Instance-Level Image Translation by Deep Attention Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_DA-GAN_Instance-Level_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_DA-GAN_Instance-Level_Image_CVPR_2018_paper.html","abstract":"Unsupervised image translation, which aims in translating two independent sets of images, is challenging in discovering the correct correspondences without paired data. Existing works build upon Generative Adversarial Networks (GANs) such that the distribution of the translated images are indistinguishable from the distribution of the target set. However, such set-level constraints cannot learn the instance-level correspondences (e.g. aligned semantic parts in object transfiguration task). This limitation often results in false positives (e.g. geometric or semantic artifacts), and further leads to mode collapse problem. To address the above issues, we propose a novel framework for instance-level image translation by Deep Attention GAN (DA-GAN). Such a design enables DA-GAN to decompose the task of translating samples from two sets into translating instances in a highly-structured latent space. Specifically, we jointly learn a deep attention encoder, and the instance-level correspondences could be consequently discovered through attending on the learned instances. Therefore, the constraints could be exploited on both set-level and instance-level. Comparisons against several state-of-the- arts demonstrate the superiority of our approach, and the broad application capability, e.g, pose morphing, data augmentation, etc., pushes the margin of domain translation problem.","中文标题":"DA-GAN: 通过深度注意力生成对抗网络实现实例级图像翻译","摘要翻译":"无监督图像翻译旨在翻译两个独立的图像集，在没有配对数据的情况下发现正确的对应关系是具有挑战性的。现有的工作基于生成对抗网络（GANs），使得翻译图像的分布与目标集的分布无法区分。然而，这种集合级约束无法学习实例级对应关系（例如，在对象变形任务中对齐的语义部分）。这一限制经常导致误报（例如，几何或语义伪影），并进一步导致模式崩溃问题。为了解决上述问题，我们提出了一种新颖的框架，通过深度注意力生成对抗网络（DA-GAN）实现实例级图像翻译。这样的设计使DA-GAN能够将翻译两个集合中的样本的任务分解为在高度结构化的潜在空间中翻译实例。具体来说，我们联合学习一个深度注意力编码器，并且通过关注学习到的实例可以发现实例级对应关系。因此，可以在集合级和实例级上利用约束。与几种最先进技术的比较证明了我们方法的优越性，以及广泛的应用能力，例如姿态变形、数据增强等，推动了领域翻译问题的边界。","领域":"图像翻译/生成对抗网络/实例级对应","问题":"无监督图像翻译中实例级对应关系的发现","动机":"解决现有方法在无监督图像翻译中无法学习实例级对应关系，导致误报和模式崩溃问题","方法":"提出了一种新颖的框架，通过深度注意力生成对抗网络（DA-GAN）实现实例级图像翻译，联合学习深度注意力编码器以发现实例级对应关系","关键词":["图像翻译","生成对抗网络","实例级对应","深度注意力编码器"],"涉及的技术概念":"生成对抗网络（GANs）用于图像翻译，深度注意力编码器用于发现实例级对应关系，集合级和实例级约束用于提高翻译质量"},{"order":586,"title":"Unsupervised Learning of Depth and Ego-Motion From Monocular Video Using 3D Geometric Constraints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mahjourian_Unsupervised_Learning_of_CVPR_2018_paper.html","abstract":"We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video).  Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures.   We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames.  We also incorporate validity masks to avoid penalizing areas in which no useful information exists.  We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion.  Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible.  We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.","中文标题":"使用3D几何约束从单目视频中无监督学习深度和自我运动","摘要翻译":"我们提出了一种新颖的方法，用于从单目视频中无监督学习深度和自我运动。无监督学习消除了对单独监督信号（深度或自我运动的地面真值，或多视图视频）的需求。先前在无监督深度学习中的工作使用像素级或基于梯度的损失，这些损失仅考虑小局部邻域中的像素。我们的主要贡献是明确考虑整个场景的推断3D几何，并强制执行估计的3D点云和自我运动在连续帧之间的一致性。这是一个具有挑战性的任务，并通过一种新颖的（近似）反向传播算法来解决，用于对齐3D结构。我们将这种新颖的基于3D的损失与基于使用估计深度和自我运动从相邻帧重建帧的光度质量的2D损失相结合。我们还引入了有效性掩码，以避免在没有有用信息的区域进行惩罚。我们在KITTI数据集和用未校准的手机摄像头捕获的视频数据集上测试了我们的算法。我们提出的方法在这两个数据集上持续改进了深度估计，并在深度和自我运动方面优于最先进的技术。因为我们只需要一个简单的视频，所以在大型和多样化的数据集上学习深度和自我运动成为可能。我们通过在低质量未校准视频数据集上训练并在KITTI上评估来证明这一点，排名在之前仅在KITTI上训练的最佳表现方法之中。","领域":"3D重建/自我运动估计/深度估计","问题":"从单目视频中无监督学习深度和自我运动","动机":"消除对单独监督信号的需求，提高深度和自我运动估计的准确性","方法":"结合3D几何约束和2D光度质量损失，使用有效性掩码避免无效区域的惩罚，通过新颖的反向传播算法对齐3D结构","关键词":["3D重建","自我运动估计","深度估计"],"涉及的技术概念":"无监督学习、3D几何约束、自我运动、深度估计、反向传播算法、光度质量损失、有效性掩码"},{"order":587,"title":"FOTS: Fast Oriented Text Spotting With a Unified Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_FOTS_Fast_Oriented_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_FOTS_Fast_Oriented_CVPR_2018_paper.html","abstract":"Incidental scene text spotting is considered one of the most difficult and valuable challenges in the document analysis community. Most existing methods treat text detection and recognition as separate tasks. In this work, we propose a unified end-to-end trainable Fast Oriented Text Spotting (FOTS) network for simultaneous detection and recognition, sharing computation and visual information among the two complementary tasks. Specifically, RoIRotate is introduced to share convolutional features between detection and recognition. Benefiting from convolution sharing strategy, our FOTS has little computation overhead compared to baseline text detection network, and the joint training method makes our method perform better than these two-stage methods. Experiments on ICDAR 2015, ICDAR 2017 MLT, and ICDAR 2013 datasets demonstrate that the proposed method outperforms state-of-the-art methods significantly, which further allows us to develop the first real-time oriented text spotting system which surpasses all previous state-of-the-art results by more than 5% on ICDAR 2015 text spotting task while keeping 22.6 fps.","中文标题":"FOTS: 使用统一网络的快速定向文本识别","摘要翻译":"偶然场景文本识别被认为是文档分析社区中最困难且最有价值的挑战之一。大多数现有方法将文本检测和识别视为独立的任务。在这项工作中，我们提出了一个统一的端到端可训练的快速定向文本识别（FOTS）网络，用于同时进行检测和识别，在两个互补任务之间共享计算和视觉信息。具体来说，引入了RoIRotate以在检测和识别之间共享卷积特征。得益于卷积共享策略，我们的FOTS与基线文本检测网络相比几乎没有计算开销，联合训练方法使我们的方法比这些两阶段方法表现更好。在ICDAR 2015、ICDAR 2017 MLT和ICDAR 2013数据集上的实验表明，所提出的方法显著优于最先进的方法，这进一步使我们能够开发第一个实时定向文本识别系统，该系统在ICDAR 2015文本识别任务上比所有先前的最先进结果高出5%以上，同时保持22.6 fps。","领域":"文本识别/文档分析/实时系统","问题":"解决偶然场景文本识别中的文本检测和识别问题","动机":"现有方法将文本检测和识别视为独立任务，导致效率低下和性能不佳","方法":"提出了一个统一的端到端可训练的快速定向文本识别（FOTS）网络，通过RoIRotate共享卷积特征，实现检测和识别的联合训练","关键词":["文本识别","文档分析","实时系统"],"涉及的技术概念":{"RoIRotate":"一种技术，用于在文本检测和识别之间共享卷积特征，以提高效率和性能","卷积共享策略":"一种策略，通过在检测和识别任务之间共享卷积特征来减少计算开销","联合训练方法":"一种训练方法，通过同时优化检测和识别任务来提高整体性能"}},{"order":588,"title":"Mobile Video Object Detection With Temporally-Aware Feature Maps","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Mobile_Video_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Mobile_Video_Object_CVPR_2018_paper.html","abstract":"This paper introduces an online model for object detection in videos with real-time performance on mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.","中文标题":"具有时间感知特征图的移动视频目标检测","摘要翻译":"本文介绍了一种用于视频中目标检测的在线模型，该模型在移动和嵌入式设备上具有实时性能。我们的方法将快速单图像目标检测与卷积长短期记忆（LSTM）层相结合，创建了一种交织的递归-卷积架构。此外，我们提出了一种高效的Bottleneck-LSTM层，与常规LSTM相比，显著降低了计算成本。我们的网络通过使用Bottleneck-LSTM来跨帧精炼和传播特征图，实现了时间感知。这种方法在视频中的检测速度显著快于现有方法，在模型大小和计算成本上优于最快的单帧模型，同时在Imagenet VID 2015数据集上达到了与更昂贵的单帧模型相当的准确性。我们的模型在移动CPU上达到了高达15 FPS的实时推理速度。","领域":"视频分析/目标检测/移动计算","问题":"在移动和嵌入式设备上实现实时视频目标检测","动机":"为了在资源受限的设备上实现高效的视频目标检测，同时保持高准确性和实时性能","方法":"结合快速单图像目标检测和卷积LSTM层，提出Bottleneck-LSTM层以减少计算成本，通过跨帧精炼和传播特征图实现时间感知","关键词":["视频目标检测","卷积LSTM","Bottleneck-LSTM","实时性能","移动计算"],"涉及的技术概念":{"卷积长短期记忆（LSTM）":"一种特殊的递归神经网络，能够学习长期依赖信息，适用于处理序列数据","Bottleneck-LSTM":"一种改进的LSTM结构，旨在减少计算成本，同时保持或提高性能","特征图":"在卷积神经网络中，通过卷积操作从输入图像中提取的特征表示","实时推理速度":"模型处理输入数据并输出结果的速度，通常以每秒帧数（FPS）衡量"}},{"order":589,"title":"Weakly Supervised Phrase Localization With Multi-Scale Anchored Transformer Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Weakly_Supervised_Phrase_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel weakly supervised model, Multi-scale Anchored Transformer Network (MATN), to accurately localize free-form textual phrases with only image-level supervision. The proposed MATN takes region proposals as localization anchors, and learns a multi-scale correspondence network to continuously search for phrase regions referring to the anchors. In this way, MATN can exploit useful cues from these anchors to reliably reason about locations of the regions described by the phrases given only image-level supervision. Through differentiable sampling on image spatial feature maps, MATN introduces a novel training objective to simultaneously minimize a contrastive reconstruction loss between different phrases from a single image and a set of triplet losses among multiple images with similar phrases. Superior to existing region proposal based methods, MATN searches for the optimal bounding box over the entire feature map instead of selecting a sub-optimal one from discrete region proposals. We evaluate MATN on the Flickr30K Entities and ReferItGame datasets. The experimental results show that MATN significantly outperforms the state-of-the-art methods.","中文标题":"弱监督短语定位与多尺度锚定变换网络","摘要翻译":"在本文中，我们提出了一种新颖的弱监督模型，多尺度锚定变换网络（MATN），以仅使用图像级监督准确定位自由形式的文本短语。提出的MATN将区域提议作为定位锚点，并学习一个多尺度对应网络，以持续搜索与锚点相关的短语区域。通过这种方式，MATN可以利用这些锚点中的有用线索，可靠地推理出仅给定图像级监督的短语描述区域的位置。通过对图像空间特征图的可微分采样，MATN引入了一种新颖的训练目标，以同时最小化来自单个图像的不同短语之间的对比重建损失和具有相似短语的多个图像之间的三重损失。优于现有的基于区域提议的方法，MATN在整个特征图上搜索最佳边界框，而不是从离散的区域提议中选择次优的。我们在Flickr30K Entities和ReferItGame数据集上评估了MATN。实验结果表明，MATN显著优于最先进的方法。","领域":"视觉问答/图像标注/语义分割","问题":"在仅使用图像级监督的情况下，准确定位自由形式的文本短语","动机":"提高在弱监督条件下对图像中自由形式文本短语的定位准确性","方法":"提出多尺度锚定变换网络（MATN），利用区域提议作为定位锚点，学习多尺度对应网络，通过可微分采样和对比重建损失及三重损失进行训练","关键词":["弱监督学习","短语定位","多尺度变换网络"],"涉及的技术概念":"MATN模型通过引入多尺度锚定变换网络，利用图像级监督进行自由形式文本短语的定位。该模型采用区域提议作为定位锚点，并通过可微分采样技术，在图像空间特征图上搜索最佳边界框，同时最小化对比重建损失和三重损失，以提高定位的准确性和效率。"},{"order":590,"title":"Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.html","abstract":"In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.  An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.","中文标题":"重新审视牛津和巴黎：大规模图像检索基准测试","摘要翻译":"在本文中，我们解决了在标准和流行的Oxford 5k和Paris 6k数据集上进行图像检索基准测试的问题。特别是，解决了注释错误、数据集的大小和挑战级别的问题：为这两个数据集创建了新的注释，特别关注地面实况的可靠性。引入了三种不同难度级别的新协议。这些协议允许公平比较不同方法，包括那些使用数据集预处理阶段的方法。为每个数据集引入了15个新的挑战性查询。最后，选择了一组新的1M个困难、半自动清理的干扰项。在新的基准上进行了对最先进方法的广泛比较。评估了从基于局部特征的方法到现代基于CNN的方法的不同类型的方法。通过结合两者的优点取得了最佳结果。最重要的是，图像检索似乎远未解决。","领域":"图像检索/基准测试/数据集标注","问题":"解决图像检索基准测试中的注释错误、数据集大小和挑战级别问题","动机":"提高图像检索基准测试的准确性和可靠性，以便更公平地比较不同方法","方法":"创建新的注释，引入三种不同难度级别的新协议，增加新的挑战性查询，选择新的干扰项，并在新的基准上对最先进的方法进行广泛比较","关键词":["图像检索","基准测试","数据集标注","挑战性查询","干扰项"],"涉及的技术概念":"本文涉及的技术概念包括图像检索、基准测试、数据集标注、挑战性查询、干扰项、局部特征方法、CNN（卷积神经网络）方法。"},{"order":591,"title":"Cross-Dataset Adaptation for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.html","abstract":"We investigate the problem of cross-dataset adaptation for visual question answering (Visual QA). Our goal is to train a Visual QA model on a source dataset  but apply it to another target one. Analogous to domain adaptation for visual recognition, this setting is appealing when the target dataset does not have a sufficient amount of labeled data to learn an \`\`in-domain'' model.  The key challenge is that the two datasets are constructed differently, resulting in the cross-dataset mismatch on images, questions, or answers.  We overcome this difficulty by proposing a novel domain adaptation algorithm. Our method reduces the difference in statistical distributions by transforming the feature representation of the data in the target dataset. Moreover, it maximizes the likelihood of answering questions (in the target dataset) correctly using the Visual QA model trained on the source dataset. We empirically studied the effectiveness of the proposed approach on adapting among several popular Visual QA datasets. We show that the proposed method improves over baselines where there is no adaptation and several other adaptation methods. We both quantitatively and qualitatively analyze when the adaptation can be mostly effective.","中文标题":"跨数据集适应的视觉问答","摘要翻译":"我们研究了视觉问答（Visual QA）中的跨数据集适应问题。我们的目标是在源数据集上训练一个视觉问答模型，但将其应用于另一个目标数据集。类似于视觉识别中的领域适应，当目标数据集没有足够的标记数据来学习一个“领域内”模型时，这种设置非常吸引人。关键挑战在于两个数据集的构建方式不同，导致在图像、问题或答案上存在跨数据集不匹配。我们通过提出一种新颖的领域适应算法来克服这一困难。我们的方法通过转换目标数据集中数据的特征表示来减少统计分布的差异。此外，它通过使用在源数据集上训练的视觉问答模型来最大化正确回答（目标数据集中的）问题的可能性。我们实证研究了所提出方法在几个流行的视觉问答数据集之间适应的有效性。我们展示了所提出的方法在没有适应和几种其他适应方法的基线之上有所改进。我们定量和定性分析了适应在何时最为有效。","领域":"视觉问答/领域适应/特征表示","问题":"解决视觉问答中的跨数据集适应问题","动机":"当目标数据集没有足够的标记数据来学习一个“领域内”模型时，跨数据集适应非常吸引人","方法":"提出一种新颖的领域适应算法，通过转换目标数据集中数据的特征表示来减少统计分布的差异，并最大化正确回答问题的可能性","关键词":["视觉问答","跨数据集适应","领域适应算法"],"涉及的技术概念":"领域适应算法通过转换目标数据集中数据的特征表示来减少统计分布的差异，并最大化使用源数据集训练的视觉问答模型正确回答目标数据集中问题的可能性。"},{"order":592,"title":"Globally Optimal Inlier Set Maximization for Atlanta Frame Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Globally_Optimal_Inlier_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Globally_Optimal_Inlier_CVPR_2018_paper.html","abstract":"In this work, we describe man-made structures via an appropriate structure assumption, called Atlanta world, which contains a vertical direction (typically the gravity direction) and a set of horizontal directions orthogonal to the vertical direction. Contrary to the commonly used Manhattan world assumption, the horizontal directions in Atlanta world are not necessarily orthogonal to each other. While Atlanta world permits to encompass a wider range of scenes, this makes the solution space larger and the problem more challenging. Given a set of inputs, such as lines in a calibrated image or surface normals, we propose the first globally optimal method of inlier set maximization for Atlanta direction estimation. We define a novel search space for Atlanta world, as well as its parameterization, and solve this challenging problem by a branch-and-bound framework. Experimental results with synthetic and real-world datasets have successfully confirmed the validity of our approach.","中文标题":"全局最优内点集最大化用于亚特兰大框架估计","摘要翻译":"在这项工作中，我们通过一个适当的结构假设来描述人造结构，称为亚特兰大世界，它包含一个垂直方向（通常是重力方向）和一组与垂直方向正交的水平方向。与常用的曼哈顿世界假设相反，亚特兰大世界中的水平方向不一定彼此正交。虽然亚特兰大世界允许涵盖更广泛的场景，但这使得解空间更大，问题更具挑战性。给定一组输入，如校准图像中的线条或表面法线，我们提出了第一个用于亚特兰大方向估计的全局最优内点集最大化方法。我们为亚特兰大世界定义了一个新的搜索空间及其参数化，并通过分支定界框架解决了这一挑战性问题。合成和真实世界数据集的实验结果成功证实了我们方法的有效性。","领域":"三维重建/几何处理/优化算法","问题":"在亚特兰大世界假设下，如何从一组输入（如校准图像中的线条或表面法线）中估计方向，并实现全局最优内点集最大化","动机":"亚特兰大世界假设允许涵盖更广泛的场景，但这也使得解空间更大，问题更具挑战性，因此需要一种新的方法来有效解决这一问题","方法":"定义了一个新的搜索空间及其参数化，并通过分支定界框架解决了这一挑战性问题","关键词":["亚特兰大世界","内点集最大化","分支定界","方向估计"],"涉及的技术概念":"亚特兰大世界假设是一种描述人造结构的方法，它包含一个垂直方向和一组水平方向，这些水平方向不一定彼此正交。内点集最大化是一种优化方法，旨在找到一组数据点，这些点满足特定的几何约束。分支定界是一种用于解决优化问题的算法框架，它通过系统地搜索解空间来找到全局最优解。"},{"order":593,"title":"End-to-End Convolutional Semantic Embeddings","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/You_End-to-End_Convolutional_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/You_End-to-End_Convolutional_Semantic_CVPR_2018_paper.html","abstract":"Semantic embeddings for images and sentences have been widely studied recently. The ability of deep neural networks on learning rich and robust visual and textual representations offers the opportunity to develop effective semantic embedding models. Currently, the state-of-the-art approaches in semantic learning first employ deep neural networks to encode images and sentences into a common semantic space. Then, the learning objective is to ensure a larger similarity between matching image and sentence pairs than randomly sampled pairs. Usually, Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are employed for learning image and sentence representations,  respectively. On one hand, CNNs are known to produce robust visual features at different levels and RNNs are known for capturing dependencies in sequential data. Therefore, this simple framework can be sufficiently effective in learning visual and textual semantics. On the other hand, different from CNNs, RNNs cannot produce middle-level (e.g. phrase-level in text) representations. As a result, only global representations are available for semantic learning. This could potentially limit the performance of the model due to the hierarchical structures in images and sentences. In this work, we apply Convolutional Neural Networks to process both images and sentences. Consequently, we can employ mid-level representations to assist global semantic learning by introducing a new learning objective on the convolutional layers. The experimental results show that our proposed textual CNN models with the new learning objective lead to better performance than the state-of-the-art approaches.","中文标题":"端到端卷积语义嵌入","摘要翻译":"最近，图像和句子的语义嵌入得到了广泛研究。深度神经网络在学习丰富且稳健的视觉和文本表示方面的能力，为开发有效的语义嵌入模型提供了机会。目前，语义学习中的最先进方法首先使用深度神经网络将图像和句子编码到一个共同的语义空间中。然后，学习目标是确保匹配的图像和句子对之间的相似性大于随机采样的对。通常，卷积神经网络（CNNs）和循环神经网络（RNNs）分别用于学习图像和句子的表示。一方面，CNNs以在不同层次上产生稳健的视觉特征而闻名，而RNNs则以捕捉序列数据中的依赖关系而著称。因此，这个简单的框架在学习视觉和文本语义方面可以非常有效。另一方面，与CNNs不同，RNNs不能产生中级（例如文本中的短语级）表示。因此，只有全局表示可用于语义学习。这可能会由于图像和句子中的层次结构而限制模型的性能。在这项工作中，我们应用卷积神经网络来处理图像和句子。因此，我们可以通过引入卷积层上的新学习目标，利用中级表示来辅助全局语义学习。实验结果表明，我们提出的带有新学习目标的文本CNN模型比最先进的方法表现更好。","领域":"语义嵌入/图像理解/自然语言处理","问题":"如何更有效地学习图像和句子的语义嵌入","动机":"现有的语义学习方法主要依赖于全局表示，这可能会限制模型性能，因为图像和句子具有层次结构","方法":"应用卷积神经网络处理图像和句子，并引入新的学习目标以利用中级表示辅助全局语义学习","关键词":["语义嵌入","卷积神经网络","循环神经网络"],"涉及的技术概念":{"语义嵌入":"将图像和句子编码到一个共同的语义空间中，以便于比较它们的相似性","卷积神经网络（CNNs）":"一种深度学习模型，特别适合处理图像数据，能够提取不同层次的视觉特征","循环神经网络（RNNs）":"一种深度学习模型，适合处理序列数据，如文本，能够捕捉序列中的依赖关系","中级表示":"在文本中指短语级别的表示，在图像中指介于局部和全局之间的特征表示"}},{"order":594,"title":"Referring Image Segmentation via Recurrent Refinement Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Referring_Image_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Referring_Image_Segmentation_CVPR_2018_paper.html","abstract":"We address the problem of image segmentation from natural language descriptions. Existing deep learning-based methods encode image representations based on the output of the last convolutional layer. One general issue is that the resulting image representation lacks multi-scale semantics, which are key components in advanced segmentation systems. In this paper, we utilize the feature pyramids inherently existing in convolutional neural networks to capture the semantics at different scales. To produce suitable information flow through the path of feature hierarchy, we propose Recurrent Refinement Network (RRN) that takes pyramidal features as input to refine the segmentation mask progressively. Experimental results on four available datasets show that our approach outperforms multiple baselines and state-of-the-art.","中文标题":"通过循环精炼网络进行参考图像分割","摘要翻译":"我们解决了从自然语言描述进行图像分割的问题。现有的基于深度学习的方法基于最后一个卷积层的输出编码图像表示。一个普遍的问题是，生成的图像表示缺乏多尺度语义，这是高级分割系统中的关键组成部分。在本文中，我们利用卷积神经网络中固有的特征金字塔来捕捉不同尺度的语义。为了在特征层次结构的路径上产生合适的信息流，我们提出了循环精炼网络（RRN），它以金字塔特征为输入，逐步精炼分割掩码。在四个可用数据集上的实验结果表明，我们的方法优于多个基线和最先进的方法。","领域":"图像分割/自然语言处理/卷积神经网络","问题":"从自然语言描述进行图像分割时，现有方法生成的图像表示缺乏多尺度语义","动机":"提高图像分割的准确性和效率，通过捕捉不同尺度的语义来精炼分割掩码","方法":"提出循环精炼网络（RRN），利用卷积神经网络中的特征金字塔，逐步精炼分割掩码","关键词":["图像分割","自然语言描述","特征金字塔","循环精炼网络"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征","特征金字塔":"在卷积神经网络中，不同层次的特征图可以形成金字塔结构，捕捉不同尺度的语义信息","循环精炼网络（RRN）":"一种网络结构，通过循环机制逐步精炼图像分割结果，利用特征金字塔作为输入"}},{"order":595,"title":"Two Can Play This Game: Visual Dialog With Discriminative Question Generation and Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jain_Two_Can_Play_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jain_Two_Can_Play_CVPR_2018_paper.html","abstract":"Human conversation is a complex mechanism with subtle nuances. It is hence an ambitious goal to develop artificial intelligence agents that can participate fluently in a conversation. While we are still far from achieving this goal, recent progress in visual question answering, image captioning, and visual question generation shows that dialog systems may be realizable in the not too distant future. To this end, a novel dataset was introduced recently and encouraging results were demonstrated, particularly for question answering. In this paper, we demonstrate a simple symmetric discriminative baseline, that can be applied to both predicting an answer as well as predicting a question. We show that this method performs on par with the state of the art, even memory net based methods. In addition, for the first time on the visual dialog dataset, we assess the performance of a system asking questions, and demonstrate how visual dialog can be generated from discriminative question generation and question answering.","中文标题":"两人可以玩这个游戏：通过区分性提问生成和回答进行视觉对话","摘要翻译":"人类对话是一个复杂且充满微妙差别的机制。因此，开发能够流畅参与对话的人工智能代理是一个雄心勃勃的目标。虽然我们离实现这一目标还很远，但最近在视觉问答、图像字幕生成和视觉问题生成方面的进展表明，对话系统在不久的将来可能成为现实。为此，最近引入了一个新的数据集，并展示了令人鼓舞的结果，特别是在问答方面。在本文中，我们展示了一个简单的对称区分性基线，它可以应用于预测答案以及预测问题。我们展示了这种方法与最先进的方法，甚至基于记忆网络的方法表现相当。此外，我们首次在视觉对话数据集上评估了系统提问的性能，并展示了如何通过区分性提问生成和问题回答生成视觉对话。","领域":"视觉问答/视觉对话/图像字幕生成","问题":"开发能够流畅参与对话的人工智能代理","动机":"探索视觉对话系统的实现可能性，特别是在视觉问答和问题生成方面的应用","方法":"采用一个简单的对称区分性基线方法，用于预测答案和问题，并与最先进的方法进行比较","关键词":["视觉问答","视觉对话","图像字幕生成"],"涉及的技术概念":"视觉问答（Visual Question Answering, VQA）是指让计算机理解图像内容并回答相关问题；视觉对话（Visual Dialog）涉及基于图像的对话系统，能够进行多轮对话；图像字幕生成（Image Captioning）是指为图像生成描述性文本。"},{"order":596,"title":"Generative Adversarial Learning Towards Fast Weakly Supervised Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Generative_Adversarial_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Generative_Adversarial_Learning_CVPR_2018_paper.html","abstract":"Weakly supervised object detection has attracted extensive research efforts in recent years. Without the need of annotating bounding boxes, the existing methods usually follow a two/multi-stage pipeline with an online compulsive stage to extract object proposals, which is an order of magnitude slower than fast fully supervised object detectors such as SSD [31] and YOLO [34]. In this paper, we speedup online weakly supervised object detectors by orders of magnitude by proposing a novel generative adversarial learning paradigm. In the proposed paradigm, the generator is a one-stage object detector to generate bounding boxes from images. To guide the learning of object-level generator, a surrogator is introduced to mine high-quality bounding boxes for training. We further adapt a structural similarity loss in combination with an adversarial loss into the training objective, which solves the challenge that the bounding boxes produced by the surrogator may not well capture their ground truth. Our one-stage detector outperforms all existing schemes in terms of detection accuracy, running at 118 frames per second, which is up to 438x faster than the state-of-the-art weakly supervised detectors [8, 30, 15, 27, 45]. The code will be available publicly soon.","中文标题":"生成对抗学习实现快速弱监督检测","摘要翻译":"近年来，弱监督目标检测吸引了广泛的研究努力。无需标注边界框，现有方法通常遵循一个两阶段/多阶段流程，包含一个在线强制阶段以提取目标提议，这比快速全监督目标检测器如SSD [31]和YOLO [34]慢一个数量级。在本文中，我们通过提出一种新颖的生成对抗学习范式，将在线弱监督目标检测器的速度提高了几个数量级。在所提出的范式中，生成器是一个单阶段目标检测器，用于从图像生成边界框。为了指导对象级生成器的学习，引入了一个替代器来挖掘高质量的边界框进行训练。我们进一步将结构相似性损失与对抗性损失结合到训练目标中，解决了替代器生成的边界框可能无法很好地捕捉其真实值的挑战。我们的单阶段检测器在检测精度方面优于所有现有方案，运行速度为每秒118帧，比最先进的弱监督检测器快438倍 [8, 30, 15, 27, 45]。代码将很快公开。","领域":"目标检测/生成对抗网络/弱监督学习","问题":"提高弱监督目标检测的速度和精度","动机":"现有弱监督目标检测方法速度慢，需要提高检测速度同时保持或提高检测精度","方法":"提出一种新颖的生成对抗学习范式，包括一个单阶段目标检测器作为生成器，引入替代器挖掘高质量边界框进行训练，并结合结构相似性损失与对抗性损失","关键词":["目标检测","生成对抗网络","弱监督学习","单阶段检测器","结构相似性损失","对抗性损失"],"涉及的技术概念":{"弱监督目标检测":"一种目标检测方法，不需要精确的边界框标注，而是使用图像级别的标签进行训练。","生成对抗网络":"一种深度学习模型，由生成器和判别器组成，通过对抗过程学习生成数据。","单阶段检测器":"一种目标检测方法，直接在图像上预测目标类别和位置，无需生成目标提议。","结构相似性损失":"一种损失函数，用于衡量生成图像与真实图像在结构上的相似性。","对抗性损失":"在生成对抗网络中，用于衡量生成器生成的图像与真实图像之间的差异。"}},{"order":597,"title":"A Deeper Look at Power Normalizations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Koniusz_A_Deeper_Look_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Koniusz_A_Deeper_Look_CVPR_2018_paper.html","abstract":"Power Normalizations (PN) are very useful non-linear operators in the context of Bag-of-Words data representations as they tackle problems such as feature imbalance. In this paper, we reconsider these operators in the deep learning setup by introducing a novel layer that implements PN for non-linear pooling of feature maps. Specifically, by using a kernel formulation, our layer combines the feature vectors and their respective spatial locations in the feature maps produced by the last convolutional layer of CNN. Linearization of such a kernel results in a positive definite matrix capturing the second-order statistics of the feature vectors, to which PN operators are applied. We study two types of PN functions, namely (i) MaxExp and (ii) Gamma, addressing their role and meaning in the context of non-linear pooling. We also provide a probabilistic interpretation of these operators and derive their surrogates with well-behaved gradients for end-to-end CNN learning. We apply our theory to practice by implementing the PN layer on a ResNet-50 model and showcase experiments on four benchmarks for fine-grained recognition, scene recognition, and material classification. Our results demonstrate state-of-the-part performance across all these tasks.","中文标题":"深入探讨幂归一化","摘要翻译":"幂归一化（PN）在词袋数据表示中是非常有用的非线性算子，因为它们解决了特征不平衡等问题。在本文中，我们通过在深度学习设置中引入一种新颖的层来重新考虑这些算子，该层实现了用于特征图非线性池化的PN。具体来说，通过使用核公式，我们的层结合了由CNN的最后一个卷积层产生的特征向量及其在特征图中的相应空间位置。这种核的线性化产生了一个正定矩阵，捕获了特征向量的二阶统计量，PN算子被应用于此。我们研究了两种类型的PN函数，即（i）MaxExp和（ii）Gamma，探讨了它们在非线性池化背景下的作用和意义。我们还提供了这些算子的概率解释，并推导出具有良好梯度的替代品，用于端到端的CNN学习。我们通过在一个ResNet-50模型上实现PN层并将理论应用于实践，并在四个基准测试上展示了细粒度识别、场景识别和材料分类的实验。我们的结果表明，在所有任务中都达到了最先进的性能。","领域":"细粒度识别/场景识别/材料分类","问题":"解决特征不平衡问题","动机":"探讨幂归一化在深度学习中的应用及其在非线性池化中的作用和意义","方法":"引入一种新颖的层实现幂归一化，通过核公式结合特征向量及其空间位置，研究MaxExp和Gamma两种PN函数，并提供概率解释和替代品","关键词":["幂归一化","非线性池化","特征向量","二阶统计量","MaxExp","Gamma"],"涉及的技术概念":"幂归一化（PN）是一种用于解决特征不平衡问题的非线性算子。在深度学习中，通过引入一种新颖的层来实现PN，该层使用核公式结合特征向量及其在特征图中的空间位置，产生一个正定矩阵捕获特征向量的二阶统计量。研究了MaxExp和Gamma两种PN函数，并提供了这些算子的概率解释和具有良好梯度的替代品，用于端到端的CNN学习。"},{"order":598,"title":"Dimensionality's Blessing: Clustering Images by Underlying Distribution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Dimensionalitys_Blessing_Clustering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Dimensionalitys_Blessing_Clustering_CVPR_2018_paper.html","abstract":"Many high dimensional vector distances tend to a constant. This is typically considered a negative “contrast-loss” phenomenon that hinders clustering and other machine learning techniques. We reinterpret “contrast-loss” as a blessing. Re-deriving “contrast-loss” using the law of large numbers, we show it results in a distribution’s instances concentrating on a thin “hyper-shell”. The hollow center means apparently chaotically overlapping distributions are actually intrinsically separable. We use this to develop distribution-clustering, an elegant algorithm for grouping of data points by their (unknown) underlying distribution. Distribution-clustering, creates notably clean clusters from raw unlabeled data, estimates the number of clusters for itself and is inherently robust to “outliers” which form their own clusters. This enables trawling for patterns in unorganized data and may be the key to enabling machine intelligence.","中文标题":"维度的祝福：通过底层分布聚类图像","摘要翻译":"许多高维向量距离趋向于一个常数。这通常被认为是一种负面的“对比损失”现象，阻碍了聚类和其他机器学习技术。我们重新解释“对比损失”为一种祝福。通过大数定律重新推导“对比损失”，我们展示了它导致分布的实例集中在一个薄的“超壳”上。空心的中心意味着表面上混乱重叠的分布实际上是内在可分离的。我们利用这一点开发了分布聚类，这是一种优雅的算法，用于根据数据的（未知）底层分布对数据点进行分组。分布聚类从原始未标记的数据中创建出特别干净的聚类，自行估计聚类的数量，并且对形成自己聚类的“异常值”具有固有的鲁棒性。这使得在无组织的数据中搜寻模式成为可能，并可能是实现机器智能的关键。","领域":"高维数据分析/聚类算法/数据挖掘","问题":"高维向量距离趋向于常数，导致聚类和其他机器学习技术受阻","动机":"重新解释高维数据中的“对比损失”现象，将其视为一种优势，以开发新的聚类方法","方法":"利用大数定律重新推导“对比损失”，开发分布聚类算法，该算法能够从原始未标记数据中创建干净的聚类，并自行估计聚类数量","关键词":["高维数据分析","聚类算法","数据挖掘"],"涉及的技术概念":"高维向量距离、对比损失、大数定律、超壳、分布聚类、异常值、机器智能"},{"order":599,"title":"Eliminating Background-Bias for Robust Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.html","abstract":"Person re-identification is an important topic in intelligent surveillance and computer vision. It aims to accurately measure visual similarities between person images for determining whether two images correspond to the same person. State-of-the-art methods mainly utilize deep learning based approaches for learning visual features for describing person appearances. However, we observe that existing deep learning models are biased to capture too much relevance between background appearances of person images. We design a series of experiments with newly created datasets to validate the influence of background information. To solve the background bias problem, we propose a person-region guided pooling deep neural network based on human parsing maps to learn more discriminative person-part features, and propose to augment training data with person images with random background. Extensive experiments demonstrate the robustness and effectiveness of our proposed method.","中文标题":"消除背景偏差以实现鲁棒的人员重识别","摘要翻译":"人员重识别是智能监控和计算机视觉中的一个重要课题。它旨在准确测量人员图像之间的视觉相似性，以确定两幅图像是否对应同一个人。最先进的方法主要利用基于深度学习的方法来学习描述人员外观的视觉特征。然而，我们观察到现有的深度学习模型倾向于捕捉人员图像背景外观之间的过多相关性。我们设计了一系列实验，并使用新创建的数据集来验证背景信息的影响。为了解决背景偏差问题，我们提出了一种基于人体解析图的人员区域引导池化深度神经网络，以学习更具区分性的人员部分特征，并建议使用具有随机背景的人员图像来增强训练数据。大量实验证明了我们提出方法的鲁棒性和有效性。","领域":"智能监控/人员重识别/深度学习","问题":"现有深度学习模型在人员重识别中过度依赖背景信息，导致识别偏差","动机":"为了提高人员重识别的准确性和鲁棒性，减少背景信息对识别结果的影响","方法":"提出了一种基于人体解析图的人员区域引导池化深度神经网络，并通过增加具有随机背景的人员图像来增强训练数据","关键词":["人员重识别","背景偏差","深度学习","人体解析图","数据增强"],"涉及的技术概念":"人员重识别是指在智能监控和计算机视觉中，通过比较不同图像中的人员视觉特征来确定是否为同一个人的技术。深度学习是一种机器学习方法，通过构建多层的神经网络来学习数据的深层次特征。人体解析图是指通过图像处理技术将人体从背景中分离出来，并进一步解析出人体的各个部分。数据增强是一种通过增加训练数据的多样性来提高模型泛化能力的技术。"},{"order":600,"title":"Learning to Evaluate Image Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Learning_to_Evaluate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cui_Learning_to_Evaluate_CVPR_2018_paper.html","abstract":"Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.","中文标题":"学习评估图像描述","摘要翻译":"图像描述评估指标面临两个挑战。首先，常用的指标如CIDEr、METEOR、ROUGE和BLEU往往与人类判断不相关。其次，每个指标对病态描述构建都有已知的盲点，而基于规则的指标一旦识别出这些盲点，就缺乏修复这些盲点的规定。例如，新提出的SPICE与人类判断相关性很好，但未能捕捉句子的句法结构。为了解决这两个挑战，我们提出了一种新颖的基于学习的判别评估指标，该指标直接训练以区分人类和机器生成的描述。此外，我们进一步提出了一种数据增强方案，在训练期间明确将病态变换作为负面示例。所提出的指标通过三种鲁棒性测试及其与人类判断的相关性进行评估。大量实验表明，所提出的数据增强方案不仅使我们的指标对几种病态变换更加鲁棒，而且还提高了其与人类判断的相关性。我们的指标在Flickr 8k的描述级别人类相关性和COCO的系统级别人类相关性上都优于其他指标。所提出的方法可以作为基于学习的评估指标，与现有的基于规则的指标互补。","领域":"图像描述评估/自然语言处理/深度学习","问题":"解决图像描述评估指标与人类判断不相关及对病态描述构建的盲点问题","动机":"提高图像描述评估指标与人类判断的相关性，并解决现有指标对病态描述构建的盲点问题","方法":"提出一种基于学习的判别评估指标，并采用数据增强方案在训练中引入病态变换作为负面示例","关键词":["图像描述评估","数据增强","判别评估指标"],"涉及的技术概念":{"CIDEr":"一种用于评估图像描述的指标，侧重于描述的内容","METEOR":"一种用于评估文本生成的指标，考虑了同义词和词形变化","ROUGE":"一种用于评估自动摘要和机器翻译的指标，基于n-gram的重叠","BLEU":"一种用于评估机器翻译的指标，基于n-gram的精确度","SPICE":"一种新提出的图像描述评估指标，与人类判断有很好的相关性，但未能捕捉句子的句法结构","数据增强":"一种技术，通过在训练数据中引入变换或噪声来提高模型的泛化能力"}},{"order":601,"title":"Single-Shot Object Detection With Enriched Semantics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Object_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Single-Shot_Object_Detection_CVPR_2018_paper.html","abstract":"We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a global activation module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a global activation module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of 32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on VOC2007 with an inference speed of 13.0 milliseconds per image.","中文标题":"单次射击目标检测与丰富语义","摘要翻译":"我们提出了一种新颖的单次射击目标检测网络，名为具有丰富语义的检测（DES）。我们的动机是通过语义分割分支和全局激活模块来丰富典型深度检测器中目标检测特征的语义。分割分支由弱分割地面真值监督，即不需要额外的注释。与此同时，我们采用了一个全局激活模块，它以自监督的方式学习通道和对象类别之间的关系。在PASCAL VOC和MS COCO检测数据集上的综合实验结果证明了所提出方法的有效性。特别是，基于VGG16的DES在VOC2007测试上实现了81.7的mAP，在COCO测试开发集上实现了32.8的mAP，在Titan Xp GPU上每张图像的推理速度为31.5毫秒。使用较低分辨率的版本，我们在VOC2007上实现了79.7的mAP，每张图像的推理速度为13.0毫秒。","领域":"目标检测/语义分割/自监督学习","问题":"如何在单次射击目标检测中丰富特征的语义","动机":"通过增强目标检测特征的语义来提高检测性能","方法":"采用语义分割分支和全局激活模块来丰富特征语义，其中分割分支由弱分割地面真值监督，全局激活模块以自监督方式学习通道和对象类别之间的关系","关键词":["单次射击目标检测","语义分割","全局激活模块","自监督学习"],"涉及的技术概念":{"单次射击目标检测":"一种目标检测方法，能够在一次前向传播中同时预测对象的类别和位置","语义分割":"一种图像分割技术，旨在为图像中的每个像素分配一个类别标签","全局激活模块":"一种模块，用于学习特征通道之间的关系，以增强特征的表达能力","自监督学习":"一种学习方法，通过设计任务让模型从未标注的数据中学习有用的特征"}},{"order":602,"title":"Low-Shot Learning With Imprinted Weights","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Low-Shot_Learning_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Low-Shot_Learning_With_CVPR_2018_paper.html","abstract":"Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings.","中文标题":"低样本学习与印记权重","摘要翻译":"人类视觉能够在仅看到一两个训练样本后立即识别出新的视觉类别。我们描述了如何在低样本学习期间通过直接从新的训练样本设置最终层权重来为ConvNet分类器添加类似的能力。我们称这个过程为权重印记，因为它基于该训练样本的嵌入层激活的适当缩放副本直接为新类别设置权重。印记过程为使用随机梯度下降的训练提供了宝贵的补充，因为它提供了即时的良好分类性能，并为未来的任何进一步微调提供了初始化。我们展示了这个印记过程如何与基于代理的嵌入相关。然而，它的不同之处在于，每个新类别只学习一个印记权重向量，而不是依赖于通常与嵌入方法一起使用的训练实例的最近邻距离。我们的实验表明，使用印记权重的平均比使用最近邻实例嵌入提供了更好的泛化能力。","领域":"低样本学习/卷积神经网络/权重印记","问题":"如何在仅有一两个训练样本的情况下，使卷积神经网络分类器能够立即识别新的视觉类别。","动机":"模仿人类视觉系统在极少训练样本下识别新类别的能力，提高卷积神经网络在低样本学习场景下的性能。","方法":"通过直接从新的训练样本设置最终层权重，即权重印记，来为卷积神经网络分类器添加识别新类别的能力。印记过程基于训练样本的嵌入层激活的适当缩放副本直接为新类别设置权重。","关键词":["低样本学习","卷积神经网络","权重印记","嵌入层","随机梯度下降"],"涉及的技术概念":{"低样本学习":"一种机器学习方法，旨在使用非常有限的训练样本来训练模型。","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据。","权重印记":"一种直接基于训练样本的嵌入层激活设置新类别权重的方法。","嵌入层":"神经网络中的一层，用于将输入数据转换为固定大小的向量表示。","随机梯度下降":"一种优化算法，用于最小化损失函数，通过迭代更新模型参数。"}},{"order":603,"title":"Neural Motifs: Scene Graph Parsing With Global Context","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zellers_Neural_Motifs_Scene_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zellers_Neural_Motifs_Scene_CVPR_2018_paper.html","abstract":"We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.","中文标题":"神经主题：全局上下文下的场景图解析","摘要翻译":"我们研究了生成视觉场景的结构化图表示的问题。我们的工作分析了主题的作用：场景图中经常出现的子结构。我们在Visual Genome数据集上提出了关于这种重复结构的新定量见解。我们的分析表明，对象标签对关系标签具有高度预测性，但反之则不然。我们还发现，即使在更大的子图中也存在重复模式：超过50%的图包含至少涉及两个关系的主题。我们的分析激发了一个新的基线：给定对象检测，预测具有给定标签的对象对之间在训练集中看到的最频繁关系。这个基线在评估设置中相对于之前的最先进技术平均提高了3.6%。然后，我们介绍了堆叠主题网络，这是一种旨在捕捉场景图中高阶主题的新架构，它进一步在我们的强基线上平均提高了7.1%的相对增益。我们的代码可在github.com/rowanz/neural-motifs获取。","领域":"场景理解/图神经网络/视觉关系检测","问题":"生成视觉场景的结构化图表示","动机":"分析场景图中经常出现的子结构（主题）的作用，并基于这些见解改进场景图解析的性能","方法":"提出了一个新的基线方法，通过预测对象对之间最频繁的关系来改进性能，并引入了堆叠主题网络以捕捉更高阶的主题","关键词":["场景图","主题","图神经网络","视觉关系检测","结构化表示"],"涉及的技术概念":{"场景图":"用于表示视觉场景中对象及其关系的图结构","主题":"在场景图中经常出现的子结构","图神经网络":"一种处理图结构数据的神经网络","视觉关系检测":"识别图像中对象之间的视觉关系","结构化表示":"以结构化的方式（如图）表示视觉场景中的信息"}},{"order":604,"title":"Variational Autoencoders for Deforming 3D Mesh Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tan_Variational_Autoencoders_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tan_Variational_Autoencoders_for_CVPR_2018_paper.html","abstract":"3D geometric contents are becoming increasingly popular. In this paper, we study the problem of analyzing deforming 3D meshes using deep neural networks. Deforming 3D meshes are ﬂexible to represent 3D animation sequences as well as collections of objects of the same category, allowing diverse shapes with large-scale non-linear deformations. We propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. The framework is easy to train, and requires very few training examples. We also propose an extended model which allows ﬂexibly adjusting the signiﬁcance of different latent variables by altering the prior distribution. Extensive experiments demonstrate that our general framework is able to learn a reasonable representation for a collection of deformable shapes, and produce competitive results for a variety of applications, including shape generation, shape interpolation, shape space embedding and shape exploration, outperforming state-of-the-art methods.","中文标题":"变分自编码器用于变形3D网格模型","摘要翻译":"3D几何内容正变得越来越受欢迎。在本文中，我们研究了使用深度神经网络分析变形3D网格的问题。变形3D网格能够灵活地表示3D动画序列以及同一类别对象的集合，允许具有大规模非线性变形的多样化形状。我们提出了一个新颖的框架，称为网格变分自编码器（mesh VAE），以探索3D表面的概率潜在空间。该框架易于训练，并且需要非常少的训练样本。我们还提出了一个扩展模型，该模型允许通过改变先验分布来灵活调整不同潜在变量的重要性。大量实验证明，我们的通用框架能够学习到可变形形状集合的合理表示，并在形状生成、形状插值、形状空间嵌入和形状探索等多种应用中产生具有竞争力的结果，优于最先进的方法。","领域":"3D几何处理/形状分析/动画技术","问题":"分析变形3D网格","动机":"3D几何内容越来越受欢迎，需要有效的方法来分析变形3D网格","方法":"提出了网格变分自编码器（mesh VAE）框架，探索3D表面的概率潜在空间，并提出了一个扩展模型以灵活调整不同潜在变量的重要性","关键词":["3D几何处理","形状分析","动画技术","变分自编码器","形状生成","形状插值","形状空间嵌入","形状探索"],"涉及的技术概念":"变分自编码器（VAE）是一种生成模型，通过学习数据的潜在表示来生成新的数据样本。在本文中，VAE被应用于3D网格模型，以探索和生成变形的3D形状。通过改变潜在变量的先验分布，可以调整这些变量在形状生成中的重要性，从而实现更灵活的形状控制和生成。"},{"order":605,"title":"Fast Monte-Carlo Localization on Aerial Vehicles Using Approximate Continuous Belief Representations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dhawale_Fast_Monte-Carlo_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dhawale_Fast_Monte-Carlo_Localization_CVPR_2018_paper.html","abstract":"Size, weight, and power constrained platforms impose constraints on computational resources that introduce unique challenges in implementing localization algorithms. We present a framework to perform fast localization on such platforms enabled by the compressive capabilities of Gaussian Mixture Model representations of point cloud data. Given raw structural data from a depth sensor and pitch and roll estimates from an on-board attitude reference system, a multi-hypothesis particle filter localizes the vehicle by exploiting the likelihood of the data originating from the mixture model. We demonstrate analysis of this likelihood in the vicinity of the ground truth pose and detail its utilization in a particle filter-based vehicle localization strategy, and later present results of real-time implementations on a desktop system and an off-the-shelf embedded platform that outperform localization results from running a state-of-the-art algorithm on the same environment.","中文标题":"使用近似连续信念表示的空中飞行器快速蒙特卡洛定位","摘要翻译":"尺寸、重量和功率受限的平台对计算资源施加了限制，这在实现定位算法时引入了独特的挑战。我们提出了一个框架，通过点云数据的高斯混合模型表示的压缩能力，在此类平台上执行快速定位。给定来自深度传感器的原始结构数据以及来自机载姿态参考系统的俯仰和滚动估计，多假设粒子滤波器通过利用数据源自混合模型的可能性来定位飞行器。我们展示了在地面真实姿态附近对这种可能性的分析，并详细说明了其在基于粒子滤波器的飞行器定位策略中的利用，随后展示了在桌面系统和现成嵌入式平台上实时实现的结果，这些结果优于在同一环境中运行的最先进算法的定位结果。","领域":"空中飞行器定位/高斯混合模型/粒子滤波","问题":"在尺寸、重量和功率受限的平台上实现快速定位算法","动机":"解决在资源受限的平台上实现高效定位算法的挑战","方法":"利用高斯混合模型表示点云数据的压缩能力，结合多假设粒子滤波器进行定位","关键词":["高斯混合模型","粒子滤波","空中飞行器定位"],"涉及的技术概念":"高斯混合模型用于表示点云数据，多假设粒子滤波器用于定位，通过分析数据源自混合模型的可能性来实现定位。"},{"order":606,"title":"DeLS-3D: Deep Localization and Segmentation With a 3D Semantic Map","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_DeLS-3D_Deep_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_DeLS-3D_Deep_Localization_CVPR_2018_paper.html","abstract":"For applications such as augmented reality, autonomous driving, self-localization/camera pose estimation and scene parsing are crucial technologies. In this paper, we propose a unified framework to tackle these two problems simultaneously. The uniqueness of our design is a sensor fusion scheme which integrates camera videos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robustness and efficiency of the system.Specifically, we first have an initial coarse camera pose obtained from consumer-grade GPS/IMU, based on which a label map can be rendered from the 3D semantic map. Then, the rendered label map and the RGB image are jointly fed into a pose CNN, yielding a corrected camera pose. In addition, to incorporate temporal information, a multi-layer recurrent neural network (RNN) is further deployed improve the pose accuracy. Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces per-pixel semantic label. In order to validate our approach, we build a dataset with registered 3D point clouds and video camera images. Both the point clouds and the images are semantically-labeled. Each video frame has ground truth pose from highly accurate motion sensors. We show that practically, pose estimation solely relying on images like PoseNet~cite{Kendall_2015_ICCV} may fail due to street view confusion, and it is important to fuse multiple sensors. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system. In particular, we show that scene parsing and pose estimation are mutually beneficial to achieve a more robust and accurate system.","中文标题":"DeLS-3D: 使用3D语义地图进行深度定位与分割","摘要翻译":"对于增强现实、自动驾驶等应用，自我定位/相机姿态估计和场景解析是关键技术。在本文中，我们提出了一个统一的框架来同时解决这两个问题。我们设计的独特之处在于一个传感器融合方案，该方案集成了相机视频、运动传感器（GPS/IMU）和3D语义地图，以实现系统的鲁棒性和效率。具体来说，我们首先从消费级GPS/IMU获得初始粗略的相机姿态，基于此可以从3D语义地图渲染出标签图。然后，将渲染的标签图和RGB图像一起输入到姿态CNN中，产生校正后的相机姿态。此外，为了结合时间信息，进一步部署了多层递归神经网络（RNN）以提高姿态精度。最后，基于RNN的姿态，我们渲染一个新的标签图，该标签图与RGB图像一起输入到分割CNN中，产生每个像素的语义标签。为了验证我们的方法，我们构建了一个包含注册的3D点云和摄像机图像的数据集。点云和图像都进行了语义标注。每个视频帧都有来自高精度运动传感器的真实姿态。我们展示了实际上，像PoseNet~cite{Kendall_2015_ICCV}这样仅依赖图像的姿态估计可能会因街景混淆而失败，融合多个传感器是重要的。最后，进行了各种消融研究，证明了所提出系统的有效性。特别是，我们展示了场景解析和姿态估计相互促进，以实现更鲁棒和准确的系统。","领域":"增强现实/自动驾驶/传感器融合","问题":"同时解决自我定位/相机姿态估计和场景解析的问题","动机":"提高增强现实和自动驾驶等应用中的自我定位和场景解析的准确性和鲁棒性","方法":"提出一个统一的框架，通过传感器融合方案集成相机视频、运动传感器（GPS/IMU）和3D语义地图，使用姿态CNN和分割CNN进行姿态校正和语义标签生成，并通过多层RNN提高姿态精度","关键词":["传感器融合","3D语义地图","姿态估计","场景解析","递归神经网络"],"涉及的技术概念":"本文涉及的技术概念包括传感器融合、3D语义地图、姿态CNN、分割CNN、多层递归神经网络（RNN）、GPS/IMU、RGB图像、语义标签、点云注册等。"},{"order":607,"title":"LiDAR-Video Driving Dataset: Learning Driving Policies Effectively","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html","abstract":"Learning autonomous-driving policies is one of the most challenging but promising tasks for computer vision. Most researchers believe that future research and applications should combine cameras, video recorders and laser scanners to obtain comprehensive semantic understanding of real traffic. However, current approaches only learn from large-scale videos, due to the lack of benchmarks that consist of precise laser-scanner data. In this paper, we are the first to propose a LiDAR-Video dataset, which provides large-scale high-quality point clouds scanned by a Velodyne laser, videos recorded by a dashboard camera and standard drivers' behaviors. Extensive experiments demonstrate that extra depth information help networks to determine driving policies indeed.","中文标题":"LiDAR-视频驾驶数据集：有效学习驾驶策略","摘要翻译":"学习自动驾驶策略是计算机视觉中最具挑战性但最有前途的任务之一。大多数研究人员认为，未来的研究和应用应结合摄像头、视频记录仪和激光扫描仪，以获得对真实交通的全面语义理解。然而，由于缺乏包含精确激光扫描数据的基准，当前的方法仅从大规模视频中学习。在本文中，我们首次提出了一个LiDAR-视频数据集，该数据集提供了由Velodyne激光扫描的大规模高质量点云、由仪表盘摄像头录制的视频以及标准驾驶员行为。大量实验证明，额外的深度信息确实有助于网络确定驾驶策略。","领域":"自动驾驶/激光雷达/视频分析","问题":"缺乏包含精确激光扫描数据的基准，限制了自动驾驶策略的学习效果","动机":"为了获得对真实交通的全面语义理解，需要结合摄像头、视频记录仪和激光扫描仪的数据","方法":"提出了一个LiDAR-视频数据集，包含大规模高质量点云、视频和标准驾驶员行为，通过实验证明额外深度信息有助于网络确定驾驶策略","关键词":["自动驾驶","激光雷达","视频分析","点云","深度信息"],"涉及的技术概念":{"LiDAR":"激光雷达，一种通过发射激光来测量物体距离的遥感技术，用于生成精确的点云数据。","点云":"由激光雷达扫描得到的大量点数据，用于表示物体的三维形状和位置。","深度信息":"指物体与观察点之间的距离信息，对于理解场景的三维结构至关重要。","自动驾驶策略":"指自动驾驶系统根据环境信息做出驾驶决策的规则或算法。"}},{"order":608,"title":"Logo Synthesis and Manipulation With Clustered Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sage_Logo_Synthesis_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sage_Logo_Synthesis_and_CVPR_2018_paper.html","abstract":"Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld.","中文标题":"使用聚类生成对抗网络进行标志合成与操作","摘要翻译":"为新品牌设计标志是一个设计师与客户之间漫长而乏味的反复过程。在本文中，我们探讨了机器学习能在多大程度上解决设计师的创造性任务。为此，我们构建了一个包含60万+从全球网络爬取的标志的数据集——LLD。在这样的多模态数据上训练生成对抗网络（GANs）进行标志合成并不简单，并且会导致一些最先进方法的模式崩溃。我们提出了通过聚类获得的合成标签来解开和稳定GAN训练，并在CIFAR-10和ImageNet-small上验证了这种方法的通用性。我们能够生成高多样性的合理标志，并展示了潜在空间探索技术，以交互方式简化标志设计任务。GANs可以通过聚类实现的合成标签处理多模态数据，我们的结果展示了这种技术在标志合成和操作中的创造潜力。我们的数据集和模型可在https://data.vision.ee.ethz.ch/sagea/lld公开获取。","领域":"标志设计/生成对抗网络/数据聚类","问题":"解决在多模态数据上训练生成对抗网络进行标志合成时的模式崩溃问题","动机":"探索机器学习在标志设计这一创造性任务中的应用潜力，简化设计师与客户之间的反复过程","方法":"提出使用通过聚类获得的合成标签来解开和稳定GAN训练，并在CIFAR-10和ImageNet-small上验证了这种方法的通用性","关键词":["标志合成","生成对抗网络","数据聚类","潜在空间探索"],"涉及的技术概念":"生成对抗网络（GANs）是一种深度学习模型，通过两个网络——生成器和判别器的对抗过程来生成数据。数据聚类是一种将数据集中的对象分组的技术，使得同一组（即一个簇）中的对象比其他组的对象更相似。潜在空间探索技术指的是在生成模型的潜在空间中进行操作，以探索和生成新的数据样本。"},{"order":609,"title":"Egocentric Basketball Motion Planning From a Single First-Person Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bertasius_Egocentric_Basketball_Motion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bertasius_Egocentric_Basketball_Motion_CVPR_2018_paper.html","abstract":"We present a model that uses a single first-person image to generate an egocentric basketball motion sequence in the form of a 12D camera configuration trajectory, which encodes a player's 3D location and 3D head orientation throughout the sequence. To do this, we first introduce a future convolutional neural network (CNN) that predicts an initial sequence of 12D camera configurations, aiming to capture how real players move during a one-on-one basketball game. We also introduce a goal verifier network, which is trained to verify that a given camera configuration is consistent with the final goals of real one-on-one basketball players. Next, we propose an inverse synthesis procedure to synthesize a refined sequence of 12D camera configurations that (1) sufficiently matches the initial configurations predicted by the future CNN, while (2) maximizing the output of the goal verifier network. Finally, by following the trajectory resulting from the refined camera configuration sequence, we obtain the complete 12D motion sequence.  Our model generates realistic basketball motion sequences that capture the goals of real players, outperforming standard deep learning approaches such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and generative adversarial networks (GANs).","中文标题":"从单一第一人称图像进行自我中心篮球运动规划","摘要翻译":"我们提出了一个模型，该模型使用单一的第一人称图像生成以12D相机配置轨迹形式的自我中心篮球运动序列，该轨迹编码了玩家在整个序列中的3D位置和3D头部方向。为此，我们首先引入了一个未来卷积神经网络（CNN），它预测12D相机配置的初始序列，旨在捕捉真实玩家在一对一篮球比赛中的移动方式。我们还引入了一个目标验证网络，该网络被训练来验证给定的相机配置是否与真实一对一篮球玩家的最终目标一致。接下来，我们提出了一个逆合成程序来合成一个精炼的12D相机配置序列，该序列（1）充分匹配未来CNN预测的初始配置，同时（2）最大化目标验证网络的输出。最后，通过遵循由精炼相机配置序列产生的轨迹，我们获得了完整的12D运动序列。我们的模型生成的篮球运动序列捕捉了真实玩家的目标，优于标准的深度学习方法，如循环神经网络（RNNs）、长短期记忆网络（LSTMs）和生成对抗网络（GANs）。","领域":"运动分析/第一人称视角/篮球运动","问题":"如何从单一的第一人称图像生成逼真的篮球运动序列","动机":"捕捉并模拟真实篮球玩家在一对一比赛中的运动轨迹和头部方向","方法":"使用未来卷积神经网络预测初始相机配置序列，引入目标验证网络确保配置与玩家目标一致，通过逆合成程序精炼相机配置序列","关键词":["第一人称视角","篮球运动","运动序列生成"],"涉及的技术概念":"12D相机配置轨迹（编码玩家的3D位置和3D头部方向）、未来卷积神经网络（CNN）、目标验证网络、逆合成程序、循环神经网络（RNNs）、长短期记忆网络（LSTMs）、生成对抗网络（GANs）"},{"order":610,"title":"Human-Centric Indoor Scene Synthesis Using Stochastic Grammar","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Human-Centric_Indoor_Scene_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Human-Centric_Indoor_Scene_CVPR_2018_paper.html","abstract":"We present a human-centric method to sample and synthesize 3D room layouts and 2D images thereof, for the purpose of obtaining large-scale 2D/3D image data with the perfect per-pixel ground truth. An attributed spatial And-Or graph (S-AOG) is proposed to represent indoor scenes. The S-AOG is a probabilistic grammar model, in which the terminal nodes are object entities including room, furniture, and supported objects. Human contexts as contextual relations are encoded by Markov Random Fields (MRF) on the terminal nodes. We learn the distributions from an indoor scene dataset and sample new layouts using Monte Carlo Markov Chain. Experiments demonstrate that the proposed method can robustly sample a large variety of realistic room layouts based on three criteria: (i) visual realism comparing to a state-of-the-art room arrangement method, (ii) accuracy of the affordance maps with respect to ground-truth, and (ii) the functionality and naturalness of synthesized rooms evaluated by human subjects.","中文标题":"使用随机语法的人本室内场景合成","摘要翻译":"我们提出了一种以人为本的方法来采样和合成3D房间布局及其2D图像，目的是获得具有完美像素级地面真实的大规模2D/3D图像数据。提出了一种属性空间与或图（S-AOG）来表示室内场景。S-AOG是一种概率语法模型，其中终端节点是包括房间、家具和支持对象的对象实体。人类上下文作为上下文关系通过马尔可夫随机场（MRF）在终端节点上编码。我们从室内场景数据集中学习分布，并使用蒙特卡洛马尔可夫链采样新布局。实验表明，所提出的方法能够基于三个标准稳健地采样各种现实的房间布局：（i）与最先进的房间布置方法相比的视觉真实感，（ii）关于地面真实的可负担性图的准确性，以及（iii）由人类受试者评估的合成房间的功能性和自然性。","领域":"室内场景合成/3D建模/概率模型","问题":"如何有效地合成具有高真实感和功能性的室内场景布局","动机":"为了获得大规模且具有完美像素级地面真实的2D/3D图像数据，以支持计算机视觉和图形学的研究和应用","方法":"提出了一种属性空间与或图（S-AOG）来表示室内场景，并通过马尔可夫随机场（MRF）编码人类上下文关系，使用蒙特卡洛马尔可夫链从室内场景数据集中学习分布并采样新布局","关键词":["室内场景合成","3D建模","概率模型","马尔可夫随机场","蒙特卡洛马尔可夫链"],"涉及的技术概念":{"属性空间与或图（S-AOG）":"一种用于表示室内场景的概率语法模型，其中终端节点包括房间、家具和支持对象","马尔可夫随机场（MRF）":"用于在终端节点上编码人类上下文关系的概率模型","蒙特卡洛马尔可夫链":"一种从概率分布中采样的方法，用于生成新的室内场景布局"}},{"order":611,"title":"Rotation-Sensitive Regression for Oriented Scene Text Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Rotation-Sensitive_Regression_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liao_Rotation-Sensitive_Regression_for_CVPR_2018_paper.html","abstract":"Text in natural images is of arbitrary orientations, requiring detection in terms of oriented bounding boxes. Normally, a multi-oriented text detector often involves two key tasks: 1) text presence detection, which is a classification problem disregarding text orientation; 2) oriented bounding box regression, which concerns about text orientation. Previous methods rely on shared features for both tasks, resulting in degraded performance due to the incompatibility of the two tasks. To address this issue, we propose to perform classification and regression on features of different characteristics, extracted by two network branches of different designs. Concretely, the regression branch extracts rotation-sensitive features by actively rotating the convolutional filters, while the classification branch extracts rotation-invariant features by pooling the rotation-sensitive features. The proposed method named Rotation-sensitive Regression Detector (RRD) achieves state-of-the-art performance on several oriented scene text benchmark datasets, including ICDAR 2015, MSRA-TD500, RCTW-17, and COCO-Text. Furthermore, RRD achieves a significant improvement on a ship collection dataset, demonstrating its generality on oriented object detection.","中文标题":"面向场景文本检测的旋转敏感回归","摘要翻译":"自然图像中的文本具有任意方向，需要以定向边界框的形式进行检测。通常，多方向文本检测器涉及两个关键任务：1）文本存在检测，这是一个不考虑文本方向的分类问题；2）定向边界框回归，这涉及到文本方向。以前的方法依赖于两个任务的共享特征，由于这两个任务的不兼容性，导致性能下降。为了解决这个问题，我们提出在不同设计的两个网络分支上提取不同特征进行分类和回归。具体来说，回归分支通过主动旋转卷积滤波器提取旋转敏感特征，而分类分支通过池化旋转敏感特征提取旋转不变特征。所提出的方法名为旋转敏感回归检测器（RRD），在多个定向场景文本基准数据集上实现了最先进的性能，包括ICDAR 2015、MSRA-TD500、RCTW-17和COCO-Text。此外，RRD在船舶收集数据集上实现了显著改进，展示了其在定向物体检测上的通用性。","领域":"文本检测/物体检测/卷积神经网络","问题":"多方向文本检测中分类和回归任务的不兼容性问题","动机":"提高多方向文本检测的准确性和效率","方法":"通过两个不同设计的网络分支分别提取旋转敏感和旋转不变特征，进行分类和回归","关键词":["文本检测","旋转敏感回归","卷积神经网络"],"涉及的技术概念":"旋转敏感特征是通过旋转卷积滤波器提取的，而旋转不变特征是通过池化旋转敏感特征得到的。这种方法旨在解决多方向文本检测中分类和回归任务的不兼容性问题，通过分离这两个任务的特征提取过程来提高检测性能。"},{"order":612,"title":"Separating Self-Expression and Visual Content in Hashtag Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Veit_Separating_Self-Expression_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Veit_Separating_Self-Expression_and_CVPR_2018_paper.html","abstract":"The variety, abundance, and structured nature of hashtags make them an interesting data source for training vision models. For instance, hashtags have the potential to significantly reduce the problem of manual supervision and annotation when learning vision models for a large number of concepts. However, a key challenge when learning from hashtags is that they are inherently subjective because they are provided by users as a form of self-expression. As a consequence, hashtags may have synonyms (different hashtags referring to the same visual content) and may be polysemous (the same hashtag referring to different visual content). These challenges limit the effectiveness of approaches that simply treat hashtags as image-label pairs. This paper presents an approach that extends upon modeling simple image-label pairs with a joint model of images, hashtags, and users. We demonstrate the efficacy of such approaches in image tagging and retrieval experiments, and show how the joint model can be used to perform user-conditional retrieval and tagging.","中文标题":"在标签监督中分离自我表达和视觉内容","摘要翻译":"标签的多样性、丰富性和结构性使其成为训练视觉模型的一个有趣的数据源。例如，在学习大量概念的视觉模型时，标签有潜力显著减少手动监督和注释的问题。然而，从标签中学习的一个关键挑战是它们本质上是主观的，因为它们是由用户作为自我表达的一种形式提供的。因此，标签可能有同义词（不同的标签指代相同的视觉内容）并且可能是多义的（相同的标签指代不同的视觉内容）。这些挑战限制了简单将标签视为图像-标签对的方法的有效性。本文提出了一种方法，通过联合模型扩展了简单的图像-标签对模型，该模型包括图像、标签和用户。我们在图像标记和检索实验中证明了这种方法的有效性，并展示了如何使用联合模型执行用户条件检索和标记。","领域":"社交媒体分析/图像理解/用户行为分析","问题":"如何有效利用用户生成的标签来训练视觉模型，同时解决标签的主观性和多义性问题","动机":"减少视觉模型训练过程中对手动监督和注释的依赖，利用社交媒体上的丰富标签数据","方法":"提出了一种联合模型，该模型不仅考虑图像和标签之间的关系，还引入了用户因素，以更准确地理解和利用标签信息","关键词":["社交媒体","图像标记","用户行为"],"涉及的技术概念":"标签的主观性、标签的同义词和多义性、联合模型、用户条件检索和标记"},{"order":613,"title":"Distort-and-Recover: Color Enhancement Using Deep Reinforcement Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Distort-and-Recover_Color_Enhancement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Distort-and-Recover_Color_Enhancement_CVPR_2018_paper.html","abstract":"Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a \`distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the \`distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/","中文标题":"扭曲与恢复：使用深度强化学习的色彩增强","摘要翻译":"基于学习的色彩增强方法通常学习从输入图像到修饰图像的映射。大多数现有方法需要昂贵的输入-修饰图像对或以不可解释的方式产生结果。在本文中，我们提出了一种基于深度强化学习（DRL）的色彩增强方法，以明确模拟人类修饰过程的逐步性质。我们将色彩增强过程建模为马尔可夫决策过程，其中动作被定义为全局色彩调整操作。然后，我们训练我们的代理以学习动作的最优全局增强序列。此外，我们提出了一种‘扭曲与恢复’训练方案，该方案仅需要高质量参考图像进行训练，而不是输入和修饰图像对。给定高质量参考图像，我们扭曲图像的色彩分布并形成扭曲-参考图像对进行训练。通过大量实验，我们展示了我们的方法产生了不错的增强结果，并且我们的DRL方法比之前的监督方法更适合‘扭曲与恢复’训练方案。补充材料和代码可在https://sites.google.com/view/distort-and-recover/获取。","领域":"色彩增强/深度强化学习/图像修饰","问题":"如何有效地进行色彩增强，同时减少对昂贵输入-修饰图像对的依赖，并提高结果的可解释性。","动机":"现有色彩增强方法需要昂贵的输入-修饰图像对或以不可解释的方式产生结果，这限制了它们的应用和效果。","方法":"提出了一种基于深度强化学习的色彩增强方法，通过将色彩增强过程建模为马尔可夫决策过程，并采用‘扭曲与恢复’训练方案，仅需高质量参考图像进行训练。","关键词":["色彩增强","深度强化学习","图像修饰","马尔可夫决策过程","扭曲与恢复"],"涉及的技术概念":"深度强化学习（DRL）是一种结合了深度学习和强化学习的技术，用于解决决策问题。马尔可夫决策过程（MDP）是一种数学框架，用于建模决策问题，其中结果部分随机，部分受决策者控制。‘扭曲与恢复’训练方案是一种创新的训练方法，通过扭曲高质量参考图像的色彩分布来生成训练数据，从而减少对昂贵输入-修饰图像对的依赖。"},{"order":614,"title":"Im2Flow: Motion Hallucination From Static Images for Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.html","abstract":"Existing methods to recognize actions in static images take the images at their face value, learning the appearances---objects, scenes, and body poses---that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold.  First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map.  Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition.  On seven datasets, we demonstrate the power of the approach.  It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.","中文标题":"Im2Flow: 从静态图像中幻觉运动以进行动作识别","摘要翻译":"现有的静态图像动作识别方法仅从表面价值学习图像，即学习区分每个动作类别的外观——物体、场景和身体姿势。然而，这些模型缺乏定义人类活动的丰富动态结构和运动。我们提出了一种方法，通过幻觉单张快照中隐含的未观察到的未来运动来帮助静态图像动作识别。关键思想是从数千个未标记的视频中学习短期动态的先验，推断新静态图像上的预期光流，然后训练利用这两类信息的判别模型。我们的主要贡献有两个。首先，我们设计了一个编码器-解码器卷积神经网络和一种新颖的光流编码，可以将静态图像转换为准确的光流图。其次，我们展示了幻觉光流在识别中的力量，成功地将学习到的运动转移到标准的双流网络中进行活动识别。在七个数据集上，我们展示了该方法的力量。它不仅实现了密集光流预测的最先进准确性，而且持续增强了动作和动态场景的识别。","领域":"动作识别/光流预测/动态场景理解","问题":"静态图像动作识别中缺乏动态结构和运动信息的问题","动机":"为了增强静态图像动作识别的准确性，通过幻觉未观察到的未来运动来补充静态图像中的动态信息","方法":"设计了一个编码器-解码器卷积神经网络和一种新颖的光流编码，从静态图像中预测光流，并利用这些信息训练判别模型","关键词":["动作识别","光流预测","动态场景理解"],"涉及的技术概念":"编码器-解码器卷积神经网络用于从静态图像中预测光流，光流编码技术用于将静态图像转换为光流图，双流网络用于活动识别"},{"order":615,"title":"Finding \\"It\\": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Finding_It_Weakly-Supervised_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Finding_It_Weakly-Supervised_CVPR_2018_paper.html","abstract":"Grounding textual phrases in visual content with standalone image-sentence pairs is a challenging task. When we consider grounding in instructional videos, this problem becomes profoundly more complex: the latent temporal structure of instructional videos breaks independence assumptions and necessitates contextual understanding for resolving ambiguous visual-linguistic cues. Furthermore, dense annotations and video data scale mean supervised approaches are prohibitively costly. In this work, we propose to tackle this new task with a weakly-supervised framework for reference-aware visual grounding in instructional videos, where only the temporal alignment between the transcription and the video segment are available for supervision. We introduce the visually grounded action graph, a structured representation capturing the latent dependency between grounding and references in video. For optimization, we propose a new reference-aware multiple instance learning (RA-MIL) objective for weak supervision of grounding in videos. We evaluate our approach over unconstrained videos from YouCookII and RoboWatch, augmented with new reference-grounding test set annotations. We demonstrate that our jointly optimized, reference-aware approach simultaneously improves visual grounding, reference-resolution, and generalization to unseen instructional video categories.","中文标题":"寻找“它”：教学视频中的弱监督参考感知视觉定位","摘要翻译":"在视觉内容中定位文本短语与独立的图像-句子对是一项具有挑战性的任务。当我们考虑在教学视频中进行定位时，这个问题变得更加复杂：教学视频的潜在时间结构打破了独立性假设，并需要上下文理解来解决模糊的视觉-语言线索。此外，密集的注释和视频数据规模意味着监督方法的成本过高。在这项工作中，我们提出了一个弱监督框架来解决教学视频中的参考感知视觉定位这一新任务，其中只有转录和视频片段之间的时间对齐可用于监督。我们引入了视觉基础动作图，这是一种结构化表示，捕捉了视频中定位和参考之间的潜在依赖关系。为了优化，我们提出了一种新的参考感知多实例学习（RA-MIL）目标，用于视频中定位的弱监督。我们在来自YouCookII和RoboWatch的无约束视频上评估了我们的方法，并增加了新的参考定位测试集注释。我们证明了我们的联合优化、参考感知方法同时提高了视觉定位、参考解析和对未见过的教学视频类别的泛化能力。","领域":"教学视频分析/视觉语言定位/弱监督学习","问题":"在教学视频中定位文本短语的挑战","动机":"解决教学视频中由于时间结构和上下文依赖性导致的视觉-语言线索模糊问题，以及减少对密集注释的依赖","方法":"提出了一种弱监督框架，包括视觉基础动作图和参考感知多实例学习（RA-MIL）目标","关键词":["教学视频","视觉定位","弱监督学习","参考感知","多实例学习"],"涉及的技术概念":{"视觉基础动作图":"一种结构化表示，用于捕捉视频中定位和参考之间的潜在依赖关系","参考感知多实例学习（RA-MIL）":"一种新的优化目标，用于视频中定位的弱监督"}},{"order":616,"title":"Actor and Action Video Segmentation From a Sentence","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gavrilyuk_Actor_and_Action_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gavrilyuk_Actor_and_Action_CVPR_2018_paper.html","abstract":"This paper strives for pixel-level segmentation of actors and their actions in video content. Different from existing works, which all learn to segment from a fixed vocabulary of actor and action pairs, we infer the segmentation from a natural language input sentence. This allows to distinguish between fine-grained actors in the same super-category, identify actor and action instances, and segment pairs that are outside of the actor and action vocabulary. We propose a fully-convolutional model for pixel-level actor and action segmentation using an encoder-decoder architecture optimized for video. To show the potential of actor and action video segmentation from a sentence, we extend two popular actor and action datasets with more than 7,500 natural language descriptions. Experiments demonstrate the quality of the sentence-guided segmentations, the generalization ability of our model, and its advantage for traditional actor and action segmentation compared to the state-of-the-art.","中文标题":"从句子中进行演员和动作视频分割","摘要翻译":"本文致力于在视频内容中实现演员及其动作的像素级分割。与现有的工作不同，现有工作都是从固定的演员和动作对词汇中学习分割，我们通过自然语言输入句子推断分割。这使得我们能够区分同一超类别中的细粒度演员，识别演员和动作实例，并分割出演员和动作词汇之外的配对。我们提出了一个全卷积模型，用于使用针对视频优化的编码器-解码器架构进行像素级演员和动作分割。为了展示从句子中进行演员和动作视频分割的潜力，我们扩展了两个流行的演员和动作数据集，增加了超过7500条自然语言描述。实验证明了句子引导分割的质量，我们模型的泛化能力，以及与传统演员和动作分割相比的优势。","领域":"视频理解/自然语言处理/图像分割","问题":"实现视频中演员及其动作的像素级分割","动机":"现有方法局限于固定的演员和动作对词汇，无法处理细粒度区分和词汇外的配对","方法":"提出一个全卷积模型，使用编码器-解码器架构进行像素级分割，并通过自然语言输入句子推断分割","关键词":["视频分割","自然语言处理","像素级分割","编码器-解码器架构"],"涉及的技术概念":{"像素级分割":"指在图像或视频中对每个像素进行分类，以实现精确的对象或场景分割。","全卷积模型":"一种深度学习模型，专门设计用于处理图像分割任务，能够接受任意大小的输入图像并输出相应大小的分割图。","编码器-解码器架构":"一种常用的神经网络架构，编码器用于提取输入数据的特征，解码器则根据这些特征生成输出，广泛应用于图像分割、机器翻译等任务。","自然语言处理":"指使计算机能够理解、解释和生成人类语言的技术，本文中用于从自然语言描述中推断视频分割。"}},{"order":617,"title":"Egocentric Activity Recognition on a Budget","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Possas_Egocentric_Activity_Recognition_CVPR_2018_paper.html","abstract":"Recent advances in embedded technology have enabled more pervasive machine learning. One of the common applications in this field is Egocentric Activity Recognition (EAR), where users wearing a device such as a smartphone or smartglasses are able to receive feedback from the embedded device. Recent research on activity recognition has mainly focused on improving accuracy by using resource intensive techniques such as multi-stream deep networks. Although this approach has provided state-of-the-art results, in most cases it neglects the natural resource constraints (e.g. battery) of wearable devices. We develop a Reinforcement Learning model-free method to learn energy-aware policies that maximize the use of low-energy cost predictors while keeping competitive accuracy levels. Our results show that a policy trained on an egocentric dataset is able use the synergy between motion sensors and vision to effectively tradeoff energy expenditure and accuracy on smartglasses operating in realistic, real-world conditions.","中文标题":"预算内的自我中心活动识别","摘要翻译":"嵌入式技术的最近进展使得机器学习更加普及。在这一领域的常见应用之一是自我中心活动识别（EAR），用户佩戴如智能手机或智能眼镜等设备能够从嵌入式设备接收反馈。最近的活动识别研究主要集中在通过使用资源密集型技术（如多流深度网络）来提高准确性。尽管这种方法提供了最先进的结果，但在大多数情况下，它忽略了可穿戴设备的自然资源限制（例如电池）。我们开发了一种无模型的强化学习方法，以学习能量感知策略，这些策略在保持竞争力的准确性水平的同时，最大限度地利用低能量成本预测器。我们的结果表明，在自我中心数据集上训练的策略能够利用运动传感器和视觉之间的协同作用，在现实世界的智能眼镜操作条件下有效地权衡能量消耗和准确性。","领域":"可穿戴技术/嵌入式系统/强化学习","问题":"在资源受限的可穿戴设备上实现高效且准确的活动识别","动机":"解决可穿戴设备在活动识别中面临的资源限制问题，特别是电池寿命，同时保持识别准确性","方法":"开发了一种无模型的强化学习方法，学习能量感知策略，以在保持竞争力的准确性水平的同时，最大限度地利用低能量成本预测器","关键词":["自我中心活动识别","能量感知策略","强化学习","可穿戴设备"],"涉及的技术概念":"自我中心活动识别（EAR）是指通过用户佩戴的设备（如智能手机或智能眼镜）来识别用户的活动。强化学习是一种机器学习方法，它通过奖励和惩罚来学习策略，以在特定环境中实现目标。无模型强化学习指的是不依赖于环境模型的学习方法，直接通过试错来学习策略。能量感知策略是指在设计算法或系统时考虑到能量消耗，以优化设备的电池寿命。"},{"order":618,"title":"CNN in MRF: Video Object Segmentation via Inference in a CNN-Based Higher-Order Spatio-Temporal MRF","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bao_CNN_in_MRF_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bao_CNN_in_MRF_CVPR_2018_paper.html","abstract":"This paper addresses the problem of video object segmentation, where the initial object mask is given in the first frame of an input video. We propose a novel spatio-temporal Markov Random Field (MRF) model defined over pixels to handle this problem. Unlike conventional MRF models, the spatial dependencies among pixels in our model are encoded by a Convolutional Neural Network (CNN). Specifically, for a given object, the probability of a labeling to a set of spatially neighboring pixels can be predicted by a CNN trained for this specific object. As a result, higher-order, richer dependencies among pixels in the set can be implicitly modeled by the CNN. With temporal dependencies established by optical flow, the resulting MRF model combines both spatial and temporal cues for tackling video object segmentation. However, performing inference in the MRF model is very difficult due to the very high-order dependencies. To this end, we propose a novel CNN-embedded algorithm to perform approximate inference in the MRF. This algorithm proceeds by alternating between a temporal fusion step and a feed-forward CNN step. When initialized with an appearance-based one-shot segmentation CNN, our model outperforms the winning entries of the DAVIS 2017 Challenge, without resorting to model ensembling or any dedicated detectors.","中文标题":"CNN在MRF中的应用：通过基于CNN的高阶时空MRF推理进行视频对象分割","摘要翻译":"本文解决了视频对象分割的问题，其中初始对象掩码在输入视频的第一帧中给出。我们提出了一种新颖的时空马尔可夫随机场（MRF）模型，该模型在像素上定义以处理此问题。与传统的MRF模型不同，我们模型中像素之间的空间依赖性由卷积神经网络（CNN）编码。具体来说，对于给定对象，可以通过为此特定对象训练的CNN预测一组空间相邻像素的标签概率。因此，CNN可以隐式地建模集合中像素之间的高阶、更丰富的依赖性。通过光流建立的时间依赖性，生成的MRF模型结合了空间和时间线索以解决视频对象分割问题。然而，由于非常高阶的依赖性，在MRF模型中执行推理非常困难。为此，我们提出了一种新颖的CNN嵌入算法，以在MRF中执行近似推理。该算法通过在时间融合步骤和前馈CNN步骤之间交替进行。当使用基于外观的一次性分割CNN初始化时，我们的模型在DAVIS 2017挑战赛中胜出，而无需依赖模型集成或任何专用检测器。","领域":"视频对象分割/时空模型/卷积神经网络","问题":"视频对象分割","动机":"解决在给定初始对象掩码的情况下，如何有效地分割视频中的对象的问题","方法":"提出了一种新颖的时空马尔可夫随机场（MRF）模型，该模型通过卷积神经网络（CNN）编码像素之间的空间依赖性，并结合光流建立的时间依赖性，以及一种CNN嵌入算法进行近似推理","关键词":["视频对象分割","时空马尔可夫随机场","卷积神经网络","光流","近似推理"],"涉及的技术概念":{"时空马尔可夫随机场（MRF）模型":"一种在像素上定义的模型，用于处理视频对象分割问题，结合了空间和时间线索","卷积神经网络（CNN）":"用于编码像素之间的空间依赖性，并预测一组空间相邻像素的标签概率","光流":"用于建立像素之间的时间依赖性","CNN嵌入算法":"一种在MRF中执行近似推理的算法，通过在时间融合步骤和前馈CNN步骤之间交替进行"}},{"order":619,"title":"Action Sets: Weakly Supervised Action Segmentation Without Ordering Constraints","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Richard_Action_Sets_Weakly_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Richard_Action_Sets_Weakly_CVPR_2018_paper.html","abstract":"Action detection and temporal segmentation of actions in videos are topics of increasing interest. While fully supervised systems have gained much attention lately, full annotation of each action within the video is costly and impractical for large amounts of video data. Thus, weakly supervised action detection and temporal segmentation methods are of great importance. While most works in this area assume an ordered sequence of occurring actions to be given, our approach only uses a set of actions. Such action sets provide much less supervision since neither action ordering nor the number of action occurrences are known. In exchange, they can be easily obtained, for instance, from meta-tags, while ordered sequences still require human annotation. We introduce a system that automatically learns to temporally segment and label actions in a video, where the only supervision that is used are action sets. An evaluation on three datasets shows that our method still achieves good results although the amount of supervision is significantly smaller than for other related methods.","中文标题":"动作集：无需排序约束的弱监督动作分割","摘要翻译":"视频中的动作检测和时间分割是越来越受关注的话题。虽然完全监督的系统最近获得了大量关注，但对视频中每个动作的完全注释对于大量视频数据来说是成本高昂且不切实际的。因此，弱监督的动作检测和时间分割方法非常重要。虽然该领域的大多数工作假设给定了一个有序的动作序列，但我们的方法仅使用一组动作。这样的动作集提供的监督要少得多，因为既不知道动作的顺序，也不知道动作出现的次数。作为交换，它们可以很容易地获得，例如从元标签中获得，而有序序列仍然需要人工注释。我们引入了一个系统，该系统自动学习在视频中时间分割和标记动作，其中使用的唯一监督是动作集。在三个数据集上的评估显示，尽管监督量显著小于其他相关方法，但我们的方法仍然取得了良好的结果。","领域":"视频分析/动作识别/弱监督学习","问题":"视频中动作的检测和时间分割","动机":"减少对视频数据完全注释的依赖，利用更易获得的动作集进行弱监督学习","方法":"引入一个系统，该系统仅使用动作集作为监督，自动学习在视频中进行时间分割和标记动作","关键词":["动作检测","时间分割","弱监督学习","动作集"],"涉及的技术概念":"动作集：一组动作，不包含动作的顺序或出现次数的信息；弱监督学习：使用不完全或较弱的监督信息进行学习的方法；时间分割：将视频按时间分割成不同的动作片段。"},{"order":620,"title":"Low-Latency Video Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Low-Latency_Video_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Low-Latency_Video_Semantic_CVPR_2018_paper.html","abstract":"Recent years have seen remarkable progress in semantic segmentation. Yet, it remains a challenging task to apply segmentation techniques to video-based applications. Specifically, the high throughput of video streams, the sheer cost of running fully convolutional networks, together with the low-latency requirements in many real-world applications, e.g. autonomous driving, present a significant challenge to the design of the video segmentation framework. To tackle this combined challenge, we develop a framework for video semantic segmentation, which incorporates two novel components:(1) a feature propagation module that adaptively fuses features over time via spatially variant convolution, thus reducing the cost of per-frame computation; and (2) an adaptive scheduler that dynamically allocate computation based on accuracy prediction. Both components work together to ensure low latency while maintaining high segmentation quality. On both Cityscapes and CamVid, the proposed framework obtained competitive performance compared to the state of the art, while substantially reducing the latency, from 360 ms to 119 ms.","中文标题":"低延迟视频语义分割","摘要翻译":"近年来，语义分割领域取得了显著进展。然而，将分割技术应用于基于视频的应用仍然是一个具有挑战性的任务。具体来说，视频流的高吞吐量、运行全卷积网络的巨大成本，以及许多现实世界应用（如自动驾驶）中的低延迟要求，对视频分割框架的设计提出了重大挑战。为了应对这一综合挑战，我们开发了一个视频语义分割框架，该框架包含两个新颖的组件：（1）一个特征传播模块，通过空间变异卷积自适应地融合时间上的特征，从而减少每帧计算成本；（2）一个自适应调度器，基于精度预测动态分配计算。这两个组件共同工作，确保低延迟的同时保持高分割质量。在Cityscapes和CamVid上，所提出的框架与现有技术相比获得了竞争性的性能，同时显著降低了延迟，从360毫秒降至119毫秒。","领域":"视频分析/自动驾驶/实时系统","问题":"视频流的高吞吐量、运行全卷积网络的巨大成本以及低延迟要求对视频分割框架设计的挑战","动机":"为了在保持高分割质量的同时实现低延迟，满足如自动驾驶等实时应用的需求","方法":"开发了一个包含特征传播模块和自适应调度器的视频语义分割框架，特征传播模块通过空间变异卷积自适应地融合时间上的特征，自适应调度器基于精度预测动态分配计算","关键词":["视频语义分割","低延迟","特征传播","自适应调度"],"涉及的技术概念":"语义分割、全卷积网络、空间变异卷积、精度预测、实时系统"},{"order":621,"title":"Fine-Grained Video Captioning for Sports Narrative","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Fine-Grained_Video_Captioning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Fine-Grained_Video_Captioning_CVPR_2018_paper.html","abstract":"Despite recent emergence of video caption methods, how to generate fine-grained video descriptions (i.e., long and detailed commentary about individual movements of multiple subjects as well as their frequent interactions) is far from being solved, which however has great applications such as automatic sports narrative. To this end, this work makes the following contributions. First, to facilitate this novel research of fine-grained video caption, we collected a novel dataset called Fine-grained Sports Narrative dataset (FSN) that contains 2K sports videos with ground-truth narratives from YouTube.com. Second, we develop a novel performance evaluation metric named Fine-grained Captioning Evaluation (FCE) to cope with this novel task. Considered as an extension of the widely used METEOR, it measures not only the linguistic performance but also whether the action details and their temporal orders are correctly described. Third, we propose a new framework for fine-grained sports narrative task. This network features three branches: 1) a spatio-temporal entity localization and role discovering sub-network; 2) a fine-grained action modeling sub-network for local skeleton motion description; and 3) a group relationship modeling sub-network to model interactions between players. We further fuse the features and decode them into long narratives by a hierarchically recurrent structure. Extensive experiments on the FSN dataset demonstrates the validity of the proposed framework for fine-grained video caption.","中文标题":"细粒度体育叙事视频字幕生成","摘要翻译":"尽管最近出现了视频字幕生成方法，但如何生成细粒度的视频描述（即关于多个主体个体动作及其频繁互动的长而详细的评论）远未解决，然而这在自动体育叙事等应用中具有巨大潜力。为此，本工作做出了以下贡献。首先，为了促进这一细粒度视频字幕的新研究，我们收集了一个名为细粒度体育叙事数据集（FSN）的新数据集，该数据集包含来自YouTube.com的2K体育视频及其真实叙事。其次，我们开发了一种新的性能评估指标，名为细粒度字幕评估（FCE），以应对这一新任务。它被视为广泛使用的METEOR的扩展，不仅衡量语言表现，还衡量动作细节及其时间顺序是否正确描述。第三，我们提出了一个新的细粒度体育叙事任务框架。该网络具有三个分支：1）一个时空实体定位和角色发现子网络；2）一个用于局部骨骼运动描述的细粒度动作建模子网络；3）一个群体关系建模子网络，用于建模玩家之间的互动。我们进一步融合这些特征，并通过分层递归结构将它们解码为长叙事。在FSN数据集上的大量实验证明了所提出框架在细粒度视频字幕生成中的有效性。","领域":"视频字幕生成/体育分析/自然语言处理","问题":"生成细粒度的视频描述，特别是关于多个主体个体动作及其频繁互动的长而详细的评论","动机":"自动体育叙事等应用需要细粒度的视频描述，但目前的方法远未解决这一问题","方法":"收集新数据集FSN，开发新的性能评估指标FCE，并提出一个新的细粒度体育叙事任务框架，该框架包括时空实体定位和角色发现子网络、细粒度动作建模子网络和群体关系建模子网络，并通过分层递归结构融合特征并解码为长叙事","关键词":["细粒度视频描述","体育叙事","自动字幕生成"],"涉及的技术概念":"细粒度视频描述指的是对视频中多个主体的个体动作及其频繁互动进行详细描述。FSN数据集是一个包含2K体育视频及其真实叙事的新数据集。FCE是一种新的性能评估指标，用于评估细粒度字幕生成任务，它不仅衡量语言表现，还衡量动作细节及其时间顺序是否正确描述。提出的框架包括三个子网络：时空实体定位和角色发现子网络、细粒度动作建模子网络和群体关系建模子网络，并通过分层递归结构融合特征并解码为长叙事。"},{"order":622,"title":"End-to-End Learning of Motion Representation for Video Understanding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_End-to-End_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_End-to-End_Learning_of_CVPR_2018_paper.html","abstract":"Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach.  Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.","中文标题":"端到端学习视频理解中的运动表示","摘要翻译":"尽管最近端到端学习的表示取得了成功，但在视频分析任务中，手工制作的光流特征仍然被广泛使用。为了填补这一空白，我们提出了TVNet，一种新颖的端到端可训练神经网络，用于从数据中学习类似光流的特征。TVNet包含一个特定的光流求解器，即TV-L1方法，并通过将其优化迭代展开为神经层来初始化。因此，TVNet可以直接使用，无需任何额外的学习。此外，它可以自然地与其他任务特定的网络连接，形成一个端到端的架构，从而使我们的方法比当前的多阶段方法更高效，避免了预计算和存储特征到磁盘的需要。最后，TVNet的参数可以通过端到端训练进一步微调。这使得TVNet能够学习到比精确光流更丰富和任务特定的模式。在两个动作识别基准上的大量实验验证了所提出方法的有效性。我们的TVNet在特征提取时间方面与最快的对手竞争，同时实现了比所有比较方法更好的准确率。","领域":"视频分析/动作识别/光流估计","问题":"视频分析任务中手工制作的光流特征仍然被广泛使用，需要一种更高效的方法来学习光流特征","动机":"填补端到端学习表示与手工制作光流特征之间的空白，提高视频分析任务的效率和准确性","方法":"提出TVNet，一种端到端可训练的神经网络，通过学习类似光流的特征，并能够与其他任务特定的网络连接形成端到端架构","关键词":["视频分析","动作识别","光流估计","端到端学习","TVNet"],"涉及的技术概念":{"端到端学习":"一种学习方法，直接从输入到输出进行学习，无需手动设计特征","光流特征":"描述视频中物体运动信息的特征","TV-L1方法":"一种用于光流估计的优化方法，结合了总变差（TV）正则化和L1范数","动作识别":"识别视频中发生的动作或行为","特征提取时间":"从视频中提取特征所需的时间"}},{"order":623,"title":"Compressed Video Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Compressed_Video_Action_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html","abstract":"Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video.  This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.","中文标题":"压缩视频动作识别","摘要翻译":"训练鲁棒的深度视频表示已被证明比学习深度图像表示更具挑战性。这部分是由于原始视频流的巨大尺寸和高时间冗余；真实且有趣的信号常常被淹没在太多无关数据中。受到视频压缩（使用H.264、HEVC等）可以将多余信息减少多达两个数量级的启发，我们提出直接在压缩视频上训练深度网络。这种表示具有更高的信息密度，并且我们发现训练更容易。此外，压缩视频中的信号提供了免费但嘈杂的运动信息。我们提出了新技术来有效利用它们。我们的方法比Res3D快约4.6倍，比ResNet-152快2.7倍。在动作识别任务上，我们的方法在UCF-101、HMDB-51和Charades数据集上优于所有其他方法。","领域":"视频分析/动作识别/深度学习","问题":"如何有效训练深度视频表示以进行动作识别","动机":"原始视频流尺寸巨大且时间冗余高，导致训练深度视频表示困难","方法":"直接在压缩视频上训练深度网络，利用压缩视频中的高信息密度和免费运动信息","关键词":["视频压缩","动作识别","深度网络"],"涉及的技术概念":"视频压缩技术（如H.264、HEVC）用于减少视频数据中的冗余信息，提高信息密度；深度网络直接在压缩视频上进行训练，以利用压缩视频中的高信息密度和免费运动信息；动作识别任务在UCF-101、HMDB-51和Charades数据集上的性能比较。"},{"order":624,"title":"Features for Multi-Target Multi-Camera Tracking and Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ristani_Features_for_Multi-Target_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ristani_Features_for_Multi-Target_CVPR_2018_paper.html","abstract":"Multi-Target Multi-Camera Tracking (MTMCT) tracks many people through video taken from several cameras. Person Re-Identification (Re-ID) retrieves from a gallery images of people similar to a person query image. We learn good features for both MTMCT and Re-ID with a convolutional neural network. Our contributions include an adaptive weighted triplet loss for training and a new technique for hard-identity mining. Our method outperforms the state of the art both on the DukeMTMC benchmarks for tracking, and  on the Market-1501 and DukeMTMC-ReID benchmarks for Re-ID. We examine the correlation between good Re-ID and good MTMCT scores, and perform ablation studies to elucidate the contributions of the main components of our system. Code is available.","中文标题":"多目标多摄像头跟踪与重识别的特征","摘要翻译":"多目标多摄像头跟踪（MTMCT）通过从多个摄像头拍摄的视频中跟踪许多人。人员重识别（Re-ID）从图库中检索与查询图像中人物相似的人物图像。我们使用卷积神经网络为MTMCT和Re-ID学习良好的特征。我们的贡献包括用于训练的自适应加权三元组损失和一种新的硬身份挖掘技术。我们的方法在DukeMTMC跟踪基准测试以及Market-1501和DukeMTMC-ReID重识别基准测试中均优于现有技术。我们研究了良好Re-ID和良好MTMCT分数之间的相关性，并进行了消融研究以阐明我们系统主要组件的贡献。代码已提供。","领域":"视频监控/人员重识别/多摄像头跟踪","问题":"如何在多摄像头环境下有效跟踪多目标并进行人员重识别","动机":"提高多目标多摄像头跟踪和人员重识别的准确性和效率","方法":"使用卷积神经网络学习特征，采用自适应加权三元组损失进行训练，并开发新的硬身份挖掘技术","关键词":["多目标跟踪","人员重识别","卷积神经网络","三元组损失","硬身份挖掘"],"涉及的技术概念":{"多目标多摄像头跟踪（MTMCT）":"通过多个摄像头拍摄的视频跟踪多个目标的技术","人员重识别（Re-ID）":"从图库中检索与查询图像中人物相似的人物图像的技术","卷积神经网络":"一种深度学习模型，用于从图像中学习特征","自适应加权三元组损失":"一种用于训练深度学习模型的损失函数，通过调整权重来优化模型性能","硬身份挖掘":"一种技术，用于从数据中挖掘难以识别的身份信息，以提高模型的识别能力"}},{"order":625,"title":"AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_AVA_A_Video_CVPR_2018_paper.html","abstract":"This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions,  rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips.  AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 15.8% mAP, underscoring the need for developing new approaches for video understanding.","中文标题":"AVA：一个时空局部原子视觉动作的视频数据集","摘要翻译":"本文介绍了一个时空局部原子视觉动作（AVA）的视频数据集。AVA数据集在437个15分钟的视频片段中密集标注了80个原子视觉动作，其中动作在空间和时间上被定位，产生了1.59M个动作标签，每个人经常有多个标签。我们数据集的关键特点是：（1）原子视觉动作的定义，而不是复合动作；（2）精确的时空标注，可能对每个人有多个标注；（3）在15分钟的视频片段中对这些原子动作进行详尽标注；（4）人们在连续片段中时间上相连；（5）使用电影来收集各种动作表示。这与现有的时空动作识别数据集不同，后者通常为短视频片段中的复合动作提供稀疏标注。AVA以其现实的场景和动作复杂性，揭示了动作识别的内在困难。为了对此进行基准测试，我们提出了一种新的动作定位方法，该方法建立在当前最先进的方法之上，并在JHMDB和UCF101-24类别上展示了更好的性能。虽然在现有数据集上设定了新的技术水平，但AVA上的总体结果较低，为15.8% mAP，强调了开发新的视频理解方法的必要性。","领域":"动作识别/视频理解/时空分析","问题":"解决视频中原子视觉动作的时空定位问题","动机":"现有数据集通常为短视频片段中的复合动作提供稀疏标注，无法满足对视频中原子视觉动作进行详尽标注的需求","方法":"提出了一种新的动作定位方法，建立在当前最先进的方法之上，并在JHMDB和UCF101-24类别上展示了更好的性能","关键词":["原子视觉动作","时空标注","动作识别","视频理解"],"涉及的技术概念":{"原子视觉动作":"指的是视频中最基本的、不可再分的视觉动作单元","时空标注":"在视频中对动作进行空间（位置）和时间（时刻）上的精确标注","动作识别":"识别视频中发生的特定动作","视频理解":"对视频内容进行深入分析和理解，包括动作识别、场景理解等"}},{"order":626,"title":"Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Doughty_Whos_Better_Whos_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Doughty_Whos_Better_Whos_CVPR_2018_paper.html","abstract":"This paper presents a method for assessing skill from video, applicable to a variety of tasks, ranging from surgery to drawing and rolling pizza dough. We formulate the problem as pairwise (who’s better?) and overall (who’s best?) ranking of video collections, using supervised deep ranking. We propose a novel loss function that learns discriminative features when a pair of videos exhibit variance in skill, and learns shared features when a pair of videos exhibit comparable skill levels. Results demonstrate our method is applicable across tasks, with the percentage of correctly ordered pairs of videos ranging from 70% to 83% for four datasets. We demonstrate the robustness of our approach via sensitivity analysis of its parameters. We see this work as effort toward the automated organization of how-to video collections and overall, generic skill determination in video.","中文标题":"谁更好？谁最好？用于技能确定的成对深度排序","摘要翻译":"本文提出了一种从视频中评估技能的方法，适用于从手术到绘画和揉披萨面团等多种任务。我们将问题表述为视频集合的成对（谁更好？）和整体（谁最好？）排序，使用监督深度排序。我们提出了一种新颖的损失函数，当一对视频在技能上表现出差异时，学习区分性特征；当一对视频表现出相当的技能水平时，学习共享特征。结果表明，我们的方法适用于跨任务，四个数据集中正确排序的视频对百分比在70%到83%之间。我们通过对其参数的敏感性分析展示了我们方法的鲁棒性。我们将这项工作视为朝着自动化组织教学视频集合和视频中通用技能确定方向迈出的努力。","领域":"视频分析/技能评估/深度学习","问题":"如何从视频中评估和排序不同任务的技能水平","动机":"为了自动化组织教学视频集合和视频中通用技能确定","方法":"使用监督深度排序和一种新颖的损失函数来学习区分性和共享特征","关键词":["视频分析","技能评估","深度学习","成对排序","损失函数"],"涉及的技术概念":"监督深度排序是一种利用深度学习技术对视频进行排序的方法，通过比较视频对来评估技能水平。新颖的损失函数设计用于在视频对技能水平不同时学习区分性特征，在技能水平相同时学习共享特征。敏感性分析用于评估方法对参数变化的鲁棒性。"},{"order":627,"title":"MX-LSTM: Mixing Tracklets and Vislets to Jointly Forecast Trajectories and Head Poses","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hasan_MX-LSTM_Mixing_Tracklets_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hasan_MX-LSTM_Mixing_Tracklets_CVPR_2018_paper.html","abstract":"Recent approaches on trajectory forecasting use tracklets to predict the future positions of pedestrians exploiting Long Short Term Memory (LSTM) architectures. This paper shows that adding vislets, that is, short sequences of head pose estimations, allows to increase significantly the trajectory forecasting performance. We then propose to use vislets in a novel framework called MX-LSTM, capturing the interplay between tracklets and vislets thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. At the same time, MX-LSTM predicts the future head poses, increasing the standard capabilities of the long-term trajectory forecasting approaches. With standard head pose estimators and an attentional-based social pooling, Mixing-LSTM scores the new trajectory forecasting state-of-the-art in all the considered datasets (Zara01, Zara02, UCY, and TownCentre) with a dramatic margin when the pedestrians slow down, a case where most of the forecasting approaches struggle to provide an accurate solution.","中文标题":"MX-LSTM: 混合轨迹片段和视觉片段联合预测轨迹和头部姿态","摘要翻译":"最近的轨迹预测方法利用长短期记忆（LSTM）架构，通过轨迹片段预测行人的未来位置。本文展示了添加视觉片段，即头部姿态估计的短序列，可以显著提高轨迹预测的性能。我们随后提出在一个名为MX-LSTM的新框架中使用视觉片段，通过在LSTM反向传播期间对全协方差矩阵进行联合无约束优化，捕捉轨迹片段和视觉片段之间的相互作用。同时，MX-LSTM预测未来的头部姿态，增加了长期轨迹预测方法的标准能力。使用标准的头部姿态估计器和基于注意力的社会池化，Mixing-LSTM在所有考虑的数据集（Zara01、Zara02、UCY和TownCentre）中创造了新的轨迹预测最先进水平，在行人减速的情况下，大多数预测方法难以提供准确解决方案时，取得了显著的领先。","领域":"行人轨迹预测/头部姿态估计/社会行为分析","问题":"提高行人轨迹预测的准确性，特别是在行人减速的情况下","动机":"现有的轨迹预测方法在行人减速时难以提供准确预测，通过引入头部姿态估计的视觉片段，可以显著提高预测性能","方法":"提出MX-LSTM框架，通过联合无约束优化全协方差矩阵，捕捉轨迹片段和视觉片段之间的相互作用，同时预测未来的头部姿态","关键词":["轨迹预测","头部姿态估计","LSTM","社会池化"],"涉及的技术概念":"长短期记忆（LSTM）架构用于轨迹预测，视觉片段指的是头部姿态估计的短序列，MX-LSTM框架通过联合优化全协方差矩阵来捕捉轨迹和视觉片段之间的关系，基于注意力的社会池化用于提高预测性能。"},{"order":628,"title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html","abstract":"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.","中文标题":"自下而上和自上而下的注意力机制用于图像描述和视觉问答","摘要翻译":"自上而下的视觉注意力机制已广泛应用于图像描述和视觉问答（VQA）中，通过细粒度分析甚至多步推理实现更深层次的图像理解。在这项工作中，我们提出了一种结合自下而上和自上而下的注意力机制，使得注意力可以在对象和其他显著图像区域层面进行计算。这是考虑注意力的自然基础。在我们的方法中，自下而上的机制（基于Faster R-CNN）提出图像区域，每个区域都有一个相关的特征向量，而自上而下的机制确定特征权重。将这种方法应用于图像描述，我们在MSCOCO测试服务器上的结果为该任务建立了新的最先进水平，分别达到了CIDEr / SPICE / BLEU-4分数为117.9、21.5和36.9。展示了该方法的广泛适用性，将相同的方法应用于VQA，我们在2017年VQA挑战赛中获得第一名。","领域":"图像描述/视觉问答/注意力机制","问题":"如何通过结合自下而上和自上而下的注意力机制来提高图像描述和视觉问答的性能","动机":"为了通过细粒度分析和多步推理实现更深层次的图像理解，提高图像描述和视觉问答的准确性和效率","方法":"提出了一种结合自下而上（基于Faster R-CNN）和自上而下的注意力机制，其中自下而上机制提出图像区域及其特征向量，自上而下机制确定特征权重","关键词":["图像描述","视觉问答","注意力机制","Faster R-CNN","特征向量","特征权重"],"涉及的技术概念":{"自下而上注意力机制":"基于Faster R-CNN，提出图像区域及其特征向量","自上而下注意力机制":"确定特征权重，用于细粒度分析和多步推理","Faster R-CNN":"一种用于对象检测的深度学习模型，能够快速准确地识别图像中的对象","CIDEr / SPICE / BLEU-4":"用于评估图像描述质量的指标，分别衡量描述的准确性、语义一致性和与参考描述的相似度"}},{"order":629,"title":"Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Improved_Fusion_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nguyen_Improved_Fusion_of_CVPR_2018_paper.html","abstract":"A key solution to visual question answering (VQA) exists in how to fuse visual and language features extracted from an input image and question. We show that an attention mechanism that enables dense, bi-directional interactions between the two modalities contributes to boost accuracy of prediction of answers. Specifically, we present a simple architecture that is fully symmetric between visual and language representations, in which each question word attends on image regions and each image region attends on question words. It can be stacked to form a hierarchy for multi-step interactions between an image-question pair. We show through experiments that the proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0 despite its small size. We also present qualitative evaluation, demonstrating how the proposed attention mechanism can generate reasonable attention maps on images and questions, which leads to the correct answer prediction.","中文标题":"通过密集对称共同注意力改进视觉和语言表示的融合以用于视觉问答","摘要翻译":"视觉问答（VQA）的一个关键解决方案在于如何融合从输入图像和问题中提取的视觉和语言特征。我们展示了一种注意力机制，它能够在两种模态之间实现密集的双向交互，从而提高答案预测的准确性。具体来说，我们提出了一种在视觉和语言表示之间完全对称的简单架构，其中每个问题词都关注图像区域，每个图像区域也关注问题词。它可以堆叠起来形成图像-问题对之间多步交互的层次结构。我们通过实验表明，尽管体积小，所提出的架构在VQA和VQA 2.0上实现了新的最先进水平。我们还进行了定性评估，展示了所提出的注意力机制如何在图像和问题上生成合理的注意力图，从而引导出正确的答案预测。","领域":"视觉问答/注意力机制/特征融合","问题":"如何有效地融合视觉和语言特征以提高视觉问答的准确性","动机":"提高视觉问答系统中答案预测的准确性","方法":"提出了一种密集对称共同注意力机制，实现视觉和语言表示之间的双向交互，并通过堆叠形成多步交互的层次结构","关键词":["视觉问答","注意力机制","特征融合"],"涉及的技术概念":"密集对称共同注意力机制是一种在视觉和语言表示之间实现双向交互的技术，通过让每个问题词关注图像区域，每个图像区域也关注问题词，从而有效地融合视觉和语言特征，提高视觉问答系统的答案预测准确性。"},{"order":630,"title":"FlipDial: A Generative Model for Two-Way Visual Dialogue","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Massiceti_FlipDial_A_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Massiceti_FlipDial_A_Generative_CVPR_2018_paper.html","abstract":"We present FlipDial, a generative model for Visual Dialogue that simultaneously plays the role of both participants in a visually-grounded dialogue. Given context in the form of an image and an associated caption summarising the contents of the image, FlipDial learns both to answer questions and put forward questions, capable of generating entire sequences of dialogue (question-answer pairs) which are diverse and relevant to the image. To do this, FlipDial relies on a simple but surprisingly powerful idea: it uses convolutional neural networks (CNNs) to encode entire dialogues directly, implicitly capturing dialogue context, and conditional VAEs to learn the generative model. FlipDial outperforms the state-of-the-art model in the sequential answering task (1VD) on the VisDial dataset by 5 points in Mean Rank using the generated answers. We are the first to extend this paradigm to full two-way visual dialogue (2VD), where our model is capable of generating both questions and answers in sequence based on a visual input, for which we propose a set of novel evaluation measures and metrics.","中文标题":"FlipDial: 一种双向视觉对话生成模型","摘要翻译":"我们提出了FlipDial，一种用于视觉对话的生成模型，它同时扮演视觉基础对话中两个参与者的角色。给定以图像形式及其相关标题总结图像内容的上下文，FlipDial学习回答问题并提出问题，能够生成与图像相关且多样化的整个对话序列（问答对）。为此，FlipDial依赖于一个简单但异常强大的想法：它使用卷积神经网络（CNNs）直接编码整个对话，隐式捕捉对话上下文，并使用条件变分自编码器（VAEs）来学习生成模型。FlipDial在VisDial数据集上的顺序回答任务（1VD）中，使用生成的答案在平均排名上比现有最先进模型高出5分。我们是第一个将这一范式扩展到完整的双向视觉对话（2VD）的，我们的模型能够基于视觉输入顺序生成问题和答案，为此我们提出了一套新的评估措施和指标。","领域":"视觉对话/生成模型/对话系统","问题":"如何在视觉基础对话中同时生成问题和答案","动机":"为了在视觉对话中实现更自然和多样化的交互，需要一种能够同时生成问题和答案的模型。","方法":"使用卷积神经网络（CNNs）编码整个对话，并结合条件变分自编码器（VAEs）来学习生成模型。","关键词":["视觉对话","生成模型","卷积神经网络","条件变分自编码器"],"涉及的技术概念":{"卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据，能够自动提取图像特征。","条件变分自编码器（VAEs）":"一种生成模型，能够在给定条件下生成数据，用于学习数据的潜在表示。","视觉对话":"一种结合视觉信息和自然语言处理的交互式对话系统，旨在通过图像和文本的交互进行沟通。"}},{"order":631,"title":"Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Are_You_Talking_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Are_You_Talking_CVPR_2018_paper.html","abstract":"The Visual Dialogue task requires an agent to engage in a conversation about an image with a human.  It represents an extension of the Visual Question Answering task in that the agent needs to answer a question about an image, but it needs to do so in light of the previous dialogue that has taken place.  The key challenge in Visual Dialogue is thus maintaining a consistent, and natural dialogue while continuing to answer questions correctly.  We present a novel approach that combines Reinforcement Learning and Generative Adversarial Networks (GANs) to generate more human-like responses to questions.  The GAN helps overcome the relative paucity of training data, and the tendency of the typical MLE-based approach to generate overly terse answers. Critically, the GAN is tightly integrated into the attention mechanism that generates human-interpretable reasons for each answer.  This means that the discriminative model of the GAN has the task of assessing whether a candidate answer is generated by a human or not, given the provided reason.  This is significant because it drives the generative model to produce high quality answers that are well supported by the associated reasoning. The method also generates the state-of-the-art results on the primary benchmark.","中文标题":"你在和我说话吗？通过对抗学习生成有理由的视觉对话","摘要翻译":"视觉对话任务要求一个代理与人类就一幅图像进行对话。它代表了视觉问答任务的扩展，因为代理需要回答关于图像的问题，但需要根据之前发生的对话来进行回答。因此，视觉对话中的关键挑战是在继续正确回答问题的情况下，保持对话的一致性和自然性。我们提出了一种新颖的方法，结合了强化学习和生成对抗网络（GANs）来生成更类似于人类的问题回答。GAN有助于克服训练数据相对不足的问题，以及典型的基于最大似然估计（MLE）方法生成过于简洁答案的倾向。关键的是，GAN紧密集成到生成每个答案的人类可解释理由的注意力机制中。这意味着GAN的判别模型的任务是评估给定理由的情况下，候选答案是否由人类生成。这一点很重要，因为它推动生成模型生成高质量且得到相关推理支持的答案。该方法还在主要基准上生成了最先进的结果。","领域":"视觉对话/生成对抗网络/强化学习","问题":"在视觉对话任务中生成一致且自然的对话，同时正确回答问题","动机":"克服训练数据不足和生成答案过于简洁的问题，生成更类似于人类的问题回答","方法":"结合强化学习和生成对抗网络（GANs），将GAN紧密集成到生成人类可解释理由的注意力机制中","关键词":["视觉对话","生成对抗网络","强化学习","注意力机制"],"涉及的技术概念":"视觉对话任务扩展了视觉问答任务，要求代理根据之前的对话回答问题。使用强化学习和生成对抗网络（GANs）结合的方法，通过GAN克服训练数据不足和生成答案过于简洁的问题，GAN的判别模型评估候选答案是否由人类生成，推动生成模型生成高质量且得到相关推理支持的答案。"},{"order":632,"title":"Visual Question Generation as Dual Task of Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Visual_Question_Generation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Visual_Question_Generation_CVPR_2018_paper.html","abstract":"Visual question answering (VQA) and visual question generation (VQG) are two trending topics in the computer vision, but they are usually explored separately despite their intrinsic complementary relationship. In this paper, we propose an end-to-end unified model, the Invertible Question Answering Network (iQAN), to introduce question generation as a dual task of question answering to improve the VQA performance. With our proposed invertible bilinear fusion module and parameter sharing scheme, our iQAN can accomplish VQA and its dual task VQG simultaneously. By jointly trained on two tasks with our proposed dual regularizers~(termed as Dual Training), our model has a better understanding of the interactions among images, questions and answers. After training, iQAN can take either question or answer as input, and output the counterpart. Evaluated on the CLEVR and VQA2 datasets, our iQAN improves the top-1 accuracy of the prior art MUTAN VQA method by 1.33% and 0.88% (absolute increase). We also show that our proposed dual training framework can consistently improve model performances of many popular VQA architectures.","中文标题":"视觉问题生成作为视觉问题回答的对偶任务","摘要翻译":"视觉问题回答（VQA）和视觉问题生成（VQG）是计算机视觉中的两个热门话题，尽管它们之间存在内在的互补关系，但通常被分开探索。在本文中，我们提出了一个端到端的统一模型，即可逆问题回答网络（iQAN），将问题生成作为问题回答的对偶任务引入，以提高VQA的性能。通过我们提出的可逆双线性融合模块和参数共享方案，我们的iQAN可以同时完成VQA及其对偶任务VQG。通过在两个任务上联合训练，并采用我们提出的对偶正则化器（称为对偶训练），我们的模型能更好地理解图像、问题和答案之间的相互作用。训练后，iQAN可以接受问题或答案作为输入，并输出对应的部分。在CLEVR和VQA2数据集上的评估显示，我们的iQAN将现有技术MUTAN VQA方法的top-1准确率提高了1.33%和0.88%（绝对增长）。我们还展示了我们提出的对偶训练框架可以持续提高许多流行VQA架构的模型性能。","领域":"视觉问题回答/视觉问题生成/深度学习模型","问题":"如何通过将视觉问题生成作为视觉问题回答的对偶任务来提高视觉问题回答的性能","动机":"探索视觉问题回答和视觉问题生成之间的互补关系，以提高视觉问题回答的性能","方法":"提出了一个端到端的统一模型，即可逆问题回答网络（iQAN），包括可逆双线性融合模块和参数共享方案，以及对偶训练框架","关键词":["视觉问题回答","视觉问题生成","对偶任务","可逆问题回答网络","对偶训练"],"涉及的技术概念":"可逆问题回答网络（iQAN）是一种端到端的统一模型，通过可逆双线性融合模块和参数共享方案，同时完成视觉问题回答（VQA）和视觉问题生成（VQG）任务。对偶训练框架通过联合训练这两个任务，并采用对偶正则化器，提高模型对图像、问题和答案之间相互作用的理解。"},{"order":633,"title":"Unsupervised Textual Grounding: Linking Words to Image Concepts","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yeh_Unsupervised_Textual_Grounding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yeh_Unsupervised_Textual_Grounding_CVPR_2018_paper.html","abstract":"Textual grounding, i.e., linking words to objects in images, is a challenging but important task for robotics and human-computer interaction. Existing techniques benefit from recent progress in deep learning and generally formulate the task as a supervised learning problem, selecting a bounding box from a set of possible options. To train these deep net based approaches, access to a large-scale datasets is required, however, constructing such a dataset is time-consuming and expensive. Therefore, we develop a completely unsupervised mechanism for textual grounding using hypothesis testing as a mechanism to link words to detected image concepts. We demonstrate our approach on the ReferIt Game dataset and the Flickr30k data, outperforming baselines by 7.98% and 6.96% respectively.","中文标题":"无监督文本定位：将词语链接到图像概念","摘要翻译":"文本定位，即将词语与图像中的对象链接起来，对于机器人和人机交互来说是一项具有挑战性但重要的任务。现有技术得益于深度学习的最新进展，通常将该任务表述为监督学习问题，从一组可能的选项中选择一个边界框。为了训练这些基于深度网络的方法，需要访问大规模的数据集，然而，构建这样的数据集既耗时又昂贵。因此，我们开发了一种完全无监督的文本定位机制，使用假设检验作为将词语链接到检测到的图像概念的机制。我们在ReferIt Game数据集和Flickr30k数据上展示了我们的方法，分别比基线高出7.98%和6.96%。","领域":"文本定位/无监督学习/假设检验","问题":"如何在没有大规模标注数据集的情况下，有效地将文本中的词语与图像中的对象进行链接。","动机":"构建大规模标注数据集既耗时又昂贵，限制了基于深度学习的文本定位技术的发展。","方法":"开发了一种完全无监督的文本定位机制，使用假设检验作为将词语链接到检测到的图像概念的机制。","关键词":["文本定位","无监督学习","假设检验","ReferIt Game","Flickr30k"],"涉及的技术概念":"文本定位是指将文本中的词语与图像中的对象进行链接的任务。无监督学习是一种不需要标注数据的学习方法。假设检验是一种统计方法，用于判断样本数据是否支持某个假设。ReferIt Game和Flickr30k是用于文本定位任务的数据集。"},{"order":634,"title":"Focal Visual-Text Attention for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.html","abstract":"Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset.","中文标题":"焦点视觉-文本注意力用于视觉问答","摘要翻译":"最近关于语言和视觉与神经网络的见解已成功应用于简单的单图像视觉问答。然而，要解决个人照片等多媒体集合上的现实生活问答问题，我们必须查看包含照片或视频序列的整个集合。当从一个大集合中回答问题时，一个自然的问题是识别支持答案的片段。在本文中，我们描述了一种名为焦点视觉-文本注意力网络（FVTA）的新型神经网络，用于视觉问答中的集体推理，其中呈现了视觉和文本序列信息，如图像和文本元数据。FVTA引入了一种端到端的方法，利用分层过程动态确定在序列数据中关注哪些媒体和什么时间以回答问题。FVTA不仅能够很好地回答问题，而且还提供了系统结果所基于的答案理由。FVTA在MemexQA数据集上实现了最先进的性能，并在MovieQA数据集上取得了竞争性的结果。","领域":"视觉问答/多媒体理解/序列数据处理","问题":"解决在多媒体集合中进行视觉问答时，如何识别支持答案的片段的问题","动机":"为了应对现实生活中的问答问题，特别是在处理包含照片或视频序列的多媒体集合时，需要一种能够动态确定关注点和时间的方法来提高问答系统的性能。","方法":"提出了一种名为焦点视觉-文本注意力网络（FVTA）的新型神经网络，该网络采用端到端的方法，通过分层过程动态确定在序列数据中关注哪些媒体和什么时间以回答问题。","关键词":["视觉问答","多媒体理解","序列数据处理"],"涉及的技术概念":"焦点视觉-文本注意力网络（FVTA）是一种新型神经网络，用于处理视觉和文本序列信息，如图像和文本元数据，通过分层过程动态确定在序列数据中关注哪些媒体和什么时间以回答问题。"},{"order":635,"title":"SeGAN: Segmenting and Generating the Invisible","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ehsani_SeGAN_Segmenting_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ehsani_SeGAN_Segmenting_and_CVPR_2018_paper.html","abstract":"Objects often occlude each other in scenes; Inferring their appearance beyond their visible parts plays an important role in scene understanding, depth estimation, object interaction and manipulation. In this paper, we study the challenging problem of completing the appearance of occluded objects. Doing so requires knowing which pixels to paint (segmenting the invisible parts of objects) and what color to paint them (generating the invisible parts). Our proposed novel solution, SeGAN, jointly optimizes for both segmentation and generation of the invisible parts of objects. Our experimental results show that: (a) SeGAN can learn to generate the appearance of the occluded parts of objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the invisible parts of objects; (c) trained on synthetic photo realistic images, SeGAN can reliably segment natural images; (d) by reasoning about occluder-occludee relations, our method can infer depth layering.","中文标题":"SeGAN：分割和生成不可见部分","摘要翻译":"在场景中，物体经常相互遮挡；推断它们超出可见部分的外观在场景理解、深度估计、物体交互和操作中扮演着重要角色。在本文中，我们研究了完成被遮挡物体外观的挑战性问题。这样做需要知道哪些像素需要绘制（分割物体的不可见部分）以及用什么颜色绘制它们（生成不可见部分）。我们提出的新颖解决方案SeGAN，联合优化了物体的不可见部分的分割和生成。我们的实验结果表明：（a）SeGAN能够学习生成物体被遮挡部分的外观；（b）SeGAN在物体不可见部分的分割上优于最先进的分割基线；（c）在合成的照片级真实感图像上训练的SeGAN能够可靠地分割自然图像；（d）通过推理遮挡者与被遮挡者的关系，我们的方法能够推断深度层次。","领域":"场景理解/深度估计/物体交互","问题":"完成被遮挡物体外观的挑战性问题","动机":"推断物体超出可见部分的外观在场景理解、深度估计、物体交互和操作中扮演着重要角色","方法":"提出的新颖解决方案SeGAN，联合优化了物体的不可见部分的分割和生成","关键词":["场景理解","深度估计","物体交互","分割","生成"],"涉及的技术概念":"SeGAN是一种联合优化分割和生成物体不可见部分的方法，通过学习生成被遮挡部分的外观，优于现有的分割基线，能够在合成照片级真实感图像上训练后可靠地分割自然图像，并通过推理遮挡者与被遮挡者的关系推断深度层次。"},{"order":636,"title":"Cascade R-CNN: Delving Into High Quality Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.html","abstract":"In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.","中文标题":"级联R-CNN：深入高质量目标检测","摘要翻译":"在目标检测中，需要一个交并比（IoU）阈值来定义正样本和负样本。使用低IoU阈值（例如0.5）训练的目标检测器通常会产生噪声检测。然而，随着IoU阈值的增加，检测性能往往会下降。造成这种情况的两个主要因素是：1）由于正样本数量指数级减少，导致训练过程中的过拟合；2）检测器最优的IoU与输入假设的IoU在推理时的不匹配。为了解决这些问题，提出了一种多阶段目标检测架构——级联R-CNN。它由一系列使用递增IoU阈值训练的检测器组成，以逐步对接近的假阳性进行更严格的选择。检测器是分阶段训练的，利用了一个观察结果：一个检测器的输出是训练下一个更高质量检测器的良好分布。逐步改进的假设的重采样保证了所有检测器都有等量的正样本集，减少了过拟合问题。在推理时也应用了相同的级联程序，使得假设与每个阶段的检测器质量更匹配。一个简单的级联R-CNN实现在具有挑战性的COCO数据集上超越了所有单一模型的目标检测器。实验还表明，级联R-CNN广泛适用于各种检测器架构，无论基线检测器的强度如何，都能实现一致的增益。代码可在https://github.com/zhaoweicai/cascade-rcnn获取。","领域":"目标检测/深度学习/计算机视觉","问题":"目标检测中随着IoU阈值增加，检测性能下降的问题","动机":"解决目标检测器在训练过程中由于正样本数量减少导致的过拟合问题，以及推理时检测器最优IoU与输入假设IoU不匹配的问题","方法":"提出了一种多阶段目标检测架构——级联R-CNN，通过一系列使用递增IoU阈值训练的检测器，逐步对接近的假阳性进行更严格的选择，并在推理时应用相同的级联程序","关键词":["级联R-CNN","目标检测","IoU阈值","过拟合","COCO数据集"],"涉及的技术概念":"级联R-CNN是一种多阶段目标检测架构，通过递增IoU阈值训练一系列检测器来解决目标检测中的过拟合和推理时IoU不匹配的问题。这种方法利用了一个检测器的输出作为训练下一个更高质量检测器的良好分布，通过逐步改进的假设重采样来减少过拟合，并在推理时实现假设与检测器质量的更佳匹配。"},{"order":637,"title":"Learning Semantic Concepts and Order for Image and Sentence Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Learning_Semantic_Concepts_CVPR_2018_paper.html","abstract":"Image and sentence matching has made great progress recently, but it remains challenging due to the large visual semantic discrepancy. This mainly arises from that the representation of pixel-level image usually lacks of high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced image and sentence matching model, which can improve the image representation by learning semantic concepts and then organizing them in a correct semantic order. Given an image, we first use a multi-regional multi-label CNN to predict its semantic concepts, including objects, properties, actions, etc. Then, considering that different orders of semantic concepts lead to diverse semantic meanings, we use a context-gated sentence generation scheme for semantic order learning. It simultaneously uses the image global context containing concept relations as reference and the groundtruth semantic order in the matched sentence as supervision. After obtaining the improved image representation, we learn the sentence representation with a conventional LSTM, and then jointly perform image and sentence matching and sentence generation for model learning. Extensive experiments demonstrate the effectiveness of our learned semantic concepts and order, by achieving the state-of-the-art results on two public benchmark datasets.","中文标题":"学习语义概念和顺序以进行图像和句子匹配","摘要翻译":"图像和句子匹配最近取得了很大进展，但由于视觉语义差异大，仍然具有挑战性。这主要是由于像素级图像的表示通常缺乏其匹配句子中的高级语义信息。在这项工作中，我们提出了一种语义增强的图像和句子匹配模型，该模型通过学习语义概念然后以正确的语义顺序组织它们来改进图像表示。给定一张图像，我们首先使用多区域多标签CNN预测其语义概念，包括对象、属性、动作等。然后，考虑到语义概念的不同顺序会导致不同的语义含义，我们使用上下文门控的句子生成方案进行语义顺序学习。它同时使用包含概念关系的图像全局上下文作为参考，并以匹配句子中的真实语义顺序作为监督。在获得改进的图像表示后，我们使用传统的LSTM学习句子表示，然后联合执行图像和句子匹配以及句子生成以进行模型学习。大量实验证明了我们学习的语义概念和顺序的有效性，通过在两个公共基准数据集上实现了最先进的结果。","领域":"语义理解/图像表示/句子生成","问题":"解决图像和句子匹配中的视觉语义差异问题","动机":"由于像素级图像的表示通常缺乏高级语义信息，导致图像和句子匹配存在挑战","方法":"提出语义增强的图像和句子匹配模型，通过多区域多标签CNN预测语义概念，并使用上下文门控的句子生成方案进行语义顺序学习","关键词":["语义概念","图像表示","句子生成","上下文门控","LSTM"],"涉及的技术概念":"多区域多标签CNN用于预测图像的语义概念，包括对象、属性、动作等；上下文门控的句子生成方案用于学习语义顺序，同时利用图像全局上下文和匹配句子中的真实语义顺序作为监督；LSTM用于学习句子表示，并联合执行图像和句子匹配以及句子生成。"},{"order":638,"title":"Functional Map of the World","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Christie_Functional_Map_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Christie_Functional_Map_of_CVPR_2018_paper.html","abstract":"We present a new dataset, Functional Map of the World (fMoW), which aims to inspire the development of machine learning models capable of predicting the functional purpose of buildings and land use from temporal sequences of satellite images and a rich set of metadata features. The metadata provided with each image enables reasoning about location, time, sun angles, physical sizes, and other features when making predictions about objects in the image. Our dataset consists of over 1 million images from over 200 countries. For each image, we provide at least one bounding box annotation containing one of 63 categories, including a \\"false detection\\" category. We present an analysis of the dataset along with baseline approaches that reason about metadata and temporal views. Our data, code, and pretrained models have been made publicly available.","中文标题":"世界功能地图","摘要翻译":"我们提出了一个新的数据集，世界功能地图（fMoW），旨在激发能够从卫星图像的时间序列和丰富的元数据特征中预测建筑物和土地利用功能目的的机器学习模型的开发。每张图像提供的元数据使得在预测图像中的对象时能够考虑位置、时间、太阳角度、物理大小等特征。我们的数据集包含来自200多个国家的超过100万张图像。对于每张图像，我们提供了至少一个包含63个类别之一的边界框注释，其中包括一个“错误检测”类别。我们提供了对数据集的分析以及考虑元数据和时间视图的基线方法。我们的数据、代码和预训练模型已经公开。","领域":"卫星图像分析/地理信息系统/时间序列分析","问题":"如何从卫星图像的时间序列和元数据中预测建筑物和土地利用的功能目的","动机":"开发能够从卫星图像和元数据中预测建筑物和土地利用功能目的的机器学习模型","方法":"提出了一个新的数据集fMoW，包含超过100万张来自200多个国家的卫星图像，每张图像都附有元数据和至少一个边界框注释。提供了对数据集的分析和基线方法，这些方法考虑了元数据和时间视图。","关键词":["卫星图像","元数据","时间序列","边界框注释","功能预测"],"涉及的技术概念":"卫星图像分析涉及从卫星拍摄的图像中提取信息。元数据包括图像的位置、时间、太阳角度、物理大小等特征，用于辅助图像分析。时间序列分析涉及分析随时间变化的图像序列以识别模式或变化。边界框注释用于标记图像中的特定对象或区域。功能预测指的是预测建筑物或土地的用途或功能。"},{"order":639,"title":"MegDet: A Large Mini-Batch Object Detector","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_MegDet_A_Large_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Peng_MegDet_A_Large_CVPR_2018_paper.html","abstract":"The development of object detection in the era of deep learning, from R-CNN [11], Fast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly come from novel network, new framework, or loss design. How- ever, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detec- tion. In this paper, we propose a Large Mini-Batch Object Detector (MegDet) to enable the training with a large mini- batch size up to 256, so that we can effectively utilize at most 128 GPUs to significantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-GPU Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The MegDet is the backbone of our sub- mission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st place of Detection task.","中文标题":"MegDet: 一个大批量目标检测器","摘要翻译":"在深度学习时代，目标检测的发展，从R-CNN [11]、Fast/Faster R-CNN [10, 31]到最近的Mask R-CNN [14]和RetinaNet [24]，主要来自于新颖的网络、新的框架或损失设计。然而，小批量大小，作为深度神经网络训练的一个关键因素，在目标检测中尚未得到充分研究。在本文中，我们提出了一个大批量目标检测器（MegDet），以支持多达256的大批量训练，从而我们能够有效利用最多128个GPU显著缩短训练时间。技术上，我们建议使用预热学习率策略和跨GPU批量归一化，这两者共同使我们能够在更短的时间内（例如，从33小时到4小时）成功训练一个大批量检测器，并实现更好的准确率。MegDet是我们提交给COCO 2017挑战赛（mmAP 52.5%）的基础，我们在检测任务中获得了第一名。","领域":"目标检测/深度学习/批量训练","问题":"如何在目标检测中有效利用大批量进行训练","动机":"探索大批量训练在目标检测中的应用，以缩短训练时间并提高准确率","方法":"提出预热学习率策略和跨GPU批量归一化，支持大批量训练","关键词":["目标检测","大批量训练","深度学习"],"涉及的技术概念":"预热学习率策略是一种在训练初期逐渐增加学习率的方法，以避免训练初期的不稳定性。跨GPU批量归一化是一种技术，允许在多个GPU之间同步批量归一化统计数据，以支持大批量训练。"},{"order":640,"title":"Learning Globally Optimized Object Detector via Policy Gradient","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rao_Learning_Globally_Optimized_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rao_Learning_Globally_Optimized_CVPR_2018_paper.html","abstract":"In this paper, we propose a simple yet effective method to learn globally optimized detector for object detection, which is a simple modification to the standard cross-entropy gradient inspired by the REINFORCE algorithm. In our approach, the cross-entropy gradient is adaptively adjusted according to overall mean Average Precision (mAP) of the current state for each detection candidate, which leads to more effective gradient and global optimization of detection results, and brings no computational overhead. Benefiting from more precise gradients produced by the global optimization method, our framework significantly improves state-of-the-art object detectors. Furthermore, since our method is based on scores and bounding boxes without modification on the architecture of object detector, it can be easily applied to off-the-shelf modern object detection frameworks.","中文标题":"通过策略梯度学习全局优化的目标检测器","摘要翻译":"在本文中，我们提出了一种简单而有效的方法来学习全局优化的目标检测器，这是对标准交叉熵梯度的一种简单修改，灵感来自于REINFORCE算法。在我们的方法中，交叉熵梯度根据每个检测候选的当前状态的总体平均精度（mAP）自适应调整，这导致了更有效的梯度和检测结果的全局优化，并且没有带来计算开销。得益于全局优化方法产生的更精确的梯度，我们的框架显著改进了最先进的目标检测器。此外，由于我们的方法基于分数和边界框，而不修改目标检测器的架构，因此可以轻松应用于现成的现代目标检测框架。","领域":"目标检测/优化算法/深度学习","问题":"如何提高目标检测器的全局优化效果","动机":"为了提高目标检测器的性能，通过全局优化方法调整交叉熵梯度，以实现更有效的梯度和检测结果的全局优化。","方法":"提出了一种基于REINFORCE算法灵感的方法，通过自适应调整交叉熵梯度，根据每个检测候选的当前状态的总体平均精度（mAP）来实现全局优化。","关键词":["目标检测","全局优化","策略梯度","REINFORCE算法","交叉熵梯度"],"涉及的技术概念":{"交叉熵梯度":"一种用于衡量模型预测与真实标签之间差异的梯度计算方法。","REINFORCE算法":"一种策略梯度方法，用于在强化学习中优化策略。","平均精度（mAP）":"用于评估目标检测器性能的指标，计算所有类别的平均精度。","全局优化":"指在整个解空间中寻找最优解的过程，而不是局部最优解。"}},{"order":641,"title":"Photographic Text-to-Image Synthesis With a Hierarchically-Nested Adversarial Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Photographic_Text-to-Image_Synthesis_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Photographic_Text-to-Image_Synthesis_CVPR_2018_paper.html","abstract":"This paper presents a novel method to deal with the challenging task of generating photographic images conditioned on semantic image descriptions. Our method introduces accompanying hierarchical-nested adversarial objectives inside the network hierarchies, which regularize mid-level representations and assist generator training to capture the complex image statistics. We present an extensile single-stream generator architecture to better adapt the jointed discriminators and push generated images up to high resolutions. We adopt a multi-purpose adversarial loss to encourage more effective image and text information usage in order to improve the semantic consistency and image fidelity simultaneously. Furthermore, we introduce a new visual-semantic similarity measure to evaluate the semantic consistency of generated images. With extensive experimental validation on three public datasets, our method significantly improves previous state of the arts on all datasets over different evaluation metrics.","中文标题":"基于层次嵌套对抗网络的摄影文本到图像合成","摘要翻译":"本文提出了一种新颖的方法来处理基于语义图像描述生成摄影图像的挑战性任务。我们的方法在网络层次结构中引入了伴随的层次嵌套对抗目标，这些目标规范了中层表示并辅助生成器训练以捕捉复杂的图像统计信息。我们提出了一种可扩展的单流生成器架构，以更好地适应联合判别器并将生成的图像推至高分辨率。我们采用了一种多用途的对抗损失，以鼓励更有效的图像和文本信息使用，从而同时提高语义一致性和图像保真度。此外，我们引入了一种新的视觉-语义相似性度量来评估生成图像的语义一致性。通过在三个公共数据集上的广泛实验验证，我们的方法在不同评估指标上显著改进了所有数据集上的先前技术水平。","领域":"图像生成/语义理解/对抗网络","问题":"如何基于语义图像描述生成高质量的摄影图像","动机":"提高生成图像的语义一致性和图像保真度，以更好地适应复杂的图像统计信息","方法":"引入层次嵌套对抗目标，采用可扩展的单流生成器架构和多用途对抗损失，以及新的视觉-语义相似性度量","关键词":["图像生成","语义理解","对抗网络","高分辨率图像","语义一致性"],"涉及的技术概念":"层次嵌套对抗目标用于规范中层表示和辅助生成器训练；可扩展的单流生成器架构适应联合判别器并生成高分辨率图像；多用途对抗损失提高图像和文本信息的使用效率；新的视觉-语义相似性度量评估生成图像的语义一致性。"},{"order":642,"title":"Illuminant Spectra-Based Source Separation Using Flash Photography","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_Illuminant_Spectra-Based_Source_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_Illuminant_Spectra-Based_Source_CVPR_2018_paper.html","abstract":"Real-world lighting often consists of multiple illuminants with different spectra. Separating and manipulating these illuminants in post-process is a challenging problem that requires either significant manual input or calibrated scene geometry and lighting. In this work, we leverage a flash/no-flash image pair to analyze and edit scene illuminants based on their spectral differences. We derive a novel physics-based relationship between color variations in the observed flash/no-flash intensities and the spectra and surface shading corresponding to individual scene illuminants. Our technique uses this constraint to automatically separate an image into constituent images lit by each illuminant. This separation can be used to support applications like white balancing, lighting editing, and RGB photometric stereo, where we demonstrate results that outperform state-of-the-art techniques on a wide range of images.","中文标题":"基于光源光谱的闪光摄影光源分离","摘要翻译":"现实世界的光照通常由具有不同光谱的多个光源组成。在后期处理中分离和操作这些光源是一个具有挑战性的问题，需要大量的人工输入或校准的场景几何和光照。在这项工作中，我们利用闪光/无闪光图像对，基于它们的光谱差异来分析和编辑场景光源。我们推导出了一个新颖的基于物理的关系，该关系描述了观察到的闪光/无闪光强度中的颜色变化与单个场景光源的光谱和表面阴影之间的关系。我们的技术利用这一约束自动将图像分离为由每个光源照亮的组成图像。这种分离可以用于支持白平衡、光照编辑和RGB光度立体等应用，在这些应用中，我们展示了在广泛图像上超越现有技术的结果。","领域":"光源分离/图像编辑/光度立体","问题":"在后期处理中分离和操作由不同光谱的多个光源组成的现实世界光照","动机":"减少对大量人工输入或校准的场景几何和光照的依赖，实现自动化的光源分离和编辑","方法":"利用闪光/无闪光图像对，基于光谱差异分析和编辑场景光源，并推导出基于物理的关系来自动分离图像","关键词":["光源分离","图像编辑","光度立体","白平衡","光照编辑"],"涉及的技术概念":"闪光/无闪光图像对：通过比较同一场景在有闪光和无闪光条件下的图像来提取信息。基于物理的关系：利用物理原理来描述和预测图像中的颜色变化与光源光谱和表面阴影之间的关系。自动图像分离：通过算法自动将图像分解为由不同光源照亮的组成部分。"},{"order":643,"title":"Trapping Light for Time of Flight","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Trapping_Light_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Trapping_Light_for_CVPR_2018_paper.html","abstract":"We propose a novel imaging method for near-complete, surround, 3D reconstruction of geometrically complex objects, in a single shot. The key idea is to augment a time-of-flight (ToF) based 3D sensor with a multi-mirror system, called a light-trap. The shape of the trap is chosen so that light rays entering it bounce multiple times inside the trap, thereby visiting every position inside the trap multiple times from various directions. We show via simulations that this enables light rays to reach more than 99.9% of the surface of objects placed inside the trap, even those with strong occlusions, for example, lattice-shaped objects. The ToF sensor provides the path length for each light ray, which, along with the known shape of the trap, is used to reconstruct the complete paths of all the rays. This enables performing dense, surround 3D reconstructions of objects with highly complex 3D shapes, in a single shot. We have developed a proof-of-concept hardware prototype consisting of a pulsed ToF sensor, and a light trap built with planar mirrors. We demonstrate the effectiveness of the light trap based 3D reconstruction method on a variety of objects with a broad range of geometry and reflectance properties.","中文标题":"捕捉光用于飞行时间","摘要翻译":"我们提出了一种新颖的成像方法，用于在单次拍摄中对几何复杂物体进行近乎完全、环绕的3D重建。关键思想是通过一个称为光陷阱的多镜系统来增强基于飞行时间（ToF）的3D传感器。选择陷阱的形状，使得进入其中的光线在陷阱内部多次反弹，从而从各个方向多次访问陷阱内的每个位置。我们通过模拟显示，这使得光线能够到达放置在陷阱内的物体表面的99.9%以上，即使是那些具有强烈遮挡的物体，例如格子形状的物体。ToF传感器提供每条光线的路径长度，这与已知的陷阱形状一起用于重建所有光线的完整路径。这使得能够在单次拍摄中对具有高度复杂3D形状的物体进行密集、环绕的3D重建。我们已经开发了一个概念验证的硬件原型，包括一个脉冲ToF传感器和一个用平面镜构建的光陷阱。我们展示了基于光陷阱的3D重建方法在各种具有广泛几何和反射特性的物体上的有效性。","领域":"3D重建/飞行时间成像/光学传感器","问题":"如何在单次拍摄中对几何复杂物体进行近乎完全、环绕的3D重建","动机":"解决传统3D重建方法在处理具有强烈遮挡和复杂几何形状物体时的局限性","方法":"通过一个多镜系统（光陷阱）增强基于飞行时间的3D传感器，利用光线的多次反弹和已知的陷阱形状重建物体的3D形状","关键词":["3D重建","飞行时间","光陷阱","多镜系统","几何复杂物体"],"涉及的技术概念":"飞行时间（ToF）传感器通过测量光线从发射到接收的时间来计算距离，光陷阱通过多镜系统使光线在内部多次反弹，从而实现对物体表面的全方位覆盖和3D重建。"},{"order":644,"title":"The Perception-Distortion Tradeoff","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Blau_The_Perception-Distortion_Tradeoff_CVPR_2018_paper.html","abstract":"Image restoration algorithms are typically evaluated by some distortion measure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify perceived perceptual quality. In this paper, we prove mathematically that distortion and perceptual quality are at odds with each other. Specifically, we study the optimal probability for correctly discriminating the outputs of an image restoration algorithm from real images. We show that as the mean distortion decreases, this probability must increase (indicating worse perceptual quality). As opposed to the common belief, this result holds true for any distortion measure, and is not only a problem of the PSNR or SSIM criteria. However, as we show experimentally, for some measures it is less severe (e.g. distance between VGG features). We also show that generative-adversarial-nets (GANs) provide a principled way to approach the perception-distortion bound. This constitutes theoretical support to their observed success in low-level vision tasks. Based on our analysis, we propose a new methodology for evaluating image restoration methods, and use it to perform an extensive comparison between recent super-resolution algorithms.","中文标题":"感知-失真权衡","摘要翻译":"图像恢复算法通常通过一些失真度量（例如PSNR、SSIM、IFC、VIF）或通过量化感知质量的人类意见评分来评估。在本文中，我们从数学上证明了失真和感知质量是相互矛盾的。具体来说，我们研究了正确区分图像恢复算法输出与真实图像的最佳概率。我们表明，随着平均失真的减少，这个概率必须增加（表明感知质量更差）。与普遍看法相反，这一结果适用于任何失真度量，而不仅仅是PSNR或SSIM标准的问题。然而，正如我们实验所示，对于某些度量来说，这种情况不那么严重（例如VGG特征之间的距离）。我们还表明，生成对抗网络（GANs）提供了一种原则性的方法来接近感知-失真界限。这为它们在低级视觉任务中观察到的成功提供了理论支持。基于我们的分析，我们提出了一种新的评估图像恢复方法的方法论，并用它对最近的超分辨率算法进行了广泛的比较。","领域":"图像恢复/超分辨率/生成对抗网络","问题":"图像恢复算法在失真度量和感知质量之间的权衡问题","动机":"探索和证明失真和感知质量之间的矛盾关系，以及如何通过生成对抗网络（GANs）来接近感知-失真界限","方法":"数学证明失真和感知质量之间的矛盾关系，实验比较不同失真度量的影响，提出基于生成对抗网络（GANs）的新方法论","关键词":["图像恢复","超分辨率","生成对抗网络"],"涉及的技术概念":"失真度量（PSNR、SSIM、IFC、VIF）、感知质量、生成对抗网络（GANs）、VGG特征距离"},{"order":645,"title":"Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Faces","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Label_Denoising_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Label_Denoising_Adversarial_CVPR_2018_paper.html","abstract":"Lighting estimation from faces is an important task and has applications in many areas such as image editing, intrinsic image decomposition, and image forgery detection. We propose to train a deep Convolutional Neural Network (CNN) to regress lighting parameters from a single face image. Lacking massive ground truth lighting labels for face images in the wild, we use an existing method to estimate lighting parameters, which are treated as ground truth with noise. To alleviate the effect of such noise, we utilize the idea of Generative Adversarial Networks (GAN) and propose a Label Denoising Adversarial Network (LDAN). LDAN makes use of synthetic data with accurate ground truth to help train a deep CNN for lighting regression on real face images. Experiments show that our network outperforms existing methods in producing consistent lighting parameters of different faces under similar lighting conditions. To further evaluate the proposed method, we also apply it to regress object 2D key points where ground truth labels are available. Our experiments demonstrate its effectiveness on this application.","中文标题":"用于面部逆光照的标签去噪对抗网络（LDAN）","摘要翻译":"从面部估计光照是一项重要任务，在图像编辑、本质图像分解和图像伪造检测等多个领域有广泛应用。我们提出训练一个深度卷积神经网络（CNN）从单张面部图像回归光照参数。由于缺乏大量野外面部图像的真实光照标签，我们使用现有方法估计光照参数，这些参数被视为带有噪声的真实标签。为了减轻这种噪声的影响，我们利用生成对抗网络（GAN）的思想，提出了标签去噪对抗网络（LDAN）。LDAN利用具有准确真实标签的合成数据来帮助训练深度CNN，以在真实面部图像上进行光照回归。实验表明，我们的网络在产生相似光照条件下不同面部的一致光照参数方面优于现有方法。为了进一步评估所提出的方法，我们还将其应用于回归对象2D关键点，其中真实标签是可用的。我们的实验证明了其在此应用中的有效性。","领域":"光照估计/图像编辑/图像伪造检测","问题":"从单张面部图像准确估计光照参数","动机":"缺乏大量野外面部图像的真实光照标签，需要一种方法来减轻估计光照参数时的噪声影响","方法":"提出标签去噪对抗网络（LDAN），利用生成对抗网络（GAN）的思想和具有准确真实标签的合成数据来训练深度卷积神经网络（CNN）进行光照回归","关键词":["光照估计","图像编辑","图像伪造检测","卷积神经网络","生成对抗网络","标签去噪"],"涉及的技术概念":{"卷积神经网络（CNN）":"一种深度学习模型，特别适用于处理图像数据。","生成对抗网络（GAN）":"由生成器和判别器组成的网络，通过对抗过程生成数据。","光照参数":"描述光照条件的参数，如光源的位置、强度和颜色等。","标签去噪":"一种减少或消除数据标签中噪声的技术，以提高模型的训练效果。"}},{"order":646,"title":"Optimal Structured Light à La Carte","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mirdehghan_Optimal_Structured_Light_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mirdehghan_Optimal_Structured_Light_CVPR_2018_paper.html","abstract":"We consider the problem of automatically generating sequences of structured-light patterns for active stereo triangulation of a static scene. Unlike existing approaches that use predetermined patterns and reconstruction algorithms tied to them, we generate patterns on the fly in response to generic specifications: number of patterns, projector-camera arrangement, workspace constraints, spatial frequency content, etc. Our pattern sequences are specifically optimized to minimize the expected rate of correspondence errors under those specifications for an unknown scene, and are coupled to a sequence-independent algorithm for per-pixel disparity estimation. To achieve this, we derive an objective function that is easy to optimize and follows from first principles within a maximum-likelihood framework. By minimizing it, we demonstrate automatic discovery of pattern sequences, in under three minutes on a laptop, that can outperform state-of-the-art triangulation techniques.","中文标题":"最优结构化光定制","摘要翻译":"我们考虑自动生成结构化光模式序列的问题，用于静态场景的主动立体三角测量。与现有方法使用预定的模式及与之绑定的重建算法不同，我们根据通用规格动态生成模式：模式数量、投影仪-相机布置、工作空间限制、空间频率内容等。我们的模式序列特别优化，以最小化在未知场景下这些规格对应的预期对应错误率，并与一个序列无关的算法耦合，用于每像素视差估计。为此，我们推导出一个易于优化的目标函数，该函数遵循最大似然框架内的第一原理。通过最小化它，我们展示了在笔记本电脑上不到三分钟内自动发现模式序列，这些序列可以超越最先进的三角测量技术。","领域":"立体视觉/三维重建/光模式设计","问题":"自动生成用于静态场景主动立体三角测量的结构化光模式序列","动机":"现有方法使用预定的模式及与之绑定的重建算法，限制了灵活性和优化潜力","方法":"根据通用规格动态生成模式，并优化模式序列以最小化预期对应错误率，耦合序列无关的算法进行每像素视差估计","关键词":["结构化光模式","立体三角测量","视差估计","最大似然框架"],"涉及的技术概念":"结构化光模式用于三维重建，通过投影特定的光模式到场景上，然后通过相机捕捉这些模式的变化来估计场景的三维结构。主动立体三角测量是一种通过使用主动光源（如激光或投影仪）来增强立体视觉系统性能的技术。最大似然框架是一种统计方法，用于估计模型参数，使得观察到的数据在该模型下的概率最大化。"},{"order":647,"title":"Tracking Multiple Objects Outside the Line of Sight Using Speckle Imaging","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Smith_Tracking_Multiple_Objects_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Smith_Tracking_Multiple_Objects_CVPR_2018_paper.html","abstract":"This paper presents techniques for tracking non-line-of-sight (NLOS) objects using speckle imaging. We develop a novel speckle formation and motion model where both the sensor and the source view objects only indirectly via a diffuse wall. We show that this NLOS imaging scenario is analogous to direct LOS imaging with the wall acting as a virtual, bare (lens-less) sensor. This enables tracking of a single, rigidly moving NLOS object using existing speckle-based motion estimation techniques. However, when imaging multiple NLOS objects, the speckle components due to different objects are superimposed on the virtual bare sensor image, and cannot be analyzed separately for recovering the motion of individual objects. We develop a novel clustering algorithm based on the statistical and geometrical properties of speckle images, which enables identifying the motion trajectories of multiple, independently moving NLOS objects. We demonstrate, for the first time, tracking individual trajectories of multiple objects around a corner with extreme precision (< 10 microns) using only off-the-shelf imaging components.","中文标题":"使用散斑成像技术追踪视线外的多个物体","摘要翻译":"本文介绍了使用散斑成像技术追踪非视线内（NLOS）物体的方法。我们开发了一种新颖的散斑形成和运动模型，其中传感器和光源仅通过漫反射墙间接观察物体。我们展示了这种NLOS成像场景类似于直接视线成像，其中墙充当虚拟的无镜头传感器。这使得可以使用现有的基于散斑的运动估计技术来追踪单个刚性移动的NLOS物体。然而，当对多个NLOS物体进行成像时，由于不同物体引起的散斑成分在虚拟无镜头传感器图像上叠加，无法单独分析以恢复单个物体的运动。我们开发了一种基于散斑图像的统计和几何特性的新颖聚类算法，该算法能够识别多个独立移动的NLOS物体的运动轨迹。我们首次展示了仅使用现成的成像组件，以极高的精度（<10微米）追踪拐角处多个物体的个体轨迹。","领域":"散斑成像/非视线成像/运动追踪","问题":"如何在视线外追踪多个独立移动的物体","动机":"研究动机是为了解决在视线外环境中追踪多个独立移动物体的挑战，特别是在仅通过漫反射墙间接观察物体的情况下。","方法":"开发了一种新颖的散斑形成和运动模型，并提出了一种基于散斑图像统计和几何特性的聚类算法，以识别多个独立移动的NLOS物体的运动轨迹。","关键词":["散斑成像","非视线成像","运动追踪","聚类算法"],"涉及的技术概念":"散斑成像是一种利用光通过粗糙表面（如漫反射墙）后形成的散斑图案来获取物体信息的技术。非视线成像指的是在物体不在直接视线内的情况下，通过间接方式（如反射）来观察和追踪物体。运动追踪涉及识别和记录物体随时间移动的轨迹。聚类算法是一种将数据集中的对象分组，使得同一组（即一个簇）中的对象彼此相似，而与其他组中的对象相异的算法。"},{"order":648,"title":"Inferring Light Fields From Shadows","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baradad_Inferring_Light_Fields_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baradad_Inferring_Light_Fields_CVPR_2018_paper.html","abstract":"We present a method for inferring a 4D light field of a hidden scene from 2D shadows cast by a known occluder on a diffuse wall. We do this by determining how light naturally reflected off surfaces in the hidden scene interacts with the occluder. By modeling the light transport as a linear system, and incorporating prior knowledge about light field structures, we can invert the system to recover the hidden scene. We demonstrate results of our inference method across simulations and experiments with different types of occluders. For instance, using the shadow cast by a real house plant, we are able to recover low resolution light fields with different levels of texture and parallax complexity. We provide two experimental results: a human subject and two planar elements at different depths.","中文标题":"从阴影推断光场","摘要翻译":"我们提出了一种方法，用于从已知遮挡物在漫反射墙上投射的2D阴影中推断隐藏场景的4D光场。我们通过确定隐藏场景中表面自然反射的光如何与遮挡物相互作用来实现这一点。通过将光传输建模为线性系统，并结合关于光场结构的先验知识，我们可以反转系统以恢复隐藏场景。我们在模拟和实验中展示了我们的推断方法的结果，使用了不同类型的遮挡物。例如，使用真实室内植物投射的阴影，我们能够恢复具有不同纹理和视差复杂度的低分辨率光场。我们提供了两个实验结果：一个人类受试者和两个不同深度的平面元素。","领域":"光场成像/阴影分析/场景重建","问题":"如何从2D阴影中推断出隐藏场景的4D光场","动机":"探索从已知遮挡物在漫反射墙上投射的2D阴影中恢复隐藏场景的4D光场的可能性，以扩展场景重建的技术边界","方法":"通过建模光传输为线性系统，并结合光场结构的先验知识，反转系统以恢复隐藏场景","关键词":["光场成像","阴影分析","场景重建"],"涉及的技术概念":"4D光场指的是在空间中每一点从所有方向捕捉到的光的信息，2D阴影是由遮挡物在漫反射墙上投射的阴影，线性系统模型用于描述光从隐藏场景到观察点的传输过程，先验知识指的是对光场结构的预先了解，用于辅助系统的反转和场景的恢复。"},{"order":649,"title":"Modifying Non-Local Variations Across Multiple Views","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tlusty_Modifying_Non-Local_Variations_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tlusty_Modifying_Non-Local_Variations_CVPR_2018_paper.html","abstract":"We present an algorithm for modifying small non-local variations between repeating structures and patterns in multiple images of the same scene. The modification is consistent across views, even-though the images could have been photographed from different view points and under different lighting conditions. We show that when modifying each image independently the correspondence between them breaks and the geometric structure of the scene gets distorted. Our approach modifies the views while maintaining correspondence, hence, we succeed in modifying appearance and structure variations consistently. We demonstrate our methods on a number of challenging examples, photographed in different lighting, scales and view points.","中文标题":"修改跨多个视图的非局部变化","摘要翻译":"我们提出了一种算法，用于修改同一场景多张图像中重复结构和模式之间的小的非局部变化。即使这些图像可能是在不同的视角和光照条件下拍摄的，修改在所有视图中都是一致的。我们展示了当独立修改每张图像时，它们之间的对应关系会断裂，场景的几何结构会失真。我们的方法在修改视图的同时保持了对应关系，因此我们成功地一致地修改了外观和结构的变化。我们在多个具有挑战性的例子上展示了我们的方法，这些例子是在不同的光照、比例和视角下拍摄的。","领域":"图像处理/计算机视觉/三维重建","问题":"解决在多个视角和光照条件下拍摄的同一场景图像中，重复结构和模式之间的非局部变化修改问题","动机":"为了在修改图像时保持图像间的对应关系和场景的几何结构，避免独立修改每张图像导致的对应关系断裂和结构失真","方法":"提出了一种算法，该算法在修改视图的同时保持图像间的对应关系，从而一致地修改外观和结构的变化","关键词":["非局部变化","重复结构","多视图一致性"],"涉及的技术概念":"非局部变化指的是图像中不局限于局部区域的变化，可能涉及整个图像或图像中的大区域。重复结构和模式指的是图像中重复出现的视觉元素或图案。多视图一致性指的是在多个视角下拍摄的图像中，对图像进行的修改在所有视图中保持一致。"},{"order":650,"title":"Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Robust_Video_Content_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Robust_Video_Content_CVPR_2018_paper.html","abstract":"Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of misalignment blur. Extensive evaluations show that up to 5dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.","中文标题":"在CNN框架中实现鲁棒视频内容对齐与补偿以去除雨滴","摘要翻译":"去除雨滴对于提高基于户外视觉系统的鲁棒性非常重要。当前的去雨方法在快速移动摄像机拍摄的复杂动态场景或暴雨遮挡下显示出局限性。我们提出了一种新颖的去雨算法，该算法应用超像素（SP）分割将场景分解为深度一致的单位。场景内容在SP级别对齐，这证明对雨滴遮挡和快速摄像机运动具有鲁棒性。两个对齐输出张量，即最佳时间匹配张量和排序的空间-时间匹配张量，为雨滴条纹位置和被遮挡的背景内容提供了信息线索，以生成中间去雨输出。这些张量随后将作为卷积神经网络的输入特征，以恢复中间输出的高频细节，以补偿对齐模糊。广泛的评估显示，与最先进的方法相比，重建PSNR优势高达5dB。视觉检查显示，特别是对于来自快速移动摄像机的高动态场景和大量不透明降雨，实现了更干净的去雨效果。","领域":"视频去雨/超像素分割/卷积神经网络","问题":"在复杂动态场景和暴雨遮挡下去除雨滴","动机":"提高户外视觉系统在恶劣天气条件下的鲁棒性","方法":"应用超像素分割将场景分解为深度一致的单位，并在SP级别对齐场景内容，使用卷积神经网络恢复高频细节以补偿对齐模糊","关键词":["视频去雨","超像素分割","卷积神经网络","动态场景","暴雨遮挡"],"涉及的技术概念":{"超像素（SP）分割":"一种图像分割技术，将图像分割成多个具有相似特征的区域，称为超像素。","卷积神经网络（CNN）":"一种深度学习模型，特别适用于处理图像数据，能够自动提取图像特征。","PSNR（峰值信噪比）":"一种衡量图像重建质量的指标，值越高表示重建质量越好。"}},{"order":651,"title":"SfSNet: Learning Shape, Reflectance and Illuminance of Faces \`in the Wild'","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sengupta_SfSNet_Learning_Shape_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sengupta_SfSNet_Learning_Shape_CVPR_2018_paper.html","abstract":"We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real world images. This allows the network to capture low frequency variations from synthetic and high frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation.","中文标题":"SfSNet：学习野外人脸形状、反射率和照度","摘要翻译":"我们提出了SfSNet，一个端到端的学习框架，用于将不受约束的人脸图像准确分解为形状、反射率和照度。SfSNet旨在反映物理的朗伯渲染模型。SfSNet从标记的合成图像和未标记的真实世界图像的混合中学习。这使得网络能够通过光度重建损失从合成图像中捕捉低频变化，并从真实图像中捕捉高频细节。SfSNet包含一个新的分解架构，带有残差块，学习完全分离反照率和法线。这与原始图像一起用于预测光照。SfSNet在逆向渲染以及独立的法线和光照估计方面，比最先进的方法产生了显著更好的定量和定性结果。","领域":"人脸重建/逆向渲染/光照估计","问题":"如何从不受约束的人脸图像中准确分解出形状、反射率和照度","动机":"为了更准确地理解和重建人脸图像中的物理属性，如形状、反射率和照度，以便于进一步的应用，如虚拟现实、增强现实等。","方法":"采用端到端的学习框架SfSNet，结合标记的合成图像和未标记的真实世界图像，通过光度重建损失捕捉低频和高频细节，使用新的分解架构和残差块学习完全分离反照率和法线，并预测光照。","关键词":["人脸重建","逆向渲染","光照估计","朗伯渲染模型","光度重建损失"],"涉及的技术概念":{"朗伯渲染模型":"一种物理渲染模型，假设表面反射是均匀的，即从任何角度看，表面的亮度都是相同的。","光度重建损失":"一种损失函数，用于在图像重建过程中，通过比较重建图像和原始图像的光度差异来优化模型。","残差块":"一种深度学习中的网络结构，通过引入跳跃连接来帮助网络学习残差映射，从而改善深层网络的训练效果。"}},{"order":652,"title":"Deep Photo Enhancer: Unpaired Learning for Image Enhancement From Photographs With GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.html","abstract":"This paper proposes an unpaired learning method for image enhancement.  Given a set of photographs with the desired characteristics, the proposed method learns a photo enhancer which transforms an input image into an enhanced image with those characteristics. The method is based on the framework of two-way generative adversarial networks (GANs) with several improvements. First, we augment the U-Net with global features and show that it is more effective. The global U-Net acts as the generator in our GAN model. Second, we improve Wasserstein GAN (WGAN) with an adaptive weighting scheme. With this scheme, training converges faster and better, and is less sensitive to parameters than WGAN-GP. Finally, we propose to use individual batch normalization layers for generators in two-way GANs. It helps generators better adapt to their own input distributions. All together, they significantly improve the stability of GAN training for our application. Both quantitative and visual results show that the proposed method is effective for enhancing images.","中文标题":"深度照片增强器：基于GANs的未配对学习从照片中进行图像增强","摘要翻译":"本文提出了一种用于图像增强的未配对学习方法。给定一组具有所需特性的照片，所提出的方法学习一个照片增强器，该增强器将输入图像转换为具有这些特性的增强图像。该方法基于双向生成对抗网络（GANs）的框架，并进行了几项改进。首先，我们通过全局特征增强了U-Net，并显示其更有效。全局U-Net在我们的GAN模型中充当生成器。其次，我们通过自适应加权方案改进了Wasserstein GAN（WGAN）。通过这种方案，训练收敛得更快更好，并且对参数的敏感性低于WGAN-GP。最后，我们建议在双向GANs中为生成器使用单独的批量归一化层。这有助于生成器更好地适应它们自己的输入分布。所有这些都显著提高了我们应用中GAN训练的稳定性。定量和视觉结果都表明，所提出的方法对于增强图像是有效的。","领域":"图像增强/生成对抗网络/深度学习","问题":"如何从未配对的照片中学习图像增强","动机":"提高图像增强的效果和训练稳定性","方法":"基于双向生成对抗网络（GANs）的框架，通过增强U-Net、改进Wasserstein GAN（WGAN）和使用单独的批量归一化层来提高训练效果和稳定性","关键词":["图像增强","生成对抗网络","U-Net","Wasserstein GAN","批量归一化"],"涉及的技术概念":"双向生成对抗网络（GANs）是一种深度学习模型，用于生成数据。U-Net是一种卷积网络架构，常用于图像分割任务。Wasserstein GAN（WGAN）是GAN的一种变体，旨在解决传统GAN训练中的一些问题，如模式崩溃。批量归一化是一种技术，用于改善神经网络的训练过程和稳定性。"},{"order":653,"title":"LIME: Live Intrinsic Material Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Meka_LIME_Live_Intrinsic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Meka_LIME_Live_Intrinsic_CVPR_2018_paper.html","abstract":"We present the first end-to-end approach for real-time material estimation for general object shapes with uniform material that only requires a single color image as input. In addition to Lambertian surface properties, our approach fully automatically computes the specular albedo, material shininess, and a foreground segmentation. We tackle this challenging and ill-posed inverse rendering problem using recent advances in image-to-image translation techniques based on deep convolutional encoder–decoder architectures. The underlying core representations of our approach are specular shading, diffuse shading and mirror images, which allow to learn the effective and accurate separation of diffuse and specular albedo. In addition, we propose a novel highly efficient perceptual rendering loss that mimics real world image formation and obtains intermediate results even during run time. The estimation of material parameters at real-time frame rates enables exciting mixed reality applications, such as seamless illumination-consistent integration of virtual objects into realworld scenes, and virtual material cloning.We demonstrate our approach in a live setup, compare it to the state of the art, and demonstrate its effectiveness through quantitative and qualitative evaluation.","中文标题":"LIME: 实时内在材质估计","摘要翻译":"我们提出了第一个端到端的方法，用于实时估计具有均匀材质的一般物体形状的材质，仅需要单张彩色图像作为输入。除了朗伯表面属性外，我们的方法还能全自动计算镜面反射率、材质光泽度和前景分割。我们利用基于深度卷积编码器-解码器架构的图像到图像翻译技术的最新进展，解决了这一具有挑战性且不适定的逆渲染问题。我们方法的核心表示是镜面反射、漫反射和镜像图像，这使得能够学习到漫反射和镜面反射率的有效且准确的分离。此外，我们提出了一种新颖的高效感知渲染损失，它模仿现实世界的图像形成过程，并在运行时获得中间结果。实时帧率的材质参数估计使得令人兴奋的混合现实应用成为可能，例如将虚拟物体无缝且光照一致地集成到现实世界场景中，以及虚拟材质克隆。我们在实时设置中展示了我们的方法，与现有技术进行了比较，并通过定量和定性评估证明了其有效性。","领域":"逆渲染/材质估计/混合现实","问题":"实时估计具有均匀材质的一般物体形状的材质","动机":"解决逆渲染问题，实现虚拟物体与现实世界场景的无缝集成","方法":"利用深度卷积编码器-解码器架构的图像到图像翻译技术，提出高效感知渲染损失","关键词":["逆渲染","材质估计","混合现实","图像到图像翻译","感知渲染损失"],"涉及的技术概念":"深度卷积编码器-解码器架构用于图像到图像翻译，镜面反射、漫反射和镜像图像作为核心表示，高效感知渲染损失模仿现实世界的图像形成过程。"},{"order":654,"title":"Learning to Detect Features in Texture Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_to_Detect_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_to_Detect_CVPR_2018_paper.html","abstract":"Local feature detection is a fundamental task in computer vision, and hand-crafted feature detectors such as SIFT have shown success in applications including image-based localization and registration. Recent work has used features detected in texture images for precise global localization, but is limited by the performance of existing feature detectors on textures, as opposed to natural images.  We propose an effective and scalable method for learning feature detectors for textures, which combines an existing \\"ranking\\" loss with an efficient fully-convolutional architecture as well as a new training-loss term that maximizes the \\"peakedness\\" of the response map.  We demonstrate that our detector is more repeatable than existing methods, leading to improvements in a real-world texture-based localization application.","中文标题":"学习检测纹理图像中的特征","摘要翻译":"局部特征检测是计算机视觉中的一项基本任务，手工制作的特征检测器如SIFT在包括基于图像的定位和注册在内的应用中已显示出成功。最近的工作使用了在纹理图像中检测到的特征进行精确的全局定位，但由于现有特征检测器在纹理上的性能限制，与自然图像相比存在局限。我们提出了一种有效且可扩展的方法，用于学习纹理的特征检测器，该方法结合了现有的“排名”损失与一个高效的全卷积架构，以及一个新的训练损失项，该损失项最大化响应图的“峰值性”。我们证明了我们的检测器比现有方法更具重复性，从而在现实世界的基于纹理的定位应用中带来了改进。","领域":"特征检测/纹理分析/图像定位","问题":"现有特征检测器在纹理图像上的性能限制","动机":"提高纹理图像中特征检测的准确性和重复性，以改进基于纹理的定位应用","方法":"结合现有的“排名”损失与一个高效的全卷积架构，以及一个新的训练损失项，该损失项最大化响应图的“峰值性”","关键词":["特征检测","纹理分析","图像定位"],"涉及的技术概念":"局部特征检测是识别图像中关键点的过程，这些关键点对于图像匹配和识别至关重要。SIFT（尺度不变特征变换）是一种广泛使用的手工特征检测算法，能够在不同尺度和旋转下检测图像中的特征点。全卷积架构是一种深度学习模型，适用于图像处理任务，能够直接从图像中学习特征。响应图的“峰值性”指的是特征检测器在图像中检测到的特征点的显著性和集中程度。"},{"order":655,"title":"Learning to Extract a Video Sequence From a Single Motion-Blurred Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jin_Learning_to_Extract_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jin_Learning_to_Extract_CVPR_2018_paper.html","abstract":"We present a method to extract a video sequence from a single motion-blurred image.  Motion-blurred images are the result of an averaging process, where instant frames are accumulated over time during the exposure of the sensor.  Unfortunately, reversing this process is nontrivial. Firstly, averaging destroys the temporal ordering of the frames. Secondly, the recovery of a single frame is a blind deconvolution task, which is highly ill-posed.  We present a deep learning scheme that gradually reconstructs a temporal ordering by sequentially extracting pairs of frames. Our main contribution is to introduce loss functions invariant to the temporal order. This lets a neural network choose during training what frame to output among the possible combinations. We also address the ill-posedness of deblurring by designing a network with a large receptive field and implemented via resampling to achieve a higher computational efficiency. Our proposed method can successfully retrieve sharp image sequences from a single motion blurred image and can generalize well on synthetic and real datasets captured with different cameras.","中文标题":"学习从单一运动模糊图像中提取视频序列","摘要翻译":"我们提出了一种从单一运动模糊图像中提取视频序列的方法。运动模糊图像是平均过程的结果，其中瞬时帧在传感器曝光期间随时间累积。不幸的是，逆转这一过程并非易事。首先，平均过程破坏了帧的时间顺序。其次，恢复单一帧是一个盲去卷积任务，这是高度不适定的。我们提出了一种深度学习方案，通过顺序提取帧对来逐步重建时间顺序。我们的主要贡献是引入了对时间顺序不变的损失函数。这使得神经网络在训练期间可以选择在可能的组合中输出哪一帧。我们还通过设计具有大感受野并通过重采样实现更高计算效率的网络来解决去模糊的不适定性。我们提出的方法可以成功地从单一运动模糊图像中检索出清晰的图像序列，并且可以在使用不同相机捕获的合成和真实数据集上很好地泛化。","领域":"视频恢复/图像去模糊/深度学习应用","问题":"从单一运动模糊图像中恢复清晰的视频序列","动机":"运动模糊图像是瞬时帧在传感器曝光期间随时间累积的结果，逆转这一过程以恢复原始视频序列具有挑战性，因为平均过程破坏了帧的时间顺序，且恢复单一帧是一个高度不适定的盲去卷积任务。","方法":"提出了一种深度学习方案，通过顺序提取帧对来逐步重建时间顺序，并引入了对时间顺序不变的损失函数，使神经网络在训练期间可以选择输出哪一帧。此外，设计了具有大感受野并通过重采样实现更高计算效率的网络来解决去模糊的不适定性。","关键词":["视频恢复","图像去模糊","深度学习"],"涉及的技术概念":"运动模糊图像是由于传感器曝光期间瞬时帧随时间累积而形成的，逆转这一过程需要解决帧时间顺序的破坏和盲去卷积任务的不适定性。提出的方法利用深度学习技术，通过特定的损失函数和网络设计，有效地从单一运动模糊图像中恢复出清晰的视频序列。"},{"order":656,"title":"Lose the Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Anirudh_Lose_the_Views_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Anirudh_Lose_the_Views_CVPR_2018_paper.html","abstract":"Computed Tomography (CT) reconstruction is a fundamental component to a wide variety of applications ranging from security, to healthcare. The classical techniques require measuring projections, called sinograms, from a full 180 degree view of the object. However, obtaining a full-view is not always feasible, such as when scanning irregular objects that limit flexibility of scanner rotation. The resulting limited angle sinograms are known to produce highly artifact-laden reconstructions with existing techniques. In this paper, we propose to address this problem using CTNet -- a system of 1D and 2D convolutional neural networks, that operates directly on a limited angle sinogram to predict the reconstruction. We use the x-ray transform on this prediction to obtain a \`\`completed'' sinogram, as if it came from a full 180 degree view. We feed this to standard analytical and iterative reconstruction techniques to obtain the final reconstruction. We show with extensive experimentation on a challenging real world dataset that this combined strategy outperforms many competitive baselines. We also propose a measure of confidence for the reconstruction that enables a practitioner to gauge the reliability of a prediction made by  CTNet. We show that this measure is a strong indicator of quality as measured by the PSNR, while not requiring ground truth at test time. Finally, using a segmentation experiment, we show that our reconstruction also preserves the 3D structure of objects better than existing solutions.","中文标题":"失去视角：通过隐式正弦图补全进行有限角度CT重建","摘要翻译":"计算机断层扫描（CT）重建是从安全到医疗保健等多种应用的基本组成部分。传统技术需要从物体的180度全视角测量投影，称为正弦图。然而，获取全视角并不总是可行的，例如在扫描限制扫描仪旋转灵活性的不规则物体时。已知使用现有技术，由此产生的有限角度正弦图会产生高度伪影的重建。在本文中，我们提出使用CTNet来解决这个问题——一个直接操作于有限角度正弦图以预测重建的1D和2D卷积神经网络系统。我们在这个预测上使用X射线变换来获得一个“完成”的正弦图，就像它来自180度全视角一样。我们将其输入标准分析和迭代重建技术以获得最终重建。我们通过对一个具有挑战性的真实世界数据集进行广泛实验表明，这种组合策略优于许多竞争基线。我们还提出了一个重建的置信度测量，使从业者能够评估CTNet做出的预测的可靠性。我们表明，这个测量是质量的强指标，如PSNR所测量的，而在测试时不需要地面真相。最后，通过一个分割实验，我们表明我们的重建也比现有解决方案更好地保留了物体的3D结构。","领域":"医学影像重建/卷积神经网络/图像重建","问题":"有限角度CT重建产生高度伪影的问题","动机":"解决在无法获取全视角情况下，有限角度正弦图重建质量差的问题","方法":"提出CTNet系统，使用1D和2D卷积神经网络直接操作有限角度正弦图预测重建，并通过X射线变换获得完成的正弦图，最后使用标准分析和迭代重建技术获得最终重建","关键词":["CT重建","卷积神经网络","正弦图补全"],"涉及的技术概念":{"CT重建":"计算机断层扫描重建技术，用于从多个角度的投影数据重建物体的内部结构。","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征。","正弦图补全":"在有限角度CT扫描中，通过算法补全缺失的正弦图数据，以改善重建质量。","X射线变换":"一种数学变换，用于从投影数据中重建图像。"}},{"order":657,"title":"A Common Framework for Interactive Texture Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Men_A_Common_Framework_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Men_A_Common_Framework_CVPR_2018_paper.html","abstract":"In this paper, we present a general-purpose solution to interactive texture transfer problems that better preserves both local structure and visual richness. It is challenging due to the diversity of tasks and the simplicity of required user guidance. The core idea of our common framework is to use multiple custom channels to dynamically guide the synthesis process. For interactivity, users can control the spatial distribution of stylized textures via semantic channels. The structure guidance, acquired by two stages of automatic extraction and propagation of structure information, provides a prior for initialization and preserves the salient structure by searching the nearest neighbor fields (NNF) with structure coherence. Meanwhile, texture coherence is also exploited to maintain similar style with the source image. In addition, we leverage an improved PatchMatch with extended NNF and matrix operations to obtain transformable source patches with richer geometric information at high speed. We demonstrate the effectiveness and superiority of our method on a variety of scenes through extensive comparisons with state-of-the-art algorithms.","中文标题":"交互式纹理传输的通用框架","摘要翻译":"在本文中，我们提出了一种通用的解决方案，用于交互式纹理传输问题，该方案更好地保留了局部结构和视觉丰富性。由于任务的多样性和所需用户指导的简单性，这具有挑战性。我们通用框架的核心思想是使用多个自定义通道来动态指导合成过程。为了交互性，用户可以通过语义通道控制风格化纹理的空间分布。通过两个阶段自动提取和传播结构信息获得的结构指导，为初始化提供了先验，并通过搜索具有结构一致性的最近邻域（NNF）来保留显著结构。同时，还利用纹理一致性来保持与源图像相似的风格。此外，我们利用改进的PatchMatch与扩展的NNF和矩阵操作，以高速获得具有更丰富几何信息的可变换源补丁。通过与最先进算法的广泛比较，我们证明了我们方法的有效性和优越性。","领域":"纹理传输/图像合成/交互式图像处理","问题":"解决交互式纹理传输中保留局部结构和视觉丰富性的问题","动机":"由于任务的多样性和所需用户指导的简单性，交互式纹理传输具有挑战性，需要一种通用的解决方案来更好地保留局部结构和视觉丰富性","方法":"使用多个自定义通道动态指导合成过程，用户通过语义通道控制风格化纹理的空间分布，通过自动提取和传播结构信息获得结构指导，利用改进的PatchMatch与扩展的NNF和矩阵操作高速获得可变换源补丁","关键词":["纹理传输","图像合成","交互式图像处理"],"涉及的技术概念":"最近邻域（NNF）、PatchMatch算法、矩阵操作、语义通道、结构一致性、纹理一致性"},{"order":658,"title":"AMNet: Memorability Estimation With Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fajtl_AMNet_Memorability_Estimation_CVPR_2018_paper.html","abstract":"In this paper we present the design and evaluation of an end to end trainable, deep neural network with a visual attention mechanism for memorability estimation in still images. We analyze the suitability of transfer learning of deep models from image classification to the memorability task. Further on we study the impact of the attention mechanism on the memorability estimation and evaluate our network on the SUN Memorability and the LaMem dataset, the only large dataset with memorability labels to this date. Our network outperforms the existing state of the art models on both, the LaMem and SUN datasets in the term of the Spearman’s rank correlation as well as mean squared error, approaching human consistency.","中文标题":"AMNet: 使用注意力机制的记忆性估计","摘要翻译":"本文介绍了一种端到端可训练的深度神经网络的设计与评估，该网络采用视觉注意力机制用于静态图像的记忆性估计。我们分析了从图像分类到记忆性任务的深度模型迁移学习的适用性。此外，我们研究了注意力机制对记忆性估计的影响，并在SUN记忆性和LaMem数据集上评估了我们的网络，这是迄今为止唯一带有记忆性标签的大型数据集。我们的网络在LaMem和SUN数据集上的Spearman等级相关系数以及均方误差方面均优于现有的最先进模型，接近人类的一致性。","领域":"记忆性估计/视觉注意力机制/迁移学习","问题":"静态图像的记忆性估计","动机":"研究视觉注意力机制对记忆性估计的影响，并探索从图像分类到记忆性任务的深度模型迁移学习的适用性。","方法":"设计并评估了一种端到端可训练的深度神经网络，该网络采用视觉注意力机制，并在SUN记忆性和LaMem数据集上进行评估。","关键词":["记忆性估计","视觉注意力机制","迁移学习","深度神经网络"],"涉及的技术概念":{"视觉注意力机制":"一种模拟人类视觉系统的方式，使网络能够聚焦于图像的重要部分。","迁移学习":"将从一个任务中学到的知识应用到另一个相关任务上的技术。","Spearman等级相关系数":"用于评估两个变量之间等级相关性的非参数统计方法。","均方误差":"衡量估计值与实际值之间差异的指标。"}},{"order":659,"title":"Blind Predicting Similar Quality Map for Image Quality Assessment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Blind_Predicting_Similar_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pan_Blind_Predicting_Similar_CVPR_2018_paper.html","abstract":"A key problem in blind image quality assessment (BIQA) is how to effectively model the properties of human visual system in a data-driven manner. In this paper, we propose a simple and efficient BIQA model based on a novel framework which consists of a fully convolutional neural network (FCNN) and a pooling network to solve this problem. In principle, FCNN is capable of predicting a pixel-by-pixel similar quality map only from a distorted image by using the intermediate similarity maps derived from conventional full-reference image quality assessment methods. The predicted pixel-by-pixel quality maps have good consistency with the distortion correlations between the reference and distorted images. Finally, a deep pooling network regresses the quality map into a score. Experiments have demonstrated that our predictions outperform many state-of-the-art BIQA methods.","中文标题":"盲预测相似质量图用于图像质量评估","摘要翻译":"在盲图像质量评估（BIQA）中的一个关键问题是如何以数据驱动的方式有效建模人类视觉系统的特性。本文中，我们提出了一个简单且高效的BIQA模型，该模型基于一个新颖的框架，该框架由一个全卷积神经网络（FCNN）和一个池化网络组成，以解决这个问题。原则上，FCNN能够仅从失真图像中预测出逐像素的相似质量图，这是通过使用从传统的全参考图像质量评估方法中得出的中间相似图来实现的。预测的逐像素质量图与参考图像和失真图像之间的失真相关性具有良好的一致性。最后，一个深度池化网络将质量图回归为一个分数。实验证明，我们的预测优于许多最先进的BIQA方法。","领域":"图像质量评估/人类视觉系统建模/全卷积神经网络","问题":"如何有效建模人类视觉系统的特性以进行盲图像质量评估","动机":"解决盲图像质量评估中有效建模人类视觉系统特性的问题","方法":"提出一个基于全卷积神经网络和池化网络的简单且高效的BIQA模型","关键词":["盲图像质量评估","全卷积神经网络","池化网络","人类视觉系统","失真图像"],"涉及的技术概念":"全卷积神经网络（FCNN）用于从失真图像中预测逐像素的相似质量图，池化网络用于将质量图回归为一个分数。"},{"order":660,"title":"Deep End-to-End Time-of-Flight Imaging","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Deep_End-to-End_Time-of-Flight_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Deep_End-to-End_Time-of-Flight_CVPR_2018_paper.html","abstract":"We present an end-to-end image processing framework for time-of-flight (ToF) cameras. Existing ToF image processing pipelines consist of a sequence of operations including modulated exposures, denoising, phase unwrapping and multipath interference correction. While this cascaded modular design offers several benefits, such as closed-form solutions and power-efficient processing, it also suffers from error accumulation and information loss as each module can only observe the output from its direct predecessor, resulting in erroneous depth estimates. We depart from a conventional pipeline model and propose a deep convolutional neural network architecture that recovers scene depth directly from dual-frequency, raw ToF correlation measurements. To train this network, we simulate ToF images for a variety of scenes using a time-resolved renderer, devise depth-specific losses, and apply normalization and augmentation strategies to generalize this model to real captures. We demonstrate that the proposed network can efficiently exploit the spatio-temporal structures of ToF frequency measurements, and validate the performance of the joint multipath removal, denoising and phase unwrapping method on a wide range of challenging scenes.","中文标题":"深度端到端飞行时间成像","摘要翻译":"我们提出了一个用于飞行时间（ToF）相机的端到端图像处理框架。现有的ToF图像处理流程包括一系列操作，如调制曝光、去噪、相位展开和多路径干扰校正。虽然这种级联模块化设计提供了若干优势，如闭式解和能效处理，但它也遭受错误积累和信息丢失的问题，因为每个模块只能观察其直接前驱的输出，导致深度估计错误。我们摒弃了传统的流程模型，提出了一种深度卷积神经网络架构，该架构直接从双频、原始的ToF相关测量中恢复场景深度。为了训练这个网络，我们使用时间分辨渲染器模拟了各种场景的ToF图像，设计了深度特定的损失函数，并应用归一化和增强策略以使该模型泛化到实际捕获中。我们证明了所提出的网络能够有效地利用ToF频率测量的时空结构，并在各种挑战性场景中验证了联合多路径去除、去噪和相位展开方法的性能。","领域":"飞行时间成像/深度估计/图像处理","问题":"解决飞行时间相机图像处理流程中的错误积累和信息丢失问题","动机":"传统ToF图像处理流程存在错误积累和信息丢失的问题，导致深度估计不准确","方法":"提出了一种深度卷积神经网络架构，直接从双频、原始的ToF相关测量中恢复场景深度，并通过模拟ToF图像、设计深度特定的损失函数、应用归一化和增强策略来训练网络","关键词":["飞行时间成像","深度估计","卷积神经网络"],"涉及的技术概念":"飞行时间（ToF）相机、端到端图像处理框架、深度卷积神经网络、双频测量、时间分辨渲染器、深度特定的损失函数、归一化和增强策略、多路径去除、去噪、相位展开"},{"order":661,"title":"Aperture Supervision for Monocular Depth Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Srinivasan_Aperture_Supervision_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Srinivasan_Aperture_Supervision_for_CVPR_2018_paper.html","abstract":"We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image.","中文标题":"光圈监督用于单目深度估计","摘要翻译":"我们提出了一种新颖的方法，通过使用相机光圈提供的信息作为监督，来训练机器学习算法从单张图像估计场景深度。先前的工作使用深度传感器的输出或从不同视角拍摄的同一场景的图像作为监督，而我们的方法则使用从同一视角拍摄但具有不同相机光圈的图像。为了使学习算法能够使用光圈效果作为监督，我们引入了两个可微分的光圈渲染函数，它们使用输入图像和预测的深度来模拟真实相机光圈引起的景深效果。我们端到端地训练了一个单目深度估计网络，以预测最能解释这些有限光圈图像作为输入全焦点图像的散焦模糊渲染的场景深度。","领域":"深度估计/光圈渲染/单目视觉","问题":"如何从单张图像准确估计场景深度","动机":"利用相机光圈提供的信息作为监督，提高单目深度估计的准确性","方法":"引入两个可微分的光圈渲染函数，使用输入图像和预测的深度模拟真实相机光圈引起的景深效果，并端到端地训练单目深度估计网络","关键词":["深度估计","光圈渲染","单目视觉"],"涉及的技术概念":"光圈渲染函数：用于模拟真实相机光圈引起的景深效果的可微分函数；单目深度估计网络：一种端到端训练的神经网络，用于从单张图像预测场景深度。"},{"order":662,"title":"Seeing Temporal Modulation of Lights From Standard Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sakakibara_Seeing_Temporal_Modulation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sakakibara_Seeing_Temporal_Modulation_CVPR_2018_paper.html","abstract":"In this paper, we propose a novel method for measuring the temporal modulation of lights by using off-the-shelf cameras. In particular, we show that the invisible flicker patterns of various lights such as fluorescent lights can be measured by a simple combination of an off-the-shelf camera and any moving object with specular reflection. Unlike the existing methods, we do not need high speed cameras nor specially designed coded exposure cameras. Based on the extracted flicker patterns of environment lights, we also propose an efficient method for deblurring motion blurs in images. The proposed method enables us to deblur images with better frequency characteristics, which are induced by the flicker patterns of environment lights. The real image experiments show the efficiency of the proposed method.","中文标题":"从标准相机中观察光的时间调制","摘要翻译":"在本文中，我们提出了一种新颖的方法，通过使用现成的相机来测量光的时间调制。特别是，我们展示了通过简单的组合现成的相机和任何具有镜面反射的移动物体，可以测量各种光源（如荧光灯）的不可见闪烁模式。与现有方法不同，我们不需要高速相机也不需要特别设计的编码曝光相机。基于提取的环境光闪烁模式，我们还提出了一种有效的方法来去模糊图像中的运动模糊。所提出的方法使我们能够以更好的频率特性去模糊图像，这些特性是由环境光的闪烁模式引起的。真实图像实验显示了所提出方法的效率。","领域":"光调制测量/图像去模糊/环境光分析","问题":"如何利用标准相机测量光的时间调制并去模糊图像中的运动模糊","动机":"现有方法需要高速或特别设计的相机，限制了技术的广泛应用","方法":"使用现成相机和移动物体的镜面反射来测量光的时间调制，并基于此提出图像去模糊方法","关键词":["光调制","图像去模糊","环境光分析"],"涉及的技术概念":"时间调制指的是光强度随时间的变化模式，镜面反射是指光线在光滑表面上的反射，运动模糊是由于相机或物体在曝光期间移动造成的图像模糊。"},{"order":663,"title":"Statistical Tomography of Microscopic Life","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Levis_Statistical_Tomography_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Levis_Statistical_Tomography_of_CVPR_2018_paper.html","abstract":"We achieve tomography of 3D volumetric natural objects, where each projected 2D image corresponds to a different specimen. Each specimen has unknown random 3D orientation, location, and scale. This imaging scenario is relevant to microscopic and mesoscopic organisms, aerosols and hydrosols viewed naturally by a microscope. In-class scale variation inhibits prior single-particle reconstruction methods. We thus generalize tomographic recovery to account for all degrees of freedom of a similarity transformation. This enables geometric self-calibration in imaging of transparent objects. We make the computational load manageable and reach good quality reconstruction in a short time. This enables extraction of statistics that are important for a scientific study of specimen populations, specifically size distribution parameters. We apply the method to study of plankton.","中文标题":"微观生命的统计断层扫描","摘要翻译":"我们实现了对三维体积自然物体的断层扫描，其中每个投影的二维图像对应于不同的样本。每个样本具有未知的随机三维方向、位置和比例。这种成像场景与通过显微镜自然观察到的微观和介观生物、气溶胶和水溶胶相关。类内比例变化抑制了先前的单粒子重建方法。因此，我们推广了断层扫描恢复，以考虑相似变换的所有自由度。这使得在透明物体的成像中实现几何自校准成为可能。我们使计算负荷变得可管理，并在短时间内达到良好的重建质量。这使得提取对样本群体科学研究重要的统计数据成为可能，特别是大小分布参数。我们将该方法应用于浮游生物的研究。","领域":"生物成像/断层扫描/浮游生物研究","问题":"在未知随机三维方向、位置和比例的情况下，对三维体积自然物体进行断层扫描","动机":"为了克服类内比例变化对单粒子重建方法的限制，并实现对透明物体的几何自校准成像","方法":"推广断层扫描恢复以考虑相似变换的所有自由度，使计算负荷可管理并达到良好的重建质量","关键词":["断层扫描","三维重建","几何自校准","浮游生物"],"涉及的技术概念":"断层扫描是一种成像技术，通过从不同角度获取物体的二维投影图像来重建其三维结构。相似变换包括旋转、平移和缩放，是改变物体位置、方向和大小而不改变其形状的变换。几何自校准是指在成像过程中自动调整和校正几何参数，以提高成像质量。"},{"order":664,"title":"Divide and Conquer for Full-Resolution Light Field Deblurring","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mohan_Divide_and_Conquer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mohan_Divide_and_Conquer_CVPR_2018_paper.html","abstract":"The increasing popularity of computational light field (LF) cameras has necessitated the need for tackling motion blur which is a ubiquitous phenomenon in hand-held photography. The state-of-the-art method for blind deblurring of LFs of general 3D scenes is limited to handling only downsampled LF, both in spatial and angular resolution. This is due to the computational overhead involved in processing data-hungry full-resolution 4D LF altogether. Moreover, the method warrants high-end GPUs for optimization and  is ineffective for wide-angle settings and irregular camera motion. In this paper, we introduce a new blind motion deblurring strategy for LFs which alleviates these limitations significantly. Our model achieves this by isolating 4D LF motion blur across the 2D subaperture images, thus paving the way for independent deblurring of these subaperture images. Furthermore, our model accommodates  common camera motion parameterization across the subaperture images. Consequently, blind deblurring of any single subaperture image elegantly paves the way for cost-effective non-blind deblurring of the other subaperture images. Our approach is CPU-efficient computationally and can effectively deblur full-resolution LFs.","中文标题":"分而治之的全分辨率光场去模糊","摘要翻译":"计算光场（LF）相机的日益普及使得解决手持摄影中普遍存在的运动模糊问题变得必要。目前最先进的盲去模糊方法对于一般3D场景的光场仅限于处理在空间和角度分辨率上都下采样的光场。这是由于处理数据密集型的全分辨率4D光场所涉及的计算开销。此外，该方法需要高端GPU进行优化，并且对于广角设置和不规则相机运动无效。在本文中，我们介绍了一种新的光场盲运动去模糊策略，显著缓解了这些限制。我们的模型通过将4D光场运动模糊隔离到2D子孔径图像中来实现这一点，从而为这些子孔径图像的独立去模糊铺平了道路。此外，我们的模型适应了子孔径图像之间的常见相机运动参数化。因此，任何单个子孔径图像的盲去模糊优雅地为其他子孔径图像的成本效益非盲去模糊铺平了道路。我们的方法在计算上是CPU高效的，并且可以有效地去模糊全分辨率光场。","领域":"光场成像/运动去模糊/计算摄影","问题":"解决全分辨率光场图像中的运动模糊问题","动机":"由于手持摄影中普遍存在的运动模糊问题，以及现有方法在处理全分辨率光场时的计算开销和硬件要求高，需要一种更高效的去模糊方法。","方法":"通过将4D光场运动模糊隔离到2D子孔径图像中，实现独立去模糊，并适应子孔径图像间的常见相机运动参数化，从而实现成本效益的非盲去模糊。","关键词":["光场成像","运动去模糊","计算摄影"],"涉及的技术概念":"光场（LF）相机、运动模糊、4D光场、2D子孔径图像、盲去模糊、非盲去模糊、相机运动参数化"},{"order":665,"title":"Multispectral Image Intrinsic Decomposition via Subspace Constraint","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Multispectral_Image_Intrinsic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Multispectral_Image_Intrinsic_CVPR_2018_paper.html","abstract":"Multispectral images contain many clues of surface characteristics of the objects, thus can be used in many computer vision tasks, e.g., recolorization and segmentation. However, due to the complex geometry structure of natural scenes, the spectra curves of the same surface can look very different under different illuminations and from different angles. In this paper, a new Multispectral Image Intrinsic Decomposition model (MIID) is presented to decompose the shading and reflectance from a single multispectral image. We extend the Retinex model, which is proposed for RGB image intrinsic decomposition, for multispectral domain. Based on this, a subspace constraint is introduced to both the shading and reflectance spectral space to reduce the ill-posedness of the problem and make the problem solvable. A dataset of 22 scenes is given with the ground truth of shadings and reflectance to facilitate objective evaluations. The experiments demonstrate the effectiveness of the proposed method.","中文标题":"多光谱图像本征分解通过子空间约束","摘要翻译":"多光谱图像包含物体表面特性的许多线索，因此可以用于许多计算机视觉任务，例如重新着色和分割。然而，由于自然场景的复杂几何结构，同一表面的光谱曲线在不同光照和不同角度下可能看起来非常不同。本文提出了一种新的多光谱图像本征分解模型（MIID），用于从单一多光谱图像中分解出阴影和反射率。我们扩展了Retinex模型，该模型最初是为RGB图像本征分解提出的，适用于多光谱领域。基于此，引入了对阴影和反射率光谱空间的子空间约束，以减少问题的不适定性并使问题可解。提供了一个包含22个场景的数据集，其中包含阴影和反射率的真实值，以促进客观评估。实验证明了所提出方法的有效性。","领域":"光谱分析/图像分解/场景理解","问题":"从单一多光谱图像中准确分解出阴影和反射率","动机":"由于自然场景的复杂几何结构，同一表面的光谱曲线在不同光照和不同角度下可能看起来非常不同，这给多光谱图像的本征分解带来了挑战。","方法":"扩展Retinex模型用于多光谱领域，并引入子空间约束以减少问题的不适定性。","关键词":["多光谱图像","本征分解","子空间约束","Retinex模型"],"涉及的技术概念":{"多光谱图像":"包含多个光谱波段信息的图像，能够提供物体表面特性的丰富线索。","本征分解":"将图像分解为其本征组成部分，如阴影和反射率。","子空间约束":"在分解过程中引入的约束，以减少问题的不适定性。","Retinex模型":"一种用于图像本征分解的模型，最初设计用于RGB图像。"}},{"order":666,"title":"Improving Color Reproduction Accuracy on Cameras","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Karaimer_Improving_Color_Reproduction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Karaimer_Improving_Color_Reproduction_CVPR_2018_paper.html","abstract":"One of the key operations performed on a digital camera is to map the sensor-specific color space to a standard perceptual color space.   This procedure involves the application of a white-balance correction followed by a color space transform.  The current approach for this colorimetric mapping is based on an interpolation of pre-calibrated color space transforms computed for two fixed illuminations (i.e., two white-balance settings).  Images captured under different illuminations are subject to less color accuracy due to the use of this interpolation process.   In this paper, we discuss the limitations of the current colorimetric mapping approach and propose two methods that are able to improve color accuracy.  We evaluate our approach on seven different cameras and show improvements of up to  30% (DSLR cameras) and 59% (mobile phone cameras) in terms of color reproduction error.","中文标题":"提高相机色彩再现准确性","摘要翻译":"数码相机执行的关键操作之一是将传感器特定的色彩空间映射到标准的感知色彩空间。这一过程包括应用白平衡校正，然后进行色彩空间转换。当前的颜色映射方法基于对两种固定光照（即两种白平衡设置）下计算的预校准色彩空间变换的插值。由于使用这种插值过程，在不同光照下捕获的图像色彩准确性较低。在本文中，我们讨论了当前颜色映射方法的局限性，并提出了两种能够提高色彩准确性的方法。我们在七种不同的相机上评估了我们的方法，并显示在色彩再现误差方面，DSLR相机提高了30%，手机相机提高了59%。","领域":"色彩科学/图像传感器技术/数字摄影","问题":"提高在不同光照条件下捕获图像的色彩准确性","动机":"当前基于插值的颜色映射方法在不同光照条件下捕获的图像色彩准确性较低","方法":"提出了两种改进色彩准确性的方法，并在多种相机上进行了评估","关键词":["色彩再现","白平衡校正","色彩空间转换"],"涉及的技术概念":"色彩空间映射涉及将相机传感器捕获的色彩信息转换为标准色彩空间的过程，以便在不同设备上呈现一致的色彩。白平衡校正是调整图像色彩以补偿光源色温的过程，确保白色物体在不同光照下看起来是白色的。色彩空间转换是将图像从一种色彩表示方式转换为另一种，以适应不同的显示或打印需求。"},{"order":667,"title":"A Closer Look at Spatiotemporal Convolutions for Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_A_Closer_Look_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html","abstract":"In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block \`\`R(2+1)D'' which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.","中文标题":"细看时空卷积在动作识别中的应用","摘要翻译":"在本文中，我们讨论了用于视频分析的几种时空卷积形式，并研究了它们对动作识别的影响。我们的动机源于观察到应用于视频单个帧的2D CNN在动作识别中一直表现良好。在这项工作中，我们通过实验证明了在残差学习框架内，3D CNN相对于2D CNN在准确性上的优势。此外，我们展示了将3D卷积滤波器分解为单独的空间和时间组件可以显著提高准确性。我们的实证研究导致了一种新的时空卷积块“R(2+1)D”的设计，该设计产生的CNN在Sports-1M、Kinetics、UCF101和HMDB51上取得了与最先进技术相当或更优的结果。","领域":"动作识别/视频分析/卷积神经网络","问题":"如何提高动作识别的准确性","动机":"观察到2D CNN在动作识别中的良好表现，探索3D CNN的潜力","方法":"通过实验比较2D CNN和3D CNN在残差学习框架内的准确性，设计新的时空卷积块“R(2+1)D”","关键词":["时空卷积","动作识别","3D CNN","2D CNN","残差学习"],"涉及的技术概念":{"2D CNN":"二维卷积神经网络，主要用于处理图像数据","3D CNN":"三维卷积神经网络，能够处理视频数据中的时空信息","残差学习":"一种深度学习技术，通过引入残差块来帮助网络学习更深层次的特征","R(2+1)D":"一种新的时空卷积块设计，通过将3D卷积滤波器分解为空间和时间组件来提高动作识别的准确性"}},{"order":668,"title":"Inferring Shared Attention in Social Scene Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Inferring_Shared_Attention_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Inferring_Shared_Attention_CVPR_2018_paper.html","abstract":"This paper addresses a new problem of inferring shared attention in third-person social scene videos. Shared attention is a phenomenon that two or more individuals simultaneously look at a common target in social scenes. Perceiving and identifying shared attention in videos plays crucial roles in social activities and social scene understanding. We propose a spatial-temporal neural network to detect shared attention intervals in videos and predict shared attention locations in frames. In each video frame, human gaze directions and potential target boxes are two key features for spatially detecting shared attention in the social scene. In temporal domain, a convolutional Long Short- Term Memory network utilizes the temporal continuity and transition constraints to optimize the predicted shared attention heatmap. We collect a new dataset VideoCoAtt from public TV show videos, containing 380 complex video sequences with more than 492,000 frames that include diverse social scenes for shared attention study. Experiments on this dataset show that our model can effectively infer shared attention in videos. We also empirically verify the effectiveness of different components in our model.","中文标题":"推断社交场景视频中的共享注意力","摘要翻译":"本文解决了一个新问题，即在第三人称社交场景视频中推断共享注意力。共享注意力是指两个或更多个体在社交场景中同时看向一个共同目标的现象。在视频中感知和识别共享注意力在社交活动和社交场景理解中扮演着关键角色。我们提出了一种时空神经网络来检测视频中的共享注意力区间，并预测帧中的共享注意力位置。在每个视频帧中，人类注视方向和潜在目标框是空间上检测社交场景中共享注意力的两个关键特征。在时间域中，卷积长短期记忆网络利用时间连续性和转换约束来优化预测的共享注意力热图。我们从公共电视节目视频中收集了一个新的数据集VideoCoAtt，包含380个复杂视频序列，超过492,000帧，涵盖了多样化的社交场景用于共享注意力研究。在该数据集上的实验表明，我们的模型能够有效地推断视频中的共享注意力。我们还通过实验验证了模型中不同组件的有效性。","领域":"社交场景理解/视频分析/注意力机制","问题":"在第三人称社交场景视频中推断共享注意力","动机":"感知和识别共享注意力在社交活动和社交场景理解中扮演着关键角色","方法":"提出了一种时空神经网络来检测视频中的共享注意力区间，并预测帧中的共享注意力位置","关键词":["共享注意力","时空神经网络","社交场景理解"],"涉及的技术概念":{"共享注意力":"两个或更多个体在社交场景中同时看向一个共同目标的现象","时空神经网络":"一种用于处理视频数据，结合空间和时间信息的神经网络","卷积长短期记忆网络":"一种结合卷积神经网络和长短期记忆网络的结构，用于处理序列数据，如视频帧","共享注意力热图":"一种可视化工具，用于表示视频帧中共享注意力的可能位置"}},{"order":669,"title":"Making Convolutional Networks Recurrent for Visual Sequence Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Making_Convolutional_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Making_Convolutional_Networks_CVPR_2018_paper.html","abstract":"Recurrent neural networks (RNNs) have emerged as a powerful model for a broad range of machine learning problems that involve sequential data. While an abundance of work exists to understand and improve RNNs in the context of language and audio signals such as language modeling and speech recognition, relatively little attention has been paid to analyze or modify RNNs for visual sequences, which by nature have distinct properties. In this paper, we aim to bridge this gap and present the first large-scale exploration of RNNs for visual sequence learning. In particular, with the intention of leveraging the strong generalization capacity of pre-trained convolutional neural networks (CNNs), we propose a novel and effective approach, PreRNN, to make pre-trained CNNs recurrent by transforming convolutional layers or fully connected layers into recurrent layers. We conduct extensive evaluations on three representative visual sequence learning tasks: sequential face alignment, dynamic hand gesture recognition, and action recognition. Our experiments reveal that PreRNN consistently outperforms the traditional RNNs and achieves state-of-the-art results on the three applications, suggesting that PreRNN is more suitable for visual sequence learning.","中文标题":"使卷积网络递归以进行视觉序列学习","摘要翻译":"递归神经网络（RNNs）已成为处理涉及序列数据的广泛机器学习问题的强大模型。尽管已有大量工作致力于在语言和音频信号（如语言建模和语音识别）的背景下理解和改进RNNs，但相对较少关注于分析或修改RNNs以适应视觉序列，这些序列本质上具有独特的属性。在本文中，我们旨在弥合这一差距，并首次大规模探索RNNs在视觉序列学习中的应用。特别是，为了利用预训练卷积神经网络（CNNs）的强大泛化能力，我们提出了一种新颖且有效的方法，PreRNN，通过将卷积层或全连接层转换为递归层，使预训练的CNNs递归。我们在三个代表性的视觉序列学习任务上进行了广泛的评估：序列面部对齐、动态手势识别和动作识别。我们的实验表明，PreRNN在三个应用中始终优于传统的RNNs，并取得了最先进的结果，这表明PreRNN更适合于视觉序列学习。","领域":"视觉序列学习/递归神经网络/卷积神经网络","问题":"如何有效地将递归神经网络应用于视觉序列学习","动机":"尽管递归神经网络在语言和音频信号处理中表现出色，但在视觉序列学习中的应用相对较少，且视觉序列具有独特的属性，需要专门的研究和改进。","方法":"提出了一种名为PreRNN的新方法，通过将预训练的卷积神经网络的卷积层或全连接层转换为递归层，使其适用于视觉序列学习。","关键词":["递归神经网络","卷积神经网络","视觉序列学习","序列面部对齐","动态手势识别","动作识别"],"涉及的技术概念":{"递归神经网络（RNNs）":"一种处理序列数据的神经网络，能够利用其内部状态（记忆）来处理输入序列。","卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征。","PreRNN":"本文提出的方法，通过将预训练的CNNs的卷积层或全连接层转换为递归层，使其适用于视觉序列学习。","序列面部对齐":"一种视觉序列学习任务，旨在从视频序列中准确地对齐面部特征。","动态手势识别":"一种视觉序列学习任务，旨在识别和理解视频序列中的手势动作。","动作识别":"一种视觉序列学习任务，旨在识别和理解视频序列中的动作或活动。"}},{"order":670,"title":"Real-World Anomaly Detection in Surveillance Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sultani_Real-World_Anomaly_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sultani_Real-World_Anomaly_Detection_CVPR_2018_paper.html","abstract":"Surveillance videos are able to capture a variety of realistic anomalies. In this paper, we propose to learn anomalies by exploiting both normal and anomalous videos. To avoid annotating the anomalous segments or clips in training videos, which is very time consuming, we propose to learn anomaly through the deep multiple instance ranking framework by leveraging weakly labeled training videos, ie the training labels (anomalous or normal) are at video-level instead of clip-level.  In our approach, we consider normal and anomalous videos as bags and video segments as instances in multiple instance learning (MIL), and automatically learn a deep anomaly ranking model that predicts high anomaly scores for anomalous video segments. Furthermore, we introduce sparsity and temporal smoothness constraints in the ranking loss function to better localize anomaly during training.  We also introduce a new large-scale first of its kind dataset of 128 hours of videos. It consists of 1900 long and untrimmed real-world surveillance videos, with 13 realistic anomalies such as fighting, road accident, burglary, robbery, etc. as well as normal activities. This dataset can be used for two tasks. First, general anomaly detection considering all anomalies in one group and all normal activities in another group. Second, for recognizing each of 13 anomalous activities. Our experimental results show that our MIL  method for anomaly detection achieves significant improvement on anomaly detection performance as compared to the state-of-the-art approaches. We provide the results of several recent deep learning baselines on anomalous activity recognition. The low recognition performance of these baselines reveals that our dataset is very challenging and opens more opportunities for future work.","中文标题":"监控视频中的真实世界异常检测","摘要翻译":"监控视频能够捕捉到各种现实中的异常。在本文中，我们提出通过利用正常和异常视频来学习异常。为了避免在训练视频中标注异常片段或剪辑，这非常耗时，我们提出通过深度多实例排序框架利用弱标签训练视频来学习异常，即训练标签（异常或正常）是在视频级别而不是剪辑级别。在我们的方法中，我们将正常和异常视频视为包，将视频片段视为多实例学习（MIL）中的实例，并自动学习一个深度异常排序模型，该模型预测异常视频片段的高异常分数。此外，我们在排序损失函数中引入了稀疏性和时间平滑性约束，以在训练期间更好地定位异常。我们还引入了一个新的大规模数据集，这是其类别的第一个，包含128小时的视频。它由1900个长且未修剪的真实世界监控视频组成，包含13种现实中的异常，如打架、交通事故、入室盗窃、抢劫等，以及正常活动。该数据集可用于两个任务。首先，一般异常检测，将所有异常视为一组，所有正常活动视为另一组。其次，用于识别13种异常活动中的每一种。我们的实验结果表明，与最先进的方法相比，我们的MIL方法在异常检测性能上取得了显著改进。我们提供了几种最近的深度学习基线在异常活动识别上的结果。这些基线的低识别性能表明我们的数据集非常具有挑战性，并为未来的工作开辟了更多机会。","领域":"异常检测/视频分析/深度学习","问题":"在监控视频中检测现实世界的异常","动机":"减少异常检测中手动标注视频片段的时间和成本","方法":"利用深度多实例排序框架和弱标签训练视频学习异常，引入稀疏性和时间平滑性约束以更好地定位异常","关键词":["异常检测","视频分析","多实例学习","深度学习"],"涉及的技术概念":"深度多实例排序框架、弱标签训练视频、稀疏性和时间平滑性约束、异常活动识别"},{"order":671,"title":"Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.html","abstract":"Vehicle re-identification (re-ID) has the huge potential to contribute to the intelligent video surveillance. However, it suffers from challenges that different vehicle identities with a similar appearance have little inter-instance discrepancy while one vehicle usually has large intra-instance differences under viewpoint and illumination variations. Previous methods address vehicle re-ID by simply using visual features from originally captured views and usually exploit the spatial-temporal information of the vehicles to refine the results. In this paper, we propose a Viewpoint-aware Attentive Multi-view Inference (VAMI) model that only requires visual information to solve the multi-view vehicle re-ID problem. Given vehicle images of arbitrary viewpoints, the VAMI extracts the single-view feature for each input image and aims to transform the features into a global multi-view feature representation so that pairwise distance metric learning can be better optimized in such a viewpoint-invariant feature space. The VAMI adopts a viewpoint-aware attention model to select core regions at different viewpoints and implement effective multi-view feature inference by an adversarial training architecture. Extensive experiments validate the effectiveness of each proposed component and illustrate that our approach achieves consistent improvements over state-of-the-art vehicle re-ID methods on two public datasets: VeRi and VehicleID.","中文标题":"视角感知的注意力多视图推理用于车辆重识别","摘要翻译":"车辆重识别（re-ID）在智能视频监控中具有巨大的潜力。然而，它面临着挑战，即外观相似的不同车辆身份之间几乎没有实例间差异，而同一车辆在视角和光照变化下通常具有较大的实例内差异。以前的方法通过简单地使用原始捕获视图的视觉特征来解决车辆重识别问题，并通常利用车辆的空间-时间信息来优化结果。在本文中，我们提出了一种仅需要视觉信息来解决多视图车辆重识别问题的视角感知注意力多视图推理（VAMI）模型。给定任意视角的车辆图像，VAMI为每个输入图像提取单视图特征，并旨在将这些特征转换为全局多视图特征表示，以便在这种视角不变的特征空间中更好地优化成对距离度量学习。VAMI采用视角感知注意力模型来选择不同视角下的核心区域，并通过对抗训练架构实现有效的多视图特征推理。大量实验验证了每个提出组件的有效性，并说明我们的方法在两个公共数据集VeRi和VehicleID上相对于最先进的车辆重识别方法实现了持续的改进。","领域":"车辆重识别/多视图学习/对抗训练","问题":"解决多视图车辆重识别问题，特别是在视角和光照变化下，同一车辆实例内差异大，不同车辆实例间差异小的问题。","动机":"提高车辆重识别的准确性和鲁棒性，以支持智能视频监控系统。","方法":"提出视角感知注意力多视图推理（VAMI）模型，通过提取单视图特征并转换为全局多视图特征表示，利用视角感知注意力模型和对抗训练架构优化成对距离度量学习。","关键词":["车辆重识别","多视图学习","对抗训练","视角感知","注意力模型"],"涉及的技术概念":"视角感知注意力模型用于选择不同视角下的核心区域，对抗训练架构用于实现有效的多视图特征推理，全局多视图特征表示用于优化成对距离度量学习。"},{"order":672,"title":"Efficient Video Object Segmentation via Network Modulation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Efficient_Video_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Efficient_Video_Object_CVPR_2018_paper.html","abstract":"Video object segmentation targets segmenting a specific object throughout a video sequence when given only an annotated first frame. Recent deep learning based approaches find it effective to fine-tune a general-purpose segmentation model on the annotated frame using hundreds of iterations of gradient descent. Despite the high accuracy that these methods achieve, the fine-tuning process is inefficient and fails to meet the requirements of real world applications. We propose a novel approach that uses a single forward pass to adapt the segmentation model to the appearance of a specific object. Specifically, a second meta neural network named modulator is trained to manipulate the intermediate layers of the segmentation network given limited visual and spatial information of the target object. The experiments show that our approach is 70 times faster than fine-tuning approaches and achieves similar accuracy.","中文标题":"通过网络调制实现高效视频对象分割","摘要翻译":"视频对象分割的目标是在仅给定一个标注的第一帧的情况下，在整个视频序列中分割出特定对象。最近的基于深度学习的方法发现，使用数百次梯度下降迭代在标注帧上微调通用分割模型是有效的。尽管这些方法实现了高精度，但微调过程效率低下，无法满足现实世界应用的需求。我们提出了一种新颖的方法，该方法使用单次前向传递来使分割模型适应特定对象的外观。具体来说，训练了一个名为调制器的第二元神经网络，以在给定目标对象的有限视觉和空间信息的情况下操纵分割网络的中间层。实验表明，我们的方法比微调方法快70倍，并且达到了相似的精度。","领域":"视频对象分割/神经网络/模型优化","问题":"提高视频对象分割的效率，减少对大量迭代微调的依赖","动机":"现有的基于深度学习的视频对象分割方法虽然精度高，但效率低下，无法满足实际应用的需求","方法":"提出了一种使用单次前向传递和元神经网络调制器来适应特定对象外观的新方法","关键词":["视频对象分割","神经网络","模型优化","元神经网络","前向传递"],"涉及的技术概念":{"视频对象分割":"指在视频序列中分割出特定对象的技术","神经网络":"一种模拟人脑神经元网络的计算模型，用于处理复杂的数据和任务","模型优化":"指通过各种方法提高模型性能和效率的过程","元神经网络":"一种用于调整或控制其他神经网络行为的神经网络","前向传递":"神经网络中数据从输入层流向输出层的过程，不涉及反向传播"}},{"order":673,"title":"Weakly-Supervised Action Segmentation With Iterative Soft Boundary Assignment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ding_Weakly-Supervised_Action_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ding_Weakly-Supervised_Action_Segmentation_CVPR_2018_paper.html","abstract":"In this work, we address the task of weakly-supervised human action segmentation in long, untrimmed videos. Recent methods have relied on expensive learning models, such as Recurrent Neural Networks (RNN) and Hidden Markov Models (HMM). However, these methods suffer from expensive computational cost, thus are unable to be deployed in large scale. To overcome the limitations, the keys to our design are efficiency and scalability. We propose a novel action modeling framework, which consists of a new temporal convolutional network, named Temporal Convolutional Feature Pyramid Network (TCFPN), for predicting frame-wise action labels, and a novel training strategy for weakly-supervised sequence modeling, named Iterative Soft Boundary Assignment (ISBA), to align action sequences and update the network in an iterative fashion. The proposed framework is evaluated on two benchmark datasets, Breakfast and Hollywood Extended, with four different evaluation metrics. Extensive experimental results show that our methods achieve competitive or superior performance to state-of-the-art methods.","中文标题":"弱监督动作分割与迭代软边界分配","摘要翻译":"在本工作中，我们解决了长未剪辑视频中弱监督人体动作分割的任务。最近的方法依赖于昂贵的学习模型，如循环神经网络（RNN）和隐马尔可夫模型（HMM）。然而，这些方法存在计算成本高的问题，因此无法大规模部署。为了克服这些限制，我们设计的关键是效率和可扩展性。我们提出了一种新颖的动作建模框架，该框架包括一个新的时间卷积网络，称为时间卷积特征金字塔网络（TCFPN），用于预测帧级动作标签，以及一种新的弱监督序列建模训练策略，称为迭代软边界分配（ISBA），以迭代方式对齐动作序列并更新网络。所提出的框架在两个基准数据集Breakfast和Hollywood Extended上进行了评估，使用了四种不同的评估指标。大量的实验结果表明，我们的方法实现了与最先进方法竞争或更优的性能。","领域":"动作识别/视频分析/时间序列分析","问题":"长未剪辑视频中弱监督人体动作分割","动机":"克服现有方法计算成本高、无法大规模部署的限制","方法":"提出了一种新颖的动作建模框架，包括时间卷积特征金字塔网络（TCFPN）和迭代软边界分配（ISBA）训练策略","关键词":["动作分割","弱监督学习","时间卷积网络","迭代软边界分配"],"涉及的技术概念":{"时间卷积特征金字塔网络（TCFPN）":"一种新的时间卷积网络，用于预测帧级动作标签","迭代软边界分配（ISBA）":"一种新的弱监督序列建模训练策略，用于迭代对齐动作序列并更新网络"}},{"order":674,"title":"Depth-Aware Stereo Video Retargeting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Depth-Aware_Stereo_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Depth-Aware_Stereo_Video_CVPR_2018_paper.html","abstract":"As compared with traditional video retargeting, stereo video retargeting poses new challenges because stereo video contains the depth information of salient objects and its time dynamics.  In this work, we propose a depth-aware stereo video retargeting method by imposing the depth fidelity constraint.  The proposed depth-aware retargeting method reconstructs the 3D scene to obtain the depth information of salient objects. We cast it as a constrained optimization problem, where the total cost function includes the shape, temporal and depth distortions of salient objects. As a result, the solution can preserve the shape, temporal and depth fidelity of salient objects simultaneously. It is demonstrated by experimental results that the depth-aware retargeting method achieves higher retargeting quality and provides better user experience.","中文标题":"深度感知的立体视频重定向","摘要翻译":"与传统视频重定向相比，立体视频重定向提出了新的挑战，因为立体视频包含了显著物体的深度信息及其时间动态。在这项工作中，我们提出了一种通过施加深度保真约束的深度感知立体视频重定向方法。所提出的深度感知重定向方法重建3D场景以获取显著物体的深度信息。我们将其视为一个约束优化问题，其中总成本函数包括显著物体的形状、时间和深度失真。因此，该解决方案可以同时保持显著物体的形状、时间和深度保真度。实验结果表明，深度感知重定向方法实现了更高的重定向质量，并提供了更好的用户体验。","领域":"立体视觉/视频处理/3D重建","问题":"立体视频重定向中保持显著物体的形状、时间和深度保真度","动机":"立体视频包含显著物体的深度信息及其时间动态，这为视频重定向带来了新的挑战，需要一种方法来保持这些信息的保真度。","方法":"提出了一种深度感知的立体视频重定向方法，通过施加深度保真约束，重建3D场景以获取显著物体的深度信息，并将其视为一个约束优化问题来解决。","关键词":["立体视频重定向","深度感知","3D重建"],"涉及的技术概念":"深度保真约束、3D场景重建、约束优化问题、形状失真、时间失真、深度失真"},{"order":675,"title":"Instance Embedding Transfer to Unsupervised Video Object Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Instance_Embedding_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Instance_Embedding_Transfer_CVPR_2018_paper.html","abstract":"We propose a method for unsupervised video object segmentation by transferring the knowledge encapsulated in image-based instance embedding networks. The instance embedding network produces an embedding vector for each pixel that enables identifying all pixels belonging to the same object. Though trained on static images, the instance embeddings are stable over consecutive video frames, which allows us to link objects together over time. Thus, we adapt the instance networks trained on static images to video object segmentation and incorporate the embeddings with objectness and optical flow features, without model retraining or online fine-tuning. The proposed method outperforms state-of-the-art unsupervised segmentation methods in the DAVIS dataset and the FBMS dataset.","中文标题":"实例嵌入迁移到无监督视频对象分割","摘要翻译":"我们提出了一种通过迁移基于图像的实例嵌入网络中的知识来进行无监督视频对象分割的方法。实例嵌入网络为每个像素生成一个嵌入向量，使得能够识别属于同一对象的所有像素。尽管是在静态图像上训练的，实例嵌入在连续视频帧之间是稳定的，这使我们能够随时间将对象链接在一起。因此，我们将在静态图像上训练的实例网络适应于视频对象分割，并将嵌入与对象性和光流特征结合，无需模型重新训练或在线微调。所提出的方法在DAVIS数据集和FBMS数据集上优于最先进的无监督分割方法。","领域":"视频对象分割/实例嵌入/无监督学习","问题":"如何在无监督的情况下进行视频对象分割","动机":"利用静态图像训练的实例嵌入网络知识，实现视频对象分割，无需模型重新训练或在线微调","方法":"迁移基于图像的实例嵌入网络知识，结合对象性和光流特征进行视频对象分割","关键词":["实例嵌入","视频对象分割","无监督学习","对象性","光流特征"],"涉及的技术概念":"实例嵌入网络为每个像素生成嵌入向量，用于识别属于同一对象的所有像素；通过结合对象性和光流特征，实现视频对象分割，无需模型重新训练或在线微调。"},{"order":676,"title":"Future Frame Prediction for Anomaly Detection – A New Baseline","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Future_Frame_Prediction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Future_Frame_Prediction_CVPR_2018_paper.html","abstract":"Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.","中文标题":"未来帧预测用于异常检测——一个新的基线","摘要翻译":"视频中的异常检测指的是识别不符合预期行为的事件。然而，几乎所有现有的方法都是通过最小化训练数据的重建误差来解决问题，这不能保证异常事件的重建误差更大。在本文中，我们提出在视频预测框架内解决异常检测问题。据我们所知，这是第一个利用预测的未来帧与其真实值之间的差异来检测异常事件的工作。为了预测正常事件的更高质量的未来帧，除了常用的外观（空间）约束（如强度和梯度）外，我们还在视频预测中引入了运动（时间）约束，通过强制预测帧和真实帧之间的光流一致，这是第一个将时间约束引入视频预测任务的工作。这样的空间和运动约束有助于正常事件的未来帧预测，从而有助于识别那些不符合预期的异常事件。在玩具数据集和一些公开可用数据集上的大量实验验证了我们的方法在正常事件不确定性的鲁棒性和对异常事件的敏感性方面的有效性。","领域":"视频分析/异常检测/光流估计","问题":"视频中异常事件的检测","动机":"现有方法通过最小化训练数据的重建误差来解决问题，不能保证异常事件的重建误差更大，因此需要一种新的方法来提高异常检测的准确性。","方法":"在视频预测框架内解决异常检测问题，利用预测的未来帧与其真实值之间的差异来检测异常事件，并引入运动（时间）约束来提高预测质量。","关键词":["视频预测","异常检测","光流估计"],"涉及的技术概念":"本文涉及的技术概念包括视频预测、异常检测、光流估计、空间约束和时间约束。视频预测是指预测视频序列中的未来帧；异常检测是指识别视频中不符合预期行为的事件；光流估计是指估计视频帧之间的运动；空间约束和时间约束分别指在视频预测中对帧的外观和运动进行约束以提高预测质量。"},{"order":677,"title":"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html","abstract":"The purpose of this study is to determine whether current video datasets have sufficient data for training very deep convolutional neural networks (CNNs) with spatio-temporal three-dimensional (3D) kernels. Recently, the performance levels of 3D CNNs in the field of action recognition have improved significantly. However, to date, conventional research has only explored relatively shallow 3D architectures. We examine the architectures of various 3D CNNs from relatively shallow to very deep ones on current video datasets. Based on the results of those experiments, the following conclusions could be obtained: (i) ResNet-18 training resulted in significant overfitting for UCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics dataset has sufficient data for training of deep 3D CNNs, and enables training of up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet. ResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii) Kinetics pretrained simple 3D architectures outperforms complex 2D architectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on UCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has produced significant progress in various tasks in image. We believe that using deep 3D CNNs together with Kinetics will retrace the successful history of 2D CNNs and ImageNet, and stimulate advances in computer vision for videos. The codes and pretrained models used in this study are publicly available. https://github.com/kenshohara/3D-ResNets-PyTorch","中文标题":"时空3D CNN能否重走2D CNN和ImageNet的历史？","摘要翻译":"本研究的目的是确定当前的视频数据集是否有足够的数据来训练具有时空三维（3D）核的非常深的卷积神经网络（CNN）。最近，3D CNN在动作识别领域的性能水平显著提高。然而，迄今为止，传统研究仅探索了相对较浅的3D架构。我们在当前的视频数据集上检查了从相对较浅到非常深的各种3D CNN架构。基于这些实验的结果，可以得出以下结论：（i）ResNet-18训练在UCF-101、HMDB-51和ActivityNet上导致了显著的过拟合，但在Kinetics上没有。（ii）Kinetics数据集有足够的数据用于训练深3D CNN，并且能够训练多达152层的ResNets，有趣的是，这与ImageNet上的2D ResNets相似。ResNeXt-101在Kinetics测试集上达到了78.4%的平均准确率。（iii）Kinetics预训练的简单3D架构优于复杂的2D架构，预训练的ResNeXt-101在UCF-101和HMDB-51上分别达到了94.5%和70.2%。使用在ImageNet上训练的2D CNN已经在各种图像任务中产生了显著进展。我们相信，使用深3D CNN与Kinetics一起将重走2D CNN和ImageNet的成功历史，并刺激视频计算机视觉的进步。本研究中使用的代码和预训练模型是公开的。https://github.com/kenshohara/3D-ResNets-PyTorch","领域":"动作识别/视频分析/深度学习","问题":"当前视频数据集是否足够用于训练深度的时空3D卷积神经网络","动机":"探索深3D CNN架构在视频数据集上的性能，以重走2D CNN和ImageNet的成功历史","方法":"在多个视频数据集上测试从浅到深的各种3D CNN架构，并分析其性能","关键词":["3D CNN","动作识别","视频数据集","ResNet","Kinetics"],"涉及的技术概念":{"3D CNN":"三维卷积神经网络，用于处理视频等时空数据","ResNet":"一种深度残差网络，通过引入残差学习来解决深层网络训练中的退化问题","Kinetics":"一个大规模的视频动作识别数据集","UCF-101":"一个包含101类动作的视频数据集","HMDB-51":"一个包含51类动作的视频数据集","ActivityNet":"一个大规模的视频理解数据集，主要用于动作识别和检测","ResNeXt":"ResNet的改进版本，通过引入分组卷积来增加网络的宽度"}},{"order":678,"title":"Dynamic Video Segmentation Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Dynamic_Video_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Dynamic_Video_Segmentation_CVPR_2018_paper.html","abstract":"In this paper, we present a detailed design of dynamic video segmentation network (DVSNet) for fast and efficient semantic video segmentation.  DVSNet consists of two convolutional neural networks: a segmentation network and a flow network.  The former generates highly accurate semantic segmentations, but is deeper and slower.  The latter is much faster than the former, but its output requires further processing to generate less accurate semantic segmentations.  We explore the use of a decision network to adaptively assign different frame regions to different networks based on a metric called expected confidence score.  Frame regions with a higher expected confidence score traverse the flow network.  Frame regions with a lower expected confidence score have to pass through the segmentation network.  We have extensively performed experiments on various configurations of DVSNet, and investigated a number of variants for the proposed decision network.  The experimental results show that our DVSNet is able to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset.  A high speed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on the same dataset.  DVSNet is also able to reduce up to 95% of the computational workloads.","中文标题":"动态视频分割网络","摘要翻译":"本文详细介绍了动态视频分割网络（DVSNet）的设计，旨在实现快速高效的语义视频分割。DVSNet由两个卷积神经网络组成：分割网络和流网络。前者生成高精度的语义分割，但更深且更慢。后者比前者快得多，但其输出需要进一步处理以生成精度较低的语义分割。我们探索了使用决策网络根据称为预期置信度分数的度量自适应地将不同的帧区域分配给不同的网络。具有较高预期置信度分数的帧区域通过流网络处理。具有较低预期置信度分数的帧区域必须通过分割网络处理。我们对DVSNet的各种配置进行了广泛的实验，并研究了所提出决策网络的多个变体。实验结果表明，我们的DVSNet在Cityscape数据集上能够以19.8 fps的速度达到70.4%的mIoU。DVSNet的高速版本在同一数据集上能够以30.4 fps的速度提供63.2%的mIoU。DVSNet还能够减少高达95%的计算工作量。","领域":"语义分割/视频处理/卷积神经网络","问题":"实现快速高效的语义视频分割","动机":"提高语义视频分割的速度和效率，同时保持或提高分割的准确性","方法":"设计了一个由分割网络和流网络组成的动态视频分割网络（DVSNet），并引入决策网络根据预期置信度分数自适应地分配帧区域到不同的网络","关键词":["语义分割","视频处理","卷积神经网络","决策网络","预期置信度分数"],"涉及的技术概念":{"动态视频分割网络（DVSNet）":"一种由分割网络和流网络组成的网络架构，旨在实现快速高效的语义视频分割。","分割网络":"生成高精度语义分割的卷积神经网络，但计算速度较慢。","流网络":"比分割网络更快，但生成的语义分割精度较低的卷积神经网络。","决策网络":"根据预期置信度分数自适应地将帧区域分配给分割网络或流网络的网络。","预期置信度分数":"用于评估帧区域分割难易程度的度量，高分数表示容易分割，低分数表示难以分割。","mIoU":"平均交并比，用于评估语义分割准确性的指标。","fps":"每秒帧数，用于评估视频处理速度的指标。"}},{"order":679,"title":"Recognize Actions by Disentangling Components of Dynamics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Recognize_Actions_by_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Recognize_Actions_by_CVPR_2018_paper.html","abstract":"Despite the remarkable progress in action recognition over the past several years, existing methods remain limited in efficiency and effectiveness. The methods treating appearance and motion as separate streams are usually subject to the cost of optical flow computation, while those relying on 3D convolution on the original video frames often yield inferior performance in practice. In this paper, we propose a new ConvNet architecture for video representation learning, which can derive disentangled components of dynamics purely from raw video frames, without the need of optical flow estimation. Particularly, the learned representation comprises three components for representing static appearance, apparent motion, and appearance changes. We introduce 3D pooling, cost volume processing, and warped feature differences, respectively for extracting the three components above. These modules are incorporated as three branches in our unified network, which share the underlying features and are learned jointly in an end-to-end manner. On two large datasets UCF101 and Kinetics our method obtained competitive performances with high efficiency, using only the RGB frame sequence as input.","中文标题":"通过解构动态组件识别动作","摘要翻译":"尽管在过去几年中动作识别取得了显著进展，但现有方法在效率和效果上仍然有限。将外观和运动作为独立流处理的方法通常受限于光流计算的成本，而依赖原始视频帧的3D卷积方法在实践中往往表现不佳。在本文中，我们提出了一种新的ConvNet架构用于视频表示学习，该架构能够纯粹从原始视频帧中解构出动态组件，无需光流估计。特别是，学习到的表示包括三个组件，用于表示静态外观、表观运动和外观变化。我们分别引入了3D池化、成本体积处理以及扭曲特征差异，用于提取上述三个组件。这些模块作为三个分支被整合到我们的统一网络中，它们共享底层特征并以端到端的方式共同学习。在两个大型数据集UCF101和Kinetics上，我们的方法仅使用RGB帧序列作为输入，就获得了具有高效率的竞争性能。","领域":"视频理解/动作识别/动态分析","问题":"提高动作识别的效率和效果","动机":"现有方法在动作识别上存在效率和效果的限制，需要一种新的方法来克服这些限制","方法":"提出了一种新的ConvNet架构，通过3D池化、成本体积处理和扭曲特征差异来解构动态组件，无需光流估计","关键词":["视频表示学习","3D卷积","端到端学习"],"涉及的技术概念":{"ConvNet架构":"一种卷积神经网络架构，用于视频表示学习","3D池化":"一种在三维空间中进行池化操作的技术，用于提取视频帧中的特征","成本体积处理":"一种处理技术，用于从视频帧中提取运动信息","扭曲特征差异":"一种技术，用于计算视频帧之间的特征差异，以捕捉外观变化"}},{"order":680,"title":"Motion-Appearance Co-Memory Networks for Video Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Motion-Appearance_Co-Memory_Networks_CVPR_2018_paper.html","abstract":"Video Question Answering (QA) is an important task in understanding video temporal structure. We observe that there are three unique attributes of video QA compared with image QA: (1) it deals with long sequences of images containing richer information not only in quantity but also in variety; (2) motion and appearance information are usually correlated with each other and able to provide useful attention cues to the other; (3) different questions require different number of frames to infer the answer. Based these observations, we propose a motion-appearance co-memory network for video QA. Our networks are built on concepts from Dynamic Memory Network (DMN) and introduces new mechanisms for video QA. Specifically, there are three salient aspects: (1) a co-memory attention mechanism that utilizes cues from both motion and appearance to generate attention; (2) a temporal conv-deconv network to generate multi-level contextual facts; (3) a dynamic fact ensemble method to construct temporal representation dynamically for different questions. We evaluate our method on TGIF-QA dataset, and the results outperform state-of-the-art significantly on all four tasks of TGIF-QA.","中文标题":"运动-外观共记忆网络用于视频问答","摘要翻译":"视频问答（QA）是理解视频时间结构的重要任务。我们观察到，与图像QA相比，视频QA具有三个独特的属性：（1）它处理的是包含更丰富信息的图像长序列，不仅在数量上，而且在种类上；（2）运动和外观信息通常相互关联，并能够为对方提供有用的注意力线索；（3）不同的问题需要不同数量的帧来推断答案。基于这些观察，我们提出了一种用于视频QA的运动-外观共记忆网络。我们的网络建立在动态记忆网络（DMN）的概念上，并引入了新的视频QA机制。具体来说，有三个显著的特点：（1）一种共记忆注意力机制，利用运动和外观的线索来生成注意力；（2）一个时间卷积-反卷积网络，用于生成多层次的上下文事实；（3）一种动态事实集成方法，为不同的问题动态构建时间表示。我们在TGIF-QA数据集上评估了我们的方法，结果在所有四个TGIF-QA任务上显著优于最先进的技术。","领域":"视频理解/注意力机制/时间序列分析","问题":"视频问答中的时间结构理解和答案推断","动机":"视频问答相比图像问答处理的是更长的图像序列，包含更丰富的信息，且运动和外观信息相互关联，不同问题需要不同数量的帧来推断答案","方法":"提出了一种运动-外观共记忆网络，包括共记忆注意力机制、时间卷积-反卷积网络和动态事实集成方法","关键词":["视频问答","共记忆网络","注意力机制","时间卷积-反卷积网络","动态事实集成"],"涉及的技术概念":"动态记忆网络（DMN）是一种用于处理序列数据的神经网络，通过引入记忆模块来存储和更新信息。共记忆注意力机制是一种利用运动和外观信息生成注意力的方法，有助于视频问答中的答案推断。时间卷积-反卷积网络用于从视频序列中提取多层次的时间上下文信息。动态事实集成方法则根据问题的不同动态构建时间表示，以提高视频问答的准确性。"},{"order":681,"title":"Learning to Understand Image Blur","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_to_Understand_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Learning_to_Understand_CVPR_2018_paper.html","abstract":"While many approaches have been proposed to estimate and remove blur in a photo, few efforts were made to have an algorithm automatically understand the blur desirability: whether the blur is desired or not, and how it affects the quality of the photo. Such a task not only relies on low-level visual features to identify blurry regions, but also requires high-level understanding of the image content as well as user intent during photo capture. In this paper, we propose a unified framework to estimate a spatially-varying blur map and understand its desirability in terms of image quality at the same time. In particular, we use a dilated fully convolutional neural network with pyramid pooling and boundary refinement layers to generate high-quality blur response maps. If blur exists, we classify its desirability to three levels ranging from good to bad, by distilling high-level semantics and learning an attention map to adaptively localize the important content in the image. The whole framework is end-to-end jointly trained with both supervisions of pixel-wise blur responses and image-wise blur desirability levels. Considering the limitations of existing image blur datasets, we collected a new large-scale dataset with both annotations to facilitate training. The proposed methods are extensively evaluated on two datasets and demonstrate state-of-the-art performance on both tasks.","中文标题":"学习理解图像模糊","摘要翻译":"虽然已经提出了许多方法来估计和去除照片中的模糊，但很少有努力让算法自动理解模糊的合意性：模糊是否被期望，以及它如何影响照片的质量。这样的任务不仅依赖于低级视觉特征来识别模糊区域，还需要对图像内容以及用户拍摄意图的高级理解。在本文中，我们提出了一个统一的框架来估计空间变化的模糊图，并同时理解其对图像质量的合意性。特别是，我们使用了一个带有金字塔池化和边界细化层的扩张全卷积神经网络来生成高质量的模糊响应图。如果存在模糊，我们通过提炼高级语义并学习一个注意力图来自适应地定位图像中的重要内容，将其合意性分类为从好到坏的三个级别。整个框架是端到端联合训练的，具有像素级模糊响应和图像级模糊合意性级别的监督。考虑到现有图像模糊数据集的局限性，我们收集了一个新的大规模数据集，带有两种注释以促进训练。所提出的方法在两个数据集上进行了广泛评估，并在两项任务上展示了最先进的性能。","领域":"图像质量评估/模糊检测/图像理解","问题":"自动理解图像模糊的合意性及其对图像质量的影响","动机":"现有方法主要关注模糊的估计和去除，而缺乏对模糊合意性的自动理解","方法":"使用扩张全卷积神经网络结合金字塔池化和边界细化层生成模糊响应图，并通过提炼高级语义和学习注意力图来分类模糊的合意性","关键词":["模糊检测","图像质量评估","注意力机制"],"涉及的技术概念":"扩张全卷积神经网络、金字塔池化、边界细化层、注意力图、图像质量评估、模糊合意性分类"},{"order":682,"title":"Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.html","abstract":"We propose a novel end-to-end trainable, deep, encoder-decoder architecture for single-pass semantic segmentation. Our approach is based on a cascaded architecture with feature-level long-range skip connections. The encoder incorporates the structure of ResNeXt's residual building blocks and adopts the strategy of repeating a building block that aggregates a set of transformations with the same topology. The decoder features a novel architecture, consisting of blocks, that (i) capture context information, (ii) generate semantic features, and (iii) enable fusion between different output resolutions. Crucially, we introduce dense decoder shortcut connections to allow decoder blocks to use semantic feature maps from all previous decoder levels, i.e. from all higher-level feature maps. The dense decoder connections allow for effective information propagation from one decoder block to another, as well as for multi-level feature fusion that significantly improves the accuracy. Importantly, these connections allow our method to obtain state-of-the-art performance on several challenging datasets, without the need of time-consuming multi-scale averaging of previous works.","中文标题":"用于单次语义分割的密集解码器快捷连接","摘要翻译":"我们提出了一种新颖的端到端可训练的深度编码器-解码器架构，用于单次语义分割。我们的方法基于具有特征级长距离跳跃连接的级联架构。编码器结合了ResNeXt残差构建块的结构，并采用了重复构建块的策略，该构建块聚合了一组具有相同拓扑结构的变换。解码器采用了一种新颖的架构，由块组成，这些块（i）捕捉上下文信息，（ii）生成语义特征，以及（iii）实现不同输出分辨率之间的融合。关键的是，我们引入了密集解码器快捷连接，以允许解码器块使用来自所有先前解码器级别的语义特征图，即来自所有更高级别的特征图。密集解码器连接允许从一个解码器块到另一个解码器块的有效信息传播，以及多级特征融合，这显著提高了准确性。重要的是，这些连接使我们的方法能够在几个具有挑战性的数据集上获得最先进的性能，而无需耗时的多尺度平均，这是之前工作的需求。","领域":"语义分割/深度学习/图像分析","问题":"提高单次语义分割的准确性和效率","动机":"为了解决现有语义分割方法中多尺度平均耗时的问题，并提高分割的准确性","方法":"提出了一种包含密集解码器快捷连接的深度编码器-解码器架构，通过级联架构和特征级长距离跳跃连接，以及重复构建块的策略，实现上下文信息的捕捉、语义特征的生成和多级特征融合","关键词":["语义分割","编码器-解码器架构","密集解码器快捷连接"],"涉及的技术概念":"ResNeXt残差构建块、特征级长距离跳跃连接、多级特征融合、密集解码器快捷连接"},{"order":683,"title":"Generative Adversarial Image Synthesis With Decision Tree Latent Controller","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kaneko_Generative_Adversarial_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kaneko_Generative_Adversarial_Image_CVPR_2018_paper.html","abstract":"This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well.  This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning.","中文标题":"生成对抗图像合成与决策树潜在控制器","摘要翻译":"本文提出了决策树潜在控制器生成对抗网络（DTLC-GAN），这是GAN的一个扩展，能够在不需要详细监督的情况下学习层次可解释的表示。为了在潜在变量上施加层次包含结构，我们将一种称为DTLC的新架构整合到生成器输入中。DTLC具有多层树结构，其中子节点代码的开启或关闭由父节点代码控制。通过层次化地使用这种架构，我们可以获得潜在空间，其中较低层的代码根据较高层的代码被选择性地使用。为了使潜在代码在DTLC中以层次解耦的方式捕捉图像的显著语义特征，我们还提出了层次条件互信息正则化，并使用我们新定义的课程学习方法对其进行优化。这使得仅使用单个DTLC-GAN模型就可以基于信息增益以逐层方式发现层次可解释的表示。我们在各种数据集上评估了DTLC-GAN，即MNIST、CIFAR-10、Tiny ImageNet、3D Faces和CelebA，并确认DTLC-GAN可以在无监督或弱监督设置下学习层次可解释的表示。此外，我们将DTLC-GAN应用于图像检索任务，并展示了其在表示学习中的有效性。","领域":"图像合成/表示学习/图像检索","问题":"如何在不需要详细监督的情况下学习层次可解释的图像表示","动机":"为了在潜在变量上施加层次包含结构，并捕捉图像的显著语义特征","方法":"提出了决策树潜在控制器生成对抗网络（DTLC-GAN），并整合了层次条件互信息正则化和新定义的课程学习方法","关键词":["生成对抗网络","决策树","层次可解释表示","图像检索"],"涉及的技术概念":"DTLC-GAN是一种生成对抗网络的扩展，通过引入决策树潜在控制器（DTLC）来学习层次可解释的表示。DTLC具有多层树结构，通过控制子节点代码的开启或关闭来实现层次包含结构。此外，通过层次条件互信息正则化和课程学习方法，使得潜在代码能够以层次解耦的方式捕捉图像的显著语义特征。"},{"order":684,"title":"Learning a Discriminative Prior for Blind Image Deblurring","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_a_Discriminative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_a_Discriminative_CVPR_2018_paper.html","abstract":"We present an effective blind image deblurring method based on a data-driven discriminative prior. Our work is motivated by the fact that a good image prior should favor clear images over blurred images. To obtain such an image prior for deblurring, we formulate the image prior as a binary classifier which can be achieved by a deep convolutional neural network (CNN). The learned image prior has a significant discriminative property and is able to distinguish whether the image is clear or not. Embedded into the maximum a posterior (MAP) framework, it helps blind deblurring on various scenarios, including natural, face, text, and low-illumination images. However, it is difficult to optimize the deblurring method with the learned image prior as it involves a non-linear CNN. Therefore, we develop an efficient numerical approach based on the half-quadratic splitting method and gradient decent algorithm to solve the proposed model. Furthermore, the proposed model can be easily extended to non-uniform deblurring. Both qualitative and quantitative experimental results show that our method performs favorably against state-of-the-art algorithms as well as domain-specific image deblurring approaches.","中文标题":"学习用于盲图像去模糊的判别先验","摘要翻译":"我们提出了一种基于数据驱动判别先验的有效盲图像去模糊方法。我们的工作动机在于，一个好的图像先验应该倾向于清晰图像而非模糊图像。为了获得这样的图像先验用于去模糊，我们将图像先验表述为一个二元分类器，这可以通过深度卷积神经网络（CNN）实现。学习到的图像先验具有显著的判别特性，能够区分图像是否清晰。将其嵌入到最大后验（MAP）框架中，有助于在各种场景下进行盲去模糊，包括自然、人脸、文本和低光照图像。然而，由于涉及非线性CNN，使用学习到的图像先验优化去模糊方法较为困难。因此，我们开发了一种基于半二次分裂法和梯度下降算法的高效数值方法来解决所提出的模型。此外，所提出的模型可以轻松扩展到非均匀去模糊。定性和定量实验结果表明，我们的方法在性能上优于最先进的算法以及特定领域的图像去模糊方法。","领域":"图像去模糊/深度学习/卷积神经网络","问题":"如何有效地进行盲图像去模糊","动机":"一个好的图像先验应该倾向于清晰图像而非模糊图像","方法":"将图像先验表述为通过深度卷积神经网络实现的二元分类器，并开发基于半二次分裂法和梯度下降算法的高效数值方法","关键词":["盲图像去模糊","判别先验","深度卷积神经网络","半二次分裂法","梯度下降算法"],"涉及的技术概念":{"盲图像去模糊":"一种不需要预先知道模糊核的图像去模糊技术","判别先验":"一种能够区分图像是否清晰的先验知识","深度卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","半二次分裂法":"一种用于优化问题的数值方法","梯度下降算法":"一种用于最小化损失函数的优化算法"}},{"order":685,"title":"Frame-Recurrent Video Super-Resolution","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.html","abstract":"Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results.  In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.","中文标题":"帧循环视频超分辨率","摘要翻译":"近年来，视频超分辨率领域的最新进展表明，结合运动补偿的卷积神经网络能够从多个低分辨率（LR）帧中合并信息，以生成高质量的图像。当前最先进的方法处理一批LR帧以生成单个高分辨率（HR）帧，并在整个视频中以滑动窗口的方式运行此方案，有效地将问题视为大量独立的多帧超分辨率任务。这种方法有两个主要弱点：1）每个输入帧被多次处理和扭曲，增加了计算成本；2）每个输出帧在输入帧的条件下独立估计，限制了系统产生时间一致结果的能力。在这项工作中，我们提出了一种端到端可训练的帧循环视频超分辨率框架，该框架使用先前推断的HR估计来超分辨率后续帧。这自然鼓励了时间一致的结果，并通过在每一步仅扭曲一个图像来减少计算成本。此外，由于其循环性质，所提出的方法能够吸收大量先前的帧而不增加计算需求。广泛的评估和与以前方法的比较验证了我们方法的优势，并证明所提出的框架能够显著优于当前的最新技术。","领域":"视频超分辨率/卷积神经网络/运动补偿","问题":"视频超分辨率中处理多帧信息以生成高质量图像时的高计算成本和缺乏时间一致性的问题","动机":"减少视频超分辨率过程中的计算成本和提高时间一致性","方法":"提出了一种端到端可训练的帧循环视频超分辨率框架，利用先前推断的高分辨率估计来超分辨率后续帧","关键词":["视频超分辨率","卷积神经网络","运动补偿","时间一致性","计算成本"],"涉及的技术概念":"卷积神经网络（CNN）用于从多个低分辨率帧中提取和合并信息，运动补偿用于对齐帧以减少模糊，帧循环框架通过利用先前的高分辨率估计来减少计算成本和提高时间一致性。"},{"order":686,"title":"Discovering Point Lights With Intensity Distance Fields","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Discovering_Point_Lights_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Discovering_Point_Lights_CVPR_2018_paper.html","abstract":"We introduce the light localization problem. A scene is illuminated by a set of unobserved isotropic point lights.  Given the geometry, materials, and illuminated appearance of the scene, the light localization problem is to completely recover the number, positions, and intensities of the lights. We first present a scene transform that identifies likely light positions. Based on this transform, we develop an iterative algorithm to locate remaining lights and determine all light intensities. We demonstrate the success of this method in a large set of 2D synthetic scenes, and show that it extends to 3D, in both synthetic scenes and real-world scenes.","中文标题":"发现具有强度距离场的点光源","摘要翻译":"我们引入了光源定位问题。一个场景由一组未观察到的各向同性点光源照亮。给定场景的几何形状、材料和照明外观，光源定位问题是完全恢复光源的数量、位置和强度。我们首先提出了一种场景变换，以识别可能的光源位置。基于这种变换，我们开发了一种迭代算法来定位剩余的光源并确定所有光源的强度。我们在一组大型的2D合成场景中展示了这种方法的成功，并展示了它在合成场景和现实世界场景中扩展到3D的能力。","领域":"计算机图形学/光照估计/场景重建","问题":"完全恢复场景中未观察到的各向同性点光源的数量、位置和强度","动机":"为了解决在给定场景几何形状、材料和照明外观的情况下，如何准确恢复光源信息的问题","方法":"首先提出一种场景变换来识别可能的光源位置，然后开发一种迭代算法来定位剩余的光源并确定所有光源的强度","关键词":["光源定位","各向同性点光源","场景变换","迭代算法"],"涉及的技术概念":"光源定位问题涉及从场景的几何形状、材料和照明外观中恢复未观察到的各向同性点光源的数量、位置和强度。通过场景变换识别可能的光源位置，并使用迭代算法进一步定位光源和确定强度。"},{"order":687,"title":"Video Rain Streak Removal by Multiscale Convolutional Sparse Coding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Video_Rain_Streak_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Video_Rain_Streak_CVPR_2018_paper.html","abstract":"Videos captured by outdoor surveillance equipments sometimes contain unexpected rain streaks, which brings difficulty in subsequent video processing tasks. Rain streak removal from a video is thus an important topic in recent computer vision research.   In this paper, we raise two intrinsic characteristics specifically possessed by rain streaks.   Firstly, the rain streaks in a video contain repetitive local patterns sparsely scattered over different positions of the video.   Secondly, the rain streaks are with multiscale configurations due to their occurrence on positions with different distances to the cameras.   Based on such understanding, we specifically formulate both characteristics into a multiscale convolutional sparse coding (MS-CSC) model for the video rain streak removal task.   Specifically, we use multiple convolutional filters convolved on the sparse feature maps to deliver the former characteristic, and further use multiscale filters to represent different scales of rain streaks.   Such a new encoding manner makes the proposed method capable of properly extracting rain streaks from videos, thus getting fine video deraining effects. Experiments implemented on synthetic and real videos verify the superiority of the proposed method, as compared with the state-of-the-art ones along this research line, both visually and quantitatively.","中文标题":"通过多尺度卷积稀疏编码去除视频雨纹","摘要翻译":"户外监控设备捕获的视频有时会包含意外的雨纹，这给后续的视频处理任务带来了困难。因此，从视频中去除雨纹是近年来计算机视觉研究中的一个重要课题。在本文中，我们提出了雨纹特有的两个内在特征。首先，视频中的雨纹包含重复的局部模式，这些模式稀疏地散布在视频的不同位置。其次，由于雨纹出现在与相机不同距离的位置上，因此它们具有多尺度的配置。基于这样的理解，我们特别将这两个特征制定为一个多尺度卷积稀疏编码（MS-CSC）模型，用于视频雨纹去除任务。具体来说，我们使用多个卷积滤波器在稀疏特征图上进行卷积以传递前一个特征，并进一步使用多尺度滤波器来表示不同尺度的雨纹。这种新的编码方式使得所提出的方法能够正确地从视频中提取雨纹，从而获得良好的视频去雨效果。在合成视频和真实视频上进行的实验验证了所提出方法的优越性，与这一研究线上的最先进方法相比，无论是视觉上还是数量上。","领域":"视频处理/稀疏编码/多尺度分析","问题":"视频中雨纹的去除","动机":"户外监控视频中的雨纹会影响后续的视频处理任务，因此需要有效去除雨纹的方法。","方法":"提出了一种多尺度卷积稀疏编码（MS-CSC）模型，利用雨纹的重复局部模式和多尺度特性，通过多尺度滤波器在稀疏特征图上进行卷积，有效提取并去除视频中的雨纹。","关键词":["视频去雨","稀疏编码","多尺度分析"],"涉及的技术概念":"多尺度卷积稀疏编码（MS-CSC）模型是一种结合了稀疏编码和多尺度分析的技术，用于从视频中提取和去除雨纹。稀疏编码通过寻找数据中的稀疏表示来捕捉数据的内在结构，而多尺度分析则允许模型处理不同尺度的特征，从而更有效地去除视频中的雨纹。"},{"order":688,"title":"Stereoscopic Neural Style Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Stereoscopic_Neural_Style_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Stereoscopic_Neural_Style_CVPR_2018_paper.html","abstract":"This paper presents the first attempt at stereoscopic neural style transfer, which responds to the emerging demand for 3D movies or AR/VR. We start with a careful examination of applying existing monocular style transfer methods to left and right views of stereoscopic images separately. This reveals that the original disparity consistency cannot be well preserved in the final stylization results, which causes 3D fatigue to the viewers. To address this issue, we incorporate a new disparity loss into the widely adopted style loss function by enforcing the bidirectional disparity constraint in non-occluded regions. For a practical real-time solution, we propose the first feed-forward network by jointly training a stylization sub-network and a disparity sub-network, and integrate them in a feature level middle domain. Our disparity sub-network is also the first end-to-end network for simultaneous bidirectional disparity and occlusion mask estimation. Finally, our network is effectively extended to stereoscopic videos, by considering both temporal coherence and disparity consistency. We will show that the proposed method clearly outperforms the baseline algorithms both quantitatively and qualitatively.","中文标题":"立体神经风格迁移","摘要翻译":"本文首次尝试了立体神经风格迁移，以响应3D电影或AR/VR的新兴需求。我们首先仔细检查了将现有的单目风格迁移方法分别应用于立体图像的左右视图的情况。这揭示了原始视差一致性在最终风格化结果中无法得到很好的保持，这会导致观众的3D疲劳。为了解决这个问题，我们在广泛采用的风格损失函数中加入了新的视差损失，通过在非遮挡区域强制执行双向视差约束。为了实用的实时解决方案，我们提出了第一个前馈网络，通过联合训练风格化子网络和视差子网络，并将它们集成在特征级别的中间域中。我们的视差子网络也是第一个用于同时双向视差和遮挡掩码估计的端到端网络。最后，通过考虑时间相干性和视差一致性，我们的网络有效地扩展到了立体视频。我们将展示所提出的方法在数量和质量上都明显优于基线算法。","领域":"立体视觉/风格迁移/实时渲染","问题":"在立体图像风格迁移中保持视差一致性，避免3D疲劳","动机":"响应3D电影或AR/VR的新兴需求，解决现有方法在立体图像风格迁移中视差一致性保持不佳的问题","方法":"在风格损失函数中加入新的视差损失，提出第一个前馈网络联合训练风格化子网络和视差子网络，并集成在特征级别的中间域中","关键词":["立体视觉","风格迁移","实时渲染","视差损失","前馈网络"],"涉及的技术概念":"立体神经风格迁移、视差一致性、3D疲劳、风格损失函数、双向视差约束、前馈网络、风格化子网络、视差子网络、特征级别中间域、时间相干性"},{"order":689,"title":"Multi-Frame Quality Enhancement for Compressed Video","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Multi-Frame_Quality_Enhancement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Multi-Frame_Quality_Enhancement_CVPR_2018_paper.html","abstract":"The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, ignoring the similarity between consecutive frames. In this paper, we investigate that heavy quality fluctuation exists across compressed video frames, and thus low quality frames can be enhanced using the neighboring high quality frames, seen as Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as a first attempt in this direction. In our approach, we firstly develop a Support Vector Machine (SVM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are as the input. The MF-CNN compensates motion between the non-PQF and PQFs through the Motion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement subnet (QE-subnet) reduces compression artifacts of the non-PQF with the help of its nearest PQFs. Finally, the experiments validate the effectiveness and generality of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code of our MFQE approach is available at https://github.com/ryangBUAA/MFQE.git.","中文标题":"压缩视频的多帧质量增强","摘要翻译":"过去几年，深度学习在提升压缩图像/视频质量方面取得了巨大成功。现有方法主要集中于增强单帧的质量，忽略了连续帧之间的相似性。本文研究发现，压缩视频帧之间存在严重的质量波动，因此可以利用邻近的高质量帧来增强低质量帧，这被视为多帧质量增强（MFQE）。据此，本文提出了一种针对压缩视频的MFQE方法，这是该方向上的首次尝试。在我们的方法中，首先开发了一种基于支持向量机（SVM）的检测器来定位压缩视频中的峰值质量帧（PQFs）。然后，设计了一种新颖的多帧卷积神经网络（MF-CNN）来增强压缩视频的质量，其中非PQF及其最近的两个PQF作为输入。MF-CNN通过运动补偿子网（MC-subnet）补偿非PQF和PQF之间的运动。随后，质量增强子网（QE-subnet）在其最近的PQF的帮助下减少非PQF的压缩伪影。最后，实验验证了我们的MFQE方法在推进压缩视频质量增强的最新技术方面的有效性和通用性。我们的MFQE方法的代码可在https://github.com/ryangBUAA/MFQE.git获取。","领域":"视频压缩/质量增强/深度学习","问题":"压缩视频帧之间的质量波动","动机":"利用邻近的高质量帧来增强低质量帧，以提升压缩视频的整体质量","方法":"开发基于SVM的检测器定位PQFs，设计MF-CNN增强视频质量，通过MC-subnet补偿运动，利用QE-subnet减少压缩伪影","关键词":["视频压缩","质量增强","多帧处理"],"涉及的技术概念":"支持向量机（SVM）用于检测峰值质量帧（PQFs），多帧卷积神经网络（MF-CNN）用于增强视频质量，运动补偿子网（MC-subnet）用于补偿帧间运动，质量增强子网（QE-subnet）用于减少压缩伪影。"},{"order":690,"title":"CNN Based Learning Using Reflection and Retinex Models for Intrinsic Image Decomposition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baslamisli_CNN_Based_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baslamisli_CNN_Based_Learning_CVPR_2018_paper.html","abstract":"Most of the traditional work on intrinsic image decomposition rely on deriving priors about scene characteristics. On the other hand, recent research use deep learning models as in-and-out black box and do not consider the well-established, traditional image formation process as the basis of their intrinsic learning process. As a consequence, although current deep learning approaches show superior performance when considering quantitative benchmark results, traditional approaches are still dominant in achieving high qualitative results. In this paper, the aim is to exploit the best of the two worlds. A method is proposed that (1) is empowered by deep learning capabilities, (2) considers a physics-based reflection model to steer the learning process, and (3) exploits the traditional approach to obtain intrinsic images by exploiting reflectance and shading gradient information. The proposed model is fast to compute and allows for the integration of all intrinsic components. To train the new model, an object centered large-scale datasets with intrinsic ground-truth images are created. The evaluation results demonstrate that the new model outperforms existing methods. Visual inspection shows that the image formation loss function augments color reproduction and the use of gradient information produces sharper edges. Datasets, models and higher resolution images are available at https://ivi.fnwi.uva.nl/cv/retinet.","中文标题":"基于CNN的学习利用反射和Retinex模型进行本征图像分解","摘要翻译":"大多数传统的本征图像分解工作依赖于推导场景特征的先验知识。另一方面，最近的研究使用深度学习模型作为输入输出的黑箱，并没有将成熟的传统图像形成过程作为其本征学习过程的基础。因此，尽管当前的深度学习方法在考虑定量基准结果时显示出优越的性能，但传统方法在实现高质量结果方面仍然占主导地位。本文旨在利用两者的优点。提出了一种方法，该方法（1）利用深度学习的能力，（2）考虑基于物理的反射模型来指导学习过程，（3）利用传统方法通过利用反射率和阴影梯度信息来获得本征图像。所提出的模型计算速度快，并允许集成所有本征组件。为了训练新模型，创建了一个以对象为中心的大规模数据集，其中包含本征真实图像。评估结果表明，新模型优于现有方法。视觉检查显示，图像形成损失函数增强了色彩再现，梯度信息的使用产生了更锐利的边缘。数据集、模型和更高分辨率的图像可在https://ivi.fnwi.uva.nl/cv/retinet获取。","领域":"本征图像分解/深度学习/图像形成模型","问题":"如何结合传统图像形成过程和深度学习技术来提高本征图像分解的质量","动机":"传统方法在实现高质量结果方面占主导地位，而深度学习方法在定量基准结果上显示出优越性能，但缺乏对传统图像形成过程的考虑。","方法":"提出了一种结合深度学习能力、基于物理的反射模型和传统方法的新模型，通过利用反射率和阴影梯度信息来获得本征图像。","关键词":["本征图像分解","深度学习","反射模型","Retinex模型","图像形成过程"],"涉及的技术概念":{"本征图像分解":"将图像分解为反射率和阴影等本征成分的过程。","深度学习":"一种机器学习方法，通过使用多层神经网络来学习数据的层次特征。","反射模型":"基于物理的模型，用于描述光线如何从物体表面反射。","Retinex模型":"一种用于模拟人类视觉系统如何处理颜色和亮度的模型。","图像形成过程":"描述图像如何从场景中的光线和物体表面特性形成的物理过程。"}},{"order":691,"title":"Image Restoration by Estimating Frequency Distribution of Local Patches","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yoo_Image_Restoration_by_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yoo_Image_Restoration_by_CVPR_2018_paper.html","abstract":"In this paper, we propose a method to solve the image restoration problem, which tries to restore the details of a corrupted image, especially due to the loss caused by JPEG compression. We have treated an image in the frequency domain to explicitly restore the frequency components lost during image compression. In doing so, the distribution in the frequency domain is learned using the cross entropy loss.  Unlike recent approaches, we have reconstructed the details of an image without using the scheme of adversarial training. Rather, the image restoration problem is treated as a classification problem to determine the frequency coefficient for each frequency band in an image patch. In this paper, we show that the proposed method effectively restores a JPEG-compressed image with more detailed high frequency components, making the restored image more vivid.","中文标题":"通过估计局部补丁的频率分布进行图像恢复","摘要翻译":"在本文中，我们提出了一种解决图像恢复问题的方法，该方法试图恢复损坏图像的细节，特别是由于JPEG压缩造成的损失。我们在频域中处理图像，以显式恢复图像压缩过程中丢失的频率成分。在此过程中，使用交叉熵损失学习频域中的分布。与最近的方法不同，我们没有使用对抗训练的方案来重建图像的细节。相反，图像恢复问题被视为一个分类问题，以确定图像补丁中每个频带的频率系数。在本文中，我们展示了所提出的方法有效地恢复了JPEG压缩图像，具有更详细的高频成分，使恢复的图像更加生动。","领域":"图像恢复/频域分析/JPEG压缩","问题":"恢复由于JPEG压缩造成的图像细节损失","动机":"为了恢复因JPEG压缩而丢失的图像细节，特别是在高频成分方面，以提高图像的清晰度和生动性","方法":"在频域中处理图像，使用交叉熵损失学习频域分布，将图像恢复问题视为分类问题来确定每个频带的频率系数","关键词":["图像恢复","频域分析","JPEG压缩","交叉熵损失","分类问题"],"涉及的技术概念":"频域分析指的是在频率域而非空间域处理图像，以恢复图像压缩过程中丢失的频率成分。交叉熵损失是一种用于分类问题的损失函数，用于衡量模型预测的概率分布与真实分布之间的差异。JPEG压缩是一种广泛使用的图像压缩标准，它通过减少图像中的冗余信息来减小文件大小，但可能会导致图像细节的损失。"},{"order":692,"title":"Latent RANSAC","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Korman_Latent_RANSAC_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Korman_Latent_RANSAC_CVPR_2018_paper.html","abstract":"We present a method that can evaluate a RANSAC hypothesis in constant time, i.e. independent of the size of the data. A key observation here is that correct hypotheses are tightly clustered together in the latent parameter domain. In a manner similar to the generalized Hough transform we seek to find this cluster, only that we need as few as two votes for a successful detection. Rapidly locating such pairs of similar hypotheses is made possible by adapting the recent \\"Random Grids\\" range-search technique. We only perform the usual (costly) hypothesis verification stage upon the discovery of a close pair of hypotheses. We show that this event rarely happens for incorrect hypotheses, enabling a significant speedup of the RANSAC pipeline.  The suggested approach is applied and tested on three robust estimation problems: camera localization, 3D rigid alignment and 2D-homography estimation. We perform rigorous testing on both synthetic and real datasets, demonstrating an improvement in efficiency without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D alignment results on the challenging \`\`Redwood'' loop-closure challenge.","中文标题":"潜在RANSAC","摘要翻译":"我们提出了一种方法，可以在恒定时间内评估RANSAC假设，即独立于数据的大小。这里的一个关键观察是，正确的假设在潜在参数域中紧密聚集在一起。类似于广义霍夫变换，我们寻求找到这个集群，只需要两个投票即可成功检测。通过采用最近的“随机网格”范围搜索技术，可以快速定位这些相似的假设对。我们只在发现一对接近的假设时执行通常（成本高昂的）假设验证阶段。我们展示了这种情况很少发生在不正确的假设上，从而显著加快了RANSAC流程。所建议的方法被应用并测试于三个鲁棒估计问题：相机定位、3D刚性对齐和2D单应性估计。我们在合成和真实数据集上进行了严格的测试，展示了在不影响准确性的情况下效率的提高。此外，我们在具有挑战性的“Redwood”闭环挑战中实现了最先进的3D对齐结果。","领域":"相机定位/3D刚性对齐/2D单应性估计","问题":"提高RANSAC假设评估的效率","动机":"为了在不影响准确性的情况下显著加快RANSAC流程","方法":"采用“随机网格”范围搜索技术快速定位相似的假设对，只在发现接近的假设对时执行假设验证","关键词":["RANSAC","相机定位","3D刚性对齐","2D单应性估计","随机网格"],"涉及的技术概念":{"RANSAC":"随机抽样一致性算法，用于从包含大量异常值的数据集中估计数学模型参数","广义霍夫变换":"一种用于检测任意形状的技术，通过将图像空间中的点映射到参数空间中的累加器","随机网格":"一种范围搜索技术，用于快速定位相似的数据点或假设"}},{"order":693,"title":"Two-Stream Convolutional Networks for Dynamic Texture Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tesfaldet_Two-Stream_Convolutional_Networks_CVPR_2018_paper.html","abstract":"We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.","中文标题":"用于动态纹理合成的双流卷积网络","摘要翻译":"我们介绍了一种用于动态纹理合成的双流模型。我们的模型基于预训练的卷积网络（ConvNets），这些网络针对两个独立的任务：（i）物体识别，和（ii）光流预测。给定一个输入动态纹理，来自物体识别卷积网络的滤波器响应统计封装了输入纹理的每帧外观，而来自光流卷积网络的滤波器响应统计则建模了其动态。为了生成新的纹理，随机初始化的输入序列被优化以匹配示例纹理的每个流的特征统计。受到最近关于图像风格转换工作的启发，并通过双流模型实现，我们还应用合成方法将一个纹理的外观与另一个纹理的动态结合起来，以生成全新的动态纹理。我们展示了我们的方法生成了新颖、高质量的样本，这些样本既匹配输入纹理的帧间外观，也匹配其时间演变。最后，我们通过彻底的用户研究对我们的纹理合成方法进行了定量评估。","领域":"动态纹理合成/卷积神经网络/光流预测","问题":"如何有效地合成既保持帧间外观又保持时间演变的动态纹理","动机":"为了生成新颖且高质量的动态纹理样本，这些样本能够匹配输入纹理的帧间外观和时间演变","方法":"采用基于预训练卷积网络的双流模型，分别针对物体识别和光流预测任务，通过优化随机初始化的输入序列以匹配示例纹理的特征统计，并结合不同纹理的外观和动态生成全新的动态纹理","关键词":["动态纹理合成","卷积神经网络","光流预测","图像风格转换"],"涉及的技术概念":"双流模型、卷积网络（ConvNets）、物体识别、光流预测、滤波器响应统计、图像风格转换、动态纹理合成"},{"order":694,"title":"Towards Open-Set Identity Preserving Face Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bao_Towards_Open-Set_Identity_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bao_Towards_Open-Set_Identity_CVPR_2018_paper.html","abstract":"We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.","中文标题":"迈向开放集身份保持的人脸合成","摘要翻译":"我们提出了一个基于生成对抗网络的框架，以解耦人脸的身份和属性，从而我们可以方便地在开放领域中重新组合不同的身份和属性，以实现身份保持的人脸合成。以往的身份保持人脸合成过程主要局限于合成训练数据集中已知身份的人脸。为了合成训练数据集之外身份的人脸，我们的框架需要该主题的一张输入图像来生成身份向量，以及任何其他输入人脸图像来提取捕捉例如姿势、情感、光照甚至背景的属性向量。然后，我们重新组合身份向量和属性向量，以合成具有提取属性的主题的新人脸。我们提出的框架不需要以任何方式注释人脸的属性。它通过一个非对称损失函数进行训练，以更好地保持身份并稳定训练过程。它还可以有效地利用大量未标记的训练人脸图像，进一步提高不在标记训练人脸数据集中呈现的主题的合成人脸的真实性。我们的实验证明了所提出框架的有效性。我们还展示了其在更广泛的应用中的使用，包括人脸正面化、人脸属性变形和人脸对抗样本检测。","领域":"人脸合成/生成对抗网络/开放集识别","问题":"如何在开放集中实现身份保持的人脸合成","动机":"解决现有身份保持人脸合成方法局限于训练数据集中已知身份的问题，实现更广泛的应用","方法":"提出基于生成对抗网络的框架，通过解耦身份和属性向量，并重新组合它们来合成新的人脸图像","关键词":["人脸合成","生成对抗网络","开放集识别","身份保持","属性解耦"],"涉及的技术概念":"生成对抗网络（GANs）用于解耦人脸的身份和属性，非对称损失函数用于训练以保持身份和稳定训练过程，利用未标记的训练人脸图像提高合成人脸的真实性。"},{"order":695,"title":"A Revised Underwater Image Formation Model","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Akkaynak_A_Revised_Underwater_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Akkaynak_A_Revised_Underwater_CVPR_2018_paper.html","abstract":"The current underwater image formation model descends from atmospheric dehazing equations where attenuation is a weak function of wavelength. We recently showed that this model introduces significant errors and dependencies in the estimation of the direct transmission signal because underwater, light attenuates in a wavelength-dependent manner. Here, we show that the backscattered signal derived from the current model also suffers from dependencies that were previously unaccounted for. In doing so, we use oceanographic measurements to derive the physically valid space of backscatter, and further show that the wideband coefficients that govern backscatter are different than those that govern direct transmission, even though the current model treats them to be the same. We propose a revised equation for underwater image formation that takes these differences into account, and validate it through in situ experiments underwater. This revised model might explain frequent instabilities of current underwater color reconstruction models, and calls for the development of new methods.","中文标题":"修订的水下图像形成模型","摘要翻译":"当前的水下图像形成模型源自大气去雾方程，其中衰减是波长的弱函数。我们最近表明，由于水下光的衰减是波长依赖的，该模型在直接传输信号的估计中引入了显著的误差和依赖性。在这里，我们展示了从当前模型得出的反向散射信号也遭受了之前未考虑的依赖性。在此过程中，我们使用海洋学测量来推导出物理上有效的反向散射空间，并进一步表明，控制反向散射的宽带系数与控制直接传输的系数不同，尽管当前模型将它们视为相同。我们提出了一个修订的水下图像形成方程，考虑了这些差异，并通过水下原位实验进行了验证。这个修订的模型可能解释了当前水下颜色重建模型频繁不稳定的原因，并呼吁开发新方法。","领域":"水下成像/光学成像/海洋学","问题":"当前水下图像形成模型在估计直接传输信号和反向散射信号时引入的误差和依赖性","动机":"解决当前水下图像形成模型在估计直接传输信号和反向散射信号时引入的误差和依赖性，以提高水下图像重建的准确性和稳定性","方法":"使用海洋学测量推导物理上有效的反向散射空间，提出修订的水下图像形成方程，并通过水下原位实验进行验证","关键词":["水下成像","光学成像","海洋学"],"涉及的技术概念":"水下图像形成模型、大气去雾方程、波长依赖的衰减、直接传输信号、反向散射信号、宽带系数、海洋学测量、原位实验"},{"order":696,"title":"Graph-Cut RANSAC","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Barath_Graph-Cut_RANSAC_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Barath_Graph-Cut_RANSAC_CVPR_2018_paper.html","abstract":"A novel method for robust estimation, called Graph-Cut RANSAC, GC-RANSAC in short, is introduced. To separate inliers and outliers, it runs the graph-cut algorithm in the local optimization (LO) step which is applied when a so-far-the-best model is found. The proposed LO step is conceptually simple, easy to implement, globally optimal and efficient. GC-RANSAC is shown experimentally, both on synthesized tests and real image pairs, to be more geometrically accurate than state-of-the-art methods on a range of problems, e.g. line fitting, homography, affine transformation, fundamental and essential matrix estimation. It runs in real-time for many problems at a speed approximately equal to that of the less accurate alternatives (in milliseconds on standard CPU).","中文标题":"图割RANSAC","摘要翻译":"介绍了一种名为图割RANSAC（简称GC-RANSAC）的鲁棒估计新方法。为了分离内点和外点，它在局部优化（LO）步骤中运行图割算法，该步骤在找到迄今为止最佳模型时应用。所提出的LO步骤概念简单，易于实现，全局最优且高效。通过合成测试和真实图像对的实验表明，GC-RANSAC在一系列问题上（例如，线拟合、单应性、仿射变换、基础和本质矩阵估计）比最先进的方法具有更高的几何精度。对于许多问题，它以实时速度运行，速度大约等于不太准确的替代方案（在标准CPU上以毫秒计）。","领域":"几何估计/图像匹配/实时处理","问题":"在几何估计和图像匹配中准确分离内点和外点","动机":"提高几何估计和图像匹配的准确性和效率","方法":"在局部优化步骤中应用图割算法来分离内点和外点","关键词":["图割算法","RANSAC","局部优化","几何估计","图像匹配"],"涉及的技术概念":{"图割算法":"一种用于图像分割的算法，通过最小化能量函数来找到最优分割。","RANSAC":"随机抽样一致性算法，用于从包含大量外点的数据集中估计数学模型。","局部优化":"在找到迄今为止最佳模型后，进一步优化模型参数以提高估计的准确性。","几何估计":"从图像数据中估计几何模型，如线、单应性、仿射变换等。","图像匹配":"在不同图像之间找到对应点，用于估计图像间的几何关系。"}},{"order":697,"title":"Temporal Deformable Residual Networks for Action Segmentation in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html","abstract":"This paper is about temporal segmentation of human actions in videos. We introduce a new model -- temporal deformable residual network (TDRN) -- aimed at  analyzing video intervals at multiple temporal scales for labeling video frames.  Our TDRN computes two parallel temporal streams: i) Residual stream that analyzes video information at its full temporal  resolution, and ii) Pooling/unpooling stream that captures long-range video information at different scales. The former  facilitates local, fine-scale action segmentation, and the latter uses multiscale context for improving accuracy of frame classification.  These two streams are computed by a set of temporal residual modules with deformable convolutions, and fused by temporal residuals at the full video resolution. Our evaluation on the University of Dundee 50 Salads, Georgia Tech  Egocentric Activities, and JHU-ISI Gesture and Skill Assessment Working Set demonstrates that TDRN outperforms the state of the art  in frame-wise segmentation accuracy, segmental edit score, and segmental overlap F1 score.","中文标题":"时间可变形残差网络用于视频中的动作分割","摘要翻译":"本文讨论的是视频中人类动作的时间分割。我们引入了一个新模型——时间可变形残差网络（TDRN），旨在分析视频间隔的多个时间尺度以标记视频帧。我们的TDRN计算两个并行的时间流：i) 残差流，以全时间分辨率分析视频信息，ii) 池化/反池化流，捕捉不同尺度的长距离视频信息。前者促进了局部的、细粒度的动作分割，后者使用多尺度上下文来提高帧分类的准确性。这两个流由一组具有可变形卷积的时间残差模块计算，并通过全视频分辨率的时间残差融合。我们在邓迪大学50沙拉、乔治亚理工学院自我中心活动和JHU-ISI手势与技能评估工作集上的评估表明，TDRN在帧级分割准确性、分段编辑得分和分段重叠F1得分方面优于现有技术。","领域":"动作识别/视频分析/时间序列分析","问题":"视频中人类动作的时间分割","动机":"提高视频帧标记的准确性，通过分析视频间隔的多个时间尺度","方法":"引入时间可变形残差网络（TDRN），计算两个并行的时间流：残差流和池化/反池化流，通过时间残差模块和可变形卷积实现","关键词":["动作分割","时间可变形残差网络","视频分析"],"涉及的技术概念":"时间可变形残差网络（TDRN）是一种用于视频动作分割的模型，它通过分析视频的多个时间尺度来提高帧标记的准确性。该模型包括两个并行的时间流：残差流和池化/反池化流，分别用于分析视频的全时间分辨率和捕捉不同尺度的长距离视频信息。这些流通过具有可变形卷积的时间残差模块计算，并通过全视频分辨率的时间残差融合。"},{"order":698,"title":"Weakly Supervised Action Localization by Sparse Temporal Pooling Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nguyen_Weakly_Supervised_Action_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nguyen_Weakly_Supervised_Action_CVPR_2018_paper.html","abstract":"We propose a weakly supervised temporal action localization algorithm on untrimmed videos using convolutional neural networks. Our algorithm learns from video-level class labels and predicts temporal intervals of human actions with no requirement of temporal localization annotations. We design our network to identify a sparse subset of key segments associated with target actions in a video using an attention module and fuse the key segments through adaptive temporal pooling. Our loss function is comprised of two terms that minimize the video-level action classification error and enforce the sparsity of the segment selection. At inference time, we extract and score temporal proposals using temporal class activations and class-agnostic attentions to estimate the time intervals that correspond to target actions. The proposed algorithm attains state-of-the-art results on the THUMOS14 dataset and outstanding performance on ActivityNet1.3 even with its weak supervision.","中文标题":"通过稀疏时间池化网络进行弱监督动作定位","摘要翻译":"我们提出了一种在未剪辑视频上使用卷积神经网络进行弱监督时间动作定位的算法。我们的算法从视频级别的类别标签中学习，并预测人类动作的时间间隔，而无需时间定位注释。我们设计了我们的网络，通过一个注意力模块识别与目标动作相关的视频中的关键片段稀疏子集，并通过自适应时间池化融合这些关键片段。我们的损失函数由两个项组成，最小化视频级别的动作分类误差并强制片段选择的稀疏性。在推理时，我们使用时间类别激活和类别无关的注意力提取并评分时间提议，以估计与目标动作对应的时间间隔。所提出的算法在THUMOS14数据集上达到了最先进的结果，并在ActivityNet1.3上表现出色，即使是在弱监督的情况下。","领域":"动作识别/视频分析/时间序列分析","问题":"在未剪辑视频中进行时间动作定位","动机":"减少对时间定位注释的依赖，通过弱监督学习提高动作定位的效率和准确性","方法":"使用卷积神经网络和注意力模块识别关键片段，通过自适应时间池化融合关键片段，并设计包含视频级别动作分类误差最小化和片段选择稀疏性强制的损失函数","关键词":["弱监督学习","时间动作定位","卷积神经网络","注意力机制","自适应时间池化"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像和视频数据","注意力模块":"一种机制，用于让模型在处理输入数据时集中注意力于最重要的部分","自适应时间池化":"一种技术，用于从视频中提取关键片段并融合这些片段以进行进一步分析","弱监督学习":"一种学习方法，其中模型从较少的、不完全的或较粗糙的注释中学习"}},{"order":699,"title":"PoseFlow: A Deep Motion Representation for Understanding Human Behaviors in Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_PoseFlow_A_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_PoseFlow_A_Deep_CVPR_2018_paper.html","abstract":"Motion of the human body is the critical cue for understanding and characterizing human behavior in videos. Most existing approaches explore the motion cue using optical flows. However, optical flow usually contains motion on both the interested human bodies and the undesired background. This \\"noisy\\" motion representation makes it very challenging for pose estimation and action recognition in real scenarios. To address this issue, this paper presents a novel deep motion representation, called PoseFlow, which reveals human motion in videos while suppressing background and motion blur, and being robust to occlusion. For learning PoseFlow with mild computational cost, we propose a functionally structured spatial-temporal deep network, PoseFlow Net (PFN), to jointly solve the skeleton localization and matching problems of PoseFlow. Comprehensive experiments show that PFN outperforms the state-of-the-art deep flow estimation models in generating PoseFlow. Moreover, PoseFlow demonstrates its potential on  improving two challenging tasks in human video analysis: pose estimation and action recognition.","中文标题":"PoseFlow: 一种用于理解视频中人类行为的深度运动表示","摘要翻译":"人体的运动是理解和描述视频中人类行为的关键线索。大多数现有方法通过光流探索运动线索。然而，光流通常包含感兴趣的人体和不需要的背景的运动。这种“嘈杂”的运动表示使得在实际场景中进行姿态估计和动作识别非常具有挑战性。为了解决这个问题，本文提出了一种新颖的深度运动表示，称为PoseFlow，它在揭示视频中人体运动的同时抑制背景和运动模糊，并对遮挡具有鲁棒性。为了以较低的计算成本学习PoseFlow，我们提出了一个功能结构化的时空深度网络，PoseFlow Net（PFN），以联合解决PoseFlow的骨架定位和匹配问题。综合实验表明，PFN在生成PoseFlow方面优于最先进的深度流估计模型。此外，PoseFlow展示了其在改进人类视频分析中两个具有挑战性的任务：姿态估计和动作识别方面的潜力。","领域":"人体姿态估计/动作识别/视频分析","问题":"解决在视频中准确理解和描述人类行为时，由于光流中包含不需要的背景运动导致的运动表示噪声问题","动机":"为了提高在复杂背景和遮挡情况下视频中人体姿态估计和动作识别的准确性和鲁棒性","方法":"提出了一种新颖的深度运动表示PoseFlow，并开发了一个功能结构化的时空深度网络PoseFlow Net（PFN）来联合解决骨架定位和匹配问题","关键词":["人体姿态估计","动作识别","视频分析","深度运动表示","时空深度网络"],"涉及的技术概念":"PoseFlow是一种深度运动表示，旨在揭示视频中的人体运动，同时抑制背景和运动模糊，并对遮挡具有鲁棒性。PoseFlow Net（PFN）是一个功能结构化的时空深度网络，用于联合解决PoseFlow的骨架定位和匹配问题，以较低的计算成本学习PoseFlow。"},{"order":700,"title":"FFNet: Video Fast-Forwarding via Reinforcement Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lan_FFNet_Video_Fast-Forwarding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lan_FFNet_Video_Fast-Forwarding_CVPR_2018_paper.html","abstract":"For many intelligent applications with limited computation, communication, storage and energy resources, there is an imperative need of vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a \\"trailer\\" for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video (about 6%-20% improvement on coverage of important frames) with much less processing requirement (more than 80% reduction in the number of frames processed).","中文标题":"FFNet：通过强化学习的视频快进","摘要翻译":"对于许多计算、通信、存储和能源资源有限的智能应用，迫切需要一种视觉方法，能够选择输入视频的信息子集，以便在接近实时的情况下进行高效处理。在文献中，有两组相关的方法：为视频生成“预告片”或在观看/处理视频时进行快进。第一组方法由视频摘要技术支持，这需要处理整个视频以选择重要子集展示给用户。在第二组方法中，当前的快进方法依赖于手动控制或播放速度的自动调整，这往往不能准确表示视频内容，可能仍然需要处理每一帧。在本文中，我们介绍了FastForwardNet（FFNet），一个从视频摘要中获得灵感并以不同方式进行快进的强化学习代理。它是一个在线框架，能够自动快进视频，并即时向用户展示代表性的帧子集。它不需要处理整个视频，只需处理由快进代理选择的部分，这使得过程在计算上非常高效。我们提出的方法的在线性质还使用户能够在视频的任何点开始快进。在两个真实世界的数据集上的实验表明，我们的方法能够以更少的处理需求（处理的帧数减少超过80%）提供输入视频的更好表示（重要帧的覆盖率提高约6%-20%）。","领域":"视频处理/强化学习/在线学习","问题":"如何在资源有限的情况下高效处理视频，选择信息丰富的子集进行展示","动机":"智能应用在计算、通信、存储和能源资源有限的情况下，需要一种能够高效处理视频的方法","方法":"引入FastForwardNet（FFNet），一个基于强化学习的在线框架，自动快进视频并展示代表性的帧子集","关键词":["视频快进","强化学习","在线处理"],"涉及的技术概念":"视频摘要技术、强化学习代理、在线框架、帧覆盖率"},{"order":701,"title":"Multi-Shot Pedestrian Re-Identification via Sequential Decision Making","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Multi-Shot_Pedestrian_Re-Identification_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Multi-Shot_Pedestrian_Re-Identification_CVPR_2018_paper.html","abstract":"Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to verify (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance.","中文标题":"通过序列决策进行多镜头行人重识别","摘要翻译":"多镜头行人重识别问题是监控视频分析的核心。它匹配来自不同摄像头的两个行人轨迹。与现有工作通过时间序列模型（如循环神经网络）聚合单帧特征不同，本文提出了一种基于可解释强化学习的方法来解决这个问题。特别是，我们训练一个代理在每次验证一对图像。代理可以选择输出结果（相同或不同）或请求另一对图像进行验证（不确定）。通过这种方式，我们的模型隐式地学习了图像对的难度，并在模型没有积累足够证据时推迟决策。此外，通过调整不确定动作的奖励，我们可以轻松在速度和准确性之间进行权衡。在三个公开基准测试中，我们的方法与最先进的方法竞争，同时仅使用3%到6%的图像。这些有希望的结果表明，我们的方法在效率和性能上都是有利的。","领域":"行人重识别/强化学习/视频分析","问题":"解决多镜头行人重识别问题，即匹配来自不同摄像头的行人轨迹","动机":"现有方法通过时间序列模型聚合单帧特征，缺乏对图像对难度的隐式学习和决策推迟机制","方法":"提出了一种基于可解释强化学习的方法，训练代理在每次验证一对图像，并允许代理在不确定时请求更多图像进行验证","关键词":["行人重识别","强化学习","视频分析"],"涉及的技术概念":"强化学习是一种让代理通过与环境交互来学习策略的机器学习方法。在本研究中，强化学习被用来训练一个代理，该代理能够决定一对图像是否属于同一个行人，或者在不确定时请求更多图像进行验证。这种方法允许模型根据图像对的难度动态调整决策过程，从而提高识别效率和准确性。"},{"order":702,"title":"Attend and Interact: Higher-Order Object Interactions for Video Understanding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Attend_and_Interact_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Attend_and_Interact_CVPR_2018_paper.html","abstract":"Human actions often involve complex interactions across several inter-related objects in the scene. However, existing approaches to fine-grained video understanding or visual relationship detection often rely on single object representation or pairwise object relationships. Furthermore, learning interactions across multiple objects in hundreds of frames for video is computationally infeasible and performance may suffer since a large combinatorial space has to be modeled. In this paper, we propose to efficiently learn higher-order interactions between arbitrary subgroups of objects for fine-grained video understanding. We demonstrate that modeling object interactions significantly improves accuracy for both action recognition and video captioning, while saving more than 3-times the computation over traditional pairwise relationships. The proposed method is validated on two large-scale datasets: Kinetics and ActivityNet Captions. Our SINet and SINet-Caption achieve state-of-the-art performances on both datasets even though the videos are sampled at a maximum of 1 FPS. To the best of our knowledge, this is the first work modeling object interactions on open domain large-scale video datasets, and we additionally model higher-order object interactions which improves the performance with low computational costs.","中文标题":"关注与交互：视频理解中的高阶对象交互","摘要翻译":"人类行为通常涉及场景中多个相互关联对象之间的复杂交互。然而，现有的细粒度视频理解或视觉关系检测方法往往依赖于单一对象表示或成对对象关系。此外，在视频中学习数百帧中多个对象之间的交互在计算上是不可行的，并且由于需要建模一个大的组合空间，性能可能会受到影响。在本文中，我们提出了一种有效的方法，用于细粒度视频理解中任意对象子组之间的高阶交互学习。我们证明了建模对象交互显著提高了动作识别和视频字幕生成的准确性，同时比传统的成对关系节省了超过3倍的计算量。所提出的方法在两个大规模数据集上得到了验证：Kinetics和ActivityNet Captions。我们的SINet和SINet-Caption在这两个数据集上实现了最先进的性能，即使视频以最大1 FPS的速度采样。据我们所知，这是第一个在开放领域大规模视频数据集上建模对象交互的工作，并且我们还建模了高阶对象交互，从而以低计算成本提高了性能。","领域":"视频理解/动作识别/视频字幕生成","问题":"如何在视频理解中有效建模多个对象之间的高阶交互","动机":"现有方法在处理视频中多个对象之间的复杂交互时存在计算量大和性能受限的问题","方法":"提出了一种有效的方法，用于细粒度视频理解中任意对象子组之间的高阶交互学习，显著提高了动作识别和视频字幕生成的准确性，同时节省了计算量","关键词":["高阶交互","视频理解","动作识别","视频字幕生成"],"涉及的技术概念":"高阶对象交互指的是在视频理解中，不仅考虑单一对象或成对对象之间的关系，而是考虑多个对象之间的复杂交互。这种方法通过建模这些高阶交互来提高动作识别和视频字幕生成的准确性，同时通过优化计算过程来减少计算成本。"},{"order":703,"title":"Where and Why Are They Looking? Jointly Inferring Human Attention and Intentions in Complex Tasks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Where_and_Why_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Where_and_Why_CVPR_2018_paper.html","abstract":"This paper addresses a new problem - jointly inferring human attention, intentions, and tasks from videos. Given an RGB-D video where a human performs a task, we answer three questions simultaneously: 1) where the human is looking - attention prediction; 2) why the human is looking there - intention prediction; and 3) what task the human is performing - task recognition. We propose a hierarchical model of human-attention-object (HAO) which represents tasks, intentions, and attention under a unified framework. A task is represented as sequential intentions which transition to each other. An intention is composed of the human pose, attention, and objects. A beam search algorithm is adopted for inference on the HAO graph to output the attention, intention, and task results. We built a new video dataset of tasks, intentions, and attention. It contains 14 task classes, 70 intention categories, 28 object classes, 809 videos, and approximately 330,000 frames. Experiments show that our approach outperforms existing approaches.","中文标题":"他们为何及何处注视？复杂任务中人类注意力和意图的联合推断","摘要翻译":"本文解决了一个新问题——从视频中联合推断人类的注意力、意图和任务。给定一个人类执行任务的RGB-D视频，我们同时回答三个问题：1）人类正在注视何处——注意力预测；2）人类为何注视那里——意图预测；3）人类正在执行什么任务——任务识别。我们提出了一个人类-注意力-对象（HAO）的层次模型，该模型在统一框架下表示任务、意图和注意力。任务被表示为相互转换的连续意图。意图由人类姿势、注意力和对象组成。采用光束搜索算法在HAO图上进行推理，以输出注意力、意图和任务结果。我们建立了一个新的任务、意图和注意力的视频数据集。它包含14个任务类别、70个意图类别、28个对象类别、809个视频和大约330,000帧。实验表明，我们的方法优于现有方法。","领域":"人类行为理解/视频分析/注意力机制","问题":"从视频中联合推断人类的注意力、意图和任务","动机":"为了更深入地理解人类在执行复杂任务时的注意力和意图，以及他们正在执行的具体任务","方法":"提出了一个人类-注意力-对象（HAO）的层次模型，采用光束搜索算法在HAO图上进行推理","关键词":["人类行为理解","视频分析","注意力机制"],"涉及的技术概念":"RGB-D视频、注意力预测、意图预测、任务识别、人类-注意力-对象（HAO）模型、光束搜索算法"},{"order":704,"title":"Fully Convolutional Adaptation Networks for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.html","abstract":"The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the \`\`style\\" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting.","中文标题":"全卷积适应网络用于语义分割","摘要翻译":"深度神经网络的最新进展已经令人信服地展示了在大数据集上学习视觉模型的高能力。然而，收集专家标注的数据集，特别是带有像素级注释的数据集，是一个极其昂贵的过程。一个有吸引力的替代方案是渲染合成数据（例如，电脑游戏）并自动生成地面实况。然而，由于领域转移，简单地将从合成图像中学到的模型应用于真实图像可能会导致高泛化误差。在本文中，我们从视觉外观级和表示级领域适应的角度来促进这一问题的解决。前者将源域图像适应为看起来像是从目标域的“风格”中绘制的，而后者则尝试学习领域不变的表示。具体来说，我们提出了全卷积适应网络（FCAN），一种用于语义分割的新型深度架构，它结合了外观适应网络（AAN）和表示适应网络（RAN）。AAN学习从像素空间中的一个域到另一个域的转换，而RAN则以对抗学习的方式优化，以最大限度地欺骗领域鉴别器，使用学到的源和目标表示。我们在从GTA5（游戏视频）到Cityscapes（城市街景）的语义分割转移上进行了广泛的实验，与最先进的无监督适应技术相比，我们的提案取得了优异的结果。更值得注意的是，我们在无监督设置下在BDDS（驾驶摄像头视频）上获得了新的记录：mIoU为47.5%。","领域":"语义分割/领域适应/对抗学习","问题":"解决从合成图像到真实图像的语义分割中的领域转移问题","动机":"收集专家标注的数据集，特别是带有像素级注释的数据集，是一个极其昂贵的过程，而合成数据可以自动生成地面实况，但直接应用从合成图像中学到的模型到真实图像会导致高泛化误差","方法":"提出了全卷积适应网络（FCAN），结合了外观适应网络（AAN）和表示适应网络（RAN），AAN学习从像素空间中的一个域到另一个域的转换，RAN以对抗学习的方式优化，以最大限度地欺骗领域鉴别器","关键词":["语义分割","领域适应","对抗学习"],"涉及的技术概念":{"全卷积适应网络（FCAN）":"一种用于语义分割的新型深度架构，结合了外观适应网络（AAN）和表示适应网络（RAN）","外观适应网络（AAN）":"学习从像素空间中的一个域到另一个域的转换","表示适应网络（RAN）":"以对抗学习的方式优化，以最大限度地欺骗领域鉴别器，使用学到的源和目标表示","领域转移":"由于源域和目标域之间的差异，导致模型在目标域上的性能下降","对抗学习":"一种学习方法，通过让两个网络相互对抗来优化模型"}},{"order":705,"title":"Semantic Video Segmentation by Gated Recurrent Flow Propagation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nilsson_Semantic_Video_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nilsson_Semantic_Video_Segmentation_CVPR_2018_paper.html","abstract":"Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology for video segmentation that is capable of leveraging the information present in unlabeled data, besides sparsely labeled frames, in order to improve semantic estimates. Our model combines a convolutional architecture and a spatio-temporal transformer recurrent layer that is able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recognition and the gated temporal propagation modules can be trained jointly, end-to-end. The temporal, gated recurrent flow propagation component of our model can be plugged into any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our experiments in the challenging CityScapes and Camvid datasets, and for multiple deep architectures, indicate that the resulting model can leverage unlabeled temporal frames, next to a labeled one, in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost and with little extra computation.","中文标题":"通过门控循环流传播进行语义视频分割","摘要翻译":"由于需要处理和标记大量数据以构建准确模型，语义视频分割具有挑战性。在本文中，我们提出了一种深度、端到端可训练的视频分割方法，该方法能够利用未标记数据中的信息，除了稀疏标记的帧外，以提高语义估计。我们的模型结合了卷积架构和时空变换器循环层，该层能够通过光流时间传播标记信息，基于其局部估计的不确定性自适应门控。流、识别和门控时间传播模块可以联合训练，端到端。我们模型的时间门控循环流传播组件可以插入任何静态语义分割架构中，并将其转变为弱监督视频处理架构。我们在具有挑战性的CityScapes和Camvid数据集上的实验，以及对于多种深度架构的实验表明，所得到的模型能够利用未标记的时间帧，与标记的帧相邻，以提高视频分割的准确性和其时间标记的一致性，无需额外的注释成本且计算量增加很少。","领域":"语义分割/视频处理/光流估计","问题":"提高语义视频分割的准确性和时间标记的一致性","动机":"利用未标记数据中的信息以提高语义估计，减少对大量标记数据的依赖","方法":"结合卷积架构和时空变换器循环层，通过光流时间传播标记信息，并基于局部估计的不确定性进行自适应门控","关键词":["语义分割","视频处理","光流估计","门控机制","弱监督学习"],"涉及的技术概念":"卷积架构用于特征提取，时空变换器循环层用于时间信息传播，光流用于估计帧间运动，门控机制基于局部不确定性自适应控制信息流动，端到端训练方法允许模型各部分联合优化。"},{"order":706,"title":"Interpretable Video Captioning via Trajectory Structured Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.html","abstract":"Automatically describing open-domain videos with natural language are attracting increasing interest in the field of artificial intelligence. Most existing methods simply borrow ideas from image captioning and obtain a compact video representation from an ensemble of global image feature before feeding to an RNN decoder which outputs a sentence of variable length. However, it is not only arduous for the generator to focus on specific salient objects at different time given the global video representation, it is more formidable to capture the fine-grained motion information and the relation between moving instances for more subtle linguistic descriptions. In this paper, we propose a Trajectory Structured Attentional Encoder-Decoder (TSA-ED) neural network framework for more elaborate video captioning which works by integrating local spatial-temporal representation at trajectory level through structured attention mechanism. Our proposed method is based on a LSTM-based encoder-decoder framework, which incorporates an attention modeling scheme to adaptively learn the correlation between sentence structure and the moving objects in videos, and consequently generates more accurate and meticulous statement description in the decoding stage. Experimental results demonstrate that the feature representation and structured attention mechanism based on the trajectory cluster can efficiently obtain the local motion information in the video to help generate a more fine-grained video description, and achieve the state-of-the-art performance on the well-known Charades and MSVD datasets.","中文标题":"通过轨迹结构化定位实现可解释的视频字幕生成","摘要翻译":"自动用自然语言描述开放域视频在人工智能领域引起了越来越多的兴趣。大多数现有方法简单地从图像字幕生成中借鉴思想，并在输入到输出可变长度句子的RNN解码器之前，从全局图像特征的集合中获得紧凑的视频表示。然而，不仅让生成器在给定全局视频表示的情况下专注于不同时间的特定显著对象是困难的，而且捕捉细粒度的运动信息和移动实例之间的关系以进行更微妙的语言描述更是艰巨的。在本文中，我们提出了一个轨迹结构化注意力编码器-解码器（TSA-ED）神经网络框架，用于更精细的视频字幕生成，该框架通过结构化注意力机制在轨迹级别整合局部时空表示。我们提出的方法基于LSTM的编码器-解码器框架，该框架结合了注意力建模方案，以自适应地学习句子结构与视频中移动对象之间的相关性，从而在解码阶段生成更准确和细致的语句描述。实验结果表明，基于轨迹聚类的特征表示和结构化注意力机制可以有效地获取视频中的局部运动信息，以帮助生成更细粒度的视频描述，并在著名的Charades和MSVD数据集上实现了最先进的性能。","领域":"视频字幕生成/自然语言处理/深度学习","问题":"如何生成更准确和细致的视频描述","动机":"现有方法难以捕捉视频中的细粒度运动信息和移动实例之间的关系，导致生成的视频描述不够准确和细致","方法":"提出了一个轨迹结构化注意力编码器-解码器（TSA-ED）神经网络框架，通过结构化注意力机制在轨迹级别整合局部时空表示，以生成更精细的视频字幕","关键词":["视频字幕生成","轨迹结构化","注意力机制"],"涉及的技术概念":"LSTM（长短期记忆网络）、RNN（循环神经网络）、结构化注意力机制、轨迹聚类、局部时空表示"},{"order":707,"title":"Deep Hashing via Discrepancy Minimization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Deep_Hashing_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Deep_Hashing_via_CVPR_2018_paper.html","abstract":"This paper presents a discrepancy minimizing model to address the discrete optimization problem in hashing learning.  The discrete optimization introduced by binary constraint is an NP-hard mixed integer programming problem. It is usually addressed by relaxing the binary variables into continuous variables to adapt to the gradient based learning of hashing functions, especially the training of deep neural networks. To deal with the objective discrepancy caused by relaxation, we transform the original binary optimization into differentiable optimization problem over hash functions through series expansion. This transformation decouples the binary constraint and the similarity preserving hashing function optimization. The transformed objective is optimized in a tractable alternating optimization framework with gradual discrepancy minimization. Extensive experimental results on three benchmark datasets validate the efficacy of the proposed discrepancy minimizing hashing.","中文标题":"通过差异最小化进行深度哈希","摘要翻译":"本文提出了一种差异最小化模型，以解决哈希学习中的离散优化问题。由二进制约束引入的离散优化是一个NP难的混合整数规划问题。通常通过将二进制变量松弛为连续变量来解决，以适应基于梯度的哈希函数学习，特别是深度神经网络的训练。为了处理由松弛引起的目标差异，我们通过级数展开将原始二进制优化转化为可微分的哈希函数优化问题。这种转换解耦了二进制约束和保持相似性的哈希函数优化。转换后的目标在一个可处理的交替优化框架中通过逐步差异最小化进行优化。在三个基准数据集上的广泛实验结果验证了所提出的差异最小化哈希的有效性。","领域":"哈希学习/离散优化/深度神经网络","问题":"解决哈希学习中的离散优化问题","动机":"处理由二进制约束引入的离散优化问题，这是一个NP难的混合整数规划问题，通常需要将二进制变量松弛为连续变量以适应基于梯度的哈希函数学习。","方法":"通过级数展开将原始二进制优化转化为可微分的哈希函数优化问题，解耦二进制约束和保持相似性的哈希函数优化，并在交替优化框架中通过逐步差异最小化进行优化。","关键词":["哈希学习","离散优化","深度神经网络","差异最小化","二进制约束"],"涉及的技术概念":"本文涉及的技术概念包括离散优化、哈希学习、二进制约束、级数展开、可微分优化、交替优化框架和差异最小化。离散优化是指寻找在离散变量上的最优解的问题，哈希学习是指通过学习算法将数据映射到哈希码的过程，二进制约束是指在优化问题中变量只能取二进制值（0或1）的约束条件，级数展开是一种数学方法，用于将复杂函数表示为简单函数的和，可微分优化是指目标函数是可微分的优化问题，交替优化框架是一种优化策略，通过交替优化不同的变量或参数来求解问题，差异最小化是指通过最小化目标函数与实际值之间的差异来优化模型。"},{"order":708,"title":"ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.html","abstract":"We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet~cite{howard2017mobilenets} on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves $sim$13$\\times$ actual speedup over AlexNet while maintaining comparable accuracy.","中文标题":"ShuffleNet：一种专为移动设备设计的极其高效的卷积神经网络","摘要翻译":"我们介绍了一种名为ShuffleNet的计算效率极高的CNN架构，专为计算能力非常有限的移动设备（例如，10-150 MFLOPs）设计。新架构利用了两个新操作，点向群卷积和通道混洗，以在保持准确性的同时大幅降低计算成本。在ImageNet分类和MS COCO对象检测上的实验表明，ShuffleNet优于其他结构，例如在40 MFLOPs的计算预算下，在ImageNet分类任务上的top-1错误率（绝对7.8%）低于最近的MobileNet。在基于ARM的移动设备上，ShuffleNet在保持相当准确性的同时，实现了约13倍于AlexNet的实际加速。","领域":"移动计算, 神经网络优化, 高效计算","问题":"在计算能力有限的移动设备上实现高效的卷积神经网络","动机":"为了在计算资源受限的移动设备上实现高效的深度学习应用，需要设计一种既计算效率高又能保持模型准确性的神经网络架构。","方法":"采用点向群卷积和通道混洗两种新操作来减少计算成本，同时保持模型的准确性。","关键词":["ShuffleNet","点向群卷积","通道混洗"],"涉及的技术概念":{"点向群卷积":"一种减少计算成本的卷积操作，通过在通道维度上进行分组卷积来实现。","通道混洗":"一种重新排列通道信息的技术，以促进不同组之间的信息交流，提高模型的表达能力。","MFLOPs":"衡量计算复杂度的单位，表示每秒百万次浮点运算。"}},{"order":709,"title":"Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Zero-Shot_Recognition_via_CVPR_2018_paper.html","abstract":"We consider the problem of zero-shot recognition: learning a visual classifier for a category with zero training examples, just using the word embedding of the category and its relationship to other categories, which visual data are provided. The key to dealing with the unfamiliar or novel category is to transfer knowledge obtained from familiar classes to describe the unfamiliar class. In this paper, we build upon the recently introduced Graph Convolutional Network (GCN) and propose an approach that uses both semantic embeddings and the categorical relationships to predict the classifiers. Given a learned knowledge graph (KG), our approach takes as input semantic embeddings for each node (representing visual category). After a series of graph convolutions, we predict the visual classifier for each category. During training, the visual classifiers for a few categories are given to learn the GCN parameters. At test time, these filters are used to predict the visual classifiers of unseen categories. We show that our approach is robust to noise in the KG. More importantly, our approach provides significant improvement in performance compared to the current state-of-the-art results (from 2 ~ 3% on some metrics to whopping 20% on a few).","中文标题":"通过语义嵌入和知识图谱进行零样本识别","摘要翻译":"我们考虑零样本识别问题：学习一个视觉分类器，用于没有训练样本的类别，仅使用该类别的词嵌入及其与其他类别的关系，这些类别提供了视觉数据。处理不熟悉或新类别的关键是将从熟悉类别获得的知识转移到描述不熟悉的类别。在本文中，我们基于最近引入的图卷积网络（GCN），并提出了一种使用语义嵌入和类别关系来预测分类器的方法。给定一个学习到的知识图谱（KG），我们的方法将每个节点（代表视觉类别）的语义嵌入作为输入。经过一系列图卷积后，我们预测每个类别的视觉分类器。在训练期间，给出少数类别的视觉分类器来学习GCN参数。在测试时，这些过滤器用于预测未见类别的视觉分类器。我们展示了我们的方法对KG中的噪声具有鲁棒性。更重要的是，与当前最先进的结果相比，我们的方法在性能上提供了显著的改进（在某些指标上从2%到3%，在少数指标上高达20%）。","领域":"零样本学习/知识图谱/图卷积网络","问题":"零样本识别问题，即学习一个没有训练样本的类别的视觉分类器","动机":"处理不熟悉或新类别的关键是将从熟悉类别获得的知识转移到描述不熟悉的类别","方法":"基于图卷积网络（GCN），使用语义嵌入和类别关系来预测分类器","关键词":["零样本学习","知识图谱","图卷积网络","语义嵌入","视觉分类器"],"涉及的技术概念":"零样本识别指的是在没有训练样本的情况下识别新类别的能力。语义嵌入是将词语或概念转换为向量空间中的点，以便于计算它们之间的相似度。知识图谱（KG）是一种结构化的知识表示方式，用于表示实体及其之间的关系。图卷积网络（GCN）是一种处理图结构数据的神经网络，能够利用节点之间的关系信息进行学习。"},{"order":710,"title":"Referring Relationships","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Krishna_Referring_Relationships_CVPR_2018_paper.html","abstract":"Images are not simply sets of objects: each image represents a web of interconnected relationships. These relationships between entities carry semantic meaning and help a viewer differentiate between instances of an entity. For example, in an image of a soccer match, there may be multiple persons present, but each participates in different relationships: one is kicking the ball, and the other is guarding the goal. In this paper, we formulate the task of utilizing these \\"referring relationships\\" to disambiguate between entities of the same category. We introduce an iterative model that localizes the two entities in the referring relationship, conditioned on one another. We formulate the cyclic condition between the entities in a relationship by modelling predicates that connect the entities as shifts in attention from one entity to another. We demonstrate that our model can not only outperform existing approaches on three datasets --- CLEVR, VRD and Visual Genome --- but also that it produces visually meaningful predicate shifts, as an instance of interpretable neural networks. Finally, we show that by modelling predicates as attention shifts, we can even localize entities in the absence of their category, allowing our model to find completely unseen categories.","中文标题":"指称关系","摘要翻译":"图像不仅仅是对象的集合：每张图像都代表了一个相互关联的关系网络。这些实体之间的关系承载着语义意义，并帮助观察者区分实体的实例。例如，在一张足球比赛的图像中，可能有多个人在场，但每个人都参与了不同的关系：一个在踢球，另一个在守门。在本文中，我们制定了利用这些“指称关系”来消除同一类别实体之间歧义的任务。我们引入了一个迭代模型，该模型在相互条件下定位指称关系中的两个实体。我们通过将连接实体的谓词建模为从一个实体到另一个实体的注意力转移，来制定关系中实体之间的循环条件。我们证明了我们的模型不仅可以在三个数据集——CLEVR、VRD和Visual Genome——上优于现有方法，而且还产生了视觉上有意义的谓词转移，作为可解释神经网络的一个实例。最后，我们展示了通过将谓词建模为注意力转移，我们甚至可以在没有实体类别的情况下定位实体，使我们的模型能够找到完全未见过的类别。","领域":"视觉关系理解/注意力机制/可解释性","问题":"消除同一类别实体之间的歧义","动机":"利用图像中实体之间的指称关系来帮助区分实体的实例","方法":"引入一个迭代模型，通过将连接实体的谓词建模为注意力转移，来定位指称关系中的两个实体","关键词":["视觉关系理解","注意力机制","可解释性"],"涉及的技术概念":"指称关系、注意力转移、可解释神经网络、实体定位"},{"order":711,"title":"Improving Object Localization With Fitness NMS and Bounded IoU Loss","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tychsen-Smith_Improving_Object_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tychsen-Smith_Improving_Object_Localization_CVPR_2018_paper.html","abstract":"We demonstrate that many detection methods are designed to identify only a sufficently accurate bounding box, rather than the best available one. To address this issue we propose a simple and fast modification to the existing methods called Fitness NMS. This method is tested with the DeNet model and obtains a significantly improved MAP at greater localization accuracies without a loss in evaluation rate, and can be used in conjunction with Soft NMS for additional improvements. Next we derive a novel bounding box regression loss based on a set of IoU upper bounds that better matches the goal of IoU maximization while still providing good convergence properties. Following these novelties we investigate RoI clustering schemes for improving evaluation rates for the DeNet wide model variants and provide an analysis of localization performance at various input image dimensions. We obtain a MAP of 33.6%@79Hz and 41.8%@5Hz for MSCOCO and a Titan X (Maxwell).","中文标题":"通过Fitness NMS和有界IoU损失改进目标定位","摘要翻译":"我们证明了许多检测方法设计用于识别仅足够准确的边界框，而不是最佳可用的边界框。为了解决这个问题，我们提出了一个简单快速的现有方法修改，称为Fitness NMS。该方法与DeNet模型一起测试，在更高的定位精度下获得了显著改进的MAP，而不会损失评估速率，并且可以与Soft NMS结合使用以获得额外的改进。接下来，我们基于一组IoU上限推导出一种新颖的边界框回归损失，该损失更好地匹配IoU最大化的目标，同时仍然提供良好的收敛性。在这些新颖性之后，我们研究了RoI聚类方案以提高DeNet宽模型变体的评估速率，并提供了在不同输入图像尺寸下的定位性能分析。我们在MSCOCO和Titan X（Maxwell）上获得了33.6%@79Hz和41.8%@5Hz的MAP。","领域":"目标检测/边界框回归/图像识别","问题":"现有目标检测方法仅识别足够准确的边界框，而非最佳可用边界框","动机":"提高目标检测的定位精度和评估速率","方法":"提出了Fitness NMS方法改进现有检测方法，并推导了一种基于IoU上限的边界框回归损失","关键词":["Fitness NMS","边界框回归","IoU损失","DeNet模型","RoI聚类"],"涉及的技术概念":{"Fitness NMS":"一种改进现有目标检测方法的简单快速修改，旨在提高定位精度","IoU上限":"用于推导边界框回归损失的一组上限，旨在更好地匹配IoU最大化的目标","DeNet模型":"用于测试Fitness NMS方法的模型","RoI聚类":"用于提高DeNet宽模型变体评估速率的技术","MAP":"平均精度均值，用于评估目标检测性能的指标"}},{"order":712,"title":"End-to-End Deep Kronecker-Product Matching for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.html","abstract":"Person re-identification aims to robustly measure similarities between person images. The significant variation of person poses and viewing angles challenges for accurate person re-identification. The spatial layout and correspondences between query person images are vital information for tackling this problem but are ignored by most state-of-the-art methods. In this paper, we propose a novel Kronecker Product Matching module to match feature maps of different persons in an end-to-end trainable deep neural network. A novel feature soft warping scheme is designed for aligning the feature maps based on matching results, which is shown to be crucial for achieving superior accuracy. The multi-scale features based on hourglass-like networks and self residual attention are also exploited to further boost the re-identification performance. The proposed approach outperforms state-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach.","中文标题":"端到端深度克罗内克积匹配用于行人重识别","摘要翻译":"行人重识别旨在稳健地测量行人图像之间的相似性。行人姿态和视角的显著变化对准确的行人重识别提出了挑战。查询行人图像之间的空间布局和对应关系是解决这一问题的关键信息，但大多数最先进的方法忽略了这一点。在本文中，我们提出了一种新颖的克罗内克积匹配模块，用于在端到端可训练的深度神经网络中匹配不同人的特征图。设计了一种新颖的特征软扭曲方案，用于根据匹配结果对齐特征图，这对于实现卓越的准确性至关重要。还利用了基于沙漏状网络的多尺度特征和自残差注意力，以进一步提高重识别性能。所提出的方法在Market-1501、CUHK03和DukeMTMC数据集上优于最先进的方法，证明了我们提出方法的有效性和泛化能力。","领域":"行人重识别/特征匹配/神经网络","问题":"行人重识别中由于姿态和视角变化导致的相似性测量不准确问题","动机":"解决行人重识别中因忽略查询行人图像之间的空间布局和对应关系而导致的性能瓶颈","方法":"提出克罗内克积匹配模块和特征软扭曲方案，结合多尺度特征和自残差注意力机制","关键词":["行人重识别","克罗内克积匹配","特征软扭曲","多尺度特征","自残差注意力"],"涉及的技术概念":{"克罗内克积匹配模块":"一种用于在深度神经网络中匹配不同人特征图的模块","特征软扭曲方案":"一种根据匹配结果对齐特征图的方法，以提高准确性","多尺度特征":"基于沙漏状网络提取的特征，用于捕捉不同尺度的信息","自残差注意力":"一种注意力机制，用于增强网络对重要特征的关注"}},{"order":713,"title":"Semantic Visual Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.html","abstract":"Robust visual localization under a wide range of viewing conditions is a fundamental problem in computer vision. Handling the difficult cases of this problem is not only very challenging but also of high practical relevance, e.g., in the context of life-long localization for augmented reality or autonomous robots. In this paper, we propose a novel approach based on a joint 3D geometric and semantic understanding of the world, enabling it to succeed under conditions where previous approaches failed. Our method leverages a novel generative model for descriptor learning, trained on semantic scene completion as an auxiliary task. The resulting 3D descriptors are robust to missing observations by encoding high-level 3D geometric and semantic information. Experiments on several challenging large-scale localization datasets demonstrate reliable localization under extreme viewpoint, illumination, and geometry changes.","中文标题":"语义视觉定位","摘要翻译":"在广泛的观察条件下实现鲁棒的视觉定位是计算机视觉中的一个基本问题。处理这个问题的困难案例不仅非常具有挑战性，而且具有很高的实际相关性，例如，在增强现实或自主机器人的终身定位背景下。在本文中，我们提出了一种基于对世界的联合3D几何和语义理解的新方法，使其在以前的方法失败的情况下能够成功。我们的方法利用了一种新颖的生成模型进行描述符学习，训练语义场景完成作为辅助任务。由此产生的3D描述符通过编码高级3D几何和语义信息，对缺失的观察具有鲁棒性。在几个具有挑战性的大规模定位数据集上的实验表明，在极端视角、光照和几何变化下实现了可靠的定位。","领域":"增强现实/自主机器人/3D重建","问题":"在极端视角、光照和几何变化下实现鲁棒的视觉定位","动机":"处理视觉定位中的困难案例，特别是在增强现实或自主机器人的终身定位背景下，具有很高的实际相关性","方法":"提出了一种基于对世界的联合3D几何和语义理解的新方法，利用新颖的生成模型进行描述符学习，训练语义场景完成作为辅助任务","关键词":["视觉定位","3D几何","语义理解","生成模型","描述符学习","语义场景完成"],"涉及的技术概念":{"3D几何":"指的是三维空间中的形状和结构","语义理解":"指的是对场景中对象和环境的含义和关系的理解","生成模型":"一种能够生成数据的统计模型，用于学习数据的分布","描述符学习":"通过学习生成能够描述图像或场景特征的向量或描述符","语义场景完成":"一种任务，旨在从部分观察中推断出完整的三维场景及其语义信息"}},{"order":714,"title":"Objects as Context for Detecting Their Semantic Parts","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gonzalez-Garcia_Objects_as_Context_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gonzalez-Garcia_Objects_as_Context_CVPR_2018_paper.html","abstract":"We present a semantic part detection approach that effectively leverages object information. We use the object appearance and its class as indicators of what parts to expect. We also model the expected relative location of parts inside the objects based on their appearance. We achieve this with a new network module, called OffsetNet, that efficiently predicts a variable number of part locations within a given object. Our model incorporates all these cues to detect parts in the context of their objects. This leads to considerably higher performance for the challenging task of part detection compared to using part appearance alone (+5 mAP on the PASCAL-Part dataset). We also compare to other part detection methods on both PASCAL-Part and CUB200-2011 datasets.","中文标题":"对象作为检测其语义部分的上下文","摘要翻译":"我们提出了一种语义部分检测方法，该方法有效地利用了对象信息。我们使用对象的外观及其类别作为预期部分的指示器。我们还基于部分的外观，对对象内部部分的预期相对位置进行建模。我们通过一个新的网络模块，称为OffsetNet，实现了这一点，该模块有效地预测给定对象内的可变数量的部分位置。我们的模型结合了所有这些线索，以在其对象的上下文中检测部分。与仅使用部分外观相比，这导致在部分检测这一具有挑战性的任务上性能显著提高（在PASCAL-Part数据集上提高了+5 mAP）。我们还在PASCAL-Part和CUB200-2011数据集上与其他部分检测方法进行了比较。","领域":"语义分割/对象检测/图像理解","问题":"如何在对象上下文中有效地检测其语义部分","动机":"提高语义部分检测的准确性和效率，特别是在复杂场景中识别对象的各个部分","方法":"提出了一种新的网络模块OffsetNet，该模块利用对象的外观和类别信息，以及部分在对象内部的预期相对位置，来预测和检测对象的语义部分","关键词":["语义部分检测","对象上下文","OffsetNet"],"涉及的技术概念":{"语义部分检测":"识别和定位图像中对象的特定部分，如动物的头部或汽车的轮子","对象上下文":"利用对象的整体信息（如类别和外观）来辅助理解其组成部分","OffsetNet":"一种新的网络模块，用于预测对象内部语义部分的位置，考虑了部分的外观和预期位置"}},{"order":715,"title":"End-to-End Weakly-Supervised Semantic Alignment","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rocco_End-to-End_Weakly-Supervised_Semantic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rocco_End-to-End_Weakly-Supervised_Semantic_CVPR_2018_paper.html","abstract":"We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence  aligning two images depicting objects of the same category. This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter.   We present the following three principal contributions.  First, we develop a convolutional neural network architecture for semantic alignment  that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs. The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time. Second, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter.  Third, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment.","中文标题":"端到端弱监督语义对齐","摘要翻译":"我们解决了语义对齐的任务，其目标是计算密集的语义对应，对齐描绘同一类别物体的两张图像。由于类内差异大、视角变化和背景杂乱，这是一项具有挑战性的任务。我们提出了以下三个主要贡献。首先，我们开发了一种卷积神经网络架构，用于语义对齐，该架构可以从匹配图像对形式的弱图像级监督中端到端地训练。结果是，参数是从不同但语义相关的图像中丰富的外观变化中学习的，而无需在训练时进行繁琐的手动对应注释。其次，该架构的主要组成部分是一个可微分的软内点评分模块，受RANSAC内点评分程序的启发，该模块仅基于几何一致的对应关系计算对齐质量，从而减少了背景杂乱的影响。第三，我们证明了所提出的方法在多个语义对齐标准基准上实现了最先进的性能。","领域":"语义对齐/卷积神经网络/图像匹配","问题":"计算密集的语义对应，对齐描绘同一类别物体的两张图像","动机":"由于类内差异大、视角变化和背景杂乱，语义对齐是一项具有挑战性的任务","方法":"开发了一种卷积神经网络架构，用于语义对齐，该架构可以从匹配图像对形式的弱图像级监督中端到端地训练，并引入了一个可微分的软内点评分模块来计算对齐质量","关键词":["语义对齐","卷积神经网络","图像匹配","弱监督学习","RANSAC"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","弱监督学习":"一种机器学习方法，其中训练数据的标签不完全或不精确","RANSAC":"随机样本一致性，一种用于估计数学模型参数的迭代方法，能够从包含大量异常值的数据集中估计出模型参数"}},{"order":716,"title":"Dynamic Zoom-In Network for Fast Object Detection in Large Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Dynamic_Zoom-In_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Dynamic_Zoom-In_Network_CVPR_2018_paper.html","abstract":"We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%.","中文标题":"动态放大网络用于大图像中的快速目标检测","摘要翻译":"我们引入了一个通用框架，该框架在保持目标检测精度的同时，减少了计算成本，适用于高分辨率图像中出现不同大小目标的场景。检测以从粗到细的方式进行，首先在图像的下采样版本上进行，然后在被识别为可能提高检测精度的高分辨率区域序列上进行。基于强化学习，我们的方法包括一个模型（R-net），它使用粗略的检测结果来预测分析更高分辨率区域的潜在精度增益，以及另一个模型（Q-net），它顺序选择区域进行放大。在Caltech Pedestrians数据集上的实验表明，我们的方法在不降低检测精度的情况下，减少了超过50%的像素处理量。在从YFCC100M数据集收集的高分辨率测试集上，我们的方法的优势更加显著，保持了高检测性能的同时，减少了约70%的像素处理量和超过50%的检测时间。","领域":"目标检测/强化学习/高分辨率图像处理","问题":"减少高分辨率图像中目标检测的计算成本，同时保持检测精度","动机":"为了在保持目标检测精度的同时，减少高分辨率图像中目标检测的计算成本","方法":"采用从粗到细的检测方式，结合强化学习，使用R-net预测分析更高分辨率区域的潜在精度增益，Q-net顺序选择区域进行放大","关键词":["目标检测","强化学习","高分辨率图像处理"],"涉及的技术概念":"R-net模型用于预测分析更高分辨率区域的潜在精度增益，Q-net模型用于顺序选择区域进行放大，强化学习用于优化区域选择策略，以减少计算成本并保持检测精度。"},{"order":717,"title":"Learning Markov Clustering Networks for Scene Text Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Markov_Clustering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Markov_Clustering_CVPR_2018_paper.html","abstract":"A novel framework named Markov Clustering Network (MCN) is proposed for fast and robust scene text detection. MCN predicts instance-level bounding boxes by firstly converting an image into a Stochastic Flow Graph (SFG) and then performing Markov Clustering on this graph. Our method can detect text objects with arbitrary size and orientation without prior knowledge of object size. The stochastic flow graph encode objects' local correlation and semantic information. An object is modeled as strongly connected nodes, which allows flexible bottom-up detection for scale-varying and rotated objects. MCN generates bounding boxes without using Non-Maximum Suppression, and it can be fully parallelized on GPUs. The evaluation on public benchmarks shows that our method outperforms the existing methods by a large margin in detecting multioriented text objects. MCN achieves new state-of-art performance on challenging MSRA-TD500 dataset with precision of 0.88, recall of 0.79 and F-score of 0.83. Also, MCN achieves realtime inference with frame rate of 34 FPS, which is $1.5\\times$ speedup when compared with the fastest scene text detection algorithm.","中文标题":"学习马尔可夫聚类网络用于场景文本检测","摘要翻译":"提出了一种名为马尔可夫聚类网络（MCN）的新框架，用于快速且鲁棒的场景文本检测。MCN通过首先将图像转换为随机流图（SFG），然后在该图上执行马尔可夫聚类来预测实例级别的边界框。我们的方法可以在不知道对象大小的情况下检测任意大小和方向的文本对象。随机流图编码了对象的局部相关性和语义信息。一个对象被建模为强连接的节点，这允许对尺度变化和旋转的对象进行灵活的底部向上检测。MCN生成边界框时不使用非最大抑制，并且可以在GPU上完全并行化。在公共基准上的评估显示，我们的方法在检测多方向文本对象方面大大优于现有方法。MCN在具有挑战性的MSRA-TD500数据集上实现了新的最先进性能，精度为0.88，召回率为0.79，F得分为0.83。此外，MCN实现了实时推理，帧率为34 FPS，与最快的场景文本检测算法相比，速度提高了1.5倍。","领域":"场景文本检测/随机流图/马尔可夫聚类","问题":"如何在不知道对象大小的情况下，快速且鲁棒地检测任意大小和方向的场景文本对象","动机":"提高场景文本检测的效率和准确性，特别是在处理多方向和尺度变化的文本对象时","方法":"提出马尔可夫聚类网络（MCN），通过将图像转换为随机流图（SFG）并执行马尔可夫聚类来预测实例级别的边界框，无需使用非最大抑制，并可在GPU上完全并行化","关键词":["场景文本检测","随机流图","马尔可夫聚类","实例级别边界框","多方向文本对象"],"涉及的技术概念":{"马尔可夫聚类网络（MCN）":"一种用于场景文本检测的新框架，通过马尔可夫聚类在随机流图上预测文本对象的边界框","随机流图（SFG）":"一种将图像转换为图结构的方法，用于编码对象的局部相关性和语义信息","非最大抑制":"一种常用于目标检测中的后处理步骤，用于消除重叠的边界框，但MCN无需此步骤即可生成边界框","GPU并行化":"利用图形处理单元（GPU）的并行计算能力，加速算法的执行速度"}},{"order":718,"title":"Deep Reinforcement Learning of Region Proposal Networks for Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.html","abstract":"We propose drl-RPN, a deep reinforcement learning-based visual recognition model consisting of a sequential region proposal network (RPN) and an object detector. In contrast to typical RPNs, where candidate object regions (RoIs) are selected greedily via class-agnostic NMS, drl-RPN optimizes an objective closer to the final detection task. This is achieved by replacing the greedy RoI selection process with a sequential attention mechanism which is trained via deep reinforcement learning (RL). Our model is capable of accumulating class-specific evidence over time, potentially affecting subsequent proposals and classification scores, and we show that such context integration significantly boosts detection accuracy. Moreover, drl-RPN automatically decides when to stop the search process and has the benefit of being able to jointly learn the parameters of the policy and the detector, both represented as deep networks. Our model can further learn to search over a wide range of exploration-accuracy trade-offs making it possible to specify or adapt the exploration extent at test time. The resulting search trajectories are image- and category-dependent, yet rely only on a single policy over all object categories. Results on the MS COCO and PASCAL VOC challenges show that our approach outperforms established, typical state-of-the-art object detection pipelines.","中文标题":"基于深度强化学习的区域提议网络用于目标检测","摘要翻译":"我们提出了drl-RPN，一个基于深度强化学习的视觉识别模型，由顺序区域提议网络（RPN）和目标检测器组成。与典型的RPN不同，在典型的RPN中，候选目标区域（RoIs）是通过类别无关的非最大抑制（NMS）贪婪选择的，而drl-RPN优化了一个更接近最终检测任务的目标。这是通过用顺序注意力机制替换贪婪的RoI选择过程来实现的，该机制通过深度强化学习（RL）进行训练。我们的模型能够随时间积累类别特定的证据，可能影响后续的提议和分类分数，我们展示了这种上下文集成显著提高了检测准确性。此外，drl-RPN自动决定何时停止搜索过程，并具有能够联合学习策略和检测器参数的优势，两者都表示为深度网络。我们的模型可以进一步学习在广泛的探索-准确性权衡范围内搜索，使得在测试时指定或适应探索范围成为可能。由此产生的搜索轨迹是图像和类别依赖的，但仅依赖于所有对象类别上的单一策略。在MS COCO和PASCAL VOC挑战上的结果表明，我们的方法优于已建立的、典型的先进目标检测流程。","领域":"目标检测/强化学习/视觉识别","问题":"如何提高目标检测的准确性和效率","动机":"传统的区域提议网络（RPN）在候选目标区域选择上采用贪婪策略，可能无法充分优化最终检测任务的目标","方法":"提出了一种基于深度强化学习的视觉识别模型drl-RPN，通过顺序注意力机制替换贪婪的RoI选择过程，并联合学习策略和检测器参数","关键词":["目标检测","强化学习","视觉识别","顺序注意力机制","深度网络"],"涉及的技术概念":"深度强化学习（DRL）、区域提议网络（RPN）、非最大抑制（NMS）、顺序注意力机制、目标检测器、MS COCO、PASCAL VOC"},{"order":719,"title":"Beyond Holistic Object Recognition: Enriching Image Understanding With Part States","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lu_Beyond_Holistic_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Beyond_Holistic_Object_CVPR_2018_paper.html","abstract":"Important high-level vision tasks require rich semantic descriptions of objects at part level. Based upon previous work on part localization, in this paper, we address the problem of inferring rich semantics imparted by an object part in still images. Specifically, we propose to tokenize the semantic space as a discrete set of part states. Our modeling of part state is spatially localized, therefore, we formulate the part state inference problem as a pixel-wise annotation problem. An iterative part-state inference neural network that is efficient in time and accurate in performance is specifically designed for this task. Extensive experiments demonstrate that the proposed method can effectively predict the semantic states of parts and simultaneously improve part segmentation, thus benefiting a number of visual understanding applications. The other contribution of this paper is our part state dataset which contains rich part-level semantic annotations.","中文标题":"超越整体物体识别：通过部件状态丰富图像理解","摘要翻译":"重要的高级视觉任务需要在部件级别对物体进行丰富的语义描述。基于之前关于部件定位的工作，本文中我们解决了在静态图像中推断由物体部件赋予的丰富语义的问题。具体来说，我们提出将语义空间标记为一组离散的部件状态。我们的部件状态建模是空间局部化的，因此，我们将部件状态推断问题表述为像素级注释问题。为此任务专门设计了一个在时间上高效、在性能上准确的迭代部件状态推断神经网络。大量实验证明，所提出的方法能有效预测部件的语义状态，同时改进部件分割，从而有益于多种视觉理解应用。本文的另一贡献是我们的部件状态数据集，该数据集包含了丰富的部件级语义注释。","领域":"语义分割/物体识别/图像理解","问题":"在静态图像中推断由物体部件赋予的丰富语义","动机":"为了在部件级别对物体进行丰富的语义描述，以支持高级视觉任务","方法":"提出将语义空间标记为一组离散的部件状态，并设计了一个迭代部件状态推断神经网络","关键词":["语义分割","物体识别","图像理解","部件状态","神经网络"],"涉及的技术概念":"部件状态推断、像素级注释、迭代神经网络、语义分割、部件级语义注释"},{"order":720,"title":"Discriminability Objective for Training Descriptive Captions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Discriminability_Objective_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Discriminability_Objective_for_CVPR_2018_paper.html","abstract":"One property that remains lacking in image captions generated by contemporary methods is discriminability: being able to tell two images apart given the caption for one of them. We propose a way to improve this aspect of caption generation. By incorporating into the captioning training objective a loss component directly related to ability (by a machine) to disambiguate image/caption matches, we obtain systems that produce much more discriminative caption, according to human evaluation. Remarkably, our approach leads to improvement in other aspects of generated captions, reflected by a battery of standard scores such as BLEU, SPICE etc. Our approach is modular and can be applied to a variety of model/loss combinations commonly proposed for image captioning.","中文标题":"训练描述性字幕的区分性目标","摘要翻译":"当代方法生成的图像字幕中仍缺乏的一个属性是区分性：即给定一个图像的字幕，能够区分出两个不同的图像。我们提出了一种改进字幕生成这方面的方法。通过在字幕训练目标中加入一个与机器区分图像/字幕匹配能力直接相关的损失组件，我们获得了能够生成更具区分性字幕的系统，这一点得到了人类评估的证实。值得注意的是，我们的方法还改善了生成字幕的其他方面，这反映在一系列标准评分上，如BLEU、SPICE等。我们的方法是模块化的，可以应用于图像字幕生成中常见的各种模型/损失组合。","领域":"图像字幕生成/自然语言处理/计算机视觉","问题":"提高图像字幕的区分性，使其能够更好地区分不同的图像","动机":"当代图像字幕生成方法在区分性方面表现不足，需要改进以生成更具区分性的字幕","方法":"在字幕训练目标中加入与区分图像/字幕匹配能力直接相关的损失组件","关键词":["图像字幕生成","区分性","损失组件"],"涉及的技术概念":{"区分性":"指字幕能够区分不同图像的能力","损失组件":"在训练过程中用于衡量模型性能的指标，这里特指与区分图像/字幕匹配能力相关的部分","BLEU":"一种用于评估机器翻译质量的指标，通过比较机器翻译结果与参考翻译之间的相似度来评分","SPICE":"一种用于评估图像字幕质量的指标，通过分析字幕中的语义内容与图像内容的匹配程度来评分"}},{"order":721,"title":"Visual Question Answering With Memory-Augmented Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Visual_Question_Answering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ma_Visual_Question_Answering_CVPR_2018_paper.html","abstract":"In this paper, we exploit memory-augmented neural networks to predict accurate answers to visual questions, even when those answers rarely occur in the training set. The memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. We show that memory-augmented neural networks are able to maintain a relatively long-term memory of scarce training exemplars, which is important for visual question answering due to the heavy-tailed distribution of answers in a general VQA setting. Experimental results in two large-scale benchmark datasets show the favorable performance of the proposed algorithm with the comparison to state of the art.","中文标题":"使用记忆增强网络进行视觉问答","摘要翻译":"在本文中，我们利用记忆增强神经网络来预测视觉问题的准确答案，即使这些答案在训练集中很少出现。记忆网络结合了内部和外部记忆块，并选择性地关注每个训练样本。我们展示了记忆增强神经网络能够保持对稀缺训练样本的相对长期记忆，这对于视觉问答来说非常重要，因为在一般的VQA设置中，答案的分布是重尾的。在两个大规模基准数据集上的实验结果表明，与现有技术相比，所提出的算法具有优越的性能。","领域":"视觉问答/记忆增强网络/神经网络","问题":"解决视觉问答中答案在训练集中罕见出现的问题","动机":"由于在一般的视觉问答设置中，答案的分布是重尾的，因此需要一种能够有效记忆和处理罕见答案的方法。","方法":"采用记忆增强神经网络，该网络结合了内部和外部记忆块，并选择性地关注每个训练样本，以保持对稀缺训练样本的长期记忆。","关键词":["视觉问答","记忆增强网络","神经网络","重尾分布"],"涉及的技术概念":"记忆增强神经网络是一种结合了内部和外部记忆块的神经网络，能够选择性地关注每个训练样本，从而保持对稀缺训练样本的长期记忆。重尾分布指的是在数据集中，少数几个值占据了大部分的频率，而大多数值出现的频率很低。"},{"order":722,"title":"Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Structure_Inference_Net_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Structure_Inference_Net_CVPR_2018_paper.html","abstract":"Context is important for accurate visual recognition. In this work we propose an object detection algorithm that not only considers object visual appearance, but also makes use of two kinds of context including scene contextual information and object relationships within a single image. Therefore, object detection is regarded as both a cognition problem and a reasoning problem when leveraging these structured information. Specifically, this paper formulates object detection as a problem of graph structure inference, where given an image the objects are treated as nodes in a graph and relationships between the objects are modeled as edges in such graph. To this end, we present a so-called Structure Inference Network (SIN), a detector that incorporates into a typical detection framework (e.g. Faster R-CNN) with a graphical model which aims to infer object state. Comprehensive experiments on PASCAL VOC and MS COCO datasets indicate that scene context and object relationships truly improve the performance of object detection with more desirable and reasonable outputs.","中文标题":"结构推理网络：使用场景级上下文和实例级关系的对象检测","摘要翻译":"上下文对于准确的视觉识别非常重要。在这项工作中，我们提出了一种对象检测算法，该算法不仅考虑对象的视觉外观，还利用了包括场景上下文信息和单个图像内对象关系在内的两种上下文。因此，当利用这些结构化信息时，对象检测被视为既是认知问题也是推理问题。具体来说，本文将对象检测问题表述为图结构推理问题，其中给定图像中的对象被视为图中的节点，对象之间的关系被建模为图中的边。为此，我们提出了一种所谓的结构推理网络（SIN），这是一种将图形模型整合到典型检测框架（例如Faster R-CNN）中的检测器，旨在推断对象状态。在PASCAL VOC和MS COCO数据集上的综合实验表明，场景上下文和对象关系确实提高了对象检测的性能，并产生了更理想和合理的输出。","领域":"对象检测/场景理解/图结构推理","问题":"提高对象检测的准确性和合理性","动机":"利用场景上下文信息和对象关系来增强对象检测的性能","方法":"提出结构推理网络（SIN），将图形模型整合到典型检测框架中，以推断对象状态","关键词":["对象检测","场景上下文","对象关系","图结构推理"],"涉及的技术概念":"结构推理网络（SIN）是一种结合了图形模型的对象检测框架，旨在通过利用场景上下文信息和对象之间的关系来提高检测的准确性和合理性。这种方法将对象检测问题视为图结构推理问题，其中对象是图中的节点，对象之间的关系是图中的边。通过在PASCAL VOC和MS COCO数据集上的实验，证明了这种方法的有效性。"},{"order":723,"title":"Occluded Pedestrian Detection Through Guided Attention in CNNs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Occluded_Pedestrian_Detection_CVPR_2018_paper.html","abstract":"Pedestrian detection has progressed significantly in the last years. However, occluded people are notoriously hard to detect, as their appearance varies substantially depending on a wide range of partial occlusions. In this paper, we aim to propose a simple and compact method based on the FasterRCNN architecture for occluded pedestrian detection.  We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively. These findings strongly motivate us to employ an attention mechanism across channels to represent various occlusion patterns in one single model, as each occlusion pattern can be formulated as some specific combination of body parts. Therefore, an attention network with self or external guidances is proposed as an add-on to the baseline FasterRCNN detector. When evaluating on the heavy occlusion subset, we achieve a significant improvement of 8pp to the baseline FasterRCNN detector on CityPersons and on Caltech we outperform the state-of-the-art method by 4pp.","中文标题":"通过CNNs中的引导注意力进行遮挡行人检测","摘要翻译":"近年来，行人检测取得了显著进展。然而，被遮挡的行人由于外观因各种部分遮挡而有很大变化，因此极难检测。在本文中，我们旨在提出一种基于FasterRCNN架构的简单紧凑的方法，用于遮挡行人检测。我们首先解释行人检测器的CNN通道特征，发现不同通道分别激活不同身体部位的响应。这些发现强烈激励我们采用跨通道的注意力机制，以在单一模型中表示各种遮挡模式，因为每种遮挡模式都可以表述为某些特定身体部位的组合。因此，我们提出了一种带有自我或外部引导的注意力网络，作为基线FasterRCNN检测器的附加组件。在重度遮挡子集上评估时，我们在CityPersons上比基线FasterRCNN检测器显著提高了8个百分点，在Caltech上我们比最先进的方法高出4个百分点。","领域":"行人检测/遮挡处理/注意力机制","问题":"解决遮挡行人的检测问题","动机":"由于被遮挡行人的外观因遮挡而有很大变化，导致检测困难，因此需要一种方法来提高遮挡行人的检测准确率。","方法":"提出了一种基于FasterRCNN架构的简单紧凑的方法，通过解释CNN通道特征并采用跨通道的注意力机制来检测遮挡行人。","关键词":["行人检测","遮挡处理","注意力机制","FasterRCNN"],"涉及的技术概念":"CNN通道特征解释、跨通道注意力机制、FasterRCNN架构、自我或外部引导的注意力网络"},{"order":724,"title":"Reward Learning From Narrated Demonstrations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tung_Reward_Learning_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_Reward_Learning_From_CVPR_2018_paper.html","abstract":"Humans effortlessly “program” one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved [5], by providing RGB images of goal configurations [19], or supplying a demonstration to be imitated [17]. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations (NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers’ descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation. We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and-place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.","中文标题":"从叙述演示中学习奖励","摘要翻译":"人类通过自然语言交流目标和愿望，毫不费力地“编程”彼此。相比之下，人类通过指示要达到的物体位置和姿态[5]、提供目标配置的RGB图像[19]或提供要模仿的演示[17]来编程机器人行为。这些方法都无法在环境变化中泛化，并且它们以笨拙的技术术语传达目标。这项工作提出了自然语言基础和可指导行为策略的联合学习，这些策略由自然语言表达的感知检测器加强，这些检测器基于机器人代理的感官输入。我们的监督是叙述视觉演示（NVD），即与口头叙述配对的视觉演示（与无声演示相对）。我们引入了一个NVD数据集，其中教师在执行活动时详细描述它们。我们将教师的描述映射到感知奖励检测器，并使用它们在模拟中训练相应的行为策略。我们经验性地展示了我们的可指导代理（i）通过利用演示动态中的硬负挖掘配置，使用少量示例学习视觉奖励检测器，（ii）使用学习的视觉奖励检测器开发拾取和放置策略，（iii）受益于模仿自然语言目标表达句法结构的对象分解状态表示，以及（iv）可以在测试时执行涉及新位置中新对象的行为，由自然语言指导。","领域":"机器人学习/自然语言处理/视觉感知","问题":"如何使机器人通过自然语言理解和执行任务","动机":"现有方法在环境变化中泛化能力差，且以技术术语传达目标，不够直观","方法":"提出了一种联合学习自然语言基础和可指导行为策略的方法，利用叙述视觉演示（NVD）进行监督，通过将教师的描述映射到感知奖励检测器来训练行为策略","关键词":["自然语言基础","行为策略","视觉奖励检测器","对象分解状态表示"],"涉及的技术概念":"叙述视觉演示（NVD）是指与口头叙述配对的视觉演示，用于训练机器人理解和执行任务。感知奖励检测器是基于机器人感官输入的自然语言表达检测器，用于指导机器人行为。对象分解状态表示是一种模仿自然语言目标表达句法结构的状态表示方法，有助于机器人理解和执行复杂任务。"},{"order":725,"title":"Weakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Weakly-Supervised_Semantic_Segmentation_CVPR_2018_paper.html","abstract":"This paper studies the problem of learning image semantic segmentation networks only using image-level labels as supervision, which is important since it can significantly reduce human annotation efforts. Recent state-of-the-art methods on this problem first infer the sparse and discriminative regions for each object class using a deep classification network, then train semantic a segmentation network using the discriminative regions as supervision. Inspired by the traditional image segmentation methods of seeded region growing, we propose to train a semantic segmentation network starting from the discriminative regions and progressively increase the pixel-level supervision using by seeded region growing. The seeded region growing module is integrated in a deep segmentation network and can benefit from deep features. Different from conventional deep networks which have fixed/static labels, the proposed weakly-supervised network generates new labels using the contextual information within an image. The proposed method significantly outperforms the weakly-supervised semantic segmentation methods using static labels, and obtains the state-of-the-art performance, which are 63.2% mIoU score on the PASCAL VOC 2012 test set and 26.0% mIoU score on the COCO dataset.","中文标题":"基于深度种子区域增长的弱监督语义分割网络","摘要翻译":"本文研究了仅使用图像级标签作为监督学习图像语义分割网络的问题，这很重要，因为它可以显著减少人工标注的工作量。关于这个问题的最新最先进方法首先使用深度分类网络推断每个对象类的稀疏和判别区域，然后使用判别区域作为监督训练语义分割网络。受传统图像分割方法中种子区域增长的启发，我们提出从判别区域开始训练语义分割网络，并通过种子区域增长逐步增加像素级监督。种子区域增长模块被集成到深度分割网络中，并可以从深度特征中受益。与具有固定/静态标签的传统深度网络不同，所提出的弱监督网络利用图像内的上下文信息生成新标签。所提出的方法显著优于使用静态标签的弱监督语义分割方法，并获得了最先进的性能，在PASCAL VOC 2012测试集上的mIoU得分为63.2%，在COCO数据集上的mIoU得分为26.0%。","领域":"语义分割/弱监督学习/图像分析","问题":"仅使用图像级标签进行图像语义分割","动机":"减少人工标注的工作量，提高语义分割的效率和准确性","方法":"提出了一种基于深度种子区域增长的弱监督语义分割网络，通过从判别区域开始训练，并逐步增加像素级监督","关键词":["语义分割","弱监督学习","种子区域增长","深度特征","上下文信息"],"涉及的技术概念":{"图像级标签":"用于训练的图像级别标注，而非像素级别","深度分类网络":"用于图像分类的深度神经网络","判别区域":"图像中能够区分不同类别的区域","种子区域增长":"一种图像分割技术，从种子点开始，根据相似性准则逐步扩展区域","mIoU":"平均交并比，用于评估语义分割模型性能的指标"}},{"order":726,"title":"PoTion: Pose MoTion Representation for Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html","abstract":"Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition. We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by colorizing each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets.","中文标题":"PoTion：用于动作识别的姿态运动表示","摘要翻译":"大多数最先进的动作识别方法依赖于处理外观和运动独立的两流架构。在本文中，我们声称将它们联合考虑为动作识别提供了丰富的信息。我们引入了一种新颖的表示方法，优雅地编码了一些语义关键点的运动。我们使用人体关节作为这些关键点，并将我们的姿态运动表示称为PoTion。具体来说，我们首先运行一个最先进的人体姿态估计器，并在每一帧中提取人体关节的热图。我们通过时间上聚合这些概率图来获得我们的PoTion表示。这是通过根据视频剪辑中帧的相对时间为每个热图着色并将它们相加来实现的。这种整个视频剪辑的固定大小表示适合使用浅层卷积神经网络对动作进行分类。我们的实验评估显示，PoTion优于其他最先进的姿态表示。此外，它与标准的外观和运动流互补。当将PoTion与最近的两流I3D方法[5]结合时，我们在JHMDB、HMDB和UCF101数据集上获得了最先进的性能。","领域":"动作识别/姿态估计/视频分析","问题":"如何有效地结合外观和运动信息以提高动作识别的准确性","动机":"现有的动作识别方法大多独立处理外观和运动信息，忽略了它们联合考虑可能带来的丰富信息","方法":"提出了一种新颖的姿态运动表示方法PoTion，通过时间上聚合人体关节的热图来编码动作，并结合浅层卷积神经网络进行分类","关键词":["动作识别","姿态估计","视频分析"],"涉及的技术概念":"两流架构、人体姿态估计器、热图、时间聚合、卷积神经网络、I3D方法"},{"order":727,"title":"Bilateral Ordinal Relevance Multi-Instance Regression for Facial Action Unit Intensity Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Bilateral_Ordinal_Relevance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Bilateral_Ordinal_Relevance_CVPR_2018_paper.html","abstract":"Automatic intensity estimation of facial action units (AUs) is challenging in two aspects. First, capturing subtle changes of facial appearance is quiet difficult. Second, the annotation of AU intensity is scarce and expensive. Intensity annotation requires strong domain knowledge thus only experts are qualified. The majority of methods directly apply supervised learning techniques to AU intensity estimation while few methods exploit unlabeled samples to improve the performance. In this paper, we propose a novel weakly supervised regression model-Bilateral Ordinal Relevance Multi-instance Regression (BORMIR), which learns a frame-level intensity estimator with weakly labeled sequences. From a new perspective, we introduce relevance to model sequential data and consider two bag labels for each bag. The AU intensity estimation is formulated as a joint regressor and relevance learning problem. Temporal dynamics of both relevance and AU intensity are leveraged to build connections among labeled and unlabeled image frames to provide weak supervision. We also develop an efficient algorithm for optimization based on the alternating minimization framework. Evaluations on three expression databases demonstrate the effectiveness of the proposed model.","中文标题":"双边序数相关性多实例回归用于面部动作单元强度估计","摘要翻译":"面部动作单元（AUs）的自动强度估计在两个方面具有挑战性。首先，捕捉面部外观的微妙变化非常困难。其次，AU强度的注释稀缺且昂贵。强度注释需要强大的领域知识，因此只有专家才有资格。大多数方法直接将监督学习技术应用于AU强度估计，而少数方法利用未标记样本来提高性能。在本文中，我们提出了一种新颖的弱监督回归模型——双边序数相关性多实例回归（BORMIR），该模型通过弱标记序列学习帧级强度估计器。从一个新的视角，我们引入相关性来建模序列数据，并为每个包考虑两个包标签。AU强度估计被表述为一个联合回归器和相关性学习问题。利用相关性和AU强度的时间动态性在标记和未标记的图像帧之间建立连接，以提供弱监督。我们还开发了一种基于交替最小化框架的高效优化算法。在三个表情数据库上的评估证明了所提出模型的有效性。","领域":"面部表情分析/情感计算/生物特征识别","问题":"面部动作单元（AUs）强度的自动估计","动机":"捕捉面部外观的微妙变化困难，且AU强度的注释稀缺且昂贵，需要专家进行","方法":"提出了一种新颖的弱监督回归模型——双边序数相关性多实例回归（BORMIR），通过弱标记序列学习帧级强度估计器，并利用相关性和AU强度的时间动态性在标记和未标记的图像帧之间建立连接","关键词":["面部动作单元","强度估计","弱监督学习","相关性建模","时间动态性"],"涉及的技术概念":"双边序数相关性多实例回归（BORMIR）是一种弱监督回归模型，用于从弱标记序列中学习帧级强度估计器。该方法通过引入相关性来建模序列数据，并考虑每个包的两个包标签，将AU强度估计问题表述为联合回归器和相关性学习问题。此外，该方法还利用了相关性和AU强度的时间动态性，以在标记和未标记的图像帧之间建立连接，从而提供弱监督。"},{"order":728,"title":"Pulling Actions out of Context: Explicit Separation for Effective Combination","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Pulling_Actions_out_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Pulling_Actions_out_CVPR_2018_paper.html","abstract":"The ability to recognize human actions in video has many potential applications. Human action recognition, however, is tremendously challenging for computers due to the complexity of video data and the subtlety of human actions. Most current recognition systems flounder on the inability to separate human actions from co-occurring factors that usually dominate subtle human actions.   In this paper, we propose a novel approach for training a human action recognizer, one that can: (1) explicitly factorize human actions from the co-occurring factors; (2) deliberately build a model for human actions and a separate model for all correlated contextual elements; and (3) effectively combine the models for human action recognition. Our approach exploits the benefits of conjugate samples of human actions, which are video clips that are contextually similar to human action samples, but do not contain the action. Experiments on ActionThread, PASCAL VOC, UCF101, and Hollywood2 datasets demonstrate the ability to separate action from context of the proposed approach.","中文标题":"从上下文中提取动作：显式分离以实现有效组合","摘要翻译":"识别视频中的人类动作具有许多潜在应用。然而，由于视频数据的复杂性和人类动作的微妙性，人类动作识别对计算机来说是一个巨大的挑战。当前大多数识别系统因无法将人类动作与通常主导微妙人类动作的共现因素分离而陷入困境。在本文中，我们提出了一种新颖的方法来训练人类动作识别器，该方法能够：（1）显式地将人类动作从共现因素中分解出来；（2）特意为人类动作建立一个模型，并为所有相关的上下文元素建立一个单独的模型；（3）有效地结合这些模型进行人类动作识别。我们的方法利用了人类动作的共轭样本的优势，这些样本是与人类动作样本在上下文上相似但不包含该动作的视频片段。在ActionThread、PASCAL VOC、UCF101和Hollywood2数据集上的实验证明了所提出方法能够将动作从上下文中分离出来。","领域":"视频分析/动作识别/上下文理解","问题":"如何有效地从视频中识别出人类动作，同时将动作与共现的上下文因素分离","动机":"由于视频数据的复杂性和人类动作的微妙性，当前的人类动作识别系统难以将动作与共现因素分离，这限制了识别系统的性能和应用范围","方法":"提出了一种新颖的方法，通过显式地分解人类动作与共现因素，建立独立的模型分别处理人类动作和上下文元素，并有效地结合这些模型进行动作识别","关键词":["动作识别","上下文分离","共轭样本"],"涉及的技术概念":"共轭样本指的是在上下文上与人类动作样本相似但不包含该动作的视频片段，这种方法利用共轭样本来提高动作与上下文分离的效果"},{"order":729,"title":"Dynamic Feature Learning for Partial Face Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_Dynamic_Feature_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Dynamic_Feature_Learning_CVPR_2018_paper.html","abstract":"Partial face recognition (PFR) in unconstrained environment is a very important task, especially in video surveillance, mobile devices, etc. However, a few studies have tackled how to recognize an arbitrary patch of a face image. This study combines Fully Convolutional Network (FCN) with Sparse Representation Classification (SRC) to propose a novel partial face recognition approach, called Dynamic Feature Matching (DFM), to address partial face images regardless of sizes. Based on DFM, we propose a sliding loss to optimize FCN by reducing the intra-variation between a face patch and face images of a subject, which further improves the performance of DFM. The proposed DFM is evaluated on several partial face databases, including LFW, YTF and CASIA-NIR-Distance databases. Experimental results demonstrate the effectiveness and advantages of DFM in comparison with state-of-the-art PFR methods.","中文标题":"动态特征学习用于部分人脸识别","摘要翻译":"在无约束环境下的部分人脸识别（PFR）是一项非常重要的任务，特别是在视频监控、移动设备等领域。然而，很少有研究探讨如何识别人脸图像的任意部分。本研究将全卷积网络（FCN）与稀疏表示分类（SRC）结合，提出了一种新的部分人脸识别方法，称为动态特征匹配（DFM），以解决不同大小的部分人脸图像识别问题。基于DFM，我们提出了一种滑动损失来优化FCN，通过减少人脸部分与主体人脸图像之间的内部变化，进一步提高了DFM的性能。所提出的DFM在多个部分人脸数据库上进行了评估，包括LFW、YTF和CASIA-NIR-Distance数据库。实验结果证明了DFM与最先进的PFR方法相比的有效性和优势。","领域":"人脸识别/视频监控/移动设备","问题":"在无约束环境下识别任意部分的人脸图像","动机":"提高在视频监控、移动设备等场景下部分人脸识别的准确性和效率","方法":"结合全卷积网络（FCN）与稀疏表示分类（SRC），提出动态特征匹配（DFM）方法，并通过滑动损失优化FCN","关键词":["部分人脸识别","全卷积网络","稀疏表示分类","动态特征匹配"],"涉及的技术概念":{"全卷积网络（FCN）":"一种用于图像分割的深度学习模型，能够接受任意大小的输入图像并输出相应大小的分割图。","稀疏表示分类（SRC）":"一种基于稀疏表示的分类方法，通过寻找最能代表测试样本的稀疏线性组合来进行分类。","动态特征匹配（DFM）":"本研究提出的一种新的部分人脸识别方法，旨在解决不同大小的部分人脸图像识别问题。","滑动损失":"一种用于优化全卷积网络的损失函数，通过减少人脸部分与主体人脸图像之间的内部变化来提高识别性能。"}},{"order":730,"title":"Exploiting Transitivity for Learning Person Re-Identification Models on a Budget","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.html","abstract":"Minimization of labeling effort for person re-identification in camera networks is an important problem as most of the existing popular methods are supervised and they require large amount of manual annotations, acquiring which is a tedious job. In this work, we focus on this labeling effort minimization problem and approach it as a subset selection task where the objective is to select an optimal subset of image-pairs for labeling without compromising performance.  Towards this goal, our proposed scheme first represents any camera network (with k number of cameras) as an edge weighted complete k-partite graph where each vertex denotes a person and similarity scores between persons are used as edge-weights. Then in the second stage, our algorithm selects an optimal subset of pairs by solving a triangle free subgraph maximization problem on the k-partite graph. This sub-graph weight maximization problem is NP-hard (at least for k > = 4) which means for large datasets the optimization problem becomes intractable. In order to make our framework scalable, we propose two polynomial time approximately-optimal algorithms. The first algorithm is a 1/2-approximation algorithm which runs in linear time in the number of edges. The second algorithm is a greedy algorithm with sub-quadratic (in number of edges) time-complexity.  Experiments on three state-of-the-art datasets depict that the proposed approach requires on an average only  8-15 % manually labeled pairs in order to achieve the performance when all the pairs are manually annotated.","中文标题":"利用传递性在有限预算下学习行人重识别模型","摘要翻译":"在摄像机网络中最小化行人重识别的标注努力是一个重要问题，因为大多数现有的流行方法都是监督学习，它们需要大量的手动标注，而获取这些标注是一项繁琐的工作。在这项工作中，我们专注于这一标注努力最小化问题，并将其视为一个子集选择任务，目标是选择一个最优的图像对子集进行标注，而不影响性能。为实现这一目标，我们提出的方案首先将任何摄像机网络（具有k个摄像机）表示为一个边加权的完全k部图，其中每个顶点表示一个人，人与人之间的相似度分数用作边权重。然后在第二阶段，我们的算法通过在k部图上解决一个无三角形子图最大化问题来选择最优的对子集。这个子图权重最大化问题是NP难的（至少对于k >= 4），这意味着对于大数据集，优化问题变得难以处理。为了使我们的框架可扩展，我们提出了两种多项式时间的近似最优算法。第一种算法是一个1/2近似算法，它在边数上线性时间运行。第二种算法是一个贪心算法，具有次二次（在边数上）时间复杂度。在三个最先进的数据集上的实验表明，所提出的方法平均只需要8-15%的手动标注对，就能达到所有对都手动标注时的性能。","领域":"行人重识别/图论/优化算法","问题":"在摄像机网络中最小化行人重识别的标注努力","动机":"减少手动标注的繁琐工作，同时不降低行人重识别的性能","方法":"将摄像机网络表示为边加权的完全k部图，通过解决无三角形子图最大化问题选择最优标注对子集，并提出两种多项式时间的近似最优算法以提高框架的可扩展性","关键词":["行人重识别","图论","优化算法"],"涉及的技术概念":"完全k部图、无三角形子图最大化问题、NP难问题、1/2近似算法、贪心算法"},{"order":731,"title":"Deep Spatial Feature Reconstruction for Partial Person Re-Identification: Alignment-Free Approach","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/He_Deep_Spatial_Feature_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Deep_Spatial_Feature_CVPR_2018_paper.html","abstract":"Partial person re-identification (re-id) is a challenging problem, where only a partial observation of a person image is available for matching. However, few studies have offered a solution of how to identify an arbitrary patch of a person image. In this paper, we propose a fast and accurate matching method to address this problem. The proposed method leverages Fully Convolutional Network (FCN) to generate correspondingly-size spatial feature maps such that pixel-level features are consistent. To match a pair of person images of different sizes, a novel method called Deep Spatial feature Reconstruction (DSR) is further developed to avoid explicit alignment. Specifically, we exploit the reconstructing error from dictionary learning to calculate the similarity between different spatial feature maps. In that way, we expect that the proposed FCN can decrease the similarity of coupled images from different persons and vice versa. Experimental results on two partial person datasets demonstrate the efficiency and effectiveness of the proposed method in comparison with several state-of-the-art partial person re-id approaches.","中文标题":"深度空间特征重建用于部分行人重识别：无需对齐的方法","摘要翻译":"部分行人重识别（re-id）是一个具有挑战性的问题，其中只有行人图像的部分观察可用于匹配。然而，很少有研究提供了如何识别行人图像任意补丁的解决方案。在本文中，我们提出了一种快速准确的匹配方法来解决这个问题。所提出的方法利用全卷积网络（FCN）生成相应大小的空间特征图，使得像素级特征保持一致。为了匹配不同大小的行人图像对，进一步开发了一种称为深度空间特征重建（DSR）的新方法，以避免显式对齐。具体来说，我们利用字典学习中的重建误差来计算不同空间特征图之间的相似性。通过这种方式，我们期望所提出的FCN能够减少来自不同人的耦合图像的相似性，反之亦然。在两个部分行人数据集上的实验结果证明了所提出方法在效率上与几种最先进的部分行人重识别方法相比的有效性。","领域":"行人重识别/特征匹配/图像重建","问题":"解决部分行人图像匹配的问题","动机":"现有研究很少提供如何识别行人图像任意补丁的解决方案","方法":"利用全卷积网络（FCN）生成空间特征图，并通过深度空间特征重建（DSR）方法避免显式对齐，利用字典学习中的重建误差计算相似性","关键词":["行人重识别","特征匹配","图像重建"],"涉及的技术概念":"全卷积网络（FCN）用于生成空间特征图，深度空间特征重建（DSR）方法用于避免显式对齐，字典学习中的重建误差用于计算相似性"},{"order":732,"title":"Every Smile Is Unique: Landmark-Guided Diverse Smile Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Every_Smile_Is_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Every_Smile_Is_CVPR_2018_paper.html","abstract":"Each smile is unique: one person surely smiles in different ways (e.g., closing/opening the eyes or mouth). Given one input image of a neutral face, can we generate multiple smile videos with distinctive characteristics? To tackle this one-to-many video generation problem, we propose a novel deep learning architecture named Conditional Multi-Mode Network (CMM-Net). To better encode the dynamics of facial expressions, CMM-Net explicitly exploits facial landmarks for generating smile sequences. Specifically, a variational auto-encoder is used to learn a facial landmark embedding. This single embedding is then exploited by a conditional recurrent network which generates a landmark embedding sequence conditioned on a specific expression (e.g., spontaneous smile). Next, the generated landmark embeddings are fed into a multi-mode recurrent landmark generator, producing a set of landmark sequences still associated to the given smile class but clearly distinct from each other. Finally, these landmark sequences are translated into face videos. Our experimental results demonstrate the effectiveness of our CMM-Net in generating realistic videos of multiple smile expressions.","中文标题":"每个微笑都是独一无二的：地标引导的多样化微笑生成","摘要翻译":"每个微笑都是独一无二的：一个人肯定会有不同的微笑方式（例如，闭眼/睁眼或闭嘴/张嘴）。给定一张中性表情的输入图像，我们能否生成具有独特特征的多个微笑视频？为了解决这个一对多的视频生成问题，我们提出了一种名为条件多模式网络（CMM-Net）的新型深度学习架构。为了更好地编码面部表情的动态，CMM-Net明确利用面部地标来生成微笑序列。具体来说，使用变分自编码器来学习面部地标嵌入。然后，这个单一的嵌入被条件递归网络利用，该网络生成基于特定表情（例如，自然微笑）的地标嵌入序列。接下来，生成的地标嵌入被输入到多模式递归地标生成器中，产生一组仍然与给定微笑类别相关联但彼此明显不同的地标序列。最后，这些地标序列被翻译成面部视频。我们的实验结果证明了我们的CMM-Net在生成多个微笑表情的真实视频方面的有效性。","领域":"面部表情生成/视频生成/深度学习","问题":"如何从一张中性表情的图像生成多个具有独特特征的微笑视频","动机":"探索并实现从单一中性表情图像生成多样化微笑视频的可能性，以展示每个人微笑的独特性","方法":"提出了一种名为条件多模式网络（CMM-Net）的新型深度学习架构，该架构利用面部地标和变分自编码器来学习面部表情的动态，并通过条件递归网络和多模式递归地标生成器生成多样化的微笑视频","关键词":["面部表情生成","视频生成","深度学习","变分自编码器","条件递归网络"],"涉及的技术概念":"变分自编码器用于学习面部地标嵌入，条件递归网络用于生成基于特定表情的地标嵌入序列，多模式递归地标生成器用于产生多样化的地标序列，最终将这些序列翻译成面部视频。"},{"order":733,"title":"UV-GAN: Adversarial Facial UV Map Completion for Pose-Invariant Face Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_UV-GAN_Adversarial_Facial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_UV-GAN_Adversarial_Facial_CVPR_2018_paper.html","abstract":"Recently proposed robust 3D face alignment methods establish either dense or sparse correspondence between a 3D face model and a 2D facial image. The use of these methods presents new challenges as well as opportunities for facial texture analysis. In particular, by sampling the image using the fitted model, a facial UV can be created. Unfortunately, due to self-occlusion, such a UV map is always incomplete. In this paper, we propose a framework for training Deep Convolutional Neural Network (DCNN) to complete the facial UV map extracted from in-the-wild images. To this end, we first gather complete UV maps by fitting a 3D Morphable Model (3DMM) to various multiview image and video datasets, as well as leveraging on a new 3D dataset with over 3,000 identities. Second, we devise a meticulously designed architecture that combines local and global adversarial DCNNs to learn an identity-preserving facial UV completion model. We demonstrate that by attaching the completed UV to the fitted mesh and generating instances of arbitrary poses, we can increase pose variations for training deep face recognition/verification models, and minimise pose discrepancy during testing, which lead to better performance. Experiments on both controlled and in-the-wild UV datasets prove the effectiveness of our adversarial UV completion model. We achieve state-of-the-art verification accuracy, 94.05%, under the CFP frontal-profile protocol only by combining pose augmentation during training and pose discrepancy reduction during testing. We will release the first in-the-wild UV dataset (we refer as WildUV) that comprises of complete facial UV maps from 1,892 identities for research purposes.","中文标题":"UV-GAN: 用于姿态不变人脸识别的对抗性面部UV图补全","摘要翻译":"最近提出的鲁棒3D人脸对齐方法在3D人脸模型和2D面部图像之间建立了密集或稀疏的对应关系。这些方法的使用为面部纹理分析带来了新的挑战和机遇。特别是，通过使用拟合模型对图像进行采样，可以创建面部UV图。不幸的是，由于自遮挡，这样的UV图总是不完整的。在本文中，我们提出了一个框架，用于训练深度卷积神经网络（DCNN）以补全从野外图像中提取的面部UV图。为此，我们首先通过将3D可变形模型（3DMM）拟合到各种多视角图像和视频数据集，以及利用一个包含超过3,000个身份的新3D数据集，收集完整的UV图。其次，我们设计了一个精心设计的架构，结合了局部和全局对抗性DCNN，以学习一个保持身份的面部UV补全模型。我们证明，通过将补全的UV图附加到拟合的网格上并生成任意姿态的实例，我们可以增加训练深度人脸识别/验证模型的姿态变化，并在测试期间最小化姿态差异，从而提高性能。在受控和野外UV数据集上的实验证明了我们的对抗性UV补全模型的有效性。我们仅通过在训练期间结合姿态增强和在测试期间减少姿态差异，就在CFP正面-侧面协议下实现了最先进的验证准确率，达到94.05%。我们将发布第一个野外UV数据集（我们称之为WildUV），该数据集包含来自1,892个身份的完整面部UV图，供研究使用。","领域":"人脸识别/3D建模/对抗性学习","问题":"面部UV图由于自遮挡而不完整的问题","动机":"提高姿态变化以训练深度人脸识别/验证模型，并在测试期间最小化姿态差异，从而提高性能","方法":"设计了一个结合局部和全局对抗性DCNN的架构，以学习一个保持身份的面部UV补全模型","关键词":["UV图补全","3D可变形模型","对抗性学习"],"涉及的技术概念":"深度卷积神经网络（DCNN）用于补全面部UV图，3D可变形模型（3DMM）用于拟合多视角图像和视频数据集，对抗性学习用于训练保持身份的UV补全模型。"},{"order":734,"title":"Cascaded Pyramid Network for Multi-Person Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.html","abstract":"The  topic  of  multi-person  pose  estimation  has  beenlargely improved recently, especially with the developmentof convolutional neural network.  However, there still exista lot of challenging cases, such as occluded keypoints, in-visible keypoints and complex background, which cannot bewell addressed.  In this paper, we present a novel networkstructure called Cascaded Pyramid Network (CPN) whichtargets to relieve the problem from these “hard” keypoints.More specifically, our algorithm includes two stages: Glob-alNet and RefineNet.  GlobalNet is a feature pyramid net-work  which  can  successfully  localize  the  “simple”  key-points  like  eyes  and  hands  but  may  fail  to  precisely  rec-ognize the occluded or invisible keypoints.  Our RefineNettries explicitly handling the “hard” keypoints by integrat-ing  all  levels  of  feature  representations  from  the  Global-Net together with an online hard keypoint mining loss.  Ingeneral, to address the multi-person pose estimation prob-lem, a top-down pipeline is adopted to first generate a setof human bounding boxes based on a detector, followed byour CPN for keypoint localization in each human boundingbox. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with averageprecision at 73.0 on the COCO test-dev dataset and 72.1 onthe COCO test-challenge dataset, which is a 19% relativeimprovement compared with 60.5 from the COCO 2016 key-point challenge. Code and the detection results for personused will be publicly available for further research.","中文标题":"级联金字塔网络用于多人姿态估计","摘要翻译":"多人姿态估计这一主题最近得到了很大的改进，尤其是随着卷积神经网络的发展。然而，仍然存在许多具有挑战性的情况，如被遮挡的关键点、不可见的关键点和复杂的背景，这些问题尚未得到很好的解决。在本文中，我们提出了一种名为级联金字塔网络（CPN）的新型网络结构，旨在缓解这些“困难”关键点的问题。更具体地说，我们的算法包括两个阶段：GlobalNet和RefineNet。GlobalNet是一个特征金字塔网络，可以成功定位如眼睛和手等“简单”关键点，但可能无法精确识别被遮挡或不可见的关键点。我们的RefineNet通过整合GlobalNet的所有层次特征表示以及在线困难关键点挖掘损失，明确处理“困难”关键点。总的来说，为了解决多人姿态估计问题，采用了一种自上而下的流程，首先基于检测器生成一组人体边界框，然后使用我们的CPN在每个边界框中进行关键点定位。基于所提出的算法，我们在COCO关键点基准测试中取得了最先进的结果，在COCO测试开发数据集上的平均精度为73.0，在COCO测试挑战数据集上的平均精度为72.1，与COCO 2016关键点挑战的60.5相比，相对提高了19%。用于人体的代码和检测结果将公开以供进一步研究。","领域":"姿态估计/卷积神经网络/特征金字塔网络","问题":"解决多人姿态估计中被遮挡、不可见关键点及复杂背景下的关键点定位问题","动机":"尽管卷积神经网络的发展改进了多人姿态估计，但被遮挡、不可见关键点及复杂背景下的关键点定位仍然具有挑战性","方法":"提出级联金字塔网络（CPN），包括GlobalNet和RefineNet两个阶段，GlobalNet用于定位简单关键点，RefineNet通过整合所有层次的特征表示和在线困难关键点挖掘损失处理困难关键点","关键词":["姿态估计","卷积神经网络","特征金字塔网络","关键点定位","困难关键点"],"涉及的技术概念":"级联金字塔网络（CPN）是一种新型网络结构，用于多人姿态估计，包括GlobalNet和RefineNet两个阶段。GlobalNet是一个特征金字塔网络，用于定位简单关键点；RefineNet通过整合所有层次的特征表示和在线困难关键点挖掘损失，处理困难关键点。该方法在COCO关键点基准测试中取得了最先进的结果。"},{"order":735,"title":"A Face-to-Face Neural Conversation Model","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chu_A_Face-to-Face_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chu_A_Face-to-Face_Neural_CVPR_2018_paper.html","abstract":"Neural networks have recently become good at engaging in dialog. However, current approaches are based solely on verbal text, lacking the richness of a real face-to-face conversation. We propose a neural conversation model that aims to read and generate facial gestures alongside with text. This allows our model to adapt its response based on the “mood” of the conversation. In particular, we introduce an RNN encoder-decoder that exploits the movement of facial muscles, as well as the verbal conversation. The decoder consists of two layers, where the lower layer aims at generating the verbal response and coarse facial expressions, while the second layer fills in the subtle gestures, making the generated output more smooth and natural. We train our neural network by having it “watch” 250 movies. We showcase our joint face-text model in generating more natural conversations through automatic metrics and a human study. We demonstrate an example application with a face-to-face chatting avatar.","中文标题":"面对面神经对话模型","摘要翻译":"神经网络最近在参与对话方面变得非常擅长。然而，当前的方法仅基于口头文本，缺乏真实面对面对话的丰富性。我们提出了一种神经对话模型，旨在阅读和生成面部表情以及文本。这使得我们的模型能够根据对话的“情绪”调整其响应。特别是，我们引入了一种RNN编码器-解码器，它利用面部肌肉的运动以及口头对话。解码器由两层组成，其中下层旨在生成口头响应和粗略的面部表情，而第二层则填充细微的手势，使生成的输出更加流畅和自然。我们通过让我们的神经网络“观看”250部电影来训练它。我们通过自动指标和人类研究展示了我们的联合面部-文本模型在生成更自然对话方面的能力。我们展示了一个面对面聊天头像的示例应用。","领域":"情感计算/人机交互/自然语言处理","问题":"当前基于文本的对话系统缺乏真实面对面对话的丰富性，无法生成和响应面部表情。","动机":"为了增强对话系统的自然度和丰富性，使其能够理解和生成面部表情，从而更接近真实的人类对话体验。","方法":"提出了一种结合面部表情和文本的神经对话模型，采用RNN编码器-解码器结构，通过观看电影训练模型，以生成更自然的面部表情和口头响应。","关键词":["情感计算","人机交互","自然语言处理","面部表情生成","RNN编码器-解码器"],"涉及的技术概念":"RNN编码器-解码器是一种用于序列到序列学习的神经网络结构，能够处理输入序列并生成输出序列。在本研究中，它被用来同时处理口头对话和面部表情，以生成更自然的对话响应。"},{"order":736,"title":"End-to-End Recovery of Human Shape and Pose","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.html","abstract":"We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image.  In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision.  We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person.  We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.","中文标题":"端到端的人体形状和姿态恢复","摘要翻译":"我们描述了人体网格恢复（HMR），这是一个从单一RGB图像重建完整3D人体网格的端到端框架。与大多数当前计算2D或3D关节位置的方法相比，我们生成了一个更丰富、更有用的网格表示，该表示由形状和3D关节角度参数化。主要目标是最小化关键点的重投影损失，这使得我们的模型能够使用仅具有地面真实2D注释的野外图像进行训练。然而，仅重投影损失是高度不足的。在这项工作中，我们通过引入一个对手来解决这个问题，该对手被训练来判断人体形状和姿态是否真实，使用一个大型的3D人体网格数据库。我们展示了HMR可以在使用和不使用任何配对的2D到3D监督的情况下进行训练。我们不依赖于中间的2D关键点检测，而是直接从图像像素推断3D姿态和形状参数。我们的模型在给定包含人物的边界框时实时运行。我们在各种野外图像上展示了我们的方法，并优于以前输出3D网格的基于优化的方法，并在3D关节位置估计和部分分割等任务上展示了竞争性的结果。","领域":"3D人体重建/姿态估计/网格生成","问题":"从单一RGB图像重建完整3D人体网格","动机":"生成一个更丰富、更有用的网格表示，以最小化关键点的重投影损失，使模型能够使用仅具有地面真实2D注释的野外图像进行训练","方法":"引入一个对手来判断人体形状和姿态是否真实，使用一个大型的3D人体网格数据库，直接从图像像素推断3D姿态和形状参数","关键词":["3D人体重建","姿态估计","网格生成"],"涉及的技术概念":"HMR（人体网格恢复）是一个端到端框架，用于从单一RGB图像重建完整3D人体网格。它通过形状和3D关节角度参数化网格表示，最小化关键点的重投影损失，并引入一个对手来判断人体形状和姿态是否真实。该模型不依赖于中间的2D关键点检测，而是直接从图像像素推断3D姿态和形状参数，能够在给定包含人物的边界框时实时运行。"},{"order":737,"title":"Squeeze-and-Excitation Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html","abstract":"Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ∼25% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.","中文标题":"压缩与激励网络","摘要翻译":"卷积神经网络建立在卷积操作之上，该操作通过在局部感受野内融合空间和通道信息来提取信息丰富的特征。为了增强网络的表示能力，最近的一些方法展示了增强空间编码的好处。在这项工作中，我们专注于通道关系，并提出了一种新的架构单元，我们称之为“压缩与激励”（SE）块，它通过显式建模通道间的相互依赖关系来自适应地重新校准通道特征响应。我们证明了通过将这些块堆叠在一起，我们可以构建在挑战性数据集上表现出极好泛化能力的SENet架构。关键的是，我们发现SE块以最小的额外计算成本为现有的最先进深度架构带来了显著的性能提升。SENets构成了我们ILSVRC 2017分类提交的基础，该提交赢得了第一名，并将前5错误率显著降低至2.251%，相对于2016年的获胜作品实现了约25%的相对改进。代码和模型可在https://github.com/hujie-frank/SENet获取。","领域":"神经网络架构/图像分类/特征提取","问题":"如何增强卷积神经网络的表示能力，特别是在通道关系方面","动机":"为了提升卷积神经网络在图像分类等任务上的性能，特别是在不显著增加计算成本的情况下","方法":"提出了一种新的架构单元“压缩与激励”（SE）块，通过显式建模通道间的相互依赖关系来自适应地重新校准通道特征响应，并构建了SENet架构","关键词":["卷积神经网络","通道关系","特征提取","图像分类","SENet"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，特别适用于处理图像数据，通过卷积操作提取特征","压缩与激励（SE）块":"一种新的神经网络架构单元，旨在通过建模通道间的相互依赖关系来增强网络的表示能力","SENet":"基于SE块构建的神经网络架构，旨在提升图像分类等任务的性能","ILSVRC":"ImageNet大规模视觉识别挑战赛，是评估图像分类和对象检测算法性能的重要比赛"}},{"order":738,"title":"Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Islam_Revisiting_Salient_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Islam_Revisiting_Salient_Object_CVPR_2018_paper.html","abstract":"Salient object detection is a problem that has been considered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. The solution presented in this paper solves this more general problem that considers relative rank, and we propose data and metrics suitable to measuring success in a relative object saliency landscape. A novel deep learning solution is proposed based on a hierarchical representation of relative saliency and stage-wise refinement. We also show that the problem of salient object subitizing can be addressed with the same network, and our approach exceeds performance of any prior work across all metrics considered (both traditional and newly proposed).","中文标题":"重新审视显著目标检测：同时检测、排序和快速计数多个显著目标","摘要翻译":"显著目标检测是一个已经被详细考虑并提出了许多解决方案的问题。在本文中，我们认为迄今为止的工作解决的是一个相对不明确的问题。具体来说，当询问多个观察者时，对于什么构成了显著目标并没有普遍共识。这意味着某些目标比其他目标更有可能被判断为显著，并且意味着显著目标存在相对排名。本文提出的解决方案解决了这个考虑相对排名的更一般问题，并且我们提出了适合在相对目标显著性环境中衡量成功的数据和指标。基于相对显著性的分层表示和阶段细化，提出了一种新颖的深度学习解决方案。我们还展示了显著目标的快速计数问题可以用相同的网络解决，并且我们的方法在所有考虑的指标（包括传统和新提出的）上都超过了任何先前的工作。","领域":"显著目标检测/深度学习/图像分析","问题":"显著目标检测中的相对排名和快速计数问题","动机":"解决显著目标检测中由于缺乏普遍共识导致的相对排名问题，并提出新的衡量标准","方法":"基于相对显著性的分层表示和阶段细化的深度学习解决方案","关键词":["显著目标检测","相对排名","快速计数"],"涉及的技术概念":"显著目标检测是指识别图像中最吸引人注意的目标。相对排名指的是不同显著目标之间的显著性比较。快速计数是指快速识别图像中显著目标的数量。分层表示和阶段细化是深度学习中用于逐步提高模型性能的技术。"},{"order":739,"title":"Context Encoding for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Context_Encoding_for_CVPR_2018_paper.html","abstract":"Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries.  In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012.  Our single model achieves a final score of 0.5567 on ADE20K test set, which surpass the winning entry of COCO-Place Challenge in 2017.  In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset.  Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10 times more layers. The source code for the complete system are publicly available.","中文标题":"语义分割的上下文编码","摘要翻译":"最近的工作通过采用扩张/空洞卷积、利用多尺度特征和细化边界，在全卷积网络（FCN）框架下显著提高了像素级标签的空间分辨率。在本文中，我们通过引入上下文编码模块来探索全局上下文信息在语义分割中的影响，该模块捕捉场景的语义上下文并有选择地突出类别依赖的特征图。提出的上下文编码模块仅以边际的额外计算成本显著改善了语义分割结果。我们的方法在PASCAL-Context上达到了51.7%的mIoU，在PASCAL VOC 2012上达到了85.9%的mIoU，创下了新的最先进结果。我们的单一模型在ADE20K测试集上获得了0.5567的最终得分，超过了2017年COCO-Place挑战赛的获胜作品。此外，我们还探索了上下文编码模块如何改善相对浅层网络在CIFAR-10数据集上的图像分类特征表示。我们的14层网络达到了3.45%的错误率，与具有超过10倍层数的最先进方法相当。完整系统的源代码已公开。","领域":"语义分割/图像分类/特征表示","问题":"提高语义分割的准确性和效率","动机":"探索全局上下文信息对语义分割的影响，以及如何通过改进特征表示来提高图像分类的准确性","方法":"引入上下文编码模块，捕捉场景的语义上下文并有选择地突出类别依赖的特征图","关键词":["语义分割","上下文编码","图像分类"],"涉及的技术概念":"全卷积网络（FCN）、扩张/空洞卷积、多尺度特征、边界细化、上下文编码模块、类别依赖的特征图、mIoU（平均交并比）、CIFAR-10数据集、ADE20K测试集、PASCAL-Context、PASCAL VOC 2012、COCO-Place挑战赛"},{"order":740,"title":"Creating Capsule Wardrobes From Fashion Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hsiao_Creating_Capsule_Wardrobes_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hsiao_Creating_Capsule_Wardrobes_CVPR_2018_paper.html","abstract":"We propose to automatically create emph{capsule wardrobes}.  Given an inventory of candidate garments and accessories, the algorithm must assemble a minimal set of items that provides maximal mix-and-match outfits.  We pose the task as a subset selection problem.  To permit efficient subset selection over the space of all outfit combinations, we develop submodular objective functions capturing the key ingredients of visual compatibility, versatility, and user-specific preference.  Since adding garments to a capsule only expands its possible outfits, we devise an iterative approach to allow near-optimal submodular function maximization.  Finally, we present an unsupervised approach to learn visual compatibility from \`\`in the wild\\" full body outfit photos; the compatibility metric  translates well to cleaner catalog photos and improves over existing methods.  Our results on thousands of pieces from popular fashion websites show that automatic capsule creation has potential to mimic skilled fashionistas in assembling flexible wardrobes, while being significantly more scalable.","中文标题":"从时尚图像创建胶囊衣橱","摘要翻译":"我们提出自动创建胶囊衣橱。给定候选服装和配饰的库存，算法必须组装一个最小化的物品集，以提供最大化的混搭服装。我们将此任务设定为一个子集选择问题。为了在所有服装组合的空间上实现高效的子集选择，我们开发了捕捉视觉兼容性、多功能性和用户特定偏好关键要素的子模目标函数。由于向胶囊中添加服装只会扩展其可能的服装组合，我们设计了一种迭代方法，以实现接近最优的子模函数最大化。最后，我们提出了一种无监督的方法，从“野外”全身服装照片中学习视觉兼容性；这种兼容性度量很好地转化为更干净的目录照片，并改进了现有方法。我们在流行时尚网站上的数千件物品上的结果表明，自动胶囊创建有潜力模仿熟练的时尚达人在组装灵活衣橱方面的能力，同时显著更具可扩展性。","领域":"时尚推荐/视觉兼容性分析/个性化推荐","问题":"如何从大量候选服装和配饰中自动选择最小化的物品集，以创建提供最大化混搭服装的胶囊衣橱","动机":"模仿熟练的时尚达人在组装灵活衣橱方面的能力，同时实现更高的可扩展性","方法":"将任务设定为子集选择问题，开发子模目标函数捕捉视觉兼容性、多功能性和用户特定偏好，设计迭代方法实现接近最优的子模函数最大化，提出无监督方法从全身服装照片中学习视觉兼容性","关键词":["胶囊衣橱","视觉兼容性","子模函数","无监督学习","个性化推荐"],"涉及的技术概念":"子模目标函数用于捕捉视觉兼容性、多功能性和用户特定偏好；迭代方法用于实现接近最优的子模函数最大化；无监督学习方法用于从全身服装照片中学习视觉兼容性"},{"order":741,"title":"Webly Supervised Learning Meets Zero-Shot Learning: A Hybrid Approach for Fine-Grained Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Niu_Webly_Supervised_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Webly_Supervised_Learning_CVPR_2018_paper.html","abstract":"Fine-grained image classification, which targets at distinguishing subtle distinctions among various subordinate categories, remains a very difficult task due to the high annotation cost of enormous fine-grained categories. To cope with the scarcity of well-labeled training images, existing works mainly follow two research directions: 1) utilize freely available web images without human annotation; 2) only annotate some fine-grained categories and transfer the knowledge to other fine-grained categories, which falls into the scope of zero-shot learning (ZSL). However, the above two directions have their own drawbacks. For the first direction, the labels of web images are very noisy and the data distribution between web images and test images are considerably different. For the second direction, the performance gap between ZSL and traditional supervised learning is still very large. The drawbacks of the above two directions motivate us to design a new framework which can jointly leverage both web data and auxiliary labeled categories to predict the test categories that are not associated with any well-labeled training images. Comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed framework.","中文标题":"网络监督学习遇上零样本学习：一种细粒度分类的混合方法","摘要翻译":"细粒度图像分类旨在区分各种下属类别之间的细微差别，由于大量细粒度类别的高标注成本，这仍然是一个非常困难的任务。为了应对标注良好的训练图像的稀缺性，现有工作主要遵循两个研究方向：1）利用无需人工标注的免费网络图像；2）仅标注一些细粒度类别并将知识转移到其他细粒度类别，这属于零样本学习（ZSL）的范畴。然而，上述两个方向各有其缺点。对于第一个方向，网络图像的标签非常嘈杂，且网络图像与测试图像之间的数据分布差异很大。对于第二个方向，零样本学习与传统监督学习之间的性能差距仍然很大。上述两个方向的缺点促使我们设计一个新的框架，该框架可以同时利用网络数据和辅助标注类别来预测与任何标注良好的训练图像无关的测试类别。在三个基准数据集上的综合实验证明了我们提出的框架的有效性。","领域":"细粒度图像分类/零样本学习/网络监督学习","问题":"细粒度图像分类中标注成本高和训练图像稀缺的问题","动机":"解决现有方法中网络图像标签噪声大、数据分布差异大以及零样本学习与传统监督学习性能差距大的问题","方法":"设计一个同时利用网络数据和辅助标注类别的新框架，以预测与任何标注良好的训练图像无关的测试类别","关键词":["细粒度图像分类","零样本学习","网络监督学习"],"涉及的技术概念":"细粒度图像分类指的是区分非常相似类别之间的细微差别；零样本学习（ZSL）是一种在没有直接训练样本的情况下，通过辅助信息来识别新类别的方法；网络监督学习指的是利用网络上的图像进行学习，这些图像通常没有经过人工标注，标签可能存在噪声。"},{"order":742,"title":"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval With Generative Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_Look_Imagine_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_Look_Imagine_and_CVPR_2018_paper.html","abstract":"Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial for the cross-modal retrieval performance. Unlike existing image-text retrieval approaches that embed image-text pairs as single feature vectors in a common representational space, we propose to incorporate generative processes into the cross-modal feature embedding, through which we are able to learn not only the global abstract features but also the local grounded features. Extensive experiments show that our framework can well match images and sentences with complex content, and achieve the state-of-the-art cross-modal retrieval results on MSCOCO dataset.","中文标题":"看、想象与匹配：通过生成模型改进文本-视觉跨模态检索","摘要翻译":"文本-视觉跨模态检索一直是计算机视觉和自然语言处理社区中的热门研究课题。学习多模态数据的适当表示对于跨模态检索性能至关重要。与现有的将图像-文本对嵌入为共同表示空间中的单一特征向量的图像-文本检索方法不同，我们提出将生成过程纳入跨模态特征嵌入中，通过这种方法，我们不仅能够学习全局抽象特征，还能学习局部接地特征。大量实验表明，我们的框架能够很好地匹配具有复杂内容的图像和句子，并在MSCOCO数据集上实现了最先进的跨模态检索结果。","领域":"跨模态学习/生成模型/特征表示","问题":"如何提高文本-视觉跨模态检索的性能","动机":"现有的图像-文本检索方法通常将图像-文本对嵌入为共同表示空间中的单一特征向量，这限制了检索性能的提升。","方法":"提出将生成过程纳入跨模态特征嵌入中，以学习全局抽象特征和局部接地特征。","关键词":["跨模态检索","生成模型","特征表示","MSCOCO数据集"],"涉及的技术概念":"跨模态特征嵌入、生成过程、全局抽象特征、局部接地特征、MSCOCO数据集"},{"order":743,"title":"Bidirectional Attentive Fusion With Context Gating for Dense Video Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Bidirectional_Attentive_Fusion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Bidirectional_Attentive_Fusion_CVPR_2018_paper.html","abstract":"Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video contexts. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100% (Meteor score increases from 4.82 to 9.65).","中文标题":"双向注意力融合与上下文门控用于密集视频描述","摘要翻译":"密集视频描述是一项新兴任务，旨在定位并描述视频中的所有事件。我们识别并解决了该任务中的两个挑战，即（1）如何利用过去和未来的上下文来准确预测事件提议，以及（2）如何为解码器构建信息丰富的输入以生成自然的事件描述。首先，之前的工作主要是在前向方向上生成时间事件提议，这忽略了未来的视频上下文。我们提出了一种双向提议方法，有效利用过去和未来的上下文来进行提议预测。其次，在之前的工作中，结束于（几乎）同一时间的不同事件是无法区分的，导致相同的描述。我们通过用提议模块的隐藏状态和视频内容（例如，C3D特征）的注意力融合来表示每个事件来解决这个问题。我们进一步提出了一种新颖的上下文门控机制，以动态平衡当前事件及其周围上下文的贡献。我们经验性地展示了我们注意力融合的事件表示优于单独的提议隐藏状态或视频内容。通过将提议和描述模块耦合到一个统一的框架中，我们的模型在ActivityNet Captions数据集上优于最先进的技术，相对增益超过100%（Meteor分数从4.82增加到9.65）。","领域":"视频理解/事件检测/自然语言生成","问题":"如何准确预测视频中的事件提议并生成自然的事件描述","动机":"解决密集视频描述任务中利用过去和未来上下文进行事件提议预测的挑战，以及为解码器构建信息丰富输入以生成自然事件描述的问题","方法":"提出双向提议方法利用过去和未来上下文进行提议预测，通过注意力融合提议模块的隐藏状态和视频内容来表示每个事件，并引入上下文门控机制动态平衡当前事件及其周围上下文的贡献","关键词":["密集视频描述","双向提议方法","上下文门控机制"],"涉及的技术概念":"密集视频描述涉及定位和描述视频中的所有事件。双向提议方法利用过去和未来的上下文来预测事件提议。注意力融合结合了提议模块的隐藏状态和视频内容（如C3D特征）来表示每个事件。上下文门控机制用于动态平衡当前事件及其周围上下文的贡献。"},{"order":744,"title":"InLoc: Indoor Visual Localization With Dense Matching and View Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Taira_InLoc_Indoor_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Taira_InLoc_Indoor_Visual_CVPR_2018_paper.html","abstract":"We seek to predict the 6 degree-of-freedom (6DoF) pose of a query photograph with respect to a large indoor 3D map. The contributions of this work are three-fold. First, we develop a new large-scale visual localization method targeted for indoor environments. The method proceeds along three steps: (i) efficient retrieval of candidate poses that ensures scalability to large-scale environments, (ii) pose estimation using dense matching rather than local features to deal with textureless indoor scenes, and  (iii) pose verification by virtual view synthesis to cope with significant changes in viewpoint, scene layout, and occluders. Second, we collect a new dataset with reference 6DoF poses for large-scale indoor localization. Query photographs are captured by mobile phones at a different time than the reference 3D map, thus presenting a realistic indoor localization scenario. Third, we demonstrate that our method significantly outperforms current state-of-the-art indoor localization approaches on this new challenging data.","中文标题":"InLoc: 使用密集匹配和视图合成的室内视觉定位","摘要翻译":"我们寻求预测查询照片相对于大型室内3D地图的6自由度（6DoF）姿态。本工作的贡献有三方面。首先，我们开发了一种针对室内环境的新的大规模视觉定位方法。该方法沿着三个步骤进行：（i）有效检索候选姿态，确保对大规模环境的可扩展性，（ii）使用密集匹配而非局部特征进行姿态估计，以处理无纹理的室内场景，以及（iii）通过虚拟视图合成进行姿态验证，以应对视角、场景布局和遮挡物的显著变化。其次，我们收集了一个新的数据集，包含用于大规模室内定位的参考6DoF姿态。查询照片由手机在不同时间捕获，与参考3D地图不同，从而呈现了一个现实的室内定位场景。第三，我们证明了我们的方法在这个新的挑战性数据上显著优于当前最先进的室内定位方法。","领域":"室内定位/视觉定位/3D重建","问题":"预测查询照片相对于大型室内3D地图的6自由度（6DoF）姿态","动机":"开发一种针对室内环境的新的大规模视觉定位方法，以处理无纹理的室内场景和应对视角、场景布局和遮挡物的显著变化","方法":"有效检索候选姿态，使用密集匹配进行姿态估计，通过虚拟视图合成进行姿态验证","关键词":["室内定位","视觉定位","3D重建","密集匹配","视图合成"],"涉及的技术概念":"6自由度（6DoF）姿态、密集匹配、虚拟视图合成、大规模视觉定位方法、无纹理室内场景处理"},{"order":745,"title":"Towards High Performance Video Object Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_High_Performance_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_High_Performance_CVPR_2018_paper.html","abstract":"There has been significant progresses for image object detection recently. Nevertheless, video object detection has received little attention, although it is more challenging and more important in practical scenarios.  Built upon the recent works, this work proposes a unified viewpoint based on the principle of multi-frame end-to-end learning of features and cross-frame motion. Our approach extends prior works with three new techniques and steadily pushes forward the performance envelope  (speed-accuracy tradeoff), towards high performance video object detection.","中文标题":"迈向高性能视频目标检测","摘要翻译":"近年来，图像目标检测取得了显著进展。然而，尽管视频目标检测在实际场景中更具挑战性和重要性，却鲜少受到关注。基于最近的研究成果，本工作提出了一种基于多帧端到端特征学习和跨帧运动原理的统一观点。我们的方法通过三种新技术扩展了先前的工作，并稳步推进了性能极限（速度-准确度权衡），朝着高性能视频目标检测迈进。","领域":"视频分析/目标检测/深度学习","问题":"视频目标检测的性能提升","动机":"视频目标检测在实际应用中具有重要性和挑战性，但相较于图像目标检测，其研究较少。","方法":"提出了一种基于多帧端到端特征学习和跨帧运动原理的统一观点，并引入了三种新技术以扩展先前的工作。","关键词":["视频分析","目标检测","深度学习"],"涉及的技术概念":"多帧端到端学习、跨帧运动、速度-准确度权衡"},{"order":746,"title":"Neural Baby Talk","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lu_Neural_Baby_Talk_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lu_Neural_Baby_Talk_CVPR_2018_paper.html","abstract":"We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence \`template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions -- and hence language priors of associated captions -- are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk","中文标题":"神经婴儿谈话","摘要翻译":"我们引入了一种新颖的图像描述框架，该框架能够生成明确基于对象检测器在图像中发现的实体的自然语言。我们的方法将经典的槽填充方法（通常更好地基于图像）与现代神经描述方法（通常听起来更自然且更准确）相结合。我们的方法首先生成一个句子“模板”，其槽位置明确地绑定到特定的图像区域。然后，这些槽由对象检测器在区域中识别的视觉概念填充。整个架构（句子模板生成和对象检测器的槽填充）是端到端可微分的。我们在不同的图像描述任务上验证了我们提出的模型的有效性。在标准图像描述和新对象描述上，我们的模型在COCO和Flickr30k数据集上达到了最先进的水平。我们还证明了当场景组合的训练和测试分布——以及因此相关描述的语言先验——不同时，我们的模型具有独特的优势。代码已在https://github.com/jiasenlu/NeuralBabyTalk上提供。","领域":"图像描述/自然语言生成/视觉-语言理解","问题":"如何生成既自然又准确且基于图像实体的图像描述","动机":"结合经典槽填充方法和现代神经描述方法的优势，以生成更自然、更准确的图像描述","方法":"首先生成与特定图像区域绑定的句子模板，然后由对象检测器识别的视觉概念填充槽位，整个架构是端到端可微分的","关键词":["图像描述","自然语言生成","视觉-语言理解","对象检测","槽填充"],"涉及的技术概念":{"图像描述":"生成描述图像内容的自然语言文本","自然语言生成":"自动生成自然语言文本的过程","视觉-语言理解":"理解和生成与视觉内容相关的自然语言描述","对象检测":"识别图像中的对象及其位置","槽填充":"在预定义的句子模板中填充具体信息"}},{"order":747,"title":"Few-Shot Image Recognition by Predicting Parameters From Activations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.html","abstract":"In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods.","中文标题":"通过从激活中预测参数进行少样本图像识别","摘要翻译":"在本文中，我们对少样本学习问题感兴趣。特别是，我们关注一个具有挑战性的场景，其中类别数量大且每个新类别的示例数量非常有限，例如1、2或3。受到神经网络中与同一类别相关的参数和激活之间密切关系的启发，我们提出了一种新颖的方法，可以通过直接从激活中预测参数来适应预训练的神经网络到新类别。适应新类别不需要训练，并且通过单次前向传递实现快速推理。我们通过在ImageNet数据集上进行少样本图像识别来评估我们的方法，该方法在新类别上以显著优势实现了最先进的分类准确率，同时在大规模类别上保持了可比的性能。我们还在MiniImageNet数据集上测试了我们的方法，它大大优于之前的最先进方法。","领域":"少样本学习/图像识别/神经网络","问题":"解决在类别数量大且每个新类别的示例数量非常有限的情况下进行少样本图像识别的问题","动机":"受到神经网络中与同一类别相关的参数和激活之间密切关系的启发，探索一种无需训练即可适应新类别的方法","方法":"提出一种直接从激活中预测参数的方法，以适应预训练的神经网络到新类别，实现快速推理","关键词":["少样本学习","图像识别","神经网络","参数预测","激活"],"涉及的技术概念":"少样本学习是指在每个类别只有很少的样本（如1、2或3个）的情况下进行学习。本文提出的方法通过直接从神经网络的激活中预测参数，来适应预训练的神经网络到新类别，这种方法不需要额外的训练，并且能够通过单次前向传递实现快速推理。"},{"order":748,"title":"Iterative Visual Reasoning Beyond Convolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Iterative_Visual_Reasoning_CVPR_2018_paper.html","abstract":"We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs in parallel; and a global graph-reasoning module. Our graph has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to class nodes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, eg achieving an $8.4%$ absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.","中文标题":"超越卷积的迭代视觉推理","摘要翻译":"我们提出了一个新颖的迭代视觉推理框架。我们的框架超越了当前缺乏超越卷积堆栈推理能力的识别系统。该框架由两个核心模块组成：一个使用空间记忆并行存储先前信念的局部模块；以及一个全局图推理模块。我们的图有三个组成部分：a) 一个知识图，其中我们将类表示为节点，并构建边来编码它们之间的不同类型的语义关系；b) 当前图像的区域图，其中图像中的区域是节点，这些区域之间的空间关系是边；c) 一个将区域分配给类节点的分配图。局部模块和全局模块迭代展开并相互交叉反馈预测以精炼估计。最终预测是通过结合两个模块的最佳结果与注意力机制来完成的。我们展示了相对于普通卷积神经网络的强大性能，例如在ADE上通过每类平均精度实现了8.4%的绝对改进。分析还表明，该框架对于推理中的缺失区域具有弹性。","领域":"视觉推理/语义关系/空间关系","问题":"当前识别系统缺乏超越卷积堆栈的推理能力","动机":"提出一个能够进行迭代视觉推理的框架，以超越现有系统的限制","方法":"采用局部模块和全局图推理模块，通过迭代和交叉反馈预测来精炼估计，并结合注意力机制进行最终预测","关键词":["视觉推理","语义关系","空间关系","注意力机制"],"涉及的技术概念":"空间记忆用于存储先前信念，知识图用于表示类及其语义关系，区域图用于表示图像区域及其空间关系，分配图用于将区域分配给类节点，注意力机制用于结合局部和全局模块的最佳结果进行最终预测。"},{"order":749,"title":"Visual Question Reasoning on General Dependency Tree","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Visual_Question_Reasoning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cao_Visual_Question_Reasoning_CVPR_2018_paper.html","abstract":"The collaborative reasoning for  understanding each image-question pair is very critical but under-explored for an interpretable Visual Question Answering (VQA) system. Although very recent works also tried the explicit compositional processes to assemble multiple sub-tasks embedded in the questions, their models heavily rely on the annotations or hand-crafted rules to obtain valid reasoning layout, leading to either heavy labor or poor performance on composition reasoning. In this paper, to enable global context reasoning for better aligning image and language domains in diverse and unrestricted cases, we propose a novel reasoning network called Adversarial Composition Modular Network (ACMN). This network comprises of two collaborative modules: i) an adversarial attention module to exploit the local visual evidence for each word parsed from the question; ii) a residual composition module to compose the previously mined evidence. Given a dependency parse tree for each question, the adversarial attention module progressively discovers salient regions of one word by densely combining regions of child word nodes in an adversarial manner. Then residual composition module merges the hidden representations of an arbitrary number of children through sum pooling and residual connection. Our ACMN is thus capable of building an interpretable VQA system that gradually dives the image cues following a question-driven reasoning route and makes global reasoning by incorporating the learned knowledge of all attention modules in a principled manner. Experiments on relational datasets demonstrate the superiority of our ACMN and visualization results show the explainable capability of our reasoning system.","中文标题":"基于通用依赖树的视觉问题推理","摘要翻译":"理解每个图像-问题对的协作推理对于可解释的视觉问答（VQA）系统非常关键，但这一领域尚未得到充分探索。尽管最近的工作也尝试了明确的组合过程来组装嵌入问题中的多个子任务，但他们的模型严重依赖注释或手工制定的规则来获得有效的推理布局，导致要么劳动强度大，要么在组合推理上表现不佳。在本文中，为了在多样化和无限制的情况下实现全局上下文推理，以更好地对齐图像和语言领域，我们提出了一种新的推理网络，称为对抗性组合模块网络（ACMN）。该网络由两个协作模块组成：i）一个对抗性注意力模块，用于利用从问题中解析出的每个词的局部视觉证据；ii）一个残差组合模块，用于组合先前挖掘的证据。给定每个问题的依赖解析树，对抗性注意力模块通过以对抗方式密集组合子词节点的区域，逐步发现一个词的显著区域。然后，残差组合模块通过求和池化和残差连接合并任意数量子节点的隐藏表示。因此，我们的ACMN能够构建一个可解释的VQA系统，该系统逐步按照问题驱动的推理路线深入图像线索，并通过以原则方式整合所有注意力模块的学习知识进行全局推理。在关系数据集上的实验证明了我们的ACMN的优越性，可视化结果展示了我们推理系统的可解释能力。","领域":"视觉问答/自然语言处理/图像理解","问题":"如何在没有大量注释或手工规则的情况下，实现有效的视觉问答系统推理","动机":"探索协作推理在视觉问答系统中的应用，以提高系统的可解释性和性能","方法":"提出了一种新的推理网络ACMN，包括对抗性注意力模块和残差组合模块，以实现全局上下文推理","关键词":["视觉问答","对抗性注意力","残差组合"],"涉及的技术概念":"对抗性注意力模块用于利用从问题中解析出的每个词的局部视觉证据；残差组合模块用于组合先前挖掘的证据。ACMN网络通过依赖解析树逐步发现词的显著区域，并通过求和池化和残差连接合并子节点的隐藏表示，实现全局推理。"},{"order":750,"title":"CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_CVM-Net_Cross-View_Matching_CVPR_2018_paper.html","abstract":"The problem of localization on a geo-referenced aerial/satellite map given a query ground view image remains challenging due to the drastic change in viewpoint that causes traditional image descriptors based matching to fail. We leverage on the recent success of deep learning to propose the CVM-Net for the cross-view image-based ground-to-aerial geo-localization task. Specifically, our network is based on the Siamese architecture to do metric learning for the matching task. We first use the fully convolutional layers to extract local image features, which are then encoded into global image descriptors using the powerful NetVLAD. As part of the training procedure, we also introduce a simple yet effective weighted soft margin ranking loss function that not only speeds up the training convergence but also improves the final matching accuracy. Experimental results show that our proposed network significantly outperforms the state-of-the-art approaches on two existing benchmarking datasets.","中文标题":"CVM-Net: 基于图像的从地面到空中的地理定位跨视图匹配网络","摘要翻译":"给定查询地面视图图像，在具有地理参考的航空/卫星地图上进行定位的问题仍然具有挑战性，这是由于视角的剧烈变化导致基于传统图像描述符的匹配失败。我们利用深度学习的最新成功，提出了CVM-Net，用于基于图像的从地面到空中的跨视图地理定位任务。具体来说，我们的网络基于Siamese架构，用于匹配任务的度量学习。我们首先使用全卷积层提取局部图像特征，然后使用强大的NetVLAD将其编码为全局图像描述符。作为训练过程的一部分，我们还引入了一种简单但有效的加权软边际排序损失函数，不仅加快了训练收敛速度，还提高了最终的匹配精度。实验结果表明，我们提出的网络在两个现有的基准数据集上显著优于最先进的方法。","领域":"地理定位/跨视图匹配/深度学习","问题":"解决在给定地面视图图像的情况下，在具有地理参考的航空/卫星地图上进行定位的挑战","动机":"由于视角的剧烈变化导致基于传统图像描述符的匹配失败，需要一种新的方法来解决这一挑战","方法":"基于Siamese架构的CVM-Net网络，使用全卷积层提取局部图像特征，并通过NetVLAD编码为全局图像描述符，引入加权软边际排序损失函数以加快训练收敛速度和提高匹配精度","关键词":["地理定位","跨视图匹配","深度学习","Siamese架构","NetVLAD","加权软边际排序损失函数"],"涉及的技术概念":{"Siamese架构":"一种用于度量学习的神经网络架构，通过共享权重的两个相同子网络处理输入对，用于比较输入之间的相似性","NetVLAD":"一种用于图像特征编码的技术，能够将局部图像特征聚合成全局图像描述符，适用于图像检索和匹配任务","加权软边际排序损失函数":"一种用于训练过程中的损失函数，通过引入权重和软边际来优化排序任务，加快训练收敛速度并提高匹配精度"}},{"order":751,"title":"Revisiting Dilated Convolution: A Simple Approach for Weakly- and Semi-Supervised Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Revisiting_Dilated_Convolution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Revisiting_Dilated_Convolution_CVPR_2018_paper.html","abstract":"Despite remarkable progress, weakly supervised segmentation methods are still inferior to their fully supervised counterparts. We obverse that the performance gap mainly comes from the inability of producing dense and integral pixel-level object localization for training images only with image-level labels. In this work, we revisit the dilated convolution proposed in [1] and shed light on how it enables the classification network to generate dense object localization. By substantially enlarging the receptive fields of convolutional kernels with different dilation rates, the classification network can localize the object regions even when they are not so discriminative for classification and finally produce reliable object regions for benefiting both weakly- and semi- supervised semantic segmentation. Despite the apparent simplicity of dilated convolution, we are able to obtain superior performance for semantic segmentation tasks. In particular, it achieves 60.8% and 67.6% mean Intersection-over-Union (mIoU) on Pascal VOC 2012 test set in weakly- (only image-level labels are available) and semi- (1,464 segmentation masks are available) settings, which are the new state-of-the-arts.","中文标题":"重新审视扩张卷积：一种用于弱监督和半监督语义分割的简单方法","摘要翻译":"尽管取得了显著进展，弱监督分割方法仍然不如完全监督的方法。我们观察到，性能差距主要来自于无法为仅带有图像级标签的训练图像生成密集且完整的像素级对象定位。在这项工作中，我们重新审视了[1]中提出的扩张卷积，并阐明了它如何使分类网络能够生成密集的对象定位。通过使用不同的扩张率大幅扩大卷积核的感受野，分类网络即使在对象区域对分类不那么具有区分性时也能定位对象区域，并最终为弱监督和半监督语义分割生成可靠的对象区域。尽管扩张卷积的明显简单性，我们能够在语义分割任务中获得卓越的性能。特别是在Pascal VOC 2012测试集上，在弱监督（仅提供图像级标签）和半监督（提供1,464个分割掩码）设置下，分别达到了60.8%和67.6%的平均交并比（mIoU），这是新的最先进水平。","领域":"语义分割/弱监督学习/半监督学习","问题":"弱监督和半监督语义分割中生成密集且完整的像素级对象定位的挑战","动机":"解决弱监督分割方法在生成密集和完整像素级对象定位方面的不足，以缩小与完全监督方法的性能差距","方法":"重新审视并利用扩张卷积，通过不同的扩张率扩大卷积核的感受野，使分类网络能够定位对分类不那么具有区分性的对象区域，从而生成可靠的对象区域","关键词":["扩张卷积","弱监督学习","半监督学习","语义分割","对象定位"],"涉及的技术概念":"扩张卷积是一种通过引入扩张率来扩大卷积核感受野的技术，使得网络能够在更大的区域内捕捉信息，从而有助于定位对象区域。平均交并比（mIoU）是评估语义分割性能的常用指标，通过计算预测分割与真实分割之间的交集与并集的比例来衡量准确性。"},{"order":752,"title":"Low-Shot Learning From Imaginary Data","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Low-Shot_Learning_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Low-Shot_Learning_From_CVPR_2018_paper.html","abstract":"Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, i.e., learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning (''learning to learn'') by combining a meta-learner with a ''hallucinator'' that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark.","中文标题":"从虚构数据中进行少样本学习","摘要翻译":"人类能够快速学习新的视觉概念，可能是因为他们能够轻松地可视化或想象新物体在不同视角下的样子。将这种能够幻想出新概念实例的能力融入机器视觉系统，可能会帮助其更好地进行少样本学习，即从少量例子中学习概念。我们提出了一种新的少样本学习方法，该方法利用了这一想法。我们的方法基于元学习（“学习如何学习”）的最新进展，通过将元学习器与一个产生额外训练例子的“幻想器”结合起来，并联合优化这两个模型。我们的幻想器可以融入多种元学习器中，并提供了显著的性能提升：在仅有一个训练例子可用时，分类准确率最多提高了6个百分点，在具有挑战性的ImageNet少样本分类基准上达到了最先进的性能。","领域":"少样本学习/元学习/图像分类","问题":"如何从少量例子中学习新的视觉概念","动机":"人类能够通过想象新物体在不同视角下的样子来快速学习新的视觉概念，这种能力可能对机器视觉系统进行少样本学习有帮助。","方法":"提出了一种新的少样本学习方法，该方法结合了元学习器和一个能够产生额外训练例子的“幻想器”，并联合优化这两个模型。","关键词":["少样本学习","元学习","图像分类","幻想器"],"涉及的技术概念":"元学习（Meta-learning）是一种让模型学会如何学习的方法，通过这种方式，模型可以在面对新任务时快速适应。幻想器（Hallucinator）是一种能够生成额外训练数据的模型，这些数据可以帮助模型在少样本学习任务中表现得更好。ImageNet是一个大规模的图像分类基准，用于评估图像识别算法的性能。"},{"order":753,"title":"DoubleFusion: Real-Time Capture of Human Performances With Inner Body Shapes From a Single Depth Sensor","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_DoubleFusion_Real-Time_Capture_CVPR_2018_paper.html","abstract":"We propose DoubleFusion, a new real-time system that combines volumetric dynamic reconstruction with data-driven template fitting to simultaneously reconstruct detailed geometry, non-rigid motion and the inner human body shape from a single depth camera. One of the key contributions of this method is a double layer representation consisting of a complete parametric body shape inside and a gradually fused outer surface layer. A pre-defined node graph on the body surface parameterizes the non-rigid deformations near the body and a free-form dynamically changing graph parameterizes the outer surface layer far from the body allowing more general reconstruction. We further propose a joint motion tracking method based on the double layer representation to enable robust and fast motion tracking performance. Moreover, the inner body shape is optimized online and forced to fit inside the outer surface layer. Overall, our method enables increasingly denoised, detailed and complete surface reconstructions, fast motion tracking performance and plausible inner body shape reconstruction in real-time. In particular, experiments show improved fast motion tracking and loop closure performance on more challenging scenarios.","中文标题":"DoubleFusion：使用单一深度传感器实时捕捉人体表现与内部体型","摘要翻译":"我们提出了DoubleFusion，一种新的实时系统，该系统结合了体积动态重建与数据驱动的模板拟合，以从单一深度相机同时重建详细的几何形状、非刚性运动和人体内部体型。该方法的一个关键贡献是双层表示，包括内部完整的参数化体型和逐渐融合的外表面层。身体表面上的预定义节点图参数化身体附近的非刚性变形，而自由形式的动态变化图参数化远离身体的外表面层，允许更一般的重建。我们进一步提出了一种基于双层表示的联合运动跟踪方法，以实现鲁棒且快速的运动跟踪性能。此外，内部体型在线优化并强制适应外表面层。总体而言，我们的方法能够实现越来越去噪、详细和完整的表面重建，快速运动跟踪性能以及实时可信的内部体型重建。特别是，实验显示在更具挑战性的场景中，快速运动跟踪和循环闭合性能有所提高。","领域":"三维重建/运动捕捉/人体建模","问题":"如何从单一深度传感器实时捕捉人体的详细几何形状、非刚性运动和内部体型","动机":"为了在实时应用中实现更准确和详细的人体表现捕捉，包括内部体型的重建","方法":"提出了一种结合体积动态重建与数据驱动模板拟合的系统，采用双层表示法，包括内部参数化体型和外表面层，以及基于此的联合运动跟踪方法","关键词":["三维重建","运动捕捉","人体建模","实时系统","深度相机"],"涉及的技术概念":"双层表示法包括内部参数化体型和外表面层，预定义节点图用于参数化身体附近的非刚性变形，自由形式的动态变化图用于参数化远离身体的外表面层，联合运动跟踪方法用于实现鲁棒且快速的运动跟踪性能，内部体型在线优化以适应外表面层"},{"order":754,"title":"DensePose: Dense Human Pose Estimation in the Wild","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Guler_DensePose_Dense_Human_CVPR_2018_paper.html","abstract":"In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense  correspondences for 50K persons appearing in the  COCO dataset by introducing  an efficient annotation pipeline. We then use our dataset to train CNN-based systems that  deliver dense correspondence \\"in the wild\\", namely in the presence of background, occlusions and scale variations. We improve our training set's effectiveness by training an inpainting network that can fill in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org.","中文标题":"DensePose: 野外密集人体姿态估计","摘要翻译":"在这项工作中，我们建立了RGB图像与基于表面的人体表示之间的密集对应关系，这一任务我们称之为密集人体姿态估计。我们通过引入一个高效的注释管道，为COCO数据集中出现的50K人收集了密集对应关系。然后，我们使用我们的数据集来训练基于CNN的系统，这些系统能够在野外提供密集对应关系，即在存在背景、遮挡和尺度变化的情况下。我们通过训练一个能够填补缺失地面真实值的修复网络来提高我们训练集的有效性，并报告了相对于过去可实现的最佳结果的改进。我们实验了全卷积网络和基于区域的模型，并观察到后者的优越性。我们通过级联进一步提高了准确性，获得了一个在单个GPU上每秒多帧提供高精度结果的系统。补充材料、数据、代码和视频可在项目页面http://densepose.org上找到。","领域":"人体姿态估计/密集对应/图像修复","问题":"在复杂背景下实现RGB图像与人体表面表示的密集对应关系","动机":"提高在野外环境下人体姿态估计的准确性和鲁棒性","方法":"引入高效注释管道收集密集对应关系，训练基于CNN的系统，使用修复网络填补缺失的地面真实值，实验全卷积网络和基于区域的模型，通过级联提高准确性","关键词":["密集对应","人体姿态估计","图像修复","全卷积网络","基于区域的模型"],"涉及的技术概念":"密集对应关系指的是在RGB图像和人体表面表示之间建立详细的点对点映射。CNN（卷积神经网络）是一种深度学习模型，特别适用于处理图像数据。修复网络是一种用于填补图像中缺失或损坏部分的技术。全卷积网络是一种没有全连接层的卷积神经网络，适用于像素级预测任务。基于区域的模型是一种在图像中识别和分类对象的方法，通常用于目标检测和分割任务。级联是一种通过串联多个模型或处理步骤来提高系统性能的技术。"},{"order":755,"title":"Ordinal Depth Supervision for 3D Human Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.html","abstract":"Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.","中文标题":"用于3D人体姿态估计的序数深度监督","摘要翻译":"我们目前从单张图像训练端到端系统进行3D人体姿态估计的能力受到自然图像3D注释有限可用性的限制。大多数数据集是在工作室环境中使用运动捕捉（MoCap）系统捕获的，很难达到像MPII或LSP这样的2D人体姿态数据集的多样性。为了减轻对精确3D地面实况的需求，我们提出使用由人体关节的序数深度提供的较弱监督信号。这种信息可以通过人类注释者为广泛的图像和姿态获取。我们在不同设置中展示了使用这些序数关系训练卷积网络（ConvNets）的有效性和灵活性，始终能够与使用精确3D关节坐标训练的ConvNets竞争。此外，为了展示该方法的潜力，我们为流行的LSP和MPII数据集增加了序数深度注释。这一扩展使我们能够在非工作室条件下进行定量和定性评估。同时，这些序数注释可以轻松地融入到典型ConvNets的3D人体姿态训练过程中。通过这种包含，我们在相关基准测试中实现了新的最先进性能，并验证了序数深度监督对于3D人体姿态的有效性。","领域":"3D人体姿态估计/卷积网络/序数深度监督","问题":"3D人体姿态估计中3D注释的有限可用性","动机":"减轻对精确3D地面实况的需求，通过使用较弱的监督信号来提高训练数据的多样性和可用性","方法":"提出使用人体关节的序数深度作为监督信号，训练卷积网络进行3D人体姿态估计","关键词":["3D人体姿态估计","序数深度监督","卷积网络","数据增强"],"涉及的技术概念":"序数深度监督是一种较弱的监督信号，它不需要精确的3D坐标，而是依赖于人体关节之间的相对深度关系。这种方法可以更容易地从广泛的图像和姿态中获取注释，从而增加训练数据的多样性和可用性。卷积网络（ConvNets）是一种深度学习模型，特别适用于处理图像数据。通过将序数深度注释融入到ConvNets的训练过程中，可以在3D人体姿态估计任务中实现竞争性的性能。"},{"order":756,"title":"Consensus Maximization for Semantic Region Correspondences","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Speciale_Consensus_Maximization_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Speciale_Consensus_Maximization_for_CVPR_2018_paper.html","abstract":"We propose a novel method for the geometric registration of semantically labeled regions. We approximate semantic regions by ellipsoids, and leverage their convexity to formulate the correspondence search effectively as a constrained optimization problem that maximizes the number of matched regions, and which we solve globally optimal in a branch-and-bound fashion. To this end, we derive suitable linear matrix inequality constraints which describe ellipsoid-to-ellipsoid assignment conditions. Our approach is robust to large percentages of outliers and thus applicable to difficult correspondence search problems. In multiple experiments we demonstrate the flexibility and robustness of our approach on a number of challenging vision problems.","中文标题":"语义区域对应的一致性最大化","摘要翻译":"我们提出了一种新颖的方法，用于语义标记区域的几何配准。我们通过椭球体近似语义区域，并利用其凸性将对应搜索有效地表述为一个约束优化问题，该问题旨在最大化匹配区域的数量，并以分支限界的方式全局最优地解决。为此，我们推导了适合的线性矩阵不等式约束，这些约束描述了椭球体到椭球体的分配条件。我们的方法对大量异常值具有鲁棒性，因此适用于困难的对应搜索问题。在多项实验中，我们展示了我们的方法在多个具有挑战性的视觉问题上的灵活性和鲁棒性。","领域":"几何配准/语义分割/优化问题","问题":"语义标记区域的几何配准","动机":"解决语义区域对应搜索中的困难，特别是在存在大量异常值的情况下。","方法":"通过椭球体近似语义区域，并利用其凸性将对应搜索表述为约束优化问题，采用分支限界法全局最优地解决。","关键词":["几何配准","语义分割","优化问题","椭球体近似","分支限界法"],"涉及的技术概念":"椭球体近似用于表示语义区域，线性矩阵不等式约束用于描述椭球体之间的分配条件，分支限界法用于全局优化。"},{"order":757,"title":"Robust Hough Transform Based 3D Reconstruction From Circular Light Fields","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vianello_Robust_Hough_Transform_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vianello_Robust_Hough_Transform_CVPR_2018_paper.html","abstract":"Light-field imaging is based on images taken on a regular grid. Thus, high-quality 3D reconstructions are obtainable by analyzing orientations in epipolar plane images (EPIs). Unfortunately, such data only allows to evaluate one side of the object. Moreover, a constant intensity along each orientation is mandatory for most of the approaches. This paper presents a novel method which allows to reconstruct depth information from data acquired with a circular camera motion, termed circular light fields. With this approach it is possible to determine the full 360 degree view of target objects. Additionally, circular light fields allow retrieving depth from datasets acquired with telecentric lenses, which is not possible with linear light fields. The proposed method finds trajectories of 3D points in the EPIs by means of a modified Hough transform. For this purpose, binary EPI-edge images are used, which not only allow to obtain reliable depth information, but also overcome the limitation of constant intensity along trajectories. Experimental results on synthetic and real datasets demonstrate the quality of the proposed algorithm.","中文标题":"基于鲁棒霍夫变换的圆形光场三维重建","摘要翻译":"光场成像基于在规则网格上拍摄的图像。因此，通过分析极平面图像（EPIs）中的方向，可以获得高质量的三维重建。不幸的是，这样的数据只能评估物体的一侧。此外，对于大多数方法来说，沿每个方向的恒定强度是必须的。本文提出了一种新方法，可以从通过圆形相机运动获取的数据中重建深度信息，称为圆形光场。通过这种方法，可以确定目标物体的完整360度视图。此外，圆形光场允许从使用远心镜头获取的数据集中检索深度，这是线性光场无法实现的。所提出的方法通过改进的霍夫变换在EPIs中找到三维点的轨迹。为此，使用二值EPI边缘图像，这不仅允许获得可靠的深度信息，而且克服了沿轨迹恒定强度的限制。在合成和真实数据集上的实验结果证明了所提出算法的质量。","领域":"三维重建/光场成像/远心镜头","问题":"从圆形光场中重建三维深度信息","动机":"解决传统线性光场无法评估物体完整360度视图和从远心镜头数据集中检索深度的问题","方法":"使用改进的霍夫变换在极平面图像（EPIs）中找到三维点的轨迹，并利用二值EPI边缘图像克服沿轨迹恒定强度的限制","关键词":["三维重建","光场成像","远心镜头","霍夫变换","极平面图像"],"涉及的技术概念":"光场成像是一种基于在规则网格上拍摄图像的成像技术，极平面图像（EPIs）是光场成像中的一种图像表示形式，用于分析图像中的方向信息。霍夫变换是一种在图像处理中用于检测形状的技术，本文中通过改进的霍夫变换在EPIs中找到三维点的轨迹。远心镜头是一种特殊类型的镜头，用于在成像过程中保持物体的大小不变，即使物体与镜头之间的距离发生变化。"},{"order":758,"title":"Alive Caricature From 2D to 3D","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Alive_Caricature_From_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Alive_Caricature_From_CVPR_2018_paper.html","abstract":"Caricature is an art form that expresses subjects in abstract, simple and exaggerated views. While many caricatures are 2D images, this paper presents an algorithm for creating expressive 3D caricatures from 2D caricature images with minimum user interaction. The key idea of our approach is to introduce an intrinsic deformation representation that has the capability of extrapolation, enabling us to create a deformation space from standard face datasets, which maintains face constraints and meanwhile is sufficiently large for producing exaggerated face models. Built upon the proposed deformation representation, an optimization model is formulated to find the 3D caricature that captures the style of the 2D caricature image automatically. The experiments show that our approach has better capability in expressing caricatures than those fitting approaches directly using classical parametric face models such as 3DMM and FaceWareHouse. Moreover, our approach is based on standard face datasets and avoids constructing complicated 3D caricature training sets, which provides great flexibility in real applications.","中文标题":"从2D到3D的生动漫画","摘要翻译":"漫画是一种以抽象、简单和夸张的视角表达主题的艺术形式。虽然许多漫画是2D图像，但本文提出了一种算法，用于从2D漫画图像中创建具有最少用户交互的表现力3D漫画。我们方法的关键思想是引入一种具有外推能力的内在变形表示，使我们能够从标准人脸数据集中创建一个变形空间，该空间保持了人脸约束，同时足够大以产生夸张的人脸模型。基于提出的变形表示，我们制定了一个优化模型，以自动捕捉2D漫画图像风格的3D漫画。实验表明，与直接使用经典参数化人脸模型（如3DMM和FaceWareHouse）的拟合方法相比，我们的方法在表达漫画方面具有更好的能力。此外，我们的方法基于标准人脸数据集，避免了构建复杂的3D漫画训练集，这在实际应用中提供了极大的灵活性。","领域":"3D建模/人脸识别/图像变形","问题":"如何从2D漫画图像自动生成具有表现力的3D漫画","动机":"为了在保持人脸约束的同时，从2D漫画图像中自动生成具有夸张效果的3D漫画，减少用户交互的需求","方法":"引入一种具有外推能力的内在变形表示，基于此制定优化模型，自动捕捉2D漫画图像风格的3D漫画","关键词":["3D建模","人脸识别","图像变形"],"涉及的技术概念":"内在变形表示、外推能力、变形空间、优化模型、3DMM、FaceWareHouse"},{"order":759,"title":"Nonlinear 3D Face Morphable Model","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tran_Nonlinear_3D_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_Nonlinear_3D_Face_CVPR_2018_paper.html","abstract":"As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.","中文标题":"非线性3D人脸可变形模型","摘要翻译":"作为3D面部形状和纹理的经典统计模型，3D可变形模型（3DMM）广泛应用于面部分析，例如模型拟合、图像合成。传统的3DMM是从一组良好控制的2D面部图像及其相关的3D面部扫描中学习得到的，并由两组PCA基函数表示。由于训练数据的类型和数量以及线性基的限制，3DMM的表示能力可能受到限制。为了解决这些问题，本文提出了一种创新框架，从大量无约束的面部图像中学习非线性3DMM模型，而无需收集3D面部扫描。具体来说，给定一张面部图像作为输入，网络编码器估计投影、形状和纹理参数。两个解码器作为非线性3DMM，分别将形状和纹理参数映射到3D形状和纹理。利用投影参数、3D形状和纹理，设计了一个新颖的解析可微渲染层来重建原始输入面部。整个网络仅需弱监督即可端到端训练。我们展示了我们的非线性3DMM相较于其线性对应物的优越表示能力，以及其在面部对齐和3D重建中的贡献。","领域":"面部重建/面部对齐/3D建模","问题":"传统3D可变形模型（3DMM）由于训练数据的类型和数量以及线性基的限制，其表示能力可能受到限制。","动机":"提高3D可变形模型（3DMM）的表示能力，以更好地应用于面部对齐和3D重建。","方法":"提出了一种创新框架，从大量无约束的面部图像中学习非线性3DMM模型，设计了一个新颖的解析可微渲染层来重建原始输入面部，整个网络仅需弱监督即可端到端训练。","关键词":["3D可变形模型","面部重建","面部对齐","3D建模"],"涉及的技术概念":"3D可变形模型（3DMM）是一种用于3D面部形状和纹理的统计模型，通过PCA基函数表示。本文提出的非线性3DMM模型通过神经网络编码器和解码器估计和映射形状和纹理参数，设计了一个解析可微渲染层用于面部重建。"},{"order":760,"title":"Through-Wall Human Pose Estimation Using Radio Signals","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.html","abstract":"This paper demonstrates accurate human pose estimation through walls and occlusions. We leverage the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body. We introduce a deep neural network approach that parses such radio signals to estimate 2D poses. Since humans cannot annotate radio signals, we use state-of-the-art vision model to provide cross-modal supervision. Specifically, during training the system uses synchronized wireless and visual inputs, extracts pose information from the visual stream, and uses it to guide the training process.  Once trained, the network uses only the wireless signal for pose estimation. We show that, when tested on visible scenes,  the radio-based system is almost as accurate as the vision-based system used to train it.  Yet, unlike vision-based pose estimation, the radio-based system can estimate 2D poses through walls despite never trained on such scenarios. Demo videos are available at our website (http://rfpose.csail.mit.edu).","中文标题":"使用无线电信号进行穿墙人体姿态估计","摘要翻译":"本文展示了通过墙壁和遮挡物进行准确的人体姿态估计。我们利用了WiFi频率的无线信号能够穿透墙壁并从人体反射回来的特性。我们引入了一种深度神经网络方法，该方法解析此类无线电信号以估计2D姿态。由于人类无法对无线电信号进行注释，我们使用最先进的视觉模型来提供跨模态监督。具体来说，在训练过程中，系统使用同步的无线和视觉输入，从视觉流中提取姿态信息，并利用它来指导训练过程。一旦训练完成，网络仅使用无线信号进行姿态估计。我们展示了，在可见场景下测试时，基于无线电的系统几乎与用于训练它的基于视觉的系统一样准确。然而，与基于视觉的姿态估计不同，基于无线电的系统可以从未训练过的场景中通过墙壁估计2D姿态。演示视频可在我们的网站(http://rfpose.csail.mit.edu)上获取。","领域":"无线传感/人体姿态估计/跨模态学习","问题":"在存在墙壁和遮挡物的情况下进行准确的人体姿态估计","动机":"利用无线信号穿透墙壁的特性，实现无需视觉信息即可进行人体姿态估计","方法":"引入深度神经网络方法解析无线电信号，利用视觉模型提供跨模态监督进行训练","关键词":["无线信号","2D姿态估计","跨模态监督"],"涉及的技术概念":"深度神经网络用于解析无线电信号，跨模态监督利用视觉信息指导无线信号的训练过程，实现无需视觉信息即可进行人体姿态估计"},{"order":761,"title":"What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_What_Makes_a_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_What_Makes_a_CVPR_2018_paper.html","abstract":"The ability to capture temporal information has been critical to the development of video understanding models. While there have been numerous attempts at modeling motion in videos, an explicit analysis of the effect of temporal information for video understanding is still missing. In this work, we aim to bridge this gap and ask the following question: How important is the motion in the video for recognizing the action? To this end, we propose two novel frameworks: (i) class-agnostic temporal generator and (ii) motion-invariant frame selector to reduce/remove motion for an ablation analysis without introducing other artifacts. This isolates the analysis of motion from other aspects of the video. The proposed frameworks provide a much tighter estimate of the effect of motion (from 25% to 6% on UCF101 and 15% to 5% on Kinetics) compared to baselines in our analysis. Our analysis provides critical insights about existing models like C3D, and how it could be made to achieve comparable results with a sparser set of frames.","中文标题":"视频之所以为视频：分析视频理解模型和数据集中的时间信息","摘要翻译":"捕捉时间信息的能力对于视频理解模型的发展至关重要。尽管已经有许多尝试在视频中建模运动，但对于时间信息对视频理解影响的明确分析仍然缺失。在这项工作中，我们旨在填补这一空白，并提出以下问题：视频中的运动对于识别动作有多重要？为此，我们提出了两个新颖的框架：（i）类别无关的时间生成器和（ii）运动不变帧选择器，以减少/移除运动进行消融分析，而不引入其他伪影。这将运动分析从视频的其他方面中隔离出来。与我们的分析中的基线相比，所提出的框架提供了对运动影响的更紧密估计（在UCF101上从25%到6%，在Kinetics上从15%到5%）。我们的分析提供了关于现有模型（如C3D）的关键见解，以及如何通过更稀疏的帧集实现可比的结果。","领域":"视频理解/动作识别/时间序列分析","问题":"视频中的运动对于识别动作的重要性","动机":"填补时间信息对视频理解影响分析的空白","方法":"提出了类别无关的时间生成器和运动不变帧选择器两个框架，以减少/移除运动进行消融分析","关键词":["视频理解","动作识别","时间序列分析"],"涉及的技术概念":"时间信息捕捉、运动建模、消融分析、类别无关的时间生成器、运动不变帧选择器、UCF101、Kinetics、C3D模型"},{"order":762,"title":"Fast Video Object Segmentation by Reference-Guided Mask Propagation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Oh_Fast_Video_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Oh_Fast_Video_Object_CVPR_2018_paper.html","abstract":"We present an efficient method for the semi-supervised video object segmentation. Our method achieves accuracy competitive with state-of-the-art methods while running in a fraction of time compared to others. To this end, we propose a deep Siamese encoder-decoder network that is designed to take advantage of mask propagation and object detection while avoiding the weaknesses of both approaches. Our network, learned through a two-stage training process that exploits both synthetic and real data, works robustly without any online learning or post-processing. We validate our method on four benchmark sets that cover single and multiple object segmentation. On all the benchmark sets, our method shows comparable accuracy while having the order of magnitude faster runtime. We also provide extensive ablation and add-on studies to analyze and evaluate our framework.","中文标题":"通过参考引导的掩码传播实现快速视频对象分割","摘要翻译":"我们提出了一种高效的半监督视频对象分割方法。我们的方法在达到与最先进方法相竞争的准确度的同时，运行时间仅为其他方法的一小部分。为此，我们提出了一种深度Siamese编码器-解码器网络，该网络旨在利用掩码传播和对象检测的优势，同时避免这两种方法的弱点。我们的网络通过利用合成和真实数据的两阶段训练过程学习，无需任何在线学习或后处理即可稳健工作。我们在涵盖单对象和多对象分割的四个基准集上验证了我们的方法。在所有基准集上，我们的方法显示出相当的准确度，同时运行速度快了一个数量级。我们还提供了广泛的消融和附加研究，以分析和评估我们的框架。","领域":"视频对象分割/深度学习/计算机视觉","问题":"半监督视频对象分割的效率问题","动机":"提高视频对象分割的效率和准确度，减少运行时间","方法":"提出了一种深度Siamese编码器-解码器网络，利用掩码传播和对象检测的优势，通过两阶段训练过程学习","关键词":["视频对象分割","Siamese网络","掩码传播","对象检测","两阶段训练"],"涉及的技术概念":{"半监督视频对象分割":"一种视频处理技术，旨在从视频序列中分割出特定对象，仅使用少量标注数据","Siamese编码器-解码器网络":"一种神经网络架构，用于比较两个输入，常用于图像匹配和对象识别","掩码传播":"一种技术，通过视频帧之间的信息传播来生成对象的分割掩码","对象检测":"识别图像或视频中的对象并确定其位置的过程","两阶段训练":"一种训练策略，首先在合成数据上训练模型，然后在真实数据上进行微调"}},{"order":763,"title":"NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.html","abstract":"Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbi-based loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods.","中文标题":"神经网络-维特比：弱监督视频学习框架","摘要翻译":"视频学习是计算机视觉中的一项重要任务，近年来受到了越来越多的关注。由于即使是少量的视频也很容易包含数百万帧，因此不依赖于帧级注释的方法尤为重要。在这项工作中，我们提出了一种新颖的学习算法，该算法采用基于维特比的损失，允许对弱注释视频数据进行在线和增量学习。此外，我们展示了显式上下文和长度建模在视频分割和标注任务中带来了巨大的改进，并将这些模型纳入我们的框架中。在几个动作分割基准上，我们相比当前最先进的方法获得了高达10%的改进。","领域":"视频分割/动作识别/弱监督学习","问题":"解决视频学习中不依赖帧级注释的弱监督学习问题","动机":"由于视频数据量大，帧级注释成本高，因此需要开发不依赖帧级注释的视频学习方法","方法":"提出了一种基于维特比损失的学习算法，支持在线和增量学习，并引入了显式上下文和长度模型以提高视频分割和标注的性能","关键词":["视频分割","动作识别","弱监督学习"],"涉及的技术概念":"维特比算法是一种动态规划算法，用于寻找最可能的隐藏状态序列。在本研究中，它被用于构建损失函数，以支持弱监督视频学习。显式上下文和长度建模指的是在模型训练过程中，明确考虑视频片段的上下文信息和长度信息，以提高模型在视频分割和标注任务上的表现。"},{"order":764,"title":"Actor and Observer: Joint Modeling of First and Third-Person Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sigurdsson_Actor_and_Observer_CVPR_2018_paper.html","abstract":"Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos, involving 112 people, with 4000 paired videos. This enables learning the link between the two, actor and observer perspectives. Thereby, we address one of the biggest bottlenecks facing egocentric vision research, providing a link from first-person to the abundant third-person data on the web. We use this data to learn a joint representation of first and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the first-person domain.","中文标题":"演员与观察者：第一人称与第三人称视频的联合建模","摘要翻译":"认知神经科学中的几种理论表明，当人们与世界互动或模拟互动时，他们是从第一人称的自我中心视角进行的，并且能够在第三人称（观察者）和第一人称（演员）之间无缝转移知识。尽管如此，由于缺乏数据，学习这样的人类行为识别模型一直未能实现。本文朝着这个方向迈出了一步，引入了Charades-Ego，这是一个大规模的第一人称和第三人称配对视频数据集，涉及112人，包含4000对视频。这使得学习两者之间的联系，即演员和观察者的视角成为可能。因此，我们解决了自我中心视觉研究面临的最大瓶颈之一，提供了从第一人称到网络上丰富的第三人称数据的链接。我们利用这些数据学习第一人称和第三人称视频的联合表示，仅使用弱监督，并展示了其在将知识从第三人称领域转移到第一人称领域的有效性。","领域":"自我中心视觉/行为识别/视频理解","问题":"缺乏数据导致无法学习人类行为识别模型，特别是第一人称和第三人称视频之间的联系。","动机":"解决自我中心视觉研究中最大的瓶颈之一，即从第一人称到第三人称数据的知识转移。","方法":"引入Charades-Ego数据集，学习第一人称和第三人称视频的联合表示，仅使用弱监督。","关键词":["自我中心视觉","行为识别","视频理解"],"涉及的技术概念":"第一人称视角（自我中心视角）、第三人称视角（观察者视角）、人类行为识别、视频数据集、联合表示学习、知识转移、弱监督学习。"},{"order":765,"title":"HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_HSA-RNN_Hierarchical_Structure-Adaptive_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhao_HSA-RNN_Hierarchical_Structure-Adaptive_CVPR_2018_paper.html","abstract":"Although video summarization has achieved great success in recent years, few approaches have realized the influence of video structure on the summarization results. As we know, the video data follow a hierarchical structure, i.e., a video is composed of shots, and a shot is composed of several frames. Generally, shots provide the activity-level information for people to understand the video content. While few existing summarization approaches pay attention to the shot segmentation procedure. They generate shots by some trivial strategies, such as fixed length segmentation, which may destroy the underlying hierarchical structure of video data and further reduce the quality of generated summaries. To address this problem, we propose a structure-adaptive video summarization approach that integrates shot segmentation and video summarization into a Hierarchical Structure-Adaptive RNN, denoted as HSA-RNN. We evaluate the proposed approach on four popular datasets, i.e., SumMe, TVsum, CoSum and VTW. The experimental results have demonstrated the effectiveness of HSA-RNN in the video summarization task.","中文标题":"HSA-RNN: 用于视频摘要的分层结构自适应RNN","摘要翻译":"尽管近年来视频摘要取得了巨大成功，但很少有方法意识到视频结构对摘要结果的影响。众所周知，视频数据遵循分层结构，即视频由镜头组成，镜头由若干帧组成。通常，镜头为人们理解视频内容提供了活动级别的信息。然而，现有的摘要方法很少关注镜头分割过程。它们通过一些简单的策略生成镜头，如固定长度分割，这可能会破坏视频数据的潜在分层结构，并进一步降低生成摘要的质量。为了解决这个问题，我们提出了一种结构自适应的视频摘要方法，将镜头分割和视频摘要集成到一个分层结构自适应RNN中，称为HSA-RNN。我们在四个流行的数据集上评估了所提出的方法，即SumMe、TVsum、CoSum和VTW。实验结果证明了HSA-RNN在视频摘要任务中的有效性。","领域":"视频摘要/视频分析/深度学习","问题":"现有视频摘要方法忽视视频结构对摘要结果的影响，特别是镜头分割过程。","动机":"提高视频摘要的质量，通过考虑视频的分层结构来生成更准确的摘要。","方法":"提出了一种结构自适应的视频摘要方法，将镜头分割和视频摘要集成到一个分层结构自适应RNN（HSA-RNN）中。","关键词":["视频摘要","分层结构","RNN","镜头分割","结构自适应"],"涉及的技术概念":"分层结构自适应RNN（HSA-RNN）是一种结合了镜头分割和视频摘要的深度学习模型，旨在通过考虑视频的分层结构来提高摘要的质量。镜头分割是指将视频分割成由若干帧组成的镜头，而视频摘要则是从视频中提取关键信息以生成简短的内容概述。"},{"order":766,"title":"Fast and Accurate Online Video Object Segmentation via Tracking Parts","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_Fast_and_Accurate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Cheng_Fast_and_Accurate_CVPR_2018_paper.html","abstract":"Online video object segmentation is a challenging task as it entails to process the image sequence timely and accurately. To segment a target object through the video, numerous CNN-based methods have been developed by heavily finetuning on the object mask in the first frame, which is time-consuming for online applications. In this paper, we propose a fast and accurate video object segmentation algorithm that can immediately start the segmentation process once receiving the images. We first utilize a part-based tracking method to deal with challenging factors such as large deformation, occlusion, and cluttered background. Based on the tracked bounding boxes of parts, we construct a region-of-interest segmentation network to generate part masks. Finally, a similarity-based scoring function is adopted to refine these object parts by comparing them to the visual information in the first frame. Our method performs favorably against state-of-the-art algorithms in accuracy on the DAVIS benchmark dataset, while achieving much faster runtime performance.","中文标题":"快速准确的在线视频对象分割通过跟踪部分","摘要翻译":"在线视频对象分割是一项具有挑战性的任务，因为它需要及时且准确地处理图像序列。为了通过视频分割目标对象，已经开发了许多基于CNN的方法，这些方法在第一帧的对象掩码上进行了大量的微调，这对于在线应用来说是耗时的。在本文中，我们提出了一种快速准确的视频对象分割算法，该算法一旦接收到图像就可以立即开始分割过程。我们首先利用基于部分的跟踪方法来处理诸如大变形、遮挡和杂乱背景等挑战性因素。基于跟踪到的部分边界框，我们构建了一个感兴趣区域分割网络以生成部分掩码。最后，采用基于相似性的评分函数，通过将这些对象部分与第一帧中的视觉信息进行比较来精炼这些对象部分。我们的方法在DAVIS基准数据集上的准确性优于最先进的算法，同时实现了更快的运行时性能。","领域":"视频对象分割/目标跟踪/图像分割","问题":"在线视频对象分割的及时性和准确性","动机":"解决现有基于CNN的方法在在线应用中的耗时问题，提高视频对象分割的速度和准确性","方法":"利用基于部分的跟踪方法处理挑战性因素，构建感兴趣区域分割网络生成部分掩码，采用基于相似性的评分函数精炼对象部分","关键词":["视频对象分割","目标跟踪","图像分割"],"涉及的技术概念":"CNN（卷积神经网络）：一种深度学习模型，用于图像识别和分割等任务。DAVIS基准数据集：一个用于视频对象分割的公开数据集，用于评估算法的性能。基于部分的跟踪方法：一种通过跟踪对象的各个部分来处理对象变形、遮挡等问题的技术。感兴趣区域分割网络：一种专门设计用于分割图像中特定区域的神经网络。基于相似性的评分函数：一种通过比较对象部分与参考图像中的视觉信息来精炼分割结果的方法。"},{"order":767,"title":"Now You Shake Me: Towards Automatic 4D Cinema","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Now_You_Shake_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Now_You_Shake_CVPR_2018_paper.html","abstract":"We are interested in enabling automatic 4D cinema by parsing physical and special effects from untrimmed movies. These include effects such as physical interactions, water splashing, light, and shaking, and are grounded to either a character in the scene or the camera. We collect a new dataset referred to as the Movie4D dataset which annotates over 9K effects in 63 movies. We propose a Conditional Random Field model atop a neural network that brings together visual and audio information, as well as semantics in the form of person tracks. Our model further exploits correlations of effects between different characters in the clip as well as across movie threads. We propose effect detection and classification as two tasks, and present results along with ablation studies on our dataset, paving the way towards 4D cinema in everyone’s homes.","中文标题":"现在你摇动我：迈向自动4D影院","摘要翻译":"我们致力于通过从未剪辑的电影中解析物理和特效来实现自动4D影院。这些效果包括物理互动、水花飞溅、光线和震动等，并且与场景中的角色或摄像机相关联。我们收集了一个新的数据集，称为Movie4D数据集，该数据集在63部电影中标注了超过9K个效果。我们提出了一个基于神经网络的条件随机场模型，该模型结合了视觉和音频信息，以及人物轨迹的语义信息。我们的模型进一步利用了不同角色之间以及电影线程之间效果的关联。我们提出了效果检测和分类作为两项任务，并在我们的数据集上展示了结果以及消融研究，为每个人家中的4D影院铺平了道路。","领域":"4D影院/效果检测/多媒体分析","问题":"如何从未剪辑的电影中自动解析物理和特效以实现4D影院","动机":"实现自动4D影院，使家庭影院体验更加丰富和沉浸","方法":"提出一个基于神经网络的条件随机场模型，结合视觉和音频信息以及人物轨迹的语义信息，利用不同角色之间以及电影线程之间效果的关联","关键词":["4D影院","效果检测","多媒体分析","条件随机场","神经网络"],"涉及的技术概念":"条件随机场模型是一种统计建模方法，用于结构化预测问题，如序列标记。在这里，它被用来结合视觉和音频信息以及人物轨迹的语义信息，以检测和分类电影中的物理和特效。"},{"order":768,"title":"Viewpoint-Aware Video Summarization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanehira_Viewpoint-Aware_Video_Summarization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kanehira_Viewpoint-Aware_Video_Summarization_CVPR_2018_paper.html","abstract":"This paper introduces a novel variant of video summarization, namely building a summary that depends on the particular aspect of a video the viewer focuses on. We refer to this as viewpoint. To infer what the desired viewpoint may be, we assume that several other videos are available, especially groups of videos, e.g., as folders on a person's phone or laptop. The semantic similarity between videos in a group vs. the dissimilarity between groups is used to produce viewpoint-specific summaries. For considering similarity as well as avoiding redundancy, output summary should be (A) diverse, (B) representative of videos in the same group, and (C) discriminative against videos in the different groups. To satisfy these requirements (A)-(C) simultaneously, we proposed a novel video summarization method from multiple groups of videos. Inspired by Fisher's discriminant criteria, it selects summary by optimizing the combination of three terms (a) inner-summary, (b) inner-group, and (c) between-group variances defined on the feature representation of summary, which can simply represent (A)-(C). Moreover, we developed a novel dataset to investigate how well the generated summary reflects the underlying viewpoint. Quantitative and qualitative experiments conducted on the dataset demonstrate the effectiveness of proposed method.","中文标题":"视角感知的视频摘要","摘要翻译":"本文介绍了一种新颖的视频摘要变体，即构建一个依赖于观众关注的视频特定方面的摘要。我们将其称为视角。为了推断出所需的视角可能是什么，我们假设有其他几个视频可用，特别是视频组，例如个人手机或笔记本电脑上的文件夹。组内视频之间的语义相似性与组间视频的差异性被用来生成特定视角的摘要。为了考虑相似性并避免冗余，输出摘要应具备以下特点：(A) 多样性，(B) 代表同一组视频，(C) 区分不同组的视频。为了同时满足这些要求(A)-(C)，我们提出了一种从多组视频中生成视频摘要的新方法。受Fisher判别准则的启发，它通过优化三个项的组合来选择摘要：(a) 摘要内部，(b) 组内，和(c) 组间方差，这些方差定义在摘要的特征表示上，可以简单地代表(A)-(C)。此外，我们开发了一个新的数据集来研究生成的摘要如何反映潜在的视角。在该数据集上进行的定量和定性实验证明了所提出方法的有效性。","领域":"视频摘要/视角识别/语义分析","问题":"如何根据观众的特定视角生成视频摘要","动机":"为了提供更加个性化和相关的视频摘要，满足观众对特定视角内容的需求","方法":"提出了一种新颖的视频摘要方法，通过优化摘要内部、组内和组间方差的组合来选择摘要，以反映观众的特定视角","关键词":["视频摘要","视角识别","语义分析","Fisher判别准则","特征表示"],"涉及的技术概念":"视频摘要是指从视频中提取关键帧或片段以形成视频的简短概述。视角识别涉及确定观众关注的视频特定方面。语义分析用于评估视频内容的相似性和差异性。Fisher判别准则是一种统计方法，用于区分不同类别的数据。特征表示是指将视频内容转换为可用于分析和比较的数值或向量形式。"},{"order":769,"title":"Photometric Stereo in Participating Media Considering Shape-Dependent Forward Scatter","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fujimura_Photometric_Stereo_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fujimura_Photometric_Stereo_in_CVPR_2018_paper.html","abstract":"Images captured in participating media such as murky water, fog, or smoke are degraded by scattered light. Thus, the use of traditional three-dimensional (3D) reconstruction techniques in such environments is difficult. In this paper, we propose a photometric stereo method for participating media. The proposed method differs from previous studies with respect to modeling shape-dependent forward scatter. In the proposed model, forward scatter is described as an analytical form using lookup tables and is represented by spatially-variant kernels. We also propose an approximation of a large-scale dense matrix as a sparse matrix, which enables the removal of forward scatter. Experiments with real and synthesized data demonstrate that the proposed method improves 3D reconstruction in participating media.","中文标题":"考虑形状依赖前向散射的参与介质中的光度立体视觉","摘要翻译":"在浑浊的水、雾或烟雾等参与介质中捕获的图像由于散射光而退化。因此，在这样的环境中使用传统的三维（3D）重建技术是困难的。在本文中，我们提出了一种用于参与介质的光度立体视觉方法。所提出的方法在建模形状依赖的前向散射方面与以往的研究不同。在所提出的模型中，前向散射被描述为使用查找表的解析形式，并由空间变核表示。我们还提出了一种将大规模密集矩阵近似为稀疏矩阵的方法，这使得去除前向散射成为可能。使用真实和合成数据的实验表明，所提出的方法改善了参与介质中的3D重建。","领域":"光度立体视觉/三维重建/散射介质","问题":"在参与介质中捕获的图像由于散射光而退化，导致传统的三维重建技术难以应用。","动机":"为了改善在参与介质中的三维重建质量，需要一种能够有效处理形状依赖前向散射的方法。","方法":"提出了一种新的光度立体视觉方法，该方法通过使用查找表的解析形式和空间变核来描述前向散射，并提出了一种将大规模密集矩阵近似为稀疏矩阵的方法来去除前向散射。","关键词":["光度立体视觉","三维重建","散射介质","前向散射","稀疏矩阵"],"涉及的技术概念":"光度立体视觉是一种从不同光照条件下的图像中恢复物体表面形状的技术。参与介质指的是能够散射光线的介质，如浑浊的水、雾或烟雾。前向散射是指光线在传播过程中向前方散射的现象。稀疏矩阵是一种大部分元素为零的矩阵，适用于存储和处理大规模数据。"},{"order":770,"title":"Direction-Aware Spatial Context Features for Shadow Detection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Direction-Aware_Spatial_Context_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Direction-Aware_Spatial_Context_CVPR_2018_paper.html","abstract":"Shadow detection is a fundamental and challenging task, since it requires an understanding of global image semantics and there are various backgrounds around shadows. This paper presents a novel network for shadow detection by analyzing image context in a direction-aware manner. To achieve this, we first formulate the direction-aware attention mechanism in a spatial recurrent neural network (RNN) by introducing attention weights when aggregating spatial context features in the RNN. By learning these weights through training, we can recover direction-aware spatial context (DSC) for detecting shadows. This design is developed into the DSC module and embedded in a CNN to learn DSC features at different levels. Moreover, a weighted cross entropy loss is designed to make the training more effective. We employ two common shadow detection benchmark datasets and perform various experiments to evaluate our network. Experimental results show that our network outperforms state-of-the-art methods and achieves 97% accuracy and 38% reduction on balance error rate.","中文标题":"方向感知的空间上下文特征用于阴影检测","摘要翻译":"阴影检测是一项基础且具有挑战性的任务，因为它需要理解图像的全局语义，并且阴影周围有各种背景。本文通过以方向感知的方式分析图像上下文，提出了一种新颖的阴影检测网络。为此，我们首先在空间循环神经网络（RNN）中通过引入注意力权重来制定方向感知的注意力机制，在RNN中聚合空间上下文特征时使用这些权重。通过训练学习这些权重，我们可以恢复方向感知的空间上下文（DSC）以检测阴影。这一设计被发展为DSC模块，并嵌入到CNN中以学习不同层次的DSC特征。此外，设计了一种加权交叉熵损失以使训练更加有效。我们使用了两个常见的阴影检测基准数据集，并进行了各种实验来评估我们的网络。实验结果表明，我们的网络优于最先进的方法，并实现了97%的准确率和38%的平衡错误率降低。","领域":"阴影检测/图像语义理解/注意力机制","问题":"阴影检测","动机":"阴影检测需要理解图像的全局语义，并且阴影周围有各种背景，这是一项具有挑战性的任务。","方法":"提出了一种新颖的网络，通过以方向感知的方式分析图像上下文，使用空间循环神经网络（RNN）中的方向感知注意力机制来聚合空间上下文特征，并嵌入到CNN中以学习不同层次的DSC特征。","关键词":["阴影检测","方向感知","空间上下文特征","注意力机制","加权交叉熵损失"],"涉及的技术概念":"方向感知的注意力机制、空间循环神经网络（RNN）、卷积神经网络（CNN）、加权交叉熵损失"},{"order":771,"title":"Discriminative Learning of Latent Features for Zero-Shot Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Discriminative_Learning_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Discriminative_Learning_of_CVPR_2018_paper.html","abstract":"Zero-shot learning (ZSL) aims to recognize unseen image categories by learning an embedding space between image and semantic representations. For years, among existing works, it has been the center task to learn the proper mapping matrices aligning the visual and semantic space, whilst the importance to learn discriminative representations for ZSL is ignored. In this work, we retrospect existing methods and demonstrate the necessity to learn discriminative representations for both visual and semantic instances of ZSL. We propose an end-to-end network that is capable of 1) automatically discovering discriminative regions by a zoom network; and 2) learning discriminative semantic representations in an augmented space introduced for both user-defined and latent attributes. Our proposed method is tested extensively on two challenging ZSL datasets, and the experiment results show that the proposed method significantly outperforms state-of-the-art methods.","中文标题":"潜在特征的判别学习用于零样本识别","摘要翻译":"零样本学习（ZSL）旨在通过学习图像和语义表示之间的嵌入空间来识别未见过的图像类别。多年来，在现有的工作中，学习适当的映射矩阵以对齐视觉和语义空间一直是中心任务，而学习判别表示对于ZSL的重要性被忽视了。在这项工作中，我们回顾了现有方法，并证明了为ZSL的视觉和语义实例学习判别表示的必要性。我们提出了一个端到端的网络，该网络能够：1）通过缩放网络自动发现判别区域；2）在引入的增强空间中学习判别语义表示，该空间适用于用户定义和潜在属性。我们提出的方法在两个具有挑战性的ZSL数据集上进行了广泛测试，实验结果表明，所提出的方法显著优于最先进的方法。","领域":"零样本学习/图像识别/语义表示","问题":"如何在零样本学习中学习判别性的视觉和语义表示","动机":"现有方法忽视了学习判别表示对于零样本学习的重要性","方法":"提出一个端到端的网络，通过缩放网络自动发现判别区域，并在增强空间中学习判别语义表示","关键词":["零样本学习","判别表示","缩放网络","增强空间"],"涉及的技术概念":"零样本学习（ZSL）是一种机器学习方法，旨在识别训练数据中未出现的类别。嵌入空间是指将图像和语义表示映射到一个共同的空间中，以便于比较和识别。判别表示指的是能够有效区分不同类别的特征表示。缩放网络是一种能够自动识别图像中关键区域的网络结构。增强空间是指通过引入额外信息（如用户定义和潜在属性）来扩展语义表示的空间。"},{"order":772,"title":"Learning to Adapt Structured Output Space for Semantic Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tsai_Learning_to_Adapt_CVPR_2018_paper.html","abstract":"Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. To further improve our method, we utilize multi-level output adaptation based on feature maps at different levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.","中文标题":"学习适应结构化输出空间以进行语义分割","摘要翻译":"基于卷积神经网络的语义分割方法依赖于像素级真实标签的监督，但可能无法很好地泛化到未见过的图像域。由于标注过程既繁琐又劳动密集，开发能够将源真实标签适应到目标域的算法具有重要意义。在本文中，我们提出了一种用于语义分割领域适应的对抗学习方法。考虑到语义分割作为包含源域和目标域之间空间相似性的结构化输出，我们在输出空间采用了对抗学习。为了进一步增强适应模型，我们构建了一个多层次对抗网络，以在不同特征层次上有效地执行输出空间领域适应。为了进一步改进我们的方法，我们利用了基于不同层次特征图的多层次输出适应。在各种领域适应设置下进行了广泛的实验和消融研究，包括合成到真实和跨城市场景。我们展示了所提出的方法在准确性和视觉质量方面优于最先进的方法。","领域":"语义分割/领域适应/对抗学习","问题":"如何使基于卷积神经网络的语义分割方法更好地泛化到未见过的图像域","动机":"由于像素级真实标签的标注过程既繁琐又劳动密集，开发能够将源真实标签适应到目标域的算法具有重要意义","方法":"提出了一种用于语义分割领域适应的对抗学习方法，构建多层次对抗网络以在不同特征层次上执行输出空间领域适应，并利用基于不同层次特征图的多层次输出适应","关键词":["语义分割","领域适应","对抗学习","多层次对抗网络","输出空间适应"],"涉及的技术概念":"卷积神经网络（CNN）用于语义分割，对抗学习用于领域适应，多层次对抗网络用于增强模型适应性，多层次输出适应基于不同层次的特征图。"},{"order":773,"title":"Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.html","abstract":"Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.","中文标题":"使用不确定性权衡场景几何和语义损失的多任务学习","摘要翻译":"许多深度学习应用受益于具有多个回归和分类目标的多任务学习。在本文中，我们观察到，这类系统的性能强烈依赖于每个任务损失之间的相对权重。手动调整这些权重是一个困难且昂贵的过程，使得多任务学习在实践中变得不可行。我们提出了一种原则性的多任务深度学习方法，通过考虑每个任务的同方差不确定性来权衡多个损失函数。这使我们能够在分类和回归设置中同时学习具有不同单位或尺度的各种量。我们展示了我们的模型从单眼输入图像中学习每像素深度回归、语义分割和实例分割。也许令人惊讶的是，我们展示了我们的模型可以学习多任务权重，并且在每个任务上单独训练的模型上表现更优。","领域":"场景理解/语义分割/深度估计","问题":"多任务学习中各任务损失权重的自动调整","动机":"手动调整多任务学习中各任务损失权重困难且昂贵，限制了多任务学习的实际应用","方法":"提出一种基于同方差不确定性的多任务深度学习方法，自动权衡多个损失函数","关键词":["多任务学习","同方差不确定性","损失权重","语义分割","深度估计"],"涉及的技术概念":"多任务学习指的是同时学习多个相关任务以提高学习效率和预测性能。同方差不确定性是指模型预测的不确定性，与输入数据无关。损失权重在多任务学习中用于平衡不同任务对模型训练的影响。语义分割是将图像分割成多个区域，每个区域对应一个语义类别。深度估计是从单眼图像中估计每个像素的深度信息。"},{"order":774,"title":"Jointly Localizing and Describing Events for Dense Video Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Jointly_Localizing_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Jointly_Localizing_and_CVPR_2018_paper.html","abstract":"Automatically describing a video with natural language is regarded as a fundamental challenge in computer vision. The problem nevertheless is not trivial especially when a video contains multiple events to be worthy of mention, which often happens in real videos. A valid question is how to temporally localize and then describe events, which is known as \`\`dense video captioning.\\" In this paper, we present a novel framework for dense video captioning that unifies the localization of temporal event proposals and sentence generation of each proposal, by jointly training them in an end-to-end manner. To combine these two worlds, we integrate a new design, namely descriptiveness regression, into a single shot detection structure to infer the descriptive complexity of each detected proposal via sentence generation. This in turn adjusts the temporal locations of each event proposal. Our model differs from existing dense video captioning methods since we propose a joint and global optimization of detection and captioning, and the framework uniquely capitalizes on an attribute-augmented video captioning architecture. Extensive experiments are conducted on ActivityNet Captions dataset and our framework shows clear improvements when compared to the state-of-the-art techniques. More remarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions official test set.","中文标题":"联合定位和描述事件以实现密集视频字幕","摘要翻译":"自动用自然语言描述视频被视为计算机视觉中的一个基本挑战。然而，当视频包含多个值得提及的事件时，这个问题并不简单，这在真实视频中经常发生。一个有效的问题是如何在时间上定位然后描述事件，这被称为“密集视频字幕”。在本文中，我们提出了一个新颖的框架，用于密集视频字幕，该框架通过联合训练时间事件提议的定位和每个提议的句子生成，以端到端的方式统一它们。为了结合这两个方面，我们将一个新设计，即描述性回归，集成到单次检测结构中，以通过句子生成推断每个检测到的提议的描述复杂性。这反过来调整每个事件提议的时间位置。我们的模型与现有的密集视频字幕方法不同，因为我们提出了检测和字幕的联合和全局优化，并且该框架独特地利用了属性增强的视频字幕架构。在ActivityNet Captions数据集上进行了广泛的实验，与最先进的技术相比，我们的框架显示出明显的改进。更值得注意的是，我们在ActivityNet Captions官方测试集上获得了新的记录：METEOR为12.96%。","领域":"视频字幕/事件检测/自然语言生成","问题":"如何在视频中同时定位和描述多个事件","动机":"解决视频中包含多个值得提及的事件时，自动生成描述性字幕的挑战","方法":"提出一个新颖的框架，通过联合训练时间事件提议的定位和句子生成，采用描述性回归和单次检测结构，实现端到端的密集视频字幕","关键词":["密集视频字幕","事件检测","自然语言生成"],"涉及的技术概念":"描述性回归是一种新设计，用于通过句子生成推断每个检测到的提议的描述复杂性，从而调整事件提议的时间位置。单次检测结构用于同时检测视频中的多个事件。属性增强的视频字幕架构用于提高字幕生成的准确性和描述性。"},{"order":775,"title":"Going From Image to Video Saliency: Augmenting Image Salience With Dynamic Attentional Push","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gorji_Going_From_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gorji_Going_From_Image_CVPR_2018_paper.html","abstract":"We present a novel method to incorporate the recent advent in static saliency models to predict the saliency in videos.  Our model augments the static saliency models with the Attentional Push effect of the photographer and the scene actors in a shared attention setting.  We demonstrate that not only it is imperative to use static Attentional Push cues, noticeable performance improvement is achievable by learning the time-varying nature of Attentional Push.  We propose a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) structure which augments state-of-the-art in static saliency models with dynamic Attentional Push. Our network contains four pathways, a saliency pathway and three Attentional Push pathways.  The multi-pathway structure is followed by an augmenting convnet that learns to combine the complementary and time-varying outputs of the ConvLSTMs by minimizing the relative entropy between the augmented saliency and viewers fixation patterns on videos. We evaluate our model by comparing the performance of several augmented static saliency models with state-of-the-art in spatiotemporal saliency on three largest dynamic eye tracking datasets, HOLLYWOOD2, UCF-Sport and DIEM. Experimental results illustrates that solid performance gain is achievable using the proposed methodology.","中文标题":"从图像显著性到视频显著性：利用动态注意力推动增强图像显著性","摘要翻译":"我们提出了一种新颖的方法，将静态显著性模型的最新进展应用于视频显著性的预测。我们的模型通过摄影师和场景演员在共享注意力设置中的注意力推动效应来增强静态显著性模型。我们证明，不仅使用静态注意力推动线索是必要的，通过学习注意力推动的时间变化性质，还可以实现显著的性能提升。我们提出了一种多流卷积长短期记忆网络（ConvLSTM）结构，该结构通过动态注意力推动增强了最先进的静态显著性模型。我们的网络包含四个路径，一个显著性路径和三个注意力推动路径。多路径结构之后是一个增强卷积网络，它通过学习最小化增强显著性与观众在视频上的注视模式之间的相对熵来结合ConvLSTMs的互补和时间变化输出。我们通过比较几种增强静态显著性模型与最先进的时空显著性模型在三个最大的动态眼动数据集HOLLYWOOD2、UCF-Sport和DIEM上的表现来评估我们的模型。实验结果表明，使用所提出的方法可以实现显著的性能提升。","领域":"视频显著性检测/注意力机制/眼动追踪","问题":"如何有效地将静态显著性模型应用于视频显著性预测","动机":"探索利用动态注意力推动效应增强静态显著性模型，以提高视频显著性预测的准确性","方法":"提出了一种多流卷积长短期记忆网络（ConvLSTM）结构，通过动态注意力推动增强静态显著性模型，并采用增强卷积网络结合ConvLSTMs的输出","关键词":["视频显著性检测","注意力机制","眼动追踪","ConvLSTM","相对熵"],"涉及的技术概念":"静态显著性模型、动态注意力推动、多流卷积长短期记忆网络（ConvLSTM）、增强卷积网络、相对熵、眼动追踪数据集"},{"order":776,"title":"M3: Multimodal Memory Modelling for Video Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_M3_Multimodal_Memory_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_M3_Multimodal_Memory_CVPR_2018_paper.html","abstract":"Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress. However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to long-term sequential problems [35] and working memory is the key factor of visual attention [33], we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to [10], the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the state-of-the-art methods in terms of BLEU and METEOR.","中文标题":"M3：用于视频字幕的多模态记忆建模","摘要翻译":"视频字幕自动将视频片段翻译成自然语言句子，是计算机视觉中一项非常重要的任务。得益于最近的深度学习技术，视频字幕取得了巨大进展。然而，由于长期多模态依赖建模和语义对齐问题，从视觉序列空间到语言空间的有效映射仍然是一个具有挑战性的问题。受到记忆建模对长期序列问题具有潜在优势[35]以及工作记忆是视觉注意力的关键因素[33]的启发，我们提出了一种多模态记忆模型（M3）来描述视频，该模型构建了一个视觉和文本共享的记忆，以建模长期的视觉-文本依赖，并进一步指导视觉注意力在描述的视觉目标上，以解决视觉-文本对齐问题。具体来说，类似于[10]，提出的M3附加了一个外部记忆，通过多次读写操作与视频和句子交互，以存储和检索视觉和文本内容。为了评估所提出的模型，我们在两个公共数据集上进行了实验：MSVD和MSR-VTT。实验结果表明，我们的方法在BLEU和METEOR方面优于大多数最先进的方法。","领域":"视频字幕/多模态学习/记忆网络","问题":"从视觉序列空间到语言空间的有效映射，特别是长期多模态依赖建模和语义对齐问题。","动机":"记忆建模对长期序列问题具有潜在优势，工作记忆是视觉注意力的关键因素。","方法":"提出了一种多模态记忆模型（M3），构建视觉和文本共享的记忆，以建模长期的视觉-文本依赖，并进一步指导视觉注意力在描述的视觉目标上，以解决视觉-文本对齐问题。","关键词":["视频字幕","多模态学习","记忆网络"],"涉及的技术概念":"多模态记忆模型（M3）通过构建视觉和文本共享的记忆，利用多次读写操作与视频和句子交互，以存储和检索视觉和文本内容，从而解决视觉-文本对齐问题。"},{"order":777,"title":"Emotional Attention: A Study of Image Sentiment and Visual Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Emotional_Attention_A_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Emotional_Attention_A_CVPR_2018_paper.html","abstract":"Image sentiment influences visual perception. Emotion-eliciting stimuli such as happy faces and poisonous snakes are generally prioritized in human attention. However, little research has evaluated the interrelationships of image sentiment and visual saliency. In this paper, we present the first study to focus on the relation between emotional properties of an image and visual attention. We first create the EMOtional attention dataset (EMOd). It is a diverse set of emotion-eliciting images, and each image has (1) eye-tracking data collected from 16 subjects, (2) intensive image context labels including object contour, object sentiment, object semantic category, and high-level perceptual attributes such as image aesthetics and elicited emotions. We perform extensive analyses on EMOd to identify how image sentiment relates to human attention. We discover an emotion prioritization effect: for our images, emotion-eliciting content attracts human attention strongly, but such advantage diminishes dramatically after initial fixation. Aiming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context of the image scene. The proposed network outperforms the state-of-the-art on three benchmark datasets, by effectively capturing the relative importance of human attention within an image. The code, models, and dataset are available online at https://nus-sesame.top/emotionalattention/.","中文标题":"情感注意力：图像情感与视觉注意力的研究","摘要翻译":"图像情感影响视觉感知。通常，引发情感的刺激物，如笑脸和毒蛇，在人类注意力中会被优先处理。然而，关于图像情感与视觉显著性之间相互关系的研究却很少。在本文中，我们首次专注于研究图像的情感属性与视觉注意力之间的关系。我们首先创建了情感注意力数据集（EMOd）。这是一个多样化的引发情感的图像集，每张图像都有（1）从16名受试者收集的眼动数据，（2）密集的图像上下文标签，包括物体轮廓、物体情感、物体语义类别以及高级感知属性，如图像美学和引发的情感。我们对EMOd进行了广泛的分析，以确定图像情感如何与人类注意力相关。我们发现了一种情感优先效应：对于我们的图像，引发情感的内容强烈吸引人类注意力，但这种优势在初始注视后急剧减弱。为了计算模拟人类情感优先效应，我们设计了一个用于显著性预测的深度神经网络，其中包括一个新颖的子网络，该子网络学习图像场景的空间和语义上下文。所提出的网络在三个基准数据集上优于现有技术，通过有效捕捉图像中人类注意力的相对重要性。代码、模型和数据集可在https://nus-sesame.top/emotionalattention/在线获取。","领域":"情感计算/视觉显著性/深度学习","问题":"图像情感与视觉显著性之间的相互关系","动机":"研究图像情感如何影响人类视觉注意力，以及如何计算模拟这种影响","方法":"创建情感注意力数据集（EMOd），设计一个深度神经网络用于显著性预测，包括一个学习图像场景空间和语义上下文的新颖子网络","关键词":["情感计算","视觉显著性","深度学习","眼动数据","图像情感"],"涉及的技术概念":"情感注意力数据集（EMOd）是一个包含眼动数据和密集图像上下文标签的多样化图像集。设计的深度神经网络包括一个子网络，用于学习图像场景的空间和语义上下文，以预测显著性。"},{"order":778,"title":"A Low Power, High Throughput, Fully Event-Based Stereo System","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Andreopoulos_A_Low_Power_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Andreopoulos_A_Low_Power_CVPR_2018_paper.html","abstract":"We introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-Neumann computation model, where no frames, arrays, or any other such data-structures are used. This is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatio-temporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. Using a cluster of TrueNorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by Dynamic Vision Sensors (DVS), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. Experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of DVS sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. System evaluation on event-based sequences demonstrates a ~200X improvement in terms of power per pixel per disparity map compared to the closest state-of-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection.","中文标题":"一种低功耗、高吞吐量、完全基于事件的立体系统","摘要翻译":"我们介绍了一种完全在基于事件的数字硬件上实现的立体对应系统，使用完全基于图的非冯·诺依曼计算模型，其中不使用帧、数组或任何其他此类数据结构。这是首次在基于事件的硬件上实现从图像采集和校正、多尺度时空立体对应、赢家通吃，到视差正则化的端到端立体管道。使用一组TrueNorth神经突触处理器，我们展示了它们处理由动态视觉传感器（DVS）实时流式传输的双边基于事件输入的能力，每秒最多可处理2,000个视差图，产生高保真视差，进而用于低功耗地重建快速变化场景中产生的事件的深度。在真实世界序列上的实验证明了该系统能够充分利用DVS传感器的异步和稀疏特性进行低功耗深度重建，在传统基于帧的相机连接到同步处理器对快速移动物体效率低下的环境中。基于事件序列的系统评估显示，与最接近的最新技术相比，每像素每视差图的功耗提高了约200倍，从尖峰注入到视差图弹出的最大延迟高达11毫秒。","领域":"立体视觉/事件相机/神经形态计算","问题":"实现低功耗、高吞吐量的完全基于事件的立体视觉系统","动机":"利用事件相机的异步和稀疏特性，以及神经形态计算模型，实现高效、低功耗的深度重建，特别是在处理快速移动物体时，传统基于帧的相机和同步处理器效率低下。","方法":"采用完全基于图的非冯·诺依曼计算模型，在TrueNorth神经突触处理器上实现端到端的立体视觉管道，包括图像采集和校正、多尺度时空立体对应、赢家通吃和视差正则化。","关键词":["立体视觉","事件相机","神经形态计算","TrueNorth处理器","动态视觉传感器"],"涉及的技术概念":{"事件相机":"一种能够捕捉场景中亮度变化的相机，输出的是异步的事件流，而不是传统的帧图像。","神经形态计算":"模仿生物神经系统工作原理的计算模型，通常用于处理异步和稀疏的数据。","TrueNorth处理器":"IBM开发的一种神经突触处理器，专为神经形态计算设计，能够高效处理大规模并行和异步的计算任务。","动态视觉传感器（DVS）":"一种特殊的事件相机，能够以极高的时间分辨率捕捉场景中的亮度变化。"}},{"order":779,"title":"VITON: An Image-Based Virtual Try-On Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Han_VITON_An_Image-Based_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Han_VITON_An_Image-Based_CVPR_2018_paper.html","abstract":"We present an image-based VIirtual Try-On Network (VITON) without using 3D information in any form, which seamlessly transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. Conditioned upon a new clothing-agnostic yet descriptive person representation, our framework first generates a coarse synthesized image with the target clothing item overlaid on that same person in the same pose. We further enhance the initial blurry clothing area with a refinement network. The network is trained to learn how much detail to utilize from the target clothing item, and where to apply to the person in order to synthesize a photo-realistic image in which the target item deforms naturally with clear visual patterns. Experiments on our newly collected Zalando dataset demonstrate its promise in the image-based virtual try-on task over state-of-the-art generative models.","中文标题":"VITON: 基于图像的虚拟试穿网络","摘要翻译":"我们提出了一种不依赖任何形式3D信息的基于图像的虚拟试穿网络（VITON），该网络采用从粗到细的策略，将所需的服装项目无缝转移到人物的相应区域。基于一种新的与服装无关但具有描述性的人物表示，我们的框架首先生成一个粗略的合成图像，其中目标服装项目覆盖在同一姿势的同一人物上。我们通过一个细化网络进一步增强初始模糊的服装区域。该网络被训练以学习从目标服装项目中利用多少细节，以及将其应用到人物的哪个位置，以合成一个照片般逼真的图像，其中目标项目自然变形，具有清晰的视觉图案。在我们新收集的Zalando数据集上的实验表明，在基于图像的虚拟试穿任务中，它优于最先进的生成模型。","领域":"虚拟试穿/图像合成/生成对抗网络","问题":"如何在不需要3D信息的情况下，实现基于图像的虚拟试穿，使服装自然变形并保持清晰的视觉图案。","动机":"为了提供一种更便捷、高效的虚拟试穿解决方案，使用户能够在没有3D信息的情况下，通过图像看到自己穿上不同服装的效果。","方法":"采用从粗到细的策略，首先生成一个粗略的合成图像，然后通过细化网络增强服装区域的细节，以合成逼真的试穿图像。","关键词":["虚拟试穿","图像合成","生成对抗网络"],"涉及的技术概念":"虚拟试穿网络（VITON）是一种基于图像的解决方案，它不依赖于3D信息，而是通过从粗到细的策略和细化网络来合成逼真的试穿图像。这种方法利用了与服装无关但具有描述性的人物表示，以及生成对抗网络（GAN）技术，以实现高质量的图像合成。"},{"order":780,"title":"Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lyu_Multi-Oriented_Scene_Text_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lyu_Multi-Oriented_Scene_Text_CVPR_2018_paper.html","abstract":"Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn’t need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84:3% on ICDAR2015 and 81:5% on MSRA-TD500.","中文标题":"通过角点定位和区域分割进行多方向场景文本检测","摘要翻译":"以往基于深度学习的先进场景文本检测方法大致可以分为两类。第一类将场景文本视为一种通用对象，并遵循通用对象检测范式，通过回归文本框位置来定位场景文本，但受到场景文本任意方向和大宽高比的困扰。第二类直接分割文本区域，但大多需要复杂的后处理。在本文中，我们提出了一种方法，结合了这两类方法的思想，同时避免了它们的缺点。我们提出通过定位文本边界框的角点和分割相对位置的文本区域来检测场景文本。在推理阶段，通过采样和分组角点生成候选框，这些候选框通过分割图进一步评分，并通过非极大值抑制（NMS）进行抑制。与之前的方法相比，我们的方法可以自然地处理长方向文本，并且不需要复杂的后处理。在ICDAR2013、ICDAR2015、MSRA-TD500、MLT和COCO-Text上的实验表明，所提出的算法在准确性和效率上都达到了更好或相当的结果。基于VGG16，它在ICDAR2015上达到了84.3%的F-measure，在MSRA-TD500上达到了81.5%。","领域":"场景文本检测/深度学习/图像分割","问题":"解决场景文本检测中任意方向和大宽高比文本的定位问题，以及减少复杂后处理的需求","动机":"结合两类现有方法的优点，避免它们的缺点，提高场景文本检测的准确性和效率","方法":"通过定位文本边界框的角点和分割相对位置的文本区域来检测场景文本，使用采样和分组角点生成候选框，并通过分割图评分和非极大值抑制（NMS）进行抑制","关键词":["场景文本检测","角点定位","区域分割","非极大值抑制"],"涉及的技术概念":"本文涉及的技术概念包括场景文本检测、角点定位、区域分割、非极大值抑制（NMS）以及使用VGG16网络进行特征提取。"},{"order":781,"title":"Multi-Content GAN for Few-Shot Font Style Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Azadi_Multi-Content_GAN_for_CVPR_2018_paper.html","abstract":"In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.","中文标题":"多内容生成对抗网络用于少样本字体风格迁移","摘要翻译":"在这项工作中，我们专注于从高度风格化的文本的部分观察中提取信息，并将这些观察推广以生成装饰字体中未观察到的字形。为了从极少的示例中生成一组遵循一致风格的多内容图像，我们提出了一种端到端的堆叠条件生成对抗网络模型，该模型沿通道考虑内容，沿网络层考虑风格。我们提出的网络将给定字形的风格迁移到未见过字形的内容上，捕捉现实世界中高度风格化的字体，如电影海报或信息图表上的字体。我们寻求迁移字体风格化（例如衬线和耳朵）以及文本风格化（例如颜色渐变和效果）。我们的实验基于我们收集的数据集，包括10,000种不同风格的字体，并展示了从极少数观察到的字形中有效推广的能力。","领域":"字体设计/风格迁移/生成对抗网络","问题":"如何从极少的示例中生成一组遵循一致风格的多内容图像","动机":"捕捉现实世界中高度风格化的字体，如电影海报或信息图表上的字体，并迁移字体风格化和文本风格化","方法":"提出了一种端到端的堆叠条件生成对抗网络模型，该模型沿通道考虑内容，沿网络层考虑风格","关键词":["字体设计","风格迁移","生成对抗网络"],"涉及的技术概念":"条件生成对抗网络（Conditional GAN）是一种生成模型，它通过条件信息来控制生成过程。在本研究中，条件信息包括内容（沿通道）和风格（沿网络层），使得模型能够从极少的示例中生成遵循特定风格的多内容图像。"},{"order":782,"title":"Audio to Body Dynamics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shlizerman_Audio_to_Body_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shlizerman_Audio_to_Body_CVPR_2018_paper.html","abstract":"We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation  of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio.  Notably, it's not  clear if body movement can be predicted from music at all and our aim in this work is to explore this possibility. In this paper, we present the first result that shows that natural body dynamics can be predicted.  We built  an LSTM network that is trained  on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation.","中文标题":"音频到身体动态","摘要翻译":"我们提出了一种方法，该方法以小提琴或钢琴演奏的音频作为输入，并输出骨架预测的视频，这些预测进一步用于动画化一个虚拟形象。关键思想是创建一个虚拟形象的动画，使其手的移动方式类似于钢琴家或小提琴家的动作，仅从音频出发。值得注意的是，目前尚不清楚是否可以从音乐中预测身体运动，我们在这项工作中的目标是探索这种可能性。在本文中，我们展示了第一个结果表明，自然的身体动态是可以预测的。我们构建了一个LSTM网络，该网络在互联网上传的小提琴和钢琴演奏视频上进行训练。预测的点被应用到装配好的虚拟形象上以创建动画。","领域":"动作捕捉/音频分析/虚拟现实","问题":"如何从音频中预测并生成与音乐演奏相对应的身体动态","动机":"探索是否可以从音乐中预测身体运动，并创建与音乐演奏相对应的虚拟形象动画","方法":"构建并训练一个LSTM网络，使用互联网上的小提琴和钢琴演奏视频作为训练数据，预测骨架点并应用于虚拟形象以生成动画","关键词":["动作捕捉","音频分析","虚拟现实","LSTM网络","虚拟形象动画"],"涉及的技术概念":"LSTM网络：一种特殊的递归神经网络，能够学习长期依赖信息，适用于处理和预测时间序列数据。骨架预测：从视频或图像中预测人体的关键点位置，用于动作捕捉和分析。虚拟形象动画：通过计算机图形学技术，将预测或设计的动作应用于虚拟形象，使其在虚拟环境中进行动画表现。"},{"order":783,"title":"Weakly Supervised Coupled Networks for Visual Sentiment Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_Weakly_Supervised_Coupled_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_Weakly_Supervised_Coupled_CVPR_2018_paper.html","abstract":"Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions on-line. In this paper, we solve the problem of visual sentiment analysis using the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image appearance. However, different image regions can have a different influence on the intended expression. This paper presents a weakly supervised coupled convolutional network with two branches to leverage the localized information. The first branch detects a sentiment specific soft map by training a fully convolutional network with the cross spatial pooling strategy, which only requires image-level labels, thereby significantly reducing the annotation burden. The second branch utilizes both the holistic and localized information by coupling the sentiment map with deep features for robust classification. We integrate the sentiment detection and classification branches into a unified deep framework and optimize the network in an end-to-end manner. Extensive experiments on six benchmark datasets demonstrate that the proposed method performs favorably against the state-ofthe-art methods for visual sentiment analysis.","中文标题":"弱监督耦合网络用于视觉情感分析","摘要翻译":"随着在线表达意见的趋势日益增长，从视觉内容自动评估情感已经获得了相当大的关注。在本文中，我们通过在识别过程中使用高级抽象来解决视觉情感分析的问题。现有的基于卷积神经网络的方法从整体图像外观学习情感表示。然而，不同的图像区域对预期表达的影响可能不同。本文提出了一种弱监督的耦合卷积网络，该网络有两个分支以利用局部信息。第一个分支通过使用跨空间池化策略训练全卷积网络来检测情感特定的软图，这只需要图像级标签，从而显著减少了注释负担。第二个分支通过将情感图与深度特征耦合来利用整体和局部信息进行鲁棒分类。我们将情感检测和分类分支集成到一个统一的深度框架中，并以端到端的方式优化网络。在六个基准数据集上的广泛实验表明，所提出的方法在视觉情感分析方面优于最先进的方法。","领域":"情感分析/卷积神经网络/图像分类","问题":"视觉内容中的情感自动评估","动机":"解决现有方法在视觉情感分析中无法充分利用不同图像区域对情感表达影响的问题","方法":"提出了一种弱监督的耦合卷积网络，该网络有两个分支：一个用于检测情感特定的软图，另一个用于利用整体和局部信息进行鲁棒分类，并将这两个分支集成到一个统一的深度框架中进行端到端优化","关键词":["情感分析","卷积神经网络","图像分类","弱监督学习","跨空间池化"],"涉及的技术概念":"卷积神经网络（CNN）用于从图像中学习特征，全卷积网络（FCN）用于生成情感特定的软图，跨空间池化策略用于减少注释负担，端到端优化用于提高模型性能"},{"order":784,"title":"Future Person Localization in First-Person Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yagi_Future_Person_Localization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yagi_Future_Person_Localization_CVPR_2018_paper.html","abstract":"We present a new task that predicts future locations of people observed in first-person videos. Consider a first-person video stream continuously recorded by a wearable camera. Given a short clip of a person that is extracted from the complete stream, we aim to predict that person's location in future frames. To facilitate this future person localization ability, we make the following three key observations: a) First-person videos typically involve significant ego-motion which greatly affects the location of the target person in future frames; b) Scales of the target person act as a salient cue to estimate a perspective effect in first-person videos; c) First-person videos often capture people up-close, making it easier to leverage target poses (e.g., where they look) for predicting their future locations. We incorporate these three observations into a prediction framework with a multi-stream convolution-deconvolution architecture. Experimental results reveal our method to be effective on our new dataset as well as on a public social interaction dataset.","中文标题":"第一人称视频中的未来人物定位","摘要翻译":"我们提出了一个新任务，即预测在第一人称视频中观察到的人的未来位置。考虑一个由可穿戴相机连续记录的第一人称视频流。给定从完整流中提取的一个人的短片段，我们的目标是预测该人在未来帧中的位置。为了促进这种未来人物定位能力，我们做出了以下三个关键观察：a) 第一人称视频通常涉及显著的自体运动，这极大地影响了目标人物在未来帧中的位置；b) 目标人物的尺度作为估计第一人称视频中透视效果的一个显著线索；c) 第一人称视频经常近距离捕捉人物，使得利用目标姿势（例如，他们看哪里）来预测他们的未来位置变得更容易。我们将这三个观察结果整合到一个具有多流卷积-反卷积架构的预测框架中。实验结果表明，我们的方法在我们新的数据集以及一个公共社交互动数据集上都是有效的。","领域":"视频分析/人物定位/深度学习","问题":"预测在第一人称视频中观察到的人的未来位置","动机":"为了促进未来人物定位能力，解决第一人称视频中由于自体运动、人物尺度和近距离捕捉等因素带来的挑战","方法":"采用多流卷积-反卷积架构的预测框架，整合了自体运动、人物尺度和目标姿势三个关键观察结果","关键词":["第一人称视频","人物定位","卷积-反卷积架构"],"涉及的技术概念":"多流卷积-反卷积架构是一种深度学习模型，用于处理视频数据，通过分析视频中的多个特征流（如自体运动、人物尺度和目标姿势）来预测人物的未来位置。"},{"order":785,"title":"Preserving Semantic Relations for Zero-Shot Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Annadani_Preserving_Semantic_Relations_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Annadani_Preserving_Semantic_Relations_CVPR_2018_paper.html","abstract":"Zero-shot learning has gained popularity due to its potential to scale recognition models without requiring additional training data. This is usually achieved by associating categories with their semantic information like attributes. However, we believe that the potential offered by this paradigm is not yet fully exploited. In this work, we propose to utilize the structure of the space spanned by the attributes using a set of relations. We devise objective functions to preserve these relations in the embedding space, thereby inducing semanticity to the embedding space. Through extensive experimental evaluation on five benchmark datasets, we demonstrate that inducing semanticity to the embedding space is beneficial for zero-shot learning. The proposed approach outperforms the state-of-the-art on the standard zero-shot setting as well as the more realistic generalized zero-shot setting. We also demonstrate how the proposed approach can be useful for making approximate semantic inferences about an image belonging to a category for which attribute information is not available.","中文标题":"保持语义关系的零样本学习","摘要翻译":"零样本学习因其在不需额外训练数据的情况下扩展识别模型的潜力而受到欢迎。这通常通过将类别与其语义信息（如属性）关联来实现。然而，我们认为这一范式提供的潜力尚未被充分利用。在本工作中，我们提出利用属性所跨越的空间结构，通过一组关系来利用这一结构。我们设计了目标函数以在嵌入空间中保持这些关系，从而在嵌入空间中引入语义性。通过在五个基准数据集上的广泛实验评估，我们证明了在嵌入空间中引入语义性对零样本学习是有益的。所提出的方法在标准零样本设置以及更现实的广义零样本设置中均优于现有技术。我们还展示了所提出的方法如何有助于对属于没有属性信息的类别的图像进行近似语义推断。","领域":"零样本学习/语义嵌入/属性学习","问题":"如何在零样本学习中更有效地利用语义信息以提高模型性能","动机":"现有零样本学习方法未能充分利用语义信息提供的潜力","方法":"利用属性空间的结构，设计目标函数以在嵌入空间中保持语义关系","关键词":["零样本学习","语义嵌入","属性学习"],"涉及的技术概念":"零样本学习是一种机器学习方法，旨在识别训练数据中未出现过的类别。语义嵌入指的是将语义信息（如属性）嵌入到向量空间中，以便于机器理解和处理。属性学习涉及从数据中学习能够描述对象特征的属性，这些属性可以用于分类和识别任务。"},{"order":786,"title":"Show Me a Story: Towards Coherent Neural Story Illustration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ravi_Show_Me_a_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ravi_Show_Me_a_CVPR_2018_paper.html","abstract":"We propose an end-to-end network for the visual illustration of a sequence of sentences forming a story. At the core of our model is the ability to model the inter-related nature of the sentences within a story, as well as the ability to learn coherence to support reference resolution. The framework takes the form of an encoder-decoder architecture, where sentences are encoded using a hierarchical two-level sentence-story GRU, combined with an encoding of coherence, and sequentially decoded using predicted feature representation into a consistent illustrative image sequence. We optimize all parameters of our network in an end-to-end fashion with respect to order embedding loss, encoding entailment between images and sentences. Experiments on the VIST storytelling dataset cite{vist} highlight the importance of our algorithmic choices and efficacy of our overall model.","中文标题":"给我讲个故事：迈向连贯的神经故事插图","摘要翻译":"我们提出了一个端到端的网络，用于将形成故事的句子序列进行视觉插图。我们模型的核心是能够建模故事中句子之间的相互关系，以及学习连贯性以支持参考解析。该框架采用编码器-解码器架构，其中句子使用分层的两级句子-故事GRU进行编码，结合连贯性编码，并顺序解码为一致的插图图像序列。我们以端到端的方式优化我们网络的所有参数，关于顺序嵌入损失，编码图像和句子之间的蕴含关系。在VIST讲故事数据集上的实验强调了我们的算法选择的重要性和我们整体模型的有效性。","领域":"故事插图生成/序列到序列模型/连贯性学习","问题":"如何从一系列描述故事的句子生成连贯的视觉插图","动机":"为了支持故事叙述中的视觉插图生成，需要模型能够理解句子间的相互关系并保持连贯性","方法":"采用编码器-解码器架构，使用分层的两级句子-故事GRU进行句子编码，结合连贯性编码，顺序解码为插图图像序列，并通过顺序嵌入损失优化模型参数","关键词":["故事插图生成","序列到序列模型","连贯性学习","GRU","端到端网络"],"涉及的技术概念":{"端到端网络":"一种直接从输入到输出进行学习的网络架构，无需手动设计中间步骤","GRU":"门控循环单元，一种循环神经网络，用于处理序列数据","顺序嵌入损失":"一种损失函数，用于确保生成的图像序列与输入句子序列的顺序和内容相匹配","编码器-解码器架构":"一种常用的神经网络架构，用于将输入数据编码为固定长度的向量，然后解码为输出数据"}},{"order":787,"title":"Reconstruction Network for Video Captioning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Reconstruction_Network_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Reconstruction_Network_for_CVPR_2018_paper.html","abstract":"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) with a novel encoder-decoder-reconstructor architecture, which leverages both the forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder makes use of the forward flow to produce the sentence description based on the encoded video semantic features. Two types of reconstructors are customized to employ the backward flow and reproduce the video features based on the hidden state sequence generated by the decoder. The generation loss yielded by encoder-decoder and the reconstruction loss introduced by reconstructor are jointly drawn into training the proposed RecNet in an end-to-end fashion. Experimental results on benchmark datasets demonstrate that the proposed reconstructor could boost the encoder-decoder models and leads to significant gains on video caption accuracy.","中文标题":"视频描述的重建网络","摘要翻译":"本文解决了用自然语言描述视频序列视觉内容的问题。与之前主要利用视频内容线索进行语言描述的视频描述工作不同，我们提出了一种具有新颖编码器-解码器-重建器架构的重建网络（RecNet），该网络利用前向（视频到句子）和后向（句子到视频）流进行视频描述。具体来说，编码器-解码器利用前向流基于编码的视频语义特征生成句子描述。定制了两种类型的重建器以利用后向流，并基于解码器生成的隐藏状态序列重现视频特征。编码器-解码器产生的生成损失和重建器引入的重建损失共同用于以端到端的方式训练提出的RecNet。基准数据集上的实验结果表明，所提出的重建器可以增强编码器-解码器模型，并在视频描述准确性上带来显著提升。","领域":"视频描述/自然语言处理/深度学习","问题":"如何更有效地用自然语言描述视频序列的视觉内容","动机":"现有的视频描述方法主要依赖于视频内容的线索，缺乏对描述句子与视频内容之间关系的深入利用","方法":"提出了一种新颖的编码器-解码器-重建器架构的重建网络（RecNet），利用前向和后向流进行视频描述，并通过生成损失和重建损失联合训练模型","关键词":["视频描述","重建网络","编码器-解码器","重建器","自然语言处理"],"涉及的技术概念":{"编码器-解码器":"一种用于将输入数据（如视频）转换为另一种形式（如自然语言描述）的模型架构","重建器":"一种用于从解码器生成的隐藏状态序列中重现视频特征的组件","前向流":"从视频到句子的信息流动过程","后向流":"从句子到视频的信息流动过程","生成损失":"编码器-解码器在生成句子描述时产生的损失","重建损失":"重建器在重现视频特征时引入的损失"}},{"order":788,"title":"Fast Spectral Ranking for Similarity Search","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Iscen_Fast_Spectral_Ranking_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Fast_Spectral_Ranking_CVPR_2018_paper.html","abstract":"Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline.  This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.","中文标题":"快速光谱排序用于相似性搜索","摘要翻译":"尽管深度学习在特定对象检索的图像表示方面取得了成功，但最近的研究表明，学习到的表示仍然位于高维空间中的流形上。这使得欧几里得最近邻搜索对于此任务存在偏差。即使已经离线计算了最近邻图，在线探索流形仍然昂贵。这项工作引入了一种显式嵌入，将流形搜索简化为欧几里得搜索，随后是点积相似性搜索。这相当于在频域中对稀疏信号进行线性图滤波。为了加速在线搜索，我们离线计算了图的近似傅里叶基。我们在包括包含小物体的挑战性Instre数据集在内的特定对象检索数据集上改进了现有技术。在10^5图像的规模下，离线成本仅为几个小时，而查询时间与标准相似性搜索相当。","领域":"图像检索/流形学习/图信号处理","问题":"高维空间中流形上的最近邻搜索偏差问题","动机":"解决深度学习表示在特定对象检索中由于位于高维流形上而导致的欧几里得最近邻搜索偏差问题，并降低在线探索流形的成本。","方法":"引入显式嵌入将流形搜索简化为欧几里得搜索和点积相似性搜索，相当于在频域中对稀疏信号进行线性图滤波，并离线计算图的近似傅里叶基以加速在线搜索。","关键词":["流形学习","图信号处理","图像检索"],"涉及的技术概念":"流形学习指的是在高维空间中识别和利用数据的低维流形结构；图信号处理涉及在图结构数据上进行的信号处理技术，如傅里叶变换；图像检索是指从大量图像中查找与查询图像相似或相关的图像。"},{"order":789,"title":"Mining on Manifolds: Metric Learning Without Labels","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Iscen_Mining_on_Manifolds_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Mining_on_Manifolds_CVPR_2018_paper.html","abstract":"In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss.   The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.","中文标题":"流形上的挖掘：无需标签的度量学习","摘要翻译":"在本工作中，我们提出了一种新颖的无监督框架，用于挖掘难以训练的例子。该方法的唯一输入是与目标应用相关的图像集合和有意义的初始表示，例如由预训练的CNN提供。正例是单个流形上的远点，而负例是不同流形上的近点。这两种类型的例子都是通过欧几里得和流形相似性之间的不一致揭示的。发现的例子可以用于任何判别性损失的训练。该方法应用于预训练网络的无监督微调，用于细粒度分类和特定对象检索。我们的模型与完全或部分监督的先前模型相当或优于它们。","领域":"细粒度分类/特定对象检索/无监督学习","问题":"如何在无监督的情况下挖掘难以训练的例子以改进模型的性能","动机":"为了在没有标签的情况下提高模型在细粒度分类和特定对象检索任务中的性能，需要一种有效的方法来挖掘训练中的正例和负例。","方法":"提出了一种无监督框架，通过比较欧几里得和流形相似性来挖掘正例和负例，这些例子随后用于模型的训练。","关键词":["无监督学习","度量学习","细粒度分类","特定对象检索"],"涉及的技术概念":"欧几里得相似性、流形相似性、预训练CNN、判别性损失、无监督微调"},{"order":790,"title":"PIXOR: Real-Time 3D Object Detection From Point Clouds","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.html","abstract":"We address the problem of real-time 3D object detection from point clouds in the context of autonomous driving. Speed is critical as detection is a necessary component for safety. Existing approaches are, however, expensive in computation due to high dimensionality of point clouds. We utilize the 3D data more efficiently by representing the scene from the Bird's Eye View (BEV), and propose PIXOR, a proposal-free, single-stage detector that outputs oriented 3D object estimates decoded from pixel-wise neural network predictions. The input representation, network architecture, and model optimization are specially designed to balance high accuracy and real-time efficiency. We validate PIXOR on two datasets: the KITTI BEV object detection benchmark, and a large-scale 3D vehicle detection benchmark. In both datasets we show that the proposed detector surpasses  other state-of-the-art methods notably in terms of Average Precision (AP), while still runs at 10 FPS.","中文标题":"PIXOR：从点云实时进行3D物体检测","摘要翻译":"我们解决了在自动驾驶背景下从点云实时进行3D物体检测的问题。速度至关重要，因为检测是安全性的必要组成部分。然而，由于点云的高维度，现有方法在计算上非常昂贵。我们通过从鸟瞰图（BEV）表示场景来更有效地利用3D数据，并提出了PIXOR，一种无提议、单阶段的检测器，它输出从像素级神经网络预测解码出的定向3D物体估计。输入表示、网络架构和模型优化特别设计以平衡高精度和实时效率。我们在两个数据集上验证了PIXOR：KITTI BEV物体检测基准和大规模3D车辆检测基准。在这两个数据集中，我们展示了所提出的检测器在平均精度（AP）方面显著超越其他最先进的方法，同时仍以10 FPS运行。","领域":"自动驾驶/3D物体检测/点云处理","问题":"实时从点云进行3D物体检测","动机":"提高自动驾驶系统中3D物体检测的速度和效率，以确保安全性","方法":"提出PIXOR，一种无提议、单阶段的检测器，通过从鸟瞰图（BEV）表示场景来更有效地利用3D数据，并特别设计输入表示、网络架构和模型优化以平衡高精度和实时效率","关键词":["自动驾驶","3D物体检测","点云处理","鸟瞰图","实时效率"],"涉及的技术概念":"PIXOR是一种无提议、单阶段的检测器，它通过从鸟瞰图（BEV）表示场景来更有效地利用3D数据，并输出从像素级神经网络预测解码出的定向3D物体估计。输入表示、网络架构和模型优化特别设计以平衡高精度和实时效率。"},{"order":791,"title":"Leveraging Unlabeled Data for Crowd Counting by Learning to Rank","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Leveraging_Unlabeled_Data_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Leveraging_Unlabeled_Data_CVPR_2018_paper.html","abstract":"We propose a novel crowd counting approach that   leverages abundantly available unlabeled crowd imagery in a   learning-to-rank framework. To induce a ranking of cropped images , we use the  observation that any sub-image of a crowded scene image is guaranteed to contain the same number or fewer persons than the   super-image. This allows us to address the problem of limited size   of existing datasets for crowd counting.  We collect two crowd scene   datasets from Google using keyword searches and query-by-example   image retrieval, respectively. We demonstrate how to efficiently   learn from these unlabeled datasets by incorporating   learning-to-rank in a multi-task network which simultaneously ranks   images and estimates crowd density maps.  Experiments on two of the   most challenging crowd counting datasets show that our approach   obtains state-of-the-art results.","中文标题":"利用未标记数据进行人群计数通过学习排序","摘要翻译":"我们提出了一种新颖的人群计数方法，该方法在学习排序框架中利用了大量可用的未标记人群图像。为了诱导裁剪图像的排序，我们使用了这样一个观察结果：任何拥挤场景图像的子图像都保证包含与超图像相同或更少的人数。这使我们能够解决现有数据集在人群计数方面规模有限的问题。我们分别通过关键词搜索和示例图像检索从谷歌收集了两个人群场景数据集。我们展示了如何通过将学习排序纳入多任务网络来有效地从这些未标记数据集中学习，该网络同时排序图像并估计人群密度图。在两个最具挑战性的人群计数数据集上的实验表明，我们的方法获得了最先进的结果。","领域":"人群计数/图像排序/密度估计","问题":"解决现有数据集在人群计数方面规模有限的问题","动机":"利用大量可用的未标记人群图像来提高人群计数的准确性","方法":"在学习排序框架中利用未标记数据，通过多任务网络同时排序图像并估计人群密度图","关键词":["人群计数","图像排序","密度估计"],"涉及的技术概念":"学习排序框架、多任务网络、人群密度图估计"},{"order":792,"title":"Zero-Shot Kernel Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Zero-Shot_Kernel_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Zero-Shot_Kernel_Learning_CVPR_2018_paper.html","abstract":"In this paper, we address an open problem of zero-shot learning. Its principle is based on learning a mapping that associates feature vectors extracted from i.e. images and attribute vectors that describe objects and/or scenes of interest. In turns, this allows classifying unseen object classes and/or scenes by matching feature vectors via mapping to a newly defined attribute vector describing a new class. Due to importance of such a learning task, there exist many methods that learn semantic, probabilistic, linear or piece-wise linear mappings. In contrast, we apply well-established kernel methods to learn a non-linear mapping between the feature and attribute spaces. We propose an easy learning objective with orthogonality constraints inspired by the Linear Discriminant Analysis, Kernel-Target Alignment and Kernel Polarization methods. We evaluate the performance of our algorithm on the Polynomial as well as shift-invariant Gaussian and Cauchy kernels. Despite simplicity of our approach, we obtain state-of-the-art results on several zero-shot learning datasets and benchmarks including very recent AWA2 dataset.","中文标题":"零样本核学习","摘要翻译":"在本文中，我们解决了零样本学习的一个开放性问题。其原理基于学习一个映射，该映射将例如从图像中提取的特征向量与描述感兴趣对象和/或场景的属性向量相关联。这反过来允许通过将特征向量通过映射与新定义的描述新类别的属性向量匹配来分类未见过的对象类别和/或场景。由于这种学习任务的重要性，存在许多学习语义、概率、线性或分段线性映射的方法。相比之下，我们应用成熟的核方法来学习特征和属性空间之间的非线性映射。我们提出了一个简单的学习目标，其正交性约束受到线性判别分析、核目标对齐和核极化方法的启发。我们在多项式以及平移不变的高斯和柯西核上评估了我们算法的性能。尽管我们的方法简单，但我们在包括最新的AWA2数据集在内的几个零样本学习数据集和基准测试中获得了最先进的结果。","领域":"零样本学习/核方法/特征映射","问题":"如何有效地在零样本学习任务中分类未见过的对象类别和/或场景","动机":"解决零样本学习中的开放性问题，通过非线性映射提高分类性能","方法":"应用核方法学习特征和属性空间之间的非线性映射，提出带有正交性约束的学习目标","关键词":["零样本学习","核方法","特征映射","非线性映射","正交性约束"],"涉及的技术概念":"零样本学习是一种机器学习方法，旨在分类训练数据中未出现过的类别。核方法是一类用于模式分析的算法，通过将数据映射到高维空间来发现数据的非线性结构。特征映射是指将原始数据转换为特征空间中的表示，以便更好地进行分类或回归。非线性映射涉及将数据从一个空间转换到另一个空间，其中数据间的关系不是线性的。正交性约束是一种数学约束，用于确保映射的某些属性，如独立性或最小冗余。"},{"order":793,"title":"Differential Attention for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Patro_Differential_Attention_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Patro_Differential_Attention_for_CVPR_2018_paper.html","abstract":"In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions.","中文标题":"视觉问答的差异注意力机制","摘要翻译":"在本文中，我们旨在基于图像回答问题，前提是在训练期间提供了一系列图像的问答对数据集。许多方法通过使用基于图像的注意力来解决这个问题。这是通过在回答问题时专注于图像的特定部分来实现的。人类在解决这个问题时也会这样做。然而，之前系统关注的区域与人类关注的区域并不相关。由于这一缺点，准确性受到限制。在本文中，我们提出通过使用基于范例的方法来解决这个问题。我们获得一个或多个支持和反对的范例，以获得差异注意力区域。这种差异注意力比其他基于图像的注意力方法更接近人类的注意力。它还有助于在回答问题时获得更高的准确性。该方法在具有挑战性的基准数据集上进行了评估。我们的表现优于其他基于图像的注意力方法，并且与专注于图像和问题的其他最先进方法具有竞争力。","领域":"视觉问答/注意力机制/深度学习","问题":"提高视觉问答系统中基于图像注意力的准确性问题","动机":"现有视觉问答系统中基于图像注意力的方法关注的区域与人类关注的区域不一致，导致准确性受限。","方法":"提出了一种基于范例的方法，通过获得支持和反对的范例来确定差异注意力区域，这种方法更接近人类的注意力机制。","关键词":["视觉问答","注意力机制","深度学习"],"涉及的技术概念":"基于图像的注意力机制、差异注意力区域、基于范例的方法、视觉问答系统"},{"order":794,"title":"Learning From Noisy Web Data With Category-Level Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Niu_Learning_From_Noisy_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Niu_Learning_From_Noisy_CVPR_2018_paper.html","abstract":"Learning from web data is increasingly popular due to abundant free web resources. However, the performance gap between webly supervised learning and traditional supervised learning is still very large, due to the label noise of web data. To fill this gap, most existing methods propose to purify or augment web data using instance-level supervision, which generally requires heavy annotation. Instead, we propose to address the label noise by using more accessible category-level supervision. In particular, we build our deep probabilistic framework upon variational autoencoder (VAE), in which classification network and VAE can jointly leverage category-level hybrid information.  Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed method.","中文标题":"从具有类别级监督的噪声网络数据中学习","摘要翻译":"由于丰富的免费网络资源，从网络数据中学习变得越来越流行。然而，由于网络数据的标签噪声，网络监督学习与传统监督学习之间的性能差距仍然非常大。为了填补这一差距，大多数现有方法提出使用实例级监督来净化或增强网络数据，这通常需要大量的注释。相反，我们提出通过使用更易访问的类别级监督来解决标签噪声问题。特别是，我们在变分自编码器（VAE）上构建了我们的深度概率框架，其中分类网络和VAE可以共同利用类别级混合信息。在三个基准数据集上的大量实验证明了我们提出方法的有效性。","领域":"数据清洗/深度学习/变分自编码器","问题":"解决网络数据中的标签噪声问题","动机":"减少网络监督学习与传统监督学习之间的性能差距，同时降低数据注释的成本","方法":"提出使用类别级监督来构建深度概率框架，结合分类网络和变分自编码器（VAE）共同利用类别级混合信息","关键词":["数据清洗","变分自编码器","类别级监督"],"涉及的技术概念":{"变分自编码器（VAE）":"一种生成模型，用于学习数据的潜在表示，并通过潜在变量生成新的数据样本。","类别级监督":"一种监督学习方法，其中监督信息是以类别级别提供的，而不是实例级别。","深度概率框架":"一种结合深度学习和概率模型的框架，用于处理不确定性和噪声数据。"}},{"order":795,"title":"Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.html","abstract":"Driving Scene understanding is a key ingredient for intelligent transportation systems. To achieve systems that can operate in a complex physical and social environment, they need to understand and learn how humans drive and interact with traffic scenes. We present the Honda Research Institute Driving Dataset (HDD), a challenging dataset to enable research on learning driver behavior in real-life environments. The dataset includes 104 hours of real human driving in the San Francisco Bay Area collected using an instrumented vehicle equipped with different sensors. We provide a detailed analysis of HDD with a comparison to other driving datasets. A novel annotation methodology is introduced to enable research on driver behavior understanding from untrimmed data sequences. As the first step, baseline algorithms for driver behavior detection are trained and tested to demonstrate the feasibility of the proposed task.","中文标题":"迈向驾驶场景理解：一个用于学习驾驶员行为和因果推理的数据集","摘要翻译":"驾驶场景理解是智能交通系统的关键组成部分。为了实现能够在复杂的物理和社会环境中运行的系统，它们需要理解并学习人类如何驾驶以及与交通场景互动。我们介绍了本田研究所驾驶数据集（HDD），这是一个具有挑战性的数据集，旨在促进在真实生活环境中学习驾驶员行为的研究。该数据集包括在旧金山湾区使用配备不同传感器的仪器车辆收集的104小时真实人类驾驶数据。我们提供了HDD的详细分析，并与其他驾驶数据集进行了比较。引入了一种新的注释方法，以促进从未修剪的数据序列中理解驾驶员行为的研究。作为第一步，训练并测试了用于驾驶员行为检测的基线算法，以证明所提出任务的可行性。","领域":"智能交通系统/驾驶员行为分析/因果推理","问题":"如何在复杂的物理和社会环境中理解和学习人类驾驶行为","动机":"为了实现能够在复杂环境中运行的智能交通系统，需要深入理解人类驾驶行为及其与交通场景的互动","方法":"介绍本田研究所驾驶数据集（HDD），提供详细分析并与其他数据集比较，引入新的注释方法，训练并测试驾驶员行为检测的基线算法","关键词":["智能交通系统","驾驶员行为分析","因果推理","数据集","注释方法"],"涉及的技术概念":"本田研究所驾驶数据集（HDD）是一个包含104小时真实人类驾驶数据的数据集，用于研究驾驶员行为。该数据集通过配备不同传感器的仪器车辆在旧金山湾区收集。研究还包括对HDD的详细分析、与其他驾驶数据集的比较、新的注释方法的引入，以及用于驾驶员行为检测的基线算法的训练和测试。"},{"order":796,"title":"Learning Attribute Representations With Localization for Flexible Fashion Search","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ak_Learning_Attribute_Representations_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ak_Learning_Attribute_Representations_CVPR_2018_paper.html","abstract":"In this paper, we investigate ways of conducting a detailed fashion search using query images and attributes. A credible fashion search platform should be able to (1) find images that share the same attributes as the query image, (2) allow users to manipulate certain attributes, e.g. replace collar attribute from round to v-neck, and (3) handle region-specific attribute manipulations, e.g. replacing the color attribute of the sleeve region without changing the color attribute of other regions. A key challenge to be addressed is that fashion products have multiple attributes and it is important for each of these attributes to have representative features. To address these challenges, we propose the FashionSearchNet which uses a weakly supervised localization method to extract regions of attributes. By doing so, unrelated features can be ignored thus improving the similarity learning. Also, FashionSearchNet incorporates a new procedure that enables region awareness to be able to handle region-specific requests. FashionSearchNet outperforms the most recent fashion search techniques and is shown to be able to carry out different search scenarios using the dynamic queries.","中文标题":"学习具有定位功能的属性表示以实现灵活的时尚搜索","摘要翻译":"在本文中，我们研究了使用查询图像和属性进行详细时尚搜索的方法。一个可信的时尚搜索平台应该能够（1）找到与查询图像共享相同属性的图像，（2）允许用户操作某些属性，例如将衣领属性从圆形更改为V领，（3）处理特定区域的属性操作，例如更换袖子区域的颜色属性而不改变其他区域的颜色属性。需要解决的一个关键挑战是时尚产品具有多个属性，并且每个属性都需要有代表性的特征。为了解决这些挑战，我们提出了FashionSearchNet，它使用弱监督定位方法来提取属性区域。通过这样做，可以忽略不相关的特征，从而改进相似性学习。此外，FashionSearchNet引入了一个新程序，使区域感知能够处理特定区域的请求。FashionSearchNet优于最新的时尚搜索技术，并展示了使用动态查询执行不同搜索场景的能力。","领域":"时尚搜索/属性定位/相似性学习","问题":"如何在时尚搜索中有效地处理多属性查询和特定区域的属性操作","动机":"为了提升时尚搜索平台的灵活性和准确性，使其能够根据用户的具体需求进行属性操作和区域特定的搜索","方法":"提出了FashionSearchNet，采用弱监督定位方法提取属性区域，并引入区域感知程序处理特定区域的请求","关键词":["时尚搜索","属性定位","相似性学习","区域感知"],"涉及的技术概念":"FashionSearchNet是一个用于时尚搜索的网络模型，它通过弱监督定位方法来提取图像中的属性区域，并引入区域感知程序来处理特定区域的属性操作请求。这种方法能够忽略不相关的特征，从而提高相似性学习的准确性。"},{"order":797,"title":"Bidirectional Retrieval Made Simple","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wehrmann_Bidirectional_Retrieval_Made_CVPR_2018_paper.html","abstract":"This paper provides a very simple yet effective character-level architecture for learning bidirectional retrieval models. Aligning multimodal content is particularly challenging considering the difficulty in finding semantic correspondence between images and descriptions. We introduce an efficient character-level inception module, designed to learn textual semantic embeddings by convolving raw characters in distinct granularity levels. Our approach is capable of explicitly encoding hierarchical information from distinct base-level representations (e.g., characters, words, and sentences) into a shared multimodal space, where it maps the semantic correspondence between images and descriptions via a contrastive pairwise loss function that minimizes order-violations. Models generated by our approach are far more robust to input noise than state-of-the-art strategies based on word-embeddings.   Despite being conceptually much simpler and requiring fewer parameters, our models outperform the state-of-the-art approaches by 4.8% in the task of description retrieval and 2.7% (absolute R@1 values) in the task of image retrieval in the popular MS COCO retrieval dataset. Finally, we show that our models present solid performance for text classification as well, specially in multilingual and noisy domains.","中文标题":"双向检索简化版","摘要翻译":"本文提供了一种非常简单但有效的字符级架构，用于学习双向检索模型。考虑到在图像和描述之间找到语义对应关系的困难，对齐多模态内容尤其具有挑战性。我们引入了一个高效的字符级初始模块，旨在通过在不同粒度级别上卷积原始字符来学习文本语义嵌入。我们的方法能够将来自不同基础级别表示（例如，字符、单词和句子）的层次信息显式编码到一个共享的多模态空间中，在该空间中，它通过最小化顺序违规的对比成对损失函数来映射图像和描述之间的语义对应关系。由我们的方法生成的模型对输入噪声的鲁棒性远远超过基于词嵌入的最先进策略。尽管在概念上更简单且需要更少的参数，我们的模型在流行的MS COCO检索数据集的描述检索任务中比最先进的方法高出4.8%，在图像检索任务中高出2.7%（绝对R@1值）。最后，我们展示了我们的模型在文本分类方面也表现出色，特别是在多语言和噪声领域。","领域":"多模态学习/语义检索/文本分类","问题":"在图像和描述之间找到语义对应关系的困难","动机":"提高多模态内容对齐的效率和准确性，特别是在面对输入噪声时","方法":"引入高效的字符级初始模块，通过在不同粒度级别上卷积原始字符来学习文本语义嵌入，并将层次信息显式编码到共享的多模态空间中","关键词":["字符级架构","多模态学习","语义检索","文本分类"],"涉及的技术概念":"字符级初始模块：一种用于处理文本的模块，通过卷积不同粒度的原始字符来学习语义嵌入。对比成对损失函数：一种用于最小化顺序违规的损失函数，用于映射图像和描述之间的语义对应关系。"},{"order":798,"title":"Learning Multi-Instance Enriched Image Representations via Non-Greedy Ratio Maximization of the l1-Norm Distances","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Multi-Instance_Enriched_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Learning_Multi-Instance_Enriched_CVPR_2018_paper.html","abstract":"Multi-instance learning (MIL) has demonstrated its usefulness in many real-world image applications in recent years. However, two critical challenges prevent one from effectively using MIL in practice. First, existing MIL methods routinely model the predictive targets using the instances of input images, but rarely utilize an input image as a whole.  As a result, the useful information conveyed by the holistic representation of an input image could be potentially lost. Second, the varied numbers of the instances of the input images in a data set make it infeasible to use traditional learning models that can only deal with single-vector inputs. To tackle these two challenges, in this paper we propose a novel image representation learning method that can integrate the local patches (the instances) of an input image (the bag) and its holistic representation into one single-vector representation.  Our new method first learns a projection to preserve both global and local consistencies of the instances of an input image. It then projects the holistic representation of the same image into the learned subspace for information enrichment. Taking into account the content and characterization variations in natural scenes and photos, we develop an objective that maximizes the ratio of the summations of a number of L1-norm distances, which is difficult to solve in general. To solve our objective, we derive a new efficient non-greedy iterative algorithm and rigorously prove its convergence.  Promising results in extensive experiments have demonstrated improved performances of our new method that validate its effectiveness.","中文标题":"通过非贪婪比率最大化l1范数距离学习多实例丰富的图像表示","摘要翻译":"近年来，多实例学习（MIL）在许多实际图像应用中展示了其有效性。然而，两个关键挑战阻碍了MIL在实际中的有效使用。首先，现有的MIL方法通常使用输入图像的实例来建模预测目标，但很少将输入图像作为一个整体来利用。因此，输入图像整体表示所传达的有用信息可能会丢失。其次，数据集中输入图像的实例数量不一，使得无法使用只能处理单向量输入的传统学习模型。为了解决这两个挑战，本文提出了一种新颖的图像表示学习方法，该方法可以将输入图像（包）的局部补丁（实例）与其整体表示整合到一个单向量表示中。我们的新方法首先学习一个投影，以保持输入图像实例的全局和局部一致性。然后，它将同一图像的整体表示投影到学习到的子空间中以丰富信息。考虑到自然场景和照片中的内容和特征变化，我们开发了一个目标，该目标最大化了一些L1范数距离之和的比率，这在一般情况下难以解决。为了解决我们的目标，我们推导了一种新的高效非贪婪迭代算法，并严格证明了其收敛性。广泛的实验结果展示了我们新方法的改进性能，验证了其有效性。","领域":"图像表示学习/多实例学习/图像分析","问题":"如何有效地利用输入图像的整体表示和局部实例进行图像表示学习","动机":"现有的多实例学习方法通常只使用输入图像的实例来建模预测目标，而忽略了输入图像作为一个整体的有用信息，且无法处理实例数量不一的输入图像","方法":"提出了一种新的图像表示学习方法，通过投影保持输入图像实例的全局和局部一致性，并将图像的整体表示投影到学习到的子空间中以丰富信息，开发了一个最大化L1范数距离比率的目标，并推导了一种新的高效非贪婪迭代算法来解决该目标","关键词":["多实例学习","图像表示","L1范数距离","非贪婪算法","迭代算法"],"涉及的技术概念":"多实例学习（MIL）是一种机器学习方法，它处理的是由多个实例组成的包（bag），而不是单个实例。L1范数距离是指向量中各元素绝对值之和，用于衡量向量间的差异。非贪婪算法在每一步选择中不总是选择当前最优的选项，而是可能选择次优选项以达到全局最优。迭代算法是通过重复应用一系列步骤来逐步接近问题解决方案的算法。"},{"order":799,"title":"Learning Visual Knowledge Memory Networks for Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Learning_Visual_Knowledge_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Learning_Visual_Knowledge_CVPR_2018_paper.html","abstract":"Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can't be directly or clearly answered from visual content but require reasoning from structured human knowledge with confirmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.","中文标题":"学习视觉知识记忆网络用于视觉问答","摘要翻译":"视觉问答（VQA）需要图像和自然语言问题的联合理解，其中许多问题不能直接从视觉内容中明确回答，而是需要从结构化的人类知识中进行推理，并通过视觉内容进行确认。本文提出了视觉知识记忆网络（VKMN）来解决这个问题，它将结构化的人类知识和深度视觉特征无缝地整合到记忆网络中，形成一个端到端的学习框架。与现有的利用外部知识支持VQA的方法相比，本文更强调两个缺失的机制。首先是整合视觉内容与知识事实的机制。VKMN通过将知识三元组（主体、关系、目标）和深度视觉特征共同嵌入到视觉知识特征中来处理这个问题。其次是处理从问题和答案对扩展出的多个知识事实的机制。VKMN使用键值对结构在记忆网络中存储联合嵌入，以便容易处理多个事实。实验表明，所提出的方法在VQA v1.0和v2.0基准测试中取得了有希望的结果，同时在知识推理相关问题上优于最先进的方法。","领域":"视觉问答/知识推理/记忆网络","问题":"视觉问答中如何有效整合视觉内容和结构化人类知识进行推理","动机":"解决视觉问答中许多问题不能直接从视觉内容中明确回答，需要从结构化的人类知识中进行推理的问题","方法":"提出视觉知识记忆网络（VKMN），将结构化的人类知识和深度视觉特征整合到记忆网络中，形成端到端的学习框架，并通过嵌入知识三元组和深度视觉特征共同处理视觉内容与知识事实的整合，以及使用键值对结构存储联合嵌入处理多个知识事实","关键词":["视觉问答","知识推理","记忆网络"],"涉及的技术概念":"视觉知识记忆网络（VKMN）是一种将结构化的人类知识和深度视觉特征整合到记忆网络中的方法，通过嵌入知识三元组（主体、关系、目标）和深度视觉特征共同处理视觉内容与知识事实的整合，并使用键值对结构在记忆网络中存储联合嵌入，以便处理多个知识事实。"},{"order":800,"title":"Visual Grounding via Accumulated Attention","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Deng_Visual_Grounding_via_CVPR_2018_paper.html","abstract":"Visual Grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. The query can be a phrase, a sentence or even a multi-round dialogue. There are three main challenges in VG: 1) what is the main focus in a query; 2) how to understand an image; 3) how to locate an object. Most existing methods combine all the information curtly, which may suffer from the problem of information redundancy (i.e. ambiguous query, complicated image and a large number of objects). In this paper, we formulate these challenges as three attention problems and propose an accumulated attention (A-ATT) mechanism to reason among them jointly. Our A-ATT mechanism can circularly accumulate the attention for useful information in image, query, and objects, while the noises are ignored gradually. We evaluate the performance of A-ATT on four popular datasets (namely ReferCOCO, ReferCOCO+, ReferCOCOg, and Guesswhat?!), and the experimental results show the superiority of the proposed method in term of accuracy.","中文标题":"通过累积注意力实现视觉定位","摘要翻译":"视觉定位（VG）旨在基于自然语言查询定位图像中最相关的对象或区域。查询可以是一个短语、一个句子，甚至是一个多轮对话。VG面临三个主要挑战：1）查询中的主要焦点是什么；2）如何理解图像；3）如何定位对象。大多数现有方法将所有信息简单地结合起来，这可能会遇到信息冗余的问题（即模糊的查询、复杂的图像和大量的对象）。在本文中，我们将这些挑战表述为三个注意力问题，并提出了一种累积注意力（A-ATT）机制来共同推理它们。我们的A-ATT机制可以循环累积图像、查询和对象中有用信息的注意力，同时逐渐忽略噪声。我们在四个流行数据集（即ReferCOCO、ReferCOCO+、ReferCOCOg和Guesswhat?!）上评估了A-ATT的性能，实验结果显示所提出方法在准确性方面的优越性。","领域":"视觉定位/自然语言理解/注意力机制","问题":"如何基于自然语言查询准确定位图像中的对象或区域","动机":"解决视觉定位中的信息冗余问题，提高定位准确性","方法":"提出累积注意力（A-ATT）机制，循环累积图像、查询和对象中有用信息的注意力，同时逐渐忽略噪声","关键词":["视觉定位","自然语言查询","累积注意力","信息冗余"],"涉及的技术概念":"累积注意力（A-ATT）机制是一种通过循环累积图像、查询和对象中有用信息的注意力来解决视觉定位中信息冗余问题的方法。"},{"order":801,"title":"Beyond Trade-Off: Accelerate FCN-Based Face Detector With Higher Accuracy","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Beyond_Trade-Off_Accelerate_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_Beyond_Trade-Off_Accelerate_CVPR_2018_paper.html","abstract":"Fully convolutional neural network (FCN) has been dominating the game of face detection task for a few years with its congenital capability of sliding-window-searching with shared kernels, which boiled down all the redundant calculation, and most recent state-of-the-art methods such as Faster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one question: Can we find a universal strategy to further accelerate FCN with higher accuracy, so could accelerate all the recent FCN-based methods? To analyze this, we decompose the face searching space into two orthogonal directions, \`scale' and \`spatial'. Only a few coordinates in the space expanded by the two base vectors indicate foreground. So if FCN could ignore most of the other points, the searching space and false alarm should be significantly boiled down. Based on this philosophy, a novel method named scale estimation and spatial attention proposal (S^2AP) is proposed to pay attention to some specific scales in image pyramid and valid locations in each scales layer. Furthermore, we adopt a masked convolution operation based on the attention result to accelerate FCN calculation. Experiments show that FCN-based method RPN can be accelerated by about 4X with the help of S^2AP and masked-FCN and at the same time it can also achieve the state-of-the-art on FDDB, AFW and MALF face detection benchmarks as well.","中文标题":"超越权衡：以更高精度加速基于FCN的人脸检测器","摘要翻译":"全卷积神经网络（FCN）凭借其先天具备的共享核滑动窗口搜索能力，在过去几年中主导了人脸检测任务，减少了所有冗余计算。最近的最先进方法如Faster-RCNN、SSD、YOLO和FPN都使用FCN作为其骨干网络。这就引出了一个问题：我们能否找到一个通用策略，以更高的精度进一步加速FCN，从而加速所有最近的基于FCN的方法？为了分析这一点，我们将人脸搜索空间分解为两个正交方向，\`尺度\`和\`空间\`。由这两个基向量扩展的空间中只有少数坐标表示前景。因此，如果FCN能够忽略大多数其他点，搜索空间和误报应该会显著减少。基于这一理念，提出了一种名为尺度估计和空间注意力提议（S^2AP）的新方法，以关注图像金字塔中的特定尺度和每个尺度层中的有效位置。此外，我们采用基于注意力结果的掩码卷积操作来加速FCN计算。实验表明，基于FCN的方法RPN在S^2AP和掩码FCN的帮助下可以加速约4倍，同时它也能在FDDB、AFW和MALF人脸检测基准上达到最先进的水平。","领域":"人脸检测/卷积神经网络/图像金字塔","问题":"如何在不牺牲精度的情况下加速基于FCN的人脸检测器","动机":"探索一种通用策略，以更高的精度进一步加速FCN，从而加速所有最近的基于FCN的方法","方法":"提出了一种名为尺度估计和空间注意力提议（S^2AP）的新方法，关注图像金字塔中的特定尺度和每个尺度层中的有效位置，并采用基于注意力结果的掩码卷积操作来加速FCN计算","关键词":["人脸检测","全卷积神经网络","图像金字塔","尺度估计","空间注意力","掩码卷积"],"涉及的技术概念":{"全卷积神经网络（FCN）":"一种用于图像处理的神经网络架构，能够处理任意大小的输入图像，通过共享核滑动窗口搜索减少冗余计算。","尺度估计和空间注意力提议（S^2AP）":"一种新方法，用于关注图像金字塔中的特定尺度和每个尺度层中的有效位置，以减少搜索空间和误报。","掩码卷积操作":"一种基于注意力结果的卷积操作，用于加速FCN计算。","图像金字塔":"一种多尺度表示方法，通过在不同尺度上分析图像来检测不同大小的对象。"}},{"order":802,"title":"PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.html","abstract":"This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially \`\`pack'' multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of  network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three fine-grained classification tasks to a single ImageNet-trained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.","中文标题":"PackNet: 通过迭代剪枝向单一网络添加多个任务","摘要翻译":"本文提出了一种方法，用于向单一深度神经网络添加多个任务，同时避免灾难性遗忘。受网络剪枝技术的启发，我们利用大型深度网络中的冗余来释放参数，这些参数随后可用于学习新任务。通过执行迭代剪枝和网络重新训练，我们能够顺序地将多个任务“打包”到单一网络中，同时确保性能下降最小和存储开销最小。与之前使用代理损失来保持旧任务准确性的工作不同，我们始终优化当前任务。我们在各种网络架构和大规模数据集上进行了广泛的实验，并观察到比之前工作更好的抗灾难性遗忘的鲁棒性。特别是，我们能够将三个细粒度分类任务添加到单一经过ImageNet训练的VGG-16网络中，并实现接近每个任务单独训练网络的准确度。","领域":"神经网络剪枝/多任务学习/灾难性遗忘","问题":"如何向单一深度神经网络添加多个任务而不导致灾难性遗忘","动机":"利用大型深度网络中的冗余，通过剪枝释放参数以学习新任务，同时保持旧任务的性能","方法":"采用迭代剪枝和网络重新训练的方法，顺序地将多个任务打包到单一网络中","关键词":["神经网络剪枝","多任务学习","灾难性遗忘","VGG-16","ImageNet"],"涉及的技术概念":"网络剪枝技术用于释放网络中的冗余参数，以便这些参数可以被重新用于学习新的任务。通过迭代剪枝和网络重新训练，可以在单一网络中顺序地添加多个任务，同时最小化性能下降和存储开销。这种方法避免了使用代理损失来保持旧任务的准确性，而是始终优化当前任务。"},{"order":803,"title":"Repulsion Loss: Detecting Pedestrians in a Crowd","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Repulsion_Loss_Detecting_CVPR_2018_paper.html","abstract":"Detecting individual pedestrians in a crowd remains a challenging problem since the pedestrians often gather together and occlude each other in real-world scenarios. In this paper, we first explore how a state-of-the-art pedestrian detector is harmed by crowd occlusion via experimentation, providing insights into the crowd occlusion problem. Then, we propose a novel bounding box regression loss specifically designed for crowd scenes, termed repulsion loss. This loss is driven by two motivations: the attraction by target, and the repulsion by other surrounding objects. The repulsion term prevents the proposal from shifting to surrounding objects thus leading to more crowd-robust localization. Our detector trained by repulsion loss outperforms the state-of-the-art methods with a significant improvement in occlusion cases.","中文标题":"排斥损失：在人群中检测行人","摘要翻译":"在现实世界的场景中，由于行人经常聚集在一起并相互遮挡，因此在人群中检测单个行人仍然是一个具有挑战性的问题。在本文中，我们首先通过实验探索了最先进的行人检测器是如何受到人群遮挡的伤害的，从而提供了对人群遮挡问题的见解。然后，我们提出了一种专门为人群场景设计的新颖的边界框回归损失，称为排斥损失。这种损失由两个动机驱动：目标的吸引力和周围物体的排斥力。排斥项防止提议框向周围物体移动，从而导致更健壮的人群定位。我们通过排斥损失训练的检测器在遮挡情况下显著优于最先进的方法。","领域":"行人检测/目标检测/深度学习","问题":"在人群密集且相互遮挡的场景中准确检测行人","动机":"解决现有行人检测器在人群遮挡情况下的性能下降问题","方法":"提出了一种新颖的边界框回归损失（排斥损失），通过目标的吸引力和周围物体的排斥力来改进检测器的定位能力","关键词":["行人检测","目标检测","深度学习","边界框回归","排斥损失"],"涉及的技术概念":"排斥损失是一种专门为人群场景设计的边界框回归损失，它通过引入目标的吸引力和周围物体的排斥力来改进检测器的定位能力，特别是在人群密集和相互遮挡的情况下。"},{"order":804,"title":"Neural Sign Language Translation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Camgoz_Neural_Sign_Language_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Camgoz_Neural_Sign_Language_CVPR_2018_paper.html","abstract":"Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most research to date has considered SLR as a naive gesture recognition problem. SLR seeks to recognize a sequence of continuous signs but neglects the underlying rich grammatical and linguistic structures of sign language that differ from spoken language. In contrast, we introduce the Sign Language Translation (SLT) problem. Here, the objective is to generate spoken language translations from sign language videos, taking into account the different word orders and grammar.  We formalize SLT in the framework of Neural Machine Translation (NMT) for both end-to-end and pretrained settings (using expert knowledge). This allows us to jointly learn the spatial representations, the underlying language model, and the mapping between sign and spoken language.  To evaluate the performance of Neural SLT, we collected the first publicly available Continuous SLT dataset, RWTH-PHOENIX-Weather 2014T. It provides spoken language translations and gloss level annotations for German Sign Language videos of weather broadcasts. Our dataset contains over .95M frames with >67K signs from a sign vocabulary of >1K and >99K words from a German vocabulary of >2.8K. We report quantitative and qualitative results for various SLT setups to underpin future research in this newly established field. The upper bound for translation performance is calculated at 19.26 BLEU-4, while our end-to-end frame-level and gloss-level tokenization networks were able to achieve 9.58 and 18.13 respectively.","中文标题":"神经手语翻译","摘要翻译":"手语识别（SLR）在过去二十年中一直是一个活跃的研究领域。然而，迄今为止的大多数研究都将SLR视为一个简单的手势识别问题。SLR试图识别一系列连续的手势，但忽略了手语与口语不同的丰富语法和语言结构。相比之下，我们引入了手语翻译（SLT）问题。在这里，目标是从手语视频中生成口语翻译，考虑到不同的词序和语法。我们在神经机器翻译（NMT）的框架中形式化了SLT，适用于端到端和预训练设置（使用专家知识）。这使我们能够共同学习空间表示、基础语言模型以及手语和口语之间的映射。为了评估神经SLT的性能，我们收集了第一个公开可用的连续SLT数据集，RWTH-PHOENIX-Weather 2014T。它为德国手语天气广播视频提供了口语翻译和词汇级别注释。我们的数据集包含超过95万帧，来自一个包含超过1K手语词汇的67K手势，以及来自一个包含超过2.8K德语词汇的99K单词。我们报告了各种SLT设置的定量和定性结果，以支持这一新建立领域的未来研究。翻译性能的上限计算为19.26 BLEU-4，而我们的端到端帧级和词汇级标记化网络分别能够达到9.58和18.13。","领域":"手语翻译/神经机器翻译/语言模型","问题":"如何从手语视频中生成口语翻译，考虑到手语与口语在词序和语法上的差异","动机":"现有的手语识别研究大多忽略了手语的丰富语法和语言结构，需要一种新的方法来准确翻译手语","方法":"在神经机器翻译的框架中形式化手语翻译问题，采用端到端和预训练设置，共同学习空间表示、基础语言模型以及手语和口语之间的映射","关键词":["手语翻译","神经机器翻译","语言模型"],"涉及的技术概念":"神经机器翻译（NMT）是一种使用神经网络进行语言翻译的技术，它通过学习大量双语文本数据来预测目标语言的句子。BLEU-4是一种评估机器翻译质量的指标，通过比较机器翻译输出和参考翻译之间的n-gram重叠来计算得分。端到端学习指的是直接从输入到输出进行学习，无需手动设计特征或中间步骤。预训练设置指的是在特定任务上训练模型之前，先在大量数据上进行预训练，以提高模型性能。"},{"order":805,"title":"Non-Local Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html","abstract":"Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.","中文标题":"非局部神经网络","摘要翻译":"卷积和循环操作是每次处理一个局部邻域的基本构建块。在本文中，我们提出了非局部操作作为一种通用的构建块家族，用于捕捉长距离依赖关系。受计算机视觉中经典的非局部均值方法的启发，我们的非局部操作计算一个位置的响应作为所有位置特征的加权和。这个构建块可以插入到许多计算机视觉架构中。在视频分类任务中，即使没有任何花哨的技巧，我们的非局部模型也能在Kinetics和Charades数据集上与当前竞赛获胜者竞争或超越。在静态图像识别中，我们的非局部模型在COCO任务套件上改进了对象检测/分割和姿态估计。代码将会公开。","领域":"视频分类/静态图像识别/对象检测","问题":"捕捉长距离依赖关系","动机":"为了改进视频分类和静态图像识别中的对象检测/分割和姿态估计性能","方法":"提出非局部操作作为一种通用的构建块家族，计算一个位置的响应作为所有位置特征的加权和","关键词":["非局部操作","长距离依赖","视频分类","静态图像识别","对象检测","姿态估计"],"涉及的技术概念":"非局部操作是一种计算一个位置的响应作为所有位置特征的加权和的方法，旨在捕捉长距离依赖关系。这种方法可以插入到多种计算机视觉架构中，用于改进视频分类和静态图像识别任务中的对象检测/分割和姿态估计性能。"},{"order":806,"title":"LAMV: Learning to Align and Match Videos With Kernelized Temporal Layers","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baraldi_LAMV_Learning_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baraldi_LAMV_Learning_to_CVPR_2018_paper.html","abstract":"This paper considers a learnable approach for comparing and aligning videos. Our architecture builds upon and revisits temporal match kernels within neural networks: we propose a new temporal layer that finds temporal alignments by maximizing the scores between two sequences of vectors, according to a time-sensitive similarity metric parametrized in the Fourier domain. We learn this layer with a temporal proposal strategy, in which we minimize a triplet loss that takes into account both the localization accuracy and the recognition rate. We evaluate our approach on video alignment, copy detection and event retrieval. Our approach outperforms the state on the art on temporal video alignment and video copy detection datasets in comparable setups. It also attains the best reported results for particular event search, while precisely aligning videos.","中文标题":"LAMV: 学习使用核化时间层对齐和匹配视频","摘要翻译":"本文考虑了一种可学习的方法来比较和对齐视频。我们的架构基于并重新审视了神经网络中的时间匹配核：我们提出了一种新的时间层，该层通过最大化两个向量序列之间的分数来找到时间对齐，根据在傅里叶域中参数化的时间敏感相似性度量。我们通过时间提议策略学习这一层，其中我们最小化了一个三重损失，该损失考虑了定位准确性和识别率。我们在视频对齐、复制检测和事件检索上评估了我们的方法。在可比较的设置中，我们的方法在时间视频对齐和视频复制检测数据集上优于现有技术。它还在特定事件搜索中取得了最佳报告结果，同时精确对齐视频。","领域":"视频分析/时间序列分析/事件检测","问题":"视频比较和对齐","动机":"提高视频对齐和匹配的准确性和效率，特别是在时间敏感的应用中","方法":"提出了一种新的时间层，通过最大化两个向量序列之间的分数来找到时间对齐，使用在傅里叶域中参数化的时间敏感相似性度量，并通过时间提议策略学习这一层，最小化考虑定位准确性和识别率的三重损失","关键词":["视频对齐","时间匹配核","傅里叶域","三重损失","事件检索"],"涉及的技术概念":"时间匹配核是一种用于比较时间序列数据的技术，通过计算序列之间的相似性来找到最佳对齐。傅里叶域参数化允许在频域中分析时间序列，提供时间敏感性的度量。三重损失是一种用于训练模型的损失函数，通过比较正样本、负样本和锚点样本来提高模型的区分能力。"},{"order":807,"title":"Optimizing Video Object Detection via a Scale-Time Lattice","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Optimizing_Video_Object_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Optimizing_Video_Object_CVPR_2018_paper.html","abstract":"High-performance object detection relies on expensive convolutional networks to compute features, often leading to significant challenges in applications, e.g. those that re- quire detecting objects from video streams in real time. The key to this problem is to trade accuracy for efficiency in an effective way, i.e. reducing the computing cost while maintaining competitive performance. To seek a good balance, previous efforts usually focus on optimizing the model architectures. This paper explores an alternative approach, that is, to reallocate the computation over a scale-time space. The basic idea is to perform expensive detection sparsely and propagate the results across both scales and time with substantially cheaper networks, by exploiting the strong correlations among them. Specifically, we present a unified framework that integrates detection, temporal propagation, and across-scale refinement on a Scale-Time Lattice. On this framework, one can explore various strategies to balance performance and cost. Taking advantage of this flexibility, we further develop an adaptive scheme with the detector invoked on demand and thus obtain improved tradeoff. On ImageNet VID dataset, the proposed method can achieve a competitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed tradeoff.","中文标题":"通过尺度-时间格子优化视频目标检测","摘要翻译":"高性能目标检测依赖于昂贵的卷积网络来计算特征，这往往在应用中带来重大挑战，例如需要从视频流中实时检测目标的情况。解决这一问题的关键在于以有效的方式在准确性和效率之间进行权衡，即在保持竞争力的同时降低计算成本。为了寻求良好的平衡，以往的努力通常集中在优化模型架构上。本文探索了一种替代方法，即在尺度-时间空间上重新分配计算。基本思想是通过利用它们之间的强相关性，稀疏地执行昂贵的检测，并通过显著更便宜的网络在尺度和时间上传播结果。具体来说，我们提出了一个统一的框架，该框架在尺度-时间格子上集成了检测、时间传播和跨尺度细化。在这个框架上，可以探索各种策略来平衡性能和成本。利用这种灵活性，我们进一步开发了一种自适应方案，根据需要调用检测器，从而获得改进的权衡。在ImageNet VID数据集上，所提出的方法可以在20 fps时实现竞争性的mAP 79.6%，或在62 fps时实现79.0%的mAP，作为性能/速度的权衡。","领域":"视频分析/目标检测/计算效率","问题":"如何在保持检测性能的同时降低视频目标检测的计算成本","动机":"为了在实时视频流中高效地进行目标检测，需要在准确性和计算效率之间找到良好的平衡","方法":"提出了一种在尺度-时间空间上重新分配计算的方法，通过稀疏地执行昂贵的检测并利用更便宜的网络在尺度和时间上传播结果，以及开发了一种自适应方案来根据需要调用检测器","关键词":["视频目标检测","计算效率","尺度-时间格子"],"涉及的技术概念":"卷积网络用于计算特征，尺度-时间格子用于集成检测、时间传播和跨尺度细化，自适应方案用于根据需要调用检测器"},{"order":808,"title":"Learning Compressible 360° Video Isomers","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Su_Learning_Compressible_360deg_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Su_Learning_Compressible_360deg_CVPR_2018_paper.html","abstract":"Standard video encoders developed for conventional narrow field-of-view video are widely applied to 360° video as well, with reasonable results. However, while this approach commits arbitrarily to a projection of the spherical frames, we observe that some orientations of a 360° video, once projected, are more compressible than others. We introduce an approach to predict the sphere rotation that will yield the maximal compression rate. Given video clips in their original encoding, a convolutional neural network learns the association between a clip’s visual content and its compressibility at different rotations of a cubemap projection. Given a novel video, our learning-based approach efficiently infers the most compressible direction in one shot, without repeated rendering and compression of the source video. We validate our idea on thousands of video clips and multiple popular video codecs. The results show that this untapped dimension of 360° compression has substantial potential—“good” rotations are typically 8−10% more compressible than bad ones, and our learning approach can predict them reliably 82% of the time.","中文标题":"学习可压缩的360°视频异构体","摘要翻译":"为传统窄视场视频开发的标准视频编码器也被广泛应用于360°视频，并取得了合理的结果。然而，尽管这种方法任意地承诺了球形帧的投影，我们观察到，一旦投影，360°视频的某些方向比其他方向更可压缩。我们引入了一种方法来预测将产生最大压缩率的球体旋转。给定原始编码的视频片段，卷积神经网络学习片段视觉内容与其在立方体映射投影不同旋转下的可压缩性之间的关联。给定一个新视频，我们基于学习的方法可以一次性高效推断出最可压缩的方向，而无需重复渲染和压缩源视频。我们在数千个视频片段和多个流行的视频编解码器上验证了我们的想法。结果表明，360°压缩的这一未开发维度具有巨大潜力——“好”旋转通常比“坏”旋转可压缩性高8-10%，而我们的学习方法可以可靠地预测它们82%的时间。","领域":"视频压缩/360°视频处理/卷积神经网络","问题":"如何提高360°视频的压缩效率","动机":"现有的标准视频编码器虽然可以应用于360°视频，但未考虑不同方向的可压缩性差异，存在优化空间。","方法":"使用卷积神经网络预测360°视频在不同旋转下的可压缩性，从而找到最可压缩的方向。","关键词":["视频压缩","360°视频","卷积神经网络","立方体映射投影"],"涉及的技术概念":{"卷积神经网络":"一种深度学习模型，用于学习视频片段视觉内容与其在不同旋转下的可压缩性之间的关联。","立方体映射投影":"一种将360°视频的球形帧投影到立方体六个面的技术，用于处理和分析360°视频。","视频编解码器":"用于压缩和解压缩视频的算法，本文中提到的视频编解码器包括多个流行的标准。"}},{"order":809,"title":"Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Long_Attention_Clusters_Purely_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Long_Attention_Clusters_Purely_CVPR_2018_paper.html","abstract":"Recently, substantial research effort has focused on how to apply CNNs or RNNs to better capture temporal patterns in videos, so as to improve the accuracy of video classification. In this paper, however, we show that temporal information, especially longer-term patterns, may not be necessary to achieve competitive results on common trimmed video classification datasets. We investigate the potential of a purely attention based local feature integration. Accounting for the characteristics of such features in video classification, we propose a local feature integration framework based on attention clusters, and introduce a shifting operation to capture more diverse signals. We carefully analyze and compare the effect of different attention mechanisms, cluster sizes, and the use of the shifting operation, and also investigate the combination of attention clusters for multimodal integration. We demonstrate the effectiveness of our framework on three real-world video classification datasets. Our model achieves competitive results across all of these. In particular, on the large-scale Kinetics dataset, our framework obtains an excellent single model accuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5 accuracy on the validation set.","中文标题":"注意力集群：纯粹基于注意力的局部特征整合用于视频分类","摘要翻译":"最近，大量研究工作集中在如何应用CNNs或RNNs以更好地捕捉视频中的时间模式，从而提高视频分类的准确性。然而，在本文中，我们展示了在常见的修剪视频分类数据集上，时间信息，尤其是长期模式，可能不是实现竞争性结果的必要条件。我们研究了纯粹基于注意力的局部特征整合的潜力。考虑到视频分类中此类特征的特点，我们提出了一个基于注意力集群的局部特征整合框架，并引入了一个移位操作以捕捉更多样化的信号。我们仔细分析和比较了不同注意力机制、集群大小以及移位操作的使用效果，并研究了注意力集群在多模态整合中的组合。我们在三个真实世界的视频分类数据集上展示了我们框架的有效性。我们的模型在所有这些数据集上都取得了竞争性的结果。特别是在大规模Kinetics数据集上，我们的框架在验证集上获得了79.4%的top-1准确率和94.0%的top-5准确率，表现出色。","领域":"视频分类/注意力机制/特征整合","问题":"如何在不依赖时间信息的情况下提高视频分类的准确性","动机":"探索在视频分类中不依赖时间信息，尤其是长期模式，仍能实现竞争性结果的可能性","方法":"提出了一个基于注意力集群的局部特征整合框架，并引入了移位操作以捕捉更多样化的信号","关键词":["视频分类","注意力机制","特征整合","移位操作","多模态整合"],"涉及的技术概念":"CNNs（卷积神经网络）、RNNs（循环神经网络）、注意力机制、局部特征整合、移位操作、多模态整合"},{"order":810,"title":"What Have We Learned From Deep Representations for Action Recognition?","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Feichtenhofer_What_Have_We_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html","abstract":"As the success of deep models has led to their deployment in all areas of computer vision, it is increasingly  important  to understand how these representations work and what they are capturing.  In this paper, we shed light on deep spatiotemporal representations by visualizing what two-stream models have learned in order to recognize actions in video. We show that local detectors for appearance and motion objects arise to form distributed representations for recognizing human actions.  Key observations include the following. First, cross-stream fusion enables the learning of true spatiotemporal features rather than simply separate appearance and motion features. Second, the networks can learn local representations that are highly class specific, but also generic representations that can serve a range of classes.  Third, throughout the hierarchy of the network, features become more abstract and show increasing invariance to aspects of the data that are unimportant to desired distinctions (e.g. motion patterns across various speeds). Fourth, visualizations can be used not only to shed light on learned representations, but also to reveal idiosyncracies of training data and to explain failure cases of the system.","中文标题":"我们从深度表示中学到了什么用于动作识别？","摘要翻译":"随着深度模型的成功导致它们被部署在计算机视觉的所有领域，理解这些表示如何工作以及它们捕捉到了什么变得越来越重要。在本文中，我们通过可视化双流模型学习到的内容来阐明深度时空表示，以识别视频中的动作。我们展示了外观和运动物体的局部检测器出现，形成了用于识别人体动作的分布式表示。关键观察包括以下几点。首先，跨流融合使得学习真正的时空特征成为可能，而不仅仅是单独的外观和运动特征。其次，网络可以学习到高度类别特定的局部表示，也可以学习到能够服务于一系列类别的通用表示。第三，在网络层次结构的整个过程中，特征变得更加抽象，并显示出对数据中不重要方面的增加不变性（例如，各种速度下的运动模式）。第四，可视化不仅可以用来阐明学习到的表示，还可以揭示训练数据的特性，并解释系统的失败案例。","领域":"动作识别/时空特征学习/深度学习模型解释","问题":"理解深度模型在动作识别中的表示工作方式和捕捉内容","动机":"随着深度模型在计算机视觉领域的广泛应用，理解这些模型如何工作以及它们捕捉到了什么变得尤为重要","方法":"通过可视化双流模型学习到的内容来阐明深度时空表示，以识别视频中的动作","关键词":["动作识别","时空特征","深度学习模型解释"],"涉及的技术概念":"双流模型、跨流融合、局部检测器、分布式表示、类别特定的局部表示、通用表示、网络层次结构、特征抽象、数据不变性、可视化"},{"order":811,"title":"Controllable Video Generation With Sparse Trajectories","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hao_Controllable_Video_Generation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hao_Controllable_Video_Generation_CVPR_2018_paper.html","abstract":"Video generation and manipulation is an important yet challenging task in computer vision. Existing methods usually lack ways to explicitly control the synthesized motion. In this work, we present a conditional video generation model that allows detailed control over the motion of the generated video. Given the first frame and sparse motion trajectories specified by users, our model can synthesize a video with corresponding appearance and motion. We propose to combine the advantage of copying pixels from the given frame and hallucinating the lightness difference from scratch which help generate sharp video while keeping the model robust to occlusion and lightness change. We also propose a training paradigm that calculate trajectories from video clips, which eliminated the need of annotated training data. Experiments on several standard benchmarks demonstrate that our approach can generate realistic videos comparable to state-of-the-art video generation and video prediction methods while the motion of the generated videos can correspond well with user input.","中文标题":"可控视频生成与稀疏轨迹","摘要翻译":"视频生成和操控是计算机视觉中一个重要但具有挑战性的任务。现有方法通常缺乏明确控制合成运动的方式。在这项工作中，我们提出了一个条件视频生成模型，该模型允许对生成视频的运动进行详细控制。给定第一帧和用户指定的稀疏运动轨迹，我们的模型可以合成具有相应外观和运动的视频。我们提出结合从给定帧复制像素和从头开始幻化亮度差异的优势，这有助于生成清晰的视频，同时保持模型对遮挡和亮度变化的鲁棒性。我们还提出了一种训练范式，该范式从视频片段计算轨迹，从而消除了对注释训练数据的需求。在几个标准基准上的实验表明，我们的方法可以生成与最先进的视频生成和视频预测方法相媲美的逼真视频，同时生成视频的运动可以与用户输入很好地对应。","领域":"视频生成/运动控制/条件生成模型","问题":"现有视频生成方法缺乏明确控制合成运动的能力","动机":"为了实现对生成视频运动的详细控制，提高视频生成的质量和可控性","方法":"提出了一种条件视频生成模型，结合从给定帧复制像素和从头开始幻化亮度差异的优势，以及一种从视频片段计算轨迹的训练范式","关键词":["视频生成","运动控制","条件生成模型","稀疏轨迹","亮度差异"],"涉及的技术概念":"条件视频生成模型是一种能够根据特定条件（如第一帧和用户指定的稀疏运动轨迹）生成视频的模型。稀疏轨迹指的是视频中物体运动的简化表示，只包含关键的运动信息。亮度差异幻化是指模型能够生成视频中物体在不同亮度条件下的变化，这对于处理遮挡和亮度变化非常重要。"},{"order":812,"title":"Representing and Learning High Dimensional Data With the Optimal Transport Map From a Probabilistic Viewpoint","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Representing_and_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Representing_and_Learning_CVPR_2018_paper.html","abstract":"In this paper, we propose a generative model in the space of diffeomorphic deformation maps. More precisely, we utilize the Kantarovich-Wasserstein metric and accompanying geometry to represent an image as a deformation from templates. Moreover, we incorporate a probabilistic viewpoint by assuming that each image is locally generated from a reference image. We capture the local structure by modelling the tangent planes at reference images. %; we assume that each image is generated from one of finite number of tangent planes. % by an unobserved discrete random variable that indexes the tangent plane the image belongs to.   Once basis vectors for each tangent plane are learned via probabilistic PCA, we can sample a local coordinate, that can be inverted back to image space exactly. With experiments using 4 different datasets, we show that the generative tangent plane model in the optimal transport (OT) manifold can be learned with small numbers of images and can be used to create infinitely many \`unseen' images. In addition, the Bayesian classification accompanied with the probabilist modeling of the tangent planes shows improved accuracy over that done in the image space. Combining the results of our experiments supports our claim that certain datasets can be better represented with the Kantarovich-Wasserstein metric. We envision that the proposed method could be a practical solution to learning and representing data that is generated with templates in situatons where only limited numbers of data points are available.","中文标题":"从概率视角表示和学习高维数据的最优传输映射","摘要翻译":"在本文中，我们提出了一种在微分同胚变形映射空间中的生成模型。更准确地说，我们利用Kantarovich-Wasserstein度量及其伴随的几何学来表示图像作为模板的变形。此外，我们通过假设每个图像是从参考图像局部生成的，引入了一个概率视角。我们通过建模参考图像的切平面来捕捉局部结构。一旦通过概率PCA学习了每个切平面的基向量，我们就可以采样一个局部坐标，该坐标可以精确地反转到图像空间。通过使用4个不同数据集的实验，我们展示了在最优传输（OT）流形中的生成切平面模型可以用少量图像学习，并可用于创建无限多的“未见过的”图像。此外，伴随切平面概率建模的贝叶斯分类显示出比在图像空间中进行的分类更高的准确性。结合我们的实验结果支持我们的主张，即某些数据集可以用Kantarovich-Wasserstein度量更好地表示。我们设想，所提出的方法可以成为在只有有限数量的数据点可用的情况下学习和表示由模板生成的数据的实用解决方案。","领域":"生成模型/最优传输/概率建模","问题":"如何在有限数据点的情况下学习和表示由模板生成的高维数据","动机":"探索一种能够有效表示和学习高维数据的方法，特别是在数据点有限的情况下，以提高数据表示的准确性和生成新数据的能力。","方法":"利用Kantarovich-Wasserstein度量和微分同胚变形映射空间中的生成模型，通过概率PCA学习切平面的基向量，实现局部坐标的采样和反转到图像空间。","关键词":["生成模型","最优传输","概率建模","Kantarovich-Wasserstein度量","微分同胚变形映射","概率PCA"],"涉及的技术概念":"Kantarovich-Wasserstein度量是一种用于衡量两个概率分布之间距离的度量，常用于最优传输问题。微分同胚变形映射指的是保持拓扑结构不变的连续可逆映射。概率PCA是一种降维技术，通过概率模型来学习数据的低维表示。"},{"order":813,"title":"CLIP-Q: Deep Network Compression Learning by In-Parallel Pruning-Quantization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tung_CLIP-Q_Deep_Network_CVPR_2018_paper.html","abstract":"Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern deep networks contain millions of learned weights; a more efficient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with fine-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by In-Parallel Pruning-Quantization) compresses AlexNet by 51-fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet.","中文标题":"CLIP-Q：通过并行剪枝-量化进行深度网络压缩学习","摘要翻译":"深度神经网络在图像分类和物体检测等视觉识别任务上实现了最先进的准确率。然而，现代深度网络包含数百万个学习权重；更高效地利用计算资源将有助于各种部署场景，从资源受限的嵌入式平台到运行网络集合的计算集群。在本文中，我们将网络剪枝和权重量化结合到一个单一的学习框架中，该框架联合执行剪枝和量化，并与微调并行进行。这使我们能够利用剪枝和量化的互补性质，并从过早的剪枝错误中恢复，这是当前两阶段方法所不可能的。我们提出的CLIP-Q方法（通过并行剪枝-量化进行压缩学习）将AlexNet压缩了51倍，GoogLeNet压缩了10倍，ResNet-50压缩了15倍，同时保持了在ImageNet上的未压缩网络准确率。","领域":"网络压缩/模型优化/深度学习","问题":"深度神经网络在视觉识别任务中的计算资源利用效率问题","动机":"提高深度神经网络在各种部署场景下的计算资源利用效率，特别是在资源受限的嵌入式平台和计算集群上。","方法":"提出了一种新的学习框架CLIP-Q，该框架联合执行网络剪枝和权重量化，并与微调并行进行，以利用剪枝和量化的互补性质，并从过早的剪枝错误中恢复。","关键词":["网络剪枝","权重量化","模型压缩","并行学习","微调"],"涉及的技术概念":"网络剪枝是一种减少神经网络中不必要权重的方法，以减少模型的大小和计算需求。权重量化是通过减少权重表示的精度来压缩模型的技术。并行学习指的是同时进行多个学习任务，以提高学习效率和效果。微调是指在预训练模型的基础上进行进一步的训练，以适应特定的任务或数据集。"},{"order":814,"title":"Inference in Higher Order MRF-MAP Problems With Small and Large Cliques","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shanu_Inference_in_Higher_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shanu_Inference_in_Higher_CVPR_2018_paper.html","abstract":"Higher Order MRF-MAP formulation has been a popular technique for solving many problems in computer vision. Inference in a general MRF-MAP problem is NP Hard, but can be performed in polynomial time for the special case when potential functions are submodular. Two popular combinatorial approaches for solving such formulations are flow based and polyhedral approaches. Flow based approaches work well with small cliques and in that mode can handle problems with millions of variables. Polyhedral approaches can handle large cliques but in small numbers. We show in this paper that the variables in these seemingly disparate techniques can be mapped to each other. This allows us to combine the two styles in a joint framework exploiting the strength of both of them. Using the proposed joint framework, we are able to perform tractable inference in MRF-MAP problems with millions of variables and a mix of small and large cliques, a formulation which can not be solved by either of the two styles individually. We show applicability of this hybrid framework on object segmentation problem as an example of a situation where quality of results is significantly better than systems which are based only on the use of small or large cliques.","中文标题":"高阶MRF-MAP问题中的推理：小团与大团","摘要翻译":"高阶MRF-MAP公式已成为解决计算机视觉中许多问题的流行技术。在一般情况下，MRF-MAP问题的推理是NP难的，但在势函数为子模的特殊情况下，可以在多项式时间内进行。解决此类公式的两种流行组合方法是基于流的方法和多面体方法。基于流的方法在小团中表现良好，并且可以处理具有数百万个变量的问题。多面体方法可以处理大团，但数量较少。我们在本文中展示了这些看似不同的技术中的变量可以相互映射。这使我们能够在一个联合框架中结合这两种风格，利用它们各自的优势。使用提出的联合框架，我们能够在具有数百万个变量和混合小团与大团的MRF-MAP问题中进行可处理的推理，这种公式无法由两种风格单独解决。我们展示了这种混合框架在对象分割问题上的适用性，作为一个例子，其结果质量显著优于仅使用小团或大团的系统。","领域":"图像分割/优化算法/组合优化","问题":"解决高阶MRF-MAP问题中的推理，特别是在具有小团和大团混合的情况下","动机":"为了在具有数百万个变量和混合小团与大团的MRF-MAP问题中进行有效的推理，需要结合基于流的方法和多面体方法的优势","方法":"提出了一种联合框架，将基于流的方法和多面体方法结合起来，以利用它们各自的优势","关键词":["高阶MRF-MAP","推理","子模势函数","基于流的方法","多面体方法","对象分割"],"涉及的技术概念":"MRF-MAP问题是指马尔可夫随机场最大后验概率问题，是计算机视觉中用于图像分割等问题的数学模型。子模势函数是一种特殊的函数，它在组合优化中非常重要，因为子模性保证了某些优化问题可以在多项式时间内解决。基于流的方法通常用于处理具有小团的问题，而多面体方法则适用于处理大团但数量较少的问题。"},{"order":815,"title":"ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_ROAD_Reality_Oriented_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_ROAD_Reality_Oriented_CVPR_2018_paper.html","abstract":"Exploiting synthetic data to learn deep models has attracted increasing attention in recent years. However, the intrinsic domain difference between synthetic and real images usually causes a significant performance drop when applying the learned model to real world scenarios. This is mainly due to two reasons: 1) the model overfits to synthetic images, making the convolutional filters incompetent to extract informative representation for real images; 2) there is a distribution difference between synthetic and real data, which is also known as the domain adaptation problem. To this end, we propose a new reality oriented adaptation approach for urban scene semantic segmentation by learning from synthetic data. First, we propose a target guided distillation approach to learn the real image style, which is achieved by training the segmentation model to imitate a pretrained real style model using real images. Second, we further take advantage of the intrinsic spatial structure presented in urban scene images, and propose a spatial-aware adaptation scheme to effectively align the distribution of two domains. These two modules can be readily integrated with existing state-of-the-art semantic segmentation networks to improve their generalizability when adapting from synthetic to real urban scenes. We evaluate the proposed method on Cityscapes dataset by adapting from GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness of our method.","中文标题":"面向现实的适应：城市场景语义分割","摘要翻译":"近年来，利用合成数据学习深度模型引起了越来越多的关注。然而，合成图像与真实图像之间的内在领域差异通常会导致将学习到的模型应用于现实世界场景时性能显著下降。这主要是由于两个原因：1）模型对合成图像过拟合，使得卷积滤波器无法提取真实图像的信息表示；2）合成数据与真实数据之间存在分布差异，这也被称为领域适应问题。为此，我们提出了一种新的面向现实的适应方法，通过从合成数据中学习来进行城市场景语义分割。首先，我们提出了一种目标引导的蒸馏方法，通过训练分割模型模仿使用真实图像预训练的真实风格模型来学习真实图像风格。其次，我们进一步利用城市场景图像中呈现的内在空间结构，提出了一种空间感知的适应方案，以有效对齐两个领域的分布。这两个模块可以很容易地与现有的最先进的语义分割网络集成，以提高它们从合成到真实城市场景适应时的泛化能力。我们在Cityscapes数据集上通过从GTAV和SYNTHIA数据集适应来评估所提出的方法，结果证明了我们方法的有效性。","领域":"语义分割/领域适应/城市场景理解","问题":"合成数据与真实数据之间的领域差异导致模型在真实世界场景中性能下降","动机":"提高模型从合成数据到真实世界场景的适应能力，以改善城市场景语义分割的泛化性能","方法":"提出目标引导的蒸馏方法和空间感知的适应方案，以学习真实图像风格并有效对齐合成与真实数据的分布","关键词":["语义分割","领域适应","城市场景理解","目标引导蒸馏","空间感知适应"],"涉及的技术概念":"目标引导蒸馏方法通过模仿预训练的真实风格模型来学习真实图像风格；空间感知适应方案利用城市场景图像的内在空间结构来对齐合成与真实数据的分布。"},{"order":816,"title":"Eye In-Painting With Exemplar Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dolhansky_Eye_In-Painting_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dolhansky_Eye_In-Painting_With_CVPR_2018_paper.html","abstract":"This paper introduces a novel approach to in-painting where the identity of the object to remove or change is preserved and accounted for at inference time: Exemplar GANs (ExGANs). ExGANs are a type of conditional GAN that utilize exemplar information to produce high-quality, personalized in-painting results. We propose using exemplar information in the form of a reference image of the region to in-paint, or a perceptual code describing that object. Unlike previous conditional GAN formulations, this extra information can be inserted at multiple points within the adversarial network, thus increasing its descriptive power. We show that ExGANs can produce photo-realistic personalized in-painting results that are both perceptually and semantically plausible by applying them to the task of closed-to-open eye in-painting in natural pictures. A new benchmark dataset is also introduced for the task of eye in-painting for future comparisons.","中文标题":"使用示例生成对抗网络进行眼睛修复","摘要翻译":"本文介绍了一种新颖的修复方法，其中在推理时保留并考虑了要移除或更改对象的身份：示例生成对抗网络（ExGANs）。ExGANs是一种条件生成对抗网络，利用示例信息来产生高质量、个性化的修复结果。我们建议使用待修复区域的参考图像或描述该对象的感知代码形式的示例信息。与之前的条件生成对抗网络公式不同，这些额外信息可以在对抗网络中的多个点插入，从而增加其描述能力。我们展示了ExGANs通过在自然图片中应用闭眼到睁眼的修复任务，可以产生既感知上又语义上合理的照片级个性化修复结果。还为眼睛修复任务引入了一个新的基准数据集，以供未来比较使用。","领域":"图像修复/生成对抗网络/个性化图像处理","问题":"如何在图像修复过程中保留或更改对象的身份，并产生高质量、个性化的修复结果","动机":"为了在图像修复任务中，不仅保留对象的身份，还能产生既感知上又语义上合理的修复结果，提出了使用示例生成对抗网络的方法","方法":"提出了一种新颖的示例生成对抗网络（ExGANs），利用示例信息（如参考图像或感知代码）在对抗网络中的多个点插入，以增加描述能力，从而产生高质量、个性化的修复结果","关键词":["图像修复","生成对抗网络","个性化图像处理"],"涉及的技术概念":{"示例生成对抗网络（ExGANs）":"一种条件生成对抗网络，利用示例信息来产生高质量、个性化的修复结果","条件生成对抗网络":"一种生成对抗网络，通过条件信息（如类别标签、图像等）来指导生成过程","感知代码":"描述对象特征的代码，用于在图像修复过程中提供额外的信息"}},{"order":817,"title":"ClcNet: Improving the Efficiency of Convolutional Neural Network Using Channel Local Convolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ClcNet_Improving_the_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_ClcNet_Improving_the_CVPR_2018_paper.html","abstract":"Depthwise convolution and grouped convolution has been successfully applied to improve the efficiency of convolutional neural network (CNN). We suggest that these models can be considered as special cases of a generalized convolution operation, named channel local convolution(CLC), where an output channel is computed using a subset of the input channels. This definition entails computation dependency relations between input and output channels, which can be represented by a channel dependency graph(CDG). By modifying the CDG of grouped convolution, a new CLC kernel named interlaced grouped convolution (IGC) is created. Stacking IGC and GC kernels results in a convolution block (named CLC Block) for approximating regular convolution. By resorting to the CDG as an analysis tool, we derive the rule for setting the meta-parameters of IGC and GC and the framework for minimizing the computational cost. A new CNN model named clcNet is then constructed using CLC blocks, which shows significantly higher computational efficiency and fewer parameters compared to state-of-the-art networks, when being tested using the ImageNet-1K dataset.","中文标题":"ClcNet: 使用通道局部卷积提高卷积神经网络的效率","摘要翻译":"深度卷积和分组卷积已成功应用于提高卷积神经网络（CNN）的效率。我们提出，这些模型可以被视为一种广义卷积操作的特例，称为通道局部卷积（CLC），其中输出通道是使用输入通道的一个子集计算的。这一定义导致了输入和输出通道之间的计算依赖关系，这些关系可以通过通道依赖图（CDG）来表示。通过修改分组卷积的CDG，创建了一个新的CLC内核，称为交错分组卷积（IGC）。堆叠IGC和GC内核形成了一个卷积块（称为CLC块），用于近似常规卷积。通过将CDG作为分析工具，我们得出了设置IGC和GC元参数的规则以及最小化计算成本的框架。然后，使用CLC块构建了一个新的CNN模型，名为clcNet，当使用ImageNet-1K数据集进行测试时，显示出比最先进的网络显著更高的计算效率和更少的参数。","领域":"卷积神经网络优化/模型压缩/计算效率提升","问题":"提高卷积神经网络的计算效率和减少参数数量","动机":"现有的深度卷积和分组卷积虽然提高了CNN的效率，但仍有改进空间，特别是在计算效率和参数数量方面。","方法":"提出了一种广义卷积操作——通道局部卷积（CLC），并通过修改通道依赖图（CDG）创建了交错分组卷积（IGC）。通过堆叠IGC和GC内核形成CLC块，用于近似常规卷积，并构建了新的CNN模型clcNet。","关键词":["通道局部卷积","交错分组卷积","卷积神经网络优化","模型压缩","计算效率提升"],"涉及的技术概念":{"深度卷积":"一种卷积操作，每个输入通道与一个单独的卷积核进行卷积，然后所有结果相加。","分组卷积":"将输入通道分成若干组，每组使用不同的卷积核进行卷积，最后将所有结果合并。","通道局部卷积（CLC）":"一种广义卷积操作，输出通道是使用输入通道的一个子集计算的。","通道依赖图（CDG）":"表示输入和输出通道之间计算依赖关系的图。","交错分组卷积（IGC）":"通过修改分组卷积的CDG创建的一种新的CLC内核。","CLC块":"通过堆叠IGC和GC内核形成的卷积块，用于近似常规卷积。","clcNet":"使用CLC块构建的新的CNN模型，旨在提高计算效率和减少参数数量。"}},{"order":818,"title":"Towards Effective Low-Bitwidth Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhuang_Towards_Effective_Low-Bitwidth_CVPR_2018_paper.html","abstract":"This paper tackles the problem of training a deep convolutional neural network with both low-precision weights and low-bitwidth activations. Optimizing a low-precision network is very challenging since the training process can easily get trapped in a poor local minima, which results in substantial accuracy loss. To mitigate this problem, we propose three simple-yet-effective approaches to improve the network training.  First, we propose to use a two-stage optimization strategy to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and then quantized activations. This is in contrast to the traditional methods which optimize them simultaneously. Second, following a similar spirit of the first method, we propose another progressive optimization approach which progressively decreases the bit-width from high-precision to low-precision during the course of training. Third, we adopt a novel learning scheme to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training.  Extensive experiments on various datasets (ie, CIFAR-100 and ImageNet) show the effectiveness of the proposed methods. To highlight, using our methods to train a 4-bit precision network leads to no performance decrease in comparison with its full-precision counterpart with standard network architectures (ie, AlexNet and ResNet-50).","中文标题":"迈向有效的低比特宽度卷积神经网络","摘要翻译":"本文解决了训练具有低精度权重和低比特宽度激活的深度卷积神经网络的问题。优化低精度网络非常具有挑战性，因为训练过程很容易陷入较差的局部最小值，从而导致显著的精度损失。为了缓解这个问题，我们提出了三种简单而有效的方法来改进网络训练。首先，我们提出使用两阶段优化策略逐步找到良好的局部最小值。具体来说，我们建议首先优化具有量化权重的网络，然后优化量化激活。这与传统方法不同，传统方法是同时优化它们。其次，遵循第一种方法的类似精神，我们提出了另一种渐进优化方法，在训练过程中逐步从高精度降低到低精度。第三，我们采用了一种新颖的学习方案，联合训练一个全精度模型和低精度模型。通过这样做，全精度模型提供了指导低精度模型训练的提示。在各种数据集（即CIFAR-100和ImageNet）上的大量实验显示了所提出方法的有效性。值得一提的是，使用我们的方法训练4位精度网络，与使用标准网络架构（即AlexNet和ResNet-50）的全精度对应物相比，性能没有下降。","领域":"神经网络优化/量化技术/深度学习训练","问题":"训练具有低精度权重和低比特宽度激活的深度卷积神经网络","动机":"优化低精度网络非常具有挑战性，因为训练过程很容易陷入较差的局部最小值，从而导致显著的精度损失","方法":"提出了三种方法：两阶段优化策略、渐进优化方法和联合训练全精度模型与低精度模型","关键词":["低精度网络","量化权重","量化激活","渐进优化","联合训练"],"涉及的技术概念":"量化技术指的是将网络中的权重和激活值从高精度（如32位浮点数）转换为低精度（如4位整数）的过程，以减少模型大小和加速计算。渐进优化方法是指在训练过程中逐步降低精度，以帮助网络更好地适应低精度条件。联合训练是指同时训练一个全精度模型和一个低精度模型，利用全精度模型的输出来指导低精度模型的训练。"},{"order":819,"title":"Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kuen_Stochastic_Downsampling_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kuen_Stochastic_Downsampling_for_CVPR_2018_paper.html","abstract":"It is desirable to train convolutional networks (CNNs) to run more efficiently during inference. In many cases however, the computational budget that the system has for inference cannot be known beforehand during training, or the inference budget is dependent on the changing real-time resource availability. Thus, it is inadequate to train just inference-efficient CNNs, whose inference costs are not adjustable and cannot adapt to varied inference budgets. We propose a novel approach for cost-adjustable inference in CNNs - Stochastic Downsampling Point (SDPoint). During training, SDPoint applies feature map downsampling to a random point in the layer hierarchy, with a random downsampling ratio. The different stochastic downsampling configurations known as SDPoint instances (of the same model) have computational costs different from each other, while being trained to minimize the same prediction loss. Sharing network parameters across different instances provides significant regularization boost. During inference, one may handpick a SDPoint instance that best fits the inference budget. The effectiveness of SDPoint, as both a cost-adjustable inference approach and a regularizer, is validated through extensive experiments on image classification.","中文标题":"卷积网络中用于成本可调推理和改进正则化的随机下采样","摘要翻译":"期望训练卷积网络（CNNs）以在推理过程中更高效地运行。然而，在许多情况下，系统在训练期间无法预先知道推理的计算预算，或者推理预算依赖于实时资源可用性的变化。因此，仅训练推理效率高的CNNs是不够的，因为它们的推理成本不可调整，无法适应不同的推理预算。我们提出了一种新颖的方法，用于CNNs中的成本可调推理——随机下采样点（SDPoint）。在训练期间，SDPoint在层层次结构中的随机点应用特征图下采样，采用随机下采样比率。不同的随机下采样配置，称为SDPoint实例（同一模型的不同实例），具有彼此不同的计算成本，同时被训练以最小化相同的预测损失。在不同实例之间共享网络参数提供了显著的正则化提升。在推理过程中，可以选择最适合推理预算的SDPoint实例。SDPoint的有效性，作为一种成本可调推理方法和正则化器，通过在图像分类上的广泛实验得到了验证。","领域":"卷积神经网络/图像分类/正则化技术","问题":"如何在卷积网络中实现成本可调的推理和改进的正则化","动机":"由于推理计算预算在训练期间无法预先知道或依赖于实时资源可用性的变化，需要一种方法使卷积网络在推理时能够适应不同的计算预算。","方法":"提出随机下采样点（SDPoint）方法，在训练期间对卷积网络的特征图进行随机下采样，以生成具有不同计算成本的模型实例，这些实例共享网络参数以提升正则化效果，并在推理时根据预算选择合适的实例。","关键词":["卷积神经网络","随机下采样","成本可调推理","正则化","图像分类"],"涉及的技术概念":"卷积网络（CNNs）是一种深度学习模型，广泛用于图像识别和分类任务。随机下采样是一种减少特征图尺寸的技术，可以降低计算成本。正则化是一种用于防止模型过拟合的技术，通过共享网络参数在不同实例之间实现。"},{"order":820,"title":"Face Aging With Identity-Preserved Conditional Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Face_Aging_With_CVPR_2018_paper.html","abstract":"Face aging is of great importance for cross-age recognition and entertainment related applications. However, the lack of labeled faces of the same person across a long age range makes it challenging. Because of different aging speed of different persons, our face aging approach aims at synthesizing a face whose target age lies in some given age group instead of synthesizing a face with a certain age. By grouping faces with target age together, the objective of face aging is equivalent to transferring aging patterns of faces within the target age group to the face whose aged face is to be synthesized. Meanwhile, the synthesized face should have the same identity with the input face. Thus we propose an Identity-Preserved Conditional Generative Adversarial Networks (IPCGANs) framework, in which a Conditional Generative Adversarial Networks module functions as generating a face that looks realistic and is with the target age, an identity-preserved module preserves the identity information and an age classifier forces the generated face with the target age. Both qualitative and quantitative experiments show that our method can generate more realistic faces in terms of image quality, person identity and age consistency with human observations.","中文标题":"基于身份保持的条件生成对抗网络的人脸老化","摘要翻译":"人脸老化对于跨年龄识别和娱乐相关应用极为重要。然而，由于缺乏同一人在长时间跨度内的标记人脸，这使得任务具有挑战性。由于不同人的老化速度不同，我们的人脸老化方法旨在合成一个目标年龄位于给定年龄组的人脸，而不是合成具有特定年龄的人脸。通过将目标年龄的人脸分组，人脸老化的目标等同于将目标年龄组内的老化模式转移到需要合成老化人脸的人脸上。同时，合成的人脸应与输入人脸具有相同的身份。因此，我们提出了一个身份保持的条件生成对抗网络（IPCGANs）框架，其中条件生成对抗网络模块负责生成看起来真实且具有目标年龄的人脸，身份保持模块保留身份信息，年龄分类器强制生成的人脸具有目标年龄。定性和定量实验均表明，我们的方法在图像质量、人物身份和年龄一致性方面能够生成更符合人类观察的真实人脸。","领域":"人脸老化/生成对抗网络/身份识别","问题":"如何在没有长时间跨度内同一人的标记人脸的情况下，合成具有目标年龄且保持身份信息的人脸","动机":"解决跨年龄识别和娱乐相关应用中的人脸老化问题，特别是在缺乏长时间跨度内同一人的标记人脸的情况下","方法":"提出身份保持的条件生成对抗网络（IPCGANs）框架，包括条件生成对抗网络模块、身份保持模块和年龄分类器，以生成真实且具有目标年龄的人脸","关键词":["人脸老化","生成对抗网络","身份识别"],"涉及的技术概念":"条件生成对抗网络（Conditional Generative Adversarial Networks, CGANs）是一种生成模型，它通过条件信息（如年龄）来指导生成过程。身份保持模块用于确保生成的人脸与输入人脸具有相同的身份信息。年龄分类器用于确保生成的人脸符合目标年龄。"},{"order":821,"title":"Unsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lv_Unsupervised_Cross-Dataset_Person_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lv_Unsupervised_Cross-Dataset_Person_CVPR_2018_paper.html","abstract":"Most of the proposed person re-identification algorithms conduct supervised training and testing on single labeled datasets with small size, so directly deploying these trained models to a large-scale real-world camera network may lead to poor performance due to underfitting. It is challenging to incrementally optimize the models by using the abundant unlabeled data collected from the target domain. To address this challenge, we propose an unsupervised incremental learning algorithm, TFusion, which is aided by the transfer learning of the pedestrians' spatio-temporal patterns in the target domain. Specifically, the algorithm firstly transfers the visual classifier trained from small labeled source dataset to the unlabeled target dataset so as to learn the pedestrians' spatial-temporal patterns. Secondly, a Bayesian fusion model is proposed to combine the learned spatio-temporal patterns with visual features to achieve a significantly improved classifier. Finally, we propose a learning-to-rank based mutual promotion procedure to incrementally optimize the classifiers based on the unlabeled data in the target domain. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm gains significant improvement compared with the state-of-art cross-dataset unsupervised person re-identification algorithms.","中文标题":"通过时空模式迁移学习实现无监督跨数据集行人重识别","摘要翻译":"大多数提出的行人重识别算法在单一的小规模标注数据集上进行监督训练和测试，因此直接将这些训练好的模型部署到大规模的现实世界摄像头网络中可能会由于欠拟合而导致性能不佳。利用从目标域收集的大量未标注数据逐步优化模型具有挑战性。为了解决这一挑战，我们提出了一种无监督的增量学习算法TFusion，该算法通过迁移学习目标域中行人的时空模式来辅助。具体来说，该算法首先将从小的标注源数据集训练得到的视觉分类器迁移到未标注的目标数据集，以学习行人的时空模式。其次，提出了一种贝叶斯融合模型，将学习到的时空模式与视觉特征结合起来，以实现显著改进的分类器。最后，我们提出了一种基于学习排序的相互促进过程，以基于目标域中的未标注数据逐步优化分类器。基于多个真实监控数据集的综合实验表明，与最先进的跨数据集无监督行人重识别算法相比，我们的算法获得了显著的改进。","领域":"行人重识别/迁移学习/增量学习","问题":"解决在大规模现实世界摄像头网络中部署行人重识别模型时由于欠拟合导致的性能不佳问题","动机":"利用目标域中大量未标注数据逐步优化模型，提高行人重识别的准确性和适应性","方法":"提出了一种无监督的增量学习算法TFusion，通过迁移学习目标域中行人的时空模式，结合贝叶斯融合模型和学习排序的相互促进过程，逐步优化分类器","关键词":["行人重识别","迁移学习","增量学习","贝叶斯融合","学习排序"],"涉及的技术概念":"时空模式迁移学习、贝叶斯融合模型、学习排序、无监督学习、增量学习"},{"order":822,"title":"Feature Quantization for Defending Against Distortion of Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Feature_Quantization_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Feature_Quantization_for_CVPR_2018_paper.html","abstract":"In this work, we address the problem of improving robustness of convolutional neural networks (CNNs) to image distortion. We argue that higher moment statistics of feature distributions can be shifted due to image distortion, and the shift leads to performance decrease and cannot be reduced by ordinary normalization methods as observed in our experimental analyses. In order to mitigate this effect, we propose an approach base on feature quantization. To be specific, we propose to employ three different types of additional non-linearity in CNNs: i) a floor function with scalable resolution, ii) a power function with learnable exponents, and iii) a power function with data-dependent exponents. In the experiments, we observe that CNNs which employ the proposed methods obtain better performance in both generalization performance and robustness for various distortion types for large scale benchmark datasets. For instance, a ResNet-50 model equipped with the proposed method (+HPOW) obtains 6.95%, 5.26% and 5.61% better accuracy on the ILSVRC-12 classification tasks using images distorted with motion blur, salt and pepper and mixed distortions.","中文标题":"特征量化用于防御图像失真","摘要翻译":"在这项工作中，我们解决了提高卷积神经网络（CNNs）对图像失真的鲁棒性问题。我们认为，图像失真会导致特征分布的高阶统计量发生偏移，这种偏移会导致性能下降，并且如我们的实验分析所示，普通的归一化方法无法减少这种偏移。为了减轻这种影响，我们提出了一种基于特征量化的方法。具体来说，我们提出在CNNs中采用三种不同类型的额外非线性：i）具有可扩展分辨率的floor函数，ii）具有可学习指数的幂函数，以及iii）具有数据依赖指数的幂函数。在实验中，我们观察到采用所提出方法的CNNs在大规模基准数据集上的各种失真类型的泛化性能和鲁棒性方面都获得了更好的性能。例如，配备所提出方法（+HPOW）的ResNet-50模型在使用运动模糊、椒盐噪声和混合失真图像进行ILSVRC-12分类任务时，准确率分别提高了6.95%、5.26%和5.61%。","领域":"图像失真防御/卷积神经网络/特征量化","问题":"提高卷积神经网络对图像失真的鲁棒性","动机":"图像失真导致特征分布的高阶统计量偏移，进而导致性能下降，普通归一化方法无法有效减少这种偏移","方法":"提出基于特征量化的方法，在CNNs中采用三种不同类型的额外非线性：具有可扩展分辨率的floor函数、具有可学习指数的幂函数和具有数据依赖指数的幂函数","关键词":["图像失真","卷积神经网络","特征量化","非线性","鲁棒性"],"涉及的技术概念":"卷积神经网络（CNNs）是一种深度学习模型，广泛用于图像识别和分类任务。特征量化是一种减少数据表示精度以降低计算复杂度和存储需求的技术。非线性函数在神经网络中引入非线性特性，使网络能够学习更复杂的模式。图像失真包括运动模糊、椒盐噪声等，这些失真会影响图像的质量和识别性能。"},{"order":823,"title":"Tagging Like Humans: Diverse and Distinct Image Annotation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Tagging_Like_Humans_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Tagging_Like_Humans_CVPR_2018_paper.html","abstract":"In this work we propose a new automatic image annotation model, dubbed diverse and distinct image annotation (D2IA). The generative model D2IA is inspired by the ensemble of human annotations, which create semantically relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and distinct tag subset, in which the tags are relevant to the image contents and semantically distinct to each other, using sequential sampling from a determinantal point process (DPP) model. Multiple such tag subsets that cover diverse semantic aspects or diverse semantic levels of the image contents are generated by randomly perturbing the DPP sampling process. We leverage a generative adversarial network (GAN) model to train D2IA. We perform extensive experiments including quantitative and qualitative comparisons, as well as human subject studies, on two benchmark datasets to demonstrate that the proposed model can produce more diverse and distinct tags than the state-of-the-arts.","中文标题":"像人类一样标注：多样且独特的图像注释","摘要翻译":"在本工作中，我们提出了一种新的自动图像注释模型，称为多样且独特的图像注释（D2IA）。生成模型D2IA受到人类注释集合的启发，这些注释创建了语义相关但又独特且多样的标签。在D2IA中，我们通过从确定性点过程（DPP）模型中进行顺序采样，生成了一个相关且独特的标签子集，其中标签与图像内容相关且在语义上彼此独特。通过随机扰动DPP采样过程，生成了多个这样的标签子集，这些子集涵盖了图像内容的多样语义方面或多样语义层次。我们利用生成对抗网络（GAN）模型来训练D2IA。我们在两个基准数据集上进行了包括定量和定性比较以及人类主题研究在内的广泛实验，以证明所提出的模型能够比现有技术产生更多样且独特的标签。","领域":"图像注释/生成模型/语义分析","问题":"自动生成与图像内容相关且语义上独特且多样的标签","动机":"受到人类注释集合的启发，旨在生成更符合人类标注习惯的多样且独特的图像标签","方法":"使用确定性点过程（DPP）模型进行顺序采样生成相关且独特的标签子集，通过随机扰动DPP采样过程生成多个标签子集，利用生成对抗网络（GAN）模型训练","关键词":["图像注释","生成模型","语义分析","确定性点过程","生成对抗网络"],"涉及的技术概念":{"确定性点过程（DPP）":"一种用于生成多样且独特子集的概率模型，通过顺序采样实现","生成对抗网络（GAN）":"一种由生成器和判别器组成的深度学习模型，用于训练生成模型以产生高质量的输出"}},{"order":824,"title":"Re-Weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Re-Weighted_Adversarial_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Re-Weighted_Adversarial_Adaptation_CVPR_2018_paper.html","abstract":"Unsupervised Domain Adaptation (UDA) aims to transfer domain knowledge from existing well-defined tasks to new ones where labels are unavailable. In the real-world applications, as the domain (task) discrepancies are usually uncontrollable, it is significantly motivated to match the feature distributions even if the domain discrepancies are disparate. Additionally, as no label is available in the target domain, how to successfully adapt the classifier from the source to the target domain still remains an open question. In this paper, we propose the Re-weighted Adversarial Adaptation Network (RAAN) to reduce the feature distribution divergence and adapt the classifier when domain discrepancies are disparate. Specifically, to alleviate the need of common supports in matching the feature distribution, we choose to minimize optimal transport (OT) based Earth-Mover (EM) distance and reformulate it to a minimax objective function. Utilizing this, RAAN can be trained in an end-to-end and adversarial manner. To further adapt the classifier, we propose to match the label distribution and embed it into the adversarial training. Finally, after extensive evaluation of our method using UDA datasets of varying difficulty, RAAN achieved the state-of-the-art results and outperformed other methods by a large margin when the domain shifts are disparate.","中文标题":"重加权对抗适应网络用于无监督领域适应","摘要翻译":"无监督领域适应（UDA）旨在将领域知识从现有的定义明确的任务转移到标签不可用的新任务中。在现实世界的应用中，由于领域（任务）差异通常是不可控的，因此即使领域差异很大，匹配特征分布也是非常有动机的。此外，由于目标域中没有可用的标签，如何成功地将分类器从源域适应到目标域仍然是一个未解决的问题。在本文中，我们提出了重加权对抗适应网络（RAAN），以减少特征分布差异并在领域差异大时适应分类器。具体来说，为了减轻在匹配特征分布时对共同支持的需求，我们选择最小化基于最优传输（OT）的地球移动（EM）距离，并将其重新表述为最小最大目标函数。利用这一点，RAAN可以以端到端和对抗性的方式进行训练。为了进一步适应分类器，我们提出匹配标签分布并将其嵌入到对抗训练中。最后，在使用不同难度的UDA数据集对我们的方法进行广泛评估后，RAAN在领域转移大时取得了最先进的结果，并大幅优于其他方法。","领域":"领域适应/对抗学习/最优传输","问题":"如何在无监督领域适应中减少特征分布差异并适应分类器","动机":"在现实世界的应用中，领域差异通常是不可控的，且目标域中没有可用的标签，这促使研究如何在领域差异大时匹配特征分布并成功适应分类器。","方法":"提出重加权对抗适应网络（RAAN），通过最小化基于最优传输的地球移动距离来减少特征分布差异，并将标签分布匹配嵌入到对抗训练中以适应分类器。","关键词":["无监督领域适应","对抗学习","最优传输","地球移动距离","标签分布匹配"],"涉及的技术概念":"无监督领域适应（UDA）是一种技术，旨在将知识从有标签的源域转移到无标签的目标域。对抗学习是一种训练模型的方法，通过让两个模型相互对抗来提高性能。最优传输（OT）是一种数学框架，用于计算两个分布之间的最小成本转换。地球移动（EM）距离是最优传输中的一种度量，用于衡量两个分布之间的差异。"},{"order":825,"title":"Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_Inferring_Semantic_Layout_CVPR_2018_paper.html","abstract":"We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches.","中文标题":"推断语义布局用于分层文本到图像合成","摘要翻译":"我们提出了一种新颖的分层方法用于文本到图像合成，通过推断语义布局。我们的算法不是学习从文本到图像的直接映射，而是将生成过程分解为多个步骤，其中首先通过布局生成器从文本构建语义布局，然后通过图像生成器将布局转换为图像。提出的布局生成器通过生成对象边界框并以粗到细的方式通过估计框内的对象形状来逐步构建语义布局。图像生成器根据推断的语义布局合成图像，这提供了与文本描述匹配的图像的有用语义结构。我们的模型不仅生成语义上更有意义的图像，还允许通过修改生成的场景布局自动注释生成的图像和用户控制的生成过程。我们在具有挑战性的MS-COCO数据集上展示了所提出模型的能力，并表明该模型可以显著提高图像质量、输出的可解释性和与输入文本的语义对齐，优于现有方法。","领域":"文本到图像合成/语义布局推断/图像生成","问题":"如何从文本描述生成语义上准确且高质量的图像","动机":"提高文本到图像合成的语义准确性和图像质量，同时提供用户控制和自动注释功能","方法":"采用分层方法，首先通过布局生成器从文本推断语义布局，然后通过图像生成器将布局转换为图像。布局生成器以粗到细的方式构建语义布局，图像生成器根据语义布局合成图像。","关键词":["文本到图像合成","语义布局","图像生成","自动注释","用户控制"],"涉及的技术概念":"语义布局推断、分层图像生成、对象边界框生成、对象形状估计、MS-COCO数据集"},{"order":826,"title":"Regularizing RNNs for Caption Generation by Reconstructing the Past With the Present","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Regularizing_RNNs_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Regularizing_RNNs_for_CVPR_2018_paper.html","abstract":"Recently, caption generation with an encoder-decoder framework has been extensively studied and applied in different domains, such as image captioning, code captioning, and so on. In this paper, we propose a novel architecture, namely Auto-Reconstructor Network (ARNet), which, coupling with the conventional encoder-decoder framework, works in an end-to-end fashion to generate captions. ARNet aims at reconstructing the previous hidden state with the present one, besides behaving as the input-dependent transition operator. Therefore, ARNet encourages the current hidden state to embed more information from the previous one, which can help regularize the transition dynamics of recurrent neural networks (RNNs). Extensive experimental results show that our proposed ARNet boosts the performance over the existing encoder-decoder models on both image captioning and source code captioning tasks. Additionally, ARNet remarkably reduces the discrepancy between training and inference processes for caption generation. Furthermore, the performance on permuted sequential MNIST demonstrates that ARNet can effectively regularize RNN, especially on modeling long-term dependencies. Our code is available at: https://github.com/chenxinpeng/ARNet.","中文标题":"通过用现在重建过去来正则化RNN以生成字幕","摘要翻译":"最近，使用编码器-解码器框架的字幕生成已在不同领域得到广泛研究和应用，如图像字幕、代码字幕等。在本文中，我们提出了一种新颖的架构，即自动重建网络（ARNet），它与传统的编码器-解码器框架相结合，以端到端的方式工作以生成字幕。ARNet旨在用当前隐藏状态重建前一个隐藏状态，除了作为输入依赖的转换操作符。因此，ARNet鼓励当前隐藏状态嵌入更多来自前一个隐藏状态的信息，这可以帮助正则化循环神经网络（RNNs）的转换动态。大量实验结果表明，我们提出的ARNet在图像字幕和源代码字幕任务上提高了现有编码器-解码器模型的性能。此外，ARNet显著减少了字幕生成的训练和推理过程之间的差异。此外，在排列顺序MNIST上的表现表明，ARNet可以有效地正则化RNN，特别是在建模长期依赖方面。我们的代码可在https://github.com/chenxinpeng/ARNet获取。","领域":"图像字幕/代码字幕/循环神经网络","问题":"如何提高编码器-解码器框架在字幕生成任务中的性能","动机":"现有的编码器-解码器框架在字幕生成任务中可能无法充分利用历史信息，导致性能受限","方法":"提出自动重建网络（ARNet），通过用当前隐藏状态重建前一个隐藏状态来正则化RNN，以嵌入更多历史信息","关键词":["自动重建网络","编码器-解码器框架","循环神经网络","正则化","长期依赖"],"涉及的技术概念":"ARNet是一种新颖的架构，旨在通过重建隐藏状态来正则化RNN，以提高字幕生成的性能。它通过鼓励当前隐藏状态嵌入更多来自前一个隐藏状态的信息，帮助正则化RNN的转换动态。此外，ARNet还减少了训练和推理过程之间的差异，并在建模长期依赖方面表现出色。"},{"order":827,"title":"Unsupervised Domain Adaptation With Similarity Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pinheiro_Unsupervised_Domain_Adaptation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pinheiro_Unsupervised_Domain_Adaptation_CVPR_2018_paper.html","abstract":"The objective of unsupervised domain adaptation is to leverage features from a labeled source domain and learn a classifier for an unlabeled target domain, with a similar but different data distribution. Most deep learning approaches consist of two steps: (i) learn features that preserve a low risk on labeled samples (source domain) and (ii) make the features from both domains to be as indistinguishable as possible, so that a classifier trained on the source can also be applied on the target domain. In general, the classifiers in step (i) consist of fully-connected layers applied directly on the indistinguishable features learned in (ii). In this paper, we propose a different way to do the classification, using similarity learning. The proposed method learns a pairwise similarity function in which classification can be performed by computing distances between prototype representations of each category. The domain-invariant features and the categorical prototype representations are learned jointly and in an end-to-end fashion. At inference time, images from the target domain are compared to the prototypes and the label associated with the one that best matches the image is outputed. The approach is simple, scalable and effective. We show that our model achieves state-of-the-art performance in different large-scale unsupervised domain adaptation scenarios.","中文标题":"无监督领域适应与相似性学习","摘要翻译":"无监督领域适应的目标是从一个标记的源域中利用特征，并学习一个分类器用于未标记的目标域，这两个域的数据分布相似但不同。大多数深度学习方法包括两个步骤：(i) 学习能够保持标记样本（源域）低风险的特性，以及(ii) 使来自两个域的特性尽可能不可区分，以便在源域上训练的分类器也可以应用于目标域。通常，步骤(i)中的分类器由直接应用于在(ii)中学习的不可区分特性的全连接层组成。在本文中，我们提出了一种不同的分类方法，使用相似性学习。所提出的方法学习一个成对相似性函数，其中分类可以通过计算每个类别的原型表示之间的距离来执行。领域不变特性和类别原型表示是联合并以端到端的方式学习的。在推理时，目标域的图像与原型进行比较，并输出与图像最匹配的原型相关的标签。该方法简单、可扩展且有效。我们展示了我们的模型在不同的无监督领域适应场景中实现了最先进的性能。","领域":"领域适应/相似性学习/分类器学习","问题":"如何在无监督的情况下，利用源域的有标签数据学习一个适用于目标域的分类器，尽管两者的数据分布相似但不同。","动机":"解决无监督领域适应中的分类问题，通过相似性学习提高分类器在目标域上的适用性和性能。","方法":"提出了一种使用相似性学习的分类方法，通过联合学习领域不变特性和类别原型表示，以端到端的方式进行。在推理时，通过比较目标域图像与原型来输出最匹配的标签。","关键词":["无监督领域适应","相似性学习","分类器学习","领域不变特性","类别原型表示"],"涉及的技术概念":"无监督领域适应是指在无标签的目标域上利用有标签的源域数据来学习分类器。相似性学习是一种通过计算样本之间的距离来进行分类的方法。领域不变特性是指在不同领域间保持一致的特性，有助于分类器在不同领域间的迁移。类别原型表示是指每个类别的代表性样本，用于在相似性学习中进行比较和分类。"},{"order":828,"title":"Learning Deep Sketch Abstraction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Muhammad_Learning_Deep_Sketch_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Muhammad_Learning_Deep_Sketch_CVPR_2018_paper.html","abstract":"Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.","中文标题":"学习深度草图抽象","摘要翻译":"人类手绘草图已在多种背景下被研究，包括草图识别、合成和基于草图的细粒度图像检索（FG-SBIR）。草图分析的一个基本挑战是处理极其不同的人类绘画风格，特别是在抽象层次方面。在这项工作中，我们提出了第一个基于笔画级别的草图抽象模型，该模型基于草图抽象作为草图可识别性与用于绘制它的笔画数量之间权衡过程的洞察。具体来说，我们通过强化学习训练一个笔画移除策略模型来生成抽象草图，该策略学习预测哪些笔画可以在不影响可识别性的情况下安全移除。我们展示了我们的抽象模型可以用于各种草图分析任务，包括：（1）建模笔画显著性并理解草图识别模型的决策，（2）为给定类别或照片中的参考对象实例合成可变抽象的草图，以及（3）仅使用照片训练FG-SBIR模型，绕过昂贵的照片-草图对收集步骤。","领域":"草图识别/草图合成/细粒度图像检索","问题":"处理不同人类绘画风格的草图，特别是在抽象层次上的差异","动机":"解决草图分析中处理不同人类绘画风格的基本挑战，特别是在抽象层次上的差异","方法":"提出基于笔画级别的草图抽象模型，通过强化学习训练笔画移除策略模型来生成抽象草图","关键词":["草图抽象","笔画移除策略","强化学习","细粒度图像检索"],"涉及的技术概念":"草图抽象是指通过减少草图中的笔画数量而不影响其可识别性的过程。强化学习是一种机器学习方法，其中模型通过与环境交互来学习策略，以最大化某种累积奖励。细粒度图像检索（FG-SBIR）是一种图像检索任务，旨在从大量图像中找到与查询图像在细节上非常相似的图像。"},{"order":829,"title":"Matching Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.html","abstract":"Generative Adversarial Nets (GANs) and Conditonal GANs (CGANs) show that using a trained network as loss function (discriminator) enables to synthesize highly structured outputs (e.g. natural images). However, applying a discriminator network as a universal loss function for common supervised tasks (e.g. semantic segmentation, line detection, depth estimation) is considerably less successful. We argue that the main difficulty of applying CGANs to supervised tasks is that the generator training consists of optimizing a loss function that does not depend directly on the ground truth labels.   To overcome this, we propose to replace the discriminator with a matching network taking into account both the ground truth outputs as well as the generated examples. As a consequence, the generator loss function also depends on the targets of the training examples, thus facilitating learning. We demonstrate on three computer vision tasks that this approach can significantly outperform CGANs achieving comparable or superior results to task-specific solutions and results in stable training.  Importantly, this is a general approach that does not require the use of task-specific loss functions.","中文标题":"匹配对抗网络","摘要翻译":"生成对抗网络（GANs）和条件生成对抗网络（CGANs）表明，使用训练好的网络作为损失函数（判别器）能够合成高度结构化的输出（例如自然图像）。然而，将判别器网络作为通用损失函数应用于常见的监督任务（例如语义分割、线条检测、深度估计）则效果显著不佳。我们认为，将CGANs应用于监督任务的主要困难在于生成器训练包括优化一个不直接依赖于真实标签的损失函数。为了克服这一点，我们提出用匹配网络替换判别器，该网络同时考虑真实输出和生成的示例。因此，生成器的损失函数也依赖于训练示例的目标，从而促进学习。我们在三个计算机视觉任务上证明，这种方法可以显著优于CGANs，达到与任务特定解决方案相当或更优的结果，并实现稳定的训练。重要的是，这是一种通用方法，不需要使用任务特定的损失函数。","领域":"生成对抗网络/监督学习/计算机视觉","问题":"如何提高生成对抗网络在监督任务中的性能","动机":"现有的生成对抗网络在监督任务中效果不佳，主要因为生成器训练不直接依赖于真实标签","方法":"提出用匹配网络替换判别器，使生成器的损失函数依赖于训练示例的目标","关键词":["生成对抗网络","监督学习","匹配网络"],"涉及的技术概念":"生成对抗网络（GANs）是一种通过对抗过程估计生成模型的框架，其中生成器网络尝试生成数据，而判别器网络尝试区分真实数据和生成数据。条件生成对抗网络（CGANs）是GANs的扩展，允许模型在生成数据时考虑额外的信息。匹配网络是一种新型的网络结构，旨在通过考虑真实输出和生成的示例来改进生成器的训练过程。"},{"order":830,"title":"SoS-RSC: A Sum-of-Squares Polynomial Approach to Robustifying Subspace Clustering Algorithms","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sznaier_SoS-RSC_A_Sum-of-Squares_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sznaier_SoS-RSC_A_Sum-of-Squares_CVPR_2018_paper.html","abstract":"This paper addresses the problem of subspace clustering in the presence of outliers. Typically, this scenario is handled through a regularized optimization, whose computational complexity scales polynomially with the size of the data. Further, the regularization terms need to be manually tuned to achieve optimal performance. To circumvent these difficulties, in this paper we propose an outlier removal algorithm based on evaluating a suitable sum-ofsquares polynomial, computed directly from the data. This algorithm only requires performing two singular value decompositions of fixed size, and provides certificates on the probability of misclassifying outliers as inliers.","中文标题":"SoS-RSC：一种基于平方和多项式的鲁棒子空间聚类算法","摘要翻译":"本文解决了存在异常值情况下的子空间聚类问题。通常，这种情况通过正则化优化来处理，其计算复杂度随数据大小呈多项式增长。此外，正则化项需要手动调整以达到最佳性能。为了规避这些困难，本文提出了一种基于评估合适的平方和多项式的异常值去除算法，该多项式直接从数据中计算得出。该算法仅需执行两次固定大小的奇异值分解，并提供了将异常值误分类为内点的概率的证明。","领域":"子空间聚类/异常检测/优化算法","问题":"在存在异常值的情况下进行子空间聚类","动机":"解决传统正则化优化方法在处理子空间聚类问题时计算复杂度高和需要手动调整正则化项的问题","方法":"提出了一种基于平方和多项式的异常值去除算法，该算法通过直接计算数据中的平方和多项式来评估异常值，仅需执行两次固定大小的奇异值分解","关键词":["子空间聚类","异常检测","优化算法"],"涉及的技术概念":"平方和多项式是一种数学工具，用于构建非负多项式，这里用于评估异常值。奇异值分解（SVD）是一种线性代数技术，用于分解矩阵，这里用于简化计算过程。"},{"order":831,"title":"Resource Aware Person Re-Identification Across Multiple Resolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Resource_Aware_Person_CVPR_2018_paper.html","abstract":"Not all people are equally easy to identify: color statistics might be enough for some cases while others might require careful reasoning about high- and low-level details. However, prevailing person re-identification(re-ID) methods use one-size-fits-all high-level embeddings from deep convolutional networks for all cases. This might limit their accuracy on difficult examples or makes them needlessly expensive for the easy ones. To remedy this, we present a new person re-ID model that combines effective embeddings built on multiple convolutional network layers, trained with deep-supervision. On traditional re-ID benchmarks, our method improves substantially over the previous state-of-the-art results on all five datasets that we evaluate on. We then propose two new formulations of the person re-ID problem under resource-constraints, and show how our model can be used to effectively trade off accuracy and computation in the presence of resource constraints.","中文标题":"资源感知的多分辨率行人重识别","摘要翻译":"并非所有人都同样容易被识别：在某些情况下，颜色统计可能就足够了，而在其他情况下，则可能需要仔细推理高低层次的细节。然而，现有的行人重识别(re-ID)方法对所有情况都使用一刀切的高层次嵌入，这些嵌入来自深度卷积网络。这可能会限制它们在困难样本上的准确性，或者使它们在简单样本上不必要地昂贵。为了解决这个问题，我们提出了一种新的行人重识别模型，该模型结合了基于多个卷积网络层的有效嵌入，并通过深度监督进行训练。在传统的重识别基准测试中，我们的方法在我们评估的所有五个数据集上显著优于之前的最先进结果。然后，我们提出了两种新的资源约束下的行人重识别问题表述，并展示了我们的模型如何在资源约束的情况下有效地权衡准确性和计算。","领域":"行人重识别/卷积神经网络/资源优化","问题":"解决现有行人重识别方法在处理不同难度样本时准确性和计算资源消耗不平衡的问题","动机":"提高行人重识别在不同难度样本上的准确性，同时优化计算资源的使用","方法":"提出了一种新的行人重识别模型，该模型结合了基于多个卷积网络层的有效嵌入，并通过深度监督进行训练，以在资源约束的情况下有效地权衡准确性和计算","关键词":["行人重识别","卷积神经网络","资源优化","深度监督","多分辨率"],"涉及的技术概念":"深度卷积网络、高层次嵌入、深度监督、资源约束下的优化"},{"order":832,"title":"Learning and Using the Arrow of Time","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wei_Learning_and_Using_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wei_Learning_and_Using_CVPR_2018_paper.html","abstract":"We seek to understand the arrow of time in videos -- what makes videos look like they are playing forwards or backwards? Can we visualize the cues? Can the arrow of time be a supervisory signal useful for activity analysis? To this end, we build three large-scale video datasets and apply a learning-based approach to these tasks.   To learn the arrow of time efficiently and reliably, we design a ConvNet suitable for extended temporal footprints and for class activation visualization, and study the effect of artificial cues, such as cinematographic conventions, on learning. Our trained model achieves state-of-the-art performance on large-scale real-world video datasets.  Through cluster analysis and localization of important regions for the prediction, we examine learned visual cues that are consistent among many samples and show when and where they occur. Lastly, we use the trained ConvNet for two applications: self-supervision for action recognition, and video forensics -- determining whether Hollywood film clips have been deliberately reversed in time, often used as special effects.","中文标题":"学习和使用时间箭头","摘要翻译":"我们试图理解视频中的时间箭头——是什么让视频看起来是向前播放还是向后播放？我们能否可视化这些线索？时间箭头能否成为活动分析的有用监督信号？为此，我们构建了三个大规模视频数据集，并对这些任务应用了基于学习的方法。为了高效可靠地学习时间箭头，我们设计了一个适合扩展时间足迹和类激活可视化的卷积网络（ConvNet），并研究了诸如电影摄影惯例等人工线索对学习的影响。我们训练的模型在大规模真实世界视频数据集上实现了最先进的性能。通过聚类分析和预测重要区域的定位，我们检查了在许多样本中一致的学习视觉线索，并展示了它们何时何地出现。最后，我们使用训练好的卷积网络进行两个应用：动作识别的自我监督，以及视频取证——确定好莱坞电影片段是否被故意倒放，这通常用作特效。","领域":"视频分析/动作识别/视频取证","问题":"理解视频中的时间箭头，以及时间箭头是否可以作为活动分析的监督信号","动机":"探索视频中时间箭头的视觉线索，以及这些线索如何用于视频分析和取证","方法":"构建大规模视频数据集，设计适合扩展时间足迹和类激活可视化的卷积网络，研究人工线索对学习的影响，通过聚类分析和重要区域定位检查学习到的视觉线索","关键词":["时间箭头","视频分析","动作识别","视频取证","卷积网络","类激活可视化"],"涉及的技术概念":{"时间箭头":"视频中时间流动的方向，即视频是向前播放还是向后播放","卷积网络（ConvNet）":"一种深度学习模型，特别适合处理图像和视频数据","类激活可视化":"一种技术，用于可视化卷积网络中哪些区域对特定类别的预测贡献最大","聚类分析":"一种数据分析方法，用于将数据集中的对象分组，使得同一组内的对象相似度较高，而不同组之间的对象相似度较低","视频取证":"使用技术手段分析视频内容，以确定视频是否被篡改或编辑"}},{"order":833,"title":"Neural Style Transfer via Meta Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Neural_Style_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Neural_Style_Transfer_CVPR_2018_paper.html","abstract":"In this paper we propose a noval method to generate the specified network parameters through one feed-forward propagation in the meta networks for neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent, which lacks the generalization ability to new style in the inference stage. To tackle these issues, we build a meta network which takes in the style image and generates a corresponding image transformation network directly. Compared with optimization-based methods for every style, our meta networks can handle an arbitrary new style within 19 milliseconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449 KB, which is capable of real-time running on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models will be released.","中文标题":"通过元网络进行神经风格迁移","摘要翻译":"在本文中，我们提出了一种新颖的方法，通过在元网络中进行一次前向传播来生成指定的网络参数，用于神经风格迁移。最近的风格迁移工作通常需要为每种新风格训练图像变换网络，并且风格通过大量的随机梯度下降迭代编码在网络参数中，这在推理阶段缺乏对新风格的泛化能力。为了解决这些问题，我们构建了一个元网络，该网络接收风格图像并直接生成相应的图像变换网络。与每种风格基于优化的方法相比，我们的元网络可以在现代GPU卡上在19毫秒内处理任意新风格。由我们的元网络生成的快速图像变换网络仅449 KB，能够在移动设备上实时运行。我们还通过操作元网络中的隐藏特征来研究风格迁移网络的流形。实验很好地验证了我们方法的有效性。代码和训练模型将被发布。","领域":"风格迁移/图像生成/元学习","问题":"解决神经风格迁移中需要为每种新风格训练图像变换网络的问题，以及缺乏对新风格泛化能力的问题","动机":"提高神经风格迁移的效率和泛化能力，使其能够快速适应新风格并在移动设备上实时运行","方法":"构建一个元网络，该网络接收风格图像并直接生成相应的图像变换网络，通过一次前向传播生成网络参数，从而快速适应新风格","关键词":["风格迁移","元网络","图像变换网络","实时运行","移动设备"],"涉及的技术概念":{"元网络":"一种能够生成其他网络参数的网络，用于快速适应新任务或新风格","图像变换网络":"用于将输入图像转换为具有特定风格输出的网络","随机梯度下降":"一种优化算法，用于通过迭代最小化损失函数来训练网络参数","泛化能力":"模型在未见过的数据上表现良好的能力","前向传播":"神经网络中从输入到输出的计算过程，用于生成预测或参数"}},{"order":834,"title":"People, Penguins and Petri Dishes: Adapting Object Counting Models to New Visual Domains and Object Types Without Forgetting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Marsden_People_Penguins_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Marsden_People_Penguins_and_CVPR_2018_paper.html","abstract":"In this paper we propose a technique to adapt a convolutional neural network (CNN) based object counter  to additional visual domains and object types while still preserving the original counting function. Domain-specific normalisation and scaling operators are trained to allow the model to adjust to the statistical distributions of the various visual domains.  The developed adaptation technique is used to produce a singular patch-based counting regressor capable of counting various object types including people, vehicles, cell nuclei and wildlife.  As part of this study a challenging new cell counting dataset in the context of tissue culture and patient diagnosis is constructed. This new collection, referred to as the Dublin Cell Counting (DCC) dataset, is the first of its kind to be made available to the wider computer vision community. State-of-the-art object counting performance is achieved in both the Shanghaitech (parts A and B) and Penguins datasets while competitive performance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets, all using a shared counting model.","中文标题":"人、企鹅和培养皿：使物体计数模型适应新的视觉领域和物体类型而不遗忘","摘要翻译":"在本文中，我们提出了一种技术，用于使基于卷积神经网络（CNN）的物体计数器适应额外的视觉领域和物体类型，同时仍保留原始计数功能。训练领域特定的归一化和缩放操作符，使模型能够调整以适应各种视觉领域的统计分布。开发的适应技术用于产生一个基于补丁的单一计数回归器，能够计数包括人、车辆、细胞核和野生动物在内的各种物体类型。作为本研究的一部分，构建了一个在组织培养和患者诊断背景下的具有挑战性的新细胞计数数据集。这个新集合，称为都柏林细胞计数（DCC）数据集，是首次向更广泛的计算机视觉社区提供的此类数据集。在Shanghaitech（A和B部分）和企鹅数据集中实现了最先进的物体计数性能，同时在TRANCOS和改良骨髓（MBM）数据集上观察到了竞争性能，所有这些都使用了一个共享的计数模型。","领域":"物体计数/视觉适应/生物医学图像分析","问题":"如何使物体计数模型适应新的视觉领域和物体类型而不遗忘原有功能","动机":"为了扩展物体计数模型的应用范围，使其能够适应不同的视觉领域和物体类型，同时保持对原有物体的计数能力。","方法":"训练领域特定的归一化和缩放操作符，使模型能够调整以适应各种视觉领域的统计分布，并开发一个基于补丁的单一计数回归器。","关键词":["物体计数","视觉适应","生物医学图像分析","卷积神经网络","归一化","缩放操作符"],"涉及的技术概念":"卷积神经网络（CNN）用于物体计数，领域特定的归一化和缩放操作符用于模型适应不同视觉领域的统计分布，基于补丁的计数回归器用于计数各种物体类型。"},{"order":835,"title":"HydraNets: Specialized Dynamic Architectures for Efficient Inference","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mullapudi_HydraNets_Specialized_Dynamic_CVPR_2018_paper.html","abstract":"There is growing interest in improving the design of deep network architectures to be both accurate and low cost. This paper explores semantic specialization as a mechanism for improving the computational efficiency (accuracy-per-unit-cost) of inference in the context of image classification. Specifically, we propose a network architecture template called HydraNet, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image.  This design is made possible by a soft gating mechanism that encourages component specialization during training and accurately performs component selection during inference. We evaluate the HydraNet approach on both the CIFAR-100 and ImageNet classification tasks. On CIFAR, applying the HydraNet template to the ResNet and DenseNet family of models reduces inference cost by 2-4x while retaining the accuracy of the baseline architectures. On ImageNet, applying the HydraNet template improves accuracy up to 2.5% when compared to an efficient baseline architecture with similar inference cost.","中文标题":"HydraNets：用于高效推理的专用动态架构","摘要翻译":"随着对提高深度网络架构设计以同时实现高准确性和低成本的兴趣日益增长，本文探讨了语义专门化作为提高图像分类推理计算效率（单位成本下的准确性）的机制。具体来说，我们提出了一种名为HydraNet的网络架构模板，它能够将图像分类的最新架构转变为利用条件执行进行高效推理的动态架构。HydraNets是包含专门为视觉相似类别计算特征的独特组件的宽网络，但它们通过动态选择仅对任何一张输入图像评估少量组件来保持效率。这一设计得益于一种软门控机制，该机制在训练期间鼓励组件专门化，并在推理期间准确执行组件选择。我们在CIFAR-100和ImageNet分类任务上评估了HydraNet方法。在CIFAR上，将HydraNet模板应用于ResNet和DenseNet系列模型，推理成本减少了2-4倍，同时保持了基线架构的准确性。在ImageNet上，与具有相似推理成本的高效基线架构相比，应用HydraNet模板将准确性提高了高达2.5%。","领域":"图像分类/网络架构优化/计算效率","问题":"提高图像分类任务中深度网络架构的计算效率","动机":"探索语义专门化作为提高图像分类推理计算效率的机制","方法":"提出HydraNet网络架构模板，通过动态选择组件和软门控机制实现高效推理","关键词":["图像分类","网络架构","计算效率","动态架构","条件执行"],"涉及的技术概念":"HydraNet是一种网络架构模板，旨在通过动态选择网络中的组件来提高图像分类任务的推理效率。它利用软门控机制在训练期间鼓励组件专门化，并在推理期间准确执行组件选择，从而实现高效推理。"},{"order":836,"title":"SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.html","abstract":"We propose a deep hashing framework for sketch retrieval that, for the first time, works on a multi-million scale human sketch dataset.Leveraging on this large dataset, we explore a few sketch-specific traits that were otherwise under-studied in prior literature. Instead of following the conventional sketch recognition task, we introduce the novel problem of sketch hashing retrieval which is not only more challenging, but also offers a better testbed for large-scale sketch analysis, since: (i) more fine-grained sketch feature learning is required to accommodate the large variations in style and abstraction, and (ii) a compact binary code needs to be learned at the same time to enable efficient retrieval.Key to our network design is the embedding of unique characteristics of human sketch, where (i) a two-branch CNN-RNN architecture is adapted to explore the temporal ordering of strokes, and (ii) a novel hashing loss is specifically designed to accommodate both the temporal and abstract traits of sketches. By working with a 3.8M sketch dataset,we show that state-of-the-art hashing models specifically engineered for static images fail to perform well on temporal sketch data. Our network on the other hand not only offers the best retrieval performance on various code sizes, but also yields the best generalization performance under a zero-shot setting and when re-purposed for sketch recognition.Such superior performances effectively demonstrate the benefit of our sketch-specific design.","中文标题":"SketchMate: 用于百万级人类素描检索的深度哈希","摘要翻译":"我们提出了一个用于素描检索的深度哈希框架，这是首次在数百万规模的人类素描数据集上工作。利用这个大数据集，我们探索了一些在先前文献中未被充分研究的素描特定特性。与传统的素描识别任务不同，我们引入了素描哈希检索这一新问题，这不仅更具挑战性，而且为大规模素描分析提供了更好的测试平台，因为：(i) 需要更细粒度的素描特征学习以适应风格和抽象的巨大变化，(ii) 同时需要学习紧凑的二进制代码以实现高效检索。我们网络设计的关键在于嵌入人类素描的独特特性，其中(i) 采用了两分支CNN-RNN架构来探索笔划的时间顺序，(ii) 专门设计了一种新的哈希损失以适应素描的时间和抽象特性。通过在380万素描数据集上的工作，我们展示了专门为静态图像设计的先进哈希模型在时间素描数据上表现不佳。而我们的网络不仅在各种代码大小上提供了最佳的检索性能，而且在零样本设置下和重新用于素描识别时也表现出了最佳的泛化性能。这些卓越的性能有效地证明了我们素描特定设计的优势。","领域":"素描检索/深度学习/哈希学习","问题":"解决大规模人类素描数据集上的素描检索问题","动机":"探索素描特定特性，提高大规模素描分析的效率和准确性","方法":"采用两分支CNN-RNN架构探索笔划的时间顺序，并设计新的哈希损失以适应素描的时间和抽象特性","关键词":["素描检索","深度哈希","CNN-RNN架构","哈希损失","大规模数据集"],"涉及的技术概念":{"深度哈希框架":"一种用于高效检索的深度学习框架，通过学习紧凑的二进制代码来实现。","CNN-RNN架构":"结合卷积神经网络(CNN)和循环神经网络(RNN)的架构，用于处理具有时间序列特性的数据。","哈希损失":"一种专门设计的损失函数，用于优化哈希学习过程，以适应特定数据特性。","零样本设置":"一种评估模型泛化能力的方法，测试模型在未见过的类别上的表现。"}},{"order":837,"title":"From Source to Target and Back: Symmetric Bi-Directional Adaptive GAN","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Russo_From_Source_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Russo_From_Source_to_CVPR_2018_paper.html","abstract":"The effectiveness of GANs in producing images according to  a specific visual domain has shown potential in unsupervised domain adaptation. Source labeled images  have been modified to mimic target samples for training classifiers in the target domain, and inverse  mappings from the target to the source domain have also been evaluated, without new image generation. In this paper we aim at getting the best of both worlds by introducing a symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. We define a new class consistency loss that aligns the generators in the two directions, imposing to preserve the class identity of an image passing through both domain mappings. A detailed analysis of the reconstructed images, a thorough ablation study and extensive experiments on six different settings confirm the power of our approach.","中文标题":"从源到目标再返回：对称双向自适应GAN","摘要翻译":"GAN在根据特定视觉领域生成图像方面的有效性显示了在无监督领域适应中的潜力。源标记图像已被修改以模仿目标样本，用于在目标领域训练分类器，并且从目标到源领域的反向映射也已被评估，无需生成新图像。在本文中，我们旨在通过引入领域间的对称映射来获得两全其美的效果。我们联合优化双向图像变换，并将它们与目标自标记结合起来。我们定义了一个新的类别一致性损失，该损失在两个方向上对齐生成器，强制保留通过两个领域映射的图像的类别身份。对重建图像的详细分析、彻底的消融研究以及在六种不同设置上的广泛实验证实了我们方法的强大。","领域":"图像生成/领域适应/自监督学习","问题":"如何在无监督领域适应中有效地生成和转换图像，以保持图像的类别身份","动机":"探索在无监督领域适应中，通过对称双向映射和自标记技术，提高图像生成和转换的效果，同时保持图像的类别身份","方法":"引入对称双向自适应GAN，联合优化双向图像变换与目标自标记，定义新的类别一致性损失以对齐生成器","关键词":["图像生成","领域适应","自监督学习","对称映射","类别一致性损失"],"涉及的技术概念":{"GAN":"生成对抗网络，一种通过对抗过程估计生成模型的框架","无监督领域适应":"在没有目标领域标签的情况下，将模型从源领域适应到目标领域","对称映射":"在两个领域之间建立双向的、对称的映射关系","自标记":"利用模型自身生成的标签进行训练","类别一致性损失":"一种损失函数，用于确保图像在通过领域映射后保持其类别身份"}},{"order":838,"title":"OLÉ: Orthogonal Low-Rank Embedding - A Plug and Play Geometric Loss for Deep Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lezama_OLE_Orthogonal_Low-Rank_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lezama_OLE_Orthogonal_Low-Rank_CVPR_2018_paper.html","abstract":"Deep neural networks trained using a softmax layer at the top and the cross-entropy loss are ubiquitous tools for image classification. Yet, this does not naturally enforce intra-class similarity nor inter-class margin of the learned deep representations. To simultaneously achieve these two goals, different solutions have been proposed in the literature, such as the pairwise or triplet losses. However, these carry the extra task of selecting pairs or triplets, and the extra computational burden of computing and learning for many combinations of them. In this paper, we propose a plug-and-play loss term for deep networks that explicitly reduces intra-class variance and enforces inter-class margin simultaneously, in a simple and elegant geometric manner. For each class, the deep features are collapsed into a learned linear subspace, or union of them, and inter-class subspaces are pushed to be as orthogonal as possible. Our proposed Orthogonal Low-rank Embedding (OLE) does not require carefully crafting pairs or triplets of samples for training, and works standalone as a classification loss, being the first reported deep metric learning framework of its kind.  Because of the improved margin between features of different classes, the resulting deep networks generalize better, are more discriminative, and more robust. We demonstrate improved classification performance in general object recognition, plugging the proposed loss term into existing off-the-shelf architectures. In particular, we show the advantage of the proposed loss in the small data/model scenario, and we significantly advance the state-of-the-art on the Stanford STL-10 benchmark.","中文标题":"OLÉ: 正交低秩嵌入 - 一种即插即用的深度学习几何损失","摘要翻译":"使用顶部的softmax层和交叉熵损失训练的深度神经网络是图像分类的普遍工具。然而，这并不自然地强制学习到的深度表示的类内相似性或类间间隔。为了同时实现这两个目标，文献中提出了不同的解决方案，例如成对或三元组损失。然而，这些方法带来了选择对或三元组的额外任务，以及计算和学习许多组合的额外计算负担。在本文中，我们提出了一种即插即用的深度网络损失项，它以简单而优雅的几何方式显式地减少类内方差并同时强制类间间隔。对于每个类，深度特征被折叠成学习的线性子空间，或它们的联合，并且类间子空间被推得尽可能正交。我们提出的正交低秩嵌入（OLE）不需要精心制作训练样本的对或三元组，并且作为分类损失独立工作，是首次报道的此类深度度量学习框架。由于不同类特征之间的间隔改进，生成的深度网络具有更好的泛化能力，更具区分性，更稳健。我们展示了在一般对象识别中改进的分类性能，将提出的损失项插入现有的现成架构中。特别是，我们展示了在小数据/模型场景中提出的损失的优势，并且我们在斯坦福STL-10基准上显著推进了最先进的技术。","领域":"图像分类/深度度量学习/对象识别","问题":"如何同时减少类内方差并强制类间间隔","动机":"提高深度神经网络在图像分类任务中的泛化能力、区分性和稳健性","方法":"提出一种即插即用的损失项，通过将每个类的深度特征折叠成学习的线性子空间，并尽可能正交地推动类间子空间，来显式地减少类内方差并强制类间间隔","关键词":["图像分类","深度度量学习","对象识别"],"涉及的技术概念":"softmax层、交叉熵损失、类内相似性、类间间隔、成对损失、三元组损失、线性子空间、正交低秩嵌入（OLE）、深度度量学习框架、斯坦福STL-10基准"},{"order":839,"title":"Efficient Parametrization of Multi-Domain Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rebuffi_Efficient_Parametrization_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rebuffi_Efficient_Parametrization_of_CVPR_2018_paper.html","abstract":"A practical limitation of deep neural networks is their high degree of specialization to a single task and visual domain. In complex applications such as mobile platforms, this requires juggling several large models with detrimental effect on speed and battery life. Recently, inspired by the successes of transfer learning, several authors have proposed to learn instead universal, fixed feature extractors that, used as the first stage of any deep network, work well for all tasks and domains simultaneously. Nevertheless, such universal features are still somewhat inferior to specialized networks.  To overcome this limitation, in this paper we propose to consider instead universal parametric families of neural networks, which still contain specialized problem-specific models, but that differ only by a small number of parameters. We study different designs for such parametrizations, including series and parallel residual adapters, regularization strategies, and parameter allocations, and empirically identify the ones that yield the highest compression. We show that, in order to maximize performance, it is necessary to adapt both shallow and deep layers of a deep network, but the required changes are very small. We also show that these universal parametrization are very effective for transfer learning, where they outperform traditional fine-tuning techniques.","中文标题":"多领域深度神经网络的高效参数化","摘要翻译":"深度神经网络的一个实际限制是它们对单一任务和视觉领域的高度专业化。在诸如移动平台这样的复杂应用中，这需要同时处理几个大型模型，这对速度和电池寿命有不利影响。最近，受到迁移学习成功的启发，几位作者提出学习通用的、固定的特征提取器，作为任何深度网络的第一阶段，同时适用于所有任务和领域。然而，这样的通用特征仍然在某种程度上不如专业化的网络。为了克服这一限制，本文提出考虑通用的参数化神经网络家族，这些家族仍然包含特定问题的专业模型，但仅通过少量参数进行区分。我们研究了这种参数化的不同设计，包括串联和并联的残差适配器、正则化策略和参数分配，并通过实验确定了产生最高压缩率的设计。我们表明，为了最大化性能，有必要同时适应深度网络的浅层和深层，但所需的变化非常小。我们还表明，这些通用参数化在迁移学习中非常有效，它们优于传统的微调技术。","领域":"迁移学习/神经网络优化/模型压缩","问题":"深度神经网络对单一任务和视觉领域的高度专业化导致在复杂应用中需要同时处理多个大型模型，影响速度和电池寿命。","动机":"受到迁移学习成功的启发，旨在开发一种通用的参数化神经网络家族，以克服现有通用特征提取器在性能上仍不如专业化网络的问题。","方法":"提出通用的参数化神经网络家族，研究包括串联和并联的残差适配器、正则化策略和参数分配在内的不同设计，并通过实验确定产生最高压缩率的设计。","关键词":["迁移学习","神经网络优化","模型压缩"],"涉及的技术概念":"深度神经网络、迁移学习、特征提取器、参数化、残差适配器、正则化策略、参数分配、模型压缩"},{"order":840,"title":"Deep Density Clustering of Unconstrained Faces","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_Deep_Density_Clustering_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_Deep_Density_Clustering_CVPR_2018_paper.html","abstract":"In this paper, we consider the problem of grouping a collection of unconstrained face images in which the number of subjects is not known. We propose an unsupervised clustering algorithm called Deep Density Clustering (DDC) which is based on measuring density affinities between local neighborhoods in the feature space. By learning the minimal covering sphere for each neighborhood, information about the underlying structure is encapsulated. The encapsulation is also capable of locating high-density region of the neighborhood, which aids in measuring the neighborhood similarity. We theoretically show that the encapsulation asymptotically converges to a Parzen window density estimator. Our experiments show that DDC is a superior candidate for clustering unconstrained faces when the number of subjects is unknown. Unlike conventional linkage and density-based methods that are sensitive to the selection operating points, DDC attains more consistent and improved performance. Furthermore, the density-aware property reduces the difficulty in finding appropriate operating points.","中文标题":"无约束人脸深度密度聚类","摘要翻译":"在本文中，我们考虑了在不知道主体数量的情况下对一组无约束人脸图像进行分组的问题。我们提出了一种名为深度密度聚类（DDC）的无监督聚类算法，该算法基于测量特征空间中局部邻域之间的密度亲和力。通过学习每个邻域的最小覆盖球，封装了关于底层结构的信息。这种封装还能够定位邻域的高密度区域，这有助于测量邻域的相似性。我们从理论上证明了这种封装渐近收敛于Parzen窗口密度估计器。我们的实验表明，当主体数量未知时，DDC是聚类无约束人脸的优秀候选者。与对选择操作点敏感的传统链接和基于密度的方法不同，DDC获得了更一致和改进的性能。此外，密度感知特性降低了找到适当操作点的难度。","领域":"人脸识别/无监督学习/密度估计","问题":"在不知道主体数量的情况下对一组无约束人脸图像进行分组","动机":"解决传统链接和基于密度的方法对选择操作点敏感的问题，提高聚类无约束人脸图像的性能和一致性","方法":"提出了一种名为深度密度聚类（DDC）的无监督聚类算法，基于测量特征空间中局部邻域之间的密度亲和力，通过学习每个邻域的最小覆盖球来封装底层结构信息，并定位邻域的高密度区域以测量相似性","关键词":["无约束人脸","密度聚类","无监督学习","密度估计"],"涉及的技术概念":"深度密度聚类（DDC）是一种无监督聚类算法，通过测量特征空间中局部邻域之间的密度亲和力来工作。它通过学习每个邻域的最小覆盖球来封装底层结构信息，并能够定位邻域的高密度区域，这有助于测量邻域的相似性。理论上，这种封装方法渐近收敛于Parzen窗口密度估计器。"},{"order":841,"title":"Geometric Multi-Model Fitting With a Convex Relaxation Algorithm","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Amayo_Geometric_Multi-Model_Fitting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Amayo_Geometric_Multi-Model_Fitting_CVPR_2018_paper.html","abstract":"We propose a novel method for fitting multiple geometric models to multi-structural data via convex relaxation. Unlike greedy methods - which maximise the number of inliers - our approach efficiently searches for a soft assignment of points to geometric models by minimising the energy of the overall assignment. The inherently parallel nature of our approach, as compared to the sequential approach found in state-of-the-art energy minimisation techniques, allows for the elegant treatment of a scaling factor that occurs as the number of features in the data increases. This results in an energy minimisation that, per iteration, is as much as two orders of magnitude faster on comparable architectures thus bringing real-time, robust performance to a wider set of geometric multi-model fitting problems.  We demonstrate the versatility of our approach on two canonical problems in estimating structure from images: plane extraction from RGB-D images and homography estimation from pairs of images. Our approach seamlessly adapts to the different metrics brought forth in these distinct problems. In both cases, we report results on publicly available data-sets that in most instances outperform the state-of-the-art while simultaneously presenting run-times that are as much as an order of magnitude faster.","中文标题":"几何多模型拟合与凸松弛算法","摘要翻译":"我们提出了一种通过凸松弛将多个几何模型拟合到多结构数据的新方法。与贪婪方法——最大化内点数量——不同，我们的方法通过最小化整体分配的能量，有效地搜索点到几何模型的软分配。与最先进的能量最小化技术中的顺序方法相比，我们方法固有的并行性允许优雅地处理随着数据中特征数量增加而出现的缩放因子。这导致每次迭代的能量最小化在可比架构上快多达两个数量级，从而为更广泛的几何多模型拟合问题带来了实时、鲁棒的性能。我们在从图像估计结构的两个典型问题上展示了我们方法的多样性：从RGB-D图像中提取平面和从图像对中估计单应性。我们的方法无缝适应这些不同问题中提出的不同度量。在这两种情况下，我们报告了在大多数情况下优于最先进技术的公开可用数据集上的结果，同时展示了快多达一个数量级的运行时间。","领域":"几何模型拟合/能量最小化/并行计算","问题":"如何有效地将多个几何模型拟合到多结构数据","动机":"提高几何多模型拟合问题的实时性和鲁棒性","方法":"通过凸松弛最小化整体分配的能量，实现点到几何模型的软分配","关键词":["凸松弛","几何模型拟合","能量最小化","并行计算","实时性能"],"涉及的技术概念":"凸松弛是一种优化技术，用于近似解决难以直接求解的优化问题。几何模型拟合涉及将数学模型拟合到数据点以描述数据的几何结构。能量最小化是一种优化策略，旨在找到系统的最低能量状态。并行计算涉及同时使用多个计算资源来加速计算过程。"},{"order":842,"title":"Fast and Robust Estimation for Unit-Norm Constrained Linear Fitting Problems","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ikami_Fast_and_Robust_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ikami_Fast_and_Robust_CVPR_2018_paper.html","abstract":"M-estimator using iteratively reweighted least squares (IRLS) is one of the best-known methods for robust estimation. However, IRLS is ineffective for robust unit-norm constrained linear fitting (UCLF) problems, such as fundamental matrix estimation because of a poor initial solution. We overcome this problem by developing a novel objective function and its optimization, named iteratively reweighted eigenvalues minimization (IREM). IREM is guaranteed to decrease the objective function and achieves fast convergence and high robustness. In robust fundamental matrix estimation, IREM performs approximately 5-500 times faster than random sampling consensus (RANSAC) while preserving comparable or superior robustness.","中文标题":"快速且鲁棒的单元范数约束线性拟合问题估计","摘要翻译":"使用迭代重加权最小二乘法（IRLS）的M估计器是鲁棒估计中最著名的方法之一。然而，由于初始解不佳，IRLS对于鲁棒的单元范数约束线性拟合（UCLF）问题（如基础矩阵估计）无效。我们通过开发一种新的目标函数及其优化方法，称为迭代重加权特征值最小化（IREM），克服了这个问题。IREM保证减少目标函数，并实现快速收敛和高鲁棒性。在鲁棒基础矩阵估计中，IREM的执行速度比随机采样一致性（RANSAC）快大约5到500倍，同时保持相当或更优的鲁棒性。","领域":"基础矩阵估计/鲁棒估计/线性拟合","问题":"解决单元范数约束线性拟合问题中的鲁棒估计问题","动机":"由于迭代重加权最小二乘法（IRLS）在单元范数约束线性拟合（UCLF）问题中因初始解不佳而无效，需要开发新的方法以提高估计的鲁棒性和速度。","方法":"开发了一种新的目标函数及其优化方法，称为迭代重加权特征值最小化（IREM），该方法保证减少目标函数，并实现快速收敛和高鲁棒性。","关键词":["单元范数约束线性拟合","鲁棒估计","基础矩阵估计","迭代重加权特征值最小化"],"涉及的技术概念":{"M-estimator":"一种用于鲁棒估计的统计方法，通过最小化一个损失函数来估计参数。","迭代重加权最小二乘法（IRLS）":"一种迭代算法，用于求解加权最小二乘问题，每次迭代都会更新权重。","单元范数约束线性拟合（UCLF）":"一种线性拟合问题，其中解被约束为单位范数。","基础矩阵估计":"在计算机视觉中，用于估计两个视图之间的几何关系。","迭代重加权特征值最小化（IREM）":"一种新的优化方法，通过迭代重加权特征值来最小化目标函数，以提高鲁棒性和收敛速度。","随机采样一致性（RANSAC）":"一种用于估计数学模型参数的迭代方法，能够从包含大量异常值的数据集中估计出模型参数。"}},{"order":843,"title":"Importance Weighted Adversarial Nets for Partial Domain Adaptation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.html","abstract":"This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.","中文标题":"重要性加权对抗网络用于部分领域适应","摘要翻译":"本文提出了一种基于重要性加权对抗网络的无监督领域适应方法，特别适用于目标领域类别数少于源领域的部分领域适应。以往的领域适应方法通常假设标签空间相同，因此减少分布差异可以实现可行的知识转移。然而，在更现实的场景中，这种假设不再有效，该场景需要从更大、更多样化的源领域适应到类别数较少的较小目标领域。本文扩展了基于对抗网络的领域适应，并提出了一种新的基于对抗网络的部分领域适应方法，以识别可能来自异常类别的源样本，同时减少领域间共享类别的偏移。","领域":"领域适应/对抗网络/无监督学习","问题":"解决在目标领域类别数少于源领域的情况下，如何有效进行领域适应的问题","动机":"在现实场景中，源领域和目标领域的类别数可能不同，传统的领域适应方法无法有效处理这种情况，因此需要新的方法来识别异常类别并减少共享类别的偏移","方法":"提出了一种基于重要性加权对抗网络的部分领域适应方法，通过识别异常类别的源样本和减少共享类别的偏移来实现领域适应","关键词":["领域适应","对抗网络","无监督学习","部分领域适应","重要性加权"],"涉及的技术概念":"重要性加权对抗网络是一种改进的对抗网络，用于在无监督学习环境下进行领域适应，特别是在源领域和目标领域类别数不同的情况下。通过重要性加权，可以识别并减少异常类别的影响，同时优化共享类别的知识转移。"},{"order":844,"title":"Efficient Subpixel Refinement With Symbolic Linear Predictors","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lui_Efficient_Subpixel_Refinement_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lui_Efficient_Subpixel_Refinement_CVPR_2018_paper.html","abstract":"We present an efficient subpixel refinement method using a learning-based approach called Linear Predictors. Firstly, we present a novel technique, called Symbolic Linear Predictors, which makes the learning step efficient for subpixel refinement. This makes our approach feasible for online applications without compromising accuracy, while taking advantage of the run-time efficiency of learning based approaches. Secondly, we show how Linear Predictors can be used to predict the expected alignment error, allowing us to use only the best keypoints in resource constrained applications. We show the efficiency and accuracy of our method through extensive experiments.","中文标题":"使用符号线性预测器的高效亚像素细化","摘要翻译":"我们提出了一种使用称为线性预测器的基于学习的方法进行高效亚像素细化的方法。首先，我们介绍了一种称为符号线性预测器的新技术，这使得学习步骤对于亚像素细化变得高效。这使得我们的方法在不牺牲准确性的情况下适用于在线应用，同时利用基于学习方法的运行时效率。其次，我们展示了如何使用线性预测器来预测预期的对齐误差，使我们能够在资源受限的应用中仅使用最佳关键点。通过广泛的实验，我们展示了我们方法的效率和准确性。","领域":"亚像素细化/关键点检测/在线应用","问题":"提高亚像素细化的效率和准确性","动机":"为了在不牺牲准确性的情况下，使亚像素细化方法适用于在线应用，并提高资源受限应用中的关键点选择效率","方法":"采用符号线性预测器技术进行高效学习，并使用线性预测器预测对齐误差以选择最佳关键点","关键词":["亚像素细化","符号线性预测器","关键点检测","在线应用"],"涉及的技术概念":"符号线性预测器是一种新技术，用于提高亚像素细化过程中的学习效率。线性预测器用于预测图像对齐过程中的误差，从而在资源受限的情况下选择最佳关键点进行细化。"},{"order":845,"title":"Scale-Recurrent Network for Deep Image Deblurring","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tao_Scale-Recurrent_Network_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tao_Scale-Recurrent_Network_for_CVPR_2018_paper.html","abstract":"In single image deblurring, the \`\`coarse-to-fine'' scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches, it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively.","中文标题":"尺度循环网络用于深度图像去模糊","摘要翻译":"在单幅图像去模糊中，“由粗到细”的方案，即在金字塔中逐渐恢复不同分辨率的清晰图像，在传统的基于优化的方法和最近的基于神经网络的方法中都非常成功。在本文中，我们研究了这一策略，并提出了一个尺度循环网络（SRN-DeblurNet）用于这一去模糊任务。与许多最近基于学习的方法相比，它具有更简单的网络结构、更少的参数，并且更容易训练。我们在具有复杂运动的大规模去模糊数据集上评估了我们的方法。结果表明，我们的方法在数量和质量上都能产生比现有技术更好的结果。","领域":"图像去模糊/神经网络/深度学习","问题":"单幅图像去模糊","动机":"研究并改进“由粗到细”的去模糊策略，以提高去模糊效果","方法":"提出了一个尺度循环网络（SRN-DeblurNet），该网络具有更简单的结构、更少的参数，并且更容易训练","关键词":["图像去模糊","尺度循环网络","神经网络"],"涉及的技术概念":"“由粗到细”方案是一种在图像处理中常用的策略，通过在不同分辨率上逐步恢复图像来提高处理效果。尺度循环网络（SRN-DeblurNet）是一种专门设计的神经网络，用于图像去模糊任务，它通过简化网络结构和减少参数数量来提高训练效率和去模糊效果。"},{"order":846,"title":"DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kupyn_DeblurGAN_Blind_Motion_CVPR_2018_paper.html","abstract":"We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance   both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem -- object detection on (de-)blurred images.   The method is 5 times faster than the closest competitor -- DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from  sharp ones, allowing realistic dataset augmentation.    The model, code and the dataset are available at https://github.com/KupynOrest/DeblurGAN","中文标题":"DeblurGAN: 使用条件对抗网络进行盲运动去模糊","摘要翻译":"我们提出了DeblurGAN，一种用于运动去模糊的端到端学习方法。该方法基于条件生成对抗网络（GAN）和内容损失。DeblurGAN在结构相似性度量和视觉外观方面均达到了最先进的性能。去模糊模型的质量也在一个现实世界的问题上进行了新颖的评估——在（去）模糊图像上进行物体检测。该方法比最接近的竞争对手DeepDeblur快5倍。我们还介绍了一种从清晰图像生成合成运动模糊图像的新方法，允许进行真实的数据集增强。模型、代码和数据集可在https://github.com/KupynOrest/DeblurGAN获取。","领域":"图像去模糊/生成对抗网络/物体检测","问题":"解决运动模糊图像的去模糊问题","动机":"提高运动模糊图像的去模糊效果，同时加快处理速度","方法":"基于条件生成对抗网络（GAN）和内容损失的端到端学习方法","关键词":["运动去模糊","条件生成对抗网络","物体检测","数据集增强"],"涉及的技术概念":"条件生成对抗网络（GAN）是一种深度学习模型，用于生成数据。内容损失是一种用于衡量生成图像与目标图像之间差异的损失函数。结构相似性度量（SSIM）是一种用于衡量两幅图像相似度的指标。"},{"order":847,"title":"A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_A2-RL_Aesthetics_Aware_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_A2-RL_Aesthetics_Aware_CVPR_2018_paper.html","abstract":"Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.","中文标题":"A2-RL: 美学感知强化学习用于图像裁剪","摘要翻译":"图像裁剪旨在通过调整图像的构图来提高其美学质量。大多数弱监督裁剪方法（没有边界框监督）依赖于滑动窗口机制。滑动窗口机制需要固定的宽高比，并限制了任意大小的裁剪区域。此外，滑动窗口方法通常在输入图像上产生数万个窗口，这非常耗时。受这些挑战的启发，我们首先将美学图像裁剪制定为一个顺序决策过程，并提出了一个弱监督的美学感知强化学习（A2-RL）框架来解决这个问题。特别是，所提出的方法开发了一个美学感知的奖励函数，这对图像裁剪特别有益。类似于人类的决策过程，我们使用了一个包括当前观察和历史经验的综合状态表示。我们使用演员-评论家架构以端到端的方式训练代理。该代理在几个流行的未见裁剪数据集上进行了评估。实验结果表明，与之前的弱监督方法相比，我们的方法在候选窗口数量和时间上都大大减少，达到了最先进的性能。","领域":"图像美学评估/强化学习/图像裁剪","问题":"解决图像裁剪中美学质量提升的问题，特别是在没有边界框监督的情况下。","动机":"滑动窗口机制在图像裁剪中存在固定宽高比限制和效率低下的问题，需要一种更高效且灵活的方法来提升图像的美学质量。","方法":"提出了一种弱监督的美学感知强化学习（A2-RL）框架，通过开发美学感知的奖励函数和综合状态表示，使用演员-评论家架构以端到端的方式训练代理。","关键词":["图像美学评估","强化学习","图像裁剪","弱监督学习","演员-评论家架构"],"涉及的技术概念":{"滑动窗口机制":"一种在图像上生成多个裁剪窗口的方法，需要固定宽高比，效率低下。","美学感知强化学习（A2-RL）":"一种结合美学评估和强化学习的框架，用于图像裁剪。","演员-评论家架构":"一种强化学习中的训练方法，结合了策略网络（演员）和价值网络（评论家）的优点。","美学感知的奖励函数":"在强化学习中用于评估图像裁剪结果美学质量的函数。","综合状态表示":"在强化学习中，代理的状态表示包括当前观察和历史经验，以更好地指导决策。"}},{"order":848,"title":"Single Image Dehazing via Conditional Generative Adversarial Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Single_Image_Dehazing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Single_Image_Dehazing_CVPR_2018_paper.html","abstract":"In this paper, we present an algorithm to directly restore a clear image from a hazy image. This problem is highly ill-posed and most existing algorithms often use hand-crafted features, e.g., dark channel, color disparity, maximum contrast, to estimate transmission maps and then atmospheric lights. In contrast, we solve this problem based on a conditional generative adversarial network (cGAN), where the clear image is estimated by an end-to-end trainable neural network. Different from the generative network in basic cGAN, we propose an encoder and decoder architecture so that it can generate better results. To generate realistic clear images, we further modify the basic cGAN formulation by introducing the VGG features and a L_1-regularized gradient prior. We also synthesize a hazy dataset including indoor and outdoor scenes to train and evaluate the proposed algorithm. Extensive experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on both synthetic dataset and real world hazy images.","中文标题":"通过条件生成对抗网络进行单图像去雾","摘要翻译":"在本文中，我们提出了一种算法，直接从有雾图像中恢复清晰图像。这个问题非常不适定，大多数现有算法通常使用手工制作的特征，例如暗通道、色差、最大对比度，来估计传输图和大气光。相比之下，我们基于条件生成对抗网络（cGAN）解决了这个问题，其中清晰图像通过端到端可训练的神经网络估计。与基本cGAN中的生成网络不同，我们提出了一个编码器和解码器架构，以便生成更好的结果。为了生成逼真的清晰图像，我们进一步修改了基本cGAN公式，引入了VGG特征和L_1正则化梯度先验。我们还合成了一个包括室内和室外场景的有雾数据集，以训练和评估所提出的算法。大量实验结果表明，所提出的方法在合成数据集和真实世界有雾图像上均优于最先进的方法。","领域":"图像去雾/生成对抗网络/神经网络架构","问题":"从有雾图像中直接恢复清晰图像","动机":"现有算法通常依赖手工制作的特征来估计传输图和大气光，这限制了去雾效果和算法的泛化能力。","方法":"基于条件生成对抗网络（cGAN），采用编码器和解码器架构，并引入VGG特征和L_1正则化梯度先验来生成更逼真的清晰图像。","关键词":["图像去雾","条件生成对抗网络","VGG特征","L_1正则化梯度先验"],"涉及的技术概念":"条件生成对抗网络（cGAN）是一种生成模型，它通过对抗过程学习生成数据。VGG特征指的是使用VGG网络提取的图像特征，用于提高生成图像的质量。L_1正则化梯度先验是一种正则化技术，用于在图像生成过程中保持图像的细节和边缘。"},{"order":849,"title":"On the Duality Between Retinex and Image Dehazing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Galdran_On_the_Duality_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Galdran_On_the_Duality_CVPR_2018_paper.html","abstract":"Image dehazing deals with the removal of undesired loss of visibility in outdoor images due to the presence of fog. Retinex is a color vision model mimicking the ability of the Human Visual System to robustly discount varying illuminations when observing a scene under different spectral lighting conditions. Retinex has been widely explored in the computer vision literature for image enhancement and other related tasks. While these two problems are apparently unrelated, the goal of this work is to show that they can be connected by a simple linear relationship. Specifically, most Retinex-based algorithms have the characteristic feature of always increasing image brightness, which turns them into ideal candidates for effective image dehazing by directly applying Retinex to a hazy image whose intensities have been inverted. In this paper, we give theoretical proof that Retinex on inverted intensities is a solution to the image dehazing problem. Comprehensive qualitative and quantitative results indicate that several classical and modern implementations of Retinex can be transformed into competing image dehazing algorithms performing on pair with more complex fog removal methods, and can overcome some of the main challenges associated with this problem.","中文标题":"关于Retinex与图像去雾之间的对偶性","摘要翻译":"图像去雾处理的是由于雾的存在导致户外图像中不希望出现的能见度损失问题。Retinex是一种模仿人类视觉系统在不同光谱照明条件下观察场景时能够稳健地折扣变化光照的颜色视觉模型。Retinex在计算机视觉文献中已被广泛探索用于图像增强和其他相关任务。虽然这两个问题表面上看似无关，但本工作的目标是展示它们可以通过一个简单的线性关系连接起来。具体来说，大多数基于Retinex的算法都有一个特征，即总是增加图像亮度，这使得它们成为通过直接将Retinex应用于强度已被反转的雾化图像来进行有效图像去雾的理想候选者。在本文中，我们给出了理论证明，即在反转强度上应用Retinex是解决图像去雾问题的一种方法。全面的定性和定量结果表明，几种经典和现代的Retinex实现可以转化为与更复杂的雾去除方法相媲美的图像去雾算法，并且可以克服与此问题相关的一些主要挑战。","领域":"图像去雾/Retinex理论/图像增强","问题":"解决户外图像由于雾的存在导致的能见度损失问题","动机":"探索Retinex与图像去雾之间的潜在联系，以简化去雾过程","方法":"通过理论证明和应用Retinex于反转强度的雾化图像，提出一种新的图像去雾方法","关键词":["图像去雾","Retinex","图像增强"],"涉及的技术概念":"Retinex是一种模仿人类视觉系统在不同光照条件下观察场景时能够稳健地折扣变化光照的颜色视觉模型。本文通过将Retinex应用于强度已被反转的雾化图像，提出了一种新的图像去雾方法，并给出了理论证明。"},{"order":850,"title":"Arbitrary Style Transfer With Deep Feature Reshuffle","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_Arbitrary_Style_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_Arbitrary_Style_Transfer_CVPR_2018_paper.html","abstract":"This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods.","中文标题":"通过深度特征重排实现任意风格转换","摘要翻译":"本文介绍了一种通过重排风格图像的深度特征（即，置换特征图的空间位置）来实现任意风格转换的新方法。我们从理论上证明了，基于重排的新风格损失连接了大多数参数化和非参数化神经风格转换方法分别使用的全局和局部风格损失。这一简单想法能有效解决现有风格转换方法中的挑战性问题。一方面，与神经参数化方法相比，它可以避免局部风格模式的扭曲，并允许语义级别的转换。另一方面，与神经非参数化方法相比，它可以保持与风格图像全局相似的外观，并避免褪色伪影。基于提出的损失，我们还提出了一种渐进式特征域优化方法。实验表明，我们的方法广泛适用于各种风格，并且比现有方法产生更好的质量。","领域":"风格转换/图像生成/特征优化","问题":"解决现有风格转换方法中的局部风格模式扭曲和全局外观褪色问题","动机":"为了克服现有神经风格转换方法在保持局部风格细节和全局外观一致性方面的不足","方法":"通过重排风格图像的深度特征，提出一种新的风格损失，并采用渐进式特征域优化方法","关键词":["风格转换","深度特征重排","特征域优化"],"涉及的技术概念":"深度特征重排指的是在特征图中置换空间位置的操作，用于风格转换中。渐进式特征域优化是一种逐步优化特征域的方法，以提高风格转换的质量。"},{"order":851,"title":"Nonlocal Low-Rank Tensor Factor Analysis for Image Restoration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Nonlocal_Low-Rank_Tensor_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Nonlocal_Low-Rank_Tensor_CVPR_2018_paper.html","abstract":"Low-rank signal modeling has been widely leveraged to capture non-local correlation in image processing applications. We propose a new method that employs low-rank tensor factor analysis for tensors generated by grouped image patches. The low-rank tensors are fed into the alternative direction multiplier method (ADMM) to further improve image reconstruction. The motivating application is compressive sensing (CS), and a deep convolutional architecture is adopted to approximate the expensive matrix inversion in CS applications. An iterative algorithm based on this low-rank tensor factorization strategy, called NLR-TFA, is presented in detail. Experimental results on noiseless and noisy CS measurements demonstrate the superiority of the proposed approach, especially at low CS sampling rates.","中文标题":"非局部低秩张量因子分析用于图像恢复","摘要翻译":"低秩信号建模已被广泛应用于捕捉图像处理应用中的非局部相关性。我们提出了一种新方法，该方法利用低秩张量因子分析来处理由分组图像块生成的张量。这些低秩张量被输入到交替方向乘子法（ADMM）中，以进一步提高图像重建质量。该方法的动机应用是压缩感知（CS），并采用深度卷积架构来近似CS应用中的昂贵矩阵求逆。详细介绍了基于这种低秩张量分解策略的迭代算法，称为NLR-TFA。在无噪声和有噪声的CS测量上的实验结果证明了所提出方法的优越性，特别是在低CS采样率下。","领域":"图像恢复/压缩感知/张量分析","问题":"图像恢复中的非局部相关性捕捉和高质量重建","动机":"提高压缩感知（CS）应用中的图像重建质量，特别是在低采样率下","方法":"采用低秩张量因子分析和交替方向乘子法（ADMM）进行图像重建，并利用深度卷积架构近似矩阵求逆","关键词":["低秩信号建模","非局部相关性","压缩感知","张量因子分析","交替方向乘子法","深度卷积架构"],"涉及的技术概念":{"低秩信号建模":"一种用于捕捉信号中低维结构的建模方法，广泛应用于图像处理等领域。","非局部相关性":"指图像中相距较远的像素点之间存在的相关性，是图像恢复中的重要概念。","压缩感知（CS）":"一种信号处理技术，旨在从少量采样中恢复原始信号。","张量因子分析":"一种用于分析和处理多维数据（如张量）的方法。","交替方向乘子法（ADMM）":"一种用于解决优化问题的迭代算法，特别适用于大规模问题。","深度卷积架构":"一种深度学习模型，特别适用于处理图像数据，能够自动提取图像特征。"}},{"order":852,"title":"Avatar-Net: Multi-Scale Zero-Shot Style Transfer by Feature Decoration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.html","abstract":"Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efficiency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efficient yet effective Avatar-Net that enables visually plausible multi-scale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multi- scale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efficiency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc.","中文标题":"Avatar-Net: 通过特征装饰实现多尺度零样本风格迁移","摘要翻译":"零样本艺术风格迁移是一个重要的图像合成问题，旨在将任意风格迁移到内容图像中。然而，现有方法在泛化能力和效率之间的权衡阻碍了实时高质量零样本风格迁移的实现。在本文中，我们解决了这一困境，提出了一种高效且有效的Avatar-Net，能够实现任意风格的视觉上合理的多尺度迁移。我们方法的关键成分是一个风格装饰器，它通过从任意风格图像中语义对齐的风格特征来补充内容特征，这不仅整体上匹配了它们的特征分布，而且在装饰特征中保留了详细的风格模式。通过将这个模块嵌入到一个融合多尺度风格抽象的图像重建网络中，Avatar-Net在一次前向传递中为任何风格图像渲染多尺度风格化。我们展示了所提出方法在生成高质量风格化图像方面的最先进效果和效率，包括多重风格集成、视频风格化等一系列连续应用。","领域":"图像合成/风格迁移/特征学习","问题":"实现高质量且实时的零样本艺术风格迁移","动机":"解决现有方法在泛化能力和效率之间的权衡问题，以实现更高质量的实时风格迁移","方法":"提出Avatar-Net，利用风格装饰器通过语义对齐的风格特征补充内容特征，并嵌入到图像重建网络中实现多尺度风格化","关键词":["零样本学习","风格迁移","图像合成","多尺度处理","特征装饰"],"涉及的技术概念":"零样本艺术风格迁移指的是在没有特定风格样本的情况下，将任意风格迁移到内容图像中。风格装饰器是一种技术，用于通过从风格图像中提取的特征来装饰内容图像的特征，以实现风格迁移。多尺度处理涉及在不同尺度上分析和处理图像，以捕捉和迁移更细致的风格特征。"},{"order":853,"title":"Missing Slice Recovery for Tensors Using a Low-Rank Model in Embedded Space","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yokota_Missing_Slice_Recovery_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yokota_Missing_Slice_Recovery_CVPR_2018_paper.html","abstract":"Let us consider a case where all of the elements in some continuous slices are missing in tensor data.  In this case, the nuclear-norm and total variation regularization methods usually fail to recover the missing elements.  The key problem is capturing some delay/shift-invariant structure.  In this study, we consider a low-rank model in an embedded space of a tensor.  For this purpose, we extend a delay embedding for a time series to a \`\`multi-way delay-embedding transform'' for a tensor, which takes a given incomplete tensor as the input and outputs a higher-order incomplete Hankel tensor.  The higher-order tensor is then recovered by Tucker-based low-rank tensor factorization.  Finally, an estimated tensor can be obtained by using the inverse multi-way delay embedding transform of the recovered higher-order tensor.  Our experiments showed that the proposed method successfully recovered missing slices for some color images and functional magnetic resonance images.","中文标题":"使用嵌入空间中的低秩模型进行张量缺失切片恢复","摘要翻译":"让我们考虑一种情况，在张量数据中，某些连续切片的所有元素都缺失了。在这种情况下，核范数和总变分正则化方法通常无法恢复缺失的元素。关键问题在于捕捉一些延迟/平移不变的结构。在本研究中，我们考虑了张量嵌入空间中的低秩模型。为此，我们将时间序列的延迟嵌入扩展到张量的“多路延迟嵌入变换”，该变换以给定的不完整张量作为输入，并输出一个高阶不完整Hankel张量。然后通过基于Tucker的低秩张量分解来恢复高阶张量。最后，通过使用恢复的高阶张量的逆多路延迟嵌入变换，可以获得估计的张量。我们的实验表明，所提出的方法成功恢复了一些彩色图像和功能磁共振图像的缺失切片。","领域":"张量分解/图像恢复/信号处理","问题":"恢复张量数据中连续缺失的切片","动机":"核范数和总变分正则化方法在处理张量数据中连续缺失切片时通常失败，需要一种新方法来捕捉延迟/平移不变的结构","方法":"扩展时间序列的延迟嵌入到张量的多路延迟嵌入变换，通过基于Tucker的低秩张量分解恢复高阶张量，并使用逆变换获得估计的张量","关键词":["张量分解","图像恢复","信号处理","低秩模型","Hankel张量"],"涉及的技术概念":"核范数、总变分正则化、延迟嵌入、多路延迟嵌入变换、Hankel张量、Tucker分解、低秩张量分解"},{"order":854,"title":"Deep Semantic Face Deblurring","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Semantic_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Deep_Semantic_Face_CVPR_2018_paper.html","abstract":"In this paper, we present an effective and efficient face deblurring algorithm by exploiting semantic cues via deep convolutional neural networks (CNNs). As face images are highly structured and share several key semantic components (e.g., eyes and mouths), the semantic information of a face provides a strong prior for restoration. As such, we propose to incorporate global semantic priors as input and impose local structure losses to regularize the output within a multi-scale deep CNN. We train the network with perceptual and adversarial losses to generate photo-realistic results and develop an incremental training strategy to handle random blur kernels in the wild. Quantitative and qualitative evaluations demonstrate that the proposed face deblurring algorithm restores sharp images with more facial details and performs favorably against state-of-the-art methods in terms of restoration quality, face recognition and execution speed.","中文标题":"深度语义人脸去模糊","摘要翻译":"本文提出了一种通过深度卷积神经网络（CNNs）利用语义线索的有效且高效的人脸去模糊算法。由于人脸图像高度结构化并共享多个关键语义组件（例如眼睛和嘴巴），人脸的语义信息为恢复提供了强有力的先验。因此，我们提出将全局语义先验作为输入，并在多尺度深度CNN中施加局部结构损失以正则化输出。我们使用感知和对抗性损失训练网络以生成逼真的结果，并开发了一种增量训练策略来处理野外随机模糊核。定量和定性评估表明，所提出的人脸去模糊算法恢复了具有更多面部细节的清晰图像，并在恢复质量、人脸识别和执行速度方面优于最先进的方法。","领域":"人脸去模糊/深度学习/图像恢复","问题":"解决人脸图像在模糊情况下的恢复问题","动机":"利用人脸的语义信息作为恢复的强先验，以提高去模糊效果","方法":"采用深度卷积神经网络，结合全局语义先验和局部结构损失，使用感知和对抗性损失训练网络，并开发增量训练策略处理随机模糊核","关键词":["人脸去模糊","语义信息","深度卷积神经网络","图像恢复"],"涉及的技术概念":{"深度卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据","语义信息":"指图像中对象的含义或内容，如人脸中的眼睛和嘴巴","全局语义先验":"指整个图像的语义信息作为恢复的先验知识","局部结构损失":"在图像恢复过程中，用于保持图像局部结构一致性的损失函数","感知和对抗性损失":"用于训练生成模型，以生成逼真图像的损失函数","增量训练策略":"一种逐步增加训练难度或复杂度的训练方法，用于提高模型的泛化能力"}},{"order":855,"title":"GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.html","abstract":"In this paper, we propose a GraphBit method to learn deep binary descriptors in a directed acyclic graph unsupervisedly, representing bitwise interactions as edges between the nodes of bits. Conventional binary representation learning methods enforce each element to be binarized into zero or one. However, there are elements lying in the boundary which suffer from doubtful binarization as \`\`ambiguous bits''. Ambiguous bits fail to collect effective information for confident binarization, which are unreliable and sensitive to noise. We argue that there are implicit inner relationships between bits in binary descriptors, where the related bits can provide extra instruction as prior knowledge for ambiguity elimination. Specifically, we design a deep reinforcement learning model to learn the structure of the graph for bitwise interaction mining, reducing the uncertainty of binary codes by maximizing the mutual information with inputs and related bits, so that the ambiguous bits receive additional instruction from the graph for confident binarization. Due to the reliability of the proposed binary codes with bitwise interaction, we obtain an average improvement of 9.64%, 8.84% and 3.22% on the CIFAR-10, Brown and HPatches datasets respectively compared with the state-of-the-art unsupervised binary descriptors.","中文标题":"GraphBit: 通过深度强化学习进行位级交互挖掘","摘要翻译":"在本文中，我们提出了一种GraphBit方法，用于在无监督的有向无环图中学习深度二进制描述符，将位级交互表示为位节点之间的边。传统的二进制表示学习方法强制每个元素二值化为零或一。然而，存在位于边界上的元素，它们作为“模糊位”遭受可疑的二值化。模糊位无法收集有效信息以进行自信的二值化，这些位不可靠且对噪声敏感。我们认为二进制描述符中的位之间存在隐式的内部关系，其中相关的位可以作为先验知识提供额外的指导以消除模糊性。具体来说，我们设计了一个深度强化学习模型来学习图的结构以进行位级交互挖掘，通过最大化与输入和相关位的互信息来减少二进制代码的不确定性，从而使模糊位从图中获得额外的指导以进行自信的二值化。由于所提出的具有位级交互的二进制代码的可靠性，我们在CIFAR-10、Brown和HPatches数据集上分别获得了9.64%、8.84%和3.22%的平均改进，与最先进的无监督二进制描述符相比。","领域":"二进制描述符学习/位级交互挖掘/深度强化学习","问题":"解决二进制描述符中模糊位的二值化问题","动机":"传统的二进制表示学习方法在处理位于边界上的元素时存在模糊性问题，这些模糊位不可靠且对噪声敏感，影响了二进制描述符的准确性和可靠性。","方法":"设计了一个深度强化学习模型来学习图的结构，通过最大化与输入和相关位的互信息来减少二进制代码的不确定性，从而使模糊位从图中获得额外的指导以进行自信的二值化。","关键词":["二进制描述符","位级交互","深度强化学习","模糊位","互信息"],"涉及的技术概念":"二进制描述符学习涉及将数据表示为二进制代码的过程，位级交互挖掘关注于发现二进制代码中位之间的相互作用，深度强化学习是一种通过奖励机制来训练模型以做出决策的机器学习方法，模糊位指的是在二值化过程中难以确定其值的位，互信息是衡量两个变量之间相互依赖性的指标。"},{"order":856,"title":"Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.html","abstract":"We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage.  This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.","中文标题":"循环显著性转换网络：整合多阶段视觉线索用于小器官分割","摘要翻译":"我们的目标是从腹部CT扫描中分割小器官（例如胰腺）。由于目标通常在输入图像中占据相对较小的区域，深度神经网络很容易被复杂多变的背景所混淆。为了缓解这一问题，研究人员提出了一种从粗到细的方法，该方法使用第一（粗）阶段的预测来指示第二（细）阶段的较小输入区域。尽管这种方法有效，但它分别处理两个阶段，缺乏优化全局能量函数，限制了其整合多阶段视觉线索的能力。缺失的上下文信息导致迭代中的收敛不令人满意，有时细阶段的分割精度甚至低于粗阶段。本文提出了一种循环显著性转换网络。关键创新是一个显著性转换模块，它反复将前一次迭代的分割概率图转换为空间权重，并将这些权重应用于当前迭代。这给我们带来了双重好处。在训练中，它允许对不同输入尺度的深度网络进行联合优化。在测试中，它在整个迭代过程中传播多阶段视觉信息，以提高分割精度。在NIH胰腺分割数据集上的实验证明了最先进的精度，平均比之前的最佳结果高出2%以上。在我们自己收集的更大数据集上的几个小器官上也报告了更高的精度。此外，我们的方法具有更好的收敛特性，使其在实践中更高效和可靠。","领域":"医学图像分割/深度学习/显著性检测","问题":"从腹部CT扫描中准确分割小器官（如胰腺）","动机":"由于小器官在输入图像中占据的区域较小，深度神经网络容易被复杂多变的背景所混淆，导致分割精度不高","方法":"提出了一种循环显著性转换网络，通过显著性转换模块反复将前一次迭代的分割概率图转换为空间权重，并应用于当前迭代，以整合多阶段视觉信息，提高分割精度","关键词":["医学图像分割","显著性检测","循环神经网络"],"涉及的技术概念":"循环显著性转换网络、显著性转换模块、分割概率图、空间权重、多阶段视觉信息、联合优化、深度网络、收敛特性"},{"order":857,"title":"Thoracic Disease Identification and Localization With Limited Supervision","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Thoracic_Disease_Identification_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Thoracic_Disease_Identification_CVPR_2018_paper.html","abstract":"Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations.  To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images.We demonstrate that our approach can effectively leverage both class information as well as limited location annotation,  and significantly outperforms the comparative reference baseline in both classification and localization tasks.","中文标题":"有限监督下的胸部疾病识别与定位","摘要翻译":"从放射学图像中准确识别和定位异常在临床诊断和治疗计划中起着不可或缺的作用。为这些任务构建高精度的预测模型通常需要大量手动标注有标签和异常位置的图像。然而，在现实中，这样的标注数据获取成本高昂，尤其是那些带有位置标注的数据。我们需要能够在仅有少量位置标注的情况下也能良好工作的方法。为了应对这一挑战，我们提出了一种统一的方法，该方法通过相同的底层模型对所有图像同时执行疾病识别和定位。我们证明了我们的方法能够有效利用类别信息以及有限的位置标注，在分类和定位任务中显著优于比较参考基线。","领域":"医学影像分析/放射学/疾病诊断","问题":"在有限的位置标注数据下，如何准确识别和定位放射学图像中的异常","动机":"由于手动标注数据尤其是带有位置标注的数据获取成本高昂，需要开发能够在仅有少量位置标注的情况下也能良好工作的方法","方法":"提出了一种统一的方法，通过相同的底层模型对所有图像同时执行疾病识别和定位，有效利用类别信息以及有限的位置标注","关键词":["医学影像分析","放射学","疾病诊断","有限监督学习","异常定位"],"涉及的技术概念":"有限监督学习指的是在仅有少量标注数据的情况下进行模型训练的方法。类别信息指的是图像中异常的类型标签，而位置标注则是指异常在图像中的具体位置信息。统一的方法意味着使用同一个模型同时处理疾病识别和定位两个任务，这样可以提高模型的效率和性能。"},{"order":858,"title":"Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Quantization_of_Fully_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xu_Quantization_of_Fully_CVPR_2018_paper.html","abstract":"With pervasive applications of medical imaging in healthcare, biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. Since manual annotation suffers limited reproducibility, arduous efforts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), particularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmentation, attaining much improved performance. At the same time, quantization of DNNs has become an active research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing literature on quantization which primarily targets memory and computation complexity reduction, we apply quantization as a method to reduce overfitting in FCNs for better accuracy. Specifically, we focus on a state-of-the-art segmentation framework, suggestive annotation [22], which judiciously extracts representative annotation samples from the original training dataset, obtaining an effective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantization for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method have a reduction of up to 6.4x on memory usage.","中文标题":"全卷积网络的量化用于精确的生物医学图像分割","摘要翻译":"随着医学成像在医疗保健中的广泛应用，生物医学图像分割在定量分析、临床诊断和医疗干预中扮演着核心角色。由于手动注释存在重复性有限、努力艰巨和耗时过多的问题，自动分割被期望用于处理日益增大的组织病理学数据。最近，深度神经网络（DNNs），特别是全卷积网络（FCNs），已被广泛应用于生物医学图像分割，取得了显著改进的性能。同时，DNNs的量化已成为一个活跃的研究课题，旨在用较少的内存（精度）表示权重，以显著减少DNNs的内存和计算需求，同时保持可接受的准确性。在本文中，我们将量化技术应用于FCNs，以实现精确的生物医学图像分割。与主要针对减少内存和计算复杂性的现有量化文献不同，我们将量化作为一种减少FCNs过拟合以提高准确性的方法。具体来说，我们专注于一种最先进的分割框架，即建议性注释[22]，它从原始训练数据集中明智地提取代表性注释样本，获得一个有效的小规模平衡训练数据集。我们为该框架开发了两种新的量化过程：（1）用于高度代表性训练样本的建议性注释与量化，（2）用于高准确性的网络训练与量化。在MICCAI Gland数据集上的大量实验表明，这两种量化过程都能提高分割性能，我们提出的方法比当前最先进的性能高出1%。此外，我们的方法在内存使用上减少了6.4倍。","领域":"生物医学图像分割/神经网络量化/过拟合减少","问题":"如何在减少内存和计算需求的同时，提高生物医学图像分割的准确性","动机":"手动注释生物医学图像存在重复性有限、努力艰巨和耗时过多的问题，需要自动分割方法来处理日益增大的组织病理学数据","方法":"应用量化技术于全卷积网络，通过减少过拟合来提高分割准确性，并开发了两种新的量化过程：建议性注释与量化和网络训练与量化","关键词":["生物医学图像分割","神经网络量化","过拟合减少"],"涉及的技术概念":"全卷积网络（FCNs）是一种深度神经网络，特别适用于图像分割任务。量化技术是指减少神经网络权重和激活的精度，以减少内存和计算需求。过拟合是指模型在训练数据上表现良好，但在未见过的数据上表现不佳的现象。建议性注释是一种从大量数据中提取代表性样本的方法，用于训练更有效的模型。"},{"order":859,"title":"Visual Feature Attribution Using Wasserstein GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baumgartner_Visual_Feature_Attribution_CVPR_2018_paper.html","abstract":"Attributing the pixels of an input image to a certain category is an important and well-studied problem in computer vision, with applications ranging from weakly supervised localisation to understanding hidden effects in the data. In recent years, approaches based on interpreting a previously trained neural network classifier have become the de facto state-of-the-art and are commonly used on medical as well as natural image datasets. In this paper, we discuss a limitation of these approaches which may lead to only a subset of the category specific features being detected. To address this problem we develop a novel feature attribution technique based on Wasserstein Generative Adversarial Networks (WGAN), which does not suffer from this limitation. We show that our proposed method performs substantially better than the state-of-the-art for visual attribution on a synthetic dataset and on real 3D neuroimaging data from patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD). For AD patients the method produces compellingly realistic disease effect maps which are very close to the observed effects.","中文标题":"使用Wasserstein GANs进行视觉特征归因","摘要翻译":"将输入图像的像素归因于某个类别是计算机视觉中一个重要且被深入研究的问题，其应用范围从弱监督定位到理解数据中的隐藏效应。近年来，基于解释先前训练的神经网络分类器的方法已成为事实上的最先进技术，并常用于医学和自然图像数据集。在本文中，我们讨论了这些方法的一个局限性，即可能只检测到类别特定特征的一个子集。为了解决这个问题，我们开发了一种基于Wasserstein生成对抗网络（WGAN）的新特征归因技术，该技术不受此限制的影响。我们展示了我们提出的方法在合成数据集和来自轻度认知障碍（MCI）和阿尔茨海默病（AD）患者的真实3D神经影像数据上的视觉归因性能显著优于最先进的技术。对于AD患者，该方法产生了非常接近观察到的效应的逼真疾病效应图。","领域":"神经影像分析/医学图像处理/生成对抗网络","问题":"现有方法在视觉特征归因中可能只检测到类别特定特征的一个子集","动机":"解决现有视觉特征归因方法的局限性，提高归因的准确性和全面性","方法":"开发了一种基于Wasserstein生成对抗网络（WGAN）的新特征归因技术","关键词":["视觉特征归因","Wasserstein GANs","神经影像分析","医学图像处理"],"涉及的技术概念":"Wasserstein生成对抗网络（WGAN）是一种改进的生成对抗网络，通过引入Wasserstein距离来提高生成图像的质量和训练的稳定性。视觉特征归因是指将输入图像的像素归因于某个类别，这在医学图像分析中尤为重要，因为它可以帮助理解疾病对大脑结构的影响。"},{"order":860,"title":"Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Joo_Total_Capture_A_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Joo_Total_Capture_A_CVPR_2018_paper.html","abstract":"We present a unified deformation model for the markerless capture of multiple scales of human movement, including facial expressions, body motion, and hand gestures. An initial model is generated by locally stitching together models of the individual parts of the human body, which we refer to as the \`\`Frankenstein'' model. This model enables the full expression of part movements, including face and hands by a single seamless model. Using a large-scale capture of people wearing everyday clothes, we optimize the Frankenstein model to create \`\`Adam\\". Adam is a model that shares the same skeleton hierarchy as the initial model, but can express hair and clothing geometry, making it directly usable for fitting people as they normally appear in everyday life. Finally, we demonstrate the use of these models for total motion tracking method, simultaneously capturing the large-scale body movements and the subtle face and hand motion of a social group of people.","中文标题":"全面捕捉：用于追踪面部、手部和身体的3D变形模型","摘要翻译":"我们提出了一种统一的变形模型，用于无标记捕捉包括面部表情、身体动作和手势在内的多种尺度的人类运动。初始模型通过局部缝合人体各部分的模型生成，我们称之为“弗兰肯斯坦”模型。该模型能够通过一个无缝模型完全表达部分运动，包括面部和手部。通过大规模捕捉穿着日常服装的人，我们优化了弗兰肯斯坦模型以创建“亚当”。亚当是一个与初始模型共享相同骨骼层次结构的模型，但能够表达头发和服装的几何形状，使其直接适用于拟合人们日常生活中的正常外观。最后，我们展示了这些模型在全面运动追踪方法中的应用，同时捕捉社交群体的大规模身体运动和微妙的面部及手部动作。","领域":"3D建模/运动捕捉/人机交互","问题":"如何无标记捕捉并统一表达人类面部表情、身体动作和手势的多种尺度运动","动机":"为了更自然、全面地捕捉和表达人类的日常动作，包括面部表情、身体动作和手势，以便于在虚拟现实、增强现实和动画制作等领域中应用","方法":"提出了一种统一的变形模型，通过局部缝合人体各部分的模型生成初始模型，并优化该模型以创建能够表达头发和服装几何形状的模型，最后应用于全面运动追踪方法","关键词":["3D建模","运动捕捉","人机交互","面部表情","手势识别"],"涉及的技术概念":"3D变形模型、无标记捕捉技术、弗兰肯斯坦模型、骨骼层次结构、运动追踪方法"},{"order":861,"title":"Augmented Skeleton Space Transfer for Depth-Based Hand Pose Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Baek_Augmented_Skeleton_Space_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Baek_Augmented_Skeleton_Space_CVPR_2018_paper.html","abstract":"Crucial to the success of training a depth-based 3D hand pose estimator (HPE) is the availability of comprehensive datasets covering diverse camera perspectives, shapes, and pose variations. However, collecting such annotated datasets is challenging. We propose to complete existing databases by generating new database entries. The key idea is to synthesize data in the skeleton space (instead of doing so in the depth-map space) which enables an easy and intuitive way of manipulating data entries. Since the skeleton entries generated in this way do not have the corresponding depth map entries, we exploit them by training a separate hand pose generator (HPG) which synthesizes the depth map from the skeleton entries. By training the HPG and HPE in a single unified optimization framework enforcing that 1) the HPE agrees with the paired depth and skeleton entries; and 2) the HPG-HPE combination satisfies the cyclic consistency (both the input and the output of HPG-HPE are skeletons) observed via the newly generated unpaired skeletons, our algorithm constructs a HPE which is robust to variations that go beyond the coverage of the existing database. Our training algorithm adopts the generative adversarial networks (GAN) training process. As a by-product, we obtain a hand pose discriminator (HPD) that is capable of picking out realistic hand poses. Our algorithm exploits this capability to refine the initial skeleton estimates in testing, further improving the accuracy. We test our algorithm on four challenging benchmark datasets (ICVL, MSRA, NYU and Big Hand 2.2M datasets) and demonstrate that our approach outperforms or is on par with state-of-the-art methods quantitatively and qualitatively.","中文标题":"基于深度的手姿态估计的增强骨架空间转移","摘要翻译":"成功训练基于深度的3D手姿态估计器（HPE）的关键在于拥有覆盖多样相机视角、形状和姿态变化的综合数据集。然而，收集这样的注释数据集是具有挑战性的。我们提出通过生成新的数据库条目来完善现有数据库。关键思想是在骨架空间中合成数据（而不是在深度图空间中这样做），这使得操作数据条目变得简单直观。由于以这种方式生成的骨架条目没有相应的深度图条目，我们通过训练一个单独的手姿态生成器（HPG）来利用它们，该生成器从骨架条目合成深度图。通过在单一的统一优化框架中训练HPG和HPE，强制1）HPE与配对的深度和骨架条目一致；2）HPG-HPE组合满足通过新生成的无配对骨架观察到的循环一致性（HPG-HPE的输入和输出都是骨架），我们的算法构建了一个对超出现有数据库覆盖范围的变异具有鲁棒性的HPE。我们的训练算法采用了生成对抗网络（GAN）训练过程。作为副产品，我们获得了一个能够挑选出真实手姿态的手姿态判别器（HPD）。我们的算法利用这一能力在测试中细化初始骨架估计，进一步提高准确性。我们在四个具有挑战性的基准数据集（ICVL、MSRA、NYU和Big Hand 2.2M数据集）上测试了我们的算法，并证明我们的方法在数量和质量上优于或与最先进的方法相当。","领域":"手姿态估计/数据合成/生成对抗网络","问题":"如何在没有足够注释数据的情况下训练一个鲁棒的3D手姿态估计器","动机":"收集覆盖多样相机视角、形状和姿态变化的综合注释数据集具有挑战性","方法":"在骨架空间中合成数据，并训练一个手姿态生成器（HPG）从骨架条目合成深度图，同时在单一的统一优化框架中训练HPG和HPE，强制HPE与配对的深度和骨架条目一致，并满足循环一致性","关键词":["手姿态估计","数据合成","生成对抗网络","骨架空间","深度图"],"涉及的技术概念":"3D手姿态估计器（HPE）、手姿态生成器（HPG）、生成对抗网络（GAN）、手姿态判别器（HPD）、骨架空间、深度图、循环一致性"},{"order":862,"title":"Synthesizing Images of Humans in Unseen Poses","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_Synthesizing_Images_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Balakrishnan_Synthesizing_Images_of_CVPR_2018_paper.html","abstract":"We address the computational problem of novel human pose synthesis. Given an image of a person and a desired pose, we produce a depiction of that person in that pose, retaining the appearance of both the person and background. We present a modular generative neural network that synthesizes unseen poses using training pairs of images and poses taken from human action videos. Our network separates a scene into different body part and background layers, moves body parts to new locations and refines their appearances, and composites the new foreground with a hole-filled background. These subtasks, implemented with separate modules, are trained jointly using only a single target image as a supervised label. We use an adversarial discriminator to force our network to synthesize realistic details conditioned on pose. We demonstrate image synthesis results on three action classes: golf, yoga/workouts and tennis, and show that our method produces accurate results within action classes as well as across action classes. Given a sequence of desired poses, we also produce coherent videos of actions.","中文标题":"合成未见姿势中的人类图像","摘要翻译":"我们解决了新颖人类姿势合成的计算问题。给定一个人的图像和一个期望的姿势，我们生成该人在该姿势下的描绘，保留人和背景的外观。我们提出了一个模块化的生成神经网络，该网络使用从人类动作视频中提取的图像和姿势训练对来合成未见过的姿势。我们的网络将场景分离为不同的身体部位和背景层，将身体部位移动到新位置并细化它们的外观，并将新的前景与填充孔洞的背景合成。这些子任务通过单独的模块实现，仅使用单个目标图像作为监督标签进行联合训练。我们使用对抗性判别器来强制我们的网络根据姿势合成逼真的细节。我们在三个动作类别上展示了图像合成结果：高尔夫、瑜伽/锻炼和网球，并展示了我们的方法在动作类别内以及跨动作类别中都能产生准确的结果。给定一系列期望的姿势，我们还生成了连贯的动作视频。","领域":"人体姿势合成/生成对抗网络/视频动作分析","问题":"如何从给定的人类图像和期望的姿势合成该人在该姿势下的逼真图像","动机":"为了在保留人和背景外观的同时，生成未见姿势下的人类图像，以应用于视频动作分析等领域","方法":"采用模块化的生成神经网络，通过分离场景为不同层、移动并细化身体部位、合成新前景与背景，并使用对抗性判别器来合成逼真细节","关键词":["人体姿势合成","生成对抗网络","视频动作分析"],"涉及的技术概念":"模块化生成神经网络、对抗性判别器、图像和姿势训练对、身体部位和背景层分离、动作类别（高尔夫、瑜伽/锻炼、网球）"},{"order":863,"title":"SSNet: Scale Selection Network for Online 3D Action Prediction","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_SSNet_Scale_Selection_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_SSNet_Scale_Selection_CVPR_2018_paper.html","abstract":"In action prediction (early action recognition), the goal is to predict the class label of an ongoing action using its observed part so far. In this paper, we focus on online action prediction in streaming 3D skeleton sequences. A dilated convolutional network is introduced to model the motion dynamics in temporal dimension via a sliding window over the time axis. As there are significant temporal scale variations of the observed part of the ongoing action at different progress levels, we propose a novel window scale selection scheme to make our network focus on the performed part of the ongoing action and try to suppress the noise from the previous actions at each time step. Furthermore, an activation sharing scheme is proposed to deal with the overlapping computations among the adjacent steps, which allows our model to run more efficiently. The extensive experiments on two challenging datasets show the effectiveness of the proposed action prediction framework.","中文标题":"SSNet: 用于在线3D动作预测的尺度选择网络","摘要翻译":"在动作预测（早期动作识别）中，目标是通过观察到的动作部分来预测正在进行动作的类别标签。本文中，我们专注于流式3D骨架序列中的在线动作预测。引入了一个扩张卷积网络，通过在时间轴上的滑动窗口来建模时间维度上的运动动态。由于在不同进度级别上，正在进行动作的观察部分存在显著的时间尺度变化，我们提出了一种新颖的窗口尺度选择方案，使我们的网络专注于正在进行动作的执行部分，并尝试在每个时间步骤中抑制来自先前动作的噪声。此外，提出了一种激活共享方案来处理相邻步骤之间的重叠计算，这使得我们的模型运行更加高效。在两个具有挑战性的数据集上的广泛实验显示了所提出的动作预测框架的有效性。","领域":"动作识别/3D骨架分析/时间序列分析","问题":"在线预测流式3D骨架序列中的动作类别","动机":"解决在不同进度级别上，正在进行动作的观察部分存在显著的时间尺度变化的问题，以及提高动作预测的效率和准确性","方法":"引入扩张卷积网络建模时间维度上的运动动态，提出窗口尺度选择方案专注于正在进行动作的执行部分，并提出激活共享方案处理相邻步骤之间的重叠计算","关键词":["动作预测","3D骨架序列","扩张卷积网络","窗口尺度选择","激活共享"],"涉及的技术概念":"扩张卷积网络用于捕捉时间维度上的运动动态，窗口尺度选择方案用于处理动作观察部分的时间尺度变化，激活共享方案用于优化模型的计算效率。"},{"order":864,"title":"Detecting and Recognizing Human-Object Interactions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.html","abstract":"To understand the visual world, a machine must not only recognize individual object instances but also how they interact. Humans are often at the center of such interactions and detecting human-object interactions is an important practical and scientific problem. In this paper, we address the task of detecting (human, verb, object) triplets in challenging everyday photos. We propose a novel model that is driven by a human-centric approach. Our hypothesis is that the appearance of a person -- their pose, clothing, action -- is a powerful cue for localizing the objects they are interacting with. To exploit this cue, our model learns to predict an action-specific density over target object locations based on the appearance of a detected person. Our model also jointly learns to detect people and objects, and by fusing these predictions it efficiently infers interaction triplets in a clean, jointly trained end-to-end system we call InteractNet. We validate our approach on the recently introduced Verbs in COCO (V-COCO) and HICO-DET datasets, where we show quantitatively compelling results.","中文标题":"检测和识别人与物体的交互","摘要翻译":"为了理解视觉世界，机器不仅需要识别单个物体实例，还需要识别它们如何交互。人类往往是这些交互的中心，检测人与物体的交互是一个重要的实际和科学问题。在本文中，我们解决了在具有挑战性的日常照片中检测（人、动词、物体）三元组的任务。我们提出了一种以人为中心的方法驱动的新模型。我们的假设是，一个人的外观——他们的姿势、服装、动作——是定位他们正在交互的物体的强大线索。为了利用这一线索，我们的模型学习基于检测到的人的外观预测目标物体位置的动作特定密度。我们的模型还联合学习检测人和物体，并通过融合这些预测，在我们称为InteractNet的干净、联合训练的端到端系统中有效地推断交互三元组。我们在最近引入的COCO动词（V-COCO）和HICO-DET数据集上验证了我们的方法，展示了数量上引人注目的结果。","领域":"视觉理解/人机交互/物体识别","问题":"在具有挑战性的日常照片中检测（人、动词、物体）三元组","动机":"理解视觉世界需要机器不仅识别单个物体实例，还需要识别它们如何交互，人类往往是这些交互的中心。","方法":"提出了一种以人为中心的方法驱动的新模型，该模型学习基于检测到的人的外观预测目标物体位置的动作特定密度，并联合学习检测人和物体，通过融合这些预测在InteractNet系统中推断交互三元组。","关键词":["视觉理解","人机交互","物体识别"],"涉及的技术概念":"InteractNet是一个干净、联合训练的端到端系统，用于检测和识别人与物体的交互。它通过分析人的外观（如姿势、服装、动作）来预测目标物体的位置，并联合检测人和物体，从而有效地推断出交互三元组。"},{"order":865,"title":"Unsupervised Learning and Segmentation of Complex Activities From Video","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sener_Unsupervised_Learning_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sener_Unsupervised_Learning_and_CVPR_2018_paper.html","abstract":"This paper presents a new method for unsupervised segmentation of complex activities from video into multiple steps, or sub-activities, without any textual input. We propose an iterative discriminative-generative approach which alternates between discriminatively learning the appearance of sub-activities from the videos' visual features to sub-activity labels and generatively modelling the temporal structure of sub-activities using a Generalized Mallows Model. In addition, we introduce a model for background to account for frames unrelated to the actual activities. Our approach is validated on the challenging Breakfast Actions and Inria Instructional Videos datasets and outperforms both unsupervised and weakly-supervised state of the art.","中文标题":"从视频中进行复杂活动的无监督学习与分割","摘要翻译":"本文提出了一种新的方法，用于从视频中无监督地将复杂活动分割成多个步骤或子活动，而无需任何文本输入。我们提出了一种迭代的判别-生成方法，该方法交替使用判别学习从视频的视觉特征到子活动标签的外观，以及使用广义Mallows模型生成子活动的时间结构模型。此外，我们引入了一个背景模型，以考虑与实际活动无关的帧。我们的方法在具有挑战性的早餐动作和Inria教学视频数据集上进行了验证，并且表现优于无监督和弱监督的最新技术。","领域":"活动识别/视频分析/时间序列分析","问题":"从视频中无监督地分割复杂活动为多个步骤或子活动","动机":"为了在没有文本输入的情况下，自动识别和分割视频中的复杂活动，提高活动识别的准确性和效率","方法":"提出了一种迭代的判别-生成方法，结合了从视频的视觉特征到子活动标签的判别学习和使用广义Mallows模型的子活动时间结构生成模型，以及引入背景模型处理与实际活动无关的帧","关键词":["无监督学习","活动分割","视频分析"],"涉及的技术概念":{"无监督学习":"一种机器学习方法，不需要预先标记的数据来训练模型","广义Mallows模型":"一种用于建模和预测序列数据的统计模型，特别适用于时间序列分析","判别-生成方法":"结合了判别学习和生成模型的方法，判别学习用于分类，生成模型用于数据生成"}},{"order":866,"title":"Unsupervised Training for 3D Morphable Model Regression","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Genova_Unsupervised_Training_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Genova_Unsupervised_Training_for_CVPR_2018_paper.html","abstract":"We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.","中文标题":"无监督训练用于3D可变形模型回归","摘要翻译":"我们提出了一种方法，用于训练从图像像素到3D可变形模型坐标的回归网络，仅使用未标记的照片。训练损失基于面部识别网络的特征，通过使用可微分渲染器渲染预测的面部来即时计算。为了使从特征训练成为可能并避免网络欺骗效应，我们引入了三个目标：一个批次分布损失，鼓励输出分布与可变形模型的分布相匹配；一个回环损失，确保网络能够正确重新解释其自己的输出；以及一个多视角身份损失，从多个视角比较预测的3D面部和输入照片的特征。我们使用这些目标、一组未标记的照片和可变形模型本身训练了一个回归网络，并展示了最先进的结果。","领域":"3D面部建模/无监督学习/可微分渲染","问题":"如何仅使用未标记的照片训练一个从图像像素到3D可变形模型坐标的回归网络","动机":"为了在没有标记数据的情况下，实现从2D图像到3D面部模型的准确回归，提高3D面部建模的效率和准确性","方法":"引入三个训练目标：批次分布损失、回环损失和多视角身份损失，结合未标记的照片和可变形模型本身训练回归网络","关键词":["3D面部建模","无监督学习","可微分渲染"],"涉及的技术概念":"可微分渲染器用于即时计算面部识别网络的特征，批次分布损失、回环损失和多视角身份损失用于训练回归网络，以提高3D面部建模的准确性和效率。"},{"order":867,"title":"Video Based Reconstruction of 3D People Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Alldieck_Video_Based_Reconstruction_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Alldieck_Video_Based_Reconstruction_CVPR_2018_paper.html","abstract":"This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people. Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. We present evaluation results for a number of test subjects and analyze overall performance. Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.","中文标题":"基于视频的3D人体模型重建","摘要翻译":"本文描述了如何从单一的单目视频中获取任意人的准确3D身体模型和纹理，视频中的人在移动。基于参数化身体模型，我们提出了一个鲁棒的处理流程，实现了对穿衣人的3D模型拟合，精度达到5毫米。我们的主要贡献是一种方法，用于非刚性变形与动态人体轮廓相对应的轮廓锥，从而在共同参考框架中生成视觉外壳，实现表面重建。这使得基于大量帧的高效估计共识3D形状、纹理和植入的动画骨架成为可能。我们展示了对多个测试对象的评估结果，并分析了整体性能。仅需智能手机或网络摄像头，我们的方法使每个人都能创建自己的完全可动画的数字双胞胎，例如，用于社交VR应用或在线时尚购物的虚拟试穿。","领域":"3D重建/人体建模/动画技术","问题":"如何从单一的单目视频中准确重建3D人体模型和纹理","动机":"为了在社交VR应用或在线时尚购物中实现虚拟试穿，需要一种能够从普通视频中高效重建3D人体模型的方法","方法":"基于参数化身体模型，通过非刚性变形动态人体轮廓对应的轮廓锥，在共同参考框架中生成视觉外壳，实现表面重建","关键词":["3D重建","人体建模","动画技术","虚拟试穿","社交VR"],"涉及的技术概念":"参数化身体模型、非刚性变形、轮廓锥、视觉外壳、表面重建、共识3D形状、纹理估计、动画骨架植入"},{"order":868,"title":"Pose-Guided Photorealistic Face Rotation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Pose-Guided_Photorealistic_Face_CVPR_2018_paper.html","abstract":"Face rotation provides an effective and cheap way for data augmentation and representation learning of face recognition. It is a challenging generative learning problem due to the large pose discrepancy between two face images. This work focuses on flexible face rotation of arbitrary head poses, including extreme profile views. We propose a novel Couple-Agent Pose-Guided Generative Adversarial Network (CAPG-GAN) to generate both neutral and profile head pose face images. The head pose information is encoded by facial landmark heatmaps. It not only forms a mask image to guide the generator in learning process but also provides a flexible controllable condition during inference. A couple-agent discriminator is introduced to reinforce on the realism of synthetic arbitrary view faces. Besides the generator and conditional adversarial loss, CAPG-GAN further employs identity preserving loss and total variation regularization to preserve identity information and refine local textures respectively. Quantitative and qualitative experimental results on the Multi-PIE and LFW databases consistently show the superiority of our face rotation method over the state-of-the-art.","中文标题":"姿态引导的真实感面部旋转","摘要翻译":"面部旋转为数据增强和面部识别的表示学习提供了一种有效且廉价的方式。由于两张面部图像之间姿态差异较大，这是一个具有挑战性的生成学习问题。本工作专注于任意头部姿态的灵活面部旋转，包括极端侧面视图。我们提出了一种新颖的耦合代理姿态引导生成对抗网络（CAPG-GAN）来生成中性和侧面头部姿态的面部图像。头部姿态信息通过面部标志热图编码。它不仅形成了一幅掩码图像以指导生成器的学习过程，而且在推理过程中提供了灵活的可控条件。引入了一个耦合代理鉴别器来增强合成任意视角面部的真实感。除了生成器和条件对抗损失外，CAPG-GAN还进一步采用了身份保持损失和总变差正则化，分别用于保持身份信息和细化局部纹理。在Multi-PIE和LFW数据库上的定量和定性实验结果一致显示了我们面部旋转方法相对于现有技术的优越性。","领域":"面部识别/生成对抗网络/数据增强","问题":"解决面部图像在极端姿态差异下的生成问题","动机":"为了提供一种有效且廉价的数据增强和面部识别表示学习方式","方法":"提出了一种耦合代理姿态引导生成对抗网络（CAPG-GAN），通过面部标志热图编码头部姿态信息，使用耦合代理鉴别器增强合成面部的真实感，并采用身份保持损失和总变差正则化来保持身份信息和细化局部纹理","关键词":["面部旋转","生成对抗网络","数据增强","面部识别","姿态引导"],"涉及的技术概念":"CAPG-GAN（耦合代理姿态引导生成对抗网络）是一种新颖的生成对抗网络，用于生成中性和侧面头部姿态的面部图像。它通过面部标志热图编码头部姿态信息，不仅指导生成器的学习过程，还在推理过程中提供灵活的可控条件。此外，CAPG-GAN引入了耦合代理鉴别器来增强合成任意视角面部的真实感，并采用身份保持损失和总变差正则化来保持身份信息和细化局部纹理。"},{"order":869,"title":"Mesoscopic Facial Geometry Inference Using Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huynh_Mesoscopic_Facial_Geometry_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huynh_Mesoscopic_Facial_Geometry_CVPR_2018_paper.html","abstract":"We present a learning-based approach for synthesizing facial geometry at medium and fine scales from diffusely-lit facial texture maps.  When applied to an image sequence, the synthesized detail is temporally coherent.  Unlike current state-of-the-art methods, which assume \\"dark is deep\\", our model is trained with measured facial detail collected using polarized gradient illumination in a Light Stage. This enables us to produce plausible facial detail across the entire face, including where previous approaches may incorrectly interpret dark features as concavities such as at moles, hair stubble, and occluded pores. Instead of directly inferring 3D geometry, we propose to encode fine details in high-resolution displacement maps which are learned through a hybrid network adopting the state-of-the-art image-to-image translation network and super resolution network. To effectively capture geometric detail at both mid- and high frequencies, we factorize the learning into two separate sub-networks, enabling the full range of facial detail to be modeled.  Results from our learning-based approach compare favorably with a high-quality active facial scanning technique, and require only a single passive lighting condition without a complex scanning setup.","中文标题":"使用深度神经网络进行中观面部几何推断","摘要翻译":"我们提出了一种基于学习的方法，用于从漫射光照下的面部纹理图中合成中等到精细尺度的面部几何。当应用于图像序列时，合成的细节在时间上是连贯的。与当前最先进的方法不同，这些方法假设“暗即深”，我们的模型是在使用偏振梯度照明在光阶段收集的测量面部细节上进行训练的。这使我们能够在整个面部产生可信的面部细节，包括在以前的方法可能错误地将暗特征解释为凹陷的地方，如痣、胡茬和闭塞的毛孔。我们不是直接推断3D几何，而是提出在高分辨率位移图中编码精细细节，这些位移图通过采用最先进的图像到图像翻译网络和超分辨率网络的混合网络学习。为了有效捕捉中频和高频的几何细节，我们将学习分解为两个独立的子网络，从而能够建模全范围的面部细节。我们基于学习的方法的结果与高质量的活动面部扫描技术相比具有优势，并且只需要单一的被动光照条件，无需复杂的扫描设置。","领域":"面部几何合成/图像处理/深度学习","问题":"如何从漫射光照下的面部纹理图中合成中等到精细尺度的面部几何","动机":"解决现有方法在解释面部暗特征为凹陷时的错误，如痣、胡茬和闭塞的毛孔","方法":"采用混合网络，结合图像到图像翻译网络和超分辨率网络，学习高分辨率位移图中的精细细节，并将学习过程分解为两个独立的子网络以捕捉不同频率的几何细节","关键词":["面部几何合成","图像到图像翻译","超分辨率"],"涉及的技术概念":"深度神经网络、图像到图像翻译网络、超分辨率网络、高分辨率位移图、偏振梯度照明、光阶段"},{"order":870,"title":"Hand PointNet: 3D Hand Pose Estimation Using Point Sets","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ge_Hand_PointNet_3D_CVPR_2018_paper.html","abstract":"Convolutional Neural Network (CNN) has shown promising results for 3D hand pose estimation in depth images. Different from existing CNN-based hand pose estimation methods that take either 2D images or 3D volumes as the input, our proposed Hand PointNet directly processes the 3D point cloud that models the visible surface of the hand for pose regression. Taking the normalized point cloud as the input, our proposed hand pose regression network is able to capture complex hand structures and accurately regress a low dimensional representation of the 3D hand pose. In order to further improve the accuracy of fingertips, we design a fingertip refinement network that directly takes the neighboring points of the estimated fingertip location as input to refine the fingertip location. Experiments on three challenging hand pose datasets show that our proposed method outperforms state-of-the-art methods.","中文标题":"Hand PointNet: 使用点集进行3D手部姿态估计","摘要翻译":"卷积神经网络（CNN）在深度图像中的3D手部姿态估计方面显示出有希望的结果。与现有的基于CNN的手部姿态估计方法不同，这些方法要么以2D图像要么以3D体积作为输入，我们提出的Hand PointNet直接处理建模手部可见表面的3D点云以进行姿态回归。以归一化的点云作为输入，我们提出的手部姿态回归网络能够捕捉复杂的手部结构并准确回归3D手部姿态的低维表示。为了进一步提高指尖的准确性，我们设计了一个指尖细化网络，该网络直接以估计的指尖位置的邻近点作为输入来细化指尖位置。在三个具有挑战性的手部姿态数据集上的实验表明，我们提出的方法优于最先进的方法。","领域":"3D姿态估计/点云处理/手部姿态识别","问题":"3D手部姿态估计","动机":"提高3D手部姿态估计的准确性，特别是指尖位置的准确性","方法":"提出Hand PointNet直接处理3D点云进行姿态回归，并设计指尖细化网络以提高指尖位置的准确性","关键词":["3D姿态估计","点云处理","手部姿态识别","指尖细化"],"涉及的技术概念":"卷积神经网络（CNN）用于3D手部姿态估计，Hand PointNet直接处理3D点云进行姿态回归，指尖细化网络用于提高指尖位置的准确性。"},{"order":871,"title":"Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Nagrani_Seeing_Voices_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Nagrani_Seeing_Voices_and_CVPR_2018_paper.html","abstract":"We introduce a seemingly impossible task: given only an audio clip of someone speaking, decide which of two face images is the speaker. In this paper we study this, and a number of related cross-modal tasks, aimed at answering the question: how much can we infer from the voice about the face and vice versa? We study this task “in the wild”, employing the datasets that are now publicly available for face recognition from static images (VGGFace) and speaker identification from audio (VoxCeleb). These provide training and testing scenarios for both static and dynamic testing of cross-modal matching. We make the following contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching; (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available); and (iii) we use hu- man testing as a baseline to calibrate the difficulty of the task. We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice. The CNN matches human performance on easy examples (e.g. different gender across faces) but exceeds human performance on more challenging examples (e.g. faces with the same gender, age and nationality).","中文标题":"看见声音与听见面孔：跨模态生物特征匹配","摘要翻译":"我们引入了一个看似不可能的任务：仅给定某人说话的音频片段，决定两张面部图像中哪一个是说话者。在本文中，我们研究了这一任务以及一系列相关的跨模态任务，旨在回答一个问题：我们能从声音中推断出多少关于面孔的信息，反之亦然？我们在“野外”研究这一任务，利用现在公开可用的数据集进行静态图像的面部识别（VGGFace）和音频的说话者识别（VoxCeleb）。这些数据集为跨模态匹配的静态和动态测试提供了训练和测试场景。我们做出了以下贡献：（i）我们引入了用于二元和多路跨模态面部和音频匹配的CNN架构；（ii）我们比较了动态测试（其中视频信息可用，但音频并非来自同一视频）与静态测试（其中仅有一张静止图像可用）；（iii）我们使用人类测试作为基线来校准任务的难度。我们展示了CNN确实可以被训练来解决这一任务，在静态和动态场景中，甚至在给定声音的情况下对面部进行10路分类时，其表现远高于随机。CNN在简单示例（例如，不同性别的面部）上匹配人类表现，但在更具挑战性的示例（例如，相同性别、年龄和国籍的面部）上超过人类表现。","领域":"生物特征识别/跨模态学习/音频-视觉匹配","问题":"如何从音频片段中识别出对应的面部图像，以及从面部图像中推断出对应的音频特征","动机":"探索声音与面部之间的跨模态关系，以及这种关系在生物特征识别中的应用","方法":"引入CNN架构进行跨模态匹配，比较动态与静态测试场景，使用人类测试作为基线","关键词":["跨模态匹配","CNN架构","生物特征识别"],"涉及的技术概念":"CNN（卷积神经网络）用于跨模态匹配，VGGFace和VoxCeleb数据集用于面部和音频识别，动态与静态测试场景的比较，以及人类测试作为任务难度的基线。"},{"order":872,"title":"Learning Monocular 3D Human Pose Estimation From Multi-View Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rhodin_Learning_Monocular_3D_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rhodin_Learning_Monocular_3D_CVPR_2018_paper.html","abstract":"Accurate 3D human pose estimation from single images is possible with sophisticated deep-net architectures that have been trained on very large datasets. However, this still leaves open the problem of capturing motions for which no such database exists.  Manual annotation is tedious, slow, and error-prone. In this paper, we propose to replace most of the annotations by the use of multiple views, at training time only. Specifically, we train the system to predict the same pose in all views. Such a consistency constraint is necessary but not sufficient to predict accurate poses. We therefore complement it with a supervised loss aiming to predict the correct pose in a small set of labeled images, and with a regularization term that penalizes drift from initial predictions. Furthermore, we propose a method to estimate camera pose jointly with human pose, which lets us utilize multi-view footage where calibration is difficult, e.g., for pan-tilt or moving handheld cameras. We demonstrate the effectiveness of our approach on established benchmarks, as well as on a new Ski dataset with rotating cameras and expert ski motion, for which annotations are truly hard to obtain.","中文标题":"从多视角图像学习单目3D人体姿态估计","摘要翻译":"通过使用在非常大的数据集上训练的复杂深度网络架构，从单张图像进行准确的3D人体姿态估计是可能的。然而，这仍然留下了捕捉没有此类数据库存在的动作的问题。手动注释既繁琐又缓慢，且容易出错。在本文中，我们提出在训练时仅通过使用多视角来替换大部分注释。具体来说，我们训练系统在所有视角中预测相同的姿态。这种一致性约束是必要的，但不足以预测准确的姿态。因此，我们通过一个旨在在一小组标记图像中预测正确姿态的监督损失和一个惩罚从初始预测漂移的正则化项来补充它。此外，我们提出了一种方法，可以同时估计相机姿态和人体姿态，这使我们能够利用校准困难的多视角镜头，例如，对于俯仰或移动手持相机。我们在已建立的基准上以及在一个新的滑雪数据集上展示了我们方法的有效性，该数据集包含旋转相机和专家滑雪动作，对于这些动作，注释确实难以获得。","领域":"3D人体姿态估计/多视角学习/相机姿态估计","问题":"从单张图像进行准确的3D人体姿态估计，尤其是在没有大规模数据库支持的情况下捕捉特定动作","动机":"解决在没有大规模数据库支持的情况下捕捉特定动作的挑战，减少对繁琐、缓慢且容易出错的手动注释的依赖","方法":"在训练时使用多视角图像替换大部分注释，通过一致性约束、监督损失和正则化项来预测准确的姿态，并提出一种同时估计相机姿态和人体姿态的方法","关键词":["3D人体姿态估计","多视角学习","相机姿态估计","一致性约束","监督损失","正则化项"],"涉及的技术概念":"深度网络架构用于3D人体姿态估计，多视角图像用于训练，一致性约束确保所有视角中预测的姿态一致，监督损失用于在标记图像中预测正确姿态，正则化项防止预测漂移，同时估计相机姿态和人体姿态的方法"},{"order":873,"title":"Separating Style and Content for Generalized Style Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Separating_Style_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Separating_Style_and_CVPR_2018_paper.html","abstract":"Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special \`multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.","中文标题":"分离风格与内容以实现广义风格迁移","摘要翻译":"近年来，神经风格迁移引起了广泛关注。然而，大多数现有方法旨在明确建模不同风格之间的转换，因此学习到的模型无法推广到新风格。我们在此尝试分离风格和内容的表示，并提出一个由风格编码器、内容编码器、混合器和解码器组成的广义风格迁移网络。风格编码器和内容编码器分别用于从风格参考图像和内容参考图像中提取风格和内容因素。混合器采用双线性模型整合上述两个因素，并最终将其输入解码器以生成具有目标风格和内容的图像。为了分离风格特征和内容特征，我们利用给定图像中风格和内容的条件依赖性。在训练过程中，编码器网络学习从两组有限大小的参考图像中提取风格和内容，一组具有共享风格，另一组具有共享内容。这种学习框架允许多种风格之间的同时风格迁移，并可被视为一种特殊的“多任务”学习场景。编码器预计能够捕捉不同风格和内容的基础特征，这些特征可以推广到新的风格和内容。为了验证，我们将所提出的算法应用于中文字体迁移问题。在字符生成上的大量实验结果证明了我们方法的有效性和鲁棒性。","领域":"风格迁移/图像生成/字体设计","问题":"现有风格迁移方法无法推广到新风格的问题","动机":"实现能够推广到新风格的广义风格迁移","方法":"提出一个由风格编码器、内容编码器、混合器和解码器组成的广义风格迁移网络，利用条件依赖性分离风格和内容特征","关键词":["风格迁移","图像生成","字体设计"],"涉及的技术概念":"神经风格迁移、风格编码器、内容编码器、混合器、解码器、双线性模型、条件依赖性、多任务学习、中文字体迁移"},{"order":874,"title":"TextureGAN: Controlling Deep Image Synthesis With Texture Patches","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xian_TextureGAN_Controlling_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xian_TextureGAN_Controlling_Deep_CVPR_2018_paper.html","abstract":"In this paper, we investigate deep image synthesis guided by sketch, color, and texture. Previous image synthesis methods can be controlled by sketch and color strokes but we are the first to examine texture control. We allow a user to place a texture patch on a sketch at arbitrary locations and scales to control the desired output texture.  Our generative network learns to synthesize objects consistent with these texture suggestions. To achieve this, we develop a local texture loss in addition to adversarial and content loss to train the generative network. We conduct experiments using sketches generated from real images and textures sampled from a separate texture database and results show that our proposed algorithm is able to generate plausible images that are faithful to user controls. Ablation studies show that our proposed pipeline can generate more realistic images than adapting existing methods directly.","中文标题":"TextureGAN: 使用纹理补丁控制深度图像合成","摘要翻译":"在本文中，我们研究了由草图、颜色和纹理引导的深度图像合成。以前的图像合成方法可以通过草图和颜色笔触进行控制，但我们是第一个研究纹理控制的。我们允许用户在草图的任意位置和尺度上放置纹理补丁，以控制所需的输出纹理。我们的生成网络学会合成与这些纹理建议一致的对象。为了实现这一点，我们开发了一种局部纹理损失，除了对抗性和内容损失外，还用于训练生成网络。我们使用从真实图像生成的草图和从单独纹理数据库中采样的纹理进行实验，结果表明，我们提出的算法能够生成符合用户控制的合理图像。消融研究表明，我们提出的管道比直接适应现有方法能生成更真实的图像。","领域":"图像合成/纹理控制/生成对抗网络","问题":"如何在深度图像合成中实现纹理控制","动机":"探索在深度图像合成中引入纹理控制的可能性，以增强用户对合成图像的控制能力","方法":"开发了一种局部纹理损失，结合对抗性和内容损失来训练生成网络，允许用户在草图上任意位置和尺度放置纹理补丁","关键词":["图像合成","纹理控制","生成对抗网络"],"涉及的技术概念":"局部纹理损失、对抗性损失、内容损失、生成网络、纹理补丁"},{"order":875,"title":"Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Orekondy_Connecting_Pixels_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Orekondy_Connecting_Pixels_to_CVPR_2018_paper.html","abstract":"Images convey a broad spectrum of personal information.  If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual.  By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved.  Our findings argue for a \\"redaction by segmentation\\" paradigm.   Hence, we propose the first sizable dataset of private images \\"in the wild\\" annotated with pixel and instance level labels across a broad range of privacy classes.  We present the first model for automatic redaction of diverse private information.  It is effective at achieving various privacy-utility trade-offs within 83% of the performance of redactions based on ground-truth annotation.","中文标题":"连接像素到隐私与效用：图像中私人信息的自动编辑","摘要翻译":"图像传达了广泛的个人信息。如果这些图像在社交媒体平台上共享，这些个人信息就会被泄露，这与被描绘者的隐私相冲突。因此，我们旨在开发自动化方法来编辑此类私人信息，从而保护个人隐私。通过进行用户研究，我们发现模糊与私人信息相关的图像区域可以在保留图像效用的同时保护隐私。此外，通过改变区域的大小，可以实现不同的隐私-效用权衡。我们的研究结果支持“通过分割进行编辑”的范式。因此，我们提出了第一个大规模的真实世界私人图像数据集，该数据集在广泛的隐私类别中标注了像素和实例级别的标签。我们提出了第一个用于自动编辑多样化私人信息的模型。它在实现各种隐私-效用权衡方面有效，达到了基于真实标注编辑性能的83%。","领域":"隐私保护/图像编辑/社交媒体","问题":"自动编辑图像中的私人信息以保护个人隐私","动机":"社交媒体上共享的图像可能泄露个人信息，侵犯隐私","方法":"通过用户研究确定模糊私人信息相关区域的方法，提出“通过分割进行编辑”的范式，并开发自动编辑模型","关键词":["隐私保护","图像编辑","社交媒体"],"涉及的技术概念":"图像分割、隐私-效用权衡、自动编辑模型、像素和实例级别标签"},{"order":876,"title":"MapNet: An Allocentric Spatial Memory for Mapping Environments","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Henriques_MapNet_An_Allocentric_CVPR_2018_paper.html","abstract":"Autonomous agents need to reason about the world beyond their instantaneous sensory input. Integrating information over time, however, requires switching from an egocentric representation of a scene to an allocentric one, expressed in the world reference frame. It must also be possible to update the representation dynamically, which requires localizing and registering the sensor with respect to it. In this paper, we develop a differentiable module that satisfies such requirements, while being robust, efficient, and suitable for integration in end-to-end deep networks. The module contains an allocentric spatial memory that can be accessed associatively by feeding to it the current sensory input, resulting in localization, and then updated using an LSTM or similar mechanism. We formulate efficient localization and registration of sensory information as a dual pair of convolution/deconvolution operators in memory space. The map itself is a 2.5D representation of an environment storing information that a deep neural network module learns to distill from RGBD input. The result is a map that contains multi-task information, different from classical approaches to mapping such as structure-from-motion. We present results using synthetic mazes, a dataset of hours of recorded gameplay of the classic game Doom, and the very recent Active Vision Dataset of real images captured from a robot.","中文标题":"MapNet: 用于环境映射的全局空间记忆","摘要翻译":"自主代理需要推理超越其即时感官输入的世界。然而，随着时间的推移整合信息需要从场景的自我中心表示切换到以世界参考框架表达的全局中心表示。还必须能够动态更新表示，这需要相对于表示定位和注册传感器。在本文中，我们开发了一个满足这些要求的可微分模块，同时具有鲁棒性、高效性，并适合集成到端到端深度网络中。该模块包含一个全局空间记忆，可以通过向其提供当前感官输入来关联访问，从而实现定位，然后使用LSTM或类似机制进行更新。我们将感官信息的有效定位和注册制定为记忆空间中的卷积/反卷积操作符对。地图本身是环境的2.5D表示，存储了深度神经网络模块从RGBD输入中学习提取的信息。结果是一个包含多任务信息的地图，不同于传统的映射方法，如从运动中恢复结构。我们使用合成迷宫、经典游戏Doom数小时记录的游戏数据集以及最近从机器人捕获的真实图像的主动视觉数据集展示了结果。","领域":"自主导航/空间记忆/深度学习","问题":"如何在自主代理中实现从自我中心到全局中心的场景表示转换，并动态更新这种表示","动机":"自主代理需要超越即时感官输入，整合时间信息以推理世界，这需要从自我中心表示切换到全局中心表示，并能够动态更新这种表示","方法":"开发了一个可微分模块，包含一个全局空间记忆，可以通过当前感官输入关联访问，使用LSTM或类似机制更新，将感官信息的定位和注册制定为记忆空间中的卷积/反卷积操作符对","关键词":["自主导航","空间记忆","深度学习","LSTM","卷积/反卷积"],"涉及的技术概念":{"全局空间记忆":"一种以世界参考框架表达的空间记忆，允许自主代理从自我中心表示切换到全局中心表示","LSTM":"长短期记忆网络，一种特殊的递归神经网络，能够学习长期依赖信息","卷积/反卷积操作符":"在记忆空间中用于有效定位和注册感官信息的操作符对","2.5D表示":"一种介于2D和3D之间的环境表示方法，能够存储从RGBD输入中提取的信息"}},{"order":877,"title":"Accurate and Diverse Sampling of Sequences Based on a “Best of Many” Sample Objective","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bhattacharyya_Accurate_and_Diverse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bhattacharyya_Accurate_and_Diverse_CVPR_2018_paper.html","abstract":"For autonomous agents to successfully operate in the real world, anticipation of future events and states of their environment is a key competence. This problem has been formalized as a sequence extrapolation problem, where a number of observations are used to predict the sequence into the future. Real-world scenarios demand a model of uncertainty of such predictions, as predictions become increasingly uncertain -- in particular on long time horizons. While impressive results have been shown on point estimates, scenarios that induce multi-modal distributions over future sequences remain challenging. Our work addresses these challenges in a Gaussian Latent Variable model for sequence prediction. Our core contribution is a \`\`Best of Many'' sample objective that leads to more accurate and more diverse predictions that better capture the true variations in real-world sequence data. Beyond our analysis of improved model fit, our models also empirically outperform prior work on three diverse tasks ranging from traffic scenes to weather data.","中文标题":"基于“最佳多项”样本目标的序列准确与多样采样","摘要翻译":"为了使自主代理能够在现实世界中成功操作，预测未来事件及其环境状态是一项关键能力。这一问题已被形式化为序列外推问题，其中使用若干观测值来预测未来的序列。现实世界的场景要求对这类预测的不确定性进行建模，因为预测变得越来越不确定——尤其是在长时间跨度上。虽然在点估计上已经展示了令人印象深刻的结果，但诱导未来序列多模态分布的场景仍然具有挑战性。我们的工作在用于序列预测的高斯潜在变量模型中解决了这些挑战。我们的核心贡献是一个“最佳多项”样本目标，它导致了更准确和更多样化的预测，更好地捕捉了现实世界序列数据中的真实变化。除了我们对改进模型拟合的分析外，我们的模型在从交通场景到天气数据的三个不同任务上也经验性地优于先前的工作。","领域":"序列预测/不确定性建模/多模态分布","问题":"解决在序列预测中准确捕捉未来事件和环境状态的不确定性，特别是在长时间跨度上和多模态分布场景中的挑战。","动机":"现实世界的场景要求对预测的不确定性进行建模，因为预测在长时间跨度上变得越来越不确定，尤其是在多模态分布的场景中。","方法":"采用高斯潜在变量模型进行序列预测，并引入“最佳多项”样本目标以提高预测的准确性和多样性。","关键词":["序列预测","不确定性建模","多模态分布","高斯潜在变量模型","最佳多项样本目标"],"涉及的技术概念":"序列外推问题、高斯潜在变量模型、多模态分布、不确定性建模、最佳多项样本目标"},{"order":878,"title":"VirtualHome: Simulating Household Activities via Programs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html","abstract":"In this paper, we are interested in modeling complex activities that occur in a typical household. We propose to use programs, i.e., sequences of atomic actions and interactions, as a high level representation of complex tasks. Programs are interesting because they provide a non-ambiguous representation of a task, and allow agents to execute them. However, nowadays, there is no database providing this type of information. Towards this goal, we first crowd-source programs for a variety of activities that happen in people's homes, via a game-like interface used for teaching kids how to code. Using the collected dataset, we show how we can learn to extract programs directly from natural language descriptions or from videos.  We then implement the most common atomic (inter)actions in the Unity3D game engine, and use our programs to \\"drive'' an artificial agent to execute tasks in a simulated household environment. Our VirtualHome simulator allows us to create a large activity video dataset with rich ground-truth, enabling training and testing of video understanding models. We further showcase examples of our agent performing tasks in our VirtualHome based on language","中文标题":"VirtualHome：通过程序模拟家庭活动","摘要翻译":"在本文中，我们感兴趣的是模拟典型家庭中发生的复杂活动。我们提出使用程序，即原子动作和交互的序列，作为复杂任务的高级表示。程序之所以有趣，是因为它们提供了任务的无歧义表示，并允许代理执行它们。然而，目前没有提供此类信息的数据库。为了实现这一目标，我们首先通过一个类似游戏的界面众包了人们家中发生的各种活动的程序，该界面用于教孩子们如何编码。使用收集到的数据集，我们展示了如何直接从自然语言描述或视频中学习提取程序。然后，我们在Unity3D游戏引擎中实现了最常见的原子（交互）动作，并使用我们的程序来“驱动”一个人工代理在模拟的家庭环境中执行任务。我们的VirtualHome模拟器使我们能够创建一个具有丰富真实信息的大型活动视频数据集，从而支持视频理解模型的训练和测试。我们进一步展示了我们的代理在基于语言的VirtualHome中执行任务的示例。","领域":"家庭活动模拟/程序化任务表示/视频理解","问题":"模拟和表示家庭中的复杂活动","动机":"缺乏提供家庭活动程序化表示的数据集，以及需要一种方法来训练和测试视频理解模型","方法":"通过众包收集家庭活动的程序化表示，使用Unity3D游戏引擎实现原子动作，并利用这些程序驱动人工代理在模拟环境中执行任务","关键词":["家庭活动模拟","程序化任务表示","视频理解","Unity3D","人工代理"],"涉及的技术概念":"程序化任务表示指的是使用一系列原子动作和交互来无歧义地描述复杂任务。Unity3D是一个广泛使用的游戏开发引擎，这里用于实现和模拟家庭环境中的原子动作。人工代理是指能够执行特定任务的软件实体，这里用于在模拟环境中执行家庭活动。"},{"order":879,"title":"Generate to Adapt: Aligning Domains Using Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sankaranarayanan_Generate_to_Adapt_CVPR_2018_paper.html","abstract":"Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.","中文标题":"生成以适应：使用生成对抗网络对齐领域","摘要翻译":"领域适应是计算机视觉中一个被积极研究的问题。在这项工作中，我们提出了一种方法，利用无监督数据使源和目标分布在学习的联合特征空间中更接近。我们通过在学习的嵌入和生成对抗网络之间诱导共生关系来实现这一点。这与使用对抗框架进行真实数据生成并用此类数据重新训练深度模型的方法形成对比。我们通过在三个不同难度的任务上进行实验来展示我们方法的力量和通用性：（1）数字分类（MNIST、SVHN和USPS数据集）（2）使用OFFICE数据集进行对象识别和（3）从合成数据到真实数据的领域适应。我们的方法在大多数实验设置中实现了最先进的性能，并且是迄今为止唯一一个在不同数据集（如OFFICE和DIGITS）上表现良好的基于GAN的方法。","领域":"领域适应/生成对抗网络/特征学习","问题":"如何有效地将源领域和目标领域的分布对齐，以改善领域适应问题","动机":"解决领域适应问题，特别是在无监督环境下，通过生成对抗网络和特征学习的结合，提高模型在不同数据集上的适应性和性能","方法":"提出了一种方法，通过在学习的嵌入和生成对抗网络之间诱导共生关系，利用无监督数据使源和目标分布在学习的联合特征空间中更接近","关键词":["领域适应","生成对抗网络","特征学习","无监督学习","数字分类","对象识别"],"涉及的技术概念":"领域适应是指在机器学习中，将在一个领域（源领域）上训练的模型适应到另一个不同但相关的领域（目标领域）上的过程。生成对抗网络（GAN）是一种深度学习模型，由两部分组成：生成器和判别器，它们通过对抗过程共同学习。特征学习是指从原始数据中自动发现有用的特征表示的过程。无监督学习是一种机器学习方法，它不使用标签数据来训练模型。数字分类和对象识别是计算机视觉中的两个常见任务，分别涉及识别图像中的数字和对象。"},{"order":880,"title":"Multi-Agent Diverse Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.html","abstract":"We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task.","中文标题":"多代理多样化生成对抗网络","摘要翻译":"我们提出了MAD-GAN，这是对生成对抗网络（GANs）及其条件变体的直观推广，以解决众所周知的模式崩溃问题。首先，MAD-GAN是一个包含多个生成器和一个判别器的多代理GAN架构。其次，为了确保不同的生成器捕捉到不同的高概率模式，MAD-GAN的判别器被设计为除了区分真实和伪造样本外，还需要识别生成给定伪造样本的生成器。直观地说，为了成功完成这项任务，判别器必须学会将不同的生成器推向不同的可识别模式。我们在合成和真实数据集上进行了广泛的实验，并将MAD-GAN与不同变体的GAN进行了比较。我们展示了在图像到图像翻译和面部生成等挑战性任务中高质量多样化样本的生成。此外，我们还展示了MAD-GAN在使用高度挑战性的多样化类别数据集（例如包含森林、冰山和卧室图像的数据集）训练时能够解缠不同的模态。最后，我们展示了其在无监督特征表示任务中的有效性。","领域":"生成对抗网络/图像生成/特征表示","问题":"解决生成对抗网络中的模式崩溃问题","动机":"为了提高生成对抗网络在生成多样化样本方面的能力，特别是在图像到图像翻译和面部生成等任务中","方法":"提出了一种多代理生成对抗网络架构，通过设计一个能够识别生成伪造样本的生成器的判别器，来推动不同的生成器捕捉不同的高概率模式","关键词":["生成对抗网络","模式崩溃","多样化样本生成","图像到图像翻译","面部生成","无监督特征表示"],"涉及的技术概念":"MAD-GAN是一种多代理生成对抗网络架构，它通过引入多个生成器和一个判别器来解决模式崩溃问题。判别器不仅需要区分真实和伪造样本，还需要识别生成伪造样本的生成器，从而推动不同的生成器捕捉不同的高概率模式。这种方法在图像到图像翻译、面部生成以及无监督特征表示等任务中展示了其有效性。"},{"order":881,"title":"A PID Controller Approach for Stochastic Optimization of Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/An_A_PID_Controller_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/An_A_PID_Controller_CVPR_2018_paper.html","abstract":"Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet.","中文标题":"深度网络随机优化的PID控制器方法","摘要翻译":"深度神经网络在许多计算机视觉应用中展示了其强大的能力。最先进的深度架构，如VGG、ResNet和DenseNet，大多通过SGD-Momentum算法进行优化，该算法通过考虑权重过去和当前的梯度来更新权重。然而，SGD-Momentum存在超调问题，这阻碍了网络训练的收敛。受比例-积分-微分（PID）控制器在自动控制中显著成功的启发，我们提出了一种PID方法来加速深度网络优化。我们首先揭示了SGD-Momentum与基于PID的控制器之间的内在联系，然后提出了利用过去、当前和梯度变化来更新网络参数的优化算法。所提出的PID方法大大减少了SGD-Momentum的超调现象，并在流行的深度网络架构上实现了高达50%的加速，同时保持了竞争性的准确性，这一点在我们对包括CIFAR10、CIFAR100和Tiny-ImageNet在内的基准数据集的实验中得到了验证。","领域":"神经网络优化/自动控制/深度学习","问题":"SGD-Momentum算法在深度网络训练中的超调问题","动机":"受PID控制器在自动控制中的成功启发，旨在加速深度网络优化并减少超调现象","方法":"提出了一种基于PID控制器的优化算法，利用过去、当前和梯度变化来更新网络参数","关键词":["PID控制器","SGD-Momentum","超调问题","网络优化"],"涉及的技术概念":{"SGD-Momentum":"一种优化算法，通过考虑权重过去和当前的梯度来更新权重","PID控制器":"一种在自动控制中广泛使用的控制器，通过比例、积分和微分三个部分来调整系统输出","超调问题":"在优化过程中，参数更新幅度过大，导致系统响应超过目标值，影响收敛速度"}},{"order":882,"title":"“Learning-Compression” Algorithms for Neural Net Pruning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Carreira-Perpinan_Learning-Compression_Algorithms_for_CVPR_2018_paper.html","abstract":"Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates \\"learning\\" steps that optimize a regularized, data-dependent loss and \\"compression\\" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in nets of various sizes.","中文标题":"神经网络剪枝的“学习-压缩”算法","摘要翻译":"神经网络剪枝包括在不降低其性能的情况下移除权重。这是一个因需要压缩越来越大的网络以便它们能在移动设备上运行而重新引起关注的旧问题。传统上，剪枝是通过根据某些标准（如大小）对权重进行排名或惩罚，移除排名较低的权重并重新训练剩余的权重来完成的。我们将剪枝表述为一个优化问题，即找到在满足剪枝成本条件下最小化损失的权重。我们提供了一个通用算法来解决这个问题，该算法交替进行“学习”步骤（优化一个正则化的、数据依赖的损失）和“压缩”步骤（以数据独立的方式标记要剪枝的权重）。在压缩步骤中自然会出现幅度阈值，但与现有的幅度剪枝方法不同，我们的算法探索权重的子集，而不是从一开始就不可撤销地承诺一个特定的子集。它还能够自动学习网络中每一层要剪枝的最佳权重数量，而不会产生指数级成本的模型选择。使用单个剪枝级别的用户参数，我们在各种大小的网络中实现了最先进的剪枝。","领域":"神经网络剪枝/模型压缩/优化算法","问题":"如何在移除神经网络中的权重时不降低其性能","动机":"压缩越来越大的神经网络以便它们能在移动设备上运行","方法":"将剪枝表述为一个优化问题，交替进行“学习”步骤和“压缩”步骤，探索权重的子集并自动学习每一层要剪枝的最佳权重数量","关键词":["神经网络剪枝","模型压缩","优化算法"],"涉及的技术概念":"剪枝成本条件、正则化的数据依赖损失、数据独立的剪枝标记、幅度阈值、模型选择"},{"order":883,"title":"Large-Scale Distance Metric Learning With Uncertainty","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qian_Large-Scale_Distance_Metric_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qian_Large-Scale_Distance_Metric_CVPR_2018_paper.html","abstract":"Distance metric learning (DML) has been studied extensively in the past decades for its superior performance with distance-based algorithms. Most of the existing methods propose to learn a distance metric with pairwise or triplet constraints. However, the number of constraints is quadratic or even cubic in the number of the original examples, which makes it challenging for DML to handle the large-scale data set. Besides, the real-world data may contain various uncertainty, especially for the image data. The uncertainty can mislead the learning procedure and cause the performance degradation. By investigating the image data, we find that the original data can be observed from a small set of clean latent examples with different distortions. In this work, we propose the margin preserving metric learning framework to learn the distance metric and latent examples simultaneously. By leveraging the ideal properties of latent examples, the training efficiency can be improved significantly while the learned metric also becomes robust to the uncertainty in the original data. Furthermore, we can show that the metric is learned from latent examples only, but it can preserve the large margin property even for the original data. The empirical study on the benchmark image data sets demonstrates the efficacy and efficiency of the proposed method.","中文标题":"大规模不确定性距离度量学习","摘要翻译":"距离度量学习（DML）在过去几十年中因其在基于距离的算法中的卓越性能而被广泛研究。大多数现有方法提出通过成对或三重约束来学习距离度量。然而，约束的数量是原始样本数量的二次方甚至三次方，这使得DML处理大规模数据集具有挑战性。此外，现实世界的数据可能包含各种不确定性，尤其是图像数据。这种不确定性可能会误导学习过程并导致性能下降。通过研究图像数据，我们发现原始数据可以从一组具有不同扭曲的干净潜在样本中观察到。在这项工作中，我们提出了保持边界的度量学习框架，以同时学习距离度量和潜在样本。通过利用潜在样本的理想特性，可以显著提高训练效率，同时学习的度量对原始数据中的不确定性也变得鲁棒。此外，我们可以证明，尽管度量仅从潜在样本中学习，但它即使在原始数据上也能保持大边界特性。在基准图像数据集上的实证研究证明了所提出方法的有效性和效率。","领域":"图像识别/数据挖掘/人工智能","问题":"处理大规模数据集中的不确定性，提高距离度量学习的效率和鲁棒性","动机":"现实世界的数据，尤其是图像数据，包含各种不确定性，这些不确定性可能会误导学习过程并导致性能下降","方法":"提出了保持边界的度量学习框架，通过同时学习距离度量和潜在样本，利用潜在样本的理想特性提高训练效率和学习度量的鲁棒性","关键词":["距离度量学习","大规模数据集","不确定性处理","潜在样本","边界保持"],"涉及的技术概念":"距离度量学习（DML）是一种通过学习一个距离度量来改善基于距离的算法性能的技术。在这项工作中，通过引入潜在样本的概念，提出了一种新的框架来同时学习距离度量和潜在样本，以提高处理大规模数据集时的效率和鲁棒性。"},{"order":884,"title":"Guide Me: Interacting With Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Rupprecht_Guide_Me_Interacting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rupprecht_Guide_Me_Interacting_CVPR_2018_paper.html","abstract":"Interaction and collaboration between humans and intelligent machines has become increasingly important as machine learning methods move into real-world applications that involve end users. While much prior work lies at the intersection of natural language and vision, such as image captioning or image generation from text descriptions, less focus has been placed on the use of language to guide or improve the performance of a learned visual processing algorithm. In this paper, we explore methods to flexibly guide a trained convolutional neural network through user input to improve its performance during inference. We do so by inserting a layer that acts as a spatio-semantic guide into the network. This guide is trained to modify the network's activations, either directly via an energy minimization scheme or indirectly through a recurrent model that translates human language queries to interaction weights. Learning the verbal interaction is fully automatic and does not require manual text annotations. We evaluate the method on two datasets, showing that guiding a pre-trained network can improve performance, and provide extensive insights into the interaction between the guide and the CNN.","中文标题":"引导我：与深度网络的互动","摘要翻译":"随着机器学习方法进入涉及最终用户的现实世界应用，人类与智能机器之间的互动和协作变得越来越重要。尽管许多先前的工作位于自然语言和视觉的交汇处，例如图像字幕或从文本描述生成图像，但较少关注使用语言来指导或改进学习到的视觉处理算法的性能。在本文中，我们探索了通过用户输入灵活指导训练过的卷积神经网络以提高其在推理过程中性能的方法。我们通过在网络中插入一个作为空间语义引导的层来实现这一点。这个引导被训练来修改网络的激活，无论是通过能量最小化方案直接修改，还是通过将人类语言查询转换为交互权重的循环模型间接修改。学习语言互动是完全自动的，不需要手动文本注释。我们在两个数据集上评估了该方法，表明指导预训练网络可以提高性能，并提供了关于引导与CNN之间互动的广泛见解。","领域":"卷积神经网络/自然语言处理/用户交互","问题":"如何通过用户输入指导卷积神经网络以提高其推理性能","动机":"探索使用语言指导或改进视觉处理算法性能的方法，以增强人类与智能机器之间的互动和协作","方法":"在卷积神经网络中插入一个空间语义引导层，通过能量最小化方案或循环模型修改网络激活","关键词":["卷积神经网络","自然语言处理","用户交互"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，广泛用于图像识别和处理任务。自然语言处理（NLP）涉及使计算机能够理解、解释和生成人类语言的技术。用户交互指的是人类与计算机系统之间的互动方式，旨在通过用户输入改进系统性能。"},{"order":885,"title":"Art of Singular Vectors and Universal Adversarial Perturbations","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Khrulkov_Art_of_Singular_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Khrulkov_Art_of_Singular_CVPR_2018_paper.html","abstract":"Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 % fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.","中文标题":"奇异向量与通用对抗扰动的艺术","摘要翻译":"深度神经网络（DNNs）对对抗攻击的脆弱性在最近的研究中引起了大量关注。已经表明，对于许多执行图像分类的最先进的DNNs，存在通用对抗扰动——即与图像无关的扰动，仅需将其添加到自然图像中，就有高概率导致它们的错误分类。在这项工作中，我们提出了一种新的算法来构建这种通用扰动。我们的方法基于计算网络隐藏层Jacobian矩阵的所谓(p, q)-奇异向量。生成的扰动呈现出有趣的视觉模式，并且仅使用64张图像，我们就能够在由50000张图像组成的数据集上构建出具有超过60%欺骗率的通用扰动。我们还研究了Jacobian矩阵的最大奇异值与相应奇异向量的欺骗率之间的相关性，并展示了构建的扰动在网络间的泛化能力。","领域":"对抗样本生成/神经网络脆弱性分析/图像分类","问题":"深度神经网络对对抗攻击的脆弱性","动机":"研究深度神经网络在面对通用对抗扰动时的脆弱性，并提出新的算法来构建这种扰动，以提高网络的安全性。","方法":"基于计算网络隐藏层Jacobian矩阵的(p, q)-奇异向量来构建通用对抗扰动。","关键词":["对抗样本","奇异向量","Jacobian矩阵","欺骗率","图像分类"],"涉及的技术概念":{"对抗攻击":"一种通过添加微小扰动来误导深度神经网络的技术。","通用对抗扰动":"一种与特定图像无关的扰动，能够导致大多数自然图像被错误分类。","奇异向量":"在矩阵分析中，奇异向量是矩阵奇异值分解中的向量，用于描述矩阵的主要变化方向。","Jacobian矩阵":"在数学中，Jacobian矩阵是描述向量值函数各输出相对于各输入的偏导数的矩阵。","欺骗率":"指对抗样本成功误导分类器的比例。"}},{"order":886,"title":"Deflecting Adversarial Attacks With Pixel Deflection","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Prakash_Deflecting_Adversarial_Attacks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Prakash_Deflecting_Adversarial_Attacks_CVPR_2018_paper.html","abstract":"CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.","中文标题":"通过像素偏转防御对抗攻击","摘要翻译":"卷积神经网络（CNNs）即将成为许多关键系统的组成部分。尽管它们对自然变化具有鲁棒性，但通过精心设计的小幅、不可察觉的扰动，可以操纵图像像素值，导致模型对图像进行错误分类。我们提出了一种算法来处理图像，以便在此类对抗性操纵存在的情况下，显著保持分类准确性。图像分类器往往对自然噪声具有鲁棒性，而对抗性攻击往往对对象位置不敏感。这些观察结果激发了我们的策略，该策略通过迫使图像匹配自然图像统计来利用模型的鲁棒性来防御对抗性扰动。我们的算法通过我们称之为像素偏转的过程重新分配像素值来局部破坏图像。随后的基于小波的去噪操作软化了这种破坏，以及一些对抗性变化。我们通过实验证明，这些技术的组合能够有效地恢复真实类别，对抗各种鲁棒攻击。我们的结果与当前最先进的防御方法相比具有优势，且不需要重新训练或修改CNN。","领域":"对抗性防御/图像分类/鲁棒性","问题":"防御对抗性攻击以保持图像分类准确性","动机":"利用图像分类器对自然噪声的鲁棒性和对抗性攻击对对象位置的不敏感性，通过匹配自然图像统计来防御对抗性扰动","方法":"通过像素偏转过程重新分配像素值来局部破坏图像，随后使用基于小波的去噪操作软化破坏和对抗性变化","关键词":["对抗性防御","图像分类","鲁棒性","像素偏转","小波去噪"],"涉及的技术概念":"卷积神经网络（CNNs）用于图像分类，对抗性攻击通过小幅扰动操纵图像像素值导致错误分类，像素偏转是一种重新分配像素值的技术，基于小波的去噪操作用于软化图像破坏和对抗性变化"},{"order":887,"title":"MovieGraphs: Towards Understanding Human-Centric Situations From Videos","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Vicol_MovieGraphs_Towards_Understanding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Vicol_MovieGraphs_Towards_Understanding_CVPR_2018_paper.html","abstract":"There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to \\"read\\" people's emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips. Each graph consists of several types of nodes, to capture who is present in the clip, their emotional and physical attributes, their relationships (i.e., parent/child), and the interactions between them. Most interactions are associated with topics that provide additional details, and reasons that give motivations for actions. In addition, most interactions and many attributes are grounded in the video with time stamps. We provide a thorough analysis of our dataset, showing interesting common-sense correlations between different social aspects of scenes, as well as across scenes over time. We propose a method for querying videos and text with graphs, and show that: 1) our graphs contain rich and sufficient information to summarize and localize each scene; and 2) subgraphs allow us to describe situations at an abstract level and retrieve multiple semantically relevant situations. We also propose methods for interaction understanding via ordering, and reason understanding. MovieGraphs is the first benchmark to focus on inferred properties of human-centric situations, and opens up an exciting avenue towards socially-intelligent AI agents.","中文标题":"MovieGraphs：从视频中理解以人为中心的情境","摘要翻译":"人工智能领域对构建具有社会智能的机器人越来越感兴趣。这要求机器具备“读取”人们情绪、动机以及其他影响行为的因素的能力。为了实现这一目标，我们引入了一个名为MovieGraphs的新数据集，该数据集提供了电影片段中描绘的社会情境的详细、基于图的注释。每个图由几种类型的节点组成，以捕捉片段中出现的人物、他们的情感和身体属性、他们之间的关系（即父母/子女）以及他们之间的互动。大多数互动都与提供额外细节的主题相关联，以及给出行动动机的原因。此外，大多数互动和许多属性都与视频中的时间戳相关联。我们对我们的数据集进行了彻底的分析，展示了场景不同社会方面之间以及随时间跨场景的有趣的常识性关联。我们提出了一种用图查询视频和文本的方法，并展示了：1）我们的图包含丰富且足够的信息来总结和定位每个场景；2）子图使我们能够在抽象层面上描述情境并检索多个语义相关的情境。我们还提出了通过排序理解互动和理解原因的方法。MovieGraphs是第一个专注于以人为中心情境推断属性的基准，为具有社会智能的人工智能代理开辟了一条令人兴奋的道路。","领域":"情感计算/社会智能/视频理解","问题":"如何从视频中理解和分析以人为中心的社会情境","动机":"构建具有社会智能的机器人需要机器能够理解和分析人类的情感和动机等影响行为的因素","方法":"引入MovieGraphs数据集，提供基于图的详细社会情境注释；提出用图查询视频和文本的方法；通过排序理解互动和理解原因","关键词":["情感计算","社会智能","视频理解"],"涉及的技术概念":"MovieGraphs数据集、基于图的注释、情感和身体属性、关系、互动、时间戳、子图、语义相关情境、排序理解互动、理解原因"},{"order":888,"title":"SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mathews_SemStyle_Learning_to_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mathews_SemStyle_Learning_to_CVPR_2018_paper.html","abstract":"Linguistic style is an essential part of written communication, with the power to affect both clarity and attractiveness. With recent advances in vision and language, we can start to tackle the problem of generating image captions that are both visually grounded and appropriately styled. Existing approaches either require styled training captions aligned to images or generate captions with low relevance. We develop a model that learns to generate visually relevant styled captions from a large corpus of styled text without aligned images. The core idea of this model, called SemStyle, is to separate semantics and style. One key component is a novel and concise semantic term representation generated using natural language processing techniques and frame semantics. In addition, we develop a unified language model that decodes sentences with diverse word choices and syntax for different styles. Evaluations, both automatic and manual, show captions from SemStyle preserve image semantics, are descriptive, and are style shifted. More broadly, this work provides possibilities to learn richer image descriptions from the plethora of linguistic data available on the web.","中文标题":"SemStyle: 学习使用未对齐文本生成风格化图像描述","摘要翻译":"语言风格是书面交流的重要组成部分，具有影响清晰度和吸引力的能力。随着视觉和语言领域的最新进展，我们可以开始解决生成既视觉基础又适当风格的图像描述的问题。现有方法要么需要与图像对齐的风格化训练描述，要么生成相关性较低的描述。我们开发了一个模型，该模型能够从大量未与图像对齐的风格化文本中学习生成视觉相关的风格化描述。这个模型的核心思想，称为SemStyle，是将语义和风格分离。一个关键组成部分是使用自然语言处理技术和框架语义生成的新颖且简洁的语义术语表示。此外，我们开发了一个统一的语言模型，该模型能够解码具有不同词汇选择和语法的句子以适应不同的风格。自动和手动的评估显示，SemStyle生成的描述保留了图像语义，具有描述性，并且风格转换。更广泛地说，这项研究为从网络上大量可用的语言数据中学习更丰富的图像描述提供了可能性。","领域":"自然语言处理/图像描述生成/风格迁移","问题":"生成既视觉基础又适当风格的图像描述","动机":"解决现有方法需要与图像对齐的风格化训练描述或生成相关性较低的描述的问题","方法":"开发了一个模型，该模型能够从大量未与图像对齐的风格化文本中学习生成视觉相关的风格化描述，核心思想是将语义和风格分离，使用自然语言处理技术和框架语义生成语义术语表示，并开发统一的语言模型解码具有不同词汇选择和语法的句子以适应不同的风格","关键词":["风格化图像描述","语义和风格分离","自然语言处理"],"涉及的技术概念":"自然语言处理技术、框架语义、统一的语言模型、语义术语表示"},{"order":889,"title":"Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sattler_Benchmarking_6DOF_Outdoor_CVPR_2018_paper.html","abstract":"Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and  propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.","中文标题":"在变化条件下进行6自由度户外视觉定位的基准测试","摘要翻译":"视觉定位使自动驾驶车辆能够在周围环境中导航，并使增强现实应用能够将虚拟世界与现实世界联系起来。实用的视觉定位方法需要对各种观察条件具有鲁棒性，包括日夜变化、天气和季节变化，同时提供高度准确的6自由度（6DOF）相机姿态估计。在本文中，我们介绍了第一个专门设计用于分析这些因素对视觉定位影响的基准数据集。通过使用在各种条件下拍摄的查询图像的精心创建的真实姿态，我们通过使用最先进的定位方法进行广泛实验，评估了各种因素对6DOF相机姿态估计准确性的影响。根据我们的结果，我们得出了关于不同条件难度的结论，表明长期定位远未解决，并提出了未来工作的有希望的方向，包括基于序列的定位方法和需要更好的局部特征。我们的基准可在visuallocalization.net上获得。","领域":"自动驾驶/增强现实/视觉定位","问题":"在变化条件下进行6自由度户外视觉定位的准确性和鲁棒性问题","动机":"为了提升自动驾驶车辆和增强现实应用在复杂环境下的定位准确性和鲁棒性，需要开发能够适应各种观察条件的视觉定位方法。","方法":"引入专门设计的基准数据集，通过广泛实验评估各种因素对6DOF相机姿态估计准确性的影响，并提出未来研究方向。","关键词":["6自由度","视觉定位","基准测试","自动驾驶","增强现实"],"涉及的技术概念":"6自由度（6DOF）相机姿态估计是指在三维空间中确定相机的位置和方向，包括三个平移自由度和三个旋转自由度。视觉定位是指通过分析图像来确定相机相对于环境的位置和方向。基准测试是指通过一系列标准化的测试来评估系统或方法的性能。"},{"order":890,"title":"IVQA: Inverse Visual Question Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_IVQA_Inverse_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_IVQA_Inverse_Visual_CVPR_2018_paper.html","abstract":"We propose the inverse problem of Visual question answering (iVQA), and explore its suitability as a benchmark for visuo-linguistic understanding. The iVQA task is to generate a question that corresponds to a given image and answer pair. Since the answers are less informative than the questions, and the questions have less learnable bias, an iVQA model needs to better understand the image to be successful than a VQA model. We pose question generation as a multi-modal dynamic inference process and propose an iVQA model that can gradually adjust its focus of attention guided by both a partially generated question and the answer. For evaluation, apart from existing linguistic metrics, we propose  a new ranking metric. This metric compares the ground truth question's rank among a list of distractors, which allows the drawbacks of different algorithms and sources of error to be studied. Experimental results show that our model can generate diverse,  grammatically correct and content correlated questions that match the given answer.","中文标题":"IVQA: 逆向视觉问答","摘要翻译":"我们提出了视觉问答（VQA）的逆向问题（iVQA），并探讨了其作为视觉语言理解基准的适用性。iVQA任务是根据给定的图像和答案对生成相应的问题。由于答案比问题提供的信息少，且问题的可学习偏差较小，因此iVQA模型需要比VQA模型更好地理解图像才能成功。我们将问题生成视为一个多模态动态推理过程，并提出了一个iVQA模型，该模型可以根据部分生成的问题和答案逐步调整其注意力焦点。为了评估，除了现有的语言指标外，我们提出了一个新的排名指标。该指标比较了真实问题在一系列干扰项中的排名，从而可以研究不同算法和错误来源的缺点。实验结果表明，我们的模型能够生成多样化、语法正确且内容相关的问题，这些问题与给定的答案相匹配。","领域":"视觉问答/自然语言处理/多模态学习","问题":"如何根据给定的图像和答案生成相应的问题","动机":"探讨逆向视觉问答作为视觉语言理解基准的适用性，以及如何通过生成问题来更好地理解图像","方法":"提出一个iVQA模型，该模型将问题生成视为多模态动态推理过程，并根据部分生成的问题和答案逐步调整其注意力焦点","关键词":["逆向视觉问答","多模态学习","问题生成"],"涉及的技术概念":"视觉问答（VQA）、逆向视觉问答（iVQA）、多模态动态推理过程、注意力机制、排名指标"},{"order":891,"title":"Unsupervised Person Image Synthesis in Arbitrary Poses","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Pumarola_Unsupervised_Person_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pumarola_Unsupervised_Person_Image_CVPR_2018_paper.html","abstract":"We present a novel approach for synthesizing photo-realistic images of people in arbitrary poses using generative adversarial learning. Given an input image of a person and a desired pose represented by a 2D skeleton, our model renders the image of the same person under the new pose, synthesizing novel views of the parts visible in the input image and hallucinating those that are not seen. This problem has recently been addressed in a supervised manner, i.e., during training the ground truth images under the new poses are given  to the network. We go beyond these approaches by proposing a fully unsupervised strategy. We tackle this challenging scenario by splitting the problem into two principal subtasks.  First, we consider a pose conditioned bidirectional generator that maps back the initially rendered image to the original pose, hence being directly comparable to the input image without the need to resort to any training image. Second, we devise a novel loss function that incorporates content and style terms, and aims at producing images of high perceptual quality. Extensive experiments conducted on the DeepFashion dataset demonstrate that the images rendered by our model are very close in appearance to those obtained by fully supervised approaches.","中文标题":"任意姿态下的无监督人物图像合成","摘要翻译":"我们提出了一种新颖的方法，利用生成对抗学习合成任意姿态下的人物照片级真实感图像。给定一个人物的输入图像和由2D骨架表示的期望姿态，我们的模型在新的姿态下渲染同一人物的图像，合成输入图像中可见部分的新视图，并对未看到的部分进行幻觉。这个问题最近以监督方式被解决，即在训练期间向网络提供新姿态下的真实图像。我们通过提出一种完全无监督的策略超越了这些方法。我们通过将问题分解为两个主要子任务来应对这一挑战性场景。首先，我们考虑一个姿态条件双向生成器，它将最初渲染的图像映射回原始姿态，因此可以直接与输入图像进行比较，而无需依赖任何训练图像。其次，我们设计了一种新的损失函数，该函数结合了内容和风格项，旨在生成高感知质量的图像。在DeepFashion数据集上进行的大量实验表明，我们的模型渲染的图像在外观上非常接近完全监督方法获得的图像。","领域":"图像合成/姿态估计/生成对抗网络","问题":"在无监督条件下合成任意姿态下的人物图像","动机":"超越现有的监督方法，提出一种完全无监督的策略来合成任意姿态下的人物图像","方法":"将问题分解为两个主要子任务：使用姿态条件双向生成器将渲染图像映射回原始姿态，并设计一种新的损失函数以生成高感知质量的图像","关键词":["图像合成","姿态估计","生成对抗网络"],"涉及的技术概念":"生成对抗学习、2D骨架表示、姿态条件双向生成器、内容与风格损失函数、DeepFashion数据集"},{"order":892,"title":"Learning Descriptor Networks for 3D Shape Synthesis and Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.html","abstract":"This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for  modeling volumetric shape patterns. The maximum likelihood training of the model follows an \\"analysis by synthesis\\" scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling  from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.","中文标题":"学习用于3D形状合成和分析的描述符网络","摘要翻译":"本文提出了一种3D形状描述符网络，这是一种基于深度卷积能量的模型，用于建模体积形状模式。模型的最大似然训练遵循“通过合成进行分析”的方案，并可以解释为模式寻找和模式转移过程。该模型可以通过MCMC（如朗之万动力学）从概率分布中采样来合成3D形状模式。该模型可用于通过MCMC教学训练3D生成器网络。3D形状描述符网络的条件版本可用于3D对象恢复和3D对象超分辨率。实验表明，所提出的模型可以生成逼真的3D形状模式，并且对3D形状分析非常有用。","领域":"3D形状合成/3D形状分析/3D对象恢复","问题":"如何有效地建模和合成3D形状模式","动机":"为了改进3D形状的合成和分析，以及实现3D对象的恢复和超分辨率","方法":"提出了一种基于深度卷积能量的3D形状描述符网络，采用最大似然训练和MCMC采样技术","关键词":["3D形状描述符网络","深度卷积能量模型","MCMC采样","朗之万动力学","3D生成器网络","3D对象恢复","3D对象超分辨率"],"涉及的技术概念":{"3D形状描述符网络":"一种用于建模3D形状模式的深度卷积能量模型","最大似然训练":"一种训练模型的方法，通过最大化数据的似然函数来优化模型参数","MCMC采样":"马尔可夫链蒙特卡罗方法，用于从概率分布中采样","朗之万动力学":"一种MCMC采样技术，用于模拟物理系统中的粒子运动","3D生成器网络":"一种能够生成3D形状模式的网络","3D对象恢复":"从损坏或不完整的数据中恢复3D对象的过程","3D对象超分辨率":"提高3D对象分辨率的过程"}},{"order":893,"title":"Neural Kinematic Networks for Unsupervised Motion Retargetting","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Villegas_Neural_Kinematic_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Villegas_Neural_Kinematic_Networks_CVPR_2018_paper.html","abstract":"We propose a recurrent neural network architecture with a Forward Kinematics layer and cycle consistency based adversarial training objective for unsupervised motion retargetting. Our network captures the high-level properties of an input motion by the forward kinematics layer, and adapts them to a target character with different skeleton bone lengths (e.g., shorter, longer arms etc.). Collecting paired motion training sequences from different characters is expensive. Instead, our network utilizes cycle consistency to learn to solve the Inverse Kinematics problem in an unsupervised manner. Our method works online, i.e., it adapts the motion sequence on-the-fly as new frames are received. In our experiments, we use the Mixamo animation data to test our method for a variety of motions and characters and achieve state-of-the-art results. We also demonstrate motion retargetting from monocular human videos to 3D characters using an off-the-shelf 3D pose estimator.","中文标题":"神经运动学网络用于无监督运动重定向","摘要翻译":"我们提出了一种循环神经网络架构，该架构包含一个前向运动学层和基于循环一致性的对抗训练目标，用于无监督运动重定向。我们的网络通过前向运动学层捕捉输入运动的高级属性，并将这些属性适应到具有不同骨骼长度（例如，较短、较长的手臂等）的目标角色上。从不同角色收集成对的运动训练序列是昂贵的。相反，我们的网络利用循环一致性以无监督的方式学习解决逆运动学问题。我们的方法在线工作，即，随着新帧的接收，它实时适应运动序列。在我们的实验中，我们使用Mixamo动画数据来测试我们的方法，针对各种运动和角色，并取得了最先进的结果。我们还展示了使用现成的3D姿态估计器从单目人类视频到3D角色的运动重定向。","领域":"动画技术/运动捕捉/3D建模","问题":"解决不同角色间运动重定向的问题，特别是在骨骼长度不同的情况下。","动机":"收集不同角色的成对运动训练序列成本高昂，需要一种无监督的方法来适应运动序列。","方法":"提出了一种包含前向运动学层和基于循环一致性的对抗训练目标的循环神经网络架构，以无监督的方式学习解决逆运动学问题，并在线适应运动序列。","关键词":["运动重定向","无监督学习","循环神经网络","前向运动学","循环一致性","逆运动学"],"涉及的技术概念":"前向运动学层用于捕捉输入运动的高级属性，循环一致性用于无监督学习，逆运动学问题解决，以及在线运动序列适应。"},{"order":894,"title":"Group Consistent Similarity Learning via Deep CRF for Person Re-Identification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Group_Consistent_Similarity_CVPR_2018_paper.html","abstract":"Person re-identification benefits greatly from deep neural networks (DNN) to learn accurate similarity metrics and robust feature embeddings. However, most of the current methods impose only local constraints for similarity learning. In this paper, we incorporate constraints on large image groups by combining the CRF with deep neural networks. The proposed method aims to learn the \`\`local similarity\\" metrics for image pairs while taking into account the dependencies from all the images in a group, forming \`\`group similarities\\". Our method involves multiple images to model the relationships among the local and global similarities in a unified CRF during training, while combines multi-scale local similarities as the predicted similarity in testing. We adopt an approximate inference scheme for estimating the group similarity, enabling end-to-end training. Extensive experiments demonstrate the effectiveness of our model that combines DNN and CRF for learning robust multi-scale local similarities. The overall results outperform those by state-of-the-arts with considerable margins on three widely-used benchmarks.","中文标题":"通过深度条件随机场进行群体一致性相似度学习用于行人重识别","摘要翻译":"行人重识别极大地受益于深度神经网络（DNN）以学习准确的相似度度量和鲁棒的特征嵌入。然而，当前大多数方法仅对相似度学习施加局部约束。在本文中，我们通过将条件随机场（CRF）与深度神经网络结合，对大型图像群体施加约束。所提出的方法旨在学习图像对的“局部相似度”度量，同时考虑到群体中所有图像的依赖性，形成“群体相似度”。我们的方法在训练期间涉及多个图像，以在统一的CRF中建模局部和全局相似度之间的关系，同时在测试中结合多尺度局部相似度作为预测的相似度。我们采用了一种近似推理方案来估计群体相似度，实现了端到端的训练。大量实验证明了我们结合DNN和CRF学习鲁棒多尺度局部相似度的模型的有效性。总体结果在三个广泛使用的基准测试上以相当大的优势超过了最先进的方法。","领域":"行人重识别/相似度学习/条件随机场","问题":"如何在行人重识别中学习到既考虑局部相似度又考虑群体依赖性的相似度度量","动机":"当前的行人重识别方法大多仅考虑局部约束，忽略了群体中图像间的依赖性，这限制了相似度学习的准确性和鲁棒性。","方法":"结合条件随机场（CRF）与深度神经网络（DNN），在训练期间通过统一的CRF建模局部和全局相似度之间的关系，并在测试中结合多尺度局部相似度作为预测的相似度。采用近似推理方案估计群体相似度，实现端到端训练。","关键词":["行人重识别","相似度学习","条件随机场","深度神经网络","多尺度局部相似度"],"涉及的技术概念":{"深度神经网络（DNN）":"一种模拟人脑神经网络结构和功能的计算模型，用于学习和提取数据特征。","条件随机场（CRF）":"一种用于结构化预测的统计建模方法，常用于序列标注任务，能够考虑序列中元素间的依赖关系。","多尺度局部相似度":"指在不同尺度上计算图像局部区域的相似度，能够捕捉到图像细节和全局结构信息。","端到端训练":"一种训练方法，模型的输入到输出之间的所有步骤都是可微的，可以直接通过反向传播算法进行优化。"}},{"order":895,"title":"Learning Compositional Visual Concepts With Mutual Consistency","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gong_Learning_Compositional_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gong_Learning_Compositional_Visual_CVPR_2018_paper.html","abstract":"Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.","中文标题":"学习具有相互一致性的组合视觉概念","摘要翻译":"在图像合成和分析中，语义概念的组合性非常吸引人，因为它可以帮助分解已知数据并生成性地重新组合未知数据。例如，我们可以学习场景中光照、几何或反照率变化的概念，并尝试重新组合它们以生成物理上有意义但未见过的数据用于训练和测试。然而，在实践中，我们通常没有来自联合概念空间的样本：我们可能在一个数据集中有关于光照变化的数据，在另一个数据集中有关于几何变化的数据，而没有完全重叠。我们提出了以下问题：如何从不同的数据集中共同学习两个或更多概念，并在我们没有来自完整联合空间的样本的情况下保持相互一致性？我们在本文中提出了一个基于多个概念上的循环一致性的新颖答案，这些概念分别由生成对抗网络（GANs）表示。我们的方法，ConceptGAN，可以被理解为数据增强的替代方案，以提高现实世界应用中的韧性。定性和定量评估证明了其在生成语义上有意义的图像以及作为示例应用的一次性面部验证中的有效性。","领域":"图像合成/语义概念学习/生成对抗网络","问题":"如何从不同的数据集中共同学习两个或更多概念，并在没有来自完整联合空间的样本的情况下保持相互一致性","动机":"提高图像合成和分析中语义概念组合性的应用，以生成物理上有意义但未见过的数据用于训练和测试","方法":"基于多个概念上的循环一致性，这些概念分别由生成对抗网络（GANs）表示","关键词":["图像合成","语义概念学习","生成对抗网络","数据增强","一次性面部验证"],"涉及的技术概念":"生成对抗网络（GANs）用于表示多个概念，并通过循环一致性来学习这些概念，以提高数据增强和现实世界应用中的韧性。"},{"order":896,"title":"NestedNet: Learning Nested Sparse Structures in Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_NestedNet_Learning_Nested_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_NestedNet_Learning_Nested_CVPR_2018_paper.html","abstract":"Recently, there have been increasing demands to construct compact deep architectures to remove unnecessary redundancy and to improve the inference speed. While many recent works focus on reducing the redundancy by eliminating unneeded weight parameters, it is not possible to apply a single deep network for multiple devices with different resources. When a new device or circumstantial condition requires a new deep architecture, it is necessary to construct and train a new network from scratch. In this work, we propose a novel deep learning framework, called a nested sparse network, which exploits an n-in-1-type nested structure in a neural network. A nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level, and higher level networks share parameters with lower level networks to enable stable nested learning. The proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements, i.e., anytime property. Moreover, the proposed nested network can learn different forms of knowledge in its internal networks at different levels, enabling multiple tasks using a single  network, such as coarse-to-fine hierarchical classification. In order to train the proposed nested network, we propose efficient weight connection learning and channel and layer scheduling strategies. We evaluate our network in multiple tasks, including adaptive deep compression, knowledge distillation, and learning class hierarchy, and demonstrate that nested sparse networks perform competitively, but more efficiently, compared to existing methods.","中文标题":"NestedNet: 学习深度神经网络中的嵌套稀疏结构","摘要翻译":"近年来，构建紧凑的深度架构以去除不必要的冗余并提高推理速度的需求日益增加。虽然许多最近的工作专注于通过消除不需要的权重参数来减少冗余，但不可能为具有不同资源的多个设备应用单一的深度网络。当新设备或环境条件需要新的深度架构时，必须从头开始构建和训练新的网络。在这项工作中，我们提出了一种新颖的深度学习框架，称为嵌套稀疏网络，它利用了神经网络中的n-in-1型嵌套结构。嵌套稀疏网络由具有不同稀疏比的多级网络组成，较高级别的网络与较低级别的网络共享参数，以实现稳定的嵌套学习。所提出的框架实现了一种资源感知的通用架构，因为同一网络可以满足不同的资源需求，即随时属性。此外，所提出的嵌套网络可以在其内部网络的不同级别学习不同形式的知识，使得使用单一网络进行多任务成为可能，例如从粗到细的层次分类。为了训练所提出的嵌套网络，我们提出了有效的权重连接学习以及通道和层调度策略。我们在多个任务中评估了我们的网络，包括自适应深度压缩、知识蒸馏和学习类层次结构，并证明嵌套稀疏网络与现有方法相比具有竞争力，但更高效。","领域":"深度压缩/知识蒸馏/层次分类","问题":"如何构建一个能够适应不同资源需求的深度网络架构","动机":"为了去除深度网络中的不必要冗余，提高推理速度，并使得单一网络能够适应不同设备和环境条件的需求","方法":"提出了一种嵌套稀疏网络框架，该框架通过在不同级别的网络中实现不同的稀疏比，并让较高级别的网络与较低级别的网络共享参数，从而实现资源感知的通用架构","关键词":["嵌套稀疏网络","深度压缩","知识蒸馏","层次分类"],"涉及的技术概念":"嵌套稀疏网络是一种深度学习框架，它通过在不同级别的网络中实现不同的稀疏比，并让较高级别的网络与较低级别的网络共享参数，从而实现资源感知的通用架构。这种网络能够学习不同形式的知识，使得使用单一网络进行多任务成为可能，例如从粗到细的层次分类。为了训练这种网络，提出了有效的权重连接学习以及通道和层调度策略。"},{"order":897,"title":"Context Embedding Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Context_Embedding_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kim_Context_Embedding_Networks_CVPR_2018_paper.html","abstract":"Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. Similarity is a multi-dimensional concept that varies from individual to individual. However, existing models for learning crowd embeddings typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, the list of criteria is known in advance, or that the crowd workers are not influenced by the data that they see. To overcome these limitations we introduce Context Embedding Networks (CENs).  In addition to learning interpretable embeddings from images, CENs also model worker biases for different attributes along with the visual context i.e. the attributes highlighted by a set of images. Experiments on three noisy crowd annotated datasets show that modeling both worker bias and visual context results in more interpretable embeddings compared to existing approaches.","中文标题":"上下文嵌入网络","摘要翻译":"捕捉数据集合中主要变化兴趣的低维嵌入对于许多应用来说非常重要。构建这些嵌入的一种方法是从人群中获取相似性估计。相似性是一个多维概念，因人而异。然而，现有的学习人群嵌入的模型通常做出简化假设，例如所有个体使用相同的标准估计相似性，标准列表是预先已知的，或者人群工作者不受他们看到的数据的影响。为了克服这些限制，我们引入了上下文嵌入网络（CENs）。除了从图像中学习可解释的嵌入外，CENs还模拟了工作者对不同属性的偏见以及视觉上下文，即一组图像所突出的属性。在三个嘈杂的人群注释数据集上的实验表明，与现有方法相比，模拟工作者偏见和视觉上下文会导致更可解释的嵌入。","领域":"数据嵌入/人群计算/视觉上下文分析","问题":"如何从人群中获取更准确和可解释的相似性估计","动机":"现有模型在估计相似性时存在简化假设，无法准确反映个体差异和视觉上下文的影响","方法":"引入上下文嵌入网络（CENs），模拟工作者偏见和视觉上下文，以学习更可解释的嵌入","关键词":["低维嵌入","相似性估计","工作者偏见","视觉上下文"],"涉及的技术概念":{"低维嵌入":"一种将高维数据转换为低维表示的技术，旨在捕捉数据的主要变化","相似性估计":"评估两个或多个对象之间相似程度的过程","工作者偏见":"指人群工作者在完成任务时可能存在的个人偏好或偏差","视觉上下文":"指一组图像中突出的属性或特征，影响对图像内容的理解和解释"}},{"order":898,"title":"Iterative Learning With Open-Set Noisy Labels","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Iterative_Learning_With_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Iterative_Learning_With_CVPR_2018_paper.html","abstract":"Large-scale datasets possessing clean label annotations are crucial for training Convolutional Neural Networks (CNNs). However, labeling large-scale data can be very costly and error-prone, and even high-quality datasets are likely to contain noisy (incorrect) labels. Existing works usually employ a closed-set assumption, whereby the samples associated with noisy labels possess a true class contained within the set of known classes in the training data. However, such an assumption is too restrictive for many applications, since samples associated with noisy labels might in fact possess a true class that is not present in the training data. We refer to this more complex scenario as the open-set noisy label problem and show that it is nontrivial in order to make accurate predictions. To address this problem, we propose a novel iterative learning framework for training CNNs on datasets with open-set noisy labels. Our approach detects noisy labels and learns deep discriminative features in an iterative fashion. To benefit from the noisy label detection, we design a Siamese network to encourage clean labels and noisy labels to be dissimilar. A reweighting module is also applied to simultaneously emphasize the learning from clean labels and reduce the effect caused by noisy labels. Experiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets demonstrate that our proposed model can robustly train CNNs in the presence of a high proportion of open-set as well as closed-set noisy labels.","中文标题":"使用开放集噪声标签的迭代学习","摘要翻译":"拥有干净标签注释的大规模数据集对于训练卷积神经网络（CNNs）至关重要。然而，标注大规模数据可能非常昂贵且容易出错，即使是高质量的数据集也可能包含噪声（错误）标签。现有的工作通常采用封闭集假设，即与噪声标签相关的样本拥有一个真实类别，该类别包含在训练数据的已知类别集合中。然而，这种假设对于许多应用来说过于严格，因为与噪声标签相关的样本实际上可能拥有一个不在训练数据中的真实类别。我们将这种更复杂的场景称为开放集噪声标签问题，并表明为了做出准确的预测，这个问题并非微不足道。为了解决这个问题，我们提出了一种新颖的迭代学习框架，用于在具有开放集噪声标签的数据集上训练CNNs。我们的方法以迭代方式检测噪声标签并学习深度判别特征。为了从噪声标签检测中受益，我们设计了一个连体网络，以鼓励干净标签和噪声标签之间的差异。还应用了一个重加权模块，以同时强调从干净标签中学习并减少由噪声标签引起的影响。在CIFAR-10、ImageNet和现实世界噪声（网络搜索）数据集上的实验表明，我们提出的模型可以在存在高比例开放集以及封闭集噪声标签的情况下稳健地训练CNNs。","领域":"卷积神经网络/噪声标签处理/开放集学习","问题":"处理开放集噪声标签问题，即在训练数据中存在的噪声标签可能对应不在已知类别集合中的真实类别。","动机":"现有的封闭集噪声标签处理方法对于许多应用来说过于严格，因为与噪声标签相关的样本实际上可能拥有一个不在训练数据中的真实类别。","方法":"提出了一种新颖的迭代学习框架，包括检测噪声标签、学习深度判别特征、设计连体网络以区分干净标签和噪声标签，以及应用重加权模块来强调从干净标签中学习并减少噪声标签的影响。","关键词":["开放集噪声标签","迭代学习","连体网络","重加权模块"],"涉及的技术概念":"卷积神经网络（CNNs）是一种深度学习模型，特别适用于处理图像数据。噪声标签指的是数据集中错误或不准确的标签。开放集噪声标签问题指的是噪声标签可能对应不在训练数据已知类别集合中的真实类别。连体网络是一种特殊的神经网络架构，用于学习输入数据之间的相似性或差异性。重加权模块是一种技术，用于调整训练过程中不同样本或标签的权重，以优化模型的学习效果。"},{"order":899,"title":"Learning Transferable Architectures for Scalable Image Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html","abstract":"Developing neural network image classification models often requires significant architecture engineering.  In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \\"NASNet search space\\"\\") which enables transferability. In our experiments, we search for the best convolutional layer (or \\"cell\\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \\"NASNet architecture\\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS -- a reduction of 28% in computational demand from the previous state-of-the-art model.  When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.","中文标题":"学习可扩展图像识别的可转移架构","摘要翻译":"开发神经网络图像分类模型通常需要大量的架构工程。在本文中，我们研究了一种直接在感兴趣的数据集上学习模型架构的方法。由于这种方法在数据集较大时成本较高，我们提出在一个小数据集上搜索架构构建块，然后将该块转移到一个更大的数据集。这项工作的关键贡献是设计了一个新的搜索空间（我们称之为“NASNet搜索空间”），它实现了可转移性。在我们的实验中，我们在CIFAR-10数据集上搜索最佳卷积层（或“单元”），然后通过堆叠更多这种单元的副本（每个副本都有自己的参数）来设计卷积架构，我们将其命名为“NASNet架构”。我们还引入了一种新的正则化技术，称为ScheduledDropPath，它显著提高了NASNet模型的泛化能力。在CIFAR-10上，我们的方法找到的NASNet实现了2.4%的错误率，这是最先进的。尽管该单元没有直接在ImageNet上搜索，但从最佳单元构建的NASNet在已发表的作品中，在ImageNet上实现了82.7%的top-1和96.2%的top-5的最先进准确率。我们的模型在top-1准确率上比最佳人工设计的架构高出1.2%，同时减少了90亿次FLOPS——比之前的最先进模型减少了28%的计算需求。在不同计算成本水平下评估时，NASNets的准确率超过了最先进的人工设计模型。例如，一个小版本的NASNet也实现了74%的top-1准确率，比同等大小的、最先进的移动平台模型高出3.1%。最后，从图像分类中学到的图像特征是通用的，可以转移到其他计算机视觉问题上。在对象检测任务中，使用Faster-RCNN框架的NASNet学习到的特征超过了最先进水平4.0%，在COCO数据集上实现了43.1%的mAP。","领域":"神经网络架构搜索/图像分类/对象检测","问题":"如何高效地设计适用于大规模图像识别的神经网络架构","动机":"减少神经网络图像分类模型开发中的架构工程需求，提高模型的可转移性和泛化能力","方法":"提出在小数据集上搜索架构构建块并转移到大数据集的方法，设计新的搜索空间（NASNet搜索空间）和引入新的正则化技术（ScheduledDropPath）","关键词":["神经网络架构搜索","图像分类","对象检测","正则化技术","计算效率"],"涉及的技术概念":{"NASNet搜索空间":"一种新的搜索空间设计，使得在小数据集上搜索到的架构构建块能够有效地转移到大数据集上","ScheduledDropPath":"一种新的正则化技术，用于提高NASNet模型的泛化能力","FLOPS":"浮点运算次数，用于衡量模型的计算需求","mAP":"平均精度均值，用于评估对象检测模型的性能"}},{"order":900,"title":"SBNet: Sparse Blocks Network for Fast Inference","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ren_SBNet_Sparse_Blocks_CVPR_2018_paper.html","abstract":"Conventional deep convolutional neural networks (CNNs) apply convolution operators uniformly in space across all feature maps for hundreds of layers - this incurs a high computational cost for real-time applications. For many problems such as object detection and semantic segmentation, we are able to obtain a low-cost computation mask, either from a priori problem knowledge, or from a low-resolution segmentation network. We show that such computation masks can be used to reduce computation in the high-resolution main network. Variants of sparse activation CNNs have previously been explored on small-scale tasks and showed no degradation in terms of object classification accuracy, but often measured gains in terms of theoretical FLOPs without realizing a practical speed-up when compared to highly optimized dense convolution implementations. In this work, we leverage the sparsity structure of computation masks and propose a novel tiling-based sparse convolution algorithm. We verified the effectiveness of our sparse CNN on LiDAR-based 3D object detection, and we report significant wall-clock speed-ups compared to dense convolution without noticeable loss of accuracy.","中文标题":"SBNet: 用于快速推理的稀疏块网络","摘要翻译":"传统的深度卷积神经网络（CNNs）在所有特征图上均匀地应用卷积操作，跨越数百层——这对于实时应用来说计算成本很高。对于诸如目标检测和语义分割等问题，我们能够从先验问题知识或低分辨率分割网络中获得低成本的计算掩码。我们展示了这样的计算掩码可以用来减少高分辨率主网络中的计算。稀疏激活CNNs的变体先前已在小型任务上进行了探索，并且在目标分类准确性方面没有显示出退化，但通常在理论FLOPs方面测量增益，而没有在与高度优化的密集卷积实现相比时实现实际的速度提升。在这项工作中，我们利用计算掩码的稀疏结构，并提出了一种新颖的基于平铺的稀疏卷积算法。我们在基于LiDAR的3D目标检测上验证了我们的稀疏CNN的有效性，并且报告了与密集卷积相比显著的壁钟速度提升，而没有明显的准确性损失。","领域":"3D目标检测/语义分割/实时计算","问题":"减少高分辨率主网络中的计算成本，以实现实时应用","动机":"传统的深度卷积神经网络在实时应用中计算成本高，需要一种方法来减少计算量而不损失准确性","方法":"利用计算掩码的稀疏结构，提出了一种新颖的基于平铺的稀疏卷积算法","关键词":["稀疏卷积","3D目标检测","语义分割","实时计算"],"涉及的技术概念":"深度卷积神经网络（CNNs）、计算掩码、稀疏激活CNNs、理论FLOPs、基于LiDAR的3D目标检测"},{"order":901,"title":"Language-Based Image Editing With Recurrent Attentive Models","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Language-Based_Image_Editing_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_Language-Based_Image_Editing_CVPR_2018_paper.html","abstract":"We investigate the problem of Language-Based Image Editing (LBIE). Given a source image and a natural language description, we want to generate a target image by editing the source image based on the description. We propose a generic modeling framework for two sub-tasks of LBIE: language-based image segmentation and image colorization. The framework uses recurrent attentive models to fuse image and language features. Instead of using a fixed step size, we introduce for each region of the image a termination gate to dynamically determine after each inference step whether to continue extrapolating additional information from the textual description. The effectiveness of the framework is validated on three datasets. First, we introduce a synthetic dataset, called CoSaL, to evaluate the end-to-end performance of our LBIE system. Second, we show that the framework leads to state-of-the-art performance on image segmentation on the ReferIt dataset. Third, we present the first language-based colorization result on the Oxford-102 Flowers dataset.","中文标题":"基于语言的图像编辑与循环注意力模型","摘要翻译":"我们研究了基于语言的图像编辑（LBIE）问题。给定一个源图像和一个自然语言描述，我们希望通过根据描述编辑源图像来生成目标图像。我们提出了一个通用的建模框架，用于LBIE的两个子任务：基于语言的图像分割和图像着色。该框架使用循环注意力模型来融合图像和语言特征。我们为图像的每个区域引入了一个终止门，而不是使用固定的步长，以动态确定在每个推理步骤后是否继续从文本描述中推断额外信息。该框架的有效性在三个数据集上得到了验证。首先，我们引入了一个名为CoSaL的合成数据集，以评估我们的LBIE系统的端到端性能。其次，我们展示了该框架在ReferIt数据集上的图像分割性能达到了最先进的水平。第三，我们在Oxford-102 Flowers数据集上展示了第一个基于语言的着色结果。","领域":"图像分割/图像着色/自然语言处理","问题":"如何根据自然语言描述编辑源图像以生成目标图像","动机":"探索基于语言的图像编辑技术，以实现更自然和直观的图像编辑方式","方法":"提出一个通用的建模框架，使用循环注意力模型融合图像和语言特征，并引入终止门动态控制信息推断过程","关键词":["图像分割","图像着色","自然语言处理"],"涉及的技术概念":"循环注意力模型是一种能够处理序列数据的神经网络，通过注意力机制聚焦于输入数据的重要部分。终止门是一种机制，用于动态决定是否继续处理输入数据，以提高模型的效率和效果。"},{"order":902,"title":"Net2Vec: Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.html","abstract":"In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation.  A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.","中文标题":"Net2Vec: 量化和解释概念如何通过深度神经网络中的过滤器编码","摘要翻译":"为了理解深度网络捕捉的中间表示的意义，最近的论文尝试将特定的语义概念与单个神经网络过滤器的响应关联起来，通常通过关注极端过滤器响应发现有趣的相关性。在本文中，我们展示了这种方法可能偏向于易于解释但不一定代表表示平均行为的情况。一个更现实但更难研究的假设是语义表示是分布式的，因此必须联合研究过滤器。为了研究这一想法，同时实现多过滤器响应的系统可视化和量化，我们引入了Net2Vec框架，其中语义概念基于相应的过滤器响应映射到向量嵌入。通过研究这些嵌入，我们能够展示：1. 在大多数情况下，需要多个过滤器来编码一个概念；2. 通常过滤器不是特定于概念的，并且帮助编码多个概念；3. 与单个过滤器激活相比，过滤器嵌入能够更好地表征表示的意义及其与其他概念的关系。","领域":"神经网络解释性/语义表示/过滤器响应分析","问题":"如何量化和解释深度神经网络中过滤器如何编码语义概念","动机":"理解深度网络中间表示的意义，以及如何更准确地关联语义概念与过滤器响应","方法":"引入Net2Vec框架，将语义概念映射到基于过滤器响应的向量嵌入，通过研究这些嵌入来展示过滤器在编码概念中的作用","关键词":["神经网络解释性","语义表示","过滤器响应分析"],"涉及的技术概念":"Net2Vec框架、过滤器响应、向量嵌入、语义概念编码"},{"order":903,"title":"End-to-End Dense Video Captioning With Masked Transformer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.html","abstract":"Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, all previous methods on dense video captioning tackle this problem by building two models, i.e. an event proposal and a captioning model, for these two sub-problems. The models are either trained separately or in alternation. This prevents direct influence of the language description to the event proposal, which is important for generating accurate descriptions. To address this problem, we propose an end-to-end transformer model for dense video captioning. The encoder encodes the video into appropriate representations. The proposal decoder decodes from the encoding with different anchors to form video event proposals. The captioning decoder employs a masking network to restrict its attention to the proposal event over the encoding feature. This masking network converts the event proposal to a differentiable mask, which ensures the consistency between the proposal and captioning during training. In addition, our model employs a self-attention mechanism, which enables the use of efficient non-recurrent structure during encoding and leads to performance improvements. We demonstrate the effectiveness of this end-to-end model on ActivityNet Captions and YouCookII datasets, where we achieved 10.12 and 6.58 METEOR score, respectively.","中文标题":"端到端密集视频字幕生成与掩码Transformer","摘要翻译":"密集视频字幕生成旨在为未剪辑视频中的所有事件生成文本描述。这既涉及事件的检测也涉及事件的描述。因此，之前所有关于密集视频字幕生成的方法都是通过构建两个模型来解决这两个子问题，即事件提案模型和字幕生成模型。这些模型要么是分开训练的，要么是交替训练的。这阻止了语言描述对事件提案的直接影响，而这对生成准确的描述是重要的。为了解决这个问题，我们提出了一个用于密集视频字幕生成的端到端Transformer模型。编码器将视频编码成适当的表示。提案解码器从编码中解码，使用不同的锚点形成视频事件提案。字幕解码器采用掩码网络来限制其对编码特征上的提案事件的注意力。这个掩码网络将事件提案转换为可微分的掩码，这确保了训练过程中提案和字幕之间的一致性。此外，我们的模型采用了自注意力机制，这使得在编码过程中可以使用高效的非递归结构，并带来性能提升。我们在ActivityNet Captions和YouCookII数据集上展示了这种端到端模型的有效性，分别达到了10.12和6.58的METEOR分数。","领域":"视频理解/自然语言生成/事件检测","问题":"密集视频字幕生成中的事件检测与描述问题","动机":"解决现有方法中语言描述对事件提案直接影响不足的问题，以提高描述的准确性","方法":"提出一个端到端的Transformer模型，通过编码器、提案解码器和字幕解码器，以及掩码网络和自注意力机制，实现事件提案和字幕生成的一致性","关键词":["密集视频字幕生成","Transformer模型","事件检测","自注意力机制"],"涉及的技术概念":{"密集视频字幕生成":"为视频中的每个事件生成文本描述的任务","Transformer模型":"一种基于自注意力机制的深度学习模型，适用于处理序列数据","事件检测":"识别视频中发生的事件的过程","自注意力机制":"一种机制，允许模型在处理序列时关注序列的不同部分","掩码网络":"一种网络结构，用于限制模型对特定部分的注意力，确保训练过程中的一致性"}},{"order":904,"title":"A Neural Multi-Sequence Alignment TeCHnique (NeuMATCH)","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dogan_A_Neural_Multi-Sequence_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dogan_A_Neural_Multi-Sequence_CVPR_2018_paper.html","abstract":"The alignment of heterogeneous sequential data (video to text) is an important and challenging problem. Standard techniques for this task, including Dynamic Time Warping (DTW) and Conditional Random Fields (CRFs), suffer from inherent drawbacks. Mainly, the Markov assumption implies that, given the immediate past, future alignment decisions are independent of further history. The separation between similarity computation and alignment decision also prevents end-to-end training. In this paper, we propose an end-to-end neural architecture where alignment actions are implemented as moving data between stacks of Long Short-term Memory (LSTM) blocks. This flexible architecture supports a large variety of alignment tasks, including one-to-one, one-to-many, skipping unmatched elements, and (with extensions) non-monotonic alignment. Extensive experiments on semi-synthetic and real datasets show that our algorithm outperforms state-of-the-art baselines.","中文标题":"一种神经多序列对齐技术（NeuMATCH）","摘要翻译":"异构序列数据（视频到文本）的对齐是一个重要且具有挑战性的问题。用于此任务的标准技术，包括动态时间规整（DTW）和条件随机场（CRFs），都存在固有的缺点。主要是，马尔可夫假设意味着，在给定即时过去的情况下，未来的对齐决策与更远的历史无关。相似度计算和对齐决策之间的分离也阻碍了端到端的训练。在本文中，我们提出了一种端到端的神经架构，其中对齐动作通过在长短期记忆（LSTM）块堆栈之间移动数据来实现。这种灵活的架构支持多种对齐任务，包括一对一、一对多、跳过未匹配元素以及（通过扩展）非单调对齐。在半合成和真实数据集上的广泛实验表明，我们的算法优于最先进的基线。","领域":"序列对齐/视频文本对齐/神经网络","问题":"异构序列数据（如视频到文本）的对齐问题","动机":"解决现有技术（如DTW和CRFs）在处理序列对齐时的固有缺点，包括马尔可夫假设的限制和无法进行端到端训练的问题","方法":"提出了一种端到端的神经架构，通过在长短期记忆（LSTM）块堆栈之间移动数据来实现对齐动作，支持多种对齐任务","关键词":["序列对齐","视频文本对齐","LSTM","端到端训练"],"涉及的技术概念":{"动态时间规整（DTW）":"一种用于测量两个时间序列之间相似度的算法，常用于序列对齐任务","条件随机场（CRFs）":"一种统计建模方法，常用于序列标注和序列对齐任务","长短期记忆（LSTM）":"一种特殊的递归神经网络（RNN），能够学习长期依赖信息，适用于处理序列数据","端到端训练":"一种训练方法，允许模型从输入直接学习到输出，无需手动设计中间步骤或特征"}},{"order":905,"title":"Path Aggregation Network for Instance Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Path_Aggregation_Network_CVPR_2018_paper.html","abstract":"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction.  These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our PANet reach the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. PANet is also state-of-the-art on MVD and Cityscapes.","中文标题":"用于实例分割的路径聚合网络","摘要翻译":"信息在神经网络中的传播方式极为重要。在本文中，我们提出了路径聚合网络（PANet），旨在增强基于提议的实例分割框架中的信息流。具体来说，我们通过自下而上的路径增强，增强了整个特征层次结构，在较低层提供了精确的定位信号，从而缩短了较低层与最顶层特征之间的信息路径。我们提出了自适应特征池化，将特征网格与所有特征级别连接起来，使每个级别的有用信息直接传播到后续的提议子网络。为了进一步提高掩码预测，我们创建了一个补充分支，为每个提议捕捉不同的视图。这些改进实现简单，计算开销微乎其微。然而，它们非常有用，使我们的PANet在COCO 2017挑战赛的实例分割任务中获得了第一名，在对象检测任务中获得了第二名，而无需大批量训练。PANet在MVD和Cityscapes上也达到了最先进的水平。","领域":"实例分割/对象检测/特征增强","问题":"增强基于提议的实例分割框架中的信息流","动机":"提高实例分割和对象检测的准确性和效率","方法":"通过自下而上的路径增强和自适应特征池化来增强信息流，并创建补充分支以捕捉不同视图","关键词":["路径聚合网络","实例分割","对象检测","特征增强","自适应特征池化"],"涉及的技术概念":"路径聚合网络（PANet）是一种用于实例分割的神经网络架构，通过自下而上的路径增强和自适应特征池化来增强信息流。自适应特征池化是一种技术，它连接特征网格和所有特征级别，使每个级别的有用信息直接传播到后续的提议子网络。补充分支是一种技术，用于为每个提议捕捉不同的视图，以进一步提高掩码预测的准确性。"},{"order":906,"title":"The INaturalist Species Classification and Detection Dataset","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.html","abstract":"Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.","中文标题":"iNaturalist物种分类与检测数据集","摘要翻译":"现有的计算机视觉中使用的图像分类数据集往往在各个对象类别之间具有均匀分布的图像。相比之下，自然界是极度不平衡的，因为某些物种比其他物种更丰富且更容易拍摄。为了在具有挑战性的现实世界条件下鼓励进一步的进展，我们提出了iNaturalist物种分类与检测数据集，该数据集包含来自5000多种不同植物和动物物种的859,000张图像。它包含了视觉上相似的物种，这些物种是在世界各地各种情况下捕获的。图像是用不同类型的相机收集的，具有不同的图像质量，特征是大类不平衡，并且已经由多位公民科学家验证。我们讨论了数据集的收集，并展示了使用最先进的计算机视觉分类和检测模型进行的广泛基线实验。结果表明，当前的非集成方法仅达到67%的top one分类准确率，说明了数据集的难度。特别是，我们观察到对于训练样本数量较少的类别结果较差，这表明在低样本学习中需要更多的关注。","领域":"物种分类/图像检测/低样本学习","问题":"解决在现实世界中物种图像分类和检测的挑战，特别是在类不平衡和低样本学习的情况下。","动机":"自然界物种分布的不平衡性使得现有的均匀分布图像分类数据集难以有效应用于现实世界的物种分类和检测任务。","方法":"收集并构建了一个包含859,000张图像、覆盖5000多种物种的iNaturalist数据集，使用最先进的计算机视觉分类和检测模型进行基线实验。","关键词":["物种分类","图像检测","低样本学习","类不平衡"],"涉及的技术概念":{"类不平衡":"指数据集中不同类别的样本数量差异很大，这会影响模型的训练和性能。","低样本学习":"指在训练样本数量较少的情况下进行学习，这对于提高模型在稀有类别上的性能至关重要。","公民科学家":"指参与科学研究的非专业公众，他们通过收集数据等方式为科学研究做出贡献。"}},{"order":907,"title":"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.html","abstract":"Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of  attention weights or text-based generation of post-hoc  justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.","中文标题":"多模态解释：决策的正当性及证据指向","摘要翻译":"在许多场景中，既有效又可解释的深度模型是理想的；之前的可解释模型都是单模态的，提供基于图像的注意力权重可视化或基于文本的事后正当性生成。我们提出了一种多模态的解释方法，并认为这两种模态提供了互补的解释优势。我们收集了两个新的数据集来定义和评估这一任务，并提出了一个新颖的模型，该模型可以提供联合的文本理由生成和注意力可视化。我们的数据集定义了活动识别任务（ACT-X）和视觉问答任务（VQA-X）的分类决策的视觉和文本正当性。我们定量地展示了使用文本解释进行训练不仅产生了更好的文本正当性模型，而且更好地定位了支持决策的证据。我们还定性地展示了视觉解释比文本解释更有洞察力的情况，反之亦然，支持我们的论点，即多模态解释模型比单模态方法提供了显著的优势。","领域":"活动识别/视觉问答/注意力机制","问题":"如何提高深度模型的可解释性，使其不仅有效而且能够提供多模态的解释","动机":"现有的可解释模型多为单模态，无法充分利用视觉和文本信息的互补优势，因此需要开发多模态解释方法以提高模型的可解释性和效果","方法":"提出了一种多模态的解释方法，通过收集新的数据集并开发一个能够提供联合文本理由生成和注意力可视化的模型来定义和评估这一任务","关键词":["多模态解释","活动识别","视觉问答","注意力可视化","文本理由生成"],"涉及的技术概念":"多模态解释指的是结合视觉和文本信息来提供模型决策的解释。注意力可视化是一种技术，用于展示模型在处理输入数据时关注的重点区域。文本理由生成是指模型生成文本解释其决策过程。活动识别和视觉问答是两种具体的应用场景，分别涉及识别视频中的活动和回答关于图像的问题。"},{"order":908,"title":"StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.html","abstract":"Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.","中文标题":"StarGAN：用于多领域图像到图像翻译的统一生成对抗网络","摘要翻译":"最近的研究在双领域图像到图像翻译方面取得了显著成功。然而，现有方法在处理超过两个领域时，其可扩展性和鲁棒性有限，因为需要为每对图像领域独立构建不同的模型。为了解决这一限制，我们提出了StarGAN，这是一种新颖且可扩展的方法，能够仅使用单一模型执行多领域的图像到图像翻译。StarGAN的这种统一模型架构允许在单一网络内同时训练具有不同领域的多个数据集。这导致StarGAN在翻译图像的质量上优于现有模型，并且具有将输入图像灵活翻译到任何所需目标领域的新能力。我们通过面部属性转移和面部表情合成任务实证了我们的方法的有效性。","领域":"生成对抗网络/图像翻译/面部属性处理","问题":"现有方法在处理超过两个领域的图像到图像翻译时，可扩展性和鲁棒性有限","动机":"提高图像到图像翻译的可扩展性和鲁棒性，特别是在处理多领域时","方法":"提出StarGAN，一种使用单一模型进行多领域图像到图像翻译的统一生成对抗网络","关键词":["生成对抗网络","图像翻译","多领域处理","面部属性转移","面部表情合成"],"涉及的技术概念":{"生成对抗网络":"一种通过对抗过程估计生成模型的框架，由生成器和判别器组成","图像翻译":"将图像从一种表示形式转换为另一种表示形式的过程","多领域处理":"在多个不同领域之间进行数据处理或转换的能力","面部属性转移":"将一种面部属性（如表情、年龄等）从一张脸转移到另一张脸的过程","面部表情合成":"生成具有特定表情的面部图像的过程"}},{"order":909,"title":"High-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.html","abstract":"We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.","中文标题":"使用条件生成对抗网络进行高分辨率图像合成与语义操作","摘要翻译":"我们提出了一种新方法，使用条件生成对抗网络（条件GANs）从语义标签图合成高分辨率、逼真的图像。条件GANs已经实现了多种应用，但结果往往局限于低分辨率，且远未达到逼真。在这项工作中，我们通过一种新颖的对抗损失以及新的多尺度生成器和判别器架构，生成了2048x1024视觉上吸引人的结果。此外，我们通过两个附加功能扩展了我们的框架，以实现交互式视觉操作。首先，我们整合了对象实例分割信息，这使得对象操作成为可能，如移除/添加对象和更改对象类别。其次，我们提出了一种方法，在给定相同输入的情况下生成多样化的结果，允许用户交互式编辑对象外观。人类意见研究表明，我们的方法显著优于现有方法，在深度图像合成和编辑的质量和分辨率方面都有所进步。","领域":"图像合成/语义操作/生成对抗网络","问题":"如何从语义标签图合成高分辨率、逼真的图像","动机":"现有的条件生成对抗网络在图像合成方面存在分辨率低、逼真度不足的问题","方法":"采用新颖的对抗损失、多尺度生成器和判别器架构，以及整合对象实例分割信息和生成多样化结果的方法","关键词":["高分辨率图像合成","语义操作","条件生成对抗网络","对象实例分割","交互式编辑"],"涉及的技术概念":"条件生成对抗网络（Conditional GANs）是一种深度学习模型，用于生成符合特定条件的图像。多尺度生成器和判别器架构指的是在多个尺度上处理图像，以提高生成图像的质量和分辨率。对象实例分割信息是指识别图像中每个对象的精确边界，这对于对象级别的操作至关重要。"},{"order":910,"title":"Semi-Parametric Image Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Semi-Parametric_Image_Synthesis_CVPR_2018_paper.html","abstract":"We present a semi-parametric approach to photographic image synthesis from semantic layouts. The approach combines the complementary strengths of parametric and nonparametric techniques. The nonparametric component is a memory bank of image segments constructed from a training set of images. Given a novel semantic layout at test time, the memory bank is used to retrieve photographic references that are provided as source material to a deep network. The synthesis is performed by a deep network that draws on the provided photographic material. Experiments on multiple semantic segmentation datasets show that the presented approach yields considerably more realistic images than recent purely parametric techniques.","中文标题":"半参数化图像合成","摘要翻译":"我们提出了一种从语义布局进行摄影图像合成的半参数化方法。该方法结合了参数化和非参数化技术的互补优势。非参数化组件是一个由训练图像集构建的图像片段记忆库。在测试时给定一个新的语义布局，记忆库用于检索摄影参考，这些参考作为源材料提供给深度网络。合成由深度网络执行，该网络利用提供的摄影材料。在多个语义分割数据集上的实验表明，所提出的方法比最近的纯参数化技术产生了更逼真的图像。","领域":"图像合成/语义分割/深度网络","问题":"如何从语义布局生成更逼真的摄影图像","动机":"结合参数化和非参数化技术的优势，以提高图像合成的真实感","方法":"使用图像片段记忆库作为非参数化组件，结合深度网络进行图像合成","关键词":["图像合成","语义布局","深度网络","记忆库"],"涉及的技术概念":"半参数化方法结合了参数化（如深度网络）和非参数化（如图像片段记忆库）技术，通过记忆库检索摄影参考，然后由深度网络利用这些参考进行图像合成，以提高合成图像的真实感。"},{"order":911,"title":"BlockDrop: Dynamic Inference Paths in Residual Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_BlockDrop_Dynamic_Inference_CVPR_2018_paper.html","abstract":"Very deep convolutional neural networks offer excellent recognition results, yet their computational expense limits their impact for many real-world applications.  We introduce BlockDrop, an approach that learns to dynamically choose which layers of a deep network to execute during inference so as to best reduce total computation without degrading prediction accuracy.   Exploiting the robustness of Residual Networks (ResNets) to layer dropping, our framework selects on-the-fly which residual blocks to evaluate for a given novel image. In particular, given a pretrained ResNet, we train a policy network in an associative reinforcement learning setting for the dual reward of utilizing a minimal number of blocks while preserving recognition accuracy. We conduct extensive experiments on CIFAR and ImageNet.  The results provide strong quantitative and qualitative evidence that these learned policies not only accelerate inference but also encode meaningful visual information. Built upon a ResNet-101 model, our method achieves a speedup of 20% on average, going as high as 36% for some images, while maintaining the same 76.4% top-1 accuracy on ImageNet.","中文标题":"BlockDrop: 残差网络中的动态推理路径","摘要翻译":"非常深的卷积神经网络提供了出色的识别结果，但其计算成本限制了其在许多实际应用中的影响。我们引入了BlockDrop，这是一种学习在推理过程中动态选择深度网络哪些层执行的方法，以在不降低预测准确性的情况下最大限度地减少总计算量。利用残差网络（ResNets）对层丢弃的鲁棒性，我们的框架为给定的新图像即时选择要评估的残差块。特别是，给定一个预训练的ResNet，我们在关联强化学习环境中训练一个策略网络，以实现使用最少数量的块同时保持识别准确性的双重奖励。我们在CIFAR和ImageNet上进行了广泛的实验。结果提供了强有力的定量和定性证据，表明这些学习到的策略不仅加速了推理，而且编码了有意义的视觉信息。基于ResNet-101模型，我们的方法平均实现了20%的加速，对于某些图像甚至高达36%，同时在ImageNet上保持了相同的76.4%的top-1准确率。","领域":"神经网络优化/计算效率/图像识别","问题":"如何在不降低预测准确性的情况下减少深度卷积神经网络的计算成本","动机":"深度卷积神经网络虽然识别结果出色，但计算成本高，限制了其在实际应用中的使用","方法":"引入BlockDrop方法，通过训练策略网络动态选择执行哪些网络层，以减少计算量同时保持准确性","关键词":["动态推理","残差网络","计算效率","图像识别"],"涉及的技术概念":"BlockDrop是一种方法，用于在深度卷积神经网络中动态选择执行哪些层，以减少计算成本而不降低预测准确性。它利用残差网络（ResNets）对层丢弃的鲁棒性，通过训练一个策略网络在关联强化学习环境中实现使用最少数量的块同时保持识别准确性的双重奖励。"},{"order":912,"title":"Interpretable Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.html","abstract":"This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e., what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.","中文标题":"可解释的卷积神经网络","摘要翻译":"本文提出了一种将传统卷积神经网络（CNN）修改为可解释CNN的方法，以澄清CNN高卷积层中的知识表示。在可解释CNN中，高卷积层中的每个滤波器代表一个特定的对象部分。我们的可解释CNN使用与普通CNN相同的训练数据，不需要任何对象部分或纹理的注释进行监督。可解释CNN在学习过程中自动将高卷积层中的每个滤波器分配给一个对象部分。我们可以将我们的方法应用于具有各种结构的不同类型的CNN。可解释CNN中的显式知识表示可以帮助人们理解CNN内部的逻辑，即CNN为了预测记忆了哪些模式。实验表明，可解释CNN中的滤波器比传统CNN中的滤波器更具有语义意义。代码可在https://github.com/zqs1022/interpretableCNN获取。","领域":"神经网络解释性/深度学习模型/卷积神经网络","问题":"传统卷积神经网络在高卷积层中的知识表示不清晰，难以理解其内部逻辑和预测模式。","动机":"为了提高卷积神经网络的可解释性，使人们能够理解网络内部的逻辑和预测模式，从而增强对深度学习模型的信任和应用。","方法":"提出了一种将传统卷积神经网络修改为可解释CNN的方法，通过自动分配高卷积层中的滤波器到特定对象部分，无需额外监督，实现了显式的知识表示。","关键词":["神经网络解释性","卷积神经网络","知识表示"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，广泛用于图像识别和处理。本文提出的可解释CNN通过修改传统CNN，使其高卷积层中的每个滤波器代表一个特定的对象部分，从而提高了模型的可解释性。这种方法不需要额外的监督数据，如对象部分或纹理的注释，就能自动学习并分配滤波器到对象部分，使得CNN的内部逻辑和预测模式更加透明和易于理解。"},{"order":913,"title":"Deep Cross-Media Knowledge Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Deep_Cross-Media_Knowledge_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Huang_Deep_Cross-Media_Knowledge_CVPR_2018_paper.html","abstract":"Cross-media retrieval is a research hotspot in multimedia area, which aims to perform retrieval across different media types such as image and text. The performance of existing methods usually relies on labeled data for model training. However, cross-media data is very labor consuming to collect and label, so how to transfer valuable knowledge in existing data to new data is a key problem towards application. For achieving the goal, this paper proposes deep cross-media knowledge transfer (DCKT) approach, which transfers knowledge from a large-scale cross-media dataset to promote the model training on another small-scale cross-media dataset. The main contributions of DCKT are: (1) Two-level transfer architecture is proposed to jointly minimize the media-level and correlation-level domain discrepancies, which allows two important and complementary aspects of knowledge to be transferred: intra-media semantic and inter-media correlation knowledge. It can enrich the training information and boost the retrieval accuracy. (2) Progressive transfer mechanism is proposed to iteratively select training samples with ascending transfer difficulties, via the metric of cross-media domain consistency with adaptive feedback. It can drive the transfer process to gradually reduce vast cross-media domain discrepancy, so as to enhance the robustness of model training. For verifying the effectiveness of DCKT, we take the large-scale dataset XMediaNet as source domain, and 3 widely-used datasets as target domain for cross-media retrieval. Experimental results show that DCKT achieves promising improvement on retrieval accuracy.","中文标题":"深度跨媒体知识迁移","摘要翻译":"跨媒体检索是多媒体领域的一个研究热点，旨在跨越不同媒体类型（如图像和文本）进行检索。现有方法的性能通常依赖于标注数据进行模型训练。然而，跨媒体数据的收集和标注非常耗时，因此如何将现有数据中的有价值知识迁移到新数据中是应用中的一个关键问题。为了实现这一目标，本文提出了深度跨媒体知识迁移（DCKT）方法，该方法从大规模跨媒体数据集中迁移知识，以促进在另一个小规模跨媒体数据集上的模型训练。DCKT的主要贡献是：（1）提出了两级迁移架构，共同最小化媒体级和相关性级的领域差异，这使得两种重要且互补的知识能够被迁移：媒体内语义和媒体间相关性知识。这可以丰富训练信息并提高检索准确性。（2）提出了渐进式迁移机制，通过自适应反馈的跨媒体领域一致性度量，迭代选择迁移难度递增的训练样本。这可以驱动迁移过程逐步减少跨媒体领域差异，从而增强模型训练的鲁棒性。为了验证DCKT的有效性，我们采用大规模数据集XMediaNet作为源域，以及3个广泛使用的数据集作为目标域进行跨媒体检索。实验结果表明，DCKT在检索准确性上取得了显著的提升。","领域":"跨媒体检索/知识迁移/深度学习","问题":"如何有效地将大规模跨媒体数据集中的知识迁移到小规模跨媒体数据集中，以提高检索准确性","动机":"跨媒体数据的收集和标注非常耗时，现有方法依赖于标注数据进行模型训练，因此需要一种方法来迁移现有数据中的有价值知识到新数据中","方法":"提出了深度跨媒体知识迁移（DCKT）方法，包括两级迁移架构和渐进式迁移机制，以共同最小化媒体级和相关性级的领域差异，并迭代选择迁移难度递增的训练样本","关键词":["跨媒体检索","知识迁移","深度学习"],"涉及的技术概念":{"跨媒体检索":"跨越不同媒体类型（如图像和文本）进行检索","知识迁移":"将现有数据中的有价值知识迁移到新数据中","两级迁移架构":"共同最小化媒体级和相关性级的领域差异","渐进式迁移机制":"迭代选择迁移难度递增的训练样本，通过自适应反馈的跨媒体领域一致性度量"}},{"order":914,"title":"Interleaved Structured Sparse Convolutional Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xie_Interleaved_Structured_Sparse_CVPR_2018_paper.html","abstract":"In this paper, we study the problem of designing efficient convolutional neural network architectures with the interest in eliminating the redundancy in convolution kernels. In addition to structured sparse kernels, low-rank kernels and the product of low-rank kernels,the product of structured sparse kernels, which is a framework for interpreting the recently-developed interleaved group convolutions (IGC) and its variants (e.g. , Xception), has been attracting increasing interests.   Motivated by the observation that the convolutions contained in a group convolution in IGC can be further decomposed in the same manner, we present a modularized building block, {IGC-V2:}interleaved structured sparse convolutions. It generalizes interleaved group convolutions, which is composed of two structured sparse kernels, to the product of more structured sparse kernels, further eliminating the redundancy. We present the complementary condition and the balance condition to guide the design of structured sparse kernels, obtaining a balance between three aspects: model size and computation complexity and classification accuracy. Experimental results demonstrate the advantage on the balance between these three aspects compared to interleaved group convolutions and Xception and competitive performance with other state-of-the-art architecture design methods.","中文标题":"交错结构化稀疏卷积神经网络","摘要翻译":"在本文中，我们研究了设计高效卷积神经网络架构的问题，旨在消除卷积核中的冗余。除了结构化稀疏核、低秩核和低秩核的乘积外，结构化稀疏核的乘积，即解释最近开发的交错组卷积（IGC）及其变体（例如，Xception）的框架，已经引起了越来越多的兴趣。受到IGC中组卷积包含的卷积可以以相同方式进一步分解的观察启发，我们提出了一个模块化构建块，{IGC-V2：}交错结构化稀疏卷积。它将由两个结构化稀疏核组成的交错组卷积推广到更多结构化稀疏核的乘积，进一步消除冗余。我们提出了互补条件和平衡条件来指导结构化稀疏核的设计，在模型大小、计算复杂度和分类准确性三个方面之间取得平衡。实验结果表明，与交错组卷积和Xception相比，在平衡这三个方面具有优势，并且与其他最先进的架构设计方法相比具有竞争力。","领域":"卷积神经网络/模型压缩/网络架构设计","问题":"设计高效卷积神经网络架构以消除卷积核中的冗余","动机":"受到IGC中组卷积包含的卷积可以以相同方式进一步分解的观察启发","方法":"提出了一个模块化构建块，交错结构化稀疏卷积，并提出了互补条件和平衡条件来指导结构化稀疏核的设计","关键词":["卷积神经网络","模型压缩","网络架构设计","结构化稀疏核","交错组卷积"],"涉及的技术概念":"结构化稀疏核、低秩核、交错组卷积（IGC）、Xception、模型大小、计算复杂度、分类准确性"},{"order":915,"title":"A Variational U-Net for Conditional Appearance and Shape Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Esser_A_Variational_U-Net_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Esser_A_Variational_U-Net_CVPR_2018_paper.html","abstract":"Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance.  The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer.  Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art.","中文标题":"用于条件外观和形状生成的变分U-Net","摘要翻译":"深度生成模型在图像合成方面展示了卓越的性能。然而，在空间变形的情况下，结果会恶化，因为它们直接生成物体的图像，而不是建模其固有形状和外观之间复杂的相互作用。我们提出了一种条件U-Net，用于形状引导的图像生成，其条件是基于变分自编码器的外观输出。该方法在图像上进行端到端训练，不需要具有不同姿态或外观的同一物体的样本。实验表明，该模型能够实现条件图像生成和转移。因此，可以从查询图像中保留形状或外观，同时自由改变另一个。此外，由于其随机的潜在表示，可以采样外观，同时保留形状。在COCO、DeepFashion、鞋子、Market-1501和手提包上的定量和定性实验中，该方法展示了相对于现有技术的显著改进。","领域":"图像生成/形状建模/外观建模","问题":"解决在图像合成中直接生成物体图像导致的空间变形问题","动机":"为了建模物体固有形状和外观之间复杂的相互作用，提高图像合成的质量","方法":"提出了一种条件U-Net，结合变分自编码器的外观输出进行形状引导的图像生成，实现端到端训练","关键词":["图像生成","形状建模","外观建模","变分自编码器","条件U-Net"],"涉及的技术概念":{"深度生成模型":"用于图像合成的模型，能够生成高质量的图像","变分自编码器":"一种生成模型，用于学习数据的潜在表示，能够生成新的数据样本","条件U-Net":"一种改进的U-Net网络，能够根据特定条件（如形状或外观）生成图像","端到端训练":"一种训练方法，模型从输入到输出直接进行训练，无需中间步骤","空间变形":"指图像中物体形状或位置的改变，可能导致图像合成质量下降"}},{"order":916,"title":"Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Detach_and_Adapt_CVPR_2018_paper.html","abstract":"While representation learning aims to derive interpretable features for describing visual data, representation disentanglement further results in such features so that particular image attributes can be identified and manipulated. However, one cannot easily address this task without observing ground truth annotation for the training data. To address this problem, we propose a novel deep learning model of Cross-Domain Representation Disentangler (CDRD). By observing fully annotated source-domain data and unlabeled target-domain data of interest, our model bridges the information across data domains and transfers the attribute information accordingly. Thus, cross-domain joint feature disentanglement and adaptation can be jointly performed. In the experiments, we provide qualitative results to verify our disentanglement capability. Moreover, we further confirm that our model can be applied for solving classification tasks of unsupervised domain adaptation, and performs favorably against state-of-the-art image disentanglement and translation methods.","中文标题":"分离与适应：学习跨域解耦的深度表示","摘要翻译":"虽然表示学习旨在为描述视觉数据推导出可解释的特征，但表示解耦进一步导致这些特征，以便可以识别和操作特定的图像属性。然而，如果没有观察到训练数据的真实注释，人们不容易解决这个任务。为了解决这个问题，我们提出了一种新颖的深度学习模型——跨域表示解耦器（CDRD）。通过观察完全注释的源域数据和未标记的目标域数据，我们的模型桥接了跨数据域的信息，并相应地转移了属性信息。因此，可以联合执行跨域联合特征解耦和适应。在实验中，我们提供了定性结果以验证我们的解耦能力。此外，我们进一步确认我们的模型可以应用于解决无监督域适应的分类任务，并且在图像解耦和翻译方法方面表现优于最先进的方法。","领域":"跨域学习/特征解耦/无监督学习","问题":"在没有观察到训练数据的真实注释的情况下，如何实现图像属性的识别和操作","动机":"为了解决在没有真实注释的情况下实现图像属性的识别和操作的问题","方法":"提出了一种新颖的深度学习模型——跨域表示解耦器（CDRD），通过观察完全注释的源域数据和未标记的目标域数据，桥接跨数据域的信息，并相应地转移属性信息，实现跨域联合特征解耦和适应","关键词":["跨域学习","特征解耦","无监督学习","图像属性识别","图像属性操作"],"涉及的技术概念":"表示学习旨在为描述视觉数据推导出可解释的特征，表示解耦进一步导致这些特征，以便可以识别和操作特定的图像属性。跨域表示解耦器（CDRD）是一种深度学习模型，通过观察完全注释的源域数据和未标记的目标域数据，桥接跨数据域的信息，并相应地转移属性信息，实现跨域联合特征解耦和适应。"},{"order":917,"title":"Learning Deep Structured Active Contours End-to-End","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Marcos_Learning_Deep_Structured_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Marcos_Learning_Deep_Structured_CVPR_2018_paper.html","abstract":"The world is covered with millions of buildings, and precisely knowing each instance's position and extents is vital to a multitude of applications. Recently, automated building footprint segmentation models have shown superior detection accuracy thanks to the usage of Convolutional Neural Networks (CNN). However, even the latest evolutions struggle to precisely delineating borders, which often leads to geometric distortions and inadvertent fusion of adjacent building instances. We propose to overcome this issue by exploiting the distinct geometric properties of buildings. To this end, we present Deep Structured Active Contours (DSAC), a novel framework that integrates priors and constraints into the segmentation process, such as continuous boundaries, smooth edges, and sharp corners. To do so, DSAC employs Active Contour Models (ACM), a family of constraint- and prior-based polygonal models. We learn ACM parameterizations per instance using a CNN, and show how to incorporate all components in a structured output model, making DSAC trainable end-to-end. We evaluate DSAC on three challenging building instance segmentation datasets, where it compares favorably against state-of-the-art. Code will be made available.","中文标题":"学习端到端的深度结构化活动轮廓","摘要翻译":"世界上遍布着数以百万计的建筑物，准确知道每个实例的位置和范围对众多应用至关重要。最近，得益于卷积神经网络（CNN）的使用，自动化建筑物足迹分割模型显示出卓越的检测准确性。然而，即使是最新的进化也难以精确描绘边界，这常常导致几何失真和相邻建筑物实例的无意融合。我们提出通过利用建筑物的独特几何特性来克服这一问题。为此，我们提出了深度结构化活动轮廓（DSAC），这是一个新颖的框架，它将先验和约束集成到分割过程中，如连续边界、平滑边缘和锐角。为此，DSAC采用了活动轮廓模型（ACM），这是一类基于约束和先验的多边形模型。我们使用CNN学习每个实例的ACM参数化，并展示了如何将所有组件整合到一个结构化输出模型中，使DSAC可端到端训练。我们在三个具有挑战性的建筑物实例分割数据集上评估了DSAC，在这些数据集上，它与最先进的技术相比表现优异。代码将公开提供。","领域":"建筑物分割/几何建模/结构化输出模型","问题":"精确描绘建筑物边界，避免几何失真和相邻建筑物实例的无意融合","动机":"提高建筑物足迹分割的准确性，利用建筑物的独特几何特性","方法":"提出深度结构化活动轮廓（DSAC）框架，集成先验和约束到分割过程，使用CNN学习活动轮廓模型（ACM）参数化，并整合所有组件到结构化输出模型中，实现端到端训练","关键词":["建筑物分割","几何建模","结构化输出模型"],"涉及的技术概念":"卷积神经网络（CNN）用于学习活动轮廓模型（ACM）的参数化，深度结构化活动轮廓（DSAC）框架通过集成先验和约束改进分割过程，实现端到端训练。"},{"order":918,"title":"Deep Learning Under Privileged Information Using Heteroscedastic Dropout","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lambert_Deep_Learning_Under_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lambert_Deep_Learning_Under_CVPR_2018_paper.html","abstract":"Unlike machines, humans learn through rapid, abstract model-building. The role of a teacher is not simply to hammer home right or wrong answers, but rather to provide intuitive comments, comparisons, and explanations to a pupil. This is what the Learning Under Privileged Information (LUPI) paradigm endeavors to model by utilizing extra knowledge only available during training. We propose a new LUPI algorithm specifically designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a heteroscedastic dropout (ie. dropout with a varying variance) and make the variance of the dropout a function of privileged information. Intuitively, this corresponds to using the privileged information to control the uncertainty of the model output. We perform experiments using CNNs and RNNs for the tasks of image classification and machine translation. Our method significantly increases the sample efficiency during learning, resulting in higher accuracy with a large margin when the number of training examples is limited. We also theoretically justify the gains in  sample efficiency by providing a generalization error bound decreasing with O(1/n), where n is the number of training examples, in an oracle case.","中文标题":"使用异方差Dropout的特权信息下的深度学习","摘要翻译":"与机器不同，人类通过快速、抽象的模型构建来学习。教师的角色不仅仅是强调正确或错误的答案，而是向学生提供直观的评论、比较和解释。这就是特权信息学习（LUPI）范式试图通过利用仅在训练期间可用的额外知识来建模的内容。我们提出了一种新的LUPI算法，专门为卷积神经网络（CNNs）和循环神经网络（RNNs）设计。我们建议使用异方差dropout（即具有变化方差的dropout），并使dropout的方差成为特权信息的函数。直观地说，这对应于使用特权信息来控制模型输出的不确定性。我们使用CNNs和RNNs进行图像分类和机器翻译任务的实验。我们的方法显著提高了学习过程中的样本效率，在训练样本数量有限的情况下，以较大的优势提高了准确性。我们还通过提供一个在oracle情况下随O(1/n)减少的泛化误差界限，理论上证明了样本效率的提高，其中n是训练样本的数量。","领域":"卷积神经网络/循环神经网络/特权信息学习","问题":"提高在训练样本数量有限情况下的学习效率和准确性","动机":"模拟人类通过教师的直观评论、比较和解释来学习的过程，利用仅在训练期间可用的特权信息","方法":"提出了一种新的LUPI算法，使用异方差dropout，使dropout的方差成为特权信息的函数，以控制模型输出的不确定性","关键词":["异方差dropout","卷积神经网络","循环神经网络","特权信息学习","样本效率"],"涉及的技术概念":{"异方差dropout":"一种dropout方法，其方差是变化的，并且是特权信息的函数，用于控制模型输出的不确定性","卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据","循环神经网络（RNNs）":"一种深度学习模型，适用于处理序列数据，如文本或时间序列","特权信息学习（LUPI）":"一种学习范式，利用仅在训练期间可用的额外知识来提高学习效率和准确性"}},{"order":919,"title":"Smooth Neighbors on Teacher Graphs for Semi-Supervised Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Luo_Smooth_Neighbors_on_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Smooth_Neighbors_on_CVPR_2018_paper.html","abstract":"The recently proposed self-ensembling methods have achieved promising results in deep semi-supervised learning, which penalize inconsistent predictions of unlabeled data under different perturbations. However, they only consider adding perturbations to each single data point, while ignoring the connections between data samples. In this paper, we propose a novel method, called Smooth Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on the predictions of the teacher model, i.e., the implicit self-ensemble of models. Then the graph serves as a similarity measure with respect to which the representations of \\"similar\\" neighboring points are learned to be smooth on the low-dimensional manifold. We achieve state-of-the-art results on semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular, the improvements are significant when the labels are fewer. For the non-augmented MNIST with only 20 labels, the error rate is reduced from previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.","中文标题":"教师图上的平滑邻居用于半监督学习","摘要翻译":"最近提出的自集成方法在深度半监督学习中取得了有希望的结果，这些方法在不同扰动下对未标记数据的不一致预测进行惩罚。然而，它们只考虑对每个单一数据点添加扰动，而忽略了数据样本之间的连接。在本文中，我们提出了一种新方法，称为教师图上的平滑邻居（SNTG）。在SNTG中，基于教师模型的预测，即模型的隐式自集成，构建了一个图。然后，该图作为一个相似性度量，相对于该度量，学习“相似”邻居点的表示在低维流形上是平滑的。我们在半监督学习基准上取得了最先进的结果。对于CIFAR-10和SVHN，分别使用4000个标签和500个标签时，错误率为9.89%和3.99%。特别是，当标签较少时，改进显著。对于仅使用20个标签的非增强MNIST，错误率从之前的4.81%降低到1.36%。我们的方法还显示出对噪声标签的鲁棒性。","领域":"半监督学习/图神经网络/数据增强","问题":"解决在半监督学习中如何有效利用未标记数据的问题","动机":"现有的自集成方法仅对单个数据点添加扰动，忽略了数据样本之间的连接，这限制了模型的性能。","方法":"提出了一种新方法SNTG，通过构建基于教师模型预测的图，作为相似性度量，学习在低维流形上平滑的“相似”邻居点的表示。","关键词":["半监督学习","图神经网络","数据增强","教师模型","平滑邻居"],"涉及的技术概念":"自集成方法、教师模型、图构建、低维流形、平滑邻居、噪声标签鲁棒性"},{"order":920,"title":"Interpret Neural Networks by Identifying Critical Data Routing Paths","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html","abstract":"Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead.","中文标题":"通过识别关键数据路由路径解释神经网络","摘要翻译":"深度神经网络的可解释性旨在解释其决策背后的理由，并让用户理解智能代理，这在实际应用中的重要性使其成为一个重要问题。为了解决这个问题，我们开发了一种蒸馏引导路由方法，这是一个灵活的框架，通过识别关键数据路由路径并分析相应层的功能处理行为来解释深度神经网络。具体来说，我们提出在网络推断预测过程中，通过学习每层输出通道的相关控制门来发现单个输入样本的数据路由路径上的关键节点。因此，路由路径可以基于所有层的串联控制门的响应来表示，这反映了网络对输入模式的语义选择性以及跨不同层级的更详细的功能过程。基于这些发现，我们提出了一种对抗样本检测算法，通过学习一个分类器来区分关键数据路由路径是来自真实样本还是对抗样本。实验证明，我们的算法能够有效地实现高防御率，且训练开销较小。","领域":"神经网络解释性/对抗样本检测/深度学习安全","问题":"解释深度神经网络的决策理由和检测对抗样本","动机":"提高深度神经网络的可解释性，增强用户对智能代理的理解，并提高对抗样本的检测能力","方法":"开发了一种蒸馏引导路由方法，通过识别关键数据路由路径并分析相应层的功能处理行为来解释深度神经网络，并提出了一种对抗样本检测算法","关键词":["神经网络解释性","对抗样本检测","深度学习安全"],"涉及的技术概念":"深度神经网络的可解释性、蒸馏引导路由方法、关键数据路由路径、对抗样本检测算法"},{"order":921,"title":"Deep Spatio-Temporal Random Fields for Efficient Video Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chandra_Deep_Spatio-Temporal_Random_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chandra_Deep_Spatio-Temporal_Random_CVPR_2018_paper.html","abstract":"In this work we introduce a time- and  memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum, and (c) can be trained end-to-end alongside contemporary deep networks for video understanding. We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. Our implementation is based on the Caffe2 framework and will be available at https://github.com/siddharthachandra/gcrf-v3.0.","中文标题":"深度时空随机场用于高效视频分割","摘要翻译":"在本工作中，我们介绍了一种时间和内存效率高的结构化预测方法，该方法在空间和时间上耦合神经元决策。我们展示了通过利用深度高斯条件随机场（GCRFs）的最新进展，我们能够在密集连接的时空图上执行精确且高效的推理。我们的方法，称为VideoGCRF，具有以下特点：（a）高效，（b）具有唯一的全局最小值，（c）可以与当代深度网络一起进行端到端训练，用于视频理解。我们在时间域中实验了多种连接模式，并在视频的语义分割和实例分割任务上展示了相对于强基线的实证改进。我们的实现基于Caffe2框架，并将在https://github.com/siddharthachandra/gcrf-v3.0上提供。","领域":"视频理解/语义分割/实例分割","问题":"视频分割中的时间和内存效率问题","动机":"提高视频分割的效率和准确性，通过耦合空间和时间上的神经元决策","方法":"利用深度高斯条件随机场（GCRFs）进行精确且高效的推理，实验多种时间域连接模式","关键词":["视频分割","高斯条件随机场","端到端训练"],"涉及的技术概念":"深度高斯条件随机场（GCRFs）是一种用于结构化预测的模型，能够处理空间和时间上的依赖关系，通过耦合神经元决策来提高视频分割的效率和准确性。Caffe2是一个深度学习框架，支持高效的模型训练和推理。"},{"order":922,"title":"Customized Image Narrative Generation via Interactive Visual Question Generation and Answering","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Shin_Customized_Image_Narrative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Shin_Customized_Image_Narrative_CVPR_2018_paper.html","abstract":"Image description task has been invariably examined in a static manner with qualitative presumptions held to be universally applicable, regardless of the scope or target of the description. In practice, however, different viewers may pay attention to different aspects of the image, and yield different descriptions or interpretations under various contexts. Such diversity in perspectives is difficult to derive with conventional image description techniques. In this paper, we propose a customized image narrative generation task, in which the users are interactively engaged in the generation process by providing answers to the questions. We further attempt to learn the user's interest via repeating such interactive stages, and to automatically reflect the interest in descriptions for new images. Experimental results demonstrate that our model can generate a variety of descriptions from single image that cover a wider range of topics than conventional models, while being customizable to the target user of interaction.","中文标题":"通过交互式视觉问题生成与回答定制图像叙事生成","摘要翻译":"图像描述任务通常以静态方式被考察，其定性假设被认为普遍适用，无论描述的范围或目标如何。然而，在实践中，不同的观众可能会关注图像的不同方面，并在各种背景下产生不同的描述或解释。这种视角的多样性难以通过传统的图像描述技术得出。在本文中，我们提出了一个定制图像叙事生成任务，其中用户通过回答问题的方式交互式地参与到生成过程中。我们进一步尝试通过重复这样的交互阶段来学习用户的兴趣，并自动将这种兴趣反映在新图像的描述中。实验结果表明，我们的模型能够从单张图像生成涵盖更广泛主题的多种描述，同时能够根据交互的目标用户进行定制。","领域":"图像叙事生成/交互式学习/用户兴趣建模","问题":"传统图像描述技术难以捕捉不同观众对图像的多样视角和解释","动机":"为了生成更符合用户兴趣和视角的多样化图像描述","方法":"提出了一种定制图像叙事生成任务，通过交互式视觉问题生成与回答来学习用户兴趣，并自动将兴趣反映在新图像的描述中","关键词":["图像叙事生成","交互式学习","用户兴趣建模"],"涉及的技术概念":"本文涉及的技术概念包括图像描述、交互式视觉问题生成与回答、用户兴趣学习与建模。通过交互式问题生成与回答，模型能够学习到用户的兴趣点，并将这些兴趣点应用于新图像的描述生成中，从而实现定制化的图像叙事生成。"},{"order":923,"title":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_PWC-Net_CNNs_for_CVPR_2018_paper.html","abstract":"We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on our project website.","中文标题":"PWC-Net: 使用金字塔、变形和成本体积的光流CNN","摘要翻译":"我们提出了一个紧凑但有效的光流CNN模型，称为PWC-Net。PWC-Net是根据简单且成熟的原则设计的：金字塔处理、变形和使用成本体积。PWC-Net在一个可学习的特征金字塔中，使用当前的光流估计来变形第二张图像的CNN特征。然后，它使用变形后的特征和第一张图像的特征来构建一个成本体积，该体积由CNN处理以估计光流。PWC-Net的大小比最近的FlowNet2模型小17倍，并且更容易训练。此外，它在MPI Sintel最终通过和KITTI 2015基准测试上优于所有已发布的光流方法，在Sintel分辨率（1024x436）图像上运行速度约为35 fps。我们的模型可在我们的项目网站上获得。","领域":"光流估计/深度学习/计算机视觉","问题":"如何设计一个紧凑且有效的光流估计模型","动机":"为了提供一个比现有模型更小、更易于训练且性能更优的光流估计解决方案","方法":"采用金字塔处理、特征变形和成本体积构建的方法，通过CNN处理成本体积来估计光流","关键词":["光流估计","CNN","金字塔处理","特征变形","成本体积"],"涉及的技术概念":{"金字塔处理":"一种多尺度分析方法，用于在不同分辨率下处理图像特征","特征变形":"根据当前的光流估计调整图像特征的位置，以便更好地匹配另一图像的特征","成本体积":"用于衡量两幅图像特征之间匹配程度的体积数据，通过CNN处理以估计光流"}},{"order":924,"title":"Revisiting Deep Intrinsic Image Decompositions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_Revisiting_Deep_Intrinsic_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Fan_Revisiting_Deep_Intrinsic_CVPR_2018_paper.html","abstract":"While invaluable for many computer vision applications, decomposing a natural image into intrinsic reflectance and shading layers represents a challenging, underdetermined inverse problem. As opposed to strict reliance on conventional optimization or filtering solutions with strong prior assumptions, deep learning based approaches have also been proposed to compute intrinsic image decompositions when granted access to sufficient labeled training data.  The downside is that current data sources are quite limited, and broadly speaking fall into one of two categories: either dense fully-labeled images in synthetic/narrow settings, or weakly-labeled data from relatively diverse natural scenes.  In contrast to many previous learning-based approaches, which are often tailored to the structure of a particular dataset (and may not work well on others), we adopt core network structures that universally reflect loose prior knowledge regarding the intrinsic image formation process and can be largely shared across datasets.  We then apply flexibly supervised loss layers that are customized for each source of ground truth labels.  The resulting deep architecture achieves state-of-the-art results on all of the major intrinsic image benchmarks, and runs considerably faster than most at test time.","中文标题":"重新审视深度内在图像分解","摘要翻译":"尽管对于许多计算机视觉应用来说非常宝贵，将自然图像分解为内在反射和阴影层代表了一个具有挑战性的、未确定的逆问题。与严格依赖具有强先验假设的传统优化或过滤解决方案不同，基于深度学习的方法也被提出来，在获得足够的标记训练数据时计算内在图像分解。缺点是当前的数据源相当有限，大致可以分为两类：要么是在合成/狭窄环境中的密集全标记图像，要么是来自相对多样化的自然场景的弱标记数据。与许多以前的学习方法相比，这些方法通常针对特定数据集的结构进行定制（并且可能在其他数据集上效果不佳），我们采用了核心网络结构，这些结构普遍反映了关于内在图像形成过程的松散先验知识，并且可以在很大程度上跨数据集共享。然后，我们应用灵活监督的损失层，这些层针对每个地面真实标签的来源进行定制。由此产生的深度架构在所有主要的内在图像基准测试中都达到了最先进的结果，并且在测试时运行速度比大多数方法快得多。","领域":"图像分解/深度学习/计算机视觉","问题":"将自然图像分解为内在反射和阴影层的挑战性逆问题","动机":"解决当前数据源有限且方法通常针对特定数据集结构定制的问题","方法":"采用反映内在图像形成过程松散先验知识的核心网络结构，并应用灵活监督的损失层","关键词":["内在图像分解","深度学习","灵活监督损失层"],"涉及的技术概念":"内在图像分解是指将图像分解为反射和阴影层的过程，这是一个逆问题，因为需要从观察到的图像中推断出这些未观察到的层。深度学习方法是利用神经网络来学习这种分解，而灵活监督的损失层是指根据不同的地面真实标签来源定制的损失函数，用于指导网络学习。"},{"order":925,"title":"Multi-Cell Detection and Classification Using a Generative Convolutional Model","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yellin_Multi-Cell_Detection_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yellin_Multi-Cell_Detection_and_CVPR_2018_paper.html","abstract":"Detecting, counting, and classifying various cell types in images of human blood is important in many biomedical applications. However, these tasks can be very difficult due to the wide range of biological variability and the resolution limitations of many imaging modalities.  This paper proposes a new approach to detecting, counting and classifying white blood cell populations in holographic images, which capitalizes on the fact that the variability in a mixture of blood cells is constrained by physiology. The proposed approach is based on a probabilistic generative model that describes an image of a population of cells as the sum of atoms from a convolutional dictionary of cell templates. The class of each template is drawn from a prior distribution that captures statistical information about blood cell mixtures. The parameters of the prior distribution are learned from a database of complete blood count results obtained from patients, and the cell templates are learned from images of purified cells from a single cell class using an extension of convolutional dictionary learning. Cell detection, counting and classification is then done using an extension of convolutional sparse coding that accounts for class proportion priors. This method has been successfully used to detect, count and classify white blood cell populations in holographic images of lysed blood obtained from 20 normal blood donors and 12 abnormal clinical blood discard samples. The error from our method is under 6.8% for all class populations, compared to errors of over 28.6% for all other methods tested.","中文标题":"使用生成卷积模型进行多细胞检测与分类","摘要翻译":"在人类血液图像中检测、计数和分类各种细胞类型在许多生物医学应用中非常重要。然而，由于生物多样性的广泛范围和许多成像模式的分辨率限制，这些任务可能非常困难。本文提出了一种新的方法来检测、计数和分类全息图像中的白细胞群体，该方法利用了血液细胞混合物中的变异性受生理学约束的事实。所提出的方法基于一个概率生成模型，该模型将细胞群体的图像描述为来自细胞模板卷积字典的原子的总和。每个模板的类别是从一个先验分布中抽取的，该分布捕捉了关于血液细胞混合物的统计信息。先验分布的参数是从患者获得的完整血细胞计数结果数据库中学习的，而细胞模板是从单一细胞类别的纯化细胞图像中通过卷积字典学习的扩展学习的。然后使用考虑了类别比例先验的卷积稀疏编码的扩展来进行细胞检测、计数和分类。该方法已成功用于检测、计数和分类从20名正常血液捐赠者和12名异常临床血液废弃样本中获得的全息图像中的白细胞群体。与所有其他测试方法相比，我们方法的误差在所有类别群体中均低于6.8%，而其他方法的误差超过28.6%。","领域":"生物医学图像分析/细胞分类/全息成像","问题":"在人类血液图像中准确检测、计数和分类各种细胞类型","动机":"由于生物多样性和成像分辨率限制，准确检测、计数和分类血液细胞类型具有挑战性","方法":"提出了一种基于概率生成模型的新方法，利用卷积字典学习和卷积稀疏编码的扩展来检测、计数和分类白细胞群体","关键词":["生物医学图像分析","细胞分类","全息成像","卷积字典学习","卷积稀疏编码"],"涉及的技术概念":"概率生成模型、卷积字典学习、卷积稀疏编码、全息成像技术"},{"order":926,"title":"Learning Spatial-Aware Regressions for Visual Tracking","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Sun_Learning_Spatial-Aware_Regressions_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Sun_Learning_Spatial-Aware_Regressions_CVPR_2018_paper.html","abstract":"In this paper, we analyze the spatial information of deep features, and propose two complementary regressions for robust visual tracking. First, we propose a kernelized ridge regression model wherein the kernel value is defined as the weighted sum of similarity scores of all pairs of patches between two samples. We show that this model can be formulated as a neural network and thus can be efficiently solved. Second, we propose a fully convolutional neural network with spatially regularized kernels, through which the filter kernel corresponding to each output channel is forced to focus on a specific region of the target. Distance transform pooling is further exploited to determine the effectiveness of each output channel of the convolution layer. The outputs from the kernelized ridge regression model and the fully convolutional neural network are combined to obtain the ultimate response. Experimental results on two benchmark datasets validate the effectiveness of the proposed method.","中文标题":"学习空间感知回归用于视觉跟踪","摘要翻译":"在本文中，我们分析了深度特征的空间信息，并提出了两种互补的回归方法用于鲁棒的视觉跟踪。首先，我们提出了一种核化岭回归模型，其中核值被定义为两个样本之间所有补丁对相似性得分的加权和。我们展示了该模型可以表述为神经网络，因此可以高效求解。其次，我们提出了一种具有空间正则化核的全卷积神经网络，通过该网络，每个输出通道对应的滤波器核被强制聚焦于目标的特定区域。进一步利用距离变换池化来确定卷积层每个输出通道的有效性。将核化岭回归模型和全卷积神经网络的输出结合起来，以获得最终的响应。在两个基准数据集上的实验结果验证了所提出方法的有效性。","领域":"视觉跟踪/深度学习/卷积神经网络","问题":"如何利用深度特征的空间信息进行鲁棒的视觉跟踪","动机":"为了提高视觉跟踪的鲁棒性，需要有效利用深度特征的空间信息","方法":"提出了两种互补的回归方法：核化岭回归模型和具有空间正则化核的全卷积神经网络，并将两者的输出结合起来获得最终响应","关键词":["视觉跟踪","核化岭回归","全卷积神经网络","空间正则化","距离变换池化"],"涉及的技术概念":"核化岭回归模型通过加权相似性得分来定义核值，可以表述为神经网络；全卷积神经网络通过空间正则化核使滤波器核聚焦于目标的特定区域；距离变换池化用于确定卷积层输出通道的有效性。"},{"order":927,"title":"High Performance Visual Tracking With Siamese Region Proposal Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_High_Performance_Visual_CVPR_2018_paper.html","abstract":"Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in  VOT2015, VOT2016 and VOT2017 real-time challenges.","中文标题":"高性能视觉跟踪与Siamese区域提议网络","摘要翻译":"近年来，视觉对象跟踪已成为一个基础性课题，许多基于深度学习的跟踪器在多个基准测试中取得了最先进的性能。然而，这些跟踪器大多难以在实时速度下获得顶级性能。在本文中，我们提出了Siamese区域提议网络（Siamese-RPN），该网络通过大规模图像对进行端到端的离线训练。具体来说，它由用于特征提取的Siamese子网络和包括分类分支和回归分支的区域提议子网络组成。在推理阶段，所提出的框架被表述为局部一次性检测任务。我们可以预先计算Siamese子网络的模板分支，并将相关层表述为简单的卷积层以执行在线跟踪。得益于提议的细化，传统的多尺度测试和在线微调可以被丢弃。Siamese-RPN以160 FPS的速度运行，同时在VOT2015、VOT2016和VOT2017实时挑战中取得了领先的性能。","领域":"视觉对象跟踪/实时系统/深度学习应用","问题":"在保持实时速度的同时，提高视觉对象跟踪的性能","动机":"解决现有基于深度学习的跟踪器在实时速度下难以获得顶级性能的问题","方法":"提出Siamese区域提议网络（Siamese-RPN），通过大规模图像对进行端到端的离线训练，包括Siamese子网络用于特征提取和区域提议子网络用于分类和回归，推理阶段作为局部一次性检测任务处理","关键词":["视觉对象跟踪","Siamese网络","实时系统"],"涉及的技术概念":"Siamese区域提议网络（Siamese-RPN）是一种结合了Siamese网络和区域提议网络（RPN）的深度学习模型，用于视觉对象跟踪。Siamese网络用于特征提取，而RPN用于生成对象提议。通过端到端的训练，该模型能够在保持高准确率的同时实现实时跟踪。"},{"order":928,"title":"LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.html","abstract":"FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at https://github.com/twhui/LiteFlowNet.","中文标题":"LiteFlowNet: 用于光流估计的轻量级卷积神经网络","摘要翻译":"FlowNet2是目前用于光流估计的最先进的卷积神经网络（CNN），需要超过160M的参数才能实现准确的光流估计。在本文中，我们提出了一个替代网络，在具有挑战性的Sintel最终通过和KITTI基准测试上实现了与FlowNet2相当的性能，同时模型大小小了30倍，运行速度快了1.36倍。这是通过深入研究可能在当前框架中被忽略的架构细节实现的：（1）我们通过一个轻量级级联网络在每个金字塔级别上提出了一个更有效的流推断方法。它不仅通过早期校正提高了光流估计的准确性，还允许在我们的网络中无缝整合描述符匹配。（2）我们提出了一个新的流正则化层，通过使用特征驱动的局部卷积来改善异常值和模糊流边界的问题。（3）我们的网络拥有一个有效的金字塔特征提取结构，并采用特征扭曲而不是FlowNet2中实践的图像扭曲。我们的代码和训练模型可在https://github.com/twhui/LiteFlowNet获取。","领域":"光流估计/卷积神经网络/特征提取","问题":"如何在减少模型大小和提高运行速度的同时，保持或提高光流估计的准确性","动机":"现有的光流估计模型如FlowNet2虽然准确，但参数众多，模型庞大，运行速度慢，不利于实际应用","方法":"通过轻量级级联网络提高流推断效率，引入新的流正则化层改善异常值和模糊流边界问题，采用特征扭曲而非图像扭曲","关键词":["光流估计","卷积神经网络","特征提取","模型优化","运行速度"],"涉及的技术概念":"光流估计是指在视频序列中估计像素点从一帧到下一帧的运动。卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。特征提取是指从原始数据中提取有用信息的过程，以便于后续的分析和处理。模型优化涉及减少模型的复杂度和提高其运行效率，而不牺牲性能。"},{"order":929,"title":"VITAL: VIsual Tracking via Adversarial Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.html","abstract":"The tracking-by-detection framework consists of two stages, i.e., drawing samples around the target object in the first stage and classifying each sample as the target object or as background in the second stage. The performance of existing tracking-by-detection trackers using deep classification networks is limited by two aspects. First, the positive samples in each frame are highly spatially overlapped, and they fail to capture rich appearance variations. Second, there exists severe class imbalance between positive and negative samples. This paper presents the VITAL algorithm to address these two problems via adversarial learning. To augment positive samples, we use a generative network to randomly generate masks, which are applied to input features to capture a variety of appearance changes. With the use of adversarial learning, our network identifies the mask that maintains the most robust features of the target objects over a long temporal span. In addition, to handle the issue of class imbalance, we propose a high-order cost sensitive loss to decrease the effect of easy negative samples to facilitate training the classification network. Extensive experiments on benchmark datasets demonstrate that the proposed tracker performs favorably against state-of-the-art approaches.","中文标题":"VITAL: 通过对抗学习进行视觉跟踪","摘要翻译":"检测跟踪框架由两个阶段组成，即在第一阶段围绕目标对象绘制样本，在第二阶段将每个样本分类为目标对象或背景。使用深度分类网络的现有检测跟踪器的性能受到两个方面的限制。首先，每一帧中的正样本在空间上高度重叠，它们未能捕捉到丰富的外观变化。其次，正样本和负样本之间存在严重的类别不平衡。本文提出了VITAL算法，通过对抗学习来解决这两个问题。为了增加正样本，我们使用生成网络随机生成掩码，这些掩码应用于输入特征以捕捉各种外观变化。通过使用对抗学习，我们的网络识别出在长时间跨度内保持目标对象最稳健特征的掩码。此外，为了处理类别不平衡的问题，我们提出了一种高阶成本敏感损失，以减少简单负样本的影响，从而促进分类网络的训练。在基准数据集上的大量实验表明，所提出的跟踪器在性能上优于最先进的方法。","领域":"视觉跟踪/对抗学习/深度学习","问题":"现有检测跟踪器在捕捉目标对象外观变化和处理类别不平衡方面的限制","动机":"提高视觉跟踪的准确性和鲁棒性，通过对抗学习解决样本重叠和类别不平衡问题","方法":"使用生成网络随机生成掩码以增加正样本的多样性，并采用高阶成本敏感损失来处理类别不平衡","关键词":["视觉跟踪","对抗学习","样本增强","类别不平衡","成本敏感损失"],"涉及的技术概念":{"检测跟踪框架":"一种视觉跟踪方法，包括样本绘制和分类两个阶段","对抗学习":"一种机器学习方法，通过生成网络和判别网络的对抗过程来提升模型性能","高阶成本敏感损失":"一种损失函数，用于减少简单负样本对模型训练的影响，以处理类别不平衡问题"}},{"order":930,"title":"Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jiang_Super_SloMo_High_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jiang_Super_SloMo_High_CVPR_2018_paper.html","abstract":"Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. To train our network, we use 1,132 240-fps video clips, containing 300K individual video frames. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.","中文标题":"超级慢动作：高质量估计视频插值的多个中间帧","摘要翻译":"给定两个连续的帧，视频插值旨在生成中间帧以形成空间和时间上连贯的视频序列。虽然大多数现有方法专注于单帧插值，但我们提出了一种端到端的卷积神经网络，用于可变长度的多帧视频插值，其中运动解释和遮挡推理被联合建模。我们首先使用U-Net架构计算输入图像之间的双向光流。然后，在每个时间步长上线性组合这些流以近似中间的双向光流。然而，这些近似流仅在局部平滑区域表现良好，并在运动边界周围产生伪影。为了解决这一缺点，我们采用另一个U-Net来细化近似流并预测软可见性图。最后，两个输入图像被扭曲并线性融合以形成每个中间帧。通过在融合之前将可见性图应用于扭曲的图像，我们排除了被遮挡像素对插值中间帧的贡献，以避免伪影。由于我们学习的网络参数都不依赖于时间，我们的方法能够根据需要生成尽可能多的中间帧。为了训练我们的网络，我们使用了1,132个240-fps的视频剪辑，包含300K个单独的视频帧。在几个数据集上的实验结果，预测不同数量的插值帧，表明我们的方法始终优于现有方法。","领域":"视频插值/光流估计/遮挡处理","问题":"视频插值中高质量估计多个中间帧的问题","动机":"提高视频插值的质量，特别是在处理运动边界和遮挡时减少伪影","方法":"提出了一种端到端的卷积神经网络，通过计算和细化双向光流，以及预测软可见性图，来生成高质量的中间帧","关键词":["视频插值","光流估计","遮挡处理","卷积神经网络","U-Net"],"涉及的技术概念":{"视频插值":"生成视频序列中两个连续帧之间的中间帧，以提高视频的帧率或创建慢动作效果","光流估计":"计算图像序列中每个像素点的运动矢量，用于描述图像间的运动","遮挡处理":"在视频插值中处理因物体运动导致的遮挡问题，以避免插值帧中出现伪影","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征","U-Net":"一种用于图像分割的卷积神经网络架构，具有编码器-解码器结构，适用于精确的像素级预测"}},{"order":931,"title":"Real-World Repetition Estimation by Div, Grad and Curl","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Runia_Real-World_Repetition_Estimation_CVPR_2018_paper.html","abstract":"We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow and its differentials (gradient, divergence and curl) over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative.","中文标题":"通过散度、梯度和旋度估计现实世界中的重复","摘要翻译":"我们考虑了在视频中估计重复的问题，例如做俯卧撑、切瓜或拉小提琴。现有工作在静态和静止周期性的假设下显示出良好的结果。由于现实视频很少是完全静态和静止的，通常首选的基于傅里叶的测量方法不适用。相反，我们采用小波变换来更好地处理非静态和非静止的视频动态。从流场及其微分中，我们推导出三种基本运动类型和三维中内在周期性的三种运动连续性。在此基础上，对三维周期性的二维感知考虑了两个极端视角。接下来是二维中重复感知的18个基本案例。在实践中，为了处理重复外观的多样性，我们的理论意味着在分割的前景运动上测量随时间变化的流及其微分（梯度、散度和旋度）。为了实验，我们引入了新的QUVA重复数据集，通过包括非静态和非静止的视频来反映现实。在视频中计数重复的任务上，与深度学习的替代方法相比，我们获得了有利的结果。","领域":"视频分析/运动估计/周期性检测","问题":"在非静态和非静止的视频中准确估计重复动作","动机":"现实世界中的视频很少是完全静态和静止的，这使得现有的基于傅里叶的测量方法不适用，需要一种新的方法来处理这些视频动态。","方法":"采用小波变换处理非静态和非静止的视频动态，从流场及其微分中推导出基本运动类型和运动连续性，并在分割的前景运动上测量随时间变化的流及其微分。","关键词":["小波变换","流场分析","周期性检测"],"涉及的技术概念":"小波变换是一种数学工具，用于分析非静态和非静止信号。流场分析涉及对视频中物体运动的方向和速度的研究。周期性检测是指识别和测量视频中重复动作的频率和模式。"},{"order":932,"title":"Recurrent Pixel Embedding for Instance Grouping","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Kong_Recurrent_Pixel_Embedding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kong_Recurrent_Pixel_Embedding_CVPR_2018_paper.html","abstract":"We introduce a differentiable, end-to-end trainable framework for solving pixel-level grouping problems such as instance segmentation consisting of two novel components. First, we regress pixels into a hyper-spherical embedding space so that pixels from the same group have high cosine similarity while those from different groups have similarity below a specified margin. We analyze the choice of embedding dimension and margin, relating them to theoretical results on the problem of distributing points uniformly on the sphere. Second, to group instances, we utilize a variant of mean-shift clustering, implemented as a recurrent neural network parameterized by kernel bandwidth.  This recurrent grouping module is differentiable, enjoys convergent dynamics and probabilistic interpretability. Backpropagating the group-weighted loss through this module allows learning to focus on correcting embedding errors that won't be resolved during subsequent clustering. Our framework, while conceptually simple and theoretically abundant, is also practically effective and computationally efficient. We demonstrate substantial improvements over state-of-the-art instance segmentation for object proposal generation, as well as demonstrating the benefits of grouping loss for classification tasks such as boundary detection and semantic segmentation.","中文标题":"用于实例分组的循环像素嵌入","摘要翻译":"我们引入了一个可微分的、端到端可训练的框架，用于解决像素级分组问题，如实例分割，该框架由两个新颖的组件组成。首先，我们将像素回归到一个超球面嵌入空间中，使得来自同一组的像素具有较高的余弦相似度，而来自不同组的像素的相似度低于指定的边界。我们分析了嵌入维度和边界的选择，将它们与在球面上均匀分布点的问题的理论结果联系起来。其次，为了分组实例，我们利用了一种均值漂移聚类的变体，实现为由核带宽参数化的循环神经网络。这个循环分组模块是可微分的，具有收敛动力学和概率解释性。通过这个模块反向传播分组加权损失，使得学习能够集中在纠正那些在后续聚类过程中无法解决的嵌入错误上。我们的框架虽然在概念上简单且理论丰富，但在实践中也非常有效且计算效率高。我们展示了在对象提议生成方面相对于最先进的实例分割的显著改进，以及分组损失在边界检测和语义分割等分类任务中的好处。","领域":"实例分割/对象检测/语义分割","问题":"解决像素级分组问题，如实例分割","动机":"提高实例分割的准确性和效率，以及探索分组损失在分类任务中的应用","方法":"采用超球面嵌入空间和均值漂移聚类的变体，实现为循环神经网络，以解决像素级分组问题","关键词":["实例分割","超球面嵌入","均值漂移聚类","循环神经网络","对象提议生成","边界检测","语义分割"],"涉及的技术概念":"超球面嵌入空间是一种将数据点映射到高维球面上的技术，用于提高数据点之间的区分度。均值漂移聚类是一种基于密度的非参数聚类方法，用于发现数据中的密集区域。循环神经网络是一种具有循环连接的神经网络，能够处理序列数据，适用于时间序列分析或序列到序列的映射任务。"},{"order":933,"title":"Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Unsupervised_Saliency_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Deep_Unsupervised_Saliency_CVPR_2018_paper.html","abstract":"The success of current deep saliency detection methods heavily depends on the availability of large-scale supervision in the form of per-pixel labeling. Such supervision, while labor-intensive and not always possible, tends to hinder the generalization ability of the learned models. By contrast, traditional handcrafted features based unsupervised saliency detection methods, even though have been surpassed by the deep supervised methods, are generally dataset-independent and could be applied in the wild. This raises a natural question that \`\`Is it possible to learn saliency maps without using labeled data while improving the generalization ability?''. To this end, we present a novel perspective to unsupervised saliency detection through learning from multiple noisy labeling generated by \`\`weak'' and \`\`noisy'' unsupervised handcrafted saliency methods. Our end-to-end deep learning framework for unsupervised saliency detection consists of a latent saliency prediction module and a noise modeling module that work collaboratively and are optimized jointly. Explicit noise modeling enables us to deal with noisy saliency maps in a probabilistic way. Extensive experimental results on various benchmarking datasets show that our model not only outperforms all the unsupervised saliency methods with a large margin but also achieves comparable performance with the recent state-of-the-art supervised deep saliency methods.","中文标题":"深度无监督显著性检测：多噪声标签视角","摘要翻译":"当前深度显著性检测方法的成功在很大程度上依赖于以每像素标签形式的大规模监督的可用性。这种监督虽然劳动密集且并非总是可行，但往往会阻碍学习模型的泛化能力。相比之下，基于传统手工特征的无监督显著性检测方法，尽管已被深度监督方法超越，但通常与数据集无关，可以在野外应用。这引发了一个自然的问题：“是否可以在不使用标记数据的情况下学习显著性图，同时提高泛化能力？”。为此，我们提出了一种新的视角，通过从“弱”和“噪声”无监督手工显著性方法生成的多个噪声标签中学习，来进行无监督显著性检测。我们的端到端深度学习框架用于无监督显著性检测，包括一个潜在显著性预测模块和一个噪声建模模块，它们协同工作并共同优化。显式噪声建模使我们能够以概率方式处理噪声显著性图。在各种基准数据集上的广泛实验结果表明，我们的模型不仅大幅超越了所有无监督显著性方法，而且与最近的最先进的监督深度显著性方法相比，也达到了可比的性能。","领域":"显著性检测/无监督学习/深度学习","问题":"如何在无监督的情况下提高显著性检测的泛化能力","动机":"减少对大规模监督数据的依赖，提高显著性检测模型的泛化能力","方法":"提出了一种新的视角，通过从多个噪声标签中学习，开发了一个端到端的深度学习框架，包括潜在显著性预测模块和噪声建模模块","关键词":["显著性检测","无监督学习","深度学习","噪声建模","泛化能力"],"涉及的技术概念":"显著性检测是指在图像中自动识别出最吸引人注意的区域。无监督学习是一种机器学习方法，它不依赖于标记数据来训练模型。深度学习是一种基于人工神经网络的机器学习方法，能够学习数据的高层次特征。噪声建模是指对数据中的噪声进行建模，以便更好地处理和分析数据。泛化能力是指模型在未见过的数据上的表现能力。"},{"order":934,"title":"Learning Intrinsic Image Decomposition From Watching the World","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Intrinsic_Image_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Learning_Intrinsic_Image_CVPR_2018_paper.html","abstract":"Single-view intrinsic image decomposition is a highly ill-posed problem, making learning from large amounts of data an attractive approach. However, it is difficult to collect ground truth training data at scale for intrinsic images. In this paper, we explore a different approach to learning intrinsic images: observing image sequences over time depicting the same scene under changing illumination, and learning single-view decompositions that are consistent with these changes. This approach allows us to learn without ground truth decompositions, and instead to exploit information available from multiple images. Our trained model can then be applied at test time to single views. We describe a new learning framework based on this idea, including new loss functions that can be efficiently evaluated over entire sequences. While prior learning-based intrinsic image methods achieve good performance on specific benchmarks, we show that our approach generalizes well to several diverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild and Shading Annotations in the Wild.","中文标题":"从观察世界中学习本征图像分解","摘要翻译":"单视图本征图像分解是一个高度不适定问题，使得从大量数据中学习成为一种有吸引力的方法。然而，大规模收集本征图像的真实训练数据是困难的。在本文中，我们探索了一种不同的学习本征图像的方法：观察随时间变化的图像序列，这些序列描绘了在变化光照下的同一场景，并学习与这些变化一致的单视图分解。这种方法使我们能够在没有真实分解的情况下学习，而是利用从多幅图像中可获得的信息。我们训练好的模型可以在测试时应用于单视图。我们描述了一个基于这一思想的新学习框架，包括可以在整个序列上有效评估的新损失函数。虽然之前基于学习的本征图像方法在特定基准上取得了良好的性能，但我们展示了我们的方法在多个不同数据集上具有良好的泛化能力，包括MIT本征图像、野外本征图像和野外阴影注释。","领域":"本征图像分解/图像序列分析/光照变化分析","问题":"单视图本征图像分解的高度不适定问题","动机":"由于难以大规模收集本征图像的真实训练数据，探索一种无需真实分解即可学习本征图像的方法","方法":"通过观察随时间变化的图像序列，学习与光照变化一致的单视图分解，并开发新的学习框架和损失函数","关键词":["本征图像分解","图像序列","光照变化"],"涉及的技术概念":"本征图像分解指的是将图像分解为反射率和光照两个部分的过程。图像序列分析涉及对一系列图像进行时间上的分析，以提取场景或对象的动态信息。光照变化分析关注于图像中由于光照条件变化而引起的视觉变化，这对于理解场景的物理属性非常重要。"},{"order":935,"title":"TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_TieNet_Text-Image_Embedding_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_TieNet_Text-Image_Embedding_CVPR_2018_paper.html","abstract":"Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reports can be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).","中文标题":"TieNet: 用于胸部X光常见疾病分类和报告的文本-图像嵌入网络","摘要翻译":"胸部X光是日常临床检查中最常见的放射学检查之一。使用胸部X光报告胸部疾病通常是放射科实习生的入门任务。然而，对于以学习为导向的机器智能来说，阅读胸部X光图像仍然是一项具有挑战性的工作，原因在于：(1) 缺乏大规模可机器学习的医学图像数据集，(2) 缺乏能够模仿人类放射科医生高水平推理的技术，这需要多年的知识积累和专业训练。在本文中，我们展示了临床自由文本放射学报告可以作为解决这两个关键问题的先验知识。我们提出了一种新颖的文本-图像嵌入网络（TieNet），用于提取独特的图像和文本表示。多层次注意力模型被集成到一个端到端可训练的CNN-RNN架构中，以突出有意义的文本单词和图像区域。我们首先应用TieNet通过使用从相关报告中提取的图像特征和文本嵌入来分类胸部X光。所提出的自动注释框架在为我们的手工标签评估数据集分配疾病标签时实现了高准确率（AUCs平均超过0.9）。此外，我们将TieNet转化为一个胸部X光报告系统。它模拟了报告过程，并可以一起输出疾病分类和初步报告。与最先进的基线相比，在一个未见过的和手工标记的数据集（OpenI）上，分类结果显著改善（AUCs平均增加6%）。","领域":"医学影像分析/自然语言处理/深度学习","问题":"胸部X光图像的疾病分类和报告生成","动机":"解决胸部X光图像分析中大规模数据集缺乏和模仿人类放射科医生高水平推理的技术缺乏的问题","方法":"提出了一种新颖的文本-图像嵌入网络（TieNet），集成多层次注意力模型于CNN-RNN架构中，用于提取图像和文本的独特表示，并应用于疾病分类和报告生成","关键词":["胸部X光","疾病分类","报告生成","文本-图像嵌入","注意力模型"],"涉及的技术概念":"TieNet是一种结合了卷积神经网络（CNN）和循环神经网络（RNN）的深度学习模型，通过集成多层次注意力模型来提取图像和文本的独特表示，用于胸部X光图像的疾病分类和报告生成。这种方法利用了临床自由文本放射学报告作为先验知识，以解决医学影像分析中的关键问题。"},{"order":936,"title":"Generating Synthetic X-Ray Images of a Person From the Surface Geometry","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Teixeira_Generating_Synthetic_X-Ray_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Teixeira_Generating_Synthetic_X-Ray_CVPR_2018_paper.html","abstract":"We present a novel framework that learns to predict human anatomy from body surface. Specifically, our approach generates a synthetic X-ray image of a person only from the person's surface geometry. Furthermore, the synthetic X-ray image is parametrized and can be manipulated by adjusting a set of body markers which are also generated during the X-ray image prediction. With the proposed framework, multiple synthetic X-ray images can easily be generated by varying surface geometry. By perturbing the parameters, several additional synthetic X-ray images can be generated from the same surface geometry. As a result, our approach offers a potential to overcome the training data barrier in the medical domain. This capability is achieved by learning a pair of networks - one learns to generate the full image from the partial image and a set of parameters, and the other learns to estimate the parameters given the full image. During training, the two networks are trained iteratively such that they would converge to a solution where the predicted parameters and the full image are consistent with each other. In addition to medical data enrichment, our framework can also be used for image completion as well as anomaly detection.","中文标题":"从表面几何生成人体合成X射线图像","摘要翻译":"我们提出了一个新颖的框架，该框架能够从人体表面学习预测人体解剖结构。具体来说，我们的方法仅从人的表面几何生成合成X射线图像。此外，合成X射线图像是参数化的，并且可以通过调整一组在X射线图像预测过程中生成的身体标记来进行操作。通过所提出的框架，可以通过改变表面几何轻松生成多个合成X射线图像。通过扰动参数，可以从相同的表面几何生成几个额外的合成X射线图像。因此，我们的方法提供了克服医学领域训练数据障碍的潜力。这一能力是通过学习一对网络实现的——一个网络学习从部分图像和一组参数生成完整图像，另一个网络学习在给定完整图像的情况下估计参数。在训练过程中，这两个网络被迭代训练，以便它们能够收敛到一个解决方案，其中预测的参数和完整图像彼此一致。除了医学数据丰富之外，我们的框架还可以用于图像完成以及异常检测。","领域":"医学影像处理/图像生成/深度学习","问题":"如何从人体表面几何生成合成X射线图像","动机":"克服医学领域训练数据的障碍，提供数据丰富、图像完成和异常检测的能力","方法":"学习一对网络，一个从部分图像和参数生成完整图像，另一个从完整图像估计参数，通过迭代训练使预测的参数和完整图像一致","关键词":["合成X射线图像","表面几何","参数化","图像生成","异常检测"],"涉及的技术概念":"该框架涉及的技术概念包括从表面几何生成合成X射线图像、参数化图像生成、通过调整身体标记操作图像、以及使用深度学习网络进行图像生成和参数估计。"},{"order":937,"title":"Gibson Env: Real-World Perception for Embodied Agents","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Xia_Gibson_Env_Real-World_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Xia_Gibson_Env_Real-World_CVPR_2018_paper.html","abstract":"Perception and being active (having a certain level of motion freedom) are closely tied. Learning active perception and sensorimotor control in the physical world is cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning in simulation which consequently casts a question on transferring to real-world. In this paper, we investigate learning a real-world perception for active agents, propose Gibson virtual environment for this purpose, and showcase a set of learned complex locomotion abilities. The primary characteristics of the learning environments, which transfer into the trained agents, are I) being from the real-world and reflecting its semantic complexity, II) having a mechanism to ensure no need to further domain adaptation prior to deployment of results in real-world, III) embodiment of the agent and making it subject to constraints of space and physics.","中文标题":"Gibson环境：为具身代理提供真实世界感知","摘要翻译":"感知与活跃性（具有一定程度的运动自由）密切相关。在物理世界中学习主动感知和感觉运动控制是繁琐的，因为现有算法太慢，无法实时高效学习，而且机器人脆弱且成本高。这导致了在模拟中学习的兴起，从而引发了一个关于转移到现实世界的问题。在本文中，我们研究了为活跃代理学习真实世界感知，为此提出了Gibson虚拟环境，并展示了一组学习到的复杂运动能力。学习环境的主要特征，这些特征转移到训练过的代理中，包括：I）来自现实世界并反映其语义复杂性，II）有一种机制确保在将结果部署到现实世界之前不需要进一步的领域适应，III）代理的具身化并使其受到空间和物理约束。","领域":"具身智能/虚拟现实/机器人学","问题":"如何在模拟环境中有效学习真实世界的感知和运动控制，以便于转移到现实世界中的具身代理。","动机":"由于现有算法在物理世界中学习主动感知和感觉运动控制效率低下，且机器人成本高、易损坏，因此转向模拟环境学习成为一种解决方案。然而，这带来了如何将学习成果有效转移到现实世界的问题。","方法":"提出了Gibson虚拟环境，该环境能够反映现实世界的语义复杂性，并确保学习成果在部署到现实世界前无需进一步领域适应。同时，通过具身化代理并使其受到空间和物理约束，来学习复杂运动能力。","关键词":["具身智能","虚拟现实","机器人学","感知学习","运动控制"],"涉及的技术概念":"主动感知、感觉运动控制、模拟学习、领域适应、具身化代理、语义复杂性、空间和物理约束"},{"order":938,"title":"Reinforcement Cutting-Agent Learning for Video Object Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Han_Reinforcement_Cutting-Agent_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Han_Reinforcement_Cutting-Agent_Learning_CVPR_2018_paper.html","abstract":"Video object segmentation is a fundamental yet challenging task in computer vision community. In this paper, we formulate this problem as a Markov Decision Process, where agents are learned to segment object regions under a deep reinforcement learning framework. Essentially, learning agents for segmentation is nontrivial as segmentation is a nearly continuous decision-making process, where the number of the involved agents (pixels or superpixels) and action steps from the seed (super)pixels to the whole object mask might be incredibly huge. To overcome this difficulty, this paper simplifies the learning of segmentation agents to the learning of a cutting-agent, which only has a limited number of action units and can converge in just a few action steps. The basic assumption is that object segmentation mainly relies on the interaction between object regions and their context. Thus, with an optimal object (box) region and context (box) region, we can obtain the desirable segmentation mask through further inference. Based on this assumption, we establish a novel reinforcement cutting-agent learning framework, where the cutting-agent consists of a cutting-policy network and a cutting-execution network. The former learns policies for deciding optimal object-context box pair, while the latter executing the cutting function based on the inferred object-context box pair. With the collaborative interaction between the two networks, our method can achieve the outperforming VOS performance on two public benchmarks, which demonstrates the rationality of our assumption as well as the effectiveness of the proposed learning framework.","中文标题":"强化切割代理学习用于视频对象分割","摘要翻译":"视频对象分割是计算机视觉社区中一个基础但具有挑战性的任务。在本文中，我们将这个问题表述为一个马尔可夫决策过程，其中代理在深度强化学习框架下学习分割对象区域。本质上，学习分割代理并非易事，因为分割是一个几乎连续的决策过程，涉及的代理（像素或超像素）数量以及从种子（超）像素到整个对象掩码的动作步骤可能极其庞大。为了克服这一困难，本文将分割代理的学习简化为切割代理的学习，切割代理只有有限数量的动作单元，并且可以在仅几个动作步骤内收敛。基本假设是对象分割主要依赖于对象区域与其上下文之间的交互。因此，通过一个最优的对象（框）区域和上下文（框）区域，我们可以通过进一步的推理获得理想的分割掩码。基于这一假设，我们建立了一个新颖的强化切割代理学习框架，其中切割代理由切割策略网络和切割执行网络组成。前者学习决定最优对象-上下文框对的策略，而后者基于推断的对象-上下文框对执行切割功能。通过两个网络之间的协作交互，我们的方法可以在两个公共基准上实现卓越的视频对象分割性能，这证明了我们假设的合理性以及所提出学习框架的有效性。","领域":"视频对象分割/强化学习/马尔可夫决策过程","问题":"视频对象分割中的连续决策过程复杂性","动机":"简化分割代理学习过程，提高分割效率和准确性","方法":"建立强化切割代理学习框架，包括切割策略网络和切割执行网络","关键词":["视频对象分割","强化学习","马尔可夫决策过程","切割代理"],"涉及的技术概念":{"马尔可夫决策过程":"一种数学框架，用于建模决策问题，其中结果部分随机，部分由决策者控制。","深度强化学习":"结合深度学习和强化学习的方法，用于解决复杂的决策问题。","切割策略网络":"学习决定最优对象-上下文框对的策略的网络。","切割执行网络":"基于推断的对象-上下文框对执行切割功能的网络。"}},{"order":939,"title":"Feature Space Transfer for Data Augmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Feature_Space_Transfer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Feature_Space_Transfer_CVPR_2018_paper.html","abstract":"The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.","中文标题":"特征空间转移用于数据增强","摘要翻译":"本文考虑了特征空间中的数据增强问题。提出了一种新的架构，称为特征转移网络（FATTEN），用于建模由物体姿态变化引起的特征轨迹。该架构利用姿态和外观的参数化来参数化姿态流形。这导致了一个深度编码器/解码器网络架构，其中编码器分解为外观和姿态预测器。与之前的轨迹转移尝试不同，FATTEN可以有效地进行端到端训练，无需训练单独的特征转移函数。这是通过向解码器提供目标姿态信息并使用多任务损失来实现的，该损失惩罚类别和姿态不匹配。因此，FATTEN阻止了无法捕捉姿态流形结构的不连续或非平滑轨迹，并在涉及大姿态变化的物体识别任务中表现出良好的泛化能力。在人工ModelNet数据库上的实验结果表明，它能够成功学习将源特征映射到所需姿态的目标特征，同时保持类别身份。最值得注意的是，通过在SUN-RGBD对象上使用特征空间转移进行数据增强（关于姿态和深度），我们在转移学习设置中展示了一/少样本物体识别性能的显著提升，与当前最先进的方法相比。","领域":"物体识别/姿态估计/数据增强","问题":"如何在特征空间中进行有效的数据增强，特别是在物体姿态变化的情况下。","动机":"为了解决在物体识别任务中，由于姿态变化导致的识别性能下降问题，以及提高一/少样本学习场景下的识别准确率。","方法":"提出了一种新的特征转移网络（FATTEN）架构，该架构通过参数化姿态流形，利用深度编码器/解码器网络来建模特征轨迹，并通过多任务损失函数进行端到端训练。","关键词":["特征空间转移","数据增强","姿态估计","物体识别","一/少样本学习"],"涉及的技术概念":"特征转移网络（FATTEN）是一种深度编码器/解码器网络架构，用于建模由物体姿态变化引起的特征轨迹。它通过参数化姿态流形，利用多任务损失函数进行端到端训练，无需单独训练特征转移函数。这种方法能够有效捕捉姿态流形结构，提高物体识别任务中的泛化能力。"},{"order":940,"title":"Analytic Expressions for Probabilistic Moments of PL-DNN With Gaussian Input","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bibi_Analytic_Expressions_for_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bibi_Analytic_Expressions_for_CVPR_2018_paper.html","abstract":"The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of re- search that aims to understand and analyze the driving reasons behind the effectiveness of these networks. One important aspect of this analysis has recently gained much attention, namely the reaction of a DNN to noisy input. This has spawned research on developing adversarial input attacks as well as training strategies that make DNNs more robust against these attacks. To this end, we derive in this pa- per exact analytic expressions for the first and second moments (mean and variance) of a small piecewise linear (PL) network (Affine, ReLU, Affine) subject to general Gaussian input. We experimentally show that these expressions are tight under simple linearizations of deeper PL-DNNs, especially popular architectures in the literature (e.g. LeNet and AlexNet). Extensive experiments on image classification show that these expressions can be used to study the behaviour of the output mean of the logits for each class, the interclass confusion and the pixel-level spatial noise sensitivity of the network. Moreover, we show how these expressions can be used to systematically construct targeted and non-targeted adversarial attacks.","中文标题":"高斯输入下分段线性深度神经网络概率矩的解析表达式","摘要翻译":"深度神经网络（DNNs）在视觉识别任务中的出色表现已在多个大规模基准测试中得到证明。这一表现极大地加强了旨在理解和分析这些网络有效性背后驱动原因的研究路线。最近，这一分析的一个重要方面受到了广泛关注，即DNN对噪声输入的反应。这催生了关于开发对抗性输入攻击以及使DNN对这些攻击更加鲁棒的训练策略的研究。为此，我们在本文中推导出了受一般高斯输入影响的小型分段线性（PL）网络（Affine, ReLU, Affine）的第一和第二矩（均值和方差）的精确解析表达式。我们通过实验证明，这些表达式在更深层次的PL-DNNs的简单线性化下是紧的，尤其是在文献中流行的架构（例如LeNet和AlexNet）下。在图像分类上的广泛实验表明，这些表达式可用于研究每个类别的logits输出均值的行为、类间混淆以及网络的像素级空间噪声敏感性。此外，我们还展示了如何利用这些表达式系统地构建有针对性和无针对性的对抗性攻击。","领域":"对抗性攻击/网络鲁棒性/图像分类","问题":"研究深度神经网络在噪声输入下的行为及其对抗性攻击的鲁棒性","动机":"理解和分析深度神经网络在视觉识别任务中有效性的驱动原因，特别是其对噪声输入的反应","方法":"推导出受高斯输入影响的小型分段线性网络的第一和第二矩的精确解析表达式，并通过实验验证这些表达式在更深层次网络中的适用性","关键词":["对抗性攻击","网络鲁棒性","图像分类","分段线性网络","高斯输入"],"涉及的技术概念":{"深度神经网络（DNNs）":"一种模拟人脑神经网络结构和功能的计算模型，用于处理复杂的模式识别和分类任务。","分段线性（PL）网络":"一种神经网络，其激活函数为分段线性函数，如ReLU。","高斯输入":"输入数据服从高斯（正态）分布。","对抗性攻击":"一种通过添加微小扰动来欺骗神经网络的技术，使其做出错误预测。","logits":"在分类问题中，模型最后一层未经过softmax处理的输出，通常用于表示每个类别的得分。"}},{"order":941,"title":"Detail-Preserving Pooling in Deep Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Saeedan_Detail-Preserving_Pooling_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Saeedan_Detail-Preserving_Pooling_in_CVPR_2018_paper.html","abstract":"Most convolutional neural networks use some method for gradually downscaling the size of the hidden layers. This is commonly referred to as pooling, and is applied to reduce the number of parameters, improve invariance to certain distortions, and increase the receptive field size. Since pooling by nature is a lossy process, it is crucial that each such layer maintains the portion of the activations that is most important for the network's discriminability. Yet, simple maximization or averaging over blocks, max or average pooling, or plain downsampling in the form of strided convolutions are the standard. In this paper, we aim to leverage recent results on image downscaling for the purposes of deep learning. Inspired by the human visual system, which focuses on local spatial changes, we propose detail-preserving pooling (DPP), an adaptive pooling method that magnifies spatial changes and preserves important structural detail. Importantly, its parameters can be learned jointly with the rest of the network. We analyze some of its theoretical properties and show its empirical benefits on several datasets and networks, where DPP consistently outperforms previous pooling approaches.","中文标题":"深度网络中的细节保留池化","摘要翻译":"大多数卷积神经网络使用某种方法逐渐缩小隐藏层的大小。这通常被称为池化，用于减少参数数量、提高对某些失真的不变性以及增加感受野大小。由于池化本质上是一个有损过程，因此每一层保持对网络区分能力最重要的激活部分至关重要。然而，简单的最大化或块内平均、最大或平均池化，或以步幅卷积形式的简单下采样是标准做法。在本文中，我们旨在利用最近关于图像下采样的结果用于深度学习。受到人类视觉系统的启发，该系统专注于局部空间变化，我们提出了细节保留池化（DPP），一种自适应池化方法，它放大空间变化并保留重要的结构细节。重要的是，其参数可以与网络的其他部分联合学习。我们分析了它的一些理论特性，并在多个数据集和网络上展示了其经验优势，其中DPP始终优于以前的池化方法。","领域":"卷积神经网络/图像下采样/自适应池化","问题":"如何在卷积神经网络中进行有效的下采样，同时保留重要的结构细节","动机":"现有的池化方法（如最大池化、平均池化）虽然能减少参数数量和增加感受野大小，但会导致重要信息的丢失，影响网络的区分能力","方法":"提出了一种名为细节保留池化（DPP）的自适应池化方法，该方法放大空间变化并保留重要的结构细节，其参数可以与网络的其他部分联合学习","关键词":["卷积神经网络","图像下采样","自适应池化","细节保留"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，广泛用于图像识别和分类任务。池化是CNN中的一种操作，用于减少数据的空间尺寸，从而减少计算量和参数数量，同时增加感受野。最大池化和平均池化是两种常见的池化方法，但它们可能会导致重要信息的丢失。细节保留池化（DPP）是一种新的池化方法，旨在保留图像中的重要细节，同时减少数据尺寸。"},{"order":942,"title":"Rethinking Feature Distribution for Loss Functions in Image Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wan_Rethinking_Feature_Distribution_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wan_Rethinking_Feature_Distribution_CVPR_2018_paper.html","abstract":"We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural networks in classification tasks. Different from the softmax cross-entropy loss, our proposal is established on the assumption that the deep features of the training set follow a Gaussian Mixture distribution. By involving a classification margin and a likelihood regularization, the L-GM loss facilitates both a high classification performance and an accurate modeling of the training feature distribution. As such, the L-GM loss is superior to the softmax loss and its major variants in the sense that besides classification, it can be readily used to distinguish abnormal inputs, such as the adversarial examples, based on their features' likelihood to the training feature distribution. Extensive experiments on various recognition benchmarks like MNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate the effectiveness of our proposal.","中文标题":"重新思考图像分类中损失函数的特征分布","摘要翻译":"我们提出了一种用于深度神经网络分类任务的大间隔高斯混合（L-GM）损失。与softmax交叉熵损失不同，我们的建议基于训练集的深度特征遵循高斯混合分布的假设。通过引入分类间隔和似然正则化，L-GM损失既促进了高分类性能，又准确建模了训练特征分布。因此，L-GM损失优于softmax损失及其主要变体，因为除了分类之外，它还可以基于其特征对训练特征分布的似然性，轻松用于区分异常输入，如对抗样本。在MNIST、CIFAR、ImageNet和LFW等各种识别基准以及对抗样本上的大量实验证明了我们建议的有效性。","领域":"图像分类/异常检测/对抗样本","问题":"提高深度神经网络在图像分类任务中的性能，并准确建模训练特征分布","动机":"现有的softmax交叉熵损失在分类任务中表现良好，但在准确建模训练特征分布和区分异常输入方面存在不足","方法":"提出了一种基于高斯混合分布假设的大间隔高斯混合（L-GM）损失，通过引入分类间隔和似然正则化来提高分类性能和特征分布建模的准确性","关键词":["大间隔高斯混合损失","分类间隔","似然正则化"],"涉及的技术概念":"L-GM损失是一种新的损失函数，基于高斯混合分布假设，通过引入分类间隔和似然正则化来提高深度神经网络在图像分类任务中的性能，并准确建模训练特征分布。这种方法不仅提高了分类性能，还能有效区分异常输入，如对抗样本。"},{"order":943,"title":"Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Shift_A_Zero_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Shift_A_Zero_CVPR_2018_paper.html","abstract":"Neural networks rely on convolutions to aggregate spatial information. However, spatial convolutions are expensive in terms of model size and computation, both of which grow quadratically with respect to kernel size. In this paper, we present a parameter-free, FLOP-free \\"shift\\" operation as an alternative to spatial convolutions. We fuse shifts and point-wise convolutions to construct end-to-end trainable shift-based modules, with a hyperparameter characterizing the tradeoff between accuracy and efficiency. To demonstrate the operation's efficacy, we replace ResNet's 3x3 convolutions with shift-based modules for improved CIFAR-10 and CIFAR-100 accuracy using 60% fewer parameters; we additionally demonstrate the operation's resilience to parameter reduction on ImageNet, outperforming ResNet family members despite having millions fewer parameters. We further design a family of neural networks called ShiftNet, which achieve strong performance on classification, face verification and style transfer while demanding many fewer parameters.","中文标题":"Shift: 一种零FLOP、零参数的空间卷积替代方案","摘要翻译":"神经网络依赖卷积来聚合空间信息。然而，空间卷积在模型大小和计算方面都非常昂贵，这两者都随着核大小的增加而呈二次增长。在本文中，我们提出了一种无参数、无FLOP的“shift”操作作为空间卷积的替代方案。我们将shift操作和点卷积融合，构建了端到端可训练的基于shift的模块，并通过一个超参数来表征准确性和效率之间的权衡。为了证明该操作的有效性，我们用基于shift的模块替换了ResNet的3x3卷积，在使用60%更少参数的情况下提高了CIFAR-10和CIFAR-100的准确性；我们还在ImageNet上展示了该操作对参数减少的韧性，尽管参数数量减少了数百万，但仍优于ResNet系列成员。我们进一步设计了一系列名为ShiftNet的神经网络，这些网络在分类、面部验证和风格转换方面表现出色，同时需要的参数数量大大减少。","领域":"卷积神经网络优化/模型压缩/高效深度学习","问题":"空间卷积在模型大小和计算上的高成本问题","动机":"减少神经网络模型的大小和计算成本，同时保持或提高模型的性能","方法":"提出了一种无参数、无FLOP的shift操作，并将其与点卷积融合构建端到端可训练的基于shift的模块","关键词":["卷积神经网络","模型压缩","高效计算"],"涉及的技术概念":"空间卷积、FLOP（浮点运算次数）、点卷积、ResNet、CIFAR-10、CIFAR-100、ImageNet、ShiftNet"},{"order":944,"title":"Sketch-a-Classifier: Sketch-Based Photo Classifier Generation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Sketch-a-Classifier_Sketch-Based_Photo_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Sketch-a-Classifier_Sketch-Based_Photo_CVPR_2018_paper.html","abstract":"Contemporary deep learning techniques have made image recognition a reasonably reliable technology. However training effective photo classifiers typically takes numerous examples which limits image recognition's scalability and applicability to scenarios where images may not be available. This has motivated investigation into zero-shot learning, which addresses the issue via knowledge transfer from other modalities such as text. In this paper we investigate an alternative approach of synthesizing image classifiers: almost directly from a user's imagination, via free-hand sketch. This approach doesn't require the category to be nameable or describable via attributes as per zero-shot learning. We achieve this via training a model regression network to map from free-hand sketch space to the space of photo classifiers. It turns out that this mapping can be learned in a category-agnostic way, allowing photo classifiers for new categories to be synthesized by user with no need for annotated training photos. We also demonstrate that this modality of classifier generation can also be used to enhance the granularity of an existing photo classifier, or as a complement to name-based zero-shot learning.","中文标题":"草图分类器：基于草图的照片分类器生成","摘要翻译":"当代深度学习技术已经使图像识别成为一种相对可靠的技术。然而，训练有效的照片分类器通常需要大量的例子，这限制了图像识别的可扩展性和适用性，特别是在图像可能不可用的场景中。这激发了对零样本学习的研究，它通过从其他模态（如文本）进行知识转移来解决这个问题。在本文中，我们研究了一种合成图像分类器的替代方法：几乎直接从用户的想象中，通过手绘草图。这种方法不需要像零样本学习那样通过属性来命名或描述类别。我们通过训练一个模型回归网络来实现这一点，该网络将手绘草图空间映射到照片分类器空间。事实证明，这种映射可以以类别无关的方式学习，允许用户合成新类别的照片分类器，而无需注释的训练照片。我们还证明了这种分类器生成的方式也可以用于增强现有照片分类器的粒度，或作为基于名称的零样本学习的补充。","领域":"图像识别/零样本学习/草图理解","问题":"训练有效的照片分类器需要大量例子，限制了图像识别的可扩展性和适用性","动机":"探索一种不需要大量图像例子即可生成照片分类器的方法，以解决图像识别在图像不可用场景中的限制","方法":"训练一个模型回归网络，将手绘草图空间映射到照片分类器空间，实现类别无关的映射学习","关键词":["草图分类器","照片分类器生成","零样本学习","模型回归网络"],"涉及的技术概念":"模型回归网络是一种能够将输入从一个空间映射到另一个空间的神经网络，这里特指将手绘草图空间映射到照片分类器空间。零样本学习是一种机器学习方法，旨在通过从其他模态（如文本）转移知识来解决没有直接训练样本的问题。"},{"order":945,"title":"Light Field Intrinsics With a Deep Encoder-Decoder Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Alperovich_Light_Field_Intrinsics_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Alperovich_Light_Field_Intrinsics_CVPR_2018_paper.html","abstract":"We present a fully convolutional autoencoder for light fields, which jointly encodes stacks of horizontal and vertical epipolar plane images through a deep network of residual layers. The complex structure of the light field is thus reduced to a comparatively low-dimensional representation, which can be decoded in a variety of ways. The different pathways of upconvolution we currently support are for disparity estimation and separation of the lightfield into diffuse and specular intrinsic components. The key idea is that we can jointly perform unsupervised training for the autoencoder path of the network, and supervised training for the other decoders. This way, we find features which are both tailored to the respective tasks and generalize well to datasets for which only example light fields are available. We provide an extensive evaluation on synthetic light field data, and show that the network yields good results on previously unseen real world data captured by a Lytro Illum camera and various gantries.","中文标题":"光场本质特征与深度编码-解码网络","摘要翻译":"我们提出了一种用于光场的全卷积自编码器，它通过残差层的深度网络联合编码水平和垂直极平面图像堆栈。因此，光场的复杂结构被简化为相对低维的表示，可以通过多种方式进行解码。我们目前支持的不同上卷积路径用于视差估计和将光场分离为漫反射和镜面反射本质成分。关键思想是我们可以联合执行网络自编码器路径的无监督训练和其他解码器的有监督训练。这样，我们找到的特征既适合各自的任务，又能很好地泛化到只有示例光场可用的数据集。我们对合成光场数据进行了广泛的评估，并展示了网络在由Lytro Illum相机和各种支架捕获的先前未见过的真实世界数据上产生良好结果。","领域":"光场处理/自编码器/视差估计","问题":"光场的复杂结构简化和本质成分分离","动机":"为了找到既适合特定任务又能泛化到仅有示例光场可用的数据集的特征","方法":"使用全卷积自编码器联合编码水平和垂直极平面图像堆栈，通过残差层的深度网络进行无监督和有监督的联合训练","关键词":["光场处理","自编码器","视差估计","本质成分分离"],"涉及的技术概念":"全卷积自编码器是一种深度学习模型，用于学习输入数据的有效表示。残差层是深度网络中的一种结构，有助于训练更深的网络。视差估计是计算两个图像之间对应点差异的过程，用于深度感知。光场本质成分分离涉及将光场分解为漫反射和镜面反射成分，这对于理解光场中的光照和材质属性非常重要。"},{"order":946,"title":"Learning Generative ConvNets via Multi-Grid Modeling and Sampling","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Learning_Generative_ConvNets_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Gao_Learning_Generative_ConvNets_CVPR_2018_paper.html","abstract":"This paper proposes a multi-grid method for learning energy-based generative ConvNet models of images. For each grid, we learn an energy-based probabilistic model where the energy function is defined by a bottom-up convolutional neural network (ConvNet or CNN). Learning such a model requires generating synthesized examples from the model. Within each iteration of our learning algorithm, for each observed training image, we generate synthesized images at multiple grids by initializing the finite-step MCMC sampling from a minimal 1 x 1 version of the training image. The synthesized image at each subsequent grid is obtained by a finite-step MCMC  initialized from the synthesized image generated at the previous coarser grid. After obtaining the synthesized examples, the parameters of the models at multiple grids are updated separately and simultaneously based on the differences between synthesized and observed examples. We show that this multi-grid method can learn realistic energy-based generative ConvNet models, and it outperforms the original contrastive divergence (CD) and persistent CD.","中文标题":"通过多网格建模和采样学习生成卷积网络","摘要翻译":"本文提出了一种多网格方法，用于学习基于能量的生成卷积网络（ConvNet或CNN）图像模型。对于每个网格，我们学习一个基于能量的概率模型，其中能量函数由自下而上的卷积神经网络定义。学习这样的模型需要从模型中生成合成示例。在我们的学习算法的每次迭代中，对于每个观察到的训练图像，我们通过从训练图像的最小1x1版本初始化有限步MCMC采样，在多个网格上生成合成图像。每个后续网格上的合成图像是通过从前一个较粗网格生成的合成图像初始化的有限步MCMC获得的。在获得合成示例后，基于合成示例和观察示例之间的差异，分别并同时更新多个网格上的模型参数。我们展示了这种多网格方法可以学习到现实的基于能量的生成卷积网络模型，并且它优于原始的对比散度（CD）和持久CD。","领域":"生成模型/卷积神经网络/能量模型","问题":"如何有效地学习基于能量的生成卷积网络模型","动机":"提高生成卷积网络模型的学习效率和生成图像的质量","方法":"采用多网格方法，通过在多个网格上生成合成图像并更新模型参数来学习模型","关键词":["多网格方法","能量模型","卷积神经网络","MCMC采样","对比散度"],"涉及的技术概念":{"多网格方法":"一种在不同分辨率或尺寸的网格上处理问题的方法，用于提高模型的学习效率和生成图像的质量","能量模型":"一种基于能量函数的概率模型，用于描述数据的分布","卷积神经网络":"一种深度学习模型，特别适用于处理图像数据","MCMC采样":"马尔可夫链蒙特卡罗采样，一种从概率分布中生成样本的方法","对比散度":"一种用于训练生成模型的算法，通过比较模型生成的数据和真实数据来更新模型参数"}},{"order":947,"title":"Manifold Learning in Quotient Spaces","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mehr_Manifold_Learning_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mehr_Manifold_Learning_in_CVPR_2018_paper.html","abstract":"When learning 3D shapes we are usually interested in their intrinsic geometry rather than in their orientation. To deal with the orientation variations the usual trick consists in augmenting the data to exhibit all possible variability, and thus let the model learn both the geometry as well as the rotations. In this paper we introduce a new autoencoder model for encoding and synthesis of 3D shapes. To get rid of undesirable input variability our model learns a manifold in a quotient space of the input space. Typically, we propose to quotient the space of 3D models by the action of rotations. Thus, our quotient autoencoder allows to directly learn in the space of interest, ignoring side information. This is reflected in better performances on reconstruction and interpolation tasks, as our experiments show that our model outperforms a vanilla autoencoder on the well-known Shapenet dataset. Moreover, our model learns a rotation-invariant representation, leading to interesting results in shapes co-alignment. Finally, we extend our quotient autoencoder to quotient by non-rigid transformations.","中文标题":"商空间中的流形学习","摘要翻译":"在学习3D形状时，我们通常对其内在几何形状而非方向感兴趣。为了处理方向变化，通常的技巧是增加数据以展示所有可能的变异性，从而让模型学习几何形状以及旋转。在本文中，我们引入了一种新的自编码器模型，用于3D形状的编码和合成。为了消除不希望的输入变异性，我们的模型在输入空间的商空间中学习流形。通常，我们建议通过旋转的作用对3D模型的空间进行商。因此，我们的商自编码器允许直接在感兴趣的空间中学习，忽略侧信息。这反映在重建和插值任务上的更好表现上，因为我们的实验表明，我们的模型在著名的Shapenet数据集上优于普通自编码器。此外，我们的模型学习了一种旋转不变的表示，导致在形状共对齐方面取得了有趣的结果。最后，我们将我们的商自编码器扩展到非刚性变换的商。","领域":"3D形状分析/流形学习/自编码器","问题":"如何在3D形状学习中消除方向变化的影响，专注于形状的内在几何特征","动机":"为了更有效地学习3D形状的内在几何特征，避免方向变化对模型学习的影响","方法":"提出了一种新的自编码器模型，通过在输入空间的商空间中学习流形，直接学习感兴趣的空间，忽略侧信息","关键词":["3D形状","流形学习","自编码器","商空间","旋转不变性"],"涉及的技术概念":"商空间是一种数学概念，用于通过某种等价关系将空间划分为等价类，从而简化空间结构。在本文中，商空间被用来消除3D形状的方向变化，使得模型能够专注于形状的内在几何特征。自编码器是一种神经网络，用于学习数据的有效编码，通常用于降维和特征学习。流形学习关注于在高维数据中发现低维结构，以便更好地理解和处理数据。"},{"order":948,"title":"Learning Intelligent Dialogs for Bounding Box Annotation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Konyushkova_Learning_Intelligent_Dialogs_CVPR_2018_paper.html","abstract":"We introduce Intelligent Annotation Dialogs for bounding box annotation. We train an agent to automatically choose a sequence of actions for a human annotator to produce a bounding box in a minimal amount of time.  Specifically, we consider two actions: box verification, where the annotator verifies a box generated by an object detector, and manual box drawing. We explore two kinds of agents, one based on predicting the probability that a box will be positively verified, and the other based on reinforcement learning. We demonstrate that (1) our agents are able to learn efficient annotation strategies in several scenarios, automatically adapting to the image difficulty, the desired quality of the boxes, and the detector strength; (2) in all scenarios the resulting annotation dialogs speed up annotation compared to manual box drawing alone and box verification alone, while also outperforming any fixed combination of verification and drawing in most scenarios; (3) in a realistic scenario where the detector is iteratively re-trained, our agents evolve a series of strategies that reflect the shifting trade-off between verification and drawing as the detector grows stronger.","中文标题":"学习智能对话框用于边界框标注","摘要翻译":"我们介绍了用于边界框标注的智能标注对话框。我们训练一个代理自动选择一系列动作，以便人类标注者能在最短的时间内生成一个边界框。具体来说，我们考虑了两个动作：框验证，即标注者验证由对象检测器生成的框；以及手动绘制框。我们探索了两种代理，一种基于预测框将被正面验证的概率，另一种基于强化学习。我们证明了：(1) 我们的代理能够在几种场景中学习到高效的标注策略，自动适应图像的难度、期望的框质量以及检测器的强度；(2) 在所有场景中，与单独使用手动绘制框和单独使用框验证相比，生成的标注对话框加快了标注速度，同时在大多数场景中也优于任何固定的验证和绘制组合；(3) 在一个检测器被迭代重新训练的现实场景中，我们的代理发展出了一系列策略，这些策略反映了随着检测器变强，验证和绘制之间的权衡变化。","领域":"目标检测/人机交互/强化学习","问题":"如何高效地进行边界框标注","动机":"减少边界框标注所需时间，提高标注效率","方法":"训练代理自动选择动作序列，包括框验证和手动绘制框，探索基于预测概率和强化学习的代理","关键词":["边界框标注","智能对话框","强化学习","目标检测"],"涉及的技术概念":"智能标注对话框通过训练代理来自动选择动作序列，以提高边界框标注的效率。这涉及到两种动作：框验证和手动绘制框。代理的两种类型分别基于预测框被正面验证的概率和强化学习。这种方法能够根据图像的难度、期望的框质量和检测器的强度自动调整标注策略，从而在多种场景下提高标注效率。"},{"order":949,"title":"Boosting Adversarial Attacks With Momentum","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.html","abstract":"Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative  algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.","中文标题":"利用动量增强对抗攻击","摘要翻译":"深度神经网络容易受到对抗样本的攻击，这给这些算法带来了安全上的担忧，因为可能造成严重的后果。对抗攻击在深度学习模型部署前作为评估其鲁棒性的重要替代手段。然而，大多数现有的对抗攻击只能以较低的成功率欺骗黑箱模型。为了解决这个问题，我们提出了一类基于动量的迭代算法来增强对抗攻击。通过将动量项整合到攻击的迭代过程中，我们的方法可以稳定更新方向并在迭代过程中逃离不良的局部最大值，从而产生更具可转移性的对抗样本。为了进一步提高黑箱攻击的成功率，我们将动量迭代算法应用于模型集合，并展示了即使具有强大防御能力的对抗训练模型也容易受到我们的黑箱攻击。我们希望所提出的方法能够作为评估各种深度模型和防御方法鲁棒性的基准。通过这种方法，我们在NIPS 2017非目标对抗攻击和目标对抗攻击竞赛中获得了第一名。","领域":"对抗学习/模型鲁棒性/黑箱攻击","问题":"提高对抗攻击在黑箱模型中的成功率","动机":"评估深度学习模型的鲁棒性，并提高对抗攻击的有效性","方法":"提出了一类基于动量的迭代算法，通过整合动量项来稳定更新方向和逃离不良的局部最大值，应用于模型集合以提高黑箱攻击的成功率","关键词":["对抗攻击","动量迭代算法","模型鲁棒性","黑箱攻击"],"涉及的技术概念":{"对抗样本":"对输入数据进行微小修改，使得深度学习模型产生错误输出的样本","动量项":"在优化算法中用于加速收敛和稳定更新方向的技术","迭代算法":"通过重复应用一系列步骤来逼近问题解决方案的算法","模型集合":"将多个模型的预测结果结合起来，以提高整体性能和鲁棒性"}},{"order":950,"title":"NISP: Pruning Networks Using Neuron Importance Score Propagation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_NISP_Pruning_Networks_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_NISP_Pruning_Networks_CVPR_2018_paper.html","abstract":"To reduce the significant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods prune neurons by only considering the statistics of an individual layer or two consecutive layers (e.g., prune one layer to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In contrast, we argue that for a pruned network to retain its predictive power, it is essential to prune neurons in the entire neuron network jointly based on a unified goal: minimizing the reconstruction error of important responses in the \`\`final response layer\\" (FRL), which is the second-to-last layer before classification. Specifically, we apply feature ranking techniques to measure the importance of each neuron in the FRL, formulate network pruning as a binary integer optimization problem, and derive a closed-form solution to it for pruning neurons in earlier layers. Based on our theoretical analysis, we propose the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses to every neuron in the network. The CNN is pruned by removing neurons with least importance, and it is then fine-tuned to recover its predictive power. NISP is evaluated on several datasets with multiple CNN models and demonstrated to achieve significant acceleration and compression with negligible accuracy loss.","中文标题":"NISP：使用神经元重要性分数传播修剪网络","摘要翻译":"为了减少深度卷积神经网络（CNNs）中的显著冗余，大多数现有方法通过仅考虑单个层或两个连续层的统计信息（例如，修剪一层以最小化下一层的重建误差）来修剪神经元，忽略了深度网络中误差传播的影响。相反，我们认为，为了使修剪后的网络保留其预测能力，必须基于一个统一的目标联合修剪整个神经网络中的神经元：最小化“最终响应层”（FRL）中重要响应的重建误差，FRL是分类前的倒数第二层。具体来说，我们应用特征排名技术来测量FRL中每个神经元的重要性，将网络修剪表述为二进制整数优化问题，并为其推导出封闭形式的解决方案以修剪早期层中的神经元。基于我们的理论分析，我们提出了神经元重要性分数传播（NISP）算法，将最终响应的重要性分数传播到网络中的每个神经元。通过移除重要性最低的神经元来修剪CNN，然后对其进行微调以恢复其预测能力。NISP在多个数据集和多个CNN模型上进行了评估，证明其能够实现显著的加速和压缩，且准确率损失可忽略不计。","领域":"神经网络优化/模型压缩/深度学习加速","问题":"深度卷积神经网络中的冗余问题","动机":"减少深度卷积神经网络中的冗余，同时保留其预测能力","方法":"提出神经元重要性分数传播（NISP）算法，通过最小化最终响应层的重建误差来联合修剪整个神经网络中的神经元","关键词":["神经网络优化","模型压缩","深度学习加速"],"涉及的技术概念":{"深度卷积神经网络（CNNs）":"一种深度学习模型，特别适用于处理图像数据。","神经元修剪":"一种减少神经网络中冗余神经元的技术，以提高模型的效率和速度。","最终响应层（FRL）":"在分类任务中，倒数第二层，其输出直接影响到最终的分类结果。","特征排名技术":"用于评估和排序神经元或特征重要性的技术。","二进制整数优化问题":"一种数学优化问题，其解为二进制值（0或1）。","NISP算法":"一种新的算法，通过传播最终响应层的重要性分数来修剪神经网络中的神经元。"}},{"order":951,"title":"PointGrid: A Deep Network for 3D Shape Understanding","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Le_PointGrid_A_Deep_CVPR_2018_paper.html","abstract":"This paper presents a new deep learning architecture called PointGrid that is designed for 3D model recognition from unorganized point clouds. The new architecture embeds the input point cloud into a 3D grid by a simple, yet effective, sampling strategy and directly learns transformations and features from their raw coordinates. The proposed method is an integration of point and grid, a hybrid model, that leverages the simplicity of grid-based approaches such as VoxelNet while avoid its information loss. PointGrid learns better global information compared with PointNet and is much simpler than PointNet++, Kd-Net, Oct-Net and O-CNN, yet provides comparable recognition accuracy. With experiments on popular shape recognition benchmarks, PointGrid demonstrates competitive performance over existing deep learning methods on both classification and segmentation.","中文标题":"PointGrid：用于3D形状理解的深度网络","摘要翻译":"本文提出了一种名为PointGrid的新型深度学习架构，旨在从无组织的点云中进行3D模型识别。该新架构通过一种简单而有效的采样策略将输入点云嵌入到3D网格中，并直接从其原始坐标学习变换和特征。所提出的方法是点和网格的集成，一种混合模型，它利用了基于网格的方法（如VoxelNet）的简单性，同时避免了其信息丢失。与PointNet相比，PointGrid学习了更好的全局信息，并且比PointNet++、Kd-Net、Oct-Net和O-CNN简单得多，但提供了可比的识别准确率。通过在流行的形状识别基准上的实验，PointGrid在分类和分割方面展示了优于现有深度学习方法的竞争性能。","领域":"3D模型识别/点云处理/深度学习架构","问题":"从无组织的点云中进行3D模型识别","动机":"提高3D模型识别的准确性和效率，同时简化模型架构","方法":"提出了一种新型深度学习架构PointGrid，通过将点云嵌入到3D网格中并直接从原始坐标学习变换和特征，结合点和网格的优点，避免了信息丢失","关键词":["3D模型识别","点云处理","深度学习架构"],"涉及的技术概念":"PointGrid是一种深度学习架构，用于从点云中识别3D模型。它通过将点云嵌入到3D网格中，直接从点云的原始坐标学习变换和特征。这种方法结合了点和网格的优点，既利用了基于网格的方法的简单性，又避免了信息丢失。与现有的方法相比，PointGrid在保持或提高识别准确率的同时，简化了模型架构。"},{"order":952,"title":"Tell Me Where to Look: Guided Attention Inference Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_Tell_Me_Where_CVPR_2018_paper.html","abstract":"Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) make attention maps an explicit and natural component of the end-to-end training for the first time, (2) provide self-guidance directly on these maps by exploring supervision from the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on PASCAL VOC 2012 test and val. sets. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.","中文标题":"告诉我该看哪里：引导注意力推理网络","摘要翻译":"仅使用粗标签的弱监督学习可以通过反向传播梯度获得深度神经网络的视觉解释，如注意力图。这些注意力图随后可以作为对象定位和语义分割等任务的先验。在一个共同的框架中，我们解决了先前方法在建模此类注意力图时的三个不足：我们（1）首次使注意力图成为端到端训练的明确和自然组成部分，（2）通过探索网络自身的监督直接在这些图上提供自我指导以改进它们，以及（3）无缝地弥合了使用弱监督和额外监督（如果可用）之间的差距。尽管方法简单，但在语义分割任务上的实验证明了我们方法的有效性。我们明显超越了PASCAL VOC 2012测试和验证集上的最新技术。此外，所提出的框架不仅提供了一种解释学习者焦点的方法，而且还提供了针对特定任务的直接指导反馈。在温和的假设下，我们的方法也可以理解为现有弱监督学习者的插件，以提高其泛化性能。","领域":"语义分割/注意力机制/弱监督学习","问题":"如何改进弱监督学习中的注意力图建模，以提高对象定位和语义分割的性能","动机":"解决先前方法在建模注意力图时的不足，提高弱监督学习在视觉任务中的表现","方法":"将注意力图作为端到端训练的明确组成部分，通过自我指导改进注意力图，并弥合弱监督与额外监督之间的差距","关键词":["语义分割","注意力机制","弱监督学习","对象定位","视觉解释"],"涉及的技术概念":{"弱监督学习":"一种使用不完全或粗标签进行训练的学习方法","注意力图":"通过反向传播梯度获得的深度神经网络的视觉解释，用于指示模型关注图像中的哪些部分","端到端训练":"一种训练方法，其中模型的所有部分都同时进行训练，而不是分阶段训练","自我指导":"利用网络自身的输出作为监督信号来改进模型性能","语义分割":"一种图像分割任务，旨在为图像中的每个像素分配一个类别标签"}},{"order":953,"title":"3D Semantic Segmentation With Submanifold Sparse Convolutional Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.html","abstract":"Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard \`\`dense'' implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.","中文标题":"使用子流形稀疏卷积网络进行3D语义分割","摘要翻译":"卷积网络是分析时空数据（如图像、视频和3D形状）的事实标准。虽然其中一些数据自然是密集的（例如照片），但许多其他数据源本质上是稀疏的。例子包括使用LiDAR扫描仪或RGB-D相机获得的3D点云。标准的“密集”卷积网络实现在处理此类稀疏数据时非常低效。我们引入了新的稀疏卷积操作，旨在更高效地处理空间稀疏数据，并使用它们开发空间稀疏卷积网络。我们展示了称为子流形稀疏卷积网络（SSCNs）的模型在两个涉及3D点云语义分割的任务中的强大性能。特别是，我们的模型在最近的语义分割竞赛的测试集上优于所有先前的最先进技术。","领域":"3D点云处理/语义分割/稀疏数据处理","问题":"处理3D点云数据时，标准卷积网络效率低下","动机":"提高处理稀疏3D点云数据的效率，以支持更高效的语义分割","方法":"引入新的稀疏卷积操作，开发空间稀疏卷积网络（SSCNs）","关键词":["3D点云","语义分割","稀疏卷积网络"],"涉及的技术概念":"子流形稀疏卷积网络（SSCNs）是一种专门设计用于高效处理空间稀疏数据的卷积网络。通过引入新的稀疏卷积操作，这些网络能够更有效地处理如3D点云等稀疏数据，从而在语义分割任务中实现更高的性能。"},{"order":954,"title":"TOM-Net: Learning Transparent Object Matting From a Single Image","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_TOM-Net_Learning_Transparent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_TOM-Net_Learning_Transparent_CVPR_2018_paper.html","abstract":"This paper addresses the problem of transparent object matting. Existing image matting approaches for transparent objects often require tedious capturing procedures and long processing time, which limit their practical use. In this paper, we first formulate transparent object matting as a refractive flow estimation problem. We then propose a deep learning framework, called TOM-Net, for learning the refractive flow. Our framework comprises two parts, namely a multi-scale encoder-decoder network for producing a coarse prediction, and a residual network for refinement. At test time, TOM-Net takes a single image as input, and outputs a matte (consisting of an object mask, an attenuation mask and a refractive flow field) in a fast feed-forward pass. As no off-the-shelf dataset is available for transparent object matting, we create a large-scale synthetic dataset consisting of 178K images of transparent objects rendered in front of images sampled from the Microsoft COCO dataset. We also collect a real dataset consisting of 876 samples using 14 transparent objects and 60 background images. Promising experimental results have been achieved on both synthetic and real data, which clearly demonstrate the effectiveness of our approach.","中文标题":"TOM-Net: 从单张图像学习透明物体抠图","摘要翻译":"本文解决了透明物体抠图的问题。现有的透明物体图像抠图方法通常需要繁琐的捕捉过程和长时间的处理时间，这限制了它们的实际应用。在本文中，我们首先将透明物体抠图问题表述为折射流估计问题。然后，我们提出了一个深度学习框架，称为TOM-Net，用于学习折射流。我们的框架包括两部分，即用于生成粗略预测的多尺度编码器-解码器网络，和用于细化的残差网络。在测试时，TOM-Net以单张图像作为输入，并在快速前向传递中输出一个抠图（包括物体掩码、衰减掩码和折射流场）。由于没有现成的透明物体抠图数据集可用，我们创建了一个大规模合成数据集，包含178K张透明物体的图像，这些图像是在从Microsoft COCO数据集中采样的图像前渲染的。我们还收集了一个真实数据集，包含876个样本，使用了14个透明物体和60个背景图像。在合成数据和真实数据上都取得了有希望的实验结果，这清楚地证明了我们方法的有效性。","领域":"透明物体抠图/折射流估计/深度学习","问题":"透明物体抠图","动机":"现有的透明物体图像抠图方法需要繁琐的捕捉过程和长时间的处理时间，限制了实际应用。","方法":"提出一个深度学习框架TOM-Net，包括多尺度编码器-解码器网络和残差网络，用于从单张图像中学习折射流并输出抠图。","关键词":["透明物体抠图","折射流估计","深度学习框架","多尺度编码器-解码器网络","残差网络"],"涉及的技术概念":{"透明物体抠图":"指从图像中分离出透明物体的过程，包括生成物体掩码、衰减掩码和折射流场。","折射流估计":"指估计光线通过透明物体时折射方向的变化，是透明物体抠图的关键步骤。","深度学习框架":"指使用深度学习技术构建的软件框架，用于解决特定问题。","多尺度编码器-解码器网络":"一种网络结构，能够在不同尺度上处理图像信息，用于生成初步的预测结果。","残差网络":"一种网络结构，通过引入残差连接来优化网络性能，用于细化初步预测结果。"}},{"order":955,"title":"Translating and Segmenting Multimodal Medical Volumes With Cycle- and Shape-Consistency Generative Adversarial Network","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Translating_and_Segmenting_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Translating_and_Segmenting_CVPR_2018_paper.html","abstract":"Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized CT data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss,  which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and MRI cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively.","中文标题":"使用循环和形状一致性生成对抗网络翻译和分割多模态医学体积","摘要翻译":"合成的医学图像有几个重要的应用，例如，作为跨模态图像配准的中间媒介，以及作为补充训练样本来提高分类器的泛化能力。特别是，合成的CT数据可以为放射治疗计划提供X射线衰减图。在这项工作中，我们提出了一种通用的跨模态合成方法，具有以下目标：1）使用未配对的训练数据合成看起来真实的3D图像，2）确保一致的解剖结构，这些结构可能会因跨模态合成中的几何失真而改变，3）通过使用合成数据来改善体积分割，以应对训练样本有限的模态。我们展示了这些目标可以通过一个端到端的3D卷积神经网络（CNN）实现，该网络由相互受益的生成器和分割器组成，用于图像合成和分割任务。生成器通过对抗损失、循环一致性损失和由分割器监督的形状一致性损失进行训练，以减少几何失真。从分割的角度来看，分割器通过生成器在线生成的合成数据得到增强。生成器和分割器以端到端的训练方式相互促进。通过在包括总共4,496个CT和MRI心血管体积的数据集上进行的大量实验，我们展示了这两个任务相互受益，并且将这两个任务结合起来比单独解决它们能获得更好的性能。","领域":"医学图像合成/跨模态图像配准/放射治疗计划","问题":"如何在跨模态医学图像合成中减少几何失真并提高分割性能","动机":"提高医学图像合成的真实性和一致性，以及通过合成数据增强分割性能","方法":"提出了一种端到端的3D卷积神经网络，结合生成器和分割器，通过对抗损失、循环一致性损失和形状一致性损失进行训练","关键词":["医学图像合成","跨模态图像配准","放射治疗计划","3D卷积神经网络","对抗损失","循环一致性损失","形状一致性损失"],"涉及的技术概念":"3D卷积神经网络（CNN）用于图像合成和分割任务，生成器通过对抗损失、循环一致性损失和形状一致性损失进行训练，以减少几何失真并提高分割性能。"},{"order":956,"title":"An Unsupervised Learning Model for Deformable Medical Image Registration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Balakrishnan_An_Unsupervised_Learning_CVPR_2018_paper.html","abstract":"We present a fast learning-based algorithm for deformable, pairwise 3D medical image registration. Current registration methods optimize an objective function independently for each pair of images, which can be time-consuming for large data. We define registration as a parametric function, and optimize its parameters given a set of images from a collection of interest. Given a new pair of scans, we can quickly compute a registration field by directly evaluating the function using the learned parameters. We model this function using a CNN, and use a spatial transform layer to reconstruct one image from another while imposing smoothness constraints on the registration field. The proposed method does not require supervised information such as ground truth registration fields or anatomical landmarks. We demonstrate registration accuracy comparable to state-of-the-art 3D image registration, while operating orders of magnitude faster in practice. Our method promises to significantly speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is available at https://github.com/balakg/voxelmorph.","中文标题":"一种用于可变形医学图像配准的无监督学习模型","摘要翻译":"我们提出了一种快速的基于学习的算法，用于可变形、成对的3D医学图像配准。当前的配准方法独立地为每对图像优化目标函数，这对于大数据集来说可能非常耗时。我们将配准定义为一个参数化函数，并针对感兴趣的一组图像优化其参数。给定一对新的扫描图像，我们可以通过使用学习到的参数直接评估该函数来快速计算配准场。我们使用CNN建模该函数，并使用空间变换层从另一幅图像重建一幅图像，同时对配准场施加平滑约束。所提出的方法不需要监督信息，如真实配准场或解剖标志。我们展示了与最先进的3D图像配准相当的配准精度，同时在实际操作中速度快了几个数量级。我们的方法有望显著加快医学图像分析和处理流程，同时促进基于学习的配准及其应用的新方向。我们的代码可在https://github.com/balakg/voxelmorph获取。","领域":"医学图像分析/深度学习/图像配准","问题":"解决大规模医学图像数据配准耗时的问题","动机":"提高医学图像配准的速度和效率，减少对监督信息的依赖","方法":"使用CNN建模参数化函数，通过空间变换层重建图像并施加平滑约束","关键词":["无监督学习","3D医学图像","图像配准","CNN","空间变换层"],"涉及的技术概念":"CNN（卷积神经网络）用于建模参数化函数，空间变换层用于图像重建，平滑约束用于优化配准场。"},{"order":957,"title":"Deep Lesion Graphs in the Wild: Relationship Learning and Organization of Significant Radiology Image Findings in a Diverse Large-Scale Lesion Database","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Yan_Deep_Lesion_Graphs_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yan_Deep_Lesion_Graphs_CVPR_2018_paper.html","abstract":"Radiologists in their daily work routinely find and annotate significant abnormalities on a large number of radiology images. Such abnormalities, or lesions, have collected over years and stored in hospitals' picture archiving and communication systems. However, they are basically unsorted and lack semantic annotations like type and location. In this paper, we aim to organize and explore them by learning a deep feature representation for each lesion. A large-scale and comprehensive dataset, DeepLesion, is introduced for this task. DeepLesion contains bounding boxes and size measurements of over 32K lesions. To model their similarity relationship, we leverage multiple supervision information including types, self-supervised location coordinates, and sizes. They require little manual annotation effort but describe useful attributes of the lesions. Then, a triplet network is utilized to learn lesion embeddings with a sequential sampling strategy to depict their hierarchical similarity structure. Experiments show promising qualitative and quantitative results on lesion retrieval, clustering, and classification. The learned embeddings can be further employed to build a lesion graph for various clinically useful applications. An algorithm for intra-patient lesion matching is proposed and validated with experiments.","中文标题":"野外深度病变图：在多样化大规模病变数据库中学习关系和组织重要放射影像发现","摘要翻译":"放射科医生在日常工作中常规地发现并注释大量放射影像上的显著异常。这些异常，或称病变，多年来被收集并存储在医院的图片存档和通信系统中。然而，它们基本上未分类，缺乏如类型和位置等语义注释。在本文中，我们旨在通过学习每个病变的深度特征表示来组织和探索它们。为此任务引入了一个大规模且全面的数据集，DeepLesion。DeepLesion包含超过32K病变的边界框和大小测量。为了建模它们的相似性关系，我们利用了包括类型、自监督位置坐标和大小在内的多种监督信息。它们需要很少的手动注释努力，但描述了病变的有用属性。然后，利用三元组网络通过顺序采样策略学习病变嵌入，以描绘它们的层次相似性结构。实验在病变检索、聚类和分类上显示出有希望的定性和定量结果。学习的嵌入可以进一步用于构建病变图，用于各种临床上有用的应用。提出了一种用于患者内病变匹配的算法，并通过实验进行了验证。","领域":"医学影像分析/病变分类/病变检索","问题":"如何组织和探索大规模放射影像中的病变数据，缺乏语义注释的问题","动机":"放射影像中的病变数据量大且未分类，缺乏有效的组织和探索方法","方法":"引入DeepLesion数据集，利用多种监督信息学习病变的深度特征表示，使用三元组网络和顺序采样策略学习病变嵌入，构建病变图","关键词":["病变分类","病变检索","病变嵌入","病变图","医学影像分析"],"涉及的技术概念":{"DeepLesion":"一个包含超过32K病变的大规模数据集，用于病变的深度特征表示学习","三元组网络":"一种用于学习病变嵌入的网络结构，通过顺序采样策略描绘病变的层次相似性结构","病变图":"利用学习的病变嵌入构建的图结构，用于临床上有用的应用"}},{"order":958,"title":"Learning Distributions of Shape Trajectories From Longitudinal Datasets: A Hierarchical Model on a Manifold of Diffeomorphisms","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Bone_Learning_Distributions_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Bone_Learning_Distributions_of_CVPR_2018_paper.html","abstract":"We propose a method to learn a distribution of shape trajectories from longitudinal data, i.e. the collection of individual objects repeatedly observed at multiple time-points. The method allows to compute an average spatiotemporal trajectory of shape changes at the group level, and the individual variations of this trajectory both in terms of geometry and time dynamics. First, we formulate a non-linear mixed-effects statistical model as the combination of a generic statistical model for manifold-valued longitudinal data, a deformation model defining shape trajectories via the action of a finite-dimensional set of diffeomorphisms with a manifold structure, and an efficient numerical scheme to compute parallel transport on this manifold. Second, we introduce a MCMC-SAEM algorithm with a specific approach to shape sampling, an adaptive scheme for proposal variances, and a log-likelihood tempering strategy to estimate our model. Third, we validate our algorithm on 2D simulated data, and then estimate a scenario of alteration of the shape of the hippocampus 3D brain structure during the course of Alzheimer's disease. The method shows for instance that hippocampal atrophy progresses more quickly in female subjects, and occurs earlier in APOE4 mutation carriers. We finally illustrate the potential of our method for classifying pathological trajectories versus normal ageing.","中文标题":"从纵向数据集中学习形状轨迹的分布：流形上的层次模型","摘要翻译":"我们提出了一种方法，用于从纵向数据中学习形状轨迹的分布，即多次在不同时间点观察到的个体对象的集合。该方法允许计算群体水平上形状变化的平均时空轨迹，以及这一轨迹在几何和时间动态方面的个体差异。首先，我们制定了一个非线性混合效应统计模型，该模型结合了用于流形值纵向数据的通用统计模型、通过具有流形结构的有限维微分同胚集合的作用定义形状轨迹的变形模型，以及计算该流形上平行传输的高效数值方案。其次，我们引入了一种MCMC-SAEM算法，该算法具有特定的形状采样方法、提案方差的自适应方案和用于估计我们模型的似然温度策略。第三，我们在2D模拟数据上验证了我们的算法，然后估计了阿尔茨海默病过程中海马体3D大脑结构形状改变的情景。该方法显示，例如，海马体萎缩在女性受试者中进展更快，并且在APOE4突变携带者中发生得更早。最后，我们展示了我们的方法在分类病理轨迹与正常老化方面的潜力。","领域":"形状分析/生物医学图像分析/统计建模","问题":"如何从纵向数据中学习形状轨迹的分布，并分析群体和个体在形状变化上的差异","动机":"研究动机是为了更好地理解和分析在生物医学图像中观察到的形状变化，特别是在阿尔茨海默病等疾病过程中海马体形状的改变","方法":"采用非线性混合效应统计模型结合流形值纵向数据的通用统计模型、变形模型和高效数值方案，以及引入MCMC-SAEM算法进行模型估计","关键词":["形状轨迹","纵向数据","非线性混合效应模型","MCMC-SAEM算法","海马体萎缩"],"涉及的技术概念":"非线性混合效应统计模型用于处理流形值纵向数据，通过有限维微分同胚集合定义形状轨迹的变形模型，以及MCMC-SAEM算法用于模型估计，包括形状采样、提案方差的自适应方案和似然温度策略。"},{"order":959,"title":"CNN Driven Sparse Multi-Level B-Spline Image Registration","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Jiang_CNN_Driven_Sparse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Jiang_CNN_Driven_Sparse_CVPR_2018_paper.html","abstract":"Traditional single-grid and pyramidal B-spline parameterizations used in deformable image registration require users to specify control point spacing configurations capable of accurately capturing both global and complex local deformations.  In many cases, such grid configurations are non-obvious and largely selected based on user experience.  Recent regularization methods imposing sparsity upon the B-spline coefficients throughout simultaneous multi-grid optimization, however, have provided a promising means of determining suitable configurations automatically.  Unfortunately, imposing sparsity on over-parameterized B-spline models is computationally expensive and introduces additional difficulties such as undesirable local minima in the B-spline coefficient optimization process.  To overcome these difficulties in determining B-spline grid configurations, this paper investigates the use of convolutional neural networks (CNNs) to learn and infer expressive sparse multi-grid configurations prior to B-spline coefficient optimization.  Experimental results show that multi-grid configurations produced in this fashion using our CNN based approach provide registration quality comparable to L1-norm constrained over-parameterizations in terms of exactness, while exhibiting significantly reduced computational requirements.","中文标题":"CNN驱动的稀疏多级B样条图像配准","摘要翻译":"传统的单网格和金字塔B样条参数化方法在可变形图像配准中需要用户指定能够准确捕捉全局和复杂局部变形的控制点间距配置。在许多情况下，这样的网格配置并不明显，很大程度上依赖于用户经验来选择。然而，最近通过在同时多网格优化中对B样条系数施加稀疏性的正则化方法，提供了一种自动确定合适配置的有前途的手段。不幸的是，对过度参数化的B样条模型施加稀疏性在计算上是昂贵的，并在B样条系数优化过程中引入了诸如不希望的局部最小值等额外困难。为了克服在确定B样条网格配置中的这些困难，本文研究了在B样条系数优化之前使用卷积神经网络（CNNs）来学习和推断表达性稀疏多网格配置的使用。实验结果表明，使用我们基于CNN的方法以这种方式生成的多网格配置在准确性方面提供了与L1范数约束的过度参数化相当的配准质量，同时显著减少了计算需求。","领域":"图像配准/卷积神经网络/稀疏优化","问题":"在可变形图像配准中自动确定合适的B样条网格配置","动机":"传统的B样条参数化方法依赖于用户经验选择网格配置，且对过度参数化的B样条模型施加稀疏性计算成本高，存在局部最小值问题","方法":"使用卷积神经网络（CNNs）在B样条系数优化之前学习和推断表达性稀疏多网格配置","关键词":["图像配准","卷积神经网络","稀疏优化","B样条","多网格配置"],"涉及的技术概念":{"B样条":"一种用于图像配准的数学工具，可以灵活地表示复杂的变形","卷积神经网络（CNNs）":"一种深度学习模型，用于从数据中学习特征表示","稀疏优化":"一种优化方法，旨在通过减少模型中的非零参数数量来提高计算效率和模型性能","L1范数约束":"一种正则化技术，用于在优化过程中施加稀疏性"}},{"order":960,"title":"Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Dalca_Anatomical_Priors_in_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Dalca_Anatomical_Priors_in_CVPR_2018_paper.html","abstract":"We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images that we use to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model  that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior enables fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code, model definitions and model weights are freely available at http://github.com/adalca/neuron","中文标题":"卷积网络中的解剖先验用于无监督生物医学分割","摘要翻译":"我们考虑将生物医学图像分割成感兴趣的解剖区域的问题。我们特别关注的是，我们经常遇到没有包含图像及其手动分割的配对训练数据的情况。相反，我们使用未配对的分割图像来构建解剖先验。关键的是，这些分割可以从与当前任务不同的数据集和成像模态的成像数据中得出。我们引入了一个生成概率模型，该模型通过卷积神经网络使用学习到的先验来计算无监督设置中的分割。我们在结构性脑MRI分割的背景下对提出的方法进行了实证分析，使用了包含超过14,000次扫描的多研究数据集。我们的结果表明，解剖先验使得快速无监督分割成为可能，而这通常使用标准卷积网络是不可能实现的。解剖先验的整合可以在一系列新的临床问题中促进基于CNN的解剖分割，在这些问题中，很少有或没有注释可用，因此标准网络无法训练。代码、模型定义和模型权重可在http://github.com/adalca/neuron免费获取。","领域":"生物医学图像分割/无监督学习/卷积神经网络","问题":"在没有配对训练数据的情况下，如何将生物医学图像分割成解剖区域","动机":"解决在缺乏手动分割注释的情况下，进行生物医学图像分割的挑战","方法":"引入生成概率模型，通过卷积神经网络使用学习到的解剖先验进行无监督分割","关键词":["生物医学图像分割","无监督学习","卷积神经网络","解剖先验"],"涉及的技术概念":"解剖先验指的是在图像分割任务中，利用已知的解剖结构信息来指导分割过程。生成概率模型是一种统计模型，用于描述数据的生成过程。卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。无监督学习是一种机器学习方法，它不依赖于标记数据来训练模型。"},{"order":961,"title":"3D Registration of Curves and Surfaces Using Local Differential Information","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Raposo_3D_Registration_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Raposo_3D_Registration_of_CVPR_2018_paper.html","abstract":"This article presents for the first time a global method for registering 3D curves with 3D surfaces without requiring an initialization. The algorithm works with 2-tuples point+vector that consist in pairs of points augmented with the information of their tangents or normals. A closed-form solution for determining the alignment transformation from a pair of matching 2-tuples is proposed. In addition, the set of necessary conditions for two 2-tuples to match is derived. This allows fast search of correspondences that are used in an hypothesise-and-test framework for accomplishing global registration. Comparative experiments demonstrate that the proposed algorithm is the first effective solution for curve vs surface registration, with the method achieving accurate alignment in situations of small overlap and large percentage of outliers in a fraction of a second. The proposed framework is extended to the cases of curve vs curve and surface vs surface registration, with the former being particularly relevant since it is also a largely unsolved problem.","中文标题":"使用局部微分信息进行曲线和曲面的3D配准","摘要翻译":"本文首次提出了一种无需初始化的全局方法，用于将3D曲线与3D曲面进行配准。该算法使用点+向量的2元组工作，这些2元组由点对及其切线或法线信息增强。提出了一种从一对匹配的2元组确定对齐变换的闭式解。此外，还推导了两个2元组匹配的必要条件集。这使得在假设和测试框架中快速搜索对应关系成为可能，从而实现全局配准。比较实验表明，所提出的算法是第一个有效的曲线与曲面配准解决方案，该方法在重叠小和异常值比例大的情况下，在几分之一秒内实现了精确对齐。所提出的框架扩展到曲线与曲线和曲面与曲面配准的情况，前者尤其相关，因为它也是一个很大程度上未解决的问题。","领域":"3D配准/几何处理/计算几何","问题":"3D曲线与3D曲面的全局配准问题","动机":"解决无需初始化的3D曲线与3D曲面配准问题，特别是在重叠小和异常值比例大的情况下实现精确对齐。","方法":"使用点+向量的2元组，提出闭式解确定对齐变换，并推导匹配的必要条件集，采用假设和测试框架进行全局配准。","关键词":["3D配准","曲线配准","曲面配准"],"涉及的技术概念":"2元组（点+向量）、切线或法线信息、闭式解、对齐变换、假设和测试框架、全局配准"},{"order":962,"title":"Weakly Supervised Learning of Single-Cell Feature Embeddings","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Caicedo_Weakly_Supervised_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Caicedo_Weakly_Supervised_Learning_CVPR_2018_paper.html","abstract":"Many new applications in drug discovery and functional genomics require capturing the morphology of individual imaged cells as comprehensively as possible rather than measuring one particular feature. In these so-called profiling experiments, the goal is to compare populations of cells treated with different chemicals or genetic perturbations in order to identify biomedically important similarities. Deep convolutional neural networks (CNNs) often make excellent feature extractors but require ground truth for training; this is rarely available in biomedical profiling experiments. We therefore propose to train CNNs based on a weakly supervised approach, where the network aims to classify each treatment against all others. Using this network as a feature extractor performed comparably to a network trained on non-biological, natural images on a chemical screen benchmark task, and improved results significantly on a more challenging genetic benchmark presented for the first time.","中文标题":"弱监督学习下的单细胞特征嵌入","摘要翻译":"在药物发现和功能基因组学中的许多新应用需要尽可能全面地捕捉单个成像细胞的形态，而不是测量一个特定的特征。在这些所谓的分析实验中，目标是比较用不同化学物质或遗传扰动处理的细胞群体，以识别生物医学上重要的相似性。深度卷积神经网络（CNNs）通常是非常好的特征提取器，但需要真实标签进行训练；这在生物医学分析实验中很少可用。因此，我们提出基于弱监督方法训练CNNs，其中网络旨在将每种处理与其他所有处理进行分类。使用这种网络作为特征提取器，在化学筛选基准任务上的表现与在非生物自然图像上训练的网络相当，并且在首次提出的更具挑战性的遗传基准上显著改善了结果。","领域":"生物医学图像分析/单细胞分析/药物发现","问题":"在缺乏真实标签的情况下，如何有效地捕捉和比较单个成像细胞的形态特征","动机":"为了在药物发现和功能基因组学中识别生物医学上重要的相似性，需要一种无需大量标注数据即可有效提取单细胞特征的方法","方法":"提出了一种基于弱监督学习的深度卷积神经网络（CNNs）训练方法，通过将每种处理与其他所有处理进行分类来训练网络，然后将其用作特征提取器","关键词":["弱监督学习","单细胞特征嵌入","深度卷积神经网络","生物医学图像分析","药物发现"],"涉及的技术概念":"深度卷积神经网络（CNNs）是一种深度学习模型，特别适用于处理图像数据。弱监督学习是一种机器学习方法，它使用不完全、不精确或噪声标签的数据进行训练。单细胞特征嵌入指的是将单个细胞的复杂形态特征转换为低维空间中的向量表示，以便于比较和分析。"},{"order":963,"title":"Guided Proofreading of Automatic Segmentations for Connectomics","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Haehn_Guided_Proofreading_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Haehn_Guided_Proofreading_of_CVPR_2018_paper.html","abstract":"Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.","中文标题":"连接组学自动分割的引导校对","摘要翻译":"连接组学中的自动细胞图像分割方法会产生合并和分割错误，这些错误需要通过校对来纠正。先前的研究已经发现，对这些错误的视觉搜索是交互式校对的瓶颈。为了帮助错误纠正，我们开发了两个分类器，自动向用户推荐候选的合并和分割。这些分类器使用了一个卷积神经网络（CNN），该网络已经通过自动分割中的错误与专家标记的地面实况进行了训练。我们的分类器通过考虑分割边界周围的大上下文区域来检测潜在的错误区域。然后，用户可以通过是/否决策来执行纠正，这比以前的校对方法减少了7.5倍的信息变化。我们还提出了一种全自动模式，该模式使用概率阈值来做出合并/分割决策。使用自动方法进行的广泛实验以及新手和专家用户性能的比较表明，我们的方法在不同的连接组学数据集上优于最先进的校对方法。","领域":"生物图像分析/神经网络/图像分割","问题":"自动细胞图像分割方法在连接组学中产生的合并和分割错误","动机":"提高连接组学中自动分割的准确性，减少校对过程中的视觉搜索负担","方法":"开发两个基于卷积神经网络的分类器，自动推荐候选的合并和分割，通过考虑分割边界周围的大上下文区域来检测潜在的错误区域","关键词":["连接组学","图像分割","卷积神经网络","校对","错误纠正"],"涉及的技术概念":"卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。在本研究中，CNN被训练来识别自动分割中的错误，通过与专家标记的地面实况进行比较。这种方法允许分类器自动检测并推荐需要纠正的区域，从而加速校对过程并提高准确性。"},{"order":964,"title":"Wide Compression: Tensor Ring Nets","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Wide_Compression_Tensor_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Wide_Compression_Tensor_CVPR_2018_paper.html","abstract":"Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications.  In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep networks. Our results show that our TR-Nets approach is able to compress LeNet-5 by 11x without losing accuracy, and can compress the state-of-the-art Wide ResNet by 243x with only 2.3% degradation in Cifar10 image classification. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.","中文标题":"广泛压缩：张量环网络","摘要翻译":"深度神经网络在各种现实世界的应用中展示了最先进的性能。为了获得性能提升，这些网络变得更大更深，包含数百万甚至数十亿的参数和超过一千层。这种权衡是这些大型架构需要巨大的内存、存储和计算资源，从而限制了它们的可用性。受到最近张量环分解的启发，我们引入了张量环网络（TR-Nets），它显著压缩了深度网络的全连接层和卷积层。我们的结果表明，我们的TR-Nets方法能够在不损失准确性的情况下将LeNet-5压缩11倍，并且可以将最先进的Wide ResNet压缩243倍，在Cifar10图像分类中仅有2.3%的性能下降。总体而言，这种压缩方案在科学计算和深度学习中显示出潜力，特别是对于新兴的资源受限设备，如智能手机、可穿戴设备和物联网设备。","领域":"神经网络压缩/张量分解/资源受限设备","问题":"深度神经网络因参数和层数过多导致的内存、存储和计算资源需求巨大，限制了其在实际应用中的可用性。","动机":"为了减少深度神经网络对资源的需求，提高其在资源受限设备上的可用性。","方法":"引入张量环网络（TR-Nets），利用张量环分解技术显著压缩深度网络的全连接层和卷积层。","关键词":["神经网络压缩","张量环分解","资源受限设备"],"涉及的技术概念":"张量环分解是一种数学技术，用于将高维张量分解为一系列低维张量的环状结构，从而减少存储和计算需求。TR-Nets利用这种技术来压缩深度神经网络的全连接层和卷积层，以减少模型的大小和计算资源需求，同时尽量保持模型的性能。"},{"order":965,"title":"Improvements to Context Based Self-Supervised Learning","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Mundhenk_Improvements_to_Context_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Mundhenk_Improvements_to_Context_CVPR_2018_paper.html","abstract":"We develop a set of methods to improve on the results of self-supervised learning using context. We start with a baseline of patch based arrangement context learning and go from there. Our methods address some overt problems such as chromatic aberration as well as other potential problems such as spatial skew and mid-level feature neglect. We prevent problems with testing generalization on common self-supervised benchmark tests by using different datasets during our development. The results of our methods combined yield top scores on all standard self-supervised benchmarks, including classification and detection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \\"linear tests\\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over our baseline method of between 4.0 to 7.1 percentage points on transfer learning classification tests. We also show results on different standard network architectures to demonstrate generalization as well as portability. All data, models and programs are available at: https://gdo-datasci. llnl.gov/selfsupervised/.","中文标题":"基于上下文的自监督学习改进","摘要翻译":"我们开发了一系列方法来改进使用上下文的自监督学习结果。我们从基于补丁的排列上下文学习基线开始，并在此基础上进行改进。我们的方法解决了一些明显的问题，如色差，以及其他潜在问题，如空间偏差和中层特征忽视。通过在开发过程中使用不同的数据集，我们避免了在常见的自监督基准测试中测试泛化能力的问题。我们的方法结合后的结果在所有标准自监督基准测试中均取得了最高分，包括PASCAL VOC 2007的分类和检测、PASCAL VOC 2012的分割以及ImageNet和CSAIL Places数据集的“线性测试”。在迁移学习分类测试中，我们相对于基线方法获得了4.0到7.1个百分点的改进。我们还展示了不同标准网络架构上的结果，以证明泛化能力和可移植性。所有数据、模型和程序均可在https://gdo-datasci.llnl.gov/selfsupervised/获取。","领域":"自监督学习/迁移学习/图像分类","问题":"改进自监督学习中的上下文利用，解决色差、空间偏差和中层特征忽视等问题","动机":"提高自监督学习在图像分类、检测和分割任务中的性能，通过改进上下文利用来解决现有问题","方法":"从基于补丁的排列上下文学习基线出发，开发一系列改进方法，使用不同数据集避免泛化问题，结合多种方法提高性能","关键词":["自监督学习","迁移学习","图像分类","图像检测","图像分割"],"涉及的技术概念":"自监督学习是一种无需大量标注数据的学习方法，通过从数据本身生成监督信号来训练模型。迁移学习是指将在一个任务上学到的知识应用到另一个相关任务上。图像分类、检测和分割是计算机视觉中的基本任务，分别涉及识别图像中的对象类别、定位图像中的对象以及将图像分割成多个区域或对象。"},{"order":966,"title":"Learning Structure and Strength of CNN Filters for Small Sample Size Training","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Keshari_Learning_Structure_and_CVPR_2018_paper.html","abstract":"Convolutional Neural Networks have provided state-of-the-art results in several computer vision problems. However, due to a large number of parameters in CNNs, they require a large number of training samples which is a limiting factor for small sample size problems. To address this limitation, in this paper, we propose SSF-CNN which focuses on learning the “structure\\" and “strength\\" of filters. The structure of the filter is initialized using a dictionary based filter learning algorithm and the strength of the filter is learned using the small sample training data. The architecture provides the flexibility of training with both small and large training databases, and yields good accuracies even with small size training data. The effectiveness of the algorithm is demonstrated on MNIST, CIFAR10, NORB, Omniglot, and Newborn Face Image databases, with varying number of training samples. The results show that SSF-CNN significantly reduces the number of parameters required for training while providing high accuracies on the test database. On small problems such as newborn face recognition, the results demonstrate improvement in rank-1 identification accuracy by at least 10%.","中文标题":"学习CNN滤波器的结构和强度以进行小样本训练","摘要翻译":"卷积神经网络在多个计算机视觉问题上提供了最先进的结果。然而，由于CNN中参数数量庞大，它们需要大量的训练样本，这对于小样本问题是一个限制因素。为了解决这一限制，本文提出了SSF-CNN，它专注于学习滤波器的“结构”和“强度”。滤波器的结构使用基于字典的滤波器学习算法进行初始化，滤波器的强度则使用小样本训练数据进行学习。该架构提供了使用小和大训练数据库进行训练的灵活性，并且即使在小规模训练数据下也能产生良好的准确性。该算法的有效性在MNIST、CIFAR10、NORB、Omniglot和新生儿面部图像数据库上进行了展示，训练样本数量各不相同。结果表明，SSF-CNN显著减少了训练所需的参数数量，同时在测试数据库上提供了高准确性。在新生儿面部识别等小问题上，结果显示排名1的识别准确性至少提高了10%。","领域":"小样本学习/卷积神经网络/图像识别","问题":"解决卷积神经网络在小样本训练中参数过多的问题","动机":"由于卷积神经网络需要大量训练样本，这在样本量有限的情况下成为限制因素，因此需要一种方法能够在少量样本下有效训练CNN。","方法":"提出SSF-CNN，通过基于字典的滤波器学习算法初始化滤波器结构，并使用小样本训练数据学习滤波器强度，从而减少训练所需的参数数量。","关键词":["小样本学习","卷积神经网络","图像识别"],"涉及的技术概念":"SSF-CNN是一种专注于学习卷积神经网络滤波器结构和强度的算法，通过基于字典的滤波器学习算法初始化滤波器结构，并使用小样本训练数据学习滤波器强度，旨在减少训练所需的参数数量，提高在小样本情况下的训练效率和准确性。"},{"order":967,"title":"Boosting Self-Supervised Learning via Knowledge Transfer","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Noroozi_Boosting_Self-Supervised_Learning_CVPR_2018_paper.html","abstract":"In self-supervised learning one trains a model to solve a so-called pretext task on a dataset without the need for human annotation. The main objective, however, is to transfer this model to a target domain and task. Currently, the most effective transfer strategy is fine-tuning, which restricts one to use the same model or parts thereof for both pretext and target tasks. In this paper, we present a novel framework for self-supervised learning that overcomes limitations in designing and comparing different tasks, models, and data domains. In particular, our framework decouples the structure of the self-supervised model from the final task-specific fine-tuned model. This allows us to: 1) quantitatively assess previously incompatible models including handcrafted features; 2) show that deeper neural network models can learn better representations from the same pretext task; 3) transfer knowledge learned with a deep model to a shallower one and thus boost its learning.  We use this framework to design a novel self-supervised task, which achieves state-of-the-art performance on the common benchmarks in PASCAL VOC 2007, ILSVRC12 and Places by a significant margin. A surprising result is that our learned features shrink the mAP gap between models trained via self-supervised learning and supervised learning from $5.9$ to $2.6$ in object detection on PASCAL VOC 2007.","中文标题":"通过知识转移提升自监督学习","摘要翻译":"在自监督学习中，人们训练模型以解决所谓的预任务，而无需人工标注。然而，主要目标是将此模型转移到目标领域和任务。目前，最有效的转移策略是微调，这限制了人们必须使用相同的模型或其部分来处理预任务和目标任务。在本文中，我们提出了一个新颖的自监督学习框架，克服了设计和比较不同任务、模型和数据领域的限制。特别是，我们的框架将自监督模型的结构与最终任务特定的微调模型解耦。这使我们能够：1）定量评估以前不兼容的模型，包括手工制作的特征；2）展示更深的神经网络模型可以从相同的预任务中学习到更好的表示；3）将使用深度模型学到的知识转移到较浅的模型，从而提升其学习。我们使用这个框架设计了一个新颖的自监督任务，在PASCAL VOC 2007、ILSVRC12和Places的常见基准测试中，以显著的优势达到了最先进的性能。一个令人惊讶的结果是，我们学到的特征将自监督学习和监督学习训练的模型在PASCAL VOC 2007上的对象检测中的mAP差距从5.9缩小到2.6。","领域":"自监督学习/知识转移/模型微调","问题":"自监督学习模型在设计和比较不同任务、模型和数据领域时的限制","动机":"克服自监督学习模型设计和比较的限制，提升模型在目标领域和任务上的表现","方法":"提出一个新颖的自监督学习框架，将自监督模型的结构与最终任务特定的微调模型解耦，设计了一个新颖的自监督任务","关键词":["自监督学习","知识转移","模型微调","对象检测"],"涉及的技术概念":"自监督学习是一种无需人工标注的训练模型方法，通过解决预任务来学习数据表示。知识转移指的是将在一个任务或领域中学到的知识应用到另一个任务或领域。模型微调是一种调整预训练模型以适应特定任务的技术。对象检测是计算机视觉中的一个任务，旨在识别图像中的对象并确定其位置。"},{"order":968,"title":"The Power of Ensembles for Active Learning in Image Classification","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Beluch_The_Power_of_CVPR_2018_paper.html","abstract":"Deep learning methods have become the de-facto standard for challenging image processing tasks such as image classification. One major hurdle of deep learning approaches is that large sets of labeled data are necessary, which can be prohibitively costly to obtain, particularly in medical image diagnosis applications. Active learning techniques can alleviate this labeling effort. In this paper we investigate some recently proposed methods for active learning with high-dimensional data and convolutional neural network classifiers. We compare ensemble-based methods against Monte-Carlo Dropout and geometric approaches. We find that ensembles perform better and lead to more calibrated predictive uncertainties, which are the basis for many active learning algorithms. To investigate why Monte-Carlo Dropout uncertainties perform worse, we explore potential differences in isolation in a series of experiments. We show results for MNIST and CIFAR-10, on which we achieve a test set accuracy of $90 %$ with roughly 12,200 labeled images, and initial results on ImageNet. Additionally, we show results on a large, highly class-imbalanced diabetic retinopathy dataset. We observe that the ensemble-based active learning effectively counteracts this imbalance during acquisition.","中文标题":"集成方法在图像分类主动学习中的力量","摘要翻译":"深度学习方法已成为处理图像分类等挑战性图像处理任务的事实标准。深度学习方法的一个主要障碍是需要大量标记数据，这在医学图像诊断应用中尤其昂贵。主动学习技术可以减轻这种标记工作。在本文中，我们研究了一些最近提出的用于高维数据和卷积神经网络分类器的主动学习方法。我们将基于集成的方法与蒙特卡洛Dropout和几何方法进行了比较。我们发现集成方法表现更好，并导致更校准的预测不确定性，这是许多主动学习算法的基础。为了研究为什么蒙特卡洛Dropout的不确定性表现较差，我们在一系列实验中探索了潜在的差异。我们展示了在MNIST和CIFAR-10上的结果，在这些数据集上，我们使用大约12,200张标记图像实现了90%的测试集准确率，以及在ImageNet上的初步结果。此外，我们还展示了一个大型、高度类别不平衡的糖尿病视网膜病变数据集的结果。我们观察到，基于集成的主动学习在获取过程中有效地抵消了这种不平衡。","领域":"图像分类/主动学习/医学图像诊断","问题":"深度学习方法需要大量标记数据，这在医学图像诊断等应用中成本高昂","动机":"减轻深度学习方法中大量标记数据的成本，特别是在医学图像诊断应用中","方法":"比较基于集成的方法与蒙特卡洛Dropout和几何方法在主动学习中的应用","关键词":["集成方法","主动学习","图像分类","医学图像诊断","蒙特卡洛Dropout","几何方法"],"涉及的技术概念":{"主动学习":"一种机器学习方法，旨在通过选择最有信息量的样本进行标记来减少标记数据的数量","集成方法":"通过组合多个模型来提高预测性能的方法","蒙特卡洛Dropout":"一种在训练和推理过程中使用Dropout来估计模型不确定性的方法","几何方法":"基于数据几何特性的主动学习方法"}},{"order":969,"title":"Learning Compact Recurrent Neural Networks With Block-Term Tensor Decomposition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ye_Learning_Compact_Recurrent_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ye_Learning_Compact_Recurrent_CVPR_2018_paper.html","abstract":"Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. However, when dealing with high dimensional inputs, the training of RNNs becomes computational expensive due to the large number of model parameters. This hinders RNNs from solving many important computer vision tasks, such as Action Recognition in Videos and Image Captioning. To overcome this problem, we propose a compact and flexible structure, namely Block-Term tensor decomposition, which greatly reduces the parameters of RNNs and improves their training efficiency. Compared with alternative low-rank approximations, such as tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only more concise (when using the same rank), but also able to attain a better approximation to the original RNNs with much fewer parameters.  On three challenging tasks, including Action Recognition in Videos, Image Captioning and Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes 17,388 times fewer parameters than the standard LSTM to achieve an accuracy improvement over 15.6% in the Action Recognition task on the UCF11 dataset.","中文标题":"学习具有块项张量分解的紧凑循环神经网络","摘要翻译":"循环神经网络（RNNs）是强大的序列建模工具。然而，当处理高维输入时，由于模型参数数量庞大，RNNs的训练变得计算成本高昂。这阻碍了RNNs解决许多重要的计算机视觉任务，如视频中的动作识别和图像字幕生成。为了克服这个问题，我们提出了一种紧凑且灵活的结构，即块项张量分解，它大大减少了RNNs的参数并提高了它们的训练效率。与替代的低秩近似方法（如张量训练RNN（TT-RNN））相比，我们的方法，块项RNN（BT-RNN），不仅在使用相同秩时更为简洁，而且能够以更少的参数实现对原始RNNs的更好近似。在三个具有挑战性的任务上，包括视频中的动作识别、图像字幕生成和图像生成，BT-RNN在预测准确性和收敛速度方面均优于TT-RNN和标准RNN。具体来说，BT-LSTM在UCF11数据集上的动作识别任务中，比标准LSTM少用了17,388倍的参数，实现了超过15.6%的准确率提升。","领域":"动作识别/图像字幕生成/图像生成","问题":"高维输入下RNNs训练计算成本高昂","动机":"提高RNNs在计算机视觉任务中的训练效率和性能","方法":"提出块项张量分解方法，减少RNNs参数并提高训练效率","关键词":["循环神经网络","块项张量分解","动作识别","图像字幕生成","图像生成"],"涉及的技术概念":{"循环神经网络（RNNs）":"一种用于序列建模的神经网络，能够处理序列数据。","块项张量分解":"一种减少神经网络参数数量的方法，通过张量分解技术实现。","张量训练RNN（TT-RNN）":"一种使用张量训练分解技术来减少RNN参数的低秩近似方法。","块项RNN（BT-RNN）":"本文提出的使用块项张量分解技术来减少RNN参数并提高训练效率的方法。","BT-LSTM":"一种特定类型的BT-RNN，使用长短期记忆（LSTM）单元。"}},{"order":970,"title":"Spatially-Adaptive Filter Units for Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Tabernik_Spatially-Adaptive_Filter_Units_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Tabernik_Spatially-Adaptive_Filter_Units_CVPR_2018_paper.html","abstract":"Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter.","中文标题":"深度神经网络的空间自适应滤波单元","摘要翻译":"经典的深度卷积网络通过逐步降低分辨率或应用手工设计的扩张卷积来增加感受野大小，以防止参数数量的增加。在本文中，我们提出了一种新颖的位移聚合单元（DAU），它不需要手工设计。与单元（像素）放置在固定规则网格上的经典滤波器不同，DAU的位移是学习的，这使得滤波器能够根据给定问题空间自适应其感受野。我们在分类和语义分割任务上广泛展示了DAU的优势。与使用常规滤波器的ConvNets相比，使用DAU的ConvNets在更快的收敛速度和最多3倍的参数减少下实现了可比的性能。此外，DAU使我们能够从新的角度研究深度网络。我们研究了DAU滤波器的空间分布，并分析了滤波器为空间覆盖分配的参数数量。","领域":"卷积神经网络/语义分割/参数优化","问题":"如何在不增加参数数量的情况下增加深度卷积网络的感受野大小","动机":"为了解决经典深度卷积网络在增加感受野大小时需要手工设计扩张卷积或降低分辨率的问题，提出一种能够自动学习位移的滤波单元，以实现空间自适应的感受野。","方法":"提出了一种新颖的位移聚合单元（DAU），通过自动学习滤波器的位移来实现空间自适应的感受野，从而在分类和语义分割任务上实现更快的收敛速度和参数减少。","关键词":["位移聚合单元","感受野","参数优化"],"涉及的技术概念":"位移聚合单元（DAU）是一种新型的滤波器设计，它通过自动学习滤波器的位移来实现空间自适应的感受野，从而在深度卷积网络中实现更高效的参数使用和性能提升。"},{"order":971,"title":"SO-Net: Self-Organizing Network for Point Cloud Analysis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_SO-Net_Self-Organizing_Network_CVPR_2018_paper.html","abstract":"This paper presents SO-Net, a permutation invariant architecture for deep learning with orderless point clouds. The SO-Net models the spatial distribution of point cloud by building a Self-Organizing Map (SOM). Based on the SOM, SO-Net performs hierarchical feature extraction on individual points and SOM nodes, and ultimately represents the input point cloud by a single feature vector. The receptive field of the network can be systematically adjusted by conducting point-to-node k nearest neighbor search. In recognition tasks such as point cloud reconstruction, classification, object part segmentation and shape retrieval, our proposed network demonstrates performance that is similar with or better than state-of-the-art approaches. In addition, the training speed is significantly faster than existing point cloud recognition networks because of the parallelizability and simplicity of the proposed architecture. Our code is available at the project website.","中文标题":"SO-Net：用于点云分析的自组织网络","摘要翻译":"本文介绍了SO-Net，一种用于无序点云深度学习的排列不变架构。SO-Net通过构建自组织映射（SOM）来建模点云的空间分布。基于SOM，SO-Net对单个点和SOM节点进行分层特征提取，并最终通过单个特征向量表示输入点云。网络的感受野可以通过进行点到节点的k最近邻搜索来系统调整。在点云重建、分类、对象部分分割和形状检索等识别任务中，我们提出的网络展示了与最先进方法相似或更好的性能。此外，由于所提出架构的并行性和简单性，训练速度显著快于现有的点云识别网络。我们的代码可在项目网站上获取。","领域":"点云分析/三维视觉/深度学习","问题":"处理无序点云数据的深度学习架构设计","动机":"提高点云数据处理效率和性能，特别是在点云重建、分类、对象部分分割和形状检索等任务中","方法":"通过构建自组织映射（SOM）来建模点云的空间分布，并基于SOM进行分层特征提取，最终通过单个特征向量表示输入点云","关键词":["点云分析","自组织映射","深度学习架构"],"涉及的技术概念":"自组织映射（SOM）是一种无监督学习技术，用于将高维数据映射到低维空间，同时保持数据的拓扑结构。在本文中，SOM用于建模点云的空间分布，以便进行有效的特征提取和表示。点到节点的k最近邻搜索是一种用于调整网络感受野的技术，通过找到每个点的k个最近邻节点来实现。"},{"order":972,"title":"SGAN: An Alternative Training of Generative Adversarial Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chavdarova_SGAN_An_Alternative_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chavdarova_SGAN_An_Alternative_CVPR_2018_paper.html","abstract":"The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them.  We consider an alternative training process, named SGAN, in which several adversarial \\"local\\" pairs of networks are trained independently so that a \\"global\\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones.  The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent.  Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.","中文标题":"SGAN: 生成对抗网络的另一种训练方法","摘要翻译":"生成对抗网络（GANs）在数据合成方面展示了令人印象深刻的性能，并已广泛应用于计算机视觉任务中。尽管取得了这些成功，它们因难以训练而闻名，这导致使用它们的过程耗时且需要人工参与。我们考虑了一种名为SGAN的替代训练过程，其中几个对抗性的“局部”网络对被独立训练，以便可以针对它们训练一个“全局”监督网络对。目标是通过相应的集成对手训练全局对，以提高模式覆盖率的性能。这种方法旨在增加全局对学习不会停止的机会，防止两者陷入不满意的局部最小值，或面对实践中经常观察到的振荡。为了保证后者，全局对永远不会影响局部对。因此，SGAN训练的规则如下：全局生成器和判别器分别使用局部判别器和生成器进行训练，而局部网络则与它们固定的局部对手一起训练。在玩具问题和现实世界问题上的实验结果表明，这种方法在更好地缓解模式崩溃、收敛时的稳定性方面优于标准训练，并且令人惊讶的是，它还提高了收敛速度。","领域":"生成对抗网络/数据合成/模式覆盖","问题":"生成对抗网络训练困难，导致使用过程耗时且需要人工参与","动机":"提高生成对抗网络的训练效率和稳定性，防止陷入局部最小值和振荡","方法":"提出SGAN训练方法，通过独立训练多个局部对抗网络对，来训练一个全局监督网络对，以提高模式覆盖率和训练稳定性","关键词":["生成对抗网络","数据合成","模式覆盖","训练稳定性","收敛速度"],"涉及的技术概念":"生成对抗网络（GANs）是一种用于数据合成的深度学习模型，由生成器和判别器组成，通过对抗过程学习数据分布。SGAN是一种改进的训练方法，通过引入局部对抗网络对和全局监督网络对，旨在提高训练效率和稳定性，防止模式崩溃和振荡。"},{"order":973,"title":"SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_SketchyGAN_Towards_Diverse_CVPR_2018_paper.html","abstract":"Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.","中文标题":"SketchyGAN：面向多样化和逼真的草图到图像合成","摘要翻译":"从人类绘制的草图中合成逼真的图像是计算机图形学和视觉中的一个挑战性问题。现有方法要么需要精确的边缘图，要么依赖于现有照片的检索。在这项工作中，我们提出了一种新颖的生成对抗网络（GAN）方法，能够从包括摩托车、马和沙发在内的50个类别中合成合理的图像。我们展示了一种完全自动化的草图数据增强技术，并证明增强的数据对我们的任务有帮助。我们引入了一种新的网络构建块，适用于生成器和判别器，通过在多尺度上注入输入图像来改善信息流。与最先进的图像翻译方法相比，我们的方法生成了更逼真的图像，并显著提高了Inception分数。","领域":"图像合成/生成对抗网络/数据增强","问题":"从人类绘制的草图中合成逼真的图像","动机":"解决现有方法需要精确边缘图或依赖现有照片检索的问题，提高图像合成的逼真度和多样性","方法":"提出了一种新颖的生成对抗网络（GAN）方法，采用数据增强技术，并引入新的网络构建块以改善信息流","关键词":["图像合成","生成对抗网络","数据增强","草图处理","多尺度信息流"],"涉及的技术概念":"生成对抗网络（GAN）是一种深度学习模型，由生成器和判别器组成，用于生成逼真的图像。数据增强技术通过自动增加训练数据的多样性来提高模型的泛化能力。多尺度信息流指的是在网络的不同层次上处理输入图像，以捕捉更丰富的特征。Inception分数是一种评估生成图像质量的指标，基于Inception模型对图像进行分类的能力。"},{"order":974,"title":"Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Explicit_Loss-Error-Aware_Quantization_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_Explicit_Loss-Error-Aware_Quantization_CVPR_2018_paper.html","abstract":"Benefiting from tens of millions of hierarchically stacked learnable parameters, Deep Neural Networks (DNNs) have demonstrated overwhelming accuracy on a variety of artificial intelligence tasks. However reversely, the large size of DNN models lays a heavy burden on storage, computation and power consumption, which prohibits their deployments on the embedded and mobile systems. In this paper, we propose Explicit Loss-error-aware Quantization (ELQ), a new method that can train DNN models with very low-bit parameter values such as ternary and binary ones to approximate 32-bit floating-point counterparts without noticeable loss of predication accuracy. Unlike existing methods that usually pose the problem as a straightforward approximation of the layer-wise weights or outputs of the original full-precision model (specifically, minimizing the error of the layer-wise weights or inner products of the weights and the inputs between the original and respective quantized models), our ELQ elaborately bridges the loss perturbation from the weight quantization and an incremental quantization strategy to address DNN quantization. Through explicitly regularizing the loss perturbation and the weight approximation error in an incremental way, we show that such a new optimization method is theoretically reasonable and practically effective. As validated with two mainstream convolutional neural network families (i.e., fully convolutional and non-fully convolutional), our ELQ shows better results than the state-of-the-art quantization methods on the large scale ImageNet classification dataset. Code will be made publicly available.","中文标题":"显式损失误差感知量化用于低位深度神经网络","摘要翻译":"得益于数千万层次堆叠的可学习参数，深度神经网络（DNNs）在各种人工智能任务上展示了压倒性的准确性。然而，反过来，DNN模型的大尺寸对存储、计算和功耗造成了沉重负担，这阻碍了它们在嵌入式和移动系统上的部署。在本文中，我们提出了显式损失误差感知量化（ELQ），这是一种新方法，可以训练具有非常低位参数值（如三元和二元）的DNN模型，以近似32位浮点对应物，而不会显著损失预测准确性。与现有方法通常将问题视为原始全精度模型的层间权重或输出的直接近似（具体来说，最小化原始和相应量化模型之间的层间权重或权重与输入的内积的误差）不同，我们的ELQ精心桥接了权重量化带来的损失扰动和增量量化策略，以解决DNN量化问题。通过显式地以增量方式正则化损失扰动和权重近似误差，我们展示了这种新的优化方法在理论上是合理的，在实践中是有效的。正如在两个主流的卷积神经网络家族（即全卷积和非全卷积）上验证的那样，我们的ELQ在大规模ImageNet分类数据集上展示了比最先进的量化方法更好的结果。代码将公开提供。","领域":"神经网络量化/模型压缩/嵌入式系统","问题":"深度神经网络模型的大尺寸对存储、计算和功耗造成了沉重负担，阻碍了在嵌入式和移动系统上的部署","动机":"为了减少深度神经网络模型的存储、计算和功耗负担，使其更适合在嵌入式和移动系统上部署","方法":"提出了一种新的显式损失误差感知量化（ELQ）方法，通过显式地以增量方式正则化损失扰动和权重近似误差，训练具有非常低位参数值的DNN模型","关键词":["神经网络量化","模型压缩","嵌入式系统"],"涉及的技术概念":"显式损失误差感知量化（ELQ）是一种新的量化方法，旨在通过显式地以增量方式正则化损失扰动和权重近似误差，训练具有非常低位参数值的DNN模型，以近似32位浮点对应物，而不会显著损失预测准确性。这种方法特别适用于减少深度神经网络模型的存储、计算和功耗负担，使其更适合在嵌入式和移动系统上部署。"},{"order":975,"title":"Towards Universal Representation for Unseen Action Recognition","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_Towards_Universal_Representation_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html","abstract":"Unseen Action Recognition (UAR) aims to recognise novel action categories without training examples. While previous methods focus on inner-dataset seen/unseen splits, this paper proposes a pipeline using a large-scale training source to achieve a Universal Representation (UR) that can generalise to a more realistic Cross-Dataset UAR (CD-UAR) scenario. We first address UAR as a Generalised Multiple-Instance Learning (GMIL) problem and discover \\"building-blocks\\" from the large-scale ActivityNet dataset using distribution kernels. Essential visual and semantic components are preserved in a shared space to achieve the UR that can efficiently generalise to new datasets. Predicted UR exemplars can be improved by a simple semantic adaptation, and then an unseen action can be directly recognised using UR during the test. Without further training, extensive experiments manifest significant improvements over the UCF101 and HMDB51 benchmarks.","中文标题":"面向未见动作识别的通用表示","摘要翻译":"未见动作识别（UAR）旨在无需训练样本的情况下识别新的动作类别。虽然之前的方法侧重于数据集内部可见/未见分割，本文提出了一种使用大规模训练源来实现通用表示（UR）的流程，该表示能够推广到更现实的跨数据集UAR（CD-UAR）场景。我们首先将UAR视为广义多实例学习（GMIL）问题，并使用分布核从大规模ActivityNet数据集中发现“构建块”。在共享空间中保留了关键的视觉和语义组件，以实现能够有效推广到新数据集的UR。通过简单的语义适应可以改进预测的UR示例，然后在测试期间可以直接使用UR识别未见动作。无需进一步训练，大量实验表明在UCF101和HMDB51基准上取得了显著改进。","领域":"动作识别/跨数据集学习/语义适应","问题":"如何在无需训练样本的情况下识别新的动作类别","动机":"解决跨数据集未见动作识别的挑战，提高模型对新数据集的泛化能力","方法":"提出一种使用大规模训练源实现通用表示的流程，通过广义多实例学习问题和分布核发现关键视觉和语义组件，并在共享空间中保留这些组件以实现通用表示","关键词":["未见动作识别","通用表示","跨数据集学习","广义多实例学习","语义适应"],"涉及的技术概念":{"未见动作识别（UAR）":"识别训练数据中未出现的新动作类别","通用表示（UR）":"一种能够推广到新数据集的表示方法","跨数据集UAR（CD-UAR）":"在跨数据集场景下进行未见动作识别","广义多实例学习（GMIL）":"一种机器学习方法，用于处理每个样本由多个实例组成的问题","分布核":"用于从数据中发现关键组件的技术","语义适应":"通过调整语义信息来改进模型性能的方法"}},{"order":976,"title":"Deep Image Prior","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.html","abstract":"Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.  Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity.","中文标题":"深度图像先验","摘要翻译":"深度卷积网络已成为图像生成和修复的流行工具。通常，它们的优异性能归功于它们能够从大量示例图像中学习到真实的图像先验。在本文中，我们展示了相反的情况，即在任何学习之前，生成器网络的结构足以捕获大量的低级图像统计信息。为了做到这一点，我们展示了一个随机初始化的神经网络可以用作手工先验，在标准的逆问题（如去噪、超分辨率和修复）中取得了优异的结果。此外，同样的先验可以用于反转深度神经表示以诊断它们，并基于闪光-非闪光输入对恢复图像。除了其多样化的应用外，我们的方法还突出了标准生成器网络架构捕获的归纳偏差。它还弥合了两种非常流行的图像修复方法家族之间的差距：使用深度卷积网络的学习方法和基于手工图像先验（如自相似性）的无学习方法。","领域":"图像生成/图像修复/神经网络","问题":"解决图像生成和修复中的先验学习问题","动机":"探索生成器网络结构在捕获低级图像统计信息方面的能力，以及随机初始化神经网络作为手工先验在图像修复中的应用","方法":"使用随机初始化的神经网络作为手工先验，应用于图像去噪、超分辨率、修复等标准逆问题，以及深度神经表示的反转和基于闪光-非闪光输入对的图像恢复","关键词":["图像生成","图像修复","神经网络","手工先验","自相似性"],"涉及的技术概念":"深度卷积网络、生成器网络、图像先验、随机初始化神经网络、逆问题（去噪、超分辨率、修复）、深度神经表示、闪光-非闪光输入对、归纳偏差、自相似性"},{"order":977,"title":"ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Lin_ST-GAN_Spatial_Transformer_CVPR_2018_paper.html","abstract":"We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.","中文标题":"ST-GAN: 用于图像合成的空间变换生成对抗网络","摘要翻译":"我们解决了为前景对象寻找现实几何校正的问题，使其在合成到背景图像中时显得自然。为了实现这一点，我们提出了一种新颖的生成对抗网络（GAN）架构，该架构利用空间变换网络（STNs）作为生成器，我们称之为空间变换GAN（ST-GANs）。ST-GANs通过在几何扭曲参数空间中操作来寻求图像的真实感。特别是，我们利用了一种迭代的STN扭曲方案，并提出了一种顺序训练策略，与单一生成器的简单训练相比，取得了更好的结果。ST-GAN的一个关键优势是它对高分辨率图像的间接适用性，因为预测的扭曲参数在参考帧之间是可转移的。我们在两个应用中展示了我们的方法：（1）可视化室内家具（例如来自产品图像）在房间中可能被感知的方式，（2）幻觉配件如眼镜与真实肖像匹配时的外观。","领域":"图像合成/几何校正/高分辨率图像处理","问题":"为前景对象寻找现实几何校正，使其在合成到背景图像中时显得自然","动机":"提高图像合成的真实感，特别是在高分辨率图像中的应用","方法":"提出了一种新颖的生成对抗网络（GAN）架构，利用空间变换网络（STNs）作为生成器，采用迭代的STN扭曲方案和顺序训练策略","关键词":["图像合成","几何校正","高分辨率图像处理","空间变换网络","生成对抗网络"],"涉及的技术概念":"空间变换网络（STNs）用于生成器，生成对抗网络（GANs）用于提高图像合成的真实感，通过几何扭曲参数空间操作，以及顺序训练策略用于优化训练过程。"},{"order":978,"title":"CartoonGAN: Generative Adversarial Networks for Photo Cartoonization","pdf":"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf","html":"https://openaccess.thecvf.com/content_cvpr_2018/html/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.html","abstract":"In this paper, we propose a solution to transforming photos of real-world scenes into cartoon style images, which is valuable and challenging in computer vision and computer graphics. Our solution belongs to learning based methods, which have recently become popular to stylize images in artistic forms such as painting. However, existing methods do not produce satisfactory results for cartoonization, due to the fact that (1) cartoon styles have unique characteristics with high level simplification and abstraction, and (2) cartoon images tend to have clear edges, smooth color shading and relatively simple textures, which exhibit significant challenges for texture-descriptor-based loss functions used in existing methods. In this paper, we propose CartoonGAN, a generative adversarial network (GAN) framework for cartoon stylization. Our method takes unpaired photos and cartoon images for training, which is easy to use. Two novel losses suitable for cartoonization are proposed: (1) a semantic content loss, which is formulated as a sparse regularization in the high-level feature maps of the VGG network to cope with substantial style variation between photos and cartoons, and (2) an edge-promoting adversarial loss for preserving clear edges. We further introduce an initialization phase, to improve the convergence of the network to the target manifold. Our method is also much more efficient to train than existing methods. Experimental results show that our method is able to generate high-quality cartoon images from real-world photos (i.e., following specific artists' styles and with clear edges and smooth shading) and outperforms state-of-the-art methods.","中文标题":"CartoonGAN: 用于照片卡通化的生成对抗网络","摘要翻译":"在本文中，我们提出了一种将现实世界场景的照片转换为卡通风格图像的解决方案，这在计算机视觉和计算机图形学中既具有价值又具有挑战性。我们的解决方案属于基于学习的方法，这些方法最近在艺术形式（如绘画）的图像风格化中变得流行。然而，现有方法在卡通化方面并未产生令人满意的结果，原因是（1）卡通风格具有独特的高层次简化和抽象特征，以及（2）卡通图像往往具有清晰的边缘、平滑的颜色渐变和相对简单的纹理，这对现有方法中使用的基于纹理描述符的损失函数构成了重大挑战。在本文中，我们提出了CartoonGAN，一种用于卡通风格化的生成对抗网络（GAN）框架。我们的方法采用未配对的照片和卡通图像进行训练，易于使用。提出了两种适合卡通化的新损失：（1）语义内容损失，它被表述为VGG网络高级特征图中的稀疏正则化，以应对照片和卡通之间显著的风格变化，以及（2）边缘促进对抗损失，用于保持清晰的边缘。我们进一步引入了一个初始化阶段，以改善网络向目标流形的收敛。我们的方法在训练效率上也远高于现有方法。实验结果表明，我们的方法能够从现实世界的照片中生成高质量的卡通图像（即遵循特定艺术家的风格，具有清晰的边缘和平滑的渐变），并且优于最先进的方法。","领域":"图像风格化/生成对抗网络/卡通化","问题":"将现实世界场景的照片转换为卡通风格图像","动机":"现有方法在卡通化方面未产生令人满意的结果，因为卡通风格具有独特的高层次简化和抽象特征，以及卡通图像具有清晰的边缘、平滑的颜色渐变和相对简单的纹理，这对现有方法中使用的基于纹理描述符的损失函数构成了重大挑战。","方法":"提出了CartoonGAN，一种用于卡通风格化的生成对抗网络（GAN）框架，采用未配对的照片和卡通图像进行训练，提出了语义内容损失和边缘促进对抗损失两种新损失，并引入了一个初始化阶段以改善网络向目标流形的收敛。","关键词":["图像风格化","生成对抗网络","卡通化"],"涉及的技术概念":"生成对抗网络（GAN）、VGG网络、语义内容损失、边缘促进对抗损失、初始化阶段"}]`);export{e as default};
