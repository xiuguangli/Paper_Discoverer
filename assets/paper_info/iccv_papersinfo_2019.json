[
    {
        "order": 0,
        "title": "FaceForensics++: Learning to Detect Manipulated Facial Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rossler_FaceForensics_Learning_to_Detect_Manipulated_Facial_Images_ICCV_2019_paper.html",
        "abstract": "The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.",
        "中文标题": "FaceForensics++：学习检测被操纵的面部图像",
        "摘要翻译": "合成图像生成和操纵的快速进展已经到了对社会产生重大影响的地步。这最多会导致对数字内容的信任丧失，但通过传播虚假信息或假新闻，可能会造成进一步的伤害。本文研究了最先进的图像操纵的真实性，以及自动或人类检测它们的难度。为了标准化检测方法的评估，我们提出了一个面部操纵检测的自动化基准。特别是，该基准基于Deep-Fakes、Face2Face、FaceSwap和NeuralTextures作为随机压缩级别和大小下面部操纵的突出代表。该基准公开可用，包含一个隐藏的测试集以及一个超过180万张被操纵图像的数据库。这个数据集比可比的、公开可用的伪造数据集大一个数量级。基于这些数据，我们对数据驱动的伪造检测器进行了彻底的分析。我们展示了使用额外的领域特定知识可以将伪造检测提高到前所未有的准确性，即使在强压缩的情况下，也明显优于人类观察者。",
        "领域": "图像伪造检测/面部识别/数据驱动分析",
        "问题": "检测被操纵的面部图像",
        "动机": "合成图像生成和操纵技术的进步对社会产生了重大影响，可能导致对数字内容的信任丧失和虚假信息的传播",
        "方法": "提出了一个基于Deep-Fakes、Face2Face、FaceSwap和NeuralTextures的面部操纵检测自动化基准，并进行了数据驱动的伪造检测器分析",
        "关键词": [
            "图像伪造检测",
            "面部识别",
            "数据驱动分析"
        ],
        "涉及的技术概念": "Deep-Fakes、Face2Face、FaceSwap和NeuralTextures是用于面部图像操纵的技术，本文通过建立一个包含超过180万张被操纵图像的数据库，对这些技术进行了评估和分析，展示了使用领域特定知识可以提高伪造检测的准确性。"
    },
    {
        "order": 1,
        "title": "DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_DeepVCP_An_End-to-End_Deep_Neural_Network_for_Point_Cloud_Registration_ICCV_2019_paper.html",
        "abstract": "We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task.",
        "中文标题": "DeepVCP: 一种用于点云注册的端到端深度神经网络",
        "摘要翻译": "我们提出了DeepVCP——一种新颖的基于端到端学习的3D点云注册框架，其注册精度可与之前的最先进几何方法相媲美。与其他通常需要RANSAC程序的关键点方法不同，我们实现了使用各种深度神经网络结构来建立一个端到端可训练的网络。我们的关键点检测器通过这种端到端结构进行训练，使系统能够避免动态物体的干扰，利用静止物体上足够显著的特征，从而实现了高鲁棒性。与在现有点中搜索对应点不同，我们的关键贡献是创新性地基于一组候选点之间的学习匹配概率生成它们，这可以提高注册精度。我们使用KITTI数据集和Apollo-SouthBay数据集全面验证了我们方法的有效性。结果表明，我们的方法在注册精度和运行效率上与最先进的基于几何的方法相当，但对不准确的初始姿态具有更高的鲁棒性。包括详细的消融和可视化分析，以进一步说明我们网络的行为和见解。我们方法的低注册误差和高鲁棒性使其对依赖点云注册任务的大量应用具有吸引力。",
        "领域": "点云处理/3D视觉/自动驾驶",
        "问题": "提高点云注册的精度和鲁棒性",
        "动机": "解决现有方法在处理动态物体和初始姿态不准确时的不足",
        "方法": "使用端到端深度神经网络结构，创新性地基于学习匹配概率生成对应点",
        "关键词": [
            "点云注册",
            "深度神经网络",
            "端到端学习",
            "鲁棒性",
            "3D视觉"
        ],
        "涉及的技术概念": "DeepVCP是一种端到端学习的3D点云注册框架，通过深度神经网络结构实现，避免了RANSAC程序的需要，提高了对动态物体和初始姿态不准确的鲁棒性。关键点检测器通过端到端结构训练，利用静止物体上的显著特征，创新性地基于学习匹配概率生成对应点，从而提高注册精度。"
    },
    {
        "order": 2,
        "title": "3D-RelNet: Joint Object and Relational Network for 3D Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kulkarni_3D-RelNet_Joint_Object_and_Relational_Network_for_3D_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kulkarni_3D-RelNet_Joint_Object_and_Relational_Network_for_3D_Prediction_ICCV_2019_paper.html",
        "abstract": "We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets (SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods.",
        "中文标题": "3D-RelNet: 用于3D预测的联合对象和关系网络",
        "摘要翻译": "我们提出了一种方法来预测场景中物体的3D形状和姿态。追求这一目标的现有基于学习的方法对每个物体进行独立预测，并未利用它们之间的关系。我们认为推理这些关系至关重要，并提出了一种将这些关系纳入3D预测框架的方法。除了独立的每个物体预测外，我们还以相对3D姿态的形式预测成对关系，并证明这些可以很容易地纳入以提高物体级别的估计。我们在不同的数据集（SUNCG, NYUv2）上报告了性能，并显示我们的方法显著优于独立预测方法，同时也优于其他隐式推理方法。",
        "领域": "3D视觉/场景理解/物体识别",
        "问题": "如何有效预测场景中物体的3D形状和姿态，同时利用物体间的关系提高预测准确性",
        "动机": "现有方法独立预测每个物体的3D形状和姿态，忽略了物体间的关系，这限制了预测的准确性和场景理解的深度",
        "方法": "提出了一种联合对象和关系网络（3D-RelNet），在独立预测每个物体的3D形状和姿态的基础上，增加对物体间相对3D姿态的预测，并将这些关系纳入3D预测框架中",
        "关键词": [
            "3D形状预测",
            "3D姿态预测",
            "物体关系推理"
        ],
        "涉及的技术概念": "3D-RelNet是一种深度学习模型，旨在通过联合考虑物体及其相互关系来预测3D形状和姿态。该方法通过预测物体间的相对3D姿态，并将这些信息整合到3D预测框架中，以提高物体级别估计的准确性。这种方法在SUNCG和NYUv2等数据集上进行了测试，显示出比独立预测方法和隐式推理方法更好的性能。"
    },
    {
        "order": 3,
        "title": "Shape Reconstruction Using Differentiable Projections and Deep Priors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gadelha_Shape_Reconstruction_Using_Differentiable_Projections_and_Deep_Priors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gadelha_Shape_Reconstruction_Using_Differentiable_Projections_and_Deep_Priors_ICCV_2019_paper.html",
        "abstract": "We investigate the problem of reconstructing shapes from noisy and incomplete projections in the presence of viewpoint uncertainities. The problem is cast as an optimization over the shape given measurements obtained by a projection operator and a prior. We present differentiable projection operators for a number of reconstruction problems which when combined with the deep image prior or shape prior allows efficient inference through gradient descent. We apply our method on a variety of reconstruction problems, such as tomographic reconstruction from a few samples, visual hull reconstruction incorporating view uncertainties, and 3D shape reconstruction from noisy depth maps. Experimental results show that our approach is effective for such shape reconstruction problems, without requiring any task-specific training.",
        "中文标题": "使用可微分投影和深度先验进行形状重建",
        "摘要翻译": "我们研究了在视角不确定性的情况下，从噪声和不完整的投影中重建形状的问题。该问题被表述为在给定通过投影算子和先验获得的测量值的情况下对形状进行优化。我们为多个重建问题提出了可微分投影算子，这些算子与深度图像先验或形状先验结合时，允许通过梯度下降进行有效推理。我们将我们的方法应用于各种重建问题，如从少量样本中进行断层扫描重建、结合视角不确定性的视觉外壳重建，以及从噪声深度图中进行3D形状重建。实验结果表明，我们的方法对此类形状重建问题有效，且不需要任何特定任务的训练。",
        "领域": "三维重建/断层扫描/深度图处理",
        "问题": "从噪声和不完整的投影中重建形状",
        "动机": "解决在视角不确定性的情况下，如何有效地从噪声和不完整的投影中重建形状的问题",
        "方法": "提出可微分投影算子，并与深度图像先验或形状先验结合，通过梯度下降进行有效推理",
        "关键词": [
            "三维重建",
            "断层扫描",
            "深度图处理",
            "可微分投影",
            "深度先验"
        ],
        "涉及的技术概念": {
            "可微分投影算子": "用于多个重建问题的投影算子，允许通过梯度下降进行优化",
            "深度图像先验": "一种利用深度神经网络作为先验知识的方法，用于图像重建",
            "形状先验": "在形状重建中使用的先验知识，有助于指导重建过程",
            "梯度下降": "一种优化算法，用于最小化或最大化目标函数",
            "断层扫描重建": "从多个角度的投影数据中重建物体的内部结构",
            "视觉外壳重建": "从多个视角的轮廓信息中重建物体的三维形状",
            "3D形状重建": "从二维图像或深度图中重建三维物体的形状"
        }
    },
    {
        "order": 4,
        "title": "Feature Weighting and Boosting for Few-Shot Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Feature_Weighting_and_Boosting_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Feature_Weighting_and_Boosting_for_Few-Shot_Segmentation_ICCV_2019_paper.html",
        "abstract": "This paper is about few-shot segmentation of foreground objects in images. We train a CNN on small subsets of training images, each mimicking the few-shot setting. In each subset, one image serves as the query and the other(s) as support image(s) with ground-truth segmentation. The CNN first extracts feature maps from the query and support images. Then, a class feature vector is computed as an average of the support's feature maps over the known foreground. Finally, the target object is segmented in the query image by using a cosine similarity between the class feature vector and the query's feature map. We make two contributions by: (1) Improving discriminativeness of features so their activations are high on the foreground and low elsewhere; and (2) Boosting inference with an ensemble of experts guided with the gradient of loss incurred when segmenting the support images in testing. Our evaluations on the PASCAL-5i and COCO-20i datasets demonstrate that we significantly outperform existing approaches.",
        "中文标题": "特征加权与增强用于少样本分割",
        "摘要翻译": "本文讨论了图像中前景物体的少样本分割问题。我们在训练图像的小子集上训练卷积神经网络（CNN），每个子集都模仿少样本设置。在每个子集中，一张图像作为查询图像，其他图像作为带有真实分割的支持图像。CNN首先从查询图像和支持图像中提取特征图。然后，通过已知前景上的支持图像特征图的平均值计算类别特征向量。最后，通过使用类别特征向量与查询图像特征图之间的余弦相似度，在查询图像中分割目标物体。我们做出了两个贡献：（1）提高特征的区分性，使其在前景上的激活度高，在其他地方低；（2）通过使用在测试中分割支持图像时产生的损失梯度引导的专家集合来增强推理。我们在PASCAL-5i和COCO-20i数据集上的评估表明，我们显著优于现有方法。",
        "领域": "少样本学习/图像分割/卷积神经网络",
        "问题": "解决图像中前景物体的少样本分割问题",
        "动机": "提高在少量样本情况下图像分割的准确性和效率",
        "方法": "通过在训练图像的小子集上训练CNN，并利用类别特征向量与查询图像特征图之间的余弦相似度进行分割，同时提高特征的区分性和通过专家集合增强推理",
        "关键词": [
            "少样本学习",
            "图像分割",
            "卷积神经网络",
            "特征加权",
            "增强推理"
        ],
        "涉及的技术概念": {
            "少样本分割": "在只有少量样本的情况下进行图像分割的技术",
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "特征图": "通过卷积神经网络从图像中提取的特征表示",
            "类别特征向量": "通过支持图像特征图的平均值计算得到的向量，用于表示特定类别的特征",
            "余弦相似度": "一种衡量两个向量方向相似度的方法，用于比较类别特征向量与查询图像特征图之间的相似度",
            "专家集合": "一种通过组合多个模型或方法来提高预测准确性的技术"
        }
    },
    {
        "order": 5,
        "title": "Sampling-Free Epistemic Uncertainty Estimation Using Approximated Variance Propagation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Postels_Sampling-Free_Epistemic_Uncertainty_Estimation_Using_Approximated_Variance_Propagation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Postels_Sampling-Free_Epistemic_Uncertainty_Estimation_Using_Approximated_Variance_Propagation_ICCV_2019_paper.html",
        "abstract": "We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.",
        "中文标题": "无需采样的认知不确定性估计使用近似方差传播",
        "摘要翻译": "我们提出了一种无需采样的方法来计算神经网络的认知不确定性。认知不确定性对于在安全关键应用中部署深度神经网络是一个重要的量，因为它代表了人们可以对新数据的预测信任多少。最近，有前景的工作提出了在推理时结合噪声注入和蒙特卡罗采样来估计这一量（例如蒙特卡罗dropout）。我们的主要贡献是这些方法估计的认知不确定性的近似，它不需要采样，从而显著减少了计算开销。我们将我们的方法应用于大规模视觉任务（即语义分割和深度回归），以展示我们的方法在不确定性估计质量以及计算开销方面与基于采样的方法相比的优势。",
        "领域": "语义分割/深度回归/不确定性估计",
        "问题": "如何在减少计算开销的同时准确估计神经网络的认知不确定性",
        "动机": "为了在安全关键应用中更可靠地部署深度神经网络，需要一种更高效的方法来估计认知不确定性",
        "方法": "提出了一种无需采样的认知不确定性估计方法，通过近似方差传播来减少计算开销",
        "关键词": [
            "认知不确定性",
            "无需采样",
            "近似方差传播",
            "语义分割",
            "深度回归"
        ],
        "涉及的技术概念": "认知不确定性指的是模型对于其预测的不确定性，特别是在面对未见过的数据时。蒙特卡罗dropout是一种通过在推理时随机丢弃神经元来估计模型不确定性的方法。近似方差传播是一种无需进行多次采样即可估计模型不确定性的技术，它通过数学近似来减少计算需求。"
    },
    {
        "order": 6,
        "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Larsson_Fine-Grained_Segmentation_Networks_Self-Supervised_Segmentation_for_Improved_Long-Term_Visual_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Larsson_Fine-Grained_Segmentation_Networks_Self-Supervised_Segmentation_for_Improved_Long-Term_Visual_Localization_ICCV_2019_paper.html",
        "abstract": "Long-term visual localization is the problem of estimating the camera pose of a given query image in a scene whose appearance changes over time. It is an important problem in practice that is, for example, encountered in autonomous driving. In order to gain robustness to such changes, long-term localization approaches often use segmantic segmentations as an invariant scene representation, as the semantic meaning of each scene part should not be affected by seasonal and other changes. However, these representations are typically not very discriminative due to the very limited number of available classes. In this paper, we propose a novel neural network, the Fine-Grained Segmentation Network (FGSN), that can be used to provide image segmentations with a larger number of labels and can be trained in a self-supervised fashion. In addition, we show how FGSNs can be trained to output consistent labels across seasonal changes. We show through extensive experiments that integrating the fine-grained segmentations produced by our FGSNs into existing localization algorithms leads to substantial improvements in localization performance.",
        "中文标题": "细粒度分割网络：自监督分割以改进长期视觉定位",
        "摘要翻译": "长期视觉定位是指在场景外观随时间变化的情况下，估计给定查询图像的相机姿态的问题。这是一个在实践中非常重要的问题，例如在自动驾驶中会遇到。为了获得对此类变化的鲁棒性，长期定位方法通常使用语义分割作为不变场景表示，因为每个场景部分的语义含义不应受季节和其他变化的影响。然而，由于可用类别的数量非常有限，这些表示通常不具有很强的区分性。在本文中，我们提出了一种新颖的神经网络，即细粒度分割网络（FGSN），它可以用于提供具有更多标签的图像分割，并且可以以自监督的方式进行训练。此外，我们展示了如何训练FGSN以输出跨季节变化的一致标签。我们通过大量实验表明，将我们的FGSN生成的细粒度分割集成到现有的定位算法中，可以显著提高定位性能。",
        "领域": "自动驾驶/场景理解/视觉定位",
        "问题": "长期视觉定位中由于场景外观随时间变化导致的相机姿态估计问题",
        "动机": "提高长期视觉定位的鲁棒性和准确性，特别是在自动驾驶等应用中",
        "方法": "提出了一种新颖的细粒度分割网络（FGSN），该网络能够提供更多标签的图像分割，并以自监督的方式进行训练，同时能够输出跨季节变化的一致标签",
        "关键词": [
            "细粒度分割",
            "自监督学习",
            "视觉定位",
            "自动驾驶",
            "场景理解"
        ],
        "涉及的技术概念": "细粒度分割网络（FGSN）是一种能够提供更多标签的图像分割的神经网络，通过自监督学习进行训练，能够输出跨季节变化的一致标签，从而提高长期视觉定位的准确性和鲁棒性。"
    },
    {
        "order": 7,
        "title": "Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Abdal_Image2StyleGAN_How_to_Embed_Images_Into_the_StyleGAN_Latent_Space_ICCV_2019_paper.html",
        "abstract": "We propose an efficient algorithm to embed a given image into the latent space of StyleGAN. This embedding enables semantic image editing operations that can be applied to existing photographs. Taking the StyleGAN trained on the FFHD dataset as an example, we show results for image morphing, style transfer, and expression transfer. Studying the results of the embedding algorithm provides valuable insights into the structure of the StyleGAN latent space. We propose a set of experiments to test what class of images can be embedded, how they are embedded, what latent space is suitable for embedding, and if the embedding is semantically meaningful.",
        "中文标题": "Image2StyleGAN: 如何将图像嵌入到StyleGAN的潜在空间中？",
        "摘要翻译": "我们提出了一种高效的算法，用于将给定图像嵌入到StyleGAN的潜在空间中。这种嵌入使得可以对现有照片应用语义图像编辑操作。以在FFHD数据集上训练的StyleGAN为例，我们展示了图像变形、风格转移和表情转移的结果。研究嵌入算法的结果为了解StyleGAN潜在空间的结构提供了宝贵的见解。我们提出了一系列实验来测试可以嵌入哪类图像、它们是如何被嵌入的、哪种潜在空间适合嵌入，以及嵌入是否具有语义意义。",
        "领域": "生成对抗网络/图像编辑/语义理解",
        "问题": "如何将图像高效嵌入到StyleGAN的潜在空间中以实现语义图像编辑",
        "动机": "探索和利用StyleGAN潜在空间的结构，以实现对现有照片的语义图像编辑操作",
        "方法": "提出一种高效的嵌入算法，并通过实验测试嵌入的可行性、方法和语义意义",
        "关键词": [
            "图像嵌入",
            "语义编辑",
            "潜在空间",
            "StyleGAN"
        ],
        "涉及的技术概念": "StyleGAN是一种生成对抗网络，用于生成高质量的图像。潜在空间是指生成模型中用于生成图像的隐藏特征空间。图像嵌入是将图像转换到这个潜在空间中的过程，以便进行进一步的图像编辑和操作。"
    },
    {
        "order": 8,
        "title": "Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Universal_Adversarial_Perturbation_via_Prior_Driven_Uncertainty_Approximation_ICCV_2019_paper.html",
        "abstract": "Deep learning models have shown their vulnerabilities to universal adversarial perturbations (UAP), which are quasi-imperceptible. Compared to the conventional supervised UAPs that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations. In this paper, we propose a new unsupervised universal adversarial perturbation method, termed as Prior Driven Uncertainty Approximation (PD-UA), to generate a robust UAP by fully exploiting the model uncertainty at each network layer. Specifically, a Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Thereafter, a textural bias prior to revealing a statistical uncertainty is proposed, which helps to improve the attacking performance. The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is finally used to maintain the statistical uncertainty. Extensive experiments demonstrate that our method achieves well attacking performances on the ImageNet validation set, and significantly improves the fooling rate compared with the state-of-the-art methods.",
        "中文标题": "通过先验驱动的不确定性近似实现通用对抗扰动",
        "摘要翻译": "深度学习模型已经显示出它们对通用对抗扰动（UAP）的脆弱性，这些扰动几乎是不可察觉的。与依赖于训练数据知识的传统监督UAP相比，数据独立的无监督UAP更具适用性。现有的无监督方法未能利用模型不确定性来产生鲁棒的扰动。在本文中，我们提出了一种新的无监督通用对抗扰动方法，称为先验驱动的不确定性近似（PD-UA），通过充分利用每个网络层的模型不确定性来生成鲁棒的UAP。具体来说，部署了蒙特卡罗采样方法来激活更多神经元以增加模型不确定性，从而获得更好的对抗扰动。随后，提出了一个揭示统计不确定性的纹理偏差先验，这有助于提高攻击性能。UAP是通过带有增强动量优化器的随机梯度下降算法制作的，最后使用拉普拉斯金字塔频率模型来维持统计不确定性。大量实验证明，我们的方法在ImageNet验证集上实现了良好的攻击性能，并且与最先进的方法相比，显著提高了欺骗率。",
        "领域": "对抗样本生成/模型鲁棒性/深度学习安全",
        "问题": "如何生成鲁棒的通用对抗扰动",
        "动机": "现有的无监督方法未能充分利用模型不确定性来产生鲁棒的对抗扰动",
        "方法": "提出了一种新的无监督通用对抗扰动方法，称为先验驱动的不确定性近似（PD-UA），通过蒙特卡罗采样方法激活更多神经元以增加模型不确定性，并提出纹理偏差先验来揭示统计不确定性，最后使用拉普拉斯金字塔频率模型来维持统计不确定性",
        "关键词": [
            "通用对抗扰动",
            "模型不确定性",
            "蒙特卡罗采样",
            "纹理偏差先验",
            "拉普拉斯金字塔频率模型"
        ],
        "涉及的技术概念": "通用对抗扰动（UAP）是指对深度学习模型几乎不可察觉的扰动，能够使模型产生错误的输出。模型不确定性指的是模型对输入数据预测的不确定性程度。蒙特卡罗采样是一种通过随机采样来估计数值结果的统计方法。纹理偏差先验是一种统计方法，用于揭示数据中的统计不确定性。拉普拉斯金字塔频率模型是一种图像处理技术，用于分析图像的不同频率成分。"
    },
    {
        "order": 9,
        "title": "SANet: Scene Agnostic Network for Camera Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SANet_Scene_Agnostic_Network_for_Camera_Localization_ICCV_2019_paper.html",
        "abstract": "This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance.",
        "中文标题": "SANet: 场景无关网络用于相机定位",
        "摘要翻译": "本文提出了一种场景无关的神经架构用于相机定位，其中模型参数和场景彼此独立。尽管基于学习的方法最近取得了进展，但大多数方法需要对每个场景逐一进行训练，这不适用于需要即时构建模型的在线应用，如SLAM和机器人导航。我们的方法学习构建分层场景表示，并在给定任意场景的情况下即时预测查询RGB图像的密集场景坐标图。查询图像的6D相机姿态可以通过预测的场景坐标图进行估计。此外，密集预测可以用于其他在线机器人和增强现实应用，如避障。我们在室内和室外基准测试中展示了我们方法的有效性和效率，达到了最先进的性能。",
        "领域": "相机定位/机器人导航/增强现实",
        "问题": "解决相机在任意场景中的即时定位问题",
        "动机": "为了克服现有方法需要对每个场景逐一训练的局限性，使其适用于需要即时构建模型的在线应用",
        "方法": "提出了一种场景无关的神经架构，学习构建分层场景表示并即时预测查询RGB图像的密集场景坐标图，从而估计6D相机姿态",
        "关键词": [
            "相机定位",
            "机器人导航",
            "增强现实",
            "SLAM",
            "避障"
        ],
        "涉及的技术概念": "场景无关神经架构、分层场景表示、密集场景坐标图、6D相机姿态估计"
    },
    {
        "order": 10,
        "title": "Controllable Artistic Text Style Transfer via Shape-Matching GAN",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Controllable_Artistic_Text_Style_Transfer_via_Shape-Matching_GAN_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Controllable_Artistic_Text_Style_Transfer_via_Shape-Matching_GAN_ICCV_2019_paper.html",
        "abstract": "Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text.",
        "中文标题": "通过形状匹配GAN实现可控的艺术文本风格迁移",
        "摘要翻译": "艺术文本风格迁移是将源图像的风格迁移到目标文本以创建艺术字体的任务。最近的风格迁移方法考虑了纹理控制以增强可用性。然而，在形状变形方面控制风格程度仍然是一个重要的开放挑战。在本文中，我们提出了第一个文本风格迁移网络，该网络允许通过可调参数实时控制字形的关键风格程度。我们的关键贡献是一个新颖的双向形状匹配框架，以在没有配对真实数据的情况下，在各种变形级别上建立有效的字形-风格映射。基于这一想法，我们提出了一个尺度可控模块，使单个网络能够连续表征风格图像的多尺度形状特征，并将这些特征迁移到目标文本。所提出的方法在生成多样化、可控和高质量的风格化文本方面展示了其优于先前最先进方法的优势。",
        "领域": "艺术字体生成/风格迁移/生成对抗网络",
        "问题": "在艺术文本风格迁移中，如何控制字形变形的风格程度",
        "动机": "增强艺术文本风格迁移的可用性和控制性，特别是在形状变形方面",
        "方法": "提出了一个新颖的双向形状匹配框架和尺度可控模块，以实现对字形风格程度的实时控制",
        "关键词": [
            "艺术字体生成",
            "风格迁移",
            "生成对抗网络",
            "形状匹配",
            "尺度可控"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括艺术文本风格迁移、生成对抗网络（GAN）、形状匹配、尺度可控模块和多尺度形状特征表征。"
    },
    {
        "order": 11,
        "title": "Understanding Deep Networks via Extremal Perturbations and Smooth Masks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html",
        "abstract": "Attribution is the problem of finding which parts of an image are the most responsible for the output of a deep neural network. An important family of attribution methods is based on measuring the effect of perturbations applied to the input image, either via exhaustive search or by finding representative perturbations via optimization. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute these extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable weighing factors from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the network under stimulation. We also extend perturbation analysis to the intermediate layers of a deep neural network. This application allows us to show how compactly an image can be represented (in terms of the number of channels it requires). We also demonstrate that the consistency with which images of a given class rely on the same intermediate channel correlates well with class accuracy.",
        "中文标题": "通过极值扰动和平滑掩码理解深度网络",
        "摘要翻译": "归因问题是寻找图像的哪些部分对深度神经网络的输出最负责。一种重要的归因方法家族基于测量应用于输入图像的扰动效果，无论是通过穷举搜索还是通过优化找到代表性扰动。在本文中，我们讨论了现有扰动分析方法的一些缺点，并通过引入极值扰动的概念来解决这些问题，这些概念在理论上有基础且可解释。我们还引入了一些技术创新来计算这些极值扰动，包括一个新的区域约束和一个平滑扰动的参数家族，这使我们能够从优化问题中移除所有可调权重因子。我们分析了扰动效果作为其面积的函数，展示了对受刺激网络空间属性的极佳敏感性。我们还将扰动分析扩展到深度神经网络的中间层。这一应用使我们能够展示图像可以多么紧凑地表示（就所需的通道数量而言）。我们还证明了给定类别的图像依赖相同中间通道的一致性与类别准确性有很好的相关性。",
        "领域": "神经网络解释性/图像归因/深度学习优化",
        "问题": "如何准确识别图像中对深度神经网络输出最负责的部分",
        "动机": "现有扰动分析方法存在缺点，需要更理论化和可解释的方法来改进图像归因",
        "方法": "引入极值扰动概念，采用新的区域约束和平滑扰动的参数家族，移除优化问题中的可调权重因子，扩展扰动分析到深度神经网络的中间层",
        "关键词": [
            "极值扰动",
            "平滑掩码",
            "图像归因",
            "神经网络解释性",
            "深度学习优化"
        ],
        "涉及的技术概念": "极值扰动是一种在理论上具有基础且可解释的扰动方法，用于分析深度神经网络对图像特定部分的敏感性。平滑扰动的参数家族和新的区域约束是计算这些极值扰动的技术创新，有助于移除优化问题中的可调权重因子，提高分析的准确性和效率。通过将扰动分析扩展到深度神经网络的中间层，可以更深入地理解图像在神经网络中的表示方式和类别准确性之间的关系。"
    },
    {
        "order": 12,
        "title": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hermosilla_Total_Denoising_Unsupervised_Learning_of_3D_Point_Cloud_Cleaning_ICCV_2019_paper.html",
        "abstract": "We show that denoising of 3D point clouds can be learned unsupervised, directly from noisy 3D point cloud data only. This is achieved by extending recent ideas from learning of unsupervised image denoisers to unstructured 3D point clouds. Unsupervised image denoisers operate under the assumption that a noisy pixel observation is a random realization of a distribution around a clean pixel value, which allows appropriate learning on this distribution to eventually converge to the correct value. Regrettably, this assumption is not valid for unstructured points: 3D point clouds are subject to total noise, i.e. deviations in all coordinates, with no reliable pixel grid. Thus, an observation can be the realization of an entire manifold of clean 3D points, which makes the quality of a naive extension of unsupervised image denoisers to 3D point clouds unfortunately only little better than mean filtering. To overcome this, and to enable effective and unsupervised 3D point cloud denoising, we introduce a spatial prior term, that steers converges to the unique closest out of the many possible modes on the manifold. Our results demonstrate unsupervised denoising performance similar to that of supervised learning with clean data when given enough training examples - whereby we do not need any pairs of noisy and clean training data.",
        "中文标题": "全面去噪：无监督学习3D点云清理",
        "摘要翻译": "我们展示了3D点云的去噪可以通过无监督学习直接从噪声3D点云数据中学习得到。这是通过将最近的无监督图像去噪学习的思想扩展到非结构化的3D点云实现的。无监督图像去噪器操作的前提是噪声像素观察是围绕干净像素值的分布的随机实现，这使得对该分布进行适当学习最终能够收敛到正确的值。遗憾的是，这一假设对于非结构化点无效：3D点云受到全面噪声的影响，即所有坐标的偏差，没有可靠的像素网格。因此，一个观察可能是整个干净3D点流形的实现，这使得将无监督图像去噪器天真地扩展到3D点云的质量不幸地只比均值滤波略好。为了克服这一点，并实现有效且无监督的3D点云去噪，我们引入了一个空间先验项，它引导收敛到流形上许多可能模式中的唯一最接近的模式。我们的结果表明，当提供足够的训练样本时，无监督去噪性能与使用干净数据进行监督学习的性能相似——而我们不需要任何噪声和干净训练数据的配对。",
        "领域": "3D点云处理/无监督学习/去噪技术",
        "问题": "如何在无监督的情况下有效清理3D点云中的噪声",
        "动机": "现有的无监督图像去噪方法假设噪声像素是围绕干净像素值的随机实现，这一假设不适用于非结构化的3D点云，因为3D点云受到全面噪声的影响，需要新的方法来解决这一问题。",
        "方法": "引入了一个空间先验项，引导收敛到流形上许多可能模式中的唯一最接近的模式，从而实现有效且无监督的3D点云去噪。",
        "关键词": [
            "3D点云",
            "无监督学习",
            "去噪",
            "空间先验项",
            "流形"
        ],
        "涉及的技术概念": "无监督学习是一种机器学习方法，它不依赖于标记数据来训练模型。3D点云是由大量点组成的三维数据表示，常用于表示物体的表面。去噪技术旨在从数据中移除噪声，以提高数据质量。空间先验项是一种用于引导模型学习过程的约束，确保模型在可能的解空间中找到最合适的解。流形是数学中的一个概念，指的是局部类似于欧几里得空间的拓扑空间，用于描述数据的高维结构。"
    },
    {
        "order": 13,
        "title": "Understanding Generalized Whitening and Coloring Transform for Universal Style Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chiu_Understanding_Generalized_Whitening_and_Coloring_Transform_for_Universal_Style_Transfer_ICCV_2019_paper.html",
        "abstract": "Style transfer is a task of rendering images in the styles of other images. In the past few years, neural style transfer has achieved a great success in this task, yet suffers from either the inability to generalize to unseen style images or fast style transfer. Recently, an universal style transfer technique that applies zero-phase component analysis (ZCA) for whitening and coloring image features realizes fast and arbitrary style transfer. However, using ZCA for style transfer is empirical and does not have any theoretical support. In addition, other whitening and coloring transforms (WCT) than ZCA have not been investigated. In this report, we generalize ZCA to the general form of WCT, provide an analytical performance analysis from the angle of neural style transfer, and show why ZCA is a good choice for style transfer among different WCTs and why some WCTs are not well applicable for style transfer.",
        "中文标题": "理解通用风格迁移中的广义白化和着色变换",
        "摘要翻译": "风格迁移是一项将图像以其他图像的风格进行渲染的任务。在过去的几年中，神经风格迁移在这一任务上取得了巨大成功，但要么无法泛化到未见过的风格图像，要么无法实现快速风格迁移。最近，一种应用零相位成分分析（ZCA）进行图像特征白化和着色的通用风格迁移技术实现了快速且任意的风格迁移。然而，使用ZCA进行风格迁移是经验性的，没有任何理论支持。此外，除了ZCA之外的其他白化和着色变换（WCT）尚未被研究。在本报告中，我们将ZCA推广到WCT的一般形式，从神经风格迁移的角度提供性能分析，并展示为什么ZCA是不同WCT中进行风格迁移的良好选择，以及为什么某些WCT不适用于风格迁移。",
        "领域": "风格迁移/图像渲染/特征变换",
        "问题": "如何实现快速且任意的风格迁移，并理解不同白化和着色变换在风格迁移中的适用性",
        "动机": "解决神经风格迁移在泛化能力和速度上的限制，以及缺乏理论支持的问题",
        "方法": "将零相位成分分析（ZCA）推广到一般形式的白化和着色变换（WCT），并从神经风格迁移的角度进行性能分析",
        "关键词": [
            "风格迁移",
            "白化和着色变换",
            "零相位成分分析"
        ],
        "涉及的技术概念": "零相位成分分析（ZCA）是一种用于图像特征白化和着色的技术，旨在实现快速且任意的风格迁移。白化和着色变换（WCT）是一类用于风格迁移的技术，通过变换图像特征来实现风格迁移。"
    },
    {
        "order": 14,
        "title": "Hierarchical Self-Attention Network for Action Localization in Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pramono_Hierarchical_Self-Attention_Network_for_Action_Localization_in_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pramono_Hierarchical_Self-Attention_Network_for_Action_Localization_in_Videos_ICCV_2019_paper.html",
        "abstract": "This paper presents a novel Hierarchical Self-Attention Network (HISAN) to generate spatial-temporal tubes for action localization in videos. The essence of HISAN is to combine the two-stream convolutional neural network (CNN) with hierarchical bidirectional self-attention mechanism, which comprises of two levels of bidirectional self-attention to efficaciously capture both of the long-term temporal dependency information and spatial context information to render more precise action localization. Also, a sequence rescoring (SR) algorithm is employed to resolve the dilemma of inconsistent detection scores incurred by occlusion or background clutter. Moreover, a new fusion scheme is invoked, which integrates not only the appearance and motion information from the two-stream network, but also the motion saliency to mitigate the effect of camera motion. Simulations reveal that the new approach achieves competitive performance as the state-of-the-art works in terms of action localization and recognition accuracy on the widespread UCF101-24 and J-HMDB datasets.",
        "中文标题": "用于视频中动作定位的分层自注意力网络",
        "摘要翻译": "本文提出了一种新颖的分层自注意力网络（HISAN），用于生成视频中动作定位的空间-时间管。HISAN的核心是将双流卷积神经网络（CNN）与分层双向自注意力机制相结合，该机制包括两个层次的双向自注意力，以有效捕捉长期时间依赖信息和空间上下文信息，从而实现更精确的动作定位。此外，采用序列重评分（SR）算法来解决由于遮挡或背景杂乱引起的不一致检测分数问题。此外，还引入了一种新的融合方案，该方案不仅整合了来自双流网络的外观和运动信息，还整合了运动显著性，以减轻相机运动的影响。模拟结果表明，新方法在广泛使用的UCF101-24和J-HMDB数据集上的动作定位和识别准确性方面达到了与最先进工作相竞争的性能。",
        "领域": "动作识别/视频分析/自注意力机制",
        "问题": "视频中动作的精确定位",
        "动机": "解决视频中由于遮挡、背景杂乱或相机运动导致的动作定位不准确问题",
        "方法": "结合双流卷积神经网络与分层双向自注意力机制，采用序列重评分算法和新的融合方案",
        "关键词": [
            "动作定位",
            "自注意力机制",
            "双流卷积神经网络",
            "序列重评分",
            "运动显著性"
        ],
        "涉及的技术概念": {
            "分层自注意力网络（HISAN）": "一种结合了双流卷积神经网络和分层双向自注意力机制的网络结构，用于捕捉视频中的长期时间依赖和空间上下文信息。",
            "双流卷积神经网络（CNN）": "一种处理视频数据的神经网络结构，通常包括处理空间信息（外观）和时间信息（运动）的两个流。",
            "序列重评分（SR）算法": "一种用于解决由于遮挡或背景杂乱引起的不一致检测分数问题的算法。",
            "运动显著性": "指视频中运动区域的显著性，用于减轻相机运动对动作定位的影响。"
        }
    },
    {
        "order": 15,
        "title": "Unsupervised Pre-Training of Image Features on Non-Curated Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Caron_Unsupervised_Pre-Training_of_Image_Features_on_Non-Curated_Data_ICCV_2019_paper.html",
        "abstract": "Pre-training general-purpose visual features with convolutional neural networks without relying on annotations is a challenging and important task. Most recent efforts in unsupervised feature learning have focused on either small or highly curated datasets like ImageNet, whereas using uncurated raw datasets was found to decrease the feature quality when evaluated on a transfer task. Our goal is to bridge the performance gap between unsupervised methods trained on curated data, which are costly to obtain, and massive raw datasets that are easily available. To that effect, we propose a new unsupervised approach which leverages self-supervision and clustering to capture complementary statistics from large-scale data. We validate our approach on 96 million images from YFCC100M, achieving state-of-the-art results among unsupervised methods on standard benchmarks, which confirms the potential of unsupervised learning when only uncurated data are available. We also show that pre-training a supervised VGG-16 with our method achieves 74.9% top-1 classification accuracy on the validation set of ImageNet, which is an improvement of +0.8% over the same network trained from scratch. Our code is available at https://github.com/facebookresearch/DeeperCluster.",
        "中文标题": "非精选数据上图像特征的无监督预训练",
        "摘要翻译": "在不依赖注释的情况下，使用卷积神经网络预训练通用视觉特征是一项具有挑战性且重要的任务。最近在无监督特征学习方面的努力主要集中在小型或高度精选的数据集上，如ImageNet，而使用未精选的原始数据集在迁移任务评估时被发现会降低特征质量。我们的目标是缩小在精选数据上训练的无监督方法（这些数据获取成本高）与易于获取的大规模原始数据集之间的性能差距。为此，我们提出了一种新的无监督方法，该方法利用自监督和聚类从大规模数据中捕捉互补的统计信息。我们在YFCC100M的9600万张图像上验证了我们的方法，在标准基准测试中实现了无监督方法中的最先进结果，这证实了在只有未精选数据可用时无监督学习的潜力。我们还展示了使用我们的方法预训练的监督VGG-16在ImageNet验证集上达到了74.9%的top-1分类准确率，比从头开始训练的相同网络提高了+0.8%。我们的代码可在https://github.com/facebookresearch/DeeperCluster获取。",
        "领域": "无监督学习/特征学习/迁移学习",
        "问题": "在未精选的大规模数据集上进行无监督特征学习时，特征质量下降的问题",
        "动机": "缩小在精选数据上训练的无监督方法与大规模原始数据集之间的性能差距",
        "方法": "提出了一种新的无监督方法，利用自监督和聚类从大规模数据中捕捉互补的统计信息",
        "关键词": [
            "无监督学习",
            "特征学习",
            "迁移学习",
            "自监督",
            "聚类"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）、自监督学习、聚类、迁移学习、ImageNet、VGG-16、YFCC100M数据集"
    },
    {
        "order": 16,
        "title": "Learning Implicit Generative Models by Matching Perceptual Features",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/dos_Santos_Learning_Implicit_Generative_Models_by_Matching_Perceptual_Features_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/dos_Santos_Learning_Implicit_Generative_Models_by_Matching_Perceptual_Features_ICCV_2019_paper.html",
        "abstract": "Perceptual features (PFs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. However, the efficacy of PFs as key source of information for learning generative models is not well studied. We investigate here the use of PFs in the context of learning implicit generative models through moment matching (MM). More specifically, we propose a new effective MM approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained ConvNets. Our proposed approach improves upon existing MM methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. Our experimental results demonstrate that, due to the expressiveness of PFs from pretrained deep ConvNets, our method achieves state-of-the-art results for challenging benchmarks.",
        "中文标题": "通过匹配感知特征学习隐式生成模型",
        "摘要翻译": "感知特征（PFs）在迁移学习、风格转换和超分辨率等任务中取得了巨大成功。然而，PFs作为学习生成模型的关键信息来源的有效性尚未得到充分研究。我们在此研究了通过矩匹配（MM）在学习隐式生成模型中使用PFs的情况。更具体地说，我们提出了一种新的有效MM方法，该方法通过执行从预训练的ConvNets中提取的特征的均值和协方差匹配来学习隐式生成模型。我们提出的方法通过以下方式改进了现有的MM方法：（1）摆脱了对抗性学习中的最小/最大游戏问题；（2）避免了核函数的在线学习；（3）在使用矩的数量和所需的小批量大小方面都表现出高效性。我们的实验结果表明，由于预训练的深度ConvNets中PFs的表达能力，我们的方法在具有挑战性的基准测试中实现了最先进的结果。",
        "领域": "生成模型/迁移学习/风格转换",
        "问题": "如何有效利用感知特征学习隐式生成模型",
        "动机": "探索感知特征作为学习生成模型的关键信息来源的有效性",
        "方法": "提出了一种新的矩匹配方法，通过匹配从预训练的ConvNets中提取的特征的均值和协方差来学习隐式生成模型",
        "关键词": [
            "感知特征",
            "矩匹配",
            "隐式生成模型",
            "ConvNets"
        ],
        "涉及的技术概念": "感知特征（PFs）指的是从预训练的卷积神经网络（ConvNets）中提取的特征，这些特征在多种视觉任务中表现出色。矩匹配（MM）是一种统计方法，用于通过匹配特征分布的矩（如均值和协方差）来学习生成模型。隐式生成模型是指不直接定义数据分布，而是通过某种方式间接生成数据的模型。"
    },
    {
        "order": 17,
        "title": "Goal-Driven Sequential Data Abstraction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Muhammad_Goal-Driven_Sequential_Data_Abstraction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Muhammad_Goal-Driven_Sequential_Data_Abstraction_ICCV_2019_paper.html",
        "abstract": "Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains.",
        "中文标题": "目标驱动的序列数据抽象",
        "摘要翻译": "自动数据抽象是评估机器智能和支持摘要应用的重要能力。在前者中，人们询问机器是否能够“理解”输入数据的意义，以产生有意义但更紧凑的抽象。在后者中，这种能力被用来通过总结输入数据的本质来节省空间或人类时间。在本文中，我们研究了一个基于强化学习的通用框架，用于以目标驱动的方式学习抽象序列数据。定义不同抽象目标的能力独特地允许根据抽象的最终目的保留输入数据的不同方面。我们的强化学习目标不需要人类定义的理想抽象示例。重要的是，我们的模型整体处理输入序列，不受原始输入顺序的约束。我们的框架也是领域无关的——我们展示了在草图、视频和文本数据上的应用，并在所有领域都取得了有希望的结果。",
        "领域": "序列数据处理/强化学习/数据抽象",
        "问题": "如何以目标驱动的方式自动抽象序列数据",
        "动机": "评估机器智能的能力和支持摘要应用，通过自动数据抽象来节省空间或人类时间",
        "方法": "采用基于强化学习的通用框架，允许定义不同的抽象目标，整体处理输入序列，不受原始输入顺序的约束",
        "关键词": [
            "序列数据",
            "强化学习",
            "数据抽象",
            "目标驱动",
            "领域无关"
        ],
        "涉及的技术概念": "强化学习是一种机器学习方法，它通过奖励和惩罚来学习策略，以达到某种目标。在本文中，强化学习被用来学习如何根据不同的目标抽象序列数据。数据抽象是指从大量数据中提取出关键信息或模式，以便更有效地处理或理解数据。目标驱动的方法意味着抽象过程是根据特定的目标或需求来进行的，这允许保留输入数据的不同方面。领域无关意味着该方法可以应用于多种类型的数据，如草图、视频和文本。"
    },
    {
        "order": 18,
        "title": "Learning Local Descriptors With a CDF-Based Dynamic Soft Margin",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_Local_Descriptors_With_a_CDF-Based_Dynamic_Soft_Margin_ICCV_2019_paper.html",
        "abstract": "The triplet loss is adopted by a variety of learning tasks, such as local feature descriptor learning. However, its standard formulation with a hard margin only leverages part of the training data in each mini-batch. Moreover, the margin is often empirically chosen or determined through computationally expensive validation, and stays unchanged during the entire training session. In this work, we propose a simple yet effective method to overcome the above limitations. The core idea is to replace the hard margin with a non-parametric soft margin, which is dynamically updated. The major observation is that the difficulty of a triplet can be inferred from the cumulative distribution function of the triplets' signed distances to the decision boundary. We demonstrate through experiments on both real-valued and binary local feature descriptors that our method leads to state-of-the-art performance on popular benchmarks, while eliminating the need to determine the best margin.",
        "中文标题": "学习基于CDF的动态软边界的局部描述符",
        "摘要翻译": "三重损失被多种学习任务采用，例如局部特征描述符学习。然而，其带有硬边界的标准公式在每个小批量中仅利用了一部分训练数据。此外，边界通常是通过经验选择或通过计算成本高昂的验证确定的，并且在整个训练过程中保持不变。在这项工作中，我们提出了一种简单而有效的方法来克服上述限制。核心思想是用非参数的软边界替换硬边界，该软边界是动态更新的。主要观察是，三重难度的难度可以从三重到决策边界的签名距离的累积分布函数中推断出来。我们通过对实值和二进制局部特征描述符的实验证明，我们的方法在流行基准上达到了最先进的性能，同时消除了确定最佳边界的需要。",
        "领域": "局部特征描述符学习/深度学习优化/计算机视觉",
        "问题": "三重损失标准公式中硬边界的使用导致训练数据利用不充分和边界选择困难",
        "动机": "提高局部特征描述符学习的效率和性能，通过动态更新软边界来克服硬边界的限制",
        "方法": "提出一种非参数的软边界方法，该软边界根据三重到决策边界的签名距离的累积分布函数动态更新",
        "关键词": [
            "三重损失",
            "局部特征描述符",
            "动态软边界",
            "累积分布函数"
        ],
        "涉及的技术概念": "三重损失是一种用于学习任务（如局部特征描述符学习）的损失函数，通过比较正样本和负样本之间的距离来优化模型。硬边界是指在三重损失中设定的固定边界，用于区分正负样本。软边界是一种动态调整的边界，可以根据训练数据的分布自动调整，以提高模型的训练效率和性能。累积分布函数（CDF）是用于描述随机变量小于或等于某个值的概率的函数，在这里用于推断三重难度的难度。"
    },
    {
        "order": 19,
        "title": "Free-Form Image Inpainting With Gated Convolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Free-Form_Image_Inpainting_With_Gated_Convolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Free-Form_Image_Inpainting_With_Gated_Convolution_ICCV_2019_paper.html",
        "abstract": "We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of images without additional labelling efforts. The proposed gated convolution solves the issue of vanilla convolution that treats all input pixels as valid ones, generalizes partial convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location across all layers. Moreover, as free-form masks may appear anywhere in images with any shape, global and local GANs designed for a single rectangular mask are not applicable. Thus, we also present a patch-based GAN loss, named SN-PatchGAN, by applying spectral-normalized discriminator on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic image inpainting and user-guided extension demonstrate that our system generates higher-quality and more flexible results than previous methods. Our system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.",
        "中文标题": "使用门控卷积的自由形式图像修复",
        "摘要翻译": "我们提出了一种生成式图像修复系统，用于完成带有自由形式遮罩和指导的图像。该系统基于从数百万张图像中学习到的门控卷积，无需额外的标注工作。提出的门控卷积解决了普通卷积将所有输入像素视为有效像素的问题，通过在每一层的每个空间位置为每个通道提供可学习的动态特征选择机制，泛化了部分卷积。此外，由于自由形式遮罩可能以任何形状出现在图像的任何位置，为单一矩形遮罩设计的全局和局部GANs不适用。因此，我们还提出了一种基于补丁的GAN损失，名为SN-PatchGAN，通过在密集图像补丁上应用谱归一化判别器。SN-PatchGAN在公式上简单，训练快速且稳定。自动图像修复和用户引导扩展的结果表明，我们的系统比之前的方法生成更高质量和更灵活的结果。我们的系统帮助用户快速移除分散注意力的物体，修改图像布局，清除水印和编辑面部。代码、演示和模型可在https://github.com/JiahuiYu/generative_inpainting获取。",
        "领域": "图像修复/生成对抗网络/卷积神经网络",
        "问题": "解决图像修复中自由形式遮罩的处理问题",
        "动机": "提高图像修复的质量和灵活性，特别是在处理自由形式遮罩时",
        "方法": "采用门控卷积和基于补丁的GAN损失（SN-PatchGAN）",
        "关键词": [
            "图像修复",
            "门控卷积",
            "生成对抗网络",
            "谱归一化"
        ],
        "涉及的技术概念": {
            "门控卷积": "一种改进的卷积方法，通过为每个通道在每个空间位置提供可学习的动态特征选择机制，解决了普通卷积将所有输入像素视为有效像素的问题。",
            "SN-PatchGAN": "一种基于补丁的GAN损失，通过在密集图像补丁上应用谱归一化判别器，适用于处理自由形式遮罩。",
            "生成对抗网络": "一种深度学习模型，通过生成器和判别器的对抗过程生成数据。"
        }
    },
    {
        "order": 20,
        "title": "Jointly Aligning Millions of Images With Deep Penalised Reconstruction Congealing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Annunziata_Jointly_Aligning_Millions_of_Images_With_Deep_Penalised_Reconstruction_Congealing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Annunziata_Jointly_Aligning_Millions_of_Images_With_Deep_Penalised_Reconstruction_Congealing_ICCV_2019_paper.html",
        "abstract": "Extrapolating fine-grained pixel-level correspondences in a fully unsupervised manner from a large set of misaligned images can benefit several computer vision and graphics problems, e.g. co-segmentation, super-resolution, image edit propagation, structure-from-motion, and 3D reconstruction. Several joint image alignment and congealing techniques have been proposed to tackle this problem, but robustness to initialisation, ability to scale to large datasets, and alignment accuracy seem to hamper their wide applicability. To overcome these limitations, we propose an unsupervised joint alignment method leveraging a densely fused spatial transformer network to estimate the warping parameters for each image and a low-capacity auto-encoder whose reconstruction error is used as an auxiliary measure of joint alignment. Experimental results on digits from multiple versions of MNIST (i.e., original, perturbed, affNIST and infiMNIST) and faces from LFW, show that our approach is capable of aligning millions of images with high accuracy and robustness to different levels and types of perturbation. Moreover, qualitative and quantitative results suggest that the proposed method outperforms state-of-the-art approaches both in terms of alignment quality and robustness to initialisation.",
        "中文标题": "联合对齐数百万张图像的深度惩罚重建凝固",
        "摘要翻译": "从大量未对齐的图像中以完全无监督的方式推断细粒度像素级对应关系，可以有益于多个计算机视觉和图形问题，例如共同分割、超分辨率、图像编辑传播、从运动中恢复结构和3D重建。已经提出了几种联合图像对齐和凝固技术来解决这个问题，但对初始化的鲁棒性、扩展到大数据集的能力和对齐精度似乎限制了它们的广泛应用。为了克服这些限制，我们提出了一种无监督的联合对齐方法，利用密集融合的空间变换器网络来估计每张图像的变形参数，并使用一个低容量自动编码器，其重建误差作为联合对齐的辅助度量。在多个版本的MNIST（即原始、扰动、affNIST和infiMNIST）和LFW的人脸上的实验结果表明，我们的方法能够以高精度和对不同级别和类型的扰动的鲁棒性对齐数百万张图像。此外，定性和定量结果表明，所提出的方法在对齐质量和初始化鲁棒性方面均优于最先进的方法。",
        "领域": "图像对齐/图像重建/图像编辑",
        "问题": "从大量未对齐的图像中以完全无监督的方式推断细粒度像素级对应关系",
        "动机": "提高图像对齐的鲁棒性、扩展到大数据集的能力和对齐精度",
        "方法": "利用密集融合的空间变换器网络估计每张图像的变形参数，并使用低容量自动编码器的重建误差作为联合对齐的辅助度量",
        "关键词": [
            "图像对齐",
            "图像重建",
            "图像编辑",
            "空间变换器网络",
            "自动编码器"
        ],
        "涉及的技术概念": "空间变换器网络用于估计图像的变形参数，自动编码器用于通过重建误差辅助联合对齐。"
    },
    {
        "order": 21,
        "title": "Bayes-Factor-VAE: Hierarchical Bayesian Deep Auto-Encoder Models for Factor Disentanglement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Bayes-Factor-VAE_Hierarchical_Bayesian_Deep_Auto-Encoder_Models_for_Factor_Disentanglement_ICCV_2019_paper.html",
        "abstract": "We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.",
        "中文标题": "Bayes-Factor-VAE: 用于因子解缠的分层贝叶斯深度自编码器模型",
        "摘要翻译": "我们提出了一系列新颖的分层贝叶斯深度自编码器模型，这些模型能够识别数据中的解缠变异性因子。尽管最近许多尝试在VAE框架内专注于复杂的学目标，但它们选择标准正态作为潜在因子先验既不是最优的，也对性能有害。我们的关键观察是，负责主要变异性来源的解缠潜在变量，即相关因子，可以更适当地使用长尾分布来建模。另一方面，典型的高斯先验更适合于建模干扰因子。受此启发，我们通过在高斯潜在先验的方差上引入超先验，将VAE扩展为分层贝叶斯模型，模仿无限混合，同时保持传统VAE的可学习性和推理能力。这一分析表明，以不同方式分区和处理对应于相关因子和干扰的潜在维度的重要性。我们提出的模型，称为Bayes-Factor-VAEs，在几个具有挑战性的基准任务中，在潜在解缠方面无论是定量还是定性上都优于现有方法。",
        "领域": "生成模型/变分自编码器/贝叶斯学习",
        "问题": "如何在数据中识别解缠的变异性因子",
        "动机": "现有方法使用标准正态作为潜在因子先验，这既不是最优的，也对性能有害。",
        "方法": "通过在高斯潜在先验的方差上引入超先验，将VAE扩展为分层贝叶斯模型，模仿无限混合，同时保持传统VAE的可学习性和推理能力。",
        "关键词": [
            "分层贝叶斯模型",
            "变分自编码器",
            "因子解缠",
            "长尾分布",
            "高斯先验"
        ],
        "涉及的技术概念": "VAE（变分自编码器）是一种生成模型，用于学习数据的潜在表示。分层贝叶斯模型是一种统计模型，它通过引入超参数来扩展传统的贝叶斯模型，以更好地捕捉数据的层次结构。因子解缠是指将数据中的变异性分解为独立的因子，以便更好地理解和控制数据的生成过程。长尾分布是一种概率分布，其尾部比正态分布更重，适用于建模极端事件或异常值。高斯先验是一种假设潜在变量遵循正态分布的统计假设。"
    },
    {
        "order": 22,
        "title": "FiNet: Compatible and Diverse Fashion Image Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_FiNet_Compatible_and_Diverse_Fashion_Image_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_FiNet_Compatible_and_Diverse_Fashion_Image_Inpainting_ICCV_2019_paper.html",
        "abstract": "Visual compatibility is critical for fashion analysis, yet is missing in existing fashion image synthesis systems. In this paper, we propose to explicitly model visual compatibility through fashion image inpainting. We present Fashion Inpainting Networks (FiNet), a two-stage image-to-image generation framework that is able to perform compatible and diverse inpainting. Disentangling the generation of shape and appearance to ensure photorealistic results, our framework consists of a shape generation network and an appearance generation network. More importantly, for each generation network, we introduce two encoders interacting with one another to learn latent codes in a shared compatibility space. The latent representations are jointly optimized with the corresponding generation network to condition the synthesis process, encouraging a diverse set of generated results that are visually compatible with existing fashion garments. In addition, our framework is readily extended to clothing reconstruction and fashion transfer. Extensive experiments on fashion synthesis quantitatively and qualitatively demonstrate the effectiveness of our method.",
        "中文标题": "FiNet: 兼容且多样的时尚图像修复",
        "摘要翻译": "视觉兼容性对于时尚分析至关重要，但在现有的时尚图像合成系统中却缺失。在本文中，我们提出通过时尚图像修复来显式地建模视觉兼容性。我们介绍了时尚修复网络（FiNet），这是一个两阶段的图像到图像生成框架，能够执行兼容且多样的修复。为了确保照片般真实的结果，我们的框架将形状和外观的生成分离，包括一个形状生成网络和一个外观生成网络。更重要的是，对于每个生成网络，我们引入了两个相互作用的编码器，以在共享的兼容性空间中学习潜在代码。潜在表示与相应的生成网络联合优化，以条件化合成过程，鼓励生成与现有时尚服装视觉兼容的多样化结果。此外，我们的框架很容易扩展到服装重建和时尚转移。在时尚合成上的大量实验定量和定性地证明了我们方法的有效性。",
        "领域": "时尚分析/图像修复/图像生成",
        "问题": "现有时尚图像合成系统中视觉兼容性的缺失",
        "动机": "通过时尚图像修复显式地建模视觉兼容性，以提升时尚图像合成的质量和多样性",
        "方法": "提出了一个两阶段的图像到图像生成框架FiNet，包括形状生成网络和外观生成网络，通过两个相互作用的编码器在共享的兼容性空间中学习潜在代码，联合优化潜在表示和生成网络，以条件化合成过程",
        "关键词": [
            "时尚图像修复",
            "视觉兼容性",
            "图像生成",
            "服装重建",
            "时尚转移"
        ],
        "涉及的技术概念": "FiNet框架、形状生成网络、外观生成网络、潜在代码、共享兼容性空间、联合优化、图像到图像生成"
    },
    {
        "order": 23,
        "title": "Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.html",
        "abstract": "Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch.",
        "中文标题": "Drop to Adapt：学习无监督领域适应的判别特征",
        "摘要翻译": "最近关于领域适应的工作利用对抗训练从特征提取器和领域判别器网络的联合学习中获取领域不变的特征表示。然而，领域对抗方法由于试图在不考虑手头任务的情况下匹配领域间的分布，因此表现不佳。我们提出了Drop to Adapt（DTA），它利用对抗性dropout通过强制聚类假设来学习强判别特征。相应地，我们设计了目标函数以支持鲁棒的领域适应。我们在各种实验中证明了所提出方法的有效性，并在图像分类和语义分割任务中实现了一致的改进。我们的源代码可在https://github.com/postBG/DTA.pytorch获取。",
        "领域": "领域适应/图像分类/语义分割",
        "问题": "领域对抗方法在匹配领域间分布时未考虑具体任务，导致性能不佳",
        "动机": "提高领域适应方法在图像分类和语义分割任务中的性能",
        "方法": "利用对抗性dropout通过强制聚类假设来学习强判别特征，并设计目标函数以支持鲁棒的领域适应",
        "关键词": [
            "领域适应",
            "对抗训练",
            "聚类假设",
            "图像分类",
            "语义分割"
        ],
        "涉及的技术概念": {
            "对抗训练": "一种训练方法，通过让两个网络（如特征提取器和领域判别器）相互对抗来学习特征表示。",
            "领域不变的特征表示": "在不同领域间保持一致性，使得模型能够适应不同领域的数据。",
            "对抗性dropout": "一种改进的dropout技术，通过在训练过程中随机丢弃网络中的单元来增强模型的泛化能力。",
            "聚类假设": "假设同一类别的样本在特征空间中应该聚集在一起，从而有助于学习判别特征。"
        }
    },
    {
        "order": 24,
        "title": "Linearized Multi-Sampling for Differentiable Image Transformation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Linearized_Multi-Sampling_for_Differentiable_Image_Transformation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Linearized_Multi-Sampling_for_Differentiable_Image_Transformation_ICCV_2019_paper.html",
        "abstract": "We propose a novel image sampling method for differentiable image transformation in deep neural networks. The sampling schemes currently used in deep learning, such as Spatial Transformer Networks, rely on bilinear interpolation, which performs poorly under severe scale changes, and more importantly, results in poor gradient propagation. This is due to their strict reliance on direct neighbors. Instead, we propose to generate random auxiliary samples in the vicinity of each pixel in the sampled image, and create a linear approximation with their intensity values. We then use this approximation as a differentiable formula for the transformed image. We demonstrate that our approach produces more representative gradients with a wider basin of convergence for image alignment, which leads to considerable performance improvements when training networks for registration and classification tasks. This is not only true under large downsampling, but also when there are no scale changes. We compare our approach with multi-scale sampling and show that we outperform it. We then demonstrate that our improvements to the sampler are compatible with other tangential improvements to Spatial Transformer Networks and that it further improves their performance.",
        "中文标题": "线性化多采样用于可微分图像变换",
        "摘要翻译": "我们提出了一种新颖的图像采样方法，用于深度神经网络中的可微分图像变换。当前深度学习中使用的采样方案，如空间变换网络，依赖于双线性插值，这在严重尺度变化下表现不佳，更重要的是，导致梯度传播效果差。这是由于它们严格依赖于直接邻居。相反，我们提出在采样图像中每个像素的附近生成随机辅助样本，并用它们的强度值创建线性近似。然后，我们将此近似用作变换图像的可微分公式。我们证明了我们的方法在图像对齐中产生了更具代表性的梯度，具有更广泛的收敛盆地，这在训练网络进行配准和分类任务时带来了显著的性能改进。这不仅在大下采样下成立，而且在没有尺度变化时也成立。我们将我们的方法与多尺度采样进行比较，并显示我们优于它。然后，我们证明了我们对采样器的改进与空间变换网络的其他切线改进兼容，并进一步提高了它们的性能。",
        "领域": "图像变换/图像对齐/图像配准",
        "问题": "解决在深度神经网络中进行可微分图像变换时，现有采样方法在严重尺度变化下表现不佳和梯度传播效果差的问题",
        "动机": "提高图像变换的准确性和效率，特别是在严重尺度变化和没有尺度变化的情况下，以及改进梯度传播效果",
        "方法": "提出一种新颖的图像采样方法，通过在采样图像中每个像素的附近生成随机辅助样本，并用它们的强度值创建线性近似，以此作为变换图像的可微分公式",
        "关键词": [
            "图像采样",
            "可微分图像变换",
            "图像对齐",
            "图像配准",
            "梯度传播"
        ],
        "涉及的技术概念": "双线性插值、空间变换网络、随机辅助样本、线性近似、可微分公式、梯度传播、图像对齐、图像配准"
    },
    {
        "order": 25,
        "title": "InGAN: Capturing and Retargeting the \"DNA\" of a Natural Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shocher_InGAN_Capturing_and_Retargeting_the_DNA_of_a_Natural_Image_ICCV_2019_paper.html",
        "abstract": "Generative Adversarial Networks (GANs) typically learn a distribution of images in a large image dataset, and are then able to generate new images from this distribution. However, each natural image has its own internal statistics, captured by its unique distribution of patches. In this paper we propose an \"Internal GAN\" (InGAN) -- an image-specific GAN -- which trains on a single input image and learns its internal distribution of patches. It is then able to synthesize a plethora of new natural images of significantly different sizes, shapes and aspect-ratios - all with the same internal patch-distribution (same \"DNA\") as the input image. In particular, despite large changes in global size/shape of the image, all elements inside the image maintain their local size/shape. InGAN is fully unsupervised, requiring no additional data other than the input image itself. Once trained on the input image, it can remap the input to any size or shape in a single feedforward pass, while preserving the same internal patch distribution. InGAN provides a unified framework for a variety of tasks, bridging the gap between textures and natural images.",
        "中文标题": "InGAN: 捕捉并重定向自然图像的“DNA”",
        "摘要翻译": "生成对抗网络（GANs）通常学习大型图像数据集中的图像分布，并能够从该分布生成新图像。然而，每张自然图像都有其内部统计特性，由其独特的图像块分布所捕捉。在本文中，我们提出了一种“内部GAN”（InGAN）——一种特定于图像的GAN——它在单个输入图像上训练并学习其内部图像块分布。随后，它能够合成大量尺寸、形状和宽高比显著不同的新自然图像——所有这些图像都具有与输入图像相同的内部图像块分布（相同的“DNA”）。特别是，尽管图像的全局尺寸/形状发生了巨大变化，但图像内的所有元素都保持了其局部尺寸/形状。InGAN完全无监督，除了输入图像本身外不需要任何额外数据。一旦在输入图像上训练完成，它可以在一次前向传递中将输入重新映射到任何尺寸或形状，同时保持相同的内部图像块分布。InGAN为各种任务提供了一个统一的框架，弥合了纹理和自然图像之间的差距。",
        "领域": "图像合成/图像重定向/无监督学习",
        "问题": "如何从单个自然图像中捕捉并重定向其独特的内部图像块分布",
        "动机": "探索一种方法，使得能够从单个自然图像中捕捉其独特的内部图像块分布，并能够生成保持相同内部分布的新图像，即使图像的全局尺寸和形状发生显著变化。",
        "方法": "提出了一种特定于图像的生成对抗网络（InGAN），它在单个输入图像上训练，学习其内部图像块分布，并能够生成保持相同内部分布的新图像。",
        "关键词": [
            "图像合成",
            "图像重定向",
            "无监督学习"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，由两部分组成：生成器和判别器。生成器尝试生成与真实数据相似的假数据，而判别器则尝试区分真实数据和生成器生成的假数据。通过这种对抗过程，GANs能够学习到数据的分布，并生成新的数据样本。InGAN是一种特定于图像的GAN，它专注于学习单个图像的内部图像块分布，并能够生成保持相同内部分布的新图像。"
    },
    {
        "order": 26,
        "title": "NLNL: Negative Learning for Noisy Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_NLNL_Negative_Learning_for_Noisy_Labels_ICCV_2019_paper.html",
        "abstract": "Convolutional Neural Networks (CNNs) provide excellent performance when used for image classification. The classical method of training CNNs is by labeling images in a supervised manner as in \"input image belongs to this label\" (Positive Learning; PL), which is a fast and accurate method if the labels are assigned correctly to all images. However, if inaccurate labels, or noisy labels, exist, training with PL will provide wrong information, thus severely degrading performance. To address this issue, we start with an indirect learning method called Negative Learning (NL), in which the CNNs are trained using a complementary label as in \"input image does not belong to this complementary label.\" Because the chances of selecting a true label as a complementary label are low, NL decreases the risk of providing incorrect information. Furthermore, to improve convergence, we extend our method by adopting PL selectively, termed as Selective Negative Learning and Positive Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean data, whose choices become possible as NL progresses, thus resulting in superior performance of filtering out noisy data. With simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classification, proving the superiority of SelNLPL's noisy data filtering ability.",
        "中文标题": "NLNL：针对噪声标签的负学习",
        "摘要翻译": "卷积神经网络（CNNs）在用于图像分类时提供了卓越的性能。训练CNNs的经典方法是通过监督方式标记图像，如“输入图像属于此标签”（正学习；PL），如果所有图像的标签都正确分配，这是一种快速且准确的方法。然而，如果存在不准确的标签或噪声标签，使用PL进行训练将提供错误信息，从而严重降低性能。为了解决这个问题，我们开始使用一种称为负学习（NL）的间接学习方法，其中CNNs使用互补标签进行训练，如“输入图像不属于此互补标签”。由于选择真实标签作为互补标签的机会较低，NL降低了提供错误信息的风险。此外，为了提高收敛性，我们通过选择性地采用PL来扩展我们的方法，称为选择性负学习和正学习（SelNLPL）。PL被选择性地用于训练预期为干净的数据，随着NL的进展，这些选择成为可能，从而在过滤噪声数据方面表现出卓越的性能。通过简单的半监督训练技术，我们的方法在噪声数据分类方面达到了最先进的准确性，证明了SelNLPL在噪声数据过滤能力上的优越性。",
        "领域": "图像分类/噪声数据处理/半监督学习",
        "问题": "处理图像分类中的噪声标签问题",
        "动机": "噪声标签会严重降低卷积神经网络的性能，需要一种方法来减少错误信息的影响",
        "方法": "采用负学习（NL）方法减少错误信息风险，并通过选择性正学习（SelNLPL）提高收敛性和过滤噪声数据的能力",
        "关键词": [
            "负学习",
            "选择性学习",
            "噪声数据过滤"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNNs）": "一种深度学习模型，特别适用于处理图像数据。",
            "正学习（PL）": "传统的监督学习方法，通过直接标记图像进行训练。",
            "负学习（NL）": "一种间接学习方法，通过使用互补标签来减少错误信息的影响。",
            "选择性负学习和正学习（SelNLPL）": "结合负学习和选择性正学习的方法，以提高模型对噪声数据的过滤能力和收敛性。",
            "半监督训练": "一种训练方法，结合了监督学习和无监督学习的特点，用于处理部分标记的数据。"
        }
    },
    {
        "order": 27,
        "title": "AdaTransform: Adaptive Data Transformation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_AdaTransform_Adaptive_Data_Transformation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tang_AdaTransform_Adaptive_Data_Transformation_ICCV_2019_paper.html",
        "abstract": "Data augmentation is widely used to increase data variance in training deep neural networks. However, previous methods require either comprehensive domain knowledge or high computational cost. Can we learn data transformation automatically and efficiently with limited domain knowledge? Furthermore, can we leverage data transformation to improve not only network training but also network testing? In this work, we propose adaptive data transformation to achieve the two goals. The AdaTransform can increase data variance in training and decrease data variance in testing. Experiments on different tasks prove that it can improve generalization performance.",
        "中文标题": "AdaTransform: 自适应数据变换",
        "摘要翻译": "数据增强被广泛用于增加训练深度神经网络时的数据方差。然而，之前的方法要么需要全面的领域知识，要么计算成本高。我们能否在有限的领域知识下自动且高效地学习数据变换？此外，我们能否利用数据变换不仅改善网络训练，还能改善网络测试？在这项工作中，我们提出了自适应数据变换来实现这两个目标。AdaTransform可以在训练时增加数据方差，在测试时减少数据方差。在不同任务上的实验证明，它可以提高泛化性能。",
        "领域": "数据增强/神经网络训练/泛化性能",
        "问题": "如何在有限的领域知识下自动且高效地学习数据变换，并利用数据变换改善网络训练和测试",
        "动机": "解决之前数据增强方法需要全面领域知识或高计算成本的问题，同时探索数据变换在网络训练和测试中的双重应用",
        "方法": "提出自适应数据变换AdaTransform，通过增加训练时的数据方差和减少测试时的数据方差来提高泛化性能",
        "关键词": [
            "数据增强",
            "自适应变换",
            "泛化性能"
        ],
        "涉及的技术概念": "数据增强是一种技术，用于通过创建原始数据的修改版本来增加训练数据的多样性，从而提高模型的泛化能力。自适应数据变换指的是根据数据特性自动调整变换策略，以优化模型训练和测试过程。泛化性能指的是模型在未见过的数据上的表现能力。"
    },
    {
        "order": 28,
        "title": "Seeing What a GAN Cannot Generate",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bau_Seeing_What_a_GAN_Cannot_Generate_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bau_Seeing_What_a_GAN_Cannot_Generate_ICCV_2019_paper.html",
        "abstract": "Despite the success of Generative Adversarial Networks (GANs), mode collapse remains a serious issue during GAN training. To date, little work has focused on understanding and quantifying which modes have been dropped by a model. In this work, we visualize mode collapse at both the distribution level and the instance level. First, we deploy a semantic segmentation network to compare the distribution of segmented objects in the generated images with the target distribution in the training set. Differences in statistics reveal object classes that are omitted by a GAN. Second, given the identified omitted object classes, we visualize the GAN's omissions directly. In particular, we compare specific differences between individual photos and their approximate inversions by a GAN. To this end, we relax the problem of inversion and solve the tractable problem of inverting a GAN layer instead of the entire generator. Finally, we use this framework to analyze several recent GANs trained on multiple datasets and identify their typical failure cases.",
        "中文标题": "看到GAN无法生成的内容",
        "摘要翻译": "尽管生成对抗网络（GANs）取得了成功，但在GAN训练过程中，模式崩溃仍然是一个严重的问题。迄今为止，很少有工作集中在理解和量化模型丢弃了哪些模式上。在这项工作中，我们在分布级别和实例级别上可视化模式崩溃。首先，我们部署了一个语义分割网络来比较生成图像中分割对象的分布与训练集中的目标分布。统计上的差异揭示了被GAN忽略的对象类别。其次，给定已识别的被忽略对象类别，我们直接可视化GAN的遗漏。特别是，我们比较了单个照片与其通过GAN的近似反转之间的具体差异。为此，我们放宽了反转问题，并解决了反转GAN层而不是整个生成器的可处理问题。最后，我们使用这个框架分析了在多个数据集上训练的几种最近的GAN，并识别了它们的典型失败案例。",
        "领域": "生成对抗网络/语义分割/图像生成",
        "问题": "理解和量化生成对抗网络（GANs）在训练过程中丢弃的模式",
        "动机": "解决GAN训练中的模式崩溃问题，提高生成图像的质量和多样性",
        "方法": "部署语义分割网络比较生成图像与训练集的分布差异，可视化GAN的遗漏，通过反转GAN层分析失败案例",
        "关键词": [
            "模式崩溃",
            "语义分割",
            "图像反转"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，通过对抗过程生成数据。语义分割是一种图像分析技术，用于将图像分割成多个区域或对象。图像反转是指通过模型将图像转换回其潜在表示的过程。"
    },
    {
        "order": 29,
        "title": "Adversarial Robustness vs. Model Compression, or Both?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.html",
        "abstract": "It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting; training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.",
        "中文标题": "对抗鲁棒性与模型压缩，或两者兼得？",
        "摘要翻译": "众所周知，深度神经网络（DNNs）容易受到对抗攻击的影响，这种攻击通过在良性样本上添加精心设计的扰动来实现。基于最小最大鲁棒优化的对抗训练可以提供一种对抗攻击的安全感。然而，对抗鲁棒性需要比仅使用良性样本的自然训练更大的网络容量。本文提出了一个同时进行对抗训练和权重剪枝的框架，该框架在保持对抗鲁棒性的同时实现了模型压缩，并从根本上解决了对抗训练的困境。此外，本研究还探讨了在传统设置下关于权重剪枝的两个假设，并发现权重剪枝对于在对抗设置下减少网络模型大小至关重要；即使从大型模型继承初始化，从头开始训练一个小模型也无法实现对抗鲁棒性或高标准的准确性。代码可在https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM获取。",
        "领域": "对抗学习/模型压缩/网络优化",
        "问题": "深度神经网络在对抗攻击下的脆弱性以及模型压缩与对抗鲁棒性之间的权衡问题",
        "动机": "解决深度神经网络在对抗攻击下的脆弱性问题，同时探索模型压缩与保持对抗鲁棒性的可能性",
        "方法": "提出一个同时进行对抗训练和权重剪枝的框架，以在保持对抗鲁棒性的同时实现模型压缩",
        "关键词": [
            "对抗训练",
            "权重剪枝",
            "模型压缩",
            "对抗鲁棒性"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）的对抗攻击脆弱性、最小最大鲁棒优化、对抗训练、权重剪枝、模型压缩、网络容量、对抗鲁棒性、标准准确性"
    },
    {
        "order": 30,
        "title": "CARAFE: Content-Aware ReAssembly of FEatures",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CARAFE_Content-Aware_ReAssembly_of_FEatures_ICCV_2019_paper.html",
        "abstract": "Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit subpixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2% AP, 1.3% AP, 1.8% mIoU, 1.1dB respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection.",
        "中文标题": "CARAFE: 内容感知的特征重组",
        "摘要翻译": "特征上采样是现代卷积网络架构中的一项关键操作，例如特征金字塔。其设计对于密集预测任务（如目标检测和语义/实例分割）至关重要。在本研究中，我们提出了内容感知的特征重组（CARAFE），这是一种通用、轻量级且高效的算子，旨在实现这一目标。CARAFE具有几个吸引人的特性：（1）大视野。与之前的工作（例如双线性插值）仅利用亚像素邻域不同，CARAFE可以在大感受野内聚合上下文信息。（2）内容感知处理。与使用固定核处理所有样本（例如反卷积）不同，CARAFE实现了实例特定的内容感知处理，能够即时生成自适应核。（3）轻量级且计算快速。CARAFE引入的计算开销很小，并且可以轻松集成到现代网络架构中。我们在目标检测、实例/语义分割和修复的标准基准上进行了全面评估。CARAFE在所有任务中均显示出一致且显著的增益（分别为1.2% AP、1.3% AP、1.8% mIoU、1.1dB），且计算开销可忽略不计。它有潜力成为未来研究的强大构建块。代码和模型可在https://github.com/open-mmlab/mmdetection获取。",
        "领域": "目标检测/语义分割/实例分割",
        "问题": "提高特征上采样操作的效率和效果",
        "动机": "为了在密集预测任务中实现更高效和有效的特征上采样",
        "方法": "提出了一种名为CARAFE的内容感知特征重组算子，该算子能够在大感受野内聚合上下文信息，实现实例特定的内容感知处理，并保持轻量级和计算快速",
        "关键词": [
            "特征上采样",
            "内容感知",
            "自适应核"
        ],
        "涉及的技术概念": "CARAFE是一种新型的特征上采样算子，它通过在大感受野内聚合上下文信息来提高上采样的效果，同时通过生成自适应核来实现内容感知处理，从而在保持轻量级和计算快速的同时，显著提升了密集预测任务的性能。"
    },
    {
        "order": 31,
        "title": "COCO-GAN: Generation by Parts via Conditional Coordinating",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_COCO-GAN_Generation_by_Parts_via_Conditional_Coordinating_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_COCO-GAN_Generation_by_Parts_via_Conditional_Coordinating_ICCV_2019_paper.html",
        "abstract": "Humans can only interact with part of the surrounding environment due to biological restrictions. Therefore, we learn to reason the spatial relationships across a series of observations to piece together the surrounding environment. Inspired by such behavior and the fact that machines also have computational constraints, we propose COnditional COordinate GAN (COCO-GAN) of which the generator generates images by parts based on their spatial coordinates as the condition. On the other hand, the discriminator learns to justify realism across multiple assembled patches by global coherence, local appearance, and edge-crossing continuity. Despite the full images are never manipulated during training, we show that COCO-GAN can produce state-of-the-art-quality full images during inference. We further demonstrate a variety of novel applications enabled by our coordinate-aware framework. First, we perform extrapolation to the learned coordinate manifold and generate off-the-boundary patches. Combining with the originally generated full image, COCO-GAN can produce images that are larger than training samples, which we called \"beyond-boundary generation\". We then showcase panorama generation within a cylindrical coordinate system that inherently preserves horizontally cyclic topology. On the computation side, COCO-GAN has a built-in divide-and-conquer paradigm that reduces memory requisition during training and inference, provides high-parallelism, and can generate parts of images on-demand.",
        "中文标题": "COCO-GAN: 通过条件协调生成部分图像",
        "摘要翻译": "由于生物限制，人类只能与周围环境的一部分进行互动。因此，我们学会通过一系列观察来推理空间关系，以拼凑出周围环境。受这种行为以及机器也有计算限制的事实的启发，我们提出了条件坐标GAN（COCO-GAN），其生成器根据空间坐标作为条件生成部分图像。另一方面，判别器通过全局一致性、局部外观和边缘交叉连续性来学习判断多个组装补丁的真实性。尽管在训练期间从未操作过完整图像，但我们展示了COCO-GAN在推理期间可以生成最先进质量的完整图像。我们进一步展示了由我们的坐标感知框架启用的各种新颖应用。首先，我们对学习到的坐标流形进行外推，并生成边界外的补丁。结合最初生成的完整图像，COCO-GAN可以生成比训练样本更大的图像，我们称之为“超越边界生成”。然后，我们展示了在圆柱坐标系内的全景生成，该坐标系固有地保留了水平循环拓扑。在计算方面，COCO-GAN具有内置的分而治之范式，减少了训练和推理期间的内存需求，提供了高并行性，并且可以按需生成图像的部分。",
        "领域": "图像生成/全景生成/计算效率",
        "问题": "如何在计算资源有限的情况下生成高质量的全景图像",
        "动机": "受人类只能与周围环境的一部分进行互动的生物限制启发，以及机器也有计算限制的事实，研究如何在有限的计算资源下生成高质量的全景图像",
        "方法": "提出条件坐标GAN（COCO-GAN），通过生成器根据空间坐标生成部分图像，判别器通过全局一致性、局部外观和边缘交叉连续性判断多个组装补丁的真实性",
        "关键词": [
            "图像生成",
            "全景生成",
            "计算效率"
        ],
        "涉及的技术概念": "COCO-GAN是一种基于条件坐标的生成对抗网络，它通过分而治之的范式减少内存需求，提供高并行性，并可以按需生成图像的部分。这种方法允许在计算资源有限的情况下生成高质量的全景图像，并且能够生成比训练样本更大的图像，称为“超越边界生成”。此外，COCO-GAN在圆柱坐标系内进行全景生成，保留了水平循环拓扑。"
    },
    {
        "order": 32,
        "title": "On the Design of Black-Box Adversarial Examples by Leveraging Gradient-Free Optimization and Operator Splitting Method",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_On_the_Design_of_Black-Box_Adversarial_Examples_by_Leveraging_Gradient-Free_ICCV_2019_paper.html",
        "abstract": "Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.",
        "中文标题": "利用无梯度优化和算子分裂方法设计黑盒对抗样本",
        "摘要翻译": "鲁棒机器学习是目前最突出的主题之一，它可能有助于塑造先进AI平台的未来，这些平台不仅在平均情况下表现良好，而且在最坏情况或不利情况下也能表现良好。尽管有长期愿景，但现有的黑盒对抗攻击研究仍然局限于非常特定的威胁模型设置（例如，单一失真度量和对目标模型查询反馈的限制性假设）和/或遭受极高的查询复杂性。为了推动这一领域的进一步进展，我们引入了一个基于算子分裂方法的通用框架，即交替方向乘子法（ADMM），以设计高效、鲁棒的黑盒攻击，这些攻击适用于各种失真度量和反馈设置，而不会产生高查询复杂性。由于威胁模型的黑盒性质，提出的ADMM解决方案框架与零阶（ZO）优化和贝叶斯优化（BO）集成，因此适用于无梯度领域。这导致了两种新的黑盒对抗攻击生成方法，ZO-ADMM和BO-ADMM。我们在图像分类数据集上的实证评估表明，与最先进的攻击方法相比，我们提出的方法具有更低的函数查询复杂性，但实现了非常有竞争力的攻击成功率。",
        "领域": "对抗样本生成/优化算法/鲁棒机器学习",
        "问题": "设计高效、鲁棒的黑盒对抗攻击方法，适用于各种失真度量和反馈设置，而不会产生高查询复杂性",
        "动机": "推动鲁棒机器学习领域的发展，特别是在黑盒对抗攻击方面，以应对最坏情况或不利情况下的挑战",
        "方法": "引入基于算子分裂方法的通用框架，即交替方向乘子法（ADMM），并与零阶（ZO）优化和贝叶斯优化（BO）集成，设计出ZO-ADMM和BO-ADMM两种新的黑盒对抗攻击生成方法",
        "关键词": [
            "对抗样本生成",
            "优化算法",
            "鲁棒机器学习"
        ],
        "涉及的技术概念": {
            "算子分裂方法": "一种数学优化技术，用于分解复杂问题为更简单的子问题",
            "交替方向乘子法（ADMM）": "一种用于解决优化问题的算法，特别适用于分布式计算环境",
            "零阶（ZO）优化": "一种不依赖于梯度信息的优化方法，适用于黑盒优化问题",
            "贝叶斯优化（BO）": "一种基于概率模型的全局优化策略，适用于优化昂贵的黑盒函数"
        }
    },
    {
        "order": 33,
        "title": "AFD-Net: Aggregated Feature Difference Learning for Cross-Spectral Image Patch Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_AFD-Net_Aggregated_Feature_Difference_Learning_for_Cross-Spectral_Image_Patch_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Quan_AFD-Net_Aggregated_Feature_Difference_Learning_for_Cross-Spectral_Image_Patch_Matching_ICCV_2019_paper.html",
        "abstract": "Image patch matching across different spectral domains is more challenging than in a single spectral domain. We consider the reason is twofold: 1. the weaker discriminative feature learned by conventional methods; 2. the significant appearance difference between two images domains. To tackle these problems, we propose an aggregated feature difference learning network (AFD-Net). Unlike other methods that merely rely on the high-level features, we find the feature differences in other levels also provide useful learning information. Thus, the multi-level feature differences are aggregated to enhance the discrimination. To make features invariant across different domains, we introduce a domain invariant feature extraction network based on instance normalization (IN). In order to optimize the AFD-Net, we borrow the large margin cosine loss which can minimize intra-class distance and maximize inter-class distance between matching and non-matching samples. Extensive experiments show that AFD-Net largely outperforms the state-of-the-arts on the cross-spectral dataset, meanwhile, demonstrates a considerable generalizability on a single spectral dataset.",
        "中文标题": "AFD-Net: 用于跨光谱图像块匹配的聚合特征差异学习",
        "摘要翻译": "跨不同光谱域的图像块匹配比单一光谱域更具挑战性。我们认为原因有二：1. 传统方法学习的特征区分性较弱；2. 两个图像域之间存在显著的外观差异。为了解决这些问题，我们提出了一个聚合特征差异学习网络（AFD-Net）。与其他仅依赖高级特征的方法不同，我们发现其他层次的特征差异也提供了有用的学习信息。因此，多级特征差异被聚合以增强区分性。为了使特征在不同域之间保持不变，我们引入了一个基于实例归一化（IN）的域不变特征提取网络。为了优化AFD-Net，我们借用了大边距余弦损失，它可以最小化匹配和非匹配样本之间的类内距离并最大化类间距离。大量实验表明，AFD-Net在跨光谱数据集上大大优于现有技术，同时在单一光谱数据集上也展示了相当大的泛化能力。",
        "领域": "跨光谱图像匹配/特征学习/实例归一化",
        "问题": "跨不同光谱域的图像块匹配",
        "动机": "传统方法在跨光谱图像匹配中学习的特征区分性较弱，且不同光谱域之间存在显著的外观差异，导致匹配效果不佳。",
        "方法": "提出了一个聚合特征差异学习网络（AFD-Net），通过聚合多级特征差异来增强区分性，并引入基于实例归一化的域不变特征提取网络，使用大边距余弦损失进行优化。",
        "关键词": [
            "跨光谱图像匹配",
            "特征差异学习",
            "实例归一化",
            "大边距余弦损失"
        ],
        "涉及的技术概念": "AFD-Net是一种用于跨光谱图像块匹配的网络，通过聚合多级特征差异来增强区分性。实例归一化（IN）用于提取域不变特征，大边距余弦损失用于优化网络，以最小化类内距离并最大化类间距离。"
    },
    {
        "order": 34,
        "title": "Neural Turtle Graphics for Modeling City Road Layouts",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_Neural_Turtle_Graphics_for_Modeling_City_Road_Layouts_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chu_Neural_Turtle_Graphics_for_Modeling_City_Road_Layouts_ICCV_2019_paper.html",
        "abstract": "We propose Neural Turtle Graphics (NTG), a novel generative model for spatial graphs, and demonstrate its applications in modeling city road layouts. Specifically, we represent the road layout using a graph where nodes in the graph represent control points and edges in the graph represents road segments. NTG is a sequential generative model parameterized by a neural network. It iteratively generates a new node and an edge connecting to an existing node conditioned on the current graph. We train NTG on Open Street Map data and show it outperforms existing approaches using a set of diverse performance metrics. Moreover, our method allows users to control styles of generated road layouts mimicking existing cities as well as to sketch a part of the city road layout to be synthesized. In addition to synthesis, the proposed NTG finds uses in an analytical task of aerial road parsing. Experimental results show that it achieves state-of-the-art performance on the SpaceNet dataset.",
        "中文标题": "神经海龟图形用于城市道路布局建模",
        "摘要翻译": "我们提出了神经海龟图形（NTG），一种新颖的空间图生成模型，并展示了其在城市道路布局建模中的应用。具体来说，我们使用图来表示道路布局，图中的节点代表控制点，边代表道路段。NTG是一个由神经网络参数化的序列生成模型。它根据当前图的条件，迭代生成一个新节点和一个连接到现有节点的边。我们在Open Street Map数据上训练NTG，并展示其在多种性能指标上优于现有方法。此外，我们的方法允许用户控制生成的道路布局风格，模仿现有城市，以及绘制要合成的城市道路布局的一部分。除了合成之外，提出的NTG还在航空道路解析的分析任务中找到了用途。实验结果表明，它在SpaceNet数据集上实现了最先进的性能。",
        "领域": "空间图生成/城市道路布局/航空道路解析",
        "问题": "如何有效地生成和解析城市道路布局",
        "动机": "为了更有效地建模和解析城市道路布局，以及提供用户控制生成布局风格的能力",
        "方法": "提出了一种由神经网络参数化的序列生成模型，即神经海龟图形（NTG），用于迭代生成城市道路布局的节点和边",
        "关键词": [
            "空间图生成",
            "城市道路布局",
            "航空道路解析"
        ],
        "涉及的技术概念": "神经海龟图形（NTG）是一种新颖的空间图生成模型，通过神经网络参数化，能够迭代生成城市道路布局的节点和边。该方法在Open Street Map数据上训练，并在SpaceNet数据集上实现了最先进的性能。"
    },
    {
        "order": 35,
        "title": "DewarpNet: Single-Image Document Unwarping With Stacked 3D and 2D Regression Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_DewarpNet_Single-Image_Document_Unwarping_With_Stacked_3D_and_2D_Regression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Das_DewarpNet_Single-Image_Document_Unwarping_With_Stacked_3D_and_2D_Regression_ICCV_2019_paper.html",
        "abstract": "Capturing document images with hand-held devices in unstructured environments is a common practice nowadays. However, \"casual\" photos of documents are usually unsuitable for automatic information extraction, mainly due to physical distortion of the document paper, as well as various camera positions and illumination conditions. In this work, we propose DewarpNet, a deep-learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. Also, we contribute the largest and most comprehensive dataset for document image unwarping to date - Doc3D. This dataset features multiple ground-truth annotations, including 3D shape, surface normals, UV map, albedo image, etc. Training with Doc3D, we demonstrate state-of-the-art performance for DewarpNet with extensive qualitative and quantitative evaluations. Our network also significantly improves OCR performance on captured document images, decreasing character error rate by 42% on average. Both the code and the dataset are released.",
        "中文标题": "DewarpNet：使用堆叠的3D和2D回归网络进行单图像文档去扭曲",
        "摘要翻译": "如今，在非结构化环境中使用手持设备捕获文档图像已成为一种常见做法。然而，文档的“随意”照片通常不适合自动信息提取，主要是由于文档纸张的物理扭曲，以及各种相机位置和照明条件。在这项工作中，我们提出了DewarpNet，一种从单图像进行文档图像去扭曲的深度学习方法。我们的见解是，文档的3D几何不仅决定了其纹理的扭曲，还导致了照明效果。因此，我们的新颖之处在于在端到端流程中明确建模文档纸张的3D形状。此外，我们贡献了迄今为止最大且最全面的文档图像去扭曲数据集——Doc3D。该数据集具有多个地面真实注释，包括3D形状、表面法线、UV贴图、反照率图像等。通过Doc3D的训练，我们通过广泛的定性和定量评估展示了DewarpNet的最先进性能。我们的网络还显著提高了捕获文档图像的OCR性能，平均字符错误率降低了42%。代码和数据集均已发布。",
        "领域": "文档图像处理/3D几何建模/光学字符识别",
        "问题": "解决在非结构化环境中捕获的文档图像由于物理扭曲、相机位置和照明条件导致的自动信息提取困难问题",
        "动机": "提高文档图像的质量，以便更有效地进行自动信息提取和OCR处理",
        "方法": "提出DewarpNet，一种深度学习方法，通过明确建模文档纸张的3D形状来解决文档图像扭曲问题，并贡献了一个包含多种地面真实注释的Doc3D数据集",
        "关键词": [
            "文档图像去扭曲",
            "3D几何建模",
            "OCR性能提升"
        ],
        "涉及的技术概念": "DewarpNet是一种深度学习方法，用于从单图像进行文档图像去扭曲。它通过明确建模文档纸张的3D形状来解决文档图像扭曲问题，并利用Doc3D数据集进行训练，该数据集包含3D形状、表面法线、UV贴图、反照率图像等多种地面真实注释。"
    },
    {
        "order": 36,
        "title": "Deep Joint-Semantics Reconstructing Hashing for Large-Scale Unsupervised Cross-Modal Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Deep_Joint-Semantics_Reconstructing_Hashing_for_Large-Scale_Unsupervised_Cross-Modal_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Su_Deep_Joint-Semantics_Reconstructing_Hashing_for_Large-Scale_Unsupervised_Cross-Modal_Retrieval_ICCV_2019_paper.html",
        "abstract": "Cross-modal hashing encodes the multimedia data into a common binary hash space in which the correlations among the samples from different modalities can be effectively measured. Deep cross-modal hashing further improves the retrieval performance as the deep neural networks can generate more semantic relevant features and hash codes. In this paper, we study the unsupervised deep cross-modal hash coding and propose Deep Joint-Semantics Reconstructing Hashing (DJSRH), which has the following two main advantages. First, to learn binary codes that preserve the neighborhood structure of the original data, DJSRH constructs a novel joint-semantics affinity matrix which elaborately integrates the original neighborhood information from different modalities and accordingly is capable to capture the latent intrinsic semantic affinity for the input multi-modal instances. Second, DJSRH later trains the networks to generate binary codes that maximally reconstruct above joint-semantics relations via the proposed reconstructing framework, which is more competent for the batch-wise training as it reconstructs the specific similarity value unlike the common Laplacian constraint merely preserving the similarity order. Extensive experiments demonstrate the significant improvement by DJSRH in various cross-modal retrieval tasks.",
        "中文标题": "深度联合语义重构哈希用于大规模无监督跨模态检索",
        "摘要翻译": "跨模态哈希将多媒体数据编码到一个共同的二进制哈希空间中，在此空间中，可以有效地测量来自不同模态的样本之间的相关性。深度跨模态哈希进一步提高了检索性能，因为深度神经网络可以生成更多语义相关的特征和哈希码。在本文中，我们研究了无监督深度跨模态哈希编码，并提出了深度联合语义重构哈希（DJSRH），它具有以下两个主要优势。首先，为了学习能够保留原始数据邻域结构的二进制码，DJSRH构建了一个新颖的联合语义亲和矩阵，该矩阵精心整合了来自不同模态的原始邻域信息，因此能够捕捉输入多模态实例的潜在内在语义亲和力。其次，DJSRH随后训练网络生成二进制码，这些二进制码通过提出的重构框架最大限度地重构上述联合语义关系，这更适合于批量训练，因为它重构了特定的相似性值，而不仅仅是保留相似性顺序的常见拉普拉斯约束。大量实验证明了DJSRH在各种跨模态检索任务中的显著改进。",
        "领域": "跨模态检索/哈希编码/语义分析",
        "问题": "如何在大规模无监督跨模态检索中有效编码和检索多媒体数据",
        "动机": "提高跨模态检索的效率和准确性，通过深度学习方法生成更语义相关的特征和哈希码",
        "方法": "提出深度联合语义重构哈希（DJSRH），通过构建联合语义亲和矩阵和重构框架来学习保留原始数据邻域结构的二进制码",
        "关键词": [
            "跨模态检索",
            "哈希编码",
            "语义分析",
            "无监督学习",
            "深度神经网络"
        ],
        "涉及的技术概念": "跨模态哈希编码是一种将不同模态（如文本和图像）的数据映射到共同二进制空间的技术，以便于跨模态检索。深度神经网络用于生成语义相关的特征和哈希码，提高检索性能。联合语义亲和矩阵是一种整合不同模态邻域信息的方法，用于捕捉多模态实例的潜在语义关系。重构框架用于训练网络生成能够重构特定相似性值的二进制码，提高批量训练的效率。"
    },
    {
        "order": 37,
        "title": "Texture Fields: Learning Texture Representations in Function Space",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Oechsle_Texture_Fields_Learning_Texture_Representations_in_Function_Space_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Oechsle_Texture_Fields_Learning_Texture_Representations_in_Function_Space_ICCV_2019_paper.html",
        "abstract": "In recent years, substantial progress has been achieved in learning-based reconstruction of 3D objects. At the same time, generative models were proposed that can generate highly realistic images. However, despite this success in these closely related tasks, texture reconstruction of 3D objects has received little attention from the research community and state-of-the-art methods are either limited to comparably low resolution or constrained experimental setups. A major reason for these limitations is that common representations of texture are inefficient or hard to interface for modern deep learning techniques. In this paper, we propose Texture Fields, a novel texture representation which is based on regressing a continuous 3D function parameterized with a neural network. Our approach circumvents limiting factors like shape discretization and parameterization, as the proposed texture representation is independent of the shape representation of the 3D object. We show that Texture Fields are able to represent high frequency texture and naturally blend with modern deep learning techniques. Experimentally, we find that Texture Fields compare favorably to state-of-the-art methods for conditional texture reconstruction of 3D objects and enable learning of probabilistic generative models for texturing unseen 3D models. We believe that Texture Fields will become an important building block for the next generation of generative 3D models.",
        "中文标题": "纹理场：在函数空间中学习纹理表示",
        "摘要翻译": "近年来，在基于学习的3D对象重建方面取得了实质性进展。同时，提出了能够生成高度逼真图像的生成模型。然而，尽管在这些密切相关的任务中取得了成功，3D对象的纹理重建却很少受到研究界的关注，最先进的方法要么局限于相对较低的分辨率，要么受限于实验设置。这些限制的一个主要原因是，常见的纹理表示对于现代深度学习技术来说效率低下或难以接口。在本文中，我们提出了纹理场，一种基于用神经网络参数化的连续3D函数回归的新型纹理表示。我们的方法规避了形状离散化和参数化等限制因素，因为所提出的纹理表示与3D对象的形状表示无关。我们展示了纹理场能够表示高频纹理，并自然地与现代深度学习技术融合。实验上，我们发现纹理场在3D对象的条件纹理重建方面优于最先进的方法，并能够学习用于纹理未见过的3D模型的概率生成模型。我们相信，纹理场将成为下一代生成3D模型的重要构建块。",
        "领域": "3D重建/纹理生成/深度学习",
        "问题": "3D对象的纹理重建效率低下或难以与现代深度学习技术接口",
        "动机": "尽管在3D对象重建和图像生成方面取得了进展，但3D对象的纹理重建研究较少，现有方法存在分辨率低或实验设置受限的问题",
        "方法": "提出了一种基于神经网络参数化的连续3D函数回归的新型纹理表示——纹理场，该方法独立于3D对象的形状表示，能够表示高频纹理并自然融合现代深度学习技术",
        "关键词": [
            "3D重建",
            "纹理生成",
            "深度学习"
        ],
        "涉及的技术概念": "纹理场是一种新型的纹理表示方法，它通过神经网络参数化的连续3D函数回归来实现，这种方法不依赖于3D对象的形状表示，能够有效表示高频纹理，并且与现代深度学习技术兼容。"
    },
    {
        "order": 38,
        "title": "Learning Robust Facial Landmark Detection via Hierarchical Structured Ensemble",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Learning_Robust_Facial_Landmark_Detection_via_Hierarchical_Structured_Ensemble_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Learning_Robust_Facial_Landmark_Detection_via_Hierarchical_Structured_Ensemble_ICCV_2019_paper.html",
        "abstract": "Heatmap regression-based models have significantly advanced the progress of facial landmark detection. However, the lack of structural constraints always generates inaccurate heatmaps resulting in poor landmark detection performance. While hierarchical structure modeling methods have been proposed to tackle this issue, they all heavily rely on manually designed tree structures. The designed hierarchical structure is likely to be completely corrupted due to the missing or inaccurate prediction of landmarks. To the best of our knowledge, in the context of deep learning, no work before has investigated how to automatically model proper structures for facial landmarks, by discovering their inherent relations. In this paper, we propose a novel Hierarchical Structured Landmark Ensemble (HSLE) model for learning robust facial landmark detection, by using it as the structural constraints. Different from existing approaches of manually designing structures, our proposed HSLE model is constructed automatically via discovering the most robust patterns so HSLE has the ability to robustly depict both local and holistic landmark structures simultaneously. Our proposed HSLE can be readily plugged into any existing facial landmark detection baselines for further performance improvement. Extensive experimental results demonstrate our approach significantly outperforms the baseline by a large margin to achieve a state-of-the-art performance.",
        "中文标题": "通过层次结构化集成学习鲁棒的面部标志检测",
        "摘要翻译": "基于热图回归的模型显著推动了面部标志检测的进展。然而，缺乏结构约束总是导致生成不准确的热图，从而导致标志检测性能不佳。虽然已经提出了层次结构建模方法来解决这个问题，但它们都严重依赖于手动设计的树结构。由于标志的缺失或不准确预测，设计的层次结构可能会完全损坏。据我们所知，在深度学习的背景下，之前没有工作研究如何通过发现面部标志的内在关系来自动建模适当的结构。在本文中，我们提出了一种新颖的层次结构化标志集成（HSLE）模型，用于学习鲁棒的面部标志检测，通过将其作为结构约束。与现有的手动设计结构的方法不同，我们提出的HSLE模型是通过发现最鲁棒的模式自动构建的，因此HSLE能够同时鲁棒地描绘局部和整体的标志结构。我们提出的HSLE可以轻松地插入任何现有的面部标志检测基线中，以进一步提高性能。大量的实验结果表明，我们的方法显著优于基线，实现了最先进的性能。",
        "领域": "面部标志检测/深度学习/热图回归",
        "问题": "面部标志检测中由于缺乏结构约束导致的不准确热图生成问题",
        "动机": "解决现有面部标志检测方法中因手动设计结构导致的性能不佳问题，通过自动建模适当的结构来提高检测的鲁棒性和准确性",
        "方法": "提出了一种新颖的层次结构化标志集成（HSLE）模型，通过自动发现最鲁棒的模式来构建模型，作为结构约束，以提高面部标志检测的性能",
        "关键词": [
            "面部标志检测",
            "层次结构化",
            "热图回归",
            "鲁棒性",
            "自动建模"
        ],
        "涉及的技术概念": "热图回归是一种用于面部标志检测的技术，通过预测每个像素点的热图来定位面部标志。层次结构化指的是将面部标志按照一定的层次结构进行组织，以便更好地捕捉标志之间的关系。自动建模是指通过算法自动发现和构建模型结构，而不是依赖于手动设计。"
    },
    {
        "order": 39,
        "title": "Unsupervised Neural Quantization for Compressed-Domain Similarity Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Morozov_Unsupervised_Neural_Quantization_for_Compressed-Domain_Similarity_Search_ICCV_2019_paper.html",
        "abstract": "We tackle the problem of unsupervised visual descriptors compression, which is a key ingredient of large-scale image retrieval systems. While the deep learning machinery has benefited literally all computer vision pipelines, the existing state-of-the-art compression methods employ shallow architectures, and we aim to close this gap by our paper. In more detail, we introduce a DNN architecture for the unsupervised compressed-domain retrieval, based on multi-codebook quantization. The proposed architecture is designed to incorporate both fast data encoding and efficient distances computation via lookup tables. We demonstrate the exceptional advantage of our scheme over existing quantization approaches on several datasets of visual descriptors via outperforming the previous state-of-the-art by a large margin.",
        "中文标题": "无监督神经量化用于压缩域相似性搜索",
        "摘要翻译": "我们解决了无监督视觉描述符压缩的问题，这是大规模图像检索系统的关键组成部分。虽然深度学习技术已经几乎使所有计算机视觉流程受益，但现有的最先进的压缩方法采用了浅层架构，我们旨在通过本文缩小这一差距。更详细地说，我们引入了一种基于多码本量化的无监督压缩域检索的深度神经网络架构。所提出的架构旨在通过查找表结合快速数据编码和高效距离计算。我们通过在多个视觉描述符数据集上大幅超越之前的最先进方法，展示了我们方案相对于现有量化方法的显著优势。",
        "领域": "图像检索/数据压缩/神经网络",
        "问题": "无监督视觉描述符压缩",
        "动机": "缩小现有最先进压缩方法采用浅层架构与深度学习技术之间的差距",
        "方法": "引入基于多码本量化的深度神经网络架构，结合快速数据编码和高效距离计算",
        "关键词": [
            "无监督学习",
            "视觉描述符",
            "数据压缩",
            "图像检索",
            "多码本量化"
        ],
        "涉及的技术概念": "深度神经网络（DNN）架构、多码本量化、查找表、数据编码、距离计算"
    },
    {
        "order": 40,
        "title": "PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_PointFlow_3D_Point_Cloud_Generation_With_Continuous_Normalizing_Flows_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_PointFlow_3D_Point_Cloud_Generation_With_Continuous_Normalizing_Flows_ICCV_2019_paper.html",
        "abstract": "As 3D point clouds become the representation of choice for multiple vision and graphics applications, the ability to synthesize or reconstruct high-resolution, high-fidelity point clouds becomes crucial. Despite the recent success of deep learning models in discriminative tasks of point clouds, generating point clouds remains challenging. This paper proposes a principled probabilistic framework to generate 3D point clouds by modeling them as a distribution of distributions. Specifically, we learn a two-level hierarchy of distributions where the first level is the distribution of shapes and the second level is the distribution of points given a shape. This formulation allows us to both sample shapes and sample an arbitrary number of points from a shape. Our generative model, named PointFlow, learns each level of the distribution with a continuous normalizing flow. The invertibility of normalizing flows enables the computation of the likelihood during training and allows us to train our model in the variational inference framework. Empirically, we demonstrate that PointFlow achieves state-of-the-art performance in point cloud generation. We additionally show that our model can faithfully reconstruct point clouds and learn useful representations in an unsupervised manner. The code is available at https://github.com/stevenygd/PointFlow.",
        "中文标题": "PointFlow: 使用连续归一化流生成3D点云",
        "摘要翻译": "随着3D点云成为多种视觉和图形应用的首选表示方法，合成或重建高分辨率、高保真点云的能力变得至关重要。尽管深度学习模型在点云的判别任务中取得了近期的成功，生成点云仍然具有挑战性。本文提出了一种原则性的概率框架，通过将3D点云建模为分布的分布来生成3D点云。具体来说，我们学习了一个两级分布的层次结构，其中第一级是形状的分布，第二级是给定形状的点的分布。这种表述使我们能够从形状中采样形状和任意数量的点。我们的生成模型名为PointFlow，使用连续归一化流学习每一级的分布。归一化流的可逆性使得在训练期间能够计算似然，并允许我们在变分推理框架中训练我们的模型。经验上，我们证明了PointFlow在点云生成方面达到了最先进的性能。我们还展示了我们的模型能够忠实地重建点云，并以无监督的方式学习有用的表示。代码可在https://github.com/stevenygd/PointFlow获取。",
        "领域": "3D点云生成/连续归一化流/变分推理",
        "问题": "生成高分辨率、高保真的3D点云",
        "动机": "尽管深度学习在点云的判别任务中取得了成功，但生成点云仍然是一个挑战，需要新的方法来解决。",
        "方法": "提出了一种原则性的概率框架，通过将3D点云建模为分布的分布，并使用连续归一化流学习每一级的分布。",
        "关键词": [
            "3D点云生成",
            "连续归一化流",
            "变分推理"
        ],
        "涉及的技术概念": "连续归一化流是一种用于建模复杂分布的技术，它通过一系列可逆变换将简单分布转换为复杂分布。变分推理是一种近似推断方法，用于在概率模型中估计难以直接计算的分布。"
    },
    {
        "order": 41,
        "title": "Remote Heart Rate Measurement From Highly Compressed Facial Videos: An End-to-End Deep Learning Solution With Video Enhancement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Remote_Heart_Rate_Measurement_From_Highly_Compressed_Facial_Videos_An_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Remote_Heart_Rate_Measurement_From_Highly_Compressed_Facial_Videos_An_ICCV_2019_paper.html",
        "abstract": "Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real-world applications.",
        "中文标题": "从高度压缩的面部视频中远程测量心率：一种带有视频增强的端到端深度学习解决方案",
        "摘要翻译": "远程光电容积描记术（rPPG），旨在无需任何接触即可测量心脏活动，在许多应用中（例如远程医疗）具有巨大潜力。现有的rPPG方法依赖于分析面部视频的非常细微的细节，这些细节容易受到视频压缩的影响。在这里，我们提出了一种两阶段的端到端方法，使用隐藏的rPPG信息增强和注意力网络，这是首次尝试对抗视频压缩损失并从高度压缩的视频中恢复rPPG信号。该方法包括两部分：1）用于视频增强的时空视频增强网络（STVEN），和2）用于rPPG信号恢复的rPPG网络（rPPGNet）。rPPGNet可以独立工作以进行稳健的rPPG测量，而STVEN网络可以添加并联合训练，以进一步提高性能，特别是在高度压缩的视频上。在两个基准数据集上进行了全面的实验，结果表明，1）所提出的方法不仅在高质量视频配对的压缩视频上实现了卓越的性能，2）它还在只有压缩视频可用的新数据上表现出良好的泛化能力，这暗示了其在实际应用中的巨大潜力。",
        "领域": "远程医疗/视频处理/心率监测",
        "问题": "解决从高度压缩的面部视频中准确测量心率的问题",
        "动机": "现有的rPPG方法容易受到视频压缩的影响，需要一种新方法来对抗视频压缩损失并恢复rPPG信号",
        "方法": "提出了一种两阶段的端到端方法，包括时空视频增强网络（STVEN）和rPPG网络（rPPGNet），用于视频增强和rPPG信号恢复",
        "关键词": [
            "远程光电容积描记术",
            "视频压缩",
            "心率测量"
        ],
        "涉及的技术概念": {
            "远程光电容积描记术（rPPG）": "一种无需接触即可测量心脏活动的技术",
            "时空视频增强网络（STVEN）": "用于增强视频质量的网络",
            "rPPG网络（rPPGNet）": "用于从视频中恢复rPPG信号的网络",
            "视频压缩": "减少视频文件大小的过程，可能会影响视频质量"
        }
    },
    {
        "order": 42,
        "title": "Siamese Networks: The Tale of Two Manifolds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Roy_Siamese_Networks_The_Tale_of_Two_Manifolds_ICCV_2019_paper.html",
        "abstract": "Siamese networks are non-linear deep models that have found their ways into a broad set of problems in learning theory, thanks to their embedding capabilities. In this paper, we study Siamese networks from a new perspective and question the validity of their training procedure. We show that in the majority of cases, the objective of a Siamese network is endowed with an invariance property. Neglecting the invariance property leads to a hindrance in training the Siamese networks. To alleviate this issue, we propose two Riemannian structures and generalize a well-established accelerated stochastic gradient descent method to take into account the proposed Riemannian structures. Our empirical evaluations suggest that by making use of the Riemannian geometry, we achieve state-of-the-art results against several algorithms for the challenging problem of fine-grained image classification.",
        "中文标题": "Siamese网络：两个流形的故事",
        "摘要翻译": "Siamese网络是非线性深度模型，由于其嵌入能力，已在学习理论中的广泛问题中找到了应用。在本文中，我们从新的角度研究Siamese网络，并质疑其训练过程的有效性。我们表明，在大多数情况下，Siamese网络的目标具有不变性属性。忽视不变性属性会导致训练Siamese网络的障碍。为了缓解这个问题，我们提出了两种黎曼结构，并将一种成熟的加速随机梯度下降方法推广到考虑所提出的黎曼结构。我们的实证评估表明，通过利用黎曼几何，我们在细粒度图像分类这一挑战性问题中，对几种算法实现了最先进的结果。",
        "领域": "细粒度图像分类/黎曼几何/深度学习",
        "问题": "Siamese网络训练过程中的不变性属性被忽视，导致训练障碍",
        "动机": "质疑Siamese网络训练过程的有效性，并探索如何通过考虑不变性属性来改进训练",
        "方法": "提出两种黎曼结构，并推广加速随机梯度下降方法以考虑这些结构",
        "关键词": [
            "Siamese网络",
            "黎曼几何",
            "细粒度图像分类",
            "加速随机梯度下降"
        ],
        "涉及的技术概念": {
            "Siamese网络": "一种非线性深度模型，用于学习输入数据之间的相似性",
            "黎曼几何": "一种数学工具，用于研究流形上的几何性质，本文中用于改进Siamese网络的训练过程",
            "加速随机梯度下降": "一种优化算法，用于加速模型的训练过程，本文中推广以考虑黎曼结构"
        }
    },
    {
        "order": 43,
        "title": "Meta-Sim: Learning to Generate Synthetic Datasets",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kar_Meta-Sim_Learning_to_Generate_Synthetic_Datasets_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kar_Meta-Sim_Learning_to_Generate_Synthetic_Datasets_ICCV_2019_paper.html",
        "abstract": "Training models to high-end performance requires availability of large labeled datasets, which are expensive to get. The goal of our work is to automatically synthesize labeled datasets that are relevant for a downstream task. We propose Meta-Sim, which learns a generative model of synthetic scenes, and obtain images as well as its corresponding ground-truth via a graphics engine. We parametrize our dataset generator with a neural network, which learns to modify attributes of scene graphs obtained from probabilistic scene grammars, so as to minimize the distribution gap between its rendered outputs and target data. If the real dataset comes with a small labeled validation set, we additionally aim to optimize a meta-objective, i.e. downstream task performance. Experiments show that the proposed method can greatly improve content generation quality over a human-engineered probabilistic scene grammar, both qualitatively and quantitatively as measured by performance on a downstream task.",
        "中文标题": "Meta-Sim：学习生成合成数据集",
        "摘要翻译": "训练模型以达到高性能需要大量标注数据集，这些数据集获取成本高昂。我们工作的目标是自动合成与下游任务相关的标注数据集。我们提出了Meta-Sim，它学习合成场景的生成模型，并通过图形引擎获取图像及其对应的真实标签。我们用神经网络参数化我们的数据集生成器，该网络学习修改从概率场景语法获得的场景图的属性，以最小化其渲染输出与目标数据之间的分布差距。如果真实数据集带有一个小的标注验证集，我们还旨在优化一个元目标，即下游任务的性能。实验表明，所提出的方法可以大大改善内容生成质量，无论是在定性还是定量上，都通过下游任务的性能来衡量，优于人工设计的概率场景语法。",
        "领域": "合成数据生成/场景理解/图形引擎",
        "问题": "如何自动合成与下游任务相关的标注数据集",
        "动机": "减少获取大量标注数据集的成本",
        "方法": "提出Meta-Sim，通过神经网络参数化数据集生成器，学习修改场景图属性以最小化渲染输出与目标数据之间的分布差距，并优化下游任务性能",
        "关键词": [
            "合成数据生成",
            "场景理解",
            "图形引擎"
        ],
        "涉及的技术概念": "Meta-Sim是一个生成模型，用于自动合成与下游任务相关的标注数据集。它通过图形引擎获取图像及其对应的真实标签，并使用神经网络参数化数据集生成器，以学习修改从概率场景语法获得的场景图的属性，从而最小化其渲染输出与目标数据之间的分布差距。此外，如果存在一个小的标注验证集，Meta-Sim还会优化下游任务的性能。"
    },
    {
        "order": 44,
        "title": "Face-to-Parameter Translation for Game Character Auto-Creation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.html",
        "abstract": "Character customization system is an important component in Role-Playing Games (RPGs), where players are allowed to edit the facial appearance of their in-game characters with their own preferences rather than using default templates. This paper proposes a method for automatically creating in-game characters of players according to an input face photo. We formulate the above \"artistic creation\" process under a facial similarity measurement and parameter searching paradigm by solving an optimization problem over a large set of physically meaningful facial parameters. To effectively minimize the distance between the created face and the real one, two loss functions, i.e. a \"discriminative loss\" and a \"facial content loss\", are specifically designed. As the rendering process of a game engine is not differentiable, a generative network is further introduced as an \"imitator\" to imitate the physical behavior of the game engine so that the proposed method can be implemented under a neural style transfer framework and the parameters can be optimized by gradient descent. Experimental results demonstrate that our method achieves a high degree of generation similarity between the input face photo and the created in-game character in terms of both global appearance and local details. Our method has been deployed in a new game last year and has now been used by players over 1 million times.",
        "中文标题": "面向游戏角色自动创建的面部到参数转换",
        "摘要翻译": "角色定制系统是角色扮演游戏（RPGs）中的一个重要组成部分，它允许玩家根据自己的偏好编辑游戏角色的面部外观，而不是使用默认模板。本文提出了一种根据输入的面部照片自动创建玩家游戏角色的方法。我们将上述“艺术创作”过程置于面部相似度测量和参数搜索范式下，通过解决一个基于大量物理意义面部参数的优化问题来制定。为了有效最小化创建的面部与真实面部之间的距离，特别设计了两个损失函数，即“判别损失”和“面部内容损失”。由于游戏引擎的渲染过程不可微分，进一步引入了一个生成网络作为“模仿者”来模仿游戏引擎的物理行为，以便所提出的方法可以在神经风格迁移框架下实现，并且参数可以通过梯度下降进行优化。实验结果表明，我们的方法在输入面部照片和创建的游戏角色之间实现了高度的生成相似性，无论是在全局外观还是局部细节方面。我们的方法已于去年部署在一款新游戏中，并且现在已被玩家使用超过100万次。",
        "领域": "游戏开发/面部识别/生成模型",
        "问题": "如何根据玩家的面部照片自动创建游戏角色",
        "动机": "提高角色定制系统的个性化和用户体验，使玩家能够更真实地反映自己的面部特征在游戏角色中",
        "方法": "通过面部相似度测量和参数搜索范式，结合判别损失和面部内容损失两个损失函数，以及引入生成网络模仿游戏引擎的物理行为，实现面部到参数的转换",
        "关键词": [
            "角色定制",
            "面部相似度",
            "参数优化",
            "生成网络",
            "神经风格迁移"
        ],
        "涉及的技术概念": "面部相似度测量、参数搜索、优化问题、判别损失、面部内容损失、生成网络、神经风格迁移、梯度下降"
    },
    {
        "order": 45,
        "title": "Learning Combinatorial Embedding Networks for Deep Graph Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.html",
        "abstract": "Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.",
        "中文标题": "学习组合嵌入网络用于深度图匹配",
        "摘要翻译": "图匹配指的是在图之间找到节点对应关系，以便最大化对应节点和边的亲和力。除了其NP完全性之外，另一个重要挑战是跨图的节点和结构亲和力的有效建模以及由此产生的目标，以指导匹配过程有效地找到真实匹配，对抗噪声。为此，本文设计了一个端到端的可微分深度网络管道来学习图匹配的亲和力。它涉及一个关于节点对应的监督排列损失，以捕捉图匹配的组合性质。同时，采用深度图嵌入模型来参数化图内和图间的亲和力函数，而不是传统的浅层和简单的参数形式，例如高斯核。嵌入还可以有效地捕捉到二阶边之外的高阶结构。排列损失模型对节点数量不可知，嵌入模型在节点之间共享，使得网络允许在训练和推理时图中节点数量的变化。此外，我们的网络对类别不可知，具有一定的跨类别泛化能力。所有这些特性对于现实世界的应用都是受欢迎的。实验表明，它在最先进的图匹配学习方法中具有优越性。",
        "领域": "图匹配/深度学习/组合优化",
        "问题": "如何有效建模跨图的节点和结构亲和力，以指导图匹配过程找到真实匹配，对抗噪声",
        "动机": "解决图匹配中的NP完全性挑战，以及有效建模节点和结构亲和力的问题",
        "方法": "设计了一个端到端的可微分深度网络管道，采用深度图嵌入模型参数化亲和力函数，并引入监督排列损失来捕捉图匹配的组合性质",
        "关键词": [
            "图匹配",
            "深度学习",
            "组合优化",
            "节点对应",
            "亲和力建模"
        ],
        "涉及的技术概念": {
            "图匹配": "在图之间找到节点对应关系，以便最大化对应节点和边的亲和力",
            "NP完全性": "图匹配问题属于NP完全问题，意味着在多项式时间内无法找到所有问题的解决方案",
            "深度图嵌入模型": "用于参数化图内和图间的亲和力函数，捕捉高阶结构",
            "监督排列损失": "一种损失函数，用于捕捉图匹配的组合性质，对节点数量不可知",
            "端到端的可微分深度网络管道": "一种深度学习模型，能够从输入到输出端到端地学习，且整个网络是可微分的，便于使用梯度下降等优化方法进行训练"
        }
    },
    {
        "order": 46,
        "title": "Specifying Object Attributes and Relations in Interactive Scene Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.html",
        "abstract": "We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene_generation.",
        "中文标题": "在交互式场景生成中指定对象属性和关系",
        "摘要翻译": "我们介绍了一种从输入场景图生成图像的方法。该方法将布局嵌入和外观嵌入分开。这种双重嵌入导致生成的图像更好地匹配场景图，具有更高的视觉质量，并支持更复杂的场景图。此外，嵌入方案支持每个场景图生成多个和多样化的输出图像，用户可以进一步控制。我们展示了两种对象控制模式：（i）从其他图像导入元素，和（ii）通过选择外观原型在对象空间中进行导航。我们的代码公开在https://www.github.com/ashual/scene_generation。",
        "领域": "场景生成/图像生成/用户交互",
        "问题": "如何从场景图生成更匹配、视觉质量更高且支持更复杂场景的图像",
        "动机": "提高生成图像与场景图的匹配度、视觉质量，并支持更复杂的场景图，同时提供用户控制",
        "方法": "采用布局嵌入和外观嵌入的双重嵌入方法，支持每个场景图生成多个和多样化的输出图像，并提供两种对象控制模式",
        "关键词": [
            "场景图",
            "图像生成",
            "用户控制"
        ],
        "涉及的技术概念": "布局嵌入指的是在生成图像时对场景中对象的位置和大小进行编码的方法；外观嵌入则是对对象的外观特征进行编码。双重嵌入方法结合了这两种编码方式，以提高生成图像的质量和匹配度。用户控制模式包括从其他图像导入元素和在对象空间中选择外观原型进行导航。"
    },
    {
        "order": 47,
        "title": "Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Balakrishnan_Visual_Deprojection_Probabilistic_Recovery_of_Collapsed_Dimensions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Balakrishnan_Visual_Deprojection_Probabilistic_Recovery_of_Collapsed_Dimensions_ICCV_2019_paper.html",
        "abstract": "We introduce visual deprojection: the task of recovering an image or video that has been collapsed along a dimension. Projections arise in various contexts, such as long-exposure photography, where a dynamic scene is collapsed in time to produce a motion-blurred image, and corner cameras, where reflected light from a scene is collapsed along a spatial dimension because of an edge occluder to yield a 1D video. Deprojection is ill-posed-- often there are many plausible solutions for a given input. We first propose a probabilistic model capturing the ambiguity of the task. We then present a variational inference strategy using convolutional neural networks as functional approximators. Sampling from the inference network at test time yields plausible candidates from the distribution of original signals that are consistent with a given input projection. We evaluate the method on several datasets for both spatial and temporal deprojection tasks. We first demonstrate the method can recover human gait videos and face images from spatial projections, and then show that it can recover videos of moving digits from dramatically motion-blurred images obtained via temporal projection.",
        "中文标题": "视觉反投影：概率性恢复塌陷维度",
        "摘要翻译": "我们介绍了视觉反投影：恢复沿某一维度塌陷的图像或视频的任务。投影出现在各种情境中，例如长时间曝光摄影，其中动态场景在时间上塌陷以产生运动模糊图像，以及角落相机，其中场景的反射光由于边缘遮挡器沿空间维度塌陷以产生一维视频。反投影是不适定的——对于给定的输入，通常有许多合理的解决方案。我们首先提出了一个概率模型，捕捉任务的模糊性。然后，我们提出了一种使用卷积神经网络作为功能逼近器的变分推理策略。在测试时从推理网络采样，可以从与给定输入投影一致的原始信号分布中产生合理的候选。我们在几个数据集上评估了该方法，用于空间和时间反投影任务。我们首先展示了该方法可以从空间投影中恢复人类步态视频和面部图像，然后展示了它可以从通过时间投影获得的显著运动模糊图像中恢复移动数字的视频。",
        "领域": "图像恢复/视频处理/概率模型",
        "问题": "恢复沿某一维度塌陷的图像或视频",
        "动机": "解决由于投影导致的图像或视频信息丢失问题，恢复原始信号",
        "方法": "提出概率模型捕捉任务模糊性，使用卷积神经网络进行变分推理，从推理网络采样产生合理候选",
        "关键词": [
            "图像恢复",
            "视频处理",
            "概率模型",
            "卷积神经网络",
            "变分推理"
        ],
        "涉及的技术概念": "视觉反投影是一种恢复沿某一维度塌陷的图像或视频的技术。通过概率模型和卷积神经网络的变分推理策略，可以从塌陷的投影中恢复出原始信号。这种方法适用于处理长时间曝光摄影中的运动模糊图像和角落相机中的一维视频，通过采样推理网络，可以生成与给定投影一致的原始信号候选。"
    },
    {
        "order": 48,
        "title": "Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kuang_Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kuang_Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid_ICCV_2019_paper.html",
        "abstract": "Matching clothing images from customers and online shopping stores has rich applications in E-commerce. Existing algorithms encoded an image as a global feature vector and performed retrieval with the global representation. However, discriminative local information on clothes are submerged in this global representation, resulting in sub-optimal performance. To address this issue, we propose a novel Graph Reasoning Network (GRNet) on a Similarity Pyramid, which learns similarities between a query and a gallery cloth by using both global and local representations in multiple scales. The similarity pyramid is represented by a Graph of similarity, where nodes represent similarities between clothing components at different scales, and the final matching score is obtained by message passing along edges. In GRNet, graph reasoning is solved by training a graph convolutional network, enabling to align salient clothing components to improve clothing retrieval. To facilitate future researches, we introduce a new benchmark FindFashion, containing rich annotations of bounding boxes, views, occlusions, and cropping. Extensive experiments show that GRNet obtains new state-of-the-art results on two challenging benchmarks, e.g. pushing the top-1, top-20, and top-50 accuracies on DeepFashion to 26%, 64%, and 75% (i.e. 4%, 10%, and 10% absolute improvements), outperforming competitors with large margins. On FindFashion, GRNet achieves considerable improvements on all empirical settings.",
        "中文标题": "通过相似度金字塔上的图推理网络进行时尚检索",
        "摘要翻译": "将顾客的服装图像与在线购物商店的服装图像进行匹配在电子商务中有着丰富的应用。现有算法将图像编码为全局特征向量，并使用全局表示进行检索。然而，在这种全局表示中，服装的区分性局部信息被淹没，导致性能不佳。为了解决这个问题，我们提出了一种新颖的图推理网络（GRNet）在相似度金字塔上，该网络通过使用多尺度的全局和局部表示来学习查询和画廊服装之间的相似度。相似度金字塔由相似度图表示，其中节点表示不同尺度的服装组件之间的相似度，最终匹配分数通过沿边传递消息获得。在GRNet中，图推理通过训练图卷积网络来解决，使得能够对齐显著的服装组件以改进服装检索。为了促进未来的研究，我们引入了一个新的基准FindFashion，包含丰富的边界框、视角、遮挡和裁剪注释。大量实验表明，GRNet在两个具有挑战性的基准上获得了新的最先进结果，例如将DeepFashion上的top-1、top-20和top-50准确率推至26%、64%和75%（即4%、10%和10%的绝对改进），以较大优势超越竞争对手。在FindFashion上，GRNet在所有实验设置上都取得了显著的改进。",
        "领域": "时尚检索/图卷积网络/相似度学习",
        "问题": "现有服装图像检索算法在全局表示中淹没局部信息，导致性能不佳",
        "动机": "提高服装图像检索的准确性和效率，通过利用多尺度的全局和局部表示来学习服装之间的相似度",
        "方法": "提出了一种新颖的图推理网络（GRNet）在相似度金字塔上，通过训练图卷积网络来解决图推理，对齐显著的服装组件以改进服装检索",
        "关键词": [
            "时尚检索",
            "图卷积网络",
            "相似度学习",
            "多尺度表示",
            "服装组件对齐"
        ],
        "涉及的技术概念": "图推理网络（GRNet）是一种利用图卷积网络进行图推理的方法，用于在相似度金字塔上学习服装图像之间的相似度。相似度金字塔通过图表示不同尺度的服装组件之间的相似度，并通过消息传递机制获得最终匹配分数。这种方法能够有效地对齐服装的显著组件，从而提高服装检索的准确性。"
    },
    {
        "order": 49,
        "title": "SinGAN: Learning a Generative Model From a Single Natural Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html",
        "abstract": "We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.",
        "中文标题": "SinGAN: 从单一自然图像学习生成模型",
        "摘要翻译": "我们介绍了SinGAN，一种可以从单一自然图像中学习的无条件生成模型。我们的模型被训练来捕捉图像内部补丁的分布，然后能够生成高质量、多样化的样本，这些样本携带与图像相同的视觉内容。SinGAN包含一个全卷积GAN金字塔，每个GAN负责学习图像不同尺度的补丁分布。这使得能够生成任意大小和纵横比的新样本，这些样本具有显著的变异性，同时保持了训练图像的全局结构和精细纹理。与之前的单一图像GAN方案相比，我们的方法不仅限于纹理图像，而且是无条件的（即它从噪声生成样本）。用户研究证实，生成的样本通常被误认为是真实图像。我们展示了SinGAN在广泛的图像处理任务中的实用性。",
        "领域": "图像生成/图像处理/生成对抗网络",
        "问题": "如何从单一自然图像中学习生成高质量、多样化的样本",
        "动机": "探索一种能够从单一自然图像中学习并生成具有相同视觉内容的高质量、多样化样本的方法，以克服现有单一图像GAN方案的局限性。",
        "方法": "采用一个包含全卷积GAN金字塔的模型，每个GAN负责学习图像不同尺度的补丁分布，从而生成任意大小和纵横比的新样本。",
        "关键词": [
            "图像生成",
            "生成对抗网络",
            "图像处理"
        ],
        "涉及的技术概念": "SinGAN是一种无条件生成模型，通过训练捕捉单一自然图像内部补丁的分布，利用全卷积GAN金字塔结构学习不同尺度的补丁分布，从而生成高质量、多样化的样本。这种方法不仅限于纹理图像，且生成的样本具有显著的变异性，同时保持了训练图像的全局结构和精细纹理。"
    },
    {
        "order": 50,
        "title": "StructureFlow: Image Inpainting via Structure-Aware Appearance Flow",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ren_StructureFlow_Image_Inpainting_via_Structure-Aware_Appearance_Flow_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ren_StructureFlow_Image_Inpainting_via_Structure-Aware_Appearance_Flow_ICCV_2019_paper.html",
        "abstract": "Image inpainting techniques have shown significant improvements by using deep neural networks recently. However, most of them may either fail to reconstruct reasonable structures or restore fine-grained textures. In order to solve this problem, in this paper, we propose a two-stage model which splits the inpainting task into two parts: structure reconstruction and texture generation. In the first stage, edge-preserved smooth images are employed to train a structure reconstructor which completes the missing structures of the inputs. In the second stage, based on the reconstructed structures, a texture generator using appearance flow is designed to yield image details. Experiments on multiple publicly available datasets show the superior performance of the proposed network.",
        "中文标题": "StructureFlow: 通过结构感知的外观流进行图像修复",
        "摘要翻译": "近年来，通过使用深度神经网络，图像修复技术已经显示出显著的改进。然而，大多数技术要么无法重建合理的结构，要么无法恢复细粒度的纹理。为了解决这个问题，本文提出了一种两阶段模型，该模型将修复任务分为两部分：结构重建和纹理生成。在第一阶段，使用边缘保留的平滑图像来训练结构重建器，以完成输入图像的缺失结构。在第二阶段，基于重建的结构，设计了一个使用外观流的纹理生成器来生成图像细节。在多个公开可用的数据集上的实验显示了所提出网络的优越性能。",
        "领域": "图像修复/深度学习/生成模型",
        "问题": "图像修复中结构重建和纹理生成的不足",
        "动机": "解决现有图像修复技术在重建合理结构和恢复细粒度纹理方面的不足",
        "方法": "提出了一种两阶段模型，包括结构重建和纹理生成，使用边缘保留的平滑图像训练结构重建器，并基于重建的结构设计使用外观流的纹理生成器",
        "关键词": [
            "图像修复",
            "结构重建",
            "纹理生成",
            "外观流"
        ],
        "涉及的技术概念": "深度神经网络用于图像修复，两阶段模型分别处理结构重建和纹理生成，使用边缘保留的平滑图像训练结构重建器，外观流用于纹理生成"
    },
    {
        "order": 51,
        "title": "Wavelet Domain Style Transfer for an Effective Perception-Distortion Tradeoff in Single Image Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.html",
        "abstract": "In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to find a high-resolution (HR) version of it which is both accurate and photorealistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Specifically, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods.",
        "中文标题": "小波域风格迁移实现单图像超分辨率中感知-失真权衡的有效方法",
        "摘要翻译": "在单图像超分辨率（SISR）中，给定一个低分辨率（LR）图像，人们希望找到一个既准确又逼真的高分辨率（HR）版本。最近，研究表明在低失真和高感知质量之间存在一个基本的权衡，而生成对抗网络（GAN）被证明能有效接近感知-失真（PD）界限。在本文中，我们提出了一种基于小波域风格迁移（WDST）的新方法，它比基于GAN的方法实现了更好的PD权衡。具体来说，我们提出使用二维静态小波变换（SWT）将一幅图像分解为低频和高频子带。对于低频子带，我们通过增强网络提高其客观质量。对于高频子带，我们提出使用WDST有效提高其感知质量。凭借小波的完美重构特性，这些子带可以重新组合，以获得同时具有高客观和感知质量的图像。在各种数据集上的数值结果表明，我们的方法在现有最先进的SISR方法中实现了失真和感知质量之间的最佳权衡。",
        "领域": "图像超分辨率/小波变换/风格迁移",
        "问题": "在单图像超分辨率中实现感知质量与失真之间的有效权衡",
        "动机": "探索在单图像超分辨率中，如何通过小波域风格迁移方法实现比现有方法更优的感知-失真权衡",
        "方法": "使用二维静态小波变换将图像分解为低频和高频子带，分别通过增强网络和小波域风格迁移提高低频子带的客观质量和高频子带的感知质量，然后利用小波的完美重构特性重新组合子带，获得高质量的超分辨率图像",
        "关键词": [
            "小波变换",
            "风格迁移",
            "图像超分辨率"
        ],
        "涉及的技术概念": "小波域风格迁移（WDST）是一种利用小波变换将图像分解为不同频率子带，并在这些子带上应用风格迁移技术以提高图像感知质量的方法。二维静态小波变换（SWT）是一种图像分解技术，能够将图像分解为低频和高频子带。生成对抗网络（GAN）是一种通过对抗过程生成数据的深度学习模型，常用于提高图像的感知质量。"
    },
    {
        "order": 52,
        "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_VaTeX_A_Large-Scale_High-Quality_Multilingual_Dataset_for_Video-and-Language_Research_ICCV_2019_paper.html",
        "abstract": "We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, \\vatex is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on \\vatex: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the \\vatex dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using \\vatex for other video-and-language research.",
        "中文标题": "VaTeX: 一个用于视频与语言研究的大规模高质量多语言数据集",
        "摘要翻译": "我们介绍了一个新的大规模多语言视频描述数据集VATEX，该数据集包含超过41,250个视频和825,000个英文和中文的标题。在这些标题中，有超过206,000个英汉平行翻译对。与广泛使用的MSR-VTT数据集相比，VATEX是多语言的，规模更大，语言复杂性更高，并且在视频和自然语言描述方面更加多样化。我们还基于VATEX引入了两项视频与语言研究的任务：（1）多语言视频字幕生成，旨在使用一个紧凑的统一字幕生成模型以多种语言描述视频；（2）视频引导的机器翻译，利用视频信息作为额外的时空上下文，将源语言描述翻译成目标语言。在VATEX数据集上的大量实验表明，首先，统一的多语言模型不仅可以更高效地为视频生成英文和中文描述，而且其性能优于单语言模型。此外，我们证明了时空视频上下文可以有效地用于对齐源语言和目标语言，从而辅助机器翻译。最后，我们讨论了使用VATEX进行其他视频与语言研究的潜力。",
        "领域": "视频字幕生成/机器翻译/多语言处理",
        "问题": "如何高效地为视频生成多语言描述以及如何利用视频信息辅助机器翻译",
        "动机": "现有的视频描述数据集在语言多样性和规模上存在限制，需要一个新的多语言、大规模且高质量的数据集来推动视频与语言研究的发展",
        "方法": "引入了一个新的大规模多语言视频描述数据集VATEX，并基于此数据集提出了多语言视频字幕生成和视频引导的机器翻译两项任务",
        "关键词": [
            "视频字幕生成",
            "机器翻译",
            "多语言处理"
        ],
        "涉及的技术概念": "VATEX数据集包含大量视频和对应的多语言标题，支持多语言视频字幕生成和视频引导的机器翻译任务。多语言视频字幕生成任务旨在使用统一模型为视频生成多种语言的描述，而视频引导的机器翻译任务则利用视频的时空上下文信息辅助翻译过程。"
    },
    {
        "order": 53,
        "title": "Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Siddiquee_Learning_Fixed_Points_in_Generative_Adversarial_Networks_From_Image-to-Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Siddiquee_Learning_Fixed_Points_in_Generative_Adversarial_Networks_From_Image-to-Image_Translation_ICCV_2019_paper.html",
        "abstract": "Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN \"virtually heal\" anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN.",
        "中文标题": "学习生成对抗网络中的固定点：从图像到图像的转换到疾病检测与定位",
        "摘要翻译": "生成对抗网络（GANs）已经引领了图像到图像转换的革命。GANs的发展和普及引发了一个有趣的问题：我们能否训练一个GAN来从图像中移除一个对象（如果存在的话），同时保留图像的其余部分？具体来说，GAN能否通过将一个人的医疗图像（健康状况未知，患病或健康）转换为健康图像来“虚拟治愈”任何人，从而通过减去这两张图像来揭示患病区域？这样的任务要求GAN识别用于域转换的目标像素的最小子集，这种能力我们称之为固定点转换，目前还没有GAN具备这种能力。因此，我们提出了一种新的GAN，称为固定点GAN，通过（1）通过条件身份损失监督同域转换，和（2）通过修订的对抗性、域分类和循环一致性损失正则化跨域转换来训练。基于固定点转换，我们进一步推导出一个仅使用图像级注释的疾病检测和定位的新框架。定性和定量评估表明，所提出的方法在多域图像到图像转换方面优于现有技术，并且在疾病检测和定位方面超过了主要的弱监督定位方法。实现可在https://github.com/jlianglab/Fixed-Point-GAN获取。",
        "领域": "图像生成/医疗图像分析/弱监督学习",
        "问题": "如何训练生成对抗网络（GAN）以在保留图像其余部分的同时移除特定对象，并应用于医疗图像中的疾病检测与定位",
        "动机": "探索GAN在医疗图像处理中的应用，特别是通过将未知健康状况的医疗图像转换为健康图像来揭示疾病区域，从而辅助疾病检测和定位",
        "方法": "提出固定点GAN，通过条件身份损失监督同域转换和通过修订的对抗性、域分类和循环一致性损失正则化跨域转换来训练，并基于此开发新的疾病检测和定位框架",
        "关键词": [
            "生成对抗网络",
            "图像转换",
            "疾病检测",
            "固定点转换",
            "弱监督学习"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，由生成器和判别器组成，用于生成新的数据样本。固定点转换是指GAN识别并转换图像中特定区域的能力，这在医疗图像处理中用于疾病检测和定位。条件身份损失、对抗性损失、域分类损失和循环一致性损失是训练GAN时使用的特定损失函数，用于确保生成图像的质量和一致性。"
    },
    {
        "order": 54,
        "title": "Toward Real-World Single Image Super-Resolution: A New Benchmark and a New Model",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Toward_Real-World_Single_Image_Super-Resolution_A_New_Benchmark_and_a_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Toward_Real-World_Single_Image_Super-Resolution_A_New_Benchmark_and_a_ICCV_2019_paper.html",
        "abstract": "Most of the existing learning-based single image super-resolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degradation (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real-world LR images are far more complicated. As a consequence, the SISR models trained on simulated data become less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (RealSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a digital camera. An image registration algorithm is developed to progressively align the image pairs at different resolutions. Considering that the degradation kernels are naturally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our RealSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones.",
        "中文标题": "面向真实世界的单图像超分辨率：新基准与新模型",
        "摘要翻译": "大多数现有的基于学习的单图像超分辨率（SISR）方法是在模拟数据集上进行训练和评估的，其中低分辨率（LR）图像是通过对其高分辨率（HR）对应图像应用简单且统一的退化（即双三次下采样）生成的。然而，真实世界中的LR图像的退化要复杂得多。因此，当应用于实际场景时，基于模拟数据训练的SISR模型效果较差。在本文中，我们构建了一个真实世界的超分辨率（RealSR）数据集，其中通过调整数码相机的焦距来捕捉同一场景的成对LR-HR图像。开发了一种图像配准算法，以逐步对齐不同分辨率的图像对。考虑到我们的数据集中退化核自然是非均匀的，我们提出了一种基于拉普拉斯金字塔的核预测网络（LP-KPN），它有效地学习每个像素的核以恢复HR图像。我们的大量实验表明，与在模拟数据集上训练的SISR模型相比，在我们的RealSR数据集上训练的SISR模型在真实世界场景中提供了更好的视觉质量，具有更锐利的边缘和更精细的纹理。尽管我们的RealSR数据集仅使用两台相机（佳能5D3和尼康D810）构建，但训练后的模型能够很好地推广到其他相机设备，如索尼a7II和手机。",
        "领域": "图像超分辨率/图像恢复/图像配准",
        "问题": "解决真实世界单图像超分辨率问题，特别是在复杂退化情况下的效果提升",
        "动机": "现有基于模拟数据训练的SISR模型在应用于真实世界场景时效果不佳，需要构建更接近真实世界退化的数据集和模型",
        "方法": "构建真实世界超分辨率数据集RealSR，开发图像配准算法对齐不同分辨率的图像对，提出基于拉普拉斯金字塔的核预测网络（LP-KPN）学习每个像素的核以恢复HR图像",
        "关键词": [
            "图像超分辨率",
            "图像恢复",
            "图像配准",
            "核预测网络"
        ],
        "涉及的技术概念": {
            "单图像超分辨率（SISR）": "一种提高图像分辨率的技术，旨在从低分辨率图像中恢复出高分辨率图像",
            "图像配准": "一种将两个或多个图像对齐的技术，以便它们可以在同一坐标系下进行比较或融合",
            "拉普拉斯金字塔": "一种多分辨率分析技术，用于图像处理，能够有效地表示图像的不同尺度特征",
            "核预测网络（KPN）": "一种深度学习模型，用于预测每个像素的退化核，以便更准确地恢复高分辨率图像"
        }
    },
    {
        "order": 55,
        "title": "A Graph-Based Framework to Bridge Movies and Synopses",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_A_Graph-Based_Framework_to_Bridge_Movies_and_Synopses_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_A_Graph-Based_Framework_to_Bridge_Movies_and_Synopses_ICCV_2019_paper.html",
        "abstract": "Inspired by the remarkable advances in video analytics, research teams are stepping towards a greater ambition - movie understanding. However, compared to those activity videos in conventional datasets, movies are significantly different. Generally, movies are much longer and consist of much richer temporal structures. More importantly, the interactions among characters play a central role in expressing the underlying story. To facilitate the efforts along this direction, we construct a dataset called Movie Synopses Associations (MSA) over 327 movies, which provides a synopsis for each movie, together with annotated associations between synopsis paragraphs and movie segments. On top of this dataset, we develop a framework to perform matching between movie segments and synopsis paragraphs. This framework integrates different aspects of a movie, including event dynamics and character interactions, and allows them to be matched with parsed paragraphs, based on a graph-based formulation. Our study shows that the proposed framework remarkably improves the matching accuracy over conventional feature-based methods. It also reveals the importance of narrative structures and character interactions in movie understanding. Dataset and code are available at: https://ycxioooong.github.io/projects/moviesyn",
        "中文标题": "基于图的框架桥接电影与剧情摘要",
        "摘要翻译": "受到视频分析领域显著进展的启发，研究团队正朝着一个更大的目标迈进——电影理解。然而，与传统数据集中的活动视频相比，电影有着显著的不同。通常，电影更长，包含更丰富的时间结构。更重要的是，角色之间的互动在表达潜在故事中扮演着核心角色。为了促进这一方向的研究，我们构建了一个名为电影剧情摘要关联（MSA）的数据集，涵盖了327部电影，为每部电影提供了剧情摘要，以及剧情摘要段落与电影片段之间的注释关联。基于这个数据集，我们开发了一个框架来执行电影片段与剧情摘要段落之间的匹配。该框架整合了电影的不同方面，包括事件动态和角色互动，并允许它们与解析的段落进行匹配，基于图的形式。我们的研究表明，所提出的框架显著提高了匹配准确性，超越了传统的基于特征的方法。它还揭示了叙事结构和角色互动在电影理解中的重要性。数据集和代码可在以下网址获取：https://ycxioooong.github.io/projects/moviesyn",
        "领域": "视频分析/电影理解/叙事结构",
        "问题": "如何有效地匹配电影片段与剧情摘要段落",
        "动机": "电影相比传统数据集中的活动视频更长，时间结构更丰富，角色互动在表达故事中起核心作用，需要新的方法来理解电影",
        "方法": "构建电影剧情摘要关联数据集，开发基于图的框架整合电影的事件动态和角色互动，与解析的剧情摘要段落进行匹配",
        "关键词": [
            "视频分析",
            "电影理解",
            "叙事结构",
            "角色互动",
            "图匹配"
        ],
        "涉及的技术概念": "图匹配是一种技术，用于在不同类型的数据之间建立联系，这里用于电影片段与剧情摘要段落的匹配。事件动态指的是电影中事件的发展和变化，角色互动指的是电影中角色之间的相互作用和影响。"
    },
    {
        "order": 56,
        "title": "Generative Adversarial Training for Weakly Supervised Cloud Matting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Generative_Adversarial_Training_for_Weakly_Supervised_Cloud_Matting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Generative_Adversarial_Training_for_Weakly_Supervised_Cloud_Matting_ICCV_2019_paper.html",
        "abstract": "The detection and removal of cloud in remote sensing images are essential for earth observation applications. Most previous methods consider cloud detection as a pixel-wise semantic segmentation process (cloud v.s. background), which inevitably leads to a category-ambiguity problem when dealing with semi-transparent clouds. We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions. Our model consists of three networks, a cloud generator G, a cloud discriminator D, and a cloud matting network F, where G and D aim to generate realistic and physically meaningful cloud images by adversarial training, and F learns to predict the cloud reflectance and attenuation. Experimental results on a global set of satellite images demonstrate that our method, without ever using any pixel-wise ground truth during training, achieves comparable and even higher accuracy over other fully supervised methods, including some recent popular cloud detectors and some well-known semantic segmentation frameworks.",
        "中文标题": "生成对抗训练用于弱监督云层抠图",
        "摘要翻译": "遥感图像中云层的检测和去除对于地球观测应用至关重要。大多数先前的方法将云层检测视为像素级语义分割过程（云层与背景），这在处理半透明云层时不可避免地导致类别模糊问题。我们从一个完全不同的角度重新审视云层检测，即将其制定为前景和背景图像之间的混合能量分离过程，这可以在具有明确物理意义的图像抠图范式下等效实现。我们进一步提出了一个生成对抗框架，其中我们的模型训练既不需要任何像素级地面真实参考，也不需要任何额外的用户交互。我们的模型由三个网络组成，一个云生成器G，一个云判别器D，和一个云抠图网络F，其中G和D旨在通过对抗训练生成逼真且物理上有意义的云图像，而F学习预测云的反射和衰减。在一组全球卫星图像上的实验结果表明，我们的方法在训练过程中从未使用任何像素级地面真实数据，却实现了与其他完全监督方法相当甚至更高的准确性，包括一些最近流行的云检测器和一些著名的语义分割框架。",
        "领域": "遥感图像处理/生成对抗网络/图像抠图",
        "问题": "解决遥感图像中半透明云层的检测和去除问题",
        "动机": "传统方法在处理半透明云层时存在类别模糊问题，需要一种新的方法来准确检测和去除云层",
        "方法": "提出一个生成对抗框架，包括云生成器、云判别器和云抠图网络，通过对抗训练生成逼真的云图像并预测云的反射和衰减",
        "关键词": [
            "遥感图像",
            "生成对抗网络",
            "图像抠图"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）用于生成逼真的云图像，图像抠图技术用于分离前景和背景图像，以及通过对抗训练提高模型的准确性和物理意义。"
    },
    {
        "order": 57,
        "title": "RankSRGAN: Generative Adversarial Networks With Ranker for Image Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_RankSRGAN_Generative_Adversarial_Networks_With_Ranker_for_Image_Super-Resolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_RankSRGAN_Generative_Adversarial_Networks_With_Ranker_for_Image_Super-Resolution_ICCV_2019_paper.html",
        "abstract": "Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN",
        "中文标题": "RankSRGAN: 使用排序器的生成对抗网络用于图像超分辨率",
        "摘要翻译": "生成对抗网络（GAN）已经展示了恢复单图像超分辨率（SISR）真实细节的潜力。为了进一步提高超分辨率结果的视觉质量，PIRM2018-SR挑战赛采用了感知度量来评估感知质量，如PI、NIQE和Ma。然而，现有方法无法直接优化这些不可微分的感知度量，这些度量被证明与人类评分高度相关。为了解决这个问题，我们提出了带有排序器的超分辨率生成对抗网络（RankSRGAN），以在感知度量的方向上优化生成器。具体来说，我们首先训练一个可以学习感知度量行为的排序器，然后引入一种新颖的排序内容损失来优化感知质量。最吸引人的部分是，所提出的方法可以结合不同SR方法的优点来生成更好的结果。大量实验表明，RankSRGAN实现了视觉上令人愉悦的结果，并在感知度量上达到了最先进的性能。项目页面：https://wenlongzhang0724.github.io/Projects/RankSRGAN",
        "领域": "图像超分辨率/生成对抗网络/感知质量评估",
        "问题": "现有方法无法直接优化不可微分的感知度量，这些度量与人类评分高度相关",
        "动机": "提高超分辨率结果的视觉质量，使其更符合人类感知",
        "方法": "提出带有排序器的超分辨率生成对抗网络（RankSRGAN），通过训练一个可以学习感知度量行为的排序器，并引入排序内容损失来优化感知质量",
        "关键词": [
            "图像超分辨率",
            "生成对抗网络",
            "感知质量评估"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）用于图像超分辨率，通过训练排序器学习感知度量的行为，并引入排序内容损失来优化感知质量，结合不同SR方法的优点生成更好的结果。"
    },
    {
        "order": 58,
        "title": "From Strings to Things: Knowledge-Enabled VQA Model That Can Read and Reason",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.html",
        "abstract": "Text present in images are not merely strings, they provide useful cues about the image. Despite their utility in better image understanding, scene texts are not used in traditional visual question answering (VQA) models. In this work, we present a VQA model which can read scene texts and perform reasoning on a knowledge graph to arrive at an accurate answer. Our proposed model has three mutually interacting modules: i. proposal module to get word and visual content proposals from the image, ii. fusion module to fuse these proposals, question and knowledge base to mine relevant facts, and represent these facts as multi-relational graph, iii. reasoning module to perform a novel gated graph neural network based reasoning on this graph. The performance of our knowledge-enabled VQA model is evaluated on our newly introduced dataset, viz. text-KVQA. To the best of our knowledge, this is the first dataset which identifies the need for bridging text recognition with knowledge graph based reasoning. Through extensive experiments, we show that our proposed method outperforms traditional VQA as well as question-answering over knowledge base-based methods on text-KVQA.",
        "中文标题": "从字符串到事物：能够阅读和推理的知识增强型视觉问答模型",
        "摘要翻译": "图像中的文本不仅仅是字符串，它们提供了关于图像的有用线索。尽管场景文本在更好地理解图像方面具有实用性，但传统的视觉问答（VQA）模型并未使用它们。在这项工作中，我们提出了一个VQA模型，该模型能够阅读场景文本并在知识图谱上执行推理以得出准确的答案。我们提出的模型有三个相互作用的模块：i. 提案模块，用于从图像中获取单词和视觉内容提案，ii. 融合模块，用于融合这些提案、问题和知识库以挖掘相关事实，并将这些事实表示为多关系图，iii. 推理模块，用于在该图上执行基于门控图神经网络的新颖推理。我们的知识增强型VQA模型的性能在我们新引入的数据集text-KVQA上进行了评估。据我们所知，这是第一个识别出将文本识别与基于知识图谱的推理相结合需求的数据集。通过大量实验，我们展示了我们提出的方法在text-KVQA上优于传统的VQA以及基于知识库的问答方法。",
        "领域": "视觉问答/知识图谱/图神经网络",
        "问题": "传统视觉问答模型未利用图像中的场景文本进行更好的图像理解和问答",
        "动机": "利用图像中的场景文本提供的有用线索，通过结合知识图谱和图神经网络技术，提高视觉问答模型的准确性和理解能力",
        "方法": "提出一个包含提案模块、融合模块和推理模块的VQA模型，其中提案模块从图像中提取单词和视觉内容提案，融合模块融合提案、问题和知识库以挖掘相关事实并表示为多关系图，推理模块在图上执行基于门控图神经网络的推理",
        "关键词": [
            "视觉问答",
            "知识图谱",
            "图神经网络",
            "场景文本",
            "多关系图"
        ],
        "涉及的技术概念": "视觉问答（VQA）模型、知识图谱、图神经网络（GNN）、门控图神经网络、多关系图、文本识别"
    },
    {
        "order": 59,
        "title": "PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tang_PAMTRI_Pose-Aware_Multi-Task_Learning_for_Vehicle_Re-Identification_Using_Highly_Randomized_ICCV_2019_paper.html",
        "abstract": "In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID.",
        "中文标题": "PAMTRI: 使用高度随机化合成数据进行车辆重识别的姿态感知多任务学习",
        "摘要翻译": "与已被研究界广泛研究的人员重识别（ReID）相比，车辆重识别受到的关注较少。车辆重识别具有挑战性，原因在于1）类内变异性高（由形状和外观对视角的依赖性引起），以及2）类间变异性小（由不同制造商生产的车辆在形状和外观上的相似性引起）。为了应对这些挑战，我们提出了一个姿态感知多任务重识别（PAMTRI）框架。与之前的方法相比，这种方法包括两个创新点。首先，它通过姿态估计中的关键点、热图和分割明确推理车辆姿态和形状，克服了视角依赖性。其次，它在执行重识别的同时，通过嵌入姿态表示的多任务学习，联合分类语义车辆属性（颜色和类型）。由于手动标注具有详细姿态和属性信息的图像是不可行的，我们创建了一个大规模高度随机化的合成数据集，其中包含自动注释的车辆属性用于训练。大量实验验证了每个提出组件的有效性，表明PAMTRI在两个主流车辆重识别基准测试：VeRi和CityFlow-ReID上，相比现有技术有显著改进。",
        "领域": "车辆重识别/姿态估计/多任务学习",
        "问题": "解决车辆重识别中的高类内变异性和小类间变异性问题",
        "动机": "车辆重识别因视角依赖性和不同制造商车辆间的相似性而具有挑战性",
        "方法": "提出姿态感知多任务重识别框架，通过姿态估计和多任务学习克服视角依赖性并联合分类语义车辆属性",
        "关键词": [
            "车辆重识别",
            "姿态估计",
            "多任务学习",
            "合成数据"
        ],
        "涉及的技术概念": "姿态估计中的关键点、热图和分割用于推理车辆姿态和形状；多任务学习用于联合分类语义车辆属性；使用大规模高度随机化的合成数据集进行训练。"
    },
    {
        "order": 60,
        "title": "Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yi_Progressive_Fusion_Video_Super-Resolution_Network_via_Exploiting_Non-Local_Spatio-Temporal_Correlations_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yi_Progressive_Fusion_Video_Super-Resolution_Network_via_Exploiting_Non-Local_Spatio-Temporal_Correlations_ICCV_2019_paper.html",
        "abstract": "Most previous fusion strategies either fail to fully utilize temporal information or cost too much time, and how to effectively fuse temporal information from consecutive frames plays an important role in video super-resolution (SR). In this study, we propose a novel progressive fusion network for video SR, which is designed to make better use of spatio-temporal information and is proved to be more efficient and effective than the existing direct fusion, slow fusion or 3D convolution strategies. Under this progressive fusion framework, we further introduce an improved non-local operation to avoid the complex motion estimation and motion compensation (ME&MC) procedures as in previous video SR approaches. Extensive experiments on public datasets demonstrate that our method surpasses state-of-the-art with 0.96 dB in average, and runs about 3 times faster, while requires only about half of the parameters.",
        "中文标题": "通过利用非局部时空相关性的渐进融合视频超分辨率网络",
        "摘要翻译": "大多数先前的融合策略要么未能充分利用时间信息，要么耗时过多，如何有效地从连续帧中融合时间信息在视频超分辨率（SR）中扮演着重要角色。在本研究中，我们提出了一种新颖的渐进融合网络用于视频SR，该网络旨在更好地利用时空信息，并被证明比现有的直接融合、慢速融合或3D卷积策略更高效和有效。在这个渐进融合框架下，我们进一步引入了一种改进的非局部操作，以避免像以前的视频SR方法中那样复杂的运动估计和运动补偿（ME&MC）过程。在公共数据集上的大量实验表明，我们的方法平均超过最先进技术0.96 dB，运行速度快约3倍，同时仅需要大约一半的参数。",
        "领域": "视频超分辨率/时空信息处理/非局部操作",
        "问题": "如何有效地从连续帧中融合时间信息以提高视频超分辨率的效率和质量",
        "动机": "现有的融合策略未能充分利用时间信息或耗时过多，需要一种更高效和有效的方法来融合时空信息",
        "方法": "提出了一种新颖的渐进融合网络，并引入改进的非局部操作以避免复杂的运动估计和运动补偿过程",
        "关键词": [
            "视频超分辨率",
            "时空信息",
            "非局部操作"
        ],
        "涉及的技术概念": "渐进融合网络是一种旨在更好地利用时空信息的网络结构，通过逐步融合信息来提高视频超分辨率的效率和质量。非局部操作是一种避免复杂运动估计和运动补偿过程的技术，通过考虑全局信息来改进视频处理效果。"
    },
    {
        "order": 61,
        "title": "Counterfactual Critic Multi-Agent Training for Scene Graph Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Counterfactual_Critic_Multi-Agent_Training_for_Scene_Graph_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Counterfactual_Critic_Multi-Agent_Training_for_Scene_Graph_Generation_ICCV_2019_paper.html",
        "abstract": "Scene graphs --- objects as nodes and visual relationships as edges --- describe the whereabouts and interactions of objects in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects. For example, \"person\" on \"bike\" can help to determine the relationship \"ride\", which in turn contributes to the confidence of the two objects. However, we argue that the visual context is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes should not be penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach. CMAT is a multi-agent policy gradient method that frames objects into cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the predictions of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art performance by significant gains under various settings and metrics.",
        "中文标题": "反事实批评多智能体训练用于场景图生成",
        "摘要翻译": "场景图——以对象为节点，视觉关系为边——描述了图像中对象的位置和交互，以实现全面的场景理解。为了生成连贯的场景图，几乎所有现有方法都通过建模对象之间的消息传递来利用丰富的视觉上下文。例如，“人”在“自行车”上可以帮助确定“骑”的关系，这反过来又有助于两个对象的置信度。然而，我们认为，使用流行的基于交叉熵的监督学习范式并没有正确学习视觉上下文，该范式对图的不一致性不敏感：中心节点或非中心节点的错误不应受到同等惩罚。为此，我们提出了一种反事实批评多智能体训练（CMAT）方法。CMAT是一种多智能体策略梯度方法，它将对象框架化为合作智能体，然后直接最大化图级指标作为奖励。特别是，为了正确地将奖励分配给每个智能体，CMAT使用了一个反事实基线，通过固定其他智能体的预测来解开智能体特定的奖励。在具有挑战性的Visual Genome基准上的广泛验证表明，CMAT在各种设置和指标下通过显著增益实现了最先进的性能。",
        "领域": "场景理解/视觉关系识别/多智能体系统",
        "问题": "现有方法在生成场景图时未能正确学习视觉上下文，对图的不一致性不敏感",
        "动机": "提高场景图生成的准确性和连贯性，通过更合理地分配奖励来优化图级指标",
        "方法": "提出了一种反事实批评多智能体训练（CMAT）方法，该方法是一种多智能体策略梯度方法，通过固定其他智能体的预测来解开智能体特定的奖励",
        "关键词": [
            "场景图生成",
            "视觉上下文",
            "多智能体训练",
            "反事实基线",
            "策略梯度方法"
        ],
        "涉及的技术概念": {
            "场景图": "以对象为节点，视觉关系为边，描述图像中对象的位置和交互",
            "视觉上下文": "通过建模对象之间的消息传递来利用的丰富视觉信息",
            "反事实批评多智能体训练（CMAT）": "一种多智能体策略梯度方法，通过固定其他智能体的预测来解开智能体特定的奖励",
            "图级指标": "用于评估场景图生成性能的指标，CMAT直接最大化该指标作为奖励"
        }
    },
    {
        "order": 62,
        "title": "Generative Adversarial Networks for Extreme Learned Image Compression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Agustsson_Generative_Adversarial_Networks_for_Extreme_Learned_Image_Compression_ICCV_2019_paper.html",
        "abstract": "We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.",
        "中文标题": "生成对抗网络用于极端学习图像压缩",
        "摘要翻译": "我们提出了一种基于生成对抗网络（GANs）的学习图像压缩系统，该系统在极低的比特率下运行。我们提出的框架结合了编码器、解码器/生成器和多尺度判别器，我们联合训练这些组件以实现生成式学习压缩目标。该模型合成了它无法存储的细节，在比特率极低的情况下获得了视觉上令人愉悦的结果，而之前的方法在这些比特率下会失败并显示出强烈的伪影。此外，如果原始图像的语义标签图可用，我们的方法可以完全合成解码图像中的不重要区域，如街道和树木，从而按比例减少存储成本。一项用户研究证实，在低比特率下，我们的方法优于最先进的方法，即使这些方法使用的比特数超过我们的两倍。",
        "领域": "图像压缩/生成对抗网络/深度学习",
        "问题": "在极低比特率下实现高质量的图像压缩",
        "动机": "解决在极低比特率下传统图像压缩方法无法有效工作，导致图像质量严重下降的问题",
        "方法": "提出了一种结合编码器、解码器/生成器和多尺度判别器的框架，通过联合训练实现生成式学习压缩目标，并利用语义标签图合成解码图像中的不重要区域以减少存储成本",
        "关键词": [
            "图像压缩",
            "生成对抗网络",
            "低比特率",
            "语义标签图"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种由两个神经网络——生成器和判别器——组成的框架，通过对抗过程学习生成数据。多尺度判别器是指在不同尺度上分析图像的判别器，有助于提高生成图像的质量。语义标签图是一种图像，其中每个像素都标记有对应的语义类别，如街道、树木等。"
    },
    {
        "order": 63,
        "title": "Deep SR-ITM: Joint Learning of Super-Resolution and Inverse Tone-Mapping for 4K UHD HDR Applications",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Deep_SR-ITM_Joint_Learning_of_Super-Resolution_and_Inverse_Tone-Mapping_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Deep_SR-ITM_Joint_Learning_of_Super-Resolution_and_Inverse_Tone-Mapping_for_ICCV_2019_paper.html",
        "abstract": "Recent modern displays are now able to render high dynamic range (HDR), high resolution (HR) videos of up to 8K UHD (Ultra High Definition). Consequently, UHD HDR broadcasting and streaming have emerged as high quality premium services. However, due to the lack of original UHD HDR video content, appropriate conversion technologies are urgently needed to transform the legacy low resolution (LR) standard dynamic range (SDR) videos into UHD HDR versions. In this paper, we propose a joint super-resolution (SR) and inverse tone-mapping (ITM) framework, called Deep SR-ITM, which learns the direct mapping from LR SDR video to their HR HDR version. Joint SR and ITM is an intricate task, where high frequency details must be restored for SR, jointly with the local contrast, for ITM. Our network is able to restore fine details by decomposing the input image and focusing on the separate base (low frequency) and detail (high frequency) layers. Moreover, the proposed modulation blocks apply location-variant operations to enhance local contrast. The Deep SR-ITM shows good subjective quality with increased contrast and details, outperforming the previous joint SR-ITM method.",
        "中文标题": "Deep SR-ITM：用于4K UHD HDR应用的超分辨率和逆色调映射联合学习",
        "摘要翻译": "近年来，现代显示器已经能够渲染高达8K UHD（超高清）的高动态范围（HDR）、高分辨率（HR）视频。因此，UHD HDR广播和流媒体已成为高质量的高级服务。然而，由于缺乏原始的UHD HDR视频内容，迫切需要适当的转换技术将传统的低分辨率（LR）标准动态范围（SDR）视频转换为UHD HDR版本。在本文中，我们提出了一个联合超分辨率（SR）和逆色调映射（ITM）框架，称为Deep SR-ITM，它学习从LR SDR视频到其HR HDR版本的直接映射。联合SR和ITM是一项复杂的任务，其中必须为SR恢复高频细节，同时为ITM恢复局部对比度。我们的网络能够通过分解输入图像并专注于单独的基础（低频）和细节（高频）层来恢复精细细节。此外，所提出的调制块应用位置变异操作以增强局部对比度。Deep SR-ITM显示出良好的主观质量，增加了对比度和细节，优于之前的联合SR-ITM方法。",
        "领域": "超分辨率/逆色调映射/视频处理",
        "问题": "将低分辨率标准动态范围视频转换为超高清高动态范围视频",
        "动机": "由于缺乏原始的UHD HDR视频内容，需要转换技术将传统视频升级为UHD HDR版本",
        "方法": "提出了一个联合超分辨率和逆色调映射的框架，通过分解输入图像并专注于基础层和细节层来恢复精细细节，应用调制块增强局部对比度",
        "关键词": [
            "超分辨率",
            "逆色调映射",
            "视频转换",
            "高动态范围",
            "超高清"
        ],
        "涉及的技术概念": "超分辨率（SR）技术用于提高图像或视频的分辨率，逆色调映射（ITM）技术用于将标准动态范围（SDR）内容转换为高动态范围（HDR）内容，调制块用于增强图像的局部对比度。"
    },
    {
        "order": 64,
        "title": "Robust Change Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Robust_Change_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Robust_Change_Captioning_ICCV_2019_paper.html",
        "abstract": "Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over \"before\" and \"after\" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. \"before\" or \"after\" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.",
        "中文标题": "鲁棒的变化描述",
        "摘要翻译": "描述场景中发生了什么变化对用户来说是有用的，但前提是生成的文本能够聚焦于语义上相关的内容。因此，区分干扰因素（例如视角变化）与相关变化（例如物体移动）是非常重要的。我们提出了一种新颖的双重动态注意力模型（DUDA）来执行鲁棒的变化描述。我们的模型学会区分干扰因素与语义变化，通过双重注意力在“之前”和“之后”的图像上定位变化，并通过动态说话者自适应地聚焦于必要的视觉输入（例如“之前”或“之后”的图像）来准确地用自然语言描述它们。为了深入研究这个问题，我们收集了一个CLEVR-Change数据集，该数据集基于CLEVR引擎构建，包含5种场景变化类型。我们在我们的数据集上对多个基线进行了基准测试，并系统地研究了不同的变化类型和对干扰因素的鲁棒性。我们展示了我们的DUDA模型在变化描述和定位方面的优越性。我们还展示了我们的方法是通用的，在没有干扰因素的最近的真实Spot-the-Diff数据集上获得了最先进的结果。",
        "领域": "场景理解/自然语言生成/注意力机制",
        "问题": "如何准确描述场景中的语义相关变化，同时区分和忽略干扰因素",
        "动机": "为了生成对用户有用的场景变化描述，需要聚焦于语义上相关的变化，而非干扰因素",
        "方法": "提出了一种双重动态注意力模型（DUDA），通过双重注意力机制在“之前”和“之后”的图像上定位变化，并通过动态说话者自适应地聚焦于必要的视觉输入来准确描述变化",
        "关键词": [
            "变化描述",
            "双重注意力",
            "动态说话者",
            "CLEVR-Change数据集",
            "Spot-the-Diff数据集"
        ],
        "涉及的技术概念": {
            "双重动态注意力模型（DUDA）": "一种模型，通过双重注意力机制在“之前”和“之后”的图像上定位变化，并通过动态说话者自适应地聚焦于必要的视觉输入来准确描述变化",
            "CLEVR-Change数据集": "基于CLEVR引擎构建的数据集，包含5种场景变化类型，用于研究场景变化描述问题",
            "Spot-the-Diff数据集": "一个真实的数据集，用于测试变化描述模型的性能，该数据集没有干扰因素"
        }
    },
    {
        "order": 65,
        "title": "Instance-Guided Context Rendering for Cross-Domain Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Instance-Guided_Context_Rendering_for_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning.",
        "中文标题": "实例引导的上下文渲染用于跨域行人重识别",
        "摘要翻译": "现有的行人重识别（re-id）方法大多假设在任何目标域部署中都有大规模的身份标签可用于模型学习。这大大限制了它们在实际中的可扩展性。为了解决这一限制，我们提出了一种新颖的实例引导上下文渲染方案，该方案将源行人身份转移到多样化的目标域上下文中，以在未标记的目标域中实现有监督的重识别模型学习。与之前将源行人图像转换为有限的固定目标风格的图像合成方法不同，我们的方法生成了视觉上更可信、更多样化的合成训练数据。具体来说，我们构建了一个双重条件生成对抗网络，该网络通过丰富的上下文变化来增强每个源行人图像。为了明确实现多样化的渲染效果，我们利用大量未标记的目标实例作为图像生成的上下文指导。在Market-1501、DukeMTMC-reID和CUHK03基准上的大量实验表明，在跨域重识别模型学习中使用我们的合成数据可以显著提高重识别性能。",
        "领域": "行人重识别/图像合成/生成对抗网络",
        "问题": "解决在未标记目标域中进行有监督行人重识别模型学习的问题",
        "动机": "提高行人重识别方法在实际应用中的可扩展性",
        "方法": "提出实例引导上下文渲染方案，构建双重条件生成对抗网络，利用未标记目标实例作为上下文指导生成多样化的合成训练数据",
        "关键词": [
            "行人重识别",
            "图像合成",
            "生成对抗网络"
        ],
        "涉及的技术概念": "双重条件生成对抗网络是一种结合了条件生成对抗网络（Conditional GAN）和双重学习机制的深度学习模型，用于生成具有特定条件或属性的图像。在本研究中，它被用来通过丰富的上下文变化增强源行人图像，以生成多样化的合成训练数据，从而提高跨域行人重识别的性能。"
    },
    {
        "order": 66,
        "title": "Dynamic PET Image Reconstruction Using Nonnegative Matrix Factorization Incorporated With Deep Image Prior",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yokota_Dynamic_PET_Image_Reconstruction_Using_Nonnegative_Matrix_Factorization_Incorporated_With_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yokota_Dynamic_PET_Image_Reconstruction_Using_Nonnegative_Matrix_Factorization_Incorporated_With_ICCV_2019_paper.html",
        "abstract": "We propose a method that reconstructs dynamic positron emission tomography (PET) images from given sinograms by using non-negative matrix factorization (NMF) incorporated with a deep image prior (DIP) for appropriately constraining the spatial patterns of resultant images. The proposed method can reconstruct dynamic PET images with higher signal-to-noise ratio (SNR) and blindly decompose an image matrix into pairs of spatial and temporal factors. The former represent homogeneous tissues with different kinetic parameters and the latter represent the time activity curves that are observed in the corresponding homogeneous tissues. We employ U-Nets combined in parallel for DIP and each of the U-nets is used to extract each spatial factor decomposed from the data matrix. Experimental results show that the proposed method outperforms conventional methods and can extract spatial factors that represent the homogeneous tissues.",
        "中文标题": "使用结合深度图像先验的非负矩阵分解进行动态PET图像重建",
        "摘要翻译": "我们提出了一种方法，通过使用结合深度图像先验（DIP）的非负矩阵分解（NMF）从给定的正弦图中重建动态正电子发射断层扫描（PET）图像，以适当约束结果图像的空间模式。所提出的方法能够以更高的信噪比（SNR）重建动态PET图像，并盲目地将图像矩阵分解为空间和时间因子对。前者代表具有不同动力学参数的同质组织，后者代表在相应同质组织中观察到的时间活动曲线。我们采用并行结合的U-Nets用于DIP，每个U-Net用于提取从数据矩阵分解的每个空间因子。实验结果表明，所提出的方法优于传统方法，并且能够提取代表同质组织的空间因子。",
        "领域": "医学影像重建/信号处理/深度学习应用",
        "问题": "动态PET图像重建中的信噪比提升和图像矩阵分解",
        "动机": "提高动态PET图像重建的质量，通过更准确地分解图像矩阵来识别同质组织和其时间活动曲线",
        "方法": "结合深度图像先验的非负矩阵分解方法，使用并行U-Nets提取空间因子",
        "关键词": [
            "非负矩阵分解",
            "深度图像先验",
            "U-Nets",
            "动态PET图像重建",
            "信噪比"
        ],
        "涉及的技术概念": {
            "非负矩阵分解": "一种矩阵分解方法，用于将数据矩阵分解为两个非负矩阵的乘积，适用于图像处理和数据分析",
            "深度图像先验": "利用深度神经网络作为图像先验知识，用于图像重建和去噪等任务",
            "U-Nets": "一种卷积神经网络架构，广泛用于图像分割和重建任务，具有编码器-解码器结构",
            "动态PET图像重建": "从动态正电子发射断层扫描数据中重建图像序列，用于观察生物体内的动态过程",
            "信噪比": "信号与噪声的比例，用于衡量图像或信号的质量"
        }
    },
    {
        "order": 67,
        "title": "Attention on Attention for Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.",
        "中文标题": "注意力机制在图像字幕生成中的应用",
        "摘要翻译": "注意力机制在当前图像字幕生成的编码器/解码器框架中被广泛使用，其中在每个时间步生成编码向量的加权平均以指导字幕解码过程。然而，解码器对于注意力向量和给定的注意力查询是否相关或相关程度如何知之甚少，这可能导致解码器给出误导性的结果。在本文中，我们提出了一个注意力机制上的注意力（AoA）模块，它扩展了传统的注意力机制以确定注意力结果和查询之间的相关性。AoA首先使用注意力结果和当前上下文生成一个信息向量和一个注意力门，然后通过对它们应用元素级乘法来添加另一个注意力，并最终获得注意力信息，即预期的有用知识。我们将AoA应用于我们的图像字幕生成模型的编码器和解码器，我们将其命名为AoA网络（AoANet）。实验表明，AoANet优于所有先前发布的方法，并在MS COCO Karpathy离线测试分割上实现了129.8的CIDEr-D分数，在官方在线测试服务器上实现了129.6的CIDEr-D（C40）分数的新最先进性能。代码可在https://github.com/husthuaan/AoANet获取。",
        "领域": "图像字幕生成/注意力机制/编码器-解码器框架",
        "问题": "解码器对注意力向量和查询之间的相关性缺乏了解，可能导致误导性结果",
        "动机": "提高图像字幕生成模型中注意力机制的有效性，确保解码器能够准确理解注意力向量和查询之间的关系",
        "方法": "提出注意力机制上的注意力（AoA）模块，通过生成信息向量和注意力门，并应用元素级乘法来确定注意力结果和查询之间的相关性",
        "关键词": [
            "注意力机制",
            "图像字幕生成",
            "编码器-解码器框架"
        ],
        "涉及的技术概念": "注意力机制（Attention mechanisms）是一种在深度学习中用于提高模型对输入数据特定部分关注度的技术。在图像字幕生成中，它帮助模型在生成描述时关注图像的关键部分。编码器-解码器框架（Encoder/Decoder frameworks）是一种常用的序列到序列学习模型结构，用于将输入序列（如图像）转换为输出序列（如文本描述）。CIDEr-D是一种评估图像字幕生成质量的指标，它通过比较生成字幕和参考字幕之间的相似度来评分。"
    },
    {
        "order": 68,
        "title": "What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Afifi_What_Else_Can_Fool_Deep_Learning_Addressing_Color_Constancy_Errors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Afifi_What_Else_Can_Fool_Deep_Learning_Addressing_Color_Constancy_Errors_ICCV_2019_paper.html",
        "abstract": "There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets.",
        "中文标题": "还有什么能欺骗深度学习？解决色彩恒常性错误对深度神经网络性能的影响",
        "摘要翻译": "目前有研究针对能够欺骗深度神经网络（DNNs）产生错误结果的局部图像操作。本文研究了一种能够产生类似负面影响的全局图像操作。具体来说，我们探讨了由错误应用的计算色彩恒常性（在摄影中称为白平衡，WB）引起的强烈色偏如何负面影响针对图像分割和分类的DNNs的性能。此外，我们讨论了现有的用于提高DNNs鲁棒性的图像增强方法并不适合模拟WB错误。为了解决这个问题，提出了一种新的增强方法，可以模拟准确的色彩恒常性退化。我们还探索了使用最新的WB校正算法对训练和测试图像进行预处理，以减少错误白平衡图像的影响。我们在不同的数据集上检查了增强和预处理策略，并在CIFAR-10、CIFAR-100和ADE20K数据集上展示了显著的改进。",
        "领域": "图像分割/图像分类/色彩恒常性",
        "问题": "解决色彩恒常性错误对深度神经网络性能的负面影响",
        "动机": "探讨错误应用的计算色彩恒常性如何负面影响深度神经网络的性能，并解决现有图像增强方法不适合模拟白平衡错误的问题",
        "方法": "提出了一种新的图像增强方法模拟色彩恒常性退化，并使用最新的白平衡校正算法对图像进行预处理",
        "关键词": [
            "色彩恒常性",
            "白平衡",
            "图像增强",
            "图像预处理"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）、图像分割、图像分类、色彩恒常性（白平衡，WB）、图像增强、图像预处理"
    },
    {
        "order": 69,
        "title": "DSIC: Deep Stereo Image Compression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_DSIC_Deep_Stereo_Image_Compression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_DSIC_Deep_Stereo_Image_Compression_ICCV_2019_paper.html",
        "abstract": "In this paper we tackle the problem of stereo image compression, and leverage the fact that the two images have overlapping fields of view to further compress the representations. Our approach leverages state-of-the-art single-image compression autoencoders and enhances the compression with novel parametric skip functions to feed fully differentiable, disparity-warped features at all levels to the encoder/decoder of the second image. Moreover, we model the probabilistic dependence between the image codes using a conditional entropy model. Our experiments show an impressive 30 - 50% reduction in the second image bitrate at low bitrates compared to deep single-image compression, and a 10 - 20% reduction at higher bitrates.",
        "中文标题": "DSIC: 深度立体图像压缩",
        "摘要翻译": "在本文中，我们解决了立体图像压缩的问题，并利用两幅图像具有重叠视野的事实来进一步压缩表示。我们的方法利用了最先进的单图像压缩自动编码器，并通过新颖的参数化跳跃函数增强压缩，以在所有级别向第二幅图像的编码器/解码器提供完全可微分的、视差扭曲的特征。此外，我们使用条件熵模型对图像代码之间的概率依赖性进行建模。我们的实验显示，在低比特率下，与深度单图像压缩相比，第二幅图像的比特率显著减少了30-50%，在高比特率下减少了10-20%。",
        "领域": "图像压缩/立体视觉/深度学习",
        "问题": "立体图像压缩",
        "动机": "利用两幅图像的重叠视野来进一步压缩表示，提高压缩效率",
        "方法": "利用最先进的单图像压缩自动编码器，通过参数化跳跃函数增强压缩，使用条件熵模型对图像代码之间的概率依赖性进行建模",
        "关键词": [
            "立体图像压缩",
            "自动编码器",
            "条件熵模型"
        ],
        "涉及的技术概念": "立体图像压缩指的是对两幅具有重叠视野的图像进行压缩的技术。自动编码器是一种用于数据压缩的神经网络，通过编码和解码过程减少数据维度。条件熵模型用于描述两个随机变量之间的条件概率分布，这里用于建模图像代码之间的概率依赖性。"
    },
    {
        "order": 70,
        "title": "Dynamic Graph Attention for Referring Expression Comprehension",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.html",
        "abstract": "Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwise locating the objects referred to in complex language descriptions.",
        "中文标题": "动态图注意力用于指代表达理解",
        "摘要翻译": "指代表达理解旨在定位图像中由自然语言指代表达描述的对象实例。这项任务是组合性的，本质上需要在图像中对象之间的关系之上进行视觉推理。同时，视觉推理过程由指代表达的语言结构引导。然而，现有的方法将对象孤立处理或仅探索对象之间的一阶关系，而没有与表达的潜在复杂性对齐。因此，它们很难适应复杂指代表达的定位。在本文中，我们从语言驱动的视觉推理的角度探讨了指代表达理解的问题，并提出了一种动态图注意力网络，通过建模图像中对象之间的关系和表达的语言结构来执行多步推理。特别是，我们为图像构建了一个图，节点和边分别对应于对象及其关系，提出了一个差分分析器来预测语言引导的视觉推理过程，并在图上执行逐步推理以更新每个节点的复合对象表示。实验结果表明，所提出的方法不仅能在三个常见基准数据集上显著超越所有现有的最先进算法，而且还能生成可解释的视觉证据，用于逐步定位复杂语言描述中提到的对象。",
        "领域": "视觉推理/自然语言处理/图神经网络",
        "问题": "指代表达理解中的复杂语言描述与图像对象之间的对齐问题",
        "动机": "现有方法在处理复杂指代表达时，未能充分考虑到对象之间的关系和表达的语言结构，导致定位效果不佳",
        "方法": "提出了一种动态图注意力网络，通过构建图像对象及其关系的图结构，并利用差分分析器预测语言引导的视觉推理过程，实现多步推理",
        "关键词": [
            "指代表达理解",
            "动态图注意力网络",
            "视觉推理",
            "语言结构",
            "多步推理"
        ],
        "涉及的技术概念": "动态图注意力网络是一种结合图神经网络和注意力机制的模型，用于处理图像中对象之间的关系和自然语言表达之间的复杂交互。差分分析器用于预测基于语言引导的视觉推理过程，而逐步推理则是指在图结构上逐步更新对象表示，以更准确地定位描述中的对象。"
    },
    {
        "order": 71,
        "title": "Variable Rate Deep Image Compression With a Conditional Autoencoder",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Variable_Rate_Deep_Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Variable_Rate_Deep_Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates.",
        "中文标题": "基于条件自编码器的可变速率深度图像压缩",
        "摘要翻译": "在本文中，我们提出了一种新颖的基于条件自编码器的可变速率学习图像压缩框架。以往基于学习的图像压缩方法大多需要为不同的压缩率训练单独的网络，以便能够生成不同质量的压缩图像。相比之下，我们仅训练和部署一个由条件自编码器实现的可变速率图像压缩网络。我们提供了两个速率控制参数，即拉格朗日乘数和量化箱大小，它们作为条件变量提供给网络。通过改变拉格朗日乘数来实现对目标的粗略速率适应，而通过调整用于量化编码表示的箱大小来进一步微调速率。我们的实验结果表明，与传统的可变速率图像压缩编解码器（如JPEG2000和BPG）相比，所提出的方案提供了更好的速率-失真权衡。我们的模型还展示了与部署多个针对不同速率训练的网络的现有最先进学习图像压缩模型相当甚至有时更好的性能。",
        "领域": "图像压缩/深度学习/自编码器",
        "问题": "如何实现一个单一网络适应不同压缩率的图像压缩",
        "动机": "解决传统方法需要为不同压缩率训练多个网络的问题，提高效率和灵活性",
        "方法": "使用条件自编码器实现单一可变速率图像压缩网络，通过调整拉格朗日乘数和量化箱大小来控制压缩率",
        "关键词": [
            "可变速率",
            "图像压缩",
            "条件自编码器",
            "拉格朗日乘数",
            "量化箱大小"
        ],
        "涉及的技术概念": "条件自编码器是一种能够根据输入条件调整其输出的自编码器，这里用于根据不同的压缩率调整图像压缩。拉格朗日乘数用于在优化问题中引入约束条件，这里用于控制压缩率。量化箱大小是指在量化过程中用于将连续值转换为离散值的间隔大小，这里用于微调压缩率。"
    },
    {
        "order": 72,
        "title": "Visual Semantic Reasoning for Image-Text Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Visual_Semantic_Reasoning_for_Image-Text_Matching_ICCV_2019_paper.html",
        "abstract": "Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because the current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose a simple and interpretable reasoning model to generate visual representation that captures key objects and semantic concepts of a scene. Specifically, we first build up connections between image regions and perform reasoning with Graph Convolutional Networks to generate features with semantic relationships. Then, we propose to use the gate and memory mechanism to perform global semantic reasoning on these relationship-enhanced features, select the discriminative information and gradually generate the representation for the whole scene. Experiments validate that our method achieves a new state-of-the-art for the image-text matching on MS-COCO and Flickr30K datasets. It outperforms the current best method by 6.8% relatively for image retrieval and 4.8% relatively for caption retrieval on MS-COCO (Recall@1 using 1K test set). On Flickr30K, our model improves image retrieval by 12.6% relatively and caption retrieval by 5.8% relatively (Recall@1).",
        "中文标题": "视觉语义推理用于图像-文本匹配",
        "摘要翻译": "图像-文本匹配一直是连接视觉和语言领域的热门研究课题。由于当前图像的表示通常缺乏其对应文本标题中的全局语义概念，因此这一任务仍然具有挑战性。为了解决这个问题，我们提出了一个简单且可解释的推理模型，以生成捕捉场景关键对象和语义概念的视觉表示。具体来说，我们首先建立图像区域之间的联系，并使用图卷积网络进行推理，以生成具有语义关系的特征。然后，我们提出使用门控和记忆机制对这些关系增强的特征进行全局语义推理，选择区分性信息，并逐步生成整个场景的表示。实验验证了我们的方法在MS-COCO和Flickr30K数据集上的图像-文本匹配任务中达到了新的最先进水平。在MS-COCO上，我们的方法在图像检索和标题检索上分别比当前最佳方法相对提高了6.8%和4.8%（使用1K测试集的Recall@1）。在Flickr30K上，我们的模型在图像检索和标题检索上分别相对提高了12.6%和5.8%（Recall@1）。",
        "领域": "图像-文本匹配/语义推理/图卷积网络",
        "问题": "当前图像表示缺乏全局语义概念，难以准确匹配图像和文本",
        "动机": "提高图像-文本匹配的准确性和效率，通过捕捉场景的关键对象和语义概念",
        "方法": "提出一个简单且可解释的推理模型，使用图卷积网络进行图像区域间的推理，并采用门控和记忆机制进行全局语义推理",
        "关键词": [
            "图像-文本匹配",
            "语义推理",
            "图卷积网络",
            "门控机制",
            "记忆机制"
        ],
        "涉及的技术概念": "图卷积网络（GCN）用于在图像区域之间建立联系并进行推理，门控和记忆机制用于对增强的特征进行全局语义推理，以选择区分性信息并生成整个场景的表示。"
    },
    {
        "order": 73,
        "title": "Beyond Cartesian Representations for Local Descriptors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ebel_Beyond_Cartesian_Representations_for_Local_Descriptors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ebel_Beyond_Cartesian_Representations_for_Local_Descriptors_ICCV_2019_paper.html",
        "abstract": "The dominant approach for learning local patch descriptors relies on small image regions whose scale must be properly estimated a priori by a keypoint detector. In other words, if two patches are not in correspondence, their descriptors will not match. A strategy often used to alleviate this problem is to \"pool\" the pixel-wise features over log-polar regions, rather than regularly spaced ones. By contrast, we propose to extract the \"support region\" directly with a log-polar sampling scheme. We show that this provides us with a better representation by simultaneously oversampling the immediate neighbourhood of the point and undersampling regions far away from it. We demonstrate that this representation is particularly amenable to learning descriptors with deep networks. Our models can match descriptors across a much wider range of scales than was possible before, and also leverage much larger support regions without suffering from occlusions. We report state-of-the-art results on three different datasets",
        "中文标题": "超越笛卡尔表示法的局部描述符",
        "摘要翻译": "学习局部补丁描述符的主导方法依赖于小图像区域，这些区域的尺度必须由关键点检测器事先正确估计。换句话说，如果两个补丁不对应，它们的描述符将不匹配。通常用于缓解此问题的策略是在对数极坐标区域而不是规则间隔的区域上“池化”像素级特征。相比之下，我们提出直接使用对数极坐标采样方案提取“支持区域”。我们展示了这通过同时过采样点的邻近区域和欠采样远离它的区域，为我们提供了更好的表示。我们证明了这种表示特别适合用深度网络学习描述符。我们的模型可以在比以前更广泛的尺度范围内匹配描述符，并且还可以利用更大的支持区域而不会受到遮挡的影响。我们在三个不同的数据集上报告了最先进的结果。",
        "领域": "局部描述符学习/对数极坐标采样/深度网络",
        "问题": "局部补丁描述符的尺度估计和匹配问题",
        "动机": "解决传统方法中由于关键点检测器对尺度估计的依赖导致的描述符匹配问题，以及通过改进采样策略提高描述符的表示能力。",
        "方法": "提出直接使用对数极坐标采样方案提取支持区域，通过同时过采样点的邻近区域和欠采样远离它的区域，提高描述符的表示能力，并利用深度网络学习描述符。",
        "关键词": [
            "局部描述符",
            "对数极坐标采样",
            "深度网络",
            "尺度不变性",
            "支持区域"
        ],
        "涉及的技术概念": "局部描述符学习涉及从图像中提取特征以描述局部区域，对数极坐标采样是一种非均匀采样方法，可以更好地捕捉局部特征，深度网络用于从这些特征中学习有效的描述符。"
    },
    {
        "order": 74,
        "title": "Real Image Denoising With Feature Attention",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Anwar_Real_Image_Denoising_With_Feature_Attention_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Anwar_Real_Image_Denoising_With_Feature_Attention_ICCV_2019_paper.html",
        "abstract": "Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, its performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of the denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet.",
        "中文标题": "使用特征注意力的真实图像去噪",
        "摘要翻译": "深度卷积神经网络在包含空间不变噪声（合成噪声）的图像上表现更好；然而，在真实噪声照片上的性能有限，并且需要多阶段网络建模。为了提高去噪算法的实用性，本文提出了一种新颖的单阶段盲真实图像去噪网络（RIDNet），采用模块化架构。我们使用残差上的残差结构来缓解低频信息的流动，并应用特征注意力来利用通道依赖性。此外，通过在三个合成和四个真实噪声数据集上对19种最先进算法的定量指标和视觉质量评估，证明了我们的RIDNet的优越性。",
        "领域": "图像去噪/深度学习/特征注意力",
        "问题": "提高在真实噪声照片上的去噪性能",
        "动机": "现有深度卷积神经网络在真实噪声照片上的去噪性能有限，需要多阶段网络建模，影响了去噪算法的实用性。",
        "方法": "提出了一种新颖的单阶段盲真实图像去噪网络（RIDNet），采用模块化架构，使用残差上的残差结构缓解低频信息的流动，并应用特征注意力来利用通道依赖性。",
        "关键词": [
            "图像去噪",
            "特征注意力",
            "残差结构",
            "通道依赖性"
        ],
        "涉及的技术概念": {
            "深度卷积神经网络": "一种用于图像处理的深度学习模型，能够自动提取图像特征。",
            "空间不变噪声": "指在图像空间中分布均匀的噪声，通常为合成噪声。",
            "真实噪声照片": "指在现实世界中拍摄的照片，包含复杂的噪声。",
            "单阶段盲真实图像去噪网络（RIDNet）": "一种新颖的去噪网络架构，旨在通过单阶段处理提高去噪性能。",
            "残差上的残差结构": "一种网络结构设计，用于缓解低频信息的流动，提高网络性能。",
            "特征注意力": "一种机制，用于利用通道依赖性，增强网络对重要特征的关注。"
        }
    },
    {
        "order": 75,
        "title": "Distilling Knowledge From a Deep Pose Regressor Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saputra_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network_ICCV_2019_paper.html",
        "abstract": "This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on \"dark knowledge\" for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time.",
        "中文标题": "从深度姿态回归网络中提取知识",
        "摘要翻译": "本文提出了一种新颖的方法，用于从深度姿态回归网络中提取知识，以实现高效的视觉里程计（VO）。标准的知识提取依赖于“暗知识”以成功转移知识。由于这种知识在姿态回归中不可用，且教师的预测并不总是准确的，我们提出仅在信任教师时强调知识转移。我们通过使用教师损失作为置信度评分来实现这一点，该评分对教师预测赋予不同的相对重要性。我们通过注意力模仿损失（AIL）将此置信度评分注入到主要训练任务中，并通过注意力提示训练（AHT）方法在学习教师的中间表示时使用。据我们所知，这是首次成功从深度姿态回归网络中提取知识的工作。我们在KITTI和Malaga数据集上的评估显示，我们可以在参数减少高达92.95%和计算时间加快2.12倍的情况下，保持学生预测接近教师预测。",
        "领域": "视觉里程计/知识蒸馏/姿态估计",
        "问题": "如何从深度姿态回归网络中高效提取知识，以改进视觉里程计的性能",
        "动机": "由于标准知识提取方法在姿态回归中不可用且教师预测不总是准确，需要一种新的方法来仅在信任教师时强调知识转移",
        "方法": "使用教师损失作为置信度评分，通过注意力模仿损失（AIL）和注意力提示训练（AHT）方法注入到主要训练任务中",
        "关键词": [
            "视觉里程计",
            "知识蒸馏",
            "姿态估计"
        ],
        "涉及的技术概念": "深度姿态回归网络、视觉里程计（VO）、知识蒸馏、暗知识、教师损失、注意力模仿损失（AIL）、注意力提示训练（AHT）"
    },
    {
        "order": 76,
        "title": "Phrase Localization Without Paired Training Examples",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Phrase_Localization_Without_Paired_Training_Examples_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Phrase_Localization_Without_Paired_Training_Examples_ICCV_2019_paper.html",
        "abstract": "Localizing phrases in images is an important part of image understanding and can be useful in many applications that require mappings between textual and visual information. Existing work attempts to learn these mappings from examples of phrase-image region correspondences (strong supervision) or from phrase-image pairs (weak supervision). We postulate that such paired annotations are unnecessary, and propose the first method for the phrase localization problem where neither training procedure nor paired, task-specific data is required. Our method is simple but effective: we use off-the-shelf approaches to detect objects, scenes and colours in images, and explore different approaches to measure semantic similarity between the categories of detected visual elements and words in phrases. Experiments on two well-known phrase localization datasets show that this approach surpasses all weakly supervised methods by a large margin and performs very competitively to strongly supervised methods, and can thus be considered a strong baseline to the task. The non-paired nature of our method makes it applicable to any domain and where no paired phrase localization annotation is available.",
        "中文标题": "无需配对训练示例的短语定位",
        "摘要翻译": "在图像中定位短语是图像理解的重要组成部分，对于需要文本和视觉信息之间映射的许多应用非常有用。现有工作试图从短语-图像区域对应示例（强监督）或短语-图像对（弱监督）中学习这些映射。我们假设这种配对注释是不必要的，并提出了第一个短语定位问题的方法，其中既不需要训练过程，也不需要配对的、特定任务的数据。我们的方法简单但有效：我们使用现成的方法来检测图像中的对象、场景和颜色，并探索不同的方法来测量检测到的视觉元素类别与短语中的单词之间的语义相似性。在两个著名的短语定位数据集上的实验表明，这种方法大大超过了所有弱监督方法，并且与强监督方法相比表现非常具有竞争力，因此可以被视为该任务的强基线。我们方法的非配对性质使其适用于任何领域，并且在没有配对短语定位注释的情况下也适用。",
        "领域": "图像理解/语义相似性/视觉信息映射",
        "问题": "如何在无需配对训练示例的情况下进行短语定位",
        "动机": "现有方法依赖于配对注释，这限制了方法的通用性和应用范围，因此探索无需配对注释的短语定位方法",
        "方法": "使用现成的方法检测图像中的对象、场景和颜色，并探索不同的方法来测量检测到的视觉元素类别与短语中的单词之间的语义相似性",
        "关键词": [
            "短语定位",
            "语义相似性",
            "视觉信息映射"
        ],
        "涉及的技术概念": "短语定位是指在图像中定位与特定短语相对应的区域。语义相似性是指不同类别或概念之间的相似程度，这里特指视觉元素类别与短语中的单词之间的相似性。视觉信息映射涉及将文本信息与视觉信息进行关联和映射。"
    },
    {
        "order": 77,
        "title": "Noise Flow: Noise Modeling With Conditional Normalizing Flows",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Abdelhamed_Noise_Flow_Noise_Modeling_With_Conditional_Normalizing_Flows_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Abdelhamed_Noise_Flow_Noise_Modeling_With_Conditional_Normalizing_Flows_ICCV_2019_paper.html",
        "abstract": "Modeling and synthesizing image noise is an important aspect in many computer vision applications. The long-standing additive white Gaussian and heteroscedastic (signal-dependent) noise models widely used in the literature provide only a coarse approximation of real sensor noise. This paper introduces Noise Flow, a powerful and accurate noise model based on recent normalizing flow architectures. Noise Flow combines well-established basic parametric noise models (e.g., signal-dependent noise) with the flexibility and expressiveness of normalizing flow networks. The result is a single, comprehensive, compact noise model containing fewer than 2500 parameters yet able to represent multiple cameras and gain factors. Noise Flow dramatically outperforms existing noise models, with 0.42 nats/pixel improvement over the camera-calibrated noise level functions, which translates to 52% improvement in the likelihood of sampled noise. Noise Flow represents the first serious attempt to go beyond simple parametric models to one that leverages the power of deep learning and data-driven noise distributions.",
        "中文标题": "噪声流：使用条件归一化流进行噪声建模",
        "摘要翻译": "在许多计算机视觉应用中，建模和合成图像噪声是一个重要方面。长期以来，文献中广泛使用的加性白高斯噪声和异方差（信号依赖）噪声模型仅能粗略地近似真实传感器噪声。本文介绍了噪声流，一种基于最新归一化流架构的强大且准确的噪声模型。噪声流将成熟的基本参数噪声模型（例如，信号依赖噪声）与归一化流网络的灵活性和表现力相结合。结果是一个单一、全面、紧凑的噪声模型，包含少于2500个参数，但能够代表多个相机和增益因素。噪声流显著优于现有的噪声模型，与相机校准的噪声水平函数相比，每像素提高了0.42纳特，这意味着采样噪声的可能性提高了52%。噪声流代表了首次认真尝试超越简单参数模型，利用深度学习和数据驱动噪声分布的力量。",
        "领域": "图像噪声建模/归一化流/深度学习应用",
        "问题": "现有噪声模型仅能粗略近似真实传感器噪声，需要更准确和灵活的噪声模型。",
        "动机": "提高图像噪声建模的准确性和灵活性，以更好地服务于计算机视觉应用。",
        "方法": "结合基本参数噪声模型和归一化流网络，构建一个单一、全面、紧凑的噪声模型。",
        "关键词": [
            "图像噪声建模",
            "归一化流",
            "深度学习"
        ],
        "涉及的技术概念": "归一化流是一种生成模型，能够通过一系列可逆变换将简单分布转换为复杂分布。在本文中，归一化流被用来构建一个灵活且准确的噪声模型，能够更好地模拟真实传感器噪声。"
    },
    {
        "order": 78,
        "title": "Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Instance-Level_Future_Motion_Estimation_in_a_Single_Image_Based_on_ICCV_2019_paper.html",
        "abstract": "A novel algorithm to estimate instance-level future motion in a single image is proposed in this paper. We first represent the future motion of an instance with its direction, speed, and action classes. Then, we develop a deep neural network that exploits different levels of semantic information to perform the future motion estimation. For effective future motion classification, we adopt ordinal regression. Especially, we develop the cyclic ordinal regression scheme using binary classifiers. Experiments demonstrate that the proposed algorithm provides reliable performance and thus can be used effectively for vision applications, including single and multi object tracking. Furthermore, we release the future motion (FM) dataset, collected from diverse sources and annotated manually, as a benchmark for single-image future motion estimation.",
        "中文标题": "基于序数回归的单幅图像实例级未来运动估计",
        "摘要翻译": "本文提出了一种新颖的算法，用于估计单幅图像中的实例级未来运动。我们首先用方向、速度和动作类别来表示实例的未来运动。然后，我们开发了一个深度神经网络，利用不同层次的语义信息来执行未来运动估计。为了有效的未来运动分类，我们采用了序数回归。特别是，我们开发了使用二元分类器的循环序数回归方案。实验证明，所提出的算法提供了可靠的性能，因此可以有效地用于视觉应用，包括单目标和多目标跟踪。此外，我们发布了未来运动（FM）数据集，该数据集从多种来源收集并手动注释，作为单幅图像未来运动估计的基准。",
        "领域": "运动估计/目标跟踪/序数回归",
        "问题": "单幅图像中的实例级未来运动估计",
        "动机": "为了有效地估计单幅图像中实例的未来运动，并应用于视觉应用如目标跟踪",
        "方法": "开发了一个深度神经网络，利用不同层次的语义信息进行未来运动估计，并采用序数回归特别是循环序数回归方案进行有效的未来运动分类",
        "关键词": [
            "未来运动估计",
            "序数回归",
            "深度神经网络",
            "目标跟踪"
        ],
        "涉及的技术概念": "序数回归是一种统计方法，用于处理有序分类问题。在本研究中，它被用来分类未来运动的方向、速度和动作类别。深度神经网络是一种模仿人脑结构和功能的计算模型，用于从大量数据中学习复杂的模式和关系。循环序数回归方案是一种改进的序数回归方法，通过使用二元分类器来提高分类的准确性。"
    },
    {
        "order": 79,
        "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_to_Assemble_Neural_Module_Tree_Networks_for_Visual_Grounding_ICCV_2019_paper.html",
        "abstract": "Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.",
        "中文标题": "学习组装神经模块树网络以实现视觉定位",
        "摘要翻译": "视觉定位，即定位图像中的自然语言，本质上需要复合视觉推理。然而，现有方法将语言的复合性质过度简化为一个整体的句子嵌入或主语-谓语-宾语三元组的粗略组合。在本文中，我们提出以一种直观、可解释且复合的方式定位自然语言，正如它应该的那样。特别是，我们开发了一种名为神经模块树网络（NMTree）的新型模块化网络，该网络沿着句子的依赖解析树规范视觉定位，其中每个节点是一个神经模块，根据其语言特征计算视觉注意力，并且定位分数在自底向上的方向上根据需要累积。NMTree将视觉定位与复合推理分离，使前者仅专注于原始且易于泛化的模式。为了减少解析错误的影响，我们使用Gumbel-Softmax近似及其直通梯度估计器端到端训练模块及其组装，考虑到模块组装的离散性质。总体而言，所提出的NMTree在多个基准测试中始终优于最先进的技术。定性结果显示了详细的、可解释的定位分数计算。",
        "领域": "视觉定位/自然语言处理/模块化网络",
        "问题": "现有方法在视觉定位任务中过度简化了语言的复合性质，导致定位不准确",
        "动机": "提出一种更直观、可解释且复合的视觉定位方法，以更准确地定位图像中的自然语言",
        "方法": "开发了一种名为神经模块树网络（NMTree）的新型模块化网络，该网络沿着句子的依赖解析树规范视觉定位，使用Gumbel-Softmax近似及其直通梯度估计器端到端训练模块及其组装",
        "关键词": [
            "视觉定位",
            "神经模块树网络",
            "Gumbel-Softmax近似"
        ],
        "涉及的技术概念": {
            "视觉定位": "定位图像中的自然语言",
            "神经模块树网络（NMTree）": "一种新型模块化网络，用于规范视觉定位",
            "Gumbel-Softmax近似": "一种用于处理离散变量的技术，用于训练模块及其组装",
            "依赖解析树": "用于表示句子结构的树形图，每个节点代表句子中的一个词或短语"
        }
    },
    {
        "order": 80,
        "title": "Bottleneck Potentials in Markov Random Fields",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Abbas_Bottleneck_Potentials_in_Markov_Random_Fields_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Abbas_Bottleneck_Potentials_in_Markov_Random_Fields_ICCV_2019_paper.html",
        "abstract": "We consider general discrete Markov Random Fields(MRFs) with additional bottleneck potentials which penalize the maximum (instead of the sum) over local potential value taken by the MRF-assignment. Bottleneck potentials or analogous constructions have been considered in (i) combinatorial optimization (e.g. bottleneck shortest path problem, the minimum bottleneck spanning tree problem, bottleneck function minimization in greedoids), (ii) inverse problems with L_ infinity -norm regularization and (iii) valued constraint satisfaction on the (min,max)-pre-semirings. Bottleneck potentials for general discrete MRFs are a natural generalization of the above direction of modeling work to Maximum-A-Posteriori (MAP) inference in MRFs. To this end we propose MRFs whose objective consists of two parts: terms that factorize according to (i) (min,+), i.e. potentials as in plain MRFs, and (ii) (min,max), i.e. bottleneck potentials. To solve the ensuing inference problem, we propose high-quality relaxations and efficient algorithms for solving them. We empirically show efficacy of our approach on large scale seismic horizon tracking problems.",
        "中文标题": "马尔可夫随机场中的瓶颈潜力",
        "摘要翻译": "我们考虑了具有额外瓶颈潜力的一般离散马尔可夫随机场（MRFs），这些瓶颈潜力惩罚了MRF分配所采取的局部潜力值的最大值（而不是总和）。瓶颈潜力或类似结构已在（i）组合优化（例如瓶颈最短路径问题、最小瓶颈生成树问题、贪婪结构中的瓶颈函数最小化）、（ii）具有L_无穷范数正则化的逆问题以及（iii）在（最小，最大）预半环上的值约束满足问题中被考虑。对于一般离散MRFs的瓶颈潜力是上述建模工作方向到MRFs中最大后验（MAP）推理的自然推广。为此，我们提出了目标由两部分组成的MRFs：根据（i）（最小，+）即普通MRFs中的潜力，和（ii）（最小，最大）即瓶颈潜力进行因子化的项。为了解决随之而来的推理问题，我们提出了高质量的松弛和有效的算法来解决它们。我们在大规模地震层位跟踪问题上实证展示了我们方法的有效性。",
        "领域": "组合优化/逆问题/值约束满足",
        "问题": "如何在马尔可夫随机场中有效地处理瓶颈潜力，以优化最大后验推理",
        "动机": "为了推广组合优化、逆问题和值约束满足问题中的瓶颈潜力概念到马尔可夫随机场的最大后验推理中",
        "方法": "提出了包含（最小，+）和（最小，最大）因子化项的马尔可夫随机场模型，并开发了高质量的松弛和高效算法来解决推理问题",
        "关键词": [
            "瓶颈潜力",
            "马尔可夫随机场",
            "最大后验推理",
            "组合优化",
            "逆问题",
            "值约束满足"
        ],
        "涉及的技术概念": "瓶颈潜力是指在优化问题中，对局部潜力值的最大值进行惩罚，而不是总和。马尔可夫随机场（MRFs）是一种统计模型，用于描述一组随机变量之间的相互作用。最大后验（MAP）推理是一种在给定观测数据的情况下，寻找最可能的未观测变量配置的方法。组合优化涉及在有限的、离散的选项中找到最优解。逆问题涉及从观测数据中推断出导致这些数据的未知原因或参数。值约束满足问题涉及在满足一组约束条件的情况下，找到变量的赋值。"
    },
    {
        "order": 81,
        "title": "Vision-Infused Deep Audio Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Vision-Infused_Deep_Audio_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Vision-Infused_Deep_Audio_Inpainting_ICCV_2019_paper.html",
        "abstract": "Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e., synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).",
        "中文标题": "视觉融合的深度音频修复",
        "摘要翻译": "多模态感知对于发展交互式智能至关重要。在这项工作中，我们考虑了一个新的任务，即视觉信息融合的音频修复，即合成与伴随视频相对应的缺失音频片段。我们确定了成功修复的两个关键方面：（1）在频谱图上操作而不是原始音频是可取的。深度语义图像修复的最新进展可以被利用来超越传统音频修复的限制。（2）为了合成视觉指示的音频，需要学习一个视觉-音频联合特征空间，并同步音频和视频。为了促进大规模研究，我们通过丰富MUSIC数据集收集了一个新的多模态乐器演奏数据集，称为MUSIC-Extra-Solo（MUSICES）。大量实验证明，我们的框架能够在有或没有视觉上下文的情况下修复现实和变化的音频片段。更重要的是，我们合成的音频片段与它们的视频对应物一致，显示了我们提出的视觉融合音频修复器（VIAI）的有效性。",
        "领域": "音频修复/多模态学习/深度学习",
        "问题": "如何利用视觉信息来修复缺失的音频片段",
        "动机": "开发能够理解和利用多模态信息（特别是视觉和音频）的交互式智能系统",
        "方法": "在频谱图上操作，利用深度语义图像修复技术，学习视觉-音频联合特征空间，并通过实验验证方法的有效性",
        "关键词": [
            "音频修复",
            "多模态学习",
            "深度学习",
            "视觉-音频同步",
            "频谱图"
        ],
        "涉及的技术概念": "深度语义图像修复技术用于音频修复，视觉-音频联合特征空间的学习，以及通过实验验证音频修复的连贯性和现实性。"
    },
    {
        "order": 82,
        "title": "A Fast and Accurate One-Stage Approach to Visual Grounding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_A_Fast_and_Accurate_One-Stage_Approach_to_Visual_Grounding_ICCV_2019_paper.html",
        "abstract": "We propose a simple, fast, and accurate one-stage approach to visual grounding, inspired by the following insight. The performances of existing propose-and-rank two-stage methods are capped by the quality of the region candidates they propose in the first stage --- if none of the candidates could cover the ground truth region, there is no hope in the second stage to rank the right region to the top. To avoid this caveat, we propose a one-stage model that enables end-to-end joint optimization. The main idea is as straightforward as fusing a text query's embedding into the YOLOv3 object detector, augmented by spatial features so as to account for spatial mentions in the query. Despite being simple, this one-stage approach shows great potential in terms of both accuracy and speed for both phrase localization and referring expression comprehension, according to our experiments. Given these results along with careful investigations into some popular region proposals, we advocate for visual grounding a paradigm shift from the conventional two-stage methods to the one-stage framework.",
        "中文标题": "一种快速准确的一阶段视觉定位方法",
        "摘要翻译": "我们提出了一种简单、快速且准确的一阶段视觉定位方法，灵感来源于以下洞察。现有的提议-排序两阶段方法的性能受到第一阶段提议的区域候选质量的限制——如果没有任何候选区域能够覆盖真实区域，那么在第二阶段就没有希望将正确的区域排序到顶部。为了避免这一缺陷，我们提出了一种一阶段模型，该模型支持端到端的联合优化。主要思想是将文本查询的嵌入融合到YOLOv3对象检测器中，并通过空间特征增强，以考虑查询中的空间提及。尽管简单，但根据我们的实验，这种一阶段方法在短语定位和引用表达理解方面显示出在准确性和速度上的巨大潜力。鉴于这些结果以及对一些流行区域提议的仔细研究，我们主张视觉定位从传统的两阶段方法向一阶段框架的范式转变。",
        "领域": "视觉定位/对象检测/自然语言处理",
        "问题": "提高视觉定位的准确性和速度",
        "动机": "现有两阶段方法在区域提议质量上的限制影响了最终定位的准确性",
        "方法": "提出一种一阶段模型，通过将文本查询的嵌入融合到YOLOv3对象检测器中，并增强空间特征，实现端到端的联合优化",
        "关键词": [
            "视觉定位",
            "对象检测",
            "自然语言处理"
        ],
        "涉及的技术概念": "YOLOv3对象检测器是一种快速的对象检测算法，能够实时检测图像中的对象。文本查询的嵌入是指将文本信息转换为数值形式，以便于与图像信息进行融合处理。空间特征增强是指在处理过程中考虑对象在图像中的空间位置信息，以提高定位的准确性。"
    },
    {
        "order": 83,
        "title": "Seeing Motion in the Dark",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Seeing_Motion_in_the_Dark_ICCV_2019_paper.html",
        "abstract": "Deep learning has recently been applied with impressive results to extreme low-light imaging. Despite the success of single-image processing, extreme low-light video processing is still intractable due to the difficulty of collecting raw video data with corresponding ground truth. Collecting long-exposure ground truth, as was done for single-image processing, is not feasible for dynamic scenes. In this paper, we present deep processing of very dark raw videos: on the order of one lux of illuminance. To support this line of work, we collect a new dataset of raw low-light videos, in which high-resolution raw data is captured at video rate. At this level of darkness, the signal-to-noise ratio is extremely low (negative if measured in dB) and the traditional image processing pipeline generally breaks down. A new method is presented to address this challenging problem. By carefully designing a learning-based pipeline and introducing a new loss function to encourage temporal stability, we train a siamese network on static raw videos, for which ground truth is available, such that the network generalizes to videos of dynamic scenes at test time. Experimental results demonstrate that the presented approach outperforms state-of-the-art models for burst processing, per-frame processing, and blind temporal consistency.",
        "中文标题": "在黑暗中看见运动",
        "摘要翻译": "深度学习最近被应用于极低光成像，并取得了令人印象深刻的结果。尽管单图像处理取得了成功，但由于难以收集带有相应地面实况的原始视频数据，极低光视频处理仍然难以解决。对于动态场景，收集长时间曝光的地面实况，如单图像处理所做的那样，是不可行的。在本文中，我们提出了对非常暗的原始视频的深度处理：大约一勒克斯的照度。为了支持这项工作，我们收集了一个新的原始低光视频数据集，其中高分辨率的原始数据以视频速率捕获。在这种黑暗程度下，信噪比极低（如果以分贝测量则为负），传统的图像处理流程通常失效。提出了一种新方法来解决这一具有挑战性的问题。通过精心设计基于学习的流程并引入新的损失函数以鼓励时间稳定性，我们在静态原始视频上训练了一个孪生网络，这些视频有可用的地面实况，使得网络在测试时能够推广到动态场景的视频。实验结果表明，所提出的方法在突发处理、每帧处理和盲时间一致性方面优于最先进的模型。",
        "领域": "极低光成像/视频处理/深度学习应用",
        "问题": "极低光条件下的视频处理",
        "动机": "由于难以收集带有相应地面实况的原始视频数据，极低光视频处理仍然难以解决",
        "方法": "设计基于学习的流程并引入新的损失函数以鼓励时间稳定性，训练孪生网络以推广到动态场景的视频",
        "关键词": [
            "极低光成像",
            "视频处理",
            "深度学习应用",
            "时间稳定性",
            "孪生网络"
        ],
        "涉及的技术概念": "深度学习、极低光成像、视频处理、时间稳定性、孪生网络、损失函数"
    },
    {
        "order": 84,
        "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_HAWQ_Hessian_AWare_Quantization_of_Neural_Networks_With_Mixed-Precision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_HAWQ_Hessian_AWare_Quantization_of_Neural_Networks_With_Mixed-Precision_ICCV_2019_paper.html",
        "abstract": "Model size and inference speed/power have become a major challenge in the deployment of neural networks for many applications. A promising approach to address these problems is quantization. However, uniformly quantizing a model to ultra-low precision leads to significant accuracy degradation. A novel solution for this is to use mixed-precision quantization, as some parts of the network may allow lower precision as compared to other layers. However, there is no systematic way to determine the precision of different layers. A brute force approach is not feasible for deep networks, as the search space for mixed-precision is exponential in the number of layers. Another challenge is a similar factorial complexity for determining block-wise fine-tuning order when quantizing the model to a target precision. Here, we introduce Hessian AWare Quantization (HAWQ), a novel second-order quantization method to address these problems. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer's Hessian spectrum. Moreover, HAWQ provides a deterministic fine-tuning order for quantizing layers. We show the results of our method on Cifar-10 using ResNet20, and on ImageNet using Inception-V3, ResNet50 and SqueezeNext models. Comparing HAWQ with state-of-the-art shows that we can achieve similar/better accuracy with 8x activation compression ratio on ResNet20, as compared to DNAS, and up to 1% higher accuracy with up to 14% smaller models on ResNet50 and Inception-V3, compared to recently proposed methods of RVQuant and HAQ. Furthermore, we show that we can quantize SqueezeNext to just 1MB model size while achieving above 68% top1 accuracy on ImageNet.",
        "中文标题": "HAWQ: 基于Hessian感知的混合精度神经网络量化",
        "摘要翻译": "模型大小和推理速度/功耗已成为许多应用中神经网络部署的主要挑战。量化是解决这些问题的一个有前景的方法。然而，将模型统一量化到超低精度会导致显著的精度下降。一个新颖的解决方案是使用混合精度量化，因为网络的某些部分可能允许比其他层更低的精度。然而，没有系统的方法来确定不同层的精度。对于深度网络来说，暴力方法不可行，因为混合精度的搜索空间随层数呈指数增长。另一个挑战是在将模型量化到目标精度时，确定块级微调顺序的类似阶乘复杂度。在这里，我们介绍了Hessian感知量化（HAWQ），一种新颖的二阶量化方法来解决这些问题。HAWQ允许基于层的Hessian谱自动选择每层的相对量化精度。此外，HAWQ为量化层提供了确定的微调顺序。我们在Cifar-10上使用ResNet20，在ImageNet上使用Inception-V3、ResNet50和SqueezeNext模型展示了我们方法的结果。将HAWQ与最先进的技术进行比较，我们可以在ResNet20上实现与DNAS相似/更好的精度，激活压缩比为8倍，在ResNet50和Inception-V3上，与最近提出的RVQuant和HAQ方法相比，精度提高最多1%，模型大小减少最多14%。此外，我们展示了可以将SqueezeNext量化到仅1MB的模型大小，同时在ImageNet上实现超过68%的top1精度。",
        "领域": "神经网络量化/模型压缩/深度学习优化",
        "问题": "神经网络模型大小和推理速度/功耗的挑战，以及统一量化到超低精度导致的精度下降问题",
        "动机": "为了解决神经网络部署中的模型大小和推理速度/功耗问题，同时减少量化带来的精度损失",
        "方法": "提出了一种基于Hessian感知的混合精度量化方法（HAWQ），自动选择每层的相对量化精度，并提供确定的微调顺序",
        "关键词": [
            "神经网络量化",
            "混合精度",
            "Hessian感知",
            "模型压缩",
            "深度学习优化"
        ],
        "涉及的技术概念": "Hessian感知量化（HAWQ）是一种二阶量化方法，它利用层的Hessian谱来自动选择每层的相对量化精度，并确定量化层的微调顺序。这种方法旨在解决神经网络模型大小和推理速度/功耗的挑战，同时减少量化带来的精度损失。"
    },
    {
        "order": 85,
        "title": "Zero-Shot Grounding of Objects From Natural Language Queries",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sadhu_Zero-Shot_Grounding_of_Objects_From_Natural_Language_Queries_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sadhu_Zero-Shot_Grounding_of_Objects_From_Natural_Language_Queries_ICCV_2019_paper.html",
        "abstract": "A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, \"unseen\" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual \"seen\" settings and performs significantly better than baseline in the zero-shot setting.",
        "中文标题": "零样本自然语言查询中的对象定位",
        "摘要翻译": "短语定位系统通过自然语言查询定位图像中的特定对象。在之前的工作中，短语被限制为包含训练中遇到的名词，我们将任务扩展到零样本定位（ZSG），可以包括新颖的、“未见过的”名词。当前的短语定位系统在两级框架中使用显式对象检测网络，其中一个阶段生成稀疏提案，另一个阶段评估它们。在ZSG设置中，生成适当的提案本身成为一个障碍，因为提案生成器是在检测和定位数据集中常见的实体上训练的。我们提出了一种新的单阶段模型，称为ZSGNet，它结合了检测器网络和定位系统，并预测分类分数和回归参数。ZSG系统的评估由于查询与学习类别之间关系的影响而带来了额外的微妙之处；我们定义了四种不同的条件，这些条件包含了不同难度的级别。我们还引入了新的数据集，这些数据集是从Flickr30k Entities和Visual Genome中抽取的，使得能够对四种条件进行评估。我们的实验表明，ZSGNet在Flickr30k和ReferIt上的“见过”的设置下实现了最先进的性能，并且在零样本设置下显著优于基线。",
        "领域": "自然语言处理/计算机视觉/深度学习",
        "问题": "如何在零样本设置下通过自然语言查询定位图像中的对象",
        "动机": "扩展短语定位任务以包括新颖的、未见过的名词，解决现有系统在零样本设置下生成适当提案的障碍",
        "方法": "提出了一种新的单阶段模型ZSGNet，结合检测器网络和定位系统，预测分类分数和回归参数",
        "关键词": [
            "零样本定位",
            "自然语言查询",
            "对象定位",
            "ZSGNet"
        ],
        "涉及的技术概念": {
            "零样本定位（ZSG）": "一种能够处理包含新颖、未见过的名词的自然语言查询，定位图像中特定对象的技术",
            "ZSGNet": "一种单阶段模型，结合了检测器网络和定位系统，用于预测分类分数和回归参数",
            "Flickr30k Entities": "一个包含图像及其对应自然语言描述的数据集，用于评估图像定位系统",
            "Visual Genome": "一个包含详细图像注释的数据集，用于视觉理解任务"
        }
    },
    {
        "order": 86,
        "title": "Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.html",
        "abstract": "Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many image processing applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks.",
        "中文标题": "评估深度图像超分辨率对抗对抗攻击的鲁棒性",
        "摘要翻译": "单图像超分辨率旨在生成低分辨率图像的高分辨率版本，这在许多图像处理应用中是一个基本组成部分。本文研究了基于深度学习的超分辨率方法对抗对抗攻击的鲁棒性，这些攻击可以显著恶化超分辨率图像，而在被攻击的低分辨率图像中几乎不引起明显的失真。研究表明，最先进的深度超分辨率方法对对抗攻击极为脆弱。通过理论和实验分析了不同方法的不同鲁棒性水平。我们还对攻击的可转移性、目标攻击和通用攻击的可行性进行了分析。",
        "领域": "图像超分辨率/对抗攻击/深度学习",
        "问题": "深度学习的超分辨率方法在对抗攻击下的鲁棒性问题",
        "动机": "研究动机是为了评估和改进深度超分辨率方法在面对对抗攻击时的鲁棒性，以确保图像处理应用的安全性和可靠性。",
        "方法": "通过理论和实验分析不同深度超分辨率方法在对抗攻击下的鲁棒性，包括攻击的可转移性、目标攻击和通用攻击的可行性。",
        "关键词": [
            "图像超分辨率",
            "对抗攻击",
            "鲁棒性分析"
        ],
        "涉及的技术概念": "单图像超分辨率、对抗攻击、深度学习、鲁棒性分析、攻击可转移性、目标攻击、通用攻击"
    },
    {
        "order": 87,
        "title": "SENSE: A Shared Encoder Network for Scene-Flow Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_SENSE_A_Shared_Encoder_Network_for_Scene-Flow_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_SENSE_A_Shared_Encoder_Network_for_Scene-Flow_Estimation_ICCV_2019_paper.html",
        "abstract": "We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.",
        "中文标题": "SENSE：用于场景流估计的共享编码器网络",
        "摘要翻译": "我们引入了一个名为SENSE的紧凑网络，用于整体场景流估计，该网络在四个紧密相关的任务之间共享编码器特征：光流估计、立体视觉中的视差估计、遮挡估计和语义分割。我们的关键见解是，共享特征使网络更加紧凑，诱导更好的特征表示，并能更好地利用这些任务之间的交互来处理部分标记的数据。通过共享编码器，我们可以在训练期间灵活地为不同任务添加解码器。这种模块化设计在推理时导致了一个紧凑且高效的模型。利用这些任务之间的交互，我们可以在监督损失之外引入蒸馏和自监督损失，这可以更好地处理部分标记的真实世界数据。SENSE在多个光流基准测试中实现了最先进的结果，并且运行速度与专门为光流设计的网络一样快。它在立体视觉和场景流方面也与最先进的技术相比具有优势，同时消耗的内存要少得多。",
        "领域": "光流估计/立体视觉/语义分割",
        "问题": "整体场景流估计",
        "动机": "共享特征使网络更加紧凑，诱导更好的特征表示，并能更好地利用任务之间的交互来处理部分标记的数据",
        "方法": "通过共享编码器，在训练期间灵活地为不同任务添加解码器，利用任务之间的交互引入蒸馏和自监督损失",
        "关键词": [
            "光流估计",
            "立体视觉",
            "语义分割",
            "遮挡估计",
            "共享编码器"
        ],
        "涉及的技术概念": "SENSE网络通过共享编码器特征在光流估计、立体视觉中的视差估计、遮挡估计和语义分割四个任务之间实现特征共享，采用模块化设计，在训练期间灵活添加解码器，并利用任务之间的交互引入蒸馏和自监督损失，以处理部分标记的真实世界数据。"
    },
    {
        "order": 88,
        "title": "Towards Unconstrained End-to-End Text Spotting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qin_Towards_Unconstrained_End-to-End_Text_Spotting_ICCV_2019_paper.html",
        "abstract": "We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectification. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to significant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the Total-Text (curved) benchmark by more than 16%.",
        "中文标题": "迈向无约束的端到端文本识别",
        "摘要翻译": "我们提出了一种端到端可训练的网络，该网络能够同时检测和识别任意形状的文本，在解决不规则形状场景文本阅读这一开放问题上取得了实质性进展。我们将任意形状文本检测表述为一个实例分割问题；然后使用注意力模型解码每个不规则形状文本区域的文本内容，而无需校正。为了从图像尺度特征中提取有用的不规则形状文本实例特征，我们提出了一个简单但有效的RoI掩码步骤。此外，我们展示了可以利用现有多步骤OCR引擎的预测作为部分标记的训练数据，这显著提高了我们模型的检测和识别准确率。我们的方法在ICDAR15（直线）基准测试的端到端识别任务上超越了最先进水平4.6%，在Total-Text（曲线）基准测试上超过了16%以上。",
        "领域": "文本识别/场景文本理解/实例分割",
        "问题": "解决不规则形状场景文本的检测和识别问题",
        "动机": "提高不规则形状文本的检测和识别准确率，推动场景文本理解技术的发展",
        "方法": "提出了一种端到端可训练的网络，结合实例分割和注意力模型，以及RoI掩码步骤，利用现有OCR引擎的预测作为训练数据",
        "关键词": [
            "文本识别",
            "场景文本",
            "实例分割",
            "注意力模型",
            "RoI掩码"
        ],
        "涉及的技术概念": "端到端可训练网络、实例分割、注意力模型、RoI掩码、OCR引擎、ICDAR15基准测试、Total-Text基准测试"
    },
    {
        "order": 89,
        "title": "Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Overcoming_Catastrophic_Forgetting_With_Unlabeled_Data_in_the_Wild_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Overcoming_Catastrophic_Forgetting_With_Unlabeled_Data_in_the_Wild_ICCV_2019_paper.html",
        "abstract": "Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.",
        "中文标题": "利用野外未标记数据克服灾难性遗忘",
        "摘要翻译": "众所周知，深度神经网络的终身学习会遭受灾难性遗忘：在学习新任务时，对先前任务的性能会急剧下降。为了缓解这种影响，我们提出利用野外容易获得的大量未标记数据流。特别是，我们设计了一种新颖的类增量学习方案，包括（a）一种新的蒸馏损失，称为全局蒸馏，（b）一种避免对最新任务过拟合的学习策略，以及（c）一种基于置信度的采样方法，以有效利用未标记的外部数据。我们在包括CIFAR和ImageNet在内的各种数据集上的实验结果证明了所提出方法相对于先前方法的优越性，特别是在可以访问未标记数据流时：我们的方法显示出比最先进方法高达15.8%的准确率提升和46.5%的遗忘减少。代码可在https://github.com/kibok90/iccv2019-inc获取。",
        "领域": "终身学习/类增量学习/灾难性遗忘",
        "问题": "深度神经网络在学习新任务时对先前任务的性能急剧下降的问题",
        "动机": "为了缓解深度神经网络在终身学习过程中遇到的灾难性遗忘问题",
        "方法": "设计了一种新颖的类增量学习方案，包括全局蒸馏损失、避免过拟合的学习策略和基于置信度的采样方法",
        "关键词": [
            "终身学习",
            "类增量学习",
            "灾难性遗忘",
            "全局蒸馏",
            "置信度采样"
        ],
        "涉及的技术概念": "灾难性遗忘指的是神经网络在学习新任务时忘记旧任务的现象。全局蒸馏是一种新的蒸馏损失，旨在通过利用未标记数据来减少遗忘。基于置信度的采样方法用于有效利用未标记的外部数据，以提高学习效率和效果。"
    },
    {
        "order": 90,
        "title": "Adversarial Feedback Loop",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shama_Adversarial_Feedback_Loop_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shama_Adversarial_Feedback_Loop_ICCV_2019_paper.html",
        "abstract": "Thanks to their remarkable generative capabilities, GANs have gained great popularity, and are used abundantly in state-of-the-art methods and applications. In a GAN based model, a discriminator is trained to learn the real data distribution. To date, it has been used only for training purposes, where it's utilized to train the generator to provide real-looking outputs. In this paper we propose a novel method that makes an explicit use of the discriminator in test-time, in a feedback manner in order to improve the generator results. To the best of our knowledge it is the first time a discriminator is involved in test-time. We claim that the discriminator holds significant information on the real data distribution, that could be useful for test-time as well, a potential that has not been explored before. The approach we propose does not alter the conventional training stage. At test-time, however, it transfers the output from the generator into the discriminator, and uses feedback modules (convolutional blocks) to translate the features of the discriminator layers into corrections to the features of the generator layers, which are used eventually to get a better generator result. Our method can contribute to both conditional and unconditional GANs. As demonstrated by our experiments, it can improve the results of state-of-the-art networks for super-resolution, and image generation.",
        "中文标题": "对抗性反馈循环",
        "摘要翻译": "得益于其卓越的生成能力，GANs（生成对抗网络）已经获得了极大的流行，并在最先进的方法和应用中被广泛使用。在基于GAN的模型中，判别器被训练以学习真实数据的分布。迄今为止，它仅被用于训练目的，即用于训练生成器以提供看起来真实的输出。在本文中，我们提出了一种新颖的方法，该方法在测试时明确使用判别器，以反馈的方式改进生成器的结果。据我们所知，这是首次在测试时涉及判别器。我们声称，判别器持有关于真实数据分布的重要信息，这些信息在测试时也可能有用，这是一个以前未被探索的潜力。我们提出的方法不改变传统的训练阶段。然而，在测试时，它将生成器的输出转移到判别器，并使用反馈模块（卷积块）将判别器层的特征转换为生成器层特征的修正，最终用于获得更好的生成器结果。我们的方法可以贡献于条件和非条件的GANs。正如我们的实验所展示的，它可以改进超分辨率和图像生成的最先进网络的结果。",
        "领域": "生成对抗网络/图像生成/超分辨率",
        "问题": "如何利用判别器在测试时改进生成器的结果",
        "动机": "探索判别器在测试时的潜力，利用其关于真实数据分布的信息改进生成器的输出",
        "方法": "在测试时，将生成器的输出转移到判别器，使用反馈模块（卷积块）将判别器层的特征转换为生成器层特征的修正",
        "关键词": [
            "生成对抗网络",
            "图像生成",
            "超分辨率"
        ],
        "涉及的技术概念": "GANs（生成对抗网络）是一种由生成器和判别器组成的模型，其中生成器尝试生成尽可能接近真实数据的输出，而判别器则尝试区分生成器的输出和真实数据。本文提出了一种在测试时使用判别器的方法，通过反馈循环改进生成器的输出，这是对传统GANs使用方式的一种创新。"
    },
    {
        "order": 91,
        "title": "What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Baek_What_Is_Wrong_With_Scene_Text_Recognition_Model_Comparisons_Dataset_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Baek_What_Is_Wrong_With_Scene_Text_Recognition_Model_Comparisons_Dataset_ICCV_2019_paper.html",
        "abstract": "Many new proposals for scene text recognition (STR) models have been introduced in recent years. While each claim to have pushed the boundary of the technology, a holistic and fair comparison has been largely missing in the field due to the inconsistent choices of training and evaluation datasets. This paper addresses this difficulty with three major contributions. First, we examine the inconsistencies of training and evaluation datasets, and the performance gap results from inconsistencies. Second, we introduce a unified four-stage STR framework that most existing STR models fit into. Using this framework allows for the extensive evaluation of previously proposed STR modules and the discovery of previously unexplored module combinations. Third, we analyze the module-wise contributions to performance in terms of accuracy, speed, and memory demand, under one consistent set of training and evaluation datasets. Such analyses clean up the hindrance on the current comparisons to understand the performance gain of the existing modules. Our code is publicly available.",
        "中文标题": "场景文本识别模型比较出了什么问题？数据集和模型分析",
        "摘要翻译": "近年来，许多新的场景文本识别（STR）模型提案被提出。虽然每个提案都声称推动了技术的边界，但由于训练和评估数据集选择的不一致，该领域在很大程度上缺乏全面和公平的比较。本文通过三个主要贡献解决了这一难题。首先，我们检查了训练和评估数据集的不一致性，以及这些不一致性导致的性能差距。其次，我们引入了一个统一的四阶段STR框架，大多数现有的STR模型都可以适应这个框架。使用这个框架可以对之前提出的STR模块进行广泛评估，并发现之前未探索的模块组合。第三，我们在一组一致的训练和评估数据集下，分析了模块对性能的贡献，包括准确性、速度和内存需求。这样的分析清除了当前比较中的障碍，以理解现有模块的性能增益。我们的代码是公开可用的。",
        "领域": "场景文本识别/模型评估/数据集分析",
        "问题": "场景文本识别模型在训练和评估数据集选择上的不一致性导致的性能比较困难",
        "动机": "解决由于训练和评估数据集选择不一致导致的场景文本识别模型性能比较困难的问题",
        "方法": "通过检查数据集不一致性、引入统一的四阶段STR框架进行模块评估和组合发现、分析模块对性能的贡献",
        "关键词": [
            "场景文本识别",
            "模型比较",
            "数据集分析",
            "性能评估"
        ],
        "涉及的技术概念": "场景文本识别（STR）模型、训练和评估数据集、四阶段STR框架、模块组合、性能分析（准确性、速度、内存需求）"
    },
    {
        "order": 92,
        "title": "Symmetric Cross Entropy for Robust Learning With Noisy Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Symmetric_Cross_Entropy_for_Robust_Learning_With_Noisy_Labels_ICCV_2019_paper.html",
        "abstract": "Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overfitting to noisy labels on some classes (\"easy\" classes), but more surprisingly, it also suffers from significant under learning on some other classes (\"hard\" classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overfitting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overfitting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.",
        "中文标题": "对称交叉熵用于噪声标签下的鲁棒学习",
        "摘要翻译": "在存在噪声标签的情况下训练准确的深度神经网络（DNNs）是一项重要且具有挑战性的任务。尽管已经提出了许多用于噪声标签学习的方法，但仍有许多未解决的问题。在本文中，我们展示了使用交叉熵（CE）进行DNN学习在某些类别（“容易”类别）上对噪声标签的过拟合，但更令人惊讶的是，它在其他一些类别（“困难”类别）上也存在显著的学习不足。直观地说，CE需要一个额外的项来促进困难类别的学习，更重要的是，这个项应该是噪声容忍的，以避免对噪声标签的过拟合。受对称KL散度的启发，我们提出了对称交叉熵学习（SL）方法，通过噪声鲁棒的反向交叉熵（RCE）对称地增强CE。我们提出的SL方法同时解决了在存在噪声标签的情况下CE的学习不足和过拟合问题。我们提供了SL的理论分析，并且在一系列基准和真实世界的数据集上实证表明，SL优于最先进的方法。我们还展示了SL可以轻松地融入到现有方法中，以进一步提高它们的性能。",
        "领域": "噪声标签学习/深度学习优化/鲁棒学习",
        "问题": "在噪声标签存在的情况下，深度神经网络的学习不足和过拟合问题",
        "动机": "解决深度神经网络在噪声标签环境下学习不足和过拟合的问题，提高模型的鲁棒性和准确性",
        "方法": "提出对称交叉熵学习（SL）方法，通过引入噪声鲁棒的反向交叉熵（RCE）对称地增强传统的交叉熵（CE），以同时解决学习不足和过拟合问题",
        "关键词": [
            "噪声标签学习",
            "对称交叉熵",
            "鲁棒学习"
        ],
        "涉及的技术概念": {
            "交叉熵（CE）": "一种常用的损失函数，用于衡量模型预测概率分布与真实概率分布之间的差异",
            "对称KL散度": "一种衡量两个概率分布差异的对称性度量，用于启发对称交叉熵的提出",
            "反向交叉熵（RCE）": "一种噪声鲁棒的损失函数，用于对称地增强交叉熵，避免对噪声标签的过拟合"
        }
    },
    {
        "order": 93,
        "title": "Dynamic-Net: Tuning the Objective Without Re-Training for Synthesis Tasks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shoshan_Dynamic-Net_Tuning_the_Objective_Without_Re-Training_for_Synthesis_Tasks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shoshan_Dynamic-Net_Tuning_the_Objective_Without_Re-Training_for_Synthesis_Tasks_ICCV_2019_paper.html",
        "abstract": "One of the key ingredients for successful optimization of modern CNNs is identifying a suitable objective. To date, the objective is fixed a-priori at training time, and any variation to it requires re-training a new network. In this paper we present a first attempt at alleviating the need for re-training. Rather than fixing the network at training time, we train a \"Dynamic-Net\" that can be modified at inference time. Our approach considers an \"objective-space\" as the space of all linear combinations of two objectives, and the Dynamic-Net is emulating the traversing of this objective-space at test-time, without any further training. We show that this upgrades pre-trained networks by providing an out-of-learning extension, while maintaining the performance quality. The solution we propose is fast and allows a user to interactively modify the network, in real-time, in order to obtain the result he/she desires. We show the benefits of such an approach via several different applications.",
        "中文标题": "动态网络：无需重新训练即可调整合成任务的目标",
        "摘要翻译": "成功优化现代CNN的关键因素之一是确定合适的目标。迄今为止，目标在训练时是预先固定的，任何对目标的修改都需要重新训练一个新的网络。在本文中，我们首次尝试减轻重新训练的需求。我们不是固定训练时的网络，而是训练一个可以在推理时修改的“动态网络”。我们的方法将“目标空间”视为两个目标的所有线性组合的空间，动态网络在测试时模拟遍历这个目标空间，而无需任何进一步的训练。我们展示了这通过提供一个学习之外的扩展来升级预训练网络，同时保持性能质量。我们提出的解决方案速度快，允许用户实时交互式地修改网络，以获得他/她想要的结果。我们通过几个不同的应用展示了这种方法的好处。",
        "领域": "神经网络优化/目标函数调整/实时交互",
        "问题": "如何在不需要重新训练的情况下调整神经网络的目标函数",
        "动机": "减轻因目标函数调整而需要重新训练神经网络的需求，提高网络调整的灵活性和效率",
        "方法": "训练一个可以在推理时修改的“动态网络”，通过模拟遍历目标空间来实现目标函数的调整，而无需进一步的训练",
        "关键词": [
            "神经网络优化",
            "目标函数调整",
            "实时交互"
        ],
        "涉及的技术概念": "动态网络（Dynamic-Net）：一种可以在推理时修改的网络，允许用户实时调整目标函数。目标空间（objective-space）：两个目标的所有线性组合的空间，动态网络在测试时模拟遍历这个空间。"
    },
    {
        "order": 94,
        "title": "Sparse and Imperceivable Adversarial Attacks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.html",
        "abstract": "Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing l_0-distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axis-aligned edges makes our adversarial examples almost non-perceivable. Moreover, we adapt the Projected Gradient Descent attack to the l_0-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classifiers against sparse and imperceivable adversarial manipulations.",
        "中文标题": "稀疏且难以察觉的对抗攻击",
        "摘要翻译": "神经网络已被证明容易受到各种对抗攻击的影响。从安全角度来看，高度稀疏的对抗攻击尤其危险。另一方面，稀疏攻击的像素级扰动通常较大，因此可能被检测到。我们提出了一种新的黑盒技术来制作对抗样本，旨在最小化与原始图像的l_0距离。大量实验表明，我们的攻击方法优于或与现有技术相竞争。此外，我们可以在组件扰动上集成额外的限制。允许像素仅在高度变化的区域变化并避免沿轴对齐边缘的变化，使我们的对抗样本几乎不可察觉。此外，我们将投影梯度下降攻击适应于l_0范数，集成了组件约束。这使我们能够进行对抗训练，以增强分类器对稀疏且难以察觉的对抗操作的鲁棒性。",
        "领域": "对抗样本生成/模型鲁棒性/图像安全",
        "问题": "如何生成既稀疏又难以被察觉的对抗样本",
        "动机": "提高对抗攻击的隐蔽性，同时保持攻击的有效性，以增强模型的安全性测试",
        "方法": "提出了一种新的黑盒技术，通过最小化与原始图像的l_0距离来制作对抗样本，并集成组件扰动限制，使对抗样本几乎不可察觉。同时，将投影梯度下降攻击适应于l_0范数，进行对抗训练以增强分类器的鲁棒性。",
        "关键词": [
            "对抗样本",
            "l_0距离",
            "投影梯度下降",
            "模型鲁棒性",
            "图像安全"
        ],
        "涉及的技术概念": "l_0距离指的是向量中非零元素的数量，用于衡量向量的稀疏性。投影梯度下降是一种优化算法，用于在约束条件下寻找函数的最小值。对抗训练是一种通过引入对抗样本来增强模型鲁棒性的训练方法。"
    },
    {
        "order": 95,
        "title": "Few-Shot Learning With Embedded Class Models and Shot-Free Meta Training",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.html",
        "abstract": "We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets.",
        "中文标题": "嵌入式类别模型与无镜头元训练的少样本学习",
        "摘要翻译": "我们提出了一种适用于任何数量镜头（无镜头）的少样本学习的嵌入学习方法。与将类别原型固定为样本嵌入的欧几里得平均值不同，我们允许它们存在于更高维度的空间（嵌入式类别模型）中，并与模型参数一起学习类别原型。类别表示函数被隐式定义，这使我们能够处理每个类别可变数量的镜头，同时保持简单的恒定大小架构。类别嵌入包括度量学习，这有助于在不拥挤类别表示空间的情况下添加新类别。尽管我们的方法是通用的，并未针对基准进行调整，但在标准的少样本基准数据集上实现了最先进的性能。",
        "领域": "少样本学习/度量学习/类别嵌入",
        "问题": "解决少样本学习中的类别表示和原型学习问题",
        "动机": "为了在少样本学习中更有效地表示类别和添加新类别，同时保持模型架构的简单性",
        "方法": "提出了一种嵌入式类别模型，允许类别原型存在于更高维度的空间，并与模型参数一起学习，同时采用度量学习来优化类别嵌入",
        "关键词": [
            "少样本学习",
            "度量学习",
            "类别嵌入"
        ],
        "涉及的技术概念": "嵌入式类别模型允许类别原型存在于更高维度的空间，与模型参数一起学习。类别表示函数被隐式定义，使得处理每个类别可变数量的镜头成为可能，同时保持模型架构的简单性。类别嵌入包括度量学习，有助于在不拥挤类别表示空间的情况下添加新类别。"
    },
    {
        "order": 96,
        "title": "AutoGAN: Neural Architecture Search for Generative Adversarial Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gong_AutoGAN_Neural_Architecture_Search_for_Generative_Adversarial_Networks_ICCV_2019_paper.html",
        "abstract": "Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN",
        "中文标题": "AutoGAN: 生成对抗网络的神经架构搜索",
        "摘要翻译": "神经架构搜索（NAS）在图像分类和（最近）分割任务中取得了普遍的成功。在本文中，我们首次将NAS算法引入生成对抗网络（GANs），称为AutoGAN。NAS与GANs的结合面临其独特的挑战。我们定义了生成器架构变体的搜索空间，并使用RNN控制器来指导搜索，通过参数共享和动态重置来加速过程。采用Inception分数作为奖励，并引入多级搜索策略以渐进方式执行NAS。实验验证了AutoGAN在无条件图像生成任务上的有效性。具体来说，我们发现的架构与当前最先进的手工设计GANs相比，实现了极具竞争力的性能，例如在CIFAR-10上设置了新的最先进FID分数12.42，在STL-10上为31.01。我们还讨论了AutoGAN的当前局限性和未来潜力。代码可在https://github.com/TAMU-VITA/AutoGAN获取。",
        "领域": "生成对抗网络/神经架构搜索/图像生成",
        "问题": "如何有效地将神经架构搜索应用于生成对抗网络以优化图像生成任务",
        "动机": "探索神经架构搜索在生成对抗网络中的应用，以发现更有效的网络架构，提高图像生成的质量和效率",
        "方法": "定义生成器架构变体的搜索空间，使用RNN控制器指导搜索，采用参数共享和动态重置加速搜索过程，使用Inception分数作为奖励，并引入多级搜索策略进行渐进式NAS",
        "关键词": [
            "生成对抗网络",
            "神经架构搜索",
            "图像生成"
        ],
        "涉及的技术概念": "神经架构搜索（NAS）是一种自动化设计神经网络架构的技术，旨在发现最优的网络结构。生成对抗网络（GANs）由生成器和判别器组成，通过对抗过程生成数据。Inception分数是评估生成图像质量的一种指标。FID（Fréchet Inception Distance）分数是另一种评估生成图像质量的指标，分数越低表示生成图像的质量越高。"
    },
    {
        "order": 97,
        "title": "Enhancing Adversarial Example Transferability With an Intermediate Level Attack",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Enhancing_Adversarial_Example_Transferability_With_an_Intermediate_Level_Attack_ICCV_2019_paper.html",
        "abstract": "Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overfit to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to fine-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-specified layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.",
        "中文标题": "通过中间层攻击增强对抗样本的可转移性",
        "摘要翻译": "神经网络容易受到对抗样本的攻击，这些恶意输入旨在欺骗训练好的模型。对抗样本通常表现出黑盒转移性，即一个模型的对抗样本可以欺骗另一个模型。然而，对抗样本通常过度拟合以利用源模型的特定架构和特征表示，导致对其他目标模型的黑盒转移攻击效果不佳。我们引入了中间层攻击（ILA），该攻击试图通过增加对源模型预指定层的扰动来微调现有的对抗样本，以提高其黑盒转移性，从而改进现有技术。我们展示了我们可以在不了解目标模型的情况下选择源模型的层进行扰动，同时实现高转移性。此外，我们还提供了一些关于我们方法和使用中间特征图优化对抗样本效果的解释性见解。",
        "领域": "对抗学习/模型安全/神经网络",
        "问题": "提高对抗样本在不同模型间的转移性",
        "动机": "对抗样本通常过度拟合源模型，导致对其他模型的黑盒转移攻击效果不佳，需要一种方法来提高对抗样本的转移性。",
        "方法": "引入中间层攻击（ILA），通过增加对源模型预指定层的扰动来微调对抗样本，以提高其黑盒转移性。",
        "关键词": [
            "对抗样本",
            "黑盒转移",
            "中间层攻击"
        ],
        "涉及的技术概念": "对抗样本是指那些经过特殊设计以欺骗机器学习模型的输入。黑盒转移性指的是对抗样本能够从一个模型转移到另一个模型并保持其欺骗性的能力。中间层攻击（ILA）是一种通过增加对源模型特定层的扰动来优化对抗样本的方法，以提高其在未知目标模型上的转移性。"
    },
    {
        "order": 98,
        "title": "Dual Directed Capsule Network for Very Low Resolution Image Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_Dual_Directed_Capsule_Network_for_Very_Low_Resolution_Image_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Singh_Dual_Directed_Capsule_Network_for_Very_Low_Resolution_Image_Recognition_ICCV_2019_paper.html",
        "abstract": "Very low resolution (VLR) image recognition corresponds to classifying images with resolution 16x16 or less. Though it has widespread applicability when objects are captured at a very large stand-off distance (e.g. surveillance scenario) or from wide angle mobile cameras, it has received limited attention. This research presents a novel Dual Directed Capsule Network model, termed as DirectCapsNet, for addressing VLR digit and face recognition. The proposed architecture utilizes a combination of capsule and convolutional layers for learning an effective VLR recognition model. The architecture also incorporates two novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed targeted reconstruction loss, in order to overcome the challenges of limited information content in VLR images. The proposed losses use high resolution images as auxiliary data during training to \"direct\" discriminative feature learning. Multiple experiments for VLR digit classification and VLR face recognition are performed along with comparisons with state-of-the-art algorithms. The proposed DirectCapsNet consistently showcases state-of-the-art results; for example, on the UCCS face database, it shows over 95% face recognition accuracy when 16x16 images are matched with 80x80 images.",
        "中文标题": "双导向胶囊网络用于极低分辨率图像识别",
        "摘要翻译": "极低分辨率（VLR）图像识别对应于对分辨率16x16或更小的图像进行分类。尽管当物体在非常远的距离（例如监控场景）或从广角移动相机捕获时，它具有广泛的适用性，但它受到的关注有限。本研究提出了一种新颖的双导向胶囊网络模型，称为DirectCapsNet，用于解决VLR数字和人脸识别。所提出的架构利用胶囊层和卷积层的组合来学习有效的VLR识别模型。该架构还包含两种新颖的损失函数：（i）提出的HR-anchor损失和（ii）提出的目标重建损失，以克服VLR图像中信息内容有限的挑战。所提出的损失在训练期间使用高分辨率图像作为辅助数据来“指导”判别特征学习。进行了VLR数字分类和VLR人脸识别的多项实验，并与最先进的算法进行了比较。所提出的DirectCapsNet始终展示出最先进的结果；例如，在UCCS人脸数据库上，当16x16图像与80x80图像匹配时，它显示出超过95%的人脸识别准确率。",
        "领域": "图像识别/人脸识别/数字识别",
        "问题": "极低分辨率图像识别",
        "动机": "解决在极低分辨率条件下进行图像识别的挑战，特别是在监控场景和广角移动相机捕获的图像中。",
        "方法": "提出了一种双导向胶囊网络模型（DirectCapsNet），结合胶囊层和卷积层，以及两种新颖的损失函数（HR-anchor损失和目标重建损失），利用高分辨率图像作为辅助数据来指导判别特征学习。",
        "关键词": [
            "极低分辨率图像识别",
            "胶囊网络",
            "人脸识别",
            "数字识别"
        ],
        "涉及的技术概念": {
            "极低分辨率（VLR）图像识别": "对分辨率16x16或更小的图像进行分类。",
            "双导向胶囊网络模型（DirectCapsNet）": "一种结合胶囊层和卷积层的网络架构，用于学习有效的VLR识别模型。",
            "HR-anchor损失": "一种新颖的损失函数，用于在训练期间使用高分辨率图像作为辅助数据来指导判别特征学习。",
            "目标重建损失": "另一种新颖的损失函数，旨在克服VLR图像中信息内容有限的挑战。"
        }
    },
    {
        "order": 99,
        "title": "Co-Evolutionary Compression for Unpaired Image Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shu_Co-Evolutionary_Compression_for_Unpaired_Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shu_Co-Evolutionary_Compression_for_Unpaired_Image_Translation_ICCV_2019_paper.html",
        "abstract": "Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators.",
        "中文标题": "协同进化压缩用于无配对图像翻译",
        "摘要翻译": "生成对抗网络（GANs）已成功用于许多计算机视觉任务，尤其是图像到图像的翻译。然而，这些网络中的生成器具有复杂的架构，参数数量多，计算复杂度高。现有的方法主要是为分类任务中的深度神经网络的压缩和加速而设计的，由于目标和训练过程的不同，不能直接应用于图像翻译的GANs。为此，我们开发了一种新颖的协同进化方法，用于同时减少它们的内存使用和浮点运算次数（FLOPs）。在实践中，两个图像域的生成器被编码为两个群体，并协同优化以迭代地研究最重要的卷积滤波器。每个个体的适应度是使用参数数量、鉴别器感知正则化和循环一致性来计算的。在基准数据集上进行的大量实验证明了所提出方法在获得紧凑且有效的生成器方面的有效性。",
        "领域": "图像翻译/网络压缩/协同进化算法",
        "问题": "减少生成对抗网络在图像翻译任务中的内存使用和计算复杂度",
        "动机": "现有的网络压缩方法主要针对分类任务设计，不适用于图像翻译的生成对抗网络，因为它们的训练目标和过程不同",
        "方法": "开发了一种协同进化方法，将两个图像域的生成器编码为两个群体，通过协同优化迭代地研究最重要的卷积滤波器，使用参数数量、鉴别器感知正则化和循环一致性计算每个个体的适应度",
        "关键词": [
            "生成对抗网络",
            "图像翻译",
            "网络压缩",
            "协同进化算法"
        ],
        "涉及的技术概念": {
            "生成对抗网络（GANs）": "一种深度学习模型，由生成器和鉴别器组成，用于生成新的、与真实数据相似的数据",
            "图像到图像翻译": "将一种类型的图像转换为另一种类型的图像的任务",
            "协同进化算法": "一种模拟生物进化过程的算法，用于优化问题，通过多个群体之间的相互作用来寻找解决方案",
            "卷积滤波器": "在卷积神经网络中用于提取图像特征的滤波器",
            "循环一致性": "在图像翻译任务中，确保从域A到域B再回到域A的图像转换过程中，图像内容保持一致性的约束条件"
        }
    },
    {
        "order": 100,
        "title": "Implicit Surface Representations As Layers in Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Michalkiewicz_Implicit_Surface_Representations_As_Layers_in_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "Implicit shape representations, such as Level Sets, provide a very elegant formulation for performing computations involving curves and surfaces. However, including implicit representations into canonical Neural Network formulations is far from straightforward. This has consequently restricted existing approaches to shape inference, to significantly less effective representations, perhaps most commonly voxels occupancy maps or sparse point clouds. To overcome this limitation we propose a novel formulation that permits the use of implicit representations of curves and surfaces, of arbitrary topology, as individual layers in Neural Network architectures with end-to-end trainability. Specifically, we propose to represent the output as an oriented level set of a continuous and discretised embedding function. We investigate the benefits of our approach on the task of 3D shape prediction from a single image; and demonstrate its ability to produce a more accurate reconstruction compared to voxel-based representations. We further show that our model is flexible and can be applied to a variety of shape inference problems.",
        "中文标题": "隐式表面表示作为神经网络中的层",
        "摘要翻译": "隐式形状表示，如水平集，为执行涉及曲线和曲面的计算提供了非常优雅的公式。然而，将隐式表示纳入规范的神经网络公式远非易事。这因此限制了现有形状推断方法，使其只能使用效果显著较差的表示，最常见的是体素占用图或稀疏点云。为了克服这一限制，我们提出了一种新颖的公式，允许使用任意拓扑的曲线和曲面的隐式表示，作为具有端到端可训练性的神经网络架构中的单个层。具体来说，我们建议将输出表示为连续和离散嵌入函数的定向水平集。我们在从单张图像进行3D形状预测的任务上研究了我们的方法的好处；并展示了其与基于体素的表示相比，能够产生更准确的重建。我们进一步展示了我们的模型具有灵活性，可以应用于各种形状推断问题。",
        "领域": "3D形状预测/隐式表示/神经网络架构",
        "问题": "如何将隐式形状表示有效地纳入神经网络架构中",
        "动机": "现有的形状推断方法由于难以将隐式表示纳入神经网络，而只能使用效果较差的表示，如体素占用图或稀疏点云，这限制了形状推断的准确性和灵活性。",
        "方法": "提出了一种新颖的公式，允许使用任意拓扑的曲线和曲面的隐式表示作为神经网络架构中的单个层，具体是将输出表示为连续和离散嵌入函数的定向水平集。",
        "关键词": [
            "隐式表示",
            "3D形状预测",
            "神经网络层",
            "水平集",
            "形状推断"
        ],
        "涉及的技术概念": "隐式形状表示（如水平集）提供了一种优雅的方式来处理涉及曲线和曲面的计算。提出的方法通过将隐式表示作为神经网络中的层，解决了将隐式表示纳入神经网络的难题，从而提高了3D形状预测的准确性和模型的灵活性。"
    },
    {
        "order": 101,
        "title": "Recognizing Part Attributes With Insufficient Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Recognizing_Part_Attributes_With_Insufficient_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Recognizing_Part_Attributes_With_Insufficient_Data_ICCV_2019_paper.html",
        "abstract": "Recognizing the attributes of objects and their parts is central to many computer vision applications. Although great progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotations which are more expensive to obtain. In order to solve the data insufficiency problem and get rid of dependence on the part annotation, we introduce a novel Concept Sharing Network (CSN) for part attribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination of part location and appearance pattern) that has insufficient or zero training data, by learning the part location and appearance pattern respectively from the training data that usually mix them in a single label. Extensive experiments on CUB, Celeb A, and a newly proposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot part attribute recognition.",
        "中文标题": "在数据不足的情况下识别部分属性",
        "摘要翻译": "识别物体及其部分的属性对于许多计算机视觉应用至关重要。尽管在应用物体级识别方面取得了巨大进展，但由于部分属性识别的训练数据通常稀缺，尤其是在互联网规模的应用中，识别部分属性仍然不太适用。此外，大多数现有的部分属性识别方法依赖于部分注释，这些注释的获取成本更高。为了解决数据不足的问题并摆脱对部分注释的依赖，我们引入了一种新颖的概念共享网络（CSN）用于部分属性识别。CSN的一个巨大优势是它能够通过分别从通常将它们混合在单一标签中的训练数据中学习部分位置和外观模式，来识别训练数据不足或零训练数据的部分属性（部分位置和外观模式的组合）。在CUB、Celeb A和一个新提出的人体属性数据集上的大量实验证明了CSN的有效性及其相对于其他方法的优势，特别是对于训练样本较少的属性。进一步的实验表明，CSN还可以执行零样本部分属性识别。",
        "领域": "物体识别/属性识别/零样本学习",
        "问题": "在数据不足的情况下识别物体部分的属性",
        "动机": "解决部分属性识别中训练数据稀缺和依赖昂贵部分注释的问题",
        "方法": "引入概念共享网络（CSN），通过分别学习部分位置和外观模式来识别训练数据不足或零训练数据的部分属性",
        "关键词": [
            "物体识别",
            "属性识别",
            "零样本学习",
            "概念共享网络"
        ],
        "涉及的技术概念": "概念共享网络（CSN）是一种新颖的网络结构，用于在数据不足的情况下识别物体部分的属性。它通过分别学习部分位置和外观模式来识别部分属性，从而解决了训练数据稀缺和依赖昂贵部分注释的问题。"
    },
    {
        "order": 102,
        "title": "Self-Supervised Representation Learning From Multi-Domain Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_Self-Supervised_Representation_Learning_From_Multi-Domain_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Feng_Self-Supervised_Representation_Learning_From_Multi-Domain_Data_ICCV_2019_paper.html",
        "abstract": "We present an information-theoretically motivated constraint for self-supervised representation learning from multiple related domains. In contrast to previous self-supervised learning methods, our approach learns from multiple domains, which has the benefit of decreasing the build-in bias of individual domain, as well as leveraging information and allowing knowledge transfer across multiple domains. The proposed mutual information constraints encourage neural network to extract common invariant information across domains and to preserve peculiar information of each domain simultaneously. We adopt tractable upper and lower bounds of mutual information to make the proposed constraints solvable. The learned representation is more unbiased and robust toward the input images. Extensive experimental results on both multi-domain and large-scale datasets demonstrate the necessity and advantage of multi-domain self-supervised learning with mutual information constraints. Representations learned in our framework on state-of-the-art methods achieve improved performance than those learned on a single domain.",
        "中文标题": "从多领域数据中进行自监督表示学习",
        "摘要翻译": "我们提出了一种基于信息论的自监督表示学习约束，用于从多个相关领域学习。与之前的自监督学习方法相比，我们的方法从多个领域学习，这有助于减少单个领域的内置偏差，同时利用信息并允许跨多个领域的知识转移。提出的互信息约束鼓励神经网络提取跨领域的共同不变信息，并同时保留每个领域的独特信息。我们采用互信息的可处理上下界来使提出的约束可解。学习到的表示对输入图像更加无偏和鲁棒。在多领域和大规模数据集上的广泛实验结果证明了使用互信息约束进行多领域自监督学习的必要性和优势。在我们的框架下学习到的表示在最新方法上实现了比在单一领域学习到的表示更好的性能。",
        "领域": "自监督学习/多领域学习/表示学习",
        "问题": "如何从多个相关领域进行自监督表示学习，以减少单个领域的内置偏差并实现跨领域知识转移",
        "动机": "减少单个领域的内置偏差，利用跨领域信息，实现知识转移",
        "方法": "采用基于信息论的互信息约束，鼓励神经网络提取跨领域的共同不变信息并保留每个领域的独特信息，使用互信息的可处理上下界使约束可解",
        "关键词": [
            "自监督学习",
            "多领域学习",
            "表示学习",
            "互信息约束"
        ],
        "涉及的技术概念": "自监督表示学习是一种无需人工标注数据的学习方法，通过设计特定的预测任务来学习数据的有用表示。多领域学习涉及从多个相关但不同的数据领域学习，以提高模型的泛化能力和鲁棒性。互信息是信息论中的一个概念，用于衡量两个随机变量之间的相互依赖性，这里用于约束模型学习跨领域的共同不变信息。"
    },
    {
        "order": 103,
        "title": "A Tour of Convolutional Networks Guided by Linear Interpreters",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Michelini_A_Tour_of_Convolutional_Networks_Guided_by_Linear_Interpreters_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Michelini_A_Tour_of_Convolutional_Networks_Guided_by_Linear_Interpreters_ICCV_2019_paper.html",
        "abstract": "Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the \"articulations\" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs.",
        "中文标题": "线性解释器引导的卷积网络之旅",
        "摘要翻译": "卷积网络是由非线性单元连接的分层大型线性系统。这些单元是使网络能够适应输入的“关节”。要理解网络如何解决问题，我们必须全面观察这些关节决策。如果我们能够捕捉到特定输入下非线性单元的动作，我们就能够像它一直是线性系统那样来回重放整个系统。这也将揭示非线性动作，因为产生的线性系统，即线性解释器，依赖于输入图像。我们引入了一个称为LinearScope的钩子层，它允许我们并行运行网络和线性解释器。它的实现简单、灵活且高效。从这里我们可以进行许多好奇的探究：这些线性系统看起来像什么？当变换矩阵的行和列是图像时，它们看起来像什么？这些线性变换依赖什么类型的基础？答案取决于所呈现的问题，通过这些问题我们游览了一些用于分类、超分辨率（SR）和图像到图像翻译（I2I）的流行架构。对于分类，我们观察到流行网络使用每类像素投票策略，并严重依赖偏置参数。对于SR和I2I，我们发现CNN使用类似于人类视觉系统的小波型基础。对于I2I，我们揭示了复制移动和模板创建策略以生成输出。",
        "领域": "卷积神经网络/超分辨率/图像到图像翻译",
        "问题": "理解卷积网络如何通过非线性单元适应输入并解决问题",
        "动机": "探索卷积网络中的非线性单元如何影响网络决策，以及如何通过线性解释器揭示这些非线性动作",
        "方法": "引入LinearScope钩子层，允许并行运行网络和线性解释器，从而观察和分析网络行为",
        "关键词": [
            "卷积神经网络",
            "超分辨率",
            "图像到图像翻译"
        ],
        "涉及的技术概念": {
            "卷积网络": "由非线性单元连接的分层大型线性系统",
            "线性解释器": "依赖于输入图像的线性系统，用于揭示非线性动作",
            "LinearScope": "一种钩子层，允许并行运行网络和线性解释器",
            "超分辨率": "提高图像分辨率的技术",
            "图像到图像翻译": "将一种类型的图像转换为另一种类型的技术"
        }
    },
    {
        "order": 104,
        "title": "USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_USIP_Unsupervised_Stable_Interest_Point_Detection_From_3D_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP",
        "中文标题": "USIP：从3D点云中进行无监督稳定兴趣点检测",
        "摘要翻译": "在本文中，我们提出了USIP检测器：一种无监督稳定兴趣点检测器，能够在任意变换下从3D点云中检测出高度可重复且精确定位的关键点，而无需任何地面真实训练数据。我们的USIP检测器由一个特征提议网络组成，该网络从输入的3D点云及其通过随机生成的变换得到的变换对中学习稳定的关键点。我们提供了退化分析并提出了防止退化的解决方案。我们通过概率性Chamfer损失来鼓励关键点的高重复性和精确定位，该损失最小化了训练点云对中检测到的关键点之间的距离。在来自激光雷达、RGB-D和CAD模型的多个模拟和真实世界3D点云数据集上的重复性测试的广泛实验结果表明，我们的USIP检测器显著优于现有的手工制作和基于深度学习的3D关键点检测器。我们的代码可在项目网站上获取。https://github.com/lijx10/USIP",
        "领域": "3D点云处理/关键点检测/无监督学习",
        "问题": "在无需地面真实训练数据的情况下，从3D点云中检测出高度可重复且精确定位的关键点",
        "动机": "解决现有3D关键点检测器在无需地面真实数据的情况下，难以实现高重复性和精确定位的问题",
        "方法": "提出了一种无监督稳定兴趣点检测器（USIP），包括一个特征提议网络，通过随机生成的变换对输入3D点云进行学习，使用概率性Chamfer损失来优化关键点的重复性和定位精度",
        "关键词": [
            "3D点云",
            "关键点检测",
            "无监督学习",
            "Chamfer损失"
        ],
        "涉及的技术概念": "USIP检测器是一种无监督学习方法，用于从3D点云中检测关键点。它通过特征提议网络学习稳定的关键点，并使用概率性Chamfer损失来优化关键点的重复性和定位精度。该方法不需要地面真实训练数据，能够在任意变换下工作。"
    },
    {
        "order": 105,
        "title": "Controlling Neural Networks via Energy Dissipation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Moeller_Controlling_Neural_Networks_via_Energy_Dissipation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Moeller_Controlling_Neural_Networks_via_Energy_Dissipation_ICCV_2019_paper.html",
        "abstract": "The last decade has shown a tremendous success in solving various computer vision problems with the help of deep learning techniques. Lately, many works have demonstrated that learning-based approaches with suitable network architectures even exhibit superior performance for the solution of (ill-posed) image reconstruction problems such as deblurring, super-resolution, or medical image reconstruction. The drawback of purely learning-based methods, however, is that they cannot provide provable guarantees for the trained network to follow a given data formation process during inference. In this work we propose energy dissipating networks that iteratively compute a descent direction with respect to a given cost function or energy at the currently estimated reconstruction. Therefore, an adaptive step size rule such as a line-search, along with a suitable number of iterations can guarantee the reconstruction to follow a given data formation model encoded in the energy to arbitrary precision, and hence control the model's behavior even during test time. We prove that under standard assumptions, descent using the direction predicted by the network converges (linearly) to the global minimum of the energy. We illustrate the effectiveness of the proposed approach in experiments on single image super resolution and computed tomography (CT) reconstruction, and further illustrate extensions to convex feasibility problems.",
        "中文标题": "通过能量耗散控制神经网络",
        "摘要翻译": "过去十年中，深度学习技术在解决各种计算机视觉问题上取得了巨大成功。最近，许多工作表明，具有合适网络架构的基于学习的方法甚至在解决（不适定的）图像重建问题（如去模糊、超分辨率或医学图像重建）方面表现出优越的性能。然而，纯基于学习的方法的缺点是，它们无法为训练后的网络在推理过程中遵循给定的数据形成过程提供可证明的保证。在这项工作中，我们提出了能量耗散网络，它迭代地计算相对于给定成本函数或能量的下降方向在当前估计的重建中。因此，自适应步长规则（如线搜索）以及合适的迭代次数可以保证重建遵循编码在能量中的给定数据形成模型，达到任意精度，从而即使在测试时也能控制模型的行为。我们证明，在标准假设下，使用网络预测的方向下降（线性）收敛到能量的全局最小值。我们在单图像超分辨率和计算机断层扫描（CT）重建的实验以及凸可行性问题的扩展中说明了所提出方法的有效性。",
        "领域": "图像重建/超分辨率/医学图像处理",
        "问题": "纯基于学习的方法无法为训练后的网络在推理过程中遵循给定的数据形成过程提供可证明的保证",
        "动机": "提供一种方法，使得在图像重建等任务中，网络的行为即使在测试时也能被控制，遵循给定的数据形成模型",
        "方法": "提出能量耗散网络，迭代地计算相对于给定成本函数或能量的下降方向，使用自适应步长规则和合适的迭代次数保证重建遵循编码在能量中的给定数据形成模型",
        "关键词": [
            "能量耗散",
            "图像重建",
            "超分辨率",
            "医学图像处理",
            "凸可行性问题"
        ],
        "涉及的技术概念": "能量耗散网络是一种迭代计算下降方向的方法，用于确保重建过程遵循特定的数据形成模型。自适应步长规则（如线搜索）和合适的迭代次数是实现这一目标的关键。这种方法可以应用于图像重建、超分辨率、医学图像处理以及凸可行性问题等领域。"
    },
    {
        "order": 106,
        "title": "Small Steps and Giant Leaps: Minimal Newton Solvers for Deep Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Henriques_Small_Steps_and_Giant_Leaps_Minimal_Newton_Solvers_for_Deep_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Henriques_Small_Steps_and_Giant_Leaps_Minimal_Newton_Solvers_for_Deep_ICCV_2019_paper.html",
        "abstract": "We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network. This estimate has the same size and is similar to the momentum variable that is commonly used in  SGD . No estimate of the Hessian is maintained. We first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. We also show our optimiser's generality by testing on a large set of randomly generated architectures.",
        "中文标题": "小步与巨跃：深度学习的极小牛顿求解器",
        "摘要翻译": "我们提出了一种快速的二阶方法，可以作为当前深度学习求解器的直接替代品。与随机梯度下降（SGD）相比，它每次迭代仅需要两次额外的前向模式自动微分操作，其计算成本与两次标准前向传递相当，并且易于实现。我们的方法解决了当前二阶求解器长期存在的问题，这些求解器每次迭代都会精确地或通过共轭梯度方法反转一个近似的Hessian矩阵，这些过程比SGD步骤慢得多。相反，我们建议保持一个由逆Hessian矩阵投影的梯度估计，并在每次迭代中仅通过两次网络传递更新它。这个估计的大小与SGD中常用的动量变量相同且相似。不维护Hessian的估计。我们首先在已知解的小问题上验证了我们的方法，称为CurveBall（噪声Rosenbrock函数和退化2层线性网络），在这些问题上，当前的深度学习求解器表现不佳。然后，我们在CIFAR和ImageNet上训练了几个大型模型，包括ResNet和VGG-f网络，展示了在没有超参数调整的情况下更快的收敛速度。我们还通过在大量随机生成的架构上测试来展示我们的优化器的通用性。",
        "领域": "优化算法/深度学习/自动微分",
        "问题": "解决深度学习求解器在二阶优化中的效率问题",
        "动机": "提高深度学习模型的训练效率和收敛速度，减少计算成本",
        "方法": "提出一种新的二阶优化方法CurveBall，通过保持一个由逆Hessian矩阵投影的梯度估计，并在每次迭代中仅通过两次网络传递更新它，避免了每次迭代都反转Hessian矩阵的高成本",
        "关键词": [
            "二阶优化",
            "自动微分",
            "深度学习求解器"
        ],
        "涉及的技术概念": "随机梯度下降（SGD）、前向模式自动微分、Hessian矩阵、共轭梯度方法、动量变量、CurveBall优化器"
    },
    {
        "order": 107,
        "title": "Indices Matter: Learning to Index for Deep Image Matting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Indices_Matter_Learning_to_Index_for_Deep_Image_Matting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Indices_Matter_Learning_to_Index_for_Deep_Image_Matting_ICCV_2019_paper.html",
        "abstract": "We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of 'learning to index', and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the pooling and upsampling operators, without extra training supervision. At the core of this framework is a flexible network module, termed IndexNet, which dynamically generates indices conditioned on the feature map. Due to its flexibility, IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages. We demonstrate the effectiveness of IndexNet on the task of natural image matting where the quality of learned indices can be visually observed from predicted alpha mattes. Results on the Composition-1k matting dataset show that our model built on MobileNetv2 exhibits at least 16.1% improvement over the seminal VGG-16 based deep matting baseline, with less training data and lower model capacity. Code and models have been made available at: https://tinyurl.com/IndexNetV1.",
        "中文标题": "索引的重要性：学习索引以进行深度图像抠图",
        "摘要翻译": "我们展示了现有的上采样操作可以通过索引函数的概念进行统一。这一概念受到深度图像抠图解码过程中的观察启发，其中索引引导的反池化通常能够比其他上采样操作（如双线性插值）更好地恢复边界细节。通过将索引视为特征图的函数，我们引入了“学习索引”的概念，并提出了一个新颖的索引引导的编码器-解码器框架，其中索引是从数据中自适应自学习的，并用于指导池化和上采样操作，无需额外的训练监督。该框架的核心是一个灵活的网络模块，称为IndexNet，它根据特征图动态生成索引。由于其灵活性，IndexNet可以作为插件应用于几乎所有具有耦合下采样和上采样阶段的现成卷积网络。我们在自然图像抠图任务上展示了IndexNet的有效性，其中学习到的索引质量可以从预测的alpha遮罩中直观观察到。在Composition-1k抠图数据集上的结果显示，我们基于MobileNetv2构建的模型比基于VGG-16的深度抠图基线至少提高了16.1%，且使用更少的训练数据和更低的模型容量。代码和模型已在https://tinyurl.com/IndexNetV1上提供。",
        "领域": "图像抠图/深度学习/卷积神经网络",
        "问题": "如何更有效地恢复图像抠图中的边界细节",
        "动机": "观察到索引引导的反池化在恢复边界细节方面优于其他上采样操作，如双线性插值",
        "方法": "引入‘学习索引’的概念，提出一个索引引导的编码器-解码器框架，其中索引是从数据中自适应自学习的，并用于指导池化和上采样操作",
        "关键词": [
            "图像抠图",
            "索引函数",
            "编码器-解码器框架",
            "IndexNet"
        ],
        "涉及的技术概念": {
            "索引函数": "一种将特征图映射到索引的函数，用于指导上采样和池化操作",
            "IndexNet": "一个灵活的网络模块，能够根据特征图动态生成索引，用于改进图像抠图的质量",
            "编码器-解码器框架": "一种网络架构，通过编码器提取特征，解码器恢复图像细节，常用于图像分割和抠图任务"
        }
    },
    {
        "order": 108,
        "title": "Mixed High-Order Attention Network for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Mixed_High-Order_Attention_Network_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Attention has become more attractive in person re-identification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn.",
        "中文标题": "混合高阶注意力网络用于行人重识别",
        "摘要翻译": "注意力机制在行人重识别（ReID）中变得越来越有吸引力，因为它能够将可用资源的分配偏向于输入信号中最具信息量的部分。然而，最先进的工作仅集中于粗糙或一阶注意力设计，例如空间和通道注意力，而很少探索高阶注意力机制。我们迈出了解决这一问题的第一步。在本文中，我们首先提出了高阶注意力（HOA）模块，以在注意力机制中建模和利用复杂的高阶统计信息，从而捕捉行人之间的细微差异并产生具有区分性的注意力提议。然后，将行人ReID重新思考为零样本学习问题，我们提出了混合高阶注意力网络（MHN），以显式方式进一步增强注意力知识的区分性和丰富性。广泛的实验已经进行，以验证我们的MHN在三个大规模数据集（包括Market-1501、DukeMTMC-ReID和CUHK03-NP）上对行人ReID的优越性，相较于各种最先进的方法。代码可在http://www.bhchen.cn获取。",
        "领域": "行人重识别/注意力机制/零样本学习",
        "问题": "现有行人重识别方法主要集中于粗糙或一阶注意力设计，缺乏对高阶注意力机制的探索。",
        "动机": "探索高阶注意力机制，以捕捉行人之间的细微差异，提高行人重识别的准确性和效率。",
        "方法": "提出了高阶注意力（HOA）模块来建模和利用复杂的高阶统计信息，并提出了混合高阶注意力网络（MHN）以显式方式增强注意力知识的区分性和丰富性。",
        "关键词": [
            "高阶注意力",
            "行人重识别",
            "零样本学习"
        ],
        "涉及的技术概念": "高阶注意力（HOA）模块用于捕捉行人之间的细微差异，混合高阶注意力网络（MHN）用于增强注意力知识的区分性和丰富性。"
    },
    {
        "order": 109,
        "title": "Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Joshi_Semantic_Adversarial_Attacks_Parametric_Transformations_That_Fool_Deep_Classifiers_ICCV_2019_paper.html",
        "abstract": "Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, fine-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter specific attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classifier. We propose a novel approach to generate such \"semantic\" adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classifiers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.",
        "中文标题": "语义对抗攻击：欺骗深度分类器的参数化变换",
        "摘要翻译": "深度神经网络已被证明对带有不可察觉扰动的对抗性输入图像表现出一种有趣的脆弱性。然而，大多数对抗攻击假设对图像像素空间具有全局、细粒度的控制。在本文中，我们考虑了一个不同的设置：如果攻击者只能改变输入图像的特定属性会发生什么？这将生成可能明显不同但仍然自然且足以欺骗分类器的输入。我们提出了一种新颖的方法，通过在参数化条件生成模型的范围空间上优化特定的对抗性损失来生成这种“语义”对抗样本。我们在面部图像上训练的二元分类器上展示了我们攻击的实现，并表明这种自然外观的语义对抗样本是存在的。我们在合成数据和真实数据上评估了我们攻击的有效性，并与现有攻击方法进行了详细比较。我们用理论界限补充了我们的实证结果，这些界限证明了这种参数化对抗样本的存在。",
        "领域": "对抗性机器学习/生成模型/图像分类",
        "问题": "如何在只能改变输入图像特定属性的情况下生成自然外观且能欺骗分类器的对抗样本",
        "动机": "探索在限制条件下生成对抗样本的可能性，以揭示深度神经网络的脆弱性",
        "方法": "通过在参数化条件生成模型的范围空间上优化特定的对抗性损失来生成语义对抗样本",
        "关键词": [
            "对抗性样本",
            "参数化变换",
            "条件生成模型",
            "图像分类"
        ],
        "涉及的技术概念": "对抗性损失、参数化条件生成模型、二元分类器、理论界限"
    },
    {
        "order": 110,
        "title": "LAP-Net: Level-Aware Progressive Network for Image Dehazing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_LAP-Net_Level-Aware_Progressive_Network_for_Image_Dehazing_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a level-aware progressive network (LAP-Net) for single image dehazing. Unlike previous multi-stage algorithms that generally learn in a coarse-to-fine fashion, each stage of LAP-Net learns different levels of haze with different supervision. Then the network can progressively learn the gradually aggravating haze. With this design, each stage can focus on a region with specific haze level and restore clear details. To effectively fuse the results of varying haze levels at different stages, we develop an adaptive integration strategy to yield the final dehazed image. This strategy is achieved by a hierarchical integration scheme, which is in cooperation with the memory network and the domain knowledge of dehazing to highlight the best-restored regions of each stage. Extensive experiments on both real-world images and two dehazing benchmarks validate the effectiveness of our proposed method.",
        "中文标题": "LAP-Net: 用于图像去雾的层次感知渐进网络",
        "摘要翻译": "本文提出了一种用于单幅图像去雾的层次感知渐进网络（LAP-Net）。与以往通常以粗到细方式学习的多阶段算法不同，LAP-Net的每个阶段都在不同的监督下学习不同层次的雾度。然后，网络可以逐步学习逐渐加剧的雾度。通过这种设计，每个阶段都可以专注于具有特定雾度水平的区域并恢复清晰的细节。为了有效融合不同阶段不同雾度水平的结果，我们开发了一种自适应集成策略以生成最终的去雾图像。该策略通过分层集成方案实现，该方案与记忆网络和去雾的领域知识合作，以突出每个阶段的最佳恢复区域。在真实世界图像和两个去雾基准上的大量实验验证了我们提出方法的有效性。",
        "领域": "图像去雾/深度学习/计算机视觉",
        "问题": "单幅图像去雾",
        "动机": "解决现有去雾方法在多阶段学习中通常采用粗到细方式，无法有效处理不同层次雾度的问题。",
        "方法": "提出了一种层次感知渐进网络（LAP-Net），每个阶段学习不同层次的雾度，并采用自适应集成策略融合不同阶段的结果。",
        "关键词": [
            "图像去雾",
            "渐进网络",
            "自适应集成策略"
        ],
        "涉及的技术概念": "层次感知渐进网络（LAP-Net）是一种深度学习模型，用于单幅图像去雾。它通过在不同阶段学习不同层次的雾度，并采用自适应集成策略来融合这些结果，从而有效地去除图像中的雾度。自适应集成策略结合了记忆网络和去雾的领域知识，以突出每个阶段的最佳恢复区域。"
    },
    {
        "order": 111,
        "title": "Budget-Aware Adapters for Multi-Domain Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Berriel_Budget-Aware_Adapters_for_Multi-Domain_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Berriel_Budget-Aware_Adapters_for_Multi-Domain_Learning_ICCV_2019_paper.html",
        "abstract": "Multi-Domain Learning (MDL) refers to the problem of learning a set of models derived from a common deep architecture, each one specialized to perform a task in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL with a particular interest in obtaining domain-specific models with an adjustable budget in terms of the number of network parameters and computational complexity. Our intuition is that, as in real applications the number of domains and tasks can be very large, an effective MDL approach should not only focus on accuracy but also on having as few parameters as possible. To implement this idea we derive specialized deep models for each domain by adapting a pre-trained architecture but, differently from other methods, we propose a novel strategy to automatically adjust the computational complexity of the network. To this aim, we introduce Budget-Aware Adapters that select the most relevant feature channels to better handle data from a novel domain. Some constraints on the number of active switches are imposed in order to obtain a network respecting the desired complexity budget. Experimentally, we show that our approach leads to recognition accuracy competitive with state-of-the-art approaches but with much lighter networks both in terms of storage and computation.",
        "中文标题": "预算感知适配器用于多领域学习",
        "摘要翻译": "多领域学习（MDL）指的是从共同的深度架构中学习一组模型的问题，每个模型专门用于在特定领域（例如照片、素描、绘画）中执行任务。本文处理MDL，特别关注于在可调整的网络参数数量和计算复杂度预算下获得领域特定模型。我们的直觉是，由于在实际应用中领域和任务的数量可能非常大，一个有效的MDL方法不仅应关注准确性，还应尽可能减少参数数量。为了实现这一想法，我们通过调整预训练架构为每个领域派生专门的深度模型，但与其他方法不同，我们提出了一种新颖的策略来自动调整网络的计算复杂度。为此，我们引入了预算感知适配器，选择最相关的特征通道以更好地处理来自新领域的数据。为了获得符合所需复杂度预算的网络，我们对活动开关的数量施加了一些限制。实验表明，我们的方法在识别准确性上与最先进的方法竞争，但在存储和计算方面网络要轻得多。",
        "领域": "多领域学习/深度学习/神经网络",
        "问题": "在多领域学习中，如何在保持高准确性的同时，减少网络参数数量和计算复杂度",
        "动机": "实际应用中领域和任务数量庞大，需要一种既准确又参数少的多领域学习方法",
        "方法": "通过调整预训练架构为每个领域派生专门的深度模型，并引入预算感知适配器自动调整网络的计算复杂度",
        "关键词": [
            "多领域学习",
            "预算感知适配器",
            "计算复杂度",
            "网络参数"
        ],
        "涉及的技术概念": "多领域学习（MDL）涉及从共同的深度架构中学习一组模型，每个模型专门用于特定领域。预算感知适配器是一种选择最相关特征通道以处理新领域数据的技术，旨在自动调整网络的计算复杂度。"
    },
    {
        "order": 112,
        "title": "Hilbert-Based Generative Defense for Adversarial Examples",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Hilbert-Based_Generative_Defense_for_Adversarial_Examples_ICCV_2019_paper.html",
        "abstract": "Adversarial perturbations of clean images are usually imperceptible for human eyes, but can confidently fool deep neural networks (DNNs) to make incorrect predictions. Such vulnerability of DNNs raises serious security concerns about their practicability in security-sensitive applications. To defend against such adversarial perturbations, recently developed PixelDefend purifies a perturbed image based on PixelCNN in a raster scan order (row/column by row/column). However, such scan mode insufficiently exploits the correlations between pixels, which further limits its robustness performance. Therefore, we propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. Hilbert curve could well preserve local consistency when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled. Moreover, the defensive power can be further improved via ensembles of Hilbert curve with different orientations. Experimental results demonstrate the superiority of our method over the state-of-the-art defenses against various adversarial attacks.",
        "中文标题": "基于Hilbert的对抗样本生成防御",
        "摘要翻译": "干净图像的对抗性扰动通常对人眼来说是不可察觉的，但可以自信地欺骗深度神经网络（DNNs）做出错误的预测。DNNs的这种脆弱性引发了对其在安全敏感应用中实用性的严重安全担忧。为了防御这种对抗性扰动，最近开发的PixelDefend基于PixelCNN以光栅扫描顺序（逐行/逐列）净化受扰动的图像。然而，这种扫描模式未能充分利用像素之间的相关性，这进一步限制了其鲁棒性表现。因此，我们在本文中提出了一种更高级的Hilbert曲线扫描顺序来建模像素依赖性。Hilbert曲线在从二维图像映射到一维向量时能很好地保持局部一致性，因此可以更有效地建模邻近像素的局部特征。此外，通过不同方向的Hilbert曲线集合，可以进一步提高防御能力。实验结果证明了我们的方法在对抗各种对抗攻击方面的优越性。",
        "领域": "对抗性防御/图像安全/深度学习安全",
        "问题": "深度神经网络对对抗性扰动的脆弱性",
        "动机": "提高深度神经网络在安全敏感应用中的实用性和鲁棒性",
        "方法": "提出基于Hilbert曲线扫描顺序的像素依赖性建模方法，通过不同方向的Hilbert曲线集合提高防御能力",
        "关键词": [
            "对抗性防御",
            "Hilbert曲线",
            "像素依赖性"
        ],
        "涉及的技术概念": "对抗性扰动是指对干净图像进行微小修改，这些修改对人眼来说几乎不可察觉，但能够导致深度神经网络做出错误预测。PixelDefend是一种基于PixelCNN的方法，用于净化受对抗性扰动影响的图像，但其采用的光栅扫描顺序未能充分利用像素间的相关性。Hilbert曲线是一种空间填充曲线，能够保持局部一致性，有助于更有效地建模邻近像素的局部特征。通过集合不同方向的Hilbert曲线，可以进一步提高对抗性防御的能力。"
    },
    {
        "order": 113,
        "title": "Attention Augmented Convolutional Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.html",
        "abstract": "Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline.",
        "中文标题": "注意力增强的卷积网络",
        "摘要翻译": "卷积网络在许多计算机视觉应用中取得了巨大成功。然而，卷积操作有一个显著的弱点，即它仅在局部邻域内操作，因此错过了全局信息。另一方面，自注意力作为一种捕捉长距离交互的新兴技术，但主要应用于序列建模和生成建模任务。在本文中，我们提出通过将卷积特征图与通过一种新颖的相对自注意力机制生成的一组特征图连接起来，来增强卷积网络。特别是，我们将之前关于序列上的相对自注意力的工作扩展到图像，并讨论了一种内存高效的实现。与在通道上执行注意力而忽略空间信息的Squeeze-and-Excitation不同，我们的自注意力机制同时关注特征和空间位置，同时保持平移等变性。我们发现，注意力增强在ImageNet上的图像分类和COCO上的目标检测中，在许多不同的模型和尺度上，包括ResNets和一种最先进的移动受限网络，都带来了持续的改进，同时保持了参数数量的相似。特别是，我们的方法在ImageNet分类上比ResNet50基线提高了1.3%的top-1准确率，并且优于其他图像注意力机制，如Squeeze-and-Excitation。它还在RetinaNet基线之上，在COCO目标检测中实现了1.4 AP的改进。",
        "领域": "图像分类/目标检测/自注意力机制",
        "问题": "卷积操作仅能处理局部信息，无法捕捉全局信息",
        "动机": "为了克服卷积操作在处理全局信息方面的局限性，同时保持其处理局部信息的优势",
        "方法": "通过将卷积特征图与通过一种新颖的相对自注意力机制生成的特征图连接起来，增强卷积网络",
        "关键词": [
            "自注意力",
            "卷积网络",
            "图像分类",
            "目标检测"
        ],
        "涉及的技术概念": {
            "卷积网络": "一种深度学习模型，广泛应用于图像处理等领域，通过卷积操作提取特征",
            "自注意力": "一种机制，允许模型在处理输入时关注输入的不同部分，以捕捉长距离依赖",
            "相对自注意力": "自注意力的一种变体，考虑了元素之间的相对位置信息",
            "Squeeze-and-Excitation": "一种注意力机制，通过在通道维度上执行注意力来增强模型的表示能力",
            "平移等变性": "指模型对输入图像的平移具有不变性，即图像平移后，模型的输出特征也会相应平移"
        }
    },
    {
        "order": 114,
        "title": "Pushing the Frontiers of Unconstrained Crowd Counting: New Dataset and Benchmark Method",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Pushing_the_Frontiers_of_Unconstrained_Crowd_Counting_New_Dataset_and_ICCV_2019_paper.html",
        "abstract": "In this work, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors. Furthermore, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD) that is  2.8 larger than the most recent crowd counting datasets in terms of the number of images. It contains 4,250 images with 1.11 million annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations in addition to many distractor images, making it a very challenging dataset. Additionally, the dataset consists of rich annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset.",
        "中文标题": "推动无约束人群计数的新前沿：新数据集和基准方法",
        "摘要翻译": "在这项工作中，我们提出了一种新颖的人群计数网络，该网络通过残差估计逐步生成人群密度图。所提出的方法使用VGG16作为骨干网络，并采用由最后一层生成的密度图作为粗略预测，以使用残差学习逐步细化和生成更精细的密度图。此外，残差学习由基于不确定性的置信度加权机制指导，该机制允许在细化路径中仅流动高置信度的残差。所提出的置信度引导的深度残差计数网络（CG-DRCN）在最近的复杂数据集上进行了评估，并在误差方面取得了显著改进。此外，我们引入了一个新的大规模无约束人群计数数据集（JHU-CROWD），该数据集在图像数量上比最近的人群计数数据集大2.8倍。它包含4,250张图像和111万条注释。与现有数据集相比，所提出的数据集是在各种不同的场景和环境条件下收集的。具体来说，该数据集包括许多具有天气退化、光照变化以及许多干扰图像的图像，使其成为一个非常具有挑战性的数据集。此外，该数据集在图像级别和头部级别都包含丰富的注释。在这个数据集上评估和比较了几种最近的方法。",
        "领域": "人群计数/密度估计/残差学习",
        "问题": "如何在复杂场景下准确进行人群计数",
        "动机": "提高在复杂和多样化环境条件下人群计数的准确性和鲁棒性",
        "方法": "提出了一种置信度引导的深度残差计数网络（CG-DRCN），通过残差估计逐步生成人群密度图，并使用基于不确定性的置信度加权机制指导残差学习",
        "关键词": [
            "人群计数",
            "密度估计",
            "残差学习",
            "置信度加权"
        ],
        "涉及的技术概念": "VGG16作为骨干网络用于特征提取；残差学习用于逐步细化密度图；基于不确定性的置信度加权机制用于指导残差学习，确保只有高置信度的残差被用于密度图的细化。"
    },
    {
        "order": 115,
        "title": "Compact Trilinear Interaction for Visual Question Answering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.html",
        "abstract": "In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear teraction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets.",
        "中文标题": "用于视觉问答的紧凑三线性交互",
        "摘要翻译": "在视觉问答（VQA）中，答案与问题的意义和视觉内容有很大的相关性。因此，为了选择性地利用图像、问题和答案信息，我们提出了一种新颖的三线性交互模型，该模型同时学习这三个输入之间的高级关联。此外，为了克服交互的复杂性，我们引入了一种基于多模态张量的PARALIND分解，它有效地参数化了三个输入之间的三线性交互。此外，知识蒸馏首次应用于自由形式的开放式VQA。这不仅是为了减少计算成本和所需内存，而且是为了将知识从三线性交互模型转移到双线性交互模型。在基准数据集TDIUC、VQA-2.0和Visual7W上的广泛实验表明，当在所有三个数据集上使用单一模型时，所提出的紧凑三线性交互模型达到了最先进的结果。",
        "领域": "视觉问答/多模态学习/知识蒸馏",
        "问题": "如何在视觉问答中有效地利用图像、问题和答案信息",
        "动机": "提高视觉问答系统的准确性和效率，通过减少计算成本和内存需求，同时保持或提高性能",
        "方法": "提出了一种新颖的三线性交互模型，并引入了基于多模态张量的PARALIND分解来参数化三线性交互，以及首次在自由形式的开放式VQA中应用知识蒸馏",
        "关键词": [
            "视觉问答",
            "三线性交互",
            "知识蒸馏",
            "PARALIND分解",
            "多模态学习"
        ],
        "涉及的技术概念": {
            "视觉问答": "一种结合视觉信息和自然语言处理技术的任务，旨在回答关于图像内容的问题",
            "三线性交互": "一种同时考虑三个输入（如图像、问题和答案）之间高级关联的模型",
            "知识蒸馏": "一种将知识从复杂模型转移到简单模型的技术，以减少计算成本和内存需求",
            "PARALIND分解": "一种用于多模态张量分解的方法，旨在有效地参数化三线性交互",
            "多模态学习": "涉及处理和理解来自多种模态（如视觉和文本）信息的学习方法"
        }
    },
    {
        "order": 116,
        "title": "On the Efficacy of Knowledge Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cho_On_the_Efficacy_of_Knowledge_Distillation_ICCV_2019_paper.html",
        "abstract": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don't make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher's training early. Our results generalize across datasets and models.",
        "中文标题": "关于知识蒸馏的有效性",
        "摘要翻译": "在本文中，我们对知识蒸馏的有效性及其对学生和教师架构的依赖性进行了全面评估。从观察到更准确的教师往往不是好教师开始，我们试图梳理出影响知识蒸馏性能的因素。我们关键地发现，更大的模型并不总是更好的教师。我们表明，这是能力不匹配的结果，小学生无法模仿大教师。我们发现绕过这一问题的典型方法（如执行一系列知识蒸馏步骤）是无效的。最后，我们表明，通过提前停止教师的训练可以缓解这种影响。我们的结果在数据集和模型中具有普遍性。",
        "领域": "模型压缩/知识蒸馏/神经网络",
        "问题": "知识蒸馏的有效性及其对学生和教师架构的依赖性",
        "动机": "探索更准确的教师模型是否总是能提高知识蒸馏的效果，以及如何优化知识蒸馏过程",
        "方法": "通过实验评估不同教师和学生架构对知识蒸馏效果的影响，并提出提前停止教师训练的方法来缓解能力不匹配的问题",
        "关键词": [
            "知识蒸馏",
            "模型压缩",
            "神经网络"
        ],
        "涉及的技术概念": "知识蒸馏是一种模型压缩技术，通过训练一个较小的学生模型来模仿一个较大的教师模型的行为，以达到在保持性能的同时减少模型大小和计算资源的目的。本文探讨了教师模型的大小和准确性对学生模型性能的影响，以及如何通过调整训练策略来优化知识蒸馏的效果。"
    },
    {
        "order": 117,
        "title": "MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_MetaPruning_Meta_Learning_for_Automatic_Neural_Network_Channel_Pruning_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning.",
        "中文标题": "MetaPruning: 用于自动神经网络通道剪枝的元学习",
        "摘要翻译": "在本文中，我们提出了一种新颖的元学习方法，用于非常深度神经网络的自动通道剪枝。我们首先训练一个PruningNet，这是一种元网络，它能够为目标网络生成任何剪枝结构的权重参数。我们使用一种简单的随机结构采样方法来训练PruningNet。然后，我们应用一个进化过程来搜索性能良好的剪枝网络。由于权重是由训练好的PruningNet直接生成的，我们在搜索时不需要任何微调，因此搜索非常高效。通过为目标网络训练一个单一的PruningNet，我们可以在不同的约束条件下搜索各种剪枝网络，几乎不需要人工参与。与最先进的剪枝方法相比，我们在MobileNet V1/V2和ResNet上展示了优越的性能。代码可在https://github.com/liuzechun/MetaPruning获取。",
        "领域": "神经网络优化/自动化机器学习/深度学习模型压缩",
        "问题": "自动通道剪枝",
        "动机": "提高深度神经网络剪枝的效率和性能，减少人工参与",
        "方法": "提出了一种基于元学习的自动通道剪枝方法，通过训练PruningNet生成剪枝结构的权重参数，并应用进化算法搜索性能良好的剪枝网络",
        "关键词": [
            "元学习",
            "通道剪枝",
            "神经网络优化",
            "自动化机器学习",
            "深度学习模型压缩"
        ],
        "涉及的技术概念": "PruningNet是一种元网络，能够为目标网络生成任何剪枝结构的权重参数。使用随机结构采样方法训练PruningNet，并通过进化算法搜索性能良好的剪枝网络。这种方法在搜索时不需要微调，因此非常高效。"
    },
    {
        "order": 118,
        "title": "Towards Latent Attribute Discovery From Triplet Similarities",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nigam_Towards_Latent_Attribute_Discovery_From_Triplet_Similarities_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nigam_Towards_Latent_Attribute_Discovery_From_Triplet_Similarities_ICCV_2019_paper.html",
        "abstract": "This paper addresses the task of learning latent attributes from triplet similarity comparisons. Consider, for instance, the three shoes in Fig. 1(a). They can be compared according to color, comfort, size, or shape resulting in different rankings. Most approaches for embedding learning either make a simplifying assumption - that all inputs are comparable under a single criterion, or require expensive attribute supervision. We introduce Latent Similarity Networks (LSNs): a simple and effective technique to discover the underlying latent notions of similarity in data without any explicit attribute supervision. LSNs can be trained with standard triplet supervision and learn several latent embeddings that can be used to compare images under multiple notions of similarity. LSNs achieve state-of-the-art performance on UT-Zappos-50k Shoes and Celeb-A Faces datasets and also demonstrate the ability to uncover meaningful latent attributes.",
        "中文标题": "向从三元组相似性中发现的潜在属性迈进",
        "摘要翻译": "本文解决了从三元组相似性比较中学习潜在属性的任务。例如，考虑图1(a)中的三双鞋。它们可以根据颜色、舒适度、大小或形状进行比较，从而产生不同的排名。大多数嵌入学习方法要么做出简化假设——即所有输入在单一标准下是可比较的，要么需要昂贵的属性监督。我们引入了潜在相似性网络（LSNs）：一种简单而有效的技术，用于在没有明确属性监督的情况下发现数据中的潜在相似性概念。LSNs可以使用标准的三元组监督进行训练，并学习多个潜在嵌入，这些嵌入可用于在多种相似性概念下比较图像。LSNs在UT-Zappos-50k Shoes和Celeb-A Faces数据集上实现了最先进的性能，并展示了揭示有意义的潜在属性的能力。",
        "领域": "嵌入学习/相似性学习/属性发现",
        "问题": "如何在没有明确属性监督的情况下从三元组相似性比较中学习潜在属性",
        "动机": "现有的嵌入学习方法要么假设所有输入在单一标准下是可比较的，要么需要昂贵的属性监督，这限制了方法的适用性和效率",
        "方法": "引入潜在相似性网络（LSNs），一种无需明确属性监督即可发现数据中潜在相似性概念的简单而有效的技术",
        "关键词": [
            "潜在属性",
            "三元组相似性",
            "嵌入学习",
            "相似性学习",
            "属性发现"
        ],
        "涉及的技术概念": {
            "潜在相似性网络（LSNs）": "一种技术，用于在没有明确属性监督的情况下发现数据中的潜在相似性概念，可以使用标准的三元组监督进行训练，并学习多个潜在嵌入，这些嵌入可用于在多种相似性概念下比较图像",
            "三元组监督": "一种监督学习方法，通过比较三个样本（一个锚点样本、一个正样本和一个负样本）来学习嵌入",
            "潜在嵌入": "通过学习得到的嵌入，可以表示数据中的潜在属性或特征"
        }
    },
    {
        "order": 119,
        "title": "Sym-Parameterized Dynamic Inference for Mixed-Domain Image Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Sym-Parameterized_Dynamic_Inference_for_Mixed-Domain_Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Sym-Parameterized_Dynamic_Inference_for_Mixed-Domain_Image_Translation_ICCV_2019_paper.html",
        "abstract": "Recent advances in image-to-image translation have led to some ways to generate multiple domain images through a single network. However, there is still a limit in creating an image of a target domain without a dataset on it. We propose a method to expand the concept of `multi-domain' from data to the loss area, and to combine the characteristics of each domain to create an image. First, we introduce a sym-parameter and its learning method that can mix various losses and can synchronize them with input conditions. Then, we propose Sym-parameterized Generative Network (SGN) using it. Through experiments, we confirmed that SGN could mix the characteristics of various data and losses, and it is possible to translate images to any mixed-domain without ground truths, such as 30% Van Gogh and 20% Monet and 40% snowy.",
        "中文标题": "对称参数化动态推理用于混合域图像翻译",
        "摘要翻译": "近年来，图像到图像翻译的进展已经使得通过单一网络生成多域图像成为可能。然而，在没有目标域数据集的情况下创建目标域图像仍然存在限制。我们提出了一种方法，将“多域”的概念从数据扩展到损失区域，并结合每个域的特征来创建图像。首先，我们引入了一个对称参数及其学习方法，该方法可以混合各种损失并使其与输入条件同步。然后，我们提出了使用该对称参数的对称参数化生成网络（SGN）。通过实验，我们证实了SGN能够混合各种数据和损失的特征，并且可以在没有真实标签的情况下将图像翻译到任何混合域，例如30%的梵高风格、20%的莫奈风格和40%的雪景。",
        "领域": "图像翻译/生成对抗网络/风格迁移",
        "问题": "在没有目标域数据集的情况下创建目标域图像",
        "动机": "扩展多域概念，结合每个域的特征来创建图像，解决现有方法在无目标域数据集时的限制",
        "方法": "引入对称参数及其学习方法，提出对称参数化生成网络（SGN），通过混合各种数据和损失的特征实现图像翻译",
        "关键词": [
            "图像翻译",
            "生成对抗网络",
            "风格迁移",
            "对称参数",
            "混合域"
        ],
        "涉及的技术概念": "对称参数化生成网络（SGN）是一种新型网络架构，通过引入对称参数及其学习方法，能够混合各种损失并使其与输入条件同步，从而实现无需目标域数据集即可进行图像翻译的功能。这种方法特别适用于风格迁移和图像翻译任务，能够生成具有特定风格混合的图像。"
    },
    {
        "order": 120,
        "title": "GeoStyle: Discovering Fashion Trends and Events",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mall_GeoStyle_Discovering_Fashion_Trends_and_Events_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mall_GeoStyle_Discovering_Fashion_Trends_and_Events_ICCV_2019_paper.html",
        "abstract": "Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are > 20% more accurate than prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe.",
        "中文标题": "GeoStyle: 发现时尚趋势和事件",
        "摘要翻译": "理解时尚风格和趋势对零售商和消费者都具有巨大的潜在兴趣。人们上传到社交媒体的照片是一个历史和公开的数据源，记录了世界各地不同时间人们的着装方式。虽然我们现在有工具可以自动识别这些照片中人们穿着的服装和风格属性，但我们缺乏分析这些属性的空间和时间趋势或预测未来的能力。在本文中，我们通过提供一个自动框架来解决这一需求，该框架分析大量的街景图像以（a）发现和预测各种时尚属性以及自动发现的风格的长期趋势，以及（b）识别影响人们穿着的时空局部事件。我们展示了我们的框架在长期趋势预测上比现有技术准确度高出20%以上，并识别了数百个影响全球时尚的社会意义事件。",
        "领域": "时尚分析/时空数据分析/预测模型",
        "问题": "缺乏分析时尚属性的空间和时间趋势或预测未来的能力",
        "动机": "理解时尚风格和趋势对零售商和消费者具有巨大的潜在兴趣",
        "方法": "提供一个自动框架，分析大量的街景图像以发现和预测时尚属性的长期趋势，并识别影响人们穿着的时空局部事件",
        "关键词": [
            "时尚趋势",
            "时空分析",
            "预测模型"
        ],
        "涉及的技术概念": "自动识别服装和风格属性、分析空间和时间趋势、预测未来时尚趋势、识别时空局部事件"
    },
    {
        "order": 121,
        "title": "Better and Faster: Exponential Loss for Image Patch Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Better_and_Faster_Exponential_Loss_for_Image_Patch_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Better_and_Faster_Exponential_Loss_for_Image_Patch_Matching_ICCV_2019_paper.html",
        "abstract": "Recent studies on image patch matching are paying more attention on hard sample learning, because easy samples do not contribute much to the network optimization. They have proposed various hard negative sample mining strategies, but very few addressed this problem from the perspective of loss functions. Our research shows that the conventional Siamese and triplet losses treat all samples linearly, thus make the training time consuming. Instead, we propose the exponential Siamese and triplet losses, which can naturally focus more on hard samples and put less emphasis on easy ones, meanwhile, speed up the optimization. To assist the exponential losses, we introduce the hard positive sample mining to further enhance the effectiveness. The extensive experiments demonstrate our proposal improves both metric and descriptor learning on several well accepted benchmarks, and outperforms the state-of-the-arts on the UBC dataset. Moreover, it also shows a better generalizability on cross-spectral image matching and image retrieval tasks.",
        "中文标题": "更好更快：用于图像块匹配的指数损失",
        "摘要翻译": "最近关于图像块匹配的研究越来越关注难样本学习，因为简单样本对网络优化的贡献不大。他们提出了各种难负样本挖掘策略，但很少有人从损失函数的角度解决这个问题。我们的研究表明，传统的Siamese和triplet损失对所有样本进行线性处理，因此使得训练时间消耗大。相反，我们提出了指数Siamese和triplet损失，它们自然能更多地关注难样本，减少对简单样本的重视，同时加快优化速度。为了辅助指数损失，我们引入了难正样本挖掘，以进一步增强效果。大量实验证明，我们的提议在几个公认的基准上改进了度量和描述符学习，并在UBC数据集上超越了现有技术。此外，它在跨光谱图像匹配和图像检索任务上也显示出更好的泛化能力。",
        "领域": "图像块匹配/难样本学习/损失函数优化",
        "问题": "图像块匹配中的难样本学习问题",
        "动机": "传统的Siamese和triplet损失对所有样本进行线性处理，导致训练时间消耗大，且简单样本对网络优化的贡献不大。",
        "方法": "提出了指数Siamese和triplet损失，以及难正样本挖掘策略，以加快优化速度并增强难样本学习的效果。",
        "关键词": [
            "图像块匹配",
            "难样本学习",
            "损失函数优化",
            "Siamese损失",
            "triplet损失",
            "难正样本挖掘"
        ],
        "涉及的技术概念": {
            "Siamese损失": "一种用于比较两个输入相似度的损失函数，常用于图像匹配任务。",
            "triplet损失": "一种用于学习特征表示的损失函数，通过比较锚点、正样本和负样本之间的距离来优化模型。",
            "难样本挖掘": "一种策略，旨在识别和处理对模型训练最具挑战性的样本，以提高模型的性能。",
            "指数损失": "一种改进的损失函数，通过指数方式增加对难样本的重视，减少对简单样本的重视，从而加快模型训练速度。"
        }
    },
    {
        "order": 122,
        "title": "Accelerate CNN via Recursive Bayesian Pruning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Accelerate_CNN_via_Recursive_Bayesian_Pruning_ICCV_2019_paper.html",
        "abstract": "Channel Pruning, widely used for accelerating Convolutional Neural Networks, is an NP-hard problem due to the inter-layer dependency of channel redundancy. Existing methods generally ignored the above dependency for computation simplicity. To solve the problem, under the Bayesian framework, we here propose a layer-wise Recursive Bayesian Pruning method (RBP). A new dropout-based measurement of redundancy, which facilitate the computation of posterior assuming inter-layer dependency, is introduced. Specifically, we model the noise across layers as a Markov chain and target its posterior to reflect the inter-layer dependency. Considering the closed form solution for posterior is intractable, we derive a sparsity-inducing Dirac-like prior which regularizes the distribution of the designed noise to automatically approximate the posterior. Compared with the existing methods, no additional overhead is required when the inter-layer dependency assumed. The redundant channels can be simply identified by tiny dropout noise and directly pruned layer by layer. Experiments on popular CNN architectures have shown that the proposed method outperforms several state-of-the-arts. Particularly, we achieve up to 5.0x, 2.2x and 1.7x FLOPs reduction with little accuracy loss on the large scale dataset ILSVRC2012 for VGG16, ResNet50 and MobileNetV2, respectively.",
        "中文标题": "通过递归贝叶斯剪枝加速CNN",
        "摘要翻译": "通道剪枝广泛用于加速卷积神经网络，由于通道冗余的层间依赖性，这是一个NP难问题。现有方法通常为了计算简便而忽略了上述依赖性。为了解决这个问题，我们在贝叶斯框架下提出了一种逐层递归贝叶斯剪枝方法（RBP）。引入了一种新的基于dropout的冗余度量，这有助于在假设层间依赖性的情况下计算后验。具体来说，我们将跨层噪声建模为马尔可夫链，并以其后验反映层间依赖性。考虑到后验的闭式解难以处理，我们推导了一种诱导稀疏性的狄拉克式先验，它正则化设计噪声的分布以自动近似后验。与现有方法相比，在假设层间依赖性的情况下不需要额外的开销。冗余通道可以通过微小的dropout噪声简单地识别，并逐层直接剪枝。在流行的CNN架构上的实验表明，所提出的方法优于几种最先进的方法。特别是在大规模数据集ILSVRC2012上，对于VGG16、ResNet50和MobileNetV2，我们分别实现了高达5.0倍、2.2倍和1.7倍的FLOPs减少，且准确率损失很小。",
        "领域": "神经网络加速/模型剪枝/贝叶斯方法",
        "问题": "解决卷积神经网络中由于通道冗余的层间依赖性导致的NP难问题",
        "动机": "现有方法为了计算简便而忽略了通道冗余的层间依赖性，导致剪枝效果不佳",
        "方法": "提出了一种逐层递归贝叶斯剪枝方法（RBP），通过引入基于dropout的冗余度量和诱导稀疏性的狄拉克式先验，自动近似后验，实现高效剪枝",
        "关键词": [
            "通道剪枝",
            "递归贝叶斯剪枝",
            "dropout",
            "马尔可夫链",
            "狄拉克式先验"
        ],
        "涉及的技术概念": {
            "通道剪枝": "一种用于减少卷积神经网络计算量的技术，通过移除不重要的通道来实现",
            "递归贝叶斯剪枝": "一种基于贝叶斯理论的剪枝方法，通过递归地逐层剪枝来优化网络",
            "dropout": "一种正则化技术，通过在训练过程中随机丢弃神经元来防止过拟合",
            "马尔可夫链": "一种数学模型，用于描述一个系统在不同状态之间转移的过程，其中下一个状态只依赖于当前状态",
            "狄拉克式先验": "一种先验分布，用于在贝叶斯推断中引入稀疏性，有助于简化模型"
        }
    },
    {
        "order": 123,
        "title": "Towards Adversarially Robust Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.",
        "中文标题": "迈向对抗性鲁棒的目标检测",
        "摘要翻译": "目标检测是一项重要的视觉任务，并已成为许多视觉系统中不可或缺的组成部分，使其鲁棒性成为实际应用中越来越重要的性能因素。尽管许多最近的工作已经证明目标检测模型容易受到对抗性攻击的影响，但很少有努力致力于提高它们的鲁棒性。在这项工作中，我们朝着这个方向进行了初步尝试。我们首先从模型鲁棒性的角度重新审视并系统分析了目标检测器和许多最近开发的攻击。然后，我们提出了目标检测的多任务学习视角，并识别了任务损失的不对称作用。我们进一步开发了一种对抗性训练方法，该方法可以利用多种攻击源来提高检测模型的鲁棒性。在PASCAL-VOC和MS-COCO上的大量实验验证了所提出方法的有效性。",
        "领域": "目标检测/对抗性学习/模型鲁棒性",
        "问题": "提高目标检测模型对抗对抗性攻击的鲁棒性",
        "动机": "目标检测模型在实际应用中容易受到对抗性攻击的影响，需要提高其鲁棒性",
        "方法": "从模型鲁棒性的角度分析目标检测器和攻击方法，提出多任务学习视角，并开发一种利用多种攻击源的对抗性训练方法",
        "关键词": [
            "目标检测",
            "对抗性学习",
            "模型鲁棒性",
            "多任务学习",
            "对抗性训练"
        ],
        "涉及的技术概念": "目标检测是一种识别图像中特定对象并确定其位置的技术。对抗性攻击是指通过添加微小且通常难以察觉的扰动到输入数据中，以欺骗机器学习模型的技术。模型鲁棒性指的是模型在面对输入数据的小变化或攻击时保持性能的能力。多任务学习是一种机器学习方法，它通过同时学习多个相关任务来提高模型的泛化能力。对抗性训练是一种通过将对抗性样本纳入训练过程来提高模型鲁棒性的技术。"
    },
    {
        "order": 124,
        "title": "HBONet: Harmonious Bottleneck on Two Orthogonal Dimensions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_HBONet_Harmonious_Bottleneck_on_Two_Orthogonal_Dimensions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_HBONet_Harmonious_Bottleneck_on_Two_Orthogonal_Dimensions_ICCV_2019_paper.html",
        "abstract": "MobileNets, a class of top-performing convolutional neural network architectures in terms of accuracy and efficiency trade-off, are increasingly used in many resource-aware vision applications. In this paper, we present Harmonious Bottleneck on two Orthogonal dimensions (HBO), a novel architecture unit, specially tailored to boost the accuracy of extremely lightweight MobileNets at the level of less than 40 MFLOPs. Unlike existing bottleneck designs that mainly focus on exploring the interdependencies among the channels of either groupwise or depthwise convolutional features, our HBO improves bottleneck representation while maintaining similar complexity via jointly encoding the feature interdependencies across both spatial and channel dimensions. It has two reciprocal components, namely spatial contraction-expansion and channel expansion-contraction, nested in a bilaterally symmetric structure. The combination of two interdependent transformations performing on orthogonal dimensions of feature maps enhances the representation and generalization ability of our proposed module, guaranteeing compelling performance with limited computational resource and power. By replacing the original bottlenecks in MobileNetV2 backbone with HBO modules, we construct HBONets which are evaluated on ImageNet classification, PASCAL VOC object detection and Market-1501 person re-identification. Extensive experiments show that with the severe constraint of computational budget our models outperform MobileNetV2 counterparts by remarkable margins of at most 6.6%, 6.3% and 5.0% on the above benchmarks respectively. Code and pretrained models are available at https://github.com/d-li14/HBONet.",
        "中文标题": "HBONet: 两个正交维度上的和谐瓶颈",
        "摘要翻译": "MobileNets，一类在准确性和效率权衡方面表现优异的卷积神经网络架构，越来越多地用于许多资源感知的视觉应用中。在本文中，我们提出了两个正交维度上的和谐瓶颈（HBO），一种新颖的架构单元，专门用于提升计算量低于40 MFLOPs的极轻量级MobileNets的准确性。与现有的瓶颈设计主要集中于探索组卷积或深度卷积特征通道之间的相互依赖性不同，我们的HBO通过联合编码空间和通道维度上的特征相互依赖性，在保持相似复杂度的同时提高了瓶颈表示。它有两个相互补充的组件，即空间收缩-扩展和通道扩展-收缩，嵌套在一个双边对称结构中。在特征图的正交维度上执行的两个相互依赖的变换的组合增强了我们提出模块的表示和泛化能力，保证了在有限的计算资源和功率下的出色性能。通过用HBO模块替换MobileNetV2骨干中的原始瓶颈，我们构建了HBONets，并在ImageNet分类、PASCAL VOC对象检测和Market-1501人员重识别上进行了评估。大量实验表明，在计算预算的严格约束下，我们的模型在上述基准测试中分别以最多6.6%、6.3%和5.0%的显著优势超越了MobileNetV2的对应模型。代码和预训练模型可在https://github.com/d-li14/HBONet获取。",
        "领域": "轻量级神经网络/卷积神经网络/视觉应用",
        "问题": "提升极轻量级MobileNets在有限计算资源下的准确性",
        "动机": "为了在资源受限的设备上实现高效的视觉应用，需要提升轻量级卷积神经网络的性能",
        "方法": "提出了两个正交维度上的和谐瓶颈（HBO），通过联合编码空间和通道维度上的特征相互依赖性，提高瓶颈表示",
        "关键词": [
            "轻量级神经网络",
            "卷积神经网络",
            "视觉应用"
        ],
        "涉及的技术概念": {
            "MobileNets": "一类在准确性和效率权衡方面表现优异的卷积神经网络架构",
            "HBO": "两个正交维度上的和谐瓶颈，一种新颖的架构单元，用于提升极轻量级MobileNets的准确性",
            "空间收缩-扩展": "HBO的一个组件，用于处理特征图的空间维度",
            "通道扩展-收缩": "HBO的另一个组件，用于处理特征图的通道维度",
            "ImageNet分类": "一个大规模的视觉识别挑战，用于评估图像分类模型的性能",
            "PASCAL VOC对象检测": "一个用于评估对象检测模型性能的基准测试",
            "Market-1501人员重识别": "一个用于评估人员重识别模型性能的数据集"
        }
    },
    {
        "order": 125,
        "title": "Physical Adversarial Textures That Fool Visual Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wiyatno_Physical_Adversarial_Textures_That_Fool_Visual_Object_Tracking_ICCV_2019_paper.html",
        "abstract": "We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to find practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots.",
        "中文标题": "物理对抗性纹理欺骗视觉目标跟踪",
        "摘要翻译": "我们提出了一种创建看似不起眼的纹理的方法，当这些纹理作为海报在物理世界中展示时，会导致视觉目标跟踪系统变得混乱。当一个被视觉跟踪的目标移动到这样的海报前时，其对抗性纹理使跟踪器锁定它，从而使目标得以逃避。这种对抗性攻击评估了几种欺骗很少被针对的回归模型的优化策略：非目标性、目标性以及新提出的一系列引导对抗性损失。此外，虽然我们使用变换期望（EOT）算法生成物理对抗性样本，这些样本在不同条件下成像时能够欺骗跟踪模型，但我们比较了不同场景变量的影响，以找到具有高对抗性强度的实用攻击设置和收敛速度。我们进一步展示了使用模拟场景优化的纹理可以混淆现实世界中的摄像头和机器人跟踪系统。",
        "领域": "对抗性攻击/视觉目标跟踪/物理世界应用",
        "问题": "创建物理对抗性纹理以欺骗视觉目标跟踪系统",
        "动机": "研究如何通过物理对抗性纹理使视觉目标跟踪系统失效，从而保护隐私或安全",
        "方法": "使用变换期望（EOT）算法生成物理对抗性样本，并评估不同优化策略的效果",
        "关键词": [
            "对抗性攻击",
            "视觉目标跟踪",
            "物理世界应用",
            "变换期望算法",
            "对抗性损失"
        ],
        "涉及的技术概念": "变换期望（EOT）算法是一种用于生成物理对抗性样本的技术，能够在不同条件下成像时欺骗跟踪模型。对抗性损失是指用于优化对抗性样本以最大化模型预测错误的损失函数。"
    },
    {
        "order": 126,
        "title": "Automatic and Robust Skull Registration Based on Discrete Uniformization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Automatic_and_Robust_Skull_Registration_Based_on_Discrete_Uniformization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Automatic_and_Robust_Skull_Registration_Based_on_Discrete_Uniformization_ICCV_2019_paper.html",
        "abstract": "Skull registration plays a fundamental role in forensic science and is crucial for craniofacial reconstruction. The complicated topology, lack of anatomical features, and low quality reconstructed mesh make skull registration challenging. In this work, we propose an automatic skull registration method based on the discrete uniformization theory, which can handle complicated topologies and is robust to low quality meshes. We apply dynamic Yamabe flow to realize discrete uniformization, which modifies the mesh combinatorial structure during the flow and conformally maps the multiply connected skull surface onto a planar disk with circular holes. The 3D surfaces can be registered by matching their planar images using harmonic maps. This method is rigorous with theoretic guarantee, automatic without user intervention, and robust to low mesh quality. Our experimental results demonstrate the efficiency and efficacy of the method.",
        "中文标题": "基于离散均匀化的自动鲁棒颅骨配准",
        "摘要翻译": "颅骨配准在法医学中扮演着基础角色，对于颅面重建至关重要。复杂的拓扑结构、缺乏解剖特征以及重建网格的低质量使得颅骨配准具有挑战性。在这项工作中，我们提出了一种基于离散均匀化理论的自动颅骨配准方法，该方法能够处理复杂的拓扑结构，并且对低质量网格具有鲁棒性。我们应用动态Yamabe流来实现离散均匀化，这在流动过程中修改了网格的组合结构，并将多连通颅骨表面共形映射到带有圆形孔的平面盘上。通过使用调和映射匹配它们的平面图像，可以实现3D表面的配准。该方法具有理论保证的严谨性，无需用户干预即可自动完成，并且对网格质量低具有鲁棒性。我们的实验结果证明了该方法的效率和有效性。",
        "领域": "法医学/颅面重建/几何处理",
        "问题": "颅骨配准在复杂拓扑结构和低质量网格情况下的挑战",
        "动机": "提高颅骨配准的自动化程度和鲁棒性，以支持法医学和颅面重建",
        "方法": "基于离散均匀化理论，应用动态Yamabe流实现离散均匀化，通过调和映射匹配平面图像进行3D表面配准",
        "关键词": [
            "离散均匀化",
            "动态Yamabe流",
            "调和映射",
            "颅骨配准",
            "法医学"
        ],
        "涉及的技术概念": "离散均匀化理论是一种数学方法，用于将复杂的几何形状转换为更简单的形式，以便于分析和处理。动态Yamabe流是一种用于实现离散均匀化的技术，它通过修改网格的组合结构来实现这一目标。调和映射是一种数学工具，用于将一个几何形状映射到另一个形状，同时保持某些几何属性不变。这些技术概念在本研究中被用来解决颅骨配准的问题，特别是在处理复杂拓扑结构和低质量网格时。"
    },
    {
        "order": 127,
        "title": "O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.html",
        "abstract": "This paper proposes a novel noisy label detection approach, named O2U-net, for deep neural networks without human annotations. Different from prior work which requires specifically designed noise-robust loss functions or networks, O2U-net is easy to implement but effective. It only requires adjusting the hyper-parameters of the deep network to make its status transfer from overfitting to underfitting (O2U) cyclically. The losses of each sample are recorded during iterations. The higher the normalized average loss of a sample, the higher the probability of being noisy labels. O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels. We conduct sufficient experiments on multiple datasets in various settings. The experimental results prove the state-of-the-art of O2S-net.",
        "中文标题": "O2U-Net: 一种简单的深度神经网络噪声标签检测方法",
        "摘要翻译": "本文提出了一种新颖的噪声标签检测方法，名为O2U-net，适用于无需人工标注的深度神经网络。与之前需要特别设计的噪声鲁棒损失函数或网络的工作不同，O2U-net易于实现但效果显著。它仅需调整深度网络的超参数，使其状态从过拟合循环转移到欠拟合（O2U）。在迭代过程中记录每个样本的损失。样本的归一化平均损失越高，其作为噪声标签的概率就越高。O2U-net自然兼容主动学习和其他人工标注方法。这为带有噪声标签的学习引入了额外的灵活性。我们在多种设置下的多个数据集上进行了充分的实验。实验结果证明了O2S-net的先进性。",
        "领域": "噪声标签检测/深度神经网络/主动学习",
        "问题": "深度神经网络中的噪声标签检测问题",
        "动机": "为了在无需人工标注的情况下，有效检测深度神经网络中的噪声标签，提高模型训练的准确性和效率。",
        "方法": "通过调整深度网络的超参数，使其状态从过拟合循环转移到欠拟合（O2U），在迭代过程中记录每个样本的损失，根据归一化平均损失的高低判断样本是否为噪声标签。",
        "关键词": [
            "噪声标签检测",
            "深度神经网络",
            "主动学习",
            "过拟合",
            "欠拟合"
        ],
        "涉及的技术概念": {
            "O2U-net": "一种新颖的噪声标签检测方法，通过调整深度网络的超参数，使其状态从过拟合循环转移到欠拟合，以检测噪声标签。",
            "噪声鲁棒损失函数": "特别设计的损失函数，用于提高模型对噪声标签的鲁棒性。",
            "主动学习": "一种机器学习方法，通过选择最有信息量的样本进行标注，以提高学习效率和模型性能。",
            "过拟合": "模型在训练数据上表现良好，但在未见过的数据上表现不佳的现象。",
            "欠拟合": "模型在训练数据和未见过的数据上都表现不佳的现象。"
        }
    },
    {
        "order": 128,
        "title": "Wasserstein GAN With Quadratic Transport Cost",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Wasserstein_GAN_With_Quadratic_Transport_Cost_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Wasserstein_GAN_With_Quadratic_Transport_Cost_ICCV_2019_paper.html",
        "abstract": "Wasserstein GANs are increasingly used in Computer Vision applications as they are easier to train. Previous WGAN variants mainly use the l_1 transport cost to compute the Wasserstein distance between the real and synthetic data distributions. The l_1 transport cost restricts the discriminator to be 1-Lipschitz. However, WGANs with l_1 transport cost were recently shown to not always converge. In this paper, we propose WGAN-QC, a WGAN with quadratic transport cost. Based on the quadratic transport cost, we propose an Optimal Transport Regularizer (OTR) to stabilize the training process of WGAN-QC. We prove that the objective of the discriminator during each generator update computes the exact quadratic Wasserstein distance between real and synthetic data distributions. We also prove that WGAN-QC converges to a local equilibrium point with finite discriminator updates per generator update. We show experimentally on a Dirac distribution that WGAN-QC converges, when many of the l_1 cost WGANs fail to [22]. Qualitative and quantitative results on the CelebA, CelebA-HQ, LSUN and the ImageNet dog datasets show that WGAN-QC is better than state-of-art GAN methods. WGAN-QC has much faster runtime than other WGAN variants.",
        "中文标题": "具有二次传输成本的Wasserstein GAN",
        "摘要翻译": "Wasserstein GANs在计算机视觉应用中越来越受欢迎，因为它们更容易训练。之前的WGAN变体主要使用l_1传输成本来计算真实数据和合成数据分布之间的Wasserstein距离。l_1传输成本限制了判别器必须是1-Lipschitz的。然而，最近显示使用l_1传输成本的WGAN并不总是收敛。在本文中，我们提出了WGAN-QC，一种具有二次传输成本的WGAN。基于二次传输成本，我们提出了一个最优传输正则化器（OTR）来稳定WGAN-QC的训练过程。我们证明了在每次生成器更新期间，判别器的目标计算了真实和合成数据分布之间的精确二次Wasserstein距离。我们还证明了WGAN-QC在每次生成器更新有限次判别器更新后收敛到局部平衡点。我们在Dirac分布上的实验表明，WGAN-QC能够收敛，而许多使用l_1成本的WGAN则不能[22]。在CelebA、CelebA-HQ、LSUN和ImageNet狗数据集上的定性和定量结果表明，WGAN-QC优于最先进的GAN方法。WGAN-QC的运行时间比其他WGAN变体快得多。",
        "领域": "生成对抗网络/最优传输/图像生成",
        "问题": "解决Wasserstein GANs在训练过程中的收敛性问题",
        "动机": "提高Wasserstein GANs的稳定性和训练效率",
        "方法": "提出了一种具有二次传输成本的WGAN（WGAN-QC），并引入了最优传输正则化器（OTR）来稳定训练过程",
        "关键词": [
            "Wasserstein GAN",
            "二次传输成本",
            "最优传输正则化器"
        ],
        "涉及的技术概念": "Wasserstein距离用于衡量两个概率分布之间的差异，1-Lipschitz条件限制了判别器的梯度，最优传输正则化器（OTR）用于稳定训练过程，二次传输成本用于计算Wasserstein距离。"
    },
    {
        "order": 129,
        "title": "Few-Shot Image Recognition With Knowledge Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.html",
        "abstract": "Human can well recognize images of novel categories just after browsing few examples of these categories. One possible reason is that they have some external discriminative visual information about these categories from their prior knowledge. Inspired from this, we propose a novel Knowledge Transfer Network architecture (KTN) for few-shot image recognition. The proposed KTN model jointly incorporates visual feature learning, knowledge inferring and classifier learning into one unified framework for their optimal compatibility. First, the visual classifiers for novel categories are learned based on the convolutional neural network with the cosine similarity optimization. To fully explore the prior knowledge, a semantic-visual mapping network is then developed to conduct knowledge inference, which enables to infer the classifiers for novel categories from base categories. Finally, we design an adaptive fusion scheme to infer the desired classifiers by effectively integrating the above knowledge and visual information. Extensive experiments are conducted on two widely-used Mini-ImageNet and ImageNet Few-Shot benchmarks to evaluate the effectiveness of the proposed method. The results compared with the state-of-the-art approaches show the encouraging performance of the proposed method, especially on 1-shot and 2-shot tasks.",
        "中文标题": "基于知识迁移的少样本图像识别",
        "摘要翻译": "人类在浏览了几个新类别的示例后，就能很好地识别这些类别的图像。一个可能的原因是，他们从先前的知识中获得了一些关于这些类别的外部区分性视觉信息。受此启发，我们提出了一种新颖的知识迁移网络架构（KTN），用于少样本图像识别。提出的KTN模型将视觉特征学习、知识推断和分类器学习联合纳入一个统一的框架中，以实现它们的最佳兼容性。首先，基于卷积神经网络和余弦相似度优化，学习新类别的视觉分类器。为了充分挖掘先验知识，随后开发了一个语义-视觉映射网络来进行知识推断，这使得能够从基础类别推断出新类别的分类器。最后，我们设计了一种自适应融合方案，通过有效整合上述知识和视觉信息来推断所需的分类器。在两个广泛使用的Mini-ImageNet和ImageNet少样本基准上进行了大量实验，以评估所提出方法的有效性。与最先进的方法相比，结果显示所提出方法的性能令人鼓舞，尤其是在1-shot和2-shot任务上。",
        "领域": "少样本学习/图像识别/知识迁移",
        "问题": "解决少样本图像识别问题",
        "动机": "人类能够通过少量示例识别新类别的图像，这启发我们探索如何利用先验知识来提高少样本图像识别的性能。",
        "方法": "提出了一种知识迁移网络架构（KTN），该架构联合视觉特征学习、知识推断和分类器学习，通过卷积神经网络和余弦相似度优化学习新类别的视觉分类器，开发语义-视觉映射网络进行知识推断，并设计自适应融合方案整合知识和视觉信息。",
        "关键词": [
            "少样本学习",
            "图像识别",
            "知识迁移",
            "卷积神经网络",
            "余弦相似度",
            "语义-视觉映射",
            "自适应融合"
        ],
        "涉及的技术概念": {
            "卷积神经网络": "一种深度学习模型，特别适用于处理图像数据。",
            "余弦相似度": "一种衡量两个向量之间相似度的方法，通过计算它们之间的余弦角来评估。",
            "语义-视觉映射": "一种将语义信息（如类别标签）与视觉信息（如图像特征）关联起来的技术。",
            "自适应融合": "一种动态调整不同信息源权重的方法，以优化最终结果。"
        }
    },
    {
        "order": 130,
        "title": "Scalable Verified Training for Provably Robust Image Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gowal_Scalable_Verified_Training_for_Provably_Robust_Image_Classification_ICCV_2019_paper.html",
        "abstract": "Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of IMAGENET.",
        "中文标题": "可扩展的验证训练用于可证明鲁棒的图像分类",
        "摘要翻译": "最近的研究表明，可以训练深度神经网络，使其对范数有界的对抗性扰动具有可证明的鲁棒性。大多数这些方法基于最小化所有可能对抗性扰动的最坏情况损失的上界。尽管这些技术显示出前景，但它们往往导致难以优化的过程，难以扩展到更大的网络。通过全面分析，我们展示了一种简单的边界技术——区间边界传播（IBP），如何被利用来训练大型可证明鲁棒的神经网络，这些网络在验证准确性上超越了现有技术。虽然IBP计算的上界对于一般网络可能相当弱，但我们证明了适当的损失和巧妙的超参数调度允许网络适应，使得IBP边界紧密。这导致了一个快速且稳定的学习算法，它超越了更复杂的方法，并在MNIST、CIFAR-10和SVHN上取得了最先进的结果。它还允许我们训练最大的模型，在IMAGENET的缩小版本上超越无意义的边界进行验证。",
        "领域": "对抗性防御/神经网络验证/图像分类",
        "问题": "训练深度神经网络以使其对对抗性扰动具有可证明的鲁棒性",
        "动机": "现有的方法虽然显示出前景，但在扩展到更大网络时面临优化困难",
        "方法": "利用区间边界传播（IBP）技术，通过适当的损失和超参数调度，使网络适应以紧密IBP边界，实现快速且稳定的学习算法",
        "关键词": [
            "对抗性防御",
            "神经网络验证",
            "图像分类"
        ],
        "涉及的技术概念": "区间边界传播（IBP）是一种用于训练深度神经网络的技术，通过计算网络输出的上界来确保网络对对抗性扰动的鲁棒性。这种方法通过最小化最坏情况损失的上界来实现，适用于提高图像分类任务中的验证准确性。"
    },
    {
        "order": 131,
        "title": "Continual Learning by Asymmetric Loss Approximation With Single-Side Overestimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Continual_Learning_by_Asymmetric_Loss_Approximation_With_Single-Side_Overestimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Continual_Learning_by_Asymmetric_Loss_Approximation_With_Single-Side_Overestimation_ICCV_2019_paper.html",
        "abstract": "Catastrophic forgetting is a critical challenge in training deep neural networks. Although continual learning has been investigated as a countermeasure to the problem, it often suffers from the requirements of additional network components and the limited scalability to a large number of tasks. We propose a novel approach to continual learning by approximating a true loss function using an asymmetric quadratic function with one of its sides overestimated. Our algorithm is motivated by the empirical observation that the network parameter updates affect the target loss functions asymmetrically. In the proposed continual learning framework, we estimate an asymmetric loss function for the tasks considered in the past through a proper overestimation of its unobserved sides in training new tasks, while deriving the accurate model parameter for the observable sides. In contrast to existing approaches, our method is free from the side effects and achieves the state-of-the-art accuracy that is even close to the upper-bound performance on several challenging benchmark datasets.",
        "中文标题": "通过单侧高估的非对称损失近似进行持续学习",
        "摘要翻译": "灾难性遗忘是训练深度神经网络时的一个关键挑战。尽管持续学习已被研究作为该问题的对策，但它常常受到需要额外网络组件和对大量任务的可扩展性有限的困扰。我们提出了一种新颖的持续学习方法，通过使用一侧被高估的非对称二次函数来近似真实损失函数。我们的算法受到网络参数更新对目标损失函数影响不对称的经验观察的启发。在所提出的持续学习框架中，我们通过对训练新任务时未观察到的侧面进行适当的高估，来估计过去考虑的任务的非对称损失函数，同时为可观察的侧面导出准确的模型参数。与现有方法相比，我们的方法没有副作用，并在几个具有挑战性的基准数据集上实现了接近上限性能的最先进准确度。",
        "领域": "持续学习/深度神经网络/损失函数优化",
        "问题": "解决深度神经网络在持续学习过程中的灾难性遗忘问题",
        "动机": "现有持续学习方法需要额外网络组件且对大量任务的可扩展性有限",
        "方法": "通过使用一侧被高估的非对称二次函数来近似真实损失函数，估计过去任务的非对称损失函数，同时为可观察的侧面导出准确的模型参数",
        "关键词": [
            "持续学习",
            "灾难性遗忘",
            "非对称损失函数",
            "深度神经网络",
            "损失函数优化"
        ],
        "涉及的技术概念": "灾难性遗忘指的是在学习新任务时，神经网络会忘记之前学到的信息。持续学习是一种旨在使神经网络能够连续学习多个任务而不忘记之前任务的学习方法。非对称损失函数是一种在损失函数中引入不对称性，以更好地适应特定学习任务的方法。"
    },
    {
        "order": 132,
        "title": "Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wray_Fine-Grained_Action_Retrieval_Through_Multiple_Parts-of-Speech_Embeddings_ICCV_2019_paper.html",
        "abstract": "We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.",
        "中文标题": "通过多词性嵌入实现细粒度动作检索",
        "摘要翻译": "我们解决了文本与视频之间的跨模态细粒度动作检索问题。跨模态检索通常通过学习一个共享的嵌入空间来实现，该空间可以无差别地嵌入不同模态。在本文中，我们提出通过解耦伴随字幕中的词性（PoS）来丰富嵌入。我们为每个PoS标签构建了一个单独的多模态嵌入空间。然后，多个PoS嵌入的输出被用作集成多模态空间的输入，我们在该空间执行动作检索。所有嵌入通过结合PoS感知和PoS无关的损失进行联合训练。我们的提议使得学习专门的嵌入空间成为可能，这些空间提供了相同嵌入实体的多种视图。我们报告了在大规模EPIC数据集上细粒度动作的首次检索结果，在广义零样本设置下。结果显示我们的方法在视频到文本和文本到视频动作检索中的优势。我们还展示了在MSR-VTT数据集上解耦PoS对于跨模态视频检索通用任务的好处。",
        "领域": "细粒度动作识别/跨模态检索/词性分析",
        "问题": "跨模态细粒度动作检索",
        "动机": "提高文本与视频之间细粒度动作检索的准确性和效率",
        "方法": "通过解耦伴随字幕中的词性（PoS）来丰富嵌入，为每个PoS标签构建单独的多模态嵌入空间，并通过结合PoS感知和PoS无关的损失进行联合训练",
        "关键词": [
            "细粒度动作识别",
            "跨模态检索",
            "词性分析"
        ],
        "涉及的技术概念": "跨模态检索、共享嵌入空间、词性（PoS）解耦、多模态嵌入空间、PoS感知和PoS无关的损失、广义零样本设置"
    },
    {
        "order": 133,
        "title": "Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "Hardware-friendly network quantization (e.g., binary/uniform quantization) can efficiently accelerate the inference and meanwhile reduce memory consumption of the deep neural networks, which is crucial for model deployment on resource-limited devices like mobile phones. However, due to the discreteness of low-bit quantization, existing quantization methods often face the unstable training process and severe performance degradation. To address this problem, in this paper we propose Differentiable Soft Quantization (DSQ) to bridge the gap between the full-precision and low-bit networks. DSQ can automatically evolve during training to gradually approximate the standard quantization. Owing to its differentiable property, DSQ can help pursue the accurate gradients in backward propagation, and reduce the quantization loss in forward process with an appropriate clipping range. Extensive experiments over several popular network structures show that training low-bit neural networks with DSQ can consistently outperform state-of-the-art quantization methods. Besides, our first efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture achieves up to 1.7x speed up, compared with the open-source 8-bit high-performance inference framework NCNN [31].",
        "中文标题": "可微分软量化：桥接全精度与低比特神经网络",
        "摘要翻译": "硬件友好的网络量化（例如，二进制/均匀量化）可以有效地加速推理过程，同时减少深度神经网络的内存消耗，这对于在资源有限的设备（如手机）上部署模型至关重要。然而，由于低比特量化的离散性，现有的量化方法常常面临训练过程不稳定和性能严重下降的问题。为了解决这个问题，本文提出了可微分软量化（DSQ），以桥接全精度网络和低比特网络之间的差距。DSQ可以在训练过程中自动进化，逐渐逼近标准量化。由于其可微分的特性，DSQ可以帮助在反向传播中追求准确的梯度，并在前向过程中通过适当的裁剪范围减少量化损失。在几种流行的网络结构上进行的大量实验表明，使用DSQ训练低比特神经网络可以持续超越最先进的量化方法。此外，我们首次在ARM架构设备上部署2到4位DSQ的高效实现，与开源的8位高性能推理框架NCNN相比，实现了高达1.7倍的加速。",
        "领域": "神经网络量化/硬件加速/模型部署",
        "问题": "解决低比特量化导致的训练不稳定和性能下降问题",
        "动机": "为了在资源有限的设备上高效部署深度神经网络，需要一种既能加速推理过程又能减少内存消耗的量化方法",
        "方法": "提出可微分软量化（DSQ），通过自动进化和可微分特性，桥接全精度与低比特网络，减少量化损失",
        "关键词": [
            "神经网络量化",
            "硬件加速",
            "模型部署"
        ],
        "涉及的技术概念": "可微分软量化（DSQ）是一种新型的量化方法，它通过自动进化和可微分特性，旨在桥接全精度网络和低比特网络之间的差距。DSQ在训练过程中自动调整，以逼近标准量化，从而在反向传播中追求准确的梯度，并在前向过程中通过适当的裁剪范围减少量化损失。此外，DSQ的首次高效实现展示了在ARM架构设备上部署2到4位量化的潜力，与现有的8位高性能推理框架相比，实现了显著的加速。"
    },
    {
        "order": 134,
        "title": "Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Label-PEnet_Sequential_Label_Propagation_and_Enhancement_Networks_for_Weakly_Supervised_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ge_Label-PEnet_Sequential_Label_Propagation_and_Enhancement_Networks_for_Weakly_Supervised_ICCV_2019_paper.html",
        "abstract": "Weakly-supervised instance segmentation aims to detect and segment object instances precisely, given image-level labels only. Unlike previous methods which are composed of multiple offline stages, we propose Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) that progressively transforms image-level labels to pixel-wise labels in a coarse-to-fine manner. We design four cascaded modules including multi-label classification, object detection, instance refinement and instance segmentation, which are implemented sequentially by sharing the same backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy that generalizes labels from high level images to low-level pixels gradually with increasing accuracy. In addition, we design a proposal calibration module to explore the ability of classification networks to find key pixels that identify object parts, which serves as a post validation strategy running in the inverse order. We evaluate the efficiency of our Label-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007 and 2012. Experimental results show that Label-PEnet outperforms the state-of-art algorithms by a clear margin, and obtains comparable performance even with fully supervised approaches.",
        "中文标题": "Label-PEnet: 用于弱监督实例分割的顺序标签传播和增强网络",
        "摘要翻译": "弱监督实例分割旨在仅给定图像级标签的情况下，精确检测和分割对象实例。与之前由多个离线阶段组成的方法不同，我们提出了顺序标签传播和增强网络（称为Label-PEnet），它以从粗到细的方式逐步将图像级标签转换为像素级标签。我们设计了四个级联模块，包括多标签分类、对象检测、实例细化和实例分割，这些模块通过共享相同的主干网络顺序实现。级联管道通过课程学习策略交替训练，该策略从高级图像到低级像素逐渐泛化标签，并逐步提高准确性。此外，我们设计了一个提案校准模块，以探索分类网络找到识别对象部分的关键像素的能力，这作为按相反顺序运行的后验证策略。我们在标准基准PASCAL VOC 2007和2012上评估了我们的Label-PEnet在挖掘实例掩码方面的效率。实验结果表明，Label-PEnet明显优于最先进的算法，并且即使与完全监督的方法相比，也能获得相当的性能。",
        "领域": "实例分割/弱监督学习/图像理解",
        "问题": "在仅使用图像级标签的情况下，精确检测和分割对象实例",
        "动机": "提高弱监督实例分割的准确性和效率，减少对像素级标注的依赖",
        "方法": "提出顺序标签传播和增强网络（Label-PEnet），通过四个级联模块逐步将图像级标签转换为像素级标签，并采用课程学习策略和提案校准模块提高分割准确性",
        "关键词": [
            "实例分割",
            "弱监督学习",
            "图像理解",
            "标签传播",
            "增强网络"
        ],
        "涉及的技术概念": "顺序标签传播和增强网络（Label-PEnet）是一种新型网络架构，旨在通过级联模块和课程学习策略，从图像级标签逐步生成像素级标签，以提高弱监督实例分割的准确性。提案校准模块用于验证和优化分类网络在识别对象关键像素方面的能力。"
    },
    {
        "order": 135,
        "title": "Vehicle Re-Identification in Aerial Imagery: Dataset and Approach",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Vehicle_Re-Identification_in_Aerial_Imagery_Dataset_and_Approach_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Vehicle_Re-Identification_in_Aerial_Imagery_Dataset_and_Approach_ICCV_2019_paper.html",
        "abstract": "In this work, we construct a large-scale dataset for vehicle re-identification (ReID), which contains 137k images of 13k vehicle instances captured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based vehicle ReID dataset. To increase intra-class variation, each vehicle is captured by at least two UAVs at different locations, with diverse view-angles and flight-altitudes. We manually label a variety of vehicle attributes, including vehicle type, color, skylight, bumper, spare tire and luggage rack. Furthermore, for each vehicle image, the annotator is also required to mark the discriminative parts that helps them to distinguish this particular vehicle from others. Besides the dataset, we also design a specific vehicle ReID algorithm to make full use of the rich annotation information. It is capable of explicitly detecting discriminative parts for each specific vehicle and significantly outperforming the evaluated baselines and state-of-the-art vehicle ReID approaches.",
        "中文标题": "航空影像中的车辆重识别：数据集与方法",
        "摘要翻译": "在这项工作中，我们构建了一个大规模的车辆重识别（ReID）数据集，该数据集包含由无人机搭载的摄像头捕获的13k车辆实例的137k张图像。据我们所知，这是最大的基于无人机的车辆ReID数据集。为了增加类内变化，每辆车至少由两架无人机在不同位置捕获，具有不同的视角和飞行高度。我们手动标注了多种车辆属性，包括车辆类型、颜色、天窗、保险杠、备胎和行李架。此外，对于每张车辆图像，标注者还需要标记出有助于他们区分这辆特定车辆与其他车辆的区分性部分。除了数据集之外，我们还设计了一种特定的车辆ReID算法，以充分利用丰富的标注信息。该算法能够明确检测每辆特定车辆的区分性部分，并显著优于评估的基线和最先进的车辆ReID方法。",
        "领域": "车辆重识别/无人机影像分析/属性识别",
        "问题": "解决在航空影像中进行车辆重识别的问题",
        "动机": "构建一个大规模的无人机捕获的车辆重识别数据集，并开发一种能够充分利用丰富标注信息的车辆重识别算法",
        "方法": "构建包含137k张图像的大规模数据集，并设计一种特定的车辆ReID算法，该算法能够明确检测每辆特定车辆的区分性部分",
        "关键词": [
            "车辆重识别",
            "无人机影像",
            "属性识别"
        ],
        "涉及的技术概念": "车辆重识别（ReID）是指在不同的图像或视频中识别出同一辆车辆的技术。无人机影像分析涉及使用无人机捕获的图像进行各种分析任务。属性识别是指识别和标注图像中对象的特定属性，如颜色、类型等。"
    },
    {
        "order": 136,
        "title": "The LogBarrier Adversarial Attack: Making Effective Use of Decision Boundary Information",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Finlay_The_LogBarrier_Adversarial_Attack_Making_Effective_Use_of_Decision_Boundary_ICCV_2019_paper.html",
        "abstract": "Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the well-regarded logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks.",
        "中文标题": "LogBarrier对抗攻击：有效利用决策边界信息",
        "摘要翻译": "图像分类的对抗攻击是对图像进行的小幅扰动，旨在导致模型误分类。对抗攻击形式上对应于一个优化问题：找到一个最小范数的图像扰动，约束条件是导致误分类。已经开发了许多有效的攻击方法。然而，迄今为止，还没有基于梯度的攻击使用优化文献中的最佳实践来解决这个约束最小化问题。我们基于这些最佳实践，使用备受推崇的对数障碍方法设计了一种新的无目标攻击。平均而言，我们的攻击距离与基准数据集（MNIST、CIFAR10、ImageNet-1K）上所有最先进的攻击相似或更好。此外，我们的方法在最具挑战性的图像上表现显著更好，这些图像通常需要更大的扰动才能导致误分类。我们在几个对抗性防御模型上使用了LogBarrier攻击，并显示它比其他攻击更有效地对抗性扰动所有图像：使用LogBarrier攻击扰动所有图像所需的距离明显小于其他最先进的攻击。",
        "领域": "对抗性机器学习/图像分类/优化算法",
        "问题": "如何更有效地生成对抗性扰动以导致图像分类模型误分类",
        "动机": "现有的基于梯度的对抗攻击方法未充分利用优化文献中的最佳实践来解决约束最小化问题",
        "方法": "设计了一种新的无目标攻击方法，采用对数障碍方法，基于优化文献中的最佳实践",
        "关键词": [
            "对抗性攻击",
            "图像分类",
            "对数障碍方法",
            "优化问题"
        ],
        "涉及的技术概念": "对抗性攻击是指对输入数据进行微小修改，以欺骗机器学习模型，使其做出错误的预测。对数障碍方法是一种用于解决约束优化问题的技术，通过在目标函数中加入障碍项来避免违反约束条件。"
    },
    {
        "order": 137,
        "title": "LIP: Local Importance-Based Pooling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_LIP_Local_Importance-Based_Pooling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_LIP_Local_Importance-Based_Pooling_ICCV_2019_paper.html",
        "abstract": "Spatial downsampling layers are favored in convolutional neural networks (CNNs) to downscale feature maps for larger receptive fields and less memory consumption. However, for discriminative tasks, there is a possibility that these layers lose the discriminative details due to improper pooling strategies, which could hinder the learning process and eventually result in suboptimal models. In this paper, we present a unified framework over the existing downsampling layers (e.g., average pooling, max pooling, and strided convolution) from a local importance view. In this framework, we analyze the issues of these widely-used pooling layers and figure out the criteria for designing an effective downsampling layer. According to this analysis, we propose a conceptually simple, general, and effective pooling layer based on local importance modeling, termed as Local Importance-based Pooling (LIP). LIP can automatically enhance discriminative features during the downsampling procedure by learning adaptive importance weights based on inputs. Experiment results show that LIP consistently yields notable gains with different depths and different architectures on ImageNet classification. In the challenging MS COCO dataset, detectors with our LIP-ResNets as backbones obtain a consistent improvement (>=1.4%) over the vanilla ResNets, and especially achieve the current state-of-the-art performance in detecting small objects under the single-scale testing scheme.",
        "中文标题": "LIP: 基于局部重要性的池化",
        "摘要翻译": "在卷积神经网络（CNNs）中，空间下采样层被青睐用于下采样特征图以获得更大的感受野和更少的内存消耗。然而，对于判别任务，这些层可能由于不恰当的池化策略而丢失判别细节，这可能会阻碍学习过程并最终导致次优模型。在本文中，我们从局部重要性的角度提出了一个统一框架，覆盖现有的下采样层（例如，平均池化、最大池化和步幅卷积）。在这个框架中，我们分析了这些广泛使用的池化层的问题，并找出了设计有效下采样层的标准。根据这一分析，我们提出了一个概念上简单、通用且有效的基于局部重要性建模的池化层，称为基于局部重要性的池化（LIP）。LIP能够通过学习基于输入的自适应重要性权重，在下采样过程中自动增强判别特征。实验结果表明，LIP在不同深度和不同架构的ImageNet分类上持续产生显著的增益。在具有挑战性的MS COCO数据集上，使用我们的LIP-ResNets作为骨干的检测器在单尺度测试方案下，相比普通ResNets获得了一致的改进（>=1.4%），特别是在检测小物体方面达到了当前的最先进性能。",
        "领域": "卷积神经网络/图像分类/目标检测",
        "问题": "现有下采样层可能丢失判别细节，导致次优模型",
        "动机": "提高卷积神经网络在下采样过程中保留判别细节的能力，以增强模型性能",
        "方法": "提出基于局部重要性建模的池化层（LIP），通过学习自适应重要性权重自动增强判别特征",
        "关键词": [
            "卷积神经网络",
            "下采样",
            "池化策略",
            "局部重要性",
            "自适应权重"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）中的空间下采样层，如平均池化、最大池化和步幅卷积，用于下采样特征图以增加感受野和减少内存消耗。基于局部重要性的池化（LIP）是一种新的池化方法，它通过学习输入的自适应重要性权重，在下采样过程中自动增强判别特征，从而提高模型在图像分类和目标检测任务中的性能。"
    },
    {
        "order": 138,
        "title": "Bridging the Domain Gap for Ground-to-Aerial Image Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Regmi_Bridging_the_Domain_Gap_for_Ground-to-Aerial_Image_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Regmi_Bridging_the_Domain_Gap_for_Ground-to-Aerial_Image_Matching_ICCV_2019_paper.html",
        "abstract": "The visual entities in cross-view (e.g. ground and aerial) images exhibit drastic domain changes due to the differences in viewpoints each set of images is captured from. Existing state-of-the-art methods address the problem by learning view-invariant images descriptors. We propose a novel method for solving this task by exploiting the gener- ative powers of conditional GANs to synthesize an aerial representation of a ground-level panorama query and use it to minimize the domain gap between the two views. The synthesized image being from the same view as the ref- erence (target) image, helps the network to preserve im- portant cues in aerial images following our Joint Feature Learning approach. We fuse the complementary features from a synthesized aerial image with the original ground- level panorama features to obtain a robust query represen- tation. In addition, we employ multi-scale feature aggre- gation in order to preserve image representations at dif- ferent scales useful for solving this complex task. Experi- mental results show that our proposed approach performs significantly better than the state-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and top-1% retrieval accuracies. Furthermore, we evaluate the gen- eralization of the proposed method for urban landscapes on our newly collected cross-view localization dataset with geo-reference information.",
        "中文标题": "弥合地面到航空图像匹配的领域差距",
        "摘要翻译": "跨视角（例如地面和航空）图像中的视觉实体由于每组图像捕捉视角的差异而表现出剧烈的领域变化。现有的最先进方法通过学习视角不变的图像描述符来解决这个问题。我们提出了一种新颖的方法，通过利用条件生成对抗网络（GAN）的生成能力来合成地面全景查询的航空表示，并使用它来最小化两个视角之间的领域差距。合成的图像与参考（目标）图像来自同一视角，有助于网络在我们的联合特征学习方法中保留航空图像中的重要线索。我们将合成航空图像的互补特征与原始地面全景特征融合，以获得稳健的查询表示。此外，我们采用多尺度特征聚合以保留不同尺度的图像表示，这对于解决这一复杂任务非常有用。实验结果表明，我们提出的方法在具有挑战性的CVUSA数据集上，在top-1和top-1%检索准确率方面显著优于最先进的方法。此外，我们在新收集的带有地理参考信息的跨视角定位数据集上评估了所提出方法对城市景观的泛化能力。",
        "领域": "跨视角图像匹配/生成对抗网络/特征融合",
        "问题": "解决地面和航空图像之间的领域差距问题",
        "动机": "由于视角差异导致的跨视角图像中的视觉实体领域变化，需要一种方法来最小化这种差距，以提高图像匹配的准确性。",
        "方法": "利用条件生成对抗网络（GAN）生成地面全景的航空表示，通过联合特征学习方法融合合成航空图像和原始地面全景特征，采用多尺度特征聚合保留不同尺度的图像表示。",
        "关键词": [
            "跨视角图像匹配",
            "条件生成对抗网络",
            "特征融合",
            "多尺度特征聚合"
        ],
        "涉及的技术概念": "条件生成对抗网络（GAN）用于生成图像，联合特征学习方法用于融合不同来源的特征，多尺度特征聚合用于保留不同尺度的图像表示，以提高跨视角图像匹配的准确性和鲁棒性。"
    },
    {
        "order": 139,
        "title": "Proximal Mean-Field for Neural Network Quantization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ajanthan_Proximal_Mean-Field_for_Neural_Network_Quantization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ajanthan_Proximal_Mean-Field_for_Neural_Network_Quantization_ICCV_2019_paper.html",
        "abstract": "Compressing large Neural Networks (NN) by quantizing the parameters, while maintaining the performance is highly desirable due to reduced memory and time complexity. In this work, we cast NN quantization as a discrete labelling problem, and by examining relaxations, we design an efficient iterative optimization procedure that involves stochastic gradient descent followed by a projection. We prove that our simple projected gradient descent approach is, in fact, equivalent to a proximal version of the well-known mean-field method. These findings would allow the decades-old and theoretically grounded research on MRF optimization to be used to design better network quantization schemes. Our experiments on standard classification datasets (MNIST, CIFAR10/100, TinyImageNet) with convolutional and residual architectures show that our algorithm obtains fully-quantized networks with accuracies very close to the floating-point reference networks.",
        "中文标题": "近端均值场用于神经网络量化",
        "摘要翻译": "通过量化参数来压缩大型神经网络（NN），同时保持性能，由于减少了内存和时间复杂度，是非常可取的。在这项工作中，我们将NN量化视为一个离散标记问题，并通过检查松弛，设计了一个高效的迭代优化过程，该过程涉及随机梯度下降后跟一个投影。我们证明了我们简单的投影梯度下降方法实际上等同于著名的均值场方法的近端版本。这些发现将允许使用数十年来基于理论研究的MRF优化来设计更好的网络量化方案。我们在标准分类数据集（MNIST、CIFAR10/100、TinyImageNet）上使用卷积和残差架构进行的实验表明，我们的算法获得了与浮点参考网络精度非常接近的全量化网络。",
        "领域": "神经网络压缩/量化技术/优化算法",
        "问题": "如何在量化神经网络参数的同时保持其性能",
        "动机": "减少神经网络的内存和时间复杂度，同时保持其性能",
        "方法": "将神经网络量化视为离散标记问题，设计了一个高效的迭代优化过程，包括随机梯度下降和投影",
        "关键词": [
            "神经网络量化",
            "离散标记问题",
            "迭代优化",
            "投影梯度下降",
            "均值场方法"
        ],
        "涉及的技术概念": {
            "神经网络量化": "通过减少神经网络参数的精度来压缩网络，以减少内存和时间复杂度",
            "离散标记问题": "将量化问题转化为为每个参数分配离散标签的问题",
            "迭代优化": "通过重复应用优化步骤来逐步改进解决方案",
            "投影梯度下降": "一种优化技术，通过在每次迭代中将解投影到可行集上来更新解",
            "均值场方法": "一种用于近似复杂概率分布的统计物理方法，这里用于优化量化过程"
        }
    },
    {
        "order": 140,
        "title": "Global Feature Guided Local Pooling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kobayashi_Global_Feature_Guided_Local_Pooling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kobayashi_Global_Feature_Guided_Local_Pooling_ICCV_2019_paper.html",
        "abstract": "In deep convolutional neural networks (CNNs), local pooling operation is a key building block to effectively downsize feature maps for reducing computation cost as well as increasing robustness against input variation. There are several types of pooling operation, such as average/max-pooling, from which one has to be manually selected for building CNNs. The optimal pooling type would be dependent on characteristics of features in CNNs and classification tasks, making it hard to find out the proper pooling module in advance. In this paper, we propose a flexible pooling method which adaptively tunes the pooling functionality based on input features without manually fixing it beforehand. In the proposed method, the parameterized pooling form is derived from a probabilistic perspective to flexibly represent various types of pooling and then the parameters are estimated by means of global statistics in the input feature map. Thus, the proposed local pooling guided by global features effectively works in the CNNs trained in an end-to-end manner. The experimental results on image classification tasks demonstrate the effectiveness of the proposed pooling method in various deep CNNs.",
        "中文标题": "全局特征引导的局部池化",
        "摘要翻译": "在深度卷积神经网络（CNNs）中，局部池化操作是有效缩小特征图尺寸、降低计算成本以及增加对输入变化鲁棒性的关键构建块。存在几种类型的池化操作，如平均/最大池化，构建CNNs时必须手动选择其中一种。最优的池化类型取决于CNNs中特征的特性和分类任务，这使得事先找出合适的池化模块变得困难。在本文中，我们提出了一种灵活的池化方法，该方法根据输入特征自适应调整池化功能，而无需事先手动固定。在所提出的方法中，参数化的池化形式从概率角度导出，以灵活表示各种类型的池化，然后通过输入特征图中的全局统计量估计参数。因此，所提出的由全局特征引导的局部池化在端到端训练的CNNs中有效工作。图像分类任务的实验结果证明了所提出的池化方法在各种深度CNNs中的有效性。",
        "领域": "卷积神经网络/图像分类/特征提取",
        "问题": "如何自动选择最优的池化操作类型以适应不同的特征和分类任务",
        "动机": "由于最优池化类型依赖于特征特性和分类任务，手动选择池化操作类型既困难又不灵活",
        "方法": "提出了一种基于输入特征自适应调整池化功能的灵活池化方法，通过全局统计量估计参数，实现端到端训练",
        "关键词": [
            "卷积神经网络",
            "池化操作",
            "图像分类",
            "特征提取",
            "自适应调整"
        ],
        "涉及的技术概念": "深度卷积神经网络（CNNs）中的局部池化操作，包括平均池化和最大池化，以及如何通过全局特征统计量自适应调整池化功能，以提高图像分类任务的性能。"
    },
    {
        "order": 141,
        "title": "A Robust Learning Approach to Domain Adaptive Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Khodabandeh_A_Robust_Learning_Approach_to_Domain_Adaptive_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Khodabandeh_A_Robust_Learning_Approach_to_Domain_Adaptive_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.",
        "中文标题": "一种鲁棒学习方法用于领域自适应目标检测",
        "摘要翻译": "在目标检测的实际应用中，领域偏移是不可避免的。例如，在自动驾驶汽车中，目标领域包括无法在训练数据中全部观察到的无约束道路环境。同样，在监控应用中，由于隐私法规，可能缺乏足够代表性的训练数据。在本文中，我们从鲁棒学习的角度解决了领域适应问题，并表明该问题可以表述为使用噪声标签进行训练。我们提出了一种鲁棒的目标检测框架，该框架对边界框类别标签、位置和大小注释中的噪声具有弹性。为了适应领域偏移，模型在目标领域上使用一组噪声对象边界框进行训练，这些边界框是通过仅在源领域训练的检测模型获得的。我们在各种源/目标领域对中评估了我们方法的准确性，并证明该模型在SIM10K、Cityscapes和KITTI数据集上的多个领域适应场景中显著提高了最先进水平。",
        "领域": "自动驾驶/监控系统/目标检测",
        "问题": "解决目标检测中的领域适应问题，特别是在源领域和目标领域之间存在显著差异时。",
        "动机": "由于实际应用中领域偏移的不可避免性，如自动驾驶汽车和监控系统中的环境差异，需要一种能够适应这些变化的目标检测方法。",
        "方法": "提出了一种鲁棒的目标检测框架，该框架通过使用在源领域训练的检测模型生成的噪声对象边界框来训练目标领域的模型，从而提高对领域偏移的适应性。",
        "关键词": [
            "领域适应",
            "目标检测",
            "鲁棒学习",
            "噪声标签",
            "边界框"
        ],
        "涉及的技术概念": "领域偏移指的是源领域和目标领域之间的差异，这在实际应用中是一个常见问题。鲁棒学习是一种能够处理训练数据中噪声和异常值的学习方法。噪声标签指的是训练数据中可能包含错误的标签信息。边界框是用于目标检测中标记对象位置的矩形框。"
    },
    {
        "order": 142,
        "title": "Improving Adversarial Robustness via Guided Complement Entropy",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Improving_Adversarial_Robustness_via_Guided_Complement_Entropy_ICCV_2019_paper.html",
        "abstract": "Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can significantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving \"adversarial defense for free,\" which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like cross-entropy, we neutralize its probabilities on the incorrect classes along with a \"guided\" term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used cross-entropy training objective. We also show that our method can be used orthogonal to adversarial training across well-known methods with noticeable robustness gain. To the best of our knowledge, our approach is the first one that improves model robustness without compromising performance.",
        "中文标题": "通过引导互补熵提高对抗鲁棒性",
        "摘要翻译": "对抗鲁棒性已成为深度学习中的一个重要话题，因为精心制作的攻击样本可以显著干扰模型的性能。许多最近的方法提出通过利用对抗训练或模型蒸馏来提高对抗鲁棒性，这在模型训练中增加了额外的步骤。在本文中，我们提出了一种新的训练范式，称为引导互补熵（GCE），它能够实现“免费对抗防御”，在提高对抗鲁棒性的过程中不涉及额外的步骤。除了像交叉熵那样最大化模型在真实类别上的概率外，我们还中和了其在错误类别上的概率，并引入了一个“引导”项来平衡这两个项。我们在实验中展示了我们的方法在模型鲁棒性上取得了更好的效果，甚至比常用的交叉熵训练目标有更好的性能。我们还展示了我们的方法可以与著名的对抗训练方法正交使用，获得显著的鲁棒性提升。据我们所知，我们的方法是第一个在不影响性能的情况下提高模型鲁棒性的方法。",
        "领域": "对抗学习/模型鲁棒性/深度学习训练",
        "问题": "提高深度学习模型在对抗攻击下的鲁棒性",
        "动机": "现有的提高模型对抗鲁棒性的方法通常需要在模型训练过程中增加额外的步骤，这增加了训练的复杂性和成本。",
        "方法": "提出了一种新的训练范式——引导互补熵（GCE），通过中和模型在错误类别上的概率并引入一个引导项来平衡，实现无需额外步骤的对抗防御。",
        "关键词": [
            "对抗鲁棒性",
            "引导互补熵",
            "模型训练"
        ],
        "涉及的技术概念": "对抗鲁棒性指的是模型在面对精心设计的对抗样本时保持性能的能力。引导互补熵（GCE）是一种新的训练目标，旨在通过平衡模型在正确和错误类别上的概率来提高模型的鲁棒性，而无需增加额外的训练步骤。"
    },
    {
        "order": 143,
        "title": "Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Conditional_Coupled_Generative_Adversarial_Networks_for_Zero-Shot_Domain_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Conditional_Coupled_Generative_Adversarial_Networks_for_Zero-Shot_Domain_Adaptation_ICCV_2019_paper.html",
        "abstract": "Machine learning models trained in one domain perform poorly in the other domains due to the existence of domain shift. Domain adaptation techniques solve this problem by training transferable models from the label-rich source domain to the label-scarce target domain. Unfortunately, a majority of the existing domain adaptation techniques rely on the availability of the target-domain data, and thus limit their applications to a small community across few computer vision problems. In this paper, we tackle the challenging zero-shot domain adaptation (ZSDA) problem, where the target-domain data is non-available in the training stage. For this purpose, we propose conditional coupled generative adversarial networks (CoCoGAN) by extending the coupled generative adversarial networks (CoGAN) into a conditioning model. Compared with the existing state of the arts, our proposed CoCoGAN is able to capture the joint distribution of dual-domain samples in two different tasks, i.e. the relevant task (RT) and an irrelevant task (IRT). We train the CoCoGAN with both source-domain samples in RT and the dual-domain samples in IRT to complete the domain adaptation. While the former provide the high-level concepts of the non-available target-domain data, the latter carry the sharing correlation between the two domains in RT and IRT. To train the CoCoGAN in the absence of the target-domain data for RT, we propose a new supervisory signal, i.e. the alignment between representations across tasks. Extensive experiments carried out demonstrate that our proposed CoCoGAN outperforms existing state of the arts in image classifications.",
        "中文标题": "条件耦合生成对抗网络用于零样本域适应",
        "摘要翻译": "由于域偏移的存在，在一个域中训练的机器学习模型在其他域中表现不佳。域适应技术通过从标签丰富的源域训练可转移模型到标签稀缺的目标域来解决这个问题。不幸的是，大多数现有的域适应技术依赖于目标域数据的可用性，从而限制了它们在少数计算机视觉问题中的应用。在本文中，我们解决了具有挑战性的零样本域适应（ZSDA）问题，其中在训练阶段目标域数据不可用。为此，我们通过将耦合生成对抗网络（CoGAN）扩展为条件模型，提出了条件耦合生成对抗网络（CoCoGAN）。与现有的最先进技术相比，我们提出的CoCoGAN能够捕捉两个不同任务中双域样本的联合分布，即相关任务（RT）和不相关任务（IRT）。我们使用RT中的源域样本和IRT中的双域样本训练CoCoGAN以完成域适应。前者提供了不可用目标域数据的高级概念，而后者则携带了RT和IRT中两个域之间的共享相关性。为了在RT中缺乏目标域数据的情况下训练CoCoGAN，我们提出了一种新的监督信号，即跨任务表示之间的对齐。进行的大量实验表明，我们提出的CoCoGAN在图像分类方面优于现有的最先进技术。",
        "领域": "域适应/生成对抗网络/零样本学习",
        "问题": "解决在训练阶段目标域数据不可用的零样本域适应问题",
        "动机": "现有的域适应技术大多依赖于目标域数据的可用性，限制了其在计算机视觉问题中的应用范围",
        "方法": "提出条件耦合生成对抗网络（CoCoGAN），通过扩展耦合生成对抗网络（CoGAN）为条件模型，捕捉双域样本的联合分布，并使用源域样本和双域样本训练模型完成域适应",
        "关键词": [
            "域适应",
            "生成对抗网络",
            "零样本学习"
        ],
        "涉及的技术概念": "域适应技术通过从标签丰富的源域训练可转移模型到标签稀缺的目标域来解决域偏移问题。条件耦合生成对抗网络（CoCoGAN）通过捕捉两个不同任务中双域样本的联合分布，使用源域样本和双域样本训练模型完成域适应。在缺乏目标域数据的情况下，通过跨任务表示之间的对齐作为新的监督信号来训练模型。"
    },
    {
        "order": 144,
        "title": "Graph-Based Object Classification for Neuromorphic Vision Sensing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Graph-Based_Object_Classification_for_Neuromorphic_Vision_Sensing_ICCV_2019_paper.html",
        "abstract": "Neuromorphic vision sensing (NVS) devices represent visual information as sequences of asynchronous discrete events (a.k.a., \"spikes'\") in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, object classification with NVS streams cannot leverage on state-of-the-art convolutional neural networks (CNNs), since NVS does not produce frame representations. To circumvent this mismatch between sensing and processing with CNNs, we propose a compact graph representation for NVS. We couple this with novel residual graph CNN architectures and show that, when trained on spatio-temporal NVS data for object classification, such residual graph CNNs preserve the spatial and temporal coherence of spike events, while requiring less computation and memory. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we present and make available a 100k dataset of NVS recordings of the American sign language letters, acquired with an iniLabs DAVIS240c device under real-world conditions.",
        "中文标题": "基于图的神经形态视觉传感对象分类",
        "摘要翻译": "神经形态视觉传感（NVS）设备将视觉信息表示为响应场景反射率变化的异步离散事件序列（也称为“尖峰”）。与传统的主动像素传感（APS）不同，NVS允许在显著提高的能量效率和对照明变化的鲁棒性下实现更高的事件采样率。然而，由于NVS不产生帧表示，因此无法利用最先进的卷积神经网络（CNNs）进行NVS流的对象分类。为了解决这种传感与CNN处理之间的不匹配，我们提出了一种紧凑的NVS图表示。我们将其与新颖的残差图CNN架构相结合，并表明，当在时空NVS数据上进行对象分类训练时，这种残差图CNN保留了尖峰事件的空间和时间一致性，同时需要更少的计算和内存。最后，为了解决复杂识别任务中缺乏大型真实世界NVS数据集的问题，我们提出并提供了一个100k的NVS记录数据集，该数据集是在真实世界条件下使用iniLabs DAVIS240c设备获取的美国手语字母。",
        "领域": "神经形态视觉传感/对象分类/图神经网络",
        "问题": "神经形态视觉传感（NVS）设备产生的数据无法直接利用现有的卷积神经网络（CNNs）进行对象分类",
        "动机": "解决NVS数据与CNN处理之间的不匹配问题，提高对象分类的效率和准确性",
        "方法": "提出一种紧凑的NVS图表示，并结合新颖的残差图CNN架构进行对象分类",
        "关键词": [
            "神经形态视觉传感",
            "对象分类",
            "图神经网络",
            "残差图CNN"
        ],
        "涉及的技术概念": "神经形态视觉传感（NVS）是一种模拟生物视觉系统处理视觉信息的技术，它通过异步离散事件（尖峰）来表示视觉信息。卷积神经网络（CNNs）是一种深度学习模型，广泛用于图像识别和分类任务。图神经网络是一种处理图结构数据的神经网络，能够捕捉数据中的复杂关系。残差图CNN是一种改进的图神经网络架构，通过引入残差连接来提高网络的性能和稳定性。"
    },
    {
        "order": 145,
        "title": "A Geometry-Inspired Decision-Based Attack",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_A_Geometry-Inspired_Decision-Based_Attack_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_A_Geometry-Inspired_Decision-Based_Attack_ICCV_2019_paper.html",
        "abstract": "Deep neural networks have recently achieved tremendous success in image classification. Recent studies have however shown that they are easily misled into incorrect classification decisions by adversarial examples. Adversaries can even craft attacks by querying the model in black-box settings, where no information about the model is released except its final decision. Such decision-based attacks usually require lots of queries, while real-world image recognition systems might actually restrict the number of queries. In this paper, we propose qFool, a novel decision-based attack algorithm that can generate adversarial examples using a small number of queries. The qFool method can drastically reduce the number of queries compared to previous decision-based attacks while reaching the same quality of adversarial examples. We also enhance our method by constraining adversarial perturbations in low-frequency subspace, which can make qFool even more computationally efficient. Altogether, we manage to fool commercial image recognition systems with a small number of queries, which demonstrates the actual effectiveness of our new algorithm in practice.",
        "中文标题": "一种几何启发的基于决策的攻击",
        "摘要翻译": "深度神经网络最近在图像分类方面取得了巨大的成功。然而，最近的研究表明，它们很容易被对抗性示例误导，做出错误的分类决策。攻击者甚至可以在黑盒设置下通过查询模型来制作攻击，其中除了模型的最终决策外，不发布任何关于模型的信息。这种基于决策的攻击通常需要大量的查询，而现实世界的图像识别系统实际上可能会限制查询的数量。在本文中，我们提出了qFool，一种新颖的基于决策的攻击算法，可以使用少量查询生成对抗性示例。与之前的基于决策的攻击相比，qFool方法可以大幅减少查询数量，同时达到相同质量的对抗性示例。我们还通过在低频率子空间中约束对抗性扰动来增强我们的方法，这可以使qFool在计算上更加高效。总之，我们设法用少量查询欺骗了商业图像识别系统，这证明了我们新算法在实际中的有效性。",
        "领域": "对抗性攻击/图像识别/网络安全",
        "问题": "减少基于决策的对抗性攻击所需的查询数量",
        "动机": "现实世界的图像识别系统可能会限制查询的数量，因此需要一种能够在少量查询下生成高质量对抗性示例的方法。",
        "方法": "提出了一种名为qFool的新算法，通过在低频率子空间中约束对抗性扰动，减少生成对抗性示例所需的查询数量。",
        "关键词": [
            "对抗性攻击",
            "图像识别",
            "网络安全",
            "低频率子空间",
            "qFool算法"
        ],
        "涉及的技术概念": {
            "对抗性示例": "一种特殊设计的输入，旨在欺骗机器学习模型，使其做出错误的预测或分类。",
            "黑盒设置": "攻击者无法访问模型的内部参数或结构，只能通过输入和输出来与模型交互。",
            "低频率子空间": "在图像处理中，低频率子空间通常指的是图像中变化较慢的部分，这些部分对图像的总体结构有较大影响。",
            "qFool算法": "一种新颖的基于决策的攻击算法，旨在减少生成对抗性示例所需的查询数量。"
        }
    },
    {
        "order": 146,
        "title": "Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_Adversarial_Defense_by_Restricting_the_Hidden_Space_of_Deep_Neural_ICCV_2019_paper.html",
        "abstract": "Deep neural networks are vulnerable to adversarial attacks which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Specifically, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the art defenses.",
        "中文标题": "通过限制深度神经网络的隐藏空间进行对抗防御",
        "摘要翻译": "深度神经网络容易受到对抗攻击的影响，这些攻击通过向输入图像添加微小的扰动来欺骗网络。现有防御的鲁棒性在白盒攻击设置下大大降低，在这种设置下，攻击者拥有关于网络的完整知识，并且可以多次迭代以找到强扰动。我们观察到，这种扰动存在的主要原因是学习到的特征空间中不同类别样本的接近性。这使得通过向输入添加不可察觉的扰动可以完全改变模型的决策。为了应对这一点，我们提出按类别分离深度网络的中间特征表示。具体来说，我们强制每个类别的特征位于一个凸多面体内，该多面体与其他类别的多面体最大程度地分离。通过这种方式，网络被迫为每个类别学习独特且遥远的决策区域。我们观察到，这种对特征的简单约束大大增强了学习模型的鲁棒性，即使面对最强的白盒攻击，也不会降低在干净图像上的分类性能。我们在黑盒和白盒攻击场景中进行了广泛的评估，并显示出与最先进的防御相比的显著增益。",
        "领域": "对抗性防御/特征表示/鲁棒性",
        "问题": "深度神经网络在面对对抗攻击时的脆弱性问题",
        "动机": "提高深度神经网络对抗攻击的鲁棒性，特别是在白盒攻击设置下",
        "方法": "通过按类别分离深度网络的中间特征表示，强制每个类别的特征位于一个凸多面体内，该多面体与其他类别的多面体最大程度地分离",
        "关键词": [
            "对抗性防御",
            "特征表示",
            "鲁棒性",
            "白盒攻击",
            "黑盒攻击"
        ],
        "涉及的技术概念": "对抗攻击、白盒攻击、黑盒攻击、特征空间、凸多面体、决策区域"
    },
    {
        "order": 147,
        "title": "Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_ICCV_2019_paper.html",
        "abstract": "The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.",
        "中文标题": "高斯YOLOv3：利用定位不确定性实现自动驾驶的准确快速目标检测器",
        "摘要翻译": "在自动驾驶车辆中，目标检测算法的使用变得越来越重要，高精度和快速推理速度的目标检测对于安全的自动驾驶至关重要。自动驾驶过程中由于错误定位导致的误报（FP）可能会导致致命事故，阻碍安全高效的驾驶。因此，在自动驾驶应用中需要一种能够应对错误定位的检测算法。本文提出了一种方法，通过将YOLOv3（一阶段检测器中最具代表性的）的边界框（bbox）建模为高斯参数并重新设计损失函数，以提高检测精度同时支持实时操作。此外，本文还提出了一种预测定位不确定性的方法，该不确定性指示了bbox的可靠性。通过在检测过程中使用预测的定位不确定性，所提出的方案可以显著减少FP并增加真正例（TP），从而提高准确性。与传统的YOLOv3相比，所提出的算法高斯YOLOv3在KITTI和Berkeley deep drive（BDD）数据集上的平均精度（mAP）分别提高了3.09和3.5。尽管如此，所提出的算法能够以超过42帧每秒（fps）的速度进行实时检测，并且显示出比具有相似fps的先前方法更高的准确性。因此，所提出的算法最适合自动驾驶应用。",
        "领域": "自动驾驶/目标检测/实时系统",
        "问题": "自动驾驶中目标检测的误报问题",
        "动机": "提高自动驾驶车辆中目标检测的准确性和速度，减少误报，确保安全驾驶",
        "方法": "通过将YOLOv3的边界框建模为高斯参数并重新设计损失函数，以及预测定位不确定性来提高检测精度和速度",
        "关键词": [
            "自动驾驶",
            "目标检测",
            "实时系统",
            "高斯参数",
            "定位不确定性"
        ],
        "涉及的技术概念": {
            "YOLOv3": "一种一阶段目标检测算法，以其快速和高效著称",
            "高斯参数": "用于建模边界框的不确定性，提高检测的准确性",
            "定位不确定性": "指示边界框可靠性的度量，用于减少误报",
            "平均精度（mAP）": "评估目标检测算法性能的指标，表示检测的准确性",
            "帧每秒（fps）": "衡量检测速度的指标，表示每秒可以处理的帧数"
        }
    },
    {
        "order": 148,
        "title": "Spatial Correspondence With Generative Adversarial Network: Learning Depth From Monocular Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Spatial_Correspondence_With_Generative_Adversarial_Network_Learning_Depth_From_Monocular_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Spatial_Correspondence_With_Generative_Adversarial_Network_Learning_Depth_From_Monocular_ICCV_2019_paper.html",
        "abstract": "Depth estimation from monocular videos has important applications in many areas such as autonomous driving and robot navigation. It is a very challenging problem without knowing the camera pose since errors in camera-pose estimation can significantly affect the video-based depth estimation accuracy. In this paper, we present a novel SC-GAN network with end-to-end adversarial training for depth estimation from monocular videos without estimating the camera pose and pose change over time. To exploit cross-frame relations, SC-GAN includes a spatial correspondence module which uses Smolyak sparse grids to efficiently match the features across adjacent frames, and an attention mechanism to learn the importance of features in different directions. Furthermore, the generator in SC-GAN learns to estimate depth from the input frames, while the discriminator learns to distinguish between the ground-truth and estimated depth map for the reference frame. Experiments on the KITTI and Cityscapes datasets show that the proposed SC-GAN can achieve much more accurate depth maps than many existing state-of-the-art methods on monocular videos.",
        "中文标题": "使用生成对抗网络的空间对应：从单目视频中学习深度",
        "摘要翻译": "从单目视频中进行深度估计在自动驾驶和机器人导航等多个领域具有重要应用。这是一个非常具有挑战性的问题，因为不知道相机姿态，相机姿态估计中的错误会显著影响基于视频的深度估计准确性。在本文中，我们提出了一种新颖的SC-GAN网络，通过端到端的对抗训练，从单目视频中进行深度估计，而无需估计相机姿态和随时间变化的姿态变化。为了利用跨帧关系，SC-GAN包括一个空间对应模块，该模块使用Smolyak稀疏网格有效地匹配相邻帧之间的特征，并使用注意力机制学习不同方向特征的重要性。此外，SC-GAN中的生成器学习从输入帧中估计深度，而判别器学习区分参考帧的真实深度图和估计深度图。在KITTI和Cityscapes数据集上的实验表明，所提出的SC-GAN可以在单目视频上实现比许多现有最先进方法更准确的深度图。",
        "领域": "自动驾驶/机器人导航/深度估计",
        "问题": "从单目视频中进行深度估计，无需估计相机姿态和随时间变化的姿态变化",
        "动机": "解决在不知道相机姿态的情况下，从单目视频中进行深度估计的挑战性问题",
        "方法": "提出SC-GAN网络，通过端到端的对抗训练，利用空间对应模块和注意力机制，从单目视频中估计深度",
        "关键词": [
            "深度估计",
            "单目视频",
            "生成对抗网络",
            "空间对应",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "SC-GAN": "一种新颖的生成对抗网络，用于从单目视频中进行深度估计",
            "Smolyak稀疏网格": "用于有效地匹配相邻帧之间的特征",
            "注意力机制": "用于学习不同方向特征的重要性",
            "端到端的对抗训练": "一种训练方法，使生成器学习估计深度，判别器学习区分真实和估计的深度图"
        }
    },
    {
        "order": 149,
        "title": "Universal Perturbation Attack Against Image Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Universal_Perturbation_Attack_Against_Image_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Universal_Perturbation_Attack_Against_Image_Retrieval_ICCV_2019_paper.html",
        "abstract": "Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.",
        "中文标题": "通用扰动攻击对抗图像检索",
        "摘要翻译": "通用对抗扰动（UAPs），又称输入无关扰动，已被证明存在并能够欺骗大多数数据样本上的尖端深度学习模型。现有的UAP方法主要集中于攻击图像分类模型。然而，对于攻击图像检索系统的关注却很少。在本文中，我们首次尝试攻击图像检索系统。具体来说，图像检索攻击是使检索系统在排名列表的顶部返回与查询无关的图像。在图像检索攻击中，破坏特征之间的邻域关系起着重要作用。为此，我们提出了一种新方法，通过降低相应的排名度量来生成对抗检索的UAP，以打破图像特征的邻域关系。为了将攻击方法扩展到具有不同输入大小或不可接触网络参数的场景，提出了多尺度随机调整大小方案和排名蒸馏策略。我们在四个广泛使用的图像检索数据集上评估了所提出的方法，并报告了在不同度量标准（如mAP和mP@10）下的显著性能下降。最后，我们在现实世界的视觉搜索引擎（即Google Images）上测试了我们的攻击方法，这证明了我们方法的实际潜力。",
        "领域": "图像检索/对抗攻击/深度学习",
        "问题": "如何有效地攻击图像检索系统，使其返回与查询无关的图像",
        "动机": "现有的通用对抗扰动方法主要集中于攻击图像分类模型，而对图像检索系统的攻击研究较少，本文旨在填补这一空白",
        "方法": "提出了一种新方法，通过降低相应的排名度量来生成对抗检索的通用对抗扰动，以打破图像特征的邻域关系，并提出了多尺度随机调整大小方案和排名蒸馏策略以适应不同场景",
        "关键词": [
            "图像检索",
            "对抗攻击",
            "通用对抗扰动",
            "排名度量",
            "邻域关系"
        ],
        "涉及的技术概念": "通用对抗扰动（UAPs）是一种输入无关的扰动，能够欺骗深度学习模型；图像检索攻击旨在使检索系统返回与查询无关的图像；多尺度随机调整大小方案和排名蒸馏策略是为了适应不同输入大小或不可接触网络参数的场景。"
    },
    {
        "order": 150,
        "title": "Hyperpixel Flow: Semantic Correspondence With Multi-Layer Neural Features",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Min_Hyperpixel_Flow_Semantic_Correspondence_With_Multi-Layer_Neural_Features_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Min_Hyperpixel_Flow_Semantic_Correspondence_With_Multi-Layer_Neural_Features_ICCV_2019_paper.html",
        "abstract": "Establishing visual correspondences under large intra-class variations requires analyzing images at different levels, from features linked to semantics and context to local patterns, while being invariant to instance-specific details. To tackle these challenges, we represent images by \"hyperpixels\" that leverage a small number of relevant features selected among early to late layers of a convolutional neural network. Taking advantage of the condensed features of hyperpixels, we develop an effective real-time matching algorithm based on Hough geometric voting. The proposed method, hyperpixel flow, sets a new state of the art on three standard benchmarks as well as a new dataset, SPair-71k, which contains a significantly larger number of image pairs than existing datasets, with more accurate and richer annotations for in-depth analysis.",
        "中文标题": "超像素流：利用多层神经特征的语义对应",
        "摘要翻译": "在大类内变化下建立视觉对应关系需要从与语义和上下文相关的特征到局部模式的不同层次分析图像，同时对实例特定的细节保持不变。为了应对这些挑战，我们通过“超像素”来表示图像，这些超像素利用了从卷积神经网络的早期到晚期层中选择的少量相关特征。利用超像素的浓缩特征，我们开发了一种基于霍夫几何投票的有效实时匹配算法。所提出的方法，超像素流，在三个标准基准以及一个新数据集SPair-71k上设定了新的技术状态，该数据集包含比现有数据集显著更多的图像对，具有更准确和更丰富的注释以进行深入分析。",
        "领域": "语义对应/图像匹配/卷积神经网络",
        "问题": "在大类内变化下建立视觉对应关系",
        "动机": "需要从不同层次分析图像，同时对实例特定的细节保持不变，以建立有效的视觉对应关系",
        "方法": "利用从卷积神经网络的早期到晚期层中选择的少量相关特征表示图像，开发基于霍夫几何投票的实时匹配算法",
        "关键词": [
            "超像素",
            "语义对应",
            "霍夫几何投票",
            "图像匹配"
        ],
        "涉及的技术概念": {
            "超像素": "利用卷积神经网络多层特征选择的少量相关特征表示的图像单元",
            "霍夫几何投票": "一种基于几何形状的投票机制，用于图像匹配和对应关系建立",
            "卷积神经网络": "一种深度学习模型，特别适用于处理图像数据，通过多层卷积操作提取特征"
        }
    },
    {
        "order": 151,
        "title": "Sharpen Focus: Learning With Attention Separability and Consistency",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Sharpen_Focus_Learning_With_Attention_Separability_and_Consistency_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Sharpen_Focus_Learning_With_Attention_Separability_and_Consistency_ICCV_2019_paper.html",
        "abstract": "Recent developments in gradient-based attention modeling have seen attention maps emerge as a powerful tool for interpreting convolutional neural networks. Despite good localization for an individual class of interest, these techniques produce attention maps with substantially overlapping responses among different classes, leading to the problem of visual confusion and the need for discriminative attention. In this paper, we address this problem by means of a new framework that makes class-discriminative attention a principled part of the learning process. Our key innovations include new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual confusion. Extensive experiments on image classification benchmarks show the effectiveness of our approach in terms of improved classification accuracy, including CIFAR-100 (+3.33%), Caltech-256 (+1.64%), ImageNet (+0.92%), CUB-200-2011 (+4.8%) and PASCAL VOC2012 (+5.73%).",
        "中文标题": "锐化焦点：通过注意力可分离性和一致性学习",
        "摘要翻译": "近年来，基于梯度的注意力建模发展迅速，注意力图已成为解释卷积神经网络的有力工具。尽管对于单个感兴趣类别具有良好的定位能力，这些技术产生的注意力图在不同类别间存在大量重叠响应，导致视觉混淆问题和需要区分性注意力。在本文中，我们通过一个新框架解决了这个问题，该框架使类别区分性注意力成为学习过程的一个原则性部分。我们的关键创新包括新的学习目标，用于注意力可分离性和跨层一致性，从而提高了注意力的区分性并减少了视觉混淆。在图像分类基准上的大量实验表明，我们的方法在提高分类准确性方面有效，包括CIFAR-100（+3.33%）、Caltech-256（+1.64%）、ImageNet（+0.92%）、CUB-200-2011（+4.8%）和PASCAL VOC2012（+5.73%）。",
        "领域": "图像分类/注意力机制/卷积神经网络",
        "问题": "解决不同类别间注意力图响应重叠导致的视觉混淆问题",
        "动机": "提高卷积神经网络中注意力图的区分性，减少视觉混淆",
        "方法": "提出一个新的框架，通过引入注意力可分离性和跨层一致性的学习目标，使类别区分性注意力成为学习过程的一部分",
        "关键词": [
            "注意力机制",
            "图像分类",
            "卷积神经网络"
        ],
        "涉及的技术概念": "注意力图：用于解释卷积神经网络，显示网络在处理图像时关注的区域。注意力可分离性：指注意力图能够清晰地区分不同类别的能力。跨层一致性：指网络不同层之间的注意力图保持一致，以提高模型的解释性和性能。"
    },
    {
        "order": 152,
        "title": "Information Entropy Based Feature Pooling for Convolutional Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Information_Entropy_Based_Feature_Pooling_for_Convolutional_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Information_Entropy_Based_Feature_Pooling_for_Convolutional_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "In convolutional neural networks (CNNs), we propose to estimate the importance of a feature vector at a spatial location in the feature maps by the network's uncertainty on its class prediction, which can be quantified using the information entropy. Based on this idea, we propose the entropy-based feature weighting method for semantics-aware feature pooling which can be readily integrated into various CNN architectures for both training and inference. We demonstrate that such a location-adaptive feature weighting mechanism helps the network to concentrate on semantically important image regions, leading to improvements in the large-scale classification and weakly-supervised semantic segmentation tasks. Furthermore, the generated feature weights can be utilized in visual tasks such as weakly-supervised object localization. We conduct extensive experiments on different datasets and CNN architectures, outperforming recently proposed pooling methods and attention mechanisms in ImageNet classification as well as achieving state-of-the-arts in weakly-supervised semantic segmentation on PASCAL VOC 2012 dataset.",
        "中文标题": "基于信息熵的特征池化用于卷积神经网络",
        "摘要翻译": "在卷积神经网络（CNNs）中，我们提出通过网络对其类别预测的不确定性来估计特征图中空间位置特征向量的重要性，这种不确定性可以通过信息熵来量化。基于这一想法，我们提出了基于熵的特征加权方法，用于语义感知的特征池化，这种方法可以轻松集成到各种CNN架构中，用于训练和推理。我们证明了这种位置自适应的特征加权机制有助于网络集中关注语义上重要的图像区域，从而在大规模分类和弱监督语义分割任务中带来改进。此外，生成的特征权重可以用于视觉任务，如弱监督对象定位。我们在不同的数据集和CNN架构上进行了广泛的实验，在ImageNet分类中超越了最近提出的池化方法和注意力机制，并在PASCAL VOC 2012数据集上的弱监督语义分割中达到了最先进的水平。",
        "领域": "卷积神经网络/语义分割/对象定位",
        "问题": "如何有效地估计特征图中空间位置特征向量的重要性，以改进大规模分类和弱监督语义分割任务",
        "动机": "提高卷积神经网络在语义感知任务中的性能，通过集中关注语义上重要的图像区域",
        "方法": "提出基于信息熵的特征加权方法，用于语义感知的特征池化，并集成到各种CNN架构中",
        "关键词": [
            "信息熵",
            "特征池化",
            "语义分割",
            "对象定位"
        ],
        "涉及的技术概念": "信息熵用于量化网络对类别预测的不确定性，基于熵的特征加权方法用于语义感知的特征池化，位置自适应的特征加权机制帮助网络集中关注语义上重要的图像区域"
    },
    {
        "order": 153,
        "title": "Bayesian Optimized 1-Bit CNNs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Bayesian_Optimized_1-Bit_CNNs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Bayesian_Optimized_1-Bit_CNNs_ICCV_2019_paper.html",
        "abstract": "Deep convolutional neural networks (DCNNs) have dominated the recent developments in computer vision through making various record-breaking models. However, it is still a great challenge to achieve powerful DCNNs in resource-limited environments, such as on embedded devices and smart phones. Researchers have realized that 1-bit CNNs can be one feasible solution to resolve the issue; however, they are baffled by the inferior performance compared to the full-precision DCNNs. In this paper, we propose a novel approach, called Bayesian optimized 1-bit CNNs (denoted as BONNs), taking the advantage of Bayesian learning, a well-established strategy for hard problems, to significantly improve the performance of extreme 1-bit CNNs. We incorporate the prior distributions of full-precision kernels and features into the Bayesian framework to construct 1-bit CNNs in an end-to-end manner, which have not been considered in any previous related methods. The Bayesian losses are achieved with a theoretical support to optimize the network simultaneously in both continuous and discrete spaces, aggregating different losses jointly to improve the model capacity. Extensive experiments on the ImageNet and CIFAR datasets show that BONNs achieve the best classification performance compared to state-of-the-art 1-bit CNNs.",
        "中文标题": "贝叶斯优化的1位卷积神经网络",
        "摘要翻译": "深度卷积神经网络（DCNNs）通过创造各种破纪录的模型，主导了计算机视觉领域的最新发展。然而，在资源有限的环境中，如嵌入式设备和智能手机上，实现强大的DCNNs仍然是一个巨大的挑战。研究人员已经意识到，1位CNNs可能是解决这一问题的一个可行方案；然而，与全精度DCNNs相比，它们的性能较差，这让人感到困惑。在本文中，我们提出了一种新方法，称为贝叶斯优化的1位CNNs（简称BONNs），利用贝叶斯学习的优势，这是一种解决难题的成熟策略，以显著提高极端1位CNNs的性能。我们将全精度核和特征的先验分布纳入贝叶斯框架，以端到端的方式构建1位CNNs，这在任何先前的相关方法中都没有考虑过。贝叶斯损失是在理论支持下实现的，以同时在连续和离散空间中优化网络，联合聚合不同的损失以提高模型能力。在ImageNet和CIFAR数据集上的大量实验表明，与最先进的1位CNNs相比，BONNs实现了最佳的分类性能。",
        "领域": "卷积神经网络优化/嵌入式系统/贝叶斯学习",
        "问题": "在资源有限的环境中实现强大的深度卷积神经网络",
        "动机": "解决1位CNNs与全精度DCNNs相比性能较差的问题",
        "方法": "利用贝叶斯学习，将全精度核和特征的先验分布纳入贝叶斯框架，以端到端的方式构建1位CNNs",
        "关键词": [
            "1位卷积神经网络",
            "贝叶斯优化",
            "嵌入式设备"
        ],
        "涉及的技术概念": "贝叶斯学习是一种统计学习方法，用于估计概率模型的参数。1位卷积神经网络是一种网络结构，其权重和激活值被量化为1位，以减少计算和存储需求。嵌入式设备是指嵌入到其他设备中的计算机系统，通常资源有限。"
    },
    {
        "order": 154,
        "title": "Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Semantic-Specific_Graph_Representation_for_Multi-Label_Image_Recognition_ICCV_2019_paper.html",
        "abstract": "Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.",
        "中文标题": "学习语义特定图表示用于多标签图像识别",
        "摘要翻译": "识别图像的多标签是一个实际且具有挑战性的任务，通过搜索语义感知区域和建模标签依赖性已经取得了显著进展。然而，由于缺乏部分级别的监督或语义指导，当前的方法无法准确定位语义区域。此外，它们不能充分探索语义区域之间的相互影响，也没有明确地建模标签共现。为了解决这些问题，我们提出了一个语义特定图表示学习（SSGRL）框架，该框架由两个关键模块组成：1）一个语义解耦模块，它结合类别语义来指导学习语义特定的表示；2）一个语义交互模块，它将这些表示与基于统计标签共现构建的图相关联，并通过图传播机制探索它们的交互。在公共基准上的大量实验表明，我们的SSGRL框架在PASCAL VOC 2007 & 2012、Microsoft-COCO和Visual Genome基准上分别以2.5%、2.6%、6.7%和3.1%的mAP改进显著优于当前的最先进方法。我们的代码和模型可在https://github.com/HCPLab-SYSU/SSGRL获取。",
        "领域": "多标签图像识别/语义理解/图表示学习",
        "问题": "当前方法在多标签图像识别中无法准确定位语义区域，且不能充分探索语义区域之间的相互影响和标签共现。",
        "动机": "提高多标签图像识别的准确性，通过更准确地定位语义区域和探索语义区域之间的相互影响及标签共现。",
        "方法": "提出了一个语义特定图表示学习（SSGRL）框架，包括语义解耦模块和语义交互模块，通过图传播机制探索语义区域之间的交互。",
        "关键词": [
            "多标签图像识别",
            "语义特定图表示",
            "标签共现"
        ],
        "涉及的技术概念": "语义解耦模块结合类别语义指导学习语义特定的表示；语义交互模块通过图传播机制探索语义区域之间的交互，图是基于统计标签共现构建的。"
    },
    {
        "order": 155,
        "title": "Patchwork: A Patch-Wise Attention Network for Efficient Object Detection and Segmentation in Video Streams",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chai_Patchwork_A_Patch-Wise_Attention_Network_for_Efficient_Object_Detection_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chai_Patchwork_A_Patch-Wise_Attention_Network_for_Efficient_Object_Detection_and_ICCV_2019_paper.html",
        "abstract": "Recent advances in single-frame object detection and segmentation techniques have motivated a wide range of works to extend these methods to process video streams. In this paper, we explore the idea of hard attention aimed for latency-sensitive applications. Instead of reasoning about every frame separately, our method selects and only processes a small sub-window of the frame. Our technique then makes predictions for the full frame based on the sub-windows from previous frames and the update from the current sub-window. The latency reduction by this hard attention mechanism comes at the cost of degraded accuracy. We made two contributions to address this. First, we propose a specialized memory cell that recovers lost context when processing sub-windows. Secondly, we adopt a Q-learning-based policy training strategy that enables our approach to intelligently select the sub-windows such that the staleness in the memory hurts the performance the least. Our experiments suggest that our approach reduces the latency by approximately four times without significantly sacrificing the accuracy on the ImageNet VID video object detection dataset and the DAVIS video object segmentation dataset. We further demonstrate that we can reinvest the saved computation into other parts of the network, and thus resulting in an accuracy increase at a comparable computational cost as the original system and beating other recently proposed state-of-the-art methods in the low latency range.",
        "中文标题": "拼图：一种用于视频流中高效对象检测和分割的逐块注意力网络",
        "摘要翻译": "近年来，单帧对象检测和分割技术的进步激励了广泛的研究工作将这些方法扩展到处理视频流。在本文中，我们探索了针对延迟敏感应用的硬注意力机制。我们的方法不是单独处理每一帧，而是选择并仅处理帧的一个小子窗口。然后，我们的技术基于先前帧的子窗口和当前子窗口的更新，对完整帧进行预测。这种硬注意力机制带来的延迟减少是以准确度下降为代价的。我们做出了两项贡献来解决这个问题。首先，我们提出了一种专门的记忆单元，用于在处理子窗口时恢复丢失的上下文。其次，我们采用了一种基于Q学习的策略训练策略，使我们的方法能够智能地选择子窗口，从而使得记忆中的陈旧性对性能的影响最小。我们的实验表明，我们的方法在ImageNet VID视频对象检测数据集和DAVIS视频对象分割数据集上，将延迟减少了大约四倍，而没有显著牺牲准确度。我们进一步证明，我们可以将节省的计算资源重新投资到网络的其他部分，从而在可比较的计算成本下提高准确度，并在低延迟范围内击败其他最近提出的最先进方法。",
        "领域": "视频对象检测/视频对象分割/注意力机制",
        "问题": "在视频流中实现高效的对象检测和分割，同时减少处理延迟",
        "动机": "针对延迟敏感的应用，探索硬注意力机制以减少处理每一帧的计算需求",
        "方法": "提出一种逐块注意力网络，通过选择和处理帧的子窗口来减少延迟，并采用专门的记忆单元和基于Q学习的策略训练策略来恢复丢失的上下文和智能选择子窗口",
        "关键词": [
            "硬注意力机制",
            "延迟减少",
            "Q学习策略"
        ],
        "涉及的技术概念": "硬注意力机制是一种选择性地处理输入数据的一部分以减少计算需求的技术。Q学习是一种强化学习算法，用于训练智能体在特定环境中做出决策。记忆单元是一种用于存储和恢复上下文信息的网络组件，有助于提高处理子窗口时的准确度。"
    },
    {
        "order": 156,
        "title": "Rethinking ImageNet Pre-Training",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html",
        "abstract": "We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy. To push the envelope we demonstrate  50.9  AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of `pre-training and fine-tuning' in computer vision.",
        "中文标题": "重新思考ImageNet预训练",
        "摘要翻译": "我们报告了在COCO数据集上使用从随机初始化训练的标准模型在目标检测和实例分割方面取得的竞争性结果。这些结果并不比它们的ImageNet预训练对应物差，即使在使用基线系统（Mask R-CNN）的超参数时也是如此，这些超参数是为了微调预训练模型而优化的，唯一的例外是增加了训练迭代次数，以便随机初始化的模型可以收敛。从随机初始化训练出人意料地稳健；即使是在以下情况下，我们的结果也成立：（i）仅使用10%的训练数据，（ii）对于更深更宽的模型，以及（iii）对于多个任务和指标。实验表明，ImageNet预训练在训练早期加速了收敛，但并未必提供正则化或提高最终目标任务准确性。为了突破极限，我们展示了在不使用任何外部数据的情况下在COCO目标检测上达到50.9 AP的结果——这一结果与使用ImageNet预训练的顶级COCO 2017竞赛结果相当。这些观察结果挑战了依赖任务的ImageNet预训练的传统智慧，我们预计这些发现将鼓励人们重新思考计算机视觉中当前事实上的“预训练和微调”范式。",
        "领域": "目标检测/实例分割/模型训练",
        "问题": "ImageNet预训练在目标检测和实例分割任务中的必要性",
        "动机": "挑战并重新评估ImageNet预训练在计算机视觉任务中的传统应用，探索从随机初始化训练模型的潜力",
        "方法": "使用随机初始化的标准模型在COCO数据集上进行训练，比较与ImageNet预训练模型在目标检测和实例分割任务上的表现",
        "关键词": [
            "目标检测",
            "实例分割",
            "模型训练",
            "随机初始化",
            "ImageNet预训练"
        ],
        "涉及的技术概念": "ImageNet预训练是一种在大型图像数据集（如ImageNet）上预先训练模型，然后将其应用于特定任务（如目标检测或实例分割）的技术。本文通过实验表明，从随机初始化开始训练模型，在特定条件下可以达到与ImageNet预训练模型相当甚至更好的性能，挑战了预训练在计算机视觉任务中的必要性。"
    },
    {
        "order": 157,
        "title": "DeceptionNet: Network-Driven Domain Randomization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DeceptionNet_Network-Driven_Domain_Randomization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DeceptionNet_Network-Driven_Domain_Randomization_ICCV_2019_paper.html",
        "abstract": "We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing \"blind\" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities.",
        "中文标题": "DeceptionNet: 网络驱动的领域随机化",
        "摘要翻译": "我们提出了一种新颖的方法来解决合成数据与真实数据之间的领域适应问题。与采用“盲目”领域随机化（即通过随机背景或改变光照和着色来增强合成渲染）不同，我们利用任务网络作为其自身的对抗性指导，以最大化输出不确定性的有用增强。为此，我们设计了一种最小-最大优化方案，其中给定任务与特殊的欺骗网络竞争，以最小化任务误差，同时受到欺骗者施加的特定约束。欺骗网络从一系列可微分的像素级扰动中采样，并利用任务架构找到最具破坏性的增强。与需要目标领域未标记数据的基于GAN的方法不同，我们的方法实现了从源数据到多个目标分布的鲁棒映射。我们将我们的框架应用于增强MNIST变体的数字识别任务、Cropped LineMOD数据集上的分类和物体姿态估计任务以及Cityscapes数据集上的语义分割任务，并与多种领域适应方法进行比较，从而展示了具有优越泛化能力的相似结果。",
        "领域": "领域适应/数字识别/物体姿态估计",
        "问题": "解决合成数据与真实数据之间的领域适应问题",
        "动机": "提高从合成数据到真实数据的领域适应能力，以增强模型的泛化能力",
        "方法": "设计了一种最小-最大优化方案，利用任务网络作为对抗性指导，通过欺骗网络找到最具破坏性的增强，从而实现鲁棒的领域适应",
        "关键词": [
            "领域适应",
            "数字识别",
            "物体姿态估计",
            "语义分割"
        ],
        "涉及的技术概念": "领域随机化、最小-最大优化、对抗性指导、像素级扰动、GAN（生成对抗网络）"
    },
    {
        "order": 158,
        "title": "AttentionRNN: A Structured Spatial Attention Mechanism",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Khandelwal_AttentionRNN_A_Structured_Spatial_Attention_Mechanism_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Khandelwal_AttentionRNN_A_Structured_Spatial_Attention_Mechanism_ICCV_2019_paper.html",
        "abstract": "Visual attention mechanisms have proven to be integrally important constituent components of many modern deep neural architectures. They provide an efficient and effective way to utilize visual information selectively, which has shown to be especially valuable in multi-modal learning tasks. However, all prior attention frameworks lack the ability to explicitly model structural dependencies among attention variables, making it difficult to predict consistent attention masks. In this paper we develop a novel structured spatial attention mechanism which is end-to-end trainable and can be integrated with any feed-forward convolutional neural network. This proposed AttentionRNN layer explicitly enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. As a result, each attention value depends not only on local image or contextual information, but also on the previously predicted attention values. Our experiments show consistent quantitative and qualitative improvements on a variety of recognition tasks and datasets; including image categorization, question answering and image generation.",
        "中文标题": "AttentionRNN: 一种结构化空间注意力机制",
        "摘要翻译": "视觉注意力机制已被证明是许多现代深度神经网络架构中不可或缺的重要组成部分。它们提供了一种高效且有效的方式来选择性地利用视觉信息，这在多模态学习任务中显示出特别的价值。然而，所有先前的注意力框架都缺乏显式建模注意力变量之间结构依赖性的能力，这使得预测一致的注意力掩码变得困难。在本文中，我们开发了一种新颖的结构化空间注意力机制，它是端到端可训练的，并且可以与任何前馈卷积神经网络集成。所提出的AttentionRNN层通过在空间掩码中以双向光栅扫描和逆光栅扫描顺序顺序预测注意力值，显式地对空间注意力变量施加结构。因此，每个注意力值不仅依赖于局部图像或上下文信息，还依赖于先前预测的注意力值。我们的实验在包括图像分类、问答和图像生成在内的各种识别任务和数据集上显示出一致的定量和定性改进。",
        "领域": "图像分类/问答系统/图像生成",
        "问题": "现有注意力框架无法显式建模注意力变量之间的结构依赖性，导致难以预测一致的注意力掩码。",
        "动机": "为了提高多模态学习任务中视觉信息的选择性利用效率，需要一种能够显式建模注意力变量之间结构依赖性的新机制。",
        "方法": "开发了一种新颖的结构化空间注意力机制AttentionRNN，通过在空间掩码中以双向光栅扫描和逆光栅扫描顺序顺序预测注意力值，显式地对空间注意力变量施加结构。",
        "关键词": [
            "结构化空间注意力机制",
            "AttentionRNN",
            "双向光栅扫描",
            "逆光栅扫描",
            "图像分类",
            "问答系统",
            "图像生成"
        ],
        "涉及的技术概念": {
            "视觉注意力机制": "一种使神经网络能够选择性地关注输入数据的特定部分的技术，以提高处理效率和效果。",
            "结构化空间注意力机制": "一种改进的注意力机制，通过显式建模注意力变量之间的结构依赖性，以提高注意力掩码的一致性和预测准确性。",
            "双向光栅扫描": "一种顺序处理图像或数据的方法，通过两个相反方向的扫描来捕捉更全面的上下文信息。",
            "逆光栅扫描": "与双向光栅扫描相对，从另一个方向进行扫描，以进一步增强对上下文信息的捕捉。"
        }
    },
    {
        "order": 159,
        "title": "Defending Against Universal Perturbations With Shared Adversarial Training",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mummadi_Defending_Against_Universal_Perturbations_With_Shared_Adversarial_Training_ICCV_2019_paper.html",
        "abstract": "Classifiers such as deep neural networks have been shown to be vulnerable against adversarial perturbations on problems with high-dimensional input space. While adversarial training improves the robustness of image classifiers against such adversarial perturbations, it leaves them sensitive to perturbations on a non-negligible fraction of the inputs. In this work, we show that adversarial training is more effective in preventing universal perturbations, where the same perturbation needs to fool a classifier on many inputs. Moreover, we investigate the trade-off between robustness against universal perturbations and performance on unperturbed data and propose an extension of adversarial training that handles this trade-off more gracefully. We present results for image classification and semantic segmentation to showcase that universal perturbations that fool a model hardened with adversarial training become clearly perceptible and show patterns of the target scene.",
        "中文标题": "通过共享对抗训练防御通用扰动",
        "摘要翻译": "深度神经网络等分类器已被证明在高维输入空间问题上容易受到对抗性扰动的影响。虽然对抗训练提高了图像分类器对此类对抗性扰动的鲁棒性，但它们对非可忽略部分的输入扰动仍然敏感。在这项工作中，我们展示了对抗训练在防止通用扰动方面更为有效，其中相同的扰动需要在许多输入上欺骗分类器。此外，我们研究了对抗通用扰动的鲁棒性与未扰动数据上的性能之间的权衡，并提出了一种对抗训练的扩展，以更优雅地处理这种权衡。我们展示了图像分类和语义分割的结果，以展示那些欺骗经过对抗训练强化的模型的通用扰动变得明显可感知，并显示出目标场景的模式。",
        "领域": "对抗性防御/图像分类/语义分割",
        "问题": "提高深度神经网络对通用对抗性扰动的鲁棒性",
        "动机": "尽管对抗训练提高了模型对特定对抗性扰动的鲁棒性，但模型对通用扰动的防御能力仍然不足，需要一种更有效的方法来提高模型对通用扰动的防御能力。",
        "方法": "提出了一种扩展的对抗训练方法，通过共享对抗训练来提高模型对通用扰动的防御能力，并优雅地处理鲁棒性与性能之间的权衡。",
        "关键词": [
            "对抗性防御",
            "图像分类",
            "语义分割",
            "通用扰动",
            "对抗训练"
        ],
        "涉及的技术概念": "对抗性扰动是指在输入数据上添加的微小、难以察觉的扰动，旨在欺骗机器学习模型，使其做出错误的预测。对抗训练是一种训练方法，通过在训练过程中引入对抗性扰动来提高模型的鲁棒性。通用扰动是指一种能够欺骗模型在多个输入上做出错误预测的单一扰动。"
    },
    {
        "order": 160,
        "title": "Learning Single Camera Depth Estimation Using Dual-Pixels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Garg_Learning_Single_Camera_Depth_Estimation_Using_Dual-Pixels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Garg_Learning_Single_Camera_Depth_Estimation_Using_Dual-Pixels_ICCV_2019_paper.html",
        "abstract": "Deep learning techniques have enabled rapid progress in monocular depth estimation, but their quality is limited by the ill-posed nature of the problem and the scarcity of high quality datasets. We estimate depth from a single cam-era by leveraging the dual-pixel auto-focus hardware that is increasingly common on modern camera sensors. Classic stereo algorithms and prior learning-based depth estimation techniques underperform when applied on this dual-pixel data, the former due to too-strong assumptions about RGB image matching, and the latter due to not leveraging the understanding of optics of dual-pixel image formation. To allow learning based methods to work well on dual-pixel imagery, we identify an inherent ambiguity in the depth estimated from dual-pixel cues, and develop an approach to estimate depth up to this ambiguity. Using our approach, existing monocular depth estimation techniques can be effectively applied to dual-pixel data, and much smaller models can be constructed that still infer high quality depth. To demonstrate this, we capture a large dataset of in-the-wild 5-viewpoint RGB images paired with corresponding dual-pixel data, and show how view supervision with this data can be used to learn depth up to the unknown ambiguities. On our new task, our model is 30% more accurate than any prior work on learning-based monocular or stereoscopic depth estimation.",
        "中文标题": "使用双像素学习单相机深度估计",
        "摘要翻译": "深度学习技术使得单目深度估计取得了快速进展，但其质量受到问题的不适定性和高质量数据集的稀缺性的限制。我们通过利用现代相机传感器上越来越常见的双像素自动对焦硬件，从单个相机估计深度。经典立体算法和先前的基于学习的深度估计技术在这种双像素数据上表现不佳，前者因为对RGB图像匹配的假设过于强烈，后者因为没有利用对双像素图像形成光学的理解。为了使基于学习的方法在双像素图像上工作良好，我们识别了从双像素线索估计深度的固有模糊性，并开发了一种方法来估计深度，直到这种模糊性。使用我们的方法，现有的单目深度估计技术可以有效地应用于双像素数据，并且可以构建更小的模型，这些模型仍然可以推断出高质量的深度。为了证明这一点，我们捕获了一个包含5视点RGB图像与相应双像素数据配对的大型野外数据集，并展示了如何使用这些数据进行视图监督来学习深度，直到未知的模糊性。在我们的新任务上，我们的模型比任何先前的基于学习的单目或立体深度估计工作准确30%。",
        "领域": "深度估计/双像素成像/单目视觉",
        "问题": "单目深度估计的质量受到问题的不适定性和高质量数据集的稀缺性的限制",
        "动机": "利用现代相机传感器上越来越常见的双像素自动对焦硬件，提高单目深度估计的准确性和效率",
        "方法": "识别从双像素线索估计深度的固有模糊性，并开发一种方法来估计深度，直到这种模糊性，使现有的单目深度估计技术可以有效地应用于双像素数据",
        "关键词": [
            "深度估计",
            "双像素成像",
            "单目视觉"
        ],
        "涉及的技术概念": "双像素自动对焦硬件、单目深度估计、双像素图像形成光学、视图监督"
    },
    {
        "order": 161,
        "title": "Pose-Guided Feature Alignment for Occluded Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Miao_Pose-Guided_Feature_Alignment_for_Occluded_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Persons are often occluded by various obstacles in person retrieval scenarios. Previous person re-identification (re-id) methods, either overlook this issue or resolve it based on an extreme assumption. To alleviate the occlusion problem, we propose to detect the occluded regions, and explicitly exclude those regions during feature generation and matching. In this paper, we introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. During the feature constructing stage, our method utilizes human landmarks to generate attention maps. The generated attention maps indicate if a specific body part is occluded and guide our model to attend to the non-occluded regions. During matching, we explicitly partition the global feature into parts and use the pose landmarks to indicate which partial features belonging to the target person. Only the visible regions are utilized for the retrieval. Besides, we construct a large-scale dataset for the Occluded Person Re-ID problem, namely Occluded-DukeMTMC, which is by far the largest dataset for the Occlusion Person Re-ID. Extensive experiments are conducted on our constructed occluded re-id dataset, two partial re-id datasets, and two commonly used holistic re-id datasets. Our method largely outperforms existing person re-id methods on three occlusion datasets, while remains top performance on two holistic datasets.",
        "中文标题": "姿态引导的特征对齐用于遮挡行人重识别",
        "摘要翻译": "在行人检索场景中，行人经常被各种障碍物遮挡。以往的行人重识别（re-id）方法要么忽视了这个问题，要么基于极端假设来解决它。为了缓解遮挡问题，我们提出检测遮挡区域，并在特征生成和匹配过程中明确排除这些区域。在本文中，我们介绍了一种名为姿态引导特征对齐（PGFA）的新方法，利用姿态地标从遮挡噪声中分离出有用信息。在特征构建阶段，我们的方法利用人体地标生成注意力图。生成的注意力图指示特定身体部位是否被遮挡，并引导我们的模型关注非遮挡区域。在匹配过程中，我们明确将全局特征分割成部分，并使用姿态地标来指示哪些部分特征属于目标行人。仅利用可见区域进行检索。此外，我们为遮挡行人重识别问题构建了一个大规模数据集，即Occluded-DukeMTMC，这是迄今为止最大的遮挡行人重识别数据集。我们在构建的遮挡重识别数据集、两个部分重识别数据集和两个常用的整体重识别数据集上进行了广泛的实验。我们的方法在三个遮挡数据集上大大优于现有的行人重识别方法，同时在两个整体数据集上保持顶级性能。",
        "领域": "行人重识别/遮挡处理/姿态估计",
        "问题": "解决行人重识别中的遮挡问题",
        "动机": "以往的行人重识别方法忽视了遮挡问题或基于极端假设解决，需要一种更有效的方法来处理遮挡",
        "方法": "提出姿态引导特征对齐（PGFA）方法，利用姿态地标生成注意力图，明确排除遮挡区域，仅利用可见区域进行特征匹配",
        "关键词": [
            "行人重识别",
            "遮挡处理",
            "姿态估计",
            "注意力机制"
        ],
        "涉及的技术概念": "姿态引导特征对齐（PGFA）是一种新方法，通过利用人体姿态地标生成注意力图来指示和排除遮挡区域，从而在行人重识别中有效处理遮挡问题。此外，构建了一个名为Occluded-DukeMTMC的大规模遮挡行人重识别数据集，用于评估方法的性能。"
    },
    {
        "order": 162,
        "title": "Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks With Octave Convolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Drop_an_Octave_Reducing_Spatial_Redundancy_in_Convolutional_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Drop_an_Octave_Reducing_Spatial_Redundancy_in_Convolutional_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "In natural images, information is conveyed at different frequencies where higher frequencies are usually encoded with fine details and lower frequencies are usually encoded with global structures. Similarly, the output feature maps of a convolution layer can also be seen as a mixture of information at different frequencies. In this work, we propose to factorize the mixed feature maps by their frequencies, and design a novel Octave Convolution (OctConv) operation to store and process feature maps that vary spatially \"slower\" at a lower spatial resolution reducing both memory and computation cost. Unlike existing multi-scale methods, OctConv is formulated as a single, generic, plug-and-play convolutional unit that can be used as a direct replacement of (vanilla) convolutions without any adjustments in the network architecture. It is also orthogonal and complementary to methods that suggest better topologies or reduce channel-wise redundancy like group or depth-wise convolutions. We experimentally show that by simply replacing convolutions with OctConv, we can consistently boost accuracy for both image and video recognition tasks, while reducing memory and computational cost. An OctConv-equipped ResNet-152 can achieve 82.9% top-1 classification accuracy on ImageNet with merely 22.2 GFLOPs.",
        "中文标题": "降低一个八度：使用八度卷积减少卷积神经网络中的空间冗余",
        "摘要翻译": "在自然图像中，信息以不同的频率传递，其中较高频率通常编码细节，较低频率通常编码全局结构。同样，卷积层的输出特征图也可以看作是不同频率信息的混合。在这项工作中，我们提出按频率分解混合特征图，并设计了一种新颖的八度卷积（OctConv）操作，以较低的空间分辨率存储和处理空间变化“较慢”的特征图，从而减少内存和计算成本。与现有的多尺度方法不同，OctConv被表述为一个单一的、通用的、即插即用的卷积单元，可以直接替换（普通）卷积，而无需对网络架构进行任何调整。它也与那些建议更好拓扑或减少通道冗余的方法（如组卷积或深度卷积）正交且互补。我们通过实验证明，仅通过用OctConv替换卷积，我们就能持续提高图像和视频识别任务的准确性，同时减少内存和计算成本。配备OctConv的ResNet-152在ImageNet上仅用22.2 GFLOPs就能达到82.9%的top-1分类准确率。",
        "领域": "卷积神经网络优化/图像识别/视频识别",
        "问题": "减少卷积神经网络中的空间冗余，降低内存和计算成本",
        "动机": "自然图像中的信息以不同频率传递，卷积层输出特征图也包含不同频率信息，通过按频率分解特征图，可以更有效地处理信息",
        "方法": "设计了一种新颖的八度卷积（OctConv）操作，以较低的空间分辨率存储和处理空间变化“较慢”的特征图，从而减少内存和计算成本",
        "关键词": [
            "八度卷积",
            "空间冗余",
            "卷积神经网络优化"
        ],
        "涉及的技术概念": "八度卷积（OctConv）是一种新颖的卷积操作，旨在通过按频率分解特征图来减少卷积神经网络中的空间冗余，从而降低内存和计算成本。与现有的多尺度方法不同，OctConv是一个即插即用的卷积单元，可以直接替换普通卷积，无需调整网络架构。"
    },
    {
        "order": 163,
        "title": "Adaptive Activation Thresholding: Dynamic Routing Type Behavior for Interpretability in Convolutional Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Adaptive_Activation_Thresholding_Dynamic_Routing_Type_Behavior_for_Interpretability_in_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Adaptive_Activation_Thresholding_Dynamic_Routing_Type_Behavior_for_Interpretability_in_ICCV_2019_paper.html",
        "abstract": "There is a growing interest in strategies that can help us understand or interpret neural networks -- that is, not merely provide a prediction, but also offer additional context explaining why and how. While many current methods offer tools to perform this analysis for a given (trained) network post-hoc, recent results (especially on capsule networks) suggest that when classes map to a few high level \"concepts\" in the preceding layers of the network, the behavior of the network is easier to interpret or explain. Such training may be accomplished via dynamic/EM routing where the network \"routes\" for individual classes (or subsets of images) are dynamic and involve few nodes even if the full network may not be sparse. In this paper, we show how a simple modification of the SGD scheme can help provide dynamic/EM routing type behavior in convolutional neural networks. Through extensive experiments, we evaluate the effect of this idea for interpretability where we obtain promising results, while also showing that no compromise in attainable accuracy is involved. Further, we show that the minor modification is seemingly ad-hoc, the new algorithm can be analyzed by an approximate method which provably matches known rates for SGD.",
        "中文标题": "自适应激活阈值：卷积神经网络中动态路由类型行为的可解释性",
        "摘要翻译": "对于能够帮助我们理解或解释神经网络的策略，人们的兴趣日益增长——即不仅提供预测，还提供额外的上下文解释为什么和如何。虽然许多当前的方法提供了对给定（训练过的）网络进行事后分析的工具，但最近的结果（特别是在胶囊网络上）表明，当类别映射到网络前几层的一些高级“概念”时，网络的行为更容易解释或说明。这样的训练可以通过动态/EM路由实现，其中网络为个别类别（或图像子集）的“路由”是动态的，并且涉及少量节点，即使整个网络可能不稀疏。在本文中，我们展示了如何通过简单的SGD方案修改，可以在卷积神经网络中提供动态/EM路由类型的行为。通过广泛的实验，我们评估了这一想法对可解释性的影响，获得了有希望的结果，同时也表明在可达到的准确性上没有妥协。此外，我们展示了虽然这一小修改看似随意，但新算法可以通过近似方法进行分析，该方法可证明与SGD的已知速率相匹配。",
        "领域": "神经网络可解释性/动态路由/卷积神经网络",
        "问题": "提高卷积神经网络的可解释性",
        "动机": "探索如何使神经网络不仅提供预测，还能解释其决策过程，特别是在类别映射到网络前几层的高级概念时，网络行为更易于解释。",
        "方法": "通过简单的SGD方案修改，实现卷积神经网络中的动态/EM路由类型行为，以提高网络的可解释性。",
        "关键词": [
            "神经网络可解释性",
            "动态路由",
            "卷积神经网络",
            "SGD方案"
        ],
        "涉及的技术概念": "SGD（随机梯度下降）是一种优化算法，用于最小化损失函数。动态/EM路由是一种在神经网络中实现动态路径选择的方法，旨在提高网络的可解释性。卷积神经网络（CNN）是一种深度学习模型，广泛用于图像识别和分类任务。"
    },
    {
        "order": 164,
        "title": "Robust Person Re-Identification by Modelling Feature Uncertainty",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Robust_Person_Re-Identification_by_Modelling_Feature_Uncertainty_ICCV_2019_paper.html",
        "abstract": "We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet",
        "中文标题": "通过建模特征不确定性实现鲁棒的人员重识别",
        "摘要翻译": "我们的目标是学习能够抵抗噪声训练数据的深度人员重识别（ReID）模型。在实践中，两种类型的噪声普遍存在：（1）由人类注释者错误引起的标签噪声和（2）由人员检测器错误或遮挡引起的数据异常值。这两种类型的噪声都对训练ReID模型构成了严重的问题，但迄今为止在很大程度上被忽视了。在本文中，我们提出了一种名为DistributionNet的新型深度网络，用于鲁棒的ReID。DistributionNet不是将每个人物图像表示为特征向量，而是将其建模为高斯分布，其方差表示提取特征的不确定性。在DistributionNet中精心设计了一个损失函数，以不均匀地分配训练样本之间的不确定性。因此，噪声样本被分配了大的方差/不确定性，这有效地减轻了它们对模型拟合的负面影响。大量实验证明，我们的模型比其他抗噪声深度模型更有效。源代码可在https://github.com/TianyuanYu/DistributionNet获取。",
        "领域": "人员重识别/特征提取/噪声鲁棒性",
        "问题": "解决在存在标签噪声和数据异常值的情况下训练鲁棒的深度人员重识别模型的问题",
        "动机": "现有的ReID模型训练过程中，标签噪声和数据异常值的影响被忽视，导致模型性能下降",
        "方法": "提出DistributionNet，通过将人物图像建模为高斯分布来表征特征的不确定性，并设计特定的损失函数来不均匀分配不确定性，从而减轻噪声样本的负面影响",
        "关键词": [
            "人员重识别",
            "特征不确定性",
            "噪声鲁棒性"
        ],
        "涉及的技术概念": "DistributionNet是一种深度网络，用于人员重识别，通过将人物图像建模为高斯分布来表征特征的不确定性，并设计特定的损失函数来不均匀分配不确定性，从而减轻噪声样本的负面影响。"
    },
    {
        "order": 165,
        "title": "XRAI: Better Attributions Through Regions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kapishnikov_XRAI_Better_Attributions_Through_Regions_ICCV_2019_paper.html",
        "abstract": "Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.",
        "中文标题": "XRAI: 通过区域实现更好的归因",
        "摘要翻译": "显著性方法可以帮助理解深度神经网络。近年来，显著性方法得到了许多改进，同时也出现了新的评估方法。在本文中，我们1)提出了一种新颖的基于区域的归因方法XRAI，该方法建立在集成梯度（Sundararajan等人，2017）的基础上，2)引入了评估方法来经验性地评估基于图像的显著性图的质量（性能信息曲线（PICs）），以及3)贡献了一个基于公理的归因方法健全性检查。通过经验性实验和示例结果，我们展示了XRAI在常见模型和ImageNet数据集上比其他显著性方法产生更好的结果。",
        "领域": "显著性分析/神经网络解释/图像理解",
        "问题": "如何提高深度神经网络显著性图的质量和解释性",
        "动机": "为了更深入地理解深度神经网络的工作原理，并提高显著性方法的准确性和可靠性",
        "方法": "提出了一种基于区域的归因方法XRAI，引入了性能信息曲线（PICs）来评估显著性图的质量，并提出了基于公理的归因方法健全性检查",
        "关键词": [
            "显著性方法",
            "归因方法",
            "集成梯度",
            "性能信息曲线",
            "健全性检查"
        ],
        "涉及的技术概念": {
            "显著性方法": "用于解释深度神经网络决策过程的技术，通过突出显示输入数据中对模型输出影响最大的部分。",
            "归因方法": "一种技术，用于确定输入特征对模型输出的贡献程度。",
            "集成梯度": "一种归因方法，通过计算输入特征从基线值到实际值的梯度积分来确定特征的重要性。",
            "性能信息曲线（PICs）": "一种评估显著性图质量的方法，通过分析显著性图在不同阈值下的性能变化。",
            "健全性检查": "一种基于公理的方法，用于验证归因方法的合理性和一致性。"
        }
    },
    {
        "order": 166,
        "title": "Domain Intersection and Domain Difference",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Benaim_Domain_Intersection_and_Domain_Difference_ICCV_2019_paper.html",
        "abstract": "We present a method for recovering the shared content between two visual domains as well as the content that is unique to each domain. This allows us to map from one domain to the other, in a way in which the content that is specific for the first domain is removed and the content that is specific for the second is imported from any image in the second domain. In addition, our method enables generation of images from the intersection of the two domains as well as their union, despite having no such samples during training. The method is shown analytically to contain all the sufficient and necessary constraints. It also outperforms the literature methods in an extensive set of experiments.",
        "中文标题": "领域交集与领域差异",
        "摘要翻译": "我们提出了一种方法，用于恢复两个视觉领域之间的共享内容以及每个领域独有的内容。这使得我们能够以一种方式从一个领域映射到另一个领域，其中特定于第一个领域的内容被移除，而特定于第二个领域的内容则从第二个领域的任何图像中导入。此外，我们的方法能够生成两个领域交集以及并集的图像，尽管在训练过程中没有这样的样本。该方法在分析上被证明包含所有充分且必要的约束。它还在广泛的实验中优于文献中的方法。",
        "领域": "视觉领域映射/图像生成/领域适应",
        "问题": "如何恢复两个视觉领域之间的共享内容以及每个领域独有的内容，并实现领域间的映射和图像生成",
        "动机": "为了在视觉领域之间实现有效的内容映射和图像生成，特别是在没有交集或并集样本的情况下",
        "方法": "提出了一种包含所有充分且必要约束的方法，用于恢复共享和独有内容，并实现领域间的映射和图像生成",
        "关键词": [
            "视觉领域映射",
            "图像生成",
            "领域适应"
        ],
        "涉及的技术概念": "该方法涉及视觉领域之间的内容恢复、领域映射、图像生成技术，以及在缺乏交集或并集样本情况下的图像生成。"
    },
    {
        "order": 167,
        "title": "Learned Video Compression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rippel_Learned_Video_Compression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rippel_Learned_Video_Compression_ICCV_2019_paper.html",
        "abstract": "We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so. We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs in the low-latency mode. On standard-definition videos, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger than our algorithm. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing. We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual). Secondly, we present a framework for ML-based spatial rate control --- a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting.",
        "中文标题": "学习的视频压缩",
        "摘要翻译": "我们提出了一种新的视频编码算法，专为低延迟模式端到端学习而设计。在这种设置下，我们的方法在几乎整个比特率范围内都优于所有现有的视频编解码器。据我们所知，这是第一个基于机器学习的方法做到这一点。我们在不同分辨率的标准视频压缩测试集上评估了我们的方法，并与低延迟模式下的所有主流商业编解码器进行了基准测试。在标准清晰度视频上，HEVC/H.265、AVC/H.264和VP9通常产生的代码比我们的算法大60%。在高清1080p视频上，H.265和VP9通常产生的代码比我们的算法大20%，H.264则大35%。此外，我们的方法不会出现块状伪影和像素化，因此产生的视频在视觉上更加令人愉悦。我们提出了两个主要贡献。第一个是一种新颖的视频压缩架构，它（1）将运动估计推广到执行任何超越简单平移的学习补偿，（2）不是严格依赖先前传输的参考帧，而是维护模型学习的任意信息的状态，（3）能够联合压缩所有传输的信号（如光流和残差）。其次，我们提出了一个基于机器学习的空间速率控制框架——一种为每帧在空间上分配可变比特率的机制。这是视频编码的一个关键组成部分，据我们所知，尚未在机器学习环境中开发。",
        "领域": "视频编码/低延迟处理/空间速率控制",
        "问题": "提高视频压缩效率和质量，特别是在低延迟模式下",
        "动机": "为了在低延迟模式下提供更高效的视频压缩解决方案，同时提高视频质量，减少比特率并避免视觉伪影",
        "方法": "提出了一种新颖的视频压缩架构和基于机器学习的空间速率控制框架，通过端到端学习优化视频编码过程",
        "关键词": [
            "视频编码",
            "低延迟处理",
            "空间速率控制",
            "运动估计",
            "光流",
            "残差压缩"
        ],
        "涉及的技术概念": "运动估计、光流、残差压缩、空间速率控制、端到端学习"
    },
    {
        "order": 168,
        "title": "Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brunner_Guessing_Smart_Biased_Sampling_for_Efficient_Black-Box_Adversarial_Attacks_ICCV_2019_paper.html",
        "abstract": "We consider adversarial examples for image classification in the black-box decision-based setting. Here, an attacker cannot access confidence scores, but only the final label. Most attacks for this scenario are either unreliable or inefficient. Focusing on the latter, we show that a specific class of attacks, Boundary Attacks, can be reinterpreted as a biased sampling framework that gains efficiency from domain knowledge. We identify three such biases, image frequency, regional masks and surrogate gradients, and evaluate their performance against an ImageNet classifier. We show that the combination of these biases outperforms the state of the art by a wide margin. We also showcase an efficient way to attack the Google Cloud Vision API, where we craft convincing perturbations with just a few hundred queries. Finally, the methods we propose have also been found to work very well against strong defenses: Our targeted attack won second place in the NeurIPS 2018 Adversarial Vision Challenge.",
        "中文标题": "智能猜测：用于高效黑盒对抗攻击的偏置采样",
        "摘要翻译": "我们考虑在黑盒决策设置下的图像分类对抗样本。在这种设置下，攻击者无法访问置信度分数，只能获取最终标签。针对这种情况的大多数攻击要么不可靠，要么效率低下。针对后者，我们展示了一类特定的攻击——边界攻击，可以被重新解释为一种偏置采样框架，该框架通过领域知识提高效率。我们识别了三种这样的偏置：图像频率、区域掩码和代理梯度，并评估了它们对ImageNet分类器的性能。我们展示了这些偏置的组合在性能上大幅超越了现有技术。我们还展示了一种攻击Google Cloud Vision API的高效方法，其中我们仅用几百次查询就制作出了令人信服的扰动。最后，我们提出的方法也被发现对强防御非常有效：我们的定向攻击在NeurIPS 2018对抗视觉挑战赛中获得了第二名。",
        "领域": "对抗样本/图像分类/黑盒攻击",
        "问题": "在黑盒决策设置下，如何高效地生成对抗样本以攻击图像分类系统",
        "动机": "现有的黑盒攻击方法要么不可靠，要么效率低下，需要一种更高效的方法来生成对抗样本",
        "方法": "将边界攻击重新解释为一种偏置采样框架，利用图像频率、区域掩码和代理梯度三种偏置提高攻击效率",
        "关键词": [
            "对抗样本",
            "图像分类",
            "黑盒攻击",
            "边界攻击",
            "偏置采样"
        ],
        "涉及的技术概念": "对抗样本是指在输入数据上添加微小扰动，使得机器学习模型产生错误输出的样本。黑盒攻击是指攻击者无法直接访问目标模型的内部参数和结构，只能通过输入输出对来推断模型行为的攻击方式。边界攻击是一种通过沿着决策边界搜索来生成对抗样本的方法。偏置采样是一种通过引入先验知识或偏好来提高采样效率的方法。"
    },
    {
        "order": 169,
        "title": "Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Subramaniam_Co-Segmentation_Inspired_Attention_Networks_for_Video-Based_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person's identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets.",
        "中文标题": "基于视频的人员再识别中的共分割启发注意力网络",
        "摘要翻译": "人员再识别（Re-ID）是一个重要的现实世界监控问题，它涉及在摄像头网络中关联一个人的身份。基于视频的Re-ID方法最近受到了显著关注，因为通常可以获得视频而不仅仅是图像。在这项工作中，我们提出了一种新颖的共分割启发的视频Re-ID深度架构，并制定了一个基于共分割的注意力模块（COSAM），该模块通过无监督的方式在视频的多个帧中激活一组共同的显著特征。与大多数先前的工作不同，我们的方法能够同时关注人物及其配件。我们的即插即用且可解释的COSAM模块应用于两种深度架构（ResNet50，SE-ResNet50）上，在三个基准数据集上超越了最先进的方法。",
        "领域": "视频监控/人员再识别/深度学习",
        "问题": "解决在视频监控中跨摄像头关联人员身份的问题",
        "动机": "由于视频数据的可用性增加，基于视频的人员再识别方法受到关注，需要一种能够有效关联视频中人员身份的方法",
        "方法": "提出了一种共分割启发的视频Re-ID深度架构，并开发了基于共分割的注意力模块（COSAM），通过无监督方式激活视频帧中的共同显著特征",
        "关键词": [
            "视频监控",
            "人员再识别",
            "共分割",
            "注意力机制"
        ],
        "涉及的技术概念": "共分割启发注意力网络（Co-segmentation inspired Attention Networks）、基于共分割的注意力模块（COSAM）、ResNet50、SE-ResNet50、无监督学习"
    },
    {
        "order": 170,
        "title": "Domain-Adaptive Single-View 3D Reconstruction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pinheiro_Domain-Adaptive_Single-View_3D_Reconstruction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pinheiro_Domain-Adaptive_Single-View_3D_Reconstruction_ICCV_2019_paper.html",
        "abstract": "Single-view 3D shape reconstruction is an important but challenging problem, mainly for two reasons. First, as shape annotation is very expensive to acquire, current methods rely on synthetic data, in which ground-truth 3D annotation is easy to obtain. However, this results in domain adaptation problem when applied to natural images. The second challenge is that there are multiple shapes that can explain a given 2D image. In this paper, we propose a framework to improve over these challenges using adversarial training. On one hand, we impose domain confusion between natural and synthetic image representations to reduce the distribution gap. On the other hand, we impose the reconstruction to be `realistic' by forcing it to lie on a (learned) manifold of realistic object shapes. Our experiments show that these constraints improve performance by a large margin over baseline reconstruction models. We achieve results competitive with the state of the art with a much simpler architecture.",
        "中文标题": "领域自适应单视图三维重建",
        "摘要翻译": "单视图三维形状重建是一个重要但具有挑战性的问题，主要原因有两个。首先，由于形状标注的获取成本非常高，当前的方法依赖于合成数据，其中真实的三维标注易于获得。然而，这导致了应用于自然图像时的领域适应问题。第二个挑战是，有多个形状可以解释给定的二维图像。在本文中，我们提出了一个框架，通过对抗训练来改进这些挑战。一方面，我们在自然和合成图像表示之间施加领域混淆，以减少分布差距。另一方面，我们通过强制重建位于一个（学习的）现实物体形状的流形上，使其变得‘真实’。我们的实验表明，这些约束显著提高了基线重建模型的性能。我们以更简单的架构实现了与现有技术水平竞争的结果。",
        "领域": "三维重建/领域适应/对抗学习",
        "问题": "单视图三维形状重建中的领域适应问题和多解问题",
        "动机": "解决单视图三维重建中由于依赖合成数据导致的领域适应问题，以及由于一个二维图像可能对应多个三维形状的多解问题。",
        "方法": "采用对抗训练框架，一方面通过领域混淆减少自然和合成图像之间的分布差距，另一方面通过强制重建位于学习的现实物体形状流形上，使其变得真实。",
        "关键词": [
            "三维重建",
            "领域适应",
            "对抗学习"
        ],
        "涉及的技术概念": "领域混淆：通过对抗训练减少不同领域（如自然图像和合成图像）之间的分布差异。学习流形：通过机器学习方法学习到的表示现实物体形状的空间，用于强制重建结果位于该空间内，以提高重建的真实性。"
    },
    {
        "order": 171,
        "title": "Local Relation Networks for Image Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Local_Relation_Networks_for_Image_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Local_Relation_Networks_for_Image_Recognition_ICCV_2019_paper.html",
        "abstract": "The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.",
        "中文标题": "局部关系网络用于图像识别",
        "摘要翻译": "多年来，卷积层一直是计算机视觉中占主导地位的特征提取器。然而，卷积中的空间聚合基本上是一个模式匹配过程，它应用固定滤波器，这些滤波器在建模具有不同空间分布的视觉元素时效率低下。本文提出了一种新的图像特征提取器，称为局部关系层，它基于局部像素对的组成关系自适应地确定聚合权重。通过这种关系方法，它能够以更高效的方式将视觉元素组合成更高级别的实体，从而有利于语义推理。使用局部关系层构建的网络，称为局部关系网络（LR-Net），在大规模识别任务（如ImageNet分类）上被发现比使用常规卷积构建的对应网络具有更大的建模能力。",
        "领域": "图像识别/特征提取/语义推理",
        "问题": "传统卷积层在建模具有不同空间分布的视觉元素时效率低下",
        "动机": "提高图像特征提取的效率和效果，以更好地支持语义推理",
        "方法": "提出局部关系层，基于局部像素对的组成关系自适应地确定聚合权重，构建局部关系网络（LR-Net）",
        "关键词": [
            "局部关系网络",
            "特征提取",
            "语义推理",
            "图像识别"
        ],
        "涉及的技术概念": "卷积层：一种在计算机视觉中用于特征提取的技术，通过应用固定滤波器进行空间聚合。局部关系层：一种新的图像特征提取器，通过分析局部像素对的组成关系来自适应地确定聚合权重，以提高特征提取的效率和效果。局部关系网络（LR-Net）：使用局部关系层构建的网络，旨在提高大规模图像识别任务的建模能力。"
    },
    {
        "order": 172,
        "title": "A Delay Metric for Video Object Detection: What Average Precision Fails to Tell",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_A_Delay_Metric_for_Video_Object_Detection_What_Average_Precision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mao_A_Delay_Metric_for_Video_Object_Detection_What_Average_Precision_ICCV_2019_paper.html",
        "abstract": "Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze the object detection from video and point out that mAP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, Average Delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve mAP well. In other words, mAP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception.",
        "中文标题": "视频目标检测的延迟度量：平均精度未能揭示的内容",
        "摘要翻译": "平均精度（AP）是广泛用于评估图像和视频目标检测器检测准确性的指标。在本文中，我们从视频中分析目标检测，并指出仅使用mAP不足以捕捉视频目标检测的时间特性。为了解决这个问题，我们提出了一个全面的度量标准，平均延迟（AD），以测量和比较检测延迟。为了便于延迟评估，我们精心选择了ImageNet VID的一个子集，我们将其命名为ImageNet VIDT，重点在于复杂轨迹。通过在VIDT上广泛评估各种检测器，我们表明大多数方法显著增加了检测延迟，但仍然很好地保持了mAP。换句话说，mAP不足以反映视频目标检测器的时间特性。我们的结果表明，视频目标检测方法应该使用延迟度量进行评估，特别是对于自动驾驶车辆感知等延迟关键的应用。",
        "领域": "视频目标检测/自动驾驶/延迟评估",
        "问题": "现有平均精度（AP）指标无法充分反映视频目标检测的时间特性",
        "动机": "为了更全面地评估视频目标检测器的性能，特别是其时间特性，需要引入新的度量标准",
        "方法": "提出平均延迟（AD）作为新的度量标准，并通过在ImageNet VIDT数据集上的实验验证其有效性",
        "关键词": [
            "视频目标检测",
            "平均延迟",
            "时间特性",
            "自动驾驶",
            "延迟评估"
        ],
        "涉及的技术概念": {
            "平均精度（AP）": "用于评估检测准确性的指标",
            "平均延迟（AD）": "本文提出的用于测量和比较检测延迟的新度量标准",
            "ImageNet VIDT": "本文精心选择的ImageNet VID子集，重点在于复杂轨迹，用于延迟评估"
        }
    },
    {
        "order": 173,
        "title": "Mask-Guided Attention Network for Occluded Pedestrian Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pang_Mask-Guided_Attention_Network_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.html",
        "abstract": "Pedestrian detection relying on deep convolution neural networks has made significant progress. Though promising results have been achieved on standard pedestrians, the performance on heavily occluded pedestrians remains far from satisfactory. The main culprits are intra-class occlusions involving other pedestrians and inter-class occlusions caused by other objects, such as cars and bicycles. These results in a multitude of occlusion patterns. We propose an approach for occluded pedestrian detection with the following contributions. First, we introduce a novel mask-guided attention network that fits naturally into popular pedestrian detection pipelines. Our attention network emphasizes on visible pedestrian regions while suppressing the occluded ones by modulating full body features. Second, we empirically demonstrate that coarse-level segmentation annotations provide reasonable approximation to their dense pixel-wise counterparts. Experiments are performed on CityPersons and Caltech datasets. Our approach sets a new state-of-the-art on both datasets. Our approach obtains an absolute gain of 9.5% in log-average miss rate, compared to the best reported results [32] on the heavily occluded HO pedestrian set of CityPersons test set. Further, on the HO pedestrian set of Caltech dataset, our method achieves an absolute gain of 5.0% in log-average miss rate, compared to the best reported results [13]. Code and models are available at: https://github.com/Leotju/MGAN.",
        "中文标题": "掩码引导注意力网络用于遮挡行人检测",
        "摘要翻译": "依赖深度卷积神经网络的行人检测已取得显著进展。尽管在标准行人检测上取得了令人鼓舞的成果，但在严重遮挡的行人检测上，性能仍远未令人满意。主要原因是涉及其他行人的类内遮挡和由其他物体（如汽车和自行车）引起的类间遮挡。这些导致了多种遮挡模式。我们提出了一种用于遮挡行人检测的方法，具有以下贡献。首先，我们引入了一种新颖的掩码引导注意力网络，该网络自然地适应于流行的行人检测流程。我们的注意力网络强调可见的行人区域，同时通过调节全身特征来抑制被遮挡的区域。其次，我们经验性地证明了粗粒度分割注释提供了对其密集像素级对应物的合理近似。实验在CityPersons和Caltech数据集上进行。我们的方法在这两个数据集上都设定了新的最先进水平。与CityPersons测试集上严重遮挡的HO行人集的最佳报告结果[32]相比，我们的方法在log-average miss rate上获得了9.5%的绝对增益。此外，在Caltech数据集的HO行人集上，与最佳报告结果[13]相比，我们的方法在log-average miss rate上实现了5.0%的绝对增益。代码和模型可在https://github.com/Leotju/MGAN获取。",
        "领域": "行人检测/遮挡处理/注意力机制",
        "问题": "解决严重遮挡情况下的行人检测问题",
        "动机": "提高在严重遮挡情况下的行人检测性能",
        "方法": "提出了一种掩码引导注意力网络，强调可见行人区域并抑制被遮挡区域，同时利用粗粒度分割注释作为密集像素级注释的合理近似",
        "关键词": [
            "行人检测",
            "遮挡处理",
            "注意力机制",
            "分割注释"
        ],
        "涉及的技术概念": "深度卷积神经网络、掩码引导注意力网络、粗粒度分割注释、log-average miss rate"
    },
    {
        "order": 174,
        "title": "DiscoNet: Shapes Learning on Disconnected Manifolds for 3D Editing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mehr_DiscoNet_Shapes_Learning_on_Disconnected_Manifolds_for_3D_Editing_ICCV_2019_paper.html",
        "abstract": "Editing 3D models is a very challenging task, as it requires complex interactions with the 3D shape to reach the targeted design, while preserving the global consistency and plausibility of the shape. In this work, we present an intelligent and user-friendly 3D editing tool, where the edited model is constrained to lie onto a learned manifold of realistic shapes. Due to the topological variability of real 3D models, they often lie close to a disconnected manifold, which cannot be learned with a common learning algorithm. Therefore, our tool is based on a new deep learning model, DiscoNet, which extends 3D surface autoencoders in two ways. Firstly, our deep learning model uses several autoencoders to automatically learn each connected component of a disconnected manifold, without any supervision. Secondly, each autoencoder infers the output 3D surface by deforming a pre-learned 3D template specific to each connected component. Both advances translate into improved 3D synthesis, thus enhancing the quality of our 3D editing tool.",
        "中文标题": "DiscoNet: 在断开连接的流形上进行形状学习以进行3D编辑",
        "摘要翻译": "编辑3D模型是一项非常具有挑战性的任务，因为它需要与3D形状进行复杂的交互以达到目标设计，同时保持形状的全局一致性和合理性。在这项工作中，我们提出了一种智能且用户友好的3D编辑工具，其中编辑后的模型被限制在学习的现实形状流形上。由于真实3D模型的拓扑可变性，它们通常靠近一个断开连接的流形，这无法用常见的学习算法学习。因此，我们的工具基于一个新的深度学习模型DiscoNet，它以两种方式扩展了3D表面自动编码器。首先，我们的深度学习模型使用多个自动编码器来自动学习断开连接流形的每个连接组件，无需任何监督。其次，每个自动编码器通过变形一个预先学习的特定于每个连接组件的3D模板来推断输出3D表面。这两项进展转化为改进的3D合成，从而提高了我们的3D编辑工具的质量。",
        "领域": "3D建模/深度学习/自动编码器",
        "问题": "如何在保持3D模型全局一致性和合理性的同时，进行有效的3D编辑",
        "动机": "为了解决3D模型编辑过程中面临的复杂交互和保持模型一致性的挑战",
        "方法": "开发了一种基于深度学习模型DiscoNet的3D编辑工具，该模型通过使用多个自动编码器自动学习断开连接流形的每个连接组件，并通过变形预学习的3D模板来推断输出3D表面",
        "关键词": [
            "3D编辑",
            "断开连接流形",
            "自动编码器"
        ],
        "涉及的技术概念": "DiscoNet是一个深度学习模型，它扩展了3D表面自动编码器的应用，通过使用多个自动编码器来学习断开连接流形的每个连接组件，并通过变形预学习的3D模板来推断输出3D表面，从而改进3D合成和编辑工具的质量。"
    },
    {
        "order": 175,
        "title": "IL2M: Class Incremental Learning With Dual Memory",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.html",
        "abstract": "This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods.",
        "中文标题": "IL2M：利用双记忆的类增量学习",
        "摘要翻译": "本文提出了一种类增量学习（IL）方法，该方法利用微调和双记忆来减少图像识别中灾难性遗忘的负面影响。首先，我们简化了当前基于微调的方法，这些方法使用分类和蒸馏损失的组合来补偿过去数据的有限可用性。我们发现，当允许使用记忆时，蒸馏项实际上会损害性能。然后，我们修改了通常的类IL记忆组件。与现有工作类似，第一个记忆存储过去类的示例图像。这里引入了第二个记忆来存储过去类在最初学习时获得的统计信息。这里的直觉是，当所有数据都可用时，类被最好地建模，并且它们的初始统计信息在不同的增量状态下都是有用的。在推理过程中，由于数据集对新学习的类有利，因此出现了对新学习类的预测偏差。挑战在于使新类和过去类的预测更具可比性。为此，通过利用两个记忆的内容来修正过去类的分数。该方法在内存和推理复杂性方面几乎没有增加成本。使用三个大型公共数据集的实验表明，所提出的方法比一系列竞争性的最先进方法更有效。",
        "领域": "类增量学习/图像识别/灾难性遗忘",
        "问题": "减少图像识别中灾难性遗忘的负面影响",
        "动机": "解决在类增量学习过程中，由于数据不平衡导致的预测偏差问题，以及提高新类和过去类预测的可比性",
        "方法": "利用微调和双记忆的方法，其中第一个记忆存储过去类的示例图像，第二个记忆存储过去类的初始统计信息，通过利用这两个记忆的内容来修正过去类的分数",
        "关键词": [
            "类增量学习",
            "双记忆",
            "灾难性遗忘",
            "图像识别",
            "微调"
        ],
        "涉及的技术概念": "类增量学习（IL）是一种机器学习方法，旨在使模型能够逐步学习新类而不会忘记旧类。灾难性遗忘是指在模型学习新信息时，旧知识的丢失或遗忘。微调是一种技术，通过在新数据上继续训练预训练模型来调整模型参数。双记忆指的是使用两个不同的记忆组件来存储和利用过去类的信息，以提高模型在增量学习中的性能。"
    },
    {
        "order": 176,
        "title": "Spectral Feature Transformation for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Spectral_Feature_Transformation_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "With the surge of deep learning techniques, the field of person re-identification has witnessed rapid progress in recent years. Deep learning based methods focus on learning a discriminative feature space where data points are clustered compactly according to their corresponding identities. Most existing methods process data points individually or only involves a fraction of samples while building a similarity structure. They ignore dense informative connections among samples more or less. The lack of holistic observation eventually leads to inferior performance. To relieve the issue, we propose to formulate the whole data batch as a similarity graph. Inspired by spectral clustering, a novel module termed Spectral Feature Transformation is developed to facilitate the optimization of group-wise similarities. It adds no burden to the inference and can be applied to various scenarios. As a natural extension, we further derive a lightweight re-ranking method named Local Blurring Re-ranking which makes the underlying clustering structure around the probe set more compact. Empirical studies on four public benchmarks show the superiority of the proposed method. Code is available at https://github.com/LuckyDC/SFT_REID.",
        "中文标题": "用于行人重识别的光谱特征变换",
        "摘要翻译": "随着深度学习技术的兴起，行人重识别领域近年来取得了快速进展。基于深度学习的方法专注于学习一个区分性特征空间，其中数据点根据其对应的身份紧密聚类。大多数现有方法在构建相似性结构时单独处理数据点或仅涉及部分样本。它们在某种程度上忽略了样本之间密集的信息连接。缺乏整体观察最终导致性能不佳。为了缓解这一问题，我们提出将整个数据批次表述为相似性图。受光谱聚类的启发，开发了一个名为光谱特征变换的新模块，以促进群体相似性的优化。它不会增加推理负担，并且可以应用于各种场景。作为自然延伸，我们进一步推导了一种名为局部模糊重排序的轻量级重排序方法，使得围绕探针集的底层聚类结构更加紧凑。在四个公共基准上的实证研究显示了所提出方法的优越性。代码可在https://github.com/LuckyDC/SFT_REID获取。",
        "领域": "行人重识别/光谱聚类/特征学习",
        "问题": "现有方法在构建相似性结构时忽略样本间密集的信息连接，导致性能不佳",
        "动机": "为了缓解现有方法在行人重识别中因忽略样本间密集信息连接而导致的性能不佳问题",
        "方法": "提出将整个数据批次表述为相似性图，并开发光谱特征变换模块优化群体相似性，进一步推导局部模糊重排序方法",
        "关键词": [
            "行人重识别",
            "光谱聚类",
            "特征学习",
            "相似性图",
            "重排序"
        ],
        "涉及的技术概念": "深度学习技术用于学习区分性特征空间，光谱聚类启发下的光谱特征变换模块用于优化群体相似性，局部模糊重排序方法用于使聚类结构更加紧凑。"
    },
    {
        "order": 177,
        "title": "Deep Residual Learning in the JPEG Transform Domain",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ehrlich_Deep_Residual_Learning_in_the_JPEG_Transform_Domain_ICCV_2019_paper.html",
        "abstract": "We introduce a general method of performing Residual Network inference and learning in the JPEG transform domain that allows the network to consume compressed images as input. Our formulation leverages the linearity of the JPEG transform to redefine convolution and batch normalization with a tune-able numerical approximation for ReLu. The result is mathematically equivalent to the spatial domain network up to the ReLu approximation accuracy. A formulation for image classification and a model conversion algorithm for spatial domain networks are given as examples of the method. We show that the sparsity of the JPEG format allows for faster processing of images with little to no penalty in the network accuracy.",
        "中文标题": "JPEG变换域中的深度残差学习",
        "摘要翻译": "我们介绍了一种在JPEG变换域中执行残差网络推理和学习的一般方法，该方法允许网络以压缩图像作为输入。我们的公式利用JPEG变换的线性特性，重新定义了卷积和批量归一化，并为ReLu提供了一个可调节的数值近似。结果是数学上等同于空间域网络，直到ReLu近似精度。作为该方法的示例，给出了图像分类的公式和空间域网络的模型转换算法。我们展示了JPEG格式的稀疏性允许更快地处理图像，而几乎不会影响网络精度。",
        "领域": "图像压缩/深度学习/计算效率",
        "问题": "如何在JPEG变换域中有效地执行残差网络推理和学习，以处理压缩图像",
        "动机": "为了利用JPEG格式的稀疏性，提高图像处理的速度而不牺牲网络精度",
        "方法": "利用JPEG变换的线性特性，重新定义卷积和批量归一化，并为ReLu提供可调节的数值近似，同时提供图像分类的公式和空间域网络的模型转换算法",
        "关键词": [
            "JPEG变换域",
            "残差网络",
            "图像分类",
            "模型转换",
            "计算效率"
        ],
        "涉及的技术概念": "JPEG变换域的线性特性被用来重新定义卷积和批量归一化操作，同时为ReLu激活函数提供了一个可调节的数值近似。这种方法允许直接在压缩的JPEG图像上进行深度学习模型的推理和学习，从而提高了处理速度，同时保持了与在空间域中操作的网络相当的精度。"
    },
    {
        "order": 178,
        "title": "Asymmetric Non-Local Neural Networks for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.",
        "中文标题": "非对称非局部神经网络用于语义分割",
        "摘要翻译": "非局部模块作为语义分割的一种特别有用的技术，因其高昂的计算和GPU内存占用而受到批评。在本文中，我们提出了用于语义分割的非对称非局部神经网络，它有两个突出的组成部分：非对称金字塔非局部块（APNB）和非对称融合非局部块（AFNB）。APNB将金字塔采样模块引入非局部块，以在不牺牲性能的情况下大幅减少计算和内存消耗。AFNB是从APNB改编而来，以充分考虑到长距离依赖性的情况下融合不同层次的特征，从而显著提高了性能。在语义分割基准上的大量实验证明了我们工作的有效性和效率。特别是，我们在Cityscapes测试集上报告了81.3 mIoU的最先进性能。对于256x128的输入，APNB在GPU上的速度比非局部块快约6倍，而在GPU运行内存占用上小28倍。代码可在https://github.com/MendelXu/ANN.git获取。",
        "领域": "语义分割/神经网络优化/深度学习应用",
        "问题": "非局部模块在语义分割中的高计算和GPU内存占用问题",
        "动机": "为了解决非局部模块在语义分割中因高计算和GPU内存占用而受到批评的问题，提出了一种更高效的方法",
        "方法": "提出了非对称非局部神经网络，包括非对称金字塔非局部块（APNB）和非对称融合非局部块（AFNB），以减少计算和内存消耗，同时提高性能",
        "关键词": [
            "语义分割",
            "非局部模块",
            "神经网络优化"
        ],
        "涉及的技术概念": "非局部模块是一种用于捕捉长距离依赖性的技术，常用于语义分割任务中。本文提出的非对称非局部神经网络通过引入金字塔采样模块和特征融合策略，优化了非局部模块的计算效率和内存占用。"
    },
    {
        "order": 179,
        "title": "Permutation-Invariant Feature Restructuring for Correlation-Aware Image Set-Based Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Permutation-Invariant_Feature_Restructuring_for_Correlation-Aware_Image_Set-Based_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Permutation-Invariant_Feature_Restructuring_for_Correlation-Aware_Image_Set-Based_Recognition_ICCV_2019_paper.html",
        "abstract": "We consider the problem of comparing the similarity of image sets with variable-quantity, quality and un-ordered heterogeneous images. We use feature restructuring to exploit the correlations of both inner&inter-set images. Specifically, the residual self-attention can effectively restructure the features using the other features within a set to emphasize the discriminative images and eliminate the redundancy. Then, a sparse/collaborative learning-based dependency-guided representation scheme reconstructs the probe features conditional to the gallery features in order to adaptively align the two sets. This enables our framework to be compatible with both verification and open-set identification. We show that the parametric self-attention network and non-parametric dictionary learning can be trained end-to-end by a unified alternative optimization scheme, and that the full framework is permutation-invariant. In the numerical experiments we conducted, our method achieves top performance on competitive image set/video-based face recognition and person re-identification benchmarks.",
        "中文标题": "基于相关性感知的图像集识别的置换不变特征重构",
        "摘要翻译": "我们考虑比较具有可变数量、质量和无序异质图像的图像集相似性的问题。我们使用特征重构来利用集内和集间图像的相关性。具体来说，残差自注意力可以有效地利用集合内其他特征来重构特征，以强调区分性图像并消除冗余。然后，基于稀疏/协作学习的依赖引导表示方案根据画廊特征重建探针特征，以自适应地对齐这两个集合。这使得我们的框架既适用于验证也适用于开放集识别。我们展示了参数自注意力网络和非参数字典学习可以通过统一的交替优化方案进行端到端训练，并且整个框架是置换不变的。在我们进行的数值实验中，我们的方法在竞争性的图像集/基于视频的人脸识别和行人重识别基准上实现了顶级性能。",
        "领域": "人脸识别/行人重识别/特征学习",
        "问题": "比较具有可变数量、质量和无序异质图像的图像集相似性",
        "动机": "利用集内和集间图像的相关性，提高图像集识别的准确性和效率",
        "方法": "使用残差自注意力进行特征重构，强调区分性图像并消除冗余；采用基于稀疏/协作学习的依赖引导表示方案，根据画廊特征重建探针特征，以自适应地对齐图像集",
        "关键词": [
            "特征重构",
            "自注意力",
            "稀疏学习",
            "协作学习",
            "人脸识别",
            "行人重识别"
        ],
        "涉及的技术概念": "残差自注意力：一种利用集合内其他特征来重构特征的技术，用于强调区分性图像并消除冗余。稀疏/协作学习：一种学习方法，通过稀疏表示或协作表示来重建特征，以适应不同的图像集。置换不变性：指框架或方法在处理输入时，不依赖于输入的顺序，即输入的顺序变化不会影响输出结果。"
    },
    {
        "order": 180,
        "title": "Approximated Bilinear Modules for Temporal Modeling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Approximated_Bilinear_Modules_for_Temporal_Modeling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Approximated_Bilinear_Modules_for_Temporal_Modeling_ICCV_2019_paper.html",
        "abstract": "We consider two less-emphasized temporal properties of video: 1. Temporal cues are fine-grained; 2. Temporal modeling needs reasoning. To tackle both problems at once, we exploit approximated bilinear modules (ABMs) for temporal modeling. There are two main points making the modules effective: two-layer MLPs can be seen as a constraint approximation of bilinear operations, thus can be used to construct deep ABMs in existing CNNs while reusing pretrained parameters; frame features can be divided into static and dynamic parts because of visual repetition in adjacent frames, which enables temporal modeling to be more efficient. Multiple ABM variants and implementations are investigated, from high performance to high efficiency. Specifically, we show how two-layer subnets in CNNs can be converted to temporal bilinear modules by adding an auxiliary-branch. Besides, we introduce snippet sampling and shifting inference to boost sparse-frame video classification performance. Extensive ablation studies are conducted to show the effectiveness of proposed techniques. Our models can outperform most state-of-the-art methods on Something-Something v1 and v2 datasets without Kinetics pretraining, and are also competitive on other YouTube-like action recognition datasets. Our code is available on https://github.com/zhuxinqimac/abm-pytorch.",
        "中文标题": "用于时间建模的近似双线性模块",
        "摘要翻译": "我们考虑了视频中两个较少被强调的时间属性：1. 时间线索是细粒度的；2. 时间建模需要推理。为了同时解决这两个问题，我们利用近似双线性模块（ABMs）进行时间建模。这些模块之所以有效，主要有两点原因：两层MLPs可以被视为双线性操作的约束近似，因此可以用来在现有的CNNs中构建深度ABMs，同时重用预训练参数；由于相邻帧中的视觉重复，帧特征可以分为静态和动态部分，这使得时间建模更加高效。我们研究了从高性能到高效率的多种ABM变体和实现。具体来说，我们展示了如何通过添加辅助分支将CNNs中的两层子网转换为时间双线性模块。此外，我们引入了片段采样和移位推理，以提高稀疏帧视频分类的性能。进行了广泛的消融研究，以展示所提出技术的有效性。我们的模型在Something-Something v1和v2数据集上无需Kinetics预训练即可超越大多数最先进的方法，并且在其他类似YouTube的动作识别数据集上也具有竞争力。我们的代码可在https://github.com/zhuxinqimac/abm-pytorch上获取。",
        "领域": "视频理解/动作识别/时间建模",
        "问题": "视频时间建模中的细粒度时间线索捕捉和推理需求",
        "动机": "解决视频时间建模中细粒度时间线索捕捉和推理需求的问题",
        "方法": "利用近似双线性模块（ABMs）进行时间建模，通过两层MLPs近似双线性操作构建深度ABMs，将帧特征分为静态和动态部分以提高效率，引入片段采样和移位推理提升性能",
        "关键词": [
            "时间建模",
            "近似双线性模块",
            "动作识别"
        ],
        "涉及的技术概念": "近似双线性模块（ABMs）是一种用于视频时间建模的技术，通过两层MLPs近似双线性操作来构建深度网络，同时重用预训练参数。帧特征分为静态和动态部分，以提高时间建模的效率。片段采样和移位推理是用于提升稀疏帧视频分类性能的技术。"
    },
    {
        "order": 181,
        "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet.",
        "中文标题": "CCNet: 用于语义分割的十字交叉注意力机制",
        "摘要翻译": "全图像依赖提供了有用的上下文信息，有助于视觉理解问题。在这项工作中，我们提出了一种十字交叉网络（CCNet），以更有效和高效的方式获取这种上下文信息。具体来说，对于每个像素，CCNet中的一种新颖的十字交叉注意力模块收集其十字交叉路径上所有像素的上下文信息。通过进一步的循环操作，每个像素最终可以从所有像素中捕获全图像依赖。总体而言，CCNet具有以下优点：1）GPU内存友好。与非局部块相比，提出的循环十字交叉注意力模块需要的GPU内存使用量减少了11倍。2）高计算效率。循环十字交叉注意力显著减少了计算全图像依赖时的FLOPs，大约是非局部块的85%。3）最先进的性能。我们在包括Cityscapes、ADE20K在内的流行语义分割基准和实例分割基准COCO上进行了广泛的实验。特别是，我们的CCNet在Cityscapes测试集和ADE20K验证集上分别达到了81.4和45.22的mIoU分数，这是新的最先进结果。源代码可在https://github.com/speedinghzl/CCNet获取。",
        "领域": "语义分割/实例分割/上下文信息处理",
        "问题": "如何更有效和高效地获取全图像依赖的上下文信息以提升语义分割的性能",
        "动机": "全图像依赖提供了有用的上下文信息，有助于视觉理解问题，但现有的方法在效率和内存使用上存在不足",
        "方法": "提出了一种十字交叉网络（CCNet），其中包含一种新颖的十字交叉注意力模块，通过循环操作使每个像素捕获全图像依赖",
        "关键词": [
            "语义分割",
            "实例分割",
            "上下文信息",
            "注意力机制",
            "GPU内存优化",
            "计算效率"
        ],
        "涉及的技术概念": {
            "十字交叉注意力模块": "一种新颖的注意力机制，用于收集每个像素十字交叉路径上所有像素的上下文信息",
            "循环操作": "通过重复应用十字交叉注意力模块，使每个像素最终捕获全图像依赖",
            "GPU内存优化": "与非局部块相比，提出的循环十字交叉注意力模块显著减少了GPU内存使用量",
            "计算效率": "循环十字交叉注意力显著减少了计算全图像依赖时的FLOPs",
            "mIoU分数": "用于评估语义分割性能的指标，表示平均交并比"
        }
    },
    {
        "order": 182,
        "title": "Transformable Bottleneck Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Olszewski_Transformable_Bottleneck_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Olszewski_Transformable_Bottleneck_Networks_ICCV_2019_paper.html",
        "abstract": "We propose a novel approach to performing fine-grained 3D manipulation of image content via a convolutional neural network, which we call the Transformable Bottleneck Network (TBN). It applies given spatial transformations directly to a volumetric bottleneck within our encoder-bottleneck-decoder architecture. Multi-view supervision encourages the network to learn to spatially disentangle the feature space within the bottleneck. The resulting spatial structure can be manipulated with arbitrary spatial transformations. We demonstrate the efficacy of TBNs for novel view synthesis, achieving state-of-the-art results on a challenging benchmark. We demonstrate that the bottlenecks produced by networks trained for this task contain meaningful spatial structure that allows us to intuitively perform a variety of image manipulations in 3D, well beyond the rigid transformations seen during training. These manipulations include non-uniform scaling, non-rigid warping, and combining content from different images. Finally, we extract explicit 3D structure from the bottleneck, performing impressive 3D reconstruction from a single input image.",
        "中文标题": "可变换瓶颈网络",
        "摘要翻译": "我们提出了一种通过卷积神经网络进行细粒度3D图像内容操作的新方法，我们称之为可变换瓶颈网络（TBN）。它直接将给定的空间变换应用于我们的编码器-瓶颈-解码器架构中的体积瓶颈。多视图监督鼓励网络学习在瓶颈内空间上解耦特征空间。由此产生的空间结构可以通过任意空间变换进行操作。我们展示了TBN在新视图合成中的有效性，在一个具有挑战性的基准测试中取得了最先进的结果。我们证明了为此任务训练的网络产生的瓶颈包含有意义的空间结构，使我们能够直观地执行各种3D图像操作，远远超出了训练期间看到的刚性变换。这些操作包括非均匀缩放、非刚性扭曲和组合来自不同图像的内容。最后，我们从瓶颈中提取显式的3D结构，从单个输入图像执行令人印象深刻的3D重建。",
        "领域": "3D图像处理/视图合成/3D重建",
        "问题": "如何在3D空间中进行细粒度的图像内容操作",
        "动机": "探索通过卷积神经网络在3D空间中进行图像内容操作的可能性，以实现更自然和直观的图像变换",
        "方法": "提出了一种名为可变换瓶颈网络（TBN）的新方法，通过直接在编码器-瓶颈-解码器架构中的体积瓶颈上应用空间变换，并利用多视图监督来学习空间解耦特征空间",
        "关键词": [
            "3D图像处理",
            "视图合成",
            "3D重建",
            "空间变换",
            "卷积神经网络"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）、编码器-瓶颈-解码器架构、空间变换、多视图监督、3D重建"
    },
    {
        "order": 183,
        "title": "Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_Improving_Pedestrian_Attribute_Recognition_With_Weakly-Supervised_Multi-Scale_Attribute-Specific_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tang_Improving_Pedestrian_Attribute_Recognition_With_Weakly-Supervised_Multi-Scale_Attribute-Specific_Localization_ICCV_2019_paper.html",
        "abstract": "Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K.",
        "中文标题": "通过弱监督多尺度属性特定定位改进行人属性识别",
        "摘要翻译": "行人属性识别已成为视频监控领域的一个新兴研究课题。为了预测特定属性的存在，需要定位与该属性相关的区域。然而，在此任务中，区域注释不可用。如何划分这些与属性相关的区域仍然具有挑战性。现有方法应用了属性无关的视觉注意力或启发式身体部位定位机制来增强局部特征表示，而忽略了利用属性来定义局部特征区域。我们提出了一个灵活的属性定位模块（ALM），以自适应地发现最具区分性的区域，并在多个层次上学习每个属性的区域特征。此外，还引入了特征金字塔架构，以在低层次上通过高层次语义指导增强属性特定的定位。所提出的框架不需要额外的区域注释，并且可以通过多层次深度监督进行端到端训练。大量实验表明，所提出的方法在三个行人属性数据集上实现了最先进的结果，包括PETA、RAP和PA-100K。",
        "领域": "行人属性识别/视频监控/特征定位",
        "问题": "如何在缺乏区域注释的情况下，有效地定位与行人属性相关的区域",
        "动机": "现有方法在增强局部特征表示时，未能充分利用属性来定义局部特征区域，导致识别效果受限",
        "方法": "提出了一个灵活的属性定位模块（ALM），通过自适应地发现最具区分性的区域，并在多个层次上学习每个属性的区域特征，同时引入特征金字塔架构以增强属性特定的定位",
        "关键词": [
            "行人属性识别",
            "属性定位模块",
            "特征金字塔架构"
        ],
        "涉及的技术概念": "属性定位模块（ALM）用于自适应地发现最具区分性的区域，并在多个层次上学习每个属性的区域特征；特征金字塔架构用于在低层次上通过高层次语义指导增强属性特定的定位。"
    },
    {
        "order": 184,
        "title": "Convex Shape Prior for Multi-Object Segmentation Using a Single Level Set Function",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Convex_Shape_Prior_for_Multi-Object_Segmentation_Using_a_Single_Level_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Convex_Shape_Prior_for_Multi-Object_Segmentation_Using_a_Single_Level_ICCV_2019_paper.html",
        "abstract": "Many objects in real world have convex shapes. It is a difficult task to have representations for convex shapes with good and fast numerical solutions. This paper proposes a method to incorporate convex shape prior for multi-object segmentation using level set method. The relationship between the convexity of the segmented objects and the signed distance function corresponding to their union is analyzed theoretically. This result is combined with Gaussian mixture method for the multiple objects segmentation with convexity shape prior. Alternating direction method of multiplier (ADMM) is adopted to solve the proposed model. Special boundary conditions are also imposed to obtain efficient algorithms for 4th order partial differential equations in one step of ADMM algorithm. In addition, our method only needs one level set function regardless of the number of objects. So the increase in the number of objects does not result in the increase of model and algorithm complexity. Various numerical experiments are illustrated to show the performance and advantages of the proposed method.",
        "中文标题": "使用单一水平集函数的多目标分割的凸形状先验",
        "摘要翻译": "现实世界中的许多物体具有凸形状。要获得具有良好且快速数值解的凸形状表示是一项困难的任务。本文提出了一种方法，利用水平集方法将凸形状先验融入多目标分割中。理论上分析了分割对象的凸性与它们联合的符号距离函数之间的关系。这一结果与高斯混合方法结合，用于具有凸形状先验的多目标分割。采用交替方向乘子法（ADMM）来解决所提出的模型。在ADMM算法的一个步骤中，还施加了特殊的边界条件，以获得第四阶偏微分方程的有效算法。此外，我们的方法只需要一个水平集函数，无论对象的数量如何。因此，对象数量的增加不会导致模型和算法复杂性的增加。通过各种数值实验展示了所提出方法的性能和优势。",
        "领域": "图像分割/凸优化/偏微分方程",
        "问题": "如何在多目标分割中有效地利用凸形状先验",
        "动机": "现实世界中的许多物体具有凸形状，但现有的方法难以在保证数值解质量的同时快速表示这些形状",
        "方法": "结合水平集方法和高斯混合方法，利用交替方向乘子法（ADMM）解决模型，并施加特殊边界条件以优化算法",
        "关键词": [
            "凸形状先验",
            "多目标分割",
            "水平集方法",
            "交替方向乘子法",
            "高斯混合方法"
        ],
        "涉及的技术概念": {
            "水平集方法": "一种用于图像分割的数值技术，通过水平集函数表示分割边界",
            "凸形状先验": "在图像分割中利用物体形状的凸性作为先验知识",
            "交替方向乘子法（ADMM）": "一种解决优化问题的算法，特别适用于大规模问题",
            "高斯混合方法": "一种统计方法，用于模型多个分布的数据",
            "第四阶偏微分方程": "在图像处理中用于描述图像特征的数学模型"
        }
    },
    {
        "order": 185,
        "title": "Correlation Congruence for Knowledge Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Correlation_Congruence_for_Knowledge_Distillation_ICCV_2019_paper.html",
        "abstract": "Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.",
        "中文标题": "知识蒸馏的相关一致性",
        "摘要翻译": "大多数基于知识蒸馏（KD）的师生框架依赖于实例层面的强一致性约束。然而，它们通常忽略了多个实例之间的相关性，这对于知识转移也是非常有价值的。在这项工作中，我们提出了一个新的框架，名为知识蒸馏的相关一致性（CCKD），它不仅传递实例层面的信息，还传递实例之间的相关性。此外，提出了一种基于泰勒级数展开的广义核方法，以更好地捕捉实例之间的相关性。在图像分类任务（包括CIFAR-100、ImageNet-1K）和度量学习任务（包括ReID和人脸识别）上的实证实验和消融研究表明，所提出的CCKD显著优于原始的KD和其他基于KD的最先进方法。CCKD可以轻松部署在大多数师生框架中，如KD和基于提示的学习方法。",
        "领域": "知识蒸馏/图像分类/度量学习",
        "问题": "如何有效地在知识蒸馏过程中利用实例之间的相关性",
        "动机": "现有的知识蒸馏方法主要关注实例层面的信息传递，而忽略了实例之间的相关性，这限制了知识转移的效果。",
        "方法": "提出了一个名为CCKD的新框架，通过引入实例之间的相关性来增强知识蒸馏的效果，并采用基于泰勒级数展开的广义核方法来捕捉这些相关性。",
        "关键词": [
            "知识蒸馏",
            "相关性",
            "泰勒级数展开",
            "图像分类",
            "度量学习"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "一种模型压缩技术，通过让一个较小的学生模型模仿一个较大的教师模型的输出来传递知识。",
            "相关性": "指实例之间的相互关系和依赖，这在知识蒸馏中被用来增强知识转移的效果。",
            "泰勒级数展开": "一种数学方法，用于近似复杂函数，这里用于捕捉实例之间的相关性。",
            "图像分类": "计算机视觉中的一项任务，旨在将图像分配到预定义的类别中。",
            "度量学习": "一种学习方法，旨在学习一个距离度量，使得相似的实例在度量空间中更接近。"
        }
    },
    {
        "order": 186,
        "title": "Customizing Student Networks From Heterogeneous Teachers via Adaptive Knowledge Amalgamation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Customizing_Student_Networks_From_Heterogeneous_Teachers_via_Adaptive_Knowledge_Amalgamation_ICCV_2019_paper.html",
        "abstract": "A massive number of well-trained deep networks have been released by developers online. These networks may focus on different tasks and in many cases are optimized for different datasets. In this paper, we study how to exploit such heterogeneous pre-trained networks, known as teachers, so as to train a customized student network that tackles a set of selective tasks defined by the user. We assume no human annotations are available, and each teacher may be either single- or multi-task. To this end, we introduce a dual-step strategy that first extracts the task-specific knowledge from the heterogeneous teachers sharing the same sub-task, and then amalgamates the extracted knowledge to build the student network. To facilitate the training, we employ a selective learning scheme where, for each unlabelled sample, the student learns adaptively from only the teacher with the least prediction ambiguity. We evaluate the proposed approach on several datasets and the experimental results demonstrate that the student, learned by such adaptive knowledge amalgamation, achieves performances even better than those of the teachers.",
        "中文标题": "通过自适应知识融合从异构教师网络中定制学生网络",
        "摘要翻译": "开发者在线发布了大量训练有素的深度网络。这些网络可能专注于不同的任务，并且在许多情况下针对不同的数据集进行了优化。在本文中，我们研究了如何利用这些异构的预训练网络（称为教师网络），以便训练一个定制的学生网络，该网络解决用户定义的一组选择性任务。我们假设没有可用的注释，每个教师网络可能是单任务或多任务的。为此，我们引入了一种双步骤策略，首先从共享相同子任务的异构教师网络中提取任务特定知识，然后融合提取的知识以构建学生网络。为了促进训练，我们采用了一种选择性学习方案，其中对于每个未标记的样本，学生仅从预测歧义最小的教师那里自适应地学习。我们在几个数据集上评估了所提出的方法，实验结果表明，通过这种自适应知识融合学习的学生网络，其性能甚至超过了教师网络。",
        "领域": "知识蒸馏/网络融合/自适应学习",
        "问题": "如何利用异构的预训练教师网络训练一个定制的学生网络，以解决用户定义的选择性任务",
        "动机": "开发者在线发布了大量训练有素的深度网络，这些网络可能专注于不同的任务并针对不同的数据集进行了优化，研究如何利用这些异构的预训练网络来训练定制的学生网络，以解决用户定义的选择性任务",
        "方法": "引入一种双步骤策略，首先从共享相同子任务的异构教师网络中提取任务特定知识，然后融合提取的知识以构建学生网络，并采用选择性学习方案，学生仅从预测歧义最小的教师那里自适应地学习",
        "关键词": [
            "知识蒸馏",
            "网络融合",
            "自适应学习"
        ],
        "涉及的技术概念": "深度网络、异构教师网络、任务特定知识、知识融合、选择性学习、预测歧义"
    },
    {
        "order": 187,
        "title": "Surface Networks via General Covers",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Haim_Surface_Networks_via_General_Covers_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Haim_Surface_Networks_via_General_Covers_ICCV_2019_paper.html",
        "abstract": "Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models including spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.",
        "中文标题": "通过通用覆盖的表面网络",
        "摘要翻译": "开发用于几何数据的深度学习技术是一个活跃且富有成果的研究领域。本文通过开发一种新颖的表面到图像表示法来解决球型表面学习的问题。使用这种表示法，我们能够快速地将成功的CNN模型适应到表面设置中。表面图像表示基于从图像域到表面的覆盖图。即，该图围绕表面多次包裹，确保表面的每个部分都在图像中得到良好表示。与之前的表面到图像表示不同，我们在单个图像中提供了所有表面部分的低失真覆盖。具体来说，对于学习球形信号的使用案例，我们的表示法为深度学习中使用的几种流行的球形参数化提供了低失真的替代方案。我们已使用表面到图像表示法将标准CNN架构应用于包括球形信号在内的3D模型。我们展示了我们的方法在形状检索、形状分类和语义形状分割任务上达到了最先进或可比较的结果。",
        "领域": "几何深度学习/球形信号处理/3D模型分析",
        "问题": "球型表面学习",
        "动机": "开发适用于几何数据的深度学习技术，特别是针对球型表面的学习问题，以提高3D模型分析的准确性和效率。",
        "方法": "开发了一种新颖的表面到图像表示法，通过覆盖图将图像域映射到表面，确保表面的每个部分都在图像中得到良好表示，并应用标准CNN架构进行3D模型分析。",
        "关键词": [
            "几何深度学习",
            "球形信号处理",
            "3D模型分析",
            "表面到图像表示",
            "CNN架构"
        ],
        "涉及的技术概念": "表面到图像表示法是一种将3D表面映射到2D图像的技术，通过覆盖图实现，使得3D表面的每个部分都能在2D图像中得到良好的表示。这种方法允许将标准的卷积神经网络（CNN）架构应用于3D模型，包括球形信号的处理，从而实现形状检索、形状分类和语义形状分割等任务。"
    },
    {
        "order": 188,
        "title": "Dynamic Curriculum Learning for Imbalanced Data Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Dynamic_Curriculum_Learning_for_Imbalanced_Data_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Dynamic_Curriculum_Learning_for_Imbalanced_Data_Classification_ICCV_2019_paper.html",
        "abstract": "Human attribute analysis is a challenging task in the field of computer vision. One of the significant difficulties is brought from largely imbalance-distributed data. Conventional techniques such as re-sampling and cost-sensitive learning require prior-knowledge to train the system. To address this problem, we propose a unified framework called Dynamic Curriculum Learning (DCL) to adaptively adjust the sampling strategy and loss weight in each batch, which results in better ability of generalization and discrimination. Inspired by curriculum learning, DCL consists of two-level curriculum schedulers: (1) sampling scheduler which manages the data distribution not only from imbalance to balance but also from easy to hard; (2) loss scheduler which controls the learning importance between classification and metric learning loss. With these two schedulers, we achieve state-of-the-art performance on the widely used face attribute dataset CelebA and pedestrian attribute dataset RAP.",
        "中文标题": "动态课程学习用于不平衡数据分类",
        "摘要翻译": "人类属性分析是计算机视觉领域中的一个具有挑战性的任务。其中一个主要困难来自于数据分布的大幅不平衡。传统技术如重采样和成本敏感学习需要先验知识来训练系统。为了解决这个问题，我们提出了一个名为动态课程学习（DCL）的统一框架，以自适应地调整每批次的采样策略和损失权重，从而获得更好的泛化和区分能力。受课程学习的启发，DCL由两级课程调度器组成：（1）采样调度器，它不仅管理从不平衡到平衡的数据分布，还管理从易到难的数据分布；（2）损失调度器，它控制分类和度量学习损失之间的学习重要性。通过这两个调度器，我们在广泛使用的人脸属性数据集CelebA和行人属性数据集RAP上实现了最先进的性能。",
        "领域": "人脸识别/行人识别/属性分析",
        "问题": "处理数据分布不平衡的问题",
        "动机": "传统方法需要先验知识，且难以有效处理数据不平衡问题",
        "方法": "提出动态课程学习（DCL）框架，通过两级课程调度器自适应调整采样策略和损失权重",
        "关键词": [
            "数据不平衡",
            "课程学习",
            "属性分析"
        ],
        "涉及的技术概念": {
            "动态课程学习（DCL）": "一种自适应调整采样策略和损失权重的框架，旨在提高模型在不平衡数据上的泛化和区分能力",
            "采样调度器": "管理数据分布，从不平衡到平衡，从易到难",
            "损失调度器": "控制分类和度量学习损失之间的学习重要性",
            "CelebA": "一个广泛使用的人脸属性数据集",
            "RAP": "一个行人属性数据集"
        }
    },
    {
        "order": 189,
        "title": "Data-Free Learning of Student Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Data-Free_Learning_of_Student_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Data-Free_Learning_of_Student_Networks_ICCV_2019_paper.html",
        "abstract": "Learning portable neural networks is very essential for computer vision for the purpose that pre-trained heavy deep models can be well applied on edge devices such as mobile phones and micro sensors. Most existing deep neural network compression and speed-up methods are very effective for training compact deep models, when we can directly access the training dataset. However, training data for the given deep network are often unavailable due to some practice problems (e.g. privacy, legal issue, and transmission), and the architecture of the given network are also unknown except some interfaces. To this end, we propose a novel framework for training efficient deep neural networks by exploiting generative adversarial networks (GANs). To be specific, the pre-trained teacher networks are regarded as a fixed discriminator and the generator is utilized for derivating training samples which can obtain the maximum response on the discriminator. Then, an efficient network with smaller model size and computational complexity is trained using the generated data and the teacher network, simultaneously. Efficient student networks learned using the proposed Data-Free Learning (DFL) method achieve 92.22% and 74.47% accuracies without any training data on the CIFAR-10 and CIFAR-100 datasets, respectively. Meanwhile, our student network obtains an 80.56% accuracy on the CelebA benchmark.",
        "中文标题": "无数据学习学生网络",
        "摘要翻译": "学习便携式神经网络对于计算机视觉至关重要，目的是使预训练的重型深度模型能够很好地应用于边缘设备，如手机和微型传感器。大多数现有的深度神经网络压缩和加速方法在我们可以直接访问训练数据集时，对于训练紧凑的深度模型非常有效。然而，由于一些实际问题（例如隐私、法律问题和传输），给定深度网络的训练数据通常不可用，除了某些接口外，给定网络的架构也是未知的。为此，我们提出了一种新颖的框架，通过利用生成对抗网络（GANs）来训练高效的深度神经网络。具体来说，预训练的教师网络被视为固定的判别器，生成器用于导出可以在判别器上获得最大响应的训练样本。然后，使用生成的数据和教师网络同时训练一个具有较小模型大小和计算复杂性的高效网络。使用提出的无数据学习（DFL）方法学习的高效学生网络在CIFAR-10和CIFAR-100数据集上分别实现了92.22%和74.47%的准确率，而无需任何训练数据。同时，我们的学生网络在CelebA基准测试中获得了80.56%的准确率。",
        "领域": "神经网络压缩/生成对抗网络/边缘计算",
        "问题": "在无法直接访问训练数据集的情况下，如何训练高效的深度神经网络",
        "动机": "解决由于隐私、法律问题和传输等实际问题导致的训练数据不可用的问题",
        "方法": "利用生成对抗网络（GANs）生成训练样本，并使用这些样本和教师网络训练高效的学生网络",
        "关键词": [
            "神经网络压缩",
            "生成对抗网络",
            "边缘计算",
            "无数据学习"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）用于生成训练样本，预训练的教师网络作为固定的判别器，生成器用于导出训练样本，高效学生网络的训练"
    },
    {
        "order": 190,
        "title": "SSAP: Single-Shot Instance Segmentation With Affinity Pyramid",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_SSAP_Single-Shot_Instance_Segmentation_With_Affinity_Pyramid_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_SSAP_Single-Shot_Instance_Segmentation_With_Affinity_Pyramid_ICCV_2019_paper.html",
        "abstract": "Recently, proposal-free instance segmentation has received increasing attention due to its concise and efficient pipeline. Generally, proposal-free methods generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. We argue that treating these two sub-tasks separately is suboptimal. In fact, employing multiple separate modules significantly reduces the potential for application. The mutual benefits between the two complementary sub-tasks are also unexplored. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on a pixel-pair affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. The affinity pyramid can also be jointly learned with the semantic class labeling and achieve mutual benefits. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition module is presented to sequentially generate instances from coarse to fine. Unlike previous time-consuming graph partition methods, this module achieves 5x speedup and 9% relative improvement on Average-Precision (AP). Our approach achieves new state of the art on the challenging Cityscapes dataset.",
        "中文标题": "SSAP：基于亲和力金字塔的单次实例分割",
        "摘要翻译": "最近，由于简洁高效的流程，无提议实例分割受到了越来越多的关注。通常，无提议方法生成实例无关的语义分割标签和实例感知特征，以将像素分组到不同的对象实例中。然而，以前的方法大多为这两个子任务使用单独的模块，并且需要多次推理。我们认为，将这两个子任务分开处理是次优的。事实上，使用多个单独的模块显著降低了应用的潜力。这两个互补子任务之间的相互利益也未被探索。为此，本工作提出了一种单次无提议实例分割方法，仅需一次预测。我们的方法基于像素对亲和力金字塔，它以分层方式计算两个像素属于同一实例的概率。亲和力金字塔也可以与语义类别标签联合学习，并实现相互利益。此外，结合学习到的亲和力金字塔，提出了一种新颖的级联图分割模块，以从粗到细顺序生成实例。与以前耗时的图分割方法不同，该模块实现了5倍的速度提升和9%的平均精度（AP）相对改进。我们的方法在具有挑战性的Cityscapes数据集上达到了新的最先进水平。",
        "领域": "实例分割/语义分割/图分割",
        "问题": "如何高效地进行单次实例分割",
        "动机": "探索无提议实例分割方法中两个互补子任务之间的相互利益，提高应用潜力",
        "方法": "提出基于像素对亲和力金字塔的单次无提议实例分割方法，结合级联图分割模块从粗到细生成实例",
        "关键词": [
            "实例分割",
            "语义分割",
            "图分割",
            "亲和力金字塔",
            "级联图分割"
        ],
        "涉及的技术概念": {
            "无提议实例分割": "一种不依赖于提议框的实例分割方法，直接生成实例分割结果",
            "像素对亲和力金字塔": "一种分层计算两个像素属于同一实例概率的方法",
            "级联图分割模块": "一种从粗到细顺序生成实例的图分割方法，提高了分割效率和精度"
        }
    },
    {
        "order": 191,
        "title": "Video Face Clustering With Unknown Number of Clusters",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tapaswi_Video_Face_Clustering_With_Unknown_Number_of_Clusters_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tapaswi_Video_Face_Clustering_With_Unknown_Number_of_Clusters_ICCV_2019_paper.html",
        "abstract": "Understanding videos such as TV series and movies requires analyzing who the characters are and what they are doing. We address the challenging problem of clustering face tracks based on their identity. Different from previous work in this area, we choose to operate in a realistic and difficult setting where: (i) the number of characters is not known a priori; and (ii) face tracks belonging to minor or background characters are not discarded. To this end, we propose Ball Cluster Learning (BCL), a supervised approach to carve the embedding space into balls of equal size, one for each cluster. The learned ball radius is easily translated to a stopping criterion for iterative merging algorithms. This gives BCL the ability to estimate the number of clusters as well as their assignment, achieving promising results on commonly used datasets. We also present a thorough discussion of how existing metric learning literature can be adapted for this task.",
        "中文标题": "未知聚类数量的视频人脸聚类",
        "摘要翻译": "理解如电视剧和电影等视频内容需要分析角色是谁以及他们在做什么。我们解决了基于身份的人脸轨迹聚类的挑战性问题。与这一领域的先前工作不同，我们选择在一个现实且困难的环境下操作，其中：(i) 角色的数量不是先验已知的；(ii) 属于次要或背景角色的人脸轨迹不会被丢弃。为此，我们提出了球聚类学习（BCL），一种监督方法，将嵌入空间划分为大小相等的球，每个球对应一个聚类。学习到的球半径很容易转化为迭代合并算法的停止标准。这使得BCL能够估计聚类的数量及其分配，在常用数据集上取得了有希望的结果。我们还详细讨论了如何将现有的度量学习文献适应于这一任务。",
        "领域": "视频分析/人脸识别/聚类分析",
        "问题": "在未知角色数量的情况下，对视频中的人脸轨迹进行基于身份的聚类",
        "动机": "为了更准确地理解视频内容，需要识别和分析视频中的角色及其行为，特别是在角色数量未知且包含次要或背景角色的情况下",
        "方法": "提出了一种名为球聚类学习（BCL）的监督方法，通过将嵌入空间划分为大小相等的球来估计聚类的数量及其分配",
        "关键词": [
            "视频分析",
            "人脸识别",
            "聚类分析"
        ],
        "涉及的技术概念": "球聚类学习（BCL）是一种监督学习方法，用于在嵌入空间中创建大小相等的球，每个球代表一个聚类。这种方法通过学习的球半径来确定迭代合并算法的停止标准，从而估计聚类的数量及其分配。此外，还讨论了如何将现有的度量学习技术适应于这一任务。"
    },
    {
        "order": 192,
        "title": "Deep Closest Point: Learning Representations for Point Cloud Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.html",
        "abstract": "Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.",
        "中文标题": "深度最近点：学习点云配准的表示",
        "摘要翻译": "点云配准是应用于机器人、医学成像等领域的一个关键问题。这个问题涉及找到一个刚体变换，将一个点云对齐到另一个点云。迭代最近点（ICP）及其变体为此任务提供了简单且易于实现的迭代方法，但这些算法可能会收敛到虚假的局部最优解。为了解决ICP流程中的局部最优解和其他困难，我们提出了一种基于学习的方法，名为深度最近点（DCP），灵感来源于计算机视觉和自然语言处理中的最新技术。我们的模型由三部分组成：一个点云嵌入网络，一个结合指针生成层的基于注意力的模块以近似组合匹配，以及一个可微分的奇异值分解（SVD）层以提取最终的刚体变换。我们在ModelNet40数据集上端到端地训练我们的模型，并在多个设置中展示其性能优于ICP及其变体（例如，Go-ICP，FGR）以及最近提出的基于学习的方法PointNetLK。除了提供一种最先进的配准技术外，我们还评估了我们学习到的特征转移到未见过的对象上的适用性。我们还提供了对我们学习到的模型的初步分析，以帮助理解领域特定和/或全局特征是否促进了刚体配准。",
        "领域": "点云配准/机器人/医学成像",
        "问题": "点云配准中的局部最优解和其他困难",
        "动机": "解决ICP流程中的局部最优解和其他困难，提高点云配准的准确性和效率",
        "方法": "提出了一种基于学习的方法，包括点云嵌入网络、基于注意力的模块结合指针生成层近似组合匹配，以及可微分的奇异值分解（SVD）层提取最终的刚体变换",
        "关键词": [
            "点云配准",
            "迭代最近点",
            "深度学习"
        ],
        "涉及的技术概念": "点云配准涉及找到一个刚体变换，将一个点云对齐到另一个点云。迭代最近点（ICP）及其变体是常用的方法，但可能收敛到局部最优解。深度最近点（DCP）是一种基于学习的方法，通过点云嵌入网络、基于注意力的模块和可微分的奇异值分解（SVD）层来提高配准的准确性和效率。"
    },
    {
        "order": 193,
        "title": "Learning Propagation for Arbitrarily-Structured Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_Propagation_for_Arbitrarily-Structured_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_Propagation_for_Arbitrarily-Structured_Data_ICCV_2019_paper.html",
        "abstract": "Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels, and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.",
        "中文标题": "学习任意结构数据的传播",
        "摘要翻译": "处理包含任意结构的输入信号，例如超像素和点云，仍然是计算机视觉中的一大挑战。线性扩散，一种有效的图像处理模型，最近已与深度学习算法集成。在本文中，我们提出通过空间广义传播网络（SGPN）以全局方式学习数据点之间的成对关系，以改进任意结构数据的语义分割。该网络通过学习的线性扩散过程在一组表示任意结构数据的图上传播信息。该模块灵活，可以嵌入并与多种类型的网络（例如CNN）联合训练。我们在语义分割网络上进行实验，使用我们的传播模块对不同数据——图像、超像素和点云——进行联合训练。我们展示了与不包含此模块的网络相比，SGPN一致提高了像素和点云分割的性能。我们的方法提出了一种有效的方式来建模任意结构数据的全局成对关系。",
        "领域": "语义分割/图神经网络/点云处理",
        "问题": "处理包含任意结构的输入信号，如超像素和点云，以提高语义分割的准确性",
        "动机": "为了改进对任意结构数据的语义分割性能，需要一种能够全局建模数据点之间成对关系的方法",
        "方法": "提出空间广义传播网络（SGPN），通过学习的线性扩散过程在表示任意结构数据的图上传播信息，并与多种类型的网络联合训练",
        "关键词": [
            "语义分割",
            "图神经网络",
            "点云处理",
            "线性扩散",
            "空间广义传播网络"
        ],
        "涉及的技术概念": {
            "线性扩散": "一种有效的图像处理模型，最近已与深度学习算法集成",
            "空间广义传播网络（SGPN）": "一种通过学习的线性扩散过程在表示任意结构数据的图上传播信息的网络",
            "联合训练": "将SGPN模块与多种类型的网络（如CNN）联合训练，以提高语义分割的性能"
        }
    },
    {
        "order": 194,
        "title": "Targeted Mismatch Adversarial Attack: Query With a Flower to Retrieve the Tower",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tolias_Targeted_Mismatch_Adversarial_Attack_Query_With_a_Flower_to_Retrieve_ICCV_2019_paper.html",
        "abstract": "Access to online visual search engines implies sharing of private user content -- the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.",
        "中文标题": "目标不匹配对抗攻击：用一朵花查询以检索塔",
        "摘要翻译": "访问在线视觉搜索引擎意味着共享用户的私人内容——查询图像。我们引入了针对基于深度学习的检索系统的目标不匹配攻击概念，以生成对抗性图像来隐藏查询图像。生成的图像看起来与用户意图查询的内容完全不同，但导致相同或非常相似的检索结果。将攻击转移到完全未见过的网络是具有挑战性的。我们通过为对抗性图像构建设计各种损失函数，展示了针对部分未知系统的成功攻击。这些包括例如针对检索系统中未知的全局池化操作或未知输入分辨率的损失函数。我们在标准检索基准上评估了这些攻击，并比较了使用原始图像和对抗性图像检索的结果。",
        "领域": "对抗性攻击/图像检索/深度学习安全",
        "问题": "如何在基于深度学习的检索系统中生成对抗性图像以隐藏查询图像，同时保持检索结果的相似性",
        "动机": "保护用户隐私，防止通过查询图像泄露用户意图",
        "方法": "设计各种损失函数来构建对抗性图像，包括针对未知全局池化操作和未知输入分辨率的损失函数",
        "关键词": [
            "对抗性攻击",
            "图像检索",
            "深度学习安全",
            "隐私保护"
        ],
        "涉及的技术概念": "对抗性图像生成、损失函数设计、全局池化操作、输入分辨率、图像检索系统"
    },
    {
        "order": 195,
        "title": "Orientation-Aware Semantic Segmentation on Icosahedron Spheres",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Orientation-Aware_Semantic_Segmentation_on_Icosahedron_Spheres_ICCV_2019_paper.html",
        "abstract": "We address semantic segmentation on omnidirectional images, to leverage a holistic understanding of the surrounding scene for applications like autonomous driving systems. For the spherical domain, several methods recently adopt an icosahedron mesh, but systems are typically rotation invariant or require significant memory and parameters, thus enabling execution only at very low resolutions. In our work, we propose an orientation-aware CNN framework for the icosahedron mesh. Our representation allows for fast network operations, as our design simplifies to standard network operations of classical CNNs, but under consideration of north-aligned kernel convolutions for features on the sphere. We implement our representation and demonstrate its memory efficiency up-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular images). Finally, since our kernels operate on the tangent of the sphere, standard feature weights, pretrained on perspective data, can be directly transferred with only small need for weight refinement. In our evaluation our orientation-aware CNN becomes a new state of the art for the recent 2D3DS dataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant classification and segmentation tasks are additionally presented for comparison to prior art.",
        "中文标题": "面向二十面体球体的方向感知语义分割",
        "摘要翻译": "我们解决了全向图像的语义分割问题，以利用对周围场景的整体理解，应用于如自动驾驶系统等领域。对于球面域，最近有几种方法采用了二十面体网格，但系统通常是旋转不变的或需要大量的内存和参数，因此只能在非常低的分辨率下执行。在我们的工作中，我们提出了一个面向二十面体网格的方向感知CNN框架。我们的表示允许快速的网络操作，因为我们的设计简化为经典CNN的标准网络操作，但考虑了球面上特征的北对齐核卷积。我们实现了我们的表示，并展示了其在高达8级分辨率网格（相当于640 x 1024等距柱面图像）上的内存效率。最后，由于我们的核在球面的切线上操作，标准的特征权重，在透视数据上预训练的，可以直接转移，只需要少量的权重细化。在我们的评估中，我们的方向感知CNN成为了最新的2D3DS数据集和我们的Omni-SYNTHIA版本的SYNTHIA的新技术状态。旋转不变的分类和分割任务也被提出用于与现有技术的比较。",
        "领域": "自动驾驶/全向图像处理/语义分割",
        "问题": "解决全向图像在球面域上的语义分割问题，特别是在二十面体网格上的高效处理",
        "动机": "为了利用对周围场景的整体理解，特别是在自动驾驶系统等应用中，需要一种能够在球面域上高效执行语义分割的方法",
        "方法": "提出了一个面向二十面体网格的方向感知CNN框架，通过北对齐核卷积简化网络操作，实现快速且内存高效的语义分割",
        "关键词": [
            "全向图像",
            "二十面体网格",
            "方向感知CNN",
            "语义分割",
            "自动驾驶"
        ],
        "涉及的技术概念": "方向感知CNN框架通过北对齐核卷积在球面上进行特征处理，允许使用预训练的特征权重，简化了网络操作并提高了内存效率。这种方法特别适用于处理全向图像，如自动驾驶系统中的场景理解。"
    },
    {
        "order": 196,
        "title": "MultiSeg: Semantically Meaningful, Scale-Diverse Segmentations From Minimal User Input",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.html",
        "abstract": "Existing deep learning-based interactive image segmentation approaches typically assume the target-of-interest is always a single object and fail to account for the potential diversity in user expectations, thus requiring excessive user input when it comes to segmenting an object part or a group of objects instead. Motivated by the observation that the object part, full object, and a collection of objects essentially differ in size, we propose a new concept called scale-diversity, which characterizes the spectrum of segmentations w.r.t. different scales. To address this, we present MultiSeg, a scale-diverse interactive image segmentation network that incorporates a set of two-dimensional scale priors into the model to generate a set of scale-varying proposals that conform to the user input. We explicitly encourage segmentation diversity during training by synthesizing diverse training samples for a given image. As a result, our method allows the user to quickly locate the closest segmentation target for further refinement if necessary. Despite its simplicity, experimental results demonstrate that our proposed model is capable of quickly producing diverse yet plausible segmentation outputs, reducing the user interaction required, especially in cases where many types of segmentations (object parts or groups) are expected.",
        "中文标题": "MultiSeg：从最小用户输入中获取语义上有意义、尺度多样的分割",
        "摘要翻译": "现有的基于深度学习的交互式图像分割方法通常假设目标总是单个对象，未能考虑到用户期望的潜在多样性，因此在分割对象部分或对象组时需要过多的用户输入。基于观察到对象部分、完整对象和对象集合在大小上本质上有所不同，我们提出了一个新概念，称为尺度多样性，它描述了不同尺度下的分割谱。为了解决这个问题，我们提出了MultiSeg，一个尺度多样的交互式图像分割网络，它将一组二维尺度先验纳入模型，以生成一组符合用户输入的尺度变化提案。我们在训练期间通过为给定图像合成多样化的训练样本来明确鼓励分割多样性。因此，我们的方法允许用户快速定位最接近的分割目标以进行进一步细化。尽管其简单，实验结果表明，我们提出的模型能够快速生成多样化且合理的分割输出，减少所需的用户交互，特别是在预期有多种类型的分割（对象部分或组）的情况下。",
        "领域": "图像分割/交互式分割/尺度多样性",
        "问题": "现有交互式图像分割方法在处理对象部分或对象组时，因假设目标总是单个对象而需要过多用户输入",
        "动机": "观察到对象部分、完整对象和对象集合在大小上本质上有所不同，提出尺度多样性概念以描述不同尺度下的分割谱",
        "方法": "提出MultiSeg，一个尺度多样的交互式图像分割网络，通过将二维尺度先验纳入模型生成尺度变化提案，并在训练期间合成多样化训练样本鼓励分割多样性",
        "关键词": [
            "图像分割",
            "交互式分割",
            "尺度多样性"
        ],
        "涉及的技术概念": "尺度多样性：描述不同尺度下的分割谱；MultiSeg：一个尺度多样的交互式图像分割网络，通过将二维尺度先验纳入模型生成尺度变化提案；训练样本合成：在训练期间为给定图像合成多样化的训练样本以鼓励分割多样性"
    },
    {
        "order": 197,
        "title": "Fashion++: Minimal Edits for Outfit Improvement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hsiao_Fashion_Minimal_Edits_for_Outfit_Improvement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hsiao_Fashion_Minimal_Edits_for_Outfit_Improvement_ICCV_2019_paper.html",
        "abstract": "Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new computer vision challenge. We introduce Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. Our model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. We show how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. The edits suggested range from swapping in a new garment to tweaking its color, how it is worn (e.g., rolling up sleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that Fashion++ provides successful edits, both according to automated metrics and human opinion.",
        "中文标题": "Fashion++：服装改进的最小编辑",
        "摘要翻译": "给定一套服装，哪些小的改动最能提升其时尚感？这个问题提出了一个引人入胜的新计算机视觉挑战。我们介绍了Fashion++，一种提出对全身服装进行最小调整的方法，这些调整将对其时尚性产生最大影响。我们的模型由一个深度图像生成神经网络组成，该网络学习根据学习的每件服装编码合成服装。潜在编码根据形状和纹理明确分解，从而允许分别直接编辑适合/展示和颜色/图案/材料。我们展示了如何利用网络照片自动训练时尚性模型，并开发了一种激活最大化风格的方法，将输入图像转换为其更时尚的自我。建议的编辑范围从换入新服装到调整其颜色、穿着方式（例如，卷起袖子）或合身度（例如，使裤子更宽松）。实验表明，Fashion++提供了成功的编辑，无论是根据自动化指标还是人类意见。",
        "领域": "时尚分析/图像生成/服装设计",
        "问题": "如何通过最小编辑提升服装的时尚性",
        "动机": "探索通过计算机视觉技术自动提出提升服装时尚性的最小编辑方案",
        "方法": "使用深度图像生成神经网络，基于每件服装的编码合成服装，并通过激活最大化风格的方法进行图像转换",
        "关键词": [
            "时尚分析",
            "图像生成",
            "服装设计"
        ],
        "涉及的技术概念": "深度图像生成神经网络、每件服装编码、形状和纹理的潜在编码分解、激活最大化风格的方法"
    },
    {
        "order": 198,
        "title": "Differentiable Learning-to-Group Channels via Groupable Convolutional Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Differentiable_Learning-to-Group_Channels_via_Groupable_Convolutional_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Differentiable_Learning-to-Group_Channels_via_Groupable_Convolutional_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "Group convolution, which divides the channels of ConvNets into groups, has achieved impressive improvement over the regular convolution operation. However, existing models, e.g. ResNext, still suffers from the sub-optimal performance due to manually defining the number of groups as a constant over all of the layers. Toward addressing this issue, we present Groupable ConvNet (GroupNet) built by using a novel dynamic grouping convolution (DGConv) operation, which is able to learn the number of groups in an end-to-end manner. The proposed approach has several appealing benefits. (1) DGConv provides a unified convolution representation and covers many existing convolution operations such as regular dense convolution, group convolution, and depthwise convolution. (2) DGConv is a differentiable and flexible operation which learns to perform various convolutions from training data. (3) GroupNet trained with DGConv learns different number of groups for different convolution layers. Extensive experiments demonstrate that GroupNet outperforms its counterparts such as ResNet and ResNeXt in terms of accuracy and computational complexity. We also present introspection and reproducibility study, for the first time, showing the learning dynamics of training group numbers.",
        "中文标题": "通过可分组卷积神经网络实现可微分学习分组通道",
        "摘要翻译": "分组卷积通过将卷积网络的通道划分为组，相比常规卷积操作取得了显著的改进。然而，现有模型，如ResNext，由于手动定义所有层的组数为常数，仍然存在性能不佳的问题。为了解决这个问题，我们提出了使用新型动态分组卷积（DGConv）操作构建的可分组卷积网络（GroupNet），该操作能够以端到端的方式学习组数。所提出的方法有几个吸引人的优点。（1）DGConv提供了一个统一的卷积表示，并涵盖了许多现有的卷积操作，如常规密集卷积、分组卷积和深度卷积。（2）DGConv是一个可微分且灵活的操作，能够从训练数据中学习执行各种卷积。（3）使用DGConv训练的GroupNet为不同的卷积层学习不同的组数。大量实验证明，GroupNet在准确性和计算复杂度方面优于其对应物，如ResNet和ResNeXt。我们还首次提出了内省和可重复性研究，展示了训练组数的学习动态。",
        "领域": "卷积神经网络/动态分组卷积/深度学习优化",
        "问题": "手动定义卷积网络中所有层的组数为常数导致的性能不佳问题",
        "动机": "提高卷积网络的性能和灵活性，通过自动学习不同层的组数来优化模型",
        "方法": "提出了一种新型动态分组卷积（DGConv）操作，构建了可分组卷积网络（GroupNet），以端到端的方式学习组数",
        "关键词": [
            "分组卷积",
            "动态分组卷积",
            "卷积神经网络优化"
        ],
        "涉及的技术概念": {
            "分组卷积": "一种将卷积网络的通道划分为组的卷积操作，旨在提高模型的性能和效率。",
            "动态分组卷积（DGConv）": "一种新型卷积操作，能够自动学习卷积网络中不同层的组数，以提高模型的灵活性和性能。",
            "可分组卷积网络（GroupNet）": "使用动态分组卷积操作构建的卷积网络，能够为不同的卷积层学习不同的组数。",
            "端到端学习": "一种学习方法，模型直接从输入数据中学习到输出，无需人工干预。"
        }
    },
    {
        "order": 199,
        "title": "Robust Motion Segmentation From Pairwise Matches",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Arrigoni_Robust_Motion_Segmentation_From_Pairwise_Matches_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Arrigoni_Robust_Motion_Segmentation_From_Pairwise_Matches_ICCV_2019_paper.html",
        "abstract": "In this paper we consider the problem of motion segmentation, where only pairwise correspondences are assumed as input without prior knowledge about tracks. The problem is formulated as a two-step process. First, motion segmentation is performed on image pairs independently. Secondly, we combine independent pairwise segmentation results in a robust way into the final globally consistent segmentation. Our approach is inspired by the success of averaging methods. We demonstrate in simulated as well as in real experiments that our method is very effective in reducing the errors in the pairwise motion segmentation and can cope with large number of mismatches.",
        "中文标题": "从成对匹配中进行鲁棒的运动分割",
        "摘要翻译": "在本文中，我们考虑了运动分割的问题，其中仅假设成对对应关系作为输入，而没有关于轨迹的先验知识。该问题被表述为一个两步过程。首先，独立地在图像对上进行运动分割。其次，我们以鲁棒的方式将独立的成对分割结果结合成最终的全局一致分割。我们的方法受到平均方法成功的启发。我们在模拟和真实实验中证明，我们的方法在减少成对运动分割中的错误方面非常有效，并且能够处理大量的不匹配。",
        "领域": "运动分割/图像处理/鲁棒性分析",
        "问题": "在没有轨迹先验知识的情况下，仅使用成对对应关系进行运动分割",
        "动机": "解决成对运动分割中的错误和不匹配问题，提高分割的准确性和鲁棒性",
        "方法": "采用两步过程：首先独立地在图像对上进行运动分割，然后以鲁棒的方式将结果结合成全局一致分割",
        "关键词": [
            "运动分割",
            "成对匹配",
            "鲁棒性",
            "全局一致性"
        ],
        "涉及的技术概念": "运动分割是指在视频或图像序列中，将运动对象从背景中分离出来的过程。成对匹配指的是在两幅图像之间找到对应的特征点或区域。鲁棒性分析关注的是算法在面对噪声、异常值或错误输入时的稳定性和可靠性。全局一致性指的是在整个数据集或图像序列中保持分割结果的一致性和准确性。"
    },
    {
        "order": 200,
        "title": "Semi-Supervised Pedestrian Instance Synthesis and Detection With Mutual Reinforcement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Semi-Supervised_Pedestrian_Instance_Synthesis_and_Detection_With_Mutual_Reinforcement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Semi-Supervised_Pedestrian_Instance_Synthesis_and_Detection_With_Mutual_Reinforcement_ICCV_2019_paper.html",
        "abstract": "We propose a GAN-based scene-specific instance synthesis and classification model for semi-supervised pedestrian detection. Instead of collecting unreliable detections from unlabeled data, we adopt a class-conditional GAN for synthesizing pedestrian instances to alleviate the problem of insufficient labeled data. With the help of a base detector, we integrate pedestrian instance synthesis and detection by including a post-refinement classifier (PRC) into a minimax game. A generator and the PRC can mutually reinforce each other by synthesizing high-fidelity pedestrian instances and providing more accurate categorical information. Both of them compete with a class-conditional discriminator and a class-specific discriminator, such that the four fundamental networks in our model can be jointly trained. In our experiments, we validate that the proposed model significantly improves the performance of the base detector and achieves state-of-the-art results on multiple benchmarks. As shown in Figure 1, the result indicates the possibility of using inexpensively synthesized instances for improving semi-supervised detection models.",
        "中文标题": "半监督行人实例合成与检测的相互强化",
        "摘要翻译": "我们提出了一种基于GAN的场景特定实例合成和分类模型，用于半监督行人检测。我们采用类条件GAN来合成行人实例，以缓解标记数据不足的问题，而不是从未标记数据中收集不可靠的检测结果。在基础检测器的帮助下，我们通过将后细化分类器（PRC）纳入极小极大游戏中，将行人实例合成与检测结合起来。生成器和PRC可以通过合成高保真行人实例和提供更准确的分类信息相互强化。它们与类条件判别器和类特定判别器竞争，使得我们模型中的四个基本网络可以联合训练。在我们的实验中，我们验证了所提出的模型显著提高了基础检测器的性能，并在多个基准测试中达到了最先进的结果。如图1所示，结果表明使用低成本合成的实例来改进半监督检测模型的可能性。",
        "领域": "行人检测/实例合成/半监督学习",
        "问题": "解决半监督行人检测中标记数据不足的问题",
        "动机": "通过合成高保真行人实例来增强半监督检测模型的性能",
        "方法": "采用类条件GAN合成行人实例，并通过后细化分类器（PRC）与基础检测器结合，实现行人实例合成与检测的相互强化",
        "关键词": [
            "GAN",
            "行人检测",
            "实例合成",
            "半监督学习",
            "后细化分类器"
        ],
        "涉及的技术概念": {
            "GAN": "生成对抗网络，用于生成数据实例",
            "类条件GAN": "一种特定类型的GAN，能够根据类别标签生成特定类别的实例",
            "后细化分类器（PRC）": "用于进一步细化分类结果的分类器",
            "极小极大游戏": "一种优化策略，用于在竞争环境中训练模型",
            "基础检测器": "用于初步检测行人的模型"
        }
    },
    {
        "order": 201,
        "title": "HarDNet: A Low Memory Traffic Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chao_HarDNet_A_Low_Memory_Traffic_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chao_HarDNet_A_Low_Memory_Traffic_Network_ICCV_2019_paper.html",
        "abstract": "State-of-the-art neural network architectures such as ResNet, MobileNet, and DenseNet have achieved outstanding accuracy over low MACs and small model size counterparts. However, these metrics might not be accurate for predicting the inference time. We suggest that memory traffic for accessing intermediate feature maps can be a factor dominating the inference latency, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video. We propose a Harmonic Densely Connected Network to achieve high efficiency in terms of both low MACs and memory traffic. The new network achieves 35%, 36%, 30%, 32%, and 45% inference time reduction compared with FC-DenseNet-103, DenseNet-264, ResNet-50, ResNet-152, and SSD-VGG, respectively. We use tools including Nvidia profiler and ARM Scale-Sim to measure the memory traffic and verify that the inference latency is indeed proportional to the memory traffic consumption and the proposed network consumes low memory traffic. We conclude that one should take memory traffic into consideration when designing neural network architectures for high-resolution applications at the edge.",
        "中文标题": "HarDNet: 一种低内存流量网络",
        "摘要翻译": "诸如ResNet、MobileNet和DenseNet等最先进的神经网络架构在低MACs和小模型尺寸的对手中取得了卓越的准确性。然而，这些指标可能不足以准确预测推理时间。我们提出，访问中间特征图的内存流量可能是主导推理延迟的一个因素，特别是在高分辨率视频的实时目标检测和语义分割等任务中。我们提出了一种谐波密集连接网络，以在低MACs和内存流量方面实现高效率。与FC-DenseNet-103、DenseNet-264、ResNet-50、ResNet-152和SSD-VGG相比，新网络分别实现了35%、36%、30%、32%和45%的推理时间减少。我们使用包括Nvidia分析器和ARM Scale-Sim在内的工具来测量内存流量，并验证推理延迟确实与内存流量消耗成正比，且所提出的网络消耗低内存流量。我们得出结论，在设计用于边缘高分辨率应用的神经网络架构时，应考虑内存流量。",
        "领域": "实时目标检测/语义分割/神经网络架构设计",
        "问题": "如何减少神经网络在高分辨率视频处理任务中的推理时间",
        "动机": "现有的神经网络架构在低MACs和小模型尺寸上取得了高准确性，但这些指标可能不足以准确预测推理时间，特别是在高分辨率视频的实时目标检测和语义分割任务中，内存流量成为推理延迟的主导因素。",
        "方法": "提出了一种谐波密集连接网络（HarDNet），通过优化内存流量来减少推理时间，并使用Nvidia分析器和ARM Scale-Sim等工具验证了推理延迟与内存流量消耗的正比关系。",
        "关键词": [
            "实时目标检测",
            "语义分割",
            "神经网络架构设计",
            "内存流量优化"
        ],
        "涉及的技术概念": {
            "MACs": "乘加操作次数，是衡量神经网络计算复杂度的指标之一。",
            "内存流量": "指在神经网络推理过程中，访问和传输中间特征图所需的数据量，是影响推理延迟的关键因素之一。",
            "谐波密集连接网络": "一种新型的神经网络架构，旨在通过优化内存流量来减少推理时间，适用于高分辨率视频处理任务。",
            "Nvidia分析器": "一种用于测量和分析GPU性能的工具，包括内存流量和推理延迟等指标。",
            "ARM Scale-Sim": "一种模拟器，用于在ARM架构上模拟和测量神经网络的内存流量和性能。"
        }
    },
    {
        "order": 202,
        "title": "InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fang_InstaBoost_Boosting_Instance_Segmentation_via_Probability_Map_Guided_Copy-Pasting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fang_InstaBoost_Boosting_Instance_Segmentation_via_Probability_Map_Guided_Copy-Pasting_ICCV_2019_paper.html",
        "abstract": "Instance segmentation requires a large number of training samples to achieve satisfactory performance and benefits from proper data augmentation. To enlarge the training set and increase the diversity, previous methods have investigated using data annotation from other domain (e.g. bbox, point) in a weakly supervised mechanism. In this paper, we present a simple, efficient and effective method to augment the training set using the existing instance mask annotations. Exploiting the pixel redundancy of the background, we are able to improve the performance of Mask R-CNN for 1.7 mAP on COCO dataset and 3.3 mAP on Pascal VOC dataset by simply introducing random jittering to objects. Furthermore, we propose a location probability map based approach to explore the feasible locations that objects can be placed based on local appearance similarity. With the guidance of such map, we boost the performance of R101-Mask R-CNN on instance segmentation from 35.7 mAP to 37.9 mAP without modifying the backbone or network structure. Our method is simple to implement and does not increase the computational complexity. It can be integrated into the training pipeline of any instance segmentation model without affecting the training and inference efficiency. Our code and models have been released at https://github.com/GothicAi/InstaBoost.",
        "中文标题": "InstaBoost：通过概率图引导的复制粘贴提升实例分割",
        "摘要翻译": "实例分割需要大量的训练样本以达到满意的性能，并且从适当的数据增强中受益。为了扩大训练集并增加多样性，以前的方法已经研究了在弱监督机制下使用来自其他领域的数据注释（例如边界框、点）。在本文中，我们提出了一种简单、高效且有效的方法，利用现有的实例掩码注释来增强训练集。通过利用背景的像素冗余，我们能够通过简单地向对象引入随机抖动，将Mask R-CNN在COCO数据集上的性能提高1.7 mAP，在Pascal VOC数据集上的性能提高3.3 mAP。此外，我们提出了一种基于位置概率图的方法，以探索基于局部外观相似性对象可以放置的可行位置。在这种图的指导下，我们无需修改骨干网络或网络结构，就将R101-Mask R-CNN在实例分割上的性能从35.7 mAP提升到37.9 mAP。我们的方法实现简单，不会增加计算复杂度。它可以集成到任何实例分割模型的训练管道中，而不会影响训练和推理效率。我们的代码和模型已在https://github.com/GothicAi/InstaBoost发布。",
        "领域": "实例分割/数据增强/深度学习",
        "问题": "实例分割模型需要大量训练样本以达到满意的性能",
        "动机": "通过数据增强提高实例分割模型的性能",
        "方法": "利用现有的实例掩码注释，通过随机抖动和基于位置概率图的方法增强训练集",
        "关键词": [
            "实例分割",
            "数据增强",
            "Mask R-CNN",
            "COCO数据集",
            "Pascal VOC数据集"
        ],
        "涉及的技术概念": {
            "实例分割": "一种计算机视觉任务，旨在识别图像中的每个对象实例并为每个实例分配一个唯一的标签。",
            "数据增强": "通过创建现有数据的修改版本来增加训练数据的数量和多样性，以提高模型的泛化能力。",
            "Mask R-CNN": "一种用于实例分割的深度学习模型，能够同时进行对象检测和像素级分割。",
            "COCO数据集": "一个广泛使用的图像识别、分割和字幕数据集，包含超过20万张标注图像。",
            "Pascal VOC数据集": "一个用于对象分类、检测和分割的基准数据集。",
            "位置概率图": "一种基于图像局部外观相似性，用于确定对象可能放置位置的图。"
        }
    },
    {
        "order": 203,
        "title": "SILCO: Show a Few Images, Localize the Common Object",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_SILCO_Show_a_Few_Images_Localize_the_Common_Object_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_SILCO_Show_a_Few_Images_Localize_the_Common_Object_ICCV_2019_paper.html",
        "abstract": "Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning requires tremendous amounts of data. In this work, we propose a new task along this research direction, we call few-shot common-localization. Given a few weakly-supervised support images, we aim to localize the common object in the query image without any box annotation. This task differs from standard few-shot settings, since we aim to address the localization problem, rather than the global classification problem. To tackle this new problem, we propose a network that aims to get the most out of the support and query images. To that end, we introduce a spatial similarity module that searches the spatial commonality among the given images. We furthermore introduce a feature reweighting module to balance the influence of different support images through graph convolutional networks. To evaluate few-shot common-localization, we repurpose and reorganize the well-known Pascal VOC and MS-COCO datasets, as well as a video dataset from ImageNet VID. Experiments on the new settings for few-shot common-localization shows the importance of searching for spatial similarity and feature reweighting, outperforming baselines from related tasks.",
        "中文标题": "SILCO: 展示少量图像，定位共同对象",
        "摘要翻译": "少样本学习是一个新兴的研究课题，其动机是传统深度学习需要大量数据。在这项工作中，我们提出了一个新的研究方向任务，我们称之为少样本共同定位。给定少量弱监督的支持图像，我们的目标是在没有任何框注释的情况下定位查询图像中的共同对象。这个任务与标准的少样本设置不同，因为我们的目标是解决定位问题，而不是全局分类问题。为了解决这个新问题，我们提出了一个旨在充分利用支持和查询图像的网络。为此，我们引入了一个空间相似性模块，用于搜索给定图像之间的空间共性。此外，我们还引入了一个特征重加权模块，通过图卷积网络来平衡不同支持图像的影响。为了评估少样本共同定位，我们重新调整和组织了著名的Pascal VOC和MS-COCO数据集，以及来自ImageNet VID的视频数据集。在少样本共同定位的新设置上的实验显示了搜索空间相似性和特征重加权的重要性，优于相关任务的基线。",
        "领域": "少样本学习/目标定位/图像分析",
        "问题": "在少量弱监督图像的情况下，定位查询图像中的共同对象",
        "动机": "传统深度学习方法需要大量数据，而少样本学习旨在减少数据需求",
        "方法": "提出了一个网络，包括空间相似性模块和特征重加权模块，通过图卷积网络平衡不同支持图像的影响",
        "关键词": [
            "少样本学习",
            "目标定位",
            "图像分析",
            "空间相似性",
            "特征重加权"
        ],
        "涉及的技术概念": {
            "少样本学习": "一种机器学习方法，旨在通过少量样本学习有效模型",
            "目标定位": "在图像中确定特定对象的位置",
            "空间相似性模块": "用于搜索图像间空间共性的模块",
            "特征重加权模块": "通过图卷积网络平衡不同支持图像影响的模块",
            "图卷积网络": "一种处理图结构数据的神经网络"
        }
    },
    {
        "order": 204,
        "title": "Dynamic Multi-Scale Filters for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Dynamic_Multi-Scale_Filters_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Multi-scale representation provides an effective way to address scale variation of objects and stuff in semantic segmentation. Previous works construct multi-scale representation by utilizing different filter sizes, expanding filter sizes with dilated filters or pooling grids, and the parameters of these filters are fixed after training. These methods often suffer from heavy computational cost or have more parameters, and are not adaptive to the input image during inference. To address these problems, this paper proposes a Dynamic Multi-scale Network (DMNet) to adaptively capture multi-scale contents for predicting pixel-level semantic labels. DMNet is composed of multiple Dynamic Convolutional Modules (DCMs) arranged in parallel, each of which exploits context-aware filters to estimate semantic representation for a specific scale. The outputs of multiple DCMs are further integrated for final segmentation. We conduct extensive experiments to evaluate our DMNet on three challenging semantic segmentation and scene parsing datasets, PASCAL VOC 2012, Pascal-Context, and ADE20K. DMNet achieves a new record 84.4% mIoU on PASCAL VOC 2012 test set without MS COCO pre-trained and post-processing, and also obtains state-of-the-art performance on Pascal-Context and ADE20K.",
        "中文标题": "动态多尺度滤波器用于语义分割",
        "摘要翻译": "多尺度表示为解决语义分割中物体和物品的尺度变化提供了一种有效的方法。以往的工作通过利用不同大小的滤波器、使用扩张滤波器或池化网格扩展滤波器大小来构建多尺度表示，这些滤波器的参数在训练后是固定的。这些方法往往计算成本高或参数多，并且在推理过程中不适应输入图像。为了解决这些问题，本文提出了一种动态多尺度网络（DMNet），以自适应地捕捉多尺度内容来预测像素级语义标签。DMNet由多个并行排列的动态卷积模块（DCMs）组成，每个模块利用上下文感知滤波器来估计特定尺度的语义表示。多个DCMs的输出进一步整合以进行最终的分割。我们在三个具有挑战性的语义分割和场景解析数据集PASCAL VOC 2012、Pascal-Context和ADE20K上进行了广泛的实验来评估我们的DMNet。DMNet在PASCAL VOC 2012测试集上达到了84.4%的mIoU新记录，没有使用MS COCO预训练和后处理，并且在Pascal-Context和ADE20K上也获得了最先进的性能。",
        "领域": "语义分割/场景解析/图像理解",
        "问题": "解决语义分割中物体和物品尺度变化的问题",
        "动机": "以往的方法计算成本高或参数多，并且在推理过程中不适应输入图像",
        "方法": "提出动态多尺度网络（DMNet），由多个并行排列的动态卷积模块（DCMs）组成，每个模块利用上下文感知滤波器来估计特定尺度的语义表示",
        "关键词": [
            "语义分割",
            "动态多尺度网络",
            "上下文感知滤波器"
        ],
        "涉及的技术概念": {
            "多尺度表示": "一种有效解决语义分割中物体和物品尺度变化的方法",
            "动态卷积模块（DCMs）": "DMNet的组成部分，利用上下文感知滤波器来估计特定尺度的语义表示",
            "mIoU": "平均交并比，用于评估语义分割性能的指标"
        }
    },
    {
        "order": 205,
        "title": "RIO: 3D Object Instance Re-Localization in Changing Indoor Environments",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wald_RIO_3D_Object_Instance_Re-Localization_in_Changing_Indoor_Environments_ICCV_2019_paper.html",
        "abstract": "In this work, we introduce the task of 3D object instance re-localization (RIO): given one or multiple objects in an RGB-D scan, we want to estimate their corresponding 6DoF poses in another 3D scan of the same environment taken at a later point in time. We consider RIO a particularly important task in 3D vision since it enables a wide range of practical applications, including AI-assistants or robots that are asked to find a specific object in a 3D scene. To address this problem, we first introduce 3RScan, a novel dataset and benchmark, which features 1482 RGB-D scans of 478 environments across multiple time steps. Each scene includes several objects whose positions change over time, together with ground truth annotations of object instances and their respective 6DoF mappings among re-scans. Automatically finding 6DoF object poses leads to a particular challenging feature matching task due to varying partial observations and changes in the surrounding context. To this end, we introduce a new data-driven approach that efficiently finds matching features using a fully-convolutional 3D correspondence network operating on multiple spatial scales. Combined with a 6DoF pose optimization, our method outperforms state-of-the-art baselines on our newly-established benchmark, achieving an accuracy of 30.58%.",
        "中文标题": "RIO: 在变化的室内环境中的3D物体实例重定位",
        "摘要翻译": "在这项工作中，我们介绍了3D物体实例重定位（RIO）任务：给定一个或多个RGB-D扫描中的物体，我们希望在稍后时间点拍摄的同一环境的另一个3D扫描中估计它们对应的6自由度姿态。我们认为RIO在3D视觉中是一个特别重要的任务，因为它支持广泛的实际应用，包括被要求在一个3D场景中找到特定物体的AI助手或机器人。为了解决这个问题，我们首先介绍了3RScan，一个新颖的数据集和基准，它包含了478个环境在多个时间点的1482个RGB-D扫描。每个场景包括几个随时间改变位置的物体，以及物体实例的地面真实注释和它们在重新扫描中的6自由度映射。由于不同的部分观察和周围环境的变化，自动找到6自由度物体姿态导致了一个特别具有挑战性的特征匹配任务。为此，我们引入了一种新的数据驱动方法，该方法通过在多个空间尺度上操作的完全卷积3D对应网络有效地找到匹配特征。结合6自由度姿态优化，我们的方法在我们新建立的基准上优于最先进的基线，达到了30.58%的准确率。",
        "领域": "3D视觉/机器人视觉/室内导航",
        "问题": "在变化的室内环境中准确重定位3D物体实例的6自由度姿态",
        "动机": "支持AI助手或机器人在3D场景中寻找特定物体的实际应用",
        "方法": "引入3RScan数据集和基准，开发一种新的数据驱动方法，使用完全卷积3D对应网络在多个空间尺度上找到匹配特征，并结合6自由度姿态优化",
        "关键词": [
            "3D物体实例重定位",
            "6自由度姿态",
            "RGB-D扫描",
            "特征匹配",
            "数据驱动方法"
        ],
        "涉及的技术概念": "6自由度姿态指的是物体在三维空间中的位置（x, y, z）和方向（roll, pitch, yaw）的六个参数。RGB-D扫描是一种同时捕获彩色图像和深度信息的技术，用于创建三维模型。完全卷积3D对应网络是一种深度学习模型，用于在三维数据中找到匹配的特征点。"
    },
    {
        "order": 206,
        "title": "Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Racial_Faces_in_the_Wild_Reducing_Racial_Bias_by_Information_ICCV_2019_paper.html",
        "abstract": "Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.",
        "中文标题": "野外种族面孔：通过信息最大化适应网络减少种族偏见",
        "摘要翻译": "种族偏见是生物识别中的一个重要问题，但在深度人脸识别中尚未得到彻底研究。在本文中，我们首先贡献了一个名为野外种族面孔（RFW）的专用数据集，在该数据集上我们严格验证了四个商业API和四个最先进（SOTA）算法的种族偏见。然后，我们进一步提出了使用深度无监督领域适应的解决方案，并提出了一种深度信息最大化适应网络（IMAN），通过使用高加索人作为源域和其他种族作为目标域来减轻这种偏见。这种无监督方法同时对齐全局分布以减少领域级别的种族差距，并在集群级别学习有区别的目标表示。提出了一种新颖的互信息损失，以进一步增强网络输出的区分能力，而无需标签信息。在RFW、GBU和IJB-A数据库上的大量实验表明，IMAN成功地学习了能够很好地泛化到不同种族和不同数据库的特征。",
        "领域": "人脸识别/生物识别/无监督学习",
        "问题": "深度人脸识别中的种族偏见问题",
        "动机": "解决深度人脸识别技术中存在的种族偏见，提高算法对不同种族人群的识别准确性和公平性",
        "方法": "提出了一种深度信息最大化适应网络（IMAN），通过无监督领域适应方法，使用高加索人作为源域和其他种族作为目标域，减少种族偏见。该方法包括全局分布对齐和集群级别的有区别目标表示学习，以及提出了一种新颖的互信息损失来增强网络输出的区分能力",
        "关键词": [
            "种族偏见",
            "深度人脸识别",
            "无监督领域适应",
            "信息最大化适应网络",
            "互信息损失"
        ],
        "涉及的技术概念": {
            "深度信息最大化适应网络（IMAN）": "一种用于减少深度人脸识别中种族偏见的网络，通过无监督领域适应方法，使用高加索人作为源域和其他种族作为目标域",
            "无监督领域适应": "一种机器学习方法，旨在将源域的知识迁移到目标域，而无需目标域的标签信息",
            "互信息损失": "一种损失函数，用于增强网络输出的区分能力，而无需标签信息"
        }
    },
    {
        "order": 207,
        "title": "A Deep Step Pattern Representation for Multimodal Retinal Image Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_A_Deep_Step_Pattern_Representation_for_Multimodal_Retinal_Image_Registration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_A_Deep_Step_Pattern_Representation_for_Multimodal_Retinal_Image_Registration_ICCV_2019_paper.html",
        "abstract": "This paper presents a novel feature-based method that is built upon a convolutional neural network (CNN) to learn the deep representation for multimodal retinal image registration. We coined the algorithm deep step patterns, in short DeepSPa. Most existing deep learning based methods require a set of manually labeled training data with known corresponding spatial transformations, which limits the size of training datasets. By contrast, our method is fully automatic and scale well to different image modalities with no human intervention. We generate feature classes from simple step patterns within patches of connecting edges formed by vascular junctions in multiple retinal imaging modalities. We leverage CNN to learn and optimize the input patches to be used for image registration. Spatial transformations are estimated based on the output possibility of the fully connected layer of CNN for a pair of images. One of the key advantages of the proposed algorithm is its robustness to non-linear intensity changes, which widely exist on retinal images due to the difference of acquisition modalities. We validate our algorithm on extensive challenging datasets comprising poor quality multimodal retinal images which are adversely affected by pathologies (diseases), speckle noise and low resolutions. The experimental results demonstrate the robustness and accuracy over state-of-the-art multimodal image registration algorithms.",
        "中文标题": "多模态视网膜图像配准的深度步进模式表示",
        "摘要翻译": "本文提出了一种基于卷积神经网络（CNN）的新型特征方法，用于学习多模态视网膜图像配准的深度表示。我们将该算法命名为深度步进模式，简称DeepSPa。大多数现有的基于深度学习的方法需要一组已知对应空间变换的手动标记训练数据，这限制了训练数据集的大小。相比之下，我们的方法是完全自动的，并且能够很好地扩展到不同的图像模态，无需人工干预。我们从由血管连接形成的边缘块中的简单步进模式生成特征类，这些边缘块来自多种视网膜成像模态。我们利用CNN来学习和优化用于图像配准的输入块。空间变换是基于CNN全连接层对一对图像的输出可能性估计的。所提出算法的一个关键优势是其对非线性强度变化的鲁棒性，这种变化由于采集模态的差异而广泛存在于视网膜图像中。我们在包含受病理（疾病）、斑点噪声和低分辨率不利影响的低质量多模态视网膜图像的广泛挑战性数据集上验证了我们的算法。实验结果表明，与最先进的多模态图像配准算法相比，我们的算法具有鲁棒性和准确性。",
        "领域": "医学图像处理/视网膜图像分析/图像配准",
        "问题": "多模态视网膜图像配准",
        "动机": "现有基于深度学习的方法需要手动标记的训练数据，限制了训练数据集的大小，且对非线性强度变化的鲁棒性不足。",
        "方法": "提出了一种基于卷积神经网络（CNN）的新型特征方法，通过从视网膜图像中的血管连接边缘块生成特征类，并利用CNN学习和优化这些特征类用于图像配准。",
        "关键词": [
            "多模态视网膜图像",
            "图像配准",
            "卷积神经网络",
            "深度步进模式",
            "非线性强度变化"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）用于学习和优化图像配准的输入块；深度步进模式（DeepSPa）是一种新的特征表示方法，用于多模态视网膜图像配准；空间变换估计基于CNN全连接层的输出可能性。"
    },
    {
        "order": 208,
        "title": "Online Model Distillation for Efficient Video Inference",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mullapudi_Online_Model_Distillation_for_Efficient_Video_Inference_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mullapudi_Online_Model_Distillation_for_Efficient_Video_Inference_ICCV_2019_paper.html",
        "abstract": "High-quality computer vision models typically address the problem of understanding the general distribution of real-world images. However, most cameras observe only a very small fraction of this distribution. This offers the possibility of achieving more efficient inference by specializing compact, low-cost models to the specific distribution of frames observed by a single camera. In this paper, we employ the technique of model distillation (supervising a low-cost student model using the output of a high-cost teacher) to specialize accurate, low-cost semantic segmentation models to a target video stream. Rather than learn a specialized student model on offline data from the video stream, we train the student in an online fashion on the live video, intermittently running the teacher to provide a target for learning. Online model distillation yields semantic segmentation models that closely approximate their Mask R-CNN teacher with 7 to 17xlower inference runtime cost (11 to 26xin FLOPs), even when the target video's distribution is non-stationary. Our method requires no offline pretraining on the target video stream, achieves higher accuracy and lower cost than solutions based on flow or video object segmentation, and can exhibit better temporal stability than the original teacher. We also provide a new video dataset for evaluating the efficiency of inference over long running video streams.",
        "中文标题": "在线模型蒸馏用于高效视频推理",
        "摘要翻译": "高质量的计算机视觉模型通常解决的是理解现实世界图像的一般分布问题。然而，大多数相机只观察到这一分布的很小一部分。这提供了通过将紧凑、低成本模型专门化到单个相机观察到的帧的特定分布来实现更高效推理的可能性。在本文中，我们采用模型蒸馏技术（使用高成本教师的输出监督低成本学生模型）来将准确、低成本的语义分割模型专门化到目标视频流。我们不是在视频流的离线数据上学习专门化的学生模型，而是在实时视频上以在线方式训练学生，间歇性地运行教师以提供学习目标。在线模型蒸馏产生的语义分割模型紧密近似于其Mask R-CNN教师，推理运行时间成本降低了7到17倍（FLOPs降低了11到26倍），即使目标视频的分布是非平稳的。我们的方法不需要在目标视频流上进行离线预训练，比基于流或视频对象分割的解决方案实现了更高的准确性和更低的成本，并且可以表现出比原始教师更好的时间稳定性。我们还提供了一个新的视频数据集，用于评估长时间运行视频流的推理效率。",
        "领域": "语义分割/视频分析/模型优化",
        "问题": "如何实现高效且准确的视频语义分割",
        "动机": "大多数相机只观察到现实世界图像分布的一小部分，这为通过专门化模型到特定视频流实现更高效推理提供了可能性",
        "方法": "采用在线模型蒸馏技术，在实时视频上训练低成本学生模型，间歇性地运行高成本教师模型以提供学习目标",
        "关键词": [
            "语义分割",
            "模型蒸馏",
            "视频分析",
            "模型优化"
        ],
        "涉及的技术概念": {
            "模型蒸馏": "一种技术，通过使用高成本模型（教师）的输出监督低成本模型（学生）来训练学生模型，以实现模型压缩和加速",
            "Mask R-CNN": "一种用于对象实例分割的深度学习模型，能够同时进行对象检测和像素级分割",
            "FLOPs": "浮点运算次数，用于衡量模型的计算复杂度",
            "非平稳分布": "指数据分布随时间变化的特性，在视频流中常见"
        }
    },
    {
        "order": 209,
        "title": "Uncertainty Modeling of Contextual-Connections Between Tracklets for Unconstrained Video-Based Face Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Uncertainty_Modeling_of_Contextual-Connections_Between_Tracklets_for_Unconstrained_Video-Based_Face_ICCV_2019_paper.html",
        "abstract": "Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset.",
        "中文标题": "基于轨迹间上下文连接的不确定性建模用于无约束视频人脸识别",
        "摘要翻译": "无约束视频人脸识别是一个具有挑战性的问题，由于姿势、遮挡和模糊引起的视频内显著变化。为了解决这个问题，一个有效的想法是通过上下文连接将身份从高质量的面部传播到低质量的面部，这些连接是基于诸如身体外观等上下文构建的。然而，由于缺乏对噪声上下文连接的不确定性建模，以前的方法经常传播错误信息。在本文中，我们提出了不确定性门控图（UGG），它在轨迹之间进行基于图的身份传播，轨迹由图中的节点表示。UGG通过在推理过程中根据节点的身份分布自适应地更新边门的权重，显式地建模上下文连接的不确定性。UGG是一个通用的图形模型，可以仅在推理时应用，也可以进行端到端训练。我们通过在最近发布的具有挑战性的电影中的演员搜索和IARPA Janus监控视频基准数据集上的最新结果证明了UGG的有效性。",
        "领域": "视频分析/人脸识别/图模型",
        "问题": "解决无约束视频人脸识别中由于姿势、遮挡和模糊引起的视频内显著变化问题",
        "动机": "通过上下文连接将身份从高质量的面部传播到低质量的面部，但由于缺乏对噪声上下文连接的不确定性建模，以前的方法经常传播错误信息",
        "方法": "提出了不确定性门控图（UGG），在轨迹之间进行基于图的身份传播，显式地建模上下文连接的不确定性，并通过自适应地更新边门的权重来减少错误信息的传播",
        "关键词": [
            "视频分析",
            "人脸识别",
            "图模型",
            "不确定性建模",
            "身份传播"
        ],
        "涉及的技术概念": {
            "不确定性门控图（UGG）": "一种通用的图形模型，用于在轨迹之间进行基于图的身份传播，显式地建模上下文连接的不确定性",
            "轨迹": "视频中连续帧中检测到的同一对象的路径，由图中的节点表示",
            "上下文连接": "基于诸如身体外观等上下文构建的连接，用于将身份从高质量的面部传播到低质量的面部",
            "边门": "图中连接节点的边，其权重在推理过程中根据节点的身份分布自适应地更新，以减少错误信息的传播"
        }
    },
    {
        "order": 210,
        "title": "Rethinking Zero-Shot Learning: A Conditional Visual Classification Perspective",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Rethinking_Zero-Shot_Learning_A_Conditional_Visual_Classification_Perspective_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Rethinking_Zero-Shot_Learning_A_Conditional_Visual_Classification_Perspective_ICCV_2019_paper.html",
        "abstract": "Zero-shot learning (ZSL) aims to recognize instances of unseen classes solely based on the semantic descriptions of the classes. Existing algorithms usually formulate it as a semantic-visual correspondence problem, by learning mappings from one feature space to the other. Despite being reasonable, previous approaches essentially discard the highly precious discriminative power of visual features in an implicit way, and thus produce undesirable results. We instead reformulate ZSL as a conditioned visual classification problem, i.e., classifying visual features based on the classifiers learned from the semantic descriptions. With this reformulation, we develop algorithms targeting various ZSL settings: For the conventional setting, we propose to train a deep neural network that directly generates visual feature classifiers from the semantic attributes with an episode-based training scheme; For the generalized setting, we concatenate the learned highly discriminative classifiers for seen classes and the generated classifiers for unseen classes to classify visual features of all classes; For the transductive setting, we exploit unlabeled data to effectively calibrate the classifier generator using a novel learning-without-forgetting self-training mechanism and guide the process by a robust generalized cross-entropy loss. Extensive experiments show that our proposed algorithms significantly outperform state-of-the-art methods by large margins on most benchmark datasets in all the ZSL settings.",
        "中文标题": "重新思考零样本学习：一种条件视觉分类的视角",
        "摘要翻译": "零样本学习（ZSL）旨在仅基于类的语义描述来识别未见过的类的实例。现有的算法通常将其表述为语义-视觉对应问题，通过学习从一个特征空间到另一个特征空间的映射。尽管这种方法合理，但先前的方法本质上以隐式方式丢弃了视觉特征的高度宝贵的判别能力，从而产生了不理想的结果。我们反而将ZSL重新表述为一个条件视觉分类问题，即基于从语义描述中学习到的分类器对视觉特征进行分类。通过这种重新表述，我们开发了针对各种ZSL设置的算法：对于常规设置，我们提出训练一个深度神经网络，该网络通过基于情节的训练方案直接从语义属性生成视觉特征分类器；对于广义设置，我们将学习到的高度判别性的分类器与生成的未见类分类器连接起来，以对所有类的视觉特征进行分类；对于转导设置，我们利用未标记数据通过一种新颖的“学习而不忘记”自训练机制有效地校准分类器生成器，并通过鲁棒的广义交叉熵损失指导该过程。大量实验表明，我们提出的算法在所有ZSL设置中的大多数基准数据集上显著优于最先进的方法。",
        "领域": "零样本学习/视觉分类/深度学习",
        "问题": "如何有效利用视觉特征的判别能力进行零样本学习",
        "动机": "现有方法在零样本学习中忽视了视觉特征的判别能力，导致结果不理想",
        "方法": "将零样本学习重新表述为条件视觉分类问题，并开发针对不同设置的算法，包括直接生成视觉特征分类器的深度神经网络、连接学习到的和生成的分类器进行分类、以及利用未标记数据校准分类器生成器",
        "关键词": [
            "零样本学习",
            "视觉分类",
            "深度学习"
        ],
        "涉及的技术概念": "零样本学习（ZSL）是一种机器学习方法，旨在识别训练数据中未出现过的类别的实例。语义-视觉对应问题是指通过映射语义描述和视觉特征之间的关系来解决问题。条件视觉分类问题是指基于某些条件（如语义描述）对视觉特征进行分类。深度神经网络是一种模仿人脑结构和功能的计算模型，用于处理复杂的数据。自训练机制是一种利用未标记数据来改进模型性能的技术。广义交叉熵损失是一种用于分类问题的损失函数，能够提高模型对噪声数据的鲁棒性。"
    },
    {
        "order": 211,
        "title": "Deep Graphical Feature Learning for the Feature Matching Problem",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Deep_Graphical_Feature_Learning_for_the_Feature_Matching_Problem_ICCV_2019_paper.html",
        "abstract": "The feature matching problem is a fundamental problem in various areas of computer vision including image registration, tracking and motion analysis. Rich local representation is a key part of efficient feature matching methods. However, when the local features are limited to the coordinate of key points, it becomes challenging to extract rich local representations. Traditional approaches use pairwise or higher order handcrafted geometric features to get robust matching; this requires solving NP-hard assignment problems. In this paper, we address this problem by proposing a graph neural network model to transform coordinates of feature points into local features. With our local features, the traditional NP-hard assignment problems are replaced with a simple assignment problem which can be solved efficiently. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed method.",
        "中文标题": "深度图形特征学习用于特征匹配问题",
        "摘要翻译": "特征匹配问题是计算机视觉中包括图像配准、跟踪和运动分析在内的多个领域的一个基本问题。丰富的局部表示是高效特征匹配方法的关键部分。然而，当局部特征仅限于关键点的坐标时，提取丰富的局部表示变得具有挑战性。传统方法使用成对或更高阶的手工几何特征来获得鲁棒的匹配；这需要解决NP难分配问题。在本文中，我们通过提出一种图神经网络模型来解决这个问题，该模型将特征点的坐标转换为局部特征。使用我们的局部特征，传统的NP难分配问题被替换为一个可以高效解决的简单分配问题。在合成和真实数据集上的有希望的结果证明了所提出方法的有效性。",
        "领域": "图像配准/运动分析/图神经网络",
        "问题": "特征匹配问题，特别是在局部特征仅限于关键点坐标时，提取丰富的局部表示的挑战",
        "动机": "传统方法需要解决NP难分配问题，这限制了特征匹配的效率和效果",
        "方法": "提出一种图神经网络模型，将特征点的坐标转换为局部特征，从而将传统的NP难分配问题替换为可以高效解决的简单分配问题",
        "关键词": [
            "特征匹配",
            "图神经网络",
            "局部特征",
            "NP难问题",
            "图像配准"
        ],
        "涉及的技术概念": "图神经网络（Graph Neural Network, GNN）是一种深度学习模型，专门用于处理图结构数据。在本研究中，GNN被用来将特征点的坐标转换为更丰富的局部特征表示，从而简化了特征匹配问题。NP难问题指的是在计算复杂性理论中，那些至少与NP类中的任何问题一样难的问题，通常需要非常高的计算资源来解决。"
    },
    {
        "order": 212,
        "title": "Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Spatio-Temporal_Fusion_Based_Convolutional_Sequence_Learning_for_Lip_Reading_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Spatio-Temporal_Fusion_Based_Convolutional_Sequence_Learning_for_Lip_Reading_ICCV_2019_paper.html",
        "abstract": "Current state-of-the-art approaches for lip reading are based on sequence-to-sequence architectures that are designed for natural machine translation and audio speech recognition. Hence, these methods do not fully exploit the characteristics of the lip dynamics, causing two main drawbacks. First, the short-range temporal dependencies, which are critical to the mapping from lip images to visemes, receives no extra attention. Second, local spatial information is discarded in the existing sequence models due to the use of global average pooling (GAP). To well solve these drawbacks, we propose a Temporal Focal block to sufficiently describe short-range dependencies and a Spatio-Temporal Fusion Module (STFM) to maintain the local spatial information and to reduce the feature dimensions as well. From the experiment results, it is demonstrated that our method achieves comparable performance with the state-of-the-art approach using much less training data and much lighter Convolutional Feature Extractor. The training time is reduced by 12 days due to the convolutional structure and the local self-attention mechanism.",
        "中文标题": "基于时空融合的卷积序列学习用于唇读",
        "摘要翻译": "当前最先进的唇读方法基于序列到序列架构，这些架构是为自然机器翻译和音频语音识别设计的。因此，这些方法没有充分利用唇部动态的特性，导致两个主要缺点。首先，对从唇部图像到视觉音素的映射至关重要的短程时间依赖性没有得到额外关注。其次，由于使用全局平均池化（GAP），现有序列模型中的局部空间信息被丢弃。为了很好地解决这些缺点，我们提出了一个时间焦点块来充分描述短程依赖性，以及一个时空融合模块（STFM）来保持局部空间信息并减少特征维度。实验结果表明，我们的方法在使用更少的训练数据和更轻的卷积特征提取器的情况下，实现了与最先进方法相当的性能。由于卷积结构和局部自注意力机制，训练时间减少了12天。",
        "领域": "唇读/序列学习/时空融合",
        "问题": "现有唇读方法未能充分利用唇部动态特性，导致短程时间依赖性未得到足够关注和局部空间信息丢失",
        "动机": "解决现有唇读方法在利用唇部动态特性方面的不足，提高唇读的准确性和效率",
        "方法": "提出时间焦点块和时空融合模块（STFM），以充分描述短程依赖性并保持局部空间信息，同时减少特征维度",
        "关键词": [
            "唇读",
            "序列学习",
            "时空融合",
            "时间焦点块",
            "局部空间信息"
        ],
        "涉及的技术概念": "序列到序列架构、全局平均池化（GAP）、时间焦点块、时空融合模块（STFM）、卷积特征提取器、局部自注意力机制"
    },
    {
        "order": 213,
        "title": "Task-Driven Modular Networks for Zero-Shot Compositional Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Purushwalkam_Task-Driven_Modular_Networks_for_Zero-Shot_Compositional_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Purushwalkam_Task-Driven_Modular_Networks_for_Zero-Shot_Compositional_Learning_ICCV_2019_paper.html",
        "abstract": "One of the hallmarks of human intelligence is the ability to compose learned knowledge into novel concepts which can be recognized without a single training example. In contrast, current state-of-the-art methods require hundreds of training examples for each possible category to build reliable and accurate classifiers. To alleviate this striking difference in efficiency, we propose a task-driven modular architecture for compositional reasoning and sample efficient learning. Our architecture consists of a set of neural network modules, which are small fully connected layers operating in semantic concept space. These modules are configured through a gating function conditioned on the task to produce features representing the compatibility between the input image and the concept under consideration. This enables us to express tasks as a combination of sub-tasks and to generalize to unseen categories by reweighting a set of small modules. Furthermore, the network can be trained efficiently as it is fully differentiable and its modules operate on small sub-spaces. We focus our study on the problem of compositional zero-shot classification of object-attribute categories. We show in our experiments that current evaluation metrics are flawed as they only consider unseen object-attribute pairs. When extending the evaluation to the generalized setting which accounts also for pairs seen during training, we discover that naive baseline methods perform similarly or better than current approaches. However, our modular network is able to outperform all existing approaches on two widely-used benchmark datasets.",
        "中文标题": "任务驱动的模块化网络用于零样本组合学习",
        "摘要翻译": "人类智能的一个标志是能够将学到的知识组合成新概念，这些新概念可以在没有任何训练样本的情况下被识别。相比之下，当前最先进的方法需要为每个可能的类别提供数百个训练样本来构建可靠且准确的分类器。为了缓解这种效率上的显著差异，我们提出了一种任务驱动的模块化架构，用于组合推理和样本高效学习。我们的架构由一组神经网络模块组成，这些模块是在语义概念空间中操作的小型全连接层。这些模块通过一个基于任务的门控函数进行配置，以生成表示输入图像与考虑中的概念之间兼容性的特征。这使我们能够将任务表达为子任务的组合，并通过重新加权一组小模块来泛化到未见过的类别。此外，由于网络完全可微分且其模块在小子空间上操作，因此可以高效地进行训练。我们将研究重点放在对象-属性类别的组合零样本分类问题上。我们在实验中表明，当前的评估指标存在缺陷，因为它们只考虑了未见过的对象-属性对。当将评估扩展到广义设置，该设置还考虑了训练期间见过的对时，我们发现天真的基线方法与当前方法表现相似或更好。然而，我们的模块化网络能够在两个广泛使用的基准数据集上超越所有现有方法。",
        "领域": "零样本学习/组合学习/语义概念空间",
        "问题": "解决对象-属性类别的组合零样本分类问题",
        "动机": "为了缓解当前方法在构建可靠且准确的分类器时对大量训练样本的需求，提高学习效率",
        "方法": "提出了一种任务驱动的模块化架构，该架构由一组在语义概念空间中操作的小型全连接层组成，通过基于任务的门控函数配置模块，以生成表示输入图像与考虑中的概念之间兼容性的特征",
        "关键词": [
            "零样本学习",
            "组合学习",
            "语义概念空间"
        ],
        "涉及的技术概念": "模块化网络架构、全连接层、语义概念空间、门控函数、组合零样本分类"
    },
    {
        "order": 214,
        "title": "Minimum Delay Object Detection From Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lao_Minimum_Delay_Object_Detection_From_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lao_Minimum_Delay_Object_Detection_From_Video_ICCV_2019_paper.html",
        "abstract": "We consider the problem of detecting objects, as they come into view, from videos in an online fashion. We provide the first real-time solution that is guaranteed to minimize the delay, i.e., the time between when the object comes in view and the declared detection time, subject to acceptable levels of detection accuracy. The method leverages modern CNN-based object detectors that operate on a single frame, to aggregate detection results over frames to provide reliable detection at a rate, specified by the user, in guaranteed minimal delay. To do this, we formulate the problem as a Quickest Detection problem, which provides the aforementioned guarantees. We derive our algorithms from this theory. We show in experiments, that with an overhead of just 50 fps, we can increase the number of correct detections and decrease the overall computational cost compared to running a modern single-frame detector.",
        "中文标题": "最小延迟视频对象检测",
        "摘要翻译": "我们考虑从视频中以在线方式检测对象的问题，当对象进入视野时进行检测。我们提供了第一个实时解决方案，该方案保证在可接受的检测精度水平下最小化延迟，即对象进入视野和声明检测时间之间的时间。该方法利用现代基于CNN的对象检测器，这些检测器在单帧上操作，以在帧之间聚合检测结果，从而以用户指定的速率提供可靠的检测，并保证最小延迟。为此，我们将问题表述为最快检测问题，这提供了上述保证。我们从这一理论中推导出我们的算法。我们在实验中展示，与运行现代单帧检测器相比，仅以50 fps的开销，我们可以增加正确检测的数量并降低总体计算成本。",
        "领域": "实时对象检测/视频分析/深度学习应用",
        "问题": "在视频中实时检测对象并最小化检测延迟",
        "动机": "为了在对象进入视野时立即进行检测，同时保证检测精度，需要一种能够最小化延迟的实时解决方案。",
        "方法": "利用现代基于CNN的对象检测器在单帧上操作，并在帧之间聚合检测结果，以用户指定的速率提供可靠的检测，并保证最小延迟。将问题表述为最快检测问题，并从中推导出算法。",
        "关键词": [
            "实时对象检测",
            "视频分析",
            "深度学习应用"
        ],
        "涉及的技术概念": "CNN（卷积神经网络）用于对象检测，最快检测问题用于保证最小延迟，帧聚合技术用于提高检测的可靠性。"
    },
    {
        "order": 215,
        "title": "Occlusion-Aware Networks for 3D Human Pose Estimation in Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.html",
        "abstract": "Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a \"Cylinder Man Model\" to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets.",
        "中文标题": "遮挡感知网络在视频中的3D人体姿态估计",
        "摘要翻译": "遮挡是单目视频中3D人体姿态估计的一个关键问题。为了解决这个问题，我们引入了一个遮挡感知的深度学习框架。通过使用估计的关键点2D置信度热图和光流一致性约束，我们过滤掉了被遮挡关键点的不可靠估计。当发生遮挡时，我们有不完整的2D关键点，并将它们输入到我们的2D和3D时间卷积网络（2D和3D TCNs）中，这些网络强制时间平滑性以产生完整的3D姿态。通过使用不完整的2D关键点，而不是完整但不正确的关键点，我们的网络较少受到被遮挡关键点错误估计的影响。训练遮挡感知的3D TCN需要成对的3D姿态和带有遮挡标签的2D姿态。由于没有这样的数据集可用，我们引入了一个“圆柱人模型”来近似身体部位在3D空间中的占据。通过将模型投影到不同视角的2D平面上，我们获得并标记被遮挡的关键点，为我们提供了大量的训练数据。此外，我们使用这个模型来创建一个姿态正则化约束，倾向于将不可靠关键点的2D估计视为被遮挡。我们的方法在Human 3.6M和HumanEva-I数据集上优于最先进的方法。",
        "领域": "3D人体姿态估计/遮挡处理/时间卷积网络",
        "问题": "解决单目视频中3D人体姿态估计的遮挡问题",
        "动机": "遮挡是3D人体姿态估计中的一个主要挑战，需要有效的方法来处理被遮挡的关键点以提高估计的准确性",
        "方法": "引入遮挡感知的深度学习框架，使用2D置信度热图和光流一致性约束过滤不可靠估计，通过2D和3D时间卷积网络处理不完整的2D关键点以产生完整的3D姿态，使用“圆柱人模型”生成训练数据和姿态正则化约束",
        "关键词": [
            "3D人体姿态估计",
            "遮挡处理",
            "时间卷积网络",
            "2D置信度热图",
            "光流一致性约束"
        ],
        "涉及的技术概念": "2D置信度热图用于估计关键点的位置和置信度；光流一致性约束用于确保时间上的连续性；2D和3D时间卷积网络（TCNs）用于处理时间序列数据，强制时间平滑性；圆柱人模型用于模拟人体在3D空间中的占据，生成带有遮挡标签的训练数据；姿态正则化约束用于优化2D关键点的估计，倾向于将不可靠的估计视为被遮挡。"
    },
    {
        "order": 216,
        "title": "Transductive Episodic-Wise Adaptive Metric for Few-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Few-shot learning, which aims at extracting new concepts rapidly from extremely few examples of novel classes, has been featured into the meta-learning paradigm recently. Yet, the key challenge of how to learn a generalizable classifier with the capability of adapting to specific tasks with severely limited data still remains in this domain. To this end, we propose a Transductive Episodic-wise Adaptive Metric (TEAM) framework for few-shot learning, by integrating the meta-learning paradigm with both deep metric learning and transductive inference. With exploring the pairwise constraints and regularization prior within each task, we explicitly formulate the adaptation procedure into a standard semi-definite programming problem. By solving the problem with its closed-form solution on the fly with the setup of transduction, our approach efficiently tailors an episodic-wise metric for each task to adapt all features from a shared task-agnostic embedding space into a more discriminative task-specific metric space. Moreover, we further leverage an attention-based bi-directional similarity strategy for extracting the more robust relationship between queries and prototypes. Extensive experiments on three benchmark datasets show that our framework is superior to other existing approaches and achieves the state-of-the-art performance in the few-shot literature.",
        "中文标题": "转导式情节自适应度量用于少样本学习",
        "摘要翻译": "少样本学习旨在从极少量的新类别样本中快速提取新概念，最近已被纳入元学习范式。然而，如何学习一个能够适应特定任务且数据极其有限的泛化分类器，仍然是该领域的关键挑战。为此，我们提出了一个转导式情节自适应度量（TEAM）框架，通过将元学习范式与深度度量学习和转导推理相结合。通过探索每个任务内的成对约束和正则化先验，我们明确地将适应过程表述为一个标准的半定规划问题。通过在转导设置下即时求解该问题的闭式解，我们的方法有效地为每个任务定制了一个情节度量，将所有特征从共享的任务无关嵌入空间适应到更具区分性的任务特定度量空间。此外，我们进一步利用基于注意力的双向相似性策略来提取查询和原型之间更稳健的关系。在三个基准数据集上的大量实验表明，我们的框架优于其他现有方法，并在少样本学习文献中达到了最先进的性能。",
        "领域": "少样本学习/元学习/深度度量学习",
        "问题": "如何在数据极其有限的情况下学习一个能够适应特定任务的泛化分类器",
        "动机": "解决少样本学习领域中，如何从极少量的新类别样本中快速提取新概念并学习一个泛化分类器的挑战",
        "方法": "提出了一个转导式情节自适应度量（TEAM）框架，通过将元学习范式与深度度量学习和转导推理相结合，探索每个任务内的成对约束和正则化先验，明确地将适应过程表述为一个标准的半定规划问题，并利用基于注意力的双向相似性策略来提取查询和原型之间更稳健的关系",
        "关键词": [
            "少样本学习",
            "元学习",
            "深度度量学习",
            "转导推理",
            "半定规划",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "元学习": "一种学习范式，旨在使模型能够快速适应新任务",
            "深度度量学习": "一种学习方法，通过学习一个度量空间，使得在该空间中相似的对象更接近，不相似的对象更远",
            "转导推理": "一种推理方法，利用测试数据的信息来改进模型的性能",
            "半定规划": "一种优化问题，其解必须满足半定约束",
            "注意力机制": "一种机制，使模型能够专注于输入数据的某些部分，以提高性能"
        }
    },
    {
        "order": 217,
        "title": "Learning With Average Precision: Training Image Retrieval With a Listwise Loss",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.html",
        "abstract": "Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g., special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at: https://europe.naverlabs.com/Deep-Image-Retrieval/.",
        "中文标题": "学习平均精度：使用列表损失训练图像检索",
        "摘要翻译": "图像检索可以被表述为一个排序问题，其目标是根据与查询的相似度递减来排序数据库中的图像。最近的深度模型通过利用为排序量身定制的损失函数，在图像检索方面超越了传统方法，但仍然存在重要的理论和实际问题。首先，它们不是直接优化全局排序，而是最小化基本损失的上界，这不一定导致最优的平均精度（mAP）。其次，这些方法需要大量的工程努力才能很好地工作，例如特殊的预训练和硬负样本挖掘。在本文中，我们提出通过利用列表损失公式的最新进展直接优化全局mAP。使用直方图分箱近似，AP可以被微分，从而用于端到端学习。与现有损失相比，所提出的方法在每次迭代时同时考虑数千张图像，并消除了对特殊技巧的需求。它还在许多标准检索基准上建立了新的技术状态。模型和评估脚本已在以下网址提供：https://europe.naverlabs.com/Deep-Image-Retrieval/。",
        "领域": "图像检索/排序学习/损失函数优化",
        "问题": "直接优化全局排序以提升图像检索的平均精度（mAP）",
        "动机": "现有的深度模型虽然通过利用为排序量身定制的损失函数在图像检索方面取得了进展，但仍存在优化全局排序和减少工程努力的需求。",
        "方法": "提出了一种直接优化全局mAP的方法，通过利用列表损失公式的最新进展，并使用直方图分箱近似使AP可微分，从而用于端到端学习。",
        "关键词": [
            "图像检索",
            "平均精度",
            "列表损失",
            "直方图分箱",
            "端到端学习"
        ],
        "涉及的技术概念": "平均精度（mAP）是评估图像检索系统性能的重要指标，列表损失是一种用于排序问题的损失函数，直方图分箱是一种近似方法，用于使AP可微分，端到端学习指的是模型从输入到输出的整个学习过程。"
    },
    {
        "order": 218,
        "title": "Context-Aware Feature and Label Fusion for Facial Action Unit Intensity Estimation With Partially Labeled Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Context-Aware_Feature_and_Label_Fusion_for_Facial_Action_Unit_Intensity_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Context-Aware_Feature_and_Label_Fusion_for_Facial_Action_Unit_Intensity_ICCV_2019_paper.html",
        "abstract": "Facial action unit (AU) intensity estimation is a fundamental task for facial behaviour analysis. Most previous methods use a whole face image as input for intensity prediction. Considering that AUs are defined according to their corresponding local appearance, a few patch-based methods utilize image features of local patches. However, fusion of local features is always performed via straightforward feature concatenation or summation. Besides, these methods require fully annotated databases for model learning, which is expensive to acquire. In this paper, we propose a novel weakly supervised patch-based deep model on basis of two types of attention mechanisms for joint intensity estimation of multiple AUs. The model consists of a feature fusion module and a label fusion module. And we augment attention mechanisms of these two modules with a learnable task-related context, as one patch may play different roles in analyzing different AUs and each AU has its own temporal evolution rule. The context-aware feature fusion module is used to capture spatial relationships among local patches while the context-aware label fusion module is used to capture the temporal dynamics of AUs. The latter enables the model to be trained on a partially annotated database. Experimental evaluations on two benchmark expression databases demonstrate the superior performance of the proposed method.",
        "中文标题": "基于上下文感知的特征和标签融合的面部动作单元强度估计与部分标注数据",
        "摘要翻译": "面部动作单元（AU）强度估计是面部行为分析的一项基本任务。大多数先前的方法使用整个面部图像作为强度预测的输入。考虑到AU是根据其相应的局部外观定义的，一些基于局部块的方法利用局部块的图像特征。然而，局部特征的融合总是通过直接的特征连接或求和来执行。此外，这些方法需要完全标注的数据库进行模型学习，这获取成本较高。在本文中，我们提出了一种新颖的基于弱监督的局部块深度模型，该模型基于两种类型的注意力机制，用于多个AU的联合强度估计。该模型由特征融合模块和标签融合模块组成。我们通过可学习的任务相关上下文增强了这两个模块的注意力机制，因为一个局部块在分析不同的AU时可能扮演不同的角色，并且每个AU都有其自己的时间演化规则。上下文感知的特征融合模块用于捕捉局部块之间的空间关系，而上下文感知的标签融合模块用于捕捉AU的时间动态。后者使得模型能够在部分标注的数据库上进行训练。在两个基准表情数据库上的实验评估证明了所提出方法的优越性能。",
        "领域": "面部行为分析/注意力机制/弱监督学习",
        "问题": "面部动作单元（AU）强度估计",
        "动机": "现有的方法需要完全标注的数据库进行模型学习，这获取成本较高，且局部特征的融合方法较为简单。",
        "方法": "提出了一种基于弱监督的局部块深度模型，该模型基于两种类型的注意力机制，用于多个AU的联合强度估计。模型包括特征融合模块和标签融合模块，通过可学习的任务相关上下文增强了这两个模块的注意力机制。",
        "关键词": [
            "面部动作单元",
            "注意力机制",
            "弱监督学习",
            "特征融合",
            "标签融合"
        ],
        "涉及的技术概念": "面部动作单元（AU）强度估计是面部行为分析的一项基本任务，涉及到局部块的特征提取和融合。本文提出的方法利用了注意力机制来增强特征和标签的融合，使得模型能够在部分标注的数据库上进行训练，从而解决了完全标注数据库获取成本高的问题。"
    },
    {
        "order": 219,
        "title": "Deep Multiple-Attribute-Perceived Network for Real-World Texture Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Deep_Multiple-Attribute-Perceived_Network_for_Real-World_Texture_Recognition_ICCV_2019_paper.html",
        "abstract": "Texture recognition is a challenging visual task as multiple perceptual attributes may be perceived from the same texture image when combined with different spatial context. Some recent works building upon Convolutional Neural Network (CNN) incorporate feature encoding with orderless aggregating to provide invariance to spatial layouts. However, these existing methods ignore visual texture attributes, which are important cues for describing the real-world texture images, resulting in incomplete description and inaccurate recognition. To address this problem, we propose a novel deep Multiple-Attribute-Perceived Network (MAP-Net) by progressively learning visual texture attributes in a mutually reinforced manner. Specifically, a multi-branch network architecture is devised, in which cascaded global contexts are learned by introducing similarity constraint at each branch, and leveraged as guidance of spatial feature encoding at next branch through an attribute transfer scheme. To enhance the modeling capability of spatial transformation, a deformable pooling strategy is introduced to augment the spatial sampling with adaptive offsets to the global context, leading to perceive new visual attributes. An attribute fusion module is then introduced to jointly utilize the perceived visual attributes and the abstracted semantic concepts at each branch. Experimental results on the five most challenging texture recognition datasets have demonstrated the superiority of the proposed model against the state-of-the-arts.",
        "中文标题": "深度多属性感知网络用于现实世界纹理识别",
        "摘要翻译": "纹理识别是一项具有挑战性的视觉任务，因为当与不同的空间上下文结合时，可能会从同一纹理图像中感知到多个感知属性。一些最近的工作基于卷积神经网络（CNN），通过将特征编码与无序聚合相结合，提供了对空间布局的不变性。然而，这些现有方法忽略了视觉纹理属性，这些属性是描述现实世界纹理图像的重要线索，导致描述不完整和识别不准确。为了解决这个问题，我们提出了一种新颖的深度多属性感知网络（MAP-Net），通过以相互强化的方式逐步学习视觉纹理属性。具体来说，设计了一种多分支网络架构，其中通过在每个分支引入相似性约束来学习级联的全局上下文，并通过属性转移方案将其作为下一分支空间特征编码的指导。为了增强空间变换的建模能力，引入了可变形池化策略，通过自适应偏移量对全局上下文进行空间采样增强，从而感知新的视觉属性。然后引入属性融合模块，以联合利用每个分支感知到的视觉属性和抽象的语义概念。在五个最具挑战性的纹理识别数据集上的实验结果证明了所提出模型相对于现有技术的优越性。",
        "领域": "纹理识别/视觉属性学习/空间特征编码",
        "问题": "现有纹理识别方法忽略了视觉纹理属性，导致描述不完整和识别不准确",
        "动机": "提高现实世界纹理图像的识别准确性和描述完整性",
        "方法": "提出深度多属性感知网络（MAP-Net），通过多分支网络架构逐步学习视觉纹理属性，并引入可变形池化策略和属性融合模块",
        "关键词": [
            "纹理识别",
            "视觉属性",
            "空间特征编码",
            "可变形池化",
            "属性融合"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）、特征编码、无序聚合、空间布局不变性、视觉纹理属性、多分支网络架构、相似性约束、属性转移、可变形池化策略、自适应偏移量、属性融合模块、语义概念"
    },
    {
        "order": 220,
        "title": "Learning to Find Common Objects Across Few Image Collections",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shaban_Learning_to_Find_Common_Objects_Across_Few_Image_Collections_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shaban_Learning_to_Find_Common_Objects_Across_Few_Image_Collections_ICCV_2019_paper.html",
        "abstract": "Given a collection of bags where each bag is a set of images, our goal is to select one image from each bag such that the selected images are from the same object class. We model the selection as an energy minimization problem with unary and pairwise potential functions. Inspired by recent few-shot learning algorithms, we propose an approach to learn the potential functions directly from the data. Furthermore, we propose a fast greedy inference algorithm for energy minimization. We evaluate our approach on few-shot common object recognition as well as object co-localization tasks. Our experiments show that learning the pairwise and unary terms greatly improves the performance of the model over several well-known methods for these tasks. The proposed greedy optimization algorithm achieves performance comparable to state-of-the-art structured inference algorithms while being  10 times faster.",
        "中文标题": "学习在少量图像集合中寻找共同对象",
        "摘要翻译": "给定一组袋子，每个袋子是一组图像，我们的目标是从每个袋子中选择一张图像，使得所选图像来自相同的对象类别。我们将选择建模为一个带有单元和成对潜在函数的能量最小化问题。受到最近少样本学习算法的启发，我们提出了一种直接从数据中学习潜在函数的方法。此外，我们提出了一种快速的贪婪推理算法用于能量最小化。我们在少样本共同对象识别以及对象共定位任务上评估了我们的方法。我们的实验表明，学习成对和单元项大大提高了模型在这些任务上相对于几种知名方法的性能。所提出的贪婪优化算法在性能上与最先进的结构化推理算法相当，同时速度快了10倍。",
        "领域": "少样本学习/对象识别/能量最小化",
        "问题": "从多个图像集合中选择出属于同一对象类别的图像",
        "动机": "提高在少样本共同对象识别和对象共定位任务中的性能",
        "方法": "提出了一种直接从数据中学习潜在函数的方法，并开发了一种快速的贪婪推理算法用于能量最小化",
        "关键词": [
            "少样本学习",
            "对象识别",
            "能量最小化",
            "贪婪推理算法"
        ],
        "涉及的技术概念": "能量最小化问题涉及单元和成对潜在函数，少样本学习算法用于直接从数据中学习这些函数，贪婪推理算法用于快速求解能量最小化问题。"
    },
    {
        "order": 221,
        "title": "Distill Knowledge From NRSfM for Weakly Supervised 3D Pose Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Distill_Knowledge_From_NRSfM_for_Weakly_Supervised_3D_Pose_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Distill_Knowledge_From_NRSfM_for_Weakly_Supervised_3D_Pose_Learning_ICCV_2019_paper.html",
        "abstract": "We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision.",
        "中文标题": "从NRSfM中蒸馏知识用于弱监督3D姿态学习",
        "摘要翻译": "我们提出通过从非刚性运动结构（NRSfM）中蒸馏知识来学习3D姿态估计器。我们的方法仅使用2D地标注释。不需要3D数据、多视角/时间序列视频或对象特定的先验知识。这缓解了数据瓶颈，这是监督方法的主要关注点之一。使用NRSfM作为教师的挑战在于，当2D投影具有强烈歧义时，它们通常进行深度重建的效果不佳。直接使用这些错误的深度作为硬目标会对学生产生负面影响。相反，我们提出了一种新颖的损失函数，将深度预测与NRSfM中使用的成本函数联系起来。这使得学生姿态估计器能够通过关联图像特征来减少深度误差。在H3.6M数据集上验证，我们学习的3D姿态估计网络比NRSfM方法实现了更准确的重建。尽管使用了显著较少的监督，它也优于其他弱监督方法。",
        "领域": "3D姿态估计/知识蒸馏/弱监督学习",
        "问题": "在缺乏3D数据、多视角/时间序列视频或对象特定先验知识的情况下，如何准确进行3D姿态估计",
        "动机": "缓解监督方法中的数据瓶颈，提高3D姿态估计的准确性和效率",
        "方法": "提出一种新颖的损失函数，将深度预测与NRSfM中使用的成本函数联系起来，使得学生姿态估计器能够通过关联图像特征来减少深度误差",
        "关键词": [
            "3D姿态估计",
            "知识蒸馏",
            "弱监督学习",
            "NRSfM",
            "深度重建"
        ],
        "涉及的技术概念": {
            "NRSfM": "非刚性运动结构，一种从2D投影中恢复3D结构的方法",
            "2D地标注释": "用于训练模型的2D关键点标注",
            "深度重建": "从2D图像中估计3D深度信息的过程",
            "损失函数": "用于衡量模型预测与真实值之间差异的函数，指导模型优化"
        }
    },
    {
        "order": 222,
        "title": "RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_RGB-Infrared_Cross-Modality_Person_Re-Identification_via_Joint_Pixel_and_Feature_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_RGB-Infrared_Cross-Modality_Person_Re-Identification_via_Joint_Pixel_and_Feature_Alignment_ICCV_2019_paper.html",
        "abstract": "RGB-Infrared (IR) person re-identification is an important and challenging task due to large cross-modality variations between RGB and IR images. Most conventional approaches aim to bridge the cross-modality gap with feature alignment by feature representation learning. Different from existing methods, in this paper, we propose a novel and end-to-end Alignment Generative Adversarial Network (AlignGAN) for the RGB-IR RE-ID task. The proposed model enjoys several merits. First, it can exploit pixel alignment and feature alignment jointly. To the best of our knowledge, this is the first work to model the two alignment strategies jointly for the RGB-IR RE-ID problem. Second, the proposed model consists of a pixel generator, a feature generator and a joint discriminator. By playing a min-max game among the three components, our model is able to not only alleviate the cross-modality and intra-modality variations, but also learn identity-consistent features. Extensive experimental results on two standard benchmarks demonstrate that the proposed model performs favourably against state-of-the-art methods. Especially, on SYSU-MM01 dataset, our model can achieve an absolute gain of 15.4% and 12.9% in terms of Rank-1 and mAP.",
        "中文标题": "通过联合像素和特征对齐实现RGB-红外跨模态行人重识别",
        "摘要翻译": "RGB-红外（IR）行人重识别由于RGB和IR图像之间存在大的跨模态变化，是一项重要且具有挑战性的任务。大多数传统方法旨在通过特征表示学习来弥合跨模态差距。与现有方法不同，本文提出了一种新颖的端到端对齐生成对抗网络（AlignGAN）用于RGB-IR RE-ID任务。所提出的模型具有几个优点。首先，它可以联合利用像素对齐和特征对齐。据我们所知，这是首次将这两种对齐策略联合建模用于RGB-IR RE-ID问题。其次，所提出的模型由像素生成器、特征生成器和联合判别器组成。通过在三个组件之间进行最小最大游戏，我们的模型不仅能够减轻跨模态和模态内变化，还能学习身份一致的特征。在两个标准基准上的大量实验结果表明，所提出的模型在性能上优于最先进的方法。特别是在SYSU-MM01数据集上，我们的模型在Rank-1和mAP方面分别实现了15.4%和12.9%的绝对增益。",
        "领域": "行人重识别/跨模态学习/生成对抗网络",
        "问题": "RGB和红外图像之间的跨模态行人重识别",
        "动机": "解决RGB和红外图像之间由于跨模态变化导致的行人重识别挑战",
        "方法": "提出了一种端到端的对齐生成对抗网络（AlignGAN），联合利用像素对齐和特征对齐，通过像素生成器、特征生成器和联合判别器之间的最小最大游戏，减轻跨模态和模态内变化，学习身份一致的特征",
        "关键词": [
            "行人重识别",
            "跨模态学习",
            "生成对抗网络"
        ],
        "涉及的技术概念": "RGB-红外（IR）行人重识别、跨模态变化、特征表示学习、对齐生成对抗网络（AlignGAN）、像素对齐、特征对齐、最小最大游戏、身份一致的特征"
    },
    {
        "order": 223,
        "title": "Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Weakly_Aligned_Cross-Modal_Learning_for_Multispectral_Pedestrian_Detection_ICCV_2019_paper.html",
        "abstract": "Multispectral pedestrian detection has shown great advantages under poor illumination conditions, since the thermal modality provides complementary information for the color image. However, real multispectral data suffers from the position shift problem, i.e. the color-thermal image pairs are not strictly aligned, making one object has different positions in different modalities. In deep learning based methods, this problem makes it difficult to fuse the feature maps from both modalities and puzzles the CNN training. In this paper, we propose a novel Aligned Region CNN (AR-CNN) to handle the weakly aligned multispectral data in an end-to-end way. Firstly, we design a Region Feature Alignment (RFA) module to capture the position shift and adaptively align the region features of the two modalities. Secondly, we present a new multimodal fusion method, which performs feature re-weighting to select more reliable features and suppress the useless ones. Besides, we propose a novel RoI jitter strategy to improve the robustness to unexpected shift patterns of different devices and system settings. Finally, since our method depends on a new kind of labelling: bounding boxes that match each modality, we manually relabel the KAIST dataset by locating bounding boxes in both modalities and building their relationships, providing a new KAIST-Paired Annotation. Extensive experimental validations on existing datasets are performed, demonstrating the effectiveness and robustness of the proposed method. Code and data are available at https://github.com/luzhang16/AR-CNN.",
        "中文标题": "弱对齐跨模态学习用于多光谱行人检测",
        "摘要翻译": "多光谱行人检测在光照条件差的情况下显示出巨大优势，因为热模态为彩色图像提供了补充信息。然而，真实的多光谱数据存在位置偏移问题，即彩色-热图像对并未严格对齐，使得同一物体在不同模态中具有不同的位置。在基于深度学习的方法中，这一问题使得融合来自两种模态的特征图变得困难，并困扰了CNN的训练。在本文中，我们提出了一种新颖的对齐区域CNN（AR-CNN），以端到端的方式处理弱对齐的多光谱数据。首先，我们设计了一个区域特征对齐（RFA）模块，以捕捉位置偏移并自适应地对齐两种模态的区域特征。其次，我们提出了一种新的多模态融合方法，该方法执行特征重新加权以选择更可靠的特征并抑制无用的特征。此外，我们提出了一种新颖的RoI抖动策略，以提高对不同设备和系统设置的意外偏移模式的鲁棒性。最后，由于我们的方法依赖于一种新的标注：匹配每种模态的边界框，我们通过定位两种模态中的边界框并建立它们的关系，手动重新标注了KAIST数据集，提供了一个新的KAIST配对标注。在现有数据集上进行了广泛的实验验证，证明了所提出方法的有效性和鲁棒性。代码和数据可在https://github.com/luzhang16/AR-CNN获取。",
        "领域": "多光谱行人检测/特征融合/深度学习",
        "问题": "多光谱数据中的位置偏移问题，导致彩色-热图像对未严格对齐，影响特征融合和CNN训练。",
        "动机": "解决多光谱行人检测中由于位置偏移导致的特征融合困难，提高检测的准确性和鲁棒性。",
        "方法": "提出了一种对齐区域CNN（AR-CNN），包括区域特征对齐（RFA）模块、新的多模态融合方法和RoI抖动策略，以端到端的方式处理弱对齐的多光谱数据。",
        "关键词": [
            "多光谱行人检测",
            "特征融合",
            "区域特征对齐",
            "RoI抖动策略",
            "KAIST数据集"
        ],
        "涉及的技术概念": {
            "多光谱行人检测": "利用彩色和热成像技术进行行人检测，特别是在光照条件差的情况下。",
            "特征融合": "将来自不同模态（如彩色和热成像）的特征结合起来，以提高检测性能。",
            "区域特征对齐（RFA）": "一种模块，用于捕捉并自适应地对齐不同模态中的区域特征，以解决位置偏移问题。",
            "RoI抖动策略": "一种提高模型对不同设备和系统设置下意外偏移模式鲁棒性的策略。",
            "KAIST数据集": "一个用于多光谱行人检测研究的数据集，本文中进行了手动重新标注以匹配每种模态的边界框。"
        }
    },
    {
        "order": 224,
        "title": "MONET: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_MONET_Multiview_Semi-Supervised_Keypoint_Detection_via_Epipolar_Divergence_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yao_MONET_Multiview_Semi-Supervised_Keypoint_Detection_via_Epipolar_Divergence_ICCV_2019_paper.html",
        "abstract": "This paper presents MONET---an end-to-end semi-supervised learning framework for a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence---a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys.",
        "中文标题": "MONET：通过极线差异进行多视图半监督关键点检测",
        "摘要翻译": "本文介绍了MONET---一个端到端的半监督学习框架，用于使用多视图图像流进行关键点检测。特别是，我们考虑了如非人类物种等一般主题，在这些主题中获取大规模注释数据集具有挑战性。虽然多视图几何可用于自我监督未标记的数据，但由于表示不匹配，将几何集成到学习关键点检测器中具有挑战性。我们通过制定一种新的可微分极线约束表示来解决这种不匹配，称为极线差异---从极线到相应关键点分布的广义距离。极线差异描述了当两个视图的关键点分布产生零重投影误差时的特征。我们设计了一个双网络，通过立体校正最小化极线差异，这可以显著减轻训练中的计算复杂性和采样混叠。我们证明了我们的框架可以定位各种物种的定制关键点，例如人类、狗和猴子。",
        "领域": "关键点检测/多视图几何/半监督学习",
        "问题": "在非人类物种等一般主题中，获取大规模注释数据集具有挑战性，以及如何将多视图几何集成到学习关键点检测器中。",
        "动机": "为了解决在非人类物种等一般主题中获取大规模注释数据集的挑战，并探索如何有效地将多视图几何集成到关键点检测器的学习中。",
        "方法": "提出了一种新的可微分极线约束表示---极线差异，并设计了一个双网络通过立体校正最小化极线差异，以减轻训练中的计算复杂性和采样混叠。",
        "关键词": [
            "关键点检测",
            "多视图几何",
            "半监督学习",
            "极线差异",
            "立体校正"
        ],
        "涉及的技术概念": {
            "极线差异": "一种新的可微分极线约束表示，用于描述从极线到相应关键点分布的广义距离，当两个视图的关键点分布产生零重投影误差时的特征。",
            "双网络": "设计用于通过立体校正最小化极线差异的网络结构，以减轻训练中的计算复杂性和采样混叠。",
            "立体校正": "一种技术，用于调整图像对，使得它们看起来像是从同一视角拍摄的，从而简化多视图几何问题的解决。"
        }
    },
    {
        "order": 225,
        "title": "EvalNorm: Estimating Batch Normalization Statistics for Evaluation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_EvalNorm_Estimating_Batch_Normalization_Statistics_for_Evaluation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Singh_EvalNorm_Estimating_Batch_Normalization_Statistics_for_Evaluation_ICCV_2019_paper.html",
        "abstract": "Batch normalization (BN) has been very effective for deep learning and is widely used. However, when training with small minibatches, models using BN exhibit a significant degradation in performance. In this paper we study this peculiar behavior of BN to gain a better understanding of the problem, and identify a cause. We propose `EvalNorm' to address the issue by estimating corrected normalization statistics to use for BN during evaluation. EvalNorm supports online estimation of the corrected statistics while the model is being trained, and does not affect the training scheme of the model. As a result, EvalNorm can also be used with existing pre-trained models allowing them to benefit from our method. EvalNorm yields large gains for models trained with smaller batches. Our experiments show that EvalNorm performs 6.18% (absolute) better than vanilla BN for a batchsize of 2 on ImageNet validation set and from 1.5 to 7.0 points (absolute) gain on the COCO object detection benchmark across a variety of setups.",
        "中文标题": "EvalNorm: 评估时估计批量归一化统计量",
        "摘要翻译": "批量归一化（BN）对深度学习非常有效并被广泛使用。然而，当使用小批量进行训练时，使用BN的模型表现出显著的性能下降。在本文中，我们研究了BN的这种特殊行为，以更好地理解问题，并确定了一个原因。我们提出了`EvalNorm`，通过估计用于评估时BN的校正归一化统计量来解决这个问题。EvalNorm支持在模型训练时在线估计校正统计量，并且不影响模型的训练方案。因此，EvalNorm也可以与现有的预训练模型一起使用，使它们能够从我们的方法中受益。EvalNorm为使用较小批量训练的模型带来了显著的增益。我们的实验表明，在ImageNet验证集上，对于批量大小为2的情况，EvalNorm比普通BN表现好6.18%（绝对值），在COCO对象检测基准上，在各种设置中获得了1.5到7.0点（绝对值）的增益。",
        "领域": "深度学习优化/批量归一化/模型评估",
        "问题": "使用小批量进行训练时，批量归一化（BN）模型性能显著下降的问题",
        "动机": "为了更好地理解批量归一化在小批量训练时的性能下降问题，并找到解决方案",
        "方法": "提出EvalNorm方法，通过估计校正的归一化统计量用于评估时的批量归一化，支持在线估计校正统计量而不影响模型训练方案",
        "关键词": [
            "批量归一化",
            "小批量训练",
            "模型评估",
            "性能优化"
        ],
        "涉及的技术概念": "批量归一化（BN）是一种在深度学习中用于加速训练和提高模型性能的技术，通过归一化每一层的输入来实现。EvalNorm是一种新提出的方法，旨在解决BN在小批量训练时性能下降的问题，通过估计校正的归一化统计量用于模型评估，同时支持在线估计这些统计量，不影响模型的训练过程。"
    },
    {
        "order": 226,
        "title": "Deep Self-Learning From Noisy Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Deep_Self-Learning_From_Noisy_Labels_ICCV_2019_paper.html",
        "abstract": "ConvNets achieve good results when training from clean data, but learning from noisy labels significantly degrades performances and remains challenging. Unlike previous works constrained by many conditions, making them infeasible to real noisy cases, this work presents a novel deep self-learning framework to train a robust network on the real noisy datasets without extra supervision. The proposed approach has several appealing benefits. (1) Different from most existing work, it does not rely on any assumption on the distribution of the noisy labels, making it robust to real noises. (2) It does not need extra clean supervision or accessorial network to help training. (3) A self-learning framework is proposed to train the network in an iterative end-to-end manner, which is effective and efficient. Extensive experiments in challenging benchmarks such as Clothing1M and Food101-N show that our approach outperforms its counterparts in all empirical settings.",
        "中文标题": "从噪声标签中进行深度自学习",
        "摘要翻译": "当从干净数据训练时，卷积网络（ConvNets）能够取得良好的结果，但从噪声标签中学习会显著降低性能，这仍然是一个挑战。与之前受许多条件限制的工作不同，这些条件使得它们在实际噪声情况下不可行，本工作提出了一种新颖的深度自学习框架，以在没有额外监督的情况下在真实噪声数据集上训练一个鲁棒的网络。所提出的方法有几个吸引人的优点。（1）与大多数现有工作不同，它不依赖于噪声标签分布的任何假设，使其对真实噪声具有鲁棒性。（2）它不需要额外的干净监督或辅助网络来帮助训练。（3）提出了一种自学习框架，以迭代的端到端方式训练网络，这种方式既有效又高效。在具有挑战性的基准测试（如Clothing1M和Food101-N）中进行的大量实验表明，我们的方法在所有实验设置中都优于其对应方法。",
        "领域": "噪声标签学习/自学习框架/鲁棒网络训练",
        "问题": "在存在噪声标签的情况下训练鲁棒的卷积网络",
        "动机": "解决从噪声标签中学习时性能显著下降的问题，提出一种不依赖额外监督或辅助网络的自学习框架",
        "方法": "提出了一种深度自学习框架，该框架不依赖于噪声标签分布的假设，不需要额外的干净监督或辅助网络，以迭代的端到端方式训练网络",
        "关键词": [
            "噪声标签",
            "自学习",
            "鲁棒网络训练"
        ],
        "涉及的技术概念": "卷积网络（ConvNets）、噪声标签、自学习框架、端到端训练、鲁棒网络训练"
    },
    {
        "order": 227,
        "title": "Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Talking_With_Hands_16.2M_A_Large-Scale_Dataset_of_Synchronized_Body-Finger_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Talking_With_Hands_16.2M_A_Large-Scale_Dataset_of_Synchronized_Body-Finger_ICCV_2019_paper.html",
        "abstract": "We present a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. Our dataset features synchronized body and finger motion as well as audio data. To the best of our knowledge, it represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis. As an illustration, we propose a novel real-time finger motion synthesis method: a temporal neural network innovatively trained with an inverse kinematics (IK) loss, which adds skeletal structural information to the generative model. Our qualitative user study shows that the finger motion generated by our method is perceived as natural and conversation enhancing, while the quantitative ablation study demonstrates the effectiveness of IK loss.",
        "中文标题": "用手交谈16.2M：用于对话运动分析和合成的大规模同步身体-手指运动和音频数据集",
        "摘要翻译": "我们提出了一个包含1620万帧（50小时）的两人面对面自发对话的多模态数据集。我们的数据集以同步的身体和手指运动以及音频数据为特色。据我们所知，它代表了迄今为止最大的自然对话的运动捕捉和音频数据集。统计分析验证了手臂、手和语音特征的强个人内和人际协方差，可能为数据驱动的社会行为分析、预测和合成开辟新方向。作为示例，我们提出了一种新颖的实时手指运动合成方法：一种创新地使用逆运动学（IK）损失训练的时间神经网络，该损失为生成模型添加了骨骼结构信息。我们的定性用户研究表明，我们的方法生成的手指运动被认为自然且增强了对话，而定量消融研究证明了IK损失的有效性。",
        "领域": "运动捕捉/社会行为分析/音频处理",
        "问题": "如何从自然对话中捕捉和分析同步的身体、手指运动和音频数据，以推动社会行为分析、预测和合成的新方向",
        "动机": "为了填补大规模自然对话的运动捕捉和音频数据集的空白，并探索数据驱动的社会行为分析、预测和合成的新方法",
        "方法": "提出了一种新颖的实时手指运动合成方法，使用逆运动学（IK）损失训练的时间神经网络，为生成模型添加骨骼结构信息",
        "关键词": [
            "运动捕捉",
            "社会行为分析",
            "音频处理",
            "逆运动学",
            "时间神经网络"
        ],
        "涉及的技术概念": {
            "运动捕捉": "捕捉和分析身体和手指的运动数据",
            "逆运动学（IK）": "一种计算从末端执行器到关节角度的技术，用于生成模型以添加骨骼结构信息",
            "时间神经网络": "一种处理时间序列数据的神经网络，用于实时手指运动合成"
        }
    },
    {
        "order": 228,
        "title": "Beyond Human Parts: Dual Part-Aligned Representations for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Guo_Beyond_Human_Parts_Dual_Part-Aligned_Representations_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Person re-identification is a challenging task due to various complex factors. Recent studies have attempted to integrate human parsing results or externally defined attributes to help capture human parts or important object regions. On the other hand, there still exist many useful contextual cues that do not fall into the scope of predefined human parts or attributes. In this paper, we address the missed contextual cues by exploiting both the accurate human parts and the coarse non-human parts. In our implementation, we apply a human parsing model to extract the binary human part masks and a self-attention mechanism to capture the soft latent (non-human) part masks. We verify the effectiveness of our approach with new state-of-the-art performance on three challenging benchmarks: Market-1501, DukeMTMC-reID and CUHK03. Our implementation is available at https://github.com/ggjy/P2Net.pytorch.",
        "中文标题": "超越人体部分：用于行人重识别的双重部分对齐表示",
        "摘要翻译": "行人重识别由于各种复杂因素而成为一个具有挑战性的任务。最近的研究尝试整合人体解析结果或外部定义的属性，以帮助捕捉人体部分或重要物体区域。另一方面，仍存在许多有用的上下文线索，这些线索不属于预定义的人体部分或属性的范畴。在本文中，我们通过利用精确的人体部分和粗糙的非人体部分来解决遗漏的上下文线索。在我们的实现中，我们应用了一个人体解析模型来提取二进制人体部分掩码，并使用自注意力机制来捕捉软潜在（非人体）部分掩码。我们在三个具有挑战性的基准测试上验证了我们方法的有效性，并取得了新的最先进性能：Market-1501、DukeMTMC-reID和CUHK03。我们的实现可在https://github.com/ggjy/P2Net.pytorch获取。",
        "领域": "行人重识别/自注意力机制/人体解析",
        "问题": "解决行人重识别中遗漏的上下文线索问题",
        "动机": "为了捕捉不属于预定义人体部分或属性范畴的上下文线索，提高行人重识别的准确性",
        "方法": "应用人体解析模型提取二进制人体部分掩码，并使用自注意力机制捕捉软潜在（非人体）部分掩码",
        "关键词": [
            "行人重识别",
            "自注意力机制",
            "人体解析"
        ],
        "涉及的技术概念": "人体解析模型用于提取二进制人体部分掩码，自注意力机制用于捕捉软潜在（非人体）部分掩码，这些技术概念用于解决行人重识别中的上下文线索遗漏问题。"
    },
    {
        "order": 229,
        "title": "DSConv: Efficient Convolution Operator",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/do_Nascimento_DSConv_Efficient_Convolution_Operator_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/do_Nascimento_DSConv_Efficient_Convolution_Operator_ICCV_2019_paper.html",
        "abstract": "Quantization is a popular way of increasing the speed and lowering the memory usage of Convolution Neural Networks (CNNs). When labelled training data is available, network weights and activations have successfully been quantized down to 1-bit. The same cannot be said about the scenario when labelled training data is not available, e.g. when quantizing a pre-trained model, where current approaches show, at best, no loss of accuracy at 8-bit quantizations. We introduce DSConv, a flexible quantized convolution operator that replaces single-precision operations with their far less expensive integer counterparts, while maintaining the probability distributions over both the kernel weights and the outputs. We test our model as a plug-and-play replacement for standard convolution on most popular neural network architectures, ResNet, DenseNet, GoogLeNet, AlexNet and VGG-Net and demonstrate state-of-the-art results, with less than 1% loss of accuracy, without retraining, using only 4-bit quantization. We also show how a distillation-based adaptation stage with unlabelled data can improve results even further.",
        "中文标题": "DSConv: 高效的卷积算子",
        "摘要翻译": "量化是提高卷积神经网络（CNNs）速度和降低内存使用的一种流行方法。当有标记的训练数据可用时，网络权重和激活已成功量化为1位。然而，在没有标记训练数据的情况下，例如在量化预训练模型时，当前的方法最多只能在8位量化时保持准确率不下降。我们引入了DSConv，一种灵活的量化卷积算子，它用成本低得多的整数操作替换了单精度操作，同时保持了核权重和输出的概率分布。我们在最流行的神经网络架构上测试了我们的模型，作为标准卷积的即插即用替代品，包括ResNet、DenseNet、GoogLeNet、AlexNet和VGG-Net，并展示了最先进的结果，在仅使用4位量化且无需重新训练的情况下，准确率损失不到1%。我们还展示了如何通过基于蒸馏的适应阶段与未标记数据进一步改善结果。",
        "领域": "神经网络优化/量化技术/卷积神经网络",
        "问题": "在缺乏标记训练数据的情况下，如何有效地量化卷积神经网络以减少计算成本和内存使用，同时保持或最小化准确率的损失。",
        "动机": "提高卷积神经网络的计算效率和降低内存使用，特别是在没有标记训练数据的情况下，量化预训练模型时保持高准确率。",
        "方法": "引入DSConv，一种灵活的量化卷积算子，通过用整数操作替换单精度操作来减少计算成本，同时保持核权重和输出的概率分布。此外，采用基于蒸馏的适应阶段与未标记数据进一步改善结果。",
        "关键词": [
            "量化",
            "卷积神经网络",
            "计算效率",
            "内存优化"
        ],
        "涉及的技术概念": "DSConv是一种量化卷积算子，旨在通过使用整数操作来减少卷积神经网络的计算成本和内存使用，同时保持网络的准确率。这种方法特别适用于在没有标记训练数据的情况下量化预训练模型。通过基于蒸馏的适应阶段，可以进一步利用未标记数据来优化量化效果。"
    },
    {
        "order": 230,
        "title": "Occlusion Robust Face Recognition Based on Mask Learning With Pairwise Differential Siamese Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Song_Occlusion_Robust_Face_Recognition_Based_on_Mask_Learning_With_Pairwise_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Song_Occlusion_Robust_Face_Recognition_Based_on_Mask_Learning_With_Pairwise_ICCV_2019_paper.html",
        "abstract": "Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems.",
        "中文标题": "基于掩码学习与成对差分孪生网络的遮挡鲁棒人脸识别",
        "摘要翻译": "深度卷积神经网络（CNNs）在过去几年中推动了人脸识别的前沿。然而，现有的CNN模型在处理部分遮挡的人脸时准确度远不如人意。这些通用的人脸模型对于不同面部区域的遮挡泛化能力较差。受到人类视觉系统明确忽略遮挡并仅关注非遮挡面部区域的事实的启发，我们提出了一种掩码学习策略，以从识别中找出并丢弃损坏的特征元素。首先，通过利用遮挡和无遮挡人脸对的顶部卷积特征之间的差异，使用创新设计的成对差分孪生网络（PDSN）建立了一个掩码字典。该字典的每一项都捕捉了遮挡面部区域与损坏特征元素之间的对应关系，称为特征丢弃掩码（FDM）。当处理具有随机部分遮挡的人脸图像时，我们通过组合相关字典项生成其FDM，然后将其与原始特征相乘，以从识别中消除那些损坏的特征元素。在合成和现实的遮挡人脸数据集上的综合实验表明，所提出的算法显著优于最先进的系统。",
        "领域": "人脸识别/遮挡处理/特征学习",
        "问题": "处理部分遮挡的人脸识别准确度低的问题",
        "动机": "受到人类视觉系统忽略遮挡并仅关注非遮挡面部区域的启发，提出掩码学习策略以提高遮挡人脸的识别准确度",
        "方法": "提出了一种基于成对差分孪生网络（PDSN）的掩码学习策略，通过建立掩码字典并生成特征丢弃掩码（FDM）来消除损坏的特征元素",
        "关键词": [
            "掩码学习",
            "成对差分孪生网络",
            "特征丢弃掩码",
            "遮挡处理"
        ],
        "涉及的技术概念": "深度卷积神经网络（CNNs）、成对差分孪生网络（PDSN）、特征丢弃掩码（FDM）、掩码字典"
    },
    {
        "order": 231,
        "title": "Person Search by Text Attribute Query As Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Person_Search_by_Text_Attribute_Query_As_Zero-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Person_Search_by_Text_Attribute_Query_As_Zero-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Existing person search methods predominantly assume the availability of at least one-shot imagery sample of the queried person. This assumption is limited in circumstances where only a brief textual (or verbal) description of the target person is available. In this work, we present a deep learning method for attribute text description based person search without any query imagery. Whilst conventional cross-modality matching methods, such as global visual-textual embedding based zero-shot learning and local individual attribute recognition, are functionally applicable, they are limited by several assumptions invalid to person search in deployment scale, data quality, and/or category name semantics. We overcome these issues by formulating an Attribute-Image Hierarchical Matching (AIHM) model. It is able to more reliably match text attribute descriptions with noisy surveillance person images by jointly learning global category-level and local attribute-level textual-visual embedding as well as matching. Extensive evaluations demonstrate the superiority of our AIHM model over a wide variety of state-of-the-art methods on three publicly available attribute labelled surveillance person search benchmarks: Market-1501, DukeMTMC, and PA100K.",
        "中文标题": "通过文本属性查询进行人物搜索作为零样本学习",
        "摘要翻译": "现有的人物搜索方法主要假设至少有一个查询人物的图像样本可用。这种假设在只有目标人物的简短文本（或口头）描述可用的情况下是有限的。在这项工作中，我们提出了一种基于属性文本描述的深度学习人物搜索方法，无需任何查询图像。虽然传统的跨模态匹配方法，如基于全局视觉-文本嵌入的零样本学习和局部个体属性识别，在功能上是适用的，但它们在部署规模、数据质量和/或类别名称语义方面受到几个假设的限制。我们通过制定一个属性-图像层次匹配（AIHM）模型来克服这些问题。它能够通过联合学习全局类别级和局部属性级的文本-视觉嵌入以及匹配，更可靠地将文本属性描述与嘈杂的监控人物图像匹配。广泛的评估表明，我们的AIHM模型在三个公开可用的属性标记监控人物搜索基准测试：Market-1501、DukeMTMC和PA100K上，优于各种最先进的方法。",
        "领域": "人物搜索/零样本学习/跨模态匹配",
        "问题": "在只有目标人物的简短文本描述可用的情况下，如何有效地进行人物搜索",
        "动机": "现有的人物搜索方法依赖于至少一个查询人物的图像样本，这在只有文本描述可用的情况下存在限制",
        "方法": "提出了一种属性-图像层次匹配（AIHM）模型，通过联合学习全局类别级和局部属性级的文本-视觉嵌入以及匹配，来解决这一问题",
        "关键词": [
            "人物搜索",
            "零样本学习",
            "跨模态匹配",
            "属性-图像层次匹配"
        ],
        "涉及的技术概念": "零样本学习指的是在没有特定类别样本的情况下进行学习；跨模态匹配涉及不同模态（如文本和图像）之间的匹配；属性-图像层次匹配（AIHM）模型是一种通过联合学习全局和局部特征来进行文本和图像匹配的方法。"
    },
    {
        "order": 232,
        "title": "Explicit Shape Encoding for Real-Time Instance Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Explicit_Shape_Encoding_for_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Explicit_Shape_Encoding_for_Real-Time_Instance_Segmentation_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel top-down instance segmentation framework based on explicit shape encoding, named ESE-Seg. It largely reduces the computational consumption of the instance segmentation by explicitly decoding the multiple object shapes with tensor operations, thus performs the instance segmentation at almost the same speed as the object detection. ESE-Seg is based on a novel shape signature Inner-center Radius (IR), Chebyshev polynomial fitting and the strong modern object detectors. ESE-Seg with YOLOv3 outperforms the Mask R-CNN on Pascal VOC 2012 at mAP^r@0.5 while 7 times faster.",
        "中文标题": "实时实例分割的显式形状编码",
        "摘要翻译": "在本文中，我们提出了一种基于显式形状编码的新型自上而下实例分割框架，命名为ESE-Seg。它通过使用张量操作显式解码多个对象形状，大大减少了实例分割的计算消耗，从而以几乎与对象检测相同的速度执行实例分割。ESE-Seg基于一种新颖的形状签名内部中心半径（IR）、切比雪夫多项式拟合和强大的现代对象检测器。ESE-Seg与YOLOv3结合，在Pascal VOC 2012上的mAP^r@0.5优于Mask R-CNN，同时速度快7倍。",
        "领域": "实例分割/对象检测/张量操作",
        "问题": "减少实例分割的计算消耗，提高处理速度",
        "动机": "为了在保持高准确率的同时，显著提高实例分割的处理速度，使其接近对象检测的速度",
        "方法": "采用显式形状编码（ESE）技术，结合内部中心半径（IR）形状签名、切比雪夫多项式拟合和现代对象检测器，开发了ESE-Seg框架",
        "关键词": [
            "实例分割",
            "显式形状编码",
            "对象检测",
            "张量操作",
            "切比雪夫多项式拟合"
        ],
        "涉及的技术概念": "显式形状编码（ESE）是一种技术，通过显式地解码对象的形状来减少计算消耗。内部中心半径（IR）是一种形状签名，用于描述对象的形状特征。切比雪夫多项式拟合是一种数学方法，用于近似表示形状。YOLOv3和Mask R-CNN是现代对象检测和实例分割的先进模型。"
    },
    {
        "order": 233,
        "title": "Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Teacher_Supervises_Students_How_to_Learn_From_Partially_Labeled_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Teacher_Supervises_Students_How_to_Learn_From_Partially_Labeled_Images_ICCV_2019_paper.html",
        "abstract": "Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data and become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.",
        "中文标题": "教师指导学生如何从部分标记的图像中学习用于面部标志检测",
        "摘要翻译": "面部标志检测旨在定位人脸的解剖学定义点。在本文中，我们研究了从部分标记的面部图像中进行面部标志检测。典型的方法是（1）在标记的图像上训练检测器；（2）使用该检测器的预测作为未标记图像的伪标签生成新的训练样本；（3）在标记样本和部分伪标记样本上重新训练检测器。通过这种方式，检测器可以从标记和未标记的数据中学习并变得鲁棒。在本文中，我们提出了一种教师和两个学生之间的交互机制，以生成更可靠的未标记数据的伪标签，这对半监督面部标志检测是有益的。具体来说，两个学生被实例化为双检测器。教师学会判断学生生成的伪标签的质量，并在重新训练阶段之前过滤掉不合格的样本。通过这种方式，学生检测器从他们的教师那里获得反馈，并通过自己生成的高级数据进行重新训练。由于两个学生是由不同的样本训练的，他们的预测组合将比任何一个预测都更鲁棒，作为最终预测。在300-W和AFLW基准上的大量实验表明，教师和学生之间的交互有助于更好地利用未标记的数据，并实现了最先进的性能。",
        "领域": "面部标志检测/半监督学习/深度学习",
        "问题": "如何从部分标记的面部图像中进行有效的面部标志检测",
        "动机": "提高面部标志检测的准确性和鲁棒性，特别是在标记数据有限的情况下",
        "方法": "提出了一种教师和两个学生之间的交互机制，通过生成更可靠的伪标签来改进半监督面部标志检测",
        "关键词": [
            "面部标志检测",
            "半监督学习",
            "伪标签",
            "教师学生模型"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括面部标志检测、半监督学习、伪标签生成、教师学生模型。面部标志检测是指定位人脸的解剖学定义点；半监督学习是一种利用少量标记数据和大量未标记数据进行学习的方法；伪标签生成是指使用模型预测作为未标记数据的标签；教师学生模型是一种通过教师指导学生来提高模型性能的方法。"
    },
    {
        "order": 234,
        "title": "IMP: Instance Mask Projection for High Accuracy Semantic Segmentation of Things",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_IMP_Instance_Mask_Projection_for_High_Accuracy_Semantic_Segmentation_of_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_IMP_Instance_Mask_Projection_for_High_Accuracy_Semantic_Segmentation_of_ICCV_2019_paper.html",
        "abstract": "In this work, we present a new operator, called Instance Mask Projection (IMP), which projects a predicted instance segmentation as a new feature for semantic segmentation. It also supports back propagation and is trainable end-to end. By adding this operator, we introduce a new way to combine top-down and bottom-up information in semantic segmentation. Our experiments show the effectiveness of IMP on both clothing parsing (with complex layering, large deformations, and non-convex objects), and on street scene segmentation (with many overlapping instances and small objects). On the Varied Clothing Parsing dataset (VCP), we show instance mask projection can improve mIOU by 3 points over a state-of-the-art Panoptic FPN segmentation approach. On the ModaNet clothing parsing dataset, we show a dramatic improvement of 20.4% compared to existing baseline semantic segmentation results. In addition, the Instance Mask Projection operator works well on other (non-clothing) datasets, providing an improvement in mIOU of 3 points on \"thing\" classes of Cityscapes, a self-driving dataset, over a state-of-the-art approach.",
        "中文标题": "实例掩码投影：用于高精度物体语义分割",
        "摘要翻译": "在本工作中，我们提出了一个新的操作符，称为实例掩码投影（IMP），它将预测的实例分割投影为语义分割的新特征。该操作符还支持反向传播，并且可以端到端训练。通过添加这个操作符，我们引入了一种新的方式来结合语义分割中的自上而下和自下而上的信息。我们的实验展示了IMP在服装解析（具有复杂的层次、大变形和非凸物体）和街景分割（具有许多重叠实例和小物体）上的有效性。在Varied Clothing Parsing数据集（VCP）上，我们展示了实例掩码投影可以将mIOU提高3个百分点，优于最先进的Panoptic FPN分割方法。在ModaNet服装解析数据集上，我们展示了与现有基线语义分割结果相比，有20.4%的显著提升。此外，实例掩码投影操作符在其他（非服装）数据集上也表现良好，在自动驾驶数据集Cityscapes的“物体”类别上，与最先进的方法相比，mIOU提高了3个百分点。",
        "领域": "语义分割/实例分割/自动驾驶",
        "问题": "提高语义分割的准确度，特别是在处理复杂层次、大变形、非凸物体、重叠实例和小物体时",
        "动机": "为了更有效地结合语义分割中的自上而下和自下而上的信息，提高分割的准确度和效果",
        "方法": "引入实例掩码投影（IMP）操作符，将预测的实例分割投影为语义分割的新特征，支持反向传播和端到端训练",
        "关键词": [
            "实例分割",
            "语义分割",
            "自动驾驶",
            "服装解析",
            "街景分割"
        ],
        "涉及的技术概念": {
            "实例掩码投影（IMP）": "一种新的操作符，用于将实例分割的结果作为语义分割的特征，支持反向传播和端到端训练",
            "mIOU": "平均交并比，用于评估分割准确度的指标",
            "Panoptic FPN": "一种用于全景分割的先进方法，结合了实例分割和语义分割",
            "Varied Clothing Parsing（VCP）": "一个服装解析数据集，用于评估语义分割方法在处理复杂层次、大变形和非凸物体时的性能",
            "ModaNet": "一个服装解析数据集，用于评估语义分割方法的性能",
            "Cityscapes": "一个自动驾驶场景的数据集，用于评估语义分割方法在处理街景时的性能"
        }
    },
    {
        "order": 235,
        "title": "Semantic-Aware Knowledge Preservation for Zero-Shot Sketch-Based Image Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Semantic-Aware_Knowledge_Preservation_for_Zero-Shot_Sketch-Based_Image_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Semantic-Aware_Knowledge_Preservation_for_Zero-Shot_Sketch-Based_Image_Retrieval_ICCV_2019_paper.html",
        "abstract": "Sketch-based image retrieval (SBIR) is widely recognized as an important vision problem which implies a wide range of real-world applications. Recently, research interests arise in solving this problem under the more realistic and challenging setting of zero-shot learning. In this paper, we investigate this problem from the viewpoint of domain adaptation which we show is critical in improving feature embedding in the zero-shot scenario. Based on a framework which starts with a pre-trained model on ImageNet and fine-tunes it on the training set of SBIR benchmark, we advocate the importance of preserving previously acquired knowledge, e.g., the rich discriminative features learned from ImageNet, to improve the model's transfer ability. For this purpose, we design an approach named Semantic-Aware Knowledge prEservation (SAKE), which fine-tunes the pre-trained model in an economical way and leverages semantic information, e.g., inter-class relationship, to achieve the goal of knowledge preservation. Zero-shot experiments on two extended SBIR datasets, TU-Berlin and Sketchy, verify the superior performance of our approach. Extensive diagnostic experiments validate that knowledge preserved benefits SBIR in zero-shot settings, as a large fraction of the performance gain is from the more properly structured feature embedding for photo images.",
        "中文标题": "语义感知知识保持用于零样本基于草图的图像检索",
        "摘要翻译": "基于草图的图像检索（SBIR）被广泛认为是一个重要的视觉问题，它暗示了广泛的实际应用。最近，研究兴趣集中在解决这一问题上，在更现实和具有挑战性的零样本学习设置下。在本文中，我们从领域适应的角度探讨了这个问题，我们展示了这在提高零样本场景中的特征嵌入方面至关重要。基于一个从ImageNet预训练模型开始并在SBIR基准训练集上微调的框架，我们提倡保持先前获得的知识的重要性，例如从ImageNet学习到的丰富判别特征，以提高模型的迁移能力。为此，我们设计了一种名为语义感知知识保持（SAKE）的方法，它以经济的方式微调预训练模型，并利用语义信息，例如类间关系，以实现知识保持的目标。在两个扩展的SBIR数据集TU-Berlin和Sketchy上的零样本实验验证了我们方法的优越性能。广泛的诊断实验验证了在零样本设置下，保持的知识对SBIR有益，因为大部分性能提升来自于更适当结构的照片图像特征嵌入。",
        "领域": "图像检索/零样本学习/领域适应",
        "问题": "在零样本学习设置下提高基于草图的图像检索的性能",
        "动机": "提高模型在零样本场景中的迁移能力，通过保持从ImageNet学习到的丰富判别特征",
        "方法": "设计了一种名为语义感知知识保持（SAKE）的方法，以经济的方式微调预训练模型，并利用语义信息实现知识保持",
        "关键词": [
            "图像检索",
            "零样本学习",
            "领域适应",
            "语义信息",
            "知识保持"
        ],
        "涉及的技术概念": {
            "基于草图的图像检索（SBIR）": "一种通过草图来检索相关图像的技术",
            "零样本学习": "一种机器学习方法，旨在使模型能够识别在训练阶段未见过的类别",
            "领域适应": "一种技术，旨在使模型能够适应新的、不同的数据分布",
            "语义感知知识保持（SAKE）": "一种方法，通过微调预训练模型并利用语义信息来保持知识，以提高模型的迁移能力"
        }
    },
    {
        "order": 236,
        "title": "A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation From a Single Depth Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_A2J_Anchor-to-Joint_Regression_Network_for_3D_Articulated_Pose_Estimation_From_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_A2J_Anchor-to-Joint_Regression_Network_for_3D_Articulated_Pose_Estimation_From_ICCV_2019_paper.html",
        "abstract": "For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet- 50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.",
        "中文标题": "A2J：从单张深度图像进行3D关节姿态估计的锚点到关节回归网络",
        "摘要翻译": "针对深度图像中的3D手部和身体姿态估计任务，提出了一种新颖的基于锚点的方法，称为锚点到关节回归网络（A2J），具有端到端学习能力。在A2J中，能够在深度图像上密集设置锚点，作为关节的局部回归器，以捕捉全局-局部空间上下文信息。它们以集成方式预测关节的位置，以增强泛化能力。所提出的3D关节姿态估计范式与最先进的基于编码器-解码器的全卷积网络（FCN）、3D卷积神经网络（CNN）和基于点集的方法不同。为了发现对特定关节信息丰富的锚点，还为A2J提出了锚点提议程序。同时，使用2D CNN（即ResNet-50）作为A2J的骨干网络，而不使用耗时的3D卷积或反卷积层。在3个手部数据集和2个身体数据集上的实验验证了A2J的优越性。同时，A2J在单个NVIDIA 1080Ti GPU上的运行速度高达约100 FPS。",
        "领域": "3D姿态估计/深度图像处理/卷积神经网络",
        "问题": "从单张深度图像准确估计3D手部和身体关节姿态",
        "动机": "提高3D关节姿态估计的准确性和泛化能力，同时保持高运行速度",
        "方法": "提出了一种新颖的锚点到关节回归网络（A2J），通过密集设置锚点捕捉全局-局部空间上下文信息，以集成方式预测关节位置，并使用2D CNN（ResNet-50）作为骨干网络",
        "关键词": [
            "3D姿态估计",
            "深度图像",
            "卷积神经网络",
            "锚点回归",
            "端到端学习"
        ],
        "涉及的技术概念": "锚点到关节回归网络（A2J）是一种新颖的方法，用于从单张深度图像进行3D关节姿态估计。它通过密集设置锚点来捕捉全局-局部空间上下文信息，并以集成方式预测关节位置。与传统的基于编码器-解码器的全卷积网络（FCN）、3D卷积神经网络（CNN）和基于点集的方法不同，A2J使用2D CNN（如ResNet-50）作为骨干网络，避免了耗时的3D卷积或反卷积层，从而实现了高运行速度。"
    },
    {
        "order": 237,
        "title": "Video Instance Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Video_Instance_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Video_Instance_Segmentation_ICCV_2019_paper.html",
        "abstract": "In this paper we present a new computer vision task, named video instance segmentation. The goal of this new task is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain. To facilitate research on this new task, we propose a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks. In addition, we propose a novel algorithm called MaskTrack R-CNN for this task. Our new method introduces a new tracking branch to Mask R-CNN to jointly perform the detection, segmentation and tracking tasks simultaneously. Finally, we evaluate the proposed method and several strong baselines on our new dataset. Experimental results clearly demonstrate the advantages of the proposed algorithm and reveal insight for future improvement. We believe the video instance segmentation task will motivate the community along the line of research for video understanding.",
        "中文标题": "视频实例分割",
        "摘要翻译": "在本文中，我们提出了一个新的计算机视觉任务，名为视频实例分割。这个新任务的目标是同时进行视频中实例的检测、分割和跟踪。换句话说，这是首次将图像实例分割问题扩展到视频领域。为了促进这一新任务的研究，我们提出了一个名为YouTube-VIS的大规模基准，它包含2,883个高分辨率的YouTube视频、一个40类标签集和131k个高质量的实例掩码。此外，我们为这一任务提出了一种名为MaskTrack R-CNN的新算法。我们的新方法在Mask R-CNN中引入了一个新的跟踪分支，以同时执行检测、分割和跟踪任务。最后，我们在新数据集上评估了所提出的方法和几个强基线。实验结果清楚地展示了所提出算法的优势，并为未来的改进提供了见解。我们相信视频实例分割任务将激励社区沿着视频理解的研究方向前进。",
        "领域": "视频理解/实例分割/目标跟踪",
        "问题": "视频中实例的检测、分割和跟踪",
        "动机": "将图像实例分割问题扩展到视频领域，促进视频理解的研究",
        "方法": "提出MaskTrack R-CNN算法，在Mask R-CNN中引入新的跟踪分支，以同时执行检测、分割和跟踪任务",
        "关键词": [
            "视频实例分割",
            "MaskTrack R-CNN",
            "YouTube-VIS"
        ],
        "涉及的技术概念": {
            "视频实例分割": "一种新的计算机视觉任务，旨在同时进行视频中实例的检测、分割和跟踪",
            "MaskTrack R-CNN": "一种新算法，通过在Mask R-CNN中引入新的跟踪分支，以同时执行检测、分割和跟踪任务",
            "YouTube-VIS": "一个大规模基准，包含高分辨率的YouTube视频、标签集和高质量的实例掩码，用于视频实例分割任务的研究"
        }
    },
    {
        "order": 238,
        "title": "Active Learning for Deep Detection Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "The cost of drawing object bounding boxes (i.e. labeling) for millions of images is prohibitively high. For instance, labeling pedestrians in a regular urban image could take 35 seconds on average. Active learning aims to reduce the cost of labeling by selecting only those images that are informative to improve the detection network accuracy. In this paper, we propose a method to perform active learning of object detectors based on convolutional neural networks. We propose a new image-level scoring process to rank unlabeled images for their automatic selection, which clearly outperforms classical scores. The proposed method can be applied to videos and sets of still images. In the former case, temporal selection rules can complement our scoring process. As a relevant use case, we extensively study the performance of our method on the task of pedestrian detection. Overall, the experiments show that the proposed method performs better than random selection.",
        "中文标题": "深度检测神经网络的主动学习",
        "摘要翻译": "为数百万张图像绘制物体边界框（即标注）的成本极高。例如，在一张普通的城市图像中标注行人平均可能需要35秒。主动学习旨在通过仅选择那些对提高检测网络准确性有信息量的图像来降低标注成本。在本文中，我们提出了一种基于卷积神经网络的物体检测器主动学习方法。我们提出了一种新的图像级评分过程，用于对未标注图像进行排名以便自动选择，这种方法明显优于传统评分方法。所提出的方法可以应用于视频和静止图像集。在前者的情况下，时间选择规则可以补充我们的评分过程。作为一个相关用例，我们广泛研究了我们的方法在行人检测任务上的性能。总体而言，实验表明所提出的方法比随机选择表现更好。",
        "领域": "物体检测/行人检测/主动学习",
        "问题": "降低物体检测中标注成本的问题",
        "动机": "减少为大量图像标注物体边界框的高昂成本",
        "方法": "提出了一种基于卷积神经网络的物体检测器主动学习方法，包括一个新的图像级评分过程来对未标注图像进行排名以便自动选择",
        "关键词": [
            "物体检测",
            "行人检测",
            "主动学习",
            "卷积神经网络",
            "图像级评分"
        ],
        "涉及的技术概念": {
            "主动学习": "一种机器学习方法，旨在通过选择最有信息量的样本来减少标注成本",
            "卷积神经网络": "一种深度学习模型，特别适用于处理图像数据",
            "图像级评分": "一种评估未标注图像信息量的方法，用于主动学习中的样本选择",
            "行人检测": "计算机视觉中的一个任务，旨在从图像或视频中检测出行人"
        }
    },
    {
        "order": 239,
        "title": "TexturePose: Supervising Human Mesh Estimation With Texture Consistency",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pavlakos_TexturePose_Supervising_Human_Mesh_Estimation_With_Texture_Consistency_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pavlakos_TexturePose_Supervising_Human_Mesh_Estimation_With_Texture_Consistency_ICCV_2019_paper.html",
        "abstract": "This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/ pavlakos/projects/texturepose.",
        "中文标题": "TexturePose: 通过纹理一致性监督人体网格估计",
        "摘要翻译": "本工作解决了基于模型的人体姿态估计问题。最近的方法在直接从图像回归参数化人体模型的参数方面取得了显著进展。由于缺乏带有3D形状真实值的图像，相关方法依赖于2D注释或复杂的架构设计。在这项工作中，我们主张可以利用自然图像中免费提供的更多线索，即无需获取更多注释或修改网络架构。我们提出了一种自然的监督形式，利用人在不同帧（或视角）中的外观恒定性。这个看似微不足道且经常被忽视的线索对于基于模型的姿态估计大有裨益。我们采用的参数化模型允许我们为每一帧计算纹理图。假设人的纹理在帧之间不会发生剧烈变化，我们可以应用一种新颖的纹理一致性损失，强制纹理图中的每个点在所有帧中具有相同的纹理值。由于纹理是在这个共同的纹理图空间中传输的，因此不需要计算相机运动，甚至不需要假设帧之间的平滑性。这使得我们提出的监督方法适用于各种设置，从单眼视频到多视角图像。我们将我们的方法与需要相同甚至更多注释的强大基线进行了基准测试，并且我们始终优于它们。同时，我们在不同基准测试中实现了基于模型的姿态估计方法的最新成果。项目网站包含视频、结果和代码，可在https://seas.upenn.edu/ pavlakos/projects/texturepose找到。",
        "领域": "人体姿态估计/纹理分析/参数化模型",
        "问题": "基于模型的人体姿态估计中缺乏3D形状真实值图像的问题",
        "动机": "利用自然图像中免费提供的更多线索，无需获取更多注释或修改网络架构",
        "方法": "提出一种自然的监督形式，利用人在不同帧中的外观恒定性，应用纹理一致性损失",
        "关键词": [
            "人体姿态估计",
            "纹理一致性",
            "参数化模型"
        ],
        "涉及的技术概念": "纹理一致性损失是一种新颖的监督方法，它强制纹理图中的每个点在所有帧中具有相同的纹理值。这种方法不需要计算相机运动或假设帧之间的平滑性，适用于从单眼视频到多视角图像的各种设置。"
    },
    {
        "order": 240,
        "title": "Self-Supervised Difference Detection for Weakly-Supervised Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shimoda_Self-Supervised_Difference_Detection_for_Weakly-Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shimoda_Self-Supervised_Difference_Detection_for_Weakly-Supervised_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "To minimize the annotation costs associated with the training of semantic segmentation models, researchers have extensively investigated weakly-supervised segmentation approaches. In the current weakly-supervised segmentation methods, the most widely adopted approach is based on visualization. However, the visualization results are not generally equal to semantic segmentation. Therefore, to perform accurate semantic segmentation under the weakly supervised condition, it is necessary to consider the mapping functions that convert the visualization results into semantic segmentation. For such mapping functions, the conditional random field and iterative re-training using the outputs of a segmentation model are usually used. However, these methods do not always guarantee improvements in accuracy; therefore, if we apply these mapping functions iteratively multiple times, eventually the accuracy will not improve or will decrease. In this paper, to make the most of such mapping functions, we assume that the results of the mapping function include noise, and we improve the accuracy by removing noise. To achieve our aim, we propose the self-supervised difference detection module, which estimates noise from the results of the mapping functions by predicting the difference between the segmentation masks before and after the mapping. We verified the effectiveness of the proposed method by performing experiments on the PASCAL Visual Object Classes 2012 dataset, and we achieved 64.9% in the val set and 65.5% in the test set. Both of the results become new state-of-the-art under the same setting of weakly supervised semantic segmentation.",
        "中文标题": "自监督差异检测用于弱监督语义分割",
        "摘要翻译": "为了最小化与语义分割模型训练相关的标注成本，研究人员广泛研究了弱监督分割方法。在当前的弱监督分割方法中，最广泛采用的方法是基于可视化的。然而，可视化结果通常不等于语义分割。因此，为了在弱监督条件下执行准确的语义分割，有必要考虑将可视化结果转换为语义分割的映射函数。对于这样的映射函数，通常使用条件随机场和使用分割模型输出进行迭代重新训练。然而，这些方法并不总是保证准确性的提高；因此，如果我们多次迭代应用这些映射函数，最终准确性将不会提高或会下降。在本文中，为了充分利用这些映射函数，我们假设映射函数的结果包括噪声，并通过去除噪声来提高准确性。为了实现我们的目标，我们提出了自监督差异检测模块，该模块通过预测映射前后分割掩码之间的差异来估计映射函数结果中的噪声。我们在PASCAL Visual Object Classes 2012数据集上进行了实验，验证了所提出方法的有效性，并在验证集和测试集上分别达到了64.9%和65.5%的准确率。这两个结果在相同的弱监督语义分割设置下都成为了新的最先进水平。",
        "领域": "语义分割/弱监督学习/自监督学习",
        "问题": "在弱监督条件下提高语义分割的准确性",
        "动机": "减少语义分割模型训练中的标注成本，同时提高分割准确性",
        "方法": "提出自监督差异检测模块，通过预测映射前后分割掩码之间的差异来估计并去除映射函数结果中的噪声",
        "关键词": [
            "语义分割",
            "弱监督学习",
            "自监督学习",
            "噪声去除",
            "映射函数"
        ],
        "涉及的技术概念": "条件随机场、迭代重新训练、自监督差异检测模块、PASCAL Visual Object Classes 2012数据集"
    },
    {
        "order": 241,
        "title": "One-Shot Neural Architecture Search via Self-Evaluated Template Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_One-Shot_Neural_Architecture_Search_via_Self-Evaluated_Template_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_One-Shot_Neural_Architecture_Search_via_Self-Evaluated_Template_Network_ICCV_2019_paper.html",
        "abstract": "Neural architecture search (NAS) aims to automate the search procedure of architecture instead of manual design. Even if recent NAS approaches finish the search within days, lengthy training is still required for a specific architecture candidate to get the parameters for its accurate evaluation. Recently one-shot NAS methods are proposed to largely squeeze the tedious training process by sharing parameters across candidates. In this way, the parameters for each candidate can be directly extracted from the shared parameters instead of training them from scratch. However, they have no sense of which candidate will perform better until evaluation so that the candidates to evaluate are randomly sampled and the top-1 candidate is considered the best. In this paper, we propose a Self-Evaluated Template Network (SETN) to improve the quality of the architecture candidates for evaluation so that it is more likely to cover competitive candidates. SETN consists of two components: (1) an evaluator, which learns to indicate the probability of each individual architecture being likely to have a lower validation loss. The candidates for evaluation can thus be selectively sampled according to this evaluator. (2) a template network, which shares parameters among all candidates to amortize the training cost of generated candidates. In experiments, the architecture found by SETN achieves the state-of-the-art performance on CIFAR and ImageNet benchmarks within comparable computation costs.",
        "中文标题": "通过自评估模板网络进行一次性神经架构搜索",
        "摘要翻译": "神经架构搜索（NAS）旨在自动化架构的搜索过程，而非手动设计。尽管最近的NAS方法在几天内完成搜索，但对于特定架构候选者来说，仍需要长时间的训练以获得其准确评估所需的参数。最近提出的一次性NAS方法通过跨候选者共享参数，大大压缩了繁琐的训练过程。这样，每个候选者的参数可以直接从共享参数中提取，而不是从头开始训练。然而，在评估之前，它们无法感知哪个候选者会表现得更好，因此评估的候选者是随机抽样的，而排名第一的候选者被认为是最好的。在本文中，我们提出了一种自评估模板网络（SETN），以提高评估架构候选者的质量，从而更有可能覆盖有竞争力的候选者。SETN由两个组件组成：（1）一个评估器，它学习指示每个个体架构可能具有较低验证损失的概率。因此，可以根据这个评估器选择性地抽样评估的候选者。（2）一个模板网络，它在所有候选者之间共享参数，以分摊生成候选者的训练成本。在实验中，SETN找到的架构在CIFAR和ImageNet基准测试中实现了最先进的性能，计算成本相当。",
        "领域": "神经架构搜索/自动化机器学习/深度学习优化",
        "问题": "如何高效自动化地搜索最优神经架构",
        "动机": "减少神经架构搜索过程中的计算成本和时间，提高搜索效率和质量",
        "方法": "提出自评估模板网络（SETN），包括评估器和模板网络两个组件，通过共享参数和选择性抽样提高评估候选者的质量",
        "关键词": [
            "神经架构搜索",
            "一次性学习",
            "自动化机器学习"
        ],
        "涉及的技术概念": "神经架构搜索（NAS）是一种自动化寻找最优神经网络架构的技术，旨在减少人工设计的需求。一次性NAS方法通过共享参数减少训练成本，而自评估模板网络（SETN）则进一步通过评估器选择性抽样和模板网络共享参数来提高搜索效率和质量。"
    },
    {
        "order": 242,
        "title": "FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.html",
        "abstract": "Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation.",
        "中文标题": "FreiHAND: 从单张RGB图像无标记捕捉手部姿态和形状的数据集",
        "摘要翻译": "从单张RGB图像估计3D手部姿态是一个高度模糊的问题，依赖于无偏的训练数据集。在本文中，我们分析了在现有数据集上训练时的跨数据集泛化能力。我们发现，方法在它们训练的数据集上表现良好，但不能泛化到其他数据集或野外场景。因此，我们引入了第一个大规模、多视角的手部数据集，该数据集附有3D手部姿态和形状注释。为了注释这个真实世界的数据集，我们提出了一种迭代的、半自动的`人在环中`方法，包括手部拟合优化，以推断每个样本的3D姿态和形状。我们展示了在我们的数据集上训练的方法在其他数据集上测试时始终表现良好。此外，该数据集使我们能够训练一个网络，从单张RGB图像预测完整的关节手部形状。评估集可以作为关节手部形状估计的基准。",
        "领域": "3D手部姿态估计/手部形状重建/跨数据集泛化",
        "问题": "从单张RGB图像估计3D手部姿态的模糊性问题",
        "动机": "现有方法在训练数据集上表现良好，但不能泛化到其他数据集或野外场景",
        "方法": "引入大规模、多视角的手部数据集，并提出迭代的、半自动的`人在环中`方法进行注释",
        "关键词": [
            "3D手部姿态估计",
            "手部形状重建",
            "跨数据集泛化"
        ],
        "涉及的技术概念": "3D手部姿态估计涉及从单张RGB图像中推断手部的三维位置和方向。手部形状重建是指从图像中恢复手部的三维形状。跨数据集泛化能力指的是一个模型在未见过的数据集上的表现能力。`人在环中`方法是一种结合人类专家知识和自动化技术的注释方法，以提高数据标注的准确性和效率。"
    },
    {
        "order": 243,
        "title": "SPGNet: Semantic Prediction Guidance for Scene Parsing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_SPGNet_Semantic_Prediction_Guidance_for_Scene_Parsing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_SPGNet_Semantic_Prediction_Guidance_for_Scene_Parsing_ICCV_2019_paper.html",
        "abstract": "Multi-scale context module and single-stage encoder-decoder structure are commonly employed for semantic segmentation. The multi-scale context module refers to the operations to aggregate feature responses from a large spatial extent, while the single-stage encoder-decoder structure encodes the high-level semantic information in the encoder path and recovers the boundary information in the decoder path. In contrast, multi-stage encoder-decoder networks have been widely used in human pose estimation and show superior performance than their single-stage counterpart. However, few efforts have been attempted to bring this effective design to semantic segmentation. In this work, we propose a Semantic Prediction Guidance (SPG) module which learns to re-weight the local features through the guidance from pixel-wise semantic prediction. We find that by carefully re-weighting features across stages, a two-stage encoder-decoder network coupled with our proposed SPG module can significantly outperform its one-stage counterpart with similar parameters and computations. Finally, we report experimental results on the semantic segmentation benchmark Cityscapes, in which our SPGNet attains 81.1% on the test set using only 'fine' annotations.",
        "中文标题": "SPGNet: 场景解析的语义预测指导",
        "摘要翻译": "多尺度上下文模块和单阶段编码器-解码器结构通常用于语义分割。多尺度上下文模块指的是从大空间范围聚合特征响应的操作，而单阶段编码器-解码器结构在编码器路径中编码高级语义信息，并在解码器路径中恢复边界信息。相比之下，多阶段编码器-解码器网络已广泛应用于人体姿态估计，并显示出比单阶段网络更优越的性能。然而，很少有人尝试将这种有效设计引入语义分割。在这项工作中，我们提出了一个语义预测指导（SPG）模块，它通过学习从像素级语义预测中重新加权局部特征。我们发现，通过仔细地在各阶段重新加权特征，结合我们提出的SPG模块的两阶段编码器-解码器网络可以显著优于具有相似参数和计算量的单阶段网络。最后，我们在语义分割基准Cityscapes上报告了实验结果，其中我们的SPGNet仅使用'精细'注释就在测试集上达到了81.1%的准确率。",
        "领域": "语义分割/场景解析/特征重加权",
        "问题": "如何提高语义分割的准确性和效率",
        "动机": "探索多阶段编码器-解码器网络在语义分割中的应用，以超越单阶段网络的性能",
        "方法": "提出语义预测指导（SPG）模块，通过像素级语义预测重新加权局部特征，结合两阶段编码器-解码器网络",
        "关键词": [
            "语义分割",
            "场景解析",
            "特征重加权",
            "多阶段网络",
            "SPG模块"
        ],
        "涉及的技术概念": "多尺度上下文模块用于从大空间范围聚合特征响应；单阶段编码器-解码器结构用于编码高级语义信息和恢复边界信息；多阶段编码器-解码器网络在人体姿态估计中显示出优越性能；语义预测指导（SPG）模块通过学习从像素级语义预测中重新加权局部特征，以提高语义分割的准确性和效率。"
    },
    {
        "order": 244,
        "title": "Batch DropBlock Network for Person Re-Identification and Beyond",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dai_Batch_DropBlock_Network_for_Person_Re-Identification_and_Beyond_ICCV_2019_paper.html",
        "abstract": "Since the person re-identification task often suffers from the problem of pose changes and occlusions, some attentive local features are often suppressed when training CNNs. In this paper, we propose the Batch DropBlock (BDB) Network which is a two branch network composed of a conventional ResNet-50 as the global branch and a feature dropping branch.The global branch encodes the global salient representations.Meanwhile, the feature dropping branch consists of an attentive feature learning module called Batch DropBlock, which randomly drops the same region of all input feature maps in a batch to reinforce the attentive feature learning of local regions.The network then concatenates features from both branches and provides a more comprehensive and spatially distributed feature representation. Albeit simple, our method achieves state-of-the-art on person re-identification and it is also applicable to general metric learning tasks. For instance, we achieve 76.4% Rank-1 accuracy on the CUHK03-Detect dataset and 83.0% Recall-1 score on the Stanford Online Products dataset, outperforming the existed works by a large margin (more than 6%).",
        "中文标题": "批量DropBlock网络用于行人重识别及其他",
        "摘要翻译": "由于行人重识别任务经常受到姿态变化和遮挡问题的影响，在训练卷积神经网络时，一些注意力的局部特征往往被抑制。本文提出了批量DropBlock（BDB）网络，这是一个由传统ResNet-50作为全局分支和特征丢弃分支组成的双分支网络。全局分支编码全局显著表示。同时，特征丢弃分支由一个称为Batch DropBlock的注意力特征学习模块组成，该模块随机丢弃一批输入特征图中的相同区域，以加强局部区域的注意力特征学习。然后，网络将两个分支的特征连接起来，提供更全面和空间分布的特征表示。尽管方法简单，但我们的方法在行人重识别上达到了最先进的水平，并且也适用于一般的度量学习任务。例如，我们在CUHK03-Detect数据集上实现了76.4%的Rank-1准确率，在Stanford Online Products数据集上实现了83.0%的Recall-1分数，远超现有工作（超过6%）。",
        "领域": "行人重识别/度量学习/特征学习",
        "问题": "解决行人重识别任务中因姿态变化和遮挡导致的局部特征被抑制的问题",
        "动机": "提高行人重识别任务中局部特征的注意力学习，以应对姿态变化和遮挡带来的挑战",
        "方法": "提出了一种双分支网络结构，包括一个全局分支和一个特征丢弃分支，后者通过随机丢弃特征图中的相同区域来加强局部区域的注意力特征学习",
        "关键词": [
            "行人重识别",
            "度量学习",
            "特征学习",
            "注意力机制",
            "特征丢弃"
        ],
        "涉及的技术概念": "Batch DropBlock是一种注意力特征学习模块，通过随机丢弃一批输入特征图中的相同区域来加强局部区域的注意力特征学习。ResNet-50是一种深度残差网络，用于编码全局显著表示。"
    },
    {
        "order": 245,
        "title": "Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saini_Markerless_Outdoor_Human_Motion_Capture_Using_Multiple_Autonomous_Micro_Aerial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saini_Markerless_Outdoor_Human_Motion_Capture_Using_Multiple_Autonomous_Micro_Aerial_ICCV_2019_paper.html",
        "abstract": "Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles.",
        "中文标题": "使用多个自主微型飞行器进行无标记户外人体运动捕捉",
        "摘要翻译": "在自然场景中捕捉人体运动意味着将运动捕捉从实验室搬到野外。典型的方法依赖于固定的、校准的相机和身体上的反射标记，这大大限制了可以捕捉的运动。为了使运动捕捉真正不受限制，我们描述了第一个完全自主的户外捕捉系统，基于飞行器。我们使用多个微型飞行器（MAVs），每个都配备了一个单目RGB相机、一个IMU和一个GPS接收模块。这些设备检测人，优化它们的位置，并大致定位自己。然后，我们开发了一种适用于这种具有挑战性场景的无标记运动捕捉方法，该场景包括从上方观察的远距离主体，以及大致校准和移动的相机。我们将多个最先进的2D关节检测器与3D人体模型和强大的人体姿态先验结合起来。我们共同优化3D身体姿态和相机姿态，以稳健地适应2D测量。据我们所知，这是首次成功展示从自主飞行器进行的户外全身无标记运动捕捉。",
        "领域": "运动捕捉/无人机技术/3D建模",
        "问题": "在户外自然环境中进行无标记的人体运动捕捉",
        "动机": "将运动捕捉技术从实验室环境扩展到户外自然场景，以捕捉更真实、更不受限制的人体运动",
        "方法": "使用配备单目RGB相机、IMU和GPS接收模块的多个微型飞行器（MAVs）进行人体检测和位置优化，结合最先进的2D关节检测器、3D人体模型和人体姿态先验，共同优化3D身体姿态和相机姿态",
        "关键词": [
            "运动捕捉",
            "无人机",
            "3D建模"
        ],
        "涉及的技术概念": "微型飞行器（MAVs）是指小型无人机，配备有单目RGB相机用于捕捉图像，IMU（惯性测量单元）用于测量和报告飞行器的速度、方向和重力，GPS接收模块用于定位。2D关节检测器是指能够从2D图像中检测出人体关节位置的算法或技术。3D人体模型是指用于表示人体在三维空间中姿态和形状的模型。人体姿态先验是指基于人体运动学和生物力学的知识，用于预测或约束人体姿态的模型或算法。"
    },
    {
        "order": 246,
        "title": "Omni-Scale Feature Learning for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Omni-Scale_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We callse features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional feature streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses both pointwise and depthwise convolutions. By stacking such blocks layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, our OSNet achieves state-of-the-art performance on six person-ReID datasets. Code and models are available at: https://github.com/KaiyangZhou/deep-person-reid.",
        "中文标题": "全尺度特征学习用于行人重识别",
        "摘要翻译": "作为一个实例级别的识别问题，行人重识别（ReID）依赖于具有区分性的特征，这些特征不仅捕捉不同的空间尺度，而且封装了多个尺度的任意组合。我们将这些同质和异质尺度的特征称为全尺度特征。在本文中，设计了一种新颖的深度ReID CNN，称为全尺度网络（OSNet），用于全尺度特征学习。这是通过设计一个由多个卷积特征流组成的残差块实现的，每个流检测特定尺度的特征。重要的是，引入了一种新颖的统一聚合门，以动态地融合多尺度特征，并具有输入依赖的通道权重。为了有效地学习空间-通道相关性并避免过拟合，构建块使用了点卷积和深度卷积。通过逐层堆叠这样的块，我们的OSNet非常轻量级，并且可以在现有的ReID基准上从头开始训练。尽管模型尺寸小，我们的OSNet在六个人-ReID数据集上实现了最先进的性能。代码和模型可在https://github.com/KaiyangZhou/deep-person-reid获取。",
        "领域": "行人重识别/特征学习/卷积神经网络",
        "问题": "如何有效地学习并融合多尺度特征以提高行人重识别的性能",
        "动机": "行人重识别依赖于捕捉不同空间尺度的特征，需要一种能够动态融合多尺度特征的方法来提高识别性能",
        "方法": "设计了一种全尺度网络（OSNet），通过引入统一聚合门动态融合多尺度特征，并使用点卷积和深度卷积来学习空间-通道相关性",
        "关键词": [
            "行人重识别",
            "全尺度特征",
            "卷积神经网络",
            "统一聚合门",
            "点卷积",
            "深度卷积"
        ],
        "涉及的技术概念": "全尺度特征指的是能够捕捉不同空间尺度的特征，包括同质和异质尺度的特征。统一聚合门是一种机制，用于动态地融合多尺度特征，根据输入的不同赋予不同的通道权重。点卷积和深度卷积是两种卷积操作，用于有效地学习空间-通道相关性，同时避免过拟合。"
    },
    {
        "order": 247,
        "title": "Gated-SCNN: Gated Shape CNNs for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.",
        "中文标题": "Gated-SCNN: 用于语义分割的门控形状卷积神经网络",
        "摘要翻译": "当前最先进的图像分割方法形成了一种密集的图像表示，其中颜色、形状和纹理信息都在深度卷积神经网络（CNN）中一起处理。然而，这可能不是理想的，因为它们包含了与识别相关的非常不同类型的信息。在这里，我们提出了一种新的双流CNN架构用于语义分割，该架构明确地将形状信息作为单独的处理分支，即形状流，与经典流并行处理信息。这种架构的关键是一种新型的门，它连接了两个流的中间层。具体来说，我们使用经典流中的高级激活来门控形状流中的低级激活，有效地去除噪声并帮助形状流仅专注于处理相关的边界信息。这使我们能够使用一个非常浅的架构来处理形状流，该架构在图像级分辨率上操作。我们的实验表明，这导致了一个非常有效的架构，它在对象边界周围产生更清晰的预测，并显著提高了对更薄和更小对象的性能。我们的方法在Cityscapes基准测试中实现了最先进的性能，无论是在掩码（mIoU）还是边界（F-score）质量方面，都比强大的基线提高了2%和4%。",
        "领域": "语义分割/图像分割/卷积神经网络",
        "问题": "如何在语义分割中更有效地处理形状信息",
        "动机": "现有的图像分割方法将颜色、形状和纹理信息一起处理，这可能不是最优的，因为这些信息类型对识别的贡献不同。",
        "方法": "提出了一种新的双流CNN架构，其中形状信息作为单独的处理分支与经典流并行处理，通过新型的门连接两个流的中间层，使用经典流中的高级激活来门控形状流中的低级激活，以去除噪声并专注于处理边界信息。",
        "关键词": [
            "语义分割",
            "双流CNN",
            "形状流",
            "门控机制",
            "图像级分辨率"
        ],
        "涉及的技术概念": {
            "双流CNN架构": "一种包含两个并行处理流的卷积神经网络架构，一个流处理经典信息，另一个流专门处理形状信息。",
            "门控机制": "一种控制信息流动的机制，在这里用于连接两个流的中间层，通过高级激活控制低级激活，以去除噪声并专注于处理边界信息。",
            "图像级分辨率": "指处理图像时保持原始图像的分辨率，不进行下采样，以保留更多的细节信息。"
        }
    },
    {
        "order": 248,
        "title": "Toyota Smarthome: Real-World Activities of Daily Living",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Das_Toyota_Smarthome_Real-World_Activities_of_Daily_Living_ICCV_2019_paper.html",
        "abstract": "The performance of deep neural networks is strongly influenced by the quantity and quality of annotated data. Most of the large activity recognition datasets consist of data sourced from the web, which does not reflect challenges that exist in activities of daily living. In this paper, we introduce a large real-world video dataset for activities of daily living: Toyota Smarthome. The dataset consists of 16K RGB+D clips of 31 activity classes, performed by seniors in a smarthome. Unlike previous datasets, videos were fully unscripted. As a result, the dataset poses several challenges: high intra-class variation, high class imbalance, simple and composite activities, and activities with similar motion and variable duration. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome from other datasets for activity recognition. As recent activity recognition approaches fail to address the challenges posed by Toyota Smarthome, we present a novel activity recognition method with attention mechanism. We propose a pose driven spatio-temporal attention mechanism through 3D ConvNets. We show that our novel method outperforms state-of-the-art methods on benchmark datasets, as well as on the Toyota Smarthome dataset. We release the dataset for research use.",
        "中文标题": "丰田智能家居：现实生活中的日常活动",
        "摘要翻译": "深度神经网络的性能在很大程度上受到注释数据的数量和质量的影响。大多数大型活动识别数据集由来自网络的数据组成，这些数据并不反映日常生活中存在的挑战。在本文中，我们介绍了一个大型现实世界视频数据集，用于日常活动：丰田智能家居。该数据集包括16K RGB+D剪辑，涵盖31个活动类别，由智能家居中的老年人执行。与之前的数据集不同，视频是完全无脚本的。因此，该数据集提出了几个挑战：高类内变异、高类别不平衡、简单和复合活动，以及具有相似运动和可变持续时间的活动。活动被注释为粗略和细粒度的标签。这些特点使丰田智能家居区别于其他活动识别数据集。由于最近的活动识别方法未能解决丰田智能家居提出的挑战，我们提出了一种带有注意力机制的新型活动识别方法。我们通过3D ConvNets提出了一种姿态驱动的时空注意力机制。我们展示了我们的新方法在基准数据集以及丰田智能家居数据集上优于最先进的方法。我们发布了该数据集供研究使用。",
        "领域": "活动识别/视频分析/智能家居",
        "问题": "解决现实世界中日常活动识别的高类内变异、高类别不平衡、简单和复合活动，以及具有相似运动和可变持续时间的活动的挑战",
        "动机": "大多数大型活动识别数据集由来自网络的数据组成，这些数据并不反映日常生活中存在的挑战",
        "方法": "提出了一种带有注意力机制的新型活动识别方法，通过3D ConvNets提出了一种姿态驱动的时空注意力机制",
        "关键词": [
            "活动识别",
            "视频分析",
            "智能家居",
            "注意力机制",
            "3D ConvNets"
        ],
        "涉及的技术概念": "深度神经网络、RGB+D剪辑、活动类别、类内变异、类别不平衡、复合活动、时空注意力机制、3D ConvNets"
    },
    {
        "order": 249,
        "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Be_Your_Own_Teacher_Improve_the_Performance_of_Convolutional_Neural_ICCV_2019_paper.html",
        "abstract": "Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github.",
        "中文标题": "成为你自己的老师：通过自我蒸馏提高卷积神经网络的性能",
        "摘要翻译": "卷积神经网络已被广泛应用于各种应用场景。为了将应用边界扩展到一些对精度要求极高的领域，研究人员一直在研究通过更深或更宽的网络结构来提高精度的方法，这带来了计算和存储成本的指数级增长，延迟了响应时间。在本文中，我们提出了一个名为自我蒸馏的通用训练框架，通过缩小网络规模而不是扩大它，显著提高了卷积神经网络的性能（精度）。与传统的知识蒸馏——一种网络间的知识转换方法，它迫使学生神经网络近似预训练教师神经网络的softmax层输出——不同，提出的自我蒸馏框架在网络内部蒸馏知识。网络首先被分成几个部分。然后，网络深层部分的知识被压缩到浅层部分。实验进一步证明了所提出的自我蒸馏框架的泛化能力：平均精度提高了2.65%，从ResNeXt的最小0.61%到VGG19的最大4.07%不等。此外，它还可以在资源有限的边缘设备上提供深度可扩展推理的灵活性。我们的代码已在github上发布。",
        "领域": "神经网络优化/模型压缩/边缘计算",
        "问题": "提高卷积神经网络的性能而不显著增加计算和存储成本",
        "动机": "为了在精度要求极高的领域扩展卷积神经网络的应用，同时控制计算和存储成本的增长",
        "方法": "提出自我蒸馏框架，通过将网络深层部分的知识压缩到浅层部分来提高性能，而不是通过增加网络规模",
        "关键词": [
            "自我蒸馏",
            "卷积神经网络",
            "模型压缩",
            "边缘计算"
        ],
        "涉及的技术概念": "自我蒸馏是一种网络内部知识蒸馏的方法，不同于传统的网络间知识蒸馏。它通过将网络深层部分的知识压缩到浅层部分来提高性能，而不是通过增加网络规模。这种方法不仅提高了模型的精度，还减少了计算和存储成本，使得在资源有限的边缘设备上部署成为可能。"
    },
    {
        "order": 250,
        "title": "DensePoint: Learning Densely Contextual Representation for Efficient Point Cloud Processing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_DensePoint_Learning_Densely_Contextual_Representation_for_Efficient_Point_Cloud_Processing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_DensePoint_Learning_Densely_Contextual_Representation_for_Efficient_Point_Cloud_Processing_ICCV_2019_paper.html",
        "abstract": "Point cloud processing is very challenging, as the diverse shapes formed by irregular points are often indistinguishable. A thorough grasp of the elusive shape requires sufficiently contextual semantic information, yet few works devote to this. Here we propose DensePoint, a general architecture to learn densely contextual representation for point cloud processing. Technically, it extends regular grid CNN to irregular point configuration by generalizing a convolution operator, which holds the permutation invariance of points, and achieves efficient inductive learning of local patterns. Architecturally, it finds inspiration from dense connection mode, to repeatedly aggregate multi-level and multi-scale semantics in a deep hierarchy. As a result, densely contextual information along with rich semantics, can be acquired by DensePoint in an organic manner, making it highly effective. Extensive experiments on challenging benchmarks across four tasks, as well as thorough model analysis, verify DensePoint achieves the state of the arts.",
        "中文标题": "DensePoint: 学习密集上下文表示以高效处理点云",
        "摘要翻译": "点云处理非常具有挑战性，因为由不规则点形成的多样形状往往难以区分。彻底掌握这些难以捉摸的形状需要充分的上下文语义信息，但很少有工作致力于此。在这里，我们提出了DensePoint，一种用于点云处理的通用架构，以学习密集的上下文表示。技术上，它通过推广卷积算子将规则网格CNN扩展到不规则点配置，保持点的排列不变性，并实现局部模式的高效归纳学习。架构上，它从密集连接模式中获得灵感，在深层层次结构中重复聚合多级和多尺度的语义。因此，DensePoint能够以有机的方式获取密集的上下文信息以及丰富的语义，使其非常有效。在四个任务的挑战性基准上进行的大量实验，以及彻底的模型分析，验证了DensePoint达到了最先进的水平。",
        "领域": "点云处理/卷积神经网络/语义理解",
        "问题": "点云处理中不规则点形成的多样形状难以区分，缺乏充分的上下文语义信息",
        "动机": "为了彻底掌握点云中难以捉摸的形状，需要开发一种能够提供充分上下文语义信息的方法",
        "方法": "提出DensePoint架构，通过推广卷积算子将规则网格CNN扩展到不规则点配置，保持点的排列不变性，并实现局部模式的高效归纳学习；采用密集连接模式，在深层层次结构中重复聚合多级和多尺度的语义",
        "关键词": [
            "点云处理",
            "卷积神经网络",
            "语义理解",
            "密集连接",
            "上下文表示"
        ],
        "涉及的技术概念": "DensePoint是一种用于点云处理的架构，它通过推广卷积算子来扩展规则网格CNN到不规则点配置，保持点的排列不变性，并实现局部模式的高效归纳学习。此外，DensePoint采用密集连接模式，在深层层次结构中重复聚合多级和多尺度的语义，以获取密集的上下文信息和丰富的语义。"
    },
    {
        "order": 251,
        "title": "Relation Parsing Neural Network for Human-Object Interaction Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Relation_Parsing_Neural_Network_for_Human-Object_Interaction_Detection_ICCV_2019_paper.html",
        "abstract": "Human-Object Interaction Detection devotes to infer a triplet < human, verb, object > between human and objects. In this paper, we propose a novel model, i.e., Relation Parsing Neural Network (RPNN), to detect human-object interactions. Specifically, the network is represented by two graphs, i.e., Object-Bodypart Graph and Human-Bodypart Graph. Here, the Object-Bodypart Graph dynamically captures the relationship between body parts and the surrounding objects. The Human-Bodypart Graph infers the relationship between human and body parts, and assembles body part contexts to predict actions. These two graphs are associated through an action passing mechanism. The proposed RPNN model is able to implicitly parse a pairwise relation in two graphs without supervised labels. Experiments conducted on V-COCO and HICO-DET datasets confirm the effectiveness of the proposed RPNN network which significantly outperforms state-of-the-art methods.",
        "中文标题": "关系解析神经网络用于人-物交互检测",
        "摘要翻译": "人-物交互检测致力于推断人与物体之间的三元组<人, 动词, 物体>。在本文中，我们提出了一种新颖的模型，即关系解析神经网络（RPNN），用于检测人-物交互。具体来说，该网络由两个图表示，即物体-身体部位图和人类-身体部位图。这里，物体-身体部位图动态捕捉身体部位与周围物体之间的关系。人类-身体部位图推断人类与身体部位之间的关系，并组装身体部位上下文以预测动作。这两个图通过动作传递机制相关联。提出的RPNN模型能够在没有监督标签的情况下隐式解析两个图中的成对关系。在V-COCO和HICO-DET数据集上进行的实验证实了所提出的RPNN网络的有效性，其显著优于最先进的方法。",
        "领域": "人-物交互检测/动作识别/关系解析",
        "问题": "如何有效地检测和解析人-物之间的交互关系",
        "动机": "为了更准确地理解和预测人与物体之间的交互行为，需要一种能够解析复杂关系的方法",
        "方法": "提出了一种关系解析神经网络（RPNN），通过物体-身体部位图和人类-身体部位图两个图结构，动态捕捉和推断人-物之间的关系，并通过动作传递机制关联这两个图",
        "关键词": [
            "人-物交互检测",
            "关系解析",
            "动作识别"
        ],
        "涉及的技术概念": {
            "关系解析神经网络（RPNN）": "一种用于检测人-物交互的神经网络模型，通过两个图结构解析人-物之间的关系",
            "物体-身体部位图": "动态捕捉身体部位与周围物体之间关系的图结构",
            "人类-身体部位图": "推断人类与身体部位之间关系，并组装身体部位上下文以预测动作的图结构",
            "动作传递机制": "关联物体-身体部位图和人类-身体部位图的机制，用于解析人-物之间的交互关系"
        }
    },
    {
        "order": 252,
        "title": "Diversity With Cooperation: Ensemble Methods for Few-Shot Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dvornik_Diversity_With_Cooperation_Ensemble_Methods_for_Few-Shot_Classification_ICCV_2019_paper.html",
        "abstract": "Few-shot classification consists of learning a predictive model that is able to effectively adapt to a new class, given only a few annotated samples. To solve this challenging problem, meta-learning has become a popular paradigm that advocates the ability to \"learn to adapt\". Recent works have shown, however, that simple learning strategies without meta-learning could be competitive. In this paper, we go a step further and show that by addressing the fundamental high-variance issue of few-shot learning classifiers, it is possible to significantly outperform current meta-learning techniques. Our approach consists of designing an ensemble of deep networks to leverage the variance of the classifiers, and introducing new strategies to encourage the networks to cooperate, while encouraging prediction diversity. Evaluation is conducted on the mini-ImageNet, tiered-ImageNet and CUB datasets, where we show that even a single network obtained by distillation yields state-of-the-art results.",
        "中文标题": "多样性与合作：用于少样本分类的集成方法",
        "摘要翻译": "少样本分类包括学习一个预测模型，该模型能够在仅给定少量注释样本的情况下有效地适应新类别。为了解决这一具有挑战性的问题，元学习已成为一种流行的范式，它提倡“学会适应”的能力。然而，最近的研究表明，没有元学习的简单学习策略也可能具有竞争力。在本文中，我们更进一步，展示了通过解决少样本学习分类器的高方差基本问题，可以显著超越当前的元学习技术。我们的方法包括设计一个深度网络集成，以利用分类器的方差，并引入新策略以鼓励网络合作，同时鼓励预测多样性。评估在mini-ImageNet、tiered-ImageNet和CUB数据集上进行，我们展示了即使是通过蒸馏获得的单个网络也能产生最先进的结果。",
        "领域": "少样本学习/集成学习/深度学习",
        "问题": "解决少样本学习分类器的高方差问题",
        "动机": "通过解决少样本学习分类器的高方差问题，显著超越当前的元学习技术",
        "方法": "设计深度网络集成，利用分类器的方差，并引入新策略以鼓励网络合作，同时鼓励预测多样性",
        "关键词": [
            "少样本学习",
            "集成学习",
            "深度学习",
            "元学习",
            "高方差问题"
        ],
        "涉及的技术概念": {
            "少样本分类": "学习一个预测模型，该模型能够在仅给定少量注释样本的情况下有效地适应新类别",
            "元学习": "一种流行的范式，提倡“学会适应”的能力",
            "集成学习": "设计一个深度网络集成，以利用分类器的方差",
            "高方差问题": "少样本学习分类器面临的一个基本问题，通过解决此问题可以显著超越当前的元学习技术",
            "蒸馏": "一种技术，通过它可以从集成网络中提取单个网络，该网络能产生最先进的结果"
        }
    },
    {
        "order": 253,
        "title": "AMP: Adaptive Masked Proxies for Few-Shot Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.html",
        "abstract": "Deep learning has thrived by training on large-scale datasets. However, in robotics applications sample efficiency is critical. We propose a novel adaptive masked proxies method that constructs the final segmentation layer weights from few labelled samples. It utilizes multi-resolution average pooling on base embeddings masked with the label to act as a positive proxy for the new class, while fusing it with the previously learned class signatures. Our method is evaluated on PASCAL-5^i dataset and outperforms the state-of-the-art in the few-shot semantic segmentation. Unlike previous methods, our approach does not require a second branch to estimate parameters or prototypes, which enables it to be used with 2-stream motion and appearance based segmentation networks. We further propose a novel setup for evaluating continual learning of object segmentation which we name incremental PASCAL (iPASCAL) where our method outperforms the baseline method. Our code is publicly available at https://github.com/MSiam/AdaptiveMaskedProxies.",
        "中文标题": "AMP: 自适应掩码代理用于少样本分割",
        "摘要翻译": "深度学习通过在大规模数据集上训练而蓬勃发展。然而，在机器人应用中，样本效率至关重要。我们提出了一种新颖的自适应掩码代理方法，该方法从少量标记样本中构建最终分割层的权重。它利用多分辨率平均池化对带有标签的基础嵌入进行掩码，作为新类的正代理，同时将其与先前学习的类签名融合。我们的方法在PASCAL-5^i数据集上进行了评估，并在少样本语义分割方面超越了现有技术。与之前的方法不同，我们的方法不需要第二个分支来估计参数或原型，这使得它可以与基于运动和外观的2流分割网络一起使用。我们进一步提出了一种新的设置，用于评估对象分割的持续学习，我们称之为增量PASCAL（iPASCAL），在该设置中，我们的方法优于基线方法。我们的代码公开在https://github.com/MSiam/AdaptiveMaskedProxies。",
        "领域": "少样本学习/语义分割/机器人视觉",
        "问题": "在机器人应用中，如何提高少样本语义分割的样本效率",
        "动机": "提高在机器人应用中少样本语义分割的效率和性能",
        "方法": "提出自适应掩码代理方法，通过多分辨率平均池化和融合先前学习的类签名来构建分割层权重",
        "关键词": [
            "少样本学习",
            "语义分割",
            "自适应掩码代理",
            "多分辨率平均池化",
            "机器人视觉"
        ],
        "涉及的技术概念": {
            "自适应掩码代理": "一种从少量标记样本中构建最终分割层权重的方法",
            "多分辨率平均池化": "一种在不同分辨率下对基础嵌入进行平均池化的技术",
            "2流运动": "一种基于运动和外观的分割网络",
            "增量PASCAL（iPASCAL）": "一种新的设置，用于评估对象分割的持续学习"
        }
    },
    {
        "order": 254,
        "title": "DistInit: Learning Video Representations Without a Single Labeled Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Girdhar_DistInit_Learning_Video_Representations_Without_a_Single_Labeled_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Girdhar_DistInit_Learning_Video_Representations_Without_a_Single_Labeled_Video_ICCV_2019_paper.html",
        "abstract": "Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models has not been able to keep up with the ever increasing depth and sophistication of these networks. In this work we propose an alternative approach to learning video representations that requires no semantically labeled videos, and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as \"teachers\" to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.",
        "中文标题": "DistInit: 无需单个标记视频学习视频表示",
        "摘要翻译": "视频识别模型在过去几年中取得了显著进展，从基于手工特征训练的浅层分类器发展到深度时空网络。然而，训练这些模型所需的标记视频数据未能跟上这些网络日益增加的深度和复杂性。在这项工作中，我们提出了一种学习视频表示的替代方法，该方法不需要语义标记的视频，而是利用多年来收集和标记大型干净静态图像数据集的努力。我们通过在蒸馏框架中使用在图像数据集上预训练的最先进模型作为“教师”来训练视频模型来实现这一点。我们证明了我们的方法尽管仅使用静态图像网络的监督进行训练，但仍能学习真正的时空特征。此外，它能够跨不同的输入模态学习良好的表示，使用完全未整理的原始视频数据源和不同的2D教师模型。我们的方法获得了强大的转移性能，比使用基于图像的模型引导视频架构的标准技术高出16%。我们相信，我们的方法为从未标记的视频数据中学习时空表示开辟了新的途径。",
        "领域": "视频识别/时空特征学习/模型蒸馏",
        "问题": "训练深度时空网络所需的标记视频数据不足",
        "动机": "利用现有的静态图像数据集来训练视频模型，以解决标记视频数据不足的问题",
        "方法": "使用在图像数据集上预训练的最先进模型作为“教师”，在蒸馏框架中训练视频模型",
        "关键词": [
            "视频表示",
            "模型蒸馏",
            "时空特征"
        ],
        "涉及的技术概念": "蒸馏框架是一种模型训练技术，其中一个大模型（教师模型）的知识被转移到一个较小的模型（学生模型）中。这种方法通常用于压缩模型或在不直接访问原始数据的情况下训练模型。时空特征指的是同时包含空间（图像）和时间（视频帧序列）信息的数据特征。"
    },
    {
        "order": 255,
        "title": "Enhancing 2D Representation via Adjacent Views for 3D Shape Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Enhancing_2D_Representation_via_Adjacent_Views_for_3D_Shape_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Enhancing_2D_Representation_via_Adjacent_Views_for_3D_Shape_Retrieval_ICCV_2019_paper.html",
        "abstract": "Multi-view shape descriptors obtained from various 2D images are commonly adopted in 3D shape retrieval. One major challenge is that significant shape information are discarded during 2D view rendering through projection. In this paper, we propose a convolutional neural network based method, CenterNet, to enhance each individual 2D view using its neighboring ones. By exploiting cross-view correlations, CenterNet learns how adjacent views can be maximally incorporated for an enhanced 2D representation to effectively describe shapes. We observe that a very small amount of, e.g., six, enhanced 2D views, are already sufficient for a panoramic shape description. Thus, by simply aggregating features from six enhanced 2D views, we arrive at a highly compact yet discriminative shape descriptor. The proposed shape descriptor significantly outperforms state-of-the-art 3D shape retrieval methods on the ModelNet and ShapeNetCore55 benchmarks, and also exhibits robustness against object occlusion.",
        "中文标题": "通过相邻视角增强2D表示以进行3D形状检索",
        "摘要翻译": "在3D形状检索中，通常采用从各种2D图像中获得的多视角形状描述符。一个主要挑战是在通过投影进行2D视角渲染过程中，大量的形状信息被丢弃。在本文中，我们提出了一种基于卷积神经网络的方法，CenterNet，通过使用相邻视角来增强每个单独的2D视角。通过利用跨视角相关性，CenterNet学习如何最大限度地结合相邻视角以获得增强的2D表示，从而有效地描述形状。我们观察到，非常少量的，例如六个，增强的2D视角已经足以进行全景形状描述。因此，通过简单地聚合来自六个增强2D视角的特征，我们得到了一个高度紧凑但具有区分性的形状描述符。所提出的形状描述符在ModelNet和ShapeNetCore55基准测试中显著优于最先进的3D形状检索方法，并且还表现出对物体遮挡的鲁棒性。",
        "领域": "3D形状检索/卷积神经网络/多视角学习",
        "问题": "在3D形状检索中，2D视角渲染过程中大量形状信息被丢弃的问题",
        "动机": "提高3D形状检索的准确性和效率，通过增强2D视角表示来更有效地描述形状",
        "方法": "提出了一种基于卷积神经网络的方法，CenterNet，通过利用相邻视角的跨视角相关性来增强2D表示",
        "关键词": [
            "3D形状检索",
            "卷积神经网络",
            "多视角学习",
            "形状描述符",
            "物体遮挡"
        ],
        "涉及的技术概念": {
            "多视角形状描述符": "从各种2D图像中获得的用于描述3D形状的描述符",
            "卷积神经网络": "一种深度学习模型，特别适用于处理图像数据",
            "跨视角相关性": "不同视角之间的关联性，用于增强单一视角的表示",
            "全景形状描述": "通过少量增强的2D视角来描述整个3D形状",
            "物体遮挡": "在3D形状检索中，物体被部分遮挡的情况"
        }
    },
    {
        "order": 256,
        "title": "Universal Semi-Supervised Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kalluri_Universal_Semi-Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kalluri_Universal_Semi-Supervised_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "In recent years, the need for semantic segmentation has arisen across several different applications and environments. However, the expense and redundancy of annotation often limits the quantity of labels available for training in any domain, while deployment is easier if a single model works well across domains. In this paper, we pose the novel problem of universal semi-supervised semantic segmentation and propose a solution framework, to meet the dual needs of lower annotation and deployment costs. In contrast to counterpoints such as fine tuning, joint training or unsupervised domain adaptation, universal semi-supervised segmentation ensures that across all domains: (i) a single model is deployed, (ii) unlabeled data is used, (iii) performance is improved, (iv) only a few labels are needed and (v) label spaces may differ. To address this, we minimize supervised as well as within and cross-domain unsupervised losses, introducing a novel feature alignment objective based on pixel-aware entropy regularization for the latter. We demonstrate quantitative advantages over other approaches on several combinations of segmentation datasets across different geographies (Germany, England, India) and environments (outdoors, indoors), as well as qualitative insights on the aligned representations.",
        "中文标题": "通用半监督语义分割",
        "摘要翻译": "近年来，语义分割的需求在多个不同的应用和环境中出现。然而，注释的成本和冗余常常限制了任何领域中可用于训练的标签数量，而如果单一模型能在多个领域中表现良好，部署将更加容易。在本文中，我们提出了通用半监督语义分割的新问题，并提出了一个解决方案框架，以满足降低注释和部署成本的双重需求。与微调、联合训练或无监督领域适应等对比点不同，通用半监督分割确保在所有领域中：(i) 部署单一模型，(ii) 使用未标记数据，(iii) 性能得到提升，(iv) 仅需少量标签，以及(v) 标签空间可能不同。为此，我们最小化了监督损失以及领域内和跨领域的无监督损失，为后者引入了一种基于像素感知熵正则化的新特征对齐目标。我们在不同地理（德国、英国、印度）和环境（户外、室内）的多个分割数据集组合上展示了相对于其他方法的定量优势，以及对对齐表示的定性见解。",
        "领域": "语义分割/半监督学习/领域适应",
        "问题": "降低语义分割的注释和部署成本",
        "动机": "解决注释成本高和模型跨领域部署难的问题",
        "方法": "提出通用半监督语义分割框架，通过最小化监督损失和领域内及跨领域的无监督损失，引入基于像素感知熵正则化的特征对齐目标",
        "关键词": [
            "语义分割",
            "半监督学习",
            "领域适应"
        ],
        "涉及的技术概念": "语义分割是一种将图像分割成多个区域或对象的技术，每个区域或对象被赋予一个语义标签。半监督学习是一种利用少量标记数据和大量未标记数据进行学习的方法。领域适应是指将在一个领域（源领域）上训练的模型适应到另一个不同但相关的领域（目标领域）上的过程。像素感知熵正则化是一种用于特征对齐的技术，旨在通过最小化像素级别的熵来改善模型在不同领域上的表现。"
    },
    {
        "order": 257,
        "title": "Zero-Shot Anticipation for Instructional Activities",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sener_Zero-Shot_Anticipation_for_Instructional_Activities_ICCV_2019_paper.html",
        "abstract": "How can we teach a robot to predict what will happen next for an activity it has never seen before? We address the problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.",
        "中文标题": "零样本预测教学活动的未来动作",
        "摘要翻译": "我们如何教会机器人预测它从未见过的活动的下一步会发生什么？我们通过提出一个层次模型来解决零样本预测的问题，该模型从大规模文本语料库中泛化教学知识，并将这些知识转移到视觉领域。给定教学视频的一部分，我们的模型预测未来多个步骤的连贯且合理的动作，所有这些都以丰富的自然语言形式呈现。为了展示我们模型的预测能力，我们引入了Tasty Videos数据集，这是一个包含2511个食谱的集合，用于零样本学习、识别和预测。",
        "领域": "机器人学习/自然语言处理/视频理解",
        "问题": "如何让机器人在从未见过的活动中预测未来动作",
        "动机": "提高机器人在未知活动中的预测能力，以便更好地辅助或执行任务",
        "方法": "提出一个层次模型，从大规模文本语料库中泛化教学知识并转移到视觉领域，预测未来动作",
        "关键词": [
            "零样本学习",
            "视频理解",
            "自然语言处理"
        ],
        "涉及的技术概念": "零样本预测指的是在没有直接观察到的情况下预测未来事件或动作的能力。层次模型是一种能够处理不同层次抽象信息的模型结构，这里用于从文本到视觉的知识转移。Tasty Videos数据集是一个专门用于零样本学习、识别和预测的食谱视频集合。"
    },
    {
        "order": 258,
        "title": "Adversarial Fine-Grained Composition Learning for Unseen Attribute-Object Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wei_Adversarial_Fine-Grained_Composition_Learning_for_Unseen_Attribute-Object_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wei_Adversarial_Fine-Grained_Composition_Learning_for_Unseen_Attribute-Object_Recognition_ICCV_2019_paper.html",
        "abstract": "Recognizing unseen attribute-object pairs never appearing in the training data is a challenging task, since an object often refers to a specific entity while an attribute is an abstract semantic description. Besides, attributes are highly correlated to objects, i.e., an attribute tends to describe different visual features of various objects. Existing methods mainly employ two classifiers to recognize attribute and object separately, or simply simulate the composition of attribute and object, which ignore the inherent discrepancy and correlation between them. In this paper, we propose a novel adversarial fine-grained composition learning model for unseen attribute-object pair recognition. Considering their inherent discrepancy, we leverage multi-scale feature integration to capture discriminative fine-grained features from a given image. Besides, we devise a quintuplet loss to depict more accurate correlations between attributes and objects. Adversarial learning is employed to model the discrepancy and correlations among attributes and objects. Extensive experiments on two challenging benchmarks indicate that our method consistently outperforms state-of-the-art competitors by a large margin.",
        "中文标题": "对抗性细粒度组合学习用于未见属性-物体识别",
        "摘要翻译": "识别训练数据中从未出现过的属性-物体对是一项具有挑战性的任务，因为物体通常指的是特定的实体，而属性是抽象的语义描述。此外，属性与物体高度相关，即一个属性倾向于描述不同物体的不同视觉特征。现有方法主要采用两个分类器分别识别属性和物体，或者简单地模拟属性和物体的组合，这忽略了它们之间的固有差异和相关性。在本文中，我们提出了一种新颖的对抗性细粒度组合学习模型，用于未见属性-物体对的识别。考虑到它们的固有差异，我们利用多尺度特征集成从给定图像中捕捉区分性细粒度特征。此外，我们设计了一个五元组损失来描绘属性和物体之间更准确的相关性。采用对抗性学习来建模属性和物体之间的差异和相关性。在两个具有挑战性的基准上进行的大量实验表明，我们的方法始终以较大优势优于最先进的竞争对手。",
        "领域": "细粒度图像识别/属性学习/对抗性学习",
        "问题": "识别训练数据中从未出现过的属性-物体对",
        "动机": "现有方法忽略了属性和物体之间的固有差异和相关性",
        "方法": "提出了一种对抗性细粒度组合学习模型，利用多尺度特征集成捕捉细粒度特征，设计五元组损失描绘属性和物体之间的相关性，采用对抗性学习建模差异和相关性",
        "关键词": [
            "细粒度图像识别",
            "属性学习",
            "对抗性学习"
        ],
        "涉及的技术概念": {
            "多尺度特征集成": "从图像中捕捉不同尺度的特征，以增强模型的识别能力",
            "五元组损失": "一种损失函数设计，用于更准确地描绘属性和物体之间的相关性",
            "对抗性学习": "一种学习方法，通过模拟对抗过程来增强模型的泛化能力和鲁棒性"
        }
    },
    {
        "order": 259,
        "title": "Accelerate Learning of Deep Hashing With Gradient Attention",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Accelerate_Learning_of_Deep_Hashing_With_Gradient_Attention_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Accelerate_Learning_of_Deep_Hashing_With_Gradient_Attention_ICCV_2019_paper.html",
        "abstract": "Recent years have witnessed the success of learning to hash in fast large-scale image retrieval. As deep learning has shown its superior performance on many computer vision applications, recent designs of learning-based hashing models have been moving from shallow ones to deep architectures. However, based on our analysis, we find that gradient descent based algorithms used in deep hashing models would potentially cause hash codes of a pair of training instances to be updated towards the directions of each other simultaneously during optimization. In the worst case, the paired hash codes switch their directions after update, and consequently, their corresponding distance in the Hamming space remain unchanged. This makes the overall learning process highly inefficient. To address this issue, we propose a new deep hashing model integrated with a novel gradient attention mechanism. Extensive experimental results on three benchmark datasets show that our proposed algorithm is able to accelerate the learning process and obtain competitive retrieval performance compared with state-of-the-art deep hashing models.",
        "中文标题": "通过梯度注意力加速深度哈希学习",
        "摘要翻译": "近年来，在快速大规模图像检索中，学习哈希取得了成功。随着深度学习在许多计算机视觉应用上显示出其卓越性能，基于学习的哈希模型设计已从浅层架构转向深层架构。然而，根据我们的分析，我们发现基于梯度下降的算法在深度哈希模型中使用时，可能会导致一对训练实例的哈希码在优化过程中同时向彼此的方向更新。在最坏的情况下，配对的哈希码在更新后交换了它们的方向，因此它们在汉明空间中的相应距离保持不变。这使得整个学习过程非常低效。为了解决这个问题，我们提出了一种新的深度哈希模型，该模型集成了新颖的梯度注意力机制。在三个基准数据集上的大量实验结果表明，与最先进的深度哈希模型相比，我们提出的算法能够加速学习过程并获得有竞争力的检索性能。",
        "领域": "图像检索/哈希学习/深度学习",
        "问题": "深度哈希模型在优化过程中，一对训练实例的哈希码可能同时向彼此的方向更新，导致学习过程低效",
        "动机": "提高深度哈希模型的学习效率和检索性能",
        "方法": "提出了一种新的深度哈希模型，集成了新颖的梯度注意力机制",
        "关键词": [
            "图像检索",
            "哈希学习",
            "梯度注意力"
        ],
        "涉及的技术概念": "深度哈希模型、梯度下降算法、梯度注意力机制、汉明空间"
    },
    {
        "order": 260,
        "title": "Auto-ReID: Searching for a Part-Aware ConvNet for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Quan_Auto-ReID_Searching_for_a_Part-Aware_ConvNet_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Prevailing deep convolutional neural networks (CNNs) for person re-IDentification (reID) are usually built upon ResNet or VGG backbones, which were originally designed for classification. Because reID is different from classification, the architecture should be modified accordingly. We propose to automatically search for a CNN architecture that is specifically suitable for the reID task. There are three aspects to be tackled. First, body structural information plays an important role in reID but it is not encoded in backbones. Second, Neural Architecture Search (NAS) automates the process of architecture design without human effort, but no existing NAS methods incorporate the structure information of input images. Third, reID is essentially a retrieval task but current NAS algorithms are merely designed for classification. To solve these problems, we propose a retrieval-based search algorithm over a specifically designed reID search space, named Auto-ReID. Our Auto-ReID enables the automated approach to find an efficient and effective CNN architecture for reID. Extensive experiments demonstrate that the searched architecture achieves state-of-the-art performance while reducing 50% parameters and 53% FLOPs compared to others.",
        "中文标题": "Auto-ReID: 为行人重识别搜索部分感知的卷积网络",
        "摘要翻译": "当前用于行人重识别（reID）的深度卷积神经网络（CNNs）通常基于ResNet或VGG骨干网络，这些网络最初是为分类任务设计的。由于reID与分类不同，因此应相应地修改架构。我们提出自动搜索一个特别适合reID任务的CNN架构。需要解决的方面有三个。首先，身体结构信息在reID中扮演重要角色，但它并未在骨干网络中编码。其次，神经架构搜索（NAS）自动化了架构设计过程，无需人工干预，但现有的NAS方法并未包含输入图像的结构信息。第三，reID本质上是一个检索任务，但当前的NAS算法仅设计用于分类。为了解决这些问题，我们提出了一种基于检索的搜索算法，针对特别设计的reID搜索空间，名为Auto-ReID。我们的Auto-ReID使得自动化方法能够找到一个既高效又有效的CNN架构用于reID。大量实验证明，搜索到的架构在减少50%参数和53%FLOPs的同时，达到了最先进的性能。",
        "领域": "行人重识别/神经架构搜索/卷积神经网络",
        "问题": "如何自动搜索一个特别适合行人重识别任务的CNN架构",
        "动机": "由于现有的CNN架构是为分类任务设计的，而行人重识别任务与分类任务不同，需要一种能够自动搜索适合行人重识别任务的CNN架构的方法。",
        "方法": "提出了一种基于检索的搜索算法，针对特别设计的reID搜索空间，名为Auto-ReID，以自动化地找到一个既高效又有效的CNN架构用于reID。",
        "关键词": [
            "行人重识别",
            "神经架构搜索",
            "卷积神经网络"
        ],
        "涉及的技术概念": {
            "深度卷积神经网络（CNNs）": "一种深度学习模型，特别适用于处理图像数据。",
            "ResNet或VGG骨干网络": "两种广泛使用的CNN架构，最初设计用于图像分类任务。",
            "神经架构搜索（NAS）": "一种自动化技术，用于搜索最优的神经网络架构。",
            "检索任务": "一种任务类型，旨在从大量数据中找到与查询最相关的项。",
            "FLOPs": "浮点运算次数，用于衡量模型的计算复杂度。"
        }
    },
    {
        "order": 261,
        "title": "Making the Invisible Visible: Action Recognition Through Walls and Occlusions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Making_the_Invisible_Visible_Action_Recognition_Through_Walls_and_Occlusions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Making_the_Invisible_Visible_Action_Recognition_Through_Walls_and_Occlusions_ICCV_2019_paper.html",
        "abstract": "Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition.",
        "中文标题": "让不可见变为可见：通过墙壁和遮挡进行动作识别",
        "摘要翻译": "理解人们的行为和互动通常依赖于看到他们。从视觉数据中自动化动作识别的过程一直是计算机视觉社区研究的主题。但如果光线太暗，或者人被遮挡或位于墙后呢？在本文中，我们介绍了一种神经网络模型，该模型能够通过墙壁和遮挡，在光线条件不佳的情况下检测人类动作。我们的模型以射频（RF）信号作为输入，生成3D人体骨架作为中间表示，并识别多人随时间的动作和互动。通过将输入转换为基于骨架的中间表示，我们的模型可以从基于视觉和基于RF的数据集中学习，并允许这两个任务相互帮助。我们展示了我们的模型在可见场景中实现了与基于视觉的动作识别系统相当的准确性，而在人们不可见的情况下仍能准确工作，从而解决了超出当今基于视觉的动作识别限制的场景。",
        "领域": "动作识别/射频信号处理/3D骨架重建",
        "问题": "在光线不足、遮挡或墙壁后等条件下进行人类动作识别",
        "动机": "解决在视觉数据不可用或受限的情况下自动化动作识别的挑战",
        "方法": "采用神经网络模型，以射频信号为输入，生成3D人体骨架作为中间表示，识别多人动作和互动",
        "关键词": [
            "动作识别",
            "射频信号",
            "3D骨架重建"
        ],
        "涉及的技术概念": "射频信号（RF signals）用于捕捉人体动作信息，3D人体骨架作为动作识别的中间表示，神经网络模型用于处理和分析这些数据以识别动作和互动。"
    },
    {
        "order": 262,
        "title": "SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_SVD_A_Large-Scale_Short_Video_Dataset_for_Near-Duplicate_Video_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_SVD_A_Large-Scale_Short_Video_Dataset_for_Near-Duplicate_Video_Retrieval_ICCV_2019_paper.html",
        "abstract": "With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difficult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io.",
        "中文标题": "SVD: 一个用于近似重复视频检索的大规模短视频数据集",
        "摘要翻译": "随着实际应用中视频数据的爆炸性增长，近似重复视频检索（NDVR）变得不可或缺且具有挑战性，尤其是对于短视频。然而，所有现有的NDVR数据集都是为长视频引入的。此外，由于收集和标注近似重复视频的高成本，大多数数据集规模小且缺乏多样性。在本文中，我们为NDVR任务引入了一个名为SVD的大规模短视频数据集。SVD包含超过500,000个短视频和超过30,000个标注的近似重复视频。我们使用多种视频挖掘技术来构建正/负对。此外，我们设计了时间和空间变换来模拟实际应用中的用户攻击行为，以构建更难的SVD变体。实验表明，现有的最先进的NDVR方法，包括基于实值的方法和基于哈希的方法，在这个具有挑战性的数据集上都无法达到满意的性能。SVD数据集的发布将促进NDVR领域的研究和系统工程。SVD数据集可在https://svdbase.github.io获取。",
        "领域": "视频检索/数据挖掘/信息检索",
        "问题": "解决短视频近似重复视频检索的挑战",
        "动机": "现有数据集主要针对长视频，且规模小、缺乏多样性，无法满足短视频近似重复视频检索的需求",
        "方法": "引入大规模短视频数据集SVD，使用视频挖掘技术构建正/负对，设计时间和空间变换模拟用户攻击行为",
        "关键词": [
            "短视频",
            "近似重复视频检索",
            "数据集",
            "视频挖掘",
            "用户攻击行为"
        ],
        "涉及的技术概念": {
            "近似重复视频检索（NDVR）": "一种技术，用于在大量视频数据中查找内容相似或重复的视频片段。",
            "视频挖掘技术": "用于从视频数据中提取有用信息的技术，包括但不限于视频内容分析、特征提取等。",
            "正/负对": "在机器学习中，正对指的是两个相似或相同的样本，负对指的是两个不相似的样本。",
            "时间和空间变换": "对视频进行时间上的加速、减速或空间上的旋转、缩放等操作，以模拟不同的观看条件或攻击行为。"
        }
    },
    {
        "order": 263,
        "title": "Second-Order Non-Local Attention Networks for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xia_Second-Order_Non-Local_Attention_Networks_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Recent efforts have shown promising results for person re-identification by designing part-based architectures to allow a neural network to learn discriminative representations from semantically coherent parts. Some efforts use soft attention to reallocate distant outliers to their most similar parts, while others adjust part granularity to incorporate more distant positions for learning the relationships. Others seek to generalize part-based methods by introducing a dropout mechanism on consecutive regions of the feature map to enhance distant region relationships. However, only few prior efforts model the distant or non-local positions of the feature map directly for the person re-ID task. In this paper, we propose a novel attention mechanism to directly model long-range relationships via second-order feature statistics. When combined with a generalized DropBlock module, our method performs equally to or better than state-of-the-art results for mainstream person re-identification datasets, including Market1501, CUHK03, and DukeMTMC-reID.",
        "中文标题": "二阶非局部注意力网络用于行人重识别",
        "摘要翻译": "最近的研究通过设计基于部分的架构，使神经网络能够从语义一致的部分学习区分性表示，展示了行人重识别的有希望的结果。一些研究使用软注意力将远处的异常值重新分配到它们最相似的部分，而另一些则调整部分粒度以包含更远的位置来学习关系。还有一些研究通过在特征图的连续区域引入dropout机制来增强远距离区域关系，从而寻求泛化基于部分的方法。然而，只有少数先前的研究直接为行人重识别任务建模特征图的远处或非局部位置。在本文中，我们提出了一种新颖的注意力机制，通过二阶特征统计直接建模长距离关系。当与广义的DropBlock模块结合时，我们的方法在主流行人重识别数据集（包括Market1501、CUHK03和DukeMTMC-reID）上的表现与最先进的结果相当或更好。",
        "领域": "行人重识别/注意力机制/特征学习",
        "问题": "如何有效地建模行人重识别任务中的长距离关系",
        "动机": "现有的基于部分的方法在建模远距离关系方面存在不足，需要一种新的机制来直接建模这些关系以提高行人重识别的性能。",
        "方法": "提出了一种新颖的注意力机制，通过二阶特征统计直接建模长距离关系，并结合广义的DropBlock模块来增强模型性能。",
        "关键词": [
            "行人重识别",
            "注意力机制",
            "二阶特征统计",
            "DropBlock模块"
        ],
        "涉及的技术概念": "二阶特征统计用于直接建模长距离关系，DropBlock模块用于增强模型对远距离区域关系的捕捉能力。"
    },
    {
        "order": 264,
        "title": "Recursive Visual Sound Separation Using Minus-Plus Net",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Recursive_Visual_Sound_Separation_Using_Minus-Plus_Net_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Recursive_Visual_Sound_Separation_Using_Minus-Plus_Net_ICCV_2019_paper.html",
        "abstract": "Sounds provide rich semantics, complementary to visual data, for many tasks. However, in practice, sounds from multiple sources are often mixed together. In this paper we propose a novel framework, referred to as MinusPlus Network (MP-Net), for the task of visual sound separation. MP-Net separates sounds recursively in the order of average energy, removing the separated sound from the mixture at the end of each prediction, until the mixture becomes empty or contains only noise. In this way, MP-Net could be applied to sound mixtures with arbitrary numbers and types of sounds. Moreover, while MP-Net keeps removing sounds with large energy from the mixture, sounds with small energy could emerge and become clearer, so that the separation is more accurate. Compared to previous methods, MP-Net obtains state-of-the-art results on two large scale datasets, across mixtures with different types and numbers of sounds.",
        "中文标题": "使用Minus-Plus网络进行递归视觉声音分离",
        "摘要翻译": "声音为许多任务提供了丰富的语义，与视觉数据互补。然而，在实践中，来自多个来源的声音经常混合在一起。在本文中，我们提出了一种新颖的框架，称为MinusPlus网络（MP-Net），用于视觉声音分离任务。MP-Net按照平均能量的顺序递归地分离声音，在每次预测结束时从混合物中移除已分离的声音，直到混合物变为空或仅包含噪声。这样，MP-Net可以应用于具有任意数量和类型声音的声音混合物。此外，当MP-Net不断从混合物中移除能量较大的声音时，能量较小的声音可能会出现并变得更清晰，从而使分离更加准确。与之前的方法相比，MP-Net在两种大规模数据集上获得了最先进的结果，涵盖了不同类型和数量的声音混合物。",
        "领域": "声音分离/视觉语义/递归处理",
        "问题": "从混合声音中分离出各个来源的声音",
        "动机": "声音与视觉数据互补，但在实践中，声音经常混合在一起，需要有效的方法进行分离",
        "方法": "提出MinusPlus网络（MP-Net），按照平均能量的顺序递归地分离声音，并在每次预测结束时从混合物中移除已分离的声音",
        "关键词": [
            "声音分离",
            "递归处理",
            "视觉语义"
        ],
        "涉及的技术概念": "MinusPlus网络（MP-Net）是一种新颖的框架，用于视觉声音分离任务，通过递归地按照平均能量顺序分离声音，并在每次预测结束时从混合物中移除已分离的声音，直到混合物变为空或仅包含噪声。这种方法可以应用于具有任意数量和类型声音的声音混合物，并且在处理过程中，能量较小的声音会变得更清晰，从而提高分离的准确性。"
    },
    {
        "order": 265,
        "title": "Block Annotation: Better Image Annotation With Sub-Image Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Block_Annotation_Better_Image_Annotation_With_Sub-Image_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Block_Annotation_Better_Image_Annotation_With_Sub-Image_Decomposition_ICCV_2019_paper.html",
        "abstract": "Image datasets with high-quality pixel-level annotations are valuable for semantic segmentation: labelling every pixel in an image ensures that rare classes and small objects are annotated. However, full-image annotations are expensive, with experts spending up to 90 minutes per image. We propose block sub-image annotation as a replacement for full-image annotation. Despite the attention cost of frequent task switching, we find that block annotations can be crowdsourced at higher quality compared to full-image annotation with equal monetary cost using existing annotation tools developed for full-image annotation. Surprisingly, we find that 50% pixels annotated with blocks allows semantic segmentation to achieve equivalent performance to 100% pixels annotated. Furthermore, as little as 12% of pixels annotated allows performance as high as 98% of the performance with dense annotation. In weakly-supervised settings, block annotation outperforms existing methods by 3-4% (absolute) given equivalent annotation time. To recover the necessary global structure for applications such as characterizing spatial context and affordance relationships, we propose an effective method to inpaint block-annotated images with high-quality labels without additional human effort. As such, fewer annotations can also be used for these applications compared to full-image annotation.",
        "中文标题": "块注释：通过子图像分解实现更好的图像注释",
        "摘要翻译": "具有高质量像素级注释的图像数据集对于语义分割非常有价值：标注图像中的每个像素确保了稀有类别和小物体的标注。然而，全图像注释成本高昂，专家每张图像最多需要花费90分钟。我们提出了块子图像注释作为全图像注释的替代方案。尽管频繁任务切换的注意力成本较高，但我们发现，与使用现有全图像注释工具的全图像注释相比，块注释可以以更高的质量进行众包，且货币成本相同。令人惊讶的是，我们发现，使用块注释标注50%的像素可以使语义分割达到与标注100%像素相同的性能。此外，仅标注12%的像素即可达到密集标注性能的98%。在弱监督设置中，给定相同的注释时间，块注释比现有方法高出3-4%（绝对值）。为了恢复必要的全局结构，例如表征空间上下文和可供性关系的应用，我们提出了一种有效的方法，无需额外人力即可用高质量标签修复块注释图像。因此，与全图像注释相比，这些应用也可以使用更少的注释。",
        "领域": "语义分割/图像注释/众包",
        "问题": "如何降低高质量像素级图像注释的成本",
        "动机": "全图像注释成本高昂，需要寻找更经济的替代方案",
        "方法": "提出块子图像注释方法，通过众包实现高质量注释，并提出一种无需额外人力即可修复块注释图像的方法",
        "关键词": [
            "语义分割",
            "图像注释",
            "众包"
        ],
        "涉及的技术概念": "块子图像注释是一种替代全图像注释的方法，通过标注图像的一部分来降低注释成本。众包是指将任务分配给大量非专业人员进行。语义分割是一种图像处理技术，旨在将图像分割成多个区域，每个区域对应一个特定的类别。"
    },
    {
        "order": 266,
        "title": "Fast Computation of Content-Sensitive Superpixels and Supervoxels Using Q-Distances",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Fast_Computation_of_Content-Sensitive_Superpixels_and_Supervoxels_Using_Q-Distances_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Fast_Computation_of_Content-Sensitive_Superpixels_and_Supervoxels_Using_Q-Distances_ICCV_2019_paper.html",
        "abstract": "State-of-the-art researches model the data of images and videos as low-dimensional manifolds and generate superpixels/supervoxels in a content-sensitive way, which is achieved by computing geodesic centroidal Voronoi tessellation (GCVT) on manifolds. However, computing exact GCVTs is slow due to computationally expensive geodesic distances. In this paper, we propose a much faster queue-based graph distance (called q-distance). Our key idea is that for manifold regions in which q-distances are different from geodesic distances, GCVT is prone to placing more generators in them, and therefore after few iterations, the q-distance-induced tessellation is an exact GCVT. This idea works well in practice and we also prove it theoretically under moderate assumption. Our method is simple and easy to implement. It runs 6-8 times faster than state-of-the-art GCVT computation, and has an optimal approximation ratio O(1) and a linear time complexity O(N) for N-pixel images or N-voxel videos. A thorough evaluation of 31 superpixel methods on five image datasets and 8 supervoxel methods on four video datasets shows that our method consistently achieves the best over-segmentation accuracy. We also demonstrate the advantage of our method on one image and two video applications.",
        "中文标题": "使用Q-距离快速计算内容敏感的超级像素和超级体素",
        "摘要翻译": "最先进的研究将图像和视频数据建模为低维流形，并以内容敏感的方式生成超级像素/超级体素，这是通过在流形上计算测地线中心Voronoi镶嵌（GCVT）来实现的。然而，由于计算测地线距离的计算成本高，计算精确的GCVT速度很慢。在本文中，我们提出了一种更快的基于队列的图距离（称为q-距离）。我们的关键思想是，对于q-距离与测地线距离不同的流形区域，GCVT倾向于在其中放置更多的生成器，因此在几次迭代后，q-距离引起的镶嵌就是精确的GCVT。这个想法在实践中效果很好，我们也在适度假设下从理论上证明了这一点。我们的方法简单易实现。它比最先进的GCVT计算快6-8倍，并且对于N像素图像或N体素视频具有最优的近似比O(1)和线性时间复杂度O(N)。对五个图像数据集上的31种超级像素方法和四个视频数据集上的8种超级体素方法的全面评估表明，我们的方法始终实现最佳过分割精度。我们还展示了我们的方法在一个图像和两个视频应用中的优势。",
        "领域": "图像分割/视频分析/流形学习",
        "问题": "如何快速计算内容敏感的超级像素和超级体素",
        "动机": "现有的测地线中心Voronoi镶嵌（GCVT）计算方法由于计算测地线距离的高成本而速度慢",
        "方法": "提出了一种基于队列的图距离（q-距离），通过减少计算复杂度来加速GCVT的计算",
        "关键词": [
            "超级像素",
            "超级体素",
            "q-距离",
            "GCVT",
            "图像分割",
            "视频分析"
        ],
        "涉及的技术概念": "测地线中心Voronoi镶嵌（GCVT）是一种在流形上生成超级像素/超级体素的方法，通过计算测地线距离来实现。本文提出的q-距离是一种更快的图距离计算方法，用于近似GCVT，从而加速内容敏感的超级像素和超级体素的生成。"
    },
    {
        "order": 267,
        "title": "Unsupervised Video Interpolation Using Cycle Consistency",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Reda_Unsupervised_Video_Interpolation_Using_Cycle_Consistency_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Reda_Unsupervised_Video_Interpolation_Using_Cycle_Consistency_ICCV_2019_paper.html",
        "abstract": "Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pre-trained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets.",
        "中文标题": "使用循环一致性进行无监督视频插值",
        "摘要翻译": "学习通过插值合成高帧率视频需要大量的高帧率训练视频，然而这些视频在高分辨率下尤其稀缺。在此，我们提出了无监督技术，直接从低帧率视频合成高帧率视频，利用循环一致性。对于连续的三帧，我们优化模型以最小化中心帧与其循环重建之间的差异，循环重建是通过从插值的中间帧插值回来获得的。仅这一简单的无监督约束就达到了与使用真实中间帧进行监督相当的结果。我们进一步引入了一个伪监督损失项，该损失项强制插值帧与预训练插值模型的预测一致。伪监督损失项与循环一致性一起使用，可以有效地将预训练模型适应到新的目标域。在没有额外数据且完全无监督的情况下，我们的技术显著提高了预训练模型在新目标域上的表现，将Slowflow评估数据集上的PSNR值从32.84dB提高到33.05dB，将Sintel评估数据集上的PSNR值从31.82dB提高到32.53dB。",
        "领域": "视频插值/无监督学习/循环一致性",
        "问题": "高帧率视频数据稀缺，尤其是在高分辨率下，难以直接训练模型进行视频插值。",
        "动机": "为了解决高帧率视频数据稀缺的问题，提出了一种无监督的方法来直接从低帧率视频合成高帧率视频。",
        "方法": "利用循环一致性优化模型，最小化中心帧与其循环重建之间的差异，并引入伪监督损失项以增强插值帧与预训练模型预测的一致性。",
        "关键词": [
            "视频插值",
            "无监督学习",
            "循环一致性",
            "伪监督损失"
        ],
        "涉及的技术概念": "循环一致性是一种确保模型输出与输入之间一致性的技术，通过最小化输入与通过模型处理后的输出之间的差异来训练模型。伪监督损失是一种利用预训练模型的预测作为监督信号来进一步优化模型的方法。PSNR（峰值信噪比）是衡量图像或视频质量的指标，值越高表示质量越好。"
    },
    {
        "order": 268,
        "title": "Probabilistic Deep Ordinal Regression Based on Gaussian Processes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Probabilistic_Deep_Ordinal_Regression_Based_on_Gaussian_Processes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Probabilistic_Deep_Ordinal_Regression_Based_on_Gaussian_Processes_ICCV_2019_paper.html",
        "abstract": "With excellent representation power for complex data, deep neural networks (DNNs) based approaches are state-of-the-art for ordinal regression problem which aims to classify instances into ordinal categories. However, DNNs are not able to capture uncertainties and produce probabilistic interpretations. As a probabilistic model, Gaussian Processes (GPs) on the other hand offers uncertainty information, which is nonetheless lack of scalability for large datasets. This paper adapts traditional GPs regression for ordinal regression problem by using both conjugate and non-conjugate ordinal likelihood. Based on that, it proposes a deep neural network with a GPs layer on the top, which is trained end-to-end by the stochastic gradient descent method for both neural network parameters and GPs parameters. The parameters in the ordinal likelihood function are learned as neural network parameters so that the proposed framework is able to produce fitted likelihood functions for training sets and make probabilistic predictions for test points. Experimental results on three real-world benchmarks -- image aesthetics rating, historical image grading and age group estimation -- demonstrate that in terms of mean absolute error, the proposed approach outperforms state-of-the-art ordinal regression approaches and provides the confidence for predictions.",
        "中文标题": "基于高斯过程的概率深度序数回归",
        "摘要翻译": "深度神经网络（DNNs）方法因其对复杂数据的出色表示能力，在旨在将实例分类为序数类别的序数回归问题上处于领先地位。然而，DNNs无法捕捉不确定性并提供概率解释。作为一种概率模型，高斯过程（GPs）提供了不确定性信息，但对于大数据集缺乏可扩展性。本文通过使用共轭和非共轭序数似然，将传统的GPs回归适应于序数回归问题。在此基础上，提出了一种在顶部带有GPs层的深度神经网络，该网络通过随机梯度下降方法对神经网络参数和GPs参数进行端到端训练。序数似然函数中的参数被学习为神经网络参数，使得所提出的框架能够为训练集生成拟合的似然函数，并为测试点做出概率预测。在三个真实世界的基准测试——图像美学评分、历史图像分级和年龄组估计——上的实验结果表明，就平均绝对误差而言，所提出的方法优于最先进的序数回归方法，并为预测提供了置信度。",
        "领域": "序数回归/高斯过程/深度神经网络",
        "问题": "解决深度神经网络在序数回归问题上无法捕捉不确定性和提供概率解释的问题",
        "动机": "为了在序数回归问题中引入不确定性信息，并提高模型的可扩展性和预测的置信度",
        "方法": "提出了一种结合高斯过程和深度神经网络的模型，通过使用共轭和非共轭序数似然，对神经网络参数和高斯过程参数进行端到端训练",
        "关键词": [
            "序数回归",
            "高斯过程",
            "深度神经网络",
            "不确定性",
            "概率预测"
        ],
        "涉及的技术概念": {
            "深度神经网络（DNNs）": "一种能够学习复杂数据表示的神经网络，广泛应用于各种机器学习任务。",
            "高斯过程（GPs）": "一种概率模型，能够提供预测的不确定性信息，适用于小数据集。",
            "序数回归": "一种分类问题，其中类别是有序的，例如评级或评分。",
            "共轭和非共轭序数似然": "用于序数回归问题的两种似然函数，分别对应于不同的统计假设。",
            "随机梯度下降": "一种优化算法，用于训练神经网络，通过迭代地调整参数以最小化损失函数。"
        }
    },
    {
        "order": 269,
        "title": "Pix2Pose: Pixel-Wise Coordinate Regression of Objects for 6D Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.",
        "中文标题": "Pix2Pose: 用于6D姿态估计的物体像素级坐标回归",
        "摘要翻译": "仅使用RGB图像估计物体的6D姿态仍然具有挑战性，因为存在遮挡和对称性等问题。此外，没有专家知识或专用扫描设备，很难构建具有精确纹理的3D模型。为了解决这些问题，我们提出了一种新的姿态估计方法Pix2Pose，它可以在没有纹理模型的情况下预测每个物体像素的3D坐标。设计了一个自动编码器架构来估计每个像素的3D坐标和预期误差。然后，这些像素级预测在多个阶段用于形成2D-3D对应关系，以直接使用PnP算法与RANSAC迭代计算姿态。我们的方法通过利用生成对抗训练的最新成果来精确恢复被遮挡的部分，从而对遮挡具有鲁棒性。此外，提出了一种新的损失函数——变换器损失，通过引导预测到最近的对称姿态来处理对称物体。在包含对称和遮挡物体的三个不同基准数据集上的评估表明，我们的方法仅使用RGB图像就优于现有技术。",
        "领域": "姿态估计/3D重建/生成对抗网络",
        "问题": "仅使用RGB图像估计物体的6D姿态，解决遮挡和对称性问题",
        "动机": "解决在没有精确纹理的3D模型和专家知识或专用扫描设备的情况下，估计物体6D姿态的挑战",
        "方法": "提出Pix2Pose方法，使用自动编码器架构预测每个物体像素的3D坐标和预期误差，通过生成对抗训练恢复被遮挡部分，使用变换器损失处理对称物体",
        "关键词": [
            "6D姿态估计",
            "RGB图像",
            "自动编码器",
            "生成对抗网络",
            "变换器损失"
        ],
        "涉及的技术概念": "6D姿态估计指的是在三维空间中确定物体的位置和方向，包括三个平移和三个旋转参数。RGB图像是指由红色、绿色和蓝色三个颜色通道组成的图像。自动编码器是一种神经网络，用于学习数据的有效编码。生成对抗网络（GAN）是一种由生成器和判别器组成的框架，用于生成新的数据样本。变换器损失是一种新的损失函数，用于处理对称物体，通过引导预测到最近的对称姿态。"
    },
    {
        "order": 270,
        "title": "Progressive-X: Efficient, Anytime, Multi-Model Fitting Algorithm",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Barath_Progressive-X_Efficient_Anytime_Multi-Model_Fitting_Algorithm_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Barath_Progressive-X_Efficient_Anytime_Multi-Model_Fitting_Algorithm_ICCV_2019_paper.html",
        "abstract": "The Progressive-X algorithm, Prog-X in short, is proposed for geometric multi-model fitting. The method interleaves sampling and consolidation of the current data interpretation via repetitive hypothesis proposal, fast rejection, and integration of the new hypothesis into the kept instance set by labeling energy minimization. Due to exploring the data progressively, the method has several beneficial properties compared with the state-of-the-art. First, a clear criterion, adopted from RANSAC, controls the termination and stops the algorithm when the probability of finding a new model with a reasonable number of inliers falls below a threshold. Second, Prog-X is an any-time algorithm. Thus, whenever is interrupted, e.g. due to a time limit, the returned instances cover real and, likely, the most dominant ones. The method is superior to the state-of-the-art in terms of accuracy in both synthetic experiments and on publicly available real-world datasets for homography, two-view motion, and motion segmentation.",
        "中文标题": "渐进-X：高效、随时、多模型拟合算法",
        "摘要翻译": "渐进-X算法，简称Prog-X，是为几何多模型拟合提出的。该方法通过重复假设提出、快速拒绝和通过标记能量最小化将新假设整合到保留的实例集中，交替进行采样和当前数据解释的整合。由于逐步探索数据，该方法与现有技术相比具有几个有益的特性。首先，采用自RANSAC的明确标准控制终止，并在找到具有合理数量内点的新模型的概率低于阈值时停止算法。其次，Prog-X是一个随时算法。因此，无论何时被中断，例如由于时间限制，返回的实例都覆盖了真实且可能是最主导的实例。在合成实验和公开可用的真实世界数据集上，该方法在单应性、双视图运动和运动分割方面的准确性优于现有技术。",
        "领域": "几何模型拟合/运动分割/单应性估计",
        "问题": "几何多模型拟合",
        "动机": "提高几何多模型拟合的效率和准确性，特别是在处理复杂场景时。",
        "方法": "通过重复假设提出、快速拒绝和通过标记能量最小化将新假设整合到保留的实例集中，交替进行采样和当前数据解释的整合。",
        "关键词": [
            "几何模型拟合",
            "运动分割",
            "单应性估计",
            "RANSAC",
            "能量最小化"
        ],
        "涉及的技术概念": "RANSAC（随机抽样一致性）是一种用于估计数学模型参数的迭代方法，通过从数据集中随机选择子集来估计模型参数，并评估这些参数在完整数据集上的拟合程度。能量最小化是一种优化技术，用于找到使某个能量函数最小化的参数或状态。"
    },
    {
        "order": 271,
        "title": "Deformable Surface Tracking by Graph Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deformable_Surface_Tracking_by_Graph_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deformable_Surface_Tracking_by_Graph_Matching_ICCV_2019_paper.html",
        "abstract": "This paper addresses the problem of deformable surface tracking from monocular images. Specifically, we propose a graph-based approach that effectively explores the structure information of the surface to enhance tracking performance. Our approach solves simultaneously for feature correspondence, outlier rejection and shape reconstruction by optimizing a single objective function, which is defined by means of pairwise projection errors between graph structures instead of unary projection errors between matched points. Furthermore, an efficient matching algorithm is developed based on soft matching relaxation. For evaluation, our approach is extensively compared to state-of-the-art algorithms on a standard dataset of occluded surfaces, as well as a newly compiled dataset of different surfaces with rich, weak or repetitive texture. Experimental results reveal that our approach achieves robust tracking results for surfaces with different types of texture, and outperforms other algorithms in both accuracy and efficiency.",
        "中文标题": "通过图匹配进行可变形表面跟踪",
        "摘要翻译": "本文解决了从单目图像中进行可变形表面跟踪的问题。具体来说，我们提出了一种基于图的方法，该方法有效地探索了表面的结构信息以提高跟踪性能。我们的方法通过优化一个单一的目标函数同时解决了特征对应、异常值拒绝和形状重建的问题，该目标函数是通过图结构之间的成对投影误差而不是匹配点之间的一元投影误差来定义的。此外，基于软匹配松弛开发了一种高效的匹配算法。为了评估，我们的方法在标准遮挡表面数据集以及新编译的不同表面数据集上广泛比较了最先进的算法，这些表面具有丰富、弱或重复的纹理。实验结果表明，我们的方法对于具有不同类型纹理的表面实现了鲁棒的跟踪结果，并且在准确性和效率上都优于其他算法。",
        "领域": "表面跟踪/图匹配/形状重建",
        "问题": "从单目图像中进行可变形表面跟踪",
        "动机": "提高可变形表面跟踪的性能，特别是在处理具有不同纹理类型的表面时",
        "方法": "提出了一种基于图的方法，通过优化一个单一的目标函数同时解决特征对应、异常值拒绝和形状重建的问题，并开发了一种基于软匹配松弛的高效匹配算法",
        "关键词": [
            "可变形表面跟踪",
            "图匹配",
            "形状重建",
            "软匹配松弛"
        ],
        "涉及的技术概念": "图匹配是一种技术，用于在图像或视频中找到两个图结构之间的对应关系。软匹配松弛是一种优化技术，用于在匹配过程中允许一定程度的灵活性，以提高匹配的准确性和鲁棒性。成对投影误差是指两个图结构之间的投影误差，而不是单个点之间的误差。"
    },
    {
        "order": 272,
        "title": "Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Balanced_Datasets_Are_Not_Enough_Estimating_and_Mitigating_Gender_Bias_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Balanced_Datasets_Are_Not_Enough_Estimating_and_Mitigating_Gender_Bias_ICCV_2019_paper.html",
        "abstract": "In this work, we present a framework to measure and mitigate intrinsic biases with respect to protected variables -such as gender- in visual recognition tasks. We show that trained models significantly amplify the association of target labels with gender beyond what one would expect from biased datasets. Surprisingly, we show that even when datasets are balanced such that each label co-occurs equally with each gender, learned models amplify the association between labels and gender, as much as if data had not been balanced! To mitigate this, we adopt an adversarial approach to remove unwanted features corresponding to protected variables from intermediate representations in a deep neural network - and provide a detailed analysis of its effectiveness. Experiments on two datasets: the COCO dataset (objects), and the imSitu dataset (actions), show reductions in gender bias amplification while maintaining most of the accuracy of the original models.",
        "中文标题": "平衡数据集不足：估计和减轻深度图像表示中的性别偏见",
        "摘要翻译": "在这项工作中，我们提出了一个框架，用于测量和减轻视觉识别任务中关于受保护变量（如性别）的内在偏见。我们展示了训练模型显著放大了目标标签与性别之间的关联，超出了人们从偏见数据集中预期的程度。令人惊讶的是，我们展示了即使数据集被平衡，使得每个标签与每个性别同等共现，学习到的模型仍然放大了标签与性别之间的关联，就像数据没有被平衡一样！为了减轻这一点，我们采用了一种对抗性方法，从深度神经网络的中间表示中移除与受保护变量对应的不想要的特征，并提供了其有效性的详细分析。在两个数据集上的实验：COCO数据集（对象）和imSitu数据集（动作），显示在保持原始模型大部分准确性的同时，减少了性别偏见的放大。",
        "领域": "视觉识别/性别偏见/深度学习",
        "问题": "深度图像表示中的性别偏见问题",
        "动机": "研究动机是发现并减轻视觉识别任务中关于性别的内在偏见，即使在使用平衡数据集的情况下，训练模型仍会放大性别与标签之间的关联。",
        "方法": "采用对抗性方法从深度神经网络的中间表示中移除与受保护变量（如性别）对应的不想要的特征。",
        "关键词": [
            "视觉识别",
            "性别偏见",
            "对抗性方法"
        ],
        "涉及的技术概念": "深度神经网络、对抗性方法、中间表示、COCO数据集、imSitu数据集"
    },
    {
        "order": 273,
        "title": "CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_CDPN_Coordinates-Based_Disentangled_Pose_Network_for_Real-Time_RGB-Based_6-DoF_Object_ICCV_2019_paper.html",
        "abstract": "6-DoF object pose estimation from a single RGB image is a fundamental and long-standing problem in computer vision. Current leading approaches solve it by training deep networks to either regress both rotation and translation from image directly or to construct 2D-3D correspondences and further solve them via PnP indirectly. We argue that rotation and translation should be treated differently for their significant difference. In this work, we propose a novel 6-DoF pose estimation approach: Coordinates-based Disentangled Pose Network (CDPN), which disentangles the pose to predict rotation and translation separately to achieve highly accurate and robust pose estimation. Our method is flexible, efficient, highly accurate and can deal with texture-less and occluded objects. Extensive experiments on LINEMOD and Occlusion datasets are conducted and demonstrate the superiority of our approach. Concretely, our approach significantly exceeds the state-of-the- art RGB-based methods on commonly used metrics.",
        "中文标题": "CDPN：基于坐标的解耦姿态网络用于实时RGB图像的6自由度物体姿态估计",
        "摘要翻译": "从单一RGB图像进行6自由度物体姿态估计是计算机视觉中一个基础且长期存在的问题。当前领先的方法通过训练深度网络直接从图像回归旋转和平移，或构建2D-3D对应关系并通过PnP间接解决这一问题。我们认为，由于旋转和平移的显著差异，应该对它们进行不同的处理。在这项工作中，我们提出了一种新颖的6自由度姿态估计方法：基于坐标的解耦姿态网络（CDPN），该方法通过解耦姿态来分别预测旋转和平移，以实现高精度和鲁棒的姿态估计。我们的方法灵活、高效、精度高，并且能够处理无纹理和遮挡的物体。在LINEMOD和Occlusion数据集上进行的广泛实验证明了我们方法的优越性。具体来说，我们的方法在常用指标上显著超过了最先进的基于RGB的方法。",
        "领域": "物体姿态估计/三维重建/视觉定位",
        "问题": "从单一RGB图像进行6自由度物体姿态估计",
        "动机": "由于旋转和平移的显著差异，应该对它们进行不同的处理，以提高姿态估计的精度和鲁棒性。",
        "方法": "提出了一种基于坐标的解耦姿态网络（CDPN），通过解耦姿态来分别预测旋转和平移。",
        "关键词": [
            "6自由度",
            "姿态估计",
            "解耦姿态网络",
            "RGB图像",
            "无纹理物体",
            "遮挡物体"
        ],
        "涉及的技术概念": "6自由度（6-DoF）指的是物体在三维空间中的位置和方向，包括三个平移自由度和三个旋转自由度。PnP（Perspective-n-Point）是一种通过已知的3D点和它们在图像中的2D投影来估计相机姿态的方法。解耦姿态网络（CDPN）是一种新颖的网络架构，旨在通过分别处理旋转和平移来提高姿态估计的精度和鲁棒性。"
    },
    {
        "order": 274,
        "title": "Deep Meta Learning for Real-Time Target-Aware Visual Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Deep_Meta_Learning_for_Real-Time_Target-Aware_Visual_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Deep_Meta_Learning_for_Real-Time_Target-Aware_Visual_Tracking_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel on-line visual tracking framework based on the Siamese matching network and meta-learner network, which run at real-time speeds. Conventional deep convolutional feature-based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters, which involve solving complex optimization tasks to adapt to the new appearance of a target object. To alleviate this complex process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target objects by adding target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed while maintaining competitive performance among other state-of-the-art tracking algorithms.",
        "中文标题": "基于深度元学习的实时目标感知视觉跟踪",
        "摘要翻译": "本文提出了一种基于Siamese匹配网络和元学习器网络的新型在线视觉跟踪框架，该框架以实时速度运行。传统的基于深度卷积特征的判别式视觉跟踪算法需要持续重新训练分类器或相关滤波器，这涉及解决复杂的优化任务以适应目标对象的新外观。为了简化这一复杂过程，我们提出的算法结合并利用元学习器网络，通过添加目标感知特征空间，为匹配网络提供目标对象的新外观信息。目标特定特征空间的参数通过元学习器网络的单次前向传递即时提供。通过消除在跟踪过程中持续解决复杂优化任务的必要性，实验结果表明，我们的算法在保持与其他最先进跟踪算法竞争性能的同时，以实时速度运行。",
        "领域": "视觉跟踪/元学习/实时系统",
        "问题": "传统视觉跟踪算法需要持续重新训练分类器或相关滤波器以适应目标对象的新外观，涉及复杂的优化任务。",
        "动机": "简化视觉跟踪过程中的复杂优化任务，实现实时跟踪。",
        "方法": "提出了一种结合Siamese匹配网络和元学习器网络的在线视觉跟踪框架，通过元学习器网络提供目标感知特征空间，实现实时跟踪。",
        "关键词": [
            "视觉跟踪",
            "元学习",
            "实时系统",
            "Siamese网络",
            "目标感知"
        ],
        "涉及的技术概念": "Siamese匹配网络用于比较两个输入之间的相似性，元学习器网络用于快速适应新任务或新数据，目标感知特征空间是指能够根据目标对象的外观变化动态调整的特征表示空间。"
    },
    {
        "order": 275,
        "title": "Teacher Guided Architecture Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bashivan_Teacher_Guided_Architecture_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bashivan_Teacher_Guided_Architecture_Search_ICCV_2019_paper.html",
        "abstract": "Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efficiency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (fixed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efficiency over performance-guided methods. Our approach finds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only  300 neurons from primate visual system provides enough signal to find a network with an Imagenet top-1 error that is significantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain's sensory processing networks.",
        "中文标题": "教师引导的架构搜索",
        "摘要翻译": "近年来，计算机视觉神经网络的大部分改进源于新网络架构的发现。大多数先前的工作使用有限训练后候选模型的性能来自动指导搜索，以可行的方式进行。是否可以通过使用未知详细架构的高性能网络（例如灵长类视觉系统）的测量来指导搜索，从而在计算效率上实现进一步的提升？作为实现这一目标的一步，我们使用表示相似性分析来评估候选网络与（固定的、高性能的）教师网络内部激活的相似性。我们展示了采用这种评估指标可以在搜索效率上比性能指导的方法提高一个数量级。我们的方法找到了一种卷积单元结构，其性能与之前使用其他方法找到的相似，但总计算成本比神经架构搜索（NAS）低两个数量级，比渐进神经架构搜索（PNAS）低四倍以上。我们进一步展示了仅从灵长类视觉系统的300个神经元中获取的测量数据就足以找到一个网络，其在Imagenet上的top-1错误率显著低于仅通过性能指导的架构搜索所达到的水平。这些结果表明，在可以访问感兴趣教师网络（如大脑的感觉处理网络）的部分或全部内部表示的情况下，表示匹配可以用于加速网络架构搜索。",
        "领域": "神经网络架构搜索/计算效率/表示相似性分析",
        "问题": "如何提高神经网络架构搜索的计算效率",
        "动机": "探索通过使用高性能网络的内部表示来指导搜索，以提高计算效率",
        "方法": "使用表示相似性分析评估候选网络与教师网络内部激活的相似性，以指导架构搜索",
        "关键词": [
            "神经网络架构搜索",
            "计算效率",
            "表示相似性分析",
            "教师网络",
            "灵长类视觉系统"
        ],
        "涉及的技术概念": {
            "表示相似性分析": "一种评估不同网络内部激活相似性的方法，用于指导网络架构搜索",
            "神经架构搜索（NAS）": "一种自动搜索最优神经网络架构的方法",
            "渐进神经架构搜索（PNAS）": "一种逐步构建和评估神经网络架构的方法，旨在减少搜索空间和计算成本",
            "Imagenet top-1错误率": "在Imagenet数据集上，模型预测的最可能类别与实际类别不符的比例，用于评估模型性能"
        }
    },
    {
        "order": 276,
        "title": "Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Structured_Modeling_of_Joint_Deep_Feature_and_Prediction_Refinement_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Structured_Modeling_of_Joint_Deep_Feature_and_Prediction_Refinement_for_ICCV_2019_paper.html",
        "abstract": "Recent saliency models extensively explore to incorporate multi-scale contextual information from Convolutional Neural Networks (CNNs). Besides direct fusion strategies, many approaches introduce message-passing to enhance CNN features or predictions. However, the messages are mainly transmitted in two ways, by feature-to-feature passing, and by prediction-to-prediction passing. In this paper, we add message-passing between features and predictions and propose a deep unified CRF saliency model . We design a novel cascade CRFs architecture with CNN to jointly refine deep features and predictions at each scale and progressively compute a final refined saliency map. We formulate the CRF graphical model that involves message-passing of feature-feature, feature-prediction, and prediction-prediction, from the coarse scale to the finer scale, to update the features and the corresponding predictions. Also, we formulate the mean-field updates for joint end-to-end model training with CNN through back propagation. The proposed deep unified CRF saliency model is evaluated over six datasets and shows highly competitive performance among the state of the arts.",
        "中文标题": "联合深度特征与预测优化的结构化建模用于显著目标检测",
        "摘要翻译": "最近的显著性模型广泛探索了如何从卷积神经网络（CNNs）中融入多尺度上下文信息。除了直接的融合策略外，许多方法引入了消息传递来增强CNN特征或预测。然而，消息主要通过两种方式传递，即特征到特征的传递和预测到预测的传递。在本文中，我们增加了特征与预测之间的消息传递，并提出了一个深度统一的CRF显著性模型。我们设计了一种新颖的级联CRFs架构与CNN结合，以在每个尺度上联合优化深度特征和预测，并逐步计算最终的优化显著性图。我们制定了CRF图模型，该模型涉及从粗尺度到细尺度的特征-特征、特征-预测和预测-预测的消息传递，以更新特征和相应的预测。此外，我们制定了通过反向传播进行联合端到端模型训练的平均场更新。所提出的深度统一CRF显著性模型在六个数据集上进行了评估，并在现有技术中显示出极具竞争力的性能。",
        "领域": "显著目标检测/卷积神经网络/条件随机场",
        "问题": "如何有效地融合多尺度上下文信息以优化显著目标检测",
        "动机": "现有的显著性模型在融合多尺度上下文信息时，主要采用特征到特征或预测到预测的消息传递方式，缺乏特征与预测之间的直接交互，限制了模型的性能。",
        "方法": "提出了一种深度统一的CRF显著性模型，通过设计级联CRFs架构与CNN结合，实现特征与预测之间的消息传递，从而在每个尺度上联合优化深度特征和预测，逐步计算最终的优化显著性图。",
        "关键词": [
            "显著目标检测",
            "卷积神经网络",
            "条件随机场",
            "消息传递",
            "多尺度上下文信息"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）用于提取特征；条件随机场（CRF）用于建模特征和预测之间的关系；消息传递机制用于在特征和预测之间传递信息；级联CRFs架构用于逐步优化显著性图。"
    },
    {
        "order": 277,
        "title": "C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.html",
        "abstract": "We propose C3DPO, a method for extracting 3D models of deformable objects from 2D keypoint annotations in unconstrained images. We do so by learning a deep network that reconstructs a 3D object from a single view at a time, accounting for partial occlusions, and explicitly factoring the effects of viewpoint changes and object deformations. In order to achieve this factorization, we introduce a novel regularization technique. We first show that the factorization is successful if, and only if, there exists a certain canonicalization function of the reconstructed shapes. Then, we learn the canonicalization function together with the reconstruction one, which constrains the result to be consistent. We demonstrate state-of-the-art reconstruction results for methods that do not use ground-truth 3D supervision for a number of benchmarks, including Up3D and PASCAL3D+.",
        "中文标题": "C3DPO: 用于非刚性运动结构的规范3D姿态网络",
        "摘要翻译": "我们提出了C3DPO，一种从无约束图像中的2D关键点注释中提取可变形物体3D模型的方法。我们通过学习一个深度网络来实现这一点，该网络一次从单一视角重建3D物体，考虑到部分遮挡，并明确地分解视角变化和物体变形的影响。为了实现这种分解，我们引入了一种新颖的正则化技术。我们首先展示了这种分解成功当且仅当存在重建形状的某种规范化函数。然后，我们学习规范化函数与重建函数一起，这限制了结果的一致性。我们展示了在不使用真实3D监督的情况下，对于包括Up3D和PASCAL3D+在内的多个基准测试，达到了最先进的重建结果。",
        "领域": "3D重建/姿态估计/深度学习",
        "问题": "从2D关键点注释中提取可变形物体的3D模型",
        "动机": "解决在无约束图像中，由于视角变化和物体变形导致的3D重建难题",
        "方法": "学习一个深度网络，该网络从单一视角重建3D物体，并引入一种新颖的正则化技术来分解视角变化和物体变形的影响",
        "关键词": [
            "3D重建",
            "姿态估计",
            "正则化技术"
        ],
        "涉及的技术概念": "C3DPO是一种深度网络，用于从2D关键点注释中重建3D物体模型。它通过考虑部分遮挡和明确分解视角变化与物体变形的影响来实现这一点。引入的正则化技术确保了重建形状的规范化，从而提高了重建结果的准确性和一致性。"
    },
    {
        "order": 278,
        "title": "Looking to Relations for Future Trajectory Forecast",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Looking_to_Relations_for_Future_Trajectory_Forecast_ICCV_2019_paper.html",
        "abstract": "Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on the public benchmark datasets demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods.",
        "中文标题": "关注关系以预测未来轨迹",
        "摘要翻译": "推断道路使用者之间以及道路使用者与其周围物理空间之间的关系行为是有效建模和预测道路场景中参与者采用的导航策略的重要步骤。为此，我们提出了一个关系感知框架来预测未来轨迹。我们的系统旨在从道路使用者之间以及与环境之间的互动中推断关系信息。第一个模块涉及时空特征的视觉编码，它捕捉了人类与人类以及人类与空间随时间变化的互动。接下来的模块明确地从时空互动中构建成对关系，并通过考虑目标道路使用者的过去轨迹来识别对其未来运动有高度影响的更具描述性的关系。生成的关系特征用于预测目标的未来位置，以热图的形式，并额外考虑空间依赖性和不确定性。在公共基准数据集上的广泛评估证明了所提出框架的鲁棒性和有效性，其表现优于最先进的方法。",
        "领域": "自动驾驶/行人轨迹预测/交通行为分析",
        "问题": "如何有效预测道路场景中参与者的未来轨迹",
        "动机": "为了更准确地建模和预测道路场景中参与者的导航策略，需要理解道路使用者之间以及他们与周围环境之间的关系行为",
        "方法": "提出了一个关系感知框架，通过视觉编码捕捉时空特征，构建成对关系，并利用这些关系特征预测未来位置",
        "关键词": [
            "自动驾驶",
            "行人轨迹预测",
            "交通行为分析"
        ],
        "涉及的技术概念": "时空特征视觉编码、成对关系构建、热图预测、空间依赖性和不确定性考虑"
    },
    {
        "order": 279,
        "title": "FACSIMILE: Fast and Accurate Scans From an Image in Less Than a Second",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Smith_FACSIMILE_Fast_and_Accurate_Scans_From_an_Image_in_Less_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Smith_FACSIMILE_Fast_and_Accurate_Scans_From_an_Image_in_Less_ICCV_2019_paper.html",
        "abstract": "Current methods for body shape estimation either lack detail or require many images. They are usually architecturally complex and computationally expensive. We propose FACSIMILE (FAX), a method that estimates a detailed body from a single photo, lowering the bar for creating virtual representations of humans. Our approach is easy to implement and fast to execute, making it easily deployable. FAX uses an image-translation network which recovers geometry at the original resolution of the image. Counterintuitively, the main loss which drives FAX is on per-pixel surface normals instead of per-pixel depth, making it possible to estimate detailed body geometry without any depth supervision. We evaluate our approach both qualitatively and quantitatively, and compare with a state-of-the-art method.",
        "中文标题": "FACSIMILE: 在一秒内从图像中快速准确扫描",
        "摘要翻译": "当前的身体形状估计方法要么缺乏细节，要么需要许多图像。它们通常在架构上复杂且计算成本高。我们提出了FACSIMILE（FAX），一种从单张照片估计详细身体的方法，降低了创建人类虚拟表示的门槛。我们的方法易于实现且执行速度快，使其易于部署。FAX使用图像翻译网络，该网络在图像的原始分辨率下恢复几何形状。反直觉的是，驱动FAX的主要损失是在每像素表面法线上，而不是每像素深度上，这使得无需任何深度监督即可估计详细的身体几何形状。我们定性和定量地评估了我们的方法，并与最先进的方法进行了比较。",
        "领域": "人体建模/图像翻译/几何恢复",
        "问题": "从单张照片中快速准确地估计详细的身体形状",
        "动机": "降低创建人类虚拟表示的门槛，提供一种易于实现且执行速度快的方法",
        "方法": "使用图像翻译网络在图像的原始分辨率下恢复几何形状，主要损失在每像素表面法线上",
        "关键词": [
            "人体建模",
            "图像翻译",
            "几何恢复"
        ],
        "涉及的技术概念": "图像翻译网络：一种用于从一种图像域转换到另一种图像域的神经网络。每像素表面法线：描述图像中每个像素点表面方向的向量，用于估计物体的几何形状。"
    },
    {
        "order": 280,
        "title": "Selectivity or Invariance: Boundary-Aware Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Selectivity_or_Invariance_Boundary-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Su_Selectivity_or_Invariance_Boundary-Aware_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Typically, a salient object detection (SOD) model faces opposite requirements in processing object interiors and boundaries. The features of interiors should be invariant to strong appearance change so as to pop-out the salient object as a whole, while the features of boundaries should be selective to slight appearance change to distinguish salient objects and background. To address this selectivity-invariance dilemma, we propose a novel boundary-aware network with successive dilation for image-based SOD. In this network, the feature selectivity at boundaries is enhanced by incorporating a boundary localization stream, while the feature invariance at interiors is guaranteed with a complex interior perception stream. Moreover, a transition compensation stream is adopted to amend the probable failures in transitional regions between interiors and boundaries. In particular, an integrated successive dilation module is proposed to enhance the feature invariance at interiors and transitional regions. Extensive experiments on six datasets show that the proposed approach outperforms 16 state-of-the-art methods.",
        "中文标题": "选择性或不变性：边界感知的显著目标检测",
        "摘要翻译": "通常，显著目标检测（SOD）模型在处理目标内部和边界时面临相反的要求。内部特征应对强烈外观变化保持不变，以便将显著目标作为一个整体突出显示，而边界特征应对轻微外观变化具有选择性，以区分显著目标和背景。为了解决这种选择性-不变性困境，我们提出了一种新颖的边界感知网络，采用连续扩张进行基于图像的SOD。在该网络中，通过结合边界定位流增强了边界处的特征选择性，而通过复杂的内部感知流保证了内部特征的不变性。此外，采用过渡补偿流来修正内部和边界之间过渡区域可能的失败。特别是，提出了一个集成的连续扩张模块，以增强内部和过渡区域的特征不变性。在六个数据集上的广泛实验表明，所提出的方法优于16种最先进的方法。",
        "领域": "显著目标检测/图像分割/特征学习",
        "问题": "显著目标检测模型在处理目标内部和边界时面临的选择性-不变性困境",
        "动机": "为了解决显著目标检测中内部特征需要不变性而边界特征需要选择性的矛盾",
        "方法": "提出了一种边界感知网络，通过边界定位流增强边界特征选择性，通过内部感知流保证内部特征不变性，并通过过渡补偿流修正过渡区域的失败，同时提出集成连续扩张模块增强内部和过渡区域的特征不变性",
        "关键词": [
            "显著目标检测",
            "边界感知",
            "连续扩张",
            "特征选择性",
            "特征不变性"
        ],
        "涉及的技术概念": "显著目标检测（SOD）是一种识别图像中最吸引人注意的目标的技术。边界感知网络是一种专门设计来处理图像中目标边界和内部特征的网络结构。连续扩张模块是一种用于增强网络对图像内部和过渡区域特征不变性的技术。"
    },
    {
        "order": 281,
        "title": "Learning to Reconstruct 3D Manhattan Wireframes From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Learning_to_Reconstruct_3D_Manhattan_Wireframes_From_a_Single_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Learning_to_Reconstruct_3D_Manhattan_Wireframes_From_a_Single_Image_ICCV_2019_paper.html",
        "abstract": "From a single view of an urban environment, we propose a method to effectively exploit the global structural regularities for obtaining a compact, accurate, and intuitive 3D wireframe representation. Our method trains a single convolutional neural network to simultaneously detect salient junctions and straight lines, as well as predict their 3D depth and vanishing points. Compared with state-of-the-art learning-based wireframe detection methods, our network is much simpler and more unified, leading to better 2D wireframe detection. With a global structural prior (such as Manhattan assumption), our method further reconstructs a full 3D wireframe model, a compact vector representation suitable for a variety of high-level vision tasks such as AR and CAD. We conduct extensive evaluations of our method on a large new synthetic dataset of urban scenes as well as real images. Our code and datasets will be published along with the paper.",
        "中文标题": "从单张图像学习重建3D曼哈顿线框",
        "摘要翻译": "从城市环境的单一视角出发，我们提出了一种方法，有效利用全局结构规律性，以获得紧凑、准确且直观的3D线框表示。我们的方法训练一个单一的卷积神经网络，同时检测显著的交点和直线，以及预测它们的3D深度和消失点。与最先进的基于学习的线框检测方法相比，我们的网络更简单、更统一，从而实现了更好的2D线框检测。通过全局结构先验（如曼哈顿假设），我们的方法进一步重建了一个完整的3D线框模型，这是一种适用于各种高级视觉任务（如增强现实和计算机辅助设计）的紧凑矢量表示。我们在一个大型新的城市场景合成数据集以及真实图像上对我们的方法进行了广泛的评估。我们的代码和数据集将随论文一起发布。",
        "领域": "3D重建/增强现实/计算机辅助设计",
        "问题": "从单张图像中重建3D曼哈顿线框",
        "动机": "为了从单一视角的城市环境图像中，有效利用全局结构规律性，获得紧凑、准确且直观的3D线框表示，以支持高级视觉任务如增强现实和计算机辅助设计。",
        "方法": "训练一个单一的卷积神经网络，同时检测显著的交点和直线，以及预测它们的3D深度和消失点，并利用全局结构先验（如曼哈顿假设）重建完整的3D线框模型。",
        "关键词": [
            "3D重建",
            "增强现实",
            "计算机辅助设计"
        ],
        "涉及的技术概念": "卷积神经网络用于检测显著的交点和直线，以及预测3D深度和消失点；全局结构先验（如曼哈顿假设）用于重建3D线框模型。"
    },
    {
        "order": 282,
        "title": "Anchor Diffusion for Unsupervised Video Object Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Anchor_Diffusion_for_Unsupervised_Video_Object_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Anchor_Diffusion_for_Unsupervised_Video_Object_Segmentation_ICCV_2019_paper.html",
        "abstract": "Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approach tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators, we introduce a technique to establish dense correspondences between pixel embeddings of a reference \"anchor\" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of 81.7%, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the video saliency dataset ViSal, showing results competitive with the state of the art.",
        "中文标题": "无监督视频对象分割的锚点扩散",
        "摘要翻译": "无监督视频对象分割通常通过基于循环神经网络和光流的方法来解决。尽管这些方法复杂，但它们往往倾向于短期时间依赖性，因此容易积累不准确性，导致随时间推移的漂移。此外，单独的简单（静态）图像分割模型可以与这些方法竞争，这进一步表明应该重新考虑时间依赖性的建模方式。基于这些观察，本文探索了简单而有效的策略来建模长期时间依赖性。受非局部操作符的启发，我们引入了一种技术，在参考“锚点”帧和当前帧的像素嵌入之间建立密集对应关系。这使得可以在不依赖于中间帧的情况下学习任意长距离的成对依赖性。在没有在线监督的情况下，我们的方法可以在具有挑战性的场景中抑制背景并精确分割前景对象，同时保持随时间推移的一致性能。我们的方法在DAVIS-2016无监督方法排行榜上以81.7%的平均IoU排名第一，同时仍然与最先进的在线半监督方法竞争。我们进一步在FBMS数据集和视频显著性数据集ViSal上评估了我们的方法，显示了与最先进技术竞争的结果。",
        "领域": "视频对象分割/时间依赖性建模/像素嵌入",
        "问题": "解决无监督视频对象分割中时间依赖性建模的问题",
        "动机": "现有方法倾向于短期时间依赖性，容易积累不准确性，导致漂移，需要重新考虑时间依赖性的建模方式",
        "方法": "引入一种技术，在参考“锚点”帧和当前帧的像素嵌入之间建立密集对应关系，以学习任意长距离的成对依赖性",
        "关键词": [
            "无监督学习",
            "视频对象分割",
            "时间依赖性",
            "像素嵌入",
            "非局部操作符"
        ],
        "涉及的技术概念": "循环神经网络、光流、图像分割模型、非局部操作符、像素嵌入、密集对应关系、成对依赖性、在线监督、平均IoU、DAVIS-2016、FBMS数据集、ViSal数据集"
    },
    {
        "order": 283,
        "title": "Online Unsupervised Learning of the 3D Kinematic Structure of Arbitrary Rigid Bodies",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nunes_Online_Unsupervised_Learning_of_the_3D_Kinematic_Structure_of_Arbitrary_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nunes_Online_Unsupervised_Learning_of_the_3D_Kinematic_Structure_of_Arbitrary_ICCV_2019_paper.html",
        "abstract": "This work addresses the problem of 3D kinematic structure learning of arbitrary articulated rigid bodies from RGB-D data sequences. Typically, this problem is addressed by offline methods that process a batch of frames, assuming that complete point trajectories are available. However, this approach is not feasible when considering scenarios that require continuity and fluidity, for instance, human-robot interaction. In contrast, we propose to tackle this problem in an online unsupervised fashion, by recursively maintaining the metric distance of the scene's 3D structure, while achieving real-time performance. The influence of noise is mitigated by building a similarity measure based on a linear embedding representation and incorporating this representation into the original metric distance. The kinematic structure is then estimated based on a combination of implicit motion and spatial properties. The proposed approach achieves competitive performance both quantitatively and qualitatively in terms of estimation accuracy, even compared to offline methods.",
        "中文标题": "在线无监督学习任意刚体的3D运动结构",
        "摘要翻译": "本工作解决了从RGB-D数据序列中学习任意铰接刚体的3D运动结构的问题。通常，这个问题是通过离线方法处理的，这些方法处理一批帧，假设完整的点轨迹是可用的。然而，当考虑到需要连续性和流畅性的场景时，例如人机交互，这种方法并不可行。相比之下，我们提出以在线无监督的方式解决这个问题，通过递归地维持场景3D结构的度量距离，同时实现实时性能。通过构建基于线性嵌入表示的相似性度量并将此表示纳入原始度量距离中，减轻了噪声的影响。然后，基于隐式运动和空间属性的组合估计运动结构。所提出的方法在估计准确性方面，无论是定量还是定性，都达到了与离线方法相竞争的性能。",
        "领域": "3D重建/运动分析/人机交互",
        "问题": "从RGB-D数据序列中学习任意铰接刚体的3D运动结构",
        "动机": "解决需要连续性和流畅性的场景（如人机交互）中，离线方法不可行的问题",
        "方法": "在线无监督方式，递归维持场景3D结构的度量距离，构建基于线性嵌入表示的相似性度量，结合隐式运动和空间属性估计运动结构",
        "关键词": [
            "3D运动结构",
            "RGB-D数据",
            "在线无监督学习"
        ],
        "涉及的技术概念": "RGB-D数据序列指的是包含颜色（RGB）和深度（D）信息的数据序列。线性嵌入表示是一种将高维数据映射到低维空间的技术，用于构建相似性度量。隐式运动指的是不直接观察到的运动，通过其他属性推断出来。"
    },
    {
        "order": 284,
        "title": "Delving Deep Into Hybrid Annotations for 3D Human Recovery in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rong_Delving_Deep_Into_Hybrid_Annotations_for_3D_Human_Recovery_in_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rong_Delving_Deep_Into_Hybrid_Annotations_for_3D_Human_Recovery_in_ICCV_2019_paper.html",
        "abstract": "Though much progress has been achieved in single-image 3D human recovery, estimating 3D model for in-the-wild images remains a formidable challenge. The reason lies in the fact that obtaining high-quality 3D annotations for in-the-wild images is an extremely hard task that consumes enormous amount of resources and manpower. To tackle this problem, previous methods adopt a hybrid training strategy that exploits multiple heterogeneous types of annotations including 3D and 2D while leaving the efficacy of each annotation not thoroughly investigated. In this work, we aim to perform a comprehensive study on cost and effectiveness trade-off between different annotations. Specifically, we focus on the challenging task of in-the-wild 3D human recovery from single images when paired 3D annotations are not fully available. Through extensive experiments, we obtain several observations: 1) 3D annotations are efficient, whereas traditional 2D annotations such as 2D keypoints and body part segmentation are less competent in guiding 3D human recovery. 2) Dense Correspondence such as DensePose is effective. When there are no paired in-the-wild 3D annotations available, the model exploiting dense correspondence can achieve 92% of the performance compared to a model trained with paired 3D data. We show that incorporating dense correspondence into in-the-wild 3D human recovery is promising and competitive due to its high efficiency and relatively low annotating cost. Our model trained with dense correspondence can serve as a strong reference for future research.",
        "中文标题": "深入探讨混合注释在野外3D人体恢复中的应用",
        "摘要翻译": "尽管在单图像3D人体恢复方面取得了很大进展，但对于野外图像的3D模型估计仍然是一个巨大的挑战。原因在于，为野外图像获取高质量的3D注释是一项极其困难的任务，需要消耗大量的资源和人力。为了解决这个问题，以前的方法采用了一种混合训练策略，利用包括3D和2D在内的多种异质类型注释，但并未彻底研究每种注释的效果。在这项工作中，我们旨在对不同注释之间的成本和效果进行全面的研究。具体来说，我们专注于在没有完全配对的3D注释的情况下，从单图像中进行野外3D人体恢复的挑战性任务。通过大量实验，我们获得了几个观察结果：1）3D注释是有效的，而传统的2D注释（如2D关键点和身体部位分割）在指导3D人体恢复方面能力较弱。2）密集对应（如DensePose）是有效的。当没有配对的野外3D注释可用时，利用密集对应的模型可以达到与使用配对3D数据训练的模型相比92%的性能。我们展示了将密集对应纳入野外3D人体恢复中是有前途和竞争力的，因为它具有高效率和相对较低的注释成本。我们使用密集对应训练的模型可以作为未来研究的强有力参考。",
        "领域": "3D人体恢复/密集对应/混合训练策略",
        "问题": "在野外图像中估计3D人体模型",
        "动机": "解决为野外图像获取高质量3D注释的困难，研究不同注释类型在3D人体恢复中的成本和效果",
        "方法": "采用混合训练策略，利用包括3D和2D在内的多种异质类型注释，并特别研究密集对应在3D人体恢复中的应用",
        "关键词": [
            "3D人体恢复",
            "密集对应",
            "混合训练策略"
        ],
        "涉及的技术概念": "3D注释、2D注释（如2D关键点和身体部位分割）、密集对应（如DensePose）"
    },
    {
        "order": 285,
        "title": "Soft Rasterizer: A Differentiable Renderer for Image-Based 3D Reasoning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Soft_Rasterizer_A_Differentiable_Renderer_for_Image-Based_3D_Reasoning_ICCV_2019_paper.html",
        "abstract": "Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence able to be learned. Unlike the state-of-the-art differentiable renderers, which only approximate the rendering gradient in the back propagation, we propose a truly differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervision signals to mesh vertices and their attributes from various forms of image representations, including silhouette, shading and color images. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and far-range vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach is able to handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renderers. Code is available at https://github.com/ShichenLiu/SoftRas.",
        "中文标题": "软光栅化器：一种用于基于图像的3D推理的可微分渲染器",
        "摘要翻译": "渲染通过模拟图像形成的物理过程，架起了2D视觉与3D场景之间的桥梁。通过反转这样的渲染器，可以考虑一种从2D图像推断3D信息的学习方法。然而，标准的图形渲染器涉及一个称为光栅化的基本离散化步骤，这阻止了渲染过程的可微分性，从而无法被学习。与仅在后向传播中近似渲染梯度的最先进的可微分渲染器不同，我们提出了一种真正可微分的渲染框架，该框架能够（1）直接使用可微分函数渲染彩色网格，以及（2）从各种形式的图像表示（包括轮廓、阴影和彩色图像）向后传播有效的监督信号到网格顶点及其属性。我们框架的关键是一种新颖的公式，该公式将渲染视为一种聚合函数，该函数融合了所有网格三角形相对于渲染像素的概率贡献。这种公式使我们的框架能够将梯度流向被遮挡和远距离的顶点，这是以前的最先进技术无法实现的。我们展示了通过使用所提出的渲染器，可以在3D无监督单视图重建方面实现显著的质和量上的改进。实验还表明，我们的方法能够处理基于图像的形状拟合中的挑战性任务，这对现有的可微分渲染器来说仍然不简单。代码可在https://github.com/ShichenLiu/SoftRas获取。",
        "领域": "3D重建/可微分渲染/图像处理",
        "问题": "解决标准图形渲染器中光栅化步骤导致的不可微分性问题，从而实现从2D图像到3D信息的有效学习",
        "动机": "为了从2D图像中推断3D信息，需要一种能够进行有效学习的渲染方法，而现有的渲染器由于光栅化步骤的不可微分性，无法满足这一需求",
        "方法": "提出了一种真正可微分的渲染框架，通过将渲染视为一种聚合函数，融合所有网格三角形相对于渲染像素的概率贡献，从而实现梯度流向被遮挡和远距离的顶点",
        "关键词": [
            "3D重建",
            "可微分渲染",
            "图像处理"
        ],
        "涉及的技术概念": "渲染、光栅化、可微分渲染、3D重建、图像处理、网格三角形、概率贡献、梯度流向"
    },
    {
        "order": 286,
        "title": "Few-Shot Generalization for Single-Image 3D Reconstruction via Priors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wallace_Few-Shot_Generalization_for_Single-Image_3D_Reconstruction_via_Priors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wallace_Few-Shot_Generalization_for_Single-Image_3D_Reconstruction_via_Priors_ICCV_2019_paper.html",
        "abstract": "Recent work on single-view 3D reconstruction shows impressive results, but has been restricted to a few fixed categories where extensive training data is available. The problem of generalizing these models to new classes with limited training data is largely open. To address this problem, we present a new model architecture that reframes single-view 3D reconstruction as learnt, category agnostic refinement of a provided, category-specific prior. The provided prior shape for a novel class can be obtained from as few as one 3D shape from this class. Our model can start reconstructing objects from the novel class using this prior without seeing any training image for this class and without any retraining. Our model outperforms category-agnostic baselines and remains competitive with more sophisticated baselines that finetune on the novel categories. Additionally, our network is capable of improving the reconstruction given multiple views despite not being trained on task of multi-view reconstruction.",
        "中文标题": "通过先验实现单图像3D重建的少样本泛化",
        "摘要翻译": "最近关于单视图3D重建的研究展示了令人印象深刻的结果，但这些研究仅限于少数几个有大量训练数据的固定类别。将这些模型泛化到具有有限训练数据的新类别的问题在很大程度上仍未解决。为了解决这个问题，我们提出了一种新的模型架构，该架构将单视图3D重建重新定义为对提供的类别特定先验的学习、类别无关的细化。对于新类别的先验形状可以从该类别的仅一个3D形状中获得。我们的模型可以开始使用这个先验重建新类别的对象，而无需看到该类别的任何训练图像，也无需进行任何重新训练。我们的模型优于类别无关的基线，并且与在新类别上进行微调的更复杂的基线保持竞争力。此外，尽管我们的网络没有在多视图重建任务上进行训练，但它能够在给定多个视图的情况下改进重建。",
        "领域": "3D重建/少样本学习/深度学习",
        "问题": "如何将单视图3D重建模型泛化到具有有限训练数据的新类别",
        "动机": "现有的单视图3D重建模型仅限于少数几个有大量训练数据的固定类别，缺乏泛化到新类别的能力",
        "方法": "提出了一种新的模型架构，通过学习和类别无关的细化，利用类别特定的先验进行单视图3D重建",
        "关键词": [
            "3D重建",
            "少样本学习",
            "深度学习"
        ],
        "涉及的技术概念": "单视图3D重建、类别特定先验、少样本学习、模型泛化、多视图重建"
    },
    {
        "order": 287,
        "title": "Learnable Triangulation of Human Pose",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Iskakov_Learnable_Triangulation_of_Human_Pose_ICCV_2019_paper.html",
        "abstract": "We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The first (baseline) solution is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images. The second, more complex, solution is based on volumetric aggregation of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps. Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multi-view state of the art on the Human3.6M dataset.",
        "中文标题": "可学习的人体姿态三角测量",
        "摘要翻译": "我们提出了两种基于新的可学习三角测量方法的多视角3D人体姿态估计解决方案，这些方法结合了来自多个2D视角的3D信息。第一个（基线）解决方案是一个基本的可微分代数三角测量，增加了从输入图像估计的置信度权重。第二个更复杂的解决方案基于从2D骨干网络提取的2D特征图的体积聚合，然后通过3D卷积进行细化，生成最终的3D关节热图。关键的是，这两种方法都是端到端可微分的，这使我们能够直接优化目标度量。我们展示了解决方案在数据集间的可转移性，并在Human3.6M数据集上显著改进了多视角的最新技术。",
        "领域": "3D人体姿态估计/多视角视觉/深度学习",
        "问题": "多视角3D人体姿态估计",
        "动机": "提高多视角3D人体姿态估计的准确性和效率",
        "方法": "采用两种新的可学习三角测量方法，包括基本的可微分代数三角测量和基于2D特征图体积聚合的复杂方法，并通过3D卷积进行细化",
        "关键词": [
            "3D人体姿态估计",
            "多视角视觉",
            "可学习三角测量",
            "3D卷积"
        ],
        "涉及的技术概念": "可学习三角测量方法结合了多个2D视角的3D信息，通过可微分代数三角测量和2D特征图的体积聚合，以及3D卷积的细化，实现端到端的优化，提高3D人体姿态估计的准确性。"
    },
    {
        "order": 288,
        "title": "Human Mesh Recovery From Monocular Images via a Skeleton-Disentangled Representation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Human_Mesh_Recovery_From_Monocular_Images_via_a_Skeleton-Disentangled_Representation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Human_Mesh_Recovery_From_Monocular_Images_via_a_Skeleton-Disentangled_Representation_ICCV_2019_paper.html",
        "abstract": "We describe an end-to-end method for recovering 3D human body mesh from single images and monocular videos. Different from the existing methods try to obtain all the complex 3D pose, shape, and camera parameters from one coupling feature, we propose a skeleton-disentangling based framework, which divides this task into multi-level spatial and temporal granularity in a decoupling manner. In spatial, we propose an effective and pluggable \"disentangling the skeleton from the details\" (DSD) module. It reduces the complexity and decouples the skeleton, which lays a good foundation for temporal modeling. In temporal, the self-attention based temporal convolution network is proposed to efficiently exploit the short and long-term temporal cues. Furthermore, an unsupervised adversarial training strategy, temporal shuffles and order recovery, is designed to promote the learning of motion dynamics. The proposed method outperforms the state-of-the-art 3D human mesh recovery methods by 15.4% MPJPE and 23.8% PA-MPJPE on Human3.6M. State-of-the-art results are also achieved on the 3D pose in the wild (3DPW) dataset without any fine-tuning. Especially, ablation studies demonstrate that skeleton-disentangled representation is crucial for better temporal modeling and generalization.",
        "中文标题": "从单目图像通过骨架解耦表示恢复人体网格",
        "摘要翻译": "我们描述了一种从单张图像和单目视频中恢复3D人体网格的端到端方法。与现有方法试图从一个耦合特征中获取所有复杂的3D姿态、形状和相机参数不同，我们提出了一个基于骨架解耦的框架，该框架以解耦的方式将此任务划分为多层次的空间和时间粒度。在空间上，我们提出了一个有效且可插拔的“从细节中解耦骨架”（DSD）模块。它降低了复杂性并解耦了骨架，为时间建模奠定了良好的基础。在时间上，提出了基于自注意力的时间卷积网络，以有效利用短期和长期的时间线索。此外，设计了一种无监督的对抗训练策略，即时间洗牌和顺序恢复，以促进运动动态的学习。所提出的方法在Human3.6M上比最先进的3D人体网格恢复方法在MPJPE和PA-MPJPE上分别提高了15.4%和23.8%。在3D pose in the wild (3DPW)数据集上也达到了最先进的结果，无需任何微调。特别是，消融研究表明，骨架解耦表示对于更好的时间建模和泛化至关重要。",
        "领域": "3D人体重建/时间序列分析/自注意力机制",
        "问题": "从单目图像和视频中恢复3D人体网格",
        "动机": "现有方法试图从一个耦合特征中获取所有复杂的3D姿态、形状和相机参数，这增加了任务的复杂性。",
        "方法": "提出了一个基于骨架解耦的框架，包括空间上的DSD模块和时间上的自注意力时间卷积网络，以及无监督的对抗训练策略。",
        "关键词": [
            "3D人体重建",
            "时间序列分析",
            "自注意力机制",
            "骨架解耦",
            "无监督学习"
        ],
        "涉及的技术概念": "骨架解耦表示（Skeleton-Disentangled Representation）、自注意力机制（Self-Attention Mechanism）、时间卷积网络（Temporal Convolutional Network）、无监督对抗训练（Unsupervised Adversarial Training）、MPJPE（Mean Per Joint Position Error）、PA-MPJPE（Procrustes Aligned Mean Per Joint Position Error）"
    },
    {
        "order": 289,
        "title": "Digging Into Self-Supervised Monocular Depth Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.html",
        "abstract": "Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.",
        "中文标题": "深入自监督单目深度估计",
        "摘要翻译": "逐像素的真实深度数据在大规模获取上具有挑战性。为了克服这一限制，自监督学习已成为训练模型执行单目深度估计的有前途的替代方案。在本文中，我们提出了一系列改进措施，这些措施共同作用，使得与竞争的自监督方法相比，深度图在数量和质量上都有所提高。自监督单目训练的研究通常探索越来越复杂的架构、损失函数和图像形成模型，所有这些最近都有助于缩小与完全监督方法的差距。我们展示了一个令人惊讶的简单模型及其相关的设计选择，导致了优越的预测。特别是，我们提出了（i）最小重投影损失，旨在鲁棒地处理遮挡，（ii）全分辨率多尺度采样方法，减少视觉伪影，以及（iii）自动掩码损失，以忽略违反相机运动假设的训练像素。我们分别展示了每个组件的有效性，并在KITTI基准上展示了高质量、最先进的结果。",
        "领域": "深度估计/自监督学习/计算机视觉",
        "问题": "大规模获取逐像素的真实深度数据的挑战",
        "动机": "克服获取真实深度数据的限制，提高单目深度估计的准确性和质量",
        "方法": "提出最小重投影损失、全分辨率多尺度采样方法和自动掩码损失，以改进自监督单目深度估计",
        "关键词": [
            "深度估计",
            "自监督学习",
            "单目视觉"
        ],
        "涉及的技术概念": "自监督学习是一种不需要大量标注数据就能训练模型的方法，通过模型自身生成监督信号。单目深度估计是指从单一视角的图像中估计场景的深度信息。最小重投影损失是一种用于处理图像中遮挡问题的损失函数。全分辨率多尺度采样方法用于减少深度估计中的视觉伪影。自动掩码损失用于在训练过程中自动忽略那些违反相机运动假设的像素，以提高模型的鲁棒性。"
    },
    {
        "order": 290,
        "title": "xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.html",
        "abstract": "We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm. away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset -- xR-EgoPose -- offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.",
        "中文标题": "xR-EgoPose: 从HMD相机获取的自我中心3D人体姿态",
        "摘要翻译": "我们提出了一种新的解决方案，用于从安装在头戴式虚拟现实设备边缘的向下看的鱼眼相机捕获的单目图像中进行自我中心3D人体姿态估计。这种不寻常的视角，距离用户的脸仅2厘米，导致图像具有独特的视觉外观，其特征在于严重的自遮挡和强烈的透视失真，从而导致下体和上体之间的分辨率差异极大。我们的贡献有两个方面。首先，我们提出了一种新的编码器-解码器架构，具有专门设计的新颖双分支解码器，以考虑2D关节位置的不确定性变化。我们在合成和真实世界数据集上的定量评估表明，我们的策略在准确性上比现有的自我中心姿态估计方法有显著提高。我们的第二个贡献是一个新的大规模逼真合成数据集——xR-EgoPose——提供了383K帧高质量渲染的人物图像，这些人物具有多样化的肤色、体型、服装，在各种背景和光照条件下进行一系列动作。我们的实验表明，我们新的合成训练语料库中的高变异性导致了对真实世界镜头的良好泛化，并在具有地面实况的真实世界数据集上达到了最先进的结果。此外，在Human3.6M基准上的评估表明，我们的方法在从第三人称视角进行3D人体姿态估计这一更经典问题上的性能与顶级方法相当。",
        "领域": "虚拟现实/人体姿态估计/计算机图形学",
        "问题": "从安装在头戴式虚拟现实设备上的鱼眼相机捕获的单目图像中进行自我中心3D人体姿态估计",
        "动机": "解决由于相机视角接近用户脸部导致的图像自遮挡和透视失真问题，提高姿态估计的准确性",
        "方法": "提出了一种新的编码器-解码器架构，具有专门设计的新颖双分支解码器，以考虑2D关节位置的不确定性变化，并创建了一个大规模逼真合成数据集xR-EgoPose用于训练和评估",
        "关键词": [
            "自我中心视角",
            "3D人体姿态估计",
            "鱼眼相机",
            "虚拟现实",
            "合成数据集"
        ],
        "涉及的技术概念": "编码器-解码器架构、双分支解码器、2D关节位置不确定性、自遮挡、透视失真、合成数据集、Human3.6M基准"
    },
    {
        "order": 291,
        "title": "Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture From Images \"In the Wild\"",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zuffi_Three-D_Safari_Learning_to_Estimate_Zebra_Pose_Shape_and_Texture_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zuffi_Three-D_Safari_Learning_to_Estimate_Zebra_Pose_Shape_and_Texture_ICCV_2019_paper.html",
        "abstract": "We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other. To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. Learning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision. Moreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. We show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture.",
        "中文标题": "三维野生动物园：从野外图像中学习估计斑马的姿态、形状和纹理",
        "摘要翻译": "我们提出了第一种从野外获取的图像中自动进行动物三维姿态、形状和纹理捕捉的方法。特别是，我们专注于从一系列图像中捕捉关于格氏斑马的三维信息。格氏斑马是非洲最濒危的物种之一，仅剩下几千只。捕捉这些动物的形状和姿态可以为生物学家和环保主义者提供关于动物健康和行为的信息。与人类姿态、形状和纹理估计的研究相比，濒危物种的训练数据有限，动物处于复杂的自然场景中，存在遮挡，它们自然伪装，成群结队，彼此相似。为了克服这些挑战，我们将最新的SMAL动物模型集成到基于网络的回归流程中，我们在具有姿态、形状和背景变化的合成生成图像上端到端地训练该流程。超越人类形状和姿态估计的最先进方法，我们的方法在训练过程中学习斑马的形状空间。仅使用光度损失从图像中学习这样的形状空间是新颖的，该方法可以用于在有限的三维监督下学习其他设置中的形状。此外，我们将三维姿态和形状预测与纹理合成任务结合起来，从单张图像中获得动物的完整纹理图。我们展示了预测的纹理图允许对网络特征进行新颖的每实例无监督优化。这种方法，SMALST（具有学习形状和纹理的SMAL），超越了以前的工作，以前的工作假设手动关键点和/或分割，直接从像素回归到三维动物形状、姿态和纹理。",
        "领域": "三维重建/动物行为分析/纹理合成",
        "问题": "从野外图像中自动捕捉动物的三维姿态、形状和纹理",
        "动机": "为生物学家和环保主义者提供关于濒危动物健康和行为的信息",
        "方法": "集成SMAL动物模型到基于网络的回归流程中，使用合成生成的图像进行端到端训练，学习斑马的形状空间，并结合三维姿态和形状预测与纹理合成任务",
        "关键词": [
            "三维重建",
            "动物行为分析",
            "纹理合成"
        ],
        "涉及的技术概念": "SMAL动物模型、基于网络的回归流程、光度损失、三维姿态和形状预测、纹理合成、无监督优化"
    },
    {
        "order": 292,
        "title": "Tracking Without Bells and Whistles",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bergmann_Tracking_Without_Bells_and_Whistles_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bergmann_Tracking_Without_Bells_and_Whistles_ICCV_2019_paper.html",
        "abstract": "The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.",
        "中文标题": "无需花哨的追踪",
        "摘要翻译": "在视频序列中追踪多个对象的问题提出了几项具有挑战性的任务。对于通过检测进行追踪，这些任务包括对象重新识别、运动预测和处理遮挡。我们提出了一种（无需花哨的）追踪器，它无需特别针对这些任务中的任何一项即可完成追踪，特别是，我们没有对追踪数据进行任何训练或优化。为此，我们利用对象检测器的边界框回归来预测对象在下一帧中的位置，从而将检测器转换为追踪器。我们展示了追踪器的潜力，并通过简单的重新识别和相机运动补偿扩展，在三个多对象追踪基准上提供了新的最先进技术。然后，我们对几种最先进的追踪方法的性能和失败案例进行了分析，与我们的追踪器进行比较。令人惊讶的是，在应对复杂追踪场景（即小对象、遮挡对象或缺失检测）时，没有一种专门的追踪方法明显更好。然而，我们的方法解决了大多数简单的追踪场景。因此，我们激励我们的方法作为一种新的追踪范式，并指出了有前途的未来研究方向。总体而言，追踪器比任何当前的追踪方法都提供了优越的追踪性能，我们的分析揭示了剩余和未解决的追踪挑战，以激发未来的研究方向。",
        "领域": "多对象追踪/视频分析/对象检测",
        "问题": "在视频序列中追踪多个对象，包括对象重新识别、运动预测和处理遮挡",
        "动机": "提出一种无需特别针对追踪任务中的任何一项即可完成追踪的方法，无需对追踪数据进行任何训练或优化",
        "方法": "利用对象检测器的边界框回归来预测对象在下一帧中的位置，将检测器转换为追踪器，并通过简单的重新识别和相机运动补偿扩展",
        "关键词": [
            "多对象追踪",
            "对象检测",
            "边界框回归"
        ],
        "涉及的技术概念": {
            "追踪器": "一种无需特别针对追踪任务中的任何一项即可完成追踪的方法",
            "边界框回归": "一种预测对象在下一帧中位置的技术",
            "重新识别": "在视频序列中识别和追踪特定对象的过程",
            "相机运动补偿": "调整视频序列以补偿相机运动的技术"
        }
    },
    {
        "order": 293,
        "title": "Learning Object-Specific Distance From a Monocular Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Learning_Object-Specific_Distance_From_a_Monocular_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Learning_Object-Specific_Distance_From_a_Monocular_Image_ICCV_2019_paper.html",
        "abstract": "Environment perception, including object detection and distance estimation, is one of the most crucial tasks for autonomous driving. Many attentions have been paid on the object detection task, but distance estimation only arouse few interests in the computer vision community. Observing that the traditional inverse perspective mapping algorithm performs poorly for objects far away from the camera or on the curved road, in this paper, we address the challenging distance estimation problem by developing the first end-to-end learning-based model to directly predict distances for given objects in the images. Besides the introduction of a learning-based base model, we further design an enhanced model with a keypoint regressor, where a projection loss is defined to enforce a better distance estimation, especially for objects close to the camera. To facilitate the research on this task, we construct the extented KITTI and nuScenes (mini) object detection datasets with a distance for each object. Our experiments demonstrate that our proposed methods outperform alternative approaches (e.g., the traditional IPM, SVR) on object-specific distance estimation, particularly for the challenging cases that objects are on a curved road. Moreover, the performance margin implies the effectiveness of our enhanced method.",
        "中文标题": "从单目图像中学习物体特定距离",
        "摘要翻译": "环境感知，包括物体检测和距离估计，是自动驾驶中最关键的任务之一。物体检测任务已经引起了广泛关注，但距离估计在计算机视觉社区中仅引起了少量兴趣。观察到传统的逆透视映射算法对于远离相机或在弯曲道路上的物体表现不佳，本文通过开发第一个端到端的学习模型来直接预测图像中给定物体的距离，解决了这一具有挑战性的距离估计问题。除了引入基于学习的基础模型外，我们进一步设计了一个带有关键点回归器的增强模型，其中定义了一个投影损失以强制执行更好的距离估计，特别是对于靠近相机的物体。为了促进这一任务的研究，我们构建了扩展的KITTI和nuScenes（迷你）物体检测数据集，每个物体都有距离。我们的实验表明，我们提出的方法在物体特定距离估计上优于其他方法（例如，传统的IPM，SVR），特别是在物体位于弯曲道路上的挑战性情况下。此外，性能差距暗示了我们增强方法的有效性。",
        "领域": "自动驾驶/物体检测/距离估计",
        "问题": "解决从单目图像中准确估计物体距离的问题，特别是在物体远离相机或位于弯曲道路上的挑战性情况下。",
        "动机": "传统的逆透视映射算法在物体远离相机或位于弯曲道路上时表现不佳，因此需要开发更有效的距离估计方法。",
        "方法": "开发了一个端到端的学习模型来直接预测图像中给定物体的距离，并设计了一个带有关键点回归器的增强模型，通过定义投影损失来改善距离估计。",
        "关键词": [
            "自动驾驶",
            "物体检测",
            "距离估计",
            "端到端学习",
            "关键点回归器",
            "投影损失"
        ],
        "涉及的技术概念": "逆透视映射算法（IPM）、端到端学习模型、关键点回归器、投影损失、KITTI数据集、nuScenes数据集。"
    },
    {
        "order": 294,
        "title": "DeepHuman: 3D Human Reconstruction From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_DeepHuman_3D_Human_Reconstruction_From_a_Single_Image_ICCV_2019_paper.html",
        "abstract": "We propose DeepHuman, an image-guided volume-to-volume translation CNN for 3D human reconstruction from a single RGB image. To reduce the ambiguities associated with the reconstruction of invisible areas, our method leverages a dense semantic representation generated from SMPL model as an additional input. One key feature of our network is that it fuses different scales of image features into the 3D space through volumetric feature transformation, which helps to recover accurate surface geometry. The surface details are further refined through a normal refinement network, which can be concatenated with the volume generation network using our proposed volumetric normal projection layer. We also contribute THuman, a 3D real-world human model dataset containing approximately 7000 models. The network is trained using training data generated from the dataset. Overall, due to the specific design of our network and the diversity in our dataset, our method enables 3D human model estimation given only a single image and outperforms state-of-the-art approaches.",
        "中文标题": "DeepHuman: 从单张图像进行3D人体重建",
        "摘要翻译": "我们提出了DeepHuman，一种用于从单张RGB图像进行3D人体重建的图像引导的体积到体积转换CNN。为了减少与重建不可见区域相关的不确定性，我们的方法利用了从SMPL模型生成的密集语义表示作为额外输入。我们网络的一个关键特征是通过体积特征变换将不同尺度的图像特征融合到3D空间中，这有助于恢复准确的表面几何形状。表面细节通过法线细化网络进一步细化，该网络可以通过我们提出的体积法线投影层与体积生成网络连接。我们还贡献了THuman，一个包含大约7000个模型的3D真实世界人体模型数据集。网络使用从该数据集生成的训练数据进行训练。总体而言，由于我们网络的特定设计和数据集的多样性，我们的方法能够在仅给定单张图像的情况下进行3D人体模型估计，并优于最先进的方法。",
        "领域": "3D重建/人体建模/深度学习",
        "问题": "从单张RGB图像进行3D人体重建",
        "动机": "减少与重建不可见区域相关的不确定性，提高3D人体模型估计的准确性",
        "方法": "利用SMPL模型生成的密集语义表示作为额外输入，通过体积特征变换融合不同尺度的图像特征到3D空间，使用法线细化网络进一步细化表面细节",
        "关键词": [
            "3D重建",
            "人体建模",
            "深度学习",
            "体积特征变换",
            "法线细化"
        ],
        "涉及的技术概念": "SMPL模型是一种用于人体形状和姿态建模的参数化模型，能够生成密集的语义表示。体积到体积转换CNN是一种深度学习网络，用于将2D图像特征转换为3D体积数据。体积特征变换是一种技术，用于将不同尺度的2D图像特征融合到3D空间中。法线细化网络用于进一步细化3D模型的表面细节。体积法线投影层是一种网络层，用于将法线细化网络与体积生成网络连接。THuman是一个包含大量3D真实世界人体模型的数据集，用于训练和验证3D重建算法。"
    },
    {
        "order": 295,
        "title": "Object-Driven Multi-Layer Scene Decomposition From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dhamo_Object-Driven_Multi-Layer_Scene_Decomposition_From_a_Single_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dhamo_Object-Driven_Multi-Layer_Scene_Decomposition_From_a_Single_Image_ICCV_2019_paper.html",
        "abstract": "We present a method that tackles the challenge of predicting color and depth behind the visible content of an image. Our approach aims at building up a Layered Depth Image (LDI) from a single RGB input, which is an efficient representation that arranges the scene in layers, including originally occluded regions. Unlike previous work, we enable an adaptive scheme for the number of layers and incorporate semantic encoding for better hallucination of partly occluded objects. Additionally, our approach is object-driven, which especially boosts the accuracy for the occluded intermediate objects. The framework consists of two steps. First, we individually complete each object in terms of color and depth, while estimating the scene layout. Second, we rebuild the scene based on the regressed layers and enforce the recomposed image to resemble the structure of the original input. The learned representation enables various applications, such as 3D photography and diminished reality, all from a single RGB image.",
        "中文标题": "从单张图像进行对象驱动的多层场景分解",
        "摘要翻译": "我们提出了一种方法，旨在解决预测图像可见内容背后的颜色和深度的挑战。我们的方法旨在从单个RGB输入构建分层深度图像（LDI），这是一种将场景按层排列的高效表示，包括最初被遮挡的区域。与之前的工作不同，我们实现了层数的自适应方案，并加入了语义编码以更好地幻觉部分被遮挡的对象。此外，我们的方法是对象驱动的，这特别提高了被遮挡中间对象的准确性。该框架包括两个步骤。首先，我们在估计场景布局的同时，分别完成每个对象的颜色和深度。其次，我们基于回归的层重建场景，并强制重新组合的图像类似于原始输入的结构。学习到的表示使得从单个RGB图像实现各种应用成为可能，例如3D摄影和减损现实。",
        "领域": "3D重建/场景理解/图像合成",
        "问题": "从单张RGB图像预测被遮挡区域的颜色和深度",
        "动机": "提高从单张图像进行场景分解的准确性和效率，特别是对于被遮挡对象的处理",
        "方法": "采用对象驱动的方法，通过自适应层数和语义编码来构建分层深度图像（LDI），并分两步完成场景的重建",
        "关键词": [
            "分层深度图像",
            "语义编码",
            "3D摄影",
            "减损现实"
        ],
        "涉及的技术概念": "分层深度图像（LDI）是一种将场景按层排列的表示方法，包括被遮挡的区域。语义编码用于提高部分被遮挡对象的幻觉准确性。3D摄影和减损现实是从单张RGB图像实现的应用。"
    },
    {
        "order": 296,
        "title": "Unsupervised 3D Reconstruction Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cha_Unsupervised_3D_Reconstruction_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cha_Unsupervised_3D_Reconstruction_Networks_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose 3D unsupervised reconstruction networks (3D-URN), which reconstruct the 3D structures of instances in a given object category from their 2D feature points under an orthographic camera model. 3D-URN consists of a 3D shape reconstructor and a rotation estimator, which are trained in a fully-unsupervised manner incorporating the proposed unsupervised loss functions. The role of the 3D shape reconstructor is to reconstruct the 3D shape of an instance from its 2D feature points, and the rotation estimator infers the camera pose. After training, 3D-URN can infer the 3D structure of an unseen instance in the same category, which is not possible in the conventional schemes of non-rigid structure from motion and structure from category. The experimental result shows the state-of-the-art performance, which demonstrates the effectiveness of the proposed method.",
        "中文标题": "无监督三维重建网络",
        "摘要翻译": "在本文中，我们提出了三维无监督重建网络（3D-URN），该网络在正交相机模型下从给定对象类别的实例的二维特征点重建其三维结构。3D-URN由一个三维形状重建器和一个旋转估计器组成，它们通过结合提出的无监督损失函数以完全无监督的方式进行训练。三维形状重建器的作用是从实例的二维特征点重建其三维形状，而旋转估计器则推断相机姿态。训练后，3D-URN能够推断出同一类别中未见实例的三维结构，这在传统的非刚性运动结构和类别结构方案中是不可能的。实验结果表明了最先进的性能，证明了所提出方法的有效性。",
        "领域": "三维重建/无监督学习/计算机视觉",
        "问题": "从二维特征点无监督地重建三维结构",
        "动机": "解决传统方法无法从同一类别的未见实例中推断三维结构的问题",
        "方法": "提出了一种包含三维形状重建器和旋转估计器的无监督网络，通过无监督损失函数进行训练",
        "关键词": [
            "三维重建",
            "无监督学习",
            "正交相机模型"
        ],
        "涉及的技术概念": "3D-URN网络包括三维形状重建器和旋转估计器，通过无监督损失函数进行训练，能够从二维特征点重建三维形状并推断相机姿态，进而推断出同一类别中未见实例的三维结构。"
    },
    {
        "order": 297,
        "title": "Perspective-Guided Convolution Networks for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Perspective-Guided_Convolution_Networks_for_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Perspective-Guided_Convolution_Networks_for_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios. Code is available at https://github.com/Zhaoyi-Yan/PGCNet.",
        "中文标题": "透视引导卷积网络用于人群计数",
        "摘要翻译": "本文提出了一种新颖的透视引导卷积（PGC）用于基于卷积神经网络（CNN）的人群计数（即PGCNet），旨在克服由于透视效应导致的人群场景内尺度变化的剧烈问题。虽然大多数最先进的方法采用多尺度或多列架构来解决此类问题，但它们通常无法建模连续的尺度变化，因为只考虑了离散的代表性尺度。PGCNet则利用透视信息来引导特征图的空间变化平滑，然后再将其输入到连续的卷积中。PGCNet还引入了一个有效的透视估计分支，该分支可以在监督设置或弱监督设置下进行训练，前提是该分支已经进行了预训练。我们的PGCNet是单列的，计算量适度增加，在四个基准数据集上的广泛实验结果显示，我们的方法相对于最先进的方法有所改进。此外，我们还引入了Crowd Surveillance，这是一个用于人群计数的大规模数据集，包含13,000多张高分辨率图像，具有挑战性的场景。代码可在https://github.com/Zhaoyi-Yan/PGCNet获取。",
        "领域": "人群计数/透视估计/卷积神经网络",
        "问题": "克服由于透视效应导致的人群场景内尺度变化的剧烈问题",
        "动机": "现有方法无法有效建模连续的尺度变化，因为只考虑了离散的代表性尺度",
        "方法": "利用透视信息来引导特征图的空间变化平滑，并引入透视估计分支",
        "关键词": [
            "人群计数",
            "透视引导卷积",
            "透视估计"
        ],
        "涉及的技术概念": {
            "透视引导卷积（PGC）": "一种新颖的卷积方法，利用透视信息来引导特征图的空间变化平滑",
            "卷积神经网络（CNN）": "一种深度学习模型，用于处理具有网格结构的数据，如图像",
            "透视估计分支": "PGCNet中的一个组成部分，用于估计透视信息，可以在监督或弱监督设置下进行训练",
            "Crowd Surveillance数据集": "一个大规模的人群计数数据集，包含13,000多张高分辨率图像，具有挑战性的场景"
        }
    },
    {
        "order": 298,
        "title": "A Neural Network for Detailed Human Depth Estimation From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_A_Neural_Network_for_Detailed_Human_Depth_Estimation_From_a_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tang_A_Neural_Network_for_Detailed_Human_Depth_Estimation_From_a_ICCV_2019_paper.html",
        "abstract": "This paper presents a neural network to estimate a detailed depth map of the foreground human in a single RGB image. The result captures geometry details such as cloth wrinkles, which are important in visualization applications. To achieve this goal, we separate the depth map into a smooth base shape and a residual detail shape and design a network with two branches to regress them respectively. We design a training strategy to ensure both base and detail shapes can be faithfully learned by the corresponding network branches. Furthermore, we introduce a novel network layer to fuse a rough depth map and surface normals to further improve the final result. Quantitative comparison with fused `ground truth' captured by real depth cameras and qualitative examples on unconstrained Internet images demonstrate the strength of the proposed method.",
        "中文标题": "从单张图像进行详细人体深度估计的神经网络",
        "摘要翻译": "本文提出了一种神经网络，用于从单张RGB图像中估计前景人体的详细深度图。结果捕捉了如衣物皱纹等几何细节，这些细节在可视化应用中非常重要。为了实现这一目标，我们将深度图分离为平滑的基础形状和残差细节形状，并设计了一个具有两个分支的网络来分别回归它们。我们设计了一种训练策略，以确保基础和细节形状能够被相应的网络分支忠实学习。此外，我们引入了一种新的网络层，融合粗略深度图和表面法线，以进一步提高最终结果。与真实深度相机捕获的融合'地面真实'进行定量比较，以及在不受限制的互联网图像上的定性示例，证明了所提出方法的优势。",
        "领域": "人体姿态估计/深度估计/图像生成",
        "问题": "从单张RGB图像中估计前景人体的详细深度图",
        "动机": "捕捉如衣物皱纹等几何细节，这些细节在可视化应用中非常重要",
        "方法": "将深度图分离为平滑的基础形状和残差细节形状，并设计了一个具有两个分支的网络来分别回归它们，引入新的网络层融合粗略深度图和表面法线",
        "关键词": [
            "深度估计",
            "神经网络",
            "图像生成"
        ],
        "涉及的技术概念": "深度图、RGB图像、几何细节、衣物皱纹、平滑基础形状、残差细节形状、网络分支、训练策略、表面法线、融合层"
    },
    {
        "order": 299,
        "title": "Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Niemeyer_Occupancy_Flow_4D_Reconstruction_by_Learning_Particle_Dynamics_ICCV_2019_paper.html",
        "abstract": "Deep learning based 3D reconstruction techniques have recently achieved impressive results. However, while state-of-the-art methods are able to output complex 3D geometry, it is not clear how to extend these results to time-varying topologies. Approaches treating each time step individually lack continuity and exhibit slow inference, while traditional 4D reconstruction methods often utilize a template model or discretize the 4D space at fixed resolution. In this work, we present Occupancy Flow, a novel spatio-temporal representation of time-varying 3D geometry with implicit correspondences. Towards this goal, we learn a temporally and spatially continuous vector field which assigns a motion vector to every point in space and time. In order to perform dense 4D reconstruction from images or sparse point clouds, we combine our method with a continuous 3D representation. Implicitly, our model yields correspondences over time, thus enabling fast inference while providing a sound physical description of the temporal dynamics. We show that our method can be used for interpolation and reconstruction tasks, and demonstrate the accuracy of the learned correspondences. We believe that Occupancy Flow is a promising new 4D representation which will be useful for a variety of spatio-temporal reconstruction tasks.",
        "中文标题": "占用流：通过学习粒子动力学进行4D重建",
        "摘要翻译": "基于深度学习的3D重建技术最近取得了令人印象深刻的成果。然而，尽管最先进的方法能够输出复杂的3D几何形状，但如何将这些结果扩展到随时间变化的拓扑结构尚不明确。单独处理每个时间步的方法缺乏连续性且推理速度慢，而传统的4D重建方法通常使用模板模型或以固定分辨率离散化4D空间。在这项工作中，我们提出了占用流，这是一种具有隐式对应关系的时间变化3D几何形状的新时空表示。为了实现这一目标，我们学习了一个时间和空间连续的向量场，该向量场为空间和时间的每个点分配一个运动向量。为了从图像或稀疏点云进行密集的4D重建，我们将我们的方法与连续的3D表示相结合。隐式地，我们的模型产生了随时间变化的对应关系，从而在提供时间动态的合理物理描述的同时实现了快速推理。我们展示了我们的方法可以用于插值和重建任务，并证明了学习到的对应关系的准确性。我们相信，占用流是一种有前途的新4D表示，将对各种时空重建任务有用。",
        "领域": "3D重建/时空数据处理/动态场景分析",
        "问题": "如何有效地扩展3D重建技术以处理随时间变化的拓扑结构",
        "动机": "现有的3D重建技术在处理随时间变化的拓扑结构时存在连续性问题且推理速度慢，需要一种新的表示方法来克服这些限制",
        "方法": "提出了一种名为占用流的新时空表示方法，通过学习一个时间和空间连续的向量场来为空间和时间的每个点分配运动向量，并与连续的3D表示相结合进行密集的4D重建",
        "关键词": [
            "4D重建",
            "时空表示",
            "向量场",
            "动态场景分析"
        ],
        "涉及的技术概念": "占用流是一种新的时空表示方法，通过学习一个时间和空间连续的向量场来实现4D重建。这种方法能够为空间和时间的每个点分配一个运动向量，从而在提供时间动态的合理物理描述的同时实现快速推理。"
    },
    {
        "order": 300,
        "title": "3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shu_3D_Point_Cloud_Generative_Adversarial_Network_Based_on_Tree_Structured_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shu_3D_Point_Cloud_Generative_Adversarial_Network_Based_on_Tree_Structured_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel generative adversarial network (GAN) for 3D point clouds generation, which is called tree-GAN. To achieve state-of-the-art performance for multi-class 3D point cloud generation, a tree-structured graph convolution network (TreeGCN) is introduced as a generator for tree-GAN. Because TreeGCN performs graph convolutions within a tree, it can use ancestor information to boost the representation power for features. To evaluate GANs for 3D point clouds accurately, we develop a novel evaluation metric called Frechet point cloud distance (FPD). Experimental results demonstrate that the proposed tree-GAN outperforms state-of-the-art GANs in terms of both conventional metrics and FPD, and can generate point clouds for different semantic parts without prior knowledge.",
        "中文标题": "基于树结构图卷积的三维点云生成对抗网络",
        "摘要翻译": "在本文中，我们提出了一种新颖的生成对抗网络（GAN），用于三维点云的生成，称为tree-GAN。为了实现多类三维点云生成的最先进性能，引入了树结构图卷积网络（TreeGCN）作为tree-GAN的生成器。由于TreeGCN在树内执行图卷积，它可以使用祖先信息来增强特征的表示能力。为了准确评估三维点云的GANs，我们开发了一种新的评估指标，称为Frechet点云距离（FPD）。实验结果表明，所提出的tree-GAN在传统指标和FPD方面均优于最先进的GANs，并且可以在没有先验知识的情况下生成不同语义部分的点云。",
        "领域": "三维点云生成/生成对抗网络/图卷积网络",
        "问题": "多类三维点云生成",
        "动机": "提高三维点云生成的表示能力和评估准确性",
        "方法": "引入树结构图卷积网络（TreeGCN）作为生成器，并开发新的评估指标Frechet点云距离（FPD）",
        "关键词": [
            "三维点云生成",
            "生成对抗网络",
            "图卷积网络",
            "Frechet点云距离"
        ],
        "涉及的技术概念": {
            "生成对抗网络（GAN）": "一种深度学习模型，由生成器和判别器组成，用于生成新的数据样本。",
            "树结构图卷积网络（TreeGCN）": "一种在图结构数据上执行卷积操作的网络，特别适用于树结构数据，能够利用祖先信息增强特征表示。",
            "Frechet点云距离（FPD）": "一种新的评估指标，用于准确评估三维点云生成的质量。"
        }
    },
    {
        "order": 301,
        "title": "End-to-End Wireframe Parsing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_End-to-End_Wireframe_Parsing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_End-to-End_Wireframe_Parsing_ICCV_2019_paper.html",
        "abstract": "We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn.",
        "中文标题": "端到端线框解析",
        "摘要翻译": "我们提出了一种概念上简单但有效的算法，用于检测给定图像中的线框。与之前的方法相比，这些方法首先预测一个中间热图，然后使用启发式算法提取直线，我们的方法是端到端可训练的，可以直接输出包含语义上有意义和几何上显著的连接点和线的矢量化线框。为了更好地理解输出的质量，我们提出了一种新的线框评估指标，该指标惩罚重叠的线段和不正确的线连接性。我们进行了广泛的实验，并表明我们的方法显著优于之前最先进的线框和线提取算法。我们希望我们的简单方法可以作为未来线框解析研究的基线。代码已在https://github.com/zhou13/lcnn公开提供。",
        "领域": "线框解析/图像理解/几何建模",
        "问题": "如何有效地检测和解析图像中的线框",
        "动机": "提高线框检测和解析的准确性和效率，为图像理解和几何建模提供更精确的工具",
        "方法": "提出了一种端到端可训练的算法，直接输出矢量化线框，并引入新的线框评估指标",
        "关键词": [
            "线框解析",
            "矢量化线框",
            "线框评估指标"
        ],
        "涉及的技术概念": "端到端训练、矢量化线框、线框评估指标、启发式算法、中间热图"
    },
    {
        "order": 302,
        "title": "DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.html",
        "abstract": "We present DenseRaC, a novel end-to-end framework for jointly estimating 3D human pose and body shape from a monocular RGB image. Our two-step framework takes the body pixel-to-surface correspondence map (i.e., IUV map) as proxy representation and then performs estimation of parameterized human pose and shape. Specifically, given an estimated IUV map, we develop a deep neural network optimizing 3D body reconstruction losses and further integrating a render-and-compare scheme to minimize differences between the input and the rendered output, i.e., dense body landmarks, body part masks, and adversarial priors. To boost learning, we further construct a large-scale synthetic dataset (MOCA) utilizing web-crawled Mocap sequences, 3D scans and animations. The generated data covers diversified camera views, human actions and body shapes, and is paired with full ground truth. Our model jointly learns to represent the 3D human body from hybrid datasets, mitigating the problem of unpaired training data. Our experiments show that DenseRaC obtains superior performance against state of the art on public benchmarks of various human-related tasks.",
        "中文标题": "DenseRaC: 通过密集渲染与比较联合估计3D姿态与形状",
        "摘要翻译": "我们提出了DenseRaC，一种新颖的端到端框架，用于从单目RGB图像中联合估计3D人体姿态和体型。我们的两步框架将身体像素到表面的对应图（即IUV图）作为代理表示，然后执行参数化人体姿态和形状的估计。具体来说，给定一个估计的IUV图，我们开发了一个深度神经网络，优化3D身体重建损失，并进一步集成渲染与比较方案，以最小化输入与渲染输出之间的差异，即密集的身体地标、身体部位掩码和对抗性先验。为了促进学习，我们进一步构建了一个大规模合成数据集（MOCA），利用网络爬取的Mocap序列、3D扫描和动画。生成的数据涵盖了多样化的相机视角、人体动作和体型，并与完整的地面实况配对。我们的模型联合学习从混合数据集中表示3D人体，缓解了未配对训练数据的问题。我们的实验表明，DenseRaC在各种人体相关任务的公共基准上获得了优于现有技术的性能。",
        "领域": "3D人体重建/姿态估计/体型估计",
        "问题": "从单目RGB图像中联合估计3D人体姿态和体型",
        "动机": "解决现有方法在处理未配对训练数据时的局限性，提高3D人体姿态和体型估计的准确性",
        "方法": "采用两步框架，首先使用IUV图作为代理表示，然后通过深度神经网络优化3D身体重建损失，并集成渲染与比较方案来最小化输入与渲染输出之间的差异",
        "关键词": [
            "3D人体重建",
            "姿态估计",
            "体型估计",
            "IUV图",
            "渲染与比较"
        ],
        "涉及的技术概念": "IUV图是一种表示身体像素到表面对应关系的图，用于代理表示人体姿态和形状。渲染与比较方案是一种通过比较输入图像与渲染输出图像之间的差异来优化模型的方法。对抗性先验是一种利用对抗性网络来生成更真实数据的技术。"
    },
    {
        "order": 303,
        "title": "Joint Monocular 3D Vehicle Detection and Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Joint_Monocular_3D_Vehicle_Detection_and_Tracking_ICCV_2019_paper.html",
        "abstract": "Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.",
        "中文标题": "联合单目3D车辆检测与跟踪",
        "摘要翻译": "车辆的3D尺寸和轨迹是预测车辆未来位置和基于这些预测规划未来自主动作的关键线索。在本文中，我们提出了一种新颖的在线框架，用于从单目视频中进行3D车辆检测和跟踪。该框架不仅能够关联运动中车辆的检测结果，还能从移动平台上捕获的一系列2D图像中估计其完整的3D边界框信息。我们的方法利用3D框深度排序匹配进行鲁棒的实例关联，并利用3D轨迹预测进行遮挡车辆的重新识别。我们还设计了一个基于LSTM的运动学习模块，以实现更准确的长期运动外推。我们在模拟、KITTI和Argoverse数据集上的实验表明，我们的3D跟踪管道提供了鲁棒的数据关联和跟踪。在Argoverse上，我们的基于图像的方法在30米范围内跟踪3D车辆的性能显著优于以LiDAR为中心的基线方法。",
        "领域": "自动驾驶/3D视觉/视频分析",
        "问题": "从单目视频中准确检测和跟踪3D车辆",
        "动机": "提高自动驾驶系统中对车辆未来位置预测的准确性，以及基于这些预测规划自主动作的能力",
        "方法": "提出了一种在线框架，利用3D框深度排序匹配进行实例关联，使用3D轨迹预测进行遮挡车辆的重新识别，并设计了一个基于LSTM的运动学习模块",
        "关键词": [
            "3D车辆检测",
            "3D跟踪",
            "单目视频",
            "自动驾驶",
            "LSTM"
        ],
        "涉及的技术概念": "3D框深度排序匹配用于实例关联，3D轨迹预测用于遮挡车辆的重新识别，基于LSTM的运动学习模块用于长期运动外推"
    },
    {
        "order": 304,
        "title": "Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-Directional Dependencies of Body Parts",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.html",
        "abstract": "Not all the human body parts have the same degree of freedom (DOF) due to the physiological structure. For example, the limbs may move more flexibly and freely than the torso does. Most of the existing 3D pose estimation methods, despite the very promising results achieved, treat the body joints equally and consequently often lead to larger reconstruction errors on the limbs. In this paper, we propose a progressive approach that explicitly accounts for the distinct DOFs among the body parts. We model parts with higher DOFs like the elbows, as dependent components of the corresponding parts with lower DOFs like the torso, of which the 3D locations can be more reliably estimated. Meanwhile, the high-DOF parts may, in turn, impose a constraint on where the low-DOF ones lie. As a result, parts with different DOFs supervise one another, yielding physically constrained and plausible pose-estimation results. To further facilitate the prediction of the high-DOF parts, we introduce a pose-attribution estimation, where the relative location of a limb joint with respect to the torso, which has the least DOF of a human body, is explicitly estimated and further fed to the joint-estimation module. The proposed approach achieves very promising results, outperforming the state of the art on several benchmarks.",
        "中文标题": "并非所有部分都平等：通过建模身体部分双向依赖关系的3D姿态估计",
        "摘要翻译": "由于生理结构的原因，并非所有的人体部分都具有相同的自由度（DOF）。例如，四肢可能比躯干更灵活自由地移动。尽管现有的3D姿态估计方法取得了非常令人鼓舞的结果，但它们通常平等对待身体关节，因此常常导致四肢的重建误差较大。在本文中，我们提出了一种渐进式方法，明确考虑了身体部分之间不同的自由度。我们将具有较高自由度的部分（如肘部）建模为具有较低自由度部分（如躯干）的依赖组件，这些部分的3D位置可以更可靠地估计。同时，高自由度部分可能反过来对低自由度部分的位置施加约束。因此，具有不同自由度的部分相互监督，产生物理约束和合理的姿态估计结果。为了进一步促进高自由度部分的预测，我们引入了姿态属性估计，其中明确估计了肢体关节相对于躯干的相对位置，躯干是人体的自由度最小的部分，并将其进一步输入到关节估计模块中。所提出的方法取得了非常令人鼓舞的结果，在多个基准测试中优于现有技术。",
        "领域": "3D姿态估计/人体运动分析/计算机视觉",
        "问题": "现有3D姿态估计方法平等对待身体关节，导致四肢重建误差较大",
        "动机": "考虑到人体不同部分具有不同的自由度，提出一种能够考虑这些差异的方法，以提高3D姿态估计的准确性",
        "方法": "提出一种渐进式方法，明确考虑身体部分之间不同的自由度，将高自由度部分建模为低自由度部分的依赖组件，并引入姿态属性估计以促进高自由度部分的预测",
        "关键词": [
            "3D姿态估计",
            "自由度",
            "渐进式方法",
            "姿态属性估计"
        ],
        "涉及的技术概念": "自由度（DOF）指的是物体在空间中能够自由移动的方向数。在本文中，通过考虑人体不同部分的自由度差异，提出了一种新的3D姿态估计方法，旨在提高姿态估计的准确性和物理合理性。"
    },
    {
        "order": 305,
        "title": "Visualization of Convolutional Neural Networks for Monocular Depth Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.html",
        "abstract": "Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and outdoor scene datasets. The results provide several findings that help exploration of the above question.",
        "中文标题": "卷积神经网络在单目深度估计中的可视化",
        "摘要翻译": "最近，卷积神经网络（CNNs）在单目深度估计任务中取得了巨大成功。一个基本但尚未解答的问题是：CNNs如何从单张图像中推断深度。为了回答这个问题，我们通过识别输入图像中与深度估计相关的像素来考虑CNN推理的可视化。我们将其表述为一个优化问题，即识别出最小数量的图像像素，CNN可以从这些像素中估计出与整个图像估计值差异最小的深度图。为了应对通过深度CNN进行优化的困难，我们提出使用另一个网络在前向计算中预测这些相关的图像像素。在我们的实验中，我们首先展示了这种方法的有效性，然后将其应用于室内和室外场景数据集上的不同深度估计网络。结果提供了几个有助于探索上述问题的发现。",
        "领域": "深度估计/神经网络可视化/优化问题",
        "问题": "如何从单张图像中推断深度",
        "动机": "探索卷积神经网络如何从单张图像中推断深度",
        "方法": "通过识别输入图像中与深度估计相关的像素来可视化CNN推理，并利用另一个网络预测这些相关像素",
        "关键词": [
            "深度估计",
            "神经网络可视化",
            "优化问题"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）用于从单张图像中估计深度，通过识别与深度估计相关的图像像素来可视化CNN的推理过程，并采用另一个网络来预测这些相关像素，以解决通过深度CNN进行优化的困难。"
    },
    {
        "order": 306,
        "title": "Incremental Class Discovery for Semantic Segmentation With RGBD Sensing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nakajima_Incremental_Class_Discovery_for_Semantic_Segmentation_With_RGBD_Sensing_ICCV_2019_paper.html",
        "abstract": "This work addresses the task of open world semantic segmentation using RGBD sensing to discover new semantic classes over time. Although there are many types of objects in the real-word, current semantic segmentation methods make a closed world assumption and are trained only to segment a limited number of object classes. Towards a more open world approach, we propose a novel method that incrementally learns new classes for image segmentation. The proposed system first segments each RGBD frame using both color and geometric information, and then aggregates that information to build a single segmented dense 3D map of the environment. The segmented 3D map representation is a key component of our approach as it is used to discover new object classes by identifying coherent regions in the 3D map that have no semantic label. The use of coherent region in the 3D map as a primitive element, rather than traditional elements such as surfels or voxels, also significantly reduces the computational complexity and memory use of our method. It thus leads to semi-real-time performance at 10.7 Hz when incrementally updating the dense 3D map at every frame. Through experiments on the NYUDv2 dataset, we demonstrate that the proposed method is able to correctly cluster objects of both known and unseen classes. We also show the quantitative comparison with the state-of-the-art supervised methods, the processing time of each step, and the influences of each component.",
        "中文标题": "基于RGBD感知的语义分割增量类发现",
        "摘要翻译": "本工作解决了使用RGBD感知进行开放世界语义分割的任务，以随时间发现新的语义类别。尽管现实世界中有许多类型的对象，但当前的语义分割方法做出了封闭世界的假设，并且仅训练以分割有限数量的对象类别。为了采用更开放世界的方法，我们提出了一种新颖的方法，该方法增量学习图像分割的新类别。所提出的系统首先使用颜色和几何信息分割每个RGBD帧，然后聚合该信息以构建环境的单个分割密集3D地图。分割的3D地图表示是我们方法的关键组成部分，因为它用于通过识别3D地图中没有语义标签的连贯区域来发现新的对象类别。使用3D地图中的连贯区域作为基本元素，而不是传统的元素（如surfels或体素），也显著降低了我们方法的计算复杂性和内存使用。因此，当每帧增量更新密集3D地图时，它导致了10.7 Hz的半实时性能。通过在NYUDv2数据集上的实验，我们证明了所提出的方法能够正确聚类已知和未见类别的对象。我们还展示了与最先进的监督方法的定量比较，每个步骤的处理时间，以及每个组件的影响。",
        "领域": "语义分割/3D地图构建/增量学习",
        "问题": "解决开放世界语义分割中增量发现新语义类别的问题",
        "动机": "当前语义分割方法仅能处理有限数量的对象类别，无法适应现实世界中不断出现的新类别",
        "方法": "提出了一种新颖的方法，通过使用RGBD感知技术，结合颜色和几何信息进行图像分割，并构建分割的密集3D地图来发现新类别",
        "关键词": [
            "语义分割",
            "3D地图",
            "增量学习"
        ],
        "涉及的技术概念": "RGBD感知技术结合颜色和几何信息进行图像分割，构建分割的密集3D地图，通过识别3D地图中没有语义标签的连贯区域来发现新的对象类别，显著降低了计算复杂性和内存使用，实现了半实时性能"
    },
    {
        "order": 307,
        "title": "Fingerspelling Recognition in the Wild With Iterative Visual Attention",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Fingerspelling_Recognition_in_the_Wild_With_Iterative_Visual_Attention_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Fingerspelling_Recognition_in_the_Wild_With_Iterative_Visual_Attention_ICCV_2019_paper.html",
        "abstract": "Sign language recognition is a challenging gesture sequence recognition problem, characterized by quick and highly coarticulated motion. In this paper we focus on recognition of fingerspelling sequences in American Sign Language (ASL) videos collected in the wild, mainly from YouTube and Deaf social media. Most previous work on sign language recognition has focused on controlled settings where the data is recorded in a studio environment and the number of signers is limited. Our work aims to address the challenges of real-life data, reducing the need for detection or segmentation modules commonly used in this domain. We propose an end-to-end model based on an iterative attention mechanism, without explicit hand detection or segmentation. Our approach dynamically focuses on increasingly high-resolution regions of interest. It out-performs prior work by a large margin. We also introduce a newly collected data set of crowdsourced annotations of fingerspelling in the wild, and show that performance can be further improved with this additional data set.",
        "中文标题": "野外手语拼写识别的迭代视觉注意力机制",
        "摘要翻译": "手语识别是一个具有挑战性的手势序列识别问题，其特点是快速且高度协同发音的动作。在本文中，我们专注于识别从YouTube和聋人社交媒体等野外收集的美国手语（ASL）视频中的手语拼写序列。大多数先前的手语识别工作都集中在控制设置上，其中数据是在工作室环境中录制的，并且签名者的数量有限。我们的工作旨在解决现实生活数据中的挑战，减少该领域常用的检测或分割模块的需求。我们提出了一种基于迭代注意力机制的端到端模型，无需显式的手部检测或分割。我们的方法动态地聚焦于越来越高的分辨率感兴趣区域。它在很大程度上优于之前的工作。我们还引入了一个新收集的野外手语拼写众包注释数据集，并展示了使用这个额外数据集可以进一步提高性能。",
        "领域": "手语识别/手势识别/视频分析",
        "问题": "识别野外收集的美国手语视频中的手语拼写序列",
        "动机": "解决现实生活数据中的挑战，减少对手部检测或分割模块的依赖",
        "方法": "提出了一种基于迭代注意力机制的端到端模型，无需显式的手部检测或分割，动态聚焦于高分辨率感兴趣区域",
        "关键词": [
            "手语识别",
            "手势识别",
            "视频分析",
            "注意力机制",
            "端到端模型"
        ],
        "涉及的技术概念": {
            "迭代注意力机制": "一种动态聚焦于视频中高分辨率感兴趣区域的技术，用于提高手语拼写序列的识别准确率",
            "端到端模型": "一种直接从输入到输出进行学习的模型，无需中间步骤或模块，如手部检测或分割",
            "众包注释数据集": "通过众包方式收集和注释的数据集，用于训练和验证模型，提高模型的泛化能力和性能"
        }
    },
    {
        "order": 308,
        "title": "Extreme View Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Extreme_View_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Extreme_View_Synthesis_ICCV_2019_paper.html",
        "abstract": "We present Extreme View Synthesis, a solution for novel view extrapolation that works even when the number of input images is small---as few as two. In this context, occlusions and depth uncertainty are two of the most pressing issues, and worsen as the degree of extrapolation increases. We follow the traditional paradigm of performing depth-based warping and refinement, with a few key improvements. First, we estimate a depth probability volume, rather than just a single depth value for each pixel of the novel view. This allows us to leverage depth uncertainty in challenging regions, such as depth discontinuities. After using it to get an initial estimate of the novel view, we explicitly combine learned image priors and the depth uncertainty to synthesize a refined image with less artifacts. Our method is the first to show visually pleasing results for baseline magnifications of up to 30x.",
        "中文标题": "极端视角合成",
        "摘要翻译": "我们提出了极端视角合成，这是一种即使在输入图像数量很少的情况下（少至两张）也能工作的新视角外推解决方案。在这种情况下，遮挡和深度不确定性是两个最紧迫的问题，并且随着外推程度的增加而恶化。我们遵循传统的基于深度的扭曲和细化范式，并进行了一些关键改进。首先，我们估计一个深度概率体积，而不仅仅是为新视角的每个像素估计一个单一的深度值。这使我们能够在具有挑战性的区域（如深度不连续性）利用深度不确定性。在使用它获得新视角的初始估计后，我们明确地将学习到的图像先验和深度不确定性结合起来，以合成一个具有较少伪影的细化图像。我们的方法是第一个展示出基线放大倍数高达30倍的视觉上令人愉悦的结果。",
        "领域": "三维重建/视角合成/深度估计",
        "问题": "在输入图像数量极少的情况下进行新视角外推时，遮挡和深度不确定性是主要问题。",
        "动机": "解决在极少数输入图像情况下进行新视角外推时的遮挡和深度不确定性问题，以生成视觉上令人愉悦的结果。",
        "方法": "估计深度概率体积以利用深度不确定性，结合学习到的图像先验和深度不确定性来合成细化图像。",
        "关键词": [
            "视角合成",
            "深度估计",
            "三维重建"
        ],
        "涉及的技术概念": {
            "深度概率体积": "一种表示每个像素可能深度分布的方法，而不仅仅是单一深度值。",
            "图像先验": "从大量图像数据中学习到的关于图像结构和内容的统计规律。",
            "深度不确定性": "在估计场景深度时存在的不确定性，特别是在深度不连续区域。"
        }
    },
    {
        "order": 309,
        "title": "Co-Separating Sounds of Visual Objects",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Co-Separating_Sounds_of_Visual_Objects_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Co-Separating_Sounds_of_Visual_Objects_ICCV_2019_paper.html",
        "abstract": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel. Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds. We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos. Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair. Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training. We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
        "中文标题": "共同分离视觉对象的声音",
        "摘要翻译": "从视频中学习物体的声音是具有挑战性的，因为它们通常在单个音频通道中严重重叠。当前的方法通过使用人工混合的视频剪辑进行训练来回避这个问题，但这给训练数据的收集带来了不便的限制，甚至可能阻止学习“真实”混合声音的属性。我们引入了一种共同分离训练范式，允许从未标记的多源视频中学习物体级别的声音。我们新颖的训练目标要求深度神经网络对看起来相似的物体分离出的音频能够一致地被识别，同时为每个源训练对重现准确的视频级别音频轨道。我们的方法在现实测试视频中解开了声音，即使在训练期间未单独观察到物体的情况下也是如此。我们在MUSIC、AudioSet和AV-Bench数据集上获得了视觉引导音频源分离和音频去噪的最先进结果。",
        "领域": "音频源分离/音频去噪/视频分析",
        "问题": "从视频中分离出重叠的物体声音",
        "动机": "现有方法依赖于人工混合的视频剪辑进行训练，这限制了训练数据的收集并可能影响学习真实混合声音的能力。",
        "方法": "引入了一种共同分离训练范式，通过要求深度神经网络对相似物体分离出的音频一致可识别，并重现准确的视频级别音频轨道，从未标记的多源视频中学习物体级别的声音。",
        "关键词": [
            "音频源分离",
            "音频去噪",
            "视频分析"
        ],
        "涉及的技术概念": "共同分离训练范式、深度神经网络、视频级别音频轨道、视觉引导音频源分离、音频去噪"
    },
    {
        "order": 310,
        "title": "SSF-DAN: Separated Semantic Feature Based Domain Adaptation Network for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Du_SSF-DAN_Separated_Semantic_Feature_Based_Domain_Adaptation_Network_for_Semantic_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Du_SSF-DAN_Separated_Semantic_Feature_Based_Domain_Adaptation_Network_for_Semantic_ICCV_2019_paper.html",
        "abstract": "Despite the great success achieved by supervised fully convolutional models in semantic segmentation, training the models requires a large amount of labor-intensive work to generate pixel-level annotations. Recent works exploit synthetic data to train the model for semantic segmentation, but the domain adaptation between real and synthetic images remains a challenging problem. In this work, we propose a Separated Semantic Feature based domain adaptation network, named SSF-DAN, for semantic segmentation. First, a Semantic-wise Separable Discriminator (SS-D) is designed to independently adapt semantic features across the target and source domains, which addresses the inconsistent adaptation issue in the class-wise adversarial learning. In SS-D, a progressive confidence strategy is included to achieve a more reliable separation. Then, an efficient Class-wise Adversarial loss Reweighting module (CA-R) is introduced to balance the class-wise adversarial learning process, which leads the generator to focus more on poorly adapted classes. The presented framework demonstrates robust performance, superior to state-of-the-art methods on benchmark datasets.",
        "中文标题": "SSF-DAN: 基于分离语义特征的领域适应网络用于语义分割",
        "摘要翻译": "尽管监督式全卷积模型在语义分割方面取得了巨大成功，但训练这些模型需要大量劳动密集型工作来生成像素级注释。最近的工作利用合成数据来训练语义分割模型，但真实图像和合成图像之间的领域适应仍然是一个具有挑战性的问题。在这项工作中，我们提出了一种基于分离语义特征的领域适应网络，名为SSF-DAN，用于语义分割。首先，设计了一个语义可分离判别器（SS-D），以独立适应目标域和源域之间的语义特征，解决了类对抗学习中的不一致适应问题。在SS-D中，包含了一个渐进置信策略，以实现更可靠的分离。然后，引入了一个高效的类对抗损失重加权模块（CA-R），以平衡类对抗学习过程，使生成器更关注适应不良的类别。所提出的框架展示了鲁棒的性能，在基准数据集上优于最先进的方法。",
        "领域": "语义分割/领域适应/对抗学习",
        "问题": "真实图像和合成图像之间的领域适应问题",
        "动机": "减少训练语义分割模型所需的像素级注释工作量",
        "方法": "提出了一种基于分离语义特征的领域适应网络（SSF-DAN），包括语义可分离判别器（SS-D）和类对抗损失重加权模块（CA-R）",
        "关键词": [
            "语义分割",
            "领域适应",
            "对抗学习",
            "语义可分离判别器",
            "类对抗损失重加权"
        ],
        "涉及的技术概念": "语义分割是指将图像分割成多个区域，每个区域对应一个语义类别。领域适应是指将在一个领域（如合成图像）上训练的模型适应到另一个领域（如真实图像）上。对抗学习是一种通过让两个模型相互对抗来学习的方法，其中一个模型尝试生成数据，另一个模型尝试区分生成的数据和真实数据。语义可分离判别器（SS-D）是一种设计用于独立适应不同语义特征的判别器。类对抗损失重加权模块（CA-R）是一种用于平衡类对抗学习过程的模块，使生成器更关注适应不良的类别。"
    },
    {
        "order": 311,
        "title": "PointAE: Point Auto-Encoder for 3D Statistical Shape and Texture Modelling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dai_PointAE_Point_Auto-Encoder_for_3D_Statistical_Shape_and_Texture_Modelling_ICCV_2019_paper.html",
        "abstract": "The outcome of standard statistical shape modelling is a vector space representation of objects. Any convex combination of vectors of a set of object class examples generates a real and valid example. In this paper, we propose a Point Auto-Encoder (PointAE) with skip-connection, attention blocks for 3D statistical shape modelling directly on 3D points. The proposed PointAE is able to refine the correspondence with a correspondence refinement block. The data with refined correspondence can be fed to the PointAE again and bootstrap the constructed statistical models. Instead of two seperate models, PointAE can simultaneously model the shape and texture variation. The extensive evaluation in three open-sourced datasets demonstrates that the proposed method achieves better performance in representation ability of the shape variations.",
        "中文标题": "PointAE：用于3D统计形状和纹理建模的点自动编码器",
        "摘要翻译": "标准统计形状建模的结果是对象的向量空间表示。一组对象类示例的向量的任何凸组合都会生成一个真实且有效的示例。在本文中，我们提出了一种带有跳跃连接、注意力块的点自动编码器（PointAE），用于直接在3D点上进行3D统计形状建模。提出的PointAE能够通过对应关系细化块来细化对应关系。具有细化对应关系的数据可以再次输入到PointAE中，并引导构建的统计模型。PointAE可以同时建模形状和纹理变化，而不是两个独立的模型。在三个开源数据集上的广泛评估表明，所提出的方法在形状变化的表示能力方面实现了更好的性能。",
        "领域": "3D建模/自动编码器/统计形状分析",
        "问题": "如何在3D点上直接进行统计形状建模，并同时处理形状和纹理变化",
        "动机": "为了改进3D统计形状建模的表示能力，特别是在处理形状和纹理变化方面，需要一种能够直接在3D点上进行建模的方法。",
        "方法": "提出了一种带有跳跃连接和注意力块的点自动编码器（PointAE），用于直接在3D点上进行3D统计形状建模，并通过对应关系细化块来细化对应关系。",
        "关键词": [
            "3D建模",
            "自动编码器",
            "统计形状分析",
            "纹理变化",
            "对应关系细化"
        ],
        "涉及的技术概念": {
            "点自动编码器（PointAE）": "一种直接在3D点上进行3D统计形状建模的自动编码器，带有跳跃连接和注意力块。",
            "跳跃连接": "在神经网络中，跳跃连接允许信息从一层直接传递到更深层，有助于解决梯度消失问题。",
            "注意力块": "一种机制，使模型能够专注于输入数据的特定部分，提高模型的表示能力。",
            "对应关系细化块": "用于改进3D点之间对应关系的模块，有助于提高统计形状建模的准确性。"
        }
    },
    {
        "order": 312,
        "title": "View Independent Generative Adversarial Network for Novel View Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_View_Independent_Generative_Adversarial_Network_for_Novel_View_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_View_Independent_Generative_Adversarial_Network_for_Novel_View_Synthesis_ICCV_2019_paper.html",
        "abstract": "Synthesizing novel views from a 2D image requires to infer 3D structure and project it back to 2D from a new viewpoint. In this paper, we propose an encoder-decoder based generative adversarial network VI-GAN to tackle this problem. Our method is to let the network, after seeing many images of objects belonging to the same category in different views, obtain essential knowledge of intrinsic properties of the objects. To this end, an encoder is designed to extract view-independent feature that characterizes intrinsic properties of the input image, which includes 3D structure, color, texture etc. We also make the decoder hallucinate the image of a novel view based on the extracted feature and an arbitrary user-specific camera pose. Extensive experiments demonstrate that our model can synthesize high-quality images in different views with continuous camera poses, and is general for various applications.",
        "中文标题": "视角独立生成对抗网络用于新视角合成",
        "摘要翻译": "从2D图像合成新视角需要推断3D结构并从新视角将其投影回2D。在本文中，我们提出了一种基于编码器-解码器的生成对抗网络VI-GAN来解决这个问题。我们的方法是让网络在看过属于同一类别的对象在不同视角下的许多图像后，获得对象内在属性的基本知识。为此，设计了一个编码器来提取表征输入图像内在属性的视角独立特征，包括3D结构、颜色、纹理等。我们还让解码器基于提取的特征和任意用户指定的相机姿态来幻觉出新视角的图像。大量实验证明，我们的模型可以在不同视角下合成高质量的图像，具有连续的相机姿态，并且适用于各种应用。",
        "领域": "3D重建/图像合成/视角转换",
        "问题": "从单一2D图像合成新视角的3D图像",
        "动机": "为了从单一2D图像中推断出3D结构并生成新视角的图像，以扩展图像的应用范围和增强视觉效果",
        "方法": "采用基于编码器-解码器的生成对抗网络VI-GAN，通过编码器提取视角独立特征，解码器基于这些特征和用户指定的相机姿态生成新视角图像",
        "关键词": [
            "3D重建",
            "图像合成",
            "视角转换"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）、编码器-解码器结构、视角独立特征、3D结构推断、图像幻觉"
    },
    {
        "order": 313,
        "title": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_BMN_Boundary-Matching_Network_for_Temporal_Action_Proposal_Generation_ICCV_2019_paper.html",
        "abstract": "Temporal action proposal generation is an challenging and promising task which aims to locate temporal regions in real-world videos where action or event may occur. Current bottom-up proposal generation methods can generate proposals with precise boundary, but cannot efficiently generate adequately reliable confidence scores for retrieving proposals. To address these difficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate confidence scores of densely distributed proposals, which denote a proposal as a matching pair of starting and ending boundaries and combine all densely distributed BM pairs into the BM confidence map. Based on BM mechanism, we propose an effective, efficient and end-to-end proposal generation method, named Boundary-Matching Network (BMN), which generates proposals with precise temporal boundaries as well as reliable confidence scores simultaneously. The two-branches of BMN are jointly trained in an unified framework. We conduct experiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where BMN shows significant performance improvement with remarkable efficiency and generalizability. Further, combining with existing action classifier, BMN can achieve state-of-the-art temporal action detection performance.",
        "中文标题": "BMN: 边界匹配网络用于时序动作提议生成",
        "摘要翻译": "时序动作提议生成是一项具有挑战性且前景广阔的任务，旨在定位现实世界视频中可能发生动作或事件的时间区域。当前的自下而上的提议生成方法可以生成具有精确边界的提议，但无法有效地生成足够可靠的置信度分数以检索提议。为了解决这些困难，我们引入了边界匹配（BM）机制来评估密集分布提议的置信度分数，该机制将提议表示为开始和结束边界的匹配对，并将所有密集分布的BM对组合成BM置信度图。基于BM机制，我们提出了一种有效、高效且端到端的提议生成方法，称为边界匹配网络（BMN），它同时生成具有精确时间边界和可靠置信度分数的提议。BMN的两个分支在一个统一的框架中联合训练。我们在两个具有挑战性的数据集上进行了实验：THUMOS-14和ActivityNet-1.3，BMN在这些数据集上显示出显著的性能提升，具有显著的效率和泛化能力。此外，结合现有的动作分类器，BMN可以实现最先进的时序动作检测性能。",
        "领域": "视频分析/动作识别/时序分析",
        "问题": "生成具有精确时间边界和可靠置信度分数的时序动作提议",
        "动机": "当前的方法无法有效地生成足够可靠的置信度分数以检索提议",
        "方法": "引入边界匹配（BM）机制，提出边界匹配网络（BMN）方法",
        "关键词": [
            "时序动作提议生成",
            "边界匹配机制",
            "置信度分数"
        ],
        "涉及的技术概念": "边界匹配（BM）机制通过将提议表示为开始和结束边界的匹配对，并将所有密集分布的BM对组合成BM置信度图，来评估密集分布提议的置信度分数。边界匹配网络（BMN）是一种有效、高效且端到端的提议生成方法，能够同时生成具有精确时间边界和可靠置信度分数的提议。"
    },
    {
        "order": 314,
        "title": "SpaceNet MVOI: A Multi-View Overhead Imagery Dataset",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Weir_SpaceNet_MVOI_A_Multi-View_Overhead_Imagery_Dataset_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Weir_SpaceNet_MVOI_A_Multi-View_Overhead_Imagery_Dataset_ICCV_2019_paper.html",
        "abstract": "Detection and segmentation of objects in overheard imagery is a challenging task. The variable density, random orientation, small size, and instance-to-instance heterogeneity of objects in overhead imagery calls for approaches distinct from existing models designed for natural scene datasets. Though new overhead imagery datasets are being developed, they almost universally comprise a single view taken from directly overhead (\"at nadir\"), failing to address a critical variable: look angle. By contrast, views vary in real-world overhead imagery, particularly in dynamic scenarios such as natural disasters where first looks are often over 40 degrees off-nadir. This represents an important challenge to computer vision methods, as changing view angle adds distortions, alters resolution, and changes lighting. At present, the impact of these perturbations for algorithmic detection and segmentation of objects is untested. To address this problem, we present an open source Multi-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance. We benchmark multiple leading segmentation and object detection models on: (1) building detection, (2) generalization to unseen viewing angles and resolutions, and (3) sensitivity of building footprint extraction to changes in resolution. We find that state of the art segmentation and object detection models struggle to identify buildings in off-nadir imagery and generalize poorly to unseen views, presenting an important benchmark to explore the broadly relevant challenge of detecting small, heterogeneous target objects in visually dynamic contexts.",
        "中文标题": "SpaceNet MVOI: 一个多视角航空影像数据集",
        "摘要翻译": "在航空影像中检测和分割物体是一项具有挑战性的任务。航空影像中物体的可变密度、随机方向、小尺寸以及实例间的异质性要求采用与现有自然场景数据集模型不同的方法。尽管新的航空影像数据集正在开发中，但它们几乎都只包含从正上方（“天底”）拍摄的单一视角，未能解决一个关键变量：视角角度。相比之下，现实世界中的航空影像视角多变，特别是在自然灾害等动态场景中，首次观察往往偏离天底超过40度。这对计算机视觉方法构成了一个重要挑战，因为视角的变化会增加失真、改变分辨率并影响光照。目前，这些变化对物体检测和分割算法的影响尚未得到测试。为了解决这个问题，我们提出了一个开源的多视角航空影像数据集，称为SpaceNet MVOI，包含从广泛视角范围（-32.5度到54.0度）的27个独特视角。这些图像覆盖了相同的665平方公里地理区域，并标注了126,747个建筑足迹标签，使得可以直接评估视角变化对模型性能的影响。我们对多个领先的分割和物体检测模型进行了基准测试，包括：（1）建筑检测，（2）对未见视角和分辨率的泛化能力，以及（3）建筑足迹提取对分辨率变化的敏感性。我们发现，最先进的分割和物体检测模型在偏离天底的影像中识别建筑物时表现不佳，并且对未见视角的泛化能力差，这为探索在视觉动态环境中检测小型、异质目标物体的广泛相关挑战提供了一个重要的基准。",
        "领域": "航空影像分析/物体检测/图像分割",
        "问题": "在航空影像中检测和分割物体，特别是处理视角变化带来的挑战",
        "动机": "现有航空影像数据集大多只包含单一视角，未能解决视角变化对物体检测和分割算法的影响",
        "方法": "提出了一个包含多视角航空影像的开源数据集SpaceNet MVOI，并利用该数据集对多个领先的分割和物体检测模型进行了基准测试",
        "关键词": [
            "航空影像",
            "物体检测",
            "图像分割",
            "视角变化",
            "建筑检测"
        ],
        "涉及的技术概念": "多视角航空影像数据集、物体检测、图像分割、视角变化对算法性能的影响、建筑足迹提取"
    },
    {
        "order": 315,
        "title": "Multi-Garment Net: Learning to Dress 3D People From Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhatnagar_Multi-Garment_Net_Learning_to_Dress_3D_People_From_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bhatnagar_Multi-Garment_Net_Learning_to_Dress_3D_People_From_Images_ICCV_2019_paper.html",
        "abstract": "We present Multi-Garment Network (MGN), a method to predict body shape and clothing, layered on top of the SMPL model from a few frames (1-8) of a video. Several experiments demonstrate that this representation allows higher level of control when compared to single mesh or voxel representations of shape. Our model allows to predict garment geometry, relate it to the body shape, and transfer it to new body shapes and poses. To train MGN, we leverage a digital wardrobe containing 712 digital garments in correspondence, obtained with a novel method to register a set of clothing templates to a dataset of real 3D scans of people in different clothing and poses. Garments from the digital wardrobe, or predicted by MGN, can be used to dress any body shape in arbitrary poses. We will make publicly available the digital wardrobe, the MGN model, and code to dress SMPL with the garments at https://virtualhumans.mpi-inf.mpg.de/mgn",
        "中文标题": "多服装网络：从图像中学习为3D人物穿衣",
        "摘要翻译": "我们提出了多服装网络（MGN），一种从视频的几帧（1-8帧）中预测身体形状和服装的方法，该方法建立在SMPL模型之上。多项实验表明，与单一网格或体素形状表示相比，这种表示方法允许更高级别的控制。我们的模型能够预测服装几何形状，将其与身体形状相关联，并将其转移到新的身体形状和姿势上。为了训练MGN，我们利用了一个包含712件数字服装的数字衣柜，这些服装是通过一种新方法注册到一组真实3D扫描数据上获得的，这些扫描数据包括穿着不同服装和摆出不同姿势的人。数字衣柜中的服装或由MGN预测的服装可以用来为任意姿势的任何身体形状穿衣。我们将在https://virtualhumans.mpi-inf.mpg.de/mgn上公开提供数字衣柜、MGN模型以及用服装为SMPL穿衣的代码。",
        "领域": "3D建模/虚拟现实/服装设计",
        "问题": "如何从少量视频帧中准确预测3D人物的身体形状和服装，并实现服装的转移和适配",
        "动机": "提高3D人物建模的准确性和灵活性，使得服装能够适应不同的身体形状和姿势",
        "方法": "利用数字衣柜和SMPL模型，通过一种新方法注册服装模板到真实3D扫描数据上，训练MGN模型预测服装几何形状并实现服装的转移",
        "关键词": [
            "3D建模",
            "虚拟现实",
            "服装设计"
        ],
        "涉及的技术概念": "SMPL模型是一种参数化的人体模型，用于表示和变形3D人体形状。数字衣柜是一个包含多件数字服装的集合，这些服装可以通过特定的方法注册到3D人体模型上。MGN（多服装网络）是一种深度学习模型，用于从视频帧中预测3D人物的身体形状和服装，并实现服装的转移和适配。"
    },
    {
        "order": 316,
        "title": "Cascaded Context Pyramid for Full-Resolution 3D Semantic Scene Completion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Cascaded_Context_Pyramid_for_Full-Resolution_3D_Semantic_Scene_Completion_ICCV_2019_paper.html",
        "abstract": "Semantic Scene Completion (SSC) aims to simultaneously predict the volumetric occupancy and semantic category of a 3D scene. It helps intelligent devices to understand and interact with the surrounding scenes. Due to the high-memory requirement, current methods only produce low-resolution completion predictions, and generally lose the object details. Furthermore, they also ignore the multi-scale spatial contexts, which play a vital role for the 3D inference. To address these issues, in this work we propose a novel deep learning framework, named Cascaded Context Pyramid Network (CCPNet), to jointly infer the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The proposed CCPNet improves the labeling coherence with a cascaded context pyramid. Meanwhile, based on the low-level features, it progressively restores the fine-structures of objects with Guided Residual Refinement (GRR) modules. Our proposed framework has three outstanding advantages: (1) it explicitly models the 3D spatial context for performance improvement; (2) full-resolution 3D volumes are produced with structure-preserving details; (3) light-weight models with low-memory requirements are captured with a good extensibility. Extensive experiments demonstrate that in spite of taking a single-view depth map, our proposed framework can generate high-quality SSC results, and outperforms state-of-the-art approaches on both the synthetic SUNCG and real NYU datasets.",
        "中文标题": "级联上下文金字塔用于全分辨率3D语义场景补全",
        "摘要翻译": "语义场景补全（SSC）旨在同时预测3D场景的体积占用和语义类别。它帮助智能设备理解并与其周围场景互动。由于高内存需求，当前方法仅能产生低分辨率的补全预测，并且通常会丢失物体细节。此外，它们也忽略了多尺度空间上下文，这对3D推理至关重要。为了解决这些问题，在本工作中，我们提出了一种新颖的深度学习框架，名为级联上下文金字塔网络（CCPNet），以从单一深度图像联合推断体积3D场景的占用和语义标签。提出的CCPNet通过级联上下文金字塔提高了标签的一致性。同时，基于低级特征，它通过引导残差细化（GRR）模块逐步恢复物体的精细结构。我们提出的框架有三个突出优势：（1）它显式地建模3D空间上下文以提高性能；（2）生成具有结构保持细节的全分辨率3D体积；（3）通过良好的扩展性捕获低内存需求的轻量级模型。大量实验证明，尽管采用单视图深度图，我们提出的框架能够生成高质量的SSC结果，并在合成SUNCG和真实NYU数据集上均优于最先进的方法。",
        "领域": "3D场景理解/语义分割/深度学习",
        "问题": "解决3D语义场景补全中的高内存需求和低分辨率预测问题",
        "动机": "提高智能设备对周围场景的理解和互动能力，通过改进3D场景的语义补全质量",
        "方法": "提出级联上下文金字塔网络（CCPNet），通过级联上下文金字塔和引导残差细化（GRR）模块，从单一深度图像联合推断3D场景的占用和语义标签",
        "关键词": [
            "3D语义场景补全",
            "级联上下文金字塔",
            "引导残差细化"
        ],
        "涉及的技术概念": "级联上下文金字塔网络（CCPNet）是一种深度学习框架，用于从单一深度图像中推断3D场景的体积占用和语义标签。引导残差细化（GRR）模块用于逐步恢复物体的精细结构。"
    },
    {
        "order": 317,
        "title": "Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Weakly_Supervised_Temporal_Action_Localization_Through_Contrast_Based_Evaluation_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Weakly_Supervised_Temporal_Action_Localization_Through_Contrast_Based_Evaluation_Networks_ICCV_2019_paper.html",
        "abstract": "Weakly-supervised temporal action localization (WS-TAL) is a promising but challenging task with only video-level action categorical labels available during training. Without requiring temporal action boundary annotations in training data, WS-TAL could possibly exploit automatically retrieved video tags as video-level labels. However, such coarse video-level supervision inevitably incurs confusions, especially in untrimmed videos containing multiple action instances. To address this challenge, we propose the Contrast-based Localization EvaluAtioN Network (CleanNet) with our new action proposal evaluator, which provides pseudo-supervision by leveraging the temporal contrast in snippet-level action classification predictions. Essentially, the new action proposal evaluator enforces an additional temporal contrast constraint so that high-evaluation-score action proposals are more likely to coincide with true action instances. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Experiments on THUMOS14 and ActivityNet datasets validate the efficacy of CleanNet against existing state-ofthe- art WS-TAL algorithms.",
        "中文标题": "基于对比评估网络的弱监督时序动作定位",
        "摘要翻译": "弱监督时序动作定位（WS-TAL）是一项有前景但具有挑战性的任务，在训练期间只有视频级别的动作类别标签可用。WS-TAL不需要训练数据中的时序动作边界注释，可能利用自动检索的视频标签作为视频级别标签。然而，这种粗糙的视频级别监督不可避免地会引起混淆，特别是在包含多个动作实例的未修剪视频中。为了应对这一挑战，我们提出了基于对比的定位评估网络（CleanNet）及其新的动作提案评估器，该评估器通过利用片段级别动作分类预测中的时序对比提供伪监督。本质上，新的动作提案评估器强制执行额外的时序对比约束，使得高评估分数的动作提案更可能与真实的动作实例重合。此外，新的动作定位模块是CleanNet的一个组成部分，支持端到端训练。这与许多现有的WS-TAL方法形成对比，其中动作定位仅是一个后处理步骤。在THUMOS14和ActivityNet数据集上的实验验证了CleanNet相对于现有最先进的WS-TAL算法的有效性。",
        "领域": "视频分析/动作识别/时序分析",
        "问题": "在仅有视频级别动作类别标签的情况下，如何有效地进行时序动作定位",
        "动机": "解决在未修剪视频中由于粗糙的视频级别监督引起的动作实例混淆问题",
        "方法": "提出了基于对比的定位评估网络（CleanNet）及其新的动作提案评估器，通过利用片段级别动作分类预测中的时序对比提供伪监督，并强制执行额外的时序对比约束",
        "关键词": [
            "弱监督学习",
            "时序动作定位",
            "视频分析"
        ],
        "涉及的技术概念": "CleanNet是一种新的网络架构，它通过引入动作提案评估器来提供伪监督，该评估器利用片段级别的动作分类预测中的时序对比，以改善动作定位的准确性。此外，CleanNet支持端到端训练，与现有方法相比，动作定位不再是单独的后处理步骤。"
    },
    {
        "order": 318,
        "title": "Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sindagi_Multi-Level_Bottom-Top_and_Top-Bottom_Feature_Fusion_for_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets",
        "中文标题": "多层次自下而上和自上而下特征融合用于人群计数",
        "摘要翻译": "人群计数在图像内和数据集间尺度变化大的情况下提出了巨大的挑战。在高度拥挤的场景中，这些问题进一步加剧。基于深度网络多尺度特征直接融合的方法似乎是解决这一问题的明显方案。然而，在拥挤场景的人群计数中，这些融合方法并未带来显著的改进。这通常是由于它们在有效结合多尺度特征以解决人群计数等问题方面的能力有限。为了克服这一点，我们专注于如何有效利用网络中不同层的信息。具体来说，我们提出了一个网络，包括：(i) 多层次自下而上和自上而下融合（MBTTBF）方法，以在多个层次上结合从浅层到深层的信息，反之亦然，(ii) 尺度互补特征提取块（SCFB），涉及跨尺度残差函数，以明确允许沿融合路径从相邻卷积层流动的互补特征。此外，为了提高多尺度融合的有效性，我们采用了一种原则性的方法来生成尺度感知的真实密度图进行训练。在包含高度拥挤场景的三个数据集（ShanghaiTech、UCF_CC_50和UCF-QNRF）上进行的实验表明，所提出的方法在所有数据集中均能优于几种最近的方法。",
        "领域": "人群计数/特征融合/深度学习",
        "问题": "解决在高度拥挤场景中人群计数时，多尺度特征融合效果不佳的问题",
        "动机": "由于现有方法在有效结合多尺度特征以解决人群计数等问题方面的能力有限，特别是在高度拥挤的场景中，因此需要一种更有效的方法来利用网络中不同层的信息。",
        "方法": "提出了一个网络，包括多层次自下而上和自上而下融合（MBTTBF）方法和尺度互补特征提取块（SCFB），并采用了一种原则性的方法来生成尺度感知的真实密度图进行训练。",
        "关键词": [
            "人群计数",
            "特征融合",
            "深度学习"
        ],
        "涉及的技术概念": "多层次自下而上和自上而下融合（MBTTBF）方法、尺度互补特征提取块（SCFB）、跨尺度残差函数、尺度感知的真实密度图"
    },
    {
        "order": 319,
        "title": "Skeleton-Aware 3D Human Shape Reconstruction From Point Clouds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Skeleton-Aware_3D_Human_Shape_Reconstruction_From_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "This work addresses the problem of 3D human shape reconstruction from point clouds. Considering that human shapes are of high dimensions and with large articulations, we adopt the state-of-the-art parametric human body model, SMPL, to reduce the dimension of learning space and generate smooth and valid reconstruction. However, SMPL parameters, especially pose parameters, are not easy to learn because of ambiguity and locality of the pose representation. Thus, we propose to incorporate skeleton awareness into the deep learning based regression of SMPL parameters for 3D human shape reconstruction. Our basic idea is to use the state-of-the-art technique PointNet++ to extract point features, and then map point features to skeleton joint features and finally to SMPL parameters for the reconstruction from point clouds. Particularly, we develop an end-to-end framework, where we propose a graph aggregation module to augment PointNet++ by extracting better point features, an attention module to better map unordered point features into ordered skeleton joint features, and a skeleton graph module to extract better joint features for SMPL parameter regression. The entire framework network is first trained in an end-to-end manner on synthesized dataset, and then online fine-tuned on unseen dataset with unsupervised loss to bridges gaps between training and testing. The experiments on multiple datasets show that our method is on par with the state-of-the-art solution.",
        "中文标题": "基于点云的骨架感知三维人体形状重建",
        "摘要翻译": "本工作解决了从点云进行三维人体形状重建的问题。考虑到人体形状具有高维度和大关节的特点，我们采用了最先进的参数化人体模型SMPL，以减少学习空间的维度并生成平滑且有效的重建。然而，SMPL参数，尤其是姿态参数，由于姿态表示的模糊性和局部性，不易学习。因此，我们提出将骨架感知融入到基于深度学习的SMPL参数回归中，以实现三维人体形状重建。我们的基本思想是使用最先进的技术PointNet++来提取点特征，然后将点特征映射到骨架关节特征，最后映射到SMPL参数，以从点云进行重建。特别是，我们开发了一个端到端的框架，其中我们提出了一个图聚合模块来增强PointNet++，通过提取更好的点特征，一个注意力模块来更好地将无序的点特征映射到有序的骨架关节特征，以及一个骨架图模块来提取更好的关节特征用于SMPL参数回归。整个框架网络首先在合成数据集上以端到端的方式进行训练，然后在未见过的数据集上使用无监督损失进行在线微调，以弥合训练和测试之间的差距。在多个数据集上的实验表明，我们的方法与最先进的解决方案相当。",
        "领域": "三维重建/人体建模/深度学习",
        "问题": "从点云进行三维人体形状重建",
        "动机": "由于人体形状的高维度和大关节特点，以及SMPL参数尤其是姿态参数学习的困难，需要一种更有效的方法来进行三维人体形状重建。",
        "方法": "采用PointNet++技术提取点特征，通过图聚合模块、注意力模块和骨架图模块，将点特征映射到骨架关节特征，再映射到SMPL参数，实现端到端的三维人体形状重建。",
        "关键词": [
            "三维重建",
            "人体建模",
            "点云处理",
            "SMPL模型",
            "骨架感知"
        ],
        "涉及的技术概念": "SMPL模型是一种参数化人体模型，用于减少学习空间的维度并生成平滑且有效的重建。PointNet++是一种用于处理点云数据的技术，能够提取点特征。图聚合模块、注意力模块和骨架图模块是本研究中提出的特定模块，用于增强点特征提取、点特征到骨架关节特征的映射以及关节特征的提取，以支持SMPL参数回归。"
    },
    {
        "order": 320,
        "title": "Progressive Sparse Local Attention for Video Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Guo_Progressive_Sparse_Local_Attention_for_Video_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Guo_Progressive_Sparse_Local_Attention_for_Video_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.",
        "中文标题": "渐进式稀疏局部注意力用于视频目标检测",
        "摘要翻译": "将基于图像的目标检测器转移到视频领域仍然是一个具有挑战性的问题。之前的努力大多利用光流在帧之间传播特征，旨在实现准确性和效率之间的良好权衡。然而，引入额外的模型来估计光流会显著增加整体模型的大小。光流与高级特征之间的差距也可能阻碍其准确建立空间对应关系。本文提出了一种名为渐进式稀疏局部注意力（PSLA）的新模块，而不是依赖光流，该模块在局部区域内以逐渐稀疏的步长建立帧间特征的空间对应关系，并利用这种对应关系传播特征。基于PSLA，提出了递归特征更新（RFU）和密集特征转换（DenseFT），分别用于建模时间外观和丰富特征表示，在一个新颖的视频目标检测框架中。在ImageNet VID上的实验表明，与现有方法相比，我们的方法在模型大小较小和运行速度可接受的情况下实现了最佳准确性。",
        "领域": "视频目标检测/特征传播/时间建模",
        "问题": "如何有效地将基于图像的目标检测器应用于视频领域，同时保持模型的高准确性和效率。",
        "动机": "现有的方法大多依赖光流进行特征传播，这不仅增加了模型的大小，而且由于光流与高级特征之间的差距，难以准确建立空间对应关系。",
        "方法": "提出了一种名为渐进式稀疏局部注意力（PSLA）的新模块，该模块在局部区域内以逐渐稀疏的步长建立帧间特征的空间对应关系，并利用这种对应关系传播特征。此外，还提出了递归特征更新（RFU）和密集特征转换（DenseFT）来分别建模时间外观和丰富特征表示。",
        "关键词": [
            "视频目标检测",
            "特征传播",
            "时间建模",
            "渐进式稀疏局部注意力",
            "递归特征更新",
            "密集特征转换"
        ],
        "涉及的技术概念": {
            "渐进式稀疏局部注意力（PSLA）": "一种在局部区域内以逐渐稀疏的步长建立帧间特征空间对应关系的新模块。",
            "递归特征更新（RFU）": "用于建模时间外观的技术。",
            "密集特征转换（DenseFT）": "用于丰富特征表示的技术。",
            "光流": "一种估计图像序列中物体运动的技术，常用于视频处理中。"
        }
    },
    {
        "order": 321,
        "title": "View-Consistent 4D Light Field Superpixel Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Khan_View-Consistent_4D_Light_Field_Superpixel_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Khan_View-Consistent_4D_Light_Field_Superpixel_Segmentation_ICCV_2019_paper.html",
        "abstract": "Many 4D light field processing applications rely on superpixel segmentations, for which occlusion-aware view consistency is important. Yet, existing methods often enforce consistency by propagating clusters from a central view only, which can lead to inconsistent superpixels for non-central views. Our proposed approach combines an occlusion-aware angular segmentation in horizontal and vertical EPI spaces with an occlusion-aware clustering and propagation step across all views. Qualitative video demonstrations show that this helps to remove flickering and inconsistent boundary shapes versus the state-of-the-art approach, and quantitative metrics reflect these findings with improved boundary accuracy and view consistency scores.",
        "中文标题": "视角一致的4D光场超像素分割",
        "摘要翻译": "许多4D光场处理应用依赖于超像素分割，其中遮挡感知的视角一致性非常重要。然而，现有方法通常仅通过从中心视角传播聚类来强制一致性，这可能导致非中心视角的超像素不一致。我们提出的方法结合了水平和垂直EPI空间中的遮挡感知角度分割与跨所有视角的遮挡感知聚类和传播步骤。定性视频演示显示，与最先进的方法相比，这有助于消除闪烁和不一致的边界形状，定量指标通过改进的边界准确性和视角一致性得分反映了这些发现。",
        "领域": "光场处理/超像素分割/视角一致性",
        "问题": "解决4D光场处理中非中心视角超像素不一致的问题",
        "动机": "提高4D光场处理应用中超像素分割的视角一致性，消除闪烁和不一致的边界形状",
        "方法": "结合水平和垂直EPI空间中的遮挡感知角度分割与跨所有视角的遮挡感知聚类和传播步骤",
        "关键词": [
            "4D光场",
            "超像素分割",
            "视角一致性",
            "遮挡感知",
            "EPI空间"
        ],
        "涉及的技术概念": {
            "4D光场": "一种捕捉光线方向和强度的技术，用于生成具有深度信息的图像",
            "超像素分割": "将图像分割成具有相似颜色、纹理等特征的区域，这些区域比传统像素更大",
            "视角一致性": "确保从不同视角观察到的图像特征保持一致",
            "遮挡感知": "识别和处理图像中由于物体遮挡导致的信息缺失",
            "EPI空间": "光场数据的一种表示方式，通过水平和垂直方向的视角变化来捕捉场景的深度信息"
        }
    },
    {
        "order": 322,
        "title": "Learning Lightweight Lane Detection CNNs by Self Attention Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Learning_Lightweight_Lane_Detection_CNNs_by_Self_Attention_Distillation_ICCV_2019_paper.html",
        "abstract": "Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing top- down and layer-wise attention distillation within the net- work itself. SAD can be easily incorporated in any feed- forward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet- 18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks.",
        "中文标题": "通过自注意力蒸馏学习轻量级车道检测卷积神经网络",
        "摘要翻译": "由于车道标注中固有的非常微妙和稀疏的监督信号，训练用于车道检测的深度模型具有挑战性。如果没有从更丰富的上下文中学习，这些模型在具有挑战性的场景中往往会失败，例如严重遮挡、模糊车道和不良光照条件。在本文中，我们提出了一种新颖的知识蒸馏方法，即自注意力蒸馏（SAD），它允许模型从自身学习，并在没有任何额外监督或标签的情况下获得实质性改进。具体来说，我们观察到，从训练到合理水平的模型中提取的注意力图会编码丰富的上下文信息。这些有价值的上下文信息可以作为“免费”监督的一种形式，通过在网络内部执行自上而下和逐层的注意力蒸馏来进一步进行表示学习。SAD可以轻松地融入任何前馈卷积神经网络（CNN）中，并且不会增加推理时间。我们在三个流行的车道检测基准（TuSimple、CULane和BDD100K）上使用轻量级模型（如ENet、ResNet-18和ResNet-34）验证了SAD。最轻的模型ENet-SAD表现相当甚至超过了现有算法。值得注意的是，ENet-SAD的参数比最先进的SCNN少20倍，运行速度快10倍，同时在所有基准测试中仍然实现了令人信服的性能。",
        "领域": "车道检测/知识蒸馏/卷积神经网络",
        "问题": "解决在具有挑战性的场景中车道检测模型失败的问题",
        "动机": "由于车道标注中固有的非常微妙和稀疏的监督信号，训练用于车道检测的深度模型具有挑战性",
        "方法": "提出了一种新颖的知识蒸馏方法，即自注意力蒸馏（SAD），允许模型从自身学习，并在没有任何额外监督或标签的情况下获得实质性改进",
        "关键词": [
            "车道检测",
            "知识蒸馏",
            "卷积神经网络",
            "自注意力蒸馏",
            "轻量级模型"
        ],
        "涉及的技术概念": {
            "自注意力蒸馏（SAD）": "一种新颖的知识蒸馏方法，允许模型从自身学习，通过执行自上而下和逐层的注意力蒸馏来进一步进行表示学习",
            "卷积神经网络（CNN）": "一种前馈神经网络，广泛用于图像识别和处理任务",
            "轻量级模型": "指参数较少、计算效率高的模型，如ENet、ResNet-18和ResNet-34"
        }
    },
    {
        "order": 323,
        "title": "AMASS: Archive of Motion Capture As Surface Shapes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mahmood_AMASS_Archive_of_Motion_Capture_As_Surface_Shapes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mahmood_AMASS_Archive_of_Motion_Capture_As_Surface_Shapes_ICCV_2019_paper.html",
        "abstract": "Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with markerbased mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community.",
        "中文标题": "AMASS：动作捕捉作为表面形状的档案",
        "摘要翻译": "大型数据集是近年来使用深度学习的计算机视觉进展的基石。相比之下，现有的人类动作捕捉（mocap）数据集规模小且动作有限，阻碍了学习人类动作模型的进展。虽然有许多不同的数据集可用，但它们各自使用不同的身体参数化，使得将它们整合到一个单一的元数据集中变得困难。为了解决这个问题，我们引入了AMASS，这是一个大型且多样化的人类动作数据库，通过在一个共同的框架和参数化内表示它们，统一了15个不同的基于光学标记的mocap数据集。我们使用一种新方法MoSh++实现了这一点，该方法将mocap数据转换为由绑定身体模型表示的逼真3D人体网格。这里我们使用SMPL [Loper et al., 2015]，它被广泛使用，并提供了标准的骨骼表示以及完全绑定的表面网格。该方法适用于任意标记集，同时恢复软组织动力学和逼真的手部动作。我们使用一个新的4D身体扫描数据集评估MoSh++并调整其超参数，该数据集与基于标记的mocap联合记录。AMASS的一致表示使其对于动画、可视化和生成深度学习训练数据非常有用。我们的数据集比以前的人类动作集合要丰富得多，拥有超过40小时的动作数据，涵盖300多个受试者，超过11000个动作，并将公开提供给研究社区。",
        "领域": "动作捕捉/3D建模/深度学习",
        "问题": "现有的人类动作捕捉数据集规模小、动作有限，且各自使用不同的身体参数化，难以整合",
        "动机": "为了促进学习人类动作模型的进展，需要一个大而多样化的人类动作数据库",
        "方法": "引入AMASS数据库，使用MoSh++方法将mocap数据转换为逼真的3D人体网格，统一15个不同的mocap数据集",
        "关键词": [
            "动作捕捉",
            "3D建模",
            "深度学习"
        ],
        "涉及的技术概念": "AMASS是一个大型且多样化的人类动作数据库，通过MoSh++方法将mocap数据转换为逼真的3D人体网格，使用SMPL模型提供标准的骨骼表示和完全绑定的表面网格，适用于任意标记集，恢复软组织动力学和逼真的手部动作。"
    },
    {
        "order": 324,
        "title": "GLoSH: Global-Local Spherical Harmonics for Intrinsic Image Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_GLoSH_Global-Local_Spherical_Harmonics_for_Intrinsic_Image_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_GLoSH_Global-Local_Spherical_Harmonics_for_Intrinsic_Image_Decomposition_ICCV_2019_paper.html",
        "abstract": "Traditional intrinsic image decomposition focuses on decomposing images into reflectance and shading, leaving surfaces normals and lighting entangled in shading. In this work, we propose a Global-Local Spherical Harmonics (GLoSH) lighting model to improve the lighting component, and jointly predict reflectance and surface normals. The global SH models the holistic lighting while local SH account for the spatial variation of lighting. Also, a novel non-negative lighting constraint is proposed to encourage the estimated SH to be physically meaningful. To seamlessly reflect the GLoSH model, we design a coarse-to-fine network structure. The coarse network predicts global SH, reflectance and normals, and the fine network predicts their local residuals. Lacking labels for reflectance and lighting, we apply synthetic data for model pre-training and fine-tune the model with real data in a self-supervised way. Compared to the state-of-the-art methods only targeting normals or reflectance and shading, our method recovers all components and achieves consistently better results on three real datasets, IIW, SAW and NYUv2.",
        "中文标题": "GLoSH: 用于本征图像分解的全局-局部球谐函数",
        "摘要翻译": "传统的本征图像分解主要关注将图像分解为反射率和阴影，而将表面法线和光照纠缠在阴影中。在本研究中，我们提出了一种全局-局部球谐函数（GLoSH）光照模型来改进光照成分，并联合预测反射率和表面法线。全局球谐函数模型化整体光照，而局部球谐函数则考虑光照的空间变化。此外，提出了一种新的非负光照约束，以鼓励估计的球谐函数具有物理意义。为了无缝反映GLoSH模型，我们设计了一个从粗到细的网络结构。粗网络预测全局球谐函数、反射率和法线，而细网络预测它们的局部残差。由于缺乏反射率和光照的标签，我们使用合成数据进行模型预训练，并以自监督的方式用真实数据微调模型。与仅针对法线或反射率和阴影的最先进方法相比，我们的方法恢复了所有成分，并在三个真实数据集IIW、SAW和NYUv2上取得了一致更好的结果。",
        "领域": "本征图像分解/光照估计/表面法线预测",
        "问题": "如何更准确地分解图像中的反射率、阴影、表面法线和光照",
        "动机": "传统方法在分解图像时，表面法线和光照在阴影中纠缠不清，影响了分解的准确性",
        "方法": "提出了一种全局-局部球谐函数（GLoSH）光照模型，设计了一个从粗到细的网络结构，使用合成数据进行预训练，并以自监督的方式用真实数据微调模型",
        "关键词": [
            "本征图像分解",
            "球谐函数",
            "光照估计",
            "表面法线预测",
            "自监督学习"
        ],
        "涉及的技术概念": "全局-局部球谐函数（GLoSH）光照模型用于改进光照成分，非负光照约束确保估计的球谐函数具有物理意义，从粗到细的网络结构用于预测全局和局部成分，使用合成数据进行预训练和自监督微调以提高模型在真实数据上的表现。"
    },
    {
        "order": 325,
        "title": "SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_SplitNet_Sim2Sim_and_Task2Task_Transfer_for_Embodied_Visual_Navigation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gordon_SplitNet_Sim2Sim_and_Task2Task_Transfer_for_Embodied_Visual_Navigation_ICCV_2019_paper.html",
        "abstract": "We propose SplitNet, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset",
        "中文标题": "SplitNet: 从仿真到仿真和任务到任务的迁移用于具身视觉导航",
        "摘要翻译": "我们提出了SplitNet，一种用于解耦视觉感知和策略学习的方法。通过引入辅助任务和选择性学习模型的部分，我们明确地将视觉导航的学习目标分解为感知世界和基于感知行动。我们展示了在模拟器之间迁移时相对于基线模型的改进，这是向仿真到现实（Sim2Real）迈出的鼓舞人心的一步。此外，SplitNet在相同模拟器的未见环境中泛化得更好，并且更快更有效地迁移到新的具身导航任务。进一步地，仅给定目标域的一小部分样本，SplitNet就能匹配传统端到端管道的性能，后者接收整个数据集。",
        "领域": "具身视觉导航/仿真到现实迁移/辅助任务学习",
        "问题": "解耦视觉感知和策略学习以提高视觉导航任务的性能",
        "动机": "提高视觉导航模型在模拟器之间迁移的能力，以及在新环境和任务中的泛化能力和迁移效率",
        "方法": "通过引入辅助任务和选择性学习模型的部分，明确分解视觉导航的学习目标为感知世界和基于感知行动",
        "关键词": [
            "具身视觉导航",
            "仿真到现实迁移",
            "辅助任务学习"
        ],
        "涉及的技术概念": "SplitNet是一种方法，旨在通过解耦视觉感知和策略学习来改进视觉导航任务。它通过引入辅助任务和选择性学习模型的部分来实现这一点，从而明确地将学习目标分解为感知世界和基于感知行动。这种方法在模拟器之间的迁移、在新环境和任务中的泛化能力以及迁移效率方面显示出改进。"
    },
    {
        "order": 326,
        "title": "Surface Normals and Shape From Water",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Murai_Surface_Normals_and_Shape_From_Water_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Murai_Surface_Normals_and_Shape_From_Water_ICCV_2019_paper.html",
        "abstract": "In this paper, we introduce a novel method for reconstructing surface normals and depth of dynamic objects in water. Past shape recovery methods have leveraged various visual cues for estimating shape (e.g., depth) or surface normals. Methods that estimate both compute one from the other. We show that these two geometric surface properties can be simultaneously recovered for each pixel when the object is observed underwater. Our key idea is to leverage multi-wavelength near-infrared light absorption along different underwater light paths in conjunction with surface shading. We derive a principled theory for this surface normals and shape from water method and a practical calibration method for determining its imaging parameters values. By construction, the method can be implemented as a one-shot imaging system. We prototype both an off-line and a video-rate imaging system and demonstrate the effectiveness of the method on a number of real-world static and dynamic objects. The results show that the method can recover intricate surface features that are otherwise inaccessible.",
        "中文标题": "水面法线和形状重建",
        "摘要翻译": "本文介绍了一种新颖的方法，用于重建水中动态物体的表面法线和深度。过去的形状恢复方法利用了各种视觉线索来估计形状（例如，深度）或表面法线。估计这两种几何表面属性的方法通常是从一个计算另一个。我们展示了当物体在水下被观察时，这两种几何表面属性可以同时为每个像素恢复。我们的关键思想是利用多波长近红外光在不同水下光路中的吸收与表面阴影相结合。我们为这种水面法线和形状重建方法推导出了一个原理性的理论，并提供了一个实用的校准方法来确定其成像参数值。通过构建，该方法可以实现为一次性成像系统。我们原型化了一个离线和视频速率成像系统，并在多个现实世界的静态和动态物体上展示了该方法的有效性。结果表明，该方法可以恢复原本无法访问的复杂表面特征。",
        "领域": "水下成像/表面重建/近红外成像",
        "问题": "重建水中动态物体的表面法线和深度",
        "动机": "现有的形状恢复方法通常只能估计形状或表面法线之一，而无法同时恢复这两种几何表面属性。",
        "方法": "利用多波长近红外光在不同水下光路中的吸收与表面阴影相结合，推导出一个原理性的理论，并实现为一次性成像系统。",
        "关键词": [
            "水下成像",
            "表面重建",
            "近红外成像"
        ],
        "涉及的技术概念": "多波长近红外光吸收、表面阴影、一次性成像系统、视频速率成像系统"
    },
    {
        "order": 327,
        "title": "Cascaded Parallel Filtering for Memory-Efficient Image-Based Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Cascaded_Parallel_Filtering_for_Memory-Efficient_Image-Based_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Cascaded_Parallel_Filtering_for_Memory-Efficient_Image-Based_Localization_ICCV_2019_paper.html",
        "abstract": "Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner.",
        "中文标题": "级联并行过滤用于内存高效的基于图像的定位",
        "摘要翻译": "基于图像的定位（IBL）旨在估计给定查询图像的6自由度相机姿态。相机姿态可以从查询图像与从运动恢复结构（SfM）模型之间的2D-3D匹配中计算得出。尽管IBL最近取得了进展，但同时解决大型SfM模型的内存消耗和匹配模糊问题仍然很困难。在这项工作中，我们提出了一种级联并行过滤方法，该方法利用特征、可见性和几何信息在二进制特征表示下过滤错误匹配。核心思想是，在得出辅助相机姿态进行最终过滤之前，我们将具有挑战性的过滤任务分为两个并行任务。一个任务专注于保留可能正确的匹配，而另一个任务专注于获得高质量匹配以促进后续更强大的过滤。此外，我们提出的方法通过引入质量感知的空间重构方法和主焦距增强的姿态估计方法提高了定位精度。在真实世界数据集上的实验结果表明，我们的方法以内存高效的方式实现了非常有竞争力的定位性能。",
        "领域": "图像定位/相机姿态估计/结构从运动",
        "问题": "解决大型SfM模型的内存消耗和匹配模糊问题",
        "动机": "提高基于图像的定位的准确性和内存效率",
        "方法": "提出了一种级联并行过滤方法，利用特征、可见性和几何信息过滤错误匹配，并通过质量感知的空间重构和主焦距增强的姿态估计提高定位精度",
        "关键词": [
            "图像定位",
            "相机姿态估计",
            "结构从运动"
        ],
        "涉及的技术概念": "6自由度相机姿态、2D-3D匹配、从运动恢复结构（SfM）模型、二进制特征表示、质量感知的空间重构方法、主焦距增强的姿态估计方法"
    },
    {
        "order": 328,
        "title": "Reasoning About Human-Object Interactions Through Dual Attention Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_Reasoning_About_Human-Object_Interactions_Through_Dual_Attention_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_Reasoning_About_Human-Object_Interactions_Through_Dual_Attention_Networks_ICCV_2019_paper.html",
        "abstract": "Objects are entities we act upon, where the functionality of an object is determined by how we interact with it. In this work we propose a Dual Attention Network model which reasons about human-object interactions. The dual-attentional framework weights the important features for objects and actions respectively. As a result, the recognition of objects and actions mutually benefit each other. The proposed model shows competitive classification performance on the human-object interaction dataset Something-Something. Besides, it can perform weak spatiotemporal localization and affordance segmentation, despite being trained only with video-level labels. The model not only finds when an action is happening and which object is being manipulated, but also identifies which part of the object is being interacted with.",
        "中文标题": "通过双注意力网络推理人-物交互",
        "摘要翻译": "物体是我们作用的对象，物体的功能由我们如何与之交互决定。在这项工作中，我们提出了一个双注意力网络模型，用于推理人-物交互。这个双注意力框架分别对物体和动作的重要特征进行加权。结果，物体和动作的识别相互受益。所提出的模型在Something-Something人-物交互数据集上展示了有竞争力的分类性能。此外，尽管仅使用视频级标签进行训练，它仍能执行弱时空定位和功能分割。该模型不仅能发现动作何时发生以及哪个物体被操作，还能识别出物体的哪个部分正在被交互。",
        "领域": "人机交互/视频理解/功能分割",
        "问题": "如何有效地推理和理解人-物交互",
        "动机": "理解人-物交互对于视频理解和功能分割等任务至关重要，但现有方法在处理这一问题时存在局限性。",
        "方法": "提出了一个双注意力网络模型，该模型通过加权物体和动作的重要特征来推理人-物交互，从而实现了物体和动作识别的相互促进。",
        "关键词": [
            "双注意力网络",
            "人-物交互",
            "功能分割",
            "视频理解"
        ],
        "涉及的技术概念": "双注意力网络是一种模型，它通过分别对物体和动作的重要特征进行加权来推理人-物交互。这种方法不仅提高了物体和动作识别的准确性，还能在没有详细标注的情况下进行弱时空定位和功能分割。"
    },
    {
        "order": 329,
        "title": "Person-in-WiFi: Fine-Grained Person Perception Using WiFi",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Person-in-WiFi_Fine-Grained_Person_Perception_Using_WiFi_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Person-in-WiFi_Fine-Grained_Person_Perception_Using_WiFi_ICCV_2019_paper.html",
        "abstract": "Fine-grained person perception such as body segmentation and pose estimation has been achieved with many 2D and 3D sensors such as RGB/depth cameras, radars (e.g. RF-Pose), and LiDARs. These solutions require 2D images, depth maps or 3D point clouds of person bodies as input. In this paper, we take one step forward to show that fine-grained person perception is possible even with 1D sensors: WiFi antennas. Specifically, we used two sets of WiFi antennas to acquire signals, i.e., one transmitter set and one receiver set. Each set contains three antennas horizontally lined-up as a regular household WiFi router. The WiFi signal generated by a transmitter antenna, penetrates through and reflects on human bodies, furniture, and walls, and then superposes at a receiver antenna as 1D signal samples. We developed a deep learning approach that uses annotations on 2D images, takes the received 1D WiFi signals as input, and performs body segmentation and pose estimation in an end-to-end manner. To our knowledge, our solution is the first work based on off-the-shelf WiFi antennas and standard IEEE 802.11n WiFi signals. Demonstrating comparable results to image-based solutions, our WiFi-based person perception solution is cheaper and more ubiquitous than radars and LiDARs, while invariant to illumination and has little privacy concern comparing to cameras.",
        "中文标题": "WiFi中的人：使用WiFi进行细粒度的人体感知",
        "摘要翻译": "细粒度的人体感知，如身体分割和姿态估计，已经通过许多2D和3D传感器实现，如RGB/深度相机、雷达（例如RF-Pose）和LiDAR。这些解决方案需要人体的2D图像、深度图或3D点云作为输入。在本文中，我们进一步展示了即使使用1D传感器：WiFi天线，也能实现细粒度的人体感知。具体来说，我们使用了两组WiFi天线来获取信号，即一组发射天线和一组接收天线。每组包含三个水平排列的天线，类似于普通家用WiFi路由器。发射天线产生的WiFi信号穿透并反射在人体、家具和墙壁上，然后在接收天线处叠加为1D信号样本。我们开发了一种深度学习方法，该方法使用2D图像上的注释，将接收到的1D WiFi信号作为输入，并以端到端的方式执行身体分割和姿态估计。据我们所知，我们的解决方案是基于现成的WiFi天线和标准IEEE 802.11n WiFi信号的首次工作。与基于图像的解决方案相比，我们的基于WiFi的人体感知解决方案更便宜、更普遍，同时对光照不变，与相机相比几乎没有隐私问题。",
        "领域": "人体感知/无线传感/深度学习",
        "问题": "如何利用WiFi信号实现细粒度的人体感知",
        "动机": "探索使用成本更低、更普遍的WiFi信号进行人体感知，同时解决传统方法中的隐私和光照问题",
        "方法": "使用两组WiFi天线获取信号，开发深度学习方法，利用2D图像注释和1D WiFi信号进行身体分割和姿态估计",
        "关键词": [
            "WiFi信号",
            "人体感知",
            "深度学习",
            "身体分割",
            "姿态估计"
        ],
        "涉及的技术概念": "WiFi天线、1D信号样本、深度学习、身体分割、姿态估计、IEEE 802.11n WiFi信号"
    },
    {
        "order": 330,
        "title": "Restoration of Non-Rigidly Distorted Underwater Images Using a Combination of Compressive Sensing and Local Polynomial Image Representations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/James_Restoration_of_Non-Rigidly_Distorted_Underwater_Images_Using_a_Combination_of_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/James_Restoration_of_Non-Rigidly_Distorted_Underwater_Images_Using_a_Combination_of_ICCV_2019_paper.html",
        "abstract": "Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage. The source code, datasets and supplemental material can be accessed at [??], [??].",
        "中文标题": "使用压缩感知和局部多项式图像表示结合恢复非刚性扭曲的水下图像",
        "摘要翻译": "静态场景在水波表面下的图像表现出严重的非刚性扭曲。水流的物理特性表明，水面具有时空平滑性和时间周期性。因此，它们在三维离散傅里叶（DFT）基中具有稀疏表示。受此启发，我们将此类视频序列的恢复任务提出为压缩感知（CS）问题。我们首先通过跟踪水下场景视频序列帧中的一些显著特征点开始。利用这些点的轨迹，我们展示了所有其他（未跟踪的）点的运动场可以使用典型的CS求解器有效估计。这本身在非刚性运动估计领域是一个新颖的贡献。我们展示了这种方法在水下图像恢复方面优于现有技术算法。我们进一步考虑了一种基于图像帧局部多项式扩展的简单光流算法（PEOF）。令人惊讶的是，我们证明了PEOF在数值测量方面更有效，并且经常优于所有现有技术方法。最后，我们证明了由CS步骤和PEOF组成的两阶段方法比仅使用PEOF阶段更准确地保留了图像结构，并提高了（视觉和数值）视频质量。源代码、数据集和补充材料可以在[??]，[??]访问。",
        "领域": "水下图像恢复/非刚性运动估计/压缩感知",
        "问题": "恢复因水波表面引起的非刚性扭曲的水下图像",
        "动机": "利用水面的时空平滑性和时间周期性，通过压缩感知技术恢复水下图像",
        "方法": "首先跟踪视频序列中的显著特征点，利用这些点的轨迹估计未跟踪点的运动场，然后结合局部多项式扩展的光流算法（PEOF）进行图像恢复",
        "关键词": [
            "水下图像恢复",
            "非刚性运动估计",
            "压缩感知",
            "局部多项式扩展",
            "光流算法"
        ],
        "涉及的技术概念": {
            "压缩感知（CS）": "一种信号处理技术，用于从少量测量中恢复信号，基于信号的稀疏性",
            "局部多项式扩展（PEOF）": "一种基于图像帧局部多项式扩展的光流算法，用于估计图像序列中的运动场",
            "三维离散傅里叶变换（3D DFT）": "一种将信号从时域转换到频域的数学工具，用于分析信号的频率成分"
        }
    },
    {
        "order": 331,
        "title": "Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wen_Pixel2Mesh_Multi-View_3D_Mesh_Generation_via_Deformation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wen_Pixel2Mesh_Multi-View_3D_Mesh_Generation_via_Deformation_ICCV_2019_paper.html",
        "abstract": "We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.",
        "中文标题": "Pixel2Mesh++: 通过变形实现多视图3D网格生成",
        "摘要翻译": "我们研究了从已知相机姿态的少量彩色图像生成3D网格表示中的形状问题。虽然许多先前的工作学习直接从先验中幻想形状，但我们通过利用图卷积网络的跨视图信息来进一步提高形状质量。我们的模型不是建立从图像到3D形状的直接映射函数，而是学习预测一系列变形以迭代改进粗糙形状。受传统多视图几何方法的启发，我们的网络在初始网格顶点位置附近采样，并使用从多个输入图像构建的感知特征统计来推理最佳变形。大量实验表明，我们的模型生成的准确3D形状不仅从输入视角看起来视觉上合理，而且与任意视角对齐良好。在物理驱动架构的帮助下，我们的模型还展示了跨不同语义类别、输入图像数量和网格初始化质量的泛化能力。",
        "领域": "3D形状生成/图卷积网络/多视图几何",
        "问题": "从少量已知相机姿态的彩色图像生成3D网格表示中的形状",
        "动机": "提高从少量图像生成3D形状的质量，通过利用跨视图信息和迭代改进形状",
        "方法": "使用图卷积网络预测一系列变形以迭代改进粗糙形状，并利用多视图几何方法推理最佳变形",
        "关键词": [
            "3D形状生成",
            "图卷积网络",
            "多视图几何",
            "形状变形",
            "感知特征统计"
        ],
        "涉及的技术概念": "图卷积网络用于处理和分析图结构数据，多视图几何方法用于从多个视角推理形状，感知特征统计用于评估和优化形状变形。"
    },
    {
        "order": 332,
        "title": "DMM-Net: Differentiable Mask-Matching Network for Video Object Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_DMM-Net_Differentiable_Mask-Matching_Network_for_Video_Object_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_DMM-Net_Differentiable_Mask-Matching_Network_for_Video_Object_Segmentation_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose the differentiable mask-matching network (DMM-Net) for solving the video object segmentation problem where the initial object masks are provided. Relying on the Mask R-CNN backbone, we extract mask proposals per frame and formulate the matching between object templates and proposals as a linear assignment problem where thA heading inside a blocke cost matrix is predicted by a deep convolutional neural network. We propose a differentiable matching layer which unrolls a projected gradient descent algorithm in which the projection step exploits the Dykstra's algorithm. We prove that under mild conditions, the matching is guaranteed to converge to the optimal one. In practice, it achieves similar performance compared to the Hungarian algorithm during inference. Meanwhile, we can back-propagate through it to learn the cost matrix. After matching, a U-Net style architecture is exploited to refine the matched mask per time step. On DAVIS 2017 dataset, DMM-Net achieves the best performance without online learning on the first frames and the 2nd best with it. Without any fine-tuning, DMM-Net performs comparably to state-of-the-art methods on SegTrack v2 dataset. At last, our differentiable matching layer is very simple to implement; we attach the PyTorch code in the supplementary material which is less than 50 lines long.",
        "中文标题": "DMM-Net: 可微分掩码匹配网络用于视频对象分割",
        "摘要翻译": "本文提出了一种可微分掩码匹配网络（DMM-Net），用于解决提供初始对象掩码的视频对象分割问题。基于Mask R-CNN骨干网络，我们每帧提取掩码提议，并将对象模板与提议之间的匹配问题表述为一个线性分配问题，其中成本矩阵由深度卷积神经网络预测。我们提出了一种可微分匹配层，该层展开了一个投影梯度下降算法，其中投影步骤利用了Dykstra算法。我们证明了在温和条件下，匹配保证收敛到最优解。在实践中，它在推理过程中实现了与匈牙利算法相似的性能。同时，我们可以通过它反向传播以学习成本矩阵。匹配后，利用U-Net风格的架构来细化每个时间步的匹配掩码。在DAVIS 2017数据集上，DMM-Net在没有在线学习第一帧的情况下实现了最佳性能，在有在线学习的情况下实现了第二佳性能。无需任何微调，DMM-Net在SegTrack v2数据集上的表现与最先进的方法相当。最后，我们的可微分匹配层实现非常简单；我们在补充材料中附上了不到50行的PyTorch代码。",
        "领域": "视频对象分割/深度学习/计算机视觉",
        "问题": "视频对象分割问题，特别是在提供初始对象掩码的情况下",
        "动机": "提高视频对象分割的准确性和效率，特别是在处理动态场景和复杂背景时",
        "方法": "提出了一种可微分掩码匹配网络（DMM-Net），利用Mask R-CNN提取掩码提议，通过深度卷积神经网络预测成本矩阵，并采用可微分匹配层进行优化",
        "关键词": [
            "视频对象分割",
            "可微分匹配",
            "Mask R-CNN",
            "U-Net",
            "Dykstra算法"
        ],
        "涉及的技术概念": {
            "Mask R-CNN": "一种用于对象检测和分割的深度学习模型，能够生成高质量的掩码",
            "可微分匹配层": "一种新的网络层，能够通过反向传播学习成本矩阵，优化对象模板与提议之间的匹配",
            "Dykstra算法": "一种用于解决约束优化问题的算法，用于在可微分匹配层中进行投影步骤",
            "U-Net": "一种常用于图像分割的卷积神经网络架构，用于细化匹配后的掩码"
        }
    },
    {
        "order": 333,
        "title": "FAB: A Robust Facial Landmark Detection Framework for Motion-Blurred Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_FAB_A_Robust_Facial_Landmark_Detection_Framework_for_Motion-Blurred_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_FAB_A_Robust_Facial_Landmark_Detection_Framework_for_Motion-Blurred_Videos_ICCV_2019_paper.html",
        "abstract": "Recently, facial landmark detection algorithms have achieved remarkable performance on static images. However, these algorithms are neither accurate nor stable in motion-blurred videos. The missing of structure information makes it difficult for state-of-the-art facial landmark detection algorithms to yield good results. In this paper, we propose a framework named FAB that takes advantage of structure consistency in the temporal dimension for facial landmark detection in motion-blurred videos. A structure predictor is proposed to predict the missing face structural information temporally, which serves as a geometry prior. This allows our framework to work as a virtuous circle. On one hand, the geometry prior helps our structure-aware deblurring network generates high quality deblurred images which lead to better landmark detection results. On the other hand, better landmark detection results help structure predictor generate better geometry prior for the next frame. Moreover, it is a flexible video-based framework that can incorporate any static image-based methods to provide a performance boost on video datasets. Extensive experiments on Blurred-300VW, the proposed Real-world Motion Blur (RWMB) datasets and 300VW demonstrate the superior performance to the state-of-the-art methods. Datasets and model will be publicly available at  https://github.com/KeqiangSun/FAB  https://github.com/KeqiangSun/FAB .",
        "中文标题": "FAB: 一种用于运动模糊视频的鲁棒面部标志检测框架",
        "摘要翻译": "最近，面部标志检测算法在静态图像上取得了显著的性能。然而，这些算法在运动模糊的视频中既不准确也不稳定。结构信息的缺失使得最先进的面部标志检测算法难以产生良好的结果。在本文中，我们提出了一个名为FAB的框架，该框架利用时间维度上的结构一致性来进行运动模糊视频中的面部标志检测。提出了一个结构预测器来预测缺失的面部结构信息，这作为几何先验。这使得我们的框架能够作为一个良性循环工作。一方面，几何先验帮助我们的结构感知去模糊网络生成高质量的去模糊图像，从而带来更好的标志检测结果。另一方面，更好的标志检测结果帮助结构预测器为下一帧生成更好的几何先验。此外，它是一个灵活的基于视频的框架，可以结合任何基于静态图像的方法，以在视频数据集上提供性能提升。在Blurred-300VW、提出的真实世界运动模糊（RWMB）数据集和300VW上的大量实验证明了其优于最先进方法的性能。数据集和模型将在https://github.com/KeqiangSun/FAB公开提供。",
        "领域": "面部标志检测/视频处理/去模糊",
        "问题": "在运动模糊的视频中进行准确和稳定的面部标志检测",
        "动机": "现有的面部标志检测算法在运动模糊的视频中表现不佳，因为结构信息的缺失",
        "方法": "提出了一个名为FAB的框架，利用时间维度上的结构一致性，通过结构预测器预测缺失的面部结构信息，作为几何先验，形成一个良性循环",
        "关键词": [
            "面部标志检测",
            "运动模糊",
            "去模糊",
            "结构预测器",
            "几何先验"
        ],
        "涉及的技术概念": "FAB框架、结构预测器、几何先验、结构感知去模糊网络、Blurred-300VW数据集、真实世界运动模糊（RWMB）数据集、300VW数据集"
    },
    {
        "order": 334,
        "title": "Learning Perspective Undistortion of Portraits",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Learning_Perspective_Undistortion_of_Portraits_ICCV_2019_paper.html",
        "abstract": "Near-range portrait photographs often contain perspective distortion artifacts that bias human perception and challenge both facial recognition and reconstruction techniques. We present the first deep learning based approach to remove such artifacts from unconstrained portraits. In contrast to the previous state-of-the-art approach [??], our method handles even portraits with extreme perspective distortion, as we avoid the inaccurate and error-prone step of first fitting a 3D face model. Instead, we predict a distortion correction flow map that encodes a per-pixel displacement that removes distortion artifacts when applied to the input image. Our method also automatically infers missing facial features, i.e. occluded ears caused by strong perspective distortion, with coherent details. We demonstrate that our approach significantly outperforms the previous state-of-the-art both qualitatively and quantitatively, particularly for portraits with extreme perspective distortion or facial expressions. We further show that our technique benefits a number of fundamental tasks, significantly improving the accuracy of both face recognition and 3D reconstruction and enables a novel camera calibration technique from a single portrait. Moreover, we also build the first perspective portrait database with a large diversity in identities, expression and poses.",
        "中文标题": "学习肖像的透视失真校正",
        "摘要翻译": "近景肖像照片通常包含透视失真伪影，这些伪影会扭曲人类感知，并对面部识别和重建技术构成挑战。我们提出了第一个基于深度学习的方法，用于从无约束的肖像中去除此类伪影。与之前的最先进方法[??]相比，我们的方法甚至能够处理具有极端透视失真的肖像，因为我们避免了首先拟合3D面部模型这一不准确且容易出错的步骤。相反，我们预测一个失真校正流图，该流图编码了每个像素的位移，当应用于输入图像时，可以去除失真伪影。我们的方法还能自动推断出缺失的面部特征，即由于强烈透视失真而遮挡的耳朵，并保持细节的一致性。我们证明，我们的方法在质量和数量上都显著优于之前的最先进方法，特别是对于具有极端透视失真或面部表情的肖像。我们进一步展示了我们的技术对许多基本任务有益，显著提高了面部识别和3D重建的准确性，并实现了一种从单张肖像进行相机校准的新技术。此外，我们还建立了第一个具有身份、表情和姿势多样性的透视肖像数据库。",
        "领域": "面部识别/3D重建/相机校准",
        "问题": "近景肖像照片中的透视失真伪影问题",
        "动机": "解决透视失真伪影对面部识别和重建技术的挑战",
        "方法": "预测失真校正流图，自动推断缺失的面部特征",
        "关键词": [
            "透视失真",
            "面部识别",
            "3D重建",
            "相机校准"
        ],
        "涉及的技术概念": "透视失真伪影、3D面部模型拟合、失真校正流图、面部特征推断、相机校准技术"
    },
    {
        "order": 335,
        "title": "A Differential Volumetric Approach to Multi-View Photometric Stereo",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Logothetis_A_Differential_Volumetric_Approach_to_Multi-View_Photometric_Stereo_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Logothetis_A_Differential_Volumetric_Approach_to_Multi-View_Photometric_Stereo_ICCV_2019_paper.html",
        "abstract": "Highly accurate 3D volumetric reconstruction is still an open research topic where the main difficulty is usually related to merging some rough estimations with high frequency details. One of the most promising methods is the fusion between multi-view stereo and photometric stereo images. Beside the intrinsic difficulties that multi-view stereo and photometric stereo in order to work reliably, supplementary problems arise when considered together. In this work, we present a volumetric approach to the multi-view photometric stereo problem. The key point of our method is the signed distance field parameterisation and its relation to the surface normal. This is exploited in order to obtain a linear partial differential equation which is solved in a variational framework, that combines multiple images from multiple points of view in a single system. In addition, the volumetric approach is naturally implemented on an octree, which allows for fast ray-tracing that reliably alleviates occlusions and cast shadows. Our approach is evaluated on synthetic and real data-sets and achieves state-of-the-art results.",
        "中文标题": "多视角光度立体学的微分体积方法",
        "摘要翻译": "高精度的3D体积重建仍然是一个开放的研究课题，其中主要的困难通常与将一些粗略估计与高频细节合并有关。最有前途的方法之一是多视角立体和光度立体图像的融合。除了多视角立体和光度立体为了可靠工作所固有的困难外，当它们被一起考虑时还会出现额外的问题。在这项工作中，我们提出了一种针对多视角光度立体问题的体积方法。我们方法的关键点是符号距离场参数化及其与表面法线的关系。这是为了获得一个线性偏微分方程，该方程在变分框架中求解，将来自多个视角的多个图像组合在一个系统中。此外，体积方法自然地在八叉树上实现，这允许快速光线追踪，可靠地减轻遮挡和投射阴影。我们的方法在合成和真实数据集上进行了评估，并达到了最先进的结果。",
        "领域": "3D重建/光度立体学/多视角立体",
        "问题": "高精度的3D体积重建，特别是将多视角立体和光度立体图像融合以合并粗略估计与高频细节",
        "动机": "解决多视角立体和光度立体图像融合时出现的困难，实现高精度的3D体积重建",
        "方法": "提出了一种体积方法，利用符号距离场参数化及其与表面法线的关系，获得线性偏微分方程并在变分框架中求解，结合多个视角的图像，并在八叉树上实现快速光线追踪以减轻遮挡和投射阴影",
        "关键词": [
            "3D重建",
            "光度立体学",
            "多视角立体",
            "符号距离场",
            "线性偏微分方程",
            "变分框架",
            "八叉树",
            "光线追踪"
        ],
        "涉及的技术概念": {
            "符号距离场": "一种表示物体表面距离的方法，用于参数化体积数据",
            "线性偏微分方程": "在数学中，描述物理现象的一种方程，这里用于3D重建",
            "变分框架": "一种数学框架，用于求解优化问题，这里用于求解线性偏微分方程",
            "八叉树": "一种树数据结构，用于空间分割，支持快速光线追踪",
            "光线追踪": "一种渲染技术，用于模拟光线与物体的交互，这里用于减轻遮挡和投射阴影"
        }
    },
    {
        "order": 336,
        "title": "Towards Photorealistic Reconstruction of Highly Multiplexed Lensless Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Khan_Towards_Photorealistic_Reconstruction_of_Highly_Multiplexed_Lensless_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Khan_Towards_Photorealistic_Reconstruction_of_Highly_Multiplexed_Lensless_Images_ICCV_2019_paper.html",
        "abstract": "Recent advancements in fields like Internet of Things (IoT), augmented reality, etc. have led to an unprecedented demand for miniature cameras with low cost that can be integrated anywhere and can be used for distributed monitoring. Mask-based lensless imaging systems make such inexpensive and compact models realizable. However, reduction in the size and cost of these imagers comes at the expense of their image quality due to the high degree of multiplexing inherent in their design. In this paper, we present a method to obtain image reconstructions from mask-based lensless measurements that are more photorealistic than those currently available in the literature. We particularly focus on FlatCam, a lensless imager consisting of a coded mask placed over a bare CMOS sensor. Existing techniques for reconstructing FlatCam measurements suffer from several drawbacks including lower resolution and dynamic range than lens-based cameras. Our approach overcomes these drawbacks using a fully trainable non-iterative deep learning based model. Our approach is based on two stages: an inversion stage that maps the measurement into the space of intermediate reconstruction and a perceptual enhancement stage that improves this intermediate reconstruction based on perceptual and signal distortion metrics. Our proposed method is fast and produces photo-realistic reconstruction as demonstrated on many real and challenging scenes.",
        "中文标题": "迈向高度多路复用无透镜图像的光真实感重建",
        "摘要翻译": "近年来，物联网（IoT）、增强现实等领域的进步导致了对低成本微型相机的空前需求，这些相机可以集成到任何地方，并可用于分布式监控。基于掩模的无透镜成像系统使得这种廉价且紧凑的模型成为可能。然而，由于设计中固有的高度多路复用，这些成像器在尺寸和成本上的降低是以牺牲图像质量为代价的。在本文中，我们提出了一种从基于掩模的无透镜测量中获得图像重建的方法，这些重建比目前文献中可用的更具光真实感。我们特别关注FlatCam，这是一种由放置在裸CMOS传感器上的编码掩模组成的无透镜成像器。现有的FlatCam测量重建技术存在几个缺点，包括分辨率低于基于透镜的相机和动态范围。我们的方法通过使用完全可训练的非迭代深度学习模型克服了这些缺点。我们的方法基于两个阶段：一个反演阶段，将测量映射到中间重建空间；一个感知增强阶段，基于感知和信号失真度量改进这个中间重建。我们提出的方法速度快，并在许多真实和具有挑战性的场景中展示了光真实感重建。",
        "领域": "无透镜成像/深度学习/图像重建",
        "问题": "提高基于掩模的无透镜成像系统的图像质量",
        "动机": "满足物联网、增强现实等领域对低成本、微型相机的需求，同时克服现有无透镜成像系统在图像质量上的不足",
        "方法": "采用两阶段方法：反演阶段将测量映射到中间重建空间，感知增强阶段基于感知和信号失真度量改进中间重建",
        "关键词": [
            "无透镜成像",
            "图像重建",
            "深度学习"
        ],
        "涉及的技术概念": "基于掩模的无透镜成像系统、FlatCam、完全可训练的非迭代深度学习模型、反演阶段、感知增强阶段、感知和信号失真度量"
    },
    {
        "order": 337,
        "title": "Attentional Feature-Pair Relation Networks for Accurate Face Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Attentional_Feature-Pair_Relation_Networks_for_Accurate_Face_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Attentional_Feature-Pair_Relation_Networks_for_Accurate_Face_Recognition_ICCV_2019_paper.html",
        "abstract": "Human face recognition is one of the most important research areas in biometrics. However, the robust face recognition under a drastic change of the facial pose, expression, and illumination is a big challenging problem for its practical application. Such variations make face recognition more difficult. In this paper, we propose a novel face recognition method, called Attentional Feature-pair Relation Network (AFRN), which represents the face by the relevant pairs of local appearance block features with their attention scores. The AFRN represents the face by all possible pairs of the 9x9 local appearance block features, the importance of each pair is considered by the attention map that is obtained from the low-rank bilinear pooling, and each pair is weighted by its corresponding attention score. To increase the accuracy, we select top-K pairs of local appearance block features as relevant facial information and drop the remaining irrelevant. The weighted top-K pairs are propagated to extract the joint feature-pair relation by using bilinear attention network. In experiments, we show the effectiveness of the proposed AFRN and achieve the outstanding performance in the 1:1 face verification and 1:N face identification tasks compared to existing state-of-the-art methods on the challenging LFW, YTF, CALFW, CPLFW, CFP, AgeDB, IJB-A, IJB-B, and IJB-C datasets.",
        "中文标题": "注意力特征对关系网络用于精确人脸识别",
        "摘要翻译": "人脸识别是生物识别技术中最重要的研究领域之一。然而，在面部姿态、表情和光照剧烈变化的情况下进行鲁棒的人脸识别，对于其实际应用来说是一个巨大的挑战。这些变化使得人脸识别变得更加困难。在本文中，我们提出了一种新颖的人脸识别方法，称为注意力特征对关系网络（AFRN），它通过相关对的局部外观块特征及其注意力分数来表示人脸。AFRN通过所有可能的9x9局部外观块特征对来表示人脸，每对的重要性由从低秩双线性池化获得的注意力图考虑，并且每对由其相应的注意力分数加权。为了提高准确性，我们选择前K对局部外观块特征作为相关的面部信息，并丢弃其余不相关的。加权的top-K对被传播以通过使用双线性注意力网络提取联合特征对关系。在实验中，我们展示了所提出的AFRN的有效性，并在具有挑战性的LFW、YTF、CALFW、CPLFW、CFP、AgeDB、IJB-A、IJB-B和IJB-C数据集上，与现有的最先进方法相比，在1:1人脸验证和1:N人脸识别任务中取得了出色的性能。",
        "领域": "人脸识别/生物识别/注意力机制",
        "问题": "在面部姿态、表情和光照剧烈变化的情况下进行鲁棒的人脸识别",
        "动机": "解决实际应用中因面部姿态、表情和光照变化导致的人脸识别难题",
        "方法": "提出了一种新颖的人脸识别方法，称为注意力特征对关系网络（AFRN），通过相关对的局部外观块特征及其注意力分数来表示人脸，并选择前K对局部外观块特征作为相关的面部信息，通过双线性注意力网络提取联合特征对关系",
        "关键词": [
            "人脸识别",
            "注意力机制",
            "生物识别"
        ],
        "涉及的技术概念": "AFRN（注意力特征对关系网络）通过相关对的局部外观块特征及其注意力分数来表示人脸，使用低秩双线性池化获得注意力图，并通过双线性注意力网络提取联合特征对关系。"
    },
    {
        "order": 338,
        "title": "Asymmetric Cross-Guided Attention Network for Actor and Action Video Segmentation From Natural Language Query",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Asymmetric_Cross-Guided_Attention_Network_for_Actor_and_Action_Video_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Asymmetric_Cross-Guided_Attention_Network_for_Actor_and_Action_Video_Segmentation_ICCV_2019_paper.html",
        "abstract": "Actor and action video segmentation from natural language query aims to selectively segment the actor and its action in a video based on an input textual description. Previous works mostly focus on learning simple correlation between two heterogeneous features of vision and language via dynamic convolution or fully convolutional classification. However, they ignore the linguistic variation of natural language query and have difficulty in modeling global visual context, which leads to unsatisfactory segmentation performance. To address these issues, we propose an asymmetric cross-guided attention network for actor and action video segmentation from natural language query. Specifically, we frame an asymmetric cross-guided attention network, which consists of vision guided language attention to reduce the linguistic variation of input query and language guided vision attention to incorporate query-focused global visual context simultaneously. Moreover, we adopt multi-resolution fusion scheme and weighted loss for foreground and background pixels to obtain further performance improvement. Extensive experiments on Actor-Action Dataset Sentences and J-HMDB Sentences show that our proposed approach notably outperforms state-of-the-art methods.",
        "中文标题": "基于自然语言查询的演员和动作视频分割的非对称交叉引导注意力网络",
        "摘要翻译": "基于自然语言查询的演员和动作视频分割旨在根据输入的文本描述选择性地分割视频中的演员及其动作。以往的工作大多通过动态卷积或全卷积分类来学习视觉和语言两种异质特征之间的简单关联。然而，它们忽略了自然语言查询的语言变化，并且在建模全局视觉上下文方面存在困难，这导致了不满意的分割性能。为了解决这些问题，我们提出了一种用于基于自然语言查询的演员和动作视频分割的非对称交叉引导注意力网络。具体来说，我们构建了一个非对称交叉引导注意力网络，它包括视觉引导的语言注意力以减少输入查询的语言变化，以及语言引导的视觉注意力以同时融入查询聚焦的全局视觉上下文。此外，我们采用多分辨率融合方案和对前景和背景像素的加权损失，以进一步获得性能提升。在Actor-Action Dataset Sentences和J-HMDB Sentences上的大量实验表明，我们提出的方法显著优于最先进的方法。",
        "领域": "视频理解/自然语言处理/注意力机制",
        "问题": "如何有效地从自然语言查询中分割视频中的演员及其动作",
        "动机": "以往的方法在处理自然语言查询的语言变化和建模全局视觉上下文方面存在不足，导致分割性能不理想",
        "方法": "提出了一种非对称交叉引导注意力网络，包括视觉引导的语言注意力和语言引导的视觉注意力，以及采用多分辨率融合方案和加权损失",
        "关键词": [
            "视频分割",
            "自然语言查询",
            "注意力机制"
        ],
        "涉及的技术概念": "非对称交叉引导注意力网络通过视觉引导的语言注意力减少输入查询的语言变化，通过语言引导的视觉注意力融入查询聚焦的全局视觉上下文，采用多分辨率融合方案和加权损失以提升性能"
    },
    {
        "order": 339,
        "title": "Revisiting Radial Distortion Absolute Pose",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Larsson_Revisiting_Radial_Distortion_Absolute_Pose_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Larsson_Revisiting_Radial_Distortion_Absolute_Pose_ICCV_2019_paper.html",
        "abstract": "To model radial distortion there are two main approaches; either the image points are undistorted such that they correspond to pinhole projections, or the pinhole projections are distorted such that they align with the image measurements. Depending on the application, either of the two approaches can be more suitable. For example, distortion models are commonly used in Structure-from-Motion since they simplify measuring the reprojection error in images. Surprisingly, all previous minimal solvers for pose estimation with radial distortion use undistortion models. In this paper we aim to fill this gap in the literature by proposing the first minimal solvers which can jointly estimate distortion models together with camera pose. We present a general approach which can handle rational models of arbitrary degree for both distortion and undistortion.",
        "中文标题": "重新审视径向畸变绝对姿态",
        "摘要翻译": "为了建模径向畸变，主要有两种方法：要么对图像点进行去畸变处理，使其对应于针孔投影，要么对针孔投影进行畸变处理，使其与图像测量结果对齐。根据应用的不同，这两种方法中的任何一种都可能更为合适。例如，畸变模型通常用于从运动中恢复结构（Structure-from-Motion），因为它们简化了图像中重投影误差的测量。令人惊讶的是，所有之前用于带有径向畸变的姿态估计的最小解算器都使用了去畸变模型。在本文中，我们旨在通过提出第一个能够同时估计畸变模型和相机姿态的最小解算器来填补文献中的这一空白。我们提出了一种通用方法，可以处理任意程度的畸变和去畸变的有理模型。",
        "领域": "三维重建/相机标定/姿态估计",
        "问题": "在带有径向畸变的姿态估计中，现有方法仅使用去畸变模型，缺乏同时估计畸变模型和相机姿态的最小解算器。",
        "动机": "填补文献中关于同时估计畸变模型和相机姿态的最小解算器的空白。",
        "方法": "提出了一种通用方法，能够处理任意程度的畸变和去畸变的有理模型，并首次实现了同时估计畸变模型和相机姿态的最小解算器。",
        "关键词": [
            "径向畸变",
            "姿态估计",
            "最小解算器",
            "相机标定"
        ],
        "涉及的技术概念": {
            "径向畸变": "一种图像畸变类型，通常由镜头引起，导致图像边缘的直线出现弯曲。",
            "针孔投影": "一种理想化的相机模型，假设光线通过一个无限小的孔进入相机，形成图像。",
            "重投影误差": "在三维重建中，将三维点投影回二维图像平面时，投影点与实际图像点之间的距离。",
            "最小解算器": "用于求解特定问题的最小参数集，通常用于姿态估计等问题中。"
        }
    },
    {
        "order": 340,
        "title": "Action Recognition With Spatial-Temporal Discriminative Filter Banks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Martinez_Action_Recognition_With_Spatial-Temporal_Discriminative_Filter_Banks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Martinez_Action_Recognition_With_Spatial-Temporal_Discriminative_Filter_Banks_ICCV_2019_paper.html",
        "abstract": "Action recognition has seen a dramatic performance improvement in the last few years. Most of the current state-of-the-art literature either aims at improving performance through changes to the backbone CNN network, or exploring different trade-offs between computational efficiency and performance, again through altering the backbone network. However, almost all of these works maintain the same last layers of the network, which simply consist of a global average pooling followed by a fully connected layer. In this work we focus on how to improve the representation capacity of the network, but rather than altering the backbone, we focus on improving the last layers of the network, where changes have low impact in terms of computational cost. In particular, we hypothesize that current architectures have poor sensitivity to finer details and we exploit recent advances in the fine-grained recognition literature to improve our model in this aspect. With the proposed approach, we obtain state-of-the-art performance on Kinetics-400 and Something-Something-V1, the two major large-scale action recognition benchmarks.",
        "中文标题": "使用时空判别滤波器库进行动作识别",
        "摘要翻译": "在过去的几年里，动作识别的性能有了显著的提升。当前大多数最先进的文献要么旨在通过改变骨干CNN网络来提高性能，要么通过改变骨干网络探索计算效率和性能之间的不同权衡。然而，几乎所有这些工作都保持了网络的最后几层不变，这些层仅由全局平均池化和全连接层组成。在这项工作中，我们专注于如何提高网络的表示能力，但不是通过改变骨干网络，而是专注于改进网络的最后几层，这些改变在计算成本方面影响较小。特别是，我们假设当前架构对更精细的细节敏感度较差，并利用细粒度识别文献中的最新进展来改进我们模型在这一方面的表现。通过提出的方法，我们在Kinetics-400和Something-Something-V1这两个主要的大规模动作识别基准上获得了最先进的性能。",
        "领域": "动作识别/细粒度识别/神经网络架构",
        "问题": "提高动作识别网络的表示能力，特别是对更精细细节的敏感度",
        "动机": "当前动作识别网络的最后几层对更精细细节的敏感度较差，限制了性能的进一步提升",
        "方法": "专注于改进网络的最后几层，利用细粒度识别文献中的最新进展来提高模型对更精细细节的敏感度",
        "关键词": [
            "动作识别",
            "细粒度识别",
            "神经网络架构"
        ],
        "涉及的技术概念": "全局平均池化、全连接层、骨干CNN网络、细粒度识别"
    },
    {
        "order": 341,
        "title": "Estimating the Fundamental Matrix Without Point Correspondences With Application to Transmission Imaging",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wurfl_Estimating_the_Fundamental_Matrix_Without_Point_Correspondences_With_Application_to_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wurfl_Estimating_the_Fundamental_Matrix_Without_Point_Correspondences_With_Application_to_ICCV_2019_paper.html",
        "abstract": "We present a general method to estimate the fundamental matrix from a pair of images under perspective projection without the need for image point correspondences. Our method is particularly well-suited for transmission imaging, where state-of-the-art feature detection and matching approaches generally do not perform well. Estimation of the fundamental matrix plays a central role in auto-calibration methods for reflection imaging. Such methods are currently not applicable to transmission imaging. Furthermore, our method extends an existing technique proposed for reflection imaging which potentially avoids the outlier-prone feature matching step from an orthographic projection model to a perspective model. Our method exploits the idea that under a linear attenuation model line integrals along corresponding epipolar lines are equal if we compute their derivatives in orthogonal direction to their common epipolar plane. We use the fundamental matrix to parametrize this equality. Our method estimates the matrix by formulating a non-convex optimization problem, minimizing an error in our measurement of this equality. We believe this technique will enable the application of the large body of work on image-based camera pose estimation to transmission imaging leading to more accurate and more general motion compensation and auto-calibration algorithms, particularly in medical X-ray and Computed Tomography imaging.",
        "中文标题": "无需点对应关系估计基础矩阵及其在透射成像中的应用",
        "摘要翻译": "我们提出了一种通用方法，用于在透视投影下从一对图像中估计基础矩阵，而无需图像点对应关系。我们的方法特别适用于透射成像，其中最先进的特征检测和匹配方法通常表现不佳。基础矩阵的估计在反射成像的自动校准方法中起着核心作用。这些方法目前不适用于透射成像。此外，我们的方法扩展了现有的反射成像技术，该技术可能避免了从正交投影模型到透视模型的易受异常值影响的特征匹配步骤。我们的方法利用了在线性衰减模型下，如果我们计算它们在共同极平面正交方向上的导数，则沿相应极线的线积分相等的思想。我们使用基础矩阵来参数化这种相等性。我们的方法通过制定一个非凸优化问题来估计矩阵，最小化我们对这种相等性测量的误差。我们相信，这种技术将使基于图像的相机姿态估计的大量工作应用于透射成像，从而在医学X射线和计算机断层扫描成像中实现更准确和更通用的运动补偿和自动校准算法。",
        "领域": "透射成像/自动校准/运动补偿",
        "问题": "在透射成像中无需图像点对应关系估计基础矩阵",
        "动机": "解决透射成像中特征检测和匹配方法表现不佳的问题，扩展自动校准方法的应用范围",
        "方法": "利用线性衰减模型下的线积分相等性，通过非凸优化问题估计基础矩阵",
        "关键词": [
            "基础矩阵",
            "透射成像",
            "自动校准",
            "运动补偿",
            "非凸优化"
        ],
        "涉及的技术概念": "基础矩阵是计算机视觉中用于描述两个视图之间几何关系的关键矩阵。透射成像是一种成像技术，常用于医学成像，如X射线和计算机断层扫描。自动校准方法用于自动调整成像系统的参数以提高图像质量。运动补偿技术用于减少或消除由于物体或相机运动引起的图像模糊。非凸优化是一种数学优化方法，用于寻找函数的最小值，即使函数可能有多个局部最小值。"
    },
    {
        "order": 342,
        "title": "Unconstrained Motion Deblurring for Dual-Lens Cameras",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mohan_Unconstrained_Motion_Deblurring_for_Dual-Lens_Cameras_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mohan_Unconstrained_Motion_Deblurring_for_Dual-Lens_Cameras_ICCV_2019_paper.html",
        "abstract": "Recently, there has been a renewed interest in leveraging multiple cameras, but under unconstrained settings. They have been quite successfully deployed in smartphones, which have become de facto choice for many photographic applications. However, akin to normal cameras, the functionality of multi-camera systems can be marred by motion blur which is a ubiquitous phenomenon in hand-held cameras. Despite the far-reaching potential of unconstrained camera arrays, there is not a single deblurring method for such systems. In this paper, we propose a generalized blur model that elegantly explains the intrinsically coupled image formation model for dual-lens set-up, which are by far most predominant in smartphones. While image aesthetics is the main objective in normal camera deblurring, any method conceived for our problem is additionally tasked with ascertaining consistent scene-depth in the deblurred images. We reveal an intriguing challenge that stems from an inherent ambiguity unique to this problem which naturally disrupts this coherence. We address this issue by devising a judicious prior, and based on our model and prior propose a practical blind deblurring method for dual-lens cameras, that achieves state-of-the-art performance.",
        "中文标题": "双镜头相机的无约束运动去模糊",
        "摘要翻译": "最近，利用多摄像头在无约束设置下的应用重新引起了人们的兴趣。它们已成功部署在智能手机中，成为许多摄影应用的实际选择。然而，与普通相机类似，多摄像头系统的功能可能会受到运动模糊的影响，这是手持相机中普遍存在的现象。尽管无约束相机阵列具有深远的潜力，但目前还没有针对此类系统的去模糊方法。在本文中，我们提出了一个广义模糊模型，优雅地解释了双镜头设置中固有的耦合图像形成模型，这是迄今为止智能手机中最主要的配置。虽然图像美学是普通相机去模糊的主要目标，但针对我们问题的任何方法还需要确保去模糊图像中的场景深度一致性。我们揭示了一个源自该问题特有的内在模糊性的有趣挑战，这自然破坏了这种一致性。我们通过设计一个明智的先验来解决这个问题，并基于我们的模型和先验提出了一种实用的双镜头相机盲去模糊方法，实现了最先进的性能。",
        "领域": "图像去模糊/多摄像头系统/智能手机摄影",
        "问题": "解决双镜头相机在无约束设置下的运动模糊问题",
        "动机": "多摄像头系统在智能手机中的应用日益增多，但运动模糊问题影响了其功能，目前缺乏针对此类系统的去模糊方法。",
        "方法": "提出了一个广义模糊模型来解释双镜头设置中的图像形成过程，并设计了一个明智的先验来解决场景深度一致性问题，最终提出了一种实用的盲去模糊方法。",
        "关键词": [
            "图像去模糊",
            "多摄像头系统",
            "智能手机摄影"
        ],
        "涉及的技术概念": "广义模糊模型、双镜头设置、图像形成模型、运动模糊、场景深度一致性、盲去模糊方法"
    },
    {
        "order": 343,
        "title": "AGSS-VOS: Attention Guided Single-Shot Video Object Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_AGSS-VOS_Attention_Guided_Single-Shot_Video_Object_Segmentation_ICCV_2019_paper.html",
        "abstract": "Most video object segmentation approaches process objects separately. This incurs high computational cost when multiple objects exist. In this paper, we propose AGSS-VOS to segment multiple objects in one feed-forward path via instance-agnostic and instance-specific modules. Information from the two modules is fused via an attention-guided decoder to simultaneously segment all object instances in one path. The whole framework is end-to-end trainable with instance IoU loss. Experimental results on Youtube- VOS and DAVIS-2017 dataset demonstrate that AGSS-VOS achieves competitive results in terms of both accuracy and efficiency.",
        "中文标题": "AGSS-VOS: 注意力引导的单次视频对象分割",
        "摘要翻译": "大多数视频对象分割方法分别处理对象。当存在多个对象时，这会带来高计算成本。在本文中，我们提出了AGSS-VOS，通过实例无关和实例特定的模块在一次前馈路径中分割多个对象。来自这两个模块的信息通过注意力引导的解码器融合，以在一次路径中同时分割所有对象实例。整个框架是端到端可训练的，使用实例IoU损失。在Youtube-VOS和DAVIS-2017数据集上的实验结果表明，AGSS-VOS在准确性和效率方面都取得了竞争性的结果。",
        "领域": "视频对象分割/注意力机制/端到端学习",
        "问题": "如何在存在多个对象的情况下，以较低的计算成本进行视频对象分割",
        "动机": "减少多个对象分割时的计算成本，提高分割的准确性和效率",
        "方法": "提出AGSS-VOS方法，通过实例无关和实例特定的模块在一次前馈路径中分割多个对象，并使用注意力引导的解码器融合信息，实现端到端训练",
        "关键词": [
            "视频对象分割",
            "注意力机制",
            "端到端学习"
        ],
        "涉及的技术概念": "实例无关模块、实例特定模块、注意力引导的解码器、实例IoU损失、Youtube-VOS数据集、DAVIS-2017数据集"
    },
    {
        "order": 344,
        "title": "EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multi-modal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -- RGB, Flow and Audio -- and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.",
        "中文标题": "EPIC-Fusion: 用于自我中心行为识别的视听时间绑定",
        "摘要翻译": "我们专注于自我中心行为识别的多模态融合，并提出了一种新颖的多模态时间绑定架构，即在时间偏移范围内结合模态。我们使用三种模态——RGB、光流和音频——训练该架构，并通过中级融合与融合表示的稀疏时间采样相结合。与之前的工作相比，模态在时间聚合之前被融合，随着时间的推移共享模态融合权重。我们提出的架构是端到端训练的，优于单个模态以及模态的后期融合。我们展示了音频在自我中心视觉中的重要性，基于每个类别，用于识别行为以及交互对象。我们的方法在最大的自我中心数据集EPIC-Kitchens的可见和不可见测试集上，使用公共排行榜的所有指标，均达到了最先进的结果。",
        "领域": "自我中心行为识别/多模态融合/时间绑定",
        "问题": "如何有效地融合多模态信息以提升自我中心行为识别的准确性",
        "动机": "探索多模态融合在自我中心行为识别中的应用，特别是音频信息在识别行为和交互对象中的重要性",
        "方法": "提出了一种新颖的多模态时间绑定架构，通过中级融合与稀疏时间采样相结合，端到端训练，优于单个模态及后期融合",
        "关键词": [
            "自我中心行为识别",
            "多模态融合",
            "时间绑定",
            "音频视觉",
            "EPIC-Kitchens"
        ],
        "涉及的技术概念": "多模态融合指的是将来自不同模态（如RGB、光流和音频）的信息结合起来，以提高行为识别的准确性。时间绑定是指在特定的时间偏移范围内结合这些模态信息。中级融合是指在特征提取的中间阶段进行融合，而稀疏时间采样则是指在时间维度上对融合后的表示进行采样，以减少计算量并提高效率。"
    },
    {
        "order": 345,
        "title": "Stochastic Exposure Coding for Handling Multi-ToF-Camera Interference",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Stochastic_Exposure_Coding_for_Handling_Multi-ToF-Camera_Interference_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Stochastic_Exposure_Coding_for_Handling_Multi-ToF-Camera_Interference_ICCV_2019_paper.html",
        "abstract": "As continuous-wave time-of-flight (C-ToF) cameras become popular in 3D imaging applications, they need to contend with the problem of multi-camera interference (MCI). In a multi-camera environment, a ToF camera may receive light from the sources of other cameras, resulting in large depth errors. In this paper, we propose stochastic exposure coding (SEC), a novel approach for mitigating. SEC involves dividing a camera's integration time into multiple slots, and switching the camera off and on stochastically during each slot. This approach has two benefits. First, by appropriately choosing the on probability for each slot, the camera can effectively filter out both the AC and DC components of interfering signals, thereby mitigating depth errors while also maintaining high signal-to-noise ratio. This enables high accuracy depth recovery with low power consumption. Second, this approach can be implemented without modifying the C-ToF camera's coding functions, and thus, can be used with a wide range of cameras with minimal changes. We demonstrate the performance benefits of SEC with theoretical analysis, simulations and real experiments, across a wide range of imaging scenarios.",
        "中文标题": "随机曝光编码处理多ToF相机干扰",
        "摘要翻译": "随着连续波飞行时间（C-ToF）相机在3D成像应用中的普及，它们需要应对多相机干扰（MCI）的问题。在多相机环境中，ToF相机可能会接收到来自其他相机的光源，从而导致较大的深度误差。在本文中，我们提出了随机曝光编码（SEC），一种新颖的方法来缓解这一问题。SEC涉及将相机的积分时间划分为多个时隙，并在每个时隙中随机地开关相机。这种方法有两个好处。首先，通过适当选择每个时隙的开启概率，相机可以有效地滤除干扰信号的交流（AC）和直流（DC）分量，从而在保持高信噪比的同时减轻深度误差。这使得在低功耗的情况下实现高精度的深度恢复成为可能。其次，这种方法可以在不修改C-ToF相机编码功能的情况下实现，因此，只需最小改动即可适用于多种相机。我们通过理论分析、模拟和实际实验，在多种成像场景中展示了SEC的性能优势。",
        "领域": "3D成像/飞行时间相机/多相机系统",
        "问题": "多相机环境中的ToF相机干扰问题",
        "动机": "解决多相机环境下ToF相机因接收其他相机光源导致的深度误差问题",
        "方法": "提出随机曝光编码（SEC），通过随机开关相机来滤除干扰信号的AC和DC分量",
        "关键词": [
            "随机曝光编码",
            "多相机干扰",
            "深度误差",
            "信噪比",
            "低功耗"
        ],
        "涉及的技术概念": "连续波飞行时间（C-ToF）相机、多相机干扰（MCI）、随机曝光编码（SEC）、交流（AC）和直流（DC）分量、信噪比、深度恢复"
    },
    {
        "order": 346,
        "title": "Global-Local Temporal Representations for Video Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Global-Local_Temporal_Representations_for_Video_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "This paper proposes the Global-Local Temporal Representation (GLTR) to exploit the multi-scale temporal cues in video sequences for video person Re-Identification (ReID). GLTR is constructed by first modeling the short-term temporal cues among adjacent frames, then capturing the long-term relations among inconsecutive frames. Specifically, the short-term temporal cues are modeled by parallel dilated convolutions with different temporal dilation rates to represent the motion and appearance of pedestrian. The long-term relations are captured by a temporal self-attention model to alleviate the occlusions and noises in video sequences. The short and long-term temporal cues are aggregated as the final GLTR by a simple single-stream CNN. GLTR shows substantial superiority to existing features learned with body part cues or metric learning on four widely-used video ReID datasets. For instance, it achieves Rank-1 Accuracy of 87.02% on MARS dataset without re-ranking, better than current state-of-the art.",
        "中文标题": "全局-局部时间表示用于视频行人重识别",
        "摘要翻译": "本文提出了全局-局部时间表示（GLTR），以利用视频序列中的多尺度时间线索进行视频行人重识别（ReID）。GLTR首先通过建模相邻帧之间的短期时间线索，然后捕捉非连续帧之间的长期关系来构建。具体来说，短期时间线索通过具有不同时间膨胀率的并行膨胀卷积来建模，以表示行人的运动和外观。长期关系通过时间自注意力模型捕捉，以减轻视频序列中的遮挡和噪声。短期和长期时间线索通过一个简单的单流CNN聚合为最终的GLTR。GLTR在四个广泛使用的视频ReID数据集上显示出比现有使用身体部位线索或度量学习学习的特征的显著优势。例如，它在MARS数据集上实现了87.02%的Rank-1准确率，无需重新排名，优于当前的最新技术。",
        "领域": "行人重识别/视频分析/时间序列分析",
        "问题": "视频行人重识别中的多尺度时间线索利用",
        "动机": "提高视频行人重识别的准确率，通过更好地利用视频序列中的多尺度时间线索",
        "方法": "构建全局-局部时间表示（GLTR），通过并行膨胀卷积建模短期时间线索，时间自注意力模型捕捉长期关系，并通过单流CNN聚合",
        "关键词": [
            "行人重识别",
            "时间序列分析",
            "自注意力模型"
        ],
        "涉及的技术概念": "并行膨胀卷积用于建模短期时间线索，时间自注意力模型用于捕捉长期关系，单流CNN用于聚合短期和长期时间线索"
    },
    {
        "order": 347,
        "title": "QUARCH: A New Quasi-Affine Reconstruction Stratum From Vague Relative Camera Orientation Knowledge",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Adlakha_QUARCH_A_New_Quasi-Affine_Reconstruction_Stratum_From_Vague_Relative_Camera_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Adlakha_QUARCH_A_New_Quasi-Affine_Reconstruction_Stratum_From_Vague_Relative_Camera_ICCV_2019_paper.html",
        "abstract": "We present a new quasi-affine reconstruction of a scene and its application to camera self-calibration. We refer to this reconstruction as QUARCH (QUasi-Affine Reconstruction with respect to Camera centers and the Hodographs of horopters). A QUARCH can be obtained by solving a semidefinite programming problem when, (i) the images have been captured by a moving camera with constant intrinsic parameters, and (ii) a vague knowledge of the relative orientation (under or over 120 degrees) between camera pairs is available. The resulting reconstruction comes close enough to an affine one allowing thus an easy upgrade of the QUARCH to its affine and metric counterparts. We also present a constrained Levenberg-Marquardt method for nonlinear optimization subject to Linear Matrix Inequality (LMI) constraints so as to ensure that the QUARCH LMIs are satisfied during optimization. Experiments with synthetic and real data show the benefits of QUARCH in reliably obtaining a metric reconstruction.",
        "中文标题": "QUARCH：一种基于模糊相对相机方向知识的新准仿射重建层",
        "摘要翻译": "我们提出了一种新的场景准仿射重建及其在相机自校准中的应用。我们将这种重建称为QUARCH（关于相机中心和视差曲线轨迹的准仿射重建）。当(i)图像由具有恒定内在参数的移动相机捕获，并且(ii)可获得相机对之间相对方向（低于或高于120度）的模糊知识时，可以通过解决半定规划问题来获得QUARCH。所得重建足够接近仿射重建，从而可以轻松将QUARCH升级为其仿射和度量对应物。我们还提出了一种受线性矩阵不等式（LMI）约束的非线性优化的约束Levenberg-Marquardt方法，以确保在优化过程中满足QUARCH的LMI。合成和真实数据的实验显示了QUARCH在可靠获得度量重建方面的优势。",
        "领域": "三维重建/相机自校准/非线性优化",
        "问题": "如何在仅知道相机对之间相对方向的模糊知识的情况下，进行场景的准仿射重建并应用于相机自校准。",
        "动机": "为了在相机自校准中实现更可靠的三维重建，特别是在仅能获得相机对之间相对方向的模糊知识的情况下。",
        "方法": "通过解决半定规划问题获得QUARCH，并采用受线性矩阵不等式（LMI）约束的Levenberg-Marquardt方法进行非线性优化。",
        "关键词": [
            "准仿射重建",
            "相机自校准",
            "半定规划",
            "非线性优化",
            "线性矩阵不等式"
        ],
        "涉及的技术概念": {
            "QUARCH": "一种关于相机中心和视差曲线轨迹的准仿射重建方法。",
            "半定规划": "一种数学优化方法，用于解决包括QUARCH在内的优化问题。",
            "Levenberg-Marquardt方法": "一种用于非线性最小二乘问题的数值优化算法。",
            "线性矩阵不等式（LMI）": "一种在优化问题中使用的约束条件，确保优化过程中的某些矩阵性质。"
        }
    },
    {
        "order": 348,
        "title": "Weakly-Supervised Action Localization With Background Modeling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Weakly-Supervised_Action_Localization_With_Background_Modeling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Weakly-Supervised_Action_Localization_With_Background_Modeling_ICCV_2019_paper.html",
        "abstract": "We describe a latent approach that learns to detect actions in long sequences given training videos with only whole-video class labels. Our approach makes use of two innovations to attention-modeling in weakly-supervised learning. First, and most notably, our framework uses an attention model to extract both foreground and background frames who's appearance is explicitly modeled. Most prior work ignores the background, but we show that modeling it allows our system to learn a richer notions of actions and their temporal extents. Second, we combine bottom-up, class-agnostic attention modules with top-down, class-specific activation maps, using the latter as form of self-supervision for the former. Doing so allows our model to learn a more accurate model of attention without explicit temporal supervision. These modifications lead to  10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed weakly-supervised system outperforms the recent state-of-the-art by at least 4.3% AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used to aggressively scale-up learning to in-the-wild, uncurated Instagram videos (where relevant frames and videos are automatically selected through attentional processing). This allows our weakly supervised approach to even outperform fully-supervised methods for action detection at some overlap thresholds.",
        "中文标题": "基于背景建模的弱监督动作定位",
        "摘要翻译": "我们描述了一种潜在的方法，该方法能够在仅提供整个视频类别标签的训练视频的情况下，学习检测长序列中的动作。我们的方法在弱监督学习中的注意力建模方面采用了两种创新。首先，也是最重要的，我们的框架使用注意力模型来提取前景和背景帧，这些帧的外观被明确建模。大多数先前的工作忽略了背景，但我们表明，对背景进行建模使我们的系统能够学习到更丰富的动作概念及其时间范围。其次，我们将自下而上、类别无关的注意力模块与自上而下、类别特定的激活图结合起来，使用后者作为前者的一种自我监督形式。这样做使我们的模型能够在没有明确时间监督的情况下学习到更准确的注意力模型。这些修改使得在THUMOS14上的AP@IoU=0.5比现有系统提高了10%。我们提出的弱监督系统比最近的最新技术至少提高了4.3%的AP@IoU=0.5。最后，我们证明了弱监督学习可以用于大规模学习到野外、未经整理的Instagram视频（其中相关帧和视频通过注意力处理自动选择）。这使得我们的弱监督方法在某些重叠阈值下甚至优于完全监督的动作检测方法。",
        "领域": "动作识别/视频分析/注意力机制",
        "问题": "在仅提供整个视频类别标签的情况下，如何有效地检测长视频序列中的动作",
        "动机": "提高动作检测的准确性，特别是在没有明确时间监督的情况下，通过建模背景和结合自下而上与自上而下的注意力机制",
        "方法": "使用注意力模型提取前景和背景帧，结合自下而上、类别无关的注意力模块与自上而下、类别特定的激活图，利用后者作为前者的自我监督形式",
        "关键词": [
            "动作识别",
            "视频分析",
            "注意力机制"
        ],
        "涉及的技术概念": "注意力模型用于提取视频中的前景和背景帧，自下而上和自上而下的注意力机制结合，以及使用类别特定的激活图作为自我监督的形式来提高动作检测的准确性。"
    },
    {
        "order": 349,
        "title": "Convolutional Approximations to the General Non-Line-of-Sight Imaging Operator",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ahn_Convolutional_Approximations_to_the_General_Non-Line-of-Sight_Imaging_Operator_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ahn_Convolutional_Approximations_to_the_General_Non-Line-of-Sight_Imaging_Operator_ICCV_2019_paper.html",
        "abstract": "Non-line-of-sight (NLOS) imaging aims to reconstruct scenes outside the field of view of an imaging system. A common approach is to measure the so-called light transients, which facilitates reconstructions through ellipsoidal tomography that involves solving a linear least-squares. Unfortunately, the corresponding linear operator is very high-dimensional and lacks structures that facilitate fast solvers, and so, the ensuing optimization is a computationally daunting task. We introduce a computationally tractable framework for solving the ellipsoidal tomography problem. Our main observation is that the Gram of the ellipsoidal tomography operator is convolutional, either exactly under certain idealized imaging conditions, or approximately in practice. This, in turn, allows us to obtain the ellipsoidal tomography solution by using efficient deconvolution procedures to solve a linear least-squares problem involving the Gram operator. The computational tractability of our approach also facilitates the use of various regularizers during the deconvolution procedure. We demonstrate the advantages of our framework in a variety of simulated and real experiments.",
        "中文标题": "卷积近似于一般非视线成像算子",
        "摘要翻译": "非视线（NLOS）成像旨在重建成像系统视野之外的场景。一种常见的方法是测量所谓的光瞬态，这通过涉及解决线性最小二乘问题的椭球体层析成像促进重建。不幸的是，相应的线性算子维度非常高，并且缺乏促进快速求解器的结构，因此，随之而来的优化是一个计算上令人畏惧的任务。我们引入了一个计算上可行的框架来解决椭球体层析成像问题。我们的主要观察是，椭球体层析成像算子的Gram是卷积的，要么在某些理想化的成像条件下精确地是卷积的，要么在实践中近似地是卷积的。这反过来允许我们通过使用高效的去卷积程序来解决涉及Gram算子的线性最小二乘问题，从而获得椭球体层析成像解。我们方法的计算可行性也促进了在去卷积过程中使用各种正则化器。我们在各种模拟和真实实验中展示了我们框架的优势。",
        "领域": "非视线成像/椭球体层析成像/去卷积",
        "问题": "解决非视线成像中椭球体层析成像问题的高维线性算子计算难题",
        "动机": "由于非视线成像中的线性算子维度高且缺乏结构，导致优化计算困难，需要一种计算上可行的方法来解决这一问题",
        "方法": "利用椭球体层析成像算子的Gram是卷积的这一观察，通过高效的去卷积程序解决线性最小二乘问题，从而获得椭球体层析成像解",
        "关键词": [
            "非视线成像",
            "椭球体层析成像",
            "去卷积"
        ],
        "涉及的技术概念": "非视线成像（NLOS imaging）是一种旨在重建成像系统视野之外场景的技术。椭球体层析成像（ellipsoidal tomography）是一种通过测量光瞬态来重建场景的方法，涉及解决线性最小二乘问题。去卷积（deconvolution）是一种信号处理技术，用于反转卷积过程，以恢复原始信号。"
    },
    {
        "order": 350,
        "title": "AdvIT: Adversarial Frames Identifier Based on Temporal Consistency in Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_AdvIT_Adversarial_Frames_Identifier_Based_on_Temporal_Consistency_in_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_AdvIT_Adversarial_Frames_Identifier_Based_on_Temporal_Consistency_in_Videos_ICCV_2019_paper.html",
        "abstract": "Deep neural networks (DNNs) have been widely applied in various applications, including autonomous driving and surveillance systems. However, DNNs are found to be vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead a learner to make incorrect predictions. While several defense and detection approaches are proposed for static image classification, many security-critical tasks use videos as their input and require efficient processing. In this paper, we propose an efficient and effective method advIT to detect adversarial frames within videos against different types of attacks based on temporal consistency property of videos. In particular, we apply optical flow estimation to the target and previous frames to generate pseudo frames and evaluate the consistency of the learner output between these pseudo frames and target. High inconsistency indicates that the target frame is adversarial. We conduct extensive experiments on various learning tasks including video semantic segmentation, human pose estimation, object detection, and action recognition, and demonstrate that we can achieve above 95% adversarial frame detection rate. To consider adaptive attackers, we show that even if an adversary has access to the detector and performs a strong adaptive attack based on the state of the art expectation of transformation method, the detection rate stays almost the same. We also tested the transferability among different optical flow estimators and show that it is hard for attackers to attack one and transfer the perturbation to others. In addition, as efficiency is important in video analysis, we show that advIT can achieve real-time detection in about 0.03--0.4 seconds.",
        "中文标题": "AdvIT: 基于视频时间一致性的对抗帧识别器",
        "摘要翻译": "深度神经网络（DNNs）已被广泛应用于各种应用中，包括自动驾驶和监控系统。然而，DNNs被发现容易受到对抗样本的攻击，这些对抗样本是精心设计的输入，旨在误导学习器做出错误的预测。虽然已经提出了几种针对静态图像分类的防御和检测方法，但许多安全关键任务使用视频作为输入，并需要高效处理。在本文中，我们提出了一种高效且有效的方法advIT，基于视频的时间一致性属性来检测视频中的对抗帧，以抵御不同类型的攻击。特别是，我们对目标和前一帧应用光流估计以生成伪帧，并评估这些伪帧与目标之间学习器输出的一致性。高度不一致表明目标帧是对抗性的。我们在包括视频语义分割、人体姿态估计、物体检测和动作识别在内的各种学习任务上进行了广泛的实验，并证明我们可以实现超过95%的对抗帧检测率。考虑到适应性攻击者，我们展示了即使攻击者能够访问检测器并基于最先进的变换方法期望执行强适应性攻击，检测率也几乎保持不变。我们还测试了不同光流估计器之间的可转移性，并表明攻击者很难攻击一个并将扰动转移到其他估计器。此外，由于效率在视频分析中很重要，我们展示了advIT可以在大约0.03--0.4秒内实现实时检测。",
        "领域": "视频分析/对抗样本检测/光流估计",
        "问题": "检测视频中的对抗帧",
        "动机": "深度神经网络在视频应用中容易受到对抗样本的攻击，需要一种高效的方法来检测这些对抗帧，以保障安全关键任务的安全性。",
        "方法": "应用光流估计生成伪帧，通过评估伪帧与目标帧之间学习器输出的一致性来检测对抗帧。",
        "关键词": [
            "对抗样本检测",
            "光流估计",
            "视频分析"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）、对抗样本、光流估计、视频语义分割、人体姿态估计、物体检测、动作识别、实时检测"
    },
    {
        "order": 351,
        "title": "Homography From Two Orientation- and Scale-Covariant Features",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Barath_Homography_From_Two_Orientation-_and_Scale-Covariant_Features_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Barath_Homography_From_Two_Orientation-_and_Scale-Covariant_Features_ICCV_2019_paper.html",
        "abstract": "This paper proposes a geometric interpretation of the angles and scales which the orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two new general constraints are derived on the scales and rotations which can be used in any geometric model estimation tasks. Using these formulas, two new constraints on homography estimation are introduced. Exploiting the derived equations, a solver for estimating the homography from the minimal number of two correspondences is proposed. Also, it is shown how the normalization of the point correspondences affects the rotation and scale parameters, thus achieving numerically stable results. Due to requiring merely two feature pairs, robust estimators, e.g. RANSAC, do significantly fewer iterations than by using the four-point algorithm. When using covariant features, e.g. SIFT, this additional information is given at no cost. The method is tested in a synthetic environment and on publicly available real-world datasets.",
        "中文标题": "从两个方向和尺度协变特征中估计单应性",
        "摘要翻译": "本文提出了方向和尺度协变特征检测器（例如SIFT）提供的角度和尺度的几何解释。推导出了两个新的关于尺度和旋转的通用约束，这些约束可以用于任何几何模型估计任务。利用这些公式，引入了两个关于单应性估计的新约束。利用推导出的方程，提出了一个从最小数量的两个对应关系中估计单应性的求解器。同时，展示了点对应关系的归一化如何影响旋转和尺度参数，从而实现数值稳定的结果。由于仅需要两个特征对，鲁棒估计器（例如RANSAC）的迭代次数比使用四点算法时显著减少。当使用协变特征（例如SIFT）时，这些额外信息是免费提供的。该方法在合成环境和公开可用的真实世界数据集上进行了测试。",
        "领域": "几何模型估计/单应性估计/特征检测",
        "问题": "如何从最小数量的特征对应关系中准确估计单应性",
        "动机": "提高单应性估计的效率和准确性，减少鲁棒估计器的迭代次数",
        "方法": "提出了两个新的关于尺度和旋转的通用约束，并利用这些约束开发了一个从两个对应关系中估计单应性的求解器",
        "关键词": [
            "单应性估计",
            "特征检测",
            "几何模型估计"
        ],
        "涉及的技术概念": {
            "方向和尺度协变特征检测器": "如SIFT，能够提供特征点的方向和尺度信息",
            "单应性": "一种几何变换，用于描述两个平面之间的映射关系",
            "RANSAC": "一种鲁棒估计方法，用于从包含噪声的数据中估计数学模型参数",
            "四点算法": "一种用于估计单应性的传统方法，需要四个对应点"
        }
    },
    {
        "order": 352,
        "title": "Grouped Spatial-Temporal Aggregation for Efficient Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Grouped_Spatial-Temporal_Aggregation_for_Efficient_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Grouped_Spatial-Temporal_Aggregation_for_Efficient_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "Temporal reasoning is an important aspect of video analysis. 3D CNN shows good performance by exploring spatial-temporal features jointly in an unconstrained way, but it also increases the computational cost a lot. Previous works try to reduce the complexity by decoupling the spatial and temporal filters. In this paper, we propose a novel decomposition method that decomposes the feature channels into spatial and temporal groups in parallel. This decomposition can make two groups focus on static and dynamic cues separately. We call this grouped spatial-temporal aggregation (GST). This decomposition is more parameter-efficient and enables us to quantitatively analyze the contributions of spatial and temporal features in different layers. We verify our model on several action recognition tasks that require temporal reasoning and show its effectiveness.",
        "中文标题": "分组时空聚合用于高效动作识别",
        "摘要翻译": "时间推理是视频分析的一个重要方面。3D CNN通过以无约束的方式联合探索时空特征显示出良好的性能，但它也大大增加了计算成本。以前的工作尝试通过解耦空间和时间滤波器来降低复杂性。在本文中，我们提出了一种新颖的分解方法，该方法将特征通道并行分解为空间组和时间组。这种分解可以使两组分别专注于静态和动态线索。我们称之为分组时空聚合（GST）。这种分解在参数上更为高效，并使我们能够定量分析不同层中空间和时间特征的贡献。我们在几个需要时间推理的动作识别任务上验证了我们的模型，并展示了其有效性。",
        "领域": "动作识别/视频分析/时空特征学习",
        "问题": "降低3D CNN在动作识别中的计算成本",
        "动机": "3D CNN在探索时空特征方面表现出色，但计算成本高，需要一种更高效的方法来降低复杂性。",
        "方法": "提出了一种新颖的分解方法，将特征通道并行分解为空间组和时间组，使两组分别专注于静态和动态线索，称为分组时空聚合（GST）。",
        "关键词": [
            "动作识别",
            "视频分析",
            "时空特征学习",
            "3D CNN",
            "计算成本"
        ],
        "涉及的技术概念": "3D CNN（三维卷积神经网络）用于视频分析，通过同时考虑空间和时间维度来提取特征。分组时空聚合（GST）是一种新的特征分解方法，旨在提高参数效率并允许对空间和时间特征的贡献进行定量分析。"
    },
    {
        "order": 353,
        "title": "RANet: Ranking Attention Network for Fast Video Object Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_RANet_Ranking_Attention_Network_for_Fast_Video_Object_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_RANet_Ranking_Attention_Network_for_Fast_Video_Object_Segmentation_ICCV_2019_paper.html",
        "abstract": "Despite online learning (OL) techniques have boosted the performance of semi-supervised video object segmentation (VOS) methods, the huge time costs of OL greatly restricts their practicality. Matching based and propagation based methods run at a faster speed by avoiding OL techniques. However, they are limited by sub-optimal accuracy, due to mismatching and drifting problems. In this paper, we develop a real-time yet very accurate Ranking Attention Network (RANet) for VOS. Specifically, to integrate the insights of matching based and propagation based methods, we employ an encoder-decoder framework to learn pixel-level similarity and segmentation in an end-to-end manner. To better utilize the similarity maps, we propose a novel ranking attention module, which automatically ranks and selects these maps for fine-grained VOS performance. Experiments on DAVIS16 and DAVIS17 datasets show that our RANet achieves the best speed-accuracy trade-off, e.g., with 33 milliseconds per frame and J&F=85.5% on DAVIS16. With OL, our RANet reaches J&F=87.1% on DAVIS16, exceeding state-of-the-art VOS methods. The code can be found at https://github.com/Storife/RANet.",
        "中文标题": "RANet: 用于快速视频对象分割的排序注意力网络",
        "摘要翻译": "尽管在线学习（OL）技术已经提升了半监督视频对象分割（VOS）方法的性能，但OL的巨大时间成本极大地限制了其实用性。基于匹配和基于传播的方法通过避免OL技术以更快的速度运行。然而，由于不匹配和漂移问题，它们的准确性受到限制。在本文中，我们开发了一种实时且非常准确的排序注意力网络（RANet）用于VOS。具体来说，为了整合基于匹配和基于传播方法的见解，我们采用编码器-解码器框架以端到端的方式学习像素级相似性和分割。为了更好地利用相似性图，我们提出了一种新颖的排序注意力模块，该模块自动排序并选择这些图以实现细粒度的VOS性能。在DAVIS16和DAVIS17数据集上的实验表明，我们的RANet实现了最佳的速度-准确性权衡，例如，在DAVIS16上每帧33毫秒和J&F=85.5%。使用OL，我们的RANet在DAVIS16上达到J&F=87.1%，超过了最先进的VOS方法。代码可以在https://github.com/Storife/RANet找到。",
        "领域": "视频对象分割/注意力机制/编码器-解码器框架",
        "问题": "解决半监督视频对象分割中的时间成本和准确性限制问题",
        "动机": "在线学习技术虽然提升了半监督视频对象分割的性能，但其巨大的时间成本限制了实用性，需要一种既能快速运行又能保持高准确性的方法。",
        "方法": "开发了一种实时且准确的排序注意力网络（RANet），采用编码器-解码器框架以端到端的方式学习像素级相似性和分割，并提出排序注意力模块自动排序并选择相似性图以提高性能。",
        "关键词": [
            "视频对象分割",
            "注意力机制",
            "编码器-解码器框架",
            "排序注意力模块"
        ],
        "涉及的技术概念": "在线学习（OL）技术用于提升半监督视频对象分割（VOS）方法的性能，但存在时间成本高的问题。基于匹配和基于传播的方法通过避免OL技术以更快的速度运行，但准确性受限。本文提出的RANet通过编码器-解码器框架和排序注意力模块，实现了快速且准确的视频对象分割。"
    },
    {
        "order": 354,
        "title": "Agile Depth Sensing Using Triangulation Light Curtains",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bartels_Agile_Depth_Sensing_Using_Triangulation_Light_Curtains_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bartels_Agile_Depth_Sensing_Using_Triangulation_Light_Curtains_ICCV_2019_paper.html",
        "abstract": "Depth sensors like LIDARs and Kinect use a fixed depth acquisition strategy that is independent of the scene of interest. Due to the low spatial and temporal resolution of these sensors, this strategy can undersample parts of the scene that are important (small or fast moving objects), or oversample areas that are not informative for the task at hand (a fixed planar wall). In this paper, we present an approach and system to dynamically and adaptively sample the depths of a scene using the principle of triangulation light curtains. The approach directly detects the presence or absence of objects at specified 3D lines. These 3D lines can be sampled sparsely, non-uniformly, or densely only at specified regions. The depth sampling can be varied in real-time, enabling quick object discovery or detailed exploration of areas of interest. These results are achieved using a novel prototype light curtain system that is based on a 2D rolling shutter camera with higher light efficiency, working range, and faster adaptation than previous work, making it useful broadly for autonomous navigation and exploration.",
        "中文标题": "使用三角测量光幕进行敏捷深度感知",
        "摘要翻译": "像LIDAR和Kinect这样的深度传感器使用固定的深度获取策略，这种策略与感兴趣的场景无关。由于这些传感器的空间和时间分辨率较低，这种策略可能会对场景中重要的部分（小或快速移动的物体）进行欠采样，或者对当前任务不具信息量的区域（固定的平面墙）进行过采样。在本文中，我们提出了一种方法和系统，利用三角测量光幕原理动态和自适应地采样场景的深度。该方法直接检测指定3D线上物体的存在与否。这些3D线可以稀疏地、非均匀地或仅在指定区域密集地采样。深度采样可以实时变化，从而实现快速物体发现或对感兴趣区域的详细探索。这些成果是通过一种新型的光幕系统原型实现的，该系统基于具有更高光效率、工作范围和更快适应性的2D滚动快门相机，使其在自主导航和探索中具有广泛的应用价值。",
        "领域": "深度感知/自主导航/3D视觉",
        "问题": "解决深度传感器在空间和时间分辨率上的限制，以及固定深度获取策略导致的欠采样和过采样问题。",
        "动机": "提高深度感知的效率和准确性，特别是在处理小或快速移动物体时，以及在对特定区域进行详细探索时。",
        "方法": "提出了一种基于三角测量光幕原理的动态和自适应深度采样方法和系统，使用新型光幕系统原型实现。",
        "关键词": [
            "深度感知",
            "三角测量光幕",
            "自适应采样",
            "3D视觉",
            "自主导航"
        ],
        "涉及的技术概念": "三角测量光幕是一种利用三角测量原理进行深度感知的技术，通过检测指定3D线上物体的存在与否来实现深度采样。2D滚动快门相机是一种具有高光效率、工作范围和快速适应性的相机技术，用于实现新型光幕系统原型。"
    },
    {
        "order": 355,
        "title": "Hiding Video in Audio via Reversible Generative Models",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Hiding_Video_in_Audio_via_Reversible_Generative_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Hiding_Video_in_Audio_via_Reversible_Generative_Models_ICCV_2019_paper.html",
        "abstract": "We present a method for hiding video content inside audio files while preserving the perceptual fidelity of the cover audio. This is a form of cross-modal steganography and is particularly challenging due to the high bitrate of video. Our scheme uses recent advances in flow-based generative models, which enable mapping audio to latent codes such that nearby codes correspond to perceptually similar signals. We show that compressed video data can be concealed in the latent codes of audio sequences while preserving the fidelity of both the hidden video and the cover audio. We can embed 128x128 video inside same-duration audio, or higher-resolution video inside longer audio sequences. Quantitative experiments show that our approach outperforms relevant baselines in steganographic capacity and fidelity.",
        "中文标题": "通过可逆生成模型在音频中隐藏视频",
        "摘要翻译": "我们提出了一种方法，用于将视频内容隐藏在音频文件中，同时保持封面音频的感知保真度。这是一种跨模态隐写术，由于视频的高比特率，这尤其具有挑战性。我们的方案利用了基于流的生成模型的最新进展，这些模型能够将音频映射到潜在代码，使得附近的代码对应于感知上相似的信号。我们展示了压缩的视频数据可以隐藏在音频序列的潜在代码中，同时保持隐藏视频和封面音频的保真度。我们可以在相同持续时间的音频中嵌入128x128的视频，或在更长的音频序列中嵌入更高分辨率的视频。定量实验表明，我们的方法在隐写容量和保真度方面优于相关基线。",
        "领域": "隐写术/生成模型/音频处理",
        "问题": "如何在保持音频感知保真度的同时，将视频内容隐藏在音频文件中",
        "动机": "探索跨模态隐写术的可能性，特别是处理高比特率视频的挑战",
        "方法": "利用基于流的生成模型将音频映射到潜在代码，使得压缩的视频数据可以隐藏在音频序列的潜在代码中",
        "关键词": [
            "隐写术",
            "生成模型",
            "音频处理",
            "视频隐藏",
            "潜在代码"
        ],
        "涉及的技术概念": "基于流的生成模型是一种能够将数据（如音频）映射到潜在空间的模型，使得在潜在空间中的邻近点对应于感知上相似的数据。这种方法允许在保持数据保真度的同时，进行有效的数据隐藏和恢复。"
    },
    {
        "order": 356,
        "title": "Temporal Structure Mining for Weakly Supervised Action Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Temporal_Structure_Mining_for_Weakly_Supervised_Action_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Temporal_Structure_Mining_for_Weakly_Supervised_Action_Detection_ICCV_2019_paper.html",
        "abstract": "Different from the fully-supervised action detection problem that is dependent on expensive frame-level annotations, weakly supervised action detection (WSAD) only needs video-level annotations, making it more practical for real-world applications. Existing WSAD methods detect action instances by scoring each video segment (a stack of frames) individually. Most of them fail to model the temporal relations among video segments and cannot effectively characterize action instances possessing latent temporal structure. To alleviate this problem in WSAD, we propose the temporal structure mining (TSM) approach. In TSM, each action instance is modeled as a multi-phase process and phase evolving within an action instance, i.e., the temporal structure, is exploited. Meanwhile, the video background is modeled by a background phase, which separates different action instances in an untrimmed video. In this framework, phase filters are used to calculate the confidence scores of the presence of an action's phases in each segment. Since in the WSAD task, frame-level annotations are not available and thus phase filters cannot be trained directly. To tackle the challenge, we treat each segment's phase as a hidden variable. We use segments' confidence scores from each phase filter to construct a table and determine hidden variables, i.e., phases of segments, by a maximal circulant path discovery along the table. Experiments conducted on three benchmark datasets demonstrate the state-of-the-art performance of the proposed TSM.",
        "中文标题": "时间结构挖掘用于弱监督动作检测",
        "摘要翻译": "与依赖昂贵的帧级注释的完全监督动作检测问题不同，弱监督动作检测（WSAD）仅需要视频级注释，使其在现实世界的应用中更加实用。现有的WSAD方法通过单独评分每个视频片段（一系列帧）来检测动作实例。大多数方法未能建模视频片段之间的时间关系，无法有效表征具有潜在时间结构的动作实例。为了缓解WSAD中的这一问题，我们提出了时间结构挖掘（TSM）方法。在TSM中，每个动作实例被建模为一个多阶段过程，并利用动作实例内的阶段演变，即时间结构。同时，视频背景通过背景阶段建模，该背景阶段在未修剪的视频中分离不同的动作实例。在此框架中，使用阶段过滤器计算每个片段中动作阶段存在的置信度分数。由于在WSAD任务中，帧级注释不可用，因此无法直接训练阶段过滤器。为了解决这一挑战，我们将每个片段的阶段视为隐藏变量。我们使用来自每个阶段过滤器的片段的置信度分数构建一个表，并通过沿表的最大循环路径发现来确定隐藏变量，即片段的阶段。在三个基准数据集上进行的实验证明了所提出的TSM的最先进性能。",
        "领域": "动作检测/视频分析/时间序列分析",
        "问题": "弱监督动作检测中未能有效建模视频片段之间的时间关系，无法有效表征具有潜在时间结构的动作实例",
        "动机": "减少对昂贵帧级注释的依赖，使动作检测在现实世界的应用中更加实用",
        "方法": "提出时间结构挖掘（TSM）方法，将每个动作实例建模为多阶段过程，利用动作实例内的阶段演变（时间结构），并通过背景阶段建模视频背景，使用阶段过滤器计算每个片段中动作阶段存在的置信度分数，将每个片段的阶段视为隐藏变量，通过最大循环路径发现确定隐藏变量",
        "关键词": [
            "弱监督学习",
            "动作检测",
            "时间结构挖掘",
            "视频分析"
        ],
        "涉及的技术概念": "时间结构挖掘（TSM）是一种用于弱监督动作检测的方法，它通过建模动作实例的多阶段过程和时间结构来改进动作检测的准确性。该方法利用阶段过滤器计算动作阶段存在的置信度分数，并通过最大循环路径发现技术确定视频片段的阶段，从而在缺乏帧级注释的情况下有效检测动作实例。"
    },
    {
        "order": 357,
        "title": "Asynchronous Single-Photon 3D Imaging",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_Asynchronous_Single-Photon_3D_Imaging_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_Asynchronous_Single-Photon_3D_Imaging_ICCV_2019_paper.html",
        "abstract": "Single-photon avalanche diodes (SPADs) are becoming popular in time-of-flight depth-ranging due to their unique ability to capture individual photons with picosecond timing resolution. However, ambient light (e.g., sunlight) incident on a SPAD-based 3D camera leads to severe non-linear distortions (pileup) in the measured waveform, resulting in large depth errors. We propose asynchronous single-photon 3D imaging, a family of acquisition schemes to mitigate pileup during data acquisition itself. Asynchronous acquisition temporally misaligns SPAD measurement windows and the laser cycles through deterministically predefined or randomized offsets. Our key insight is that pileup distortions can be \"averaged out\" by choosing a sequence of offsets that span the entire depth range. We develop a generalized image formation model and perform theoretical analysis to explore the space of asynchronous acquisition schemes and design high-performance schemes. Our simulations and experiments demonstrate an improvement in depth accuracy of up to an order of magnitude as compared to the state-of-the-art, across a wide range of imaging scenarios, including those with high ambient flux.",
        "中文标题": "异步单光子三维成像",
        "摘要翻译": "单光子雪崩二极管（SPADs）因其独特的能够以皮秒级时间分辨率捕获单个光子的能力，在飞行时间深度测距中变得越来越受欢迎。然而，基于SPAD的三维相机上的环境光（例如，阳光）会导致测量波形中的严重非线性失真（堆积），从而导致较大的深度误差。我们提出了异步单光子三维成像，一系列在数据采集过程中减轻堆积的采集方案。异步采集通过确定性预定义或随机偏移使SPAD测量窗口和激光周期在时间上不对齐。我们的关键见解是，通过选择跨越整个深度范围的偏移序列，可以“平均掉”堆积失真。我们开发了一个广义的图像形成模型，并进行了理论分析，以探索异步采集方案的空间并设计高性能方案。我们的模拟和实验表明，在包括高环境通量在内的广泛成像场景中，与最先进的技术相比，深度精度提高了多达一个数量级。",
        "领域": "三维成像/光子计数/深度测距",
        "问题": "环境光导致的基于SPAD的三维相机测量波形中的非线性失真（堆积）问题",
        "动机": "提高在包括高环境通量在内的广泛成像场景中的深度测距精度",
        "方法": "提出异步单光子三维成像，通过异步采集方案减轻堆积，开发广义图像形成模型并进行理论分析",
        "关键词": [
            "单光子雪崩二极管",
            "异步采集",
            "深度测距"
        ],
        "涉及的技术概念": "单光子雪崩二极管（SPADs）是一种能够以皮秒级时间分辨率捕获单个光子的传感器，常用于飞行时间深度测距。异步采集是一种通过使测量窗口和激光周期在时间上不对齐来减轻堆积失真的技术。广义图像形成模型用于描述和优化异步采集方案。"
    },
    {
        "order": 358,
        "title": "GSLAM: A General SLAM Framework and Benchmark",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.html",
        "abstract": "SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their SLAM systems. Our core contribution is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM.",
        "中文标题": "GSLAM: 一个通用的SLAM框架与基准",
        "摘要翻译": "SLAM技术最近取得了许多成功，并吸引了高科技公司的关注。然而，如何统一现有或新兴算法的接口，并有效地进行速度、鲁棒性和可移植性的基准测试仍然存在问题。在本文中，我们提出了一个名为GSLAM的新颖SLAM平台，它不仅提供评估功能，还为研究人员提供了快速开发其SLAM系统的有用工具包。我们的核心贡献是一个通用的、跨平台且完全开源的SLAM接口，旨在处理与输入数据集、SLAM实现、可视化和应用程序的交互，在一个统一的框架内。通过这个平台，用户可以通过插件形式实现自己的功能以获得更好的性能，并进一步推动SLAM的实际应用。",
        "领域": "机器人导航/增强现实/自动驾驶",
        "问题": "如何统一SLAM算法的接口并有效进行基准测试",
        "动机": "解决SLAM技术在速度、鲁棒性和可移植性基准测试方面的问题，促进SLAM技术的实际应用",
        "方法": "提出了一个名为GSLAM的通用、跨平台、全开源的SLAM平台，提供评估功能和开发工具包",
        "关键词": [
            "SLAM",
            "基准测试",
            "跨平台",
            "开源",
            "插件"
        ],
        "涉及的技术概念": "SLAM（同时定位与地图构建）技术，用于机器人导航、增强现实和自动驾驶等领域，通过算法实现环境地图的构建和自身位置的定位。GSLAM平台提供了一个统一的框架，用于处理输入数据集、SLAM实现、可视化和应用程序的交互，支持插件形式的自定义功能实现。"
    },
    {
        "order": 359,
        "title": "Spatial-Temporal Relation Networks for Multi-Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Spatial-Temporal_Relation_Networks_for_Multi-Object_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Spatial-Temporal_Relation_Networks_for_Multi-Object_Tracking_ICCV_2019_paper.html",
        "abstract": "Recent progress in multiple object tracking (MOT) has shown that a robust similarity score is a key to the success of trackers. A good similarity score is expected to reflect multiple cues, e.g. appearance, location, and topology, over a long period of time. However, these cues are heterogeneous, making them hard to be combined in a unified network. As a result, existing methods usually encode them in separate networks or require a complex training approach. In this paper, we present a unified framework for similarity measurement based on spatial-temporal relation network which could simultaneously encode various cues and perform reasoning across both spatial and temporal domains. We also study the feature representation of a tracklet-object pair in depth, showing a proper design of the pair features can well empower the trackers. The resulting approach is named spatial-temporal relation networks (STRN). It runs in a feed-forward way and can be trained in an end-to-end manner. The state-of-the-art accuracy was achieved on all of the MOT15~17 benchmarks using public detection and online settings.",
        "中文标题": "时空关系网络用于多目标跟踪",
        "摘要翻译": "多目标跟踪（MOT）的最新进展表明，强大的相似性评分是跟踪器成功的关键。一个好的相似性评分应能反映长时间内的多种线索，例如外观、位置和拓扑结构。然而，这些线索是异质的，使得它们难以在统一网络中被结合。因此，现有方法通常将它们编码在单独的网络中或需要复杂的训练方法。在本文中，我们提出了一个基于时空关系网络的统一框架，用于相似性测量，该框架可以同时编码各种线索并在空间和时间域中进行推理。我们还深入研究了轨迹-对象对的特征表示，表明适当设计对特征可以很好地增强跟踪器。所得到的方法被命名为时空关系网络（STRN）。它以前馈方式运行，并且可以以端到端的方式进行训练。在使用公共检测和在线设置的所有MOT15~17基准测试中，达到了最先进的准确度。",
        "领域": "多目标跟踪/时空分析/特征表示",
        "问题": "如何有效地结合多目标跟踪中的异质线索（如外观、位置和拓扑结构）以提高跟踪器的性能",
        "动机": "现有方法在处理多目标跟踪中的异质线索时，通常需要将它们编码在单独的网络中或采用复杂的训练方法，这限制了跟踪器的性能和效率",
        "方法": "提出了一个基于时空关系网络的统一框架，该框架能够同时编码各种线索并在空间和时间域中进行推理，以及深入研究了轨迹-对象对的特征表示",
        "关键词": [
            "多目标跟踪",
            "时空关系网络",
            "特征表示"
        ],
        "涉及的技术概念": "时空关系网络（STRN）是一种能够同时编码多种线索（如外观、位置和拓扑结构）并在空间和时间域中进行推理的统一框架。它通过前馈方式运行，并可以以端到端的方式进行训练，从而在多目标跟踪任务中实现了最先进的准确度。"
    },
    {
        "order": 360,
        "title": "Temporal Recurrent Networks for Online Action Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Temporal_Recurrent_Networks_for_Online_Action_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Temporal_Recurrent_Networks_for_Online_Action_Detection_ICCV_2019_paper.html",
        "abstract": "Most work on temporal action detection is formulated as an offline problem, in which the start and end times of actions are determined after the entire video is fully observed. However, important real-time applications including surveillance and driver assistance systems require identifying actions as soon as each video frame arrives, based only on current and historical observations. In this paper, we propose a novel framework, the Temporal Recurrent Network (TRN), to model greater temporal context of each frame by simultaneously performing online action detection and anticipation of the immediate future. At each moment in time, our approach makes use of both accumulated historical evidence and predicted future information to better recognize the action that is currently occurring, and integrates both of these into a unified end-to-end architecture. We evaluate our approach on two popular online action detection datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14. The results show that TRN significantly outperforms the state-of-the-art.",
        "中文标题": "时间循环网络用于在线动作检测",
        "摘要翻译": "大多数关于时间动作检测的工作被表述为一个离线问题，其中动作的开始和结束时间是在整个视频完全观察后确定的。然而，包括监控和驾驶员辅助系统在内的重要实时应用需要在每个视频帧到达时立即识别动作，仅基于当前和历史的观察。在本文中，我们提出了一个新颖的框架，时间循环网络（TRN），通过同时执行在线动作检测和即时未来的预测，来建模每一帧的更大时间上下文。在每个时刻，我们的方法利用累积的历史证据和预测的未来信息来更好地识别当前正在发生的动作，并将这两者整合到一个统一的端到端架构中。我们在两个流行的在线动作检测数据集HDD和TVSeries，以及另一个广泛使用的数据集THUMOS'14上评估了我们的方法。结果表明，TRN显著优于现有技术。",
        "领域": "动作检测/视频分析/实时系统",
        "问题": "实时识别视频中的动作",
        "动机": "为了满足监控和驾驶员辅助系统等实时应用的需求，需要在视频帧到达时立即识别动作。",
        "方法": "提出了时间循环网络（TRN），通过同时执行在线动作检测和即时未来的预测，来建模每一帧的更大时间上下文，并利用累积的历史证据和预测的未来信息来识别当前正在发生的动作。",
        "关键词": [
            "动作检测",
            "视频分析",
            "实时系统"
        ],
        "涉及的技术概念": "时间循环网络（TRN）是一种新颖的框架，用于在线动作检测和即时未来的预测，通过建模每一帧的更大时间上下文来识别当前正在发生的动作。"
    },
    {
        "order": 361,
        "title": "Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Cross-Dataset_Person_Re-Identification_via_Unsupervised_Pose_Disentanglement_and_Adaptation_ICCV_2019_paper.html",
        "abstract": "Person re-identification (re-ID) aims at recognizing the same person from images taken across different cameras. To address this challenging task, existing re-ID models typically rely on a large amount of labeled training data, which is not practical for real-world applications. To alleviate this limitation, researchers now targets at cross-dataset re-ID which focuses on generalizing the discriminative ability to the unlabeled target domain when given a labeled source domain dataset. To achieve this goal, our proposed Pose Disentanglement and Adaptation Network (PDA-Net) aims at learning deep image representation with pose and domain information properly disentangled. With the learned cross-domain pose invariant feature space, our proposed PDA-Net is able to perform pose disentanglement across domains without supervision in identities, and the resulting features can be applied to cross-dataset re-ID. Both of our qualitative and quantitative results on two benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art cross-dataset Re-ID approaches.",
        "中文标题": "跨数据集行人重识别：通过无监督姿态解缠与适应",
        "摘要翻译": "行人重识别（re-ID）旨在从不同摄像头拍摄的图像中识别出同一个人。为了解决这一挑战性任务，现有的re-ID模型通常依赖于大量标注的训练数据，这在实际应用中并不实用。为了缓解这一限制，研究人员现在将目标转向跨数据集re-ID，该任务侧重于在给定标注的源域数据集的情况下，将判别能力泛化到未标注的目标域。为了实现这一目标，我们提出的姿态解缠与适应网络（PDA-Net）旨在学习深度图像表示，并正确解缠姿态和域信息。通过学习到的跨域姿态不变特征空间，我们提出的PDA-Net能够在没有身份监督的情况下跨域执行姿态解缠，并且所得到的特征可以应用于跨数据集re-ID。我们在两个基准数据集上的定性和定量结果都证实了我们方法的有效性及其相对于最先进的跨数据集Re-ID方法的优越性。",
        "领域": "行人重识别/姿态估计/域适应",
        "问题": "解决跨数据集行人重识别任务中，如何在没有目标域标注数据的情况下，将源域学习到的判别能力泛化到目标域。",
        "动机": "实际应用中难以获取大量标注数据，需要开发能够在没有目标域标注数据的情况下，有效进行行人重识别的方法。",
        "方法": "提出姿态解缠与适应网络（PDA-Net），通过学习深度图像表示并正确解缠姿态和域信息，实现跨域姿态不变特征空间的学习，从而在没有身份监督的情况下跨域执行姿态解缠。",
        "关键词": [
            "行人重识别",
            "姿态解缠",
            "域适应"
        ],
        "涉及的技术概念": "姿态解缠与适应网络（PDA-Net）是一种深度学习模型，旨在通过解缠图像中的姿态和域信息，学习跨域的姿态不变特征空间，从而在没有目标域标注数据的情况下，实现有效的跨数据集行人重识别。"
    },
    {
        "order": 362,
        "title": "Elaborate Monocular Point and Line SLAM With Robust Initialization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Elaborate_Monocular_Point_and_Line_SLAM_With_Robust_Initialization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Elaborate_Monocular_Point_and_Line_SLAM_With_Robust_Initialization_ICCV_2019_paper.html",
        "abstract": "This paper presents a monocular indirect SLAM system which performs robust initialization and accurate localization. For initialization, we utilize a matrix factorization-based method. Matrix factorization-based methods require that extracted feature points must be tracked in all used frames. Since consistent tracking is difficult in challenging environments, a geometric interpolation that utilizes epipolar geometry is proposed. For localization, 3D lines are utilized. We propose the use of Plu cker line coordinates to represent geometric information of lines. We also propose orthonormal representation of Plu cker line coordinates and Jacobians of lines for better optimization. Experimental results show that the proposed initialization generates consistent and robust map in linear time with fast convergence even in challenging scenes. And localization using proposed line representations is faster, more accurate and memory efficient than other state-of-the-art methods.",
        "中文标题": "精细的单目点与线SLAM系统及其鲁棒初始化",
        "摘要翻译": "本文提出了一种单目间接SLAM系统，该系统执行鲁棒的初始化和精确的定位。对于初始化，我们利用了一种基于矩阵分解的方法。基于矩阵分解的方法要求提取的特征点必须在所有使用的帧中被跟踪。由于在挑战性环境中一致的跟踪是困难的，因此提出了一种利用对极几何的几何插值方法。对于定位，利用了3D线。我们提出了使用Plucker线坐标来表示线的几何信息。我们还提出了Plucker线坐标的正交表示和线的雅可比矩阵，以便更好地进行优化。实验结果表明，所提出的初始化方法在挑战性场景中也能以线性时间生成一致且鲁棒的地图，并且收敛速度快。使用所提出的线表示的定位比其他最先进的方法更快、更准确且内存效率更高。",
        "领域": "SLAM/三维重建/几何优化",
        "问题": "在挑战性环境中实现鲁棒的初始化和精确的定位",
        "动机": "提高单目SLAM系统在挑战性环境中的性能和效率",
        "方法": "采用基于矩阵分解的初始化方法和Plucker线坐标进行定位，提出几何插值方法和正交表示以及线的雅可比矩阵优化",
        "关键词": [
            "单目SLAM",
            "鲁棒初始化",
            "Plucker线坐标",
            "几何优化"
        ],
        "涉及的技术概念": "矩阵分解、对极几何、Plucker线坐标、正交表示、雅可比矩阵"
    },
    {
        "order": 363,
        "title": "Bridging the Gap Between Detection and Tracking: A Unified Approach",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.html",
        "abstract": "Object detection models have been a source of inspiration for many tracking-by-detection algorithms over the past decade. Recent deep trackers borrow designs or modules from the latest object detection methods, such as bounding box regression, RPN and ROI pooling, and can deliver impressive performance. In this paper, instead of redesigning a new tracking-by-detection algorithm, we aim to explore a general framework for building trackers directly upon almost any advanced object detector. To achieve this, three key gaps must be bridged: (1) Object detectors are class-specific, while trackers are class-agnostic. (2) Object detectors do not differentiate intra-class instances, while this is a critical capability of a tracker. (3) Temporal cues are important for stable long-term tracking while they are not considered in still-image detectors. To address the above issues, we first present a simple target-guidance module for guiding the detector to locate target-relevant objects. Then a meta-learner is adopted for the detector to fast learn and adapt a target-distractor classifier online. We further introduce an anchored updating strategy to alleviate the problem of overfitting. The framework is instantiated on SSD and FasterRCNN, the typical one- and two-stage detectors, respectively. Experiments on OTB, UAV123 and NfS have verified our framework and show that our trackers can benefit from deeper backbone networks, as opposed to many recent trackers.",
        "中文标题": "弥合检测与跟踪之间的差距：一种统一的方法",
        "摘要翻译": "在过去的十年中，物体检测模型一直是许多通过检测进行跟踪算法的灵感来源。最近的深度跟踪器从最新的物体检测方法中借鉴了设计或模块，如边界框回归、RPN和ROI池化，并能够提供令人印象深刻的性能。在本文中，我们不是重新设计一个新的通过检测进行跟踪的算法，而是旨在探索一个通用框架，用于直接在几乎任何先进的物体检测器上构建跟踪器。为了实现这一点，必须弥合三个关键差距：（1）物体检测器是类别特定的，而跟踪器是类别无关的。（2）物体检测器不区分类别内的实例，而这是跟踪器的一个关键能力。（3）时间线索对于稳定的长期跟踪很重要，而静止图像检测器不考虑这些线索。为了解决上述问题，我们首先提出了一个简单的目标引导模块，用于引导检测器定位与目标相关的物体。然后采用元学习器使检测器能够快速在线学习和适应目标-干扰物分类器。我们进一步引入了一种锚定更新策略，以减轻过拟合问题。该框架分别在SSD和FasterRCNN上实例化，这是典型的一阶段和两阶段检测器。在OTB、UAV123和NfS上的实验验证了我们的框架，并表明我们的跟踪器可以从更深的骨干网络中受益，这与许多最近的跟踪器相反。",
        "领域": "物体检测/目标跟踪/元学习",
        "问题": "如何直接在先进的物体检测器上构建跟踪器，并解决检测与跟踪之间的关键差距",
        "动机": "探索一个通用框架，用于直接在几乎任何先进的物体检测器上构建跟踪器，以弥合检测与跟踪之间的差距",
        "方法": "提出了一个目标引导模块和元学习器，以及引入锚定更新策略，以解决检测与跟踪之间的关键差距",
        "关键词": [
            "物体检测",
            "目标跟踪",
            "元学习",
            "边界框回归",
            "RPN",
            "ROI池化"
        ],
        "涉及的技术概念": {
            "物体检测模型": "用于识别图像中物体的模型，通常包括边界框回归、RPN（区域提议网络）和ROI（感兴趣区域）池化等技术。",
            "通过检测进行跟踪": "一种跟踪方法，利用物体检测的结果来进行目标跟踪。",
            "目标引导模块": "用于引导检测器定位与目标相关的物体的模块。",
            "元学习器": "一种学习算法，能够快速在线学习和适应目标-干扰物分类器。",
            "锚定更新策略": "一种策略，用于减轻过拟合问题，通过锚定更新来稳定跟踪过程。",
            "SSD和FasterRCNN": "分别是典型的一阶段和两阶段物体检测器。"
        }
    },
    {
        "order": 364,
        "title": "A Learned Representation for Scalable Vector Graphics",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lopes_A_Learned_Representation_for_Scalable_Vector_Graphics_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lopes_A_Learned_Representation_for_Scalable_Vector_Graphics_ICCV_2019_paper.html",
        "abstract": "Dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. In spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. In this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. This model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. We demonstrate these results on a large dataset of fonts crawled from the web and highlight how such a model captures the statistical dependencies and richness of this dataset. We envision that our model can find use as a tool for graphic designers to facilitate font design.",
        "中文标题": "可缩放矢量图形的学习表示",
        "摘要翻译": "生成模型的显著进展已经使得人工渲染的面孔、动物和自然界中的其他物体达到了接近照片的质量。尽管取得了这样的进展，对视觉和图像的更高层次理解并非来自于对物体的详尽建模，而是识别最能概括物体方面的高层次属性。在这项工作中，我们尝试通过构建矢量图形的序列生成模型来模拟字体的绘制过程。该模型的优势在于为图像提供了一个尺度不变的表示，其潜在表示可以被系统地操作和利用以执行风格传播。我们在从网络爬取的大量字体数据集上展示了这些结果，并强调了这样的模型如何捕捉该数据集的统计依赖性和丰富性。我们预见到我们的模型可以作为图形设计师的工具，以促进字体设计。",
        "领域": "生成模型/矢量图形/字体设计",
        "问题": "如何为矢量图形提供一个尺度不变的表示，并利用这种表示进行风格传播",
        "动机": "尽管生成模型在渲染质量上取得了显著进展，但对视觉和图像的更高层次理解需要识别能概括物体方面的高层次属性，而非详尽建模",
        "方法": "构建矢量图形的序列生成模型，以模拟字体的绘制过程，并利用该模型的潜在表示进行风格传播",
        "关键词": [
            "生成模型",
            "矢量图形",
            "字体设计",
            "风格传播"
        ],
        "涉及的技术概念": "序列生成模型是一种能够按顺序生成数据的模型，常用于文本、音频或图像生成。矢量图形是使用几何图形如点、线、曲线和多边形来表示图像，与位图图像相比，矢量图形可以无限缩放而不失真。风格传播是指将一种风格从一个图像或对象转移到另一个图像或对象的过程。"
    },
    {
        "order": 365,
        "title": "StartNet: Online Detection of Action Start in Untrimmed Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_StartNet_Online_Detection_of_Action_Start_in_Untrimmed_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_StartNet_Online_Detection_of_Action_Start_in_Untrimmed_Videos_ICCV_2019_paper.html",
        "abstract": "We propose StartNet to address Online Detection of Action Start (ODAS) where action starts and their associated categories are detected in untrimmed, streaming videos. Previous methods aim to localize action starts by learning feature representations that can directly separate the start point from its preceding background. It is challenging due to the subtle appearance difference near the action starts and the lack of training data. Instead, StartNet decomposes ODAS into two stages: action classification (using ClsNet) and start point localization (using LocNet). ClsNet focuses on per-frame labeling and predicts action score distributions online. Based on the predicted action scores of the past and current frames, LocNet conducts class-agnostic start detection by optimizing long-term localization rewards using policy gradient methods. The proposed framework is validated on two large-scale datasets, THUMOS'14 and ActivityNet. The experimental results show that StartNet significantly outperforms the state-of-the-art by 15%-30% p-mAP under the offset tolerance of 1-10 seconds on THUMOS'14, and achieves comparable performance on ActivityNet with 10 times smaller time offset.",
        "中文标题": "StartNet：未剪辑视频中动作开始的在线检测",
        "摘要翻译": "我们提出了StartNet，以解决在线检测动作开始（ODAS）的问题，在未剪辑的流媒体视频中检测动作开始及其相关类别。以前的方法旨在通过学习能够直接将开始点与其前面的背景分离的特征表示来定位动作开始。由于动作开始附近的细微外观差异和训练数据的缺乏，这具有挑战性。相反，StartNet将ODAS分解为两个阶段：动作分类（使用ClsNet）和开始点定位（使用LocNet）。ClsNet专注于每帧标签，并在线预测动作分数分布。基于过去和当前帧的预测动作分数，LocNet通过使用策略梯度方法优化长期定位奖励来进行类别无关的开始检测。所提出的框架在两个大规模数据集THUMOS'14和ActivityNet上进行了验证。实验结果表明，在THUMOS'14上，StartNet在1-10秒的偏移容忍度下显著优于最先进的方法15%-30% p-mAP，并在ActivityNet上实现了可比的性能，时间偏移小10倍。",
        "领域": "动作识别/视频分析/在线学习",
        "问题": "在未剪辑的流媒体视频中在线检测动作开始及其类别",
        "动机": "解决动作开始检测中由于细微外观差异和训练数据缺乏带来的挑战",
        "方法": "将动作开始检测分解为动作分类和开始点定位两个阶段，分别使用ClsNet进行每帧标签预测和LocNet进行类别无关的开始检测",
        "关键词": [
            "动作识别",
            "视频分析",
            "在线学习"
        ],
        "涉及的技术概念": "动作分类（ClsNet）用于每帧标签预测，开始点定位（LocNet）用于基于策略梯度方法的类别无关开始检测，以及在线学习在流媒体视频中的应用。"
    },
    {
        "order": 366,
        "title": "Adaptive Density Map Generation for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Adaptive_Density_Map_Generation_for_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Adaptive_Density_Map_Generation_for_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations.",
        "中文标题": "自适应密度图生成用于人群计数",
        "摘要翻译": "人群计数是计算机视觉中的一个重要主题，因为它在监控系统中的实际应用。人群计数算法的典型设计分为两个步骤。首先，从地面真实点图生成人群图像的地面真实密度图（密度图生成），例如，通过与高斯核卷积。其次，设计深度学习模型从输入图像预测密度图（密度图估计）。大多数研究工作集中在密度图估计问题上，而密度图生成问题尚未得到充分探索。特别是，密度图可以被视为用于训练人群计数网络的中间表示。在端到端训练的意义上，用于生成密度图的手工方法可能对特定网络或数据集不是最优的。为了解决这个问题，我们首先展示了不同密度图的影响，并且通过使用学习的细化网络细化现有的密度图可以获得更好的地面真实密度图，该网络与计数器联合训练。然后，我们提出了一个自适应密度图生成器，它以注释点图作为输入，并为计数器学习密度图表示。计数器和生成器在端到端框架内联合训练。在流行计数数据集上的实验结果证实了所提出的可学习密度图表示的有效性。",
        "领域": "人群计数/密度图生成/深度学习",
        "问题": "如何优化人群计数中的密度图生成过程",
        "动机": "现有的人群计数方法在密度图生成方面未得到充分探索，且手工方法可能不是最优的",
        "方法": "提出一个自适应密度图生成器，与计数器联合训练，以学习最优的密度图表示",
        "关键词": [
            "人群计数",
            "密度图生成",
            "深度学习",
            "端到端训练",
            "自适应生成器"
        ],
        "涉及的技术概念": {
            "密度图生成": "通过与高斯核卷积从地面真实点图生成密度图的过程",
            "密度图估计": "使用深度学习模型从输入图像预测密度图",
            "端到端训练": "一种训练方法，其中所有组件（如密度图生成器和计数器）联合训练，以优化整体性能",
            "自适应密度图生成器": "一种能够根据输入数据自动调整其参数以生成最优密度图的模型"
        }
    },
    {
        "order": 367,
        "title": "Learning the Model Update for Siamese Trackers",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_the_Model_Update_for_Siamese_Trackers_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_the_Model_Update_for_Siamese_Trackers_ICCV_2019_paper.html",
        "abstract": "Siamese approaches address the visual tracking problem by extracting an appearance template from the current frame, which is used to localize the target in the next frame. In general, this template is linearly combined with the accumulated template from the previous frame, resulting in an exponential decay of information over time. While such an approach to updating has led to improved results, its simplicity limits the potential gain likely to be obtained by learning to update. Therefore, we propose to replace the handcrafted update function with a method which learns to update. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame aims to estimate the optimal template for the next frame. The UpdateNet is compact and can easily be integrated into existing Siamese trackers. We demonstrate the generality of the proposed approach by applying it to two Siamese trackers, SiamFC and DaSiamRPN. Extensive experiments on VOT2016, VOT2018, LaSOT, and TrackingNet datasets demonstrate that our UpdateNet effectively predicts the new target template, outperforming the standard linear update. On the large-scale TrackingNet dataset, our UpdateNet improves the results of DaSiamRPN with an absolute gain of 3.9% in terms of success score.",
        "中文标题": "学习Siamese跟踪器的模型更新",
        "摘要翻译": "Siamese方法通过从当前帧提取外观模板来解决视觉跟踪问题，该模板用于在下一帧中定位目标。通常，该模板与前一帧的累积模板线性结合，导致信息随时间呈指数衰减。虽然这种更新方法带来了改进的结果，但其简单性限制了通过学习更新可能获得的潜在增益。因此，我们提出用学习更新的方法替代手工设计的更新函数。我们使用一个称为UpdateNet的卷积神经网络，给定初始模板、累积模板和当前帧的模板，旨在估计下一帧的最佳模板。UpdateNet紧凑且易于集成到现有的Siamese跟踪器中。我们通过将其应用于两个Siamese跟踪器SiamFC和DaSiamRPN，展示了所提出方法的通用性。在VOT2016、VOT2018、LaSOT和TrackingNet数据集上的大量实验表明，我们的UpdateNet有效地预测了新的目标模板，优于标准的线性更新。在大型TrackingNet数据集上，我们的UpdateNet将DaSiamRPN的结果提高了3.9%的成功分数。",
        "领域": "视觉跟踪/卷积神经网络/目标定位",
        "问题": "如何更有效地更新Siamese跟踪器中的目标模板以提高跟踪性能",
        "动机": "现有的线性更新方法虽然简单有效，但其简单性限制了通过学习更新可能获得的潜在增益",
        "方法": "提出使用一个称为UpdateNet的卷积神经网络来学习更新目标模板，替代现有的手工设计更新函数",
        "关键词": [
            "视觉跟踪",
            "卷积神经网络",
            "目标定位",
            "Siamese跟踪器",
            "UpdateNet"
        ],
        "涉及的技术概念": {
            "Siamese跟踪器": "一种通过比较目标与搜索区域的特征来进行目标跟踪的方法",
            "卷积神经网络": "一种深度学习模型，特别适用于处理图像数据",
            "UpdateNet": "本文提出的用于学习更新目标模板的卷积神经网络",
            "VOT2016, VOT2018, LaSOT, TrackingNet": "用于视觉跟踪算法评估的公开数据集"
        }
    },
    {
        "order": 368,
        "title": "ELF: Embedded Localisation of Features in Pre-Trained CNN",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Benbihi_ELF_Embedded_Localisation_of_Features_in_Pre-Trained_CNN_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Benbihi_ELF_Embedded_Localisation_of_Features_in_Pre-Trained_CNN_ICCV_2019_paper.html",
        "abstract": "This paper introduces a novel feature detector based only on information embedded inside a CNN trained on standard tasks (e.g. classification). While previous works already show that the features of a trained CNN are suitable descriptors, we show here how to extract the feature locations from the network to build a detector. This information is computed from the gradient of the feature map with respect to the input image. This provides a saliency map with local maxima on relevant keypoint locations. Contrary to recent CNN-based detectors, this method requires neither supervised training nor finetuning. We evaluate how repeatable and how 'matchable' the detected keypoints are with the repeatability and matching scores. Matchability is measured with a simple descriptor introduced for the sake of the evaluation. This novel detector reaches similar performances on the standard evaluation HPatches dataset, as well as comparable robustness against illumination and viewpoint changes on Webcam and photo-tourism images. These results show that a CNN trained on a standard task embeds feature location information that is as relevant as when the CNN is specifically trained for feature detection.",
        "中文标题": "ELF: 预训练CNN中的特征嵌入定位",
        "摘要翻译": "本文介绍了一种仅基于在标准任务（如分类）上训练的CNN内部嵌入信息的新型特征检测器。虽然之前的工作已经表明，训练好的CNN的特征是合适的描述符，但我们在这里展示了如何从网络中提取特征位置以构建检测器。该信息是从特征图相对于输入图像的梯度计算得出的。这提供了一个在相关关键点位置上具有局部最大值的显著性图。与最近的基于CNN的检测器不同，该方法既不需要监督训练也不需要微调。我们评估了检测到的关键点的可重复性和可匹配性，使用可重复性和匹配分数进行衡量。可匹配性是通过为评估目的引入的简单描述符来测量的。这种新型检测器在标准评估HPatches数据集上达到了相似的性能，以及在Webcam和照片旅游图像上对光照和视角变化的可比鲁棒性。这些结果表明，在标准任务上训练的CNN嵌入的特征位置信息与专门为特征检测训练的CNN一样相关。",
        "领域": "特征检测/图像匹配/显著性检测",
        "问题": "如何从预训练的CNN中提取特征位置信息以构建特征检测器",
        "动机": "探索预训练CNN中嵌入的特征位置信息，以构建无需额外监督训练或微调的特征检测器",
        "方法": "通过计算特征图相对于输入图像的梯度来提取特征位置信息，构建显著性图，并评估关键点的可重复性和可匹配性",
        "关键词": [
            "特征检测",
            "图像匹配",
            "显著性检测",
            "CNN",
            "梯度计算"
        ],
        "涉及的技术概念": "特征检测器、CNN（卷积神经网络）、特征图、梯度计算、显著性图、关键点、可重复性、匹配分数、HPatches数据集、光照变化、视角变化"
    },
    {
        "order": 369,
        "title": "Video Classification With Channel-Separated Convolutional Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tran_Video_Classification_With_Channel-Separated_Convolutional_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tran_Video_Classification_With_Channel-Separated_Convolutional_Networks_ICCV_2019_paper.html",
        "abstract": "Group convolution has been shown to offer great computational savings in various 2D convolutional architectures for image classification. It is natural to ask: 1) if group convolution can help to alleviate the high computational cost of video classification networks; 2) what factors matter the most in 3D group convolutional networks; and 3) what are good computation/accuracy trade-offs with 3D group convolutional networks. This paper studies the effects of different design choices in 3D group convolutional networks for video classification. We empirically demonstrate that the amount of channel interactions plays an important role in the accuracy of 3D group convolutional networks. Our experiments suggest two main findings. First, it is a good practice to factorize 3D convolutions by separating channel interactions and spatiotemporal interactions as this leads to improved accuracy and lower computational cost. Second, 3D channel-separated convolutions provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions. These two empirical findings lead us to design an architecture -- Channel-Separated Convolutional Network (CSN) -- which is simple, efficient, yet accurate. On Sports1M and Kinetics, our CSNs are comparable with or better than the state-of-the-art while being 2-3 times more efficient.",
        "中文标题": "使用通道分离卷积网络进行视频分类",
        "摘要翻译": "组卷积已被证明在各种用于图像分类的2D卷积架构中提供了巨大的计算节省。很自然地会问：1）组卷积是否有助于减轻视频分类网络的高计算成本；2）在3D组卷积网络中，哪些因素最为重要；3）使用3D组卷积网络时，什么是好的计算/准确性权衡。本文研究了3D组卷积网络中不同设计选择对视频分类的影响。我们通过实验证明，通道交互的数量在3D组卷积网络的准确性中起着重要作用。我们的实验提出了两个主要发现。首先，通过分离通道交互和时空交互来分解3D卷积是一个好的做法，因为这可以提高准确性并降低计算成本。其次，与3D卷积相比，3D通道分离卷积提供了一种正则化形式，导致训练准确性较低但测试准确性较高。这两个实证发现引导我们设计了一个架构——通道分离卷积网络（CSN）——它简单、高效且准确。在Sports1M和Kinetics上，我们的CSN与最先进的技术相当或更好，同时效率提高了2-3倍。",
        "领域": "视频分类/卷积神经网络/计算效率",
        "问题": "如何降低视频分类网络的计算成本并提高准确性",
        "动机": "探索组卷积在3D卷积网络中的应用，以解决视频分类中的高计算成本问题",
        "方法": "通过实验研究3D组卷积网络中不同设计选择的影响，提出并设计通道分离卷积网络（CSN）",
        "关键词": [
            "组卷积",
            "3D卷积",
            "视频分类",
            "计算效率",
            "通道分离"
        ],
        "涉及的技术概念": "组卷积是一种减少卷积神经网络计算成本的技术，通过在卷积操作中分组处理输入通道来实现。3D卷积网络用于处理视频数据，通过同时考虑时间和空间维度来提取特征。通道分离卷积网络（CSN）是一种特定的网络架构，旨在通过分离通道交互和时空交互来提高视频分类的准确性和计算效率。"
    },
    {
        "order": 370,
        "title": "Fast-deepKCF Without Boundary Effect",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Fast-deepKCF_Without_Boundary_Effect_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Fast-deepKCF_Without_Boundary_Effect_ICCV_2019_paper.html",
        "abstract": "In recent years, correlation filter based trackers (CF trackers) have received much attention because of their top performance. Most CF trackers, however, suffer from low frame-per-second (fps) in pursuit of higher localization accuracy by relaxing the boundary effect or exploiting the high-dimensional deep features. In order to achieve real-time tracking speed while maintaining high localization accuracy, in this paper, we propose a novel CF tracker, fdKCF*, which casts aside the popular acceleration tool, i.e., fast Fourier transform, employed by all existing CF trackers, and exploits the inherent high-overlap among real (i.e., noncyclic) and dense samples to efficiently construct the kernel matrix. Our fdKCF* enjoys the following three advantages. (i) It is efficiently trained in kernel space and spatial domain without the boundary effect. (ii) Its fps is almost independent of the number of feature channels. Therefore, it is almost real-time, i.e., 24 fps on OTB-2015, even though the high-dimensional deep features are employed. (iii) Its localization accuracy is state-of-the-art. Extensive experiments on four public benchmarks, OTB-2013, OTB-2015, VOT2016, and VOT2017, show that the proposed fdKCF* achieves the state-of-the-art localization performance with remarkably faster speed than C-COT and ECO.",
        "中文标题": "无边界效应的快速深度KCF",
        "摘要翻译": "近年来，基于相关滤波器的跟踪器（CF跟踪器）因其卓越的性能而受到广泛关注。然而，大多数CF跟踪器在追求更高的定位精度时，通过放宽边界效应或利用高维深度特征，导致每秒帧数（fps）较低。为了在保持高定位精度的同时实现实时跟踪速度，本文提出了一种新颖的CF跟踪器fdKCF*，它摒弃了所有现有CF跟踪器采用的流行加速工具——快速傅里叶变换，并利用真实（即非循环）和密集样本之间的固有高重叠来高效构建核矩阵。我们的fdKCF*具有以下三个优势。（i）它在核空间和空间域中高效训练，没有边界效应。（ii）其fps几乎与特征通道的数量无关。因此，即使采用高维深度特征，它也是几乎实时的，即在OTB-2015上达到24 fps。（iii）其定位精度是最先进的。在四个公共基准测试（OTB-2013、OTB-2015、VOT2016和VOT2017）上的大量实验表明，所提出的fdKCF*在定位性能上达到了最先进水平，速度显著快于C-COT和ECO。",
        "领域": "目标跟踪/相关滤波器/实时系统",
        "问题": "如何在保持高定位精度的同时实现实时跟踪速度",
        "动机": "大多数CF跟踪器在追求更高定位精度时牺牲了实时性，需要一种新的方法来解决这一问题",
        "方法": "提出了一种新颖的CF跟踪器fdKCF*，摒弃了快速傅里叶变换，利用真实和密集样本之间的高重叠来高效构建核矩阵",
        "关键词": [
            "目标跟踪",
            "相关滤波器",
            "实时系统",
            "核矩阵",
            "定位精度"
        ],
        "涉及的技术概念": "相关滤波器（CF）跟踪器、快速傅里叶变换（FFT）、核矩阵、每秒帧数（fps）、高维深度特征、定位精度、OTB-2013、OTB-2015、VOT2016、VOT2017、C-COT、ECO"
    },
    {
        "order": 371,
        "title": "Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Attention-Aware_Polarity_Sensitive_Embedding_for_Affective_Image_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yao_Attention-Aware_Polarity_Sensitive_Embedding_for_Affective_Image_Retrieval_ICCV_2019_paper.html",
        "abstract": "Images play a crucial role for people to express their opinions online due to the increasing popularity of social networks. While an affective image retrieval system is useful for obtaining visual contents with desired emotions from a massive repository, the abstract and subjective characteristics make the task challenging. To address the problem, this paper introduces an Attention-aware Polarity Sensitive Embedding (APSE) network to learn affective representations in an end-to-end manner. First, to automatically discover and model the informative regions of interest, we develop a hierarchical attention mechanism, in which both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we present a weighted emotion-pair loss to take the inter- and intra-polarity relationships of the emotional labels into consideration. Guided by attention module, we weight the sample pairs adaptively which further improves the performance of feature embedding. Extensive experiments on four popular benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches.",
        "中文标题": "注意力感知的极性敏感嵌入用于情感图像检索",
        "摘要翻译": "由于社交网络的日益普及，图像在人们在线表达意见中扮演着至关重要的角色。虽然情感图像检索系统对于从海量存储库中获取具有期望情感的视觉内容非常有用，但抽象和主观的特性使得这一任务具有挑战性。为了解决这个问题，本文介绍了一种注意力感知的极性敏感嵌入（APSE）网络，以端到端的方式学习情感表示。首先，为了自动发现和建模信息丰富的感兴趣区域，我们开发了一种分层注意力机制，其中聚合了极性和情感特定的注意力表示，用于判别性特征嵌入。其次，我们提出了一种加权情感对损失，以考虑情感标签的极间和极内关系。在注意力模块的指导下，我们自适应地加权样本对，这进一步提高了特征嵌入的性能。在四个流行的基准数据集上的广泛实验表明，所提出的方法在性能上优于最先进的方法。",
        "领域": "情感计算/图像检索/注意力机制",
        "问题": "如何从海量图像库中有效检索出具有特定情感的图像",
        "动机": "社交网络中图像作为表达意见的重要方式，需要一种有效的方法来检索具有特定情感的图像",
        "方法": "开发了一种分层注意力机制和加权情感对损失，以端到端的方式学习情感表示",
        "关键词": [
            "情感图像检索",
            "注意力机制",
            "极性敏感嵌入",
            "加权情感对损失"
        ],
        "涉及的技术概念": {
            "分层注意力机制": "一种自动发现和建模信息丰富的感兴趣区域的方法，通过聚合极性和情感特定的注意力表示来实现",
            "加权情感对损失": "一种考虑情感标签的极间和极内关系的损失函数，用于提高特征嵌入的性能",
            "端到端学习": "一种直接从输入到输出进行学习的方法，无需手动设计特征提取步骤"
        }
    },
    {
        "order": 372,
        "title": "Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Joint_Group_Feature_Selection_and_Discriminative_Filter_Learning_for_Robust_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Joint_Group_Feature_Selection_and_Discriminative_Filter_Learning_for_Robust_ICCV_2019_paper.html",
        "abstract": "We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at https://github.com/XU-TIANYANG/GFS-DCF.",
        "中文标题": "联合组特征选择与判别滤波器学习用于鲁棒视觉对象跟踪",
        "摘要翻译": "我们提出了一种新的基于判别相关滤波器（GFS-DCF）的视觉对象跟踪的组特征选择方法。该方法的关键创新在于跨通道和空间维度执行组特征选择，从而精确定位多通道特征与滤波系统的结构相关性。与广泛使用的空间正则化或特征选择方法相比，据我们所知，这是首次在基于DCF的跟踪中提倡通道选择。我们证明了我们的GFS-DCF方法能够显著提高配备深度神经网络特征的DCF跟踪器的性能。此外，我们的GFS-DCF实现了联合特征选择和滤波器学习，增强了学习到的滤波器的判别性和可解释性。为了进一步提高性能，我们通过约束滤波器在时间帧上的平滑性，使用高效的低秩近似自适应地整合历史信息。通过设计，在跟踪过程中动态学习特定的时空通道配置，突出相关特征，减轻判别性较低表示的性能下降影响，并减少信息冗余。在OTB2013、OTB2015、VOT2017、VOT2018和TrackingNet上获得的实验结果证明了我们的GFS-DCF的优点及其相对于最先进跟踪器的优越性。代码公开在https://github.com/XU-TIANYANG/GFS-DCF。",
        "领域": "视觉对象跟踪/特征选择/滤波器学习",
        "问题": "提高视觉对象跟踪的准确性和鲁棒性",
        "动机": "现有的视觉对象跟踪方法在处理多通道特征时，未能充分利用通道间的结构相关性，导致跟踪性能受限。",
        "方法": "提出了一种新的组特征选择方法（GFS-DCF），通过跨通道和空间维度执行组特征选择，联合特征选择和滤波器学习，以及自适应地整合历史信息来提高跟踪性能。",
        "关键词": [
            "视觉对象跟踪",
            "组特征选择",
            "判别相关滤波器",
            "低秩近似"
        ],
        "涉及的技术概念": "组特征选择（Group Feature Selection）是一种特征选择方法，旨在从一组特征中选择出最相关的子集。判别相关滤波器（Discriminative Correlation Filters, DCF）是一种用于视觉对象跟踪的滤波器，通过学习目标的外观模型来区分目标和背景。低秩近似（Low-rank Approximation）是一种数学技术，用于近似表示数据，以减少计算复杂度和存储需求。"
    },
    {
        "order": 373,
        "title": "Program-Guided Image Manipulators",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Program-Guided_Image_Manipulators_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Program-Guided_Image_Manipulators_ICCV_2019_paper.html",
        "abstract": "Humans are capable of building holistic representations for images at various levels, from local objects, to pairwise relations, to global structures. The interpretation of structures involves reasoning over repetition and symmetry of the objects in the image. In this paper, we present the Program-Guided Image Manipulator (PG-IM), inducing neuro-symbolic program-like representations to represent and manipulate images. Given an image, PG-IM detects repeated patterns, induces symbolic programs, and manipulates the image using a neural network that is guided by the program. PG-IM learns from a single image, exploiting its internal statistics. Despite trained only on image inpainting, PG-IM is directly capable of extrapolation and regularity editing in a unified framework. Extensive experiments show that PG-IM achieves superior performance on all the tasks.",
        "中文标题": "程序引导的图像操纵器",
        "摘要翻译": "人类能够为图像构建从局部对象、成对关系到全局结构的多层次整体表示。结构的解释涉及对图像中对象的重复和对称性的推理。在本文中，我们提出了程序引导的图像操纵器（PG-IM），它诱导出类似神经符号程序的表示来代表和操纵图像。给定一张图像，PG-IM检测重复模式，诱导符号程序，并使用由程序引导的神经网络来操纵图像。PG-IM从单张图像中学习，利用其内部统计信息。尽管仅在图像修复上训练，PG-IM能够直接在一个统一的框架内进行外推和规律性编辑。大量实验表明，PG-IM在所有任务上都实现了卓越的性能。",
        "领域": "图像修复/图像编辑/神经符号学习",
        "问题": "如何从单张图像中学习并利用其内部统计信息进行图像操纵",
        "动机": "探索一种能够理解和操纵图像结构的方法，特别是通过检测重复模式和诱导符号程序来实现图像编辑",
        "方法": "提出程序引导的图像操纵器（PG-IM），通过检测图像中的重复模式，诱导符号程序，并使用由程序引导的神经网络来操纵图像",
        "关键词": [
            "图像修复",
            "图像编辑",
            "神经符号学习"
        ],
        "涉及的技术概念": {
            "神经符号程序": "一种结合了神经网络和符号逻辑的表示方法，用于理解和操纵图像",
            "图像修复": "一种图像处理技术，用于恢复或重建图像中缺失或损坏的部分",
            "外推": "在图像编辑中，指基于现有图像内容预测和生成图像外部或未观察到的部分",
            "规律性编辑": "指在图像编辑过程中，对图像中的重复模式或对称性进行修改或调整"
        }
    },
    {
        "order": 374,
        "title": "Predicting the Future: A Jointly Learnt Model for Action Anticipation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gammulle_Predicting_the_Future_A_Jointly_Learnt_Model_for_Action_Anticipation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gammulle_Predicting_the_Future_A_Jointly_Learnt_Model_for_Action_Anticipation_ICCV_2019_paper.html",
        "abstract": "Inspired by human neurological structures for action anticipation, we present an action anticipation model that enables the prediction of plausible future actions by forecasting both the visual and temporal future. In contrast to current state-of-the-art methods which first learn a model to predict future video features and then perform action anticipation using these features, the proposed framework jointly learns to perform the two tasks, future visual and temporal representation synthesis, and early action anticipation. The joint learning framework ensures that the predicted future embeddings are informative to the action anticipation task. Furthermore, through extensive experimental evaluations we demonstrate the utility of using both visual and temporal semantics of the scene, and illustrate how this representation synthesis could be achieved through a recurrent Generative Adversarial Network (GAN) framework. Our model outperforms the current state-of-the-art methods on multiple datasets: UCF101, UCF101-24, UT-Interaction and TV Human Interaction.",
        "中文标题": "预测未来：一个联合学习的动作预期模型",
        "摘要翻译": "受到人类神经结构对动作预期的启发，我们提出了一个动作预期模型，该模型通过预测视觉和时间上的未来来预测可能的未来动作。与当前最先进的方法相比，这些方法首先学习一个模型来预测未来的视频特征，然后使用这些特征进行动作预期，而提出的框架联合学习执行两个任务，即未来视觉和时间表示合成，以及早期动作预期。联合学习框架确保预测的未来嵌入对动作预期任务是有信息量的。此外，通过广泛的实验评估，我们证明了使用场景的视觉和时间语义的效用，并说明了如何通过循环生成对抗网络（GAN）框架实现这种表示合成。我们的模型在多个数据集上优于当前最先进的方法：UCF101、UCF101-24、UT-Interaction和TV Human Interaction。",
        "领域": "动作预期/视频分析/生成对抗网络",
        "问题": "如何有效地预测未来动作",
        "动机": "受到人类神经结构对动作预期的启发，旨在提高动作预测的准确性和效率",
        "方法": "提出一个联合学习框架，同时进行未来视觉和时间表示合成以及早期动作预期，使用循环生成对抗网络（GAN）框架实现表示合成",
        "关键词": [
            "动作预期",
            "视频分析",
            "生成对抗网络"
        ],
        "涉及的技术概念": "联合学习框架、未来视觉和时间表示合成、早期动作预期、循环生成对抗网络（GAN）"
    },
    {
        "order": 375,
        "title": "Zero-Shot Emotion Recognition via Affective Structural Embedding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhan_Zero-Shot_Emotion_Recognition_via_Affective_Structural_Embedding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_Zero-Shot_Emotion_Recognition_via_Affective_Structural_Embedding_ICCV_2019_paper.html",
        "abstract": "Image emotion recognition attracts much attention in recent years due to its wide applications. It aims to classify the emotional response of humans, where candidate emotion categories are generally defined by specific psychological theories, such as Ekman's six basic emotions. However, with the development of psychological theories, emotion categories become increasingly diverse, fine-grained, and difficult to collect samples. In this paper, we investigate zero-shot learning (ZSL) problem in the emotion recognition task, which tries to recognize the new unseen emotions. Specifically, we propose a novel affective-structural embedding framework, utilizing mid-level semantic representation, i.e., adjective-noun pairs (ANP) features, to construct an affective embedding space. By doing this, the learned intermediate space can narrow the semantic gap between low-level visual and high-level semantic features. In addition, we introduce an affective adversarial constraint to retain the discriminative capacity of visual features and the affective structural information of semantic features during training process. Our method is evaluated on five widely used affective datasets and the perimental results show the proposed algorithm outperforms the state-of-the-art approaches.",
        "中文标题": "通过情感结构嵌入实现零样本情感识别",
        "摘要翻译": "近年来，图像情感识别因其广泛的应用而受到大量关注。它旨在对人类的情感反应进行分类，其中候选情感类别通常由特定的心理学理论定义，如埃克曼的六种基本情感。然而，随着心理学理论的发展，情感类别变得越来越多样化、细粒度化，并且难以收集样本。在本文中，我们研究了情感识别任务中的零样本学习（ZSL）问题，该问题试图识别新的未见过的情感。具体来说，我们提出了一种新颖的情感结构嵌入框架，利用中级语义表示，即形容词-名词对（ANP）特征，构建情感嵌入空间。通过这样做，学习到的中间空间可以缩小低级视觉特征和高级语义特征之间的语义差距。此外，我们引入了情感对抗约束，以在训练过程中保留视觉特征的判别能力和语义特征的情感结构信息。我们的方法在五个广泛使用的情感数据集上进行了评估，实验结果表明，所提出的算法优于最先进的方法。",
        "领域": "情感计算/零样本学习/语义嵌入",
        "问题": "解决情感识别任务中的零样本学习问题，即识别新的未见过的情感",
        "动机": "随着心理学理论的发展，情感类别变得越来越多样化、细粒度化，并且难以收集样本，需要一种方法来识别未见过的情感",
        "方法": "提出了一种新颖的情感结构嵌入框架，利用中级语义表示（形容词-名词对特征）构建情感嵌入空间，并引入情感对抗约束以保留视觉特征的判别能力和语义特征的情感结构信息",
        "关键词": [
            "情感识别",
            "零样本学习",
            "语义嵌入",
            "情感结构嵌入",
            "形容词-名词对特征"
        ],
        "涉及的技术概念": "零样本学习（ZSL）是一种机器学习方法，旨在识别训练数据中未出现过的类别。情感结构嵌入框架是一种利用中级语义表示（如形容词-名词对特征）来构建情感嵌入空间的方法，以缩小低级视觉特征和高级语义特征之间的语义差距。情感对抗约束是一种在训练过程中保留视觉特征判别能力和语义特征情感结构信息的技术。"
    },
    {
        "order": 376,
        "title": "Sampling Wisely: Deep Image Embedding by Top-K Precision Optimization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Sampling_Wisely_Deep_Image_Embedding_by_Top-K_Precision_Optimization_ICCV_2019_paper.html",
        "abstract": "Deep image embedding aims at learning a convolutional neural network (CNN) based mapping function that maps an image to a feature vector. The embedding quality is usually evaluated by the performance in image search tasks. Since very few users bother to open the second page search results, top-k precision mostly dominates the user experience and thus is one of the crucial evaluation metrics for the embedding quality. Despite being extensively studied, existing algorithms are usually based on heuristic observation without theoretical guarantee. Consequently, gradient descent direction on the training loss is mostly inconsistent with the direction of optimizing the concerned evaluation metric. This inconsistency certainly misleads the training direction and degrades the performance. In contrast to existing works, in this paper, we propose a novel deep image embedding algorithm with end-to-end optimization to top-k precision, the evaluation metric that is closely related to user experience. Specially, our loss function is constructed with wisely selected \"misplaced\" images along the top k nearest neighbor decision boundary, so that the gradient descent update directly promotes the concerned metric, top-k precision. Further more, our theoretical analysis on the upper bounding and consistency properties of the proposed loss supports that minimizing our proposed loss is equivalent to maximizing top-k precision. Experiments show that our proposed algorithm outperforms all compared state-of-the-art deep image embedding algorithms on three benchmark datasets.",
        "中文标题": "明智采样：通过Top-K精度优化进行深度图像嵌入",
        "摘要翻译": "深度图像嵌入旨在学习一个基于卷积神经网络（CNN）的映射函数，该函数将图像映射到特征向量。嵌入质量通常通过图像搜索任务中的表现来评估。由于很少有用户会打开第二页的搜索结果，top-k精度大多主导了用户体验，因此是评估嵌入质量的关键指标之一。尽管已有广泛研究，现有算法通常基于启发式观察，没有理论保证。因此，训练损失上的梯度下降方向与优化相关评估指标的方向大多不一致。这种不一致性无疑误导了训练方向并降低了性能。与现有工作相比，在本文中，我们提出了一种新颖的深度图像嵌入算法，通过端到端优化来优化top-k精度，这是一个与用户体验密切相关的评估指标。特别地，我们的损失函数是通过明智选择的位于top k最近邻决策边界上的“错位”图像构建的，以便梯度下降更新直接促进相关指标，即top-k精度。此外，我们对所提出损失的上界和一致性属性的理论分析支持了最小化我们提出的损失等同于最大化top-k精度的观点。实验表明，我们提出的算法在三个基准数据集上优于所有比较的最先进的深度图像嵌入算法。",
        "领域": "图像搜索/卷积神经网络/特征向量",
        "问题": "现有深度图像嵌入算法在优化top-k精度时缺乏理论保证，导致训练方向与优化目标不一致",
        "动机": "提高深度图像嵌入算法在图像搜索任务中的top-k精度，以改善用户体验",
        "方法": "提出一种新颖的深度图像嵌入算法，通过端到端优化来优化top-k精度，特别地，通过明智选择的位于top k最近邻决策边界上的“错位”图像构建损失函数，使梯度下降更新直接促进top-k精度",
        "关键词": [
            "深度图像嵌入",
            "top-k精度",
            "卷积神经网络",
            "特征向量",
            "图像搜索"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）用于图像到特征向量的映射；top-k精度作为评估嵌入质量的关键指标；端到端优化方法用于直接优化top-k精度；通过选择特定图像构建损失函数以促进梯度下降更新直接优化top-k精度。"
    },
    {
        "order": 377,
        "title": "Human-Aware Motion Deblurring",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Human-Aware_Motion_Deblurring_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Human-Aware_Motion_Deblurring_ICCV_2019_paper.html",
        "abstract": "This paper proposes a human-aware deblurring model that disentangles the motion blur between foreground (FG) humans and background (BG). The proposed model is based on a triple-branch encoder-decoder architecture. The first two branches are learned for sharpening FG humans and BG details, respectively; while the third one produces global, harmonious results by comprehensively fusing multi-scale deblurring information from the two domains. The proposed model is further endowed with a supervised, human-aware attention mechanism in an end-to-end fashion. It learns a soft mask that encodes FG human information and explicitly drives the FG/BG decoder-branches to focus on their specific domains. Above designs lead to a fully differentiable motion deblurring network, which can be trained end-to-end. To further benefit the research towards Human-aware Image Deblurring, we introduce a large-scale dataset, named HIDE, which consists of 8,422 blurry and sharp image pairs with 65,784 densely annotated FG human bounding boxes. HIDE is specifically built to span a broad range of scenes, human object sizes, motion patterns, and background complexities. Extensive experiments on public benchmarks and our dataset demonstrate that our model performs favorably against the state-of-the-art motion deblurring methods, especially in capturing semantic details.",
        "中文标题": "人类感知的运动去模糊",
        "摘要翻译": "本文提出了一种人类感知的去模糊模型，该模型能够分离前景（FG）人类和背景（BG）之间的运动模糊。所提出的模型基于三重分支编码器-解码器架构。前两个分支分别用于锐化FG人类和BG细节；而第三个分支通过综合融合来自两个领域的多尺度去模糊信息，产生全局和谐的结果。所提出的模型进一步以端到端的方式配备了有监督的人类感知注意力机制。它学习了一个编码FG人类信息的软掩码，并明确驱动FG/BG解码器分支专注于其特定领域。上述设计导致了一个完全可微的运动去模糊网络，可以端到端训练。为了进一步促进人类感知图像去模糊的研究，我们引入了一个名为HIDE的大规模数据集，该数据集由8,422对模糊和清晰图像组成，包含65,784个密集注释的FG人类边界框。HIDE专门构建以涵盖广泛的场景、人类对象大小、运动模式和背景复杂性。在公共基准和我们的数据集上的广泛实验表明，我们的模型在捕捉语义细节方面优于最先进的运动去模糊方法。",
        "领域": "图像去模糊/人类感知计算/深度学习",
        "问题": "分离前景人类和背景之间的运动模糊",
        "动机": "提高图像去模糊技术，特别是在捕捉语义细节方面，以更好地服务于人类感知计算",
        "方法": "基于三重分支编码器-解码器架构的模型，配备有监督的人类感知注意力机制，学习软掩码以编码FG人类信息，并驱动FG/BG解码器分支专注于其特定领域",
        "关键词": [
            "图像去模糊",
            "人类感知计算",
            "深度学习"
        ],
        "涉及的技术概念": "三重分支编码器-解码器架构、有监督的人类感知注意力机制、软掩码、端到端训练、多尺度去模糊信息融合"
    },
    {
        "order": 378,
        "title": "Calibration of Axial Fisheye Cameras Through Generic Virtual Central Models",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brousseau_Calibration_of_Axial_Fisheye_Cameras_Through_Generic_Virtual_Central_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brousseau_Calibration_of_Axial_Fisheye_Cameras_Through_Generic_Virtual_Central_Models_ICCV_2019_paper.html",
        "abstract": "Fisheye cameras are notoriously hard to calibrate using traditional plane-based methods. This paper proposes a new calibration method for large field of view cameras. Similarly to planar calibration, it relies on multiple images of a planar calibration grid with dense correspondences, typically obtained using structured light. By relying on the grids themselves instead of the distorted image plane, we can build a rectilinear Generic Virtual Central (GVC) camera. Instead of relying on a single GVC camera, our method proposes a selection of multiple GVC cameras which can cover any field of view and be trivially aligned to provide a very accurate generic central model. We demonstrate that this approach can directly model axial cameras, assuming the distortion center is located on the camera axis. Experimental validation is provided on both synthetic and real fisheye cameras featuring up to a 280deg field of view. To our knowledge, this is one of the only practical methods to calibrate axial cameras.",
        "中文标题": "通过通用虚拟中心模型校准轴向鱼眼相机",
        "摘要翻译": "鱼眼相机使用传统的基于平面的方法进行校正是出了名的困难。本文提出了一种新的用于大视场相机的校正方法。与平面校正类似，它依赖于具有密集对应关系的平面校正网格的多个图像，通常使用结构光获得。通过依赖网格本身而不是扭曲的图像平面，我们可以构建一个直线通用虚拟中心（GVC）相机。我们的方法不是依赖单一的GVC相机，而是提出了一种选择多个GVC相机的方法，这些相机可以覆盖任何视场，并且可以轻松对齐以提供非常准确的通用中心模型。我们证明了这种方法可以直接建模轴向相机，假设失真中心位于相机轴上。实验验证在合成和真实的鱼眼相机上进行，这些相机的视场可达280度。据我们所知，这是唯一实用的校准轴向相机的方法之一。",
        "领域": "相机校准/鱼眼相机/大视场相机",
        "问题": "鱼眼相机使用传统方法难以校准的问题",
        "动机": "为了解决鱼眼相机使用传统基于平面的方法进行校正的困难，提出一种新的校正方法",
        "方法": "提出了一种新的校正方法，通过构建直线通用虚拟中心（GVC）相机，并选择多个GVC相机覆盖任何视场，轻松对齐以提供准确的通用中心模型",
        "关键词": [
            "相机校准",
            "鱼眼相机",
            "大视场相机",
            "通用虚拟中心模型"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括鱼眼相机、大视场相机、相机校准、通用虚拟中心（GVC）模型、结构光、密集对应关系、平面校正网格等。"
    },
    {
        "order": 379,
        "title": "FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html",
        "abstract": "Beyond current image-based virtual try-on systems that have attracted increasing attention, we move a step forward to developing a video virtual try-on system that precisely transfers clothes onto the person and generates visually realistic videos conditioned on arbitrary poses. Besides the challenges in image-based virtual try-on (e.g., clothes fidelity, image synthesis), video virtual try-on further requires spatiotemporal consistency. Directly adopting existing image-based approaches often fails to generate coherent video with natural and realistic textures. In this work, we propose Flow-navigated Warping Generative Adversarial Network (FW-GAN), a novel framework that learns to synthesize the video of virtual try-on based on a person image, the desired clothes image, and a series of target poses. FW-GAN aims to synthesize the coherent and natural video while manipulating the pose and clothes. It consists of: (i) a flow-guided fusion module that warps the past frames to assist synthesis, which is also adopted in the discriminator to help enhance the coherence and quality of the synthesized video; (ii) a warping net that is designed to warp clothes image for the refinement of clothes textures; (iii) a parsing constraint loss that alleviates the problem caused by the misalignment of segmentation maps from images with different poses and various clothes. Experiments on our newly collected dataset show that FW-GAN can synthesize high-quality video of virtual try-on and significantly outperforms other methods both qualitatively and quantitatively.",
        "中文标题": "FW-GAN: 用于视频虚拟试衣的流导航变形生成对抗网络",
        "摘要翻译": "超越当前已引起越来越多关注的基于图像的虚拟试衣系统，我们进一步开发了一种视频虚拟试衣系统，该系统能够精确地将衣物转移到人物身上，并根据任意姿势生成视觉上逼真的视频。除了基于图像的虚拟试衣面临的挑战（例如，衣物保真度、图像合成）之外，视频虚拟试衣还需要时空一致性。直接采用现有的基于图像的方法往往无法生成具有自然和真实纹理的连贯视频。在这项工作中，我们提出了流导航变形生成对抗网络（FW-GAN），这是一个新颖的框架，它学习基于人物图像、所需衣物图像和一系列目标姿势来合成虚拟试衣的视频。FW-GAN旨在合成连贯且自然的视频，同时操纵姿势和衣物。它包括：（i）一个流引导的融合模块，该模块通过变形过去的帧来辅助合成，该模块也被用于判别器中，以帮助增强合成视频的连贯性和质量；（ii）一个变形网络，设计用于变形衣物图像以细化衣物纹理；（iii）一个解析约束损失，该损失减轻了由于不同姿势和各种衣物的图像分割图不对齐而引起的问题。在我们新收集的数据集上的实验表明，FW-GAN能够合成高质量的虚拟试衣视频，并且在质量和数量上显著优于其他方法。",
        "领域": "虚拟试衣/视频合成/生成对抗网络",
        "问题": "视频虚拟试衣系统中的时空一致性和衣物纹理自然性问题",
        "动机": "开发一种能够精确转移衣物并生成视觉上逼真视频的系统，超越现有的基于图像的虚拟试衣系统",
        "方法": "提出了流导航变形生成对抗网络（FW-GAN），包括流引导的融合模块、变形网络和解析约束损失，以合成连贯且自然的视频",
        "关键词": [
            "虚拟试衣",
            "视频合成",
            "生成对抗网络"
        ],
        "涉及的技术概念": "流引导的融合模块通过变形过去的帧来辅助合成，变形网络用于变形衣物图像以细化衣物纹理，解析约束损失用于减轻由于不同姿势和各种衣物的图像分割图不对齐而引起的问题。"
    },
    {
        "order": 380,
        "title": "On the Global Optima of Kernelized Adversarial Representation Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sadeghi_On_the_Global_Optima_of_Kernelized_Adversarial_Representation_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sadeghi_On_the_Global_Optima_of_Kernelized_Adversarial_Representation_Learning_ICCV_2019_paper.html",
        "abstract": "Adversarial representation learning is a promising paradigm for obtaining data representations that are invariant to certain sensitive attributes while retaining the information necessary for predicting target attributes. Existing approaches solve this problem through iterative adversarial minimax optimization and lack theoretical guarantees. In this paper, we first study the \"linear\" form of this problem i.e., the setting where all the players are linear functions. We show that the resulting optimization problem is both non-convex and non-differentiable. We obtain an exact closed-form expression for its global optima through spectral learning and provide performance guarantees in terms of analytical bounds on the achievable utility and invariance. We then extend this solution and analysis to non-linear functions through kernel representation. Numerical experiments on UCI, Extended Yale B and CIFAR-100 datasets indicate that, (a) practically, our solution is ideal for \"imparting\" provable invariance to any biased pre-trained data representation, and (b) the global optima of the \"kernel\" form can provide a comparable trade-off between utility and invariance in comparison to iterative minimax optimization of existing deep neural network based approaches, but with provable guarantees.",
        "中文标题": "关于核化对抗表示学习的全局最优解",
        "摘要翻译": "对抗表示学习是一种有前景的范式，用于获取对某些敏感属性不变的数据表示，同时保留预测目标属性所需的信息。现有方法通过迭代对抗极小极大优化解决此问题，但缺乏理论保证。在本文中，我们首先研究了此问题的“线性”形式，即所有参与者都是线性函数的情况。我们展示了由此产生的优化问题既非凸也非可微。通过谱学习，我们获得了其全局最优解的精确闭式表达式，并在可实现的效用和不变性方面提供了分析界限的性能保证。然后，我们通过核表示将此解决方案和分析扩展到非线性函数。在UCI、Extended Yale B和CIFAR-100数据集上的数值实验表明，（a）实际上，我们的解决方案非常适合“赋予”任何有偏见的预训练数据表示可证明的不变性，（b）与现有的基于深度神经网络的迭代极小极大优化方法相比，“核”形式的全局最优解可以在效用和不变性之间提供可比较的权衡，但具有可证明的保证。",
        "领域": "对抗学习/表示学习/核方法",
        "问题": "如何在保留预测目标属性所需信息的同时，获取对某些敏感属性不变的数据表示",
        "动机": "现有方法通过迭代对抗极小极大优化解决此问题，但缺乏理论保证",
        "方法": "首先研究问题的线性形式，获得全局最优解的精确闭式表达式，并通过核表示扩展到非线性函数",
        "关键词": [
            "对抗表示学习",
            "全局最优解",
            "核方法",
            "谱学习",
            "不变性"
        ],
        "涉及的技术概念": "对抗表示学习是一种旨在获取对某些敏感属性不变的数据表示，同时保留预测目标属性所需信息的范式。本文通过研究问题的线性形式，使用谱学习获得全局最优解的精确闭式表达式，并通过核表示扩展到非线性函数，提供了在效用和不变性方面的分析界限的性能保证。"
    },
    {
        "order": 381,
        "title": "Fast Video Object Segmentation via Dynamic Targeting Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Fast_Video_Object_Segmentation_via_Dynamic_Targeting_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Fast_Video_Object_Segmentation_via_Dynamic_Targeting_Network_ICCV_2019_paper.html",
        "abstract": "We propose a new model for fast and accurate video object segmentation. It consists of two convolutional neural networks, a Dynamic Targeting Network (DTN) and a Mask Refinement Network (MRN). DTN locates the object by dynamically focusing on regions of interest surrounding the target object. The target region is predicted by DTN via two sub-streams, Box Propagation (BP) and Box Re-identification (BR). The BP stream is faster but less effective at objects with large deformation or occlusion. The BR stream performs better in difficult scenarios at a higher computation cost. We propose a Decision Module (DM) to adaptively determine which sub-stream to use for each frame. Finally, MRN is exploited to predict segmentation within the target region. Experimental results on two public datasets demonstrate that the proposed model significantly outperforms existing methods without online training in both accuracy and efficiency, and is comparable to online training-based methods in accuracy with an order of magnitude faster speed.",
        "中文标题": "通过动态目标网络实现快速视频对象分割",
        "摘要翻译": "我们提出了一种新的模型，用于快速且准确的视频对象分割。该模型由两个卷积神经网络组成，分别是动态目标网络（DTN）和掩码细化网络（MRN）。DTN通过动态聚焦于目标对象周围的感兴趣区域来定位对象。目标区域由DTN通过两个子流预测，即框传播（BP）和框重新识别（BR）。BP流在处理大变形或遮挡的对象时速度更快但效果较差。BR流在困难场景下表现更好，但计算成本更高。我们提出了一个决策模块（DM）来自适应地确定每帧使用哪个子流。最后，利用MRN预测目标区域内的分割。在两个公共数据集上的实验结果表明，所提出的模型在准确性和效率上显著优于现有方法，无需在线训练，并且在准确性上与基于在线训练的方法相当，速度却快了一个数量级。",
        "领域": "视频对象分割/卷积神经网络/自适应决策",
        "问题": "快速且准确的视频对象分割",
        "动机": "提高视频对象分割的准确性和效率，特别是在处理大变形或遮挡的对象时",
        "方法": "采用动态目标网络（DTN）和掩码细化网络（MRN）进行对象定位和分割，通过决策模块（DM）自适应选择框传播（BP）或框重新识别（BR）子流",
        "关键词": [
            "视频对象分割",
            "卷积神经网络",
            "自适应决策"
        ],
        "涉及的技术概念": {
            "动态目标网络（DTN）": "用于动态聚焦于目标对象周围的感兴趣区域来定位对象",
            "掩码细化网络（MRN）": "用于预测目标区域内的分割",
            "框传播（BP）": "一种快速但处理大变形或遮挡对象效果较差的子流",
            "框重新识别（BR）": "一种在困难场景下表现更好但计算成本更高的子流",
            "决策模块（DM）": "用于自适应地确定每帧使用哪个子流"
        }
    },
    {
        "order": 382,
        "title": "Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ghosh_Interactive_Sketch__Fill_Multiclass_Sketch-to-Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ghosh_Interactive_Sketch__Fill_Multiclass_Sketch-to-Image_Translation_ICCV_2019_paper.html",
        "abstract": "We propose an interactive GAN-based sketch-to-image translation method that helps novice users easily create images of simple objects. The user starts with a sparse sketch and a desired object category, and the network then recommends its plausible completion(s) and shows a corresponding synthesized image. This enables a feedback loop, where the user can edit the sketch based on the network's recommendations, while the network is able to better synthesize the image that the user might have in mind. In order to use a single model for a wide array of object classes, we introduce a gating-based approach for class conditioning, which allows us to generate distinct classes without feature mixing, from a single generator network.",
        "中文标题": "交互式草图与填充：多类别草图到图像转换",
        "摘要翻译": "我们提出了一种基于GAN的交互式草图到图像转换方法，帮助新手用户轻松创建简单物体的图像。用户从稀疏草图和所需物体类别开始，网络随后推荐其合理的完成情况并显示相应的合成图像。这形成了一个反馈循环，用户可以根据网络的推荐编辑草图，而网络能够更好地合成用户可能想象的图像。为了使用单一模型处理广泛的物体类别，我们引入了一种基于门控的类别条件方法，这使得我们能够从单一生成器网络中生成不同的类别，而无需特征混合。",
        "领域": "图像生成/用户交互/生成对抗网络",
        "问题": "如何使新手用户能够轻松创建简单物体的图像",
        "动机": "帮助新手用户通过交互式草图到图像转换方法，轻松创建图像",
        "方法": "提出了一种基于GAN的交互式草图到图像转换方法，引入基于门控的类别条件方法",
        "关键词": [
            "图像生成",
            "用户交互",
            "生成对抗网络",
            "草图到图像转换"
        ],
        "涉及的技术概念": "GAN（生成对抗网络）用于图像生成，门控机制用于类别条件处理，以实现从单一生成器网络生成不同类别的图像。"
    },
    {
        "order": 383,
        "title": "Micro-Baseline Structured Light",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saragadam_Micro-Baseline_Structured_Light_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saragadam_Micro-Baseline_Structured_Light_ICCV_2019_paper.html",
        "abstract": "We propose Micro-baseline Structured Light (MSL), a novel 3D imaging approach designed for small form-factor devices such as cell-phones and miniature robots. MSL operates with small projector-camera baseline and low-cost projection hardware, and can recover scene depths with computationally lightweight algorithms. The main observation is that a small baseline leads to small disparities, enabling a first-order approximation of the non-linear SL image formation model. This leads to the key theoretical result of the paper: the MSL equation, a linearized version of SL image formation. MSL equation is under-constrained due to two unknowns (depth and albedo) at each pixel, but can be efficiently solved using a local least squares approach. We analyze the performance of MSL in terms of various system parameters such as projected pattern and baseline, and provide guidelines for optimizing performance. Armed with these insights, we build a prototype to experimentally examine the theory and its practicality.",
        "中文标题": "微基线结构光",
        "摘要翻译": "我们提出了微基线结构光（MSL），这是一种新颖的3D成像方法，专为手机和微型机器人等小型设备设计。MSL使用小型投影仪-相机基线和低成本投影硬件操作，并可以通过计算轻量级算法恢复场景深度。主要观察结果是，小基线导致小视差，使得非线性SL图像形成模型的一阶近似成为可能。这导致了本文的关键理论结果：MSL方程，即SL图像形成的线性化版本。由于每个像素有两个未知数（深度和反照率），MSL方程是欠约束的，但可以使用局部最小二乘法有效解决。我们分析了MSL在各种系统参数（如投影图案和基线）方面的性能，并提供了优化性能的指南。基于这些见解，我们构建了一个原型，以实验性地检验该理论及其实用性。",
        "领域": "3D成像/小型设备/结构光",
        "问题": "如何在小型设备上实现高效且低成本的3D成像",
        "动机": "为了在手机和微型机器人等小型设备上实现高效且低成本的3D成像",
        "方法": "提出微基线结构光（MSL）方法，使用小型投影仪-相机基线和低成本投影硬件，通过计算轻量级算法恢复场景深度，并利用局部最小二乘法解决MSL方程",
        "关键词": [
            "3D成像",
            "小型设备",
            "结构光"
        ],
        "涉及的技术概念": "微基线结构光（MSL）是一种3D成像技术，专为小型设备设计，通过小型投影仪-相机基线和低成本投影硬件操作，使用计算轻量级算法恢复场景深度。MSL方程是SL图像形成的线性化版本，由于每个像素有两个未知数（深度和反照率），是欠约束的，但可以通过局部最小二乘法有效解决。"
    },
    {
        "order": 384,
        "title": "Addressing Model Vulnerability to Distributional Shifts Over Image Transformation Sets",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Volpi_Addressing_Model_Vulnerability_to_Distributional_Shifts_Over_Image_Transformation_Sets_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Volpi_Addressing_Model_Vulnerability_to_Distributional_Shifts_Over_Image_Transformation_Sets_ICCV_2019_paper.html",
        "abstract": "We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts.",
        "中文标题": "解决模型对图像变换集分布变化的脆弱性问题",
        "摘要翻译": "我们关注计算机视觉模型对分布变化的脆弱性。我们提出了一个组合优化问题，该问题允许评估图像空间中给定模型在应用图像变换的输入方面更脆弱的区域，并使用标准搜索算法来面对它。我们进一步将这一想法嵌入到训练过程中，在该过程中，我们根据当前模型在迭代过程中最脆弱的图像变换定义新的数据增强规则。对分类和语义分割问题的实证评估表明，所设计的算法能够训练出对内容保持的图像操作和一般分布变化更具鲁棒性的模型。",
        "领域": "图像增强/模型鲁棒性/分布变化",
        "问题": "计算机视觉模型对图像变换集分布变化的脆弱性",
        "动机": "提高计算机视觉模型对内容保持的图像操作和分布变化的鲁棒性",
        "方法": "提出组合优化问题评估模型脆弱区域，并基于此定义新的数据增强规则进行模型训练",
        "关键词": [
            "图像变换",
            "模型脆弱性",
            "数据增强",
            "组合优化",
            "模型鲁棒性"
        ],
        "涉及的技术概念": "组合优化问题用于评估模型在特定图像变换下的脆弱区域；数据增强规则根据模型脆弱性动态调整，以提高模型对分布变化的鲁棒性。"
    },
    {
        "order": 385,
        "title": "Solving Vision Problems via Filtering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Young_Solving_Vision_Problems_via_Filtering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Young_Solving_Vision_Problems_via_Filtering_ICCV_2019_paper.html",
        "abstract": "We propose a new, filtering approach for solving a large number of regularized inverse problems commonly found in computer vision. Traditionally, such problems are solved by finding the solution to the system of equations that expresses the first-order optimality conditions of the problem. This can be slow if the system of equations is dense due to the use of nonlocal regularization, necessitating iterative solvers such as successive over-relaxation or conjugate gradients. In this paper, we show that similar solutions can be obtained more easily via filtering, obviating the need to solve a potentially dense system of equations using slow iterative methods. Our filtered solutions are very similar to the true ones, but often up to 10 times faster to compute.",
        "中文标题": "通过滤波解决视觉问题",
        "摘要翻译": "我们提出了一种新的滤波方法，用于解决计算机视觉中常见的许多正则化逆问题。传统上，这类问题通过求解表达问题一阶最优性条件的方程组来解决。如果由于使用非局部正则化而导致方程组密集，这可能会很慢，需要使用连续超松弛或共轭梯度等迭代求解器。在本文中，我们展示了通过滤波可以更容易地获得类似的解，从而避免了使用缓慢的迭代方法求解可能密集的方程组的需要。我们的滤波解与真实解非常相似，但计算速度通常快10倍。",
        "领域": "图像复原/图像去噪/图像增强",
        "问题": "解决计算机视觉中常见的正则化逆问题",
        "动机": "传统方法在解决由于非局部正则化导致的密集方程组时速度较慢，需要寻找更高效的解决方案",
        "方法": "提出了一种新的滤波方法，避免了使用缓慢的迭代方法求解可能密集的方程组",
        "关键词": [
            "滤波",
            "正则化逆问题",
            "非局部正则化"
        ],
        "涉及的技术概念": "滤波方法用于解决计算机视觉中的正则化逆问题，特别是那些由于非局部正则化导致的密集方程组问题。这种方法通过避免使用传统的迭代求解器（如连续超松弛或共轭梯度）来提高计算效率。"
    },
    {
        "order": 386,
        "title": "Attention-Based Autism Spectrum Disorder Screening With Privileged Modality",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Attention-Based_Autism_Spectrum_Disorder_Screening_With_Privileged_Modality_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Attention-Based_Autism_Spectrum_Disorder_Screening_With_Privileged_Modality_ICCV_2019_paper.html",
        "abstract": "This paper presents a novel framework for automatic and quantitative screening of autism spectrum disorder (ASD). It is motivated to address two issues in the current clinical settings: 1) short of clinical resources with the prevalence of ASD (1.7% in the United States), and 2) subjectivity of ASD screening. This work differentiates itself with three unique features: first, it proposes an ASD screening with privileged modality framework that integrates information from two behavioral modalities during training and improves the performance on each single modality at testing. The proposed framework does not require overlap in subjects between the modalities. Second, it develops the first computational model to classify people with ASD using a photo-taking task where subjects freely explore their environment in a more ecological setting. Photo-taking reveals attentional preference of subjects, differentiating people with ASD from healthy people, and is also easy to implement in real-world clinical settings without requiring advanced diagnostic instruments. Third, this study for the first time takes advantage of the temporal information in eye movements while viewing images, encoding more detailed behavioral differences between ASD people and healthy controls. Experiments show that our ASD screening models can achieve superior performance, outperforming the previous state-of-the-art methods by a considerable margin. Moreover, our framework using diverse modalities demonstrates performance improvement on both the photo-taking and image-viewing tasks, providing a general paradigm that takes in multiple sources of behavioral data for a more accurate ASD screening. The framework is also applicable to various scenarios where one-to-one pairwise relationship is difficult to obtain across different modalities.",
        "中文标题": "基于注意力的自闭症谱系障碍筛查与特权模态",
        "摘要翻译": "本文提出了一种新颖的框架，用于自闭症谱系障碍（ASD）的自动和定量筛查。该研究旨在解决当前临床环境中的两个问题：1）ASD的普遍性（在美国为1.7%）导致临床资源短缺，2）ASD筛查的主观性。这项工作通过三个独特的特点区分自己：首先，它提出了一个ASD筛查与特权模态框架，该框架在训练期间整合了来自两种行为模态的信息，并在测试时提高了每个单一模态的性能。所提出的框架不需要模态之间的受试者重叠。其次，它开发了第一个计算模型，使用拍照任务对ASD患者进行分类，其中受试者在更生态的环境中自由探索他们的环境。拍照揭示了受试者的注意力偏好，区分了ASD患者和健康人，并且在实际临床环境中易于实施，不需要先进的诊断仪器。第三，这项研究首次利用了观看图像时眼动的时间信息，编码了ASD患者和健康对照组之间更详细的行为差异。实验表明，我们的ASD筛查模型可以实现卓越的性能，显著优于之前的最先进方法。此外，我们的框架使用多种模态展示了在拍照和图像观看任务上的性能改进，提供了一个通用范式，该范式接受多种行为数据源，以实现更准确的ASD筛查。该框架也适用于各种场景，其中一对一的对关系难以在不同模态之间获得。",
        "领域": "自闭症筛查/行为分析/眼动追踪",
        "问题": "自闭症谱系障碍（ASD）的自动和定量筛查",
        "动机": "解决ASD筛查中临床资源短缺和筛查主观性的问题",
        "方法": "提出了一种ASD筛查与特权模态框架，整合两种行为模态信息，开发基于拍照任务的计算模型，利用眼动时间信息",
        "关键词": [
            "自闭症筛查",
            "行为分析",
            "眼动追踪",
            "特权模态",
            "拍照任务"
        ],
        "涉及的技术概念": "特权模态框架、行为模态整合、眼动时间信息编码、拍照任务、ASD筛查模型"
    },
    {
        "order": 387,
        "title": "l-Net: Reconstruct Hyperspectral Images From a Snapshot Measurement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Miao_l-Net_Reconstruct_Hyperspectral_Images_From_a_Snapshot_Measurement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Miao_l-Net_Reconstruct_Hyperspectral_Images_From_a_Snapshot_Measurement_ICCV_2019_paper.html",
        "abstract": "We propose the l-net, which reconstructs hyperspectral images (e.g., with 24 spectral channels) from a single shot measurement. This task is usually termed snapshot compressive-spectral imaging (SCI), which enjoys low cost, low bandwidth and high-speed sensing rate via capturing the three-dimensional (3D) signal i.e., (x, y, l), using a 2D snapshot. Though proposed more than a decade ago, the poor quality and low-speed of reconstruction algorithms preclude wide applications of SCI. To address this challenge, in this paper, we develop a dual-stage generative model to reconstruct the desired 3D signal in SCI, dubbed l-net. Results on both simulation and real datasets demonstrate the significant advantages of l-net, which leads to >4dB improvement in PSNR for real-mask-in-the-loop simulation data compared to the current state-of-the-art. Furthermore, l-net can finish the reconstruction task within sub-seconds instead of hours taken by the most recently proposed DeSCI algorithm, thus speeding up the reconstruction >1000 times.",
        "中文标题": "l-Net: 从快照测量重建高光谱图像",
        "摘要翻译": "我们提出了l-net，它可以从单次快照测量中重建高光谱图像（例如，具有24个光谱通道）。这项任务通常被称为快照压缩光谱成像（SCI），它通过使用2D快照捕获三维（3D）信号（即（x, y, l））来享受低成本、低带宽和高速传感率的优势。尽管十多年前就提出了SCI，但重建算法的质量差和速度慢阻碍了其广泛应用。为了应对这一挑战，本文中我们开发了一种双阶段生成模型来重建SCI中所需的3D信号，称为l-net。在模拟和真实数据集上的结果都展示了l-net的显著优势，与当前最先进的技术相比，在真实掩模循环模拟数据中PSNR提高了>4dB。此外，l-net可以在亚秒内完成重建任务，而不是最近提出的DeSCI算法所需的数小时，从而将重建速度提高了>1000倍。",
        "领域": "高光谱成像/压缩感知/信号重建",
        "问题": "快照压缩光谱成像（SCI）中高质量和高速重建高光谱图像的问题",
        "动机": "解决SCI技术由于重建算法质量差和速度慢而难以广泛应用的问题",
        "方法": "开发了一种双阶段生成模型l-net，用于重建SCI中的3D信号",
        "关键词": [
            "高光谱成像",
            "压缩感知",
            "信号重建",
            "双阶段生成模型"
        ],
        "涉及的技术概念": "快照压缩光谱成像（SCI）是一种通过2D快照捕获3D信号的技术，具有低成本、低带宽和高速传感率的优势。l-net是一种双阶段生成模型，旨在提高SCI中高光谱图像的重建质量和速度。PSNR（峰值信噪比）是衡量重建图像质量的一个指标。DeSCI算法是最近提出的一种用于SCI重建的算法，但其重建速度较慢。"
    },
    {
        "order": 388,
        "title": "Attract or Distract: Exploit the Margin of Open Set",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_Attract_or_Distract_Exploit_the_Margin_of_Open_Set_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Feng_Attract_or_Distract_Exploit_the_Margin_of_Open_Set_ICCV_2019_paper.html",
        "abstract": "Open set domain adaptation aims to diminish the domain shift across domains, with partially shared classes. There exist unknown target samples out of the knowledge of source domain. Compared to the close set setting, how to separate the unknown (unshared) class from the known (shared) ones plays the key role. Whereas, previous methods did not emphasize the semantic structure of the open set data, which may introduce bias into the domain alignment and confuse the classifier around the decision boundary. In this paper, we exploit the semantic structure of open set data from two aspects: 1) Semantic Categorical Alignment, which aims to achieve good separability of target known classes by categorically aligning the centroid of target with the source. 2) Semantic Contrastive Mapping, which aims to push the unknown class away from the decision boundary. Empirically, we demonstrate that our method performs favourably against the state-of-the-art methods on representative benchmarks, e.g. Digits and Office-31 datasets.",
        "中文标题": "吸引或分散：利用开放集的边界",
        "摘要翻译": "开放集领域适应旨在减少跨领域的领域转移，这些领域部分共享类别。存在一些目标样本，它们超出了源领域的知识范围。与封闭集设置相比，如何将未知（未共享）类别与已知（共享）类别分离起着关键作用。然而，之前的方法并未强调开放集数据的语义结构，这可能会在领域对齐中引入偏差，并在决策边界周围混淆分类器。在本文中，我们从两个方面利用开放集数据的语义结构：1）语义类别对齐，旨在通过将目标的质心与源类别对齐，实现目标已知类别的良好可分性。2）语义对比映射，旨在将未知类别推离决策边界。经验上，我们证明了我们的方法在代表性基准测试上，如Digits和Office-31数据集，表现优于最先进的方法。",
        "领域": "领域适应/开放集识别/语义结构分析",
        "问题": "如何在开放集领域适应中有效分离未知类别与已知类别",
        "动机": "减少跨领域的领域转移，提高开放集数据的分类准确性",
        "方法": "通过语义类别对齐和语义对比映射利用开放集数据的语义结构",
        "关键词": [
            "领域适应",
            "开放集识别",
            "语义结构分析"
        ],
        "涉及的技术概念": "开放集领域适应、语义类别对齐、语义对比映射、决策边界、领域转移"
    },
    {
        "order": 389,
        "title": "GAN-Based Projector for Faster Recovery With Convergence Guarantees in Linear Inverse Problems",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Raj_GAN-Based_Projector_for_Faster_Recovery_With_Convergence_Guarantees_in_Linear_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Raj_GAN-Based_Projector_for_Faster_Recovery_With_Convergence_Guarantees_in_Linear_ICCV_2019_paper.html",
        "abstract": "A Generative Adversarial Network (GAN) with generator G trained to model the prior of images has been shown to perform better than sparsity-based regularizers in ill-posed inverse problems. Here, we propose a new method of deploying a GAN-based prior to solve linear inverse problems using projected gradient descent (PGD). Our method learns a network-based projector for use in the PGD algorithm, eliminating expensive computation of the Jacobian of G. Experiments show that our approach provides a speed-up of 60-80x over earlier GAN-based recovery methods along with better accuracy in compressed sensing. Our main theoretical result is that if the measurement matrix is moderately conditioned on the manifold range(G) and the projector is d-approximate, then the algorithm is guaranteed to reach O(d) reconstruction error in O(log(1/d)) steps in the low noise regime. Additionally, we propose a fast method to design such measurement matrices for a given G. Extensive experiments demonstrate the efficacy of this method by requiring 5-10x fewer measurements than random Gaussian measurement matrices for comparable recovery performance. Because the learning of the GAN and projector is decoupled from the measurement operator, our GAN-based projector and recovery algorithm are applicable without retraining to all linear inverse problems in which the measurement operator is moderately conditioned for range(G), as confirmed by experiments on compressed sensing, super-resolution, and inpainting.",
        "中文标题": "基于GAN的投影仪用于线性逆问题的更快恢复与收敛保证",
        "摘要翻译": "一个生成对抗网络（GAN），其生成器G被训练来建模图像的先验，已被证明在不适定逆问题中比基于稀疏性的正则化器表现更好。在这里，我们提出了一种新方法，利用基于GAN的先验通过投影梯度下降（PGD）来解决线性逆问题。我们的方法学习了一个基于网络的投影仪用于PGD算法中，消除了G的雅可比矩阵的昂贵计算。实验表明，与早期的基于GAN的恢复方法相比，我们的方法提供了60-80倍的速度提升，并在压缩感知中提供了更好的准确性。我们的主要理论结果是，如果测量矩阵在流形范围(G)上适度条件化且投影仪是d-近似的，则该算法在低噪声情况下保证在O(log(1/d))步内达到O(d)重建误差。此外，我们提出了一种快速方法来为给定的G设计这样的测量矩阵。大量实验证明了这种方法的有效性，因为它需要比随机高斯测量矩阵少5-10倍的测量次数即可获得可比的恢复性能。由于GAN和投影仪的学习与测量操作符解耦，我们的基于GAN的投影仪和恢复算法适用于所有线性逆问题，在这些问题中测量操作符对范围(G)适度条件化，正如在压缩感知、超分辨率和修复上的实验所证实的那样。",
        "领域": "压缩感知/超分辨率/图像修复",
        "问题": "解决线性逆问题中的恢复速度和准确性问题",
        "动机": "提高基于GAN的恢复方法的速度和准确性，同时减少计算成本",
        "方法": "提出了一种新的基于GAN的先验方法，通过投影梯度下降（PGD）算法解决线性逆问题，并学习一个基于网络的投影仪以减少计算成本",
        "关键词": [
            "生成对抗网络",
            "投影梯度下降",
            "压缩感知",
            "超分辨率",
            "图像修复"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）用于建模图像的先验，投影梯度下降（PGD）算法用于解决线性逆问题，通过减少计算雅可比矩阵的成本来提高恢复速度和准确性。"
    },
    {
        "order": 390,
        "title": "Deep Depth From Aberration Map",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kashiwagi_Deep_Depth_From_Aberration_Map_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kashiwagi_Deep_Depth_From_Aberration_Map_ICCV_2019_paper.html",
        "abstract": "Passive and convenient depth estimation from single-shot image is still an open problem. Existing depth from defocus methods require multiple input images or special hardware customization. Recent deep monocular depth estimation is also limited to an image with sufficient contextual information. In this work, we propose a novel method which realizes a single-shot deep depth measurement based on physical depth cue using only an off-the-shelf camera and lens. When a defocused image is taken by a camera, it contains various types of aberrations corresponding to distances from the image sensor and positions in the image plane. We call these minute and complexly compound aberrations as Aberration Map (A-Map) and we found that A-Map can be utilized as reliable physical depth cue. Additionally, our deep network named A-Map Analysis Network (AMA-Net) is also proposed, which can effectively learn and estimate depth via A-Map. To evaluate validity and robustness of our approach, we have conducted extensive experiments using both real outdoor scenes and simulated images. The qualitative result shows the accuracy and availability of the method in comparison with a state-of-the-art deep context-based method.",
        "中文标题": "从像差图中获取深度",
        "摘要翻译": "从单张图像进行被动且便捷的深度估计仍然是一个未解决的问题。现有的基于散焦的深度估计方法需要多张输入图像或特殊的硬件定制。最近的深度单目深度估计也仅限于具有足够上下文信息的图像。在这项工作中，我们提出了一种新颖的方法，该方法仅使用现成的相机和镜头，基于物理深度线索实现单次拍摄的深度测量。当相机拍摄散焦图像时，它包含与图像传感器距离和图像平面位置相对应的各种类型的像差。我们将这些微小且复杂复合的像差称为像差图（A-Map），并发现A-Map可以作为可靠的物理深度线索。此外，我们还提出了名为A-Map分析网络（AMA-Net）的深度网络，它可以通过A-Map有效地学习和估计深度。为了评估我们方法的有效性和鲁棒性，我们使用真实户外场景和模拟图像进行了广泛的实验。定性结果显示，与最先进的基于上下文的深度方法相比，该方法的准确性和可用性。",
        "领域": "深度估计/像差分析/单目视觉",
        "问题": "从单张图像进行深度估计",
        "动机": "解决现有方法需要多张输入图像或特殊硬件定制的问题，以及深度单目深度估计仅限于具有足够上下文信息的图像的限制",
        "方法": "提出了一种基于物理深度线索的单次拍摄深度测量方法，利用像差图（A-Map）作为深度线索，并开发了A-Map分析网络（AMA-Net）来学习和估计深度",
        "关键词": [
            "深度估计",
            "像差图",
            "单目视觉"
        ],
        "涉及的技术概念": "像差图（A-Map）是指当相机拍摄散焦图像时，图像中包含的与图像传感器距离和图像平面位置相对应的各种类型的像差。A-Map分析网络（AMA-Net）是一种深度网络，能够通过A-Map有效地学习和估计深度。"
    },
    {
        "order": 391,
        "title": "Image Aesthetic Assessment Based on Pairwise Comparison  A Unified Approach to Score Regression, Binary Classification, and Personalization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Image_Aesthetic_Assessment_Based_on_Pairwise_Comparison__A_Unified_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Image_Aesthetic_Assessment_Based_on_Pairwise_Comparison__A_Unified_ICCV_2019_paper.html",
        "abstract": "We propose a unified approach to three tasks of aesthetic score regression, binary aesthetic classification, and personalized aesthetics. First, we develop a comparator to estimate the ratio of aesthetic scores for two images. Then, we construct a pairwise comparison matrix for multiple reference images and an input image, and predict the aesthetic score of the input via the eigenvalue decomposition of the matrix. By varying the reference images, the proposed algorithm can be used for binary aesthetic classification and personalized aesthetics, as well as generic score regression. Experimental results demonstrate that the proposed unified algorithm provides the state-of-the-art performances in all three tasks of image aesthetics.",
        "中文标题": "基于成对比较的图像美学评估：一种统一的分数回归、二元分类和个性化方法",
        "摘要翻译": "我们提出了一种统一的方法来处理美学分数回归、二元美学分类和个性化美学三个任务。首先，我们开发了一个比较器来估计两幅图像的美学分数比率。然后，我们为多个参考图像和输入图像构建了一个成对比较矩阵，并通过矩阵的特征值分解来预测输入图像的美学分数。通过改变参考图像，所提出的算法可以用于二元美学分类和个性化美学，以及通用的分数回归。实验结果表明，所提出的统一算法在图像美学的所有三个任务中都提供了最先进的性能。",
        "领域": "图像美学评估/个性化推荐/分数回归",
        "问题": "解决图像美学评估中的分数回归、二元分类和个性化问题",
        "动机": "为了提供一个统一的框架来处理图像美学评估中的多个任务，包括美学分数回归、二元美学分类和个性化美学，以提高评估的准确性和灵活性。",
        "方法": "开发了一个比较器来估计两幅图像的美学分数比率，构建了一个成对比较矩阵，并通过特征值分解来预测输入图像的美学分数。通过改变参考图像，该方法适用于二元美学分类和个性化美学，以及通用的分数回归。",
        "关键词": [
            "图像美学评估",
            "成对比较",
            "特征值分解",
            "个性化美学",
            "分数回归"
        ],
        "涉及的技术概念": "比较器用于估计两幅图像的美学分数比率，成对比较矩阵用于构建多个参考图像与输入图像之间的关系，特征值分解用于从矩阵中提取美学分数，这些技术概念共同构成了一个统一的图像美学评估框架。"
    },
    {
        "order": 392,
        "title": "Scoot: A Perceptual Metric for Facial Sketches",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Scoot_A_Perceptual_Metric_for_Facial_Sketches_ICCV_2019_paper.html",
        "abstract": "While it is trivial for humans to quickly assess the perceptual similarity between two images, the underlying mechanism are thought to be quite complex. Despite this, the most widely adopted perceptual metrics today, such as SSIM and FSIM, are simple, shallow functions, and fail to consider many factors of human perception. Recently, the facial modeling community has observed that the inclusion of both structure and texture has a significant positive benefit for face sketch synthesis (FSS). But how perceptual are these so-called \"perceptual features\"? Which elements are critical for their success? In this paper, we design a perceptual metric, called Structure Co-Occurrence Texture (Scoot), which simultaneously considers the block-level spatial structure and co-occurrence texture statistics. To test the quality of metrics, we propose three novel meta-measures based on various reliable properties. Extensive experiments verify that our Scoot metric exceeds the performance of prior work. Besides, we built the first largest scale (152k judgments) human-perception-based sketch database that can evaluate how well a metric consistent with human perception. Our results suggest that \"spatial structure\" and \"co-occurrence texture\" are two generally applicable perceptual features in face sketch synthesis.",
        "中文标题": "Scoot: 面部素描的感知度量",
        "摘要翻译": "虽然人类可以轻松快速地评估两幅图像之间的感知相似性，但其背后的机制被认为相当复杂。尽管如此，目前最广泛采用的感知度量，如SSIM和FSIM，都是简单、浅显的函数，未能考虑人类感知的许多因素。最近，面部建模社区观察到，结构和纹理的包含对面部素描合成（FSS）有显著的积极影响。但这些所谓的“感知特征”有多感知？哪些元素对它们的成功至关重要？在本文中，我们设计了一种感知度量，称为结构共现纹理（Scoot），它同时考虑了块级空间结构和共现纹理统计。为了测试度量的质量，我们提出了三种基于各种可靠属性的新颖元度量。大量实验验证了我们的Scoot度量超越了之前的工作。此外，我们建立了第一个最大规模（152k判断）的基于人类感知的素描数据库，可以评估度量与人类感知的一致性。我们的结果表明，“空间结构”和“共现纹理”是面部素描合成中两个普遍适用的感知特征。",
        "领域": "面部素描合成/感知度量/图像质量评估",
        "问题": "如何准确评估面部素描的感知相似性",
        "动机": "现有的感知度量未能充分考虑人类感知的复杂性，特别是在面部素描合成领域",
        "方法": "设计了一种新的感知度量Scoot，同时考虑块级空间结构和共现纹理统计，并通过大量实验和建立大规模人类感知素描数据库来验证其有效性",
        "关键词": [
            "面部素描合成",
            "感知度量",
            "空间结构",
            "共现纹理"
        ],
        "涉及的技术概念": {
            "SSIM": "结构相似性指数，用于衡量两幅图像之间的相似性",
            "FSIM": "特征相似性指数，用于图像质量评估",
            "FSS": "面部素描合成，指通过算法生成面部素描图像的过程",
            "Scoot": "本文提出的感知度量，同时考虑空间结构和共现纹理统计"
        }
    },
    {
        "order": 393,
        "title": "MIC: Mining Interclass Characteristics for Improved Metric Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Roth_MIC_Mining_Interclass_Characteristics_for_Improved_Metric_Learning_ICCV_2019_paper.html",
        "abstract": "Metric learning seeks to embed images of objects such that class-defined relations are captured by the embedding space. However, variability in images is not just due to different depicted object classes, but also depends on other latent characteristics such as viewpoint or illumination. In addition to these structured properties, random noise further obstructs the visual relations of interest. The common approach to metric learning is to enforce a representation that is invariant under all factors but the ones of interest. In contrast, we propose to explicitly learn the latent characteristics that are shared by and go across object classes. We can then directly explain away structured visual variability, rather than assuming it to be unknown random noise. We propose a novel surrogate task to learn visual characteristics shared across classes with a separate encoder. This encoder is trained jointly with the encoder for class information by reducing their mutual information. On five standard image retrieval benchmarks the approach significantly improves upon the state-of-the-art. Code is available at https://github.com/Confusezius/metric-learning-mining-interclass-characteristics.",
        "中文标题": "MIC：挖掘类间特征以改进度量学习",
        "摘要翻译": "度量学习旨在嵌入对象的图像，使得嵌入空间能够捕捉到由类别定义的关系。然而，图像的变异性不仅源于不同的对象类别，还依赖于其他潜在特征，如视角或光照。除了这些结构化属性外，随机噪声进一步阻碍了感兴趣的视觉关系。度量学习的常见方法是强制表示在所有因素下不变，除了感兴趣的因素。相比之下，我们提出明确学习那些由对象类别共享并跨越对象类别的潜在特征。然后，我们可以直接解释结构化视觉变异性，而不是假设其为未知的随机噪声。我们提出了一种新颖的替代任务，通过一个单独的编码器学习跨类共享的视觉特征。该编码器通过减少与类别信息编码器的互信息来联合训练。在五个标准的图像检索基准上，该方法显著优于现有技术。代码可在https://github.com/Confusezius/metric-learning-mining-interclass-characteristics获取。",
        "领域": "图像检索/度量学习/特征学习",
        "问题": "如何有效捕捉和利用跨对象类别的潜在特征以改进度量学习",
        "动机": "现有的度量学习方法通常假设所有非目标因素为随机噪声，忽略了图像中存在的结构化变异性，这限制了度量学习的性能。",
        "方法": "提出了一种新颖的替代任务，通过一个单独的编码器学习跨类共享的视觉特征，并通过减少与类别信息编码器的互信息来联合训练。",
        "关键词": [
            "度量学习",
            "图像检索",
            "特征学习",
            "跨类特征",
            "结构化变异性"
        ],
        "涉及的技术概念": "度量学习是一种旨在通过嵌入空间捕捉对象类别关系的方法。本文提出的方法通过一个单独的编码器学习跨类共享的视觉特征，并通过减少与类别信息编码器的互信息来联合训练，从而直接解释结构化视觉变异性，而不是假设其为未知的随机噪声。"
    },
    {
        "order": 394,
        "title": "Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Delving_Into_Robust_Object_Detection_From_Unmanned_Aerial_Vehicles_A_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Delving_Into_Robust_Object_Detection_From_Unmanned_Aerial_Vehicles_A_ICCV_2019_paper.html",
        "abstract": "Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is becoming increasingly useful. Despite the great success of the generic object detection methods trained on ground-to-ground images, a huge performance drop is observed when they are directly applied to images captured by UAVs. The unsatisfactory performance is owing to many UAV-specific nuisances, such as varying flying altitudes, adverse weather conditions, dynamically changing viewing angles, etc. Those nuisances constitute a large number of fine-grained domains, across which the detection model has to stay robust. Fortunately, UAVs will record meta-data that depict those varying attributes, which are either freely available along with the UAV images, or can be easily obtained. We propose to utilize those free meta-data in conjunction with associated UAV images to learn domain-robust features via an adversarial training framework dubbed Nuisance Disentangled Feature Transform (NDFT), for the specific challenging problem of object detection in UAV images, achieving a substantial gain in robustness to those nuisances. We demonstrate the effectiveness of our proposed algorithm, by showing state-of-the- art performance (single model) on two existing UAV-based object detection benchmarks. The code is available at https://github.com/TAMU-VITA/UAV-NDFT.",
        "中文标题": "深入探讨无人机稳健目标检测：一种深度干扰解耦方法",
        "摘要翻译": "从无人机（UAV）捕获的图像中进行目标检测变得越来越有用。尽管在地面到地面图像上训练的通用目标检测方法取得了巨大成功，但当它们直接应用于无人机捕获的图像时，观察到性能大幅下降。这种不令人满意的性能归因于许多无人机特有的干扰，如变化的飞行高度、恶劣的天气条件、动态变化的视角等。这些干扰构成了大量细粒度的领域，检测模型必须在这些领域中保持稳健。幸运的是，无人机会记录描述这些变化属性的元数据，这些元数据要么与无人机图像一起免费提供，要么可以轻松获得。我们提出利用这些免费的元数据与相关的无人机图像一起，通过一种称为干扰解耦特征变换（NDFT）的对抗训练框架来学习领域稳健的特征，针对无人机图像中的目标检测这一特定挑战性问题，实现了对这些干扰的显著稳健性提升。我们通过展示在两个现有的基于无人机的目标检测基准上的最先进性能（单一模型）来证明我们提出的算法的有效性。代码可在https://github.com/TAMU-VITA/UAV-NDFT获取。",
        "领域": "无人机视觉/目标检测/对抗学习",
        "问题": "无人机捕获图像中的目标检测性能下降",
        "动机": "提高无人机图像中目标检测的稳健性，以应对飞行高度、天气条件和视角变化等干扰",
        "方法": "利用无人机记录的元数据与图像结合，通过干扰解耦特征变换（NDFT）的对抗训练框架学习领域稳健的特征",
        "关键词": [
            "无人机视觉",
            "目标检测",
            "对抗学习"
        ],
        "涉及的技术概念": "干扰解耦特征变换（NDFT）是一种对抗训练框架，旨在通过利用无人机记录的元数据与图像结合，学习能够在不同干扰条件下保持稳健的目标检测特征。"
    },
    {
        "order": 395,
        "title": "A Dataset of Multi-Illumination Images in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Murmann_A_Dataset_of_Multi-Illumination_Images_in_the_Wild_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Murmann_A_Dataset_of_Multi-Illumination_Images_in_the_Wild_ICCV_2019_paper.html",
        "abstract": "Collections of images under a single, uncontrolled illumination have enabled the rapid advancement of core computer vision tasks like classification, detection, and segmentation. But even with modern learning techniques, many inverse problems involving lighting and material understanding remain too severely ill-posed to be solved with single-illumination datasets. The data simply does not contain the necessary supervisory signals. Multi-illumination datasets are notoriously hard to capture, so the data is typically collected at small scale, in controlled environments, either using multiple light sources, or robotic gantries. This leads to image collections that are not representative of the variety and complexity of real world scenes. We introduce a new multi-illumination dataset of more than 1000 real scenes, each captured in high dynamic range and high resolution, under 25 lighting conditions. We demonstrate the richness of this dataset by training state-of-the-art models for three challenging applications: single-image illumination estimation, image relighting, and mixed-illuminant white balance.",
        "中文标题": "野外多光照图像数据集",
        "摘要翻译": "在单一、不受控制的光照条件下收集的图像集合已经推动了核心计算机视觉任务（如分类、检测和分割）的快速发展。但即使使用现代学习技术，许多涉及光照和材质理解的逆问题仍然过于严重不适定，无法通过单一光照数据集解决。这些数据根本不包含必要的监督信号。多光照数据集因其难以捕捉而闻名，因此数据通常是在受控环境中小规模收集的，要么使用多个光源，要么使用机器人龙门架。这导致图像集合无法代表现实世界场景的多样性和复杂性。我们引入了一个新的多光照数据集，包含超过1000个真实场景，每个场景在25种光照条件下以高动态范围和高分辨率捕捉。我们通过训练最先进的模型来展示这个数据集的丰富性，应用于三个具有挑战性的应用：单图像光照估计、图像重照明和混合光源白平衡。",
        "领域": "光照估计/图像重照明/白平衡",
        "问题": "解决单一光照数据集在处理涉及光照和材质理解的逆问题时的不足",
        "动机": "现有数据集无法代表现实世界场景的多样性和复杂性，限制了计算机视觉任务的发展",
        "方法": "引入一个新的多光照数据集，包含超过1000个真实场景，每个场景在25种光照条件下以高动态范围和高分辨率捕捉，并训练最先进的模型应用于单图像光照估计、图像重照明和混合光源白平衡",
        "关键词": [
            "多光照数据集",
            "光照估计",
            "图像重照明",
            "白平衡"
        ],
        "涉及的技术概念": "高动态范围捕捉、高分辨率图像、单图像光照估计、图像重照明、混合光源白平衡"
    },
    {
        "order": 396,
        "title": "Monocular Neural Image Based Rendering With Continuous View Control",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Monocular_Neural_Image_Based_Rendering_With_Continuous_View_Control_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Monocular_Neural_Image_Based_Rendering_With_Continuous_View_Control_ICCV_2019_paper.html",
        "abstract": "We propose a method to produce a continuous stream of novel views under fine-grained (e.g., 1 degree step-size) camera control at interactive rates. A novel learning pipeline determines the output pixels directly from the source color. Injecting geometric transformations, including perspective projection, 3D rotation and translation into the network forces implicit reasoning about the underlying geometry. The latent 3D geometry representation is compact and meaningful under 3D transformation, being able to produce geometrically accurate views for both single objects and natural scenes. Our experiments show that both proposed components, the transforming encoder-decoder and depth-guided appearance mapping, lead to significantly improved generalization beyond the training views and in consequence to more accurate view synthesis under continuous 6-DoF camera control. Finally, we show that our method outperforms state-of-the-art baseline methods on public datasets.",
        "中文标题": "基于单目神经图像的连续视角控制渲染",
        "摘要翻译": "我们提出了一种方法，在细粒度（例如，1度步长）相机控制下以交互速率生成连续的新视角流。一种新颖的学习管道直接从源颜色确定输出像素。将几何变换（包括透视投影、3D旋转和平移）注入网络，强制对底层几何进行隐式推理。潜在的3D几何表示在3D变换下紧凑且有意义，能够为单个物体和自然场景生成几何上准确的视图。我们的实验表明，所提出的两个组件，即变换编码器-解码器和深度引导的外观映射，显著提高了训练视图之外的泛化能力，从而在连续的6自由度相机控制下实现了更准确的视图合成。最后，我们展示了我们的方法在公共数据集上优于最先进的基线方法。",
        "领域": "3D视觉/视图合成/几何学习",
        "问题": "如何在细粒度相机控制下生成连续且几何准确的新视角",
        "动机": "为了在交互速率下实现细粒度的相机控制，生成连续且几何准确的新视角，需要一种能够直接从源颜色确定输出像素并强制对底层几何进行隐式推理的方法。",
        "方法": "提出了一种新颖的学习管道，通过将几何变换（包括透视投影、3D旋转和平移）注入网络，直接从源颜色确定输出像素，并使用变换编码器-解码器和深度引导的外观映射来提高泛化能力和视图合成的准确性。",
        "关键词": [
            "3D视觉",
            "视图合成",
            "几何学习",
            "连续视角控制",
            "深度引导"
        ],
        "涉及的技术概念": "透视投影、3D旋转和平移是几何变换的三种形式，用于在3D空间中对物体进行变换。变换编码器-解码器是一种网络结构，用于处理输入数据并生成输出数据，同时保持数据的几何特性。深度引导的外观映射是一种技术，用于根据深度信息调整外观，以提高视图合成的准确性。"
    },
    {
        "order": 397,
        "title": "Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rakin_Bit-Flip_Attack_Crushing_Neural_Network_With_Progressive_Bit_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rakin_Bit-Flip_Attack_Crushing_Neural_Network_With_Progressive_Bit_Search_ICCV_2019_paper.html",
        "abstract": "Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%. Code is released at: https://github.com/elliothe/Neural_Network_Weight_Attack",
        "中文标题": "位翻转攻击：通过渐进位搜索摧毁神经网络",
        "摘要翻译": "最近，深度神经网络（DNN）在不同应用和组件中的几个重要安全问题被提出。DNN最广泛研究的安全问题来自其恶意输入，即对抗样本。然而，DNN参数的安全挑战尚未得到充分探索。在这项工作中，我们首次提出了一种新颖的DNN权重攻击方法，称为位翻转攻击（BFA），该方法可以通过恶意翻转其权重存储内存系统（即DRAM）中的极少量位来摧毁神经网络。位翻转操作可以通过众所周知的Row-Hammer攻击进行，而我们的主要贡献是开发一种算法来识别DNN权重参数（以二进制位存储在内存中）中最脆弱的位，这些位可以在最少的位翻转次数下最大化准确率下降。我们提出的BFA利用了一种渐进位搜索（PBS）方法，该方法结合了梯度排名和渐进搜索来识别最脆弱的位进行翻转。借助PBS，我们仅通过9300万位中的13次位翻转就成功攻击了ResNet-18，使其完全失效（即top-1准确率从69.8%降至0.1%），而随机翻转100位仅使准确率下降不到1%。代码发布于：https://github.com/elliothe/Neural_Network_Weight_Attack",
        "领域": "网络安全/深度学习安全/硬件安全",
        "问题": "深度神经网络参数的安全性问题",
        "动机": "探索并解决深度神经网络参数面临的安全挑战，特别是通过位翻转攻击来摧毁神经网络",
        "方法": "提出了一种新颖的位翻转攻击（BFA）方法，利用渐进位搜索（PBS）结合梯度排名和渐进搜索来识别并翻转DNN权重参数中最脆弱的位",
        "关键词": [
            "位翻转攻击",
            "渐进位搜索",
            "深度神经网络安全",
            "Row-Hammer攻击",
            "权重参数攻击"
        ],
        "涉及的技术概念": {
            "位翻转攻击（BFA）": "一种通过翻转神经网络权重存储系统中的特定位来攻击神经网络的方法",
            "渐进位搜索（PBS）": "一种结合梯度排名和渐进搜索的算法，用于识别DNN权重参数中最脆弱的位",
            "Row-Hammer攻击": "一种通过快速重复访问DRAM中的行来引起邻近行位翻转的攻击技术",
            "深度神经网络（DNN）": "一种模拟人脑神经网络结构和功能的计算模型，用于处理复杂的数据和任务"
        }
    },
    {
        "order": 398,
        "title": "Self-Supervised Representation Learning via Neighborhood-Relational Encoding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sabokrou_Self-Supervised_Representation_Learning_via_Neighborhood-Relational_Encoding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sabokrou_Self-Supervised_Representation_Learning_via_Neighborhood-Relational_Encoding_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhood-relational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classification, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results confirm the performance of our method is better or at least comparable with the state-of-the-art for each specific application, but with a generic and self-supervised approach.",
        "中文标题": "通过邻域关系编码的自监督表示学习",
        "摘要翻译": "在本文中，我们提出了一种新颖的自监督表示学习方法，该方法利用了训练数据中的邻域关系编码（NRE）。传统的无监督学习方法仅专注于训练深度网络以理解视觉数据的基本特征，主要是能够从潜在空间重建数据。它们往往忽略了样本之间的关系，而这些关系可以作为自监督的重要指标。与之前的工作不同，NRE旨在保留数据流形上的局部邻域结构。因此，它对异常值不太敏感。我们将NRE组件与编码器-解码器结构集成，以学习考虑其局部邻域信息的样本表示。这种判别性和无监督的表示学习方案由于其不依赖于密集的注释要求，因此适用于不同的计算机视觉任务。我们评估了我们提出的方法在不同任务中的表现，包括基于学习到的潜在表示的分类、检测和分割。此外，我们采用我们提出的方法的自编码能力，用于防御对抗性示例攻击和视频异常检测等应用。结果证实，我们的方法在每种特定应用中的性能优于或至少与最先进的方法相当，但采用了一种通用且自监督的方法。",
        "领域": "自监督学习/表示学习/邻域关系编码",
        "问题": "如何在无监督的情况下学习到能够反映样本间关系的表示",
        "动机": "传统无监督学习方法忽略了样本间的关系，而这些关系对于自监督学习是重要的",
        "方法": "提出了一种新颖的自监督表示学习方法，通过邻域关系编码（NRE）保留数据流形上的局部邻域结构，并与编码器-解码器结构集成",
        "关键词": [
            "自监督学习",
            "表示学习",
            "邻域关系编码",
            "编码器-解码器结构",
            "对抗性示例防御",
            "视频异常检测"
        ],
        "涉及的技术概念": "邻域关系编码（NRE）是一种旨在保留数据流形上局部邻域结构的方法，对异常值不敏感。编码器-解码器结构用于学习考虑局部邻域信息的样本表示。自监督表示学习不依赖于密集的注释要求，适用于多种计算机视觉任务。"
    },
    {
        "order": 399,
        "title": "Learning Filter Basis for Convolutional Neural Network Compression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Learning_Filter_Basis_for_Convolutional_Neural_Network_Compression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Learning_Filter_Basis_for_Convolutional_Neural_Network_Compression_ICCV_2019_paper.html",
        "abstract": "Convolutional neural networks (CNNs) based solutions have achieved state-of-the-art performances for many computer vision tasks, including classification and super-resolution of images. Usually the success of these methods comes with a cost of millions of parameters due to stacking deep convolutional layers. Moreover, quite a large number of filters are also used for a single convolutional layer, which exaggerates the parameter burden of current methods. Thus, in this paper, we try to reduce the number of parameters of CNNs by learning a basis of the filters in convolutional layers. For the forward pass, the learned basis is used to approximate the original filters and then used as parameters for the convolutional layers. We validate our proposed solution for multiple CNN architectures on image classification and image super-resolution benchmarks and compare favorably to the existing state-of-the-art in terms of reduction of parameters and preservation of accuracy. Code is available at https://github.com/ofsoundof/learning_filter_basis",
        "中文标题": "学习卷积神经网络压缩的滤波器基础",
        "摘要翻译": "基于卷积神经网络（CNNs）的解决方案在许多计算机视觉任务中取得了最先进的性能，包括图像分类和超分辨率。通常，这些方法的成功伴随着数百万参数的成本，这是由于堆叠了深层的卷积层。此外，单个卷积层也使用了相当多的滤波器，这加剧了当前方法的参数负担。因此，在本文中，我们尝试通过学习卷积层中滤波器的基础来减少CNNs的参数数量。在前向传播过程中，学习到的基础用于近似原始滤波器，然后用作卷积层的参数。我们在图像分类和图像超分辨率基准上验证了我们提出的解决方案，并在参数减少和精度保持方面与现有的最先进方法进行了比较。代码可在https://github.com/ofsoundof/learning_filter_basis获取。",
        "领域": "神经网络压缩/图像分类/图像超分辨率",
        "问题": "减少卷积神经网络的参数数量",
        "动机": "当前卷积神经网络方法由于使用深层卷积层和大量滤波器，导致参数数量庞大，增加了计算负担。",
        "方法": "通过学习卷积层中滤波器的基础来近似原始滤波器，从而减少参数数量。",
        "关键词": [
            "神经网络压缩",
            "图像分类",
            "图像超分辨率"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）、滤波器基础学习、参数减少、图像分类、图像超分辨率"
    },
    {
        "order": 400,
        "title": "Multi-View Image Fusion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Trinidad_Multi-View_Image_Fusion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Trinidad_Multi-View_Image_Fusion_ICCV_2019_paper.html",
        "abstract": "We present an end-to-end learned system for fusing multiple misaligned photographs of the same scene into a chosen target view. We demonstrate three use cases: 1) color transfer for inferring color for a monochrome view, 2) HDR fusion for merging misaligned bracketed exposures, and 3) detail transfer for reprojecting a high definition image to the point of view of an affordable VR180-camera. While the system can be trained end-to-end, it consists of three distinct steps: feature extraction, image warping and fusion. We present a novel cascaded feature extraction method that enables us to synergetically learn optical flow at different resolution levels. We show that this significantly improves the network's ability to learn large disparities. Finally, we demonstrate that our alignment architecture outperforms a state-of-the art optical flow network on the image warping task when both systems are trained in an identical manner.",
        "中文标题": "多视图图像融合",
        "摘要翻译": "我们提出了一个端到端学习系统，用于将同一场景的多张未对齐照片融合到选定的目标视图中。我们展示了三个用例：1）用于推断单色视图颜色的颜色转移，2）用于合并未对齐的包围曝光的HDR融合，以及3）用于将高清晰度图像重新投影到经济型VR180相机视角的细节转移。虽然该系统可以端到端训练，但它由三个不同的步骤组成：特征提取、图像扭曲和融合。我们提出了一种新颖的级联特征提取方法，使我们能够在不同分辨率级别上协同学习光流。我们表明，这显著提高了网络学习大差异的能力。最后，我们证明了我们的对齐架构在图像扭曲任务上优于最先进的光流网络，当两个系统以相同的方式训练时。",
        "领域": "图像融合/光流估计/虚拟现实",
        "问题": "如何有效地将多张未对齐的同一场景照片融合到选定的目标视图中",
        "动机": "提高图像融合的质量和效率，特别是在处理大差异和未对齐图像时",
        "方法": "采用端到端学习系统，包括特征提取、图像扭曲和融合三个步骤，并提出了一种新颖的级联特征提取方法",
        "关键词": [
            "图像融合",
            "光流估计",
            "虚拟现实",
            "颜色转移",
            "HDR融合",
            "细节转移"
        ],
        "涉及的技术概念": "端到端学习系统、特征提取、图像扭曲、融合、级联特征提取方法、光流、大差异学习、对齐架构"
    },
    {
        "order": 401,
        "title": "Employing Deep Part-Object Relationships for Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Employing_Deep_Part-Object_Relationships_for_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Employing_Deep_Part-Object_Relationships_for_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Despite Convolutional Neural Networks (CNNs) based methods have been successful in detecting salient objects, their underlying mechanism that decides the salient intensity of each image part separately cannot avoid inconsistency of parts within the same salient object. This would ultimately result in an incomplete shape of the detected salient object. To solve this problem, we dig into part-object relationships and take the unprecedented attempt to employ these relationships endowed by the Capsule Network (CapsNet) for salient object detection. The entire salient object detection system is built directly on a Two-Stream Part-Object Assignment Network (TSPOANet) consisting of three algorithmic steps. In the first step, the learned deep feature maps of the input image are transformed to a group of primary capsules. In the second step, we feed the primary capsules into two identical streams, within each of which low-level capsules (parts) will be assigned to their familiar high-level capsules (object) via a locally connected routing. In the final step, the two streams are integrated in the form of a fully connected layer, where the relevant parts can be clustered together to form a complete salient object. Experimental results demonstrate the superiority of the proposed salient object detection network over the state-of-the-art methods.",
        "中文标题": "利用深度部分-对象关系进行显著对象检测",
        "摘要翻译": "尽管基于卷积神经网络（CNNs）的方法在检测显著对象方面取得了成功，但它们决定每个图像部分显著强度的底层机制无法避免同一显著对象内部部分之间的不一致性。这最终会导致检测到的显著对象形状不完整。为了解决这个问题，我们深入研究了部分-对象关系，并首次尝试利用胶囊网络（CapsNet）赋予的这些关系进行显著对象检测。整个显著对象检测系统直接建立在由三个算法步骤组成的两流部分-对象分配网络（TSPOANet）上。在第一步中，输入图像的学习深度特征图被转换为一组初级胶囊。在第二步中，我们将初级胶囊输入到两个相同的流中，在每个流中，低级胶囊（部分）将通过局部连接的路由分配给它们熟悉的高级胶囊（对象）。在最后一步中，两个流以全连接层的形式集成，其中相关部分可以聚集在一起形成一个完整的显著对象。实验结果表明，所提出的显著对象检测网络优于最先进的方法。",
        "领域": "显著对象检测/胶囊网络/特征学习",
        "问题": "解决显著对象检测中由于部分-对象关系不一致导致的显著对象形状不完整问题",
        "动机": "提高显著对象检测的准确性和完整性，通过利用部分-对象关系来避免显著对象内部部分之间的不一致性",
        "方法": "采用两流部分-对象分配网络（TSPOANet），通过将输入图像的深度特征图转换为初级胶囊，并在两个流中通过局部连接的路由将低级胶囊分配给高级胶囊，最后通过全连接层集成两个流，形成完整的显著对象",
        "关键词": [
            "显著对象检测",
            "胶囊网络",
            "部分-对象关系",
            "两流网络",
            "特征学习"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）用于显著对象检测，胶囊网络（CapsNet）用于处理部分-对象关系，两流部分-对象分配网络（TSPOANet）用于集成和形成完整的显著对象"
    },
    {
        "order": 402,
        "title": "AWSD: Adaptive Weighted Spatiotemporal Distillation for Video Representation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tavakolian_AWSD_Adaptive_Weighted_Spatiotemporal_Distillation_for_Video_Representation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tavakolian_AWSD_Adaptive_Weighted_Spatiotemporal_Distillation_for_Video_Representation_ICCV_2019_paper.html",
        "abstract": "We propose an Adaptive Weighted Spatiotemporal Distillation (AWSD) technique for video representation by encoding the appearance and dynamics of the videos into a single RGB image map. This is obtained by adaptively dividing the videos into small segments and comparing two consecutive segments. This allows using pre-trained models on still images for video classification while successfully capturing the spatiotemporal variations in the videos. The adaptive segment selection enables effective encoding of the essential discriminative information of untrimmed videos. Based on Gaussian Scale Mixture, we compute the weights by extracting the mutual information between two consecutive segments. Unlike pooling-based methods, our AWSD gives more importance to the frames that characterize actions or events thanks to its adaptive segment length selection. We conducted extensive experimental analysis to evaluate the effectiveness of our proposed method and compared our results against those of recent state-of-the-art methods on four benchmark datatsets, including UCF101, HMDB51, ActivityNet v1.3, and Maryland. The obtained results on these benchmark datatsets showed that our method significantly outperforms earlier works and sets the new state-of-the-art performance in video classification. Code is available at the project webpage: https://mohammadt68.github.io/AWSD/",
        "中文标题": "AWSD：用于视频表示的自适应加权时空蒸馏",
        "摘要翻译": "我们提出了一种自适应加权时空蒸馏（AWSD）技术，用于视频表示，通过将视频的外观和动态编码成单一的RGB图像图。这是通过自适应地将视频分割成小片段并比较两个连续的片段来实现的。这使得可以使用在静态图像上预训练的模型进行视频分类，同时成功捕捉视频中的时空变化。自适应片段选择能够有效编码未剪辑视频的基本判别信息。基于高斯尺度混合，我们通过提取两个连续片段之间的互信息来计算权重。与基于池化的方法不同，我们的AWSD由于其自适应片段长度选择，对表征动作或事件的帧给予更多重视。我们进行了广泛的实验分析，以评估我们提出的方法的有效性，并将我们的结果与包括UCF101、HMDB51、ActivityNet v1.3和Maryland在内的四个基准数据集上的最新方法进行了比较。在这些基准数据集上获得的结果表明，我们的方法显著优于早期的工作，并在视频分类中设定了新的最先进性能。代码可在项目网页上获取：https://mohammadt68.github.io/AWSD/",
        "领域": "视频分类/时空特征提取/自适应学习",
        "问题": "如何有效地将视频的外观和动态信息编码成单一的RGB图像图，以便使用预训练的静态图像模型进行视频分类",
        "动机": "为了捕捉视频中的时空变化并有效编码未剪辑视频的基本判别信息，同时利用预训练的静态图像模型进行视频分类",
        "方法": "提出了一种自适应加权时空蒸馏（AWSD）技术，通过自适应地将视频分割成小片段并比较两个连续的片段，基于高斯尺度混合计算权重，从而有效编码视频的时空信息",
        "关键词": [
            "视频分类",
            "时空特征提取",
            "自适应学习"
        ],
        "涉及的技术概念": "自适应加权时空蒸馏（AWSD）技术通过将视频分割成小片段并比较连续片段来编码视频的时空信息，利用高斯尺度混合计算权重，强调表征动作或事件的帧，从而实现有效的视频分类。"
    },
    {
        "order": 403,
        "title": "End-to-End Learning of Representations for Asynchronous Event-Based Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gehrig_End-to-End_Learning_of_Representations_for_Asynchronous_Event-Based_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gehrig_End-to-End_Learning_of_Representations_for_Asynchronous_Event-Based_Data_ICCV_2019_paper.html",
        "abstract": "Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as \"events\". They have appealing advantages over frame based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatio-temporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations by means of strictly differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.",
        "中文标题": "端到端学习基于异步事件数据的表示",
        "摘要翻译": "事件相机是一种视觉传感器，记录像素级亮度变化的异步流，称为“事件”。与基于帧的相机相比，它们在计算机视觉方面具有吸引人的优势，包括高时间分辨率、高动态范围和无运动模糊。由于事件信号的稀疏、非均匀时空布局，模式识别算法通常将事件聚合成基于网格的表示，然后通过标准视觉管道（例如卷积神经网络（CNN））进行处理。在这项工作中，我们引入了一个通用框架，通过严格可微的操作将事件流转换为基于网格的表示。我们的框架有两个主要优势：（i）允许以端到端的方式学习输入事件表示与任务专用网络，（ii）提出了一个分类法，统一了文献中的大多数现有事件表示，并识别了新的表示。经验上，我们展示了我们的端到端学习事件表示的方法在光流估计和对象识别上比最先进的方法提高了约12%。",
        "领域": "事件相机/光流估计/对象识别",
        "问题": "如何有效地将事件相机捕获的异步事件数据转换为适合深度学习模型处理的表示",
        "动机": "事件相机提供的高时间分辨率、高动态范围和无运动模糊的数据，为计算机视觉任务提供了新的可能性，但需要有效的方法来处理这些数据的稀疏和非均匀特性",
        "方法": "引入一个通用框架，通过严格可微的操作将事件流转换为基于网格的表示，并允许以端到端的方式学习输入事件表示与任务专用网络",
        "关键词": [
            "事件相机",
            "光流估计",
            "对象识别",
            "端到端学习",
            "可微操作"
        ],
        "涉及的技术概念": "事件相机是一种新型的视觉传感器，能够以极高的时间分辨率记录像素级亮度变化，这种变化被称为“事件”。与传统的基于帧的相机相比，事件相机能够提供高动态范围和无运动模糊的图像数据。卷积神经网络（CNN）是一种深度学习模型，广泛应用于图像识别和处理任务中。端到端学习指的是模型直接从输入到输出进行学习，无需人工设计中间步骤或特征提取。"
    },
    {
        "order": 404,
        "title": "Enhancing Low Light Videos by Exploring High Sensitivity Camera Noise",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Enhancing_Low_Light_Videos_by_Exploring_High_Sensitivity_Camera_Noise_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Enhancing_Low_Light_Videos_by_Exploring_High_Sensitivity_Camera_Noise_ICCV_2019_paper.html",
        "abstract": "Enhancing low light videos, which consists of denoising and brightness adjustment, is an intriguing but knotty problem. Under low light condition, due to high sensitivity camera setting, commonly negligible noises become obvious and severely deteriorate the captured videos. To recover high quality videos, a mass of image/video denoising/enhancing algorithms are proposed, most of which follow a set of simple assumptions about the statistic characters of camera noise, e.g., independent and identically distributed(i.i.d.), white, additive, Gaussian, Poisson or mixture noises. However, the practical noise under high sensitivity setting in real captured videos is complex and inaccurate to model with these assumptions. In this paper, we explore the physical origins of the practical high sensitivity noise in digital cameras, model them mathematically, and propose to enhance the low light videos based on the noise model by using an LSTM-based neural network. Specifically, we generate the training data with the proposed noise model and train the network with the dark noisy video as input and clear-bright video as output. Extensive comparisons on both synthetic and real captured low light videos with the state-of-the-art methods are conducted to demonstrate the effectiveness of the proposed method.",
        "中文标题": "通过探索高灵敏度相机噪声增强低光视频",
        "摘要翻译": "增强低光视频，包括去噪和亮度调整，是一个引人入胜但棘手的问题。在低光条件下，由于高灵敏度相机设置，通常可以忽略的噪声变得明显，并严重恶化捕获的视频。为了恢复高质量视频，提出了大量的图像/视频去噪/增强算法，其中大多数遵循关于相机噪声统计特征的一组简单假设，例如独立同分布(i.i.d.)、白噪声、加性、高斯、泊松或混合噪声。然而，在实际捕获的视频中，高灵敏度设置下的实际噪声复杂且难以用这些假设准确建模。在本文中，我们探索了数字相机中实际高灵敏度噪声的物理起源，对它们进行数学建模，并提出了基于噪声模型使用基于LSTM的神经网络增强低光视频的方法。具体来说，我们使用提出的噪声模型生成训练数据，并以暗噪声视频作为输入，清晰明亮的视频作为输出训练网络。通过对合成和实际捕获的低光视频与最先进方法的广泛比较，证明了所提出方法的有效性。",
        "领域": "视频增强/噪声建模/神经网络",
        "问题": "低光视频中的噪声去除和亮度调整",
        "动机": "高灵敏度相机设置下，低光视频中的噪声变得明显且难以通过现有假设准确建模，影响视频质量",
        "方法": "探索高灵敏度噪声的物理起源，进行数学建模，并基于噪声模型使用LSTM神经网络进行视频增强",
        "关键词": [
            "低光视频",
            "噪声建模",
            "LSTM神经网络",
            "视频增强"
        ],
        "涉及的技术概念": "高灵敏度相机噪声、独立同分布(i.i.d.)噪声、白噪声、加性噪声、高斯噪声、泊松噪声、混合噪声、LSTM神经网络"
    },
    {
        "order": 405,
        "title": "Self-Supervised Deep Depth Denoising",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sterzentsenko_Self-Supervised_Deep_Depth_Denoising_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sterzentsenko_Self-Supervised_Deep_Depth_Denoising_ICCV_2019_paper.html",
        "abstract": "Depth perception is considered an invaluable source of information for various vision tasks. However, depth maps acquired using consumer-level sensors still suffer from non-negligible noise. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned non-uniform noise, while preserving geometric details. Despite the effort, deep depth denoising is still an open challenge mainly due to the lack of clean data that could be used as ground truth. In this paper, we propose a fully convolutional deep autoencoder that learns to denoise depth maps, surpassing the lack of ground truth data. Specifically, the proposed autoencoder exploits multiple views of the same scene from different points of view in order to learn to suppress noise in a self-supervised end-to-end manner using depth and color information during training, yet only depth during inference. To enforce self-supervision, we leverage a differentiable rendering technique to exploit photometric supervision, which is further regularized using geometric and surface priors. As the proposed approach relies on raw data acquisition, a large RGB-D corpus is collected using Intel RealSense sensors. Complementary to a quantitative evaluation, we demonstrate the effectiveness of the proposed self-supervised denoising approach on established 3D reconstruction applications. Code is avalable at https://github.com/VCL3D/DeepDepthDenoising",
        "中文标题": "自监督深度去噪",
        "摘要翻译": "深度感知被认为是各种视觉任务中宝贵的信息来源。然而，使用消费级传感器获取的深度图仍然存在不可忽视的噪声。这一事实最近促使研究人员利用传统滤波器以及深度学习范式来抑制上述非均匀噪声，同时保留几何细节。尽管付出了努力，深度去噪仍然是一个开放的挑战，主要是由于缺乏可以作为地面实况的干净数据。在本文中，我们提出了一种全卷积深度自编码器，它学会了去噪深度图，克服了地面实况数据缺乏的问题。具体来说，所提出的自编码器利用同一场景从不同视角的多个视图，在训练期间使用深度和颜色信息以自监督的端到端方式学习抑制噪声，而在推理期间仅使用深度。为了加强自监督，我们利用可微分渲染技术来利用光度监督，这进一步通过几何和表面先验进行正则化。由于所提出的方法依赖于原始数据采集，因此使用Intel RealSense传感器收集了一个大型RGB-D语料库。除了定量评估外，我们还在已建立的3D重建应用上展示了所提出的自监督去噪方法的有效性。代码可在https://github.com/VCL3D/DeepDepthDenoising获取。",
        "领域": "深度感知/3D重建/自监督学习",
        "问题": "深度图中存在的非均匀噪声问题",
        "动机": "由于缺乏干净的深度图数据作为地面实况，深度去噪仍然是一个挑战",
        "方法": "提出了一种全卷积深度自编码器，利用同一场景的多个视图和可微分渲染技术进行自监督学习，以抑制深度图中的噪声",
        "关键词": [
            "深度感知",
            "3D重建",
            "自监督学习"
        ],
        "涉及的技术概念": "全卷积深度自编码器、可微分渲染技术、光度监督、几何和表面先验、RGB-D语料库"
    },
    {
        "order": 406,
        "title": "Bilinear Attention Networks for Person Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fang_Bilinear_Attention_Networks_for_Person_Retrieval_ICCV_2019_paper.html",
        "abstract": "This paper investigates a novel Bilinear attention (Bi-attention) block, which discovers and uses second order statistical information in an input feature map, for the purpose of person retrieval. The Bi-attention block uses bilinear pooling to model the local pairwise feature interactions along each channel, while preserving the spatial structural information. We propose an Attention in Attention (AiA) mechanism to build inter-dependency among the second order local and global features with the intent to make better use of, or pay more attention to, such higher order statistical relationships. The proposed network, equipped with the proposed Bi-attention is referred to as Bilinear ATtention network (BAT-net). Our approach outperforms current state-of-the-art by a considerable margin across the standard benchmark datasets (e.g., CUHK03, Market-1501, DukeMTMC-reID and MSMT17).",
        "中文标题": "用于人物检索的双线性注意力网络",
        "摘要翻译": "本文研究了一种新颖的双线性注意力（Bi-attention）块，该块发现并利用输入特征图中的二阶统计信息，用于人物检索。Bi-attention块使用双线性池化来建模每个通道上的局部成对特征交互，同时保留空间结构信息。我们提出了一种注意力中的注意力（AiA）机制，以建立二阶局部和全局特征之间的相互依赖性，旨在更好地利用或更多地关注此类高阶统计关系。所提出的网络，配备了所提出的Bi-attention，被称为双线性注意力网络（BAT-net）。我们的方法在标准基准数据集（例如，CUHK03、Market-1501、DukeMTMC-reID和MSMT17）上显著优于当前的最先进技术。",
        "领域": "人物检索/注意力机制/特征交互",
        "问题": "如何更有效地利用输入特征图中的二阶统计信息进行人物检索",
        "动机": "为了提升人物检索的准确性和效率，通过发现并利用输入特征图中的二阶统计信息，以及建立局部和全局特征之间的相互依赖性",
        "方法": "提出了一种双线性注意力块（Bi-attention），使用双线性池化建模局部成对特征交互，并引入注意力中的注意力（AiA）机制来增强二阶局部和全局特征之间的相互依赖性",
        "关键词": [
            "双线性注意力",
            "人物检索",
            "注意力机制",
            "特征交互",
            "双线性池化"
        ],
        "涉及的技术概念": "双线性注意力（Bi-attention）块是一种能够发现并利用输入特征图中二阶统计信息的机制，通过双线性池化来建模局部成对特征交互，同时保留空间结构信息。注意力中的注意力（AiA）机制用于建立二阶局部和全局特征之间的相互依赖性，以更好地利用高阶统计关系。"
    },
    {
        "order": 407,
        "title": "ERL-Net: Entangled Representation Learning for Single Image De-Raining",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_ERL-Net_Entangled_Representation_Learning_for_Single_Image_De-Raining_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_ERL-Net_Entangled_Representation_Learning_for_Single_Image_De-Raining_ICCV_2019_paper.html",
        "abstract": "Despite the significant progress achieved in image de-raining by training an encoder-decoder network within the image-to-image translation formulation, blurry results with missing details indicate the deficiency of the existing models. By interpreting the de-raining encoder-decoder network as a conditional generator, within which the decoder acts as a generator conditioned on the embedding learned by the encoder, the unsatisfactory output can be attributed to the low-quality embedding learned by the encoder. In this paper, we hypothesize that there exists an inherent mapping between the low-quality embedding to a latent optimal one, with which the generator (decoder) can produce much better results. To improve the de-raining results significantly over existing models, we propose to learn this mapping by formulating a residual learning branch, that is capable of adaptively adding residuals to the original low-quality embedding in a representation entanglement manner. Using an embedding learned this way, the decoder is able to generate much more satisfactory de-raining results with better detail recovery and rain artefacts removal, providing new state-of-the-art results on four benchmark datasets with considerable improvement (i.e., on the challenging Rain100H data, an improvement of 4.19dB on PSNR and 5% on SSIM is obtained). The entanglement can be easily adopted into any encoder-decoder based image restoration networks. Besides, we propose a series of evaluation metrics to investigate the specific contribution of the proposed entangled representation learning mechanism. Codes are available at .",
        "中文标题": "ERL-Net: 用于单幅图像去雨的纠缠表示学习",
        "摘要翻译": "尽管通过在图像到图像翻译框架内训练编码器-解码器网络在图像去雨方面取得了显著进展，但模糊的结果和缺失的细节表明现有模型的不足。通过将去雨编码器-解码器网络解释为条件生成器，其中解码器作为生成器，其条件是由编码器学习的嵌入，不满意的输出可以归因于编码器学习的低质量嵌入。在本文中，我们假设存在从低质量嵌入到潜在最优嵌入的固有映射，使用这种映射，生成器（解码器）可以产生更好的结果。为了显著改进现有模型的去雨结果，我们提出通过制定一个残差学习分支来学习这种映射，该分支能够以表示纠缠的方式自适应地向原始低质量嵌入添加残差。使用这种方式学习的嵌入，解码器能够生成更令人满意的去雨结果，具有更好的细节恢复和雨痕去除，在四个基准数据集上提供了新的最先进结果，并取得了相当大的改进（例如，在具有挑战性的Rain100H数据上，PSNR提高了4.19dB，SSIM提高了5%）。这种纠缠可以很容易地应用于任何基于编码器-解码器的图像恢复网络。此外，我们提出了一系列评估指标来研究所提出的纠缠表示学习机制的具体贡献。代码可在。",
        "领域": "图像去雨/图像恢复/表示学习",
        "问题": "现有图像去雨模型生成的图像模糊且细节缺失",
        "动机": "提高图像去雨的质量，通过改进编码器学习的嵌入质量来生成更清晰的去雨图像",
        "方法": "提出了一种残差学习分支，通过表示纠缠的方式自适应地向原始低质量嵌入添加残差，从而改进去雨结果",
        "关键词": [
            "图像去雨",
            "表示学习",
            "残差学习",
            "图像恢复"
        ],
        "涉及的技术概念": "编码器-解码器网络、条件生成器、残差学习、表示纠缠、PSNR（峰值信噪比）、SSIM（结构相似性）"
    },
    {
        "order": 408,
        "title": "Deep Restoration of Vintage Photographs From Scanned Halftone Prints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Deep_Restoration_of_Vintage_Photographs_From_Scanned_Halftone_Prints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Deep_Restoration_of_Vintage_Photographs_From_Scanned_Halftone_Prints_ICCV_2019_paper.html",
        "abstract": "A great number of invaluable historical photographs unfortunately only exist in the form of halftone prints in old publications such as newspapers or books. Their original continuous-tone films have long been lost or irreparably damaged. There have been attempts to digitally restore these vintage halftone prints to the original film quality or higher. However, even using powerful deep convolutional neural networks, it is still difficult to obtain satisfactory results. The main challenge is that the degradation process is complex and compounded while little to no real data is available for properly training a data-driven method. In this research, we adopt a novel strategy of two-stage deep learning, in which the restoration task is divided into two stages: the removal of printing artifacts and the inverse of halftoning. The advantage of our technique is that only the simple first stage requires unsupervised training in order to make the combined network generalize on real halftone prints, while the more complex second stage of inverse halftoning can be easily trained with synthetic data. Extensive experimental results demonstrate the efficacy of the proposed technique for real halftone prints; the new technique significantly outperforms the existing ones in visual quality.",
        "中文标题": "从扫描的半色调印刷品中深度恢复复古照片",
        "摘要翻译": "不幸的是，大量珍贵的历史照片仅以旧出版物（如报纸或书籍）中的半色调印刷品形式存在。它们的原始连续色调胶片早已丢失或无法修复地损坏。已有尝试将这些复古半色调印刷品数字恢复到原始胶片质量或更高。然而，即使使用强大的深度卷积神经网络，仍然难以获得令人满意的结果。主要挑战在于退化过程复杂且复合，而几乎没有真实数据可用于适当训练数据驱动的方法。在本研究中，我们采用了一种新颖的两阶段深度学习策略，其中恢复任务分为两个阶段：印刷伪影的去除和半色调的反转。我们技术的优势在于，只有简单的第一阶段需要无监督训练，以使组合网络能够泛化到真实的半色调印刷品上，而更复杂的第二阶段半色调反转可以轻松使用合成数据进行训练。广泛的实验结果证明了所提出技术对真实半色调印刷品的有效性；新技术在视觉质量上显著优于现有技术。",
        "领域": "图像恢复/深度学习/历史照片处理",
        "问题": "从半色调印刷品中恢复复古照片到原始胶片质量或更高",
        "动机": "大量珍贵历史照片仅以半色调印刷品形式存在，且原始胶片已丢失或损坏，需要一种有效的方法进行恢复",
        "方法": "采用两阶段深度学习策略，分为印刷伪影去除和半色调反转两个阶段，其中第一阶段使用无监督训练，第二阶段使用合成数据训练",
        "关键词": [
            "图像恢复",
            "深度学习",
            "历史照片处理"
        ],
        "涉及的技术概念": "深度卷积神经网络、两阶段深度学习、无监督训练、合成数据训练、半色调反转"
    },
    {
        "order": 409,
        "title": "Discriminative Feature Learning With Consistent Attention Regularization for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Discriminative_Feature_Learning_With_Consistent_Attention_Regularization_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Person re-identification (Re-ID) has undergone a rapid development with the blooming of deep neural network. Most methods are very easily affected by target misalignment and background clutter in the training process. In this paper, we propose a simple yet effective feedforward attention network to address the two mentioned problems, in which a novel consistent attention regularizer and an improved triplet loss are designed to learn foreground attentive features for person Re-ID. Specifically, the consistent attention regularizer aims to keep the deduced foreground masks similar from the low-level, mid-level and high-level feature maps. As a result, the network will focus on the foreground regions at the lower layers, which is benefit to learn discriminative features from the foreground regions at the higher layers. Last but not least, the improved triplet loss is introduced to enhance the feature learning capability, which can jointly minimize the intra-class distance and maximize the inter-class distance in each triplet unit. Experimental results on the Market1501, DukeMTMC-reID and CUHK03 datasets have shown that our method outperforms most of the state-of-the-art approaches.",
        "中文标题": "用于行人重识别的具有一致注意力正则化的判别特征学习",
        "摘要翻译": "随着深度神经网络的蓬勃发展，行人重识别（Re-ID）经历了快速发展。大多数方法在训练过程中非常容易受到目标错位和背景杂波的影响。在本文中，我们提出了一种简单而有效的前馈注意力网络来解决上述两个问题，其中设计了一种新颖的一致注意力正则化器和改进的三重损失，以学习行人Re-ID的前景注意力特征。具体来说，一致注意力正则化器旨在保持从低级、中级和高级特征图中推导出的前景掩码相似。因此，网络将在较低层次上关注前景区域，这有利于在较高层次上从前景区域学习判别特征。最后但同样重要的是，引入了改进的三重损失以增强特征学习能力，它可以在每个三重单元中共同最小化类内距离并最大化类间距离。在Market1501、DukeMTMC-reID和CUHK03数据集上的实验结果表明，我们的方法优于大多数最先进的方法。",
        "领域": "行人重识别/注意力机制/特征学习",
        "问题": "解决行人重识别中目标错位和背景杂波的问题",
        "动机": "提高行人重识别的准确性和鲁棒性，通过关注前景区域来学习更具判别性的特征",
        "方法": "提出了一种前馈注意力网络，包括一致注意力正则化器和改进的三重损失，以学习前景注意力特征",
        "关键词": [
            "行人重识别",
            "注意力机制",
            "特征学习",
            "三重损失"
        ],
        "涉及的技术概念": "深度神经网络、前馈注意力网络、一致注意力正则化器、改进的三重损失、前景注意力特征、类内距离、类间距离"
    },
    {
        "order": 410,
        "title": "Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Cost-Aware_Fine-Grained_Recognition_for_IoTs_Based_on_Sequential_Fixations_ICCV_2019_paper.html",
        "abstract": "We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images. Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, we train a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching(DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.",
        "中文标题": "基于顺序注视的物联网细粒度识别成本意识",
        "摘要翻译": "我们考虑在具有有限功率的边缘摄像设备上进行细粒度分类的问题。边缘设备必须谨慎与云交互，以最小化通信比特数来节省电力，云在接收到边缘输入后返回分类标签。为了处理细粒度分类，我们采用具有焦点视场的顺序注视视角来建模云边缘交互。我们提出了一种新颖的基于深度强化学习的焦点模型DRIFT，它顺序生成并识别混合锐度图像。DRIFT的训练仅需要图像级别的类别标签，并鼓励注视包含任务相关信息，同时保持数据效率。具体来说，我们使用一种新颖的通过条件评论家和教练的深度确定性策略梯度（DDPGC3）算法来训练焦点演员网络。此外，我们提出塑造奖励以在每次注视后提供信息反馈，以更好地指导RL训练。我们通过在五个细粒度分类基准数据集上的评估展示了DRIFT在此任务上的有效性，并显示所提出的方法在传输像素减少超过3倍的情况下实现了最先进的性能。",
        "领域": "边缘计算/细粒度分类/深度强化学习",
        "问题": "在功率有限的边缘摄像设备上进行细粒度分类",
        "动机": "为了在边缘设备上实现细粒度分类，同时最小化与云的通信以节省电力",
        "方法": "采用基于深度强化学习的焦点模型DRIFT，顺序生成并识别混合锐度图像，使用DDPGC3算法训练焦点演员网络，并通过塑造奖励提供信息反馈",
        "关键词": [
            "边缘计算",
            "细粒度分类",
            "深度强化学习",
            "焦点模型",
            "DDPGC3算法"
        ],
        "涉及的技术概念": {
            "边缘计算": "在靠近数据源的地方进行数据处理，以减少延迟和带宽使用",
            "细粒度分类": "对非常相似的对象进行分类，需要高精度的识别能力",
            "深度强化学习": "一种机器学习方法，通过奖励机制来训练模型做出决策",
            "焦点模型": "模拟人类视觉系统，通过顺序注视来识别图像中的关键区域",
            "DDPGC3算法": "一种深度确定性策略梯度算法，通过条件评论家和教练来优化策略"
        }
    },
    {
        "order": 411,
        "title": "Perceptual Deep Depth Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.html",
        "abstract": "RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.",
        "中文标题": "感知深度深度超分辨率",
        "摘要翻译": "RGBD图像，结合了来自各种类型深度传感器的高分辨率颜色和较低分辨率的深度，变得越来越普遍。通过利用颜色信息，可以显著提高深度图的分辨率；深度学习方法使得结合颜色和深度信息变得特别容易。然而，融合这两种数据源可能会导致各种伪影。如果深度图用于重建3D形状，例如用于虚拟现实应用，上采样图像的视觉质量尤为重要。我们方法的主要思想是通过渲染生成的3D表面来测量深度图上采样的质量。我们证明，当与训练过的CNN或简单的深度先验一起使用时，一个简单的基于视觉外观的损失可以显著改善3D形状，这是通过许多现有的感知指标来衡量的。我们将这种方法与许多现有的优化和学习技术进行了比较。",
        "领域": "3D重建/虚拟现实/深度感知",
        "问题": "提高深度图的分辨率并减少融合颜色和深度信息时产生的伪影",
        "动机": "为了在虚拟现实等应用中重建高质量的3D形状，需要提高深度图的分辨率和视觉质量",
        "方法": "通过渲染生成的3D表面来测量深度图上采样的质量，并使用基于视觉外观的损失与CNN或深度先验结合，以改善3D形状",
        "关键词": [
            "深度图",
            "3D重建",
            "视觉质量",
            "上采样",
            "CNN",
            "深度先验"
        ],
        "涉及的技术概念": "RGBD图像结合了高分辨率的颜色信息和低分辨率的深度信息，深度学习方法用于融合这两种信息以提高深度图的分辨率。通过渲染3D表面来评估深度图上采样的质量，使用基于视觉外观的损失与卷积神经网络（CNN）或深度先验结合，以改善3D形状的重建质量。"
    },
    {
        "order": 412,
        "title": "Semi-Supervised Domain Adaptation via Minimax Entropy",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saito_Semi-Supervised_Domain_Adaptation_via_Minimax_Entropy_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saito_Semi-Supervised_Domain_Adaptation_via_Minimax_Entropy_ICCV_2019_paper.html",
        "abstract": "Contemporary domain adaptation methods are very effective at aligning feature distributions of source and target domains without any target supervision. However, we show that these techniques perform poorly when even a few labeled examples are available in the target domain. To address this semi-supervised domain adaptation (SSDA) setting, we propose a novel Minimax Entropy (MME) approach that adversarially optimizes an adaptive few-shot model. Our base model consists of a feature encoding network, followed by a classification layer that computes the features' similarity to estimated prototypes (representatives of each class). Adaptation is achieved by alternately maximizing the conditional entropy of unlabeled target data with respect to the classifier and minimizing it with respect to the feature encoder. We empirically demonstrate the superiority of our method over many baselines, including conventional feature alignment and few-shot methods, setting a new state of the art for SSDA. Our code is available at http://cs-people.bu.edu/keisaito/research/MME.html.",
        "中文标题": "通过最小最大熵的半监督领域适应",
        "摘要翻译": "当代的领域适应方法在没有目标监督的情况下，非常有效地对齐了源领域和目标领域的特征分布。然而，我们展示了当目标领域中有少量标记样本可用时，这些技术表现不佳。为了解决这种半监督领域适应（SSDA）设置，我们提出了一种新颖的最小最大熵（MME）方法，该方法对抗性地优化了一个自适应的小样本模型。我们的基础模型由一个特征编码网络组成，随后是一个分类层，该层计算特征与估计原型（每个类的代表）的相似性。适应是通过交替地最大化未标记目标数据相对于分类器的条件熵和最小化其相对于特征编码器的条件熵来实现的。我们经验性地证明了我们的方法优于许多基线，包括传统的特征对齐和小样本方法，为SSDA设定了新的技术水平。我们的代码可在http://cs-people.bu.edu/keisaito/research/MME.html获取。",
        "领域": "领域适应/小样本学习/对抗性学习",
        "问题": "解决在目标领域有少量标记样本时的半监督领域适应问题",
        "动机": "现有领域适应方法在目标领域有少量标记样本时表现不佳，需要一种新的方法来提高性能",
        "方法": "提出最小最大熵（MME）方法，通过对抗性地优化自适应小样本模型，交替地最大化未标记目标数据相对于分类器的条件熵和最小化其相对于特征编码器的条件熵",
        "关键词": [
            "领域适应",
            "小样本学习",
            "对抗性学习",
            "最小最大熵"
        ],
        "涉及的技术概念": "特征编码网络、分类层、条件熵、特征对齐、小样本方法"
    },
    {
        "order": 413,
        "title": "Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Context-Aware_Image_Matting_for_Simultaneous_Foreground_and_Alpha_Estimation_ICCV_2019_paper.html",
        "abstract": "Natural image matting is an important problem in computer vision and graphics. It is an ill-posed problem when only an input image is available without any external information. While the recent deep learning approaches have shown promising results, they only estimate the alpha matte. This paper presents a context-aware natural image matting method for simultaneous foreground and alpha matte estimation. Our method employs two encoder networks to extract essential information for matting. Particularly, we use a matting encoder to learn local features and a context encoder to obtain more global context information. We concatenate the outputs from these two encoders and feed them into decoder networks to simultaneously estimate the foreground and alpha matte. To train this whole deep neural network, we employ both the standard Laplacian loss and the feature loss: the former helps to achieve high numerical performance while the latter leads to more perceptually plausible results. We also report several data augmentation strategies that greatly improve the network's generalization performance. Our qualitative and quantitative experiments show that our method enables high-quality matting for a single natural image.",
        "中文标题": "上下文感知的图像抠图用于同时估计前景和Alpha",
        "摘要翻译": "自然图像抠图是计算机视觉和图形学中的一个重要问题。当只有输入图像而没有任何外部信息时，这是一个不适定问题。尽管最近的深度学习方法显示出有希望的结果，但它们仅估计alpha遮罩。本文提出了一种上下文感知的自然图像抠图方法，用于同时估计前景和alpha遮罩。我们的方法采用两个编码器网络来提取抠图所需的关键信息。特别是，我们使用一个抠图编码器来学习局部特征，并使用一个上下文编码器来获取更多的全局上下文信息。我们将这两个编码器的输出连接起来，并将它们输入到解码器网络中，以同时估计前景和alpha遮罩。为了训练这个深度神经网络，我们采用了标准的拉普拉斯损失和特征损失：前者有助于实现高数值性能，而后者则导致更符合感知的结果。我们还报告了几种数据增强策略，这些策略大大提高了网络的泛化性能。我们的定性和定量实验表明，我们的方法能够为单一自然图像实现高质量的抠图。",
        "领域": "图像抠图/深度学习/计算机图形学",
        "问题": "在只有输入图像而没有外部信息的情况下，同时估计前景和alpha遮罩",
        "动机": "解决自然图像抠图这一不适定问题，提高抠图质量",
        "方法": "采用两个编码器网络分别学习局部特征和全局上下文信息，通过解码器网络同时估计前景和alpha遮罩，并使用拉普拉斯损失和特征损失进行训练",
        "关键词": [
            "图像抠图",
            "深度学习",
            "计算机图形学"
        ],
        "涉及的技术概念": {
            "自然图像抠图": "指从自然图像中精确地提取前景对象的过程，通常用于图像编辑和合成。",
            "alpha遮罩": "用于表示图像中每个像素的透明度，是图像抠图的关键输出之一。",
            "编码器网络": "深度学习中的一种网络结构，用于从输入数据中提取特征。",
            "解码器网络": "与编码器网络相对，用于从编码的特征中重建或生成输出数据。",
            "拉普拉斯损失": "一种用于图像处理任务的损失函数，有助于提高数值性能。",
            "特征损失": "一种损失函数，用于确保输出在感知上更加合理。",
            "数据增强": "通过变换原始数据来增加训练数据的多样性，以提高模型的泛化能力。"
        }
    },
    {
        "order": 414,
        "title": "3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Armeni_3D_Scene_Graph_A_Structure_for_Unified_Semantics_3D_Space_ICCV_2019_paper.html",
        "abstract": "A comprehensive semantic understanding of a scene is important for many applications - but in what space should diverse semantic information (e.g., objects, scene categories, material types, 3D shapes, etc.) be grounded and what should be its structure? Aspiring to have one unified structure that hosts diverse types of semantics, we follow the Scene Graph paradigm in 3D, generating a 3D Scene Graph. Given a 3D mesh and registered panoramic images, we construct a graph that spans the entire building and includes semantics on objects (e.g., class, material, shape and other attributes), rooms (e.g., function, illumination type, etc.) and cameras (e.g., location, etc.), as well as the relationships among these entities. However, this process is prohibitively labor heavy if done manually. To alleviate this we devise a semi-automatic framework that employs existing detection methods and enhances them using two main constraints: I. framing of query images sampled on panoramas to maximize the performance of 2D detectors, and II. multi-view consistency enforcement across 2D detections that originate in different camera locations.",
        "中文标题": "3D场景图：统一语义、3D空间和相机的结构",
        "摘要翻译": "对场景的全面语义理解对于许多应用来说非常重要——但是，多样化的语义信息（例如，物体、场景类别、材料类型、3D形状等）应该基于什么空间，以及它的结构应该是什么？为了拥有一个能够承载多样化语义类型的统一结构，我们遵循3D中的场景图范式，生成了一个3D场景图。给定一个3D网格和注册的全景图像，我们构建了一个跨越整个建筑的图，包括物体（例如，类别、材料、形状和其他属性）、房间（例如，功能、照明类型等）和相机（例如，位置等）的语义，以及这些实体之间的关系。然而，如果手动完成，这个过程将极其繁重。为了缓解这一问题，我们设计了一个半自动框架，该框架采用现有的检测方法，并通过两个主要约束来增强它们：I. 在采样于全景的查询图像上进行框架设计，以最大化2D检测器的性能，II. 跨源自不同相机位置的2D检测的多视图一致性强制执行。",
        "领域": "3D场景理解/语义分割/全景图像处理",
        "问题": "如何在3D空间中统一和结构化多样化的语义信息",
        "动机": "为了实现对场景的全面语义理解，需要一个能够承载多样化语义类型的统一结构",
        "方法": "采用3D场景图范式，构建一个半自动框架，通过增强现有的检测方法，利用查询图像的框架设计和多视图一致性强制执行来构建3D场景图",
        "关键词": [
            "3D场景图",
            "语义理解",
            "全景图像",
            "半自动框架",
            "多视图一致性"
        ],
        "涉及的技术概念": "3D场景图是一种结构，用于统一表示3D空间中的语义信息、物体、房间和相机的位置及其相互关系。半自动框架通过增强现有的检测方法，利用特定的约束条件（如查询图像的框架设计和多视图一致性强制执行）来自动化3D场景图的构建过程。"
    },
    {
        "order": 415,
        "title": "Layout-Induced Video Representation for Recognizing Agent-in-Place Actions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Layout-Induced_Video_Representation_for_Recognizing_Agent-in-Place_Actions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Layout-Induced_Video_Representation_for_Recognizing_Agent-in-Place_Actions_ICCV_2019_paper.html",
        "abstract": "We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with agents who perform them and the places where they occur, in the context of outdoor home surveillance. We introduce a novel representation to model the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training scenes to unseen scenes in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places to explicitly model scene layout. LIVR partitions the semantic features of a scene into different places to force the network to learn generic place-based feature descriptions which are independent of specific scene layouts; then, LIVR dynamically aggregates features based on connectivities of places in each specific scene to model its layout. We introduce a new Agent-in-Place Action (APA) dataset(The dataset is pending legal review and will be released upon the acceptance of this paper.) to show that our method allows neural network models to generalize significantly better to unseen scenes.",
        "中文标题": "布局诱导的视频表示用于识别场所中的代理行为",
        "摘要翻译": "我们针对户外家庭监控场景中的场所布局建模，以识别与执行行为的代理及其发生场所相关的行为。我们引入了一种新颖的表示方法，以建模场景布局的几何和拓扑结构，从而使网络能够从训练场景中观察到的布局推广到测试集中的未见场景。这种布局诱导的视频表示（LIVR）抽象了低层次的外观差异，并编码了场所的几何和拓扑关系，以明确建模场景布局。LIVR将场景的语义特征划分为不同的场所，迫使网络学习独立于特定场景布局的通用场所特征描述；然后，LIVR根据每个特定场景中场所的连接性动态聚合特征，以建模其布局。我们引入了一个新的场所中的代理行为（APA）数据集（该数据集正在接受法律审查，将在本文接受后发布），以展示我们的方法使神经网络模型能够显著更好地推广到未见场景。",
        "领域": "视频理解/场景理解/行为识别",
        "问题": "识别与特定场所和代理相关的行为",
        "动机": "提高神经网络模型在未见场景中的泛化能力",
        "方法": "引入布局诱导的视频表示（LIVR）来建模场景布局的几何和拓扑结构，通过划分场景的语义特征到不同场所并动态聚合特征来建模特定场景的布局",
        "关键词": [
            "视频理解",
            "场景理解",
            "行为识别"
        ],
        "涉及的技术概念": "布局诱导的视频表示（LIVR）是一种新颖的表示方法，用于抽象低层次的外观差异并编码场所的几何和拓扑关系，以明确建模场景布局。通过将场景的语义特征划分为不同的场所，并基于场所的连接性动态聚合特征，LIVR能够有效地建模特定场景的布局，从而提高神经网络模型在未见场景中的泛化能力。"
    },
    {
        "order": 416,
        "title": "Boosting Few-Shot Visual Learning With Self-Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.html",
        "abstract": "Few-shot learning and self-supervised learning address different facets of the same problem: how to train a model with little or no labeled data. Few-shot learning aims for optimization methods and models that can learn efficiently to recognize patterns in the low data regime. Self-supervised learning focuses instead on unlabeled data and looks into it for the supervisory signal to feed high capacity deep neural networks. In this work we exploit the complementarity of these two domains and propose an approach for improving few-shot learning through self-supervision. We use self-supervision as an auxiliary task in a few-shot learning pipeline, enabling feature extractors to learn richer and more transferable visual representations while still using few annotated samples. Through self-supervision, our approach can be naturally extended towards using diverse unlabeled data from other datasets in the few-shot setting. We report consistent improvements across an array of architectures, datasets and self-supervision techniques. We provide the implementation code at: https://github.com/valeoai/BF3S",
        "中文标题": "通过自监督提升少样本视觉学习",
        "摘要翻译": "少样本学习和自监督学习解决了同一个问题的不同方面：如何用很少或没有标签数据来训练模型。少样本学习旨在优化方法和模型，使其能够在数据量少的情况下高效地学习识别模式。自监督学习则专注于未标记的数据，并从中寻找监督信号以供给高容量的深度神经网络。在这项工作中，我们利用这两个领域的互补性，提出了一种通过自监督改进少样本学习的方法。我们在少样本学习流程中使用自监督作为辅助任务，使特征提取器能够学习更丰富、更可转移的视觉表示，同时仍然使用少量注释样本。通过自监督，我们的方法可以自然地扩展到在少样本设置中使用来自其他数据集的多样化未标记数据。我们报告了一系列架构、数据集和自监督技术中的一致改进。我们提供了实现代码：https://github.com/valeoai/BF3S",
        "领域": "少样本学习/自监督学习/视觉表示学习",
        "问题": "如何在少量或没有标签数据的情况下训练模型",
        "动机": "利用少样本学习和自监督学习的互补性，改进少样本学习的效果",
        "方法": "在少样本学习流程中引入自监督作为辅助任务，以学习更丰富、更可转移的视觉表示",
        "关键词": [
            "少样本学习",
            "自监督学习",
            "视觉表示学习"
        ],
        "涉及的技术概念": "少样本学习旨在在数据量少的情况下高效学习识别模式；自监督学习通过未标记数据寻找监督信号；通过结合这两种方法，可以在使用少量注释样本的同时，学习到更丰富、更可转移的视觉表示。"
    },
    {
        "order": 417,
        "title": "CFSNet: Toward a Controllable Feature Space for Image Restoration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CFSNet_Toward_a_Controllable_Feature_Space_for_Image_Restoration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CFSNet_Toward_a_Controllable_Feature_Space_for_Image_Restoration_ICCV_2019_paper.html",
        "abstract": "Deep learning methods have witnessed the great progress in image restoration with specific metrics (e.g., PSNR, SSIM). However, the perceptual quality of the restored image is relatively subjective, and it is necessary for users to control the reconstruction result according to personal preferences or image characteristics, which cannot be done using existing deterministic networks. This motivates us to exquisitely design a unified interactive framework for general image restoration tasks. Under this framework, users can control continuous transition of different objectives, e.g., the perception-distortion trade-off of image super-resolution, the trade-off between noise reduction and detail preservation. We achieve this goal by controlling the latent features of the designed network. To be specific, our proposed framework, named Controllable Feature Space Network (CFSNet), is entangled by two branches based on different objectives. Our framework can adaptively learn the coupling coefficients of different layers and channels, which provides finer control of the restored image quality. Experiments on several typical image restoration tasks fully validate the effective benefits of the proposed method. Code is available at https://github.com/qibao77/CFSNet.",
        "中文标题": "CFSNet：迈向可控特征空间的图像恢复",
        "摘要翻译": "深度学习方法在图像恢复方面取得了巨大进展，特别是在特定指标（如PSNR、SSIM）上。然而，恢复图像的感知质量相对主观，用户需要根据个人偏好或图像特征控制重建结果，这是现有确定性网络无法做到的。这激励我们精心设计一个统一的交互框架，用于一般图像恢复任务。在此框架下，用户可以控制不同目标的连续过渡，例如图像超分辨率的感知-失真权衡，噪声减少与细节保留之间的权衡。我们通过控制设计网络的潜在特征来实现这一目标。具体来说，我们提出的框架，名为可控特征空间网络（CFSNet），由基于不同目标的两个分支纠缠而成。我们的框架可以自适应地学习不同层和通道的耦合系数，从而提供更精细的恢复图像质量控制。在几个典型的图像恢复任务上的实验充分验证了所提出方法的有效益处。代码可在https://github.com/qibao77/CFSNet获取。",
        "领域": "图像恢复/图像超分辨率/图像质量评估",
        "问题": "现有图像恢复方法无法根据用户偏好或图像特征控制重建结果",
        "动机": "为了提供更符合用户个人偏好或图像特征的图像恢复结果，需要设计一个能够控制重建结果的交互框架",
        "方法": "设计了一个名为CFSNet的统一交互框架，通过控制网络的潜在特征，允许用户控制不同目标的连续过渡，如感知-失真权衡和噪声减少与细节保留的权衡",
        "关键词": [
            "图像恢复",
            "图像超分辨率",
            "感知质量",
            "交互框架",
            "特征控制"
        ],
        "涉及的技术概念": "PSNR（峰值信噪比）和SSIM（结构相似性）是评估图像恢复质量的常用指标。CFSNet框架通过控制网络的潜在特征，实现了对图像恢复结果的精细控制，特别是在图像超分辨率和噪声减少与细节保留的权衡方面。"
    },
    {
        "order": 418,
        "title": "Floorplan-Jigsaw: Jointly Estimating Scene Layout and Aligning Partial Scans",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Floorplan-Jigsaw_Jointly_Estimating_Scene_Layout_and_Aligning_Partial_Scans_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Floorplan-Jigsaw_Jointly_Estimating_Scene_Layout_and_Aligning_Partial_Scans_ICCV_2019_paper.html",
        "abstract": "We present a novel approach to align partial 3D reconstructions which may not have substantial overlap. Using floorplan priors, our method jointly predicts a room layout and estimates the transformations from a set of partial 3D data. Unlike the existing methods relying on feature descriptors to establish correspondences, we exploit the 3D \"box\" structure of a typical room layout that meets the Manhattan World property. We first estimate a local layout for each partial scan separately and then combine these local layouts to form a globally aligned layout with loop closure. Without the requirement of feature matching, the proposed method enables some novel applications ranging from large or featureless scene reconstruction and modeling from sparse input. We validate our method quantitatively and qualitatively on real and synthetic scenes of various sizes and complexities. The evaluations and comparisons show superior effectiveness and accuracy of our method.",
        "中文标题": "平面图拼图：联合估计场景布局和对齐部分扫描",
        "摘要翻译": "我们提出了一种新颖的方法来对齐可能没有显著重叠的部分3D重建。利用平面图先验，我们的方法联合预测房间布局并估计从一组部分3D数据的变换。与依赖特征描述符建立对应关系的现有方法不同，我们利用了满足曼哈顿世界属性的典型房间布局的3D“盒子”结构。我们首先分别估计每个部分扫描的局部布局，然后将这些局部布局组合起来，形成一个具有环闭合的全局对齐布局。由于不需要特征匹配，所提出的方法使得从稀疏输入进行大或特征缺失场景重建和建模等一些新颖应用成为可能。我们在各种大小和复杂度的真实和合成场景上定量和定性地验证了我们的方法。评估和比较显示了我们方法的优越有效性和准确性。",
        "领域": "3D重建/场景理解/室内布局估计",
        "问题": "对齐没有显著重叠的部分3D重建",
        "动机": "解决现有方法依赖特征描述符建立对应关系的限制，实现对稀疏输入的大或特征缺失场景的有效重建和建模",
        "方法": "利用平面图先验，联合预测房间布局并估计从部分3D数据的变换，通过估计每个部分扫描的局部布局并组合成全局对齐布局",
        "关键词": [
            "3D重建",
            "场景理解",
            "室内布局估计"
        ],
        "涉及的技术概念": "3D重建、平面图先验、曼哈顿世界属性、局部布局估计、全局对齐布局、环闭合、特征匹配"
    },
    {
        "order": 419,
        "title": "Anomaly Detection in Video Sequence With Appearance-Motion Correspondence",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_Anomaly_Detection_in_Video_Sequence_With_Appearance-Motion_Correspondence_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_Anomaly_Detection_in_Video_Sequence_With_Appearance-Motion_Correspondence_ICCV_2019_paper.html",
        "abstract": "Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods.",
        "中文标题": "视频序列中的外观-运动对应异常检测",
        "摘要翻译": "由于可能事件的多样性，监控视频中的异常检测目前是一个挑战。我们提出了一个深度卷积神经网络（CNN），通过学习常见对象外观（例如行人、背景、树等）与其相关运动之间的对应关系来解决这个问题。我们的模型设计为一个重建网络和一个图像翻译模型的组合，它们共享相同的编码器。前者子网络确定视频帧中出现的最重要结构，而后者尝试将运动模板与这些结构关联起来。训练阶段仅使用正常事件的视频进行，然后模型能够为未知输入估计帧级分数。在6个基准数据集上的实验表明，所提出的方法相对于最先进的方法具有竞争力的性能。",
        "领域": "视频监控/异常检测/深度学习",
        "问题": "监控视频中的异常检测",
        "动机": "由于监控视频中可能事件的多样性，准确检测异常事件是一个挑战。",
        "方法": "提出了一种深度卷积神经网络，通过学习对象外观与其相关运动之间的对应关系来解决异常检测问题。模型结合了重建网络和图像翻译模型，共享相同的编码器。",
        "关键词": [
            "异常检测",
            "视频监控",
            "卷积神经网络",
            "外观-运动对应",
            "图像翻译"
        ],
        "涉及的技术概念": "深度卷积神经网络（CNN）用于学习对象外观与其相关运动之间的对应关系，重建网络用于确定视频帧中的重要结构，图像翻译模型用于将运动模板与这些结构关联。训练仅使用正常事件的视频，模型能够为未知输入估计帧级分数。"
    },
    {
        "order": 420,
        "title": "FDA: Feature Disruptive Attack",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ganeshan_FDA_Feature_Disruptive_Attack_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ganeshan_FDA_Feature_Disruptive_Attack_ICCV_2019_paper.html",
        "abstract": "Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network's prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new attack FDA: Feature Disruptive attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupts features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-the-art methods for Image classification, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-specific network or methodology.",
        "中文标题": "FDA: 特征破坏攻击",
        "摘要翻译": "尽管深度神经网络（DNN）在各种计算机视觉任务中表现出色，但多项研究表明它们对对抗样本的脆弱性，即通过精心设计的不可察觉噪声来操纵网络预测的图像样本。对抗样本生成方法从简单到复杂的优化技术不等。大多数这些方法通过优化目标生成对抗样本，这些目标与网络的预softmax或softmax输出相关。在这项工作中，我们（i）展示了此类攻击的缺点，（ii）提出了两个新的评估指标：旧标签新排名（OLNR）和新标签旧排名（NLOR），以量化攻击造成的损害程度，以及（iii）提出了一种新的攻击方法FDA：特征破坏攻击，以解决现有攻击的缺点。FDA通过生成图像扰动来破坏网络每一层的特征，并导致深层特征高度损坏。这使得FDA对抗样本能够严重降低深度网络的性能。我们通过实验验证，即使在存在各种防御措施的情况下，FDA生成的对抗样本也比其他最先进的方法在图像分类上更强。更重要的是，我们展示了FDA即使在没有访问特定任务网络或方法的情况下也能破坏基于特征表示的任务。",
        "领域": "对抗样本生成/图像分类/网络安全性",
        "问题": "深度神经网络对对抗样本的脆弱性",
        "动机": "解决现有对抗样本生成方法的缺点，提高对抗样本的破坏效果",
        "方法": "提出特征破坏攻击（FDA），通过生成图像扰动破坏网络每一层的特征，并引入新的评估指标OLNR和NLOR来量化攻击效果",
        "关键词": [
            "对抗样本",
            "特征破坏",
            "图像分类",
            "网络安全性"
        ],
        "涉及的技术概念": {
            "对抗样本": "通过添加不可察觉的噪声到图像中，旨在误导深度神经网络的预测",
            "特征破坏攻击（FDA）": "一种新的对抗样本生成方法，通过破坏网络每一层的特征来生成对抗样本",
            "旧标签新排名（OLNR）和新标签旧排名（NLOR）": "两个新的评估指标，用于量化对抗样本攻击对深度神经网络造成的损害程度"
        }
    },
    {
        "order": 421,
        "title": "Deep Blind Hyperspectral Image Fusion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Blind_Hyperspectral_Image_Fusion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Blind_Hyperspectral_Image_Fusion_ICCV_2019_paper.html",
        "abstract": "Hyperspectral image fusion (HIF) reconstructs high spatial resolution hyperspectral images from low spatial resolution hyperspectral images and high spatial resolution multispectral images. Previous works usually assume that the linear mapping between the point spread functions of the hyperspectral camera and the spectral response functions of the conventional camera is known. This is unrealistic in many scenarios. We propose a method for blind HIF problem based on deep learning, where the estimation of the observation model and fusion process are optimized iteratively and alternatingly during the super-resolution reconstruction. In addition, the proposed framework enforces simultaneous spatial and spectral accuracy. Using three public datasets, the experimental results demonstrate that the proposed algorithm outperforms existing blind and non-blind methods.",
        "中文标题": "深度盲超光谱图像融合",
        "摘要翻译": "超光谱图像融合（HIF）从低空间分辨率的超光谱图像和高空间分辨率的多光谱图像重建高空间分辨率的超光谱图像。以往的工作通常假设超光谱相机的点扩散函数与常规相机的光谱响应函数之间的线性映射是已知的。这在许多情况下是不现实的。我们提出了一种基于深度学习的盲HIF问题解决方法，其中在超分辨率重建过程中迭代和交替地优化观测模型的估计和融合过程。此外，所提出的框架强制同时实现空间和光谱的准确性。使用三个公共数据集的实验结果表明，所提出的算法优于现有的盲和非盲方法。",
        "领域": "超光谱图像处理/图像重建/深度学习",
        "问题": "在未知超光谱相机的点扩散函数与常规相机的光谱响应函数之间的线性映射的情况下，如何从低空间分辨率的超光谱图像和高空间分辨率的多光谱图像重建高空间分辨率的超光谱图像。",
        "动机": "以往的工作通常假设超光谱相机的点扩散函数与常规相机的光谱响应函数之间的线性映射是已知的，这在许多情况下是不现实的。",
        "方法": "提出了一种基于深度学习的盲HIF问题解决方法，其中在超分辨率重建过程中迭代和交替地优化观测模型的估计和融合过程，并强制同时实现空间和光谱的准确性。",
        "关键词": [
            "超光谱图像融合",
            "深度学习",
            "图像重建"
        ],
        "涉及的技术概念": "超光谱图像融合（HIF）是一种技术，旨在从低空间分辨率的超光谱图像和高空间分辨率的多光谱图像重建高空间分辨率的超光谱图像。深度学习是一种机器学习方法，通过使用多层神经网络来学习数据的复杂模式。图像重建是指从一组观测数据中恢复原始图像的过程。"
    },
    {
        "order": 422,
        "title": "Enforcing Geometric Constraints of Virtual Normal for Depth Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.html",
        "abstract": "Monocular depth prediction plays a crucial role in understanding 3D scene geometry. Although recent methods have achieved impressive progress in evaluation metrics such as the pixel-wise relative error, most methods neglect the geometric constraints in the 3D space. In this work, we show the importance of the high-order 3D geometric constraints for depth prediction. By designing a loss term that enforces one simple type of geometric constraints, namely, virtual normal directions determined by randomly sampled three points in the reconstructed 3D space, we can considerably improve the depth prediction accuracy. Furthermore, we can not only predict accurate depth but also achieve high-quality other 3D information from the depth without retraining new parameters, Significantly, the byproduct of this predicted depth being sufficiently accurate is that we are now able to recover good 3D structures of the scene such as the point cloud and surface normal directly from the depth, eliminating the necessity of training new sub-models as was previously done. Experiments on two challenging benchmarks: NYU Depth-V2 and KITTI demonstrate the effectiveness of our method and state-of-the-art performance.",
        "中文标题": "强制执行虚拟法线的几何约束以进行深度预测",
        "摘要翻译": "单目深度预测在理解3D场景几何中扮演着关键角色。尽管最近的方法在评估指标如像素级相对误差方面取得了显著进展，但大多数方法忽视了3D空间中的几何约束。在这项工作中，我们展示了高阶3D几何约束对深度预测的重要性。通过设计一个损失项来强制执行一种简单的几何约束，即在重建的3D空间中由随机采样的三个点确定的虚拟法线方向，我们可以显著提高深度预测的准确性。此外，我们不仅能够预测准确的深度，还能从深度中获取高质量的其它3D信息，而无需重新训练新参数。值得注意的是，这种预测深度的副产品足够准确，我们现在能够直接从深度中恢复良好的3D场景结构，如点云和表面法线，消除了之前需要训练新子模型的必要性。在两个具有挑战性的基准测试：NYU Depth-V2和KITTI上的实验证明了我们方法的有效性和最先进的性能。",
        "领域": "3D场景理解/深度预测/几何约束",
        "问题": "提高单目深度预测的准确性",
        "动机": "大多数现有方法忽视了3D空间中的几何约束，这限制了深度预测的准确性和3D场景理解的深度。",
        "方法": "设计一个损失项来强制执行虚拟法线方向的几何约束，通过随机采样的三个点在重建的3D空间中确定虚拟法线方向，从而提高深度预测的准确性。",
        "关键词": [
            "单目深度预测",
            "3D几何约束",
            "虚拟法线"
        ],
        "涉及的技术概念": "虚拟法线方向：在重建的3D空间中，通过随机采样的三个点确定的方向，用于强制执行几何约束。损失项：用于在训练过程中最小化预测值与真实值之间的差异，以提高模型的预测准确性。点云和表面法线：从深度预测中恢复的3D场景结构信息，用于进一步分析和理解场景。"
    },
    {
        "order": 423,
        "title": "Exploring Randomly Wired Neural Networks for Image Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Exploring_Randomly_Wired_Neural_Networks_for_Image_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Exploring_Randomly_Wired_Neural_Networks_for_Image_Recognition_ICCV_2019_paper.html",
        "abstract": "Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.",
        "中文标题": "探索随机连线神经网络用于图像识别",
        "摘要翻译": "用于图像识别的神经网络通过广泛的手动设计从简单的链状模型发展到具有多种连线路径的结构。ResNets和DenseNets的成功很大程度上归功于它们创新的连线方案。现在，神经架构搜索（NAS）研究正在探索连线和操作类型的联合优化，然而，尽管进行了搜索，可能的连线空间仍然受到限制，并且仍然由手动设计驱动。在本文中，我们通过随机连线神经网络的视角探索了更多样化的连接模式。为此，我们首先定义了一个随机网络生成器的概念，它封装了整个网络生成过程。封装提供了NAS和随机连线网络的统一视图。然后，我们使用三种经典的随机图模型来生成随机连线的网络图。结果令人惊讶：这些随机生成器的几种变体产生的网络实例在ImageNet基准测试中具有竞争力的准确性。这些结果表明，通过探索限制较少、有更多新颖设计空间的搜索空间，专注于设计更好的网络生成器的新努力可能会带来新的突破。",
        "领域": "神经网络架构/图像识别/随机图模型",
        "问题": "探索和优化神经网络中的连接模式以提高图像识别的准确性",
        "动机": "尽管神经架构搜索（NAS）在优化神经网络结构方面取得了一定进展，但现有的连线方案仍然受到手动设计的限制，探索更自由、更多样化的连接模式可能带来新的突破",
        "方法": "定义了一个随机网络生成器的概念，使用三种经典的随机图模型生成随机连线的网络图，并评估这些网络在ImageNet基准测试上的性能",
        "关键词": [
            "随机连线神经网络",
            "神经架构搜索",
            "随机图模型",
            "ImageNet"
        ],
        "涉及的技术概念": "随机网络生成器：一个封装了整个网络生成过程的概念，用于生成具有随机连接模式的神经网络。随机图模型：用于生成随机连线的网络图的数学模型，包括三种经典模型。ImageNet基准测试：一个广泛使用的图像识别性能评估标准。"
    },
    {
        "order": 424,
        "title": "A Novel Unsupervised Camera-Aware Domain Adaptation Framework for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qi_A_Novel_Unsupervised_Camera-Aware_Domain_Adaptation_Framework_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Unsupervised cross-domain person re-identification (Re-ID) faces two key issues. One is the data distribution discrepancy between source and target domains, and the other is the lack of discriminative information in target domain. From the perspective of representation learning, this paper proposes a novel end-to-end deep domain adaptation framework to address them. For the first issue, we highlight the presence of camera-level sub-domains as a unique characteristic in person Re-ID, and develop a \"camera-aware\" domain adaptation method via adversarial learning. With this method, the learned representation reduces distribution discrepancy not only between source and target domains but also across all cameras. For the second issue, we exploit the temporal continuity in each camera of target domain to create discriminative information. This is implemented by dynamically generating online triplets within each batch, in order to maximally take advantage of the steadily improved representation in training process. Together, the above two methods give rise to a new unsupervised domain adaptation framework for person Re-ID. Extensive experiments and ablation studies conducted on benchmark datasets demonstrate its superiority and interesting properties.",
        "中文标题": "一种新颖的无监督相机感知域适应框架用于行人重识别",
        "摘要翻译": "无监督跨域行人重识别（Re-ID）面临两个关键问题。一个是源域和目标域之间的数据分布差异，另一个是目标域中缺乏区分性信息。从表示学习的角度出发，本文提出了一种新颖的端到端深度域适应框架来解决这些问题。对于第一个问题，我们强调了相机级别子域的存在作为行人重识别中的一个独特特征，并通过对抗学习开发了一种“相机感知”域适应方法。通过这种方法，学习到的表示不仅减少了源域和目标域之间的分布差异，还减少了所有相机之间的分布差异。对于第二个问题，我们利用目标域中每个相机的时间连续性来创建区分性信息。这是通过在每个批次内动态生成在线三元组来实现的，以便最大限度地利用训练过程中稳步改进的表示。上述两种方法共同构成了一种新的无监督域适应框架用于行人重识别。在基准数据集上进行的大量实验和消融研究证明了其优越性和有趣的性质。",
        "领域": "行人重识别/域适应/对抗学习",
        "问题": "解决无监督跨域行人重识别中的数据分布差异和目标域中缺乏区分性信息的问题",
        "动机": "从表示学习的角度出发，提出一种端到端的深度域适应框架，以解决行人重识别中的关键问题",
        "方法": "通过对抗学习开发相机感知域适应方法减少分布差异，并利用目标域中每个相机的时间连续性动态生成在线三元组以创建区分性信息",
        "关键词": [
            "行人重识别",
            "域适应",
            "对抗学习",
            "时间连续性",
            "在线三元组"
        ],
        "涉及的技术概念": {
            "无监督跨域行人重识别": "指在没有标签数据的情况下，将在一个域（源域）上训练的模型应用到另一个域（目标域）上的任务",
            "数据分布差异": "源域和目标域之间的数据分布不同，导致模型在目标域上的性能下降",
            "对抗学习": "一种通过让两个模型相互对抗来学习特征表示的方法，常用于减少不同域之间的分布差异",
            "相机级别子域": "在行人重识别中，不同相机捕获的图像可能形成不同的子域，这些子域之间的差异也是需要解决的问题",
            "时间连续性": "指在视频序列中，相邻帧之间的内容变化是连续的，可以用来生成更有区分性的训练样本",
            "在线三元组": "一种动态生成训练样本的方法，通过选择正样本和负样本来增强模型的区分能力"
        }
    },
    {
        "order": 425,
        "title": "Fully Convolutional Pixel Adaptive Image Denoiser",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cha_Fully_Convolutional_Pixel_Adaptive_Image_Denoiser_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cha_Fully_Convolutional_Pixel_Adaptive_Image_Denoiser_ICCV_2019_paper.html",
        "abstract": "We propose a new image denoising algorithm, dubbed as Fully Convolutional Adaptive Image DEnoiser (FC-AIDE), that can learn from an offline supervised training set with a fully convolutional neural network as well as adaptively fine-tune the supervised model for each given noisy image. We significantly extend the framework of the recently proposed Neural AIDE, which formulates the denoiser to be context-based pixelwise mappings and utilizes the unbiased estimator of MSE for such denoisers. The two main contributions we make are; 1) implementing a novel fully convolutional architecture that boosts the base supervised model, and 2) introducing regularization methods for the adaptive fine-tuning such that a stronger and more robust adaptivity can be attained. As a result, FC-AIDE is shown to possess many desirable features; it outperforms the recent CNN-based state-of-the-art denoisers on all of the benchmark datasets we tested, and gets particularly strong for various challenging scenarios, e.g., with mismatched image/noise characteristics or with scarce supervised training data. The source code our algorithm is available at  https://github.com/csm9493/FC-AIDE-Keras  https://github.com/csm9493/FC-AIDE-Keras .",
        "中文标题": "全卷积像素自适应图像去噪器",
        "摘要翻译": "我们提出了一种新的图像去噪算法，称为全卷积自适应图像去噪器（FC-AIDE），它可以从离线监督训练集中学习，使用全卷积神经网络，并且能够自适应地为每个给定的噪声图像微调监督模型。我们显著扩展了最近提出的Neural AIDE框架，该框架将去噪器公式化为基于上下文的像素级映射，并利用此类去噪器的MSE无偏估计器。我们的两个主要贡献是：1）实现了一种新颖的全卷积架构，增强了基础监督模型，2）引入了自适应微调的正则化方法，以实现更强和更鲁棒的自适应性。结果表明，FC-AIDE具有许多理想特性；在我们测试的所有基准数据集上，它都优于最近的基于CNN的最先进去噪器，并且在各种挑战性场景下表现尤为强劲，例如，图像/噪声特征不匹配或监督训练数据稀缺的情况。我们的算法源代码可在https://github.com/csm9493/FC-AIDE-Keras获取。",
        "领域": "图像去噪/卷积神经网络/自适应学习",
        "问题": "图像去噪",
        "动机": "提高图像去噪的效率和效果，特别是在图像/噪声特征不匹配或监督训练数据稀缺的情况下。",
        "方法": "实现了一种新颖的全卷积架构，并引入了自适应微调的正则化方法。",
        "关键词": [
            "图像去噪",
            "全卷积神经网络",
            "自适应学习"
        ],
        "涉及的技术概念": "全卷积神经网络（FCN）是一种特殊的卷积神经网络，它能够接受任意大小的输入图像，并输出相同大小的图像。自适应学习指的是模型能够根据输入数据的特点自动调整其参数，以提高处理效果。正则化方法用于防止模型过拟合，通过引入额外的约束来限制模型的复杂度。"
    },
    {
        "order": 426,
        "title": "Deep Contextual Attention for Human-Object Interaction Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Contextual_Attention_for_Human-Object_Interaction_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Contextual_Attention_for_Human-Object_Interaction_Detection_ICCV_2019_paper.html",
        "abstract": "Human-object interaction detection is an important and relatively new class of visual relationship detection tasks, essential for deeper scene understanding. Most existing approaches decompose the problem into object localization and interaction recognition. Despite showing progress, these approaches only rely on the appearances of humans and objects and overlook the available context information, crucial for capturing subtle interactions between them. We propose a contextual attention framework for human-object interaction detection. Our approach leverages context by learning contextually-aware appearance features for human and object instances. The proposed attention module then adaptively selects relevant instance-centric context information to highlight image regions likely to contain human-object interactions. Experiments are performed on three benchmarks: V-COCO, HICO-DET and HCVRD. Our approach outperforms the state-of-the-art on all datasets. On the V-COCO dataset, our method achieves a relative gain of 4.4% in terms of role mean average precision (mAP role ), compared to the existing best approach.",
        "中文标题": "深度上下文注意力用于人-物交互检测",
        "摘要翻译": "人-物交互检测是视觉关系检测任务中一个重要且相对较新的类别，对于深入理解场景至关重要。大多数现有方法将问题分解为对象定位和交互识别。尽管显示出进展，这些方法仅依赖于人和对象的外观，忽视了可用的上下文信息，这对于捕捉它们之间的微妙交互至关重要。我们提出了一个上下文注意力框架用于人-物交互检测。我们的方法通过学习上下文感知的外观特征来利用上下文信息。提出的注意力模块然后自适应地选择相关的实例中心上下文信息，以突出可能包含人-物交互的图像区域。实验在三个基准上进行：V-COCO、HICO-DET和HCVRD。我们的方法在所有数据集上都优于现有技术。在V-COCO数据集上，与现有最佳方法相比，我们的方法在角色平均精度（mAP角色）方面实现了4.4%的相对增益。",
        "领域": "视觉关系检测/场景理解/上下文感知",
        "问题": "人-物交互检测中的上下文信息利用不足",
        "动机": "为了更深入地理解场景，捕捉人-物之间的微妙交互，需要更好地利用上下文信息。",
        "方法": "提出了一个上下文注意力框架，通过学习上下文感知的外观特征，并自适应地选择相关的实例中心上下文信息来突出可能包含人-物交互的图像区域。",
        "关键词": [
            "人-物交互检测",
            "上下文注意力",
            "视觉关系检测"
        ],
        "涉及的技术概念": "上下文注意力框架是一种通过学习上下文感知的外观特征来利用上下文信息的方法，它能够自适应地选择相关的实例中心上下文信息，以突出可能包含人-物交互的图像区域。"
    },
    {
        "order": 427,
        "title": "Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Progressive_Differentiable_Architecture_Search_Bridging_the_Depth_Gap_Between_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Progressive_Differentiable_Architecture_Search_Bridging_the_Depth_Gap_Between_Search_ICCV_2019_paper.html",
        "abstract": "Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time ( 7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts",
        "中文标题": "渐进式可微分架构搜索：弥合搜索与评估之间的深度差距",
        "摘要翻译": "最近，可微分搜索方法在减少神经架构搜索的计算成本方面取得了重大进展。然而，这些方法在评估搜索到的架构或将其转移到另一个数据集时，往往报告较低的准确性。这可能是由于搜索和评估场景中架构深度之间的巨大差距所致。在本文中，我们提出了一种有效的算法，该算法允许搜索到的架构的深度在训练过程中逐渐增长。这带来了两个问题，即更重的计算开销和更弱的搜索稳定性，我们分别通过搜索空间近似和正则化来解决。在显著减少的搜索时间（单GPU上7小时）内，我们的方法在代理数据集（CIFAR10或CIFAR100）和目标数据集（ImageNet）上都达到了最先进的性能。代码可在https://github.com/chenxin061/pdarts获取。",
        "领域": "神经架构搜索/深度学习优化/自动化机器学习",
        "问题": "解决在神经架构搜索中，搜索和评估场景中架构深度差距导致评估准确性降低的问题",
        "动机": "为了减少神经架构搜索的计算成本并提高搜索架构的评估准确性",
        "方法": "提出一种渐进式可微分架构搜索算法，通过搜索空间近似和正则化解决计算开销和搜索稳定性问题",
        "关键词": [
            "神经架构搜索",
            "可微分搜索",
            "渐进式训练"
        ],
        "涉及的技术概念": "可微分搜索方法允许在搜索过程中直接优化架构参数，而无需离散搜索。渐进式训练指的是在训练过程中逐渐增加模型的复杂度或深度，以平衡训练效率和模型性能。搜索空间近似和正则化是用于减少计算开销和提高搜索稳定性的技术。"
    },
    {
        "order": 428,
        "title": "Coherent Semantic Attention for Image Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Coherent_Semantic_Attention_for_Image_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Coherent_Semantic_Attention_for_Image_Inpainting_ICCV_2019_paper.html",
        "abstract": "The latest deep learning-based approaches have shown promising results for the challenging task of inpainting missing regions of an image. However, the existing methods often generate contents with blurry textures and distorted structures due to the discontinuity of the local pixels. From a semantic-level perspective, the local pixel discontinuity is mainly because these methods ignore the semantic relevance and feature continuity of hole regions. To handle this problem, we investigate the human behavior in repairing pictures and propose a fined deep generative model-based approach with a novel coherent semantic attention (CSA) layer, which can not only preserve contextual structure but also make more effective predictions of missing parts by modeling the semantic relevance between the holes features. The task is divided into rough, refinement as two steps and we model each step with a neural network under the U-Net architecture, where the CSA layer is embedded into the encoder of refinement step. Meanwhile, we further propose consistency loss and feature patch discriminator to stabilize the network training process and improve the details. The experiments on CelebA, Places2, and Paris StreetView datasets have validated the effectiveness of our proposed methods in image inpainting tasks and can obtain images with a higher quality as compared with the existing state-of-the-art approaches. The codes and pre-trained models will be available at https://github.com/KumapowerLIU/CSA-inpainting.",
        "中文标题": "图像修复中的连贯语义注意力",
        "摘要翻译": "最新的基于深度学习的方法在修复图像缺失区域这一挑战性任务中显示出了有希望的结果。然而，现有方法由于局部像素的不连续性，常常生成具有模糊纹理和扭曲结构的内容。从语义层面的角度来看，局部像素的不连续性主要是因为这些方法忽略了孔区域的语义相关性和特征连续性。为了解决这个问题，我们研究了人类修复图片的行为，并提出了一种基于精细深度生成模型的方法，该方法具有新颖的连贯语义注意力（CSA）层，不仅能够保留上下文结构，还能通过建模孔特征之间的语义相关性来更有效地预测缺失部分。该任务分为粗略和细化两个步骤，我们在U-Net架构下用神经网络建模每个步骤，其中CSA层嵌入到细化步骤的编码器中。同时，我们进一步提出了一致性损失和特征补丁判别器，以稳定网络训练过程并改善细节。在CelebA、Places2和Paris StreetView数据集上的实验验证了我们提出的方法在图像修复任务中的有效性，并且与现有的最先进方法相比，可以获得更高质量的图像。代码和预训练模型将在https://github.com/KumapowerLIU/CSA-inpainting上提供。",
        "领域": "图像修复/生成模型/注意力机制",
        "问题": "现有图像修复方法生成的图像存在模糊纹理和扭曲结构的问题",
        "动机": "为了解决现有方法忽略孔区域语义相关性和特征连续性的问题",
        "方法": "提出了一种基于精细深度生成模型的方法，包含连贯语义注意力（CSA）层，分为粗略和细化两个步骤，并在细化步骤的编码器中嵌入CSA层，同时提出一致性损失和特征补丁判别器以稳定训练和改善细节",
        "关键词": [
            "图像修复",
            "生成模型",
            "注意力机制"
        ],
        "涉及的技术概念": "连贯语义注意力（CSA）层是一种新颖的注意力机制，用于在图像修复任务中建模孔特征之间的语义相关性，以保留上下文结构并有效预测缺失部分。U-Net架构是一种常用于图像分割和修复任务的卷积神经网络架构。一致性损失和特征补丁判别器是用于稳定网络训练过程和改善生成图像细节的技术。"
    },
    {
        "order": 429,
        "title": "Cross-View Policy Learning for Street Navigation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Cross-View_Policy_Learning_for_Street_Navigation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Cross-View_Policy_Learning_for_Street_Navigation_ICCV_2019_paper.html",
        "abstract": "The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to transfer a ground view policy, from training areas to unseen (target) parts of the city, by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Our experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to train a multi-modal policy on ground and aerial views, then transfer the ground view policy to unseen (target) parts of the city by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.",
        "中文标题": "跨视角策略学习用于街道导航",
        "摘要翻译": "在陌生环境中从视觉观察进行导航的能力是智能代理的核心组成部分，也是深度强化学习（RL）面临的持续挑战。街景可以成为此类RL代理的合理测试平台，因为它提供了地面级别的真实世界摄影图像，具有多样化的街道外观；它已被制作成一个名为StreetLearn的互动环境，并用于导航研究。然而，迄今为止，目标驱动的街道导航代理无法在没有大量重新训练的情况下转移到未见过的区域，依赖模拟并不是一个可扩展的解决方案。由于航空图像易于全球访问，我们提出通过利用航空视角观察，将地面视角策略从训练区域转移到城市的未见（目标）部分。我们的核心思想是将地面视角与航空视角配对，并学习一个可跨视角转移的联合策略。我们通过为两个视角学习相似的嵌入空间、跨视角提炼策略和丢弃视觉模态来实现这一点。我们进一步将转移学习范式重新表述为三个阶段：1）跨模态训练，当代理最初在多个城市区域训练时，2）仅航空视角适应新区域，当代理仅使用易于获得的航空视角适应保留区域时，3）仅地面视角转移，当代理在没有航空图像的情况下在未见过的地面视角上测试导航任务时。我们的实验结果表明，所提出的跨视角策略学习使代理具有更好的泛化能力，并允许更有效地转移到未见过的环境。",
        "领域": "强化学习/导航系统/多模态学习",
        "问题": "目标驱动的街道导航代理无法在没有大量重新训练的情况下转移到未见过的区域",
        "动机": "依赖模拟并不是一个可扩展的解决方案，航空图像易于全球访问，可以用于改进导航策略的转移",
        "方法": "通过将地面视角与航空视角配对，学习一个可跨视角转移的联合策略，包括学习相似的嵌入空间、跨视角提炼策略和丢弃视觉模态，并将转移学习范式重新表述为三个阶段",
        "关键词": [
            "跨视角策略学习",
            "街道导航",
            "强化学习",
            "多模态学习",
            "转移学习"
        ],
        "涉及的技术概念": "深度强化学习（RL）、嵌入空间、视觉模态、跨模态训练、航空视角、地面视角、策略提炼"
    },
    {
        "order": 430,
        "title": "Learning Compositional Neural Information Fusion for Human Parsing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Compositional_Neural_Information_Fusion_for_Human_Parsing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Compositional_Neural_Information_Fusion_for_Human_Parsing_ICCV_2019_paper.html",
        "abstract": "This work proposes to combine neural networks with the compositional hierarchy of human bodies for efficient and complete human parsing. We formulate the approach as a neural information fusion framework. Our model assembles the information from three inference processes over the hierarchy: direct inference (directly predicting each part of a human body using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). The bottom-up and top-down inferences explicitly model the compositional and decompositional relations in human bodies, respectively. In addition, the fusion of multi-source information is conditioned on the inputs, i.e., by estimating and considering the confidence of the sources. The whole model is end-to-end differentiable, explicitly modeling information flows and structures. Our approach is extensively evaluated on four popular datasets, outperforming the state-of-the-arts in all cases, with a fast processing speed of 23fps. Our code and results have been released to help ease future research in this direction.",
        "中文标题": "学习组合神经信息融合以进行人体解析",
        "摘要翻译": "本工作提出将神经网络与人体组合层次结构相结合，以实现高效且完整的人体解析。我们将该方法表述为一个神经信息融合框架。我们的模型从层次结构上的三个推理过程中组装信息：直接推理（直接使用图像信息预测人体的每个部分）、自下而上推理（从组成部分组装知识）和自上而下推理（利用父节点的上下文）。自下而上和自上而下的推理分别明确地模拟了人体中的组合和分解关系。此外，多源信息的融合是基于输入进行的，即通过估计和考虑源的置信度。整个模型是端到端可微的，明确地模拟了信息流和结构。我们的方法在四个流行数据集上进行了广泛评估，在所有情况下都优于最先进的技术，处理速度达到23fps。我们的代码和结果已经发布，以帮助简化这一方向的未来研究。",
        "领域": "人体解析/神经信息融合/组合层次结构",
        "问题": "如何高效且完整地进行人体解析",
        "动机": "结合神经网络与人体组合层次结构，以提高人体解析的效率和完整性",
        "方法": "提出一个神经信息融合框架，通过直接推理、自下而上推理和自上而下推理三个过程组装信息，并基于输入进行多源信息融合",
        "关键词": [
            "人体解析",
            "神经信息融合",
            "组合层次结构",
            "直接推理",
            "自下而上推理",
            "自上而下推理"
        ],
        "涉及的技术概念": "神经信息融合框架、直接推理、自下而上推理、自上而下推理、组合和分解关系、多源信息融合、端到端可微模型"
    },
    {
        "order": 431,
        "title": "Multinomial Distribution Learning for Effective Neural Architecture Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_Multinomial_Distribution_Learning_for_Effective_Neural_Architecture_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_Multinomial_Distribution_Learning_for_Effective_Neural_Architecture_Search_ICCV_2019_paper.html",
        "abstract": "Architectures obtained by Neural Architecture Search (NAS) have achieved highly competitive performance in various computer vision tasks. However, the prohibitive computation demand of forward-backward propagation in deep neural networks and searching algorithms makes it difficult to apply NAS in practice. In this paper, we propose a Multinomial Distribution Learning for extremely effective NAS, which considers the search space as a joint multinomial distribution, i.e., the operation between two nodes is sampled from this distribution, and the optimal network structure is obtained by the operations with the most likely probability in this distribution. Therefore, NAS can be transformed to a multinomial distribution learning problem, i.e., the distribution is optimized to have a high expectation of the performance. Besides, a hypothesis that the performance ranking is consistent in every training epoch is proposed and demonstrated to further accelerate the learning process. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of our method. On CIFAR-10, the structure searched by our method achieves 2.55% test error, while being 6.0x (only 4 GPU hours on GTX1080Ti) faster compared with state-of-the-art NAS algorithms. On ImageNet, our model achieves 74% top1 accuracy under MobileNet settings (MobileNet V1/V2), while being 1.2x faster with measured GPU latency. Test code with pre-trained models are available at https: //github.com/tanglang96/MDENAS",
        "中文标题": "多项式分布学习用于有效的神经架构搜索",
        "摘要翻译": "通过神经架构搜索（NAS）获得的架构在各种计算机视觉任务中实现了极具竞争力的性能。然而，深度神经网络中的前向-后向传播和搜索算法的计算需求巨大，使得NAS难以实际应用。在本文中，我们提出了一种用于极其有效的NAS的多项式分布学习方法，该方法将搜索空间视为联合多项式分布，即两个节点之间的操作是从该分布中采样的，而最优网络结构是通过该分布中最可能概率的操作获得的。因此，NAS可以转化为多项式分布学习问题，即优化分布以具有高性能的期望。此外，提出了一个假设，即性能排名在每个训练周期中是一致的，并证明了这一点以进一步加速学习过程。在CIFAR-10和ImageNet上的实验证明了我们方法的有效性。在CIFAR-10上，我们的方法搜索的结构实现了2.55%的测试错误率，同时比最先进的NAS算法快6.0倍（在GTX1080Ti上仅需4 GPU小时）。在ImageNet上，我们的模型在MobileNet设置（MobileNet V1/V2）下实现了74%的top1准确率，同时GPU延迟测量快了1.2倍。测试代码和预训练模型可在https://github.com/tanglang96/MDENAS获取。",
        "领域": "神经架构搜索/深度学习优化/自动化机器学习",
        "问题": "神经架构搜索（NAS）在实际应用中面临的高计算需求问题",
        "动机": "为了降低NAS的计算成本，提高其在实际应用中的可行性",
        "方法": "提出了一种将搜索空间视为联合多项式分布的方法，通过优化分布以期望高性能，并假设性能排名在每个训练周期中一致以加速学习过程",
        "关键词": [
            "神经架构搜索",
            "多项式分布学习",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "神经架构搜索（NAS）": "一种自动化设计神经网络架构的技术，旨在找到最优的网络结构以提高特定任务的性能。",
            "多项式分布学习": "一种统计学习方法，用于优化操作选择的概率分布，以期望获得高性能的网络结构。",
            "前向-后向传播": "深度神经网络训练过程中的两个主要步骤，前向传播用于计算输出，后向传播用于计算梯度并更新网络权重。",
            "CIFAR-10": "一个常用的图像分类数据集，包含10个类别的60000张32x32彩色图像。",
            "ImageNet": "一个大规模视觉识别挑战赛使用的数据集，包含超过1400万张标注图像，涵盖1000个类别。",
            "MobileNet": "一种轻量级的深度神经网络架构，专为移动和嵌入式视觉应用设计。"
        }
    },
    {
        "order": 432,
        "title": "Embedded Block Residual Network: A Recursive Restoration Model for Single-Image Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Embedded_Block_Residual_Network_A_Recursive_Restoration_Model_for_Single-Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Embedded_Block_Residual_Network_A_Recursive_Restoration_Model_for_Single-Image_ICCV_2019_paper.html",
        "abstract": "Single-image super-resolution restores the lost structures and textures from low-resolved images, which has achieved extensive attention from the research community. The top performers in this field include deep or wide convolutional neural networks, or recurrent neural networks. However, the methods enforce a single model to process all kinds of textures and structures. A typical operation is that a certain layer restores the textures based on the ones recovered by the preceding layers, ignoring the characteristics of image textures. In this paper, we believe that the lower-frequency and higher-frequency information in images have different levels of complexity and should be restored by models of different representational capacity. Inspired by this, we propose a novel embedded block residual network (EBRN) which is an incremental recovering progress for texture super-resolution. Specifically, different modules in the model restores information of different frequencies. For lower-frequency information, we use shallower modules of the network to recover; for higher-frequency information, we use deeper modules to restore. Extensive experiments indicate that the proposed EBRN model achieves superior performance and visual improvements against the state-of-the-arts.",
        "中文标题": "嵌入式块残差网络：一种用于单图像超分辨率的递归恢复模型",
        "摘要翻译": "单图像超分辨率从低分辨率图像中恢复丢失的结构和纹理，这一领域已经引起了研究界的广泛关注。该领域的顶尖表现者包括深度或宽度卷积神经网络，或循环神经网络。然而，这些方法强制单一模型处理所有类型的纹理和结构。一个典型的操作是某一层基于前几层恢复的纹理来恢复纹理，忽略了图像纹理的特性。在本文中，我们认为图像中的低频和高频信息具有不同层次的复杂性，应该由具有不同表示能力的模型来恢复。受此启发，我们提出了一种新颖的嵌入式块残差网络（EBRN），这是一种用于纹理超分辨率的增量恢复过程。具体来说，模型中的不同模块恢复不同频率的信息。对于低频信息，我们使用网络的较浅模块来恢复；对于高频信息，我们使用较深的模块来恢复。大量实验表明，所提出的EBRN模型在性能和视觉改进方面均优于现有技术。",
        "领域": "超分辨率/图像恢复/深度学习",
        "问题": "单图像超分辨率中，如何有效恢复图像的低频和高频信息",
        "动机": "现有方法强制单一模型处理所有类型的纹理和结构，忽略了图像纹理的特性，导致恢复效果不佳",
        "方法": "提出嵌入式块残差网络（EBRN），通过不同模块分别恢复图像的低频和高频信息，实现增量恢复过程",
        "关键词": [
            "超分辨率",
            "图像恢复",
            "残差网络",
            "深度学习"
        ],
        "涉及的技术概念": "嵌入式块残差网络（EBRN）是一种用于单图像超分辨率的递归恢复模型，通过不同模块分别处理图像的低频和高频信息，实现更有效的图像恢复。"
    },
    {
        "order": 433,
        "title": "Attentional Neural Fields for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Attentional_Neural_Fields_for_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Attentional_Neural_Fields_for_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "Crowd counting has recently generated huge popularity in computer vision, and is extremely challenging due to the huge scale variations of objects. In this paper, we propose the Attentional Neural Field (ANF) for crowd counting via density estimation. Within the encoder-decoder network, we introduce conditional random fields (CRFs) to aggregate multi-scale features, which can build more informative representations. To better model pair-wise potentials in CRFs, we incorperate non-local attention mechanism implemented as inter- and intra-layer attentions to expand the receptive field to the entire image respectively within the same layer and across different layers, which captures long-range dependencies to conquer huge scale variations. The CRFs coupled with the attention mechanism are seamlessly integrated into the encoder-decoder network, establishing an ANF that can be optimized end-to-end by back propagation. We conduct extensive experiments on four public datasets, including ShanghaiTech, WorldEXPO 10, UCF-CC-50 and UCF-QNRF. The results show that our ANF achieves high counting performance, surpassing most previous methods.",
        "中文标题": "注意力神经场用于人群计数",
        "摘要翻译": "人群计数最近在计算机视觉领域引起了极大的关注，由于对象的巨大尺度变化，这一任务极具挑战性。在本文中，我们提出了通过密度估计进行人群计数的注意力神经场（ANF）。在编码器-解码器网络中，我们引入了条件随机场（CRFs）来聚合多尺度特征，这可以构建更具信息量的表示。为了更好地建模CRFs中的成对潜力，我们结合了非局部注意力机制，实现为层内和层间注意力，以分别在相同层内和跨不同层扩展感受野到整个图像，这捕捉了长距离依赖以克服巨大的尺度变化。CRFs与注意力机制无缝集成到编码器-解码器网络中，建立了一个可以通过反向传播端到端优化的ANF。我们在四个公共数据集上进行了广泛的实验，包括ShanghaiTech、WorldEXPO 10、UCF-CC-50和UCF-QNRF。结果表明，我们的ANF实现了高计数性能，超越了大多数先前的方法。",
        "领域": "人群计数/密度估计/注意力机制",
        "问题": "解决人群计数中由于对象尺度巨大变化带来的挑战",
        "动机": "提高人群计数的准确性和效率，克服尺度变化带来的问题",
        "方法": "提出注意力神经场（ANF），结合条件随机场（CRFs）和非局部注意力机制，通过编码器-解码器网络进行端到端优化",
        "关键词": [
            "人群计数",
            "密度估计",
            "注意力机制",
            "条件随机场",
            "编码器-解码器网络"
        ],
        "涉及的技术概念": {
            "条件随机场（CRFs）": "用于聚合多尺度特征，构建更具信息量的表示",
            "非局部注意力机制": "实现为层内和层间注意力，扩展感受野到整个图像，捕捉长距离依赖",
            "编码器-解码器网络": "用于实现端到端的优化"
        }
    },
    {
        "order": 434,
        "title": "Learning Across Tasks and Domains",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ramirez_Learning_Across_Tasks_and_Domains_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ramirez_Learning_Across_Tasks_and_Domains_ICCV_2019_paper.html",
        "abstract": "Recent works have proven that many relevant visual tasks are closely related one to another. Yet, this connection is seldom deployed in practice due to the lack of practical methodologies to transfer learned concepts across different training processes. In this work, we introduce a novel adaptation framework that can operate across both task and domains. Our framework learns to transfer knowledge across tasks in a fully supervised domain (e.g., synthetic data) and use this knowledge on a different domain where we have only partial supervision (e.g., real data). Our proposal is complementary to existing domain adaptation techniques and extends them to cross tasks scenarios providing additional performance gains. We prove the effectiveness of our framework across two challenging tasks (i.e., monocular depth estimation and semantic segmentation) and four different domains (Synthia, Carla, Kitti, and Cityscapes).",
        "中文标题": "跨任务和领域的学习",
        "摘要翻译": "最近的研究证明了许多相关的视觉任务彼此之间密切相关。然而，由于缺乏在不同训练过程之间转移学习概念的实用方法，这种联系在实践中很少被利用。在这项工作中，我们引入了一种新颖的适应框架，该框架能够在任务和领域之间操作。我们的框架学会了在完全监督的领域（例如，合成数据）中跨任务转移知识，并在我们只有部分监督的不同领域（例如，真实数据）中使用这些知识。我们的提议是对现有领域适应技术的补充，并将它们扩展到跨任务场景，提供额外的性能增益。我们在两个具有挑战性的任务（即单目深度估计和语义分割）和四个不同领域（Synthia、Carla、Kitti和Cityscapes）上证明了我们框架的有效性。",
        "领域": "单目深度估计/语义分割/领域适应",
        "问题": "如何在不同任务和领域之间有效地转移学习概念",
        "动机": "尽管许多视觉任务之间存在密切联系，但由于缺乏实用的方法，这种联系在实践中很少被利用。",
        "方法": "引入了一种新颖的适应框架，能够在完全监督的领域学习跨任务知识，并在部分监督的领域应用这些知识。",
        "关键词": [
            "单目深度估计",
            "语义分割",
            "领域适应"
        ],
        "涉及的技术概念": "领域适应技术、跨任务知识转移、完全监督学习、部分监督学习"
    },
    {
        "order": 435,
        "title": "Searching for MobileNetV3",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.html",
        "abstract": "We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",
        "中文标题": "寻找MobileNetV3",
        "摘要翻译": "我们展示了基于互补搜索技术和新颖架构设计的下一代MobileNets。MobileNetV3通过结合硬件感知的网络架构搜索（NAS）和NetAdapt算法，随后通过新颖的架构进步进行改进，从而针对手机CPU进行了优化。本文开始探索自动化搜索算法和网络设计如何协同工作，以利用互补的方法提高整体技术水平。通过这一过程，我们创建了两个新的MobileNet模型进行发布：MobileNetV3-Large和MobileNetV3-Small，它们分别针对高资源和低资源使用场景。这些模型随后被调整并应用于目标检测和语义分割任务。对于语义分割（或任何密集像素预测）任务，我们提出了一种新的高效分割解码器Lite Reduced Atrous Spatial Pyramid Pooling（LR-ASPP）。我们在移动分类、检测和分割方面取得了新的技术水平。与MobileNetV2相比，MobileNetV3-Large在ImageNet分类上准确率提高了3.2%，同时延迟减少了20%。MobileNetV3-Small在具有可比延迟的情况下，准确率比MobileNetV2模型提高了6.6%。在COCO检测上，MobileNetV3-Large检测速度比MobileNetV2快了25%以上，准确率大致相同。对于Cityscapes分割，MobileNetV3-Large LR-ASPP在相似准确率下比MobileNetV2 R-ASPP快了34%。",
        "领域": "网络架构搜索/移动设备优化/语义分割",
        "问题": "如何在移动设备上实现更高效的图像分类、目标检测和语义分割",
        "动机": "探索自动化搜索算法和网络设计如何协同工作，以提高移动设备上的图像处理任务的整体技术水平",
        "方法": "结合硬件感知的网络架构搜索（NAS）和NetAdapt算法，随后通过新颖的架构进步进行改进，提出新的高效分割解码器LR-ASPP",
        "关键词": [
            "网络架构搜索",
            "移动设备优化",
            "语义分割",
            "目标检测",
            "图像分类"
        ],
        "涉及的技术概念": {
            "硬件感知的网络架构搜索（NAS）": "一种自动化搜索技术，用于发现最优的网络架构，特别考虑硬件特性以优化性能",
            "NetAdapt算法": "一种用于网络架构优化的算法，通过逐步调整网络结构以适应特定的硬件约束",
            "Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP)": "一种新的高效分割解码器，用于语义分割任务，旨在提高处理速度和准确率"
        }
    },
    {
        "order": 436,
        "title": "Fast Image Restoration With Multi-Bin Trainable Linear Units",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Fast_Image_Restoration_With_Multi-Bin_Trainable_Linear_Units_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Fast_Image_Restoration_With_Multi-Bin_Trainable_Linear_Units_ICCV_2019_paper.html",
        "abstract": "Tremendous advances in image restoration tasks such as denoising and super-resolution have been achieved using neural networks. Such approaches generally employ very deep architectures, large number of parameters, large receptive fields and high nonlinear modeling capacity. In order to obtain efficient and fast image restoration networks one should improve upon the above mentioned requirements. In this paper we propose a novel activation function, the multi-bin trainable linear unit (MTLU), for increasing the nonlinear modeling capacity together with lighter and shallower networks. We validate the proposed fast image restoration networks for image denoising (FDnet) and super-resolution (FSRnet) on standard benchmarks. We achieve large improvements in both memory and runtime over current state-of-the-art for comparable or better PSNR accuracies.",
        "中文标题": "使用多箱可训练线性单元进行快速图像恢复",
        "摘要翻译": "在使用神经网络进行图像恢复任务（如去噪和超分辨率）方面取得了巨大进展。这类方法通常采用非常深的架构、大量参数、大感受野和高非线性建模能力。为了获得高效且快速的图像恢复网络，应该改进上述要求。在本文中，我们提出了一种新颖的激活函数——多箱可训练线性单元（MTLU），用于在更轻、更浅的网络中增加非线性建模能力。我们在标准基准上验证了所提出的快速图像恢复网络用于图像去噪（FDnet）和超分辨率（FSRnet）的效果。在可比或更好的PSNR精度下，我们在内存和运行时间上都取得了比当前最先进技术更大的改进。",
        "领域": "图像去噪/超分辨率/激活函数",
        "问题": "提高图像恢复网络的效率和速度",
        "动机": "为了在保持或提高图像恢复质量的同时，减少网络深度和参数数量，从而加快处理速度和降低内存消耗",
        "方法": "提出了一种新的激活函数——多箱可训练线性单元（MTLU），用于在更轻、更浅的网络中增加非线性建模能力",
        "关键词": [
            "图像去噪",
            "超分辨率",
            "激活函数"
        ],
        "涉及的技术概念": "多箱可训练线性单元（MTLU）是一种新颖的激活函数，旨在增加网络的非线性建模能力，同时保持网络的轻量和浅层结构。这种方法被应用于图像去噪（FDnet）和超分辨率（FSRnet）任务中，以验证其在提高处理速度和降低内存消耗方面的有效性。"
    },
    {
        "order": 437,
        "title": "EMPNet: Neural Localisation and Mapping Using Embedded Memory Points",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Avraham_EMPNet_Neural_Localisation_and_Mapping_Using_Embedded_Memory_Points_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Avraham_EMPNet_Neural_Localisation_and_Mapping_Using_Embedded_Memory_Points_ICCV_2019_paper.html",
        "abstract": "Continuously estimating an agent's state space and a representation of its surroundings has proven vital towards full autonomy. A shared common ground among systems which successfully achieve this feat is the integration of previously encountered observations into the current state being estimated. This necessitates the use of a memory module for incorporating previously visited states whilst simultaneously offering an internal representation of the observed environment. In this work we develop a memory module which contains rigidly aligned point-embeddings that represent a coherent scene structure acquired from an RGB-D sequence of observations. The point-embeddings are extracted using modern convolutional neural network architectures, and alignment is performed by computing a dense correspondence matrix between a new observation and the current embeddings residing in the memory module. The whole framework is end-to-end trainable, resulting in a recurrent joint optimisation of the point-embeddings contained in the memory. This process amplifies the shared information across states, providing increased robustness and accuracy. We show significant improvement of our method across a set of experiments performed on the synthetic VIZDoom environment and a real world Active Vision Dataset.",
        "中文标题": "EMPNet: 使用嵌入式记忆点的神经定位与映射",
        "摘要翻译": "持续估计代理的状态空间及其周围环境的表示已被证明对实现完全自主至关重要。成功实现这一目标的系统之间的一个共同点是，将之前遇到的观察结果整合到当前估计的状态中。这需要使用一个记忆模块来合并之前访问过的状态，同时提供观察到的环境的内部表示。在这项工作中，我们开发了一个记忆模块，该模块包含严格对齐的点嵌入，这些点嵌入表示从RGB-D观察序列中获得的连贯场景结构。点嵌入是使用现代卷积神经网络架构提取的，并通过计算新观察与记忆模块中当前嵌入之间的密集对应矩阵来执行对齐。整个框架是端到端可训练的，导致记忆中点嵌入的循环联合优化。这个过程增强了跨状态的共享信息，提供了更高的鲁棒性和准确性。我们在合成VIZDoom环境和真实世界Active Vision数据集上进行的一系列实验中展示了我们方法的显著改进。",
        "领域": "自主导航/场景理解/状态估计",
        "问题": "如何有效地整合和利用历史观察数据以持续估计代理的状态和周围环境的表示",
        "动机": "为了实现完全自主，需要一种能够有效整合历史观察数据并持续估计代理状态和周围环境表示的方法",
        "方法": "开发了一个包含严格对齐点嵌入的记忆模块，这些点嵌入表示从RGB-D观察序列中获得的连贯场景结构，并通过计算新观察与记忆模块中当前嵌入之间的密集对应矩阵来执行对齐，整个框架是端到端可训练的",
        "关键词": [
            "自主导航",
            "场景理解",
            "状态估计",
            "记忆模块",
            "点嵌入",
            "RGB-D序列"
        ],
        "涉及的技术概念": "记忆模块用于整合历史观察数据，点嵌入表示场景结构，密集对应矩阵用于新观察与记忆模块中嵌入的对齐，端到端训练框架用于优化记忆中的点嵌入"
    },
    {
        "order": 438,
        "title": "Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fan_Understanding_Human_Gaze_Communication_by_Spatio-Temporal_Graph_Reasoning_ICCV_2019_paper.html",
        "abstract": "This paper addresses a new problem of understanding human gaze communication in social videos from both atomic-level and event-level, which is significant for studying human social interactions. To tackle this novel and challenging problem, we contribute a large-scale video dataset, VACATION, which covers diverse daily social scenes and gaze communication behaviors with complete annotations of objects and human faces, human attention, and communication structures and labels in both atomic-level and event-level. Together with VACATION, we propose a spatio-temporal graph neural network to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing. We further propose an event network with encoder-decoder structure to predict the event-level gaze communication. Our experiments demonstrate that the proposed model improves various baselines significantly in predicting the atomic-level and event-level gaze communications.",
        "中文标题": "通过时空图推理理解人类视线交流",
        "摘要翻译": "本文探讨了一个新问题，即从原子级别和事件级别理解社交视频中的人类视线交流，这对于研究人类社交互动具有重要意义。为了解决这一新颖且具有挑战性的问题，我们贡献了一个大规模视频数据集VACATION，该数据集涵盖了多样化的日常社交场景和视线交流行为，并提供了对象和人脸、人类注意力以及原子级别和事件级别的交流结构和标签的完整注释。与VACATION一起，我们提出了一个时空图神经网络，以明确表示社交场景中的多样化视线互动，并通过消息传递推断原子级别的视线交流。我们进一步提出了一个具有编码器-解码器结构的事件网络，以预测事件级别的视线交流。我们的实验表明，所提出的模型在预测原子级别和事件级别的视线交流方面显著优于各种基线。",
        "领域": "社交行为分析/视线交流理解/视频理解",
        "问题": "理解社交视频中人类视线交流的原子级别和事件级别问题",
        "动机": "研究人类社交互动中的视线交流行为，以更好地理解社交互动",
        "方法": "提出了一个时空图神经网络和一个具有编码器-解码器结构的事件网络，分别用于推断原子级别和预测事件级别的视线交流",
        "关键词": [
            "视线交流",
            "社交视频",
            "时空图神经网络",
            "事件网络"
        ],
        "涉及的技术概念": {
            "时空图神经网络": "一种用于处理时空数据的神经网络，能够捕捉数据中的时间和空间关系",
            "编码器-解码器结构": "一种常见的神经网络架构，用于将输入数据编码成固定长度的向量，然后解码成所需的输出格式",
            "消息传递": "在图神经网络中，节点之间通过传递消息来更新彼此的状态，以捕捉图中的复杂关系"
        }
    },
    {
        "order": 439,
        "title": "Data-Free Quantization Through Weight Equalization and Bias Correction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nagel_Data-Free_Quantization_Through_Weight_Equalization_and_Bias_Correction_ICCV_2019_paper.html",
        "abstract": "We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.",
        "中文标题": "通过权重均衡和偏差校正实现无数据量化",
        "摘要翻译": "我们介绍了一种无需微调或超参数选择的深度神经网络无数据量化方法。该方法在常见的计算机视觉架构和任务上实现了接近原始模型的性能。8位定点量化对于在现代深度学习硬件上进行高效推理至关重要。然而，将模型量化为8位运行是一项非平凡的任务，常常导致显著的性能下降或花费大量工程时间训练网络以适应量化。我们的方法依赖于利用激活函数的尺度等变性特性来均衡网络中的权重范围。此外，该方法还校正了量化过程中引入的误差偏差。这提高了量化精度性能，并且可以通过简单的API调用应用于许多常见的计算机视觉架构。对于常见的架构，如MobileNet系列，我们实现了最先进的量化模型性能。我们进一步展示了该方法也适用于其他计算机视觉架构和任务，如语义分割和对象检测。",
        "领域": "模型量化/深度学习优化/计算机视觉",
        "问题": "深度神经网络在8位定点量化时性能下降或需要大量工程时间适应量化的问题",
        "动机": "实现无需微调或超参数选择的高效深度神经网络量化，以提升在现代深度学习硬件上的推理效率",
        "方法": "利用激活函数的尺度等变性特性均衡网络中的权重范围，并校正量化过程中引入的误差偏差",
        "关键词": [
            "模型量化",
            "权重均衡",
            "偏差校正",
            "8位定点量化",
            "深度学习优化"
        ],
        "涉及的技术概念": "8位定点量化是一种将深度学习模型的权重和激活值从浮点数转换为8位整数的技术，旨在减少模型大小和加速推理过程。权重均衡是通过调整网络中权重值的范围来减少量化误差的技术。偏差校正是在量化过程中识别并修正引入的系统误差，以提高量化模型的准确性。"
    },
    {
        "order": 440,
        "title": "AVT: Unsupervised Learning of Transformation Equivariant Representations by Autoencoding Variational Transformations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_AVT_Unsupervised_Learning_of_Transformation_Equivariant_Representations_by_Autoencoding_Variational_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qi_AVT_Unsupervised_Learning_of_Transformation_Equivariant_Representations_by_Autoencoding_Variational_ICCV_2019_paper.html",
        "abstract": "The learning of Transformation-Equivariant Representations (TERs), which is introduced by Hinton et al. [??], has been considered as a principle to reveal visual structures under various transformations. It contains the celebrated Convolutional Neural Networks (CNNs) as a special case that only equivary to the translations. In contrast, we seek to train TERs for a generic class of transformations and train them in an   unsupervised  fashion. To this end, we present a novel principled method by Autoencoding Variational Transformations (AVT), compared with the conventional approach to autoencoding data. Formally, given transformed images, the AVT seeks to train the networks by maximizing the mutual information between the transformations and representations. This ensures the resultant TERs of individual images contain the   intrinsic  information about their visual structures that would equivary   extricably  under various transformations in a generalized   nonlinear  case. Technically, we show that the resultant optimization problem can be efficiently solved by maximizing a variational lower-bound of the mutual information. This variational approach introduces a transformation decoder to approximate the intractable posterior of transformations, resulting in an autoencoding architecture with a pair of the representation encoder and the transformation decoder. Experiments demonstrate the proposed AVT model sets a new record for the performances on unsupervised tasks, greatly closing the performance gap to the supervised models.",
        "中文标题": "AVT：通过自动编码变分变换无监督学习变换等变表示",
        "摘要翻译": "变换等变表示（TERs）的学习，由Hinton等人[??]引入，已被视为揭示各种变换下视觉结构的原则。它包含了著名的卷积神经网络（CNNs）作为一个特例，仅对平移等变。相比之下，我们寻求为一般类别的变换训练TERs，并以无监督的方式进行训练。为此，我们提出了一种新的原则性方法，即通过自动编码变分变换（AVT），与传统的自动编码数据方法相比。形式上，给定变换后的图像，AVT试图通过最大化变换和表示之间的互信息来训练网络。这确保了单个图像的最终TERs包含关于其视觉结构的内在信息，这些信息在各种变换下在广义非线性情况下等变。技术上，我们展示了通过最大化互信息的变分下界，可以有效地解决由此产生的优化问题。这种变分方法引入了一个变换解码器来近似变换的难处理后验，从而形成了一个具有表示编码器和变换解码器对的自动编码架构。实验表明，所提出的AVT模型在无监督任务上的表现创下了新纪录，大大缩小了与监督模型的性能差距。",
        "领域": "变换等变表示学习/无监督学习/变分方法",
        "问题": "如何在无监督的情况下训练变换等变表示（TERs）以适应一般类别的变换",
        "动机": "揭示各种变换下视觉结构的内在信息，缩小无监督模型与监督模型之间的性能差距",
        "方法": "提出了一种新的原则性方法，即通过自动编码变分变换（AVT），通过最大化变换和表示之间的互信息来训练网络，并引入变换解码器来近似变换的难处理后验",
        "关键词": [
            "变换等变表示",
            "无监督学习",
            "变分方法",
            "自动编码",
            "互信息"
        ],
        "涉及的技术概念": "变换等变表示（TERs）是一种能够揭示在各种变换下视觉结构内在信息的表示方法。自动编码变分变换（AVT）是一种新的原则性方法，通过最大化变换和表示之间的互信息来训练网络，以适应一般类别的变换。这种方法引入了一个变换解码器来近似变换的难处理后验，从而形成了一个自动编码架构。"
    },
    {
        "order": 441,
        "title": "Counting With Focus for Free",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Counting_With_Focus_for_Free_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Counting_With_Focus_for_Free_ICCV_2019_paper.html",
        "abstract": "This paper aims to count arbitrary objects in images. The leading counting approaches start from point annotations per object from which they construct density maps. Then, their training objective transforms input images to density maps through deep convolutional networks. We posit that the point annotations serve more supervision purposes than just constructing density maps. We introduce ways to repurpose the points for free. First, we propose supervised focus from segmentation, where points are converted into binary maps. The binary maps are combined with a network branch and accompanying loss function to focus on areas of interest. Second, we propose supervised focus from global density, where the ratio of point annotations to image pixels is used in another branch to regularize the overall density estimation. To assist both the density estimation and the focus from segmentation, we also introduce an improved kernel size estimator for the point annotations. Experiments on six datasets show that all our contributions reduce the counting error, regardless of the base network, resulting in state-of-the-art accuracy using only a single network. Finally, we are the first to count on WIDER FACE, allowing us to show the benefits of our approach in handling varying object scales and crowding levels. Code is available at https://github.com/shizenglin/Counting-with-Focus-for-Free",
        "中文标题": "免费聚焦计数",
        "摘要翻译": "本文旨在对图像中的任意对象进行计数。领先的计数方法从每个对象的点注释开始，从中构建密度图。然后，它们的训练目标通过深度卷积网络将输入图像转换为密度图。我们认为点注释除了构建密度图外，还有更多的监督目的。我们介绍了重新利用这些点注释的方法。首先，我们提出了从分割中获得的监督聚焦，其中点被转换为二值图。这些二值图与网络分支和相应的损失函数结合，以聚焦于感兴趣的区域。其次，我们提出了从全局密度中获得的监督聚焦，其中点注释与图像像素的比例用于另一个分支，以正则化整体密度估计。为了辅助密度估计和从分割中获得的聚焦，我们还引入了一个改进的点注释核大小估计器。在六个数据集上的实验表明，无论基础网络如何，我们所有的贡献都减少了计数误差，仅使用单一网络就达到了最先进的准确性。最后，我们是第一个在WIDER FACE上进行计数的，使我们能够展示我们的方法在处理不同对象尺度和拥挤水平方面的优势。代码可在https://github.com/shizenglin/Counting-with-Focus-for-Free获取。",
        "领域": "对象计数/密度估计/图像分割",
        "问题": "如何更准确地计数图像中的任意对象",
        "动机": "现有的计数方法主要依赖于点注释来构建密度图，但这些点注释的潜力未被充分利用，我们希望探索如何更有效地利用这些点注释来提高计数准确性。",
        "方法": "提出了两种方法：一是从分割中获得的监督聚焦，将点注释转换为二值图并与网络分支结合；二是从全局密度中获得的监督聚焦，利用点注释与图像像素的比例来正则化密度估计。此外，还引入了一个改进的点注释核大小估计器。",
        "关键词": [
            "对象计数",
            "密度估计",
            "图像分割",
            "监督聚焦",
            "核大小估计"
        ],
        "涉及的技术概念": "点注释用于构建密度图，深度卷积网络用于图像到密度图的转换，二值图用于聚焦感兴趣区域，全局密度用于正则化密度估计，改进的核大小估计器用于辅助密度估计和分割聚焦。"
    },
    {
        "order": 442,
        "title": "Controllable Attention for Structured Layered Video Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Alayrac_Controllable_Attention_for_Structured_Layered_Video_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Alayrac_Controllable_Attention_for_Structured_Layered_Video_Decomposition_ICCV_2019_paper.html",
        "abstract": "The objective of this paper is to be able to separate a video into its natural layers, and to control which of the separated layers to attend to. For example, to be able to separate reflections, transparency or object motion. We make the following three contributions: (i) we introduce a new structured neural network architecture that explicitly incorporates layers (as spatial masks) into its design. This improves separation performance over previous general purpose networks for this task; (ii) we demonstrate that we can augment the architecture to leverage external cues such as audio for controllability and to help disambiguation; and (iii) we experimentally demonstrate the effectiveness of our approach and training procedure with controlled experiments while also showing that the proposed model can be successfully applied to real-word applications such as reflection removal and action recognition in cluttered scenes.",
        "中文标题": "可控注意力用于结构化分层视频分解",
        "摘要翻译": "本文的目标是能够将视频分离成其自然层次，并控制关注哪些分离的层次。例如，能够分离反射、透明度或物体运动。我们做出了以下三项贡献：(i) 我们引入了一种新的结构化神经网络架构，该架构明确地将层次（作为空间掩码）纳入其设计中。这提高了分离性能，优于之前用于此任务的通用网络；(ii) 我们展示了可以通过增强架构来利用外部线索（如音频）以实现可控性并帮助消除歧义；(iii) 我们通过控制实验实验性地证明了我们的方法和训练程序的有效性，同时也展示了所提出的模型可以成功应用于现实世界的应用，如反射去除和杂乱场景中的动作识别。",
        "领域": "视频处理/神经网络/注意力机制",
        "问题": "如何有效地将视频分离成其自然层次，并控制关注哪些分离的层次",
        "动机": "提高视频分离性能，实现层次的可控关注，以应用于反射去除和动作识别等实际场景",
        "方法": "引入新的结构化神经网络架构，明确将层次纳入设计；增强架构以利用外部线索如音频；通过控制实验验证方法和训练程序的有效性",
        "关键词": [
            "视频分离",
            "神经网络架构",
            "注意力控制",
            "反射去除",
            "动作识别"
        ],
        "涉及的技术概念": "结构化神经网络架构、空间掩码、外部线索（如音频）、控制实验、反射去除、动作识别"
    },
    {
        "order": 443,
        "title": "A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor Arrays",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bose_A_Camera_That_CNNs_Towards_Embedded_Neural_Networks_on_Pixel_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bose_A_Camera_That_CNNs_Towards_Embedded_Neural_Networks_on_Pixel_ICCV_2019_paper.html",
        "abstract": "We present a convolutional neural network implementation for pixel processor array (PPA) sensors. PPA hardware consists of a fine-grained array of general-purpose processing elements, each capable of light capture, data storage, program execution, and communication with neighboring elements. This allows images to be stored and manipulated directly at the point of light capture, rather than having to transfer images to external processing hardware. Our CNN approach divides this array up into 4x4 blocks of processing elements, essentially trading-off image resolution for increased local memory capacity per 4x4 \"pixel\". We implement parallel operations for image addition, subtraction and bit-shifting images in this 4x4 block format. Using these components we formulate how to perform ternary weight convolutions upon these images, compactly store results of such convolutions, perform max-pooling, and transfer the resulting sub-sampled data to an attached micro-controller. We train ternary weight filter CNNs for digit recognition and a simple tracking task, and demonstrate inference of these networks upon the SCAMP5 PPA system. This work represents a first step towards embedding neural network processing capability directly onto the focal plane of a sensor.",
        "中文标题": "一种CNN相机：迈向像素处理器阵列上的嵌入式神经网络",
        "摘要翻译": "我们提出了一种用于像素处理器阵列（PPA）传感器的卷积神经网络实现。PPA硬件由一系列细粒度的通用处理元素组成，每个元素都能够进行光捕获、数据存储、程序执行以及与邻近元素的通信。这使得图像可以直接在光捕获点进行存储和操作，而无需将图像传输到外部处理硬件。我们的CNN方法将这个阵列划分为4x4的处理元素块，实质上是以图像分辨率为代价，增加了每个4x4“像素”的本地内存容量。我们实现了在这种4x4块格式下的图像加法、减法和位移动操作的并行操作。利用这些组件，我们制定了如何对这些图像执行三元权重卷积、紧凑地存储此类卷积的结果、执行最大池化，并将结果子采样数据传输到连接的微控制器。我们训练了三元权重滤波器CNN用于数字识别和简单的跟踪任务，并在SCAMP5 PPA系统上展示了这些网络的推理。这项工作代表了将神经网络处理能力直接嵌入传感器焦平面的第一步。",
        "领域": "嵌入式系统/卷积神经网络/图像传感器",
        "问题": "如何在像素处理器阵列（PPA）上实现卷积神经网络（CNN）以直接在光捕获点处理图像",
        "动机": "减少图像处理过程中对外部硬件的依赖，提高处理效率和速度",
        "方法": "将PPA阵列划分为4x4的处理元素块，实现并行图像操作，执行三元权重卷积、最大池化，并将结果数据传输到微控制器",
        "关键词": [
            "像素处理器阵列",
            "卷积神经网络",
            "三元权重卷积",
            "最大池化",
            "嵌入式系统"
        ],
        "涉及的技术概念": "像素处理器阵列（PPA）是一种硬件，由一系列能够进行光捕获、数据存储、程序执行和通信的通用处理元素组成。卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。三元权重卷积是一种减少模型参数和计算量的技术，通过将权重限制为-1、0、+1三个值来实现。最大池化是一种下采样技术，用于减少数据的空间尺寸，同时保留重要信息。"
    },
    {
        "order": 444,
        "title": "Composite Shape Modeling via Latent Space Factorization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dubrovina_Composite_Shape_Modeling_via_Latent_Space_Factorization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dubrovina_Composite_Shape_Modeling_via_Latent_Space_Factorization_ICCV_2019_paper.html",
        "abstract": "We present a novel neural network architecture, termed Decomposer-Composer, for semantic structure-aware 3D shape modeling. Our method utilizes an auto-encoder-based pipeline, and produces a novel factorized shape embedding space, where the semantic structure of the shape collection translates into a data-dependent sub-space factorization, and where shape composition and decomposition become simple linear operations on the embedding coordinates. We further propose to model shape assembly using an explicit learned part deformation module, which utilizes a 3D spatial transformer network to perform an in-network volumetric grid deformation, and which allows us to train the whole system end-to-end. The resulting network allows us to perform part-level shape manipulation, unattainable by existing approaches. Our extensive ablation study, comparison to baseline methods and qualitative analysis demonstrate the improved performance of the proposed method.",
        "中文标题": "通过潜在空间分解的复合形状建模",
        "摘要翻译": "我们提出了一种新颖的神经网络架构，称为分解者-合成者，用于语义结构感知的3D形状建模。我们的方法利用基于自动编码器的流程，并产生了一种新颖的分解形状嵌入空间，其中形状集合的语义结构转化为数据依赖的子空间分解，形状的合成和分解成为嵌入坐标上的简单线性操作。我们进一步提出使用显式学习的部分变形模块来建模形状组装，该模块利用3D空间变换网络执行网络内体积网格变形，并允许我们端到端地训练整个系统。由此产生的网络使我们能够执行现有方法无法实现的部分级别形状操作。我们广泛的消融研究、与基线方法的比较和定性分析证明了所提出方法的改进性能。",
        "领域": "3D建模/形状分析/神经网络",
        "问题": "如何实现语义结构感知的3D形状建模",
        "动机": "为了改进3D形状建模的语义结构感知能力，实现更精细的部分级别形状操作",
        "方法": "采用基于自动编码器的流程，结合分解者-合成者架构和显式学习的部分变形模块，通过3D空间变换网络实现网络内体积网格变形",
        "关键词": [
            "3D建模",
            "形状分析",
            "神经网络",
            "自动编码器",
            "空间变换网络"
        ],
        "涉及的技术概念": "自动编码器用于生成分解形状嵌入空间，3D空间变换网络用于实现网络内体积网格变形，分解者-合成者架构用于语义结构感知的3D形状建模"
    },
    {
        "order": 445,
        "title": "SynDeMo: Synergistic Deep Feature Alignment for Joint Learning of Depth and Ego-Motion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bozorgtabar_SynDeMo_Synergistic_Deep_Feature_Alignment_for_Joint_Learning_of_Depth_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bozorgtabar_SynDeMo_Synergistic_Deep_Feature_Alignment_for_Joint_Learning_of_Depth_ICCV_2019_paper.html",
        "abstract": "Despite well-established baselines, learning of scene depth and ego-motion from monocular video remains an ongoing challenge, specifically when handling scaling ambiguity issues and depth inconsistencies in image sequences. Much prior work uses either a supervised mode of learning or stereo images. The former is limited by the amount of labeled data, as it requires expensive sensors, while the latter is not always readily available as monocular sequences. In this work, we demonstrate the benefit of using geometric information from synthetic images, coupled with scene depth information, to recover the scale in depth and ego-motion estimation from monocular videos. We developed our framework using synthetic image-depth pairs and unlabeled real monocular images. We had three training objectives: first, to use deep feature alignment to reduce the domain gap between synthetic and monocular images to yield more accurate depth estimation when presented with only real monocular images at test time. Second, we learn scene specific representation by exploiting self-supervision coming from multi-view synthetic images without the need for depth labels. Third, our method uses single-view depth and pose networks, which are capable of jointly training and supervising one another mutually, yielding consistent depth and ego-motion estimates. Extensive experiments demonstrate that our depth and ego-motion models surpass the state-of-the-art, unsupervised methods and compare favorably to early supervised deep models for geometric understanding. We validate the effectiveness of our training objectives against standard benchmarks thorough an ablation study.",
        "中文标题": "SynDeMo: 深度特征协同对齐用于深度和自我运动的联合学习",
        "摘要翻译": "尽管已有成熟的基线，但从单目视频中学习场景深度和自我运动仍然是一个持续的挑战，特别是在处理图像序列中的尺度模糊问题和深度不一致性时。许多先前的工作要么使用监督学习模式，要么使用立体图像。前者受限于标记数据的数量，因为它需要昂贵的传感器，而后者并不总是像单目序列那样容易获得。在这项工作中，我们展示了使用来自合成图像的几何信息，结合场景深度信息，从单目视频中恢复深度和自我运动估计的尺度的好处。我们使用合成图像-深度对和未标记的真实单目图像开发了我们的框架。我们有三个训练目标：首先，使用深度特征对齐来减少合成图像和单目图像之间的领域差距，以便在测试时仅呈现真实单目图像时产生更准确的深度估计。其次，我们通过利用来自多视图合成图像的自我监督来学习场景特定的表示，而无需深度标签。第三，我们的方法使用单视图深度和姿态网络，它们能够相互联合训练和监督，产生一致的深度和自我运动估计。大量实验表明，我们的深度和自我运动模型超越了最先进的无监督方法，并与早期的监督深度模型在几何理解方面相比具有优势。我们通过消融研究验证了我们的训练目标对标准基准的有效性。",
        "领域": "深度估计/自我运动估计/无监督学习",
        "问题": "从单目视频中准确估计场景深度和自我运动",
        "动机": "解决单目视频中尺度模糊和深度不一致性问题，提高深度和自我运动估计的准确性",
        "方法": "使用合成图像-深度对和未标记的真实单目图像，通过深度特征对齐减少领域差距，利用多视图合成图像的自我监督学习场景特定表示，以及使用单视图深度和姿态网络进行联合训练和监督",
        "关键词": [
            "深度估计",
            "自我运动估计",
            "无监督学习",
            "深度特征对齐",
            "多视图合成图像",
            "单视图深度和姿态网络"
        ],
        "涉及的技术概念": "深度特征对齐用于减少合成图像和单目图像之间的领域差距，多视图合成图像的自我监督用于学习场景特定表示，单视图深度和姿态网络用于联合训练和监督，以产生一致的深度和自我运动估计"
    },
    {
        "order": 446,
        "title": "GANalyze: Toward Visual Definitions of Cognitive Image Properties",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Goetschalckx_GANalyze_Toward_Visual_Definitions_of_Cognitive_Image_Properties_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Goetschalckx_GANalyze_Toward_Visual_Definitions_of_Cognitive_Image_Properties_ICCV_2019_paper.html",
        "abstract": "We introduce a framework that uses Generative Adversarial Networks (GANs) to study cognitive properties like memorability. These attributes are of interest because we do not have a concrete visual definition of what they entail. What does it look like for a dog to be more memorable? GANs allow us to generate a manifold of natural-looking images with fine-grained differences in their visual attributes. By navigating this manifold in directions that increase memorability, we can visualize what it looks like for a particular generated image to become more memorable. The resulting \"visual definitions\" surface image properties (like \"object size\") that may underlie memorability. Through behavioral experiments, we verify that our method indeed discovers image manipulations that causally affect human memory performance. We further demonstrate that the same framework can be used to analyze image aesthetics and emotional valence. ganalyze.csail.mit.edu.",
        "中文标题": "GANalyze：面向认知图像属性的视觉定义",
        "摘要翻译": "我们引入了一个使用生成对抗网络（GANs）来研究如记忆性等认知属性的框架。这些属性之所以引起兴趣，是因为我们没有一个具体的视觉定义来描述它们所包含的内容。一只狗看起来更令人难忘是什么样子的？GANs允许我们生成一系列自然外观的图像，这些图像在视觉属性上有细微的差异。通过在增加记忆性的方向上导航这个流形，我们可以可视化特定生成的图像变得更令人难忘的样子。由此产生的“视觉定义”揭示了可能构成记忆性的图像属性（如“物体大小”）。通过行为实验，我们验证了我们的方法确实发现了影响人类记忆表现的图像操作。我们进一步证明，相同的框架可以用于分析图像美学和情感价值。ganalyze.csail.mit.edu。",
        "领域": "认知科学/图像生成/人类记忆",
        "问题": "如何通过视觉定义来理解和量化图像的认知属性，如记忆性",
        "动机": "探索和量化图像的认知属性，如记忆性，缺乏具体的视觉定义，这促使我们使用GANs来生成和探索这些属性的视觉表现",
        "方法": "使用生成对抗网络（GANs）生成具有细微视觉差异的自然图像，通过导航这些图像来探索和可视化增加记忆性的方向，从而揭示影响记忆性的图像属性",
        "关键词": [
            "生成对抗网络",
            "记忆性",
            "图像属性",
            "视觉定义",
            "行为实验"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，由生成器和判别器组成，用于生成新的、与真实数据相似的图像。在这个研究中，GANs被用来生成具有细微视觉差异的图像，以探索和可视化影响记忆性的图像属性。行为实验用于验证这些图像操作对人类记忆表现的影响。"
    },
    {
        "order": 447,
        "title": "Knowledge Distillation via Route Constrained Optimization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jin_Knowledge_Distillation_via_Route_Constrained_Optimization_ICCV_2019_paper.html",
        "abstract": "Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a higher lower bound of congruence loss. In this work, we consider the knowledge distillation from the perspective of curriculum learning by teacher's routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace. RCO achieves 84.3% accuracy on one-to-million task with only 0.8 M parameters, which push the SOTA by a large margin.",
        "中文标题": "通过路径约束优化的知识蒸馏",
        "摘要翻译": "基于蒸馏的学习通过假设教师模型的表示可以作为结构化和相对较弱的监督来提升小型化神经网络的性能，因此小型化模型可以更容易地学习。然而，我们发现一个收敛的重模型的表示对于训练一个小型学生模型仍然是一个强约束，这导致了更高的同余损失下限。在这项工作中，我们从课程学习的角度考虑知识蒸馏，通过教师的路由。我们不是用一个收敛的教师模型来监督学生模型，而是用从教师模型在参数空间中经过的路径中选择的一些锚点来监督它，我们称之为路径约束优化（RCO）。我们通过实验证明，这个简单的操作大大降低了知识蒸馏、提示和模仿学习的同余损失下限。在CIFAR和ImageNet等闭集分类任务上，RCO分别提高了知识蒸馏的2.14%和1.5%。为了评估泛化能力，我们还在开放集人脸识别任务MegaFace上测试了RCO。RCO在仅0.8 M参数的情况下，在一对百万任务上达到了84.3%的准确率，这大大推动了SOTA。",
        "领域": "知识蒸馏/模型压缩/课程学习",
        "问题": "如何降低知识蒸馏过程中的同余损失下限",
        "动机": "发现收敛的重模型表示对小型学生模型的训练仍然是一个强约束，导致同余损失下限较高",
        "方法": "采用路径约束优化（RCO）方法，通过教师模型在参数空间中的路径选择锚点来监督学生模型",
        "关键词": [
            "知识蒸馏",
            "模型压缩",
            "课程学习",
            "路径约束优化",
            "同余损失"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "一种模型压缩技术，通过让小型学生模型学习大型教师模型的表示来提升性能",
            "模型压缩": "减少模型大小和计算需求的技术，以便在资源受限的设备上部署",
            "课程学习": "一种训练策略，通过从简单到复杂的顺序逐步学习",
            "路径约束优化（RCO）": "一种新的知识蒸馏方法，通过教师模型在参数空间中的路径选择锚点来监督学生模型",
            "同余损失": "在知识蒸馏过程中，学生模型与教师模型表示之间的差异"
        }
    },
    {
        "order": 448,
        "title": "Deep Comprehensive Correlation Mining for Image Clustering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Deep_Comprehensive_Correlation_Mining_for_Image_Clustering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Deep_Comprehensive_Correlation_Mining_for_Image_Clustering_ICCV_2019_paper.html",
        "abstract": "Recent developed deep unsupervised methods allow us to jointly learn representation and cluster unlabelled data. These deep clustering methods %like DAC start with mainly focus on the correlation among samples, e.g., selecting high precision pairs to gradually tune the feature representation, which neglects other useful correlations. In this paper, we propose a novel clustering framework, named deep comprehensive correlation mining (DCCM), for exploring and taking full advantage of various kinds of correlations behind the unlabeled data from three aspects: 1) Instead of only using pair-wise information, pseudo-label supervision is proposed to investigate category information and learn discriminative features. 2) The features' robustness to image transformation of input space is fully explored, which benefits the network learning and significantly improves the performance. 3) The triplet mutual information among features is presented for clustering problem to lift the recently discovered instance-level deep mutual information to a triplet-level formation, which further helps to learn more discriminative features. Extensive experiments on several challenging datasets show that our method achieves good performance, e.g., attaining 62.3% clustering accuracy on CIFAR-10, which is 10.1% higher than the state-of-the-art results.",
        "中文标题": "深度全面相关性挖掘用于图像聚类",
        "摘要翻译": "最近开发的深度无监督方法使我们能够联合学习表示和聚类未标记数据。这些深度聚类方法，如DAC，主要从样本间的相关性开始，例如选择高精度对来逐步调整特征表示，这忽略了其他有用的相关性。在本文中，我们提出了一种新颖的聚类框架，名为深度全面相关性挖掘（DCCM），用于探索并充分利用未标记数据背后的各种相关性，从三个方面进行：1）不仅使用成对信息，还提出了伪标签监督来研究类别信息并学习区分性特征。2）充分探索了输入空间图像变换对特征的鲁棒性，这有利于网络学习并显著提高了性能。3）提出了特征间的三元组互信息用于聚类问题，将最近发现的实例级深度互信息提升到三元组级形式，这进一步有助于学习更多区分性特征。在几个具有挑战性的数据集上的大量实验表明，我们的方法取得了良好的性能，例如，在CIFAR-10上达到了62.3%的聚类准确率，比最先进的结果高出10.1%。",
        "领域": "图像聚类/特征学习/无监督学习",
        "问题": "如何更有效地利用未标记数据中的多种相关性进行图像聚类",
        "动机": "现有的深度聚类方法主要关注样本间的相关性，忽略了其他有用的相关性，限制了聚类性能的提升",
        "方法": "提出了一种名为深度全面相关性挖掘（DCCM）的聚类框架，通过伪标签监督、探索特征的鲁棒性和提出三元组互信息来充分利用未标记数据中的各种相关性",
        "关键词": [
            "图像聚类",
            "特征学习",
            "无监督学习",
            "伪标签监督",
            "三元组互信息"
        ],
        "涉及的技术概念": {
            "深度无监督方法": "一种能够联合学习表示和聚类未标记数据的方法",
            "伪标签监督": "一种通过生成伪标签来研究类别信息并学习区分性特征的技术",
            "三元组互信息": "一种用于聚类问题的技术，通过提升实例级深度互信息到三元组级形式来学习更多区分性特征"
        }
    },
    {
        "order": 449,
        "title": "Diverse Image Synthesis From Semantic Layouts via Conditional IMLE",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Diverse_Image_Synthesis_From_Semantic_Layouts_via_Conditional_IMLE_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Diverse_Image_Synthesis_From_Semantic_Layouts_via_Conditional_IMLE_ICCV_2019_paper.html",
        "abstract": "Most existing methods for conditional image synthesis are only able to generate a single plausible image for any given input, or at best a fixed number of plausible images. In this paper, we focus on the problem of generating images from semantic segmentation maps and present a simple new method that can generate an arbitrary number of images with diverse appearance for the same semantic layout. Unlike most existing approaches which adopt the GAN framework, our method is based on the recently introduced Implicit Maximum Likelihood Estimation (IMLE) framework. Compared to the leading approach, our method is able to generate more diverse images while producing fewer artifacts despite using the same architecture. The learned latent space also has sensible structure despite the lack of supervision that encourages such behaviour.",
        "中文标题": "从语义布局到多样化图像合成的条件IMLE方法",
        "摘要翻译": "大多数现有的条件图像合成方法只能为任何给定输入生成一个合理的图像，或者最多生成固定数量的合理图像。在本文中，我们专注于从语义分割图生成图像的问题，并提出了一种简单的新方法，该方法可以为相同的语义布局生成任意数量的具有多样化外观的图像。与大多数采用GAN框架的现有方法不同，我们的方法基于最近引入的隐式最大似然估计（IMLE）框架。与领先的方法相比，尽管使用相同的架构，我们的方法能够生成更多样化的图像，同时产生更少的伪影。尽管缺乏鼓励这种行为的监督，学习到的潜在空间也具有合理的结构。",
        "领域": "图像合成/语义分割/生成模型",
        "问题": "如何从相同的语义布局生成多样化外观的图像",
        "动机": "现有方法在生成多样化图像方面存在限制，需要一种能够生成任意数量多样化图像的新方法",
        "方法": "基于隐式最大似然估计（IMLE）框架的新方法，与传统的GAN框架不同",
        "关键词": [
            "图像合成",
            "语义分割",
            "生成模型",
            "隐式最大似然估计"
        ],
        "涉及的技术概念": "条件图像合成、语义分割图、GAN框架、隐式最大似然估计（IMLE）框架、潜在空间"
    },
    {
        "order": 450,
        "title": "Saliency-Guided Attention Network for Image-Sentence Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Saliency-Guided_Attention_Network_for_Image-Sentence_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Saliency-Guided_Attention_Network_for_Image-Sentence_Matching_ICCV_2019_paper.html",
        "abstract": "This paper studies the task of matching image and sentence, where learning appropriate representations to bridge the semantic gap between image contents and language appears to be the main challenge. Unlike previous approaches that predominantly deploy symmetrical architecture to represent both modalities, we introduce a Saliency-guided Attention Network (SAN) that is characterized by building an asymmetrical link between vision and language to efficiently learn a fine-grained cross-modal correlation. The proposed SAN mainly includes three components: saliency detector, Saliency-weighted Visual Attention (SVA) module, and Saliency-guided Textual Attention (STA) module. Concretely, the saliency detector provides the visual saliency information to drive both two attention modules. Taking advantage of the saliency information, SVA is able to learn more discriminative visual features. By fusing the visual information from SVA and intra-modal information as a multi-modal guidance, STA affords us powerful textual representations that are synchronized with visual clues. Extensive experiments demonstrate SAN can improve the state-of-the-art results on the benchmark Flickr30K and MSCOCO datasets by a large margin.",
        "中文标题": "显著性引导的注意力网络用于图像-句子匹配",
        "摘要翻译": "本文研究了图像与句子匹配的任务，其中学习适当的表示以弥合图像内容与语言之间的语义差距似乎是主要挑战。与之前主要采用对称架构来表示两种模态的方法不同，我们引入了一种显著性引导的注意力网络（SAN），其特点是在视觉和语言之间建立非对称链接，以有效学习细粒度的跨模态相关性。提出的SAN主要包括三个组件：显著性检测器、显著性加权视觉注意力（SVA）模块和显著性引导文本注意力（STA）模块。具体来说，显著性检测器提供视觉显著性信息以驱动两个注意力模块。利用显著性信息，SVA能够学习更具区分性的视觉特征。通过融合来自SVA的视觉信息和模态内信息作为多模态指导，STA为我们提供了与视觉线索同步的强大文本表示。大量实验表明，SAN可以在基准Flickr30K和MSCOCO数据集上大幅提高最先进的结果。",
        "领域": "跨模态学习/视觉-语言理解/注意力机制",
        "问题": "图像与句子匹配中的语义差距问题",
        "动机": "为了有效学习图像内容与语言之间的细粒度跨模态相关性，解决现有对称架构在表示两种模态时的不足",
        "方法": "引入显著性引导的注意力网络（SAN），包括显著性检测器、显著性加权视觉注意力（SVA）模块和显著性引导文本注意力（STA）模块，通过非对称链接在视觉和语言之间建立联系",
        "关键词": [
            "跨模态学习",
            "视觉-语言理解",
            "注意力机制"
        ],
        "涉及的技术概念": "显著性检测器用于提供视觉显著性信息，显著性加权视觉注意力（SVA）模块利用显著性信息学习更具区分性的视觉特征，显著性引导文本注意力（STA）模块通过融合视觉信息和模态内信息提供与视觉线索同步的文本表示"
    },
    {
        "order": 451,
        "title": "Distillation-Based Training for Multi-Exit Architectures",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.html",
        "abstract": "Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.",
        "中文标题": "基于蒸馏的多出口架构训练",
        "摘要翻译": "多出口架构，其中一系列处理层与早期输出层交错，允许测试样本的处理提前停止，从而节省计算时间和/或能量。在这项工作中，我们提出了一种基于知识蒸馏原理的多出口架构训练新方法。该方法通过匹配概率输出，鼓励早期出口模仿后期更准确的出口。在CIFAR100和ImageNet上的实验表明，基于蒸馏的训练显著提高了早期出口的准确性，同时保持了后期出口的最先进准确性。当训练数据有限时，该方法特别有益，并且还允许直接扩展到半监督学习，即在训练时也利用未标记的数据。此外，它只需几行代码即可实现，在训练时几乎不增加计算开销，在测试时则完全不增加。",
        "领域": "神经网络优化/模型压缩/半监督学习",
        "问题": "提高多出口架构中早期出口的准确性",
        "动机": "为了节省计算时间和能量，同时保持或提高模型的准确性",
        "方法": "基于知识蒸馏原理，通过匹配概率输出，使早期出口模仿后期更准确的出口",
        "关键词": [
            "知识蒸馏",
            "多出口架构",
            "半监督学习"
        ],
        "涉及的技术概念": "知识蒸馏是一种技术，通过让一个模型（学生模型）模仿另一个更复杂模型（教师模型）的输出，来传递知识。多出口架构指的是在神经网络的不同深度设置多个输出层，允许根据输入数据的复杂度提前终止计算。半监督学习是一种利用少量标记数据和大量未标记数据进行训练的学习方法。"
    },
    {
        "order": 452,
        "title": "CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.html",
        "abstract": "Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.",
        "中文标题": "CAMP：用于文本-图像检索的跨模态自适应消息传递",
        "摘要翻译": "文本-图像跨模态检索是语言和视觉领域中的一个具有挑战性的任务。大多数先前的方法独立地将图像和句子嵌入到一个联合嵌入空间中，并比较它们的相似性。然而，先前的方法很少在计算联合空间中的相似性之前探索图像和句子之间的交互。直观地，当在图像和句子之间进行匹配时，人类会交替关注图像中的区域和句子中的单词，并考虑到两种模态之间的交互选择最显著的信息。在本文中，我们提出了跨模态自适应消息传递（CAMP），它自适应地控制跨模态消息传递的信息流。我们的方法不仅考虑了全面和细粒度的跨模态交互，而且还通过自适应门控方案正确处理负对和无关信息。此外，与传统的文本-图像匹配的联合嵌入方法不同，我们基于融合特征推断匹配分数，并提出了一种最难的负二元交叉熵损失用于训练。在COCO和Flickr30k上的结果显著超越了最先进的方法，证明了我们方法的有效性。",
        "领域": "跨模态学习/信息检索/视觉语言理解",
        "问题": "文本-图像跨模态检索中的交互信息利用不足",
        "动机": "探索图像和句子之间的交互，以提高文本-图像跨模态检索的准确性和效率",
        "方法": "提出跨模态自适应消息传递（CAMP），通过自适应控制跨模态信息流，考虑全面和细粒度的跨模态交互，并使用自适应门控方案处理负对和无关信息，基于融合特征推断匹配分数，并采用最难的负二元交叉熵损失进行训练",
        "关键词": [
            "跨模态学习",
            "信息检索",
            "视觉语言理解",
            "自适应消息传递",
            "联合嵌入空间"
        ],
        "涉及的技术概念": "跨模态自适应消息传递（CAMP）是一种方法，它通过自适应地控制信息流来促进图像和文本之间的交互。这种方法不仅考虑了跨模态的细粒度交互，还通过自适应门控机制来过滤掉不相关的信息。此外，该方法通过融合特征来推断匹配分数，并采用了一种新的损失函数——最难的负二元交叉熵损失，以提高模型的训练效果。"
    },
    {
        "order": 453,
        "title": "Unsupervised Multi-Task Feature Learning on Point Clouds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hassani_Unsupervised_Multi-Task_Feature_Learning_on_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "We introduce an unsupervised multi-task model to jointly learn point and shape features on point clouds. We define three unsupervised tasks including clustering, reconstruction, and self-supervised classification to train a multi-scale graph-based encoder. We evaluate our model on shape classification and segmentation benchmarks. The results suggest that it outperforms prior state-of-the-art unsupervised models: In the ModelNet40 classification task, it achieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an mIoU of 68.2 and accuracy of 88.6%.",
        "中文标题": "无监督多任务特征学习在点云上的应用",
        "摘要翻译": "我们引入了一种无监督多任务模型，以联合学习点云上的点和形状特征。我们定义了三个无监督任务，包括聚类、重建和自我监督分类，以训练一个基于多尺度图的编码器。我们在形状分类和分割基准上评估了我们的模型。结果表明，它优于之前的最先进的无监督模型：在ModelNet40分类任务中，它达到了89.1%的准确率，在ShapeNet分割任务中，它达到了68.2的mIoU和88.6%的准确率。",
        "领域": "点云处理/三维形状分析/无监督学习",
        "问题": "如何在点云数据上有效地进行无监督的特征学习",
        "动机": "探索在无监督条件下，通过多任务学习提高点云数据的特征提取能力，以提升形状分类和分割的性能",
        "方法": "定义并联合训练三个无监督任务（聚类、重建和自我监督分类），使用基于多尺度图的编码器进行特征学习",
        "关键词": [
            "点云",
            "无监督学习",
            "多任务学习",
            "形状分类",
            "分割"
        ],
        "涉及的技术概念": {
            "无监督多任务模型": "一种不需要标注数据即可同时学习多个任务的模型",
            "点云": "三维空间中的点的集合，常用于表示三维物体的表面",
            "多尺度图编码器": "一种能够处理不同尺度信息的图结构数据编码器",
            "ModelNet40": "一个包含40个类别的三维形状分类数据集",
            "ShapeNet": "一个大规模的三维形状数据集，常用于分割任务",
            "mIoU": "平均交并比，是评估分割任务性能的常用指标"
        }
    },
    {
        "order": 454,
        "title": "Towards Bridging Semantic Gap to Improve Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pang_Towards_Bridging_Semantic_Gap_to_Improve_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pang_Towards_Bridging_Semantic_Gap_to_Improve_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Aggregating multi-level features is essential for capturing multi-scale context information for precise scene semantic segmentation. However, the improvement by directly fusing shallow features and deep features becomes limited as the semantic gap between them increases. To solve this problem, we explore two strategies for robust feature fusion. One is enhancing shallow features using a semantic enhancement module (SeEM) to alleviate the semantic gap between shallow features and deep features. The other strategy is feature attention, which involves discovering complementary information (i.e., boundary information) from low-level features to enhance high-level features for precise segmentation. By embedding these two strategies, we construct a parallel feature pyramid towards improving multi-level feature fusion. A Semantic Enhanced Network called SeENet is constructed with the parallel pyramid to implement precise segmentation. Experiments on three benchmark datasets demonstrate the effectiveness of our method for robust multi-level feature aggregation. As a result, our SeENet has achieved better performance than other state-of-the-art methods for semantic segmentation.",
        "中文标题": "迈向缩小语义鸿沟以改善语义分割",
        "摘要翻译": "聚合多层次特征对于捕捉多尺度上下文信息以实现精确的场景语义分割至关重要。然而，随着浅层特征和深层特征之间的语义鸿沟增加，直接融合这些特征所带来的改进变得有限。为了解决这个问题，我们探索了两种策略以实现鲁棒的特征融合。一种是使用语义增强模块（SeEM）增强浅层特征，以减轻浅层特征和深层特征之间的语义鸿沟。另一种策略是特征注意力，它涉及从低级特征中发现互补信息（即边界信息）以增强高级特征，从而实现精确分割。通过嵌入这两种策略，我们构建了一个并行特征金字塔，以改善多层次特征融合。一个名为SeENet的语义增强网络与并行金字塔一起构建，以实现精确分割。在三个基准数据集上的实验证明了我们方法在鲁棒的多层次特征聚合方面的有效性。因此，我们的SeENet在语义分割方面取得了比其他最先进方法更好的性能。",
        "领域": "语义分割/特征融合/场景理解",
        "问题": "浅层特征和深层特征之间的语义鸿沟限制了直接融合这些特征所带来的改进",
        "动机": "探索有效的方法以缩小浅层特征和深层特征之间的语义鸿沟，实现更精确的场景语义分割",
        "方法": "使用语义增强模块（SeEM）增强浅层特征和采用特征注意力策略从低级特征中发现互补信息以增强高级特征，构建并行特征金字塔和语义增强网络SeENet",
        "关键词": [
            "语义分割",
            "特征融合",
            "场景理解",
            "语义增强模块",
            "特征注意力",
            "并行特征金字塔"
        ],
        "涉及的技术概念": {
            "语义分割": "一种将图像分割成多个区域或对象的技术，每个区域或对象被赋予一个语义标签",
            "特征融合": "将来自不同层次或来源的特征结合起来，以提高模型的性能",
            "语义增强模块（SeEM）": "一种用于增强浅层特征的模块，旨在减轻浅层特征和深层特征之间的语义鸿沟",
            "特征注意力": "一种策略，通过从低级特征中发现互补信息来增强高级特征，以提高分割的精确度",
            "并行特征金字塔": "一种结构，用于改善多层次特征融合，通过并行处理不同层次的特征来实现"
        }
    },
    {
        "order": 455,
        "title": "Similarity-Preserving Knowledge Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tung_Similarity-Preserving_Knowledge_Distillation_ICCV_2019_paper.html",
        "abstract": "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.",
        "中文标题": "相似性保持的知识蒸馏",
        "摘要翻译": "知识蒸馏是一种广泛适用的技术，用于在训练有素的教师网络的指导下训练学生神经网络。例如，在神经网络压缩中，一个高容量的教师被蒸馏以训练一个紧凑的学生；在特权学习中，一个用特权数据训练的教师被蒸馏以训练一个无法访问该数据的学生。蒸馏损失决定了教师的知识如何被捕获并转移给学生。在本文中，我们提出了一种新的知识蒸馏损失形式，其灵感来自于观察到语义上相似的输入往往会在训练有素的网络中引发相似的激活模式。相似性保持的知识蒸馏指导学生网络的训练，使得在教师网络中产生相似（不相似）激活的输入对在学生网络中也能产生相似（不相似）的激活。与之前的蒸馏方法相比，学生不需要模仿教师的表示空间，而是要在其自己的表示空间中保持成对相似性。在三个公共数据集上的实验证明了我们方法的潜力。",
        "领域": "神经网络压缩/特权学习/知识蒸馏",
        "问题": "如何有效地将教师网络的知识转移给学生网络，同时保持输入对的相似性",
        "动机": "观察到语义上相似的输入在训练有素的网络中会引发相似的激活模式，这启发了我们提出一种新的知识蒸馏损失形式，以保持这种相似性",
        "方法": "提出了一种新的知识蒸馏损失形式，指导学生网络在训练过程中保持输入对在教师网络和学生网络中的激活相似性",
        "关键词": [
            "知识蒸馏",
            "神经网络压缩",
            "特权学习",
            "相似性保持"
        ],
        "涉及的技术概念": "知识蒸馏是一种技术，用于在教师网络的指导下训练学生网络，通过蒸馏损失将教师的知识转移给学生。本文提出的相似性保持的知识蒸馏方法，旨在保持输入对在教师网络和学生网络中的激活相似性，而不是简单地模仿教师的表示空间。"
    },
    {
        "order": 456,
        "title": "Reciprocal Multi-Layer Subspace Learning for Multi-View Clustering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Reciprocal_Multi-Layer_Subspace_Learning_for_Multi-View_Clustering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Reciprocal_Multi-Layer_Subspace_Learning_for_Multi-View_Clustering_ICCV_2019_paper.html",
        "abstract": "Multi-view clustering is a long-standing important research topic, however, remains challenging when handling high-dimensional data and simultaneously exploring the consistency and complementarity of different views. In this work, we present a novel Reciprocal Multi-layer Subspace Learning (RMSL) algorithm for multi-view clustering, which is composed of two main components: Hierarchical Self-Representative Layers (HSRL), and Backward Encoding Networks (BEN). Specifically, HSRL constructs reciprocal multi-layer subspace representations linked with a latent representation to hierarchically recover the underlying low-dimensional subspaces in which the high-dimensional data lie; BEN explores complex relationships among different views and implicitly enforces the subspaces of all views to be consistent with each other and more separable. The latent representation flexibly encodes complementary information from multiple views and depicts data more comprehensively. Our model can be efficiently optimized by an alternating optimization scheme. Extensive experiments on benchmark datasets show the superiority of RMSL over other state-of-the-art clustering methods.",
        "中文标题": "互惠多层子空间学习用于多视图聚类",
        "摘要翻译": "多视图聚类是一个长期存在的重要研究课题，但在处理高维数据并同时探索不同视图的一致性和互补性时仍然具有挑战性。在这项工作中，我们提出了一种新颖的互惠多层子空间学习（RMSL）算法用于多视图聚类，该算法由两个主要组件组成：分层自表示层（HSRL）和反向编码网络（BEN）。具体来说，HSRL构建了与潜在表示相连的互惠多层子空间表示，以分层恢复高维数据所在的低维子空间；BEN探索了不同视图之间的复杂关系，并隐式地强制所有视图的子空间相互一致且更可分离。潜在表示灵活地编码了来自多个视图的互补信息，并更全面地描绘了数据。我们的模型可以通过交替优化方案高效优化。在基准数据集上的大量实验表明，RMSL优于其他最先进的聚类方法。",
        "领域": "多视图聚类/子空间学习/高维数据处理",
        "问题": "处理高维数据并同时探索不同视图的一致性和互补性",
        "动机": "解决多视图聚类在处理高维数据和探索视图间一致性与互补性方面的挑战",
        "方法": "提出了一种新颖的互惠多层子空间学习（RMSL）算法，包括分层自表示层（HSRL）和反向编码网络（BEN），通过交替优化方案高效优化",
        "关键词": [
            "多视图聚类",
            "子空间学习",
            "高维数据处理"
        ],
        "涉及的技术概念": "分层自表示层（HSRL）用于构建互惠多层子空间表示，反向编码网络（BEN）用于探索视图间关系并强制子空间一致性，潜在表示用于编码互补信息"
    },
    {
        "order": 457,
        "title": "Generating Diverse and Descriptive Image Captions Using Visual Paraphrases",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Generating_Diverse_and_Descriptive_Image_Captions_Using_Visual_Paraphrases_ICCV_2019_paper.html",
        "abstract": "Recently there has been significant progress in image captioning with the help of deep learning. However, captions generated by current state-of-the-art models are still far from satisfactory, despite high scores in terms of conventional metrics such as BLEU and CIDEr. Human-written captions are diverse, informative and precise, but machine-generated captions seem to be simple, vague and dull. In this paper, aimed at improving diversity and descriptiveness characteristics of generated image captions, we propose a model utilizing visual paraphrases (different sentences describing the same image) in captioning datasets. We explore different strategies to select useful visual paraphrase pairs for training by designing a variety of scoring functions. Our model consists of two decoding stages, where a preliminary caption is generated in the first stage and then paraphrased into a more diverse and descriptive caption in the second stage. Extensive experiments are conducted on the benchmark MS COCO dataset, with automatic evaluation and human evaluation results verifying the effectiveness of our model.",
        "中文标题": "使用视觉释义生成多样化和描述性的图像标题",
        "摘要翻译": "近年来，在深度学习的帮助下，图像字幕生成取得了显著进展。然而，尽管在BLEU和CIDEr等传统指标上得分很高，当前最先进模型生成的标题仍然远未令人满意。人类编写的标题多样、信息丰富且精确，但机器生成的标题似乎简单、模糊且乏味。在本文中，旨在提高生成图像标题的多样性和描述性特征，我们提出了一个利用字幕数据集中视觉释义（描述同一图像的不同句子）的模型。我们探索了不同的策略，通过设计各种评分函数来选择有用的视觉释义对进行训练。我们的模型由两个解码阶段组成，第一阶段生成初步标题，然后在第二阶段将其释义为更丰富和描述性的标题。在基准MS COCO数据集上进行了广泛的实验，自动评估和人类评估结果验证了我们模型的有效性。",
        "领域": "图像字幕生成/自然语言处理/深度学习",
        "问题": "提高机器生成图像标题的多样性和描述性",
        "动机": "当前最先进模型生成的图像标题在多样性和描述性方面远未达到人类编写的水平，尽管在传统评估指标上得分很高。",
        "方法": "提出一个模型，利用视觉释义（描述同一图像的不同句子）来生成更丰富和描述性的图像标题。模型包括两个解码阶段：第一阶段生成初步标题，第二阶段将其释义为更丰富和描述性的标题。",
        "关键词": [
            "图像字幕生成",
            "视觉释义",
            "多样性",
            "描述性",
            "MS COCO数据集"
        ],
        "涉及的技术概念": {
            "视觉释义": "描述同一图像的不同句子，用于增加图像字幕的多样性和描述性。",
            "BLEU和CIDEr": "用于评估机器生成文本质量的传统指标。",
            "MS COCO数据集": "一个广泛使用的图像字幕生成和对象检测的基准数据集。",
            "解码阶段": "模型生成标题的过程，分为初步生成和释义两个阶段。"
        }
    },
    {
        "order": 458,
        "title": "ACMM: Aligned Cross-Modal Memory for Few-Shot Image and Sentence Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_ACMM_Aligned_Cross-Modal_Memory_for_Few-Shot_Image_and_Sentence_Matching_ICCV_2019_paper.html",
        "abstract": "Image and sentence matching has drawn much attention recently, but due to the lack of sufficient pairwise data for training, most previous methods still cannot well associate those challenging pairs of images and sentences containing rarely appeared regions and words, i.e., few-shot content. In this work, we study this challenging scenario as few-shot image and sentence matching, and accordingly propose an Aligned Cross-Modal Memory (ACMM) model to memorize the rarely appeared content. Given a pair of image and sentence, the model first includes an aligned memory controller network to produce two sets of semantically-comparable interface vectors through cross-modal alignment. Then the interface vectors are used by modality-specific read and update operations to alternatively interact with shared memory items. The memory items persistently memorize cross-modal shared semantic representations, which can be addressed out to better enhance the representation of few-shot content. We apply the proposed model to both conventional and few-shot image and sentence matching tasks, and demonstrate its effectiveness by achieving the state-of-the-art performance on two benchmark datasets.",
        "中文标题": "ACMM：用于少样本图像和句子匹配的对齐跨模态记忆",
        "摘要翻译": "图像和句子匹配最近引起了广泛关注，但由于缺乏足够的成对数据进行训练，大多数先前的方法仍然无法很好地关联那些包含罕见区域和词汇的挑战性图像和句子对，即少样本内容。在这项工作中，我们将这一挑战性场景研究为少样本图像和句子匹配，并相应地提出了一种对齐跨模态记忆（ACMM）模型来记忆这些罕见内容。给定一对图像和句子，该模型首先包括一个对齐的记忆控制器网络，通过跨模态对齐产生两组语义可比的接口向量。然后，接口向量被特定模态的读取和更新操作使用，以交替与共享的记忆项交互。记忆项持续记忆跨模态共享的语义表示，这些表示可以被调出以更好地增强少样本内容的表示。我们将提出的模型应用于传统和少样本图像和句子匹配任务，并通过在两个基准数据集上实现最先进的性能来证明其有效性。",
        "领域": "跨模态学习/少样本学习/语义表示",
        "问题": "解决少样本图像和句子匹配中的罕见内容关联问题",
        "动机": "由于缺乏足够的成对数据进行训练，现有方法难以有效关联包含罕见区域和词汇的图像和句子对",
        "方法": "提出了一种对齐跨模态记忆（ACMM）模型，通过跨模态对齐产生语义可比的接口向量，并利用特定模态的读取和更新操作与共享记忆项交互，以增强少样本内容的表示",
        "关键词": [
            "跨模态学习",
            "少样本学习",
            "语义表示"
        ],
        "涉及的技术概念": "对齐跨模态记忆（ACMM）模型、跨模态对齐、语义可比的接口向量、特定模态的读取和更新操作、共享记忆项、跨模态共享的语义表示"
    },
    {
        "order": 459,
        "title": "Many Task Learning With Task Routing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Strezoski_Many_Task_Learning_With_Task_Routing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Strezoski_Many_Task_Learning_With_Task_Routing_ICCV_2019_paper.html",
        "abstract": "Typical multi-task learning (MTL) methods rely on architectural adjustments and a large trainable parameter set to jointly optimize over several tasks. However, when the number of tasks increases so do the complexity of the architectural adjustments and resource requirements. In this paper, we introduce a method which applies a conditional feature-wise transformation over the convolutional activations that enables a model to successfully perform a large number of tasks. To distinguish from regular MTL, we introduce Many Task Learning (MaTL) as a special case of MTL where more than 20 tasks are performed by a single model. Our method dubbed Task Routing (TR) is encapsulated in a layer we call the Task Routing Layer (TRL), which applied in an MaTL scenario successfully fits hundreds of classification tasks in one model. We evaluate on 5 datasets and the Visual Decathlon (VD) challenge against strong baselines and state-of-the-art approaches.",
        "中文标题": "多任务学习与任务路由",
        "摘要翻译": "典型的多任务学习（MTL）方法依赖于架构调整和大量可训练参数集来共同优化多个任务。然而，当任务数量增加时，架构调整的复杂性和资源需求也随之增加。在本文中，我们介绍了一种方法，该方法在卷积激活上应用条件特征变换，使模型能够成功执行大量任务。为了与常规MTL区分开来，我们引入了多任务学习（MaTL）作为MTL的一个特例，其中单个模型执行超过20个任务。我们的方法称为任务路由（TR），封装在我们称为任务路由层（TRL）的层中，在MaTL场景中应用时，成功地将数百个分类任务拟合到一个模型中。我们在5个数据集和视觉十项全能（VD）挑战中评估了与强基线和最先进方法的对比。",
        "领域": "多任务学习/卷积神经网络/条件特征变换",
        "问题": "如何在增加任务数量的同时，减少架构调整的复杂性和资源需求",
        "动机": "随着任务数量的增加，传统多任务学习方法在架构调整和资源需求上面临挑战，需要一种更高效的方法来处理大量任务",
        "方法": "引入任务路由（TR）方法，通过任务路由层（TRL）在卷积激活上应用条件特征变换，使单个模型能够执行大量任务",
        "关键词": [
            "多任务学习",
            "任务路由",
            "条件特征变换"
        ],
        "涉及的技术概念": "多任务学习（MTL）是一种机器学习方法，旨在通过共享表示来同时学习多个相关任务，以提高学习效率和预测性能。卷积神经网络（CNN）是一种深度学习模型，特别适用于处理图像数据。条件特征变换是一种技术，它根据特定条件对特征进行变换，以适应不同的任务需求。"
    },
    {
        "order": 460,
        "title": "Geometric Disentanglement for Generative Latent Shape Models",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Aumentado-Armstrong_Geometric_Disentanglement_for_Generative_Latent_Shape_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Aumentado-Armstrong_Geometric_Disentanglement_for_Generative_Latent_Shape_Models_ICCV_2019_paper.html",
        "abstract": "Representing 3D shapes is a fundamental problem in artificial intelligence, which has numerous applications within computer vision and graphics. One avenue that has recently begun to be explored is the use of latent representations of generative models. However, it remains an open problem to learn a generative model of shapes that is interpretable and easily manipulated, particularly in the absence of supervised labels. In this paper, we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for 3D point clouds in a natural way, using only geometric information, that builds upon prior work utilizing generative adversarial models of point sets. Our method makes use of tools from spectral geometry to separate intrinsic and extrinsic shape information, and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner. We also propose a novel disentanglement penalty that penalizes the predicted change in the latent representation of the output,with respect to the latent variables of the initial shape. We show that the resulting latent representation exhibits intuitive and interpretable behaviour, enabling tasks such as pose transfer that cannot easily be performed by models with an entangled representation.",
        "中文标题": "生成潜在形状模型的几何解缠",
        "摘要翻译": "表示3D形状是人工智能中的一个基本问题，它在计算机视觉和图形学中有许多应用。最近开始探索的一个途径是使用生成模型的潜在表示。然而，学习一个可解释且易于操作的形状生成模型仍然是一个未解决的问题，特别是在没有监督标签的情况下。在本文中，我们提出了一种无监督的方法，仅使用几何信息，以自然的方式对3D点云的变分自编码器的潜在空间进行分区，该方法建立在利用点集生成对抗模型的先前工作之上。我们的方法利用谱几何工具分离内在和外在形状信息，然后考虑了几种分层解缠惩罚，以这种方式划分潜在空间。我们还提出了一种新颖的解缠惩罚，它惩罚了输出潜在表示相对于初始形状潜在变量的预测变化。我们展示了由此产生的潜在表示表现出直观和可解释的行为，使得诸如姿态转移等任务无法轻易由具有纠缠表示的模型执行。",
        "领域": "3D形状生成/潜在空间解缠/变分自编码器",
        "问题": "学习一个可解释且易于操作的形状生成模型",
        "动机": "在没有监督标签的情况下，探索使用生成模型的潜在表示来解决3D形状表示的问题",
        "方法": "提出了一种无监督的方法，利用谱几何工具分离内在和外在形状信息，并考虑了几种分层解缠惩罚来划分潜在空间",
        "关键词": [
            "3D形状生成",
            "潜在空间解缠",
            "变分自编码器",
            "谱几何",
            "无监督学习"
        ],
        "涉及的技术概念": "变分自编码器（VAE）是一种生成模型，通过学习数据的潜在表示来生成新的数据样本。谱几何是一种数学工具，用于分析和处理形状的几何属性。生成对抗模型（GAN）是一种通过对抗过程来生成数据的模型。潜在空间解缠是指将潜在空间中的变量分离，使得每个变量控制生成数据的一个独立特征。"
    },
    {
        "order": 461,
        "title": "Creativity Inspired Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Zero-shot learning (ZSL) aims at understanding unseen categories with no training examples from class-level descriptions. To improve the discriminative power of zero-shot learning, we model the visual learning process of unseen categories with an inspiration from the psychology of human creativity for producing novel art. We relate ZSL to human creativity by observing that zero-shot learning is about recognizing the unseen and creativity is about creating a likable unseen. We introduce a learning signal inspired by creativity literature that explores the unseen space with hallucinated class-descriptions and encourages careful deviation of their visual feature generations from seen classes while allowing knowledge transfer from seen to unseen classes. Empirically, we show consistent improvement over the state of the art of several percents on the largest available benchmarks on the challenging task or generalized ZSL from a noisy text that we focus on, using the CUB and NABirds datasets. We also show the advantage of our loss on Attribute-based ZSL on three additional datasets (AwA2, aPY, and SUN). Code is available at https://github.com/mhelhoseiny/CIZSL.",
        "中文标题": "创造力启发的零样本学习",
        "摘要翻译": "零样本学习（ZSL）旨在通过类别级别的描述来理解没有训练样本的未见类别。为了提高零样本学习的区分能力，我们从人类创造新艺术的心理过程中获得灵感，对未见类别的视觉学习过程进行建模。我们通过观察将ZSL与人类创造力联系起来，认为零样本学习是关于识别未见之物，而创造力则是关于创造令人喜爱的未见之物。我们引入了一种受创造力文献启发的学习信号，该信号通过幻觉类描述探索未见空间，并鼓励其视觉特征生成与已见类别谨慎偏离，同时允许知识从已见类别转移到未见类别。实证上，我们在CUB和NABirds数据集上，针对从噪声文本中进行的广义ZSL这一挑战性任务，在最大可用基准上展示了相对于现有技术水平的持续改进，提高了几个百分点。我们还在三个额外数据集（AwA2、aPY和SUN）上展示了我们的损失在基于属性的ZSL上的优势。代码可在https://github.com/mhelhoseiny/CIZSL获取。",
        "领域": "零样本学习/创造力建模/视觉特征生成",
        "问题": "提高零样本学习在未见类别上的区分能力",
        "动机": "从人类创造新艺术的心理过程中获得灵感，以改进零样本学习",
        "方法": "引入受创造力文献启发的学习信号，通过幻觉类描述探索未见空间，并鼓励视觉特征生成与已见类别谨慎偏离，同时允许知识从已见类别转移到未见类别",
        "关键词": [
            "零样本学习",
            "创造力建模",
            "视觉特征生成",
            "幻觉类描述",
            "知识转移"
        ],
        "涉及的技术概念": "零样本学习（ZSL）是一种机器学习方法，旨在通过类别级别的描述来理解没有训练样本的未见类别。创造力建模是指从人类创造新艺术的心理过程中获得灵感，以改进机器学习模型。视觉特征生成涉及从图像中提取和生成视觉特征，以便于分类或识别。幻觉类描述是一种技术，用于生成未见类别的描述，以探索未见空间。知识转移是指将从一个任务或类别中学到的知识应用到另一个任务或类别中。"
    },
    {
        "order": 462,
        "title": "Learning to Collocate Neural Modules for Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "We do not speak word by word from scratch; our brain quickly structures a pattern like sth do sth at someplace and then fill in the detailed description. To render existing encoder-decoder image captioners such human-like reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the \"inner pattern\" connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q&A, where the language (i.e., question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design --- one for function words and three for visual content words (e.g., noun, adjective, and verb), 2) soft module fusion and multi-step module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (e.g., adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, e.g., by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.",
        "中文标题": "学习为图像描述排列神经模块",
        "摘要翻译": "我们不是从零开始逐字说话；我们的大脑快速构建一个模式，如某物在某地做某事，然后填充详细描述。为了使现有的编码器-解码器图像描述器具有类似人类的推理能力，我们提出了一个新框架：学习排列神经模块（CNM），以生成连接视觉编码器和语言解码器的“内部模式”。与视觉问答中广泛使用的神经模块网络不同，其中语言（即问题）是完全可观察的，用于描述的CNM更具挑战性，因为语言正在生成，因此是部分可观察的。为此，我们为CNM训练做出了以下技术贡献：1）紧凑的模块设计——一个用于功能词，三个用于视觉内容词（例如，名词、形容词和动词），2）软模块融合和多步模块执行，增强了部分观察中的视觉推理，3）模块控制器的语言损失，忠实于词性搭配（例如，形容词在名词之前）。在具有挑战性的MS-COCO图像描述基准上进行的大量实验验证了我们的CNM图像描述器的有效性。特别是，CNM在Karpathy分割上实现了新的最先进的127.9 CIDEr-D，在官方服务器上实现了单模型126.0 c40。CNM对少量训练样本也具有鲁棒性，例如，通过每张图像仅训练一个句子，CNM可以将性能损失减半，与强基线相比。",
        "领域": "图像描述/神经模块网络/视觉推理",
        "问题": "如何使图像描述器具有类似人类的推理能力",
        "动机": "为了使现有的编码器-解码器图像描述器具有类似人类的推理能力",
        "方法": "提出了一个新框架：学习排列神经模块（CNM），包括紧凑的模块设计、软模块融合和多步模块执行、以及模块控制器的语言损失",
        "关键词": [
            "图像描述",
            "神经模块网络",
            "视觉推理"
        ],
        "涉及的技术概念": {
            "编码器-解码器": "一种用于图像描述的模型结构，其中编码器将图像转换为特征表示，解码器根据这些特征生成描述文本。",
            "神经模块网络": "一种模块化的神经网络结构，允许网络根据输入动态选择和组合不同的模块来处理信息。",
            "CIDEr-D": "一种用于评估图像描述质量的指标，通过计算生成的描述与参考描述之间的相似度来评分。",
            "部分可观察": "在生成语言描述的过程中，由于语言是逐步生成的，因此在任何给定时刻，只有部分信息是可观察的。"
        }
    },
    {
        "order": 463,
        "title": "Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bragman_Stochastic_Filter_Groups_for_Multi-Task_CNNs_Learning_Specialist_and_Generalist_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bragman_Stochastic_Filter_Groups_for_Multi-Task_CNNs_Learning_Specialist_and_Generalist_ICCV_2019_paper.html",
        "abstract": "The performance of multi-task learning in Convolutional Neural Networks (CNNs) hinges on the design of feature sharing between tasks within the architecture. The number of possible sharing patterns are combinatorial in the depth of the network and the number of tasks, and thus hand-crafting an architecture, purely based on the human intuitions of task relationships can be time-consuming and suboptimal. In this paper, we present a probabilistic approach to learning task-specific and shared representations in CNNs for multi-task learning. Specifically, we propose \"stochastic filter groups\" (SFG), a mechanism to assign convolution kernels in each layer to \"specialist\" and \"generalist\" groups, which are specific to and shared across different tasks, respectively. The SFG modules determine the connectivity between layers and the structures of task-specific and shared representations in the network. We employ variational inference to learn the posterior distribution over the possible grouping of kernels and network parameters. Experiments demonstrate the proposed method generalises across multiple tasks and shows improved performance over baseline methods.",
        "中文标题": "多任务CNNs的随机滤波器组：学习专家和通用卷积核",
        "摘要翻译": "卷积神经网络（CNNs）中多任务学习的性能取决于架构内任务之间特征共享的设计。可能的共享模式的数量在网络深度和任务数量上是组合的，因此，仅基于任务关系的人类直觉手工设计架构可能既耗时又不理想。在本文中，我们提出了一种概率方法，用于在CNNs中学习任务特定和共享的表示，以进行多任务学习。具体来说，我们提出了“随机滤波器组”（SFG），这是一种机制，用于将每一层的卷积核分配给“专家”和“通用”组，这些组分别特定于和共享于不同任务。SFG模块决定了层之间的连接性以及网络中任务特定和共享表示的结构。我们采用变分推理来学习可能的核分组和网络参数的后验分布。实验表明，所提出的方法在多个任务上具有普遍性，并显示出比基线方法更好的性能。",
        "领域": "多任务学习/卷积神经网络/变分推理",
        "问题": "设计卷积神经网络中任务间特征共享的模式",
        "动机": "手工设计基于任务关系的架构耗时且可能不理想",
        "方法": "提出随机滤波器组（SFG）机制，采用变分推理学习核分组和网络参数的后验分布",
        "关键词": [
            "多任务学习",
            "卷积神经网络",
            "变分推理",
            "随机滤波器组"
        ],
        "涉及的技术概念": "随机滤波器组（SFG）是一种机制，用于在卷积神经网络中自动分配卷积核到特定任务（专家）和共享任务（通用）的组中，通过变分推理学习这些分组和网络参数的后验分布，以优化多任务学习中的特征共享。"
    },
    {
        "order": 464,
        "title": "GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for Multi-Modal Data Distributions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kundu_GAN-Tree_An_Incrementally_Learned_Hierarchical_Generative_Framework_for_Multi-Modal_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_GAN-Tree_An_Incrementally_Learned_Hierarchical_Generative_Framework_for_Multi-Modal_Data_ICCV_2019_paper.html",
        "abstract": "Despite the remarkable success of generative adversarial networks, their performance seems less impressive for diverse training sets, requiring learning of discontinuous mapping functions. Though multi-mode prior or multi-generator models have been proposed to alleviate this problem, such approaches may fail depending on the empirically chosen initial mode components. In contrast to such bottom-up approaches, we present GAN-Tree, which follows a hierarchical divisive strategy to address such discontinuous multi-modal data. Devoid of any assumption on the number of modes, GAN-Tree utilizes a novel mode-splitting algorithm to effectively split the parent mode to semantically cohesive children modes, facilitating unsupervised clustering. Further, it also enables incremental addition of new data modes to an already trained GAN-Tree, by updating only a single branch of the tree structure. As compared to prior approaches, the proposed framework offers a higher degree of flexibility in choosing a large variety of mutually exclusive and exhaustive tree nodes called GAN-Set. Extensive experiments on synthetic and natural image datasets including ImageNet demonstrate the superiority of GAN-Tree against the prior state-of-the-art.",
        "中文标题": "GAN-Tree: 一种增量学习的层次生成框架用于多模态数据分布",
        "摘要翻译": "尽管生成对抗网络取得了显著的成功，但对于多样化的训练集，它们的表现似乎不那么令人印象深刻，需要学习不连续的映射函数。虽然已经提出了多模式先验或多生成器模型来缓解这个问题，但这种方法可能会根据经验选择的初始模式组件而失败。与这种自下而上的方法相反，我们提出了GAN-Tree，它遵循一种层次分割策略来解决这种不连续的多模态数据。GAN-Tree不假设模式的数量，利用一种新颖的模式分割算法有效地将父模式分割为语义上连贯的子模式，促进了无监督聚类。此外，它还允许通过仅更新树结构的单个分支，将新的数据模式增量添加到已经训练好的GAN-Tree中。与之前的方法相比，所提出的框架在选择大量互斥且穷尽的树节点（称为GAN-Set）方面提供了更高的灵活性。在包括ImageNet在内的合成和自然图像数据集上的广泛实验证明了GAN-Tree相对于先前最先进技术的优越性。",
        "领域": "生成模型/无监督学习/图像生成",
        "问题": "解决生成对抗网络在处理多样化训练集时表现不佳的问题，特别是需要学习不连续映射函数的情况。",
        "动机": "现有的多模式先验或多生成器模型可能因初始模式组件的选择而失败，需要一种更灵活的方法来处理不连续的多模态数据。",
        "方法": "提出了一种名为GAN-Tree的层次生成框架，采用模式分割算法将父模式分割为语义上连贯的子模式，并允许增量添加新的数据模式。",
        "关键词": [
            "生成对抗网络",
            "多模态数据",
            "无监督聚类",
            "模式分割",
            "增量学习"
        ],
        "涉及的技术概念": "GAN-Tree是一种层次生成框架，通过模式分割算法处理不连续的多模态数据，支持无监督聚类和增量学习。GAN-Set是框架中互斥且穷尽的树节点，提供了选择上的灵活性。"
    },
    {
        "order": 465,
        "title": "Generating Easy-to-Understand Referring Expressions for Target Identifications",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tanaka_Generating_Easy-to-Understand_Referring_Expressions_for_Target_Identifications_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tanaka_Generating_Easy-to-Understand_Referring_Expressions_for_Target_Identifications_ICCV_2019_paper.html",
        "abstract": "This paper addresses the generation of referring expressions that not only refer to objects correctly but also let humans find them quickly. As a target becomes relatively less salient, identifying referred objects itself becomes more difficult. However, the existing studies regarded all sentences that refer to objects correctly as equally good, ignoring whether they are easily understood by humans. If the target is not salient, humans utilize relationships with the salient contexts around it to help listeners to comprehend it better. To derive this information from human annotations, our model is designed to extract information from the target and from the environment. Moreover, we regard that sentences that are easily understood are those that are comprehended correctly and quickly by humans. We optimized this by using the time required to locate the referred objects by humans and their accuracies. To evaluate our system, we created a new referring expression dataset whose images were acquired from Grand Theft Auto V (GTA V), limiting targets to persons. Experimental results show the effectiveness of our approach. Our code and dataset are available at https://github.com/mikittt/easy-to-understand-REG.",
        "中文标题": "为目标识别生成易于理解的指代表达",
        "摘要翻译": "本文探讨了指代表达的生成问题，这些表达不仅正确指代对象，还能让人类快速找到它们。随着目标相对不那么显著，识别被指代的对象本身变得更加困难。然而，现有研究将所有正确指代对象的句子视为同等优秀，忽略了它们是否易于人类理解。如果目标不显著，人类会利用其周围显著环境的关系来帮助听者更好地理解。为了从人类注释中提取这些信息，我们的模型设计为从目标和环境中提取信息。此外，我们认为易于理解的句子是那些被人类正确且快速理解的句子。我们通过使用人类定位被指代对象所需的时间及其准确性来优化这一点。为了评估我们的系统，我们创建了一个新的指代表达数据集，其图像来自《侠盗猎车手V》（GTA V），并将目标限制为人。实验结果表明了我们方法的有效性。我们的代码和数据集可在https://github.com/mikittt/easy-to-understand-REG获取。",
        "领域": "自然语言处理/人机交互/游戏AI",
        "问题": "生成不仅正确指代对象，还能让人类快速找到它们的指代表达",
        "动机": "现有研究忽略了指代表达是否易于人类理解，特别是在目标不显著的情况下",
        "方法": "设计模型从目标和环境中提取信息，并通过人类定位被指代对象所需的时间及其准确性来优化指代表达的生成",
        "关键词": [
            "指代表达",
            "目标识别",
            "人机交互",
            "游戏AI"
        ],
        "涉及的技术概念": "指代表达生成、目标显著性、环境关系利用、人类注释、数据集创建"
    },
    {
        "order": 466,
        "title": "Transferability and Hardness of Supervised Classification Tasks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tran_Transferability_and_Hardness_of_Supervised_Classification_Tasks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tran_Transferability_and_Hardness_of_Supervised_Classification_Tasks_ICCV_2019_paper.html",
        "abstract": "We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets---CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks)---together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for highly transferable attributes.",
        "中文标题": "监督分类任务的可转移性与难度",
        "摘要翻译": "我们提出了一种新颖的方法来估计监督分类任务的难度和可转移性。与之前的工作不同，我们的方法是解决方案无关的，不需要也不假设有训练好的模型。相反，我们使用信息论的方法来估计这些值：将训练标签视为随机变量并探索它们的统计特性。当从源任务转移到目标任务时，我们考虑两个这样的变量之间的条件熵（即两个任务的标签分配）。我们通过分析和实证表明，这个值与转移模型的损失有关。我们进一步展示了如何使用这个值来估计任务的难度。我们在三个大规模数据集上广泛测试了我们的主张——CelebA（40个任务）、Animals with Attributes 2（85个任务）和Caltech-UCSD Birds 200（312个任务）——总共代表了437个分类任务。我们提供的结果显示，我们的难度和可转移性估计与实证难度和可转移性有很强的相关性。作为案例研究，我们将一个学习到的人脸识别模型转移到CelebA属性分类任务中，展示了对于高度可转移属性的最先进准确性。",
        "领域": "监督学习/信息论/模型转移",
        "问题": "估计监督分类任务的难度和可转移性",
        "动机": "为了在没有训练好的模型的情况下，估计监督分类任务的难度和可转移性，以便更好地理解和利用模型转移",
        "方法": "使用信息论的方法，将训练标签视为随机变量并探索它们的统计特性，通过分析两个任务标签分配之间的条件熵来估计任务难度和模型转移的损失",
        "关键词": [
            "监督分类",
            "任务难度",
            "可转移性",
            "信息论",
            "条件熵"
        ],
        "涉及的技术概念": "条件熵是信息论中的一个概念，用于衡量在已知一个随机变量的情况下，另一个随机变量的不确定性。在本研究中，条件熵被用来衡量从一个任务转移到另一个任务时的不确定性，从而估计任务的难度和模型转移的损失。"
    },
    {
        "order": 467,
        "title": "Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Aneja_Sequential_Latent_Spaces_for_Modeling_the_Intention_During_Diverse_Image_ICCV_2019_paper.html",
        "abstract": "Diverse and accurate vision+language modeling is an important goal to retain creative freedom and maintain user engagement. However, adequately capturing the intricacies of diversity in language models is challenging. Recent works commonly resort to latent variable models augmented with more or less supervision from object detectors or part-of-speech tags. In common to all those methods is the fact that the latent variable either only initializes the sentence generation process or is identical across the steps of generation. Both methods offer no fine-grained control. To address this concern, we propose Seq-CVAE which learns a latent space for every word. We encourage this temporal latent space to capture the 'intention' about how to complete the sentence by mimicking a representation which summarizes the future. We illustrate the efficacy of the proposed approach on the challenging MSCOCO dataset, significantly improving diversity metrics compared to baselines while performing on par w.r.t. sentence quality.",
        "中文标题": "序列潜在空间用于多样化图像描述中的意图建模",
        "摘要翻译": "多样且准确的视觉+语言建模是保持创作自由和维持用户参与度的重要目标。然而，充分捕捉语言模型中多样性的复杂性是具有挑战性的。最近的工作通常采用潜在变量模型，并通过对象检测器或词性标签提供或多或少的监督。这些方法的共同点是潜在变量要么仅初始化句子生成过程，要么在生成步骤中保持不变。这两种方法都没有提供细粒度的控制。为了解决这个问题，我们提出了Seq-CVAE，它为每个单词学习一个潜在空间。我们通过模仿一个总结未来的表示，鼓励这个时间潜在空间捕捉关于如何完成句子的'意图'。我们在具有挑战性的MSCOCO数据集上展示了所提出方法的有效性，与基线相比显著提高了多样性指标，同时在句子质量方面表现相当。",
        "领域": "自然语言处理/计算机视觉/生成模型",
        "问题": "如何在图像描述中实现多样性和准确性的平衡",
        "动机": "为了保持创作自由和用户参与度，需要开发能够生成多样且准确描述的模型",
        "方法": "提出Seq-CVAE模型，为每个单词学习一个潜在空间，通过模仿总结未来的表示来捕捉完成句子的意图",
        "关键词": [
            "多样化图像描述",
            "潜在变量模型",
            "Seq-CVAE"
        ],
        "涉及的技术概念": {
            "潜在变量模型": "一种统计模型，其中某些变量是未观测到的（潜在的），用于解释观测数据中的模式",
            "Seq-CVAE": "序列条件变分自编码器，一种用于序列生成的模型，通过学习每个时间步的潜在空间来捕捉生成过程中的意图",
            "MSCOCO数据集": "一个广泛使用的图像描述和对象识别数据集，包含超过120,000张图像，每张图像都有多个描述"
        }
    },
    {
        "order": 468,
        "title": "GODS: Generalized One-Class Discriminative Subspaces for Anomaly Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_GODS_Generalized_One-Class_Discriminative_Subspaces_for_Anomaly_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_GODS_Generalized_One-Class_Discriminative_Subspaces_for_Anomaly_Detection_ICCV_2019_paper.html",
        "abstract": "One-class learning is the classic problem of fitting a model to data for which annotations are available only for a single class. In this paper, we propose a novel objective for one-class learning. Our key idea is to use a pair of orthonormal frames -- as subspaces -- to \"sandwich\" the labeled data via optimizing for two objectives jointly: i) minimize the distance between the origins of the two subspaces, and ii) to maximize the margin between the hyperplanes and the data, either subspace demanding the data to be in its positive and negative orthant respectively. Our proposed objective however leads to a non-convex optimization problem, to which we resort to Riemannian optimization schemes and derive an efficient conjugate gradient scheme on the Stiefel manifold. To study the effectiveness of our scheme, we propose a new dataset Dash-Cam-Pose, consisting of clips with skeleton poses of humans seated in a car, the task being to classify the clips as normal or abnormal; the latter is when any human pose is out-of-position with regard to say an airbag deployment. Our experiments on the proposed Dash-Cam-Pose dataset, as well as several other standard anomaly/novelty detection benchmarks demonstrate the benefits of our scheme, achieving state-of-the-art one-class accuracy.",
        "中文标题": "GODS: 广义一类判别子空间用于异常检测",
        "摘要翻译": "一类学习是拟合仅对单一类别有注释数据的模型的经典问题。在本文中，我们提出了一种新颖的一类学习目标。我们的关键思想是使用一对正交框架——作为子空间——通过联合优化两个目标来“夹住”标记数据：i) 最小化两个子空间原点之间的距离，ii) 最大化超平面与数据之间的边距，每个子空间分别要求数据位于其正和负象限中。然而，我们提出的目标导致了一个非凸优化问题，我们对此采用了黎曼优化方案，并在Stiefel流形上推导出了一个有效的共轭梯度方案。为了研究我们方案的有效性，我们提出了一个新的数据集Dash-Cam-Pose，该数据集包含坐在车内的人的骨架姿势的剪辑，任务是将剪辑分类为正常或异常；后者是指任何人的姿势相对于安全气囊部署而言是位置不当的。我们在提出的Dash-Cam-Pose数据集以及几个其他标准的异常/新颖性检测基准上的实验证明了我们方案的优势，实现了一类准确性的最先进水平。",
        "领域": "异常检测/骨架姿势分析/一类学习",
        "问题": "解决一类学习中的模型拟合问题，特别是在仅有一个类别有注释数据的情况下。",
        "动机": "提高一类学习的准确性和效率，特别是在异常检测和骨架姿势分析领域。",
        "方法": "提出了一种新颖的一类学习目标，使用一对正交框架作为子空间，通过联合优化两个目标来夹住标记数据，并采用黎曼优化方案解决非凸优化问题。",
        "关键词": [
            "异常检测",
            "骨架姿势分析",
            "一类学习"
        ],
        "涉及的技术概念": "一类学习、正交框架、子空间、黎曼优化、Stiefel流形、共轭梯度方案、异常检测、骨架姿势分析"
    },
    {
        "order": 469,
        "title": "Language-Agnostic Visual-Semantic Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wehrmann_Language-Agnostic_Visual-Semantic_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wehrmann_Language-Agnostic_Visual-Semantic_Embeddings_ICCV_2019_paper.html",
        "abstract": "This paper proposes a framework for training language-invariant cross-modal retrieval models. We also introduce a novel character-based word-embedding approach, allowing the model to project similar words across languages into the same word-embedding space. In addition, by performing cross-modal retrieval at the character level, the storage requirements for a text encoder decrease substantially, allowing for lighter and more scalable retrieval architectures. The proposed language-invariant textual encoder based on characters is virtually unaffected in terms of storage requirements when novel languages are added to the system. Our contributions include new methods for building character-level-based word-embeddings, an improved loss function, and a novel cross-language alignment module that not only makes the architecture language-invariant, but also presents better predictive performance. We show that our models outperform the current state-of-the-art in both single and multi-language scenarios. This work can be seen as the basis of a new path on retrieval research, now allowing for the effective use of captions in multiple-language scenarios. Code is available at https://github.com/jwehrmann/lavse.",
        "中文标题": "语言无关的视觉-语义嵌入",
        "摘要翻译": "本文提出了一种训练语言无关的跨模态检索模型的框架。我们还引入了一种新颖的基于字符的词嵌入方法，使得模型能够将不同语言中的相似词投影到同一个词嵌入空间。此外，通过在字符级别执行跨模态检索，文本编码器的存储需求大幅减少，从而允许更轻量级和可扩展的检索架构。所提出的基于字符的语言无关文本编码器在添加新语言时，存储需求几乎不受影响。我们的贡献包括构建基于字符级别的词嵌入的新方法、改进的损失函数以及一种新颖的跨语言对齐模块，该模块不仅使架构语言无关，而且提供了更好的预测性能。我们展示了我们的模型在单语言和多语言场景下均优于当前的最新技术。这项工作可以被视为检索研究新路径的基础，现在允许在多语言场景中有效使用字幕。代码可在https://github.com/jwehrmann/lavse获取。",
        "领域": "跨模态检索/自然语言处理/计算机视觉",
        "问题": "如何在不同语言之间实现有效的跨模态检索",
        "动机": "为了在多语言环境中更有效地使用字幕进行跨模态检索，减少存储需求并提高检索架构的可扩展性",
        "方法": "提出了一种基于字符的词嵌入方法，改进的损失函数，以及一种新颖的跨语言对齐模块",
        "关键词": [
            "跨模态检索",
            "词嵌入",
            "跨语言对齐"
        ],
        "涉及的技术概念": "跨模态检索指的是在不同类型的数据（如文本和图像）之间进行检索；词嵌入是一种将词汇映射到向量空间的技术，以便于计算词汇之间的相似度；跨语言对齐模块是一种技术，用于确保不同语言的词汇能够在相同的向量空间中对齐，从而实现语言无关的检索。"
    },
    {
        "order": 470,
        "title": "Moment Matching for Multi-Source Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.html",
        "abstract": "Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/",
        "中文标题": "多源域适应的矩匹配",
        "摘要翻译": "传统的无监督域适应（UDA）假设训练数据是从单个域中采样的。这忽略了训练数据从多个源收集的更实际场景，这需要多源域适应。我们为解决这个问题做出了三个主要贡献。首先，我们收集并注释了迄今为止最大的UDA数据集，称为DomainNet，它包含六个域和大约60万张分布在345个类别中的图像，解决了多源UDA研究中的数据可用性差距。其次，我们提出了一种新的深度学习方法，即多源域适应的矩匹配（M3SDA），旨在通过动态对齐它们的特征分布的矩，将从多个标记源域学到的知识转移到未标记的目标域。第三，我们为单源和多源域适应中的矩匹配方法提供了新的理论见解。进行了广泛的实验，以展示我们的新数据集在基准测试最先进的多源域适应方法方面的能力，以及我们提出的模型的优势。数据集和代码可在http://ai.bu.edu/M3SDA/获取。",
        "领域": "域适应/深度学习/特征分布对齐",
        "问题": "解决多源域适应问题，即如何将从多个标记源域学到的知识有效地转移到未标记的目标域。",
        "动机": "现有的无监督域适应方法大多假设训练数据来自单一域，这在实际应用中存在局限性，因为训练数据往往来自多个源。因此，研究多源域适应方法具有重要的实际意义。",
        "方法": "提出了一种新的深度学习方法，即多源域适应的矩匹配（M3SDA），通过动态对齐多个源域和目标域的特征分布的矩来实现知识转移。",
        "关键词": [
            "多源域适应",
            "矩匹配",
            "特征分布对齐",
            "无监督学习"
        ],
        "涉及的技术概念": "无监督域适应（UDA）是一种机器学习技术，旨在将从一个或多个源域学到的知识应用到目标域，而无需目标域的标签。矩匹配是一种统计方法，用于通过匹配分布的低阶矩来对齐不同分布。在本文中，矩匹配被用来对齐多个源域和目标域的特征分布，以实现有效的知识转移。"
    },
    {
        "order": 471,
        "title": "Why Does a Visual Question Have Different Answers?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Why_Does_a_Visual_Question_Have_Different_Answers_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bhattacharya_Why_Does_a_Visual_Question_Have_Different_Answers_ICCV_2019_paper.html",
        "abstract": "Visual question answering is the task of returning the answer to a question about an image. A challenge is that different people often provide different answers to the same visual question. To our knowledge, this is the first work that aims to understand why. We propose a taxonomy of nine plausible reasons, and create two labelled datasets consisting of  45,000 visual questions indicating which reasons led to answer differences. We then propose a novel problem of predicting directly from a visual question which reasons will cause answer differences as well as a novel algorithm for this purpose. Experiments demonstrate the advantage of our approach over several related baselines on two diverse datasets. We publicly share the datasets and code at https://vizwiz.org.",
        "中文标题": "为什么一个视觉问题会有不同的答案？",
        "摘要翻译": "视觉问答任务是返回关于图像问题的答案。一个挑战是，不同的人经常对同一个视觉问题提供不同的答案。据我们所知，这是第一个旨在理解其原因的工作。我们提出了一个包含九个可能原因的分类法，并创建了两个标记数据集，包含45,000个视觉问题，指示哪些原因导致了答案的差异。然后，我们提出了一个新颖的问题，即直接从视觉问题预测哪些原因会导致答案差异，以及为此目的的新算法。实验证明了我们的方法在两个不同数据集上相对于几个相关基线的优势。我们在https://vizwiz.org上公开分享了数据集和代码。",
        "领域": "视觉问答/数据集构建/算法设计",
        "问题": "理解并预测视觉问答中答案差异的原因",
        "动机": "探索不同人对同一视觉问题提供不同答案的原因，以提高视觉问答系统的准确性和可靠性",
        "方法": "提出一个包含九个可能原因的分类法，创建两个标记数据集，并开发一个新颖的算法直接从视觉问题预测导致答案差异的原因",
        "关键词": [
            "视觉问答",
            "答案差异",
            "数据集构建",
            "算法设计"
        ],
        "涉及的技术概念": "视觉问答任务涉及从图像中提取信息并回答相关问题。本研究通过构建一个包含九个可能原因的分类法，旨在理解和预测视觉问答中答案差异的原因。此外，研究还涉及创建标记数据集和开发新算法，以提高视觉问答系统的性能。"
    },
    {
        "order": 472,
        "title": "Neighborhood Preserving Hashing for Scalable Video Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Neighborhood_Preserving_Hashing_for_Scalable_Video_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Neighborhood_Preserving_Hashing_for_Scalable_Video_Retrieval_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a Neighborhood Preserving Hashing (NPH) method for scalable video retrieval in an unsupervised manner. Unlike most existing deep video hashing methods which indiscriminately compress an entire video into a binary code, we embed the spatial-temporal neighborhood information into the encoding network such that the neighborhood-relevant visual content of a video can be preferentially encoded into a binary code under the guidance of the neighborhood information. Specifically, we propose a neighborhood attention mechanism which focuses on partial useful content of each input frame conditioned on the neighborhood information. We then integrate the neighborhood attention mechanism into an RNN-based reconstruction scheme to encourage the binary codes to capture the spatial-temporal structure in a video which is consistent with that in the neighborhood. As a consequence, the learned hashing functions can map similar videos to similar binary codes. Extensive experiments on three widely-used benchmark datasets validate the effectiveness of our proposed approach.",
        "中文标题": "用于可扩展视频检索的邻域保持哈希",
        "摘要翻译": "本文提出了一种无监督的邻域保持哈希（NPH）方法，用于可扩展的视频检索。与大多数现有的深度视频哈希方法不同，这些方法不加区分地将整个视频压缩成二进制代码，我们将时空邻域信息嵌入到编码网络中，使得在邻域信息的指导下，视频的邻域相关视觉内容可以优先编码成二进制代码。具体来说，我们提出了一种邻域注意力机制，该机制根据邻域信息关注每个输入帧的部分有用内容。然后，我们将邻域注意力机制集成到基于RNN的重建方案中，以鼓励二进制代码捕捉视频中的时空结构，该结构与邻域中的结构一致。因此，学习到的哈希函数可以将相似的视频映射到相似的二进制代码。在三个广泛使用的基准数据集上的大量实验验证了我们提出方法的有效性。",
        "领域": "视频检索/哈希学习/注意力机制",
        "问题": "如何在无监督的情况下，有效地进行可扩展的视频检索",
        "动机": "现有的深度视频哈希方法不加区分地将整个视频压缩成二进制代码，忽略了视频中邻域相关视觉内容的重要性，导致检索效果不佳。",
        "方法": "提出了一种邻域保持哈希（NPH）方法，通过嵌入时空邻域信息到编码网络中，并引入邻域注意力机制，优先编码邻域相关的视觉内容，同时集成到基于RNN的重建方案中，以捕捉视频中的时空结构。",
        "关键词": [
            "视频检索",
            "哈希学习",
            "注意力机制",
            "RNN",
            "时空结构"
        ],
        "涉及的技术概念": {
            "邻域保持哈希（NPH）": "一种无监督的视频哈希方法，通过嵌入时空邻域信息到编码网络中，优先编码邻域相关的视觉内容。",
            "邻域注意力机制": "一种机制，根据邻域信息关注每个输入帧的部分有用内容，以提高编码的效率和准确性。",
            "RNN": "循环神经网络，用于捕捉视频中的时空结构，与邻域信息一致。",
            "时空结构": "视频中时间和空间上的结构信息，对于视频检索至关重要。"
        }
    },
    {
        "order": 473,
        "title": "Unsupervised Domain Adaptation via Regularized Conditional Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cicek_Unsupervised_Domain_Adaptation_via_Regularized_Conditional_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cicek_Unsupervised_Domain_Adaptation_via_Regularized_Conditional_Alignment_ICCV_2019_paper.html",
        "abstract": "We propose a method for unsupervised domain adaptation that trains a shared embedding to align the joint distributions of inputs (domain) and outputs (classes), making any classifier agnostic to the domain. Joint alignment ensures that not only the marginal distributions of the domains are aligned, but the labels as well. We propose a novel objective function that encourages the class-conditional distributions to have disjoint support in feature space. We further exploit adversarial regularization to improve the performance of the classifier on the domain for which no annotated data is available.",
        "中文标题": "通过正则化条件对齐的无监督领域适应",
        "摘要翻译": "我们提出了一种无监督领域适应的方法，该方法训练一个共享嵌入以对齐输入（领域）和输出（类别）的联合分布，使得任何分类器对领域不可知。联合对齐确保不仅领域的边际分布对齐，而且标签也对齐。我们提出了一种新的目标函数，鼓励类条件分布在特征空间中具有不相交的支持。我们进一步利用对抗正则化来提高分类器在没有注释数据的领域上的性能。",
        "领域": "领域适应/特征学习/对抗学习",
        "问题": "解决无监督领域适应中领域间分布对齐的问题",
        "动机": "为了使分类器在不同领域间具有更好的泛化能力，特别是在没有标注数据的领域上",
        "方法": "提出了一种新的目标函数，通过正则化条件对齐和对抗正则化来训练共享嵌入，以对齐输入和输出的联合分布",
        "关键词": [
            "无监督学习",
            "领域适应",
            "对抗正则化"
        ],
        "涉及的技术概念": "无监督领域适应是指在没有目标领域标注数据的情况下，通过源领域的标注数据来适应目标领域的技术。联合分布对齐是指同时对齐输入（领域）和输出（类别）的分布，以确保分类器对领域不可知。对抗正则化是一种通过引入对抗样本来提高模型泛化能力的技术。"
    },
    {
        "order": 474,
        "title": "Adversarial Representation Learning for Text-to-Image Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.html",
        "abstract": "For many computer vision applications such as image captioning, visual question answering, and person search, learning discriminative feature representations at both image and text level is an essential yet challenging problem. Its challenges originate from the large word variance in the text domain as well as the difficulty of accurately measuring the distance between the features of the two modalities. Most prior work focuses on the latter challenge, by introducing loss functions that help the network learn better feature representations but fail to account for the complexity of the textual input. With that in mind, we introduce TIMAM: a Text-Image Modality Adversarial Matching approach that learns modality-invariant feature representations using adversarial and cross-modal matching objectives. In addition, we demonstrate that BERT, a publicly-available language model that extracts word embeddings, can successfully be applied in the text-to-image matching domain. The proposed approach achieves state-of-the-art cross-modal matching performance on four widely-used publicly-available datasets resulting in absolute improvements ranging from 2% to 5% in terms of rank-1 accuracy.",
        "中文标题": "对抗性表示学习用于文本到图像匹配",
        "摘要翻译": "对于许多计算机视觉应用，如图像字幕、视觉问答和人物搜索，在图像和文本级别学习区分性特征表示是一个基本但具有挑战性的问题。其挑战源于文本领域中的大词汇量以及准确测量两种模态特征之间距离的困难。大多数先前的工作都集中在后一个挑战上，通过引入损失函数帮助网络学习更好的特征表示，但未能考虑到文本输入的复杂性。考虑到这一点，我们引入了TIMAM：一种文本-图像模态对抗性匹配方法，它使用对抗性和跨模态匹配目标来学习模态不变的特征表示。此外，我们证明了BERT，一个公开可用的提取词嵌入的语言模型，可以成功地应用于文本到图像匹配领域。所提出的方法在四个广泛使用的公开可用数据集上实现了最先进的跨模态匹配性能，在rank-1准确率方面实现了2%到5%的绝对提升。",
        "领域": "图像字幕/视觉问答/人物搜索",
        "问题": "在图像和文本级别学习区分性特征表示",
        "动机": "解决文本领域中的大词汇量以及准确测量两种模态特征之间距离的困难",
        "方法": "引入TIMAM：一种文本-图像模态对抗性匹配方法，使用对抗性和跨模态匹配目标来学习模态不变的特征表示",
        "关键词": [
            "对抗性学习",
            "跨模态匹配",
            "BERT",
            "特征表示"
        ],
        "涉及的技术概念": {
            "TIMAM": "一种文本-图像模态对抗性匹配方法，用于学习模态不变的特征表示",
            "BERT": "一个公开可用的语言模型，用于提取词嵌入，成功应用于文本到图像匹配领域",
            "对抗性和跨模态匹配目标": "用于学习模态不变的特征表示的方法"
        }
    },
    {
        "order": 475,
        "title": "G3raphGround: Graph-Based Language Grounding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bajaj_G3raphGround_Graph-Based_Language_Grounding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bajaj_G3raphGround_Graph-Based_Language_Grounding_ICCV_2019_paper.html",
        "abstract": "In this paper we present an end-to-end framework for grounding of phrases in images. In contrast to previous works, our model, which we call GraphGround, uses graphs to formulate more complex, non-sequential dependencies among proposal image regions and phrases. We capture intra-modal dependencies using a separate graph neural network for each modality (visual and lingual), and then use conditional message-passing in another graph neural network to fuse their outputs and capture cross-modal relationships. This final representation results in grounding decisions. The framework supports many-to-many matching and is able to ground single phrase to multiple image regions and vice versa. We validate our design choices through a series of ablation studies and illustrate state-of-the-art performance on Flickr30k and ReferIt Game benchmark datasets.",
        "中文标题": "G3raphGround: 基于图的语言接地",
        "摘要翻译": "在本文中，我们提出了一个端到端的框架，用于在图像中对接短语。与之前的工作相比，我们的模型，我们称之为GraphGround，使用图来制定提案图像区域和短语之间更复杂的非顺序依赖关系。我们使用单独的图神经网络为每种模态（视觉和语言）捕捉模态内依赖关系，然后在另一个图神经网络中使用条件消息传递来融合它们的输出并捕捉跨模态关系。这种最终表示导致接地决策。该框架支持多对多匹配，并能够将单个短语接地到多个图像区域，反之亦然。我们通过一系列消融研究验证了我们的设计选择，并在Flickr30k和ReferIt Game基准数据集上展示了最先进的性能。",
        "领域": "视觉语言接地/图神经网络/多模态学习",
        "问题": "如何在图像中准确地将短语接地到相应的图像区域",
        "动机": "为了克服现有方法在处理复杂、非顺序依赖关系方面的限制，提出了一种新的基于图的框架，以提高短语接地的准确性和灵活性。",
        "方法": "使用图神经网络分别捕捉视觉和语言模态内的依赖关系，然后通过条件消息传递在另一个图神经网络中融合这些输出，以捕捉跨模态关系，从而实现接地决策。",
        "关键词": [
            "视觉语言接地",
            "图神经网络",
            "多模态学习",
            "条件消息传递",
            "多对多匹配"
        ],
        "涉及的技术概念": {
            "图神经网络": "一种用于处理图结构数据的神经网络，能够捕捉节点之间的关系。",
            "条件消息传递": "在图神经网络中，根据特定条件在节点之间传递信息，以融合不同模态的数据。",
            "多对多匹配": "一种匹配方式，允许一个短语对应多个图像区域，或一个图像区域对应多个短语。"
        }
    },
    {
        "order": 476,
        "title": "Self-Training With Progressive Augmentation for Unsupervised Cross-Domain Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Self-Training_With_Progressive_Augmentation_for_Unsupervised_Cross-Domain_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Person re-identification (Re-ID) has achieved great improvement with deep learning and a large amount of labelled training data. However, it remains a challenging task for adapting a model trained in a source domain of labelled data to a target domain of only unlabelled data available. In this work, we develop a self-training method with progressive augmentation framework (PAST) to promote the model performance progressively on the target dataset. Specially, our PAST framework consists of two stages, namely, conservative stage and promoting stage. The conservative stage captures the local structure of target-domain data points with triplet-based loss functions, leading to improved feature representations. The promoting stage continuously optimizes the network by appending a changeable classification layer to the last layer of the model, enabling the use of global information about the data distribution. Importantly, we propose a new self-training strategy that progressively augments the model capability by adopting conservative and promoting stages alternately. Furthermore, to improve the reliability of selected triplet samples, we introduce a ranking-based triplet loss in the conservative stage, which is a label-free objective function based on the similarities between data pairs. Experiments demonstrate that the proposed method achieves state-of-the-art person Re-ID performance under the unsupervised cross-domain setting. Code is available at: tinyurl.com/PASTReID",
        "中文标题": "自训练与渐进增强的无监督跨域行人重识别",
        "摘要翻译": "行人重识别（Re-ID）在深度学习和大量标注训练数据的帮助下取得了巨大进步。然而，将在一个有标注数据的源域上训练的模型适应到只有未标注数据可用的目标域仍然是一个具有挑战性的任务。在这项工作中，我们开发了一种带有渐进增强框架的自训练方法（PAST），以逐步提高模型在目标数据集上的性能。特别地，我们的PAST框架包括两个阶段，即保守阶段和促进阶段。保守阶段通过基于三元组的损失函数捕捉目标域数据点的局部结构，从而改善特征表示。促进阶段通过在模型的最后一层添加一个可变的分类层来持续优化网络，使得能够利用关于数据分布的全局信息。重要的是，我们提出了一种新的自训练策略，通过交替采用保守和促进阶段来逐步增强模型能力。此外，为了提高所选三元组样本的可靠性，我们在保守阶段引入了一种基于排名的三元组损失，这是一种基于数据对之间相似性的无标签目标函数。实验证明，所提出的方法在无监督跨域设置下实现了最先进的行人重识别性能。代码可在：tinyurl.com/PASTReID 获取。",
        "领域": "行人重识别/无监督学习/跨域适应",
        "问题": "如何在没有标注数据的目标域上有效进行行人重识别",
        "动机": "解决在只有未标注数据可用的目标域上，如何有效进行行人重识别的挑战",
        "方法": "开发了一种带有渐进增强框架的自训练方法（PAST），包括保守阶段和促进阶段，通过交替采用这两个阶段来逐步增强模型能力，并在保守阶段引入基于排名的三元组损失以提高所选三元组样本的可靠性",
        "关键词": [
            "行人重识别",
            "无监督学习",
            "跨域适应",
            "自训练",
            "渐进增强"
        ],
        "涉及的技术概念": {
            "自训练": "一种利用模型自身预测结果作为伪标签来进一步训练模型的方法",
            "渐进增强框架（PAST）": "包括保守阶段和促进阶段，通过交替采用这两个阶段来逐步增强模型能力",
            "基于三元组的损失函数": "用于捕捉目标域数据点的局部结构，改善特征表示",
            "基于排名的三元组损失": "一种无标签目标函数，基于数据对之间的相似性，用于提高所选三元组样本的可靠性"
        }
    },
    {
        "order": 477,
        "title": "Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Larger_Norm_More_Transferable_An_Adaptive_Feature_Norm_Approach_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Larger_Norm_More_Transferable_An_Adaptive_Feature_Norm_Approach_for_ICCV_2019_paper.html",
        "abstract": "Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN.",
        "中文标题": "更大的范数更具可转移性：一种用于无监督领域自适应的自适应特征范数方法",
        "摘要翻译": "领域自适应通过减轻分布间的领域偏移，使学习者能够安全地泛化到新环境中。以往的工作可能未能有效揭示导致目标任务上模型性能急剧下降的根本原因。在本文中，我们通过实验揭示了目标领域的错误判别主要源于其特征范数相对于源领域要小得多。为此，我们提出了一种新颖的无参数自适应特征范数方法。我们证明了逐步将两个领域的特征范数适应到较大范围的值可以带来显著的转移增益，这意味着那些具有较大范数的任务特定特征更具可转移性。我们的方法成功地统一了标准和部分领域自适应的计算，对负转移问题具有更强的鲁棒性。无需复杂的技巧，仅需几行代码，我们的方法显著提升了目标任务的性能，并大幅超越了现有技术（在Office-Home上提升了11.5%，在VisDA2017上提升了17.1%）。我们希望我们简单而有效的方法能为未来的转移学习研究提供一些启示。代码可在https://github.com/jihanyang/AFN获取。",
        "领域": "领域自适应/特征学习/转移学习",
        "问题": "解决目标领域特征范数较小导致的模型性能下降问题",
        "动机": "揭示并解决目标领域由于特征范数较小而导致的错误判别问题，以提高领域自适应的效果",
        "方法": "提出一种无参数的自适应特征范数方法，通过逐步调整两个领域的特征范数到较大范围，以增强任务特定特征的可转移性",
        "关键词": [
            "领域自适应",
            "特征范数",
            "转移学习"
        ],
        "涉及的技术概念": "领域自适应是一种技术，旨在通过减轻源领域和目标领域之间的分布差异，使模型能够在目标领域上有效泛化。特征范数指的是特征向量的长度或大小，是衡量特征重要性的一个指标。转移学习是一种机器学习方法，它利用从一个任务学到的知识来帮助解决另一个相关但不同的任务。"
    },
    {
        "order": 478,
        "title": "Multi-Modality Latent Interaction Network for Visual Question Answering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.html",
        "abstract": "Exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for Visual Question Answering (VQA). However, we argue that existing methods mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. From humans' perspective, answering a visual question requires understanding the summarizations of visual and language information. In this paper, we proposed the Multi-modality Latent Interaction module (MLI) to tackle this problem. The proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. The cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. Such MLI modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public VQA benchmarks, VQA v2.0 and TDIUC . In addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model BERT.",
        "中文标题": "多模态潜在交互网络用于视觉问答",
        "摘要翻译": "利用视觉区域和问题词之间的关系在学习视觉问答（VQA）的多模态特征方面取得了巨大成功。然而，我们认为现有方法大多模拟单个视觉区域和词之间的关系，这不足以正确回答问题。从人类的角度来看，回答视觉问题需要理解视觉和语言信息的总结。在本文中，我们提出了多模态潜在交互模块（MLI）来解决这个问题。所提出的模块学习潜在视觉和语言总结之间的跨模态关系，这些总结将视觉区域和问题总结为少量潜在表示，以避免模拟无信息的单个区域-词关系。潜在总结之间的跨模态信息被传播以融合来自两种模态的有价值信息，并用于更新视觉和词特征。这样的MLI模块可以堆叠多个阶段，以模拟两种模态之间的复杂和潜在关系，并在公共VQA基准测试VQA v2.0和TDIUC上实现了极具竞争力的性能。此外，我们展示了通过结合预训练语言模型BERT，我们方法的性能可以显著提高。",
        "领域": "视觉问答/多模态学习/自然语言处理",
        "问题": "现有方法在模拟视觉区域和问题词之间的关系时，未能充分理解视觉和语言信息的总结，导致回答问题的准确性不足。",
        "动机": "从人类回答视觉问题的角度出发，需要更深入地理解视觉和语言信息的总结，以提高视觉问答系统的性能。",
        "方法": "提出了多模态潜在交互模块（MLI），该模块学习潜在视觉和语言总结之间的跨模态关系，通过总结视觉区域和问题为少量潜在表示，避免模拟无信息的单个区域-词关系，并融合来自两种模态的有价值信息来更新视觉和词特征。",
        "关键词": [
            "视觉问答",
            "多模态学习",
            "潜在交互"
        ],
        "涉及的技术概念": "多模态潜在交互模块（MLI）是一种用于视觉问答的技术，它通过总结视觉和语言信息为潜在表示，并学习这些表示之间的跨模态关系，以提高问答系统的性能。此外，结合预训练语言模型BERT可以进一步提升系统性能。"
    },
    {
        "order": 479,
        "title": "Scene Text Visual Question Answering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Biten_Scene_Text_Visual_Question_Answering_ICCV_2019_paper.html",
        "abstract": "Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.",
        "中文标题": "场景文本视觉问答",
        "摘要翻译": "当前的视觉问答数据集没有考虑到图像中文本所传达的丰富语义信息。在这项工作中，我们提出了一个新的数据集ST-VQA，旨在强调在视觉问答过程中利用图像中作为文本线索的高级语义信息的重要性。我们使用这个数据集定义了一系列难度递增的任务，这些任务需要在视觉信息提供的上下文中阅读场景文本，以便推理并生成适当的答案。我们为这些任务提出了一个新的评估指标，以考虑推理错误以及文本识别模块的不足。此外，我们提出了一系列基线方法，这些方法为新发布的数据集提供了进一步的见解，并为进一步的研究奠定了基础。",
        "领域": "视觉问答/文本识别/语义理解",
        "问题": "如何利用图像中的文本信息进行有效的视觉问答",
        "动机": "当前视觉问答数据集缺乏对图像中文本语义信息的考虑，限制了视觉问答系统的性能和应用范围",
        "方法": "提出新的数据集ST-VQA，定义基于场景文本的视觉问答任务，并提出新的评估指标和基线方法",
        "关键词": [
            "视觉问答",
            "文本识别",
            "语义理解",
            "数据集",
            "评估指标"
        ],
        "涉及的技术概念": "ST-VQA数据集：一个旨在利用图像中文本语义信息进行视觉问答的新数据集。评估指标：用于衡量视觉问答系统性能的新指标，考虑了推理错误和文本识别模块的不足。基线方法：为ST-VQA数据集提供初步解决方案和见解的方法，为后续研究提供基础。"
    },
    {
        "order": 480,
        "title": "SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.html",
        "abstract": "Object detection has been a building block in computer vision. Though considerable progress has been made, there still exist challenges for objects with small size, arbitrary direction, and dense distribution. Apart from natural images, such issues are especially pronounced for aerial images of great importance. This paper presents a novel multi-category rotation detector for small, cluttered and rotated objects, namely SCRDet. Specifically, a sampling fusion network is devised which fuses multi-layer feature with effective anchor sampling, to improve the sensitivity to small objects. Meanwhile, the supervised pixel attention network and the channel attention network are jointly explored for small and cluttered object detection by suppressing the noise and highlighting the objects feature. For more accurate rotation estimation, the IoU constant factor is added to the smooth L1 loss to address the boundary problem for the rotating bounding box. Extensive experiments on two remote sensing public datasets DOTA, NWPU VHR-10 as well as natural image datasets COCO, VOC2007 and scene text data ICDAR2015 show the state-of-the-art performance of our detector. The code and models will be available at https://github.com/DetectionTeamUCAS.",
        "中文标题": "SCRDet：面向更鲁棒的小型、杂乱和旋转物体检测",
        "摘要翻译": "物体检测一直是计算机视觉中的一个基础构建块。尽管已经取得了相当大的进展，但对于尺寸小、方向任意和密集分布的物体仍然存在挑战。除了自然图像外，这些问题对于极为重要的航空图像尤其明显。本文提出了一种新颖的多类别旋转检测器，用于小型、杂乱和旋转的物体，即SCRDet。具体来说，设计了一种采样融合网络，该网络通过有效的锚点采样融合多层特征，以提高对小物体的敏感性。同时，联合探索了监督像素注意力网络和通道注意力网络，通过抑制噪声和突出物体特征来进行小型和杂乱物体的检测。为了更准确的旋转估计，将IoU常数因子添加到平滑L1损失中，以解决旋转边界框的边界问题。在两个遥感公共数据集DOTA、NWPU VHR-10以及自然图像数据集COCO、VOC2007和场景文本数据ICDAR2015上的大量实验显示了我们检测器的最先进性能。代码和模型将在https://github.com/DetectionTeamUCAS上提供。",
        "领域": "遥感图像分析/物体检测/注意力机制",
        "问题": "解决小型、杂乱和旋转物体检测的挑战",
        "动机": "提高对航空图像中小型、杂乱和旋转物体的检测性能",
        "方法": "设计采样融合网络融合多层特征，联合使用监督像素注意力网络和通道注意力网络，添加IoU常数因子到平滑L1损失中",
        "关键词": [
            "物体检测",
            "旋转检测",
            "注意力机制",
            "遥感图像"
        ],
        "涉及的技术概念": "采样融合网络通过有效的锚点采样融合多层特征，以提高对小物体的敏感性；监督像素注意力网络和通道注意力网络联合使用，通过抑制噪声和突出物体特征来进行小型和杂乱物体的检测；IoU常数因子被添加到平滑L1损失中，以解决旋转边界框的边界问题。"
    },
    {
        "order": 481,
        "title": "Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sheng_Unsupervised_Collaborative_Learning_of_Keyframe_Detection_and_Visual_Odometry_Towards_ICCV_2019_paper.html",
        "abstract": "In this paper we tackle the joint learning problem of keyframe detection and visual odometry towards monocular visual SLAM systems. As an important task in visual SLAM, keyframe selection helps efficient camera relocalization and effective augmentation of visual odometry. To benefit from it, we first present a deep network design for the keyframe selection, which is able to reliably detect keyframes and localize new frames, then an end-to-end unsupervised deep framework further proposed for simultaneously learning the keyframe selection and the visual odometry tasks. As far as we know, it is the first work to jointly optimize these two complementary tasks in a single deep framework. To make the two tasks facilitate each other in the learning, a collaborative optimization loss based on both geometric and visual metrics is proposed. Extensive experiments on publicly available datasets (i.e. KITTI raw dataset and its odometry split) clearly demonstrate the effectiveness of the proposed approach, and new state-of-the-art results are established on the unsupervised depth and pose estimation from monocular videos.",
        "中文标题": "无监督协作学习关键帧检测与视觉里程计朝向单目深度SLAM",
        "摘要翻译": "在本文中，我们解决了关键帧检测和视觉里程计联合学习问题，朝向单目视觉SLAM系统。作为视觉SLAM中的一项重要任务，关键帧选择有助于高效的相机重定位和视觉里程计的有效增强。为了从中受益，我们首先提出了一个用于关键帧选择的深度网络设计，该设计能够可靠地检测关键帧并定位新帧，然后进一步提出了一个端到端的无监督深度框架，用于同时学习关键帧选择和视觉里程计任务。据我们所知，这是首次在单一深度框架中联合优化这两个互补任务的工作。为了使这两个任务在学习中相互促进，提出了一种基于几何和视觉指标的协作优化损失。在公开可用的数据集（即KITTI原始数据集及其里程计分割）上进行的大量实验清楚地证明了所提出方法的有效性，并在无监督深度和姿态估计方面建立了新的最先进结果。",
        "领域": "视觉SLAM/关键帧检测/视觉里程计",
        "问题": "解决关键帧检测和视觉里程计联合学习问题",
        "动机": "提高单目视觉SLAM系统中相机重定位的效率和视觉里程计的增强效果",
        "方法": "提出一个深度网络设计用于关键帧选择，并进一步提出一个端到端的无监督深度框架，用于同时学习关键帧选择和视觉里程计任务，采用基于几何和视觉指标的协作优化损失",
        "关键词": [
            "关键帧检测",
            "视觉里程计",
            "单目视觉SLAM",
            "无监督学习",
            "深度网络"
        ],
        "涉及的技术概念": "关键帧选择有助于高效的相机重定位和视觉里程计的有效增强。提出的深度网络设计能够可靠地检测关键帧并定位新帧。端到端的无监督深度框架用于同时学习关键帧选择和视觉里程计任务。协作优化损失基于几何和视觉指标，旨在使这两个任务在学习中相互促进。"
    },
    {
        "order": 482,
        "title": "UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.html",
        "abstract": "Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.",
        "中文标题": "UM-Adapt：使用对抗性跨任务蒸馏的无监督多任务适应",
        "摘要翻译": "为了实现人类水平的泛化能力，有必要探索具有更高可转移性的适应性表示学习方法。大多数现有方法独立处理任务可转移性和跨领域适应，导致泛化能力有限。在本文中，我们提出了UM-Adapt——一个统一的框架，用于有效地执行空间结构化预测任务的无监督领域适应，同时在多任务设置中保持各个任务的平衡性能。为了实现这一点，我们提出了两种新颖的正则化策略；a) 基于轮廓的内容正则化（CCR）和b) 利用跨任务蒸馏模块利用任务间的一致性。此外，避免使用传统的临时领域鉴别器，我们重新利用跨任务蒸馏损失作为能量函数的输出，以对抗性地最小化输入领域差异。通过大量实验，我们展示了在从合成环境到自然环境的领域转移下，学习到的表示对于多个任务同时具有优越的泛化能力。UM-Adapt在ImageNet分类上产生了最先进的转移学习结果，并在PASCAL VOC 2007检测任务上实现了可比的性能，即使使用较小的骨干网络。此外，所得到的半监督框架在NYUD和Cityscapes数据集上均优于当前的全监督多任务学习最先进技术。",
        "领域": "无监督学习/多任务学习/领域适应",
        "问题": "如何在无监督的情况下，同时适应多个任务并保持各任务的平衡性能",
        "动机": "探索具有更高可转移性的适应性表示学习方法，以实现人类水平的泛化能力",
        "方法": "提出了UM-Adapt框架，包括基于轮廓的内容正则化（CCR）和跨任务蒸馏模块，以及利用跨任务蒸馏损失作为能量函数的输出来对抗性地最小化输入领域差异",
        "关键词": [
            "无监督学习",
            "多任务学习",
            "领域适应",
            "跨任务蒸馏",
            "内容正则化"
        ],
        "涉及的技术概念": "UM-Adapt框架、基于轮廓的内容正则化（CCR）、跨任务蒸馏模块、能量函数、对抗性最小化输入领域差异"
    },
    {
        "order": 483,
        "title": "Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Barroso-Laguna_Key.Net_Keypoint_Detection_by_Handcrafted_and_Learned_CNN_Filters_ICCV_2019_paper.html",
        "abstract": "We introduce a novel approach for keypoint detection task that combines handcrafted and learned CNN filters within a shallow multi-scale architecture. Handcrafted filters provide anchor structures for learned filters, which localize, score and rank repeatable features. Scale-space representation is used within the network to extract keypoints at different levels. We design a loss function to detect robust features that exist across a range of scales and to maximize the repeatability score. Our Key.Net model is trained on data synthetically created from ImageNet and evaluated on HPatches benchmark. Results show that our approach outperforms state-of-the-art detectors in terms of repeatability, matching performance and complexity.",
        "中文标题": "Key.Net: 通过手工制作和学习的CNN滤波器进行关键点检测",
        "摘要翻译": "我们介绍了一种新颖的关键点检测方法，该方法在浅层多尺度架构中结合了手工制作和学习的CNN滤波器。手工制作的滤波器为学习的滤波器提供锚定结构，这些滤波器定位、评分和排序可重复的特征。网络中使用尺度空间表示来提取不同层次的关键点。我们设计了一个损失函数来检测存在于一系列尺度中的鲁棒特征，并最大化重复性得分。我们的Key.Net模型在从ImageNet合成的数据上进行训练，并在HPatches基准上进行评估。结果表明，我们的方法在重复性、匹配性能和复杂性方面优于最先进的检测器。",
        "领域": "关键点检测/特征提取/尺度空间分析",
        "问题": "关键点检测任务中的特征重复性和鲁棒性问题",
        "动机": "提高关键点检测的重复性和鲁棒性，以增强匹配性能和降低复杂性",
        "方法": "结合手工制作和学习的CNN滤波器，使用尺度空间表示提取关键点，设计损失函数检测鲁棒特征并最大化重复性得分",
        "关键词": [
            "关键点检测",
            "CNN滤波器",
            "尺度空间表示",
            "损失函数",
            "重复性得分"
        ],
        "涉及的技术概念": "手工制作的滤波器为学习的滤波器提供锚定结构，用于定位、评分和排序可重复的特征。尺度空间表示用于提取不同层次的关键点。设计的损失函数旨在检测跨尺度存在的鲁棒特征并最大化重复性得分。"
    },
    {
        "order": 484,
        "title": "Cross-X Learning for Fine-Grained Visual Categorization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Cross-X_Learning_for_Fine-Grained_Visual_Categorization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Cross-X_Learning_for_Fine-Grained_Visual_Categorization_ICCV_2019_paper.html",
        "abstract": "Recognizing objects from subcategories with very subtle differences remains a challenging task due to the large intra-class and small inter-class variation. Recent work tackles this problem in a weakly-supervised manner: object parts are first detected and the corresponding part-specific features are extracted for fine-grained classification. However, these methods typically treat the part-specific features of each image in isolation while neglecting their relationships between different images. In this paper, we propose Cross-X learning, a simple yet effective approach that exploits the relationships between different images and between different network layers for robust multi-scale feature learning. Our approach involves two novel components: (i) a cross-category cross-semantic regularizer that guides the extracted features to represent semantic parts and, (ii) a cross-layer regularizer that improves the robustness of multi-scale features by matching the prediction distribution across multiple layers. Our approach can be easily trained end-to-end and is scalable to large datasets like NABirds. We empirically analyze the contributions of different components of our approach and demonstrate its robustness, effectiveness and state-of-the-art performance on five benchmark datasets. Code is available at https://github.com/cswluo/CrossX.",
        "中文标题": "跨X学习用于细粒度视觉分类",
        "摘要翻译": "由于类内差异大和类间差异小，识别具有非常细微差异的子类别对象仍然是一个具有挑战性的任务。最近的工作以弱监督的方式解决了这个问题：首先检测对象部分，然后提取相应的部分特定特征用于细粒度分类。然而，这些方法通常孤立地处理每张图像的部分特定特征，而忽略了它们在不同图像之间的关系。在本文中，我们提出了跨X学习，这是一种简单而有效的方法，它利用不同图像之间和不同网络层之间的关系进行鲁棒的多尺度特征学习。我们的方法包括两个新颖的组成部分：（i）跨类别跨语义正则化器，指导提取的特征表示语义部分，以及（ii）跨层正则化器，通过匹配跨多个层的预测分布来提高多尺度特征的鲁棒性。我们的方法可以轻松地进行端到端训练，并且可以扩展到像NABirds这样的大型数据集。我们实证分析了我们方法不同组成部分的贡献，并在五个基准数据集上展示了其鲁棒性、有效性和最先进的性能。代码可在https://github.com/cswluo/CrossX获取。",
        "领域": "细粒度视觉分类/多尺度特征学习/弱监督学习",
        "问题": "识别具有非常细微差异的子类别对象",
        "动机": "由于类内差异大和类间差异小，识别具有非常细微差异的子类别对象仍然是一个具有挑战性的任务",
        "方法": "提出跨X学习，利用不同图像之间和不同网络层之间的关系进行鲁棒的多尺度特征学习，包括跨类别跨语义正则化器和跨层正则化器",
        "关键词": [
            "细粒度视觉分类",
            "多尺度特征学习",
            "弱监督学习"
        ],
        "涉及的技术概念": "跨X学习是一种方法，它包括跨类别跨语义正则化器和跨层正则化器，用于指导提取的特征表示语义部分和提高多尺度特征的鲁棒性。这种方法可以轻松地进行端到端训练，并且可以扩展到大型数据集。"
    },
    {
        "order": 485,
        "title": "MVSCRF: Learning Multi-View Stereo With Conditional Random Fields",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_MVSCRF_Learning_Multi-View_Stereo_With_Conditional_Random_Fields_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xue_MVSCRF_Learning_Multi-View_Stereo_With_Conditional_Random_Fields_ICCV_2019_paper.html",
        "abstract": "We present a deep-learning architecture for multi-view stereo with conditional random fields (MVSCRF). Given an arbitrary number of input images, we first use a U-shape neural network to extract deep features incorporating both global and local information, and then build a 3D cost volume for the reference camera. Unlike previous learning based methods, we explicitly constraint the smoothness of depth maps by using conditional random fields (CRFs) after the stage of cost volume regularization. The CRFs module is implemented as recurrent neural networks so that the whole pipeline can be trained end-to-end. Our results show that the proposed pipeline outperforms previous state-of-the-arts on large-scale DTU dataset. We also achieve comparable results with state-of-the-art learning based methods on outdoor Tanks and Temples dataset without fine-tuning, which demonstrates our method's generalization ability.",
        "中文标题": "MVSCRF: 使用条件随机场学习多视角立体视觉",
        "摘要翻译": "我们提出了一种结合条件随机场（CRFs）的多视角立体视觉深度学习架构（MVSCRF）。给定任意数量的输入图像，我们首先使用U形神经网络提取包含全局和局部信息的深度特征，然后为参考相机构建3D成本体积。与之前基于学习的方法不同，我们在成本体积正则化阶段之后，通过使用条件随机场（CRFs）明确约束深度图的平滑性。CRFs模块被实现为循环神经网络，以便整个流程可以端到端训练。我们的结果表明，所提出的流程在大规模DTU数据集上优于之前的最先进技术。我们还在没有微调的情况下，在户外Tanks and Temples数据集上实现了与最先进的基于学习的方法相当的结果，这证明了我们方法的泛化能力。",
        "领域": "多视角立体视觉/条件随机场/深度学习",
        "问题": "提高多视角立体视觉中深度图的平滑性和准确性",
        "动机": "为了解决多视角立体视觉中深度图平滑性和准确性的问题，提出了一种结合条件随机场的深度学习架构，以提高性能。",
        "方法": "使用U形神经网络提取深度特征并构建3D成本体积，然后通过条件随机场（CRFs）约束深度图的平滑性，整个流程可端到端训练。",
        "关键词": [
            "多视角立体视觉",
            "条件随机场",
            "深度学习",
            "U形神经网络",
            "3D成本体积"
        ],
        "涉及的技术概念": "U形神经网络用于提取深度特征，条件随机场（CRFs）用于约束深度图的平滑性，3D成本体积用于构建参考相机的深度信息，循环神经网络实现CRFs模块，端到端训练整个流程。"
    },
    {
        "order": 486,
        "title": "Episodic Training for Domain Generalization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Episodic_Training_for_Domain_Generalization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Episodic_Training_for_Domain_Generalization_ICCV_2019_paper.html",
        "abstract": "Domain generalization (DG) is the challenging and topical problem of learning models that generalize to novel testing domains with different statistics than a set of known training domains. The simple approach of aggregating data from all source domains and training a single deep neural network end-to-end on all the data provides a surprisingly strong baseline that surpasses many prior published methods. In this paper we build on this strong baseline by designing an episodic training procedure that trains a single deep network in a way that exposes it to the domain shift that characterises a novel domain at runtime. Specifically, we decompose a deep network into feature extractor and classifier components, and then train each component by simulating it interacting with a partner who is badly tuned for the current domain. This makes both components more robust, ultimately leading to our networks producing state-of-the-art performance on three DG benchmarks. Furthermore, we consider the pervasive workflow of using an ImageNet trained CNN as a fixed feature extractor for downstream recognition tasks. Using the Visual Decathlon benchmark, we demonstrate that our episodic-DG training improves the performance of such a general purpose feature extractor by explicitly training a feature for robustness to novel problems. This shows that DG training can benefit standard practice in computer vision.",
        "中文标题": "领域泛化的情景训练",
        "摘要翻译": "领域泛化（DG）是一个具有挑战性和热点的问题，它涉及学习能够泛化到与已知训练域统计特性不同的新测试域的模型。将所有源域的数据聚合起来，并在所有数据上端到端地训练一个单一的深度神经网络，这种简单的方法提供了一个令人惊讶的强大基线，超越了许多先前发布的方法。在本文中，我们基于这一强大基线，设计了一种情景训练程序，该程序以在运行时暴露于表征新域的域转移的方式训练单一深度网络。具体来说，我们将深度网络分解为特征提取器和分类器组件，然后通过模拟每个组件与当前域不匹配的伙伴交互来训练每个组件。这使得两个组件都更加鲁棒，最终导致我们的网络在三个DG基准上产生了最先进的性能。此外，我们考虑了使用ImageNet训练的CNN作为下游识别任务的固定特征提取器的普遍工作流程。使用Visual Decathlon基准，我们证明了我们的情景DG训练通过显式训练特征以提高对新问题的鲁棒性，从而提高了这种通用特征提取器的性能。这表明DG训练可以有益于计算机视觉中的标准实践。",
        "领域": "领域泛化/特征提取/模型鲁棒性",
        "问题": "学习能够泛化到与已知训练域统计特性不同的新测试域的模型",
        "动机": "提高模型在新测试域上的泛化能力，超越现有方法",
        "方法": "设计情景训练程序，通过模拟域转移训练深度网络的特征提取器和分类器组件",
        "关键词": [
            "领域泛化",
            "情景训练",
            "特征提取",
            "模型鲁棒性",
            "深度网络"
        ],
        "涉及的技术概念": "领域泛化（DG）涉及学习能够泛化到新测试域的模型，这些新测试域与已知训练域有不同的统计特性。本文通过设计情景训练程序，模拟域转移来训练深度网络，提高模型在新域上的泛化能力。"
    },
    {
        "order": 487,
        "title": "Learning Two-View Correspondences and Geometry Using Order-Aware Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_Two-View_Correspondences_and_Geometry_Using_Order-Aware_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_Two-View_Correspondences_and_Geometry_Using_Order-Aware_Network_ICCV_2019_paper.html",
        "abstract": "Establishing correspondences between two images requires both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential matrix. Specifically, this proposed network is built hierarchically and comprises three novel operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in a canonical order and invariant to input permutations. Next, the clusters are spatially correlated to form the global context of correspondences. After that, the context-encoded clusters are recovered back to the original size through a proposed upsampling operator. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts.",
        "中文标题": "使用顺序感知网络学习两视图对应关系和几何",
        "摘要翻译": "在两幅图像之间建立对应关系需要局部和全局的空间上下文。给定两视图中特征点的假定对应关系，本文提出了顺序感知网络，该网络推断对应关系为内点的概率，并通过本质矩阵编码回归相对姿态。具体来说，这个提出的网络是分层构建的，并包含三个新颖的操作。首先，为了捕捉稀疏对应关系的局部上下文，网络通过学习软分配矩阵对无序的输入对应关系进行聚类。这些聚类处于规范顺序中，并且对输入排列不变。接着，聚类在空间上相互关联，形成对应关系的全局上下文。之后，通过提出的上采样操作，将上下文编码的聚类恢复到原始大小。我们在户外和室内数据集上进行了大量实验。两视图几何和对应关系的准确性显著超过了现有技术。",
        "领域": "视觉几何/对应关系学习/姿态估计",
        "问题": "在两幅图像之间建立准确的对应关系和几何关系",
        "动机": "提高两视图对应关系和几何估计的准确性，以超越现有技术水平",
        "方法": "提出顺序感知网络，通过分层结构和三个新颖操作（聚类、空间关联、上采样）来推断对应关系为内点的概率并回归相对姿态",
        "关键词": [
            "视觉几何",
            "对应关系学习",
            "姿态估计",
            "顺序感知网络",
            "本质矩阵"
        ],
        "涉及的技术概念": "顺序感知网络是一种分层构建的网络，通过学习软分配矩阵对无序的输入对应关系进行聚类，形成规范顺序的聚类，这些聚类在空间上相互关联形成全局上下文，最后通过上采样操作恢复到原始大小。本质矩阵用于编码两视图之间的相对姿态。"
    },
    {
        "order": 488,
        "title": "Maximum-Margin Hamming Hashing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Maximum-Margin_Hamming_Hashing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Maximum-Margin_Hamming_Hashing_ICCV_2019_paper.html",
        "abstract": "Deep hashing enables computation and memory efficient image search through end-to-end learning of feature representations and binary codes. While linear scan over binary hash codes is more efficient than over the high-dimensional representations, its linear-time complexity is still unacceptable for very large databases. Hamming space retrieval enables constant-time search through hash lookups, where for each query, there is a Hamming ball centered at the query and the data points within the ball are returned as relevant. Since inside the Hamming ball implies retrievable while outside irretrievable, it is crucial to explicitly characterize the Hamming ball. The main idea of this work is to directly embody the Hamming radius into the loss functions, leading to Maximum-Margin Hamming Hashing (MMHH), a new model specifically optimized for Hamming space retrieval. We introduce a max-margin t-distribution loss, where the t-distribution concentrates more similar data points to be within the Hamming ball, and the margin characterizes the Hamming radius such that less penalization is applied to similar data points within the Hamming ball. The loss function also introduces robustness to data noise, where the similarity supervision may be inaccurate in practical problems. The model is trained end-to-end using a new semi-batch optimization algorithm tailored to extremely imbalanced data. Our method yields state-of-the-art results on four datasets and shows superior performance on noisy data.",
        "中文标题": "最大间隔汉明哈希",
        "摘要翻译": "深度哈希通过端到端学习特征表示和二进制编码，实现了计算和内存高效的图像搜索。虽然对二进制哈希码的线性扫描比对高维表示的扫描更高效，但其线性时间复杂度对于非常大的数据库来说仍然不可接受。汉明空间检索通过哈希查找实现恒定时间搜索，其中对于每个查询，都有一个以查询为中心的汉明球，球内的数据点作为相关结果返回。由于在汉明球内意味着可检索，而在球外则不可检索，因此明确表征汉明球至关重要。这项工作的主要思想是直接将汉明半径体现到损失函数中，从而产生了最大间隔汉明哈希（MMHH），这是一种专门为汉明空间检索优化的新模型。我们引入了一种最大间隔t分布损失，其中t分布将更相似的数据点集中在汉明球内，而间隔则表征汉明半径，使得对汉明球内的相似数据点施加较少的惩罚。该损失函数还引入了对数据噪声的鲁棒性，其中在实际问题中相似性监督可能不准确。该模型使用一种新的半批量优化算法进行端到端训练，该算法专为极度不平衡的数据量身定制。我们的方法在四个数据集上取得了最先进的结果，并在噪声数据上显示出优越的性能。",
        "领域": "图像检索/哈希学习/噪声鲁棒性",
        "问题": "在大规模数据库中实现高效且准确的图像检索",
        "动机": "解决现有方法在大规模数据库上线性时间复杂度高的问题，以及提高对噪声数据的鲁棒性",
        "方法": "提出最大间隔汉明哈希（MMHH），通过将汉明半径直接体现到损失函数中，使用最大间隔t分布损失来优化汉明空间检索，并采用新的半批量优化算法进行模型训练",
        "关键词": [
            "汉明哈希",
            "图像检索",
            "噪声鲁棒性",
            "半批量优化"
        ],
        "涉及的技术概念": "深度哈希、汉明空间检索、汉明球、最大间隔t分布损失、半批量优化算法"
    },
    {
        "order": 489,
        "title": "Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Neural-Guided_RANSAC_Learning_Where_to_Sample_Model_Hypotheses_ICCV_2019_paper.html",
        "abstract": "We present Neural-Guided RANSAC (NG-RANSAC), an extension to the classic RANSAC algorithm from robust optimization. NG-RANSAC uses prior information to improve model hypothesis search, increasing the chance of finding outlier-free minimal sets. Previous works use heuristic side-information like hand-crafted descriptor distance to guide hypothesis search. In contrast, we learn hypothesis search in a principled fashion that lets us optimize an arbitrary task loss during training, leading to large improvements on classic computer vision tasks. We present two further extensions to NG-RANSAC. Firstly, using the inlier count itself as training signal allows us to train neural guidance in a self-supervised fashion. Secondly, we combine neural guidance with differentiable RANSAC to build neural networks which focus on certain parts of the input data and make the output predictions as good as possible. We evaluate NG-RANSAC on a wide array of computer vision tasks, namely estimation of epipolar geometry, horizon line estimation and camera re-localization. We achieve superior or competitive results compared to state-of-the-art robust estimators, including very recent, learned ones.",
        "中文标题": "神经引导的RANSAC：学习在何处采样模型假设",
        "摘要翻译": "我们提出了神经引导的RANSAC（NG-RANSAC），这是对来自鲁棒优化的经典RANSAC算法的扩展。NG-RANSAC利用先验信息来改进模型假设搜索，增加了找到无异常值的最小集的机会。以前的工作使用启发式的辅助信息，如手工制作的描述符距离来指导假设搜索。相比之下，我们以原则性的方式学习假设搜索，使我们能够在训练期间优化任意任务损失，从而在经典计算机视觉任务上取得显著改进。我们提出了NG-RANSAC的两个进一步扩展。首先，使用内点计数本身作为训练信号，使我们能够以自监督的方式训练神经引导。其次，我们将神经引导与可微RANSAC结合，构建专注于输入数据某些部分并使输出预测尽可能好的神经网络。我们在广泛的计算机视觉任务上评估NG-RANSAC，即对极几何估计、地平线估计和相机重定位。与包括最新学习型鲁棒估计器在内的最先进鲁棒估计器相比，我们取得了优越或竞争性的结果。",
        "领域": "对极几何估计/地平线估计/相机重定位",
        "问题": "改进模型假设搜索以增加找到无异常值的最小集的机会",
        "动机": "通过利用先验信息以原则性的方式学习假设搜索，优化任意任务损失，从而在经典计算机视觉任务上取得显著改进",
        "方法": "提出神经引导的RANSAC（NG-RANSAC），使用先验信息改进模型假设搜索，结合可微RANSAC构建神经网络，专注于输入数据某些部分并使输出预测尽可能好",
        "关键词": [
            "RANSAC",
            "模型假设搜索",
            "自监督学习",
            "可微RANSAC"
        ],
        "涉及的技术概念": "RANSAC算法是一种鲁棒估计方法，用于从包含异常值的数据中估计数学模型参数。神经引导的RANSAC（NG-RANSAC）通过引入神经网络来指导假设搜索，以提高搜索效率和准确性。自监督学习是一种训练方法，其中模型使用数据本身生成标签进行训练。可微RANSAC是一种改进的RANSAC算法，允许通过梯度下降等优化方法直接优化模型参数。"
    },
    {
        "order": 490,
        "title": "Domain Adaptation for Structured Output via Discriminative Patch Representations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.html",
        "abstract": "Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.",
        "中文标题": "通过判别性补丁表示实现结构化输出的领域适应",
        "摘要翻译": "预测如语义分割这样的结构化输出依赖于昂贵的逐像素注释来学习监督模型，如卷积神经网络。然而，在一个数据领域训练的模型可能在没有注释进行模型微调的情况下，无法很好地泛化到其他领域。为了避免注释的劳动密集型过程，我们开发了一种领域适应方法，将源数据适应到未标记的目标领域。我们提出通过构建一个聚类空间来发现补丁级输出分布的多种模式，从而学习源领域中补丁的判别性特征表示。以这些表示为指导，我们使用对抗学习方案将目标补丁在聚类空间中的特征表示推向源补丁的分布。此外，我们展示了我们的框架与现有领域适应技术互补，并在语义分割上实现了持续的改进。在多种设置下的众多基准数据集上，如合成到真实和跨城市场景，展示了广泛的消融和结果。",
        "领域": "语义分割/领域适应/对抗学习",
        "问题": "解决在不同数据领域间进行语义分割时，模型泛化能力差的问题",
        "动机": "避免昂贵的逐像素注释过程，提高模型在不同领域间的适应能力",
        "方法": "通过构建聚类空间发现补丁级输出分布的多种模式，学习源领域中补丁的判别性特征表示，并使用对抗学习方案将目标补丁的特征表示推向源补丁的分布",
        "关键词": [
            "语义分割",
            "领域适应",
            "对抗学习",
            "补丁表示",
            "聚类空间"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）用于学习监督模型；领域适应方法用于将源数据适应到未标记的目标领域；对抗学习用于调整目标领域特征表示以匹配源领域分布；聚类空间用于发现补丁级输出分布的多种模式。"
    },
    {
        "order": 491,
        "title": "Learning Meshes for Dense Visual SLAM",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bloesch_Learning_Meshes_for_Dense_Visual_SLAM_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bloesch_Learning_Meshes_for_Dense_Visual_SLAM_ICCV_2019_paper.html",
        "abstract": "Estimating motion and surrounding geometry of a moving camera remains a challenging inference problem. From an information theoretic point of view, estimates should get better as more information is included, such as is done in dense SLAM, but this is strongly dependent on the validity of the underlying models. In the present paper, we use triangular meshes as both compact and dense geometry representation. To allow for simple and fast usage, we propose a view-based formulation for which we predict the in-plane vertex coordinates directly from images and then employ the remaining vertex depth components as free variables. Flexible and continuous integration of information is achieved through the use of a residual based inference technique. This so-called factor graph encodes all information as mapping from free variables to residuals, the squared sum of which is minimised during inference. We propose the use of different types of learnable residuals, which are trained end-to-end to increase their suitability as information bearing models and to enable accurate and reliable estimation. Detailed evaluation of all components is provided on both synthetic and real data which confirms the practicability of the presented approach.",
        "中文标题": "学习用于密集视觉SLAM的网格",
        "摘要翻译": "估计移动相机的运动和周围几何仍然是一个具有挑战性的推理问题。从信息论的角度来看，随着包含更多信息，估计应该会变得更好，如在密集SLAM中所做的那样，但这强烈依赖于基础模型的有效性。在本文中，我们使用三角网格作为既紧凑又密集的几何表示。为了实现简单和快速的使用，我们提出了一种基于视图的公式，我们直接从图像预测平面内顶点坐标，然后将剩余的顶点深度分量作为自由变量。通过使用基于残差的推理技术，实现了信息的灵活和连续集成。这种所谓的因子图将所有信息编码为从自由变量到残差的映射，在推理过程中最小化其平方和。我们提出了使用不同类型的可学习残差，这些残差经过端到端训练，以增加其作为信息承载模型的适用性，并实现准确和可靠的估计。在合成数据和真实数据上对所有组件进行了详细评估，证实了所提出方法的实用性。",
        "领域": "视觉SLAM/几何建模/深度学习",
        "问题": "估计移动相机的运动和周围几何",
        "动机": "提高密集SLAM中估计的准确性和可靠性",
        "方法": "使用三角网格作为几何表示，提出基于视图的公式预测顶点坐标，使用基于残差的推理技术进行信息集成",
        "关键词": [
            "三角网格",
            "密集SLAM",
            "残差推理",
            "因子图",
            "可学习残差"
        ],
        "涉及的技术概念": {
            "三角网格": "一种用于表示三维几何形状的数据结构，由顶点、边和面组成。",
            "密集SLAM": "同时定位与地图构建（SLAM）的一种形式，旨在从传感器数据中构建环境的密集三维模型。",
            "残差推理": "一种优化技术，通过最小化预测值与实际值之间的差异（残差）来估计模型参数。",
            "因子图": "一种图形模型，用于表示变量之间的依赖关系，常用于解决优化和推理问题。",
            "可学习残差": "在机器学习模型中，残差是指预测值与实际值之间的差异，可学习残差意味着这些差异可以通过训练来优化。"
        }
    },
    {
        "order": 492,
        "title": "Conservative Wasserstein Training for Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Conservative_Wasserstein_Training_for_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Conservative_Wasserstein_Training_for_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "This paper targets the task with discrete and periodic class labels (e.g., pose/orientation estimation) in the context of deep learning. The commonly used cross-entropy or regression loss is not well matched to this problem as they ignore the periodic nature of the labels and the class similarity, or assume labels are continuous value. We propose to incorporate inter-class correlations in a Wasserstein training framework by pre-defining (i.e., using arc length of a circle) or adaptively learning the ground metric. We extend the ground metric as a linear, convex or concave increasing function w.r.t. arc length from an optimization perspective. We also propose to construct the conservative target labels which model the inlier and outlier noises using a wrapped unimodal-uniform mixture distribution. Unlike the one-hot setting, the conservative label makes the computation of Wasserstein distance more challenging. We systematically conclude the practical closed-form solution of Wasserstein distance for pose data with either one-hot or conservative target label. We evaluate our method on head, body, vehicle and 3D object pose benchmarks with exhaustive ablation studies. The Wasserstein loss obtaining superior performance over the current methods, especially using convex mapping function for ground metric, conservative label, and closed-form solution.",
        "中文标题": "保守Wasserstein训练用于姿态估计",
        "摘要翻译": "本文针对深度学习背景下具有离散和周期性类别标签（例如，姿态/方向估计）的任务。常用的交叉熵或回归损失与此问题不匹配，因为它们忽略了标签的周期性特性和类别相似性，或假设标签是连续值。我们提出在Wasserstein训练框架中通过预定义（即使用圆的弧长）或自适应学习地面度量来整合类别间相关性。我们从优化的角度将地面度量扩展为相对于弧长的线性、凸或凹递增函数。我们还提出构建保守的目标标签，这些标签使用包裹的单峰-均匀混合分布来建模内点和外点噪声。与独热设置不同，保守标签使得Wasserstein距离的计算更具挑战性。我们系统地总结了对于具有独热或保守目标标签的姿态数据的Wasserstein距离的实际闭式解。我们在头部、身体、车辆和3D物体姿态基准上评估了我们的方法，并进行了详尽的消融研究。Wasserstein损失在现有方法中获得了优越的性能，特别是使用凸映射函数作为地面度量、保守标签和闭式解时。",
        "领域": "姿态估计/深度学习/优化",
        "问题": "解决在深度学习背景下具有离散和周期性类别标签的任务中，常用损失函数不匹配的问题",
        "动机": "常用的交叉熵或回归损失忽略了标签的周期性特性和类别相似性，或假设标签是连续值，因此需要一种新的方法来更好地处理这类问题",
        "方法": "在Wasserstein训练框架中通过预定义或自适应学习地面度量来整合类别间相关性，构建保守的目标标签，并系统地总结Wasserstein距离的实际闭式解",
        "关键词": [
            "姿态估计",
            "Wasserstein距离",
            "保守标签"
        ],
        "涉及的技术概念": "Wasserstein训练框架、地面度量、保守目标标签、包裹的单峰-均匀混合分布、闭式解"
    },
    {
        "order": 493,
        "title": "Efficient Learning on Point Clouds With Basis Point Sets",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Prokudin_Efficient_Learning_on_Point_Clouds_With_Basis_Point_Sets_ICCV_2019_paper.html",
        "abstract": "With an increased availability of 3D scanning technology, point clouds are moving into the focus of computer vision as a rich representation of everyday scenes. However, they are hard to handle for machine learning algorithms due to the unordered structure. One common approach is to apply voxelization, which dramatically increases the amount of data stored and at the same time loses details through discretization. Recently, deep learning models with hand-tailored architectures were proposed to handle point clouds directly and achieve input permutation invariance. However, these architectures use an increased number of parameters and are computationally inefficient. In this work we propose basis point sets as a highly efficient and fully general way to process point clouds with machine learning algorithms. Basis point sets are a residual representation that can be computed efficiently and can be used with standard neural network architectures. Using the proposed representation as the input to a relatively simple network allows us to match the performance of PointNet on a shape classification task while using three order of magnitudes less floating point operations. In a second experiment, we show how proposed representation can be used for obtaining high resolution meshes from noisy 3D scans. Here, our network achieves performance comparable to the state-of-the-art computationally intense multi-step frameworks, in one network pass that can be done in less than 1ms.",
        "中文标题": "基于基础点集的高效点云学习",
        "摘要翻译": "随着3D扫描技术的日益普及，点云作为一种丰富的日常场景表示方法，正逐渐成为计算机视觉的焦点。然而，由于其无序的结构，点云对于机器学习算法来说难以处理。一种常见的方法是应用体素化，这虽然显著增加了存储的数据量，但同时也因离散化而丢失了细节。最近，提出了具有手工定制架构的深度学习模型，以直接处理点云并实现输入排列不变性。然而，这些架构使用了更多的参数，并且在计算上效率低下。在本研究中，我们提出了基础点集作为一种高效且完全通用的方法，用于通过机器学习算法处理点云。基础点集是一种残差表示，可以高效计算，并且可以与标准的神经网络架构一起使用。使用所提出的表示作为相对简单网络的输入，使我们能够在形状分类任务中匹配PointNet的性能，同时使用少三个数量级的浮点运算。在第二个实验中，我们展示了所提出的表示如何用于从噪声3D扫描中获取高分辨率网格。在这里，我们的网络在一次网络传递中实现了与计算密集的多步骤框架相当的性能，可以在不到1毫秒的时间内完成。",
        "领域": "3D点云处理/深度学习/形状分类",
        "问题": "点云数据的无序结构使得机器学习算法难以处理，且现有方法在计算效率和细节保留上存在不足。",
        "动机": "提高点云数据处理的效率和精度，减少计算资源的消耗。",
        "方法": "提出基础点集作为一种高效且通用的点云处理方法，利用残差表示和标准神经网络架构，实现高效的点云学习和处理。",
        "关键词": [
            "3D点云",
            "基础点集",
            "形状分类",
            "高分辨率网格"
        ],
        "涉及的技术概念": "基础点集是一种残差表示方法，用于高效处理点云数据。通过将这种表示方法应用于标准神经网络架构，可以在保持或提高性能的同时，显著减少计算资源的消耗。此外，该方法还展示了在从噪声3D扫描中获取高分辨率网格方面的应用潜力。"
    },
    {
        "order": 494,
        "title": "Semi-Supervised Learning by Augmented Distribution Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Semi-Supervised_Learning_by_Augmented_Distribution_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Semi-Supervised_Learning_by_Augmented_Distribution_Alignment_ICCV_2019_paper.html",
        "abstract": "In this work, we propose a simple yet effective semi-supervised learning approach called Augmented Distribution Alignment. We reveal that an essential sampling bias exists in semi-supervised learning due to the limited number of labeled samples, which often leads to a considerable empirical distribution mismatch between labeled data and unlabeled data. To this end, we propose to align the empirical distributions of labeled and unlabeled data to alleviate the bias. On one hand, we adopt an adversarial training strategy to minimize the distribution distance between labeled and unlabeled data as inspired by domain adaptation works. On the other hand, to deal with the small sample size issue of labeled data, we also propose a simple interpolation strategy to generate pseudo training samples. Those two strategies can be easily implemented into existing deep neural networks. We demonstrate the effectiveness of our proposed approach on the benchmark SVHN and CIFAR10 datasets. Our code is available at https://github.com/qinenergy/adanet .",
        "中文标题": "通过增强分布对齐的半监督学习",
        "摘要翻译": "在这项工作中，我们提出了一种简单而有效的半监督学习方法，称为增强分布对齐。我们揭示了由于标记样本数量有限，半监督学习中存在一种本质的采样偏差，这通常会导致标记数据和未标记数据之间的经验分布不匹配。为此，我们提出对齐标记和未标记数据的经验分布以减轻这种偏差。一方面，我们采用对抗训练策略，以最小化标记和未标记数据之间的分布距离，这是受到领域适应工作的启发。另一方面，为了处理标记数据的小样本问题，我们还提出了一种简单的插值策略来生成伪训练样本。这两种策略可以很容易地实现到现有的深度神经网络中。我们在基准SVHN和CIFAR10数据集上证明了我们提出的方法的有效性。我们的代码可在https://github.com/qinenergy/adanet获取。",
        "领域": "半监督学习/对抗训练/数据增强",
        "问题": "半监督学习中的采样偏差导致标记数据和未标记数据之间的经验分布不匹配",
        "动机": "减轻半监督学习中的采样偏差，提高模型性能",
        "方法": "采用对抗训练策略最小化标记和未标记数据之间的分布距离，并提出插值策略生成伪训练样本",
        "关键词": [
            "半监督学习",
            "对抗训练",
            "数据增强",
            "分布对齐",
            "插值策略"
        ],
        "涉及的技术概念": "对抗训练是一种通过引入对抗样本来增强模型鲁棒性的训练方法。数据增强是通过对现有数据进行变换来增加数据量和多样性，以提高模型的泛化能力。分布对齐是指通过调整数据分布来减少不同数据集之间的差异，常用于领域适应和半监督学习中。插值策略是一种通过现有数据点生成新数据点的方法，用于增加训练样本的数量。"
    },
    {
        "order": 495,
        "title": "EM-Fusion: Dynamic Object-Level SLAM With Probabilistic Data Association",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Strecke_EM-Fusion_Dynamic_Object-Level_SLAM_With_Probabilistic_Data_Association_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Strecke_EM-Fusion_Dynamic_Object-Level_SLAM_With_Probabilistic_Data_Association_ICCV_2019_paper.html",
        "abstract": "The majority of approaches for acquiring dense 3D environment maps with RGB-D cameras assumes static environments or rejects moving objects as outliers. The representation and tracking of moving objects, however, has significant potential for applications in robotics or augmented reality. In this paper, we propose a novel approach to dynamic SLAM with dense object-level representations. We represent rigid objects in local volumetric signed distance function (SDF) maps, and formulate multi-object tracking as direct alignment of RGB-D images with the SDF representations. Our main novelty is a probabilistic formulation which naturally leads to strategies for data association and occlusion handling. We analyze our approach in experiments and demonstrate that our approach compares favorably with the state-of-the-art methods in terms of robustness and accuracy.",
        "中文标题": "EM-Fusion: 具有概率数据关联的动态对象级SLAM",
        "摘要翻译": "大多数使用RGB-D相机获取密集3D环境地图的方法假设环境是静态的，或者将移动物体视为异常值而拒绝。然而，移动物体的表示和跟踪在机器人或增强现实应用中具有显著潜力。在本文中，我们提出了一种新颖的动态SLAM方法，具有密集对象级表示。我们在局部体积有符号距离函数（SDF）地图中表示刚性物体，并将多对象跟踪公式化为RGB-D图像与SDF表示的直接对齐。我们的主要创新是一个概率公式，它自然地导致了数据关联和遮挡处理的策略。我们在实验中分析了我们的方法，并证明了我们的方法在鲁棒性和准确性方面与最先进的方法相比具有优势。",
        "领域": "机器人/增强现实/三维重建",
        "问题": "在动态环境中进行密集3D环境地图的获取和移动物体的表示与跟踪",
        "动机": "移动物体的表示和跟踪在机器人或增强现实应用中具有显著潜力",
        "方法": "提出了一种新颖的动态SLAM方法，使用局部体积有符号距离函数（SDF）地图表示刚性物体，并将多对象跟踪公式化为RGB-D图像与SDF表示的直接对齐，采用概率公式进行数据关联和遮挡处理",
        "关键词": [
            "动态SLAM",
            "密集对象级表示",
            "概率数据关联",
            "RGB-D图像对齐",
            "遮挡处理"
        ],
        "涉及的技术概念": "局部体积有符号距离函数（SDF）地图用于表示刚性物体，多对象跟踪通过直接对齐RGB-D图像与SDF表示实现，采用概率公式进行数据关联和遮挡处理"
    },
    {
        "order": 496,
        "title": "Learning to Rank Proposals for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Learning_to_Rank_Proposals_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Learning_to_Rank_Proposals_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Non-Maximum Suppression (NMS) is an essential step of modern object detection models for removing duplicated candidates. The efficacy of NMS heavily affects the final detection results. Prior works exploit suppression criterions relying on either the objectiveness derived from classification or the overlapness produced by regression, both of which are heuristically designed and fail to explicitly link with the suppression rank. To address this issue, in this paper, we propose a novel Learning-to-Rank (LTR) model to produce the suppression rank via a learning procedure, thus facilitating the candidate generation and lifting the detection performance. In particular, we define a ranking score based on IoU to indicate the ranks of candidates during the NMS step, where candidates with high ranking score will be reserved and the ones with low ranking score will be eliminated. We design a lightweight network to predict the ranking score. We introduce a ranking loss to supervise the generation of these ranking scores, which encourages candidates with IoU to the ground-truth to rank higher. To facilitate the training procedure, we design a novel sampling strategy via dividing candidates into different levels and select hard pairs to adopt in the training. During the inference phase, this module can be exploited as a plugin to the current object detector. The training and inference of the overall framework is end-to-end. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy.",
        "中文标题": "学习排序提议以进行目标检测",
        "摘要翻译": "非最大抑制（NMS）是现代目标检测模型中去除重复候选框的关键步骤。NMS的效果极大地影响最终的检测结果。先前的工作利用基于分类得出的客观性或回归产生的重叠度作为抑制标准，这两者都是启发式设计的，未能明确与抑制排名联系起来。为了解决这个问题，本文提出了一种新颖的学习排序（LTR）模型，通过学习过程产生抑制排名，从而促进候选框的生成并提升检测性能。特别是，我们基于IoU定义了一个排名分数，以指示NMS步骤中候选框的排名，其中排名分数高的候选框将被保留，而排名分数低的候选框将被淘汰。我们设计了一个轻量级网络来预测排名分数。我们引入了一个排名损失来监督这些排名分数的生成，这鼓励与真实值IoU高的候选框排名更高。为了促进训练过程，我们设计了一种新颖的采样策略，通过将候选框分为不同级别并选择困难对用于训练。在推理阶段，该模块可以作为当前目标检测器的插件使用。整个框架的训练和推理是端到端的。在PASCAL VOC和MS COCO基准上的综合实验证明了我们模型在促进现有目标检测器达到最先进准确性方面的通用性和有效性。",
        "领域": "目标检测/非最大抑制/学习排序",
        "问题": "如何有效去除目标检测中的重复候选框",
        "动机": "现有的非最大抑制方法依赖于启发式设计的标准，未能明确与抑制排名联系起来，影响了检测结果的准确性",
        "方法": "提出了一种学习排序模型，通过学习过程产生抑制排名，设计了一个轻量级网络来预测排名分数，并引入排名损失来监督排名分数的生成",
        "关键词": [
            "非最大抑制",
            "学习排序",
            "目标检测",
            "IoU",
            "排名损失"
        ],
        "涉及的技术概念": {
            "非最大抑制（NMS）": "一种用于去除目标检测中重复候选框的技术",
            "学习排序（LTR）": "通过学习过程产生候选框的抑制排名",
            "IoU": "交并比，用于衡量预测框与真实框之间的重叠程度",
            "排名损失": "一种损失函数，用于监督学习排序模型中排名分数的生成，确保与真实值IoU高的候选框排名更高"
        }
    },
    {
        "order": 497,
        "title": "Cross View Fusion for 3D Human Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Cross_View_Fusion_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Cross_View_Fusion_for_3D_Human_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm).",
        "中文标题": "跨视图融合用于3D人体姿态估计",
        "摘要翻译": "我们提出了一种方法，通过在我们的模型中融入多视图几何先验，从多视图图像中恢复绝对3D人体姿态。该方法包括两个独立的步骤：(1) 在多视图图像中估计2D姿态，(2) 从多视图2D姿态中恢复3D姿态。首先，我们在CNN中引入了一种跨视图融合方案，以联合估计多个视图的2D姿态。因此，每个视图的2D姿态估计已经受益于其他视图。其次，我们提出了一种递归的图示结构模型，以从多视图2D姿态中恢复3D姿态。它以可承受的计算成本逐渐提高3D姿态的准确性。我们在两个公共数据集H36M和Total Capture上测试了我们的方法。两个数据集上的平均每关节位置误差分别为26mm和29mm，显著优于现有技术（26mm对52mm，29mm对35mm）。",
        "领域": "3D人体姿态估计/多视图几何/递归图示结构模型",
        "问题": "从多视图图像中恢复绝对3D人体姿态",
        "动机": "提高3D人体姿态估计的准确性和效率",
        "方法": "引入跨视图融合方案到CNN中联合估计多视图的2D姿态，并提出递归图示结构模型从多视图2D姿态中恢复3D姿态",
        "关键词": [
            "3D人体姿态估计",
            "多视图几何",
            "递归图示结构模型",
            "跨视图融合"
        ],
        "涉及的技术概念": "CNN（卷积神经网络）用于2D姿态估计，递归图示结构模型用于3D姿态恢复，跨视图融合方案用于提高2D姿态估计的准确性。"
    },
    {
        "order": 498,
        "title": "ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ClusterSLAM_A_SLAM_Backend_for_Simultaneous_Rigid_Body_Clustering_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_ClusterSLAM_A_SLAM_Backend_for_Simultaneous_Rigid_Body_Clustering_and_ICCV_2019_paper.html",
        "abstract": "We present a practical backend for stereo visual SLAM which can simultaneously discover individual rigid bodies and compute their motions in dynamic environments. While recent factor graph based state optimization algorithms have shown their ability to robustly solve SLAM problems by treating dynamic objects as outliers, the dynamic motions are rarely considered. In this paper, we exploit the consensus of 3D motions among the landmarks extracted from the same rigid body for clustering and estimating static and dynamic objects in a unified manner. Specifically, our algorithm builds a noise-aware motion affinity matrix upon landmarks, and uses agglomerative clustering for distinguishing those rigid bodies. Accompanied by a decoupled factor graph optimization for revising their shape and trajectory, we obtain an iterative scheme to update both cluster assignments and motion estimation reciprocally. Evaluations on both synthetic scenes and KITTI demonstrate the capability of our approach, and further experiments considering online efficiency also show the effectiveness of our method for simultaneous tracking of ego-motion and multiple objects.",
        "中文标题": "ClusterSLAM: 一种用于同时进行刚体聚类和运动估计的SLAM后端",
        "摘要翻译": "我们提出了一种实用的立体视觉SLAM后端，它能够在动态环境中同时发现个体刚体并计算它们的运动。虽然最近基于因子图的状态优化算法通过将动态物体视为异常值展示了其稳健解决SLAM问题的能力，但动态运动很少被考虑。在本文中，我们利用从同一刚体提取的地标之间的3D运动一致性，以统一的方式进行聚类和估计静态及动态物体。具体来说，我们的算法在地标上构建了一个噪声感知的运动亲和矩阵，并使用凝聚聚类来区分这些刚体。伴随着一个解耦的因子图优化来修正它们的形状和轨迹，我们获得了一个迭代方案，以相互更新聚类分配和运动估计。在合成场景和KITTI上的评估展示了我们方法的能力，考虑在线效率的进一步实验也展示了我们方法在同时跟踪自我运动和多个物体方面的有效性。",
        "领域": "立体视觉SLAM/动态环境处理/运动估计",
        "问题": "在动态环境中同时进行刚体聚类和运动估计",
        "动机": "现有的SLAM算法通常将动态物体视为异常值处理，而忽略了动态运动的考虑，这限制了SLAM在动态环境中的应用。",
        "方法": "利用从同一刚体提取的地标之间的3D运动一致性，构建噪声感知的运动亲和矩阵，并使用凝聚聚类区分刚体，同时通过解耦的因子图优化修正形状和轨迹，实现聚类分配和运动估计的相互更新。",
        "关键词": [
            "立体视觉SLAM",
            "动态环境",
            "运动估计",
            "刚体聚类",
            "因子图优化"
        ],
        "涉及的技术概念": {
            "立体视觉SLAM": "一种同时定位与地图构建技术，通过立体视觉传感器获取环境信息。",
            "动态环境": "指环境中存在移动物体的场景。",
            "运动估计": "估计物体在空间中的运动状态。",
            "刚体聚类": "将属于同一刚体的地标点进行聚类。",
            "因子图优化": "一种用于状态估计的图优化方法，通过构建和优化因子图来估计状态变量。",
            "噪声感知的运动亲和矩阵": "一种考虑噪声影响的矩阵，用于表示地标点之间的运动相似性。",
            "凝聚聚类": "一种自底向上的聚类方法，通过逐步合并最相似的簇来进行聚类。"
        }
    },
    {
        "order": 499,
        "title": "S4L: Self-Supervised Semi-Supervised Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_S4L_Self-Supervised_Semi-Supervised_Learning_ICCV_2019_paper.html",
        "abstract": "This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.",
        "中文标题": "S4L: 自监督半监督学习",
        "摘要翻译": "本工作解决了图像分类器的半监督学习问题。我们的主要见解是，半监督学习领域可以从快速发展的自监督视觉表示学习领域中受益。通过统一这两种方法，我们提出了自监督半监督学习（S4L）框架，并利用它推导出两种新颖的半监督图像分类方法。我们展示了这些方法与精心调整的基线以及现有的半监督学习方法相比的有效性。然后，我们展示了S4L和现有的半监督方法可以联合训练，从而在仅使用10%标签的情况下，在半监督ILSVRC-2012上取得了新的最先进结果。",
        "领域": "图像分类/自监督学习/半监督学习",
        "问题": "图像分类器的半监督学习",
        "动机": "利用自监督视觉表示学习的进展来提升半监督学习的效果",
        "方法": "提出自监督半监督学习（S4L）框架，并推导出两种新颖的半监督图像分类方法",
        "关键词": [
            "图像分类",
            "自监督学习",
            "半监督学习"
        ],
        "涉及的技术概念": "自监督视觉表示学习是一种无需大量标注数据即可学习图像特征的技术，而半监督学习则是在少量标注数据和大量未标注数据的情况下进行学习。S4L框架结合了这两种技术，旨在提高图像分类的准确性和效率。"
    },
    {
        "order": 500,
        "title": "Vehicle Re-Identification With Viewpoint-Aware Metric Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_Vehicle_Re-Identification_With_Viewpoint-Aware_Metric_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chu_Vehicle_Re-Identification_With_Viewpoint-Aware_Metric_Learning_ICCV_2019_paper.html",
        "abstract": "This paper considers vehicle re-identification (re-ID) problem. The extreme viewpoint variation (up to 180 degrees) poses great challenges for existing approaches. Inspired by the behavior in human's recognition process, we propose a novel viewpoint-aware metric learning approach. It learns two metrics for similar viewpoints and different viewpoints in two feature spaces, respectively, giving rise to viewpoint-aware network (VANet). During training, two types of constraints are applied jointly. During inference, viewpoint is firstly estimated and the corresponding metric is used. Experimental results confirm that VANet significantly improves re-ID accuracy, especially when the pair is observed from different viewpoints. Our method establishes the new state-of-the-art on two benchmarks.",
        "中文标题": "基于视角感知度量学习的车辆重识别",
        "摘要翻译": "本文探讨了车辆重识别（re-ID）问题。极端的视角变化（高达180度）给现有方法带来了巨大挑战。受到人类识别过程中行为的启发，我们提出了一种新颖的视角感知度量学习方法。该方法分别在两个特征空间中学习相似视角和不同视角的两种度量，从而产生了视角感知网络（VANet）。在训练过程中，联合应用了两种类型的约束。在推理过程中，首先估计视角并使用相应的度量。实验结果证实，VANet显著提高了重识别的准确性，特别是当从不同视角观察对时。我们的方法在两个基准测试中确立了新的最先进水平。",
        "领域": "车辆重识别/视角变化/度量学习",
        "问题": "解决车辆重识别中由于极端视角变化导致的识别准确率低的问题",
        "动机": "受到人类识别过程中行为的启发，旨在提高车辆重识别的准确性，特别是在不同视角下",
        "方法": "提出了一种视角感知度量学习方法，通过在不同特征空间中学习相似视角和不同视角的度量，构建视角感知网络（VANet），并在训练和推理过程中应用特定的约束和视角估计",
        "关键词": [
            "车辆重识别",
            "视角变化",
            "度量学习",
            "视角感知网络"
        ],
        "涉及的技术概念": "视角感知度量学习是一种在车辆重识别任务中，通过分别学习相似视角和不同视角的度量来提高识别准确性的方法。视角感知网络（VANet）是实现这一方法的关键技术，它通过特定的训练和推理策略来处理极端视角变化带来的挑战。"
    },
    {
        "order": 501,
        "title": "Shape-Aware Human Pose and Shape Reconstruction Using Multi-View Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_Shape-Aware_Human_Pose_and_Shape_Reconstruction_Using_Multi-View_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liang_Shape-Aware_Human_Pose_and_Shape_Reconstruction_Using_Multi-View_Images_ICCV_2019_paper.html",
        "abstract": "We propose a scalable neural network framework to reconstruct the 3D mesh of a human body from multi-view images, in the subspace of the SMPL model. Use of multi-view images can significantly reduce the projection ambiguity of the problem, increasing the reconstruction accuracy of the 3D human body under clothing. Our experiments show that this method benefits from the synthetic dataset generated from our pipeline since it has good flexibility of variable control and can provide ground-truth for validation. Our method outperforms existing methods on real-world images, especially on shape estimations.",
        "中文标题": "使用多视角图像进行形状感知的人体姿态和形状重建",
        "摘要翻译": "我们提出了一个可扩展的神经网络框架，用于从多视角图像中重建人体的3D网格，该框架在SMPL模型的子空间内操作。使用多视角图像可以显著减少问题的投影歧义，提高穿着衣物下人体3D重建的准确性。我们的实验表明，该方法受益于我们流程生成的合成数据集，因为该数据集具有良好的变量控制灵活性，并可以提供用于验证的真实数据。我们的方法在真实世界图像上的表现优于现有方法，特别是在形状估计方面。",
        "领域": "3D重建/人体姿态估计/计算机图形学",
        "问题": "从多视角图像中准确重建穿着衣物的人体3D形状",
        "动机": "减少投影歧义，提高3D人体重建的准确性",
        "方法": "提出一个可扩展的神经网络框架，在SMPL模型的子空间内操作，利用多视角图像和合成数据集进行训练和验证",
        "关键词": [
            "3D重建",
            "人体姿态估计",
            "计算机图形学"
        ],
        "涉及的技术概念": "SMPL模型是一种参数化的人体模型，用于表示人体的形状和姿态。多视角图像指的是从不同角度拍摄的同一物体的图像，用于减少重建过程中的歧义。合成数据集是通过计算机生成的图像数据集，用于训练和验证机器学习模型。"
    },
    {
        "order": 502,
        "title": "Efficient and Robust Registration on the 3D Special Euclidean Group",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bhattacharya_Efficient_and_Robust_Registration_on_the_3D_Special_Euclidean_Group_ICCV_2019_paper.html",
        "abstract": "We present a robust, fast and accurate method for registration of 3D scans. Using correspondences, our method optimizes a robust cost function on the intrinsic representation of rigid motions, i.e., the Special Euclidean group SE(3). We exploit the geometric properties of Lie groups as well as the robustness afforded by an iteratively reweighted least squares optimization. We also generalize our approach to a joint multiview method that simultaneously solves for the registration of a set of scans. Our approach significantly outperforms the state-of-the-art robust 3D registration method based on a line process in terms of both speed and accuracy. We show that this line process method is a special case of our principled geometric solution. Finally, we also present scenarios where global registration based on feature correspondences fails but multiview ICP based on our robust motion estimation is successful.",
        "中文标题": "在3D特殊欧几里得群上的高效且鲁棒的配准",
        "摘要翻译": "我们提出了一种鲁棒、快速且准确的3D扫描配准方法。利用对应关系，我们的方法在刚性运动的内在表示上优化了一个鲁棒的成本函数，即特殊欧几里得群SE(3)。我们利用了李群的几何特性以及迭代重加权最小二乘优化提供的鲁棒性。我们还将我们的方法推广到一种联合多视图方法，该方法同时解决一组扫描的配准问题。我们的方法在速度和准确性方面显著优于基于线过程的最先进的鲁棒3D配准方法。我们展示了这种线过程方法是我们原则性几何解决方案的一个特例。最后，我们还展示了基于特征对应关系的全局配准失败但基于我们鲁棒运动估计的多视图ICP成功的场景。",
        "领域": "3D重建/几何处理/运动估计",
        "问题": "3D扫描的鲁棒、快速且准确配准",
        "动机": "提高3D扫描配准的鲁棒性、速度和准确性，特别是在多视图和全局配准场景中",
        "方法": "利用李群的几何特性和迭代重加权最小二乘优化，优化刚性运动的内在表示上的鲁棒成本函数，并推广到联合多视图方法",
        "关键词": [
            "3D扫描配准",
            "特殊欧几里得群",
            "迭代重加权最小二乘优化"
        ],
        "涉及的技术概念": {
            "特殊欧几里得群SE(3)": "用于表示3D空间中的刚性运动，包括旋转和平移",
            "李群": "一种数学结构，用于描述连续变换的对称性，这里用于优化配准过程",
            "迭代重加权最小二乘优化": "一种优化技术，通过迭代调整权重来减少异常值的影响，提高配准的鲁棒性",
            "多视图ICP": "一种基于迭代最近点算法的多视图配准方法，用于同时配准多个3D扫描"
        }
    },
    {
        "order": 503,
        "title": "Privacy Preserving Image Queries for Camera Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Speciale_Privacy_Preserving_Image_Queries_for_Camera_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Speciale_Privacy_Preserving_Image_Queries_for_Camera_Localization_ICCV_2019_paper.html",
        "abstract": "Augmented/mixed reality and robotic applications are increasingly relying on cloud-based localization services, which require users to upload query images to perform camera pose estimation on a server. This raises significant privacy concerns when consumers use such services in their homes or in confidential industrial settings. Even if only image features are uploaded, the privacy concerns remain as the images can be reconstructed fairly well from feature locations and descriptors. We propose to conceal the content of the query images from an adversary on the server or a man-in-the-middle intruder. The key insight is to replace the 2D image feature points in the query image with randomly oriented 2D lines passing through their original 2D positions. It will be shown that this feature representation hides the image contents, and thereby protects user privacy, yet still provides sufficient geometric constraints to enable robust and accurate 6-DOF camera pose estimation from feature correspondences. Our proposed method can handle single- and multi-image queries as well as exploit additional information about known structure, gravity, and scale. Numerous experiments demonstrate the high practical relevance of our approach.",
        "中文标题": "隐私保护的图像查询用于相机定位",
        "摘要翻译": "增强/混合现实和机器人应用越来越依赖于基于云的定位服务，这些服务要求用户上传查询图像以在服务器上执行相机姿态估计。当消费者在家中或机密工业环境中使用此类服务时，这引发了重大的隐私问题。即使只上传图像特征，隐私问题仍然存在，因为可以从特征位置和描述符中相当好地重建图像。我们提出了一种方法，以隐藏查询图像的内容，防止服务器上的对手或中间人入侵者获取。关键见解是用通过其原始2D位置的随机方向的2D线替换查询图像中的2D图像特征点。将展示这种特征表示隐藏了图像内容，从而保护了用户隐私，同时仍然提供了足够的几何约束，以从特征对应中实现稳健和准确的6自由度相机姿态估计。我们提出的方法可以处理单图像和多图像查询，以及利用已知结构、重力和尺度的附加信息。大量实验证明了我们方法的高实用相关性。",
        "领域": "增强现实/机器人/隐私保护",
        "问题": "在基于云的定位服务中保护用户上传的查询图像的隐私",
        "动机": "增强/混合现实和机器人应用中，用户上传查询图像以进行相机姿态估计，这在家用或机密工业环境中引发了隐私问题",
        "方法": "用随机方向的2D线替换查询图像中的2D图像特征点，以隐藏图像内容，同时保持足够的几何约束进行相机姿态估计",
        "关键词": [
            "隐私保护",
            "相机定位",
            "图像特征",
            "6自由度",
            "几何约束"
        ],
        "涉及的技术概念": "2D图像特征点、2D线、6自由度相机姿态估计、特征对应、已知结构、重力、尺度"
    },
    {
        "order": 504,
        "title": "WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_WSOD2_Learning_Bottom-Up_and_Top-Down_Objectness_Distillation_for_Weakly-Supervised_Object_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_WSOD2_Learning_Bottom-Up_and_Top-Down_Objectness_Distillation_for_Weakly-Supervised_Object_ICCV_2019_paper.html",
        "abstract": "We study on weakly-supervised object detection (WSOD) which plays a vital role in relieving human involvement from object-level annotations. Predominant works integrate region proposal mechanisms with convolutional neural networks (CNN). Although CNN is proficient in extracting discriminative local features, grand challenges still exist to measure the likelihood of a bounding box containing a complete object (i.e., \"objectness\"). In this paper, we propose a novel WSOD framework with Objectness Distillation (i.e., WSOD2) by designing a tailored training mechanism for weakly-supervised object detection. Multiple regression targets are specifically determined by jointly considering bottom-up (BU) and top-down (TD) objectness from low-level measurement and CNN confidences with an adaptive linear combination. As bounding box regression can facilitate a region proposal learning to approach its regression target with high objectness during training, deep objectness representation learned from bottom-up evidences can be gradually distilled into CNN by optimization. We explore different adaptive training curves for BU/TD objectness, and show that the proposed WSOD2 can achieve state-of-the-art results.",
        "中文标题": "WSOD2: 学习自下而上和自上而下的物体性蒸馏用于弱监督物体检测",
        "摘要翻译": "我们研究了弱监督物体检测（WSOD），它在减少人类参与物体级标注方面发挥着至关重要的作用。主要的工作将区域提议机制与卷积神经网络（CNN）结合起来。尽管CNN擅长提取有区别的局部特征，但在衡量包含完整物体的边界框的可能性（即“物体性”）方面仍存在巨大挑战。在本文中，我们通过设计一个定制的训练机制，提出了一种新颖的WSOD框架，即物体性蒸馏（WSOD2）。通过联合考虑自下而上（BU）和自上而下（TD）的物体性，从低级测量和CNN置信度中自适应线性组合确定多个回归目标。由于边界框回归可以促进区域提议学习在训练期间接近其具有高物体性的回归目标，因此从自下而上的证据中学习到的深度物体性表示可以通过优化逐渐蒸馏到CNN中。我们探索了BU/TD物体性的不同自适应训练曲线，并表明所提出的WSOD2可以实现最先进的结果。",
        "领域": "物体检测/弱监督学习/卷积神经网络",
        "问题": "在弱监督物体检测中准确衡量包含完整物体的边界框的可能性",
        "动机": "减少人类参与物体级标注的需求，提高物体检测的自动化程度",
        "方法": "设计了一个定制的训练机制，通过联合考虑自下而上和自上而下的物体性，从低级测量和CNN置信度中自适应线性组合确定多个回归目标，从而蒸馏深度物体性表示到CNN中",
        "关键词": [
            "弱监督物体检测",
            "物体性蒸馏",
            "卷积神经网络",
            "自适应线性组合",
            "边界框回归"
        ],
        "涉及的技术概念": {
            "弱监督物体检测（WSOD）": "一种物体检测方法，它不需要精确的物体级标注，而是使用图像级标注来训练模型。",
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据，能够自动提取图像的特征。",
            "物体性（Objectness）": "衡量一个边界框包含一个完整物体的可能性。",
            "自下而上（BU）和自上而下（TD）物体性": "自下而上物体性指的是从低级特征（如边缘、纹理）中提取的物体性信息，而自上而下物体性指的是从高级语义信息中提取的物体性信息。",
            "自适应线性组合": "一种方法，通过动态调整不同特征的权重来优化模型的性能。",
            "边界框回归": "一种技术，用于调整预测的边界框位置，使其更接近真实的物体位置。"
        }
    },
    {
        "order": 505,
        "title": "Monocular Piecewise Depth Estimation in Dynamic Scenes by Exploiting Superpixel Relations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Di_Monocular_Piecewise_Depth_Estimation_in_Dynamic_Scenes_by_Exploiting_Superpixel_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Di_Monocular_Piecewise_Depth_Estimation_in_Dynamic_Scenes_by_Exploiting_Superpixel_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel and specially designed method for piecewise dense monocular depth estimation in dynamic scenes. We utilize spatial relations between neighboring superpixels to solve the inherent relative scale ambiguity (RSA) problem and smooth the depth map. However, directly estimating spatial relations is an ill-posed problem. Our core idea is to predict spatial relations based on the corresponding motion relations. Given two or more consecutive frames, we first compute semi-dense (CPM) or dense (optical flow) point matches between temporally neighboring images. Then we develop our method in four main stages: superpixel relations analysis, motion selection, reconstruction, and refinement. The final refinement process helps to improve the quality of the reconstruction at pixel level. Our method does not require per-object segmentation, template priors or training sets, which ensures flexibility in various applications. Extensive experiments on both synthetic and real datasets demonstrate that our method robustly handles different dynamic situations and presents competitive results to the state-of-the-art methods while running much faster than them.",
        "中文标题": "利用超像素关系在动态场景中进行单目分段深度估计",
        "摘要翻译": "在本文中，我们提出了一种新颖且特别设计的方法，用于在动态场景中进行分段密集单目深度估计。我们利用相邻超像素之间的空间关系来解决固有的相对尺度模糊（RSA）问题并平滑深度图。然而，直接估计空间关系是一个不适定问题。我们的核心思想是基于相应的运动关系来预测空间关系。给定两个或更多连续帧，我们首先计算时间相邻图像之间的半密集（CPM）或密集（光流）点匹配。然后，我们在四个主要阶段开发我们的方法：超像素关系分析、运动选择、重建和细化。最终的细化过程有助于提高像素级别的重建质量。我们的方法不需要每个对象的分割、模板先验或训练集，这确保了在各种应用中的灵活性。在合成和真实数据集上的大量实验表明，我们的方法能够稳健地处理不同的动态情况，并展示了与最先进方法相比具有竞争力的结果，同时运行速度更快。",
        "领域": "深度估计/动态场景分析/超像素处理",
        "问题": "解决动态场景中单目深度估计的固有相对尺度模糊问题",
        "动机": "为了提高动态场景中单目深度估计的准确性和效率，特别是在处理相对尺度模糊和深度图平滑方面",
        "方法": "利用相邻超像素之间的空间关系，基于运动关系预测空间关系，通过超像素关系分析、运动选择、重建和细化四个阶段进行深度估计",
        "关键词": [
            "深度估计",
            "动态场景",
            "超像素",
            "相对尺度模糊",
            "光流"
        ],
        "涉及的技术概念": {
            "相对尺度模糊（RSA）": "在单目深度估计中，由于缺乏绝对尺度信息，导致深度估计存在模糊性",
            "超像素": "图像中具有相似颜色、亮度、纹理等特征的像素集合，用于简化图像处理",
            "光流": "用于估计图像序列中像素点运动的技术，通过分析连续帧之间的像素位移来实现",
            "半密集（CPM）点匹配": "一种在图像中寻找特征点对应关系的方法，介于稀疏和密集匹配之间"
        }
    },
    {
        "order": 506,
        "title": "Algebraic Characterization of Essential Matrices and Their Averaging in Multiview Settings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kasten_Algebraic_Characterization_of_Essential_Matrices_and_Their_Averaging_in_Multiview_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kasten_Algebraic_Characterization_of_Essential_Matrices_and_Their_Averaging_in_Multiview_ICCV_2019_paper.html",
        "abstract": "Essential matrix averaging, i.e., the task of recovering camera locations and orientations in calibrated, multiview settings, is a first step in global approaches to Euclidean structure from motion. A common approach to essential matrix averaging is to separately solve for camera orientations and subsequently for camera positions. This paper presents a novel approach that solves simultaneously for both camera orientations and positions. We offer a complete characterization of the algebraic conditions that enable a unique Euclidean reconstruction of n cameras from a collection of (^n_2) essential matrices. We next use these conditions to formulate essential matrix averaging as a constrained optimization problem, allowing us to recover a consistent set of essential matrices given a (possibly partial) set of measured essential matrices computed independently for pairs of images. We finally use the recovered essential matrices to determine the global positions and orientations of the n cameras. We test our method on common SfM datasets, demonstrating high accuracy while maintaining efficiency and robustness, compared to existing methods.",
        "中文标题": "多视图设置中本质矩阵的代数特性及其平均化",
        "摘要翻译": "本质矩阵平均化，即在标定的多视图设置中恢复相机位置和方向的任务，是从运动中恢复欧几里得结构的全局方法的第一步。本质矩阵平均化的常见方法是分别求解相机方向，随后求解相机位置。本文提出了一种新颖的方法，同时求解相机方向和位置。我们提供了从一组(^n_2)本质矩阵中唯一欧几里得重建n个相机的代数条件的完整特性描述。接下来，我们利用这些条件将本质矩阵平均化表述为一个约束优化问题，使我们能够从一组（可能是部分的）独立计算的图像对本质矩阵中恢复出一致的本质矩阵集。最后，我们使用恢复的本质矩阵来确定n个相机的全局位置和方向。我们在常见的SfM数据集上测试了我们的方法，与现有方法相比，展示了高精度，同时保持了效率和鲁棒性。",
        "领域": "三维重建/相机标定/运动结构恢复",
        "问题": "在多视图设置中恢复相机位置和方向",
        "动机": "提高从运动中恢复欧几里得结构的全局方法的效率和精度",
        "方法": "提出了一种同时求解相机方向和位置的新方法，并利用代数条件将本质矩阵平均化表述为约束优化问题",
        "关键词": [
            "本质矩阵",
            "相机标定",
            "三维重建",
            "运动结构恢复"
        ],
        "涉及的技术概念": "本质矩阵平均化是指在多视图设置中，通过恢复相机的位置和方向来从运动中恢复欧几里得结构的过程。本文提出了一种同时求解相机方向和位置的新方法，并利用代数条件将本质矩阵平均化表述为约束优化问题，从而恢复出一致的本质矩阵集，最终确定相机的全局位置和方向。"
    },
    {
        "order": 507,
        "title": "Calibration Wizard: A Guidance System for Camera Calibration Based on Modelling Geometric and Corner Uncertainty",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Calibration_Wizard_A_Guidance_System_for_Camera_Calibration_Based_on_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_Calibration_Wizard_A_Guidance_System_for_Camera_Calibration_Based_on_ICCV_2019_paper.html",
        "abstract": "It is well known that the accuracy of a calibration depends strongly on the choice of camera poses from which images of a calibration object are acquired. We present a system -- Calibration Wizard -- that interactively guides a user towards taking optimal calibration images. For each new image to be taken, the system computes, from all previously acquired images, the pose that leads to the globally maximum reduction of expected uncertainty on intrinsic parameters and then guides the user towards that pose. We also show how to incorporate uncertainty in corner point position in a novel principled manner, for both, calibration and computation of the next best pose. Synthetic and real-world experiments are performed to demonstrate the effectiveness of Calibration Wizard.",
        "中文标题": "校准向导：基于几何和角点不确定性建模的相机校准指导系统",
        "摘要翻译": "众所周知，校准的准确性在很大程度上取决于从校准对象获取图像的相机姿态的选择。我们提出了一个系统——校准向导——它交互式地引导用户拍摄最佳的校准图像。对于每一张要拍摄的新图像，系统从所有先前获取的图像中计算出导致内在参数预期不确定性全局最大减少的姿态，然后引导用户朝向该姿态。我们还展示了如何以一种新颖的原则性方式将角点位置的不确定性纳入校准和计算下一个最佳姿态的过程中。通过合成和真实世界的实验来证明校准向导的有效性。",
        "领域": "相机校准/几何建模/不确定性分析",
        "问题": "如何选择最佳的相机姿态以获取校准图像，从而减少内在参数的预期不确定性",
        "动机": "提高相机校准的准确性和效率，通过交互式引导用户拍摄最佳校准图像",
        "方法": "开发了一个系统，该系统通过计算从所有先前获取的图像中导致内在参数预期不确定性全局最大减少的姿态，并引导用户朝向该姿态，同时将角点位置的不确定性纳入校准和计算下一个最佳姿态的过程中",
        "关键词": [
            "相机校准",
            "几何建模",
            "不确定性分析",
            "交互式引导",
            "角点位置"
        ],
        "涉及的技术概念": "相机校准涉及确定相机的内在参数（如焦距、主点等）和外在参数（如位置和方向）。几何建模是指使用数学模型来描述物理世界中的几何形状和关系。不确定性分析涉及评估和减少测量或计算中的不确定性。交互式引导系统通过用户界面提供实时反馈和建议，以优化特定任务（如相机校准）的执行。角点位置的不确定性分析涉及评估图像中角点检测的准确性及其对校准过程的影响。"
    },
    {
        "order": 508,
        "title": "Localization of Deep Inpainting Using High-Pass Fully Convolutional Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Localization_of_Deep_Inpainting_Using_High-Pass_Fully_Convolutional_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Localization_of_Deep_Inpainting_Using_High-Pass_Fully_Convolutional_Network_ICCV_2019_paper.html",
        "abstract": "Image inpainting has been substantially improved with deep learning in the past years. Deep inpainting can fill image regions with plausible contents, which are not visually apparent. Although inpainting is originally designed to repair images, it can even be used for malicious manipulations, e.g., removal of specific objects. Therefore, it is necessary to identify the presence of inpainting in an image. This paper presents a method to locate the regions manipulated by deep inpainting. The proposed method employs a fully convolutional network that is based on high-pass filtered image residuals. Firstly, we analyze and observe that the inpainted regions are more distinguishable from the untouched ones in the residual domain. Hence, a high-pass pre-filtering module is designed to get image residuals for enhancing inpainting traces. Then, a feature extraction module, which learns discriminative features from image residuals, is built with four concatenated ResNet blocks. The learned feature maps are finally enlarged by an up-sampling module, so that a pixel-wise inpainting localization map is obtained. The whole network is trained end-to-end with a loss addressing the class imbalance. Extensive experimental results evaluated on both synthetic and realistic images subjected to deep inpainting have shown the effectiveness of the proposed method.",
        "中文标题": "使用高通全卷积网络进行深度修复定位",
        "摘要翻译": "近年来，深度学习显著改善了图像修复技术。深度修复能够用看似合理的内容填充图像区域，这些内容在视觉上并不明显。尽管修复最初是为了修复图像而设计的，但它甚至可以被用于恶意操作，例如移除特定对象。因此，有必要识别图像中是否存在修复。本文提出了一种定位深度修复操作区域的方法。所提出的方法采用了一个基于高通滤波图像残差的全卷积网络。首先，我们分析并观察到，在残差域中，修复区域与未触及区域更容易区分。因此，设计了一个高通预滤波模块来获取图像残差，以增强修复痕迹。然后，构建了一个特征提取模块，该模块从图像残差中学习区分性特征，由四个串联的ResNet块组成。学习到的特征图最终通过上采样模块放大，从而获得像素级的修复定位图。整个网络通过解决类别不平衡的损失函数进行端到端训练。在合成图像和真实图像上进行的广泛实验结果评估了深度修复的有效性，证明了所提出方法的有效性。",
        "领域": "图像修复/恶意操作检测/图像分析",
        "问题": "识别和定位图像中经过深度修复的区域",
        "动机": "由于深度修复技术可能被用于恶意操作，如移除特定对象，因此需要一种方法来识别图像中是否存在修复，并定位修复区域。",
        "方法": "采用基于高通滤波图像残差的全卷积网络，包括高通预滤波模块、特征提取模块和上采样模块，通过端到端训练来定位修复区域。",
        "关键词": [
            "图像修复",
            "恶意操作检测",
            "图像分析",
            "全卷积网络",
            "高通滤波",
            "残差分析"
        ],
        "涉及的技术概念": "全卷积网络（FCN）是一种用于图像分割的深度学习模型，能够处理任意大小的输入图像并输出相应大小的分割图。高通滤波是一种图像处理技术，用于增强图像中的高频成分，即边缘和细节。残差分析是指分析图像与其低通滤波版本之间的差异，以突出显示图像中的特定特征。ResNet块是一种深度残差网络结构，通过引入残差连接来解决深层网络中的梯度消失问题，使得网络能够学习更深层次的特征。"
    },
    {
        "order": 509,
        "title": "Is This the Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Taira_Is_This_the_Right_Place_Geometric-Semantic_Pose_Verification_for_Indoor_ICCV_2019_paper.html",
        "abstract": "Visual localization in large and complex indoor scenes, dominated by weakly textured rooms and repeating geometric patterns, is a challenging problem with high practical relevance for applications such as Augmented Reality and robotics. To handle the ambiguities arising in this scenario, a common strategy is, first, to generate multiple estimates for the camera pose from which a given query image was taken. The pose with the largest geometric consistency with the query image, e.g., in the form of an inlier count, is then selected in a second stage. While a significant amount of research has concentrated on the first stage, there has been considerably less work on the second stage. In this paper, we thus focus on pose verification. We show that combining different modalities, namely appearance, geometry, and semantics, considerably boosts pose verification and consequently pose accuracy. We develop multiple hand-crafted as well as a trainable approach to join into the geometric-semantic verification and show significant improvements over state-of-the-art on a very challenging indoor dataset.",
        "中文标题": "这是正确的位置吗？室内视觉定位的几何-语义姿态验证",
        "摘要翻译": "在大型且复杂的室内场景中进行视觉定位，这些场景主要由纹理较弱的房间和重复的几何图案主导，是一个具有高实用相关性的挑战性问题，适用于增强现实和机器人等应用。为了处理这种情况下的歧义，一个常见的策略是首先从拍摄给定查询图像的相机姿态生成多个估计。然后，在第二阶段选择与查询图像具有最大几何一致性的姿态，例如，以内点计数的形式。虽然大量研究集中在第一阶段，但在第二阶段的工作相对较少。因此，在本文中，我们专注于姿态验证。我们展示了结合不同的模态，即外观、几何和语义，可以显著提高姿态验证，从而提高姿态准确性。我们开发了多种手工制作的方法以及一种可训练的方法来进行几何-语义验证，并在一个非常具有挑战性的室内数据集上展示了相对于最先进技术的显著改进。",
        "领域": "增强现实/机器人/室内定位",
        "问题": "在大型且复杂的室内场景中进行准确的视觉定位",
        "动机": "提高在纹理较弱和重复几何图案的室内环境中的视觉定位准确性，以支持增强现实和机器人等应用",
        "方法": "结合外观、几何和语义信息进行姿态验证，开发了多种手工制作的方法和一种可训练的方法",
        "关键词": [
            "视觉定位",
            "姿态验证",
            "几何-语义验证"
        ],
        "涉及的技术概念": "视觉定位是指在给定场景中确定相机的位置和方向的过程。姿态验证是评估和选择最符合查询图像的相机姿态的过程。几何-语义验证是一种结合几何信息和语义信息来提高姿态验证准确性的方法。"
    },
    {
        "order": 510,
        "title": "Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.html",
        "abstract": "We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html.",
        "中文标题": "液态扭曲生成对抗网络：一个统一的人体运动模仿、外观转换和新视角合成的框架",
        "摘要翻译": "我们在一个统一的框架内解决了人体运动模仿、外观转换和新视角合成的问题，这意味着一旦模型被训练，就可以用于处理所有这些任务。现有的特定任务方法主要使用2D关键点（姿势）来估计人体结构。然而，它们仅能表达位置信息，无法表征个体的个性化形状和模拟肢体旋转。在本文中，我们提出了使用3D人体网格恢复模块来分离姿势和形状，这不仅可以模拟关节位置和旋转，还可以表征个性化的身体形状。为了保留源信息，如纹理、风格、颜色和面部身份，我们提出了一个带有液态扭曲块（LWB）的液态扭曲生成对抗网络（Liquid Warping GAN），它在图像和特征空间中传播源信息，并根据参考合成图像。具体来说，源特征通过去噪卷积自编码器提取，以很好地表征源身份。此外，我们提出的方法能够支持从多个源进行更灵活的扭曲。此外，我们建立了一个新的数据集，即Impersonator（iPER）数据集，用于评估人体运动模仿、外观转换和新视角合成。大量实验证明了我们的方法在多个方面的有效性，如遮挡情况下的鲁棒性和保留面部身份、形状一致性和衣物细节。所有代码和数据集均可在https://svip-lab.github.io/project/impersonator.html上获取。",
        "领域": "人体运动模仿/外观转换/新视角合成",
        "问题": "如何在一个统一的框架内解决人体运动模仿、外观转换和新视角合成的问题",
        "动机": "现有的特定任务方法无法表征个体的个性化形状和模拟肢体旋转，需要一种能够同时处理这些任务的方法",
        "方法": "提出了使用3D人体网格恢复模块来分离姿势和形状，并提出了一个带有液态扭曲块（LWB）的液态扭曲生成对抗网络（Liquid Warping GAN）来保留源信息并根据参考合成图像",
        "关键词": [
            "3D人体网格恢复",
            "液态扭曲生成对抗网络",
            "去噪卷积自编码器"
        ],
        "涉及的技术概念": "3D人体网格恢复模块用于分离姿势和形状，液态扭曲生成对抗网络（Liquid Warping GAN）用于在图像和特征空间中传播源信息并合成图像，去噪卷积自编码器用于提取源特征以表征源身份"
    },
    {
        "order": 511,
        "title": "Gated2Depth: Real-Time Dense Lidar From Gated Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gruber_Gated2Depth_Real-Time_Dense_Lidar_From_Gated_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gruber_Gated2Depth_Real-Time_Dense_Lidar_From_Gated_Images_ICCV_2019_paper.html",
        "abstract": "We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.",
        "中文标题": "Gated2Depth：从门控图像实时生成密集激光雷达",
        "摘要翻译": "我们提出了一种成像框架，该框架将来自门控相机的三张图像转换为高分辨率深度图，其深度精度可与脉冲激光雷达测量相媲美。现有的扫描激光雷达系统由于机械限制的角采样率在大范围内实现低空间分辨率，限制了场景理解任务仅限于近距离密集采样的集群。此外，当今的脉冲激光雷达扫描仪存在高成本、高功耗、大体积的问题，并且在强背散射情况下会失效。我们放弃了点扫描，并证明可以通过从三张门控图像中学习深度，将低成本CMOS门控成像器转变为至少80米范围的密集深度相机。所提出的架构利用了门控切片之间的语义上下文，并在不需要密集深度标签的情况下通过合成判别器损失进行训练。所提出的扫描激光雷达系统替代方案是实时的，能够处理背散射，并在长距离提供密集深度。我们在模拟和北欧超过4,000公里驾驶中获取的真实世界数据上验证了我们的方法。数据和代码可在https://github.com/gruberto/Gated2Depth获取。",
        "领域": "深度估计/激光雷达/门控成像",
        "问题": "现有激光雷达系统在大范围内空间分辨率低、成本高、功耗大、体积大，且在强背散射情况下失效",
        "动机": "开发一种低成本、实时、能够处理背散射并在长距离提供密集深度的深度相机",
        "方法": "通过从三张门控图像中学习深度，将低成本CMOS门控成像器转变为密集深度相机，利用门控切片之间的语义上下文，并通过合成判别器损失进行训练",
        "关键词": [
            "深度估计",
            "激光雷达",
            "门控成像",
            "实时处理",
            "背散射处理"
        ],
        "涉及的技术概念": "门控相机、深度图、脉冲激光雷达、CMOS门控成像器、语义上下文、合成判别器损失"
    },
    {
        "order": 512,
        "title": "Clustered Object Detection in Aerial Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Clustered_Object_Detection_in_Aerial_Images_ICCV_2019_paper.html",
        "abstract": "Detecting objects in aerial images is challenging for at least two reasons: (1) target objects like pedestrians are very small in pixels, making them hardly distinguished from surrounding background; and (2) targets are in general sparsely and non-uniformly distributed, making the detection very inefficient. In this paper, we address both issues inspired by observing that these targets are often clustered. In particular, we propose a Clustered Detection (ClusDet) network that unifies object clustering and detection in an end-to-end framework. The key components in ClusDet include a cluster proposal sub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated detection network (DetecNet). Given an input image, CPNet produces object cluster regions and ScaleNet estimates object scales for these regions. Then, each scale-normalized cluster region is fed into DetecNet for object detection. ClusDet has several advantages over previous solutions: (1) it greatly reduces the number of chips for final object detection and hence achieves high running time efficiency, (2) the cluster-based scale estimation is more accurate than previously used single-object based ones, hence effectively improves the detection for small objects, and (3) the final DetecNet is dedicated for clustered regions and implicitly models the prior context information so as to boost detection accuracy. The proposed method is tested on three popular aerial image datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet achieves promising performance in comparison with state-of-the-art detectors.",
        "中文标题": "航空图像中的聚类目标检测",
        "摘要翻译": "在航空图像中检测目标至少面临两个挑战：(1) 像行人这样的目标在像素上非常小，使得它们很难与周围背景区分开来；(2) 目标通常稀疏且非均匀分布，使得检测非常低效。在本文中，我们通过观察这些目标通常是聚类的来解决了这两个问题。特别是，我们提出了一个聚类检测（ClusDet）网络，它将目标聚类和检测统一在一个端到端的框架中。ClusDet的关键组件包括一个聚类提议子网络（CPNet）、一个尺度估计子网络（ScaleNet）和一个专用的检测网络（DetecNet）。给定一个输入图像，CPNet生成目标聚类区域，ScaleNet估计这些区域的目标尺度。然后，每个尺度归一化的聚类区域被送入DetecNet进行目标检测。与之前的解决方案相比，ClusDet有几个优点：(1) 它大大减少了最终目标检测的芯片数量，从而实现了高运行时间效率，(2) 基于聚类的尺度估计比之前使用的基于单个目标的估计更准确，从而有效提高了小目标的检测，(3) 最终的DetecNet专为聚类区域设计，并隐式地建模了先验上下文信息，以提高检测准确性。所提出的方法在包括VisDrone、UAVDT和DOTA在内的三个流行的航空图像数据集上进行了测试。在所有实验中，与最先进的检测器相比，ClusDet都取得了令人鼓舞的性能。",
        "领域": "航空图像分析/目标检测/深度学习应用",
        "问题": "在航空图像中检测小且稀疏分布的目标",
        "动机": "解决航空图像中目标检测面临的挑战，包括目标小、难以与背景区分以及目标稀疏且非均匀分布导致的检测效率低下问题",
        "方法": "提出一个聚类检测（ClusDet）网络，该网络通过聚类提议子网络（CPNet）、尺度估计子网络（ScaleNet）和专用检测网络（DetecNet）统一目标聚类和检测，以提高检测效率和准确性",
        "关键词": [
            "航空图像",
            "目标检测",
            "聚类检测",
            "尺度估计",
            "深度学习"
        ],
        "涉及的技术概念": "ClusDet网络通过CPNet生成目标聚类区域，ScaleNet估计目标尺度，DetecNet进行目标检测，利用聚类和尺度归一化提高小目标检测的准确性和效率"
    },
    {
        "order": 513,
        "title": "X-Section: Cross-Section Prediction for Enhanced RGB-D Fusion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nicastro_X-Section_Cross-Section_Prediction_for_Enhanced_RGB-D_Fusion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nicastro_X-Section_Cross-Section_Prediction_for_Enhanced_RGB-D_Fusion_ICCV_2019_paper.html",
        "abstract": "Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g. in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g. for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects.",
        "中文标题": "X-Section: 增强RGB-D融合的横截面预测",
        "摘要翻译": "详细的3D重建是一个重要的挑战，应用于机器人技术、增强现实和虚拟现实，在过去几年中取得了令人印象深刻的进展。这些进展得益于深度相机（RGB-D）的可用性，以及计算能力的提高，例如以GPU的形式——同时也得益于机器学习在这一过程中的应用。在这里，我们提出了X-Section，一种RGB-D 3D重建方法，利用深度学习对物体的厚度进行预测，这些预测可以很容易地集成到体积多视图融合过程中，我们提出了对流行的KinectFusion方法的扩展。本质上，我们的方法允许在RGB-D相机感知到的内容之外完成一般室内场景的形状，这对于机器人操作任务或高效场景探索可能至关重要。预测物体厚度而不是体积使我们能够以相对较高的空间分辨率工作，而不会在使用的卷积神经网络上爆炸内存和训练数据需求。在一系列的定性和定量评估中，我们展示了如何准确预测物体厚度并重建包含多个物体的一般3D场景。",
        "领域": "3D重建/机器人技术/增强现实",
        "问题": "如何提高RGB-D相机在3D重建中的性能，特别是在预测物体厚度方面",
        "动机": "为了在机器人操作任务和高效场景探索中实现更准确的3D重建",
        "方法": "提出X-Section方法，利用深度学习预测物体厚度，并将其集成到体积多视图融合过程中，扩展了KinectFusion方法",
        "关键词": [
            "3D重建",
            "RGB-D融合",
            "深度学习",
            "物体厚度预测",
            "KinectFusion扩展"
        ],
        "涉及的技术概念": "RGB-D相机：一种能够同时捕捉彩色图像和深度信息的相机。深度学习：一种机器学习方法，通过使用多层神经网络来学习数据的复杂模式。卷积神经网络（CNN）：一种深度学习模型，特别适用于处理图像数据。KinectFusion：一种实时3D重建技术，使用Kinect传感器捕捉的数据来重建3D场景。"
    },
    {
        "order": 514,
        "title": "DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Duggal_DeepPruner_Learning_Efficient_Stereo_Matching_via_Differentiable_PatchMatch_ICCV_2019_paper.html",
        "abstract": "Our goal is to significantly speed up the runtime of current state-of-the-art stereo algorithms to enable real-time inference. Towards this goal, we developed a differentiable PatchMatch module that allows us to discard most disparities without requiring full cost volume evaluation. We then exploit this representation to learn which range to prune for each pixel. By progressively reducing the search space and effectively propagating such information, we are able to efficiently compute the cost volume for high likelihood hypotheses and achieve savings in both memory and computation.Finally, an image guided refinement module is exploited to further improve the performance. Since all our components are differentiable, the full network can be trained end-to-end. Our experiments show that our method achieves competitive results on KITTI and SceneFlow datasets while running in real-time at 62ms.",
        "中文标题": "DeepPruner: 通过可微分PatchMatch学习高效立体匹配",
        "摘要翻译": "我们的目标是显著加快当前最先进的立体算法的运行时间，以实现实时推理。为此，我们开发了一个可微分的PatchMatch模块，该模块允许我们在不需要完整成本体积评估的情况下丢弃大多数视差。然后，我们利用这种表示来学习每个像素的修剪范围。通过逐步减少搜索空间并有效传播此类信息，我们能够高效地计算高可能性假设的成本体积，并在内存和计算上实现节省。最后，利用图像引导的细化模块进一步提高性能。由于我们所有的组件都是可微分的，整个网络可以端到端训练。我们的实验表明，我们的方法在KITTI和SceneFlow数据集上实现了竞争性的结果，同时在62ms内实时运行。",
        "领域": "立体视觉/深度学习/实时系统",
        "问题": "加快立体匹配算法的运行时间以实现实时推理",
        "动机": "当前最先进的立体算法运行时间较长，难以满足实时推理的需求",
        "方法": "开发可微分PatchMatch模块，逐步减少搜索空间并有效传播信息，利用图像引导的细化模块提高性能",
        "关键词": [
            "立体匹配",
            "实时推理",
            "可微分PatchMatch",
            "成本体积",
            "图像引导细化"
        ],
        "涉及的技术概念": "PatchMatch是一种用于高效搜索图像块匹配的算法，可微分PatchMatch模块允许在不需要完整成本体积评估的情况下丢弃大多数视差，从而加快算法运行时间。图像引导的细化模块用于进一步提高立体匹配的性能。"
    },
    {
        "order": 515,
        "title": "RelGAN: Multi-Domain Image-to-Image Translation via Relative Attributes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_RelGAN_Multi-Domain_Image-to-Image_Translation_via_Relative_Attributes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_RelGAN_Multi-Domain_Image-to-Image_Translation_via_Relative_Attributes_ICCV_2019_paper.html",
        "abstract": "Multi-domain image-to-image translation has gained increasing attention recently. Previous methods take an image and some target attributes as inputs and generate an output image with the desired attributes. However, such methods have two limitations. First, these methods assume binary-valued attributes and thus cannot yield satisfactory results for fine-grained control. Second, these methods require specifying the entire set of target attributes, even if most of the attributes would not be changed. To address these limitations, we propose RelGAN, a new method for multi-domain image-to-image translation. The key idea is to use relative attributes, which describes the desired change on selected attributes. Our method is capable of modifying images by changing particular attributes of interest in a continuous manner while preserving the other attributes. Experimental results demonstrate both the quantitative and qualitative effectiveness of our method on the tasks of facial attribute transfer and interpolation.",
        "中文标题": "RelGAN: 通过相对属性进行多领域图像到图像转换",
        "摘要翻译": "多领域图像到图像转换最近受到了越来越多的关注。之前的方法以一张图像和一些目标属性作为输入，生成具有所需属性的输出图像。然而，这些方法有两个局限性。首先，这些方法假设属性是二值的，因此无法对细粒度控制产生满意的结果。其次，这些方法需要指定整个目标属性集，即使大多数属性不会改变。为了解决这些局限性，我们提出了RelGAN，一种新的多领域图像到图像转换方法。关键思想是使用相对属性，它描述了所选属性的期望变化。我们的方法能够通过以连续方式改变感兴趣的特定属性来修改图像，同时保留其他属性。实验结果证明了我们的方法在面部属性转移和插值任务上的定量和定性有效性。",
        "领域": "图像生成/面部识别/属性编辑",
        "问题": "多领域图像到图像转换中的细粒度控制和属性选择问题",
        "动机": "解决现有方法在细粒度控制和属性选择上的局限性",
        "方法": "提出RelGAN，使用相对属性进行多领域图像到图像转换，实现特定属性的连续修改",
        "关键词": [
            "图像生成",
            "面部识别",
            "属性编辑"
        ],
        "涉及的技术概念": "相对属性：描述所选属性的期望变化，用于细粒度控制图像属性。多领域图像到图像转换：将图像从一个领域转换到另一个领域，同时改变或保留特定属性。"
    },
    {
        "order": 516,
        "title": "Unsupervised Graph Association for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Graph_Association_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose an unsupervised graph association (UGA) framework to learn the underlying viewinvariant representations from the video pedestrian tracklets. The core points of UGA are mining the underlying cross-view associations and reducing the damage of noise associations. To this end, UGA is adopts a two-stage training strategy: (1) intra-camera learning stage and (2) intercamera learning stage. The former learns the intra-camera representation for each camera. While the latter builds a cross-view graph (CVG) to associate different cameras. By doing this, we can learn view-invariant representation for all person. Extensive experiments and ablation studies on seven re-id datasets demonstrate the superiority of the proposed UGA over most state-of-the-art unsupervised and domain adaptation re-id methods.",
        "中文标题": "无监督图关联用于行人重识别",
        "摘要翻译": "本文提出了一种无监督图关联（UGA）框架，旨在从视频行人轨迹中学习潜在的视角不变表示。UGA的核心点在于挖掘潜在的跨视角关联并减少噪声关联的损害。为此，UGA采用了两阶段训练策略：（1）相机内学习阶段和（2）相机间学习阶段。前者学习每个相机的相机内表示，而后者构建跨视角图（CVG）以关联不同的相机。通过这种方式，我们可以为所有行人学习视角不变的表示。在七个重识别数据集上的大量实验和消融研究证明了所提出的UGA相对于大多数最先进的无监督和领域适应重识别方法的优越性。",
        "领域": "行人重识别/视频分析/无监督学习",
        "问题": "如何从视频行人轨迹中学习视角不变的表示",
        "动机": "挖掘潜在的跨视角关联并减少噪声关联的损害，以提高行人重识别的准确性",
        "方法": "采用两阶段训练策略，包括相机内学习阶段和相机间学习阶段，后者通过构建跨视角图（CVG）来关联不同的相机",
        "关键词": [
            "无监督学习",
            "图关联",
            "视角不变表示",
            "行人重识别"
        ],
        "涉及的技术概念": {
            "无监督图关联（UGA）": "一种框架，用于从视频行人轨迹中学习潜在的视角不变表示",
            "跨视角图（CVG）": "用于关联不同相机的图结构，帮助学习视角不变的表示",
            "两阶段训练策略": "包括相机内学习阶段和相机间学习阶段，旨在分别学习相机内表示和跨相机关联"
        }
    },
    {
        "order": 517,
        "title": "Convolutional Sequence Generation for Skeleton-Based Action Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Convolutional_Sequence_Generation_for_Skeleton-Based_Action_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Convolutional_Sequence_Generation_for_Skeleton-Based_Action_Synthesis_ICCV_2019_paper.html",
        "abstract": "In this work, we aim to generate long actions represented as sequences of skeletons. The generated sequences must demonstrate continuous, meaningful human actions, while maintaining coherence among body parts. Instead of generating skeletons sequentially following an autoregressive model, we propose a framework that generates the entire sequence altogether by transforming from a sequence of latent vectors sampled from a Gaussian process (GP). This framework, named Convolutional Sequence Generation Network (CSGN), jointly models structures in temporal and spatial dimensions. It captures the temporal structure at multiple scales through the GP prior and the temporal convolutions; and establishes the spatial connection between the latent vectors and the skeleton graphs via a novel graph refining scheme. It is noteworthy that CSGN allows bidirectional transforms between the latent and the observed spaces, thus enabling semantic manipulation of the action sequences in various forms. We conducted empirical studies on multiple datasets, including a set of high-quality dancing sequences collected by us. The results show that our framework can produce long action sequences that are coherent across time steps and among body parts.",
        "中文标题": "基于骨架的动作合成的卷积序列生成",
        "摘要翻译": "在本工作中，我们的目标是生成长时间动作，这些动作以骨架序列的形式表示。生成的序列必须展示出连续、有意义的人类动作，同时保持身体部位之间的连贯性。与遵循自回归模型顺序生成骨架不同，我们提出了一个框架，该框架通过从高斯过程（GP）采样的潜在向量序列转换来一次性生成整个序列。这个名为卷积序列生成网络（CSGN）的框架，共同建模了时间和空间维度上的结构。它通过GP先验和时间卷积在多个尺度上捕捉时间结构；并通过一种新颖的图精炼方案建立了潜在向量与骨架图之间的空间连接。值得注意的是，CSGN允许潜在空间和观察空间之间的双向转换，从而使得动作序列能够以各种形式进行语义操作。我们在多个数据集上进行了实证研究，包括我们自己收集的一组高质量舞蹈序列。结果表明，我们的框架能够生成在时间步长和身体部位之间连贯的长动作序列。",
        "领域": "动作合成/骨架分析/序列生成",
        "问题": "如何生成长时间、连贯且有意义的人类动作序列",
        "动机": "为了克服自回归模型在生成骨架序列时的局限性，提出一种能够一次性生成整个动作序列的方法，以保持动作的连贯性和身体部位之间的协调性",
        "方法": "提出卷积序列生成网络（CSGN），通过从高斯过程采样的潜在向量序列转换来生成整个动作序列，利用GP先验和时间卷积捕捉时间结构，通过图精炼方案建立空间连接，并实现潜在空间与观察空间之间的双向转换",
        "关键词": [
            "动作合成",
            "骨架序列",
            "卷积序列生成网络",
            "高斯过程",
            "图精炼"
        ],
        "涉及的技术概念": "卷积序列生成网络（CSGN）是一种能够同时处理时间和空间信息的深度学习框架，它通过高斯过程（GP）先验和时间卷积来捕捉动作序列的时间结构，并通过图精炼技术建立潜在向量与骨架图之间的空间连接。这种框架支持潜在空间与观察空间之间的双向转换，使得动作序列能够进行语义上的操作和调整。"
    },
    {
        "order": 518,
        "title": "Attribute-Driven Spontaneous Motion in Unpaired Image Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Attribute-Driven_Spontaneous_Motion_in_Unpaired_Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Attribute-Driven_Spontaneous_Motion_in_Unpaired_Image_Translation_ICCV_2019_paper.html",
        "abstract": "Current image translation methods, albeit effective to produce high-quality results in various applications, still do not consider much geometric transform. We in this paper propose the spontaneous motion estimation module, along with a refinement part, to learn attribute-driven deformation between source and target domains. Extensive experiments and visualization demonstrate effectiveness of these modules. We achieve promising results in unpaired-image translation tasks, and enable interesting applications based on spontaneous motion.",
        "中文标题": "属性驱动的无配对图像翻译中的自发运动",
        "摘要翻译": "当前的图像翻译方法，尽管在各种应用中能够产生高质量的结果，但仍然没有充分考虑几何变换。我们在本文中提出了自发运动估计模块，以及一个细化部分，以学习源域和目标域之间的属性驱动变形。广泛的实验和可视化展示了这些模块的有效性。我们在无配对图像翻译任务中取得了有希望的结果，并基于自发运动实现了有趣的应用。",
        "领域": "图像翻译/几何变换/自发运动估计",
        "问题": "图像翻译方法中缺乏对几何变换的考虑",
        "动机": "提高图像翻译方法在考虑几何变换方面的能力，以实现更高质量的图像翻译结果",
        "方法": "提出自发运动估计模块和细化部分，以学习源域和目标域之间的属性驱动变形",
        "关键词": [
            "图像翻译",
            "几何变换",
            "自发运动估计"
        ],
        "涉及的技术概念": "自发运动估计模块用于学习源域和目标域之间的属性驱动变形，细化部分用于提高变形质量。"
    },
    {
        "order": 519,
        "title": "Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tulyakov_Learning_an_Event_Sequence_Embedding_for_Dense_Event-Based_Deep_Stereo_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tulyakov_Learning_an_Event_Sequence_Embedding_for_Dense_Event-Based_Deep_Stereo_ICCV_2019_paper.html",
        "abstract": "Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods.",
        "中文标题": "学习事件序列嵌入用于基于事件的密集深度立体视觉",
        "摘要翻译": "如今，基于帧的相机是机器视觉应用的首选传感器。然而，这些最初为获取静态图像而非感知动态不受控视觉环境而开发的相机，存在高功耗、高数据率、高延迟和低动态范围的问题。基于事件的图像传感器通过模仿生物视网膜来解决这些缺点。它不是在固定的时间间隔内测量每个像素的强度，而是报告显著像素强度变化的事件。每个这样的事件都由其位置、变化符号和精确到微秒的时间戳表示。异步事件序列需要特殊处理，因为传统算法仅适用于同步的、空间网格化的数据。为了解决这个问题，我们引入了一个新的事件序列嵌入模块，用于不同的应用。该模块首先通过在时间上局部聚合信息，使用一种新颖的全连接层来处理不规则采样的连续域，然后在离散空间域上构建事件序列的表示。基于这个模块，我们设计了一种基于深度学习的立体视觉方法，用于基于事件的相机。所提出的方法是第一个用于基于事件相机的基于学习的立体视觉方法，也是唯一一个产生密集结果的方法。我们展示了在Multi Vehicle Stereo Event Camera Dataset (MVSEC)上的大幅性能提升，该数据集已成为基于事件立体视觉方法基准测试的标准集。",
        "领域": "立体视觉/事件相机/深度学习",
        "问题": "解决基于事件相机的异步事件序列处理问题",
        "动机": "传统基于帧的相机在动态视觉环境中存在高功耗、高数据率、高延迟和低动态范围的问题，基于事件的图像传感器通过模仿生物视网膜来解决这些缺点，但需要特殊处理异步事件序列。",
        "方法": "引入一个新的事件序列嵌入模块，首先在时间上局部聚合信息，使用一种新颖的全连接层来处理不规则采样的连续域，然后在离散空间域上构建事件序列的表示，并基于此设计了一种基于深度学习的立体视觉方法。",
        "关键词": [
            "事件序列嵌入",
            "立体视觉",
            "事件相机"
        ],
        "涉及的技术概念": "事件序列嵌入模块通过时间上的局部信息聚合和空间域上的信息聚合来构建事件序列的表示，使用了一种新颖的全连接层来处理不规则采样的连续域。基于此模块设计的深度学习立体视觉方法，是第一个用于基于事件相机的基于学习的立体视觉方法，能够产生密集结果。"
    },
    {
        "order": 520,
        "title": "Learning a Mixture of Granularity-Specific Experts for Fine-Grained Categorization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Learning_a_Mixture_of_Granularity-Specific_Experts_for_Fine-Grained_Categorization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Learning_a_Mixture_of_Granularity-Specific_Experts_for_Fine-Grained_Categorization_ICCV_2019_paper.html",
        "abstract": "We aim to divide the problem space of fine-grained recognition into some specific regions. To achieve this, we develop a unified framework based on a mixture of experts. Due to limited data available for the fine-grained recognition problem, it is not feasible to learn diverse experts by using a data division strategy. To tackle the problem, we promote diversity among experts by combing an expert gradually-enhanced learning strategy and a Kullback-Leibler divergence based constraint. The strategy learns new experts on the dataset with the prior knowledge from former experts and adds them to the model sequentially, while the introduced constraint forces the experts to produce diverse prediction distribution. These drive the experts to learn the task from different aspects, making them specialized in different subspace problems. Experiments show that the resulting model improves the classification performance and achieves the state-of-the-art performance on several fine-grained benchmark datasets.",
        "中文标题": "学习粒度特定专家的混合模型用于细粒度分类",
        "摘要翻译": "我们的目标是将细粒度识别的问题空间划分为一些特定区域。为了实现这一目标，我们开发了一个基于专家混合的统一框架。由于细粒度识别问题可用的数据有限，使用数据划分策略学习多样化的专家是不可行的。为了解决这个问题，我们通过结合专家逐步增强的学习策略和基于Kullback-Leibler散度的约束来促进专家之间的多样性。该策略利用先前专家的先验知识在数据集上学习新的专家，并依次将它们添加到模型中，而引入的约束则迫使专家产生多样化的预测分布。这些驱动专家从不同方面学习任务，使他们在不同的子空间问题上专业化。实验表明，所得到的模型提高了分类性能，并在多个细粒度基准数据集上达到了最先进的性能。",
        "领域": "细粒度分类/专家系统/深度学习",
        "问题": "细粒度识别问题空间划分及专家多样性不足",
        "动机": "由于细粒度识别问题可用的数据有限，需要一种方法来促进专家之间的多样性，以提高分类性能。",
        "方法": "开发了一个基于专家混合的统一框架，结合专家逐步增强的学习策略和基于Kullback-Leibler散度的约束来促进专家之间的多样性。",
        "关键词": [
            "细粒度分类",
            "专家系统",
            "Kullback-Leibler散度",
            "逐步增强学习"
        ],
        "涉及的技术概念": "专家混合模型是一种集成学习方法，通过组合多个专家（即模型）的预测来提高整体性能。Kullback-Leibler散度是一种衡量两个概率分布差异的方法，用于确保专家预测的多样性。逐步增强学习策略是一种逐步添加新专家到模型中的方法，每个新专家都基于先前专家的知识进行学习。"
    },
    {
        "order": 521,
        "title": "Everybody Dance Now",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chan_Everybody_Dance_Now_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chan_Everybody_Dance_Now_ICCV_2019_paper.html",
        "abstract": "This paper presents a simple method for \"do as I do\" motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.",
        "中文标题": "现在每个人都可以跳舞",
        "摘要翻译": "本文提出了一种简单的“照我做”动作转移方法：给定一个人跳舞的源视频，我们可以在目标对象仅进行几分钟的标准动作后，将该表演转移到新的（业余）目标对象上。我们将此问题视为使用姿势作为中间表示的视频到视频翻译。为了转移动作，我们从源对象中提取姿势，并应用学习到的姿势到外观的映射来生成目标对象。我们预测两个连续的帧以获得时间上连贯的视频结果，并引入一个单独的管道用于逼真的面部合成。尽管我们的方法相当简单，但它产生了令人惊讶的引人注目的结果（见视频）。这激励我们提供一个用于可靠合成内容检测的取证工具，该工具能够区分由我们的系统合成的视频和真实数据。此外，我们发布了一个前所未有的开源视频数据集，该数据集可以合法地用于训练和动作转移。",
        "领域": "动作捕捉/视频合成/面部合成",
        "问题": "如何将一个人的舞蹈动作转移到另一个人的视频中",
        "动机": "为了创造一种简单而有效的方法，使得业余舞者能够模仿专业舞者的舞蹈动作，同时提供一种工具来检测合成内容，以区分真实与合成的视频。",
        "方法": "使用姿势作为中间表示进行视频到视频的翻译，从源视频中提取姿势并应用姿势到外观的映射来生成目标视频，预测连续帧以确保时间连贯性，并引入一个单独的管道用于逼真的面部合成。",
        "关键词": [
            "动作捕捉",
            "视频合成",
            "面部合成",
            "姿势估计",
            "视频翻译"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括视频到视频的翻译、姿势估计、姿势到外观的映射、时间连贯性处理、面部合成技术以及合成内容检测。"
    },
    {
        "order": 522,
        "title": "Point-Based Multi-View Stereo Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Point-Based_Multi-View_Stereo_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Point-Based_Multi-View_Stereo_Network_ICCV_2019_paper.html",
        "abstract": "We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet.",
        "中文标题": "基于点的多视图立体网络",
        "摘要翻译": "我们介绍了Point-MVSNet，一种新颖的基于点的深度框架，用于多视图立体（MVS）。与现有的成本体积方法不同，我们的方法直接将目标场景作为点云处理。更具体地说，我们的方法以从粗到细的方式预测深度。我们首先生成一个粗略的深度图，将其转换为点云，并通过估计当前迭代深度与地面真实深度之间的残差来迭代细化点云。我们的网络通过将3D几何先验和2D纹理信息融合到特征增强的点云中，共同且有效地利用它们，并处理点云以估计每个点的3D流。这种基于点的架构比基于成本体积的对应物具有更高的准确性、更高的计算效率和更大的灵活性。实验结果表明，与DTU和Tanks and Temples数据集上的最先进方法相比，我们的方法在重建质量上实现了显著改进。我们的源代码和训练模型可在https://github.com/callmeray/PointMVSNet获取。",
        "领域": "三维重建/点云处理/深度估计",
        "问题": "提高多视图立体（MVS）重建的准确性和效率",
        "动机": "现有的成本体积方法在处理多视图立体重建时存在准确性和效率上的限制，需要一种更有效的方法来提高重建质量。",
        "方法": "提出了一种基于点的深度框架Point-MVSNet，通过从粗到细的方式预测深度，并将3D几何先验和2D纹理信息融合到特征增强的点云中，迭代细化点云以估计每个点的3D流。",
        "关键词": [
            "多视图立体",
            "点云处理",
            "深度估计",
            "三维重建"
        ],
        "涉及的技术概念": "Point-MVSNet是一种基于点的深度框架，用于多视图立体（MVS）重建。它通过从粗到细的方式预测深度，利用3D几何先验和2D纹理信息，通过特征增强的点云进行迭代细化，以估计每个点的3D流。这种方法旨在提高重建的准确性、计算效率和灵活性。"
    },
    {
        "order": 523,
        "title": "Onion-Peel Networks for Deep Video Completion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Onion-Peel_Networks_for_Deep_Video_Completion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Onion-Peel_Networks_for_Deep_Video_Completion_ICCV_2019_paper.html",
        "abstract": "We propose the onion-peel networks for video completion. Given a set of reference images and a target image with holes, our network fills the hole by referring the contents in the reference images. Our onion-peel network progressively fills the hole from the hole boundary enabling it to exploit richer contextual information for the missing regions every step. Given a sufficient number of recurrences, even a large hole can be inpainted successfully. To attend to the missing information visible in the reference images, we propose an asymmetric attention block that computes similarities between the hole boundary pixels in the target and the non-hole pixels in the references in a non-local manner. With our attention block, our network can have an unlimited spatial-temporal window size and fill the holes with globally coherent contents. In addition, our framework is applicable to the image completion guided by the reference images without any modification, which is difficult to do with the previous methods. We validate that our method produces visually pleasing image and video inpainting results in realistic test cases.",
        "中文标题": "洋葱皮网络用于深度视频补全",
        "摘要翻译": "我们提出了用于视频补全的洋葱皮网络。给定一组参考图像和一个带有孔洞的目标图像，我们的网络通过参考参考图像中的内容来填充孔洞。我们的洋葱皮网络从孔洞边界逐步填充孔洞，使其能够在每一步利用更丰富的上下文信息来补全缺失区域。给定足够数量的递归，即使是一个大孔洞也可以成功修复。为了关注参考图像中可见的缺失信息，我们提出了一种非对称注意力块，它以非局部方式计算目标中孔洞边界像素与参考中非孔洞像素之间的相似性。通过我们的注意力块，我们的网络可以拥有无限的空间-时间窗口大小，并用全局一致的内容填充孔洞。此外，我们的框架适用于由参考图像引导的图像补全，无需任何修改，这是以前的方法难以做到的。我们验证了我们的方法在现实测试案例中产生了视觉上令人愉悦的图像和视频修复结果。",
        "领域": "视频补全/图像修复/注意力机制",
        "问题": "视频和图像中缺失区域的补全",
        "动机": "为了更有效地利用参考图像中的信息来补全视频和图像中的缺失区域，提高补全结果的视觉质量和全局一致性",
        "方法": "提出了一种洋葱皮网络，通过逐步从孔洞边界填充缺失区域，并引入非对称注意力块来计算目标与参考图像之间的相似性，以实现全局一致的补全",
        "关键词": [
            "视频补全",
            "图像修复",
            "注意力机制",
            "非局部相似性",
            "全局一致性"
        ],
        "涉及的技术概念": "洋葱皮网络是一种逐步从孔洞边界向内部填充缺失区域的网络结构，非对称注意力块用于计算目标图像中孔洞边界像素与参考图像中非孔洞像素之间的相似性，以实现非局部的信息补全。"
    },
    {
        "order": 524,
        "title": "advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Transformable_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_advPattern_Physical-World_Attacks_on_Deep_Person_Re-Identification_via_Adversarially_Transformable_ICCV_2019_paper.html",
        "abstract": "Person re-identification (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks. In this paper, we take the first attempt to implement robust physical-world attacks against deep re-ID. We propose a novel attack algorithm, called advPattern, for generating adversarial patterns on clothes, which learns the variations of image pairs across cameras to pull closer the image features from the same camera, while pushing features from different cameras farther. By wearing our crafted \"invisible cloak\", an adversary can evade person search, or impersonate a target person to fool deep re-ID models in physical world. We evaluate the effectiveness of our transformable patterns on adversaries' clothes with Market1501 and our established PRCS dataset. The experimental results show that the rank-1 accuracy of re-ID models for matching the adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore, the adversary can impersonate a target person with 47.1% rank-1 accuracy and 67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID systems are vulnerable to our physical attacks.",
        "中文标题": "advPattern: 通过对抗性可变换模式对深度行人重识别的物理世界攻击",
        "摘要翻译": "行人重识别（re-ID）是在不同摄像头视角下匹配行人图像的任务，在监控和安全应用中扮演着重要角色。受到深度学习巨大进展的启发，深度re-ID模型开始流行并获得了最先进的性能。然而，最近的研究发现深度神经网络（DNNs）对对抗样本是脆弱的，这对基于DNNs的应用构成了潜在威胁。这一现象引发了一个严重的问题，即基于深度re-ID的系统是否容易受到对抗攻击。在本文中，我们首次尝试实施对深度re-ID的鲁棒物理世界攻击。我们提出了一种新的攻击算法，称为advPattern，用于在衣物上生成对抗模式，该算法学习跨摄像头的图像对变化，以拉近来自同一摄像头的图像特征，同时推远来自不同摄像头的特征。通过穿着我们制作的“隐形斗篷”，攻击者可以逃避行人搜索，或冒充目标人物以愚弄物理世界中的深度re-ID模型。我们在Market1501和我们建立的PRCS数据集上评估了我们可变换模式在攻击者衣物上的有效性。实验结果表明，在逃避攻击下，re-ID模型匹配攻击者的rank-1准确率从87.9%下降到27.1%。此外，在冒充攻击下，攻击者可以以47.1%的rank-1准确率和67.9%的mAP冒充目标人物。结果表明，深度re-ID系统对我们的物理攻击是脆弱的。",
        "领域": "行人重识别/对抗攻击/物理世界攻击",
        "问题": "深度行人重识别系统对物理世界中的对抗攻击的脆弱性",
        "动机": "探讨深度行人重识别系统是否容易受到物理世界中的对抗攻击，并提出一种新的攻击方法以验证其脆弱性",
        "方法": "提出了一种名为advPattern的新攻击算法，通过在衣物上生成对抗模式来实施物理世界攻击，该算法学习跨摄像头的图像对变化，以拉近来自同一摄像头的图像特征，同时推远来自不同摄像头的特征",
        "关键词": [
            "行人重识别",
            "对抗攻击",
            "物理世界攻击",
            "对抗模式",
            "深度神经网络"
        ],
        "涉及的技术概念": "深度行人重识别（deep re-ID）是指利用深度学习技术在不同摄像头视角下匹配行人图像的任务。对抗攻击（adversarial attack）是指通过精心设计的输入（对抗样本）来欺骗机器学习模型，使其产生错误的输出。物理世界攻击（physical-world attack）是指在现实世界中实施的对抗攻击，与数字世界攻击相对。对抗模式（adversarial patterns）是指在物理世界中通过特定方式生成的模式，用于欺骗机器学习模型。"
    },
    {
        "order": 525,
        "title": "Multimodal Style Transfer via Graph Cuts",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Multimodal_Style_Transfer_via_Graph_Cuts_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Multimodal_Style_Transfer_via_Graph_Cuts_ICCV_2019_paper.html",
        "abstract": "An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST.",
        "中文标题": "通过图割实现多模态风格迁移",
        "摘要翻译": "近年来神经风格迁移方法中广泛使用的一个假设是，图像风格可以通过深度特征的全局统计量（如Gram矩阵或协方差矩阵）来描述。其他方法通过将风格分解为局部像素或神经补丁来表示风格。尽管最近取得了进展，但大多数现有方法统一处理风格图像的语义模式，导致在复杂风格上产生不令人满意的结果。在本文中，我们介绍了一种更灵活和通用的通用风格迁移技术：多模态风格迁移（MST）。MST明确考虑了内容和风格图像中语义模式的匹配。具体来说，风格图像特征被聚类为子风格组件，这些组件在图割公式下与局部内容特征匹配。训练一个重建网络来迁移每个子风格并渲染最终的风格化结果。我们还推广了MST以改进一些现有方法。大量实验证明了MST的优越有效性、鲁棒性和灵活性。",
        "领域": "风格迁移/图像处理/深度学习",
        "问题": "现有风格迁移方法在处理复杂风格时效果不佳",
        "动机": "提高风格迁移技术在复杂风格上的效果，使其更加灵活和通用",
        "方法": "提出多模态风格迁移（MST）技术，通过聚类风格图像特征为子风格组件，并在图割公式下与局部内容特征匹配，训练重建网络进行风格迁移",
        "关键词": [
            "风格迁移",
            "图割",
            "语义模式匹配"
        ],
        "涉及的技术概念": "多模态风格迁移（MST）是一种新的风格迁移技术，它通过将风格图像特征聚类为子风格组件，并在图割公式下与局部内容特征匹配，来改进风格迁移的效果。这种方法旨在解决现有方法在处理复杂风格时效果不佳的问题，通过训练一个重建网络来迁移每个子风格并渲染最终的风格化结果。"
    },
    {
        "order": 526,
        "title": "Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Discrete_Laplace_Operator_Estimation_for_Dynamic_3D_Reconstruction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Discrete_Laplace_Operator_Estimation_for_Dynamic_3D_Reconstruction_ICCV_2019_paper.html",
        "abstract": "We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclidean shape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association.",
        "中文标题": "动态三维重建中的离散拉普拉斯算子估计",
        "摘要翻译": "我们提出了一种从多个独立且不受控制的图像源进行动态三维重建的通用范式，这些图像源具有任意的时间采样密度和分布。我们的图论公式通过联合估计其三维几何形状及其离散拉普拉斯算子来建模观察结果之间的时空关系。为此，我们定义了一个三凸优化框架，该框架利用了欧几里得形状空间和描述其局部和全局拓扑的离散拉普拉斯算子之间的几何属性和依赖关系。我们提出了可重建性分析，对运动捕捉数据和多视图图像数据集的实验，以及探索基于几何的事件分割和数据关联的应用。",
        "领域": "三维重建/图论/优化",
        "问题": "从多个独立且不受控制的图像源进行动态三维重建",
        "动机": "为了建模观察结果之间的时空关系，并联合估计其三维几何形状及其离散拉普拉斯算子",
        "方法": "定义了一个三凸优化框架，利用欧几里得形状空间和离散拉普拉斯算子之间的几何属性和依赖关系",
        "关键词": [
            "动态三维重建",
            "离散拉普拉斯算子",
            "图论",
            "三凸优化",
            "欧几里得形状空间"
        ],
        "涉及的技术概念": "离散拉普拉斯算子用于描述局部和全局拓扑，三凸优化框架用于联合估计三维几何形状和离散拉普拉斯算子，图论用于建模时空关系。"
    },
    {
        "order": 527,
        "title": "Copy-and-Paste Networks for Deep Video Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Copy-and-Paste_Networks_for_Deep_Video_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Copy-and-Paste_Networks_for_Deep_Video_Inpainting_ICCV_2019_paper.html",
        "abstract": "We present a novel deep learning based algorithm for video inpainting. Video inpainting is a process of completing corrupted or missing regions in videos. Video inpainting has additional challenges compared to image inpainting due to the extra temporal information as well as the need for maintaining the temporal coherency. We propose a novel DNN-based framework called the Copy-and-Paste Networks for video inpainting that takes advantage of additional information in other frames of the video. The network is trained to copy corresponding contents in reference frames and paste them to fill the holes in the target frame. Our network also includes an alignment network that computes homographies between frames for the alignment, enabling the network to take information from more distant frames for robustness. Our method produces visually pleasing and temporally coherent results while running faster than the state-of-the-art optimization-based method. In addition, we extend our framework for enhancing over/under exposed frames in videos. Using this enhancement technique, we were able to significantly improve the lane detection accuracy on road videos.",
        "中文标题": "用于深度视频修复的复制粘贴网络",
        "摘要翻译": "我们提出了一种基于深度学习的新算法用于视频修复。视频修复是完成视频中损坏或缺失区域的过程。与图像修复相比，视频修复由于额外的时间信息以及需要保持时间一致性而面临更多挑战。我们提出了一种名为复制粘贴网络的新型DNN框架，用于视频修复，该框架利用了视频其他帧中的额外信息。网络被训练以复制参考帧中的相应内容并将其粘贴到目标帧中以填补空洞。我们的网络还包括一个对齐网络，该网络计算帧之间的单应性以进行对齐，使网络能够从更远的帧中获取信息以提高鲁棒性。我们的方法在运行速度上比最先进的基于优化的方法更快，同时产生视觉上令人愉悦且时间上一致的结果。此外，我们扩展了我们的框架以增强视频中过曝或欠曝的帧。使用这种增强技术，我们能够显著提高道路视频中的车道检测准确性。",
        "领域": "视频修复/帧对齐/曝光增强",
        "问题": "视频中损坏或缺失区域的修复，以及过曝或欠曝帧的增强",
        "动机": "解决视频修复中由于时间信息增加和需要保持时间一致性带来的挑战，以及提高道路视频中车道检测的准确性",
        "方法": "提出了一种名为复制粘贴网络的新型DNN框架，该框架利用视频其他帧中的额外信息进行修复，并包括一个对齐网络以计算帧之间的单应性进行对齐",
        "关键词": [
            "视频修复",
            "帧对齐",
            "曝光增强"
        ],
        "涉及的技术概念": "视频修复是指完成视频中损坏或缺失区域的过程。复制粘贴网络是一种新型DNN框架，用于视频修复，它通过复制参考帧中的内容并粘贴到目标帧中来填补空洞。对齐网络计算帧之间的单应性以进行对齐，使网络能够从更远的帧中获取信息。曝光增强技术用于改善视频中过曝或欠曝的帧，从而提高车道检测的准确性。"
    },
    {
        "order": 528,
        "title": "ABD-Net: Attentive but Diverse Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_ABD-Net_Attentive_but_Diverse_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Attention mechanisms have been found effective for person re-identification (Re-ID). However, the learned \"attentive\" features are often not naturally uncorrelated or \"diverse\", which compromises the retrieval performance based on the Euclidean distance. We advocate the complementary powers of attention and diversity for Re-ID, by proposing an Attentive but Diverse Network (ABD-Net). ABD-Net seamlessly integrates attention modules and diversity regularizations throughout the entire network to learn features that are representative, robust, and more discriminative. Specifically, we introduce a pair of complementary attention modules, focusing on channel aggregation and position awareness, respectively. Then, we plug in a novel orthogonality constraint that efficiently enforces diversity on both hidden activations and weights. Through an extensive set of ablation study, we verify that the attentive and diverse terms each contributes to the performance boosts of ABD-Net. It consistently outperforms existing state-of-the-art methods on there popular person Re-ID benchmarks.",
        "中文标题": "ABD-Net: 注意力但多样化的人员重识别",
        "摘要翻译": "注意力机制已被发现对人员重识别（Re-ID）有效。然而，学习到的“注意力”特征通常不是自然不相关或“多样化”的，这影响了基于欧几里得距离的检索性能。我们提倡将注意力和多样性的互补力量用于Re-ID，通过提出一个注意力但多样化网络（ABD-Net）。ABD-Net无缝集成了注意力模块和多样性正则化，贯穿整个网络，以学习具有代表性、鲁棒性和更具区分性的特征。具体来说，我们引入了一对互补的注意力模块，分别专注于通道聚合和位置感知。然后，我们插入了一种新颖的正交性约束，有效地在隐藏激活和权重上强制执行多样性。通过一系列广泛的消融研究，我们验证了注意力和多样化项各自对ABD-Net性能提升的贡献。它在三个流行的人员Re-ID基准测试中始终优于现有的最先进方法。",
        "领域": "人员重识别/注意力机制/特征学习",
        "问题": "学习到的注意力特征缺乏自然不相关性或多样性，影响基于欧几里得距离的检索性能",
        "动机": "提倡将注意力和多样性的互补力量用于人员重识别，以提高检索性能",
        "方法": "提出ABD-Net，集成注意力模块和多样性正则化，引入互补的注意力模块和正交性约束",
        "关键词": [
            "人员重识别",
            "注意力机制",
            "特征学习",
            "正交性约束"
        ],
        "涉及的技术概念": "注意力机制用于增强特征的代表性和区分性，正交性约束用于增加特征的多样性，从而提高人员重识别的性能。"
    },
    {
        "order": 529,
        "title": "A Closed-Form Solution to Universal Style Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_A_Closed-Form_Solution_to_Universal_Style_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_A_Closed-Form_Solution_to_Universal_Style_Transfer_ICCV_2019_paper.html",
        "abstract": "Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer.",
        "中文标题": "通用风格迁移的闭式解",
        "摘要翻译": "通用风格迁移试图显式地最小化特征空间中的损失，因此它不需要在任何预定义风格上进行训练。它通常使用VGG网络的不同层作为编码器，并训练几个解码器将特征反转为图像。因此，风格迁移的效果通过特征变换实现。尽管已经提出了许多方法，但对特征变换的理论分析仍然缺失。在本文中，我们首先提出了一种新颖的解释，将其视为最优传输问题。然后，我们展示了我们的公式与之前工作如自适应实例归一化（AdaIN）和白化与着色变换（WCT）的关系。最后，在我们的公式下，通过额外考虑Gatys的内容损失，我们推导出了一个名为最优风格迁移（OST）的闭式解。相比之下，我们的解决方案可以更好地保留结构并实现视觉上令人愉悦的结果。它简单而有效，我们通过定量和定性展示了其优势。此外，我们希望我们的理论分析能够激发神经风格迁移领域的未来工作。",
        "领域": "风格迁移/特征变换/最优传输",
        "问题": "通用风格迁移中的特征变换缺乏理论分析",
        "动机": "为了填补通用风格迁移中特征变换理论分析的空白，并探索更有效的风格迁移方法",
        "方法": "将风格迁移问题视为最优传输问题，提出了一种新颖的解释，并推导出了一个闭式解，名为最优风格迁移（OST）",
        "关键词": [
            "风格迁移",
            "特征变换",
            "最优传输",
            "闭式解",
            "VGG网络",
            "自适应实例归一化",
            "白化与着色变换"
        ],
        "涉及的技术概念": {
            "VGG网络": "一种深度卷积神经网络，用于图像识别任务，其不同层可以作为编码器用于特征提取",
            "自适应实例归一化（AdaIN）": "一种用于风格迁移的技术，通过调整实例归一化的参数来适应不同的风格",
            "白化与着色变换（WCT）": "一种特征变换技术，通过白化去除特征间的相关性，然后通过着色变换赋予特征新的风格",
            "最优传输": "一种数学理论，用于研究在最小成本下将一种分布转换为另一种分布的问题",
            "闭式解": "一种可以直接计算得到的解，不需要迭代或近似方法"
        }
    },
    {
        "order": 530,
        "title": "Content and Style Disentanglement for Artistic Style Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kotovenko_Content_and_Style_Disentanglement_for_Artistic_Style_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kotovenko_Content_and_Style_Disentanglement_for_Artistic_Style_Transfer_ICCV_2019_paper.html",
        "abstract": "Artists rarely paint in a single style throughout their career. More often they change styles or develop variations of it. In addition, artworks in different styles and even within one style depict real content differently: while Picasso's Blue Period displays a vase in a blueish tone but as a whole, his Cubist works deconstruct the object. To produce artistically convincing stylizations, style transfer models must be able to reflect these changes and variations. Recently many works have aimed to improve the style transfer task, but neglected to address the described observations. We present a novel approach which captures particularities of style and the variations within and separates style and content. This is achieved by introducing two novel losses: a fixpoint triplet style loss to learn subtle variations within one style or between different styles and a disentanglement loss to ensure that the stylization is not conditioned on the real input photo. In addition the paper proposes various evaluation methods to measure the importance of both losses on the validity, quality and variability of final stylizations. We provide qualitative results to demonstrate the performance of our approach.",
        "中文标题": "内容与风格解耦的艺术风格迁移",
        "摘要翻译": "艺术家很少在其职业生涯中只使用一种风格进行绘画。更多时候，他们会改变风格或发展出风格的变体。此外，不同风格的艺术作品，甚至同一种风格内的作品，对真实内容的描绘方式也不同：例如，毕加索的蓝色时期以蓝色调展示花瓶，但作为一个整体，而他的立体派作品则解构了物体。为了产生艺术上令人信服的风格化效果，风格迁移模型必须能够反映这些变化和变体。最近，许多工作旨在改进风格迁移任务，但忽视了上述观察。我们提出了一种新方法，该方法捕捉风格的特性及其内部变化，并将风格与内容分离。这是通过引入两种新的损失函数实现的：固定点三元组风格损失，用于学习一种风格内或不同风格之间的细微变化；以及解耦损失，以确保风格化不依赖于真实的输入照片。此外，本文提出了各种评估方法，以衡量这两种损失对最终风格化的有效性、质量和变异性的重要性。我们提供了定性结果来展示我们方法的性能。",
        "领域": "艺术风格迁移/图像生成/深度学习",
        "问题": "如何有效地分离和迁移艺术作品中的内容与风格，以产生艺术上令人信服的风格化效果",
        "动机": "艺术家经常改变或发展其绘画风格，且不同风格对真实内容的描绘方式不同，现有的风格迁移模型未能充分反映这些变化和变体",
        "方法": "引入固定点三元组风格损失和解耦损失，以捕捉风格的特性及其内部变化，并将风格与内容分离",
        "关键词": [
            "艺术风格迁移",
            "图像生成",
            "深度学习"
        ],
        "涉及的技术概念": "固定点三元组风格损失用于学习风格内或风格间的细微变化，解耦损失确保风格化不依赖于输入照片的真实性，评估方法用于衡量损失对风格化效果的影响"
    },
    {
        "order": 531,
        "title": "Deep Non-Rigid Structure From Motion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.html",
        "abstract": "Current non-rigid structure from motion (NRSfM) algorithms are mainly limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in the order of magnitude. We further propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction.",
        "中文标题": "深度非刚性运动结构",
        "摘要翻译": "当前的非刚性运动结构（NRSfM）算法主要受到以下两个方面的限制：（i）图像数量，以及（ii）它们能够处理的形状变化类型。这限制了NRSfM在视觉领域许多应用中的实际效用。在本文中，我们提出了一种新颖的深度神经网络，仅从一组2D图像坐标中恢复相机姿态和3D点。所提出的神经网络在数学上可解释为多层块稀疏字典学习问题，并且能够处理前所未有的规模和形状复杂性的问题。大量实验证明了我们方法的卓越性能，我们在精度和鲁棒性方面展示了优于所有现有最先进工作的数量级。我们进一步提出了一种基于网络权重的质量度量，这避免了需要3D地面真实数据来确定我们对重建的信心。",
        "领域": "三维重建/运动结构/深度学习",
        "问题": "非刚性运动结构算法在处理图像数量和形状变化类型方面的限制",
        "动机": "提高非刚性运动结构算法在实际视觉应用中的效用",
        "方法": "提出一种新颖的深度神经网络，通过多层块稀疏字典学习问题从2D图像坐标中恢复相机姿态和3D点",
        "关键词": [
            "非刚性运动结构",
            "深度神经网络",
            "三维重建"
        ],
        "涉及的技术概念": "非刚性运动结构（NRSfM）是一种从2D图像序列中恢复3D形状和相机运动的技术。本文提出的深度神经网络通过多层块稀疏字典学习的方法，解决了传统NRSfM算法在处理大规模和复杂形状变化时的限制。"
    },
    {
        "order": 532,
        "title": "From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_From_Open_Set_to_Closed_Set_Counting_Objects_by_Spatial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_From_Open_Set_to_Closed_Set_Counting_Objects_by_Spatial_ICCV_2019_paper.html",
        "abstract": "Visual counting, a task that predicts the number of objects from an image/video, is an open-set problem by nature, i.e., the number of population can vary in [0,+[?]) in theory. However, the collected images and labeled count values are limited in reality, which means only a small closed set is observed. Existing methods typically model this task in a regression manner, while they are likely to suffer from an unseen scene with counts out of the scope of the closed set. In fact, counting is decomposable. A dense region can always be divided until the count values of sub-regions are within the previously observed closed set. Inspired by this idea, we propose a simple but effective approach, Spatial Divide-and-Conquer Network (S-DCNet). S-DCNet learns to classify closed-set counts and can generalize to open-set counts via S-DC. S-DCNet is also efficient. To avoid repeatedly computing sub-region convolutional features, S-DC is executed on the feature map instead of on the input image. S-DCNet achieves the state-of-the-art performance on three crowd counting datasets (ShanghaiTech, UCF_CC_50 and UCF-QNRF), a vehicle counting dataset (TRANCOS) and a plant counting dataset (MTC). Compared to the previous best methods, S-DCNet brings a 20.2% relative improvement on the ShanghaiTechPart B, 20.9% on the UCF-QNRF, 22.5% on the TRANCOS and 15.1% on the MTC. Code has been made available at: https://github.com/xhp-hust-2018-2011/S-DCNet.",
        "中文标题": "从开放集到封闭集：通过空间分而治之计数对象",
        "摘要翻译": "视觉计数，即从图像/视频中预测对象数量的任务，本质上是一个开放集问题，即理论上，对象的数量可以在[0,+∞)范围内变化。然而，实际上收集的图像和标记的计数值是有限的，这意味着只能观察到一个小封闭集。现有方法通常以回归方式建模此任务，但它们可能会遇到计数超出封闭集范围的未见场景。事实上，计数是可分解的。一个密集区域总是可以被分割，直到子区域的计数值在先前观察到的封闭集内。受此想法的启发，我们提出了一种简单但有效的方法，空间分而治之网络（S-DCNet）。S-DCNet学习分类封闭集计数，并可以通过S-DC推广到开放集计数。S-DCNet也很高效。为了避免重复计算子区域的卷积特征，S-DC在特征图上执行，而不是在输入图像上。S-DCNet在三个人群计数数据集（ShanghaiTech、UCF_CC_50和UCF-QNRF）、一个车辆计数数据集（TRANCOS）和一个植物计数数据集（MTC）上实现了最先进的性能。与之前的最佳方法相比，S-DCNet在ShanghaiTechPart B上带来了20.2%的相对改进，在UCF-QNRF上为20.9%，在TRANCOS上为22.5%，在MTC上为15.1%。代码已在https://github.com/xhp-hust-2018-2011/S-DCNet上提供。",
        "领域": "人群计数/车辆计数/植物计数",
        "问题": "解决视觉计数任务中开放集问题，即预测对象数量时，对象数量理论上可以在[0,+∞)范围内变化，但实际上只能观察到有限的封闭集。",
        "动机": "现有方法通常以回归方式建模视觉计数任务，但可能会遇到计数超出封闭集范围的未见场景，因此需要一种能够处理开放集计数的方法。",
        "方法": "提出了一种空间分而治之网络（S-DCNet），通过将密集区域分割直到子区域的计数值在先前观察到的封闭集内，从而学习分类封闭集计数并推广到开放集计数。",
        "关键词": [
            "视觉计数",
            "开放集问题",
            "空间分而治之",
            "S-DCNet"
        ],
        "涉及的技术概念": {
            "视觉计数": "从图像或视频中预测对象数量的任务。",
            "开放集问题": "指对象数量理论上可以在[0,+∞)范围内变化的问题。",
            "封闭集": "实际上收集的图像和标记的计数值是有限的，只能观察到一个小封闭集。",
            "空间分而治之": "一种将密集区域分割直到子区域的计数值在先前观察到的封闭集内的方法。",
            "S-DCNet": "空间分而治之网络，一种用于视觉计数的神经网络，能够处理开放集计数问题。"
        }
    },
    {
        "order": 533,
        "title": "Compositional Video Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Compositional_Video_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Compositional_Video_Prediction_ICCV_2019_paper.html",
        "abstract": "We present an approach for pixel-level future prediction given an input image of a scene. We observe that a scene is comprised of distinct entities that undergo motion and present an approach that operationalizes this insight. We implicitly predict future states of independent entities while reasoning about their interactions, and compose future video frames using these predicted states. We overcome the inherent multi-modality of the task using a global trajectory-level latent random variable, and show that this allows us to sample diverse and plausible futures. We empirically validate our approach against alternate representations and ways of incorporating multi-modality. We examine two datasets, one comprising of stacked objects that may fall, and the other containing videos of humans performing activities in a gym, and show that our approach allows realistic stochastic video prediction across these diverse settings. See project website (https://judyye.github.io/CVP/) for video predictions.",
        "中文标题": "组合视频预测",
        "摘要翻译": "我们提出了一种方法，用于给定场景的输入图像进行像素级的未来预测。我们观察到场景由经历运动的不同实体组成，并提出了一种方法来操作这一见解。我们隐式地预测独立实体的未来状态，同时推理它们的相互作用，并使用这些预测状态组合未来的视频帧。我们通过使用全局轨迹级别的潜在随机变量来克服任务的固有多模态性，并表明这使我们能够采样多样且合理的未来。我们通过实验验证了我们的方法与其他表示和结合多模态的方式。我们检查了两个数据集，一个包含可能掉落的堆叠物体，另一个包含在健身房进行活动的人的视频，并表明我们的方法允许在这些多样化设置中进行现实的随机视频预测。请访问项目网站（https://judyye.github.io/CVP/）查看视频预测。",
        "领域": "视频预测/实体运动预测/多模态学习",
        "问题": "如何从单一图像预测未来视频帧",
        "动机": "探索场景中不同实体的运动及其相互作用，以实现对未来视频帧的准确预测",
        "方法": "使用全局轨迹级别的潜在随机变量来预测独立实体的未来状态，并组合这些状态以生成未来视频帧",
        "关键词": [
            "视频预测",
            "实体运动预测",
            "多模态学习"
        ],
        "涉及的技术概念": "全局轨迹级别的潜在随机变量用于克服视频预测任务中的多模态性，使得能够采样多样且合理的未来视频帧。"
    },
    {
        "order": 534,
        "title": "Progressive Reconstruction of Visual Structure for Image Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Progressive_Reconstruction_of_Visual_Structure_for_Image_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Progressive_Reconstruction_of_Visual_Structure_for_Image_Inpainting_ICCV_2019_paper.html",
        "abstract": "Inpainting methods aim to restore missing parts of corrupted images and play a critical role in many computer vision applications, such as object removal and image restoration. Although existing methods perform well on images with small holes, restoring large holes remains elusive. To address this issue, this paper proposes a Progressive Reconstruction of Visual Structure (PRVS) network that progressively reconstructs the structures and the associated visual feature. Specifically, we design a novel Visual Structure Reconstruction (VSR) layer to entangle reconstructions of the visual structure and visual feature, which benefits each other by sharing parameters. We repeatedly stack four VSR layers in both encoding and decoding stages of a U-Net like architecture to form the generator of a generative adversarial network (GAN) for restoring images with either small or large holes. We prove the generalization error upper bound of the PRVS network is O(1sqrt(N)), which theoretically guarantees its performance. Extensive empirical evaluations and comparisons on Places2, Paris Street View and CelebA datasets validate the strengths of the proposed approach and demonstrate that the model outperforms current state-of-the-art methods. The source code package is available at https://github.com/jingyuanli001/PRVS-Image-Inpainting.",
        "中文标题": "视觉结构渐进重建用于图像修复",
        "摘要翻译": "图像修复方法旨在恢复损坏图像中缺失的部分，并在许多计算机视觉应用中扮演着关键角色，例如对象移除和图像恢复。尽管现有方法在修复小孔洞图像上表现良好，但修复大孔洞仍然是一个难题。为了解决这个问题，本文提出了一种视觉结构渐进重建（PRVS）网络，该网络逐步重建结构及其相关的视觉特征。具体来说，我们设计了一种新颖的视觉结构重建（VSR）层，以纠缠视觉结构和视觉特征的重建，通过共享参数使彼此受益。我们在U-Net类似架构的编码和解码阶段重复堆叠四个VSR层，以形成生成对抗网络（GAN）的生成器，用于修复带有小孔洞或大孔洞的图像。我们证明了PRVS网络的泛化误差上界为O(1/sqrt(N))，这从理论上保证了其性能。在Places2、Paris Street View和CelebA数据集上的广泛实证评估和比较验证了所提出方法的优势，并证明该模型优于当前最先进的方法。源代码包可在https://github.com/jingyuanli001/PRVS-Image-Inpainting获取。",
        "领域": "图像修复/生成对抗网络/视觉特征重建",
        "问题": "修复图像中的大孔洞",
        "动机": "现有方法在修复小孔洞图像上表现良好，但修复大孔洞仍然是一个难题",
        "方法": "提出了一种视觉结构渐进重建（PRVS）网络，设计了一种新颖的视觉结构重建（VSR）层，通过共享参数使视觉结构和视觉特征的重建彼此受益，并在U-Net类似架构的编码和解码阶段重复堆叠四个VSR层，以形成生成对抗网络（GAN）的生成器",
        "关键词": [
            "图像修复",
            "生成对抗网络",
            "视觉特征重建"
        ],
        "涉及的技术概念": "视觉结构渐进重建（PRVS）网络、视觉结构重建（VSR）层、生成对抗网络（GAN）、U-Net架构、泛化误差上界"
    },
    {
        "order": 535,
        "title": "Equivariant Multi-View Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Esteves_Equivariant_Multi-View_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Esteves_Equivariant_Multi-View_Networks_ICCV_2019_paper.html",
        "abstract": "Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, where view permutation invariance is achieved through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification.",
        "中文标题": "等变多视图网络",
        "摘要翻译": "几种流行的3D视觉任务方法通过深度神经网络独立处理输入的多视图，这些网络在自然图像上进行了预训练，其中视图排列不变性是通过对所有视图进行单轮池化实现的。我们认为这种操作丢弃了重要信息，导致全局描述符表现不佳。在本文中，我们提出了一种多视图聚合的群卷积方法，其中卷积是在旋转群的一个离散子群上执行的，从而使得在所有视图上进行联合推理成为可能，以一种等变（而非不变）的方式，直到最后一层。我们进一步发展了这一思想，使其在旋转群的较小离散齐次空间上操作，其中使用极坐标视图表示来保持等变性，仅需输入视图的一小部分。我们在几个大规模3D形状检索任务中设定了新的技术水平，并展示了全景场景分类的额外应用。",
        "领域": "3D视觉/形状检索/场景分类",
        "问题": "多视图处理中信息丢失和全局描述符表现不佳的问题",
        "动机": "提高多视图处理中信息的保留和全局描述符的表现",
        "方法": "提出了一种基于群卷积的多视图聚合方法，通过在旋转群的离散子群上执行卷积，实现所有视图的联合等变推理",
        "关键词": [
            "3D视觉",
            "形状检索",
            "场景分类",
            "群卷积",
            "等变性"
        ],
        "涉及的技术概念": {
            "群卷积": "一种在特定群（如旋转群）上执行的卷积操作，用于处理具有对称性的数据",
            "等变性": "指变换操作与数据表示之间的关系，等变操作能够保持数据的某些变换性质",
            "极坐标视图表示": "一种在较小离散齐次空间上表示视图的方法，用于保持等变性"
        }
    },
    {
        "order": 536,
        "title": "Towards Precise End-to-End Weakly Supervised Object Detection Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Towards_Precise_End-to-End_Weakly_Supervised_Object_Detection_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Towards_Precise_End-to-End_Weakly_Supervised_Object_Detection_Network_ICCV_2019_paper.html",
        "abstract": "It is challenging for weakly supervised object detection network to precisely predict the positions of the objects, since there are no instance-level category annotations. Most existing methods tend to solve this problem by using a two-phase learning procedure, i.e., multiple instance learning detector followed by a fully supervised learning detector with bounding-box regression. Based on our observation, this procedure may lead to local minima for some object categories. In this paper, we propose to jointly train the two phases in an end-to-end manner to tackle this problem. Specifically, we design a single network with both multiple instance learning and bounding-box regression branches that share the same backbone. Meanwhile, a guided attention module using classification loss is added to the backbone for effectively extracting the implicit location information in the features. Experimental results on public datasets show that our method achieves state-of-the-art performance.",
        "中文标题": "迈向精确的端到端弱监督目标检测网络",
        "摘要翻译": "对于弱监督目标检测网络来说，精确预测目标的位置是具有挑战性的，因为没有实例级别的类别标注。大多数现有方法倾向于通过使用两阶段学习过程来解决这个问题，即首先使用多实例学习检测器，然后使用带有边界框回归的全监督学习检测器。根据我们的观察，这个过程可能会导致某些对象类别的局部最小值。在本文中，我们提出以端到端的方式联合训练这两个阶段来解决这个问题。具体来说，我们设计了一个单一网络，该网络具有多实例学习和边界框回归分支，它们共享相同的主干。同时，我们在主干上添加了一个使用分类损失的引导注意力模块，以有效提取特征中的隐式位置信息。在公共数据集上的实验结果表明，我们的方法达到了最先进的性能。",
        "领域": "目标检测/弱监督学习/端到端学习",
        "问题": "弱监督目标检测网络难以精确预测目标的位置",
        "动机": "现有方法可能导致某些对象类别的局部最小值，需要更有效的解决方案",
        "方法": "设计了一个单一网络，联合训练多实例学习和边界框回归分支，并添加引导注意力模块以提取隐式位置信息",
        "关键词": [
            "弱监督学习",
            "目标检测",
            "端到端学习",
            "多实例学习",
            "边界框回归",
            "引导注意力模块"
        ],
        "涉及的技术概念": {
            "弱监督学习": "一种机器学习方法，其中训练数据只有部分标注或不精确标注",
            "目标检测": "识别图像或视频中的对象并确定其位置的任务",
            "端到端学习": "一种学习方法，直接从输入到输出进行学习，无需手动设计中间步骤",
            "多实例学习": "一种监督学习方法，其中训练数据以包的形式给出，每个包包含多个实例，但只有包级别的标签",
            "边界框回归": "一种用于精确预测对象边界框位置的技术",
            "引导注意力模块": "一种通过分类损失引导网络关注重要特征的模块"
        }
    },
    {
        "order": 537,
        "title": "Variational Adversarial Active Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.html",
        "abstract": "Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on CIFAR10/100, Caltech-256, ImageNet, Cityscapes, and BDD100K. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.",
        "中文标题": "变分对抗主动学习",
        "摘要翻译": "主动学习旨在通过采样最具代表性的查询来开发标签高效的算法，这些查询将由一个预言机进行标记。我们描述了一种基于池的半监督主动学习算法，该算法以对抗的方式隐式地学习这种采样机制。我们的方法使用变分自编码器（VAE）和一个训练来区分未标记和标记数据的对抗网络来学习一个潜在空间。VAE和对抗网络之间的极小极大游戏是这样进行的：VAE试图欺骗对抗网络预测所有数据点都来自标记池，而对抗网络则学习如何区分潜在空间中的差异。我们在各种图像分类和语义分割基准数据集上广泛评估了我们的方法，并在CIFAR10/100、Caltech-256、ImageNet、Cityscapes和BDD100K上建立了新的技术水平。我们的结果表明，我们的对抗方法在大规模设置中学习了一个有效的低维潜在空间，并提供了一种计算高效的采样方法。我们的代码可在https://github.com/sinhasam/vaal获取。",
        "领域": "图像分类/语义分割/主动学习",
        "问题": "如何在主动学习中有效地采样最具代表性的查询以提高标签效率",
        "动机": "开发一种能够在半监督环境下通过对抗学习隐式学习采样机制的主动学习算法，以提高大规模数据集上的标签效率和计算效率",
        "方法": "使用变分自编码器（VAE）和对抗网络学习潜在空间，通过极小极大游戏使VAE尝试欺骗对抗网络预测所有数据点都来自标记池，而对抗网络则学习区分潜在空间中的差异",
        "关键词": [
            "变分自编码器",
            "对抗网络",
            "潜在空间",
            "极小极大游戏",
            "半监督学习"
        ],
        "涉及的技术概念": "变分自编码器（VAE）是一种生成模型，用于学习数据的潜在表示；对抗网络是一种通过对抗过程训练生成模型的方法，其中生成器尝试生成尽可能真实的数据，而判别器尝试区分真实数据和生成数据；极小极大游戏是指在对抗网络中，生成器和判别器之间的优化过程，其中生成器试图最小化判别器的性能，而判别器试图最大化其区分真实和生成数据的能力。"
    },
    {
        "order": 538,
        "title": "Interpolated Convolutional Networks for 3D Point Cloud Understanding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Interpolated_Convolutional_Networks_for_3D_Point_Cloud_Understanding_ICCV_2019_paper.html",
        "abstract": "Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.",
        "中文标题": "插值卷积网络用于3D点云理解",
        "摘要翻译": "点云是一种重要的3D表示形式。然而，由于点云的稀疏、不规则和无序的数据结构，直接在点云上应用卷积是具有挑战性的。在本文中，我们提出了一种新颖的插值卷积操作，InterpConv，以解决点云特征学习和理解的问题。关键思想是利用一组离散的核权重，并通过插值函数将点特征插值到邻近的核权重坐标上进行卷积。引入了一个归一化项来处理不同稀疏度水平的邻域。我们的InterpConv被证明是排列和稀疏不变的，并且可以直接处理不规则的输入。我们进一步设计了基于InterpConv层的插值卷积神经网络（InterpCNNs）来处理点云识别任务，包括形状分类、对象部分分割和室内场景语义解析。实验表明，该网络能够有效地捕捉细粒度的局部结构和全局形状上下文信息。所提出的方法在包括ModelNet40、ShapeNet Parts和S3DIS在内的公共基准测试中实现了最先进的性能。",
        "领域": "3D点云处理/卷积神经网络/特征学习",
        "问题": "直接在点云上应用卷积的挑战",
        "动机": "解决点云特征学习和理解的问题",
        "方法": "提出了一种新颖的插值卷积操作InterpConv，并设计了基于InterpConv层的插值卷积神经网络（InterpCNNs）",
        "关键词": [
            "3D点云",
            "卷积神经网络",
            "特征学习",
            "形状分类",
            "对象部分分割",
            "室内场景语义解析"
        ],
        "涉及的技术概念": "插值卷积操作（InterpConv）利用离散的核权重和插值函数进行卷积，引入归一化项处理不同稀疏度水平的邻域，设计的插值卷积神经网络（InterpCNNs）用于点云识别任务。"
    },
    {
        "order": 539,
        "title": "Confidence Regularized Self-Training",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.html",
        "abstract": "Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST.",
        "中文标题": "置信度正则化自训练",
        "摘要翻译": "最近的领域适应进展表明，深度自训练为无监督领域适应提供了一种强有力的手段。这些方法通常涉及在目标领域进行预测，然后将置信度高的预测作为伪标签进行重新训练的迭代过程。然而，由于伪标签可能含有噪声，自训练可能会对错误的类别赋予过高的标签置信度，导致解决方案偏离并传播错误。为了解决这个问题，我们提出了一个置信度正则化自训练（CRST）框架，将其表述为正则化自训练。我们的方法将伪标签视为连续潜在变量，通过交替优化共同优化。我们提出了两种类型的置信度正则化：标签正则化（LR）和模型正则化（MR）。CRST-LR生成软伪标签，而CRST-MR鼓励网络输出的平滑性。在图像分类和语义分割上的大量实验表明，CRSTs在性能上优于未正则化的对应方法，达到了最先进的水平。本工作的代码和模型可在https://github.com/yzou2/CRST获取。",
        "领域": "领域适应/图像分类/语义分割",
        "问题": "自训练过程中伪标签的噪声问题，导致错误的类别被赋予过高的置信度，进而导致解决方案偏离并传播错误。",
        "动机": "解决自训练过程中由于伪标签噪声导致的错误传播问题，提高无监督领域适应的性能。",
        "方法": "提出了置信度正则化自训练（CRST）框架，通过交替优化将伪标签视为连续潜在变量进行优化，并引入了标签正则化（LR）和模型正则化（MR）两种置信度正则化方法。",
        "关键词": [
            "领域适应",
            "图像分类",
            "语义分割",
            "置信度正则化",
            "自训练"
        ],
        "涉及的技术概念": {
            "自训练": "一种无监督学习方法，通过迭代预测和重新训练的过程，利用高置信度的预测作为伪标签来训练模型。",
            "伪标签": "在无监督学习中，模型对未标记数据的预测结果被用作训练标签，这些标签可能含有噪声。",
            "置信度正则化": "一种正则化技术，用于调整模型对预测结果的置信度，以减少错误传播和提高模型性能。",
            "交替优化": "一种优化策略，通过交替更新不同的变量或参数来优化目标函数。",
            "软伪标签": "与硬伪标签相对，软伪标签提供了类别概率分布，而不是单一的类别标签。"
        }
    },
    {
        "order": 540,
        "title": "Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Learn_to_Scale_Generating_Multipolar_Normalized_Density_Maps_for_Crowd_ICCV_2019_paper.html",
        "abstract": "Dense crowd counting aims to predict thousands of human instances from an image, by calculating integrals of a density map over image pixels. Existing approaches mainly suffer from the extreme density variations. Such density pattern shift poses challenges even for multi-scale model ensembling. In this paper, we propose a simple yet effective approach to tackle this problem. First, a patch-level density map is extracted by a density estimation model and further grouped into several density levels which are determined over full datasets. Second, each patch density map is automatically normalized by an online center learning strategy with a multipolar center loss. Such a design can significantly condense the density distribution into several clusters, and enable that the density variance can be learned by a single model. Extensive experiments demonstrate the superiority of the proposed method. Our work outperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on the ShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets, respectively.",
        "中文标题": "学习缩放：为人群计数生成多极归一化密度图",
        "摘要翻译": "密集人群计数的目标是通过计算图像像素上密度图的积分，从图像中预测成千上万的人类实例。现有方法主要受极端密度变化的困扰。这种密度模式的变化甚至对多尺度模型集成也构成了挑战。在本文中，我们提出了一种简单而有效的方法来解决这个问题。首先，通过密度估计模型提取补丁级密度图，并根据整个数据集确定的几个密度水平进行分组。其次，每个补丁密度图通过在线中心学习策略与多极中心损失自动归一化。这样的设计可以显著将密度分布压缩成几个簇，并使单个模型能够学习密度方差。大量实验证明了所提出方法的优越性。我们的工作在ShanghaiTech Part A、ShanghaiTech Part B、UCF_CC_50和UCF-QNRF数据集上的MAE分别优于最先进的方法4.2%、14.3%、27.1%和20.1%。",
        "领域": "人群计数/密度估计/图像分析",
        "问题": "解决密集人群计数中极端密度变化的问题",
        "动机": "现有方法在处理极端密度变化时存在困难，需要一种更有效的方法来提高计数准确性",
        "方法": "提出了一种通过密度估计模型提取补丁级密度图，并通过在线中心学习策略与多极中心损失自动归一化的方法",
        "关键词": [
            "人群计数",
            "密度估计",
            "图像分析",
            "多极归一化",
            "在线中心学习"
        ],
        "涉及的技术概念": "密度图、多极中心损失、在线中心学习策略、MAE（平均绝对误差）"
    },
    {
        "order": 541,
        "title": "Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Uy_Revisiting_Point_Cloud_Classification_A_New_Benchmark_Dataset_and_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Uy_Revisiting_Point_Cloud_Classification_A_New_Benchmark_Dataset_and_Classification_ICCV_2019_paper.html",
        "abstract": "Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy ( 92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.",
        "中文标题": "重新审视点云分类：一个基于真实世界数据的新基准数据集和分类模型",
        "摘要翻译": "深度学习技术在点云数据处理方面展现了巨大的潜力，解决了3D计算机视觉中的经典问题，如3D物体分类和分割。最近的一些3D物体分类方法在CAD模型数据集（如ModelNet40）上报告了最先进的性能，准确率高达92%。尽管取得了如此令人印象深刻的成果，本文认为，当物体处于真实世界设置中时，物体分类仍然是一个具有挑战性的任务。为了证明这一点，我们引入了ScanObjectNN，一个基于扫描室内场景数据的新真实世界点云物体数据集。通过我们的综合基准测试，我们展示了我们的数据集对现有的点云分类技术构成了巨大挑战，因为来自真实世界扫描的物体常常与背景混杂，或者由于遮挡而部分缺失。我们识别了点云物体分类中的三个关键开放问题，并提出了新的点云分类神经网络，这些网络在分类具有杂乱背景的物体方面达到了最先进的性能。我们的数据集和代码在我们的项目页面https://hkust-vgd.github.io/scanobjectnn/上公开提供。",
        "领域": "3D物体分类/点云处理/真实世界数据处理",
        "问题": "在真实世界设置中的点云物体分类",
        "动机": "尽管在CAD模型数据集上取得了高准确率，但在真实世界设置中的物体分类仍然具有挑战性，因为物体常常与背景混杂或部分缺失。",
        "方法": "引入了一个新的真实世界点云物体数据集ScanObjectNN，并提出了新的点云分类神经网络来解决具有杂乱背景的物体分类问题。",
        "关键词": [
            "点云分类",
            "真实世界数据",
            "3D物体分类"
        ],
        "涉及的技术概念": "点云数据是指由大量点组成的三维数据，常用于表示物体的表面。深度学习技术通过训练神经网络来处理这些数据，以解决如3D物体分类和分割等问题。真实世界数据处理指的是对在现实环境中采集的数据进行分析和处理，这些数据往往包含噪声和不确定性。"
    },
    {
        "order": 542,
        "title": "Anchor Loss: Modulating Loss Scale Based on Prediction Difficulty",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ryou_Anchor_Loss_Modulating_Loss_Scale_Based_on_Prediction_Difficulty_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ryou_Anchor_Loss_Modulating_Loss_Scale_Based_on_Prediction_Difficulty_ICCV_2019_paper.html",
        "abstract": "We propose a novel loss function that dynamically re-scales the cross entropy based on prediction difficulty regarding a sample. Deep neural network architectures in image classification tasks struggle to disambiguate visually similar objects. Likewise, in human pose estimation symmetric body parts often confuse the network with assigning indiscriminative scores to them. This is due to the output prediction, in which only the highest confidence label is selected without taking into consideration a measure of uncertainty. In this work, we define the prediction difficulty as a relative property coming from the confidence score gap between positive and negative labels. More precisely, the proposed loss function penalizes the network to avoid the score of a false prediction being significant. To demonstrate the efficacy of our loss function, we evaluate it on two different domains: image classification and human pose estimation. We find improvements in both applications by achieving higher accuracy compared to the baseline methods.",
        "中文标题": "锚定损失：基于预测难度调整损失规模",
        "摘要翻译": "我们提出了一种新颖的损失函数，该函数根据样本的预测难度动态重新调整交叉熵的规模。在图像分类任务中，深度神经网络架构难以区分视觉上相似的对象。同样，在人体姿态估计中，对称的身体部位常常使网络混淆，给它们分配无区别的分数。这是由于输出预测中，仅选择最高置信度标签而不考虑不确定性的度量。在这项工作中，我们将预测难度定义为来自正负标签之间置信度分数差距的相对属性。更准确地说，所提出的损失函数惩罚网络以避免错误预测的分数显著。为了证明我们损失函数的有效性，我们在两个不同的领域进行了评估：图像分类和人体姿态估计。我们发现，与基线方法相比，在这两个应用中通过实现更高的准确性都有所改进。",
        "领域": "图像分类/人体姿态估计/损失函数设计",
        "问题": "深度神经网络在图像分类和人体姿态估计任务中难以区分视觉上相似的对象和对称的身体部位，导致预测不准确。",
        "动机": "为了解决深度神经网络在处理视觉上相似对象和对称身体部位时预测不准确的问题，提出一种新的损失函数来动态调整损失规模，以提高预测的准确性。",
        "方法": "提出了一种新颖的损失函数，该函数根据样本的预测难度动态重新调整交叉熵的规模，通过惩罚网络以避免错误预测的分数显著，从而提高预测的准确性。",
        "关键词": [
            "损失函数",
            "图像分类",
            "人体姿态估计",
            "预测难度",
            "交叉熵"
        ],
        "涉及的技术概念": "交叉熵是一种常用的损失函数，用于衡量两个概率分布之间的差异。在本研究中，交叉熵被动态调整，以根据预测难度重新调整损失规模，从而提高模型在图像分类和人体姿态估计任务中的性能。"
    },
    {
        "order": 543,
        "title": "Ground-to-Aerial Image Geo-Localization With a Hard Exemplar Reweighting Triplet Loss",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Ground-to-Aerial_Image_Geo-Localization_With_a_Hard_Exemplar_Reweighting_Triplet_Loss_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Ground-to-Aerial_Image_Geo-Localization_With_a_Hard_Exemplar_Reweighting_Triplet_Loss_ICCV_2019_paper.html",
        "abstract": "The task of ground-to-aerial image geo-localization can be achieved by matching a ground view query image to a reference database of aerial/satellite images. It is highly challenging due to the dramatic viewpoint changes and unknown orientations. In this paper, we propose a novel in-batch reweighting triplet loss to emphasize the positive effect of hard exemplars during end-to-end training. We also integrate an attention mechanism into our model using feature-level contextual information. To analyze the difficulty level of each triplet, we first enforce a modified logistic regression to triplets with a distance rectifying factor. Then, the reference negative distances for corresponding anchors are set, and the relative weights of triplets are computed by comparing their difficulty to the corresponding references. To reduce the influence of extreme hard data and less useful simple exemplars, the final weights are pruned using upper and lower bound constraints. Experiments on two benchmark datasets show that the proposed approach significantly outperforms the state-of-the-art methods.",
        "中文标题": "地面到航空图像地理定位与硬样本重加权三元组损失",
        "摘要翻译": "地面到航空图像地理定位任务可以通过将地面视角的查询图像与航空/卫星图像的参考数据库进行匹配来实现。由于视角的剧烈变化和未知的方向，这一任务极具挑战性。在本文中，我们提出了一种新颖的批内重加权三元组损失，以强调在端到端训练过程中硬样本的积极作用。我们还通过使用特征级上下文信息将注意力机制集成到我们的模型中。为了分析每个三元组的难度级别，我们首先对带有距离校正因子的三元组实施修改后的逻辑回归。然后，设置相应锚点的参考负距离，并通过比较它们的难度与相应参考来计算三元组的相对权重。为了减少极端硬数据和不太有用的简单样本的影响，最终权重通过上下界约束进行修剪。在两个基准数据集上的实验表明，所提出的方法显著优于最先进的方法。",
        "领域": "地理定位/图像匹配/注意力机制",
        "问题": "解决地面视角图像与航空/卫星图像之间的匹配问题，特别是在视角剧烈变化和未知方向的情况下。",
        "动机": "由于视角变化和方向未知，地面到航空图像地理定位任务极具挑战性，需要一种有效的方法来提高匹配的准确性。",
        "方法": "提出了一种新颖的批内重加权三元组损失，强调硬样本的积极作用，并集成注意力机制使用特征级上下文信息。通过修改后的逻辑回归分析三元组难度，设置参考负距离，计算相对权重，并通过上下界约束修剪最终权重。",
        "关键词": [
            "地理定位",
            "图像匹配",
            "注意力机制",
            "三元组损失",
            "逻辑回归"
        ],
        "涉及的技术概念": "批内重加权三元组损失是一种用于强调硬样本在训练过程中积极作用的技术。注意力机制通过使用特征级上下文信息来增强模型的性能。修改后的逻辑回归用于分析三元组的难度级别，而距离校正因子和上下界约束用于调整和修剪权重，以减少极端数据的影响。"
    },
    {
        "order": 544,
        "title": "PointCloud Saliency Maps",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_PointCloud_Saliency_Maps_ICCV_2019_paper.html",
        "abstract": "3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.  classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on https://github.com/tianzheng4/PointCloud-Saliency-Maps",
        "中文标题": "点云显著性图",
        "摘要翻译": "使用PointNet及其变体进行3D点云识别已经取得了显著进展。然而，一个缺失的成分是能够自动评估点对分类性能的重要性，这通常通过显著性图来反映。显著性图是一个重要工具，因为它允许人们对点云数据执行进一步的处理。在本文中，我们提出了一种新的方法来表征关键点和段，以构建点云显著性图。我们的方法为每个点分配一个分数，反映其对模型识别损失的贡献。显著性图明确解释了哪些点是模型识别的关键。此外，高得分点的聚合表明了点云中的重要段/子集。我们构建显著性图的动机是通过点丢弃，这是一个不可微分的操作。为了克服这个问题，我们通过将点向云质心移动的可微分过程来近似点丢弃。因此，每个显著性分数可以通过损失相对于球坐标下的点的相应梯度来有效测量。在包括PointNet、PointNet++和DGCNN在内的几种最先进的点云识别模型上的广泛评估，证明了我们提出的显著性图的真实性和通用性。实验代码发布于https://github.com/tianzheng4/PointCloud-Saliency-Maps。",
        "领域": "3D点云识别/显著性图构建/深度学习模型解释",
        "问题": "自动评估点云中各点对分类性能的重要性",
        "动机": "通过构建显著性图来明确解释哪些点是模型识别的关键，并允许对点云数据执行进一步的处理",
        "方法": "提出了一种新的方法来表征关键点和段，通过为每个点分配一个反映其对模型识别损失贡献的分数，并采用可微分过程近似点丢弃操作来构建点云显著性图",
        "关键词": [
            "3D点云识别",
            "显著性图",
            "模型解释"
        ],
        "涉及的技术概念": "PointNet、PointNet++、DGCNN是用于3D点云识别的深度学习模型。显著性图是一种工具，用于评估点云中各点对分类性能的重要性。点丢弃是一种操作，通过将点向云质心移动来近似，以便于计算显著性分数。"
    },
    {
        "order": 545,
        "title": "ShellNet: Efficient Point Cloud Convolutional Neural Networks Using Concentric Shells Statistics",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_ShellNet_Efficient_Point_Cloud_Convolutional_Neural_Networks_Using_Concentric_Shells_ICCV_2019_paper.html",
        "abstract": "Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient end-to-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train.",
        "中文标题": "ShellNet: 使用同心球壳统计的高效点云卷积神经网络",
        "摘要翻译": "自从能够处理点云数据中点顺序模糊性的卷积神经网络引入以来，深度学习在3D数据方面取得了显著进展。尽管在各种场景理解任务中能够达到良好的准确度，先前的方法往往训练速度慢且网络架构复杂。在本文中，我们通过提出一种用于点云深度学习的高效端到端排列不变卷积来解决这些问题。我们简单而有效的卷积算子名为ShellConv，它使用同心球壳的统计来定义代表性特征并解决点顺序模糊性，使得传统卷积能够在这些特征上执行。基于ShellConv，我们进一步构建了一个名为ShellNet的高效神经网络，直接消费具有更大感受野的点云，同时保持较少的层数。我们通过在对象分类、对象部分分割和语义场景分割上产生最先进的结果来证明ShellNet的有效性，同时保持网络训练非常快速。",
        "领域": "点云处理/3D深度学习/场景理解",
        "问题": "解决点云数据处理中的点顺序模糊性问题，提高训练速度和简化网络架构",
        "动机": "先前的方法在点云数据处理中存在训练速度慢和网络架构复杂的问题，需要一种更高效的方法来处理点云数据",
        "方法": "提出了一种名为ShellConv的卷积算子，使用同心球壳的统计来定义代表性特征并解决点顺序模糊性，基于此构建了名为ShellNet的高效神经网络",
        "关键词": [
            "点云处理",
            "3D深度学习",
            "场景理解",
            "卷积神经网络",
            "排列不变卷积"
        ],
        "涉及的技术概念": "ShellConv是一种卷积算子，它通过使用同心球壳的统计来解决点云数据中的点顺序模糊性问题，使得传统卷积能够在这些特征上执行。ShellNet是基于ShellConv构建的神经网络，旨在直接处理具有更大感受野的点云数据，同时保持较少的网络层数，以提高训练速度和简化网络架构。"
    },
    {
        "order": 546,
        "title": "Learning to Discover Novel Visual Categories via Deep Transfer Clustering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Learning_to_Discover_Novel_Visual_Categories_via_Deep_Transfer_Clustering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Learning_to_Discover_Novel_Visual_Categories_via_Deep_Transfer_Clustering_ICCV_2019_paper.html",
        "abstract": "We consider the problem of discovering novel object categories in an image collection. While these images are unlabelled, we also assume prior knowledge of related but different image classes. We use such prior knowledge to reduce the ambiguity of clustering, and improve the quality of the newly discovered classes. Our contributions are twofold. The first contribution is to extend Deep Embedded Clustering to a transfer learning setting; we also improve the algorithm by introducing a representation bottleneck, temporal ensembling, and consistency. The second contribution is a method to estimate the number of classes in the unlabelled data. This also transfers knowledge from the known classes, using them as probes to diagnose different choices for the number of classes in the unlabelled subset. We thoroughly evaluate our method, substantially outperforming state-of-the-art techniques in a large number of benchmarks, including ImageNet, OmniGlot, CIFAR-100, CIFAR-10, and SVHN.",
        "中文标题": "通过深度迁移聚类学习发现新的视觉类别",
        "摘要翻译": "我们考虑在图像集合中发现新的物体类别的问题。虽然这些图像未标记，但我们假设存在相关但不同的图像类别的先验知识。我们利用这种先验知识来减少聚类的歧义，并提高新发现类别的质量。我们的贡献有两个方面。第一个贡献是将深度嵌入聚类扩展到迁移学习设置；我们还通过引入表示瓶颈、时间集成和一致性来改进算法。第二个贡献是估计未标记数据中类别数量的方法。这也从已知类别中转移知识，使用它们作为探针来诊断未标记子集中类别数量的不同选择。我们彻底评估了我们的方法，在包括ImageNet、OmniGlot、CIFAR-100、CIFAR-10和SVHN在内的大量基准测试中，显著优于最先进的技术。",
        "领域": "图像分类/聚类分析/迁移学习",
        "问题": "在未标记的图像集合中发现新的物体类别",
        "动机": "利用相关但不同的图像类别的先验知识，减少聚类的歧义，提高新发现类别的质量",
        "方法": "扩展深度嵌入聚类到迁移学习设置，并通过引入表示瓶颈、时间集成和一致性改进算法；估计未标记数据中类别数量的方法，从已知类别中转移知识",
        "关键词": [
            "图像分类",
            "聚类分析",
            "迁移学习",
            "表示瓶颈",
            "时间集成",
            "一致性"
        ],
        "涉及的技术概念": "深度嵌入聚类是一种将数据嵌入到低维空间后进行聚类的方法；迁移学习是一种利用已有知识来帮助解决新问题的学习方法；表示瓶颈是指在网络结构中引入限制，以减少信息量，从而迫使网络学习更有效的表示；时间集成是一种通过集成模型在不同时间点的预测来提高模型性能的技术；一致性是指模型在不同条件下预测结果的一致性，用于提高模型的鲁棒性。"
    },
    {
        "order": 547,
        "title": "Local Aggregation for Unsupervised Learning of Visual Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhuang_Local_Aggregation_for_Unsupervised_Learning_of_Visual_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhuang_Local_Aggregation_for_Unsupervised_Learning_of_Visual_Embeddings_ICCV_2019_paper.html",
        "abstract": "Unsupervised approaches to learning in neural networks are of substantial interest for furthering artificial intelligence, both because they would enable the training of networks without the need for large numbers of expensive annotations, and because they would be better models of the kind of general-purpose learning deployed by humans. However, unsupervised networks have long lagged behind the performance of their supervised counterparts, especially in the domain of large-scale visual recognition. Recent developments in training deep convolutional embeddings to maximize non-parametric instance separation and clustering objectives have shown promise in closing this gap. Here, we describe a method that trains an embedding function to maximize a metric of local aggregation, causing similar data instances to move together in the embedding space, while allowing dissimilar instances to separate. This aggregation metric is dynamic, allowing soft clusters of different scales to emerge. We evaluate our procedure on several large-scale visual recognition datasets, achieving state-of-the-art unsupervised transfer learning performance on object recognition in ImageNet, scene recognition in Places 205, and object detection in PASCAL VOC.",
        "中文标题": "局部聚合用于视觉嵌入的无监督学习",
        "摘要翻译": "神经网络中的无监督学习方法对于推进人工智能具有重要兴趣，这既因为它们能够在不需大量昂贵标注的情况下训练网络，也因为它们能更好地模拟人类所部署的通用学习类型。然而，无监督网络长期以来在性能上落后于有监督网络，尤其是在大规模视觉识别领域。最近，在训练深度卷积嵌入以最大化非参数实例分离和聚类目标方面的发展显示出缩小这一差距的潜力。在这里，我们描述了一种方法，该方法训练嵌入函数以最大化局部聚合的度量，导致相似的数据实例在嵌入空间中聚集，同时允许不相似的实例分离。这种聚合度量是动态的，允许不同尺度的软聚类出现。我们在几个大规模视觉识别数据集上评估了我们的程序，在ImageNet的对象识别、Places 205的场景识别和PASCAL VOC的对象检测上实现了最先进的无监督迁移学习性能。",
        "领域": "视觉嵌入/无监督学习/迁移学习",
        "问题": "无监督网络在大规模视觉识别领域的性能落后于有监督网络",
        "动机": "推进人工智能，减少对大量昂贵标注的依赖，模拟人类的通用学习类型",
        "方法": "训练嵌入函数以最大化局部聚合的度量，使相似实例在嵌入空间中聚集，不相似实例分离",
        "关键词": [
            "视觉嵌入",
            "无监督学习",
            "迁移学习",
            "局部聚合",
            "非参数实例分离",
            "聚类"
        ],
        "涉及的技术概念": "深度卷积嵌入、非参数实例分离、聚类目标、局部聚合度量、动态聚合度量、软聚类、大规模视觉识别数据集、无监督迁移学习"
    },
    {
        "order": 548,
        "title": "AM-LFS: AutoML for Loss Function Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_AM-LFS_AutoML_for_Loss_Function_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_AM-LFS_AutoML_for_Loss_Function_Search_ICCV_2019_paper.html",
        "abstract": "Designing an effective loss function plays an important role in visual analysis. Most existing loss function designs rely on hand-crafted heuristics that require domain experts to explore the large design space, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Loss Function Search (AM-LFS) which leverages REINFORCE to search loss functions during the training process. The key contribution of this work is the design of search space which can guarantee the generalization and transferability on different vision tasks by including a bunch of existing prevailing loss functions in a unified formulation. We also propose an efficient optimization framework which can dynamically optimize the parameters of loss function's distribution during training. Extensive experimental results on four benchmark datasets show that, without any tricks, our method outperforms existing hand-crafted loss functions in various computer vision tasks.",
        "中文标题": "AM-LFS: 自动机器学习用于损失函数搜索",
        "摘要翻译": "设计一个有效的损失函数在视觉分析中扮演着重要角色。大多数现有的损失函数设计依赖于手工制作的启发式方法，这需要领域专家探索庞大的设计空间，通常既不是最优的，也非常耗时。在本文中，我们提出了用于损失函数搜索的自动机器学习（AM-LFS），它利用REINFORCE算法在训练过程中搜索损失函数。这项工作的关键贡献是设计了搜索空间，通过将一系列现有的流行损失函数包含在一个统一的公式中，保证了在不同视觉任务上的泛化能力和可转移性。我们还提出了一个高效的优化框架，可以在训练过程中动态优化损失函数分布的参数。在四个基准数据集上的大量实验结果表明，无需任何技巧，我们的方法在各种计算机视觉任务中优于现有的手工制作的损失函数。",
        "领域": "自动机器学习/损失函数优化/视觉分析",
        "问题": "如何自动设计有效的损失函数以减少对手工启发式方法的依赖",
        "动机": "现有的损失函数设计通常需要大量的人工干预，既耗时又难以达到最优",
        "方法": "利用REINFORCE算法在训练过程中自动搜索损失函数，设计了一个包含现有流行损失函数的统一公式的搜索空间，并提出了一个动态优化损失函数分布参数的框架",
        "关键词": [
            "自动机器学习",
            "损失函数搜索",
            "REINFORCE算法",
            "视觉分析",
            "优化框架"
        ],
        "涉及的技术概念": "REINFORCE算法是一种策略梯度方法，用于在强化学习中优化策略。在本文中，它被用来在训练过程中自动搜索损失函数。损失函数是机器学习中用于衡量模型预测值与真实值差异的函数，其设计对模型性能有重要影响。"
    },
    {
        "order": 549,
        "title": "Unsupervised Deep Learning for Structured Shape Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Roufosse_Unsupervised_Deep_Learning_for_Structured_Shape_Matching_ICCV_2019_paper.html",
        "abstract": "We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely unsupervised setting, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.",
        "中文标题": "无监督深度学习用于结构化形状匹配",
        "摘要翻译": "我们提出了一种使用无监督学习计算3D形状间对应关系的新方法。我们的方法计算给定描述符函数的非线性变换，同时优化结果映射的全局结构属性，如它们的双射性或近似等距性。为此，我们使用了功能映射框架，并基于最近的FMNet架构进行描述符学习。然而，与该方法不同，我们展示了学习可以在完全无监督的环境中进行，而无需访问任何真实对应关系。这导致了一种非常通用的形状匹配方法，我们称之为SURFMNet（光谱无监督FMNet），它可以在没有任何先验信息的情况下建立3D形状集合内的对应关系。我们在广泛的挑战性基准上证明，与现有的无监督方法相比，我们的方法达到了最先进的结果，并且即使与监督学习技术相比，结果也是可比的。此外，我们的框架速度快了一个数量级，并且不依赖于测地距离计算或昂贵的后处理。",
        "领域": "3D形状分析/无监督学习/功能映射",
        "问题": "在无监督环境下计算3D形状间的对应关系",
        "动机": "开发一种无需真实对应关系即可在3D形状集合内建立对应关系的通用方法",
        "方法": "使用功能映射框架和FMNet架构进行描述符学习，优化结果映射的全局结构属性",
        "关键词": [
            "3D形状匹配",
            "无监督学习",
            "功能映射",
            "FMNet架构",
            "SURFMNet"
        ],
        "涉及的技术概念": "功能映射框架是一种用于计算形状间对应关系的数学工具，它通过将形状的几何属性映射到功能空间来实现。FMNet架构是一种用于学习形状描述符的深度学习模型，它能够捕捉形状的局部和全局特征。SURFMNet是本文提出的方法，它结合了功能映射框架和FMNet架构，在无监督环境下实现了高效的3D形状匹配。"
    },
    {
        "order": 550,
        "title": "PR Product: A Substitute for Inner Product in Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PR_Product_A_Substitute_for_Inner_Product_in_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_PR_Product_A_Substitute_for_Inner_Product_in_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "In this paper, we analyze the inner product of weight vector w and data vector x in neural networks from the perspective of vector orthogonal decomposition and prove that the direction gradient of w decreases with the angle between them close to 0 or p. We propose the Projection and Rejection Product (PR Product) to make the direction gradient of w independent of the angle and consistently larger than the one in standard inner product while keeping the forward propagation identical. As a reliable substitute for standard inner product, the PR Product can be applied into many existing deep learning modules, so we develop the PR Product version of fully connected layer, convolutional layer and LSTM layer. In static image classification, the experiments on CIFAR10 and CIFAR100 datasets demonstrate that the PR Product can robustly enhance the ability of various state-of-the-art classification networks. On the task of image captioning, even without any bells and whistles, our PR Product version of captioning model can compete or outperform the state-of-the-art models on MS COCO dataset. Code has been made available at: https://github.com/wzn0828/PR_Product.",
        "中文标题": "PR产品：神经网络中内积的替代品",
        "摘要翻译": "在本文中，我们从向量正交分解的角度分析了神经网络中权重向量w和数据向量x的内积，并证明了w的方向梯度随着它们之间的角度接近0或p而减小。我们提出了投影和拒绝积（PR积），使得w的方向梯度与角度无关，并且始终大于标准内积中的方向梯度，同时保持前向传播不变。作为标准内积的可靠替代品，PR积可以应用于许多现有的深度学习模块中，因此我们开发了全连接层、卷积层和LSTM层的PR积版本。在静态图像分类中，CIFAR10和CIFAR100数据集的实验表明，PR积能够稳健地增强各种最先进分类网络的能力。在图像描述任务中，即使没有任何花哨的技巧，我们的PR积版本的描述模型也能在MS COCO数据集上与最先进的模型竞争或超越。代码已在https://github.com/wzn0828/PR_Product上提供。",
        "领域": "神经网络优化/图像分类/图像描述",
        "问题": "神经网络中权重向量和数据向量内积的方向梯度随角度变化的问题",
        "动机": "提高神经网络中权重向量方向梯度的稳定性和效率",
        "方法": "提出投影和拒绝积（PR积）作为标准内积的替代品，开发了全连接层、卷积层和LSTM层的PR积版本",
        "关键词": [
            "神经网络",
            "内积",
            "方向梯度",
            "图像分类",
            "图像描述"
        ],
        "涉及的技术概念": {
            "向量正交分解": "一种将向量分解为相互正交的分量的方法",
            "投影和拒绝积（PR积）": "一种新的内积计算方法，旨在使权重向量的方向梯度与角度无关，并保持前向传播不变",
            "全连接层": "神经网络中的一种层，其中每个输入节点连接到每个输出节点",
            "卷积层": "神经网络中的一种层，用于处理具有网格结构的数据，如图像",
            "LSTM层": "长短期记忆网络层，一种特殊的循环神经网络层，用于处理序列数据"
        }
    },
    {
        "order": 551,
        "title": "Linearly Converging Quasi Branch and Bound Algorithms for Global Rigid Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dym_Linearly_Converging_Quasi_Branch_and_Bound_Algorithms_for_Global_Rigid_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dym_Linearly_Converging_Quasi_Branch_and_Bound_Algorithms_for_Global_Rigid_ICCV_2019_paper.html",
        "abstract": "In recent years, several branch-and-bound (BnB) algorithms have been proposed to globally optimize rigid registration problems. In this paper, we suggest a general framework to improve upon the BnB approach, which we name Quasi BnB. Quasi BnB replaces the linear lower bounds used in BnB algorithms with quadratic quasi-lower bounds which are based on the quadratic behavior of the energy in the vicinity of the global minimum. While quasi-lower bounds are not truly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we prove that it exhibits linear convergence -- it achieves epsilon accuracy in O(log(1/epsilon)) time while the time complexity of other rigid registration BnB algorithms is polynomial in 1/epsilon. Our experiments verify that Quasi-BnB is significantly more efficient than state-of-the-art BnB algorithms, especially for problems where high accuracy is desired.",
        "中文标题": "线性收敛的准分支定界算法用于全局刚性配准",
        "摘要翻译": "近年来，已经提出了几种分支定界（BnB）算法来全局优化刚性配准问题。在本文中，我们提出了一个改进BnB方法的通用框架，我们称之为准BnB。准BnB用基于能量在全局最小值附近的二次行为的二次准下界替换了BnB算法中使用的线性下界。虽然准下界并不是真正的下界，但准BnB算法是全局最优的。事实上，我们证明了它表现出线性收敛——它在O(log(1/epsilon))时间内达到epsilon精度，而其他刚性配准BnB算法的时间复杂度是1/epsilon的多项式。我们的实验验证了准BnB比最先进的BnB算法显著更高效，特别是在需要高精度的问题上。",
        "领域": "刚性配准/全局优化/算法优化",
        "问题": "提高刚性配准问题的全局优化效率和精度",
        "动机": "现有的分支定界算法在处理刚性配准问题时，虽然能够达到全局最优，但在高精度要求下效率不高。",
        "方法": "提出了一种名为准BnB的新框架，通过使用基于能量在全局最小值附近的二次行为的二次准下界，替换传统的线性下界，以提高算法的效率和精度。",
        "关键词": [
            "刚性配准",
            "全局优化",
            "算法优化",
            "准分支定界",
            "线性收敛"
        ],
        "涉及的技术概念": "分支定界算法（BnB）是一种用于全局优化的算法，通过系统地枚举所有可能的解来找到全局最优解。准下界是一种近似下界，虽然不是真正的下界，但在某些情况下可以提供更高效的搜索策略。线性收敛指的是算法在达到一定精度时，其收敛速度与迭代次数的对数成正比。"
    },
    {
        "order": 552,
        "title": "Few-Shot Object Detection via Feature Reweighting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kang_Few-Shot_Object_Detection_via_Feature_Reweighting_ICCV_2019_paper.html",
        "abstract": "Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works.",
        "中文标题": "通过特征重加权进行少样本目标检测",
        "摘要翻译": "传统的基于深度CNN的目标检测器训练需要大量的边界框注释，这对于稀有类别可能不可用。在这项工作中，我们开发了一种少样本目标检测器，可以从仅有的几个注释示例中学习检测新物体。我们提出的模型利用完全标记的基础类别，并通过元特征学习器和重加权模块在一阶段检测架构中快速适应新类别。特征学习器提取可推广到检测新物体类别的元特征，使用来自基础类别的训练数据，这些类别有足够的样本。重加权模块将来自新类别的几个支持示例转换为一个全局向量，该向量指示元特征对于检测相应物体的重要性或相关性。这两个模块与检测预测模块一起，基于一个情节式少样本学习方案和精心设计的损失函数进行端到端训练。通过广泛的实验，我们证明了我们的模型在多个数据集和设置上，对于少样本目标检测，大幅超越了成熟的基线。我们还对我们提出的模型的各个方面进行了分析，旨在为未来的少样本检测工作提供一些启示。",
        "领域": "目标检测/少样本学习/元学习",
        "问题": "解决在仅有少量注释示例的情况下检测新物体的问题",
        "动机": "传统的目标检测方法需要大量的注释数据，这对于稀有类别来说是不可行的，因此需要开发能够从少量数据中学习检测新物体的方法",
        "方法": "提出了一种结合元特征学习器和重加权模块的少样本目标检测模型，通过端到端训练快速适应新类别",
        "关键词": [
            "少样本学习",
            "目标检测",
            "元学习",
            "特征重加权"
        ],
        "涉及的技术概念": {
            "深度CNN": "深度卷积神经网络，用于图像识别和分类等任务",
            "边界框注释": "用于标记图像中物体位置的矩形框",
            "元特征学习器": "一种学习机制，旨在提取能够推广到新任务的元特征",
            "重加权模块": "一种机制，用于调整特征的重要性，以更好地适应新类别",
            "一阶段检测架构": "一种目标检测方法，直接在图像上预测物体的类别和位置，而不需要区域提议步骤",
            "情节式少样本学习": "一种学习策略，通过模拟少样本学习任务来训练模型",
            "损失函数": "用于衡量模型预测与真实值之间差异的函数，是训练模型的关键部分"
        }
    },
    {
        "order": 553,
        "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html",
        "abstract": "Regional dropout strategies have been proposed to enhance performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it suffers from information loss causing inefficiency in training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gain in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix can improve the model robustness against input corruptions and its out-of distribution detection performance.",
        "中文标题": "CutMix: 训练具有可定位特征的强分类器的正则化策略",
        "摘要翻译": "区域丢弃策略已被提出以增强卷积神经网络分类器的性能。它们已被证明能有效引导模型关注物体的较少区分性部分（例如，人的腿而不是头部），从而使网络更好地泛化并具有更好的物体定位能力。另一方面，当前的区域丢弃方法通过在训练图像上覆盖黑色像素块或随机噪声来移除信息像素。这种移除是不可取的，因为它会导致信息丢失，从而造成训练效率低下。因此，我们提出了CutMix增强策略：在训练图像之间剪切并粘贴补丁，其中地面真实标签也按补丁面积比例混合。通过有效利用训练像素并保留区域丢弃的正则化效果，CutMix在CIFAR和ImageNet分类任务以及ImageNet弱监督定位任务上始终优于最先进的增强策略。此外，与之前的增强方法不同，我们的CutMix训练的ImageNet分类器，当用作预训练模型时，在Pascal检测和MS-COCO图像字幕基准测试中实现了持续的性能提升。我们还展示了CutMix可以提高模型对输入损坏的鲁棒性及其分布外检测性能。",
        "领域": "图像分类/物体定位/模型鲁棒性",
        "问题": "如何在不丢失信息的情况下增强卷积神经网络分类器的性能和物体定位能力",
        "动机": "现有的区域丢弃方法通过移除信息像素来增强模型性能，但这种方法会导致信息丢失，影响训练效率",
        "方法": "提出CutMix增强策略，通过在训练图像之间剪切并粘贴补丁，并混合地面真实标签，以有效利用训练像素并保留区域丢弃的正则化效果",
        "关键词": [
            "图像分类",
            "物体定位",
            "模型鲁棒性",
            "增强策略",
            "正则化"
        ],
        "涉及的技术概念": {
            "区域丢弃策略": "一种通过在训练图像上覆盖黑色像素块或随机噪声来移除信息像素的方法，旨在增强卷积神经网络分类器的性能",
            "CutMix增强策略": "一种新的增强策略，通过在训练图像之间剪切并粘贴补丁，并混合地面真实标签，以有效利用训练像素并保留区域丢弃的正则化效果",
            "CIFAR和ImageNet分类任务": "两个广泛使用的图像分类基准测试，用于评估图像分类算法的性能",
            "ImageNet弱监督定位任务": "一种在ImageNet数据集上进行的任务，旨在评估模型在仅使用图像级别标签的情况下定位图像中物体的能力",
            "Pascal检测和MS-COCO图像字幕基准测试": "两个广泛使用的基准测试，分别用于评估物体检测和图像字幕生成算法的性能"
        }
    },
    {
        "order": 554,
        "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "In this paper, we introduce a new large-scale object detection dataset, Objects365, which has 365 object categories over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. It is the largest object detection dataset (with full annotation) so far and establishes a more challenging benchmark for the community. Objects365 can serve as a better feature learning dataset for localization-sensitive tasks like object detection and semantic segmentation. The Objects365 pre-trained models significantly outperform ImageNet pre-trained models with 5.6 points gain (42 vs 36.4) based on the standard setting of 90K iterations on COCO benchmark. Even compared with much long training time like 540K iterations, our Objects365 pretrained model with 90K iterations still have 2.7 points gain (42 vs 39.3). Meanwhile, the finetuning time can be greatly reduced (up to 10 times) when reaching the same accuracy. Better generalization ability of Object365 has also been verified on CityPersons, VOC segmentation, and ADE tasks. The dataset as well as the pretrained-models have been released at www.objects365.org.",
        "中文标题": "Objects365: 一个用于目标检测的大规模高质量数据集",
        "摘要翻译": "本文介绍了一个新的大规模目标检测数据集Objects365，该数据集包含365个对象类别，超过60万张训练图像。通过一个精心设计的三步注释流程，手动标注了超过1000万个高质量的边界框。它是迄今为止最大的目标检测数据集（具有完整注释），并为社区建立了一个更具挑战性的基准。Objects365可以作为定位敏感任务（如目标检测和语义分割）的更好的特征学习数据集。基于COCO基准测试的90K迭代标准设置，Objects365预训练模型显著优于ImageNet预训练模型，增益为5.6点（42对36.4）。即使与540K迭代等更长的训练时间相比，我们的Objects365预训练模型在90K迭代时仍有2.7点的增益（42对39.3）。同时，在达到相同准确度时，微调时间可以大大减少（最多10倍）。Object365的更好泛化能力也在CityPersons、VOC分割和ADE任务上得到了验证。数据集以及预训练模型已在www.objects365.org上发布。",
        "领域": "目标检测/语义分割/特征学习",
        "问题": "缺乏大规模、高质量的目标检测数据集",
        "动机": "为了建立一个更具挑战性的基准，并作为定位敏感任务的更好特征学习数据集",
        "方法": "引入了一个新的大规模目标检测数据集Objects365，通过精心设计的三步注释流程手动标注高质量的边界框，并发布了预训练模型",
        "关键词": [
            "目标检测",
            "语义分割",
            "特征学习",
            "数据集",
            "预训练模型"
        ],
        "涉及的技术概念": {
            "目标检测": "识别图像中的对象并确定其位置的技术",
            "语义分割": "将图像分割成多个区域或对象，并为每个像素分配一个类别标签的技术",
            "特征学习": "从数据中自动学习特征表示的过程，以提高模型在特定任务上的性能",
            "预训练模型": "在大规模数据集上预先训练的模型，可以用于特定任务的微调"
        }
    },
    {
        "order": 555,
        "title": "Consensus Maximization Tree Search Revisited",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Consensus_Maximization_Tree_Search_Revisited_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Consensus_Maximization_Tree_Search_Revisited_ICCV_2019_paper.html",
        "abstract": "Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach. Demo code is available at https://github.com/ZhipengCai/MaxConTreeSearch.",
        "中文标题": "共识最大化树搜索再探",
        "摘要翻译": "共识最大化在计算机视觉中广泛用于鲁棒拟合。然而，精确解决它，即找到全局最优解，是难以处理的。A*树搜索已被证明是固定参数可处理的，是最有效的精确方法之一，尽管它仍然局限于小输入。我们为提高A*树搜索做出了两个关键贡献。首先，我们展示了之前使用的共识最大化树结构实际上包含连接相邻和非相邻级别节点的路径。关键的是，连接非相邻级别的路径对于树搜索是冗余的，但之前并未避免。我们提出了一种新的加速策略，避免了这种冗余路径。在第二个贡献中，我们展示了现有的分支修剪技术也随着问题维度的增加而迅速恶化。然后，我们提出了一种新的分支修剪技术，它对维度不那么敏感，以解决这个问题。实验表明，这两种新技术都能显著加速A*树搜索，使其在以前无法处理的输入上变得相当高效。演示代码可在https://github.com/ZhipengCai/MaxConTreeSearch获取。",
        "领域": "鲁棒拟合/树搜索算法/优化算法",
        "问题": "提高A*树搜索在共识最大化问题中的效率和可扩展性",
        "动机": "解决共识最大化问题中A*树搜索方法在处理大输入时的效率低下问题",
        "方法": "提出新的加速策略避免冗余路径和新的分支修剪技术减少维度敏感性",
        "关键词": [
            "共识最大化",
            "A*树搜索",
            "分支修剪",
            "加速策略"
        ],
        "涉及的技术概念": "共识最大化是一种在计算机视觉中用于鲁棒拟合的技术，旨在找到一组数据点的最大子集，使得这些点满足某个模型。A*树搜索是一种用于寻找最优解的算法，通过评估和选择最有希望的路径来减少搜索空间。分支修剪技术用于在搜索过程中减少不必要的计算，通过提前终止那些不可能导致更好解的路径。"
    },
    {
        "order": 556,
        "title": "Towards Interpretable Object Detection by Unfolding Latent Structures",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Towards_Interpretable_Object_Detection_by_Unfolding_Latent_Structures_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Towards_Interpretable_Object_Detection_by_Unfolding_Latent_Structures_ICCV_2019_paper.html",
        "abstract": "This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. The proposed method focuses on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. It utilizes a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). It presents an AOGParsing operator that seamlessly integrates with the RoIPooling/RoIAlign operator widely used in R-CNN and is trained end-to-end. In object detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. In experiments, Faster R-CNN is used to test the proposed method on the PASCAL VOC 2007 and the COCO 2017 object detection datasets. The experimental results show that the proposed method can compute promising latent structures without hurting the performance. The code and pretrained models are available at https://github.com/iVMCL/iRCNN.",
        "中文标题": "通过展开潜在结构实现可解释的目标检测",
        "摘要翻译": "本文首先提出了一种基于展开潜在结构的思想，在视觉理解任务中制定模型可解释性的方法。随后，它通过使用流行的两阶段基于区域的卷积网络（即R-CNN）检测系统，在目标检测中进行了案例研究。所提出的方法侧重于弱监督的提取性理由生成，即在检测中自动且同时学习展开目标实例的潜在判别部分配置，而不使用任何部分配置的监督。它利用嵌入在有向无环AND-OR图（AOG）中的自上而下的分层和组合语法模型，来探索和展开感兴趣区域（RoIs）的潜在部分配置空间。它提出了一个AOGParsing操作符，该操作符与R-CNN中广泛使用的RoIPooling/RoIAlign操作符无缝集成，并进行了端到端的训练。在目标检测中，通过从AOG即时派生的最佳解析树来解释边界框，这被视为生成的定性提取性理由，用于解释检测。在实验中，使用Faster R-CNN在PASCAL VOC 2007和COCO 2017目标检测数据集上测试了所提出的方法。实验结果表明，所提出的方法可以在不损害性能的情况下计算有前途的潜在结构。代码和预训练模型可在https://github.com/iVMCL/iRCNN获取。",
        "领域": "目标检测/模型可解释性/弱监督学习",
        "问题": "如何在目标检测任务中实现模型的可解释性",
        "动机": "提高目标检测模型的可解释性，使其不仅能够检测目标，还能解释检测结果",
        "方法": "提出了一种基于展开潜在结构的弱监督提取性理由生成方法，利用嵌入在AOG中的分层和组合语法模型探索和展开RoIs的潜在部分配置空间，并通过AOGParsing操作符与RoIPooling/RoIAlign操作符无缝集成进行端到端训练",
        "关键词": [
            "目标检测",
            "模型可解释性",
            "弱监督学习",
            "AOG",
            "RoIPooling",
            "RoIAlign"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括：两阶段基于区域的卷积网络（R-CNN）、弱监督学习、有向无环AND-OR图（AOG）、RoIPooling/RoIAlign操作符、AOGParsing操作符、端到端训练。这些技术概念共同构成了实现目标检测模型可解释性的方法基础。"
    },
    {
        "order": 557,
        "title": "Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Efficient_and_Accurate_Arbitrary-Shaped_Text_Detection_With_Pixel_Aggregation_Network_ICCV_2019_paper.html",
        "abstract": "Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications. In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500.",
        "中文标题": "高效准确的任意形状文本检测与像素聚合网络",
        "摘要翻译": "场景文本检测，作为场景文本阅读系统的重要步骤，随着卷积神经网络的发展而迅速发展。然而，仍然存在两个主要挑战，阻碍了其在实际应用中的部署。第一个问题是速度与准确性之间的权衡。第二个问题是如何建模任意形状的文本实例。最近，一些方法被提出来解决任意形状文本检测的问题，但它们很少考虑整个流程的速度，这在实际应用中可能不足。在本文中，我们提出了一种高效准确的任意形状文本检测器，称为像素聚合网络（PAN），它配备了一个低计算成本的分割头和一个可学习的后处理。更具体地说，分割头由特征金字塔增强模块（FPEM）和特征融合模块（FFM）组成。FPEM是一个可级联的U形模块，可以引入多级信息以指导更好的分割。FFM可以将不同深度的FPEM提供的特征收集到最终的分割特征中。可学习的后处理通过像素聚合（PA）实现，它可以通过预测的相似度向量精确地聚合文本像素。在几个标准基准上的实验验证了所提出的PAN的优越性。值得注意的是，我们的方法在CTW1500上可以达到79.9%的竞争性F-measure，速度为84.2 FPS。",
        "领域": "场景文本检测/图像分割/卷积神经网络",
        "问题": "解决场景文本检测中速度与准确性之间的权衡以及任意形状文本实例的建模问题",
        "动机": "提高场景文本检测在实际应用中的效率和准确性，特别是对于任意形状文本的检测",
        "方法": "提出了一种名为像素聚合网络（PAN）的检测器，包括低计算成本的分割头和可学习的后处理，其中分割头由特征金字塔增强模块（FPEM）和特征融合模块（FFM）组成，后处理通过像素聚合（PA）实现",
        "关键词": [
            "场景文本检测",
            "图像分割",
            "卷积神经网络"
        ],
        "涉及的技术概念": "卷积神经网络（CNN）用于场景文本检测，特征金字塔增强模块（FPEM）和特征融合模块（FFM）用于提高分割质量，像素聚合（PA）用于精确聚合文本像素"
    },
    {
        "order": 558,
        "title": "Quasi-Globally Optimal and Efficient Vanishing Point Estimation in Manhattan World",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Quasi-Globally_Optimal_and_Efficient_Vanishing_Point_Estimation_in_Manhattan_World_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Quasi-Globally_Optimal_and_Efficient_Vanishing_Point_Estimation_in_Manhattan_World_ICCV_2019_paper.html",
        "abstract": "The image lines projected from parallel 3D lines intersect at a common point called the vanishing point (VP). Manhattan world holds for the scenes with three orthogonal VPs. In Manhattan world, given several lines in a calibrated image, we aim at clustering them by three unknown-but-sought VPs. The VP estimation can be reformulated as computing the rotation between the Manhattan frame and the camera frame. To compute this rotation, state-of-the-art methods are based on either data sampling or parameter search, and they fail to guarantee the accuracy and efficiency simultaneously. In contrast, we propose to hybridize these two strategies. We first compute two degrees of freedom (DOF) of the above rotation by two sampled image lines, and then search for the optimal third DOF based on the branch-and-bound. Our sampling accelerates our search by reducing the search space and simplifying the bound computation. Our search is not sensitive to noise and achieves quasi-global optimality in terms of maximizing the number of inliers. Experiments on synthetic and real-world images showed that our method outperforms state-of-the-art approaches in terms of accuracy and/or efficiency.",
        "中文标题": "曼哈顿世界中准全局最优且高效的消失点估计",
        "摘要翻译": "从平行的3D线投影的图像线在一个共同点相交，这个点被称为消失点（VP）。曼哈顿世界适用于具有三个正交消失点的场景。在曼哈顿世界中，给定校准图像中的几条线，我们的目标是通过三个未知但寻求的消失点对它们进行聚类。消失点估计可以重新表述为计算曼哈顿框架和相机框架之间的旋转。为了计算这个旋转，最先进的方法基于数据采样或参数搜索，它们无法同时保证准确性和效率。相比之下，我们提出将这两种策略混合使用。我们首先通过两条采样的图像线计算上述旋转的两个自由度（DOF），然后基于分支定界搜索最优的第三个自由度。我们的采样通过减少搜索空间和简化边界计算来加速我们的搜索。我们的搜索对噪声不敏感，并在最大化内点数量方面实现了准全局最优性。在合成和真实世界图像上的实验表明，我们的方法在准确性和/或效率方面优于最先进的方法。",
        "领域": "几何视觉/三维重建/图像分析",
        "问题": "在曼哈顿世界中准确且高效地估计消失点",
        "动机": "现有方法无法同时保证消失点估计的准确性和效率",
        "方法": "混合使用数据采样和参数搜索策略，首先通过采样计算旋转的两个自由度，然后基于分支定界搜索最优的第三个自由度",
        "关键词": [
            "消失点估计",
            "曼哈顿世界",
            "分支定界",
            "几何视觉",
            "三维重建"
        ],
        "涉及的技术概念": {
            "消失点（VP）": "从平行的3D线投影的图像线相交的共同点",
            "曼哈顿世界": "具有三个正交消失点的场景",
            "自由度（DOF）": "描述系统状态所需的最小独立坐标数",
            "分支定界": "一种用于解决优化问题的算法，通过系统地枚举候选解来找到最优解"
        }
    },
    {
        "order": 559,
        "title": "Scaling Object Detection by Transferring Classification Weights",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kuen_Scaling_Object_Detection_by_Transferring_Classification_Weights_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kuen_Scaling_Object_Detection_by_Transferring_Classification_Weights_ICCV_2019_paper.html",
        "abstract": "Large scale object detection datasets are constantly increasing their size in terms of the number of classes and annotations count. Yet, the number of object-level categories annotated in detection datasets is an order of magnitude smaller than image-level classification labels. State-of-the art object detection models are trained in a supervised fashion and this limits the number of object classes they can detect. In this paper, we propose a novel weight transfer network (WTN) to effectively and efficiently transfer knowledge from classification network's weights to detection network's weights to allow detection of novel classes without box supervision. We first introduce input and feature normalization schemes to curb the under-fitting during training of a vanilla WTN. We then propose autoencoder-WTN (AE-WTN) which uses reconstruction loss to preserve classification network's information over all classes in the target latent space to ensure generalization to novel classes. Compared to vanilla WTN, AE-WTN obtains absolute performance gains of 6% on two Open Images evaluation sets with 500 seen and 57 novel classes respectively, and 25% on a Visual Genome evaluation set with 200 novel classes.",
        "中文标题": "通过转移分类权重扩展目标检测",
        "摘要翻译": "大规模目标检测数据集在类别数量和标注数量方面不断增加。然而，检测数据集中标注的目标级别类别数量比图像级别分类标签少一个数量级。最先进的目标检测模型是以监督方式训练的，这限制了它们可以检测的目标类别数量。在本文中，我们提出了一种新颖的权重转移网络（WTN），以有效且高效地将知识从分类网络的权重转移到检测网络的权重，从而允许在没有框监督的情况下检测新类别。我们首先引入了输入和特征归一化方案，以遏制在训练普通WTN期间的欠拟合。然后，我们提出了自动编码器-WTN（AE-WTN），它使用重建损失来保留分类网络在目标潜在空间中所有类别的信息，以确保对新类别的泛化。与普通WTN相比，AE-WTN在两个分别包含500个已知类别和57个新类别的Open Images评估集上获得了6%的绝对性能提升，在一个包含200个新类别的Visual Genome评估集上获得了25%的绝对性能提升。",
        "领域": "目标检测/权重转移/自动编码器",
        "问题": "目标检测模型在监督学习下能够检测的类别数量有限",
        "动机": "利用分类网络的知识来扩展目标检测模型的能力，使其能够检测新类别而无需额外的框监督",
        "方法": "提出权重转移网络（WTN）和自动编码器-WTN（AE-WTN），通过输入和特征归一化以及重建损失来转移和保留分类网络的知识",
        "关键词": [
            "目标检测",
            "权重转移",
            "自动编码器",
            "类别扩展",
            "重建损失"
        ],
        "涉及的技术概念": {
            "权重转移网络（WTN）": "一种网络结构，旨在将分类网络的知识转移到检测网络中，以扩展检测模型的能力。",
            "自动编码器-WTN（AE-WTN）": "WTN的改进版本，使用自动编码器的重建损失来保留分类网络的信息，确保模型对新类别的泛化能力。",
            "输入和特征归一化": "在训练过程中采用的技术，用于减少欠拟合，提高模型的训练效率和性能。",
            "重建损失": "自动编码器中使用的一种损失函数，用于确保输入数据在潜在空间中的表示能够准确地重构原始数据。"
        }
    },
    {
        "order": 560,
        "title": "Foreground-Aware Pyramid Reconstruction for Alignment-Free Occluded Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Foreground-Aware_Pyramid_Reconstruction_for_Alignment-Free_Occluded_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Re-identifying a person across multiple disjoint camera views is important for intelligent video surveillance, smart retailing and many other applications. However, existing person re-identification methods are challenged by the ubiquitous occlusion over persons and suffer performance degradation. This paper proposes a novel occlusion-robust and alignment-free model for occluded person ReID and extends its application to realistic and crowded scenarios. The proposed model first leverages the fully convolution network (FCN) and pyramid pooling to extract spatial pyramid features. Then an alignment-free matching approach namely Foreground-aware Pyramid Reconstruction (FPR) is developed to accurately compute matching scores between occluded persons, regardless of their different scales and sizes. FPR uses the error from robust reconstruction over spatial pyramid features to measure similarities between two persons. More importantly, we design a occlusion-sensitive foreground probability generator that focuses more on clean human body parts to robustify the similarity computation with less contamination from occlusion. The FPR is easily embedded into any end-to-end person ReID models. The effectiveness of the proposed method is clearly demonstrated by the experimental results (Rank-1 accuracy) on three occluded person datasets: Partial REID (78.30%), Partial iLIDS (68.08%), Occluded REID (81.00%), and three benchmark person datasets: Market1501 (95.42%), DukeMTMC (88.64%), CUHK03 (76.08%).",
        "中文标题": "前景感知金字塔重建用于无对齐遮挡行人重识别",
        "摘要翻译": "在多个不连续的摄像机视图中重新识别行人对于智能视频监控、智能零售等许多应用非常重要。然而，现有的行人重识别方法受到普遍存在的行人遮挡的挑战，性能下降。本文提出了一种新颖的遮挡鲁棒且无需对齐的模型，用于遮挡行人重识别，并将其应用扩展到现实和拥挤的场景中。所提出的模型首先利用全卷积网络（FCN）和金字塔池化来提取空间金字塔特征。然后开发了一种无需对齐的匹配方法，即前景感知金字塔重建（FPR），以准确计算遮挡行人之间的匹配分数，无论其不同的尺度和大小。FPR使用空间金字塔特征上的鲁棒重建误差来衡量两个人之间的相似性。更重要的是，我们设计了一个遮挡敏感的前景概率生成器，它更关注干净的人体部分，以减少遮挡的污染，从而鲁棒化相似性计算。FPR可以轻松嵌入到任何端到端的行人重识别模型中。通过在三个遮挡行人数据集：Partial REID（78.30%）、Partial iLIDS（68.08%）、Occluded REID（81.00%）和三个基准行人数据集：Market1501（95.42%）、DukeMTMC（88.64%）、CUHK03（76.08%）上的实验结果（Rank-1准确率）清楚地证明了所提出方法的有效性。",
        "领域": "行人重识别/遮挡处理/视频监控",
        "问题": "解决在行人重识别中普遍存在的遮挡问题，提高识别准确率",
        "动机": "现有的行人重识别方法在存在遮挡的情况下性能下降，需要一种新的方法来提高遮挡情况下的识别准确率",
        "方法": "提出了一种新颖的遮挡鲁棒且无需对齐的模型，利用全卷积网络和金字塔池化提取特征，开发前景感知金字塔重建方法进行匹配，设计遮挡敏感的前景概率生成器以减少遮挡污染",
        "关键词": [
            "行人重识别",
            "遮挡处理",
            "前景感知",
            "金字塔重建",
            "全卷积网络"
        ],
        "涉及的技术概念": "全卷积网络（FCN）用于特征提取，金字塔池化用于提取空间金字塔特征，前景感知金字塔重建（FPR）用于计算遮挡行人之间的匹配分数，遮挡敏感的前景概率生成器用于减少遮挡对相似性计算的污染。"
    },
    {
        "order": 561,
        "title": "Scale-Aware Trident Networks for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Scale-Aware_Trident_Networks_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR.",
        "中文标题": "面向目标检测的尺度感知三叉戟网络",
        "摘要翻译": "尺度变化是目标检测中的关键挑战之一。在这项工作中，我们首先进行了一项控制实验，以研究感受野对目标检测中尺度变化的影响。基于探索实验的发现，我们提出了一种新颖的三叉戟网络（TridentNet），旨在生成具有统一表示能力的尺度特定特征图。我们构建了一个并行的多分支架构，其中每个分支共享相同的变换参数但具有不同的感受野。然后，我们采用了一种尺度感知的训练方案，通过采样适当尺度的目标实例进行训练来专门化每个分支。作为额外的好处，与普通检测器相比，三叉戟网络的快速近似版本可以在不增加任何额外参数和计算成本的情况下实现显著的改进。在COCO数据集上，我们的以ResNet-101为骨干的TridentNet实现了48.4 mAP的最新单模型结果。代码可在https://git.io/fj5vR获取。",
        "领域": "目标检测/特征提取/神经网络架构",
        "问题": "解决目标检测中尺度变化的问题",
        "动机": "研究感受野对目标检测中尺度变化的影响，并探索如何生成具有统一表示能力的尺度特定特征图",
        "方法": "提出了一种新颖的三叉戟网络（TridentNet），构建了一个并行的多分支架构，每个分支共享相同的变换参数但具有不同的感受野，并采用尺度感知的训练方案来专门化每个分支",
        "关键词": [
            "目标检测",
            "尺度变化",
            "感受野",
            "三叉戟网络",
            "特征图"
        ],
        "涉及的技术概念": "感受野指的是神经网络中一个神经元能够响应的输入区域的大小。尺度变化指的是目标在图像中的大小变化。三叉戟网络（TridentNet）是一种新颖的神经网络架构，旨在通过并行的多分支结构处理不同尺度的目标检测问题。特征图是神经网络在处理输入图像时生成的中间表示，用于捕捉图像的关键特征。"
    },
    {
        "order": 562,
        "title": "An Efficient Solution to the Homography-Based Relative Pose Problem With a Common Reference Direction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_An_Efficient_Solution_to_the_Homography-Based_Relative_Pose_Problem_With_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_An_Efficient_Solution_to_the_Homography-Based_Relative_Pose_Problem_With_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel approach to two-view minimal-case relative pose problems based on homography with a common reference direction. We explore the rank-1 constraint on the difference between the Euclidean homography matrix and the corresponding rotation, and propose an efficient two-step solution for solving both the calibrated and partially calibrated (unknown focal length) problems. We derive new 3.5-point, 3.5-point, 4-point solvers for two cameras such that the two focal lengths are unknown but equal, one of them is unknown, and both are unknown and possibly different, respectively. We present detailed analyses and comparisons with existing 6 and 7-point solvers, including results with smart phone images.",
        "中文标题": "基于共同参考方向的单应性相对姿态问题的高效解决方案",
        "摘要翻译": "在本文中，我们提出了一种基于共同参考方向的单应性的两视图最小情况相对姿态问题的新方法。我们探索了欧几里得单应矩阵与相应旋转之间差异的秩-1约束，并提出了一种高效的两步解决方案，用于解决校准和部分校准（未知焦距）问题。我们推导出了新的3.5点、3.5点、4点求解器，适用于两个相机的焦距未知但相等、其中一个未知、以及两者都未知且可能不同的情况。我们提供了详细的分析和与现有的6点和7点求解器的比较，包括使用智能手机图像的结果。",
        "领域": "相对姿态估计/单应性/相机校准",
        "问题": "解决基于共同参考方向的单应性两视图最小情况相对姿态问题",
        "动机": "探索欧几里得单应矩阵与相应旋转之间差异的秩-1约束，以提高相对姿态估计的效率和准确性",
        "方法": "提出了一种高效的两步解决方案，包括推导新的3.5点、3.5点、4点求解器，适用于不同焦距情况的两相机相对姿态问题",
        "关键词": [
            "相对姿态估计",
            "单应性",
            "相机校准",
            "欧几里得单应矩阵",
            "秩-1约束"
        ],
        "涉及的技术概念": {
            "欧几里得单应矩阵": "用于描述两个视图之间几何关系的矩阵，考虑了旋转和平移",
            "秩-1约束": "在矩阵分析中，秩-1矩阵具有特定的性质，这里用于约束单应矩阵与旋转矩阵之间的关系",
            "3.5点、4点求解器": "用于从最少点对应中求解相机相对姿态的算法，适用于不同焦距情况"
        }
    },
    {
        "order": 563,
        "title": "Collect and Select: Semantic Alignment Metric Learning for Few-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hao_Collect_and_Select_Semantic_Alignment_Metric_Learning_for_Few-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hao_Collect_and_Select_Semantic_Alignment_Metric_Learning_for_Few-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Few-shot learning aims to learn latent patterns from few training examples and has shown promises in practice. However, directly calculating the distances between the query image and support image in existing methods may cause ambiguity because dominant objects can locate anywhere on images. To address this issue, this paper proposes a Semantic Alignment Metric Learning (SAML) method for few-shot learning that aligns the semantically relevant dominant objects through a \"collect-and-select\" strategy. Specifically, we first calculate a relation matrix (RM) to \"collect\" the distances of each local region pairs of the 3D tensor extracted from a query image and the mean tensor of the support images. Then, the attention technique is adapted to \"select\" the semantically relevant pairs and put more weights on them. Afterwards, a multi-layer perceptron (MLP) is utilized to map the reweighted RMs to their corresponding similarity scores. Theoretical analysis demonstrates the generalization ability of SAML and gives a theoretical guarantee. Empirical results demonstrate that semantic alignment is achieved. Extensive experiments on benchmark datasets validate the strengths of the proposed approach and demonstrate that SAML significantly outperforms the current state-of-the-art methods. The source code is available at https://github.com/haofusheng/SAML.",
        "中文标题": "收集与选择：用于少样本学习的语义对齐度量学习",
        "摘要翻译": "少样本学习旨在从少量训练样本中学习潜在模式，并在实践中显示出前景。然而，在现有方法中直接计算查询图像和支持图像之间的距离可能会导致歧义，因为主导对象可以位于图像的任何位置。为了解决这个问题，本文提出了一种用于少样本学习的语义对齐度量学习（SAML）方法，该方法通过“收集与选择”策略对齐语义相关的主导对象。具体来说，我们首先计算一个关系矩阵（RM）来“收集”从查询图像提取的3D张量和支持图像的平均张量的每个局部区域对的距离。然后，采用注意力技术“选择”语义相关的对，并给予它们更多的权重。之后，利用多层感知器（MLP）将重新加权的RM映射到它们对应的相似度分数。理论分析证明了SAML的泛化能力，并提供了理论保证。实证结果表明实现了语义对齐。在基准数据集上的大量实验验证了所提出方法的优势，并证明SAML显著优于当前的最先进方法。源代码可在https://github.com/haofusheng/SAML获取。",
        "领域": "少样本学习/语义对齐/度量学习",
        "问题": "解决少样本学习中直接计算查询图像和支持图像之间距离导致的歧义问题",
        "动机": "由于主导对象可以位于图像的任何位置，直接计算距离可能会导致歧义，因此需要一种方法来对齐语义相关的主导对象",
        "方法": "提出了一种语义对齐度量学习（SAML）方法，通过“收集与选择”策略对齐语义相关的主导对象，包括计算关系矩阵（RM）收集局部区域对的距离，采用注意力技术选择语义相关的对并给予更多权重，利用多层感知器（MLP）映射重新加权的RM到相似度分数",
        "关键词": [
            "少样本学习",
            "语义对齐",
            "度量学习",
            "注意力技术",
            "多层感知器"
        ],
        "涉及的技术概念": {
            "少样本学习": "从少量训练样本中学习潜在模式",
            "语义对齐": "对齐语义相关的主导对象",
            "度量学习": "学习如何计算样本之间的距离",
            "注意力技术": "用于选择语义相关的对并给予更多权重",
            "多层感知器（MLP）": "用于将重新加权的RM映射到相似度分数"
        }
    },
    {
        "order": 564,
        "title": "Object-Aware Instance Labeling for Weakly Supervised Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kosugi_Object-Aware_Instance_Labeling_for_Weakly_Supervised_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kosugi_Object-Aware_Instance_Labeling_for_Weakly_Supervised_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Weakly supervised object detection (WSOD), where a detector is trained with only image-level annotations, is attracting more and more attention. As a method to obtain a well-performing detector, the detector and the instance labels are updated iteratively. In this study, for more efficient iterative updating, we focus on the instance labeling problem, a problem of which label should be annotated to each region based on the last localization result. Instead of simply labeling the top-scoring region and its highly overlapping regions as positive and others as negative, we propose more effective instance labeling methods as follows. First, to solve the problem that regions covering only some parts of the object tend to be labeled as positive, we find regions covering the whole object focusing on the context classification loss. Second, considering the situation where the other objects contained in the image can be labeled as negative, we impose a spatial restriction on regions labeled as negative. Using these instance labeling methods, we train the detector on the PASCAL VOC 2007 and 2012 and obtain significantly improved results compared with other state-of-the-art approaches.",
        "中文标题": "面向弱监督目标检测的对象感知实例标注",
        "摘要翻译": "弱监督目标检测（WSOD），即仅使用图像级注释训练检测器，正吸引越来越多的关注。作为一种获得高性能检测器的方法，检测器和实例标签被迭代更新。在本研究中，为了更有效的迭代更新，我们专注于实例标注问题，即基于上一次定位结果，应该为每个区域标注哪个标签的问题。我们提出了更有效的实例标注方法，而不是简单地将得分最高的区域及其高度重叠的区域标注为正，其他区域标注为负。首先，为了解决仅覆盖对象部分区域往往被标注为正的问题，我们专注于上下文分类损失，找到覆盖整个对象的区域。其次，考虑到图像中包含的其他对象可能被标注为负的情况，我们对标注为负的区域施加空间限制。使用这些实例标注方法，我们在PASCAL VOC 2007和2012上训练检测器，并获得了比其他最先进方法显著改进的结果。",
        "领域": "目标检测/弱监督学习/实例分割",
        "问题": "弱监督目标检测中的实例标注问题",
        "动机": "提高弱监督目标检测中实例标注的效率和准确性，以训练出更高性能的检测器",
        "方法": "提出两种改进的实例标注方法：一是通过上下文分类损失找到覆盖整个对象的区域；二是对标注为负的区域施加空间限制",
        "关键词": [
            "弱监督目标检测",
            "实例标注",
            "上下文分类损失",
            "空间限制"
        ],
        "涉及的技术概念": "弱监督目标检测（WSOD）是一种仅使用图像级注释来训练目标检测器的方法。实例标注问题是指在弱监督学习环境下，如何基于上一次的定位结果，为每个区域分配正确的标签。上下文分类损失是一种利用上下文信息来改进目标检测性能的方法。空间限制是指在实例标注过程中，对可能被错误标注为负的区域施加限制，以提高标注的准确性。"
    },
    {
        "order": 565,
        "title": "A Quaternion-Based Certifiably Optimal Solution to the Wahba Problem With Outliers",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_A_Quaternion-Based_Certifiably_Optimal_Solution_to_the_Wahba_Problem_With_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_A_Quaternion-Based_Certifiably_Optimal_Solution_to_the_Wahba_Problem_With_ICCV_2019_paper.html",
        "abstract": "The Wahba problem, also known as rotation search, seeks to find the best rotation to align two sets of vector observations given putative correspondences, and is a fundamental routine in many computer vision and robotics applications. This work proposes the first polynomial-time certifiably optimal approach for solving the Wahba problem when a large number of vector observations are outliers. Our first contribution is to formulate the Wahba problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. The second contribution is to rewrite the problem using unit quaternions and show that the TLS cost can be framed as a Quadratically-Constrained Quadratic Program (QCQP). Since the resulting optimization is still highly non-convex and hard to solve globally, our third contribution is to develop a convex Semidefinite Programming (SDP) relaxation. We show that while a naive relaxation performs poorly in general, our relaxation is tight even in the presence of large noise and outliers. We validate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite relAxation for Robust alignment), in both synthetic and real datasets showing that the algorithm outperforms RANSAC, robust local optimization techniques, global outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able to compute certifiably optimal solutions (i.e. the relaxation is exact) even in the case when 95% of the correspondences are outliers.",
        "中文标题": "基于四元数的Wahba问题带异常值的可证明最优解",
        "摘要翻译": "Wahba问题，也称为旋转搜索，旨在找到最佳旋转以对齐两组给定对应关系的向量观测，是许多计算机视觉和机器人应用中的基本例程。本文提出了第一个多项式时间可证明最优的方法，用于解决当大量向量观测为异常值时的Wahba问题。我们的第一个贡献是使用对大量虚假对应关系不敏感的截断最小二乘（TLS）成本来制定Wahba问题。第二个贡献是使用单位四元数重写问题，并展示TLS成本可以被构造成一个二次约束二次规划（QCQP）。由于由此产生的优化仍然高度非凸且难以全局解决，我们的第三个贡献是开发一个凸的半定规划（SDP）松弛。我们展示了虽然一个简单的松弛在一般情况下表现不佳，但我们的松弛即使在存在大量噪声和异常值的情况下也是紧的。我们在合成和真实数据集上验证了所提出的算法，名为QUASAR（基于四元数的半定松弛用于鲁棒对齐），表明该算法优于RANSAC、鲁棒局部优化技术、全局异常值去除程序和分支定界方法。QUASAR即使在95%的对应关系是异常值的情况下也能计算可证明的最优解（即松弛是精确的）。",
        "领域": "旋转搜索/异常值检测/优化算法",
        "问题": "解决在大量向量观测为异常值情况下的Wahba问题",
        "动机": "为了在计算机视觉和机器人应用中更有效地对齐两组向量观测，特别是在存在大量异常值的情况下",
        "方法": "使用截断最小二乘（TLS）成本制定问题，通过单位四元数重写问题并构造成二次约束二次规划（QCQP），开发凸的半定规划（SDP）松弛",
        "关键词": [
            "四元数",
            "Wahba问题",
            "异常值",
            "截断最小二乘",
            "半定规划"
        ],
        "涉及的技术概念": "Wahba问题是一种寻找最佳旋转以对齐两组向量观测的问题，广泛应用于计算机视觉和机器人领域。本文提出了一种新的方法，通过使用截断最小二乘（TLS）成本来减少异常值的影响，并通过单位四元数和二次约束二次规划（QCQP）来重写问题。为了解决由此产生的高度非凸优化问题，开发了一种凸的半定规划（SDP）松弛方法，该方法即使在存在大量噪声和异常值的情况下也能保持紧性。"
    },
    {
        "order": 566,
        "title": "Bayesian Adaptive Superpixel Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Uziel_Bayesian_Adaptive_Superpixel_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Uziel_Bayesian_Adaptive_Superpixel_Segmentation_ICCV_2019_paper.html",
        "abstract": "Superpixels provide a useful intermediate image representation. Existing superpixel methods, however, suffer from at least some of the following drawbacks: 1) topology is handled heuristically; 2) the number of superpixels is either predefined or estimated at a prohibitive cost; 3) lack of adaptiveness. As a remedy, we propose a novel probabilistic model, self-coined Bayesian Adaptive Superpixel Segmentation (BASS), together with an efficient inference. BASS is a Bayesian nonparametric mixture model that also respects topology and favors spatial coherence. The optimizationbased and topology-aware inference is parallelizable and implemented in GPU. Quantitatively, BASS achieves results that are either better than the state-of-the-art or close to it, depending on the performance index and/or dataset. Qualitatively, we argue it achieves the best results; we demonstrate this by not only subjective visual inspection but also objective quantitative performance evaluation of the downstream application of face detection. Our code is available at https://github.com/uzielroy/BASS.",
        "中文标题": "贝叶斯自适应超像素分割",
        "摘要翻译": "超像素提供了一种有用的中间图像表示。然而，现有的超像素方法至少存在以下缺点之一：1）拓扑处理是启发式的；2）超像素的数量要么是预定义的，要么是以极高的成本估计的；3）缺乏适应性。作为补救措施，我们提出了一种新的概率模型，自称为贝叶斯自适应超像素分割（BASS），以及一种高效的推理方法。BASS是一种贝叶斯非参数混合模型，它也尊重拓扑并倾向于空间一致性。基于优化和拓扑感知的推理是可并行的，并在GPU中实现。在数量上，BASS实现了要么优于现有技术，要么接近现有技术的结果，这取决于性能指标和/或数据集。在质量上，我们认为它实现了最佳结果；我们不仅通过主观视觉检查，还通过下游应用（如人脸检测）的客观定量性能评估来证明这一点。我们的代码可在https://github.com/uzielroy/BASS获取。",
        "领域": "图像分割/贝叶斯方法/并行计算",
        "问题": "现有超像素方法在拓扑处理、超像素数量预定义或估计成本高、缺乏适应性方面存在不足",
        "动机": "解决现有超像素方法的缺点，提供一种更高效、适应性更强的超像素分割方法",
        "方法": "提出了一种新的贝叶斯非参数混合模型BASS，采用基于优化和拓扑感知的推理方法，并在GPU中实现",
        "关键词": [
            "超像素",
            "贝叶斯非参数混合模型",
            "拓扑感知",
            "并行计算",
            "GPU"
        ],
        "涉及的技术概念": "贝叶斯非参数混合模型是一种统计模型，它允许模型复杂度随着数据量的增加而增加，而不需要预先指定模型参数的数量。拓扑感知指的是在模型推理过程中考虑图像的空间结构关系。并行计算和GPU实现指的是利用图形处理单元进行高效计算，以加速模型的推理过程。"
    },
    {
        "order": 567,
        "title": "Generative Modeling for Small-Data Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generative_Modeling_for_Small-Data_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Generative_Modeling_for_Small-Data_Object_Detection_ICCV_2019_paper.html",
        "abstract": "This paper explores object detection in the small data regime, where only a limited number of annotated bounding boxes are available due to data rarity and annotation expense. This is a common challenge today with machine learning being applied to many new tasks where obtaining training data is more challenging, e.g. in medical images with rare diseases that doctors sometimes only see once in their life-time. In this work we explore this problem from a generative modeling perspective by learning to generate new images with associated bounding boxes, and using these for training an object detector. We show that simply training previously proposed generative models does not yield satisfactory performance due to them optimizing for image realism rather than object detection accuracy. To this end we develop a new model with a novel unrolling mechanism that jointly optimizes the generative model and a detector such that the generated images improve the performance of the detector. We show this method outperforms the state of the art on two challenging datasets, disease detection and small data pedestrian detection, improving the average precision on NIH Chest X-ray by a relative 20% and localization accuracy by a relative 50%.",
        "中文标题": "小数据目标检测的生成建模",
        "摘要翻译": "本文探讨了在小数据情况下的目标检测问题，这种情况由于数据的稀缺性和标注成本高昂，只有有限数量的标注边界框可用。这是当今机器学习应用于许多新任务时面临的常见挑战，例如在医学图像中，医生可能一生只见过一次的罕见疾病。在这项工作中，我们从生成建模的角度探讨了这个问题，通过学习生成带有相关边界框的新图像，并使用这些图像来训练目标检测器。我们展示了简单地训练先前提出的生成模型并不能产生令人满意的性能，因为它们优化的是图像的真实性而不是目标检测的准确性。为此，我们开发了一个具有新颖展开机制的新模型，该模型联合优化生成模型和检测器，使得生成的图像能够提高检测器的性能。我们展示了这种方法在两个具有挑战性的数据集上优于现有技术，即疾病检测和小数据行人检测，将NIH胸部X射线的平均精度相对提高了20%，定位精度相对提高了50%。",
        "领域": "医学图像分析/行人检测/生成模型",
        "问题": "解决在小数据情况下目标检测的准确性问题",
        "动机": "由于数据的稀缺性和标注成本高昂，许多新任务难以获得足够的训练数据，特别是在医学图像分析等领域，这促使研究者探索新的方法来提高目标检测的准确性。",
        "方法": "开发了一个具有新颖展开机制的新模型，该模型联合优化生成模型和检测器，使得生成的图像能够提高检测器的性能。",
        "关键词": [
            "小数据目标检测",
            "生成建模",
            "医学图像分析",
            "行人检测"
        ],
        "涉及的技术概念": "生成建模是一种通过学习数据分布来生成新数据样本的技术。在本研究中，生成模型被用来生成带有标注边界框的新图像，以增加训练数据量。展开机制是一种优化技术，它允许生成模型和检测器在训练过程中相互影响，从而提高检测器的性能。"
    },
    {
        "order": 568,
        "title": "PLMP - Point-Line Minimal Problems in Complete Multi-View Visibility",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Duff_PLMP_-_Point-Line_Minimal_Problems_in_Complete_Multi-View_Visibility_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Duff_PLMP_-_Point-Line_Minimal_Problems_in_Complete_Multi-View_Visibility_ICCV_2019_paper.html",
        "abstract": "We present a complete classification of all minimal problems for generic arrangements of points and lines completely observed by calibrated perspective cameras. We show that there are only 30 minimal problems in total, no problems exist for more than 6 cameras, for more than 5 points, and for more than 6 lines. We present a sequence of tests for detecting minimality starting with counting degrees of freedom and ending with full symbolic and numeric verification of representative examples. For all minimal problems discovered, we present their algebraic degrees, i.e. the number of solutions, which measure their intrinsic difficulty. It shows how exactly the difficulty of problems grows with the number of views. Importantly, several new mini- mal problems have small degrees that might be practical in image matching and 3D reconstruction.",
        "中文标题": "PLMP - 完全多视图可见性中的点线最小问题",
        "摘要翻译": "我们提出了一个完整的分类，针对由校准的透视相机完全观察到的点和线的通用排列的所有最小问题。我们表明，总共有30个最小问题，对于超过6个相机、超过5个点和超过6条线的情况不存在问题。我们提出了一系列检测最小性的测试，从自由度计数开始，到代表性例子的完整符号和数值验证结束。对于发现的所有最小问题，我们展示了它们的代数度，即解的数量，这衡量了它们的内在难度。它展示了问题的难度如何随着视图数量的增加而增长。重要的是，几个新的最小问题具有较小的度数，可能在图像匹配和3D重建中具有实用性。",
        "领域": "几何视觉/3D重建/图像匹配",
        "问题": "分类和解决由校准的透视相机观察到的点和线的通用排列的最小问题",
        "动机": "为了理解和解决在几何视觉中，由校准的透视相机观察到的点和线的通用排列的最小问题，以及这些问题在图像匹配和3D重建中的潜在应用",
        "方法": "提出了一系列检测最小性的测试，包括自由度计数和代表性例子的完整符号和数值验证，并展示了所有最小问题的代数度",
        "关键词": [
            "最小问题",
            "代数度",
            "图像匹配",
            "3D重建"
        ],
        "涉及的技术概念": {
            "最小问题": "在几何视觉中，指那些在给定条件下，解的数量最少的问题，通常用于图像匹配和3D重建",
            "代数度": "指一个问题的解的数量，用于衡量问题的内在难度",
            "自由度计数": "一种用于确定系统或问题中独立变量数量的方法",
            "符号和数值验证": "通过数学符号和数值计算来验证问题解的正确性和完整性"
        }
    },
    {
        "order": 569,
        "title": "CapsuleVOS: Semi-Supervised Video Object Segmentation Using Capsule Routing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Duarte_CapsuleVOS_Semi-Supervised_Video_Object_Segmentation_Using_Capsule_Routing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Duarte_CapsuleVOS_Semi-Supervised_Video_Object_Segmentation_Using_Capsule_Routing_ICCV_2019_paper.html",
        "abstract": "In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.",
        "中文标题": "CapsuleVOS: 使用胶囊路由的半监督视频对象分割",
        "摘要翻译": "在本工作中，我们提出了一种基于胶囊的方法用于半监督视频对象分割。当前的视频对象分割方法是基于帧的，并且通常需要光流来捕捉帧间的时间一致性，这可能是难以计算的。为此，我们提出了一种基于视频的胶囊网络，CapsuleVOS，它可以根据参考帧和分割掩码一次分割多个帧。这种条件化是通过一种新颖的路由算法实现的，用于基于注意力的高效胶囊选择。我们解决了视频对象分割中的两个挑战性问题：1) 小对象的分割和2) 对象随时间推移的遮挡。分割小对象的问题通过一个缩放模块解决，该模块允许网络处理视频的小空间区域。除此之外，该框架利用了一种基于循环网络的新型记忆模块，有助于在对象移出帧或被遮挡时跟踪对象。该网络是端到端训练的，我们在两个基准视频对象分割数据集上展示了其有效性；它在Youtube-VOS数据集上优于当前的离线方法，同时运行速度几乎是竞争方法的两倍。代码公开在https://github.com/KevinDuarte/CapsuleVOS。",
        "领域": "视频对象分割/胶囊网络/半监督学习",
        "问题": "视频对象分割中的小对象分割和对象遮挡问题",
        "动机": "当前基于帧的视频对象分割方法需要光流来捕捉帧间的时间一致性，这难以计算，因此需要一种更高效的方法来处理视频对象分割。",
        "方法": "提出了一种基于视频的胶囊网络CapsuleVOS，通过一种新颖的路由算法实现基于注意力的高效胶囊选择，并利用缩放模块和基于循环网络的记忆模块来解决小对象分割和对象遮挡问题。",
        "关键词": [
            "胶囊网络",
            "半监督学习",
            "视频对象分割",
            "缩放模块",
            "记忆模块"
        ],
        "涉及的技术概念": "胶囊网络是一种新型的神经网络结构，能够更好地捕捉对象的空间层次关系。半监督学习是一种利用少量标注数据和大量未标注数据进行学习的方法。视频对象分割是指在视频序列中分割出感兴趣的对象。缩放模块允许网络处理视频的小空间区域，而记忆模块基于循环网络，有助于在对象移出帧或被遮挡时跟踪对象。"
    },
    {
        "order": 570,
        "title": "Transductive Learning for Zero-Shot Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Transductive_Learning_for_Zero-Shot_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rahman_Transductive_Learning_for_Zero-Shot_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Zero-shot object detection (ZSD) is a relatively unexplored research problem as compared to the conventional zero-shot recognition task. ZSD aims to detect previously unseen objects during inference. Existing ZSD works suffer from two critical issues: (a) large domain-shift between the source (seen) and target (unseen) domains since the two distributions are highly mismatched. (b) the learned model is biased against unseen classes, therefore in generalized ZSD settings, where both seen and unseen objects co-occur during inference, the learned model tends to misclassify unseen to seen categories. This brings up an important question: How effectively can a transductive setting address the aforementioned problems? To the best of our knowledge, we are the first to propose a transductive zero-shot object detection approach that convincingly reduces the domain-shift and model-bias against unseen classes. Our approach is based on a self-learning mechanism that uses a novel hybrid pseudo-labeling technique. It progressively updates learned model parameters by associating unlabeled data samples to their corresponding classes. During this process, our technique makes sure that knowledge that was previously acquired on the source domain is not forgotten. We report significant 'relative' improvements of 34.9% and 77.1% in terms of mAP and recall rates over the previous best inductive models on MSCOCO dataset.",
        "中文标题": "零样本目标检测的转导学习",
        "摘要翻译": "与传统的零样本识别任务相比，零样本目标检测（ZSD）是一个相对未被充分探索的研究问题。ZSD旨在在推理过程中检测以前未见过的对象。现有的ZSD工作存在两个关键问题：（a）源（已见）和目标（未见）领域之间存在大的领域转移，因为这两个分布高度不匹配。（b）学习到的模型对未见类别有偏见，因此在广义ZSD设置中，当推理过程中同时出现已见和未见对象时，学习到的模型倾向于将未见类别误分类为已见类别。这提出了一个重要问题：转导设置如何有效地解决上述问题？据我们所知，我们是第一个提出转导零样本目标检测方法的人，该方法显著减少了领域转移和对未见类别的模型偏见。我们的方法基于一种自我学习机制，该机制使用了一种新颖的混合伪标签技术。它通过将未标记的数据样本与其对应的类别关联起来，逐步更新学习到的模型参数。在此过程中，我们的技术确保不会忘记之前在源领域获得的知识。我们在MSCOCO数据集上报告了相对于之前最佳归纳模型在mAP和召回率方面显著的'相对'改进，分别为34.9%和77.1%。",
        "领域": "零样本学习/目标检测/转导学习",
        "问题": "解决零样本目标检测中的领域转移和模型偏见问题",
        "动机": "探索转导学习在减少零样本目标检测中领域转移和模型偏见方面的有效性",
        "方法": "提出了一种基于自我学习机制的转导零样本目标检测方法，采用新颖的混合伪标签技术逐步更新模型参数",
        "关键词": [
            "零样本学习",
            "目标检测",
            "转导学习",
            "伪标签技术",
            "自我学习机制"
        ],
        "涉及的技术概念": "零样本目标检测（ZSD）是一种旨在检测推理过程中未见对象的技术。转导学习是一种学习方法，它利用未标记数据来改进模型性能。伪标签技术是一种通过为未标记数据分配标签来训练模型的方法。自我学习机制指的是模型通过迭代过程自我改进的能力。"
    },
    {
        "order": 571,
        "title": "Variational Few-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Variational_Few-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "We propose a variational Bayesian framework for enhancing few-shot learning performance. This idea is motivated by the fact that single point based metric learning approaches are inherently noise-vulnerable and easy-to-be-biased. In a nutshell, stochastic variational inference is invoked to approximate bias-eliminated class specific sample distributions. In the meantime, a classifier-free prediction is attained by leveraging the distribution statistics on novel samples. Extensive experimental results on several benchmarks well demonstrate the effectiveness of our distribution-driven few-shot learning framework over previous point estimates based methods, in terms of superior classification accuracy and robustness.",
        "中文标题": "变分少样本学习",
        "摘要翻译": "我们提出了一个变分贝叶斯框架，用于增强少样本学习的性能。这一想法的动机是基于单点的度量学习方法本质上容易受到噪声的影响且容易产生偏差。简而言之，我们调用随机变分推断来近似消除偏差的类别特定样本分布。同时，通过利用新样本上的分布统计，实现了无分类器的预测。在多个基准上的广泛实验结果很好地证明了我们的分布驱动的少样本学习框架在分类准确性和鲁棒性方面优于之前基于点估计的方法。",
        "领域": "少样本学习/变分推断/贝叶斯学习",
        "问题": "提高少样本学习的性能，解决单点度量学习方法易受噪声影响和偏差的问题",
        "动机": "单点基于度量的学习方法容易受到噪声的影响且容易产生偏差，需要一种更鲁棒的方法来提高少样本学习的性能",
        "方法": "采用变分贝叶斯框架，通过随机变分推断近似消除偏差的类别特定样本分布，并利用新样本上的分布统计实现无分类器的预测",
        "关键词": [
            "少样本学习",
            "变分推断",
            "贝叶斯学习",
            "分类准确性",
            "鲁棒性"
        ],
        "涉及的技术概念": "变分贝叶斯框架是一种统计方法，用于近似复杂的概率分布。少样本学习是指在只有少量样本的情况下进行学习。随机变分推断是一种用于近似复杂后验分布的技术。无分类器预测是指不直接使用分类器，而是通过其他统计方法进行预测。"
    },
    {
        "order": 572,
        "title": "BAE-NET: Branched Autoencoder for Shape Co-Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_BAE-NET_Branched_Autoencoder_for_Shape_Co-Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_BAE-NET_Branched_Autoencoder_for_Shape_Co-Segmentation_ICCV_2019_paper.html",
        "abstract": "We treat shape co-segmentation as a representation learning problem and introduce BAE-NET, a branched autoencoder network, for the task. The unsupervised BAE-NET is trained with a collection of un-segmented shapes, using a shape reconstruction loss, without any ground-truth labels. Specifically, the network takes an input shape and encodes it using a convolutional neural network, whereas the decoder concatenates the resulting feature code with a point coordinate and outputs a value indicating whether the point is inside/outside the shape. Importantly, the decoder is branched: each branch learns a compact representation for one commonly recurring part of the shape collection, e.g., airplane wings. By complementing the shape reconstruction loss with a label loss, BAE-NET is easily tuned for one-shot learning. We show unsupervised, weakly supervised, and one-shot learning results by BAE-NET, demonstrating that using only a couple of exemplars, our network can generally outperform state-of-the-art supervised methods trained on hundreds of segmented shapes. Code is available at https://github.com/czq142857/BAE-NET.",
        "中文标题": "BAE-NET: 用于形状共分割的分支自编码器",
        "摘要翻译": "我们将形状共分割视为一个表示学习问题，并引入了BAE-NET，一个分支自编码器网络，用于此任务。无监督的BAE-NET通过使用形状重建损失，对未分割的形状集合进行训练，无需任何地面真实标签。具体来说，网络通过卷积神经网络对输入形状进行编码，而解码器将生成的特征代码与点坐标连接，并输出一个值，指示该点是否在形状内部/外部。重要的是，解码器是分支的：每个分支学习形状集合中一个常见重复部分的紧凑表示，例如飞机机翼。通过用标签损失补充形状重建损失，BAE-NET易于调整为一击学习。我们展示了BAE-NET的无监督、弱监督和一击学习结果，证明仅使用几个示例，我们的网络通常可以超越在数百个分割形状上训练的最先进的监督方法。代码可在https://github.com/czq142857/BAE-NET获取。",
        "领域": "形状分析/自编码器/无监督学习",
        "问题": "形状共分割",
        "动机": "解决形状共分割问题，通过无监督学习减少对大量标注数据的依赖",
        "方法": "引入分支自编码器网络BAE-NET，通过形状重建损失和标签损失进行训练，实现无监督、弱监督和一击学习",
        "关键词": [
            "形状共分割",
            "分支自编码器",
            "无监督学习",
            "一击学习"
        ],
        "涉及的技术概念": "卷积神经网络用于形状编码，解码器通过连接特征代码和点坐标来判断点的位置，分支解码器学习形状集合中常见部分的紧凑表示"
    },
    {
        "order": 573,
        "title": "Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Self-Training_and_Adversarial_Background_Regularization_for_Unsupervised_Domain_Adaptive_One-Stage_ICCV_2019_paper.html",
        "abstract": "Deep learning-based object detectors have shown remarkable improvements. However, supervised learning-based methods perform poorly when the train data and the test data have different distributions. To address the issue, domain adaptation transfers knowledge from the label-sufficient domain (source domain) to the label-scarce domain (target domain). Self-training is one of the powerful ways to achieve domain adaptation since it helps class-wise domain adaptation. Unfortunately, a naive approach that utilizes pseudo-labels as ground-truth degenerates the performance due to incorrect pseudo-labels. In this paper, we introduce a weak self-training (WST) method and adversarial background score regularization (BSR) for domain adaptive one-stage object detection. WST diminishes the adverse effects of inaccurate pseudo-labels to stabilize the learning procedure. BSR helps the network extract discriminative features for target backgrounds to reduce the domain shift. Two components are complementary to each other as BSR enhances discrimination between foregrounds and backgrounds, whereas WST strengthen class-wise discrimination. Experimental results show that our approach effectively improves the performance of the one-stage object detection in unsupervised domain adaptation setting.",
        "中文标题": "自训练和对抗性背景正则化用于无监督域自适应单阶段目标检测",
        "摘要翻译": "基于深度学习的目标检测器已经显示出显著的改进。然而，当训练数据和测试数据具有不同分布时，基于监督学习的方法表现不佳。为了解决这个问题，域适应将知识从标签充足的域（源域）转移到标签稀缺的域（目标域）。自训练是实现域适应的一种强大方法，因为它有助于类级别的域适应。不幸的是，利用伪标签作为真实标签的朴素方法由于错误的伪标签而降低了性能。在本文中，我们引入了一种弱自训练（WST）方法和对抗性背景分数正则化（BSR）用于域自适应单阶段目标检测。WST减少了不准确伪标签的负面影响，以稳定学习过程。BSR帮助网络提取目标背景的判别特征，以减少域偏移。这两个组件相互补充，因为BSR增强了前景和背景之间的区分，而WST加强了类级别的区分。实验结果表明，我们的方法有效地提高了无监督域适应设置下单阶段目标检测的性能。",
        "领域": "目标检测/域适应/自训练",
        "问题": "解决在训练数据和测试数据分布不同的情况下，基于监督学习的目标检测方法表现不佳的问题。",
        "动机": "通过域适应技术，将知识从标签充足的源域转移到标签稀缺的目标域，以提高目标检测器在目标域上的性能。",
        "方法": "引入弱自训练（WST）方法和对抗性背景分数正则化（BSR），WST减少不准确伪标签的负面影响，BSR帮助网络提取目标背景的判别特征，两者相互补充以提高性能。",
        "关键词": [
            "目标检测",
            "域适应",
            "自训练",
            "对抗性学习",
            "背景正则化"
        ],
        "涉及的技术概念": {
            "自训练": "一种利用模型自身预测结果作为伪标签来训练模型的方法，用于域适应。",
            "对抗性背景分数正则化": "一种正则化技术，通过对抗性学习来增强模型对目标背景的判别能力，以减少域偏移。",
            "域适应": "一种技术，旨在将知识从源域转移到目标域，以解决源域和目标域数据分布不同的问题。"
        }
    },
    {
        "order": 574,
        "title": "Generative Adversarial Minority Oversampling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mullick_Generative_Adversarial_Minority_Oversampling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mullick_Generative_Adversarial_Minority_Oversampling_ICCV_2019_paper.html",
        "abstract": "Class imbalance is a long-standing problem relevant to a number of real-world applications of deep learning. Oversampling techniques, which are effective for handling class imbalance in classical learning systems, can not be directly applied to end-to-end deep learning systems. We propose a three-player adversarial game between a convex generator, a multi-class classifier network, and a real/fake discriminator to perform oversampling in deep learning systems. The convex generator generates new samples from the minority classes as convex combinations of existing instances, aiming to fool both the discriminator as well as the classifier into misclassifying the generated samples. Consequently, the artificial samples are generated at critical locations near the peripheries of the classes. This, in turn, adjusts the classifier induced boundaries in a way which is more likely to reduce misclassification from the minority classes. Extensive experiments on multiple class imbalanced image datasets establish the efficacy of our proposal.",
        "中文标题": "生成对抗少数类过采样",
        "摘要翻译": "类别不平衡是一个长期存在的问题，与深度学习的多个现实世界应用相关。过采样技术对于处理经典学习系统中的类别不平衡是有效的，但不能直接应用于端到端的深度学习系统。我们提出了一个三玩家对抗游戏，包括一个凸生成器、一个多类分类器网络和一个真实/伪造判别器，以在深度学习系统中执行过采样。凸生成器从少数类中生成新样本，作为现有实例的凸组合，旨在欺骗判别器和分类器，使其误分类生成的样本。因此，人工样本在类边缘的关键位置生成。这反过来调整了分类器诱导的边界，从而更有可能减少来自少数类的误分类。在多个类别不平衡的图像数据集上的广泛实验证实了我们提议的有效性。",
        "领域": "类别不平衡处理/深度学习/图像分类",
        "问题": "深度学习系统中类别不平衡问题",
        "动机": "解决深度学习系统中类别不平衡问题，提高少数类的分类准确率",
        "方法": "提出一个三玩家对抗游戏，包括凸生成器、多类分类器网络和真实/伪造判别器，通过生成少数类的新样本来调整分类器诱导的边界",
        "关键词": [
            "类别不平衡",
            "过采样",
            "对抗游戏",
            "凸生成器",
            "多类分类器",
            "真实/伪造判别器"
        ],
        "涉及的技术概念": {
            "类别不平衡": "指数据集中各类别样本数量差异较大的情况，影响模型对少数类的学习效果。",
            "过采样": "一种处理类别不平衡的技术，通过增加少数类样本的数量来平衡数据集。",
            "对抗游戏": "一种博弈论概念，在此指生成器和判别器之间的对抗过程，旨在通过竞争提高生成样本的质量。",
            "凸生成器": "一种生成模型，通过现有实例的凸组合生成新样本。",
            "多类分类器": "能够处理多个类别的分类器，用于对样本进行分类。",
            "真实/伪造判别器": "用于区分真实样本和生成样本的模型，是生成对抗网络中的关键组成部分。"
        }
    },
    {
        "order": 575,
        "title": "VV-Net: Voxel VAE Net With Group Convolutions for Point Cloud Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Meng_VV-Net_Voxel_VAE_Net_With_Group_Convolutions_for_Point_Cloud_ICCV_2019_paper.html",
        "abstract": "We present a novel algorithm for point cloud segmentation.Our approach transforms unstructured point clouds into regular voxel grids, and further uses a kernel-based interpolated variational autoencoder (VAE) architecture to encode the local geometry within each voxel.Traditionally, the voxel representation only comprises Boolean occupancy information, which fails to capture the sparsely distributed points within voxels in a compact manner. In order to handle sparse distributions of points, we further employ radial basis functions (RBF) to compute a local, continuous representation within each voxel. Our approach results in a good volumetric representation that effectively tackles noisy point cloud datasets and is more robust for learning. Moreover, we further introduce group equivariant CNN to 3D, by defining the convolution operator on a symmetry group acting on  Z ^3 and its isomorphic sets. This improves the expressive capacity without increasing parameters, leading to more robust segmentation results.We highlight the performance on standard benchmarks and show that our approach outperforms state-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets.",
        "中文标题": "VV-Net: 使用组卷积的点云分割体素VAE网络",
        "摘要翻译": "我们提出了一种新颖的点云分割算法。我们的方法将非结构化的点云转换为规则的体素网格，并进一步使用基于核的插值变分自编码器（VAE）架构来编码每个体素内的局部几何形状。传统上，体素表示仅包含布尔占用信息，这无法以紧凑的方式捕捉体素内稀疏分布的点。为了处理点的稀疏分布，我们进一步采用径向基函数（RBF）来计算每个体素内的局部连续表示。我们的方法产生了一个良好的体积表示，有效处理了噪声点云数据集，并且在学习上更加稳健。此外，我们通过定义在作用于Z^3及其同构集上的对称群上的卷积算子，将群等变CNN引入3D。这在不增加参数的情况下提高了表达能力，导致更稳健的分割结果。我们强调了在标准基准上的表现，并展示了我们的方法在ShapeNet和S3DIS数据集上优于最先进的分割算法。",
        "领域": "点云处理/三维重建/几何深度学习",
        "问题": "点云分割",
        "动机": "传统体素表示无法有效捕捉稀疏分布的点，需要一种更紧凑且能处理噪声的方法",
        "方法": "将点云转换为体素网格，使用基于核的插值变分自编码器编码局部几何形状，采用径向基函数处理稀疏分布，引入群等变CNN提高表达能力",
        "关键词": [
            "点云分割",
            "体素网格",
            "变分自编码器",
            "径向基函数",
            "群等变CNN"
        ],
        "涉及的技术概念": {
            "点云分割": "将点云数据分割成不同的部分或对象",
            "体素网格": "将空间划分为规则的立方体单元，用于表示三维数据",
            "变分自编码器": "一种生成模型，用于学习数据的潜在表示",
            "径向基函数": "一种用于插值和逼近的函数，能够处理稀疏数据",
            "群等变CNN": "一种卷积神经网络，其卷积操作在特定的对称群上定义，以提高模型的表达能力"
        }
    },
    {
        "order": 576,
        "title": "Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Memorizing_Normality_to_Detect_Anomaly_Memory-Augmented_Deep_Autoencoder_for_Unsupervised_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gong_Memorizing_Normality_to_Detect_Anomaly_Memory-Augmented_Deep_Autoencoder_for_Unsupervised_ICCV_2019_paper.html",
        "abstract": "Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder \"generalizes\" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.",
        "中文标题": "记忆正常以检测异常：用于无监督异常检测的记忆增强深度自编码器",
        "摘要翻译": "深度自编码器已被广泛用于异常检测。在正常数据上训练的自编码器预期会对异常输入产生比正常输入更高的重建误差，这被用作识别异常的标准。然而，这一假设在实践中并不总是成立。已经观察到，有时自编码器“泛化”得非常好，以至于它也能很好地重建异常，导致异常检测的遗漏。为了减轻基于自编码器的异常检测器的这一缺点，我们提出通过增加一个记忆模块来增强自编码器，并开发了一种改进的自编码器，称为记忆增强自编码器，即MemAE。给定一个输入，MemAE首先从编码器获得编码，然后将其用作查询以检索最相关的记忆项进行重建。在训练阶段，记忆内容被更新，并被鼓励代表正常数据的原型元素。在测试阶段，学习到的记忆将被固定，重建是从正常数据的几个选定记忆记录中获得的。因此，重建将倾向于接近正常样本。因此，异常上的重建误差将被加强以进行异常检测。MemAE对数据类型没有假设，因此可以普遍应用于不同的任务。在各种数据集上的实验证明了所提出的MemAE的出色泛化能力和高有效性。",
        "领域": "异常检测/自编码器/记忆网络",
        "问题": "自编码器在异常检测中可能过度泛化，导致异常检测的遗漏",
        "动机": "解决自编码器在异常检测中可能过度泛化的问题，提高异常检测的准确性",
        "方法": "通过增加一个记忆模块来增强自编码器，开发记忆增强自编码器（MemAE），在训练阶段更新记忆内容以代表正常数据的原型元素，在测试阶段固定记忆并从正常数据的记忆记录中进行重建",
        "关键词": [
            "异常检测",
            "自编码器",
            "记忆网络"
        ],
        "涉及的技术概念": "深度自编码器是一种用于异常检测的技术，通过重建误差来识别异常。记忆增强自编码器（MemAE）通过增加记忆模块来改进自编码器，使其在训练阶段能够更新记忆内容以代表正常数据的原型元素，在测试阶段则通过固定记忆和从正常数据的记忆记录中进行重建来加强异常检测。"
    },
    {
        "order": 577,
        "title": "Memory-Based Neighbourhood Embedding for Visual Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Memory-Based_Neighbourhood_Embedding_for_Visual_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Memory-Based_Neighbourhood_Embedding_for_Visual_Recognition_ICCV_2019_paper.html",
        "abstract": "Learning discriminative image feature embeddings is of great importance to visual recognition. To achieve better feature embeddings, most current methods focus on designing different network structures or loss functions, and the estimated feature embeddings are usually only related to the input images. In this paper, we propose Memory-based Neighbourhood Embedding (MNE) to enhance a general CNN feature by considering its neighbourhood. The method aims to solve two critical problems, i.e., how to acquire more relevant neighbours in the network training and how to aggregate the neighbourhood information for a more discriminative embedding. We first augment an episodic memory module into the network, which can provide more relevant neighbours for both training and testing. Then the neighbours are organized in a tree graph with the target instance as the root node. The neighbourhood information is gradually aggregated to the root node in a bottom-up manner, and aggregation weights are supervised by the class relationships between the nodes. We apply MNE on image search and few shot learning tasks. Extensive ablation studies demonstrate the effectiveness of each component, and our method significantly outperforms the state-of-the-art approaches.",
        "中文标题": "基于记忆的邻域嵌入用于视觉识别",
        "摘要翻译": "学习具有区分性的图像特征嵌入对于视觉识别极为重要。为了获得更好的特征嵌入，当前大多数方法集中于设计不同的网络结构或损失函数，而估计的特征嵌入通常仅与输入图像相关。在本文中，我们提出了基于记忆的邻域嵌入（MNE），通过考虑其邻域来增强一般的CNN特征。该方法旨在解决两个关键问题，即如何在网络训练中获得更多相关的邻居，以及如何聚合邻域信息以获得更具区分性的嵌入。我们首先在网络中增加了一个情景记忆模块，该模块可以为训练和测试提供更多相关的邻居。然后，邻居被组织成一个以目标实例为根节点的树图。邻域信息以自底向上的方式逐渐聚合到根节点，聚合权重由节点之间的类别关系监督。我们将MNE应用于图像搜索和少样本学习任务。大量的消融研究证明了每个组件的有效性，我们的方法显著优于最先进的方法。",
        "领域": "图像搜索/少样本学习/特征嵌入",
        "问题": "如何获得更多相关的邻居以及如何聚合邻域信息以获得更具区分性的嵌入",
        "动机": "提高视觉识别中图像特征嵌入的区分性",
        "方法": "提出基于记忆的邻域嵌入（MNE），通过增加情景记忆模块和组织邻居为树图结构，自底向上聚合邻域信息，并由节点间的类别关系监督聚合权重",
        "关键词": [
            "特征嵌入",
            "邻域信息",
            "情景记忆模块",
            "树图结构",
            "自底向上聚合"
        ],
        "涉及的技术概念": "基于记忆的邻域嵌入（MNE）是一种增强CNN特征的方法，通过考虑特征邻域来提高特征嵌入的区分性。该方法包括增加一个情景记忆模块来提供更多相关的邻居，并将这些邻居组织成一个树图结构，其中目标实例作为根节点。邻域信息通过自底向上的方式聚合到根节点，聚合权重由节点之间的类别关系监督。这种方法被应用于图像搜索和少样本学习任务，通过消融研究验证了其有效性。"
    },
    {
        "order": 578,
        "title": "Self-Similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Self-Similarity_Grouping_A_Simple_Unsupervised_Cross_Domain_Adaptation_Approach_for_ICCV_2019_paper.html",
        "abstract": "Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC-Market1501) and 4.4% (Market1501-DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .",
        "中文标题": "自相似性分组：一种简单的无监督跨域适应方法用于行人重识别",
        "摘要翻译": "行人重识别（re-ID）中的域适应一直是一个具有挑战性的任务。在这项工作中，我们探索了如何利用目标域样本中存在的相似自然特征进行学习，以无监督的方式进行行人重识别。具体来说，我们提出了一种自相似性分组（SSG）方法，该方法利用未标记样本的潜在相似性（从全局身体到局部部分）自动从不同视角构建多个聚类。然后，这些独立的聚类被分配标签，作为伪身份来监督训练过程。我们反复交替进行这种分组和训练过程，直到模型稳定。尽管方法看似简单，我们的SSG在mAP上分别比最先进的方法高出4.6%（DukeMTMC-Market1501）和4.4%（Market1501-DukeMTMC）。在我们的SSG基础上，我们进一步引入了一种名为SSG++的聚类引导的半监督方法，以在开放集设置（即目标域中独立身份的数量未知）下进行一次性的域适应。在不花费太多精力进行标注的情况下，我们的SSG++可以分别在SSG的基础上进一步提高mAP 10.7%和6.9%。我们的代码可在https://github.com/OasisYang/SSG获取。",
        "领域": "行人重识别/无监督学习/域适应",
        "问题": "如何在无监督的情况下利用目标域样本中的相似自然特征进行行人重识别",
        "动机": "解决行人重识别中域适应的挑战，特别是在无监督学习环境下",
        "方法": "提出自相似性分组（SSG）方法，利用未标记样本的潜在相似性自动构建多个聚类，并通过伪身份监督训练过程",
        "关键词": [
            "行人重识别",
            "无监督学习",
            "域适应",
            "自相似性分组",
            "聚类"
        ],
        "涉及的技术概念": {
            "自相似性分组（SSG）": "一种利用未标记样本的潜在相似性自动构建多个聚类的方法",
            "伪身份": "通过聚类分配的标签，用于监督训练过程",
            "mAP": "平均精度均值，用于评估模型性能的指标",
            "开放集设置": "目标域中独立身份的数量未知的设置",
            "聚类引导的半监督方法": "在SSG基础上引入的方法，用于在开放集设置下进行一次性的域适应"
        }
    },
    {
        "order": 579,
        "title": "Topological Map Extraction From Overhead Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Topological_Map_Extraction_From_Overhead_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Topological_Map_Extraction_From_Overhead_Images_ICCV_2019_paper.html",
        "abstract": "We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.",
        "中文标题": "从俯视图像中提取拓扑地图",
        "摘要翻译": "我们提出了一种名为PolyMapper的新方法，以规避传统的（航空）图像像素级分割，并直接以矢量表示预测对象。PolyMapper直接从俯视图像中提取城市的拓扑地图，作为建筑足迹和道路网络的集合。为了统一不同类型对象的形状表示，我们还提出了一种新颖的顺序化方法，将图结构重新表述为闭合多边形。实验在现有和自收集的几个城市的大规模数据集上进行。我们的实证结果表明，我们的端到端可学习模型能够以完全自动化的方式绘制建筑足迹和道路网络的多边形，这些多边形非常接近现有在线地图服务的结构。与最先进技术的定量和定性比较也显示，我们的方法达到了良好的性能水平。据我们所知，大规模拓扑地图的自动提取是遥感社区中的一项新颖贡献，我们相信这将有助于开发具有更多几何约束的模型。",
        "领域": "遥感图像分析/地理信息系统/城市建模",
        "问题": "如何从俯视图像中自动提取城市的拓扑地图，包括建筑足迹和道路网络",
        "动机": "开发一种能够直接从俯视图像中提取城市拓扑地图的方法，以支持更精确的城市建模和规划",
        "方法": "提出PolyMapper方法，通过顺序化方法将图结构重新表述为闭合多边形，直接从俯视图像中提取建筑足迹和道路网络的矢量表示",
        "关键词": [
            "拓扑地图",
            "建筑足迹",
            "道路网络",
            "矢量表示",
            "顺序化方法"
        ],
        "涉及的技术概念": "PolyMapper是一种直接从俯视图像中提取城市拓扑地图的方法，通过将图结构重新表述为闭合多边形来统一不同类型对象的形状表示。这种方法支持端到端学习，能够自动绘制建筑足迹和道路网络的多边形，接近现有在线地图服务的结构。"
    },
    {
        "order": 580,
        "title": "Miss Detection vs. False Alarm: Adversarial Learning for Small Object Segmentation in Infrared Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Miss_Detection_vs._False_Alarm_Adversarial_Learning_for_Small_Object_ICCV_2019_paper.html",
        "abstract": "A key challenge of infrared small object segmentation (ISOS) is to balance miss detection (MD) and false alarm (FA). This usually needs \"opposite\" strategies to suppress the two terms, and has not been well resolved in the literature. In this paper, we propose a deep adversarial learning framework to improve this situation. Departing from the tradition of jointly reducing MD and FA via a single objective, we decompose this difficult task into two sub-tasks handled by two models trained adversarially, with each focusing on reducing either MD or FA. Such a new design brings forth at least three advantages. First, as each model focuses on a relatively simpler sub-task, the overall difficulty of ISOS is somehow decreased. Second, the adversarial training of the two models naturally produces a delicate balance of MD and FA, and low rates for both MD and FA could be achieved at Nash equilibrium. Third, this MD-FA detachment gives us more flexibility to develop specific models dedicated to each sub-task. To realize the above design, we propose a conditional Generative Adversarial Network comprising of two generators and one discriminator. Each generator strives for one sub-task, while the discriminator differentiates the three segmentation results from the two generators and the ground truth. Moreover, in order to better serve the sub-tasks, the two generators, based on context aggregation networks, utilzse different size of receptive fields, providing both local and global views of objects for segmentation. As verified on multiple infrared image data sets, our method consistently achieves better segmentation than many state-of-the-art ISOS methods.",
        "中文标题": "漏检与误报：红外图像中小目标分割的对抗学习",
        "摘要翻译": "红外小目标分割（ISOS）的一个关键挑战是平衡漏检（MD）和误报（FA）。这通常需要“相反”的策略来抑制这两个因素，而在文献中尚未得到很好的解决。在本文中，我们提出了一个深度对抗学习框架来改善这一状况。与通过单一目标共同减少MD和FA的传统方法不同，我们将这一困难任务分解为两个子任务，由两个对抗训练的模型处理，每个模型专注于减少MD或FA。这种新设计至少带来了三个优势。首先，由于每个模型都专注于一个相对简单的子任务，ISOS的整体难度有所降低。其次，两个模型的对抗训练自然地产生了MD和FA的微妙平衡，在纳什均衡下可以实现较低的MD和FA率。第三，这种MD-FA分离为我们提供了更多灵活性，以开发专门针对每个子任务的特定模型。为了实现上述设计，我们提出了一个包含两个生成器和一个判别器的条件生成对抗网络。每个生成器致力于一个子任务，而判别器则区分来自两个生成器和地面实况的三个分割结果。此外，为了更好地服务于子任务，基于上下文聚合网络的两个生成器利用不同大小的感受野，为分割提供对象的局部和全局视图。正如在多个红外图像数据集上验证的那样，我们的方法始终比许多最先进的ISOS方法实现更好的分割。",
        "领域": "红外图像处理/对抗学习/目标分割",
        "问题": "平衡红外小目标分割中的漏检和误报",
        "动机": "解决红外小目标分割中漏检和误报平衡的挑战，提高分割准确性",
        "方法": "提出一个深度对抗学习框架，将任务分解为两个子任务，由两个对抗训练的模型处理，每个模型专注于减少漏检或误报，并采用条件生成对抗网络实现",
        "关键词": [
            "红外小目标分割",
            "对抗学习",
            "条件生成对抗网络"
        ],
        "涉及的技术概念": "红外小目标分割（ISOS）是指在红外图像中识别和分割出小目标的过程。漏检（MD）指的是未能检测到实际存在的小目标，而误报（FA）则是指错误地将非目标区域识别为目标。对抗学习是一种机器学习方法，通过让两个模型相互对抗来提高性能。条件生成对抗网络（cGAN）是一种生成对抗网络的变体，它通过条件信息来指导生成过程，使得生成的结果更加符合预期。上下文聚合网络是一种用于图像处理的网络结构，它能够聚合不同尺度的上下文信息，以提高处理效果。"
    },
    {
        "order": 581,
        "title": "Exploiting Temporal Consistency for Real-Time Video Depth Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Exploiting_Temporal_Consistency_for_Real-Time_Video_Depth_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Exploiting_Temporal_Consistency_for_Real-Time_Video_Depth_Estimation_ICCV_2019_paper.html",
        "abstract": "Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at: https://tinyurl.com/STCLSTM",
        "中文标题": "利用时间一致性进行实时视频深度估计",
        "摘要翻译": "最近，通过利用深度卷积神经网络（CNNs）的层次特征，静态图像的深度估计精度得到了显著提高。与静态图像相比，视频帧之间存在大量信息，可以用来提高深度估计的性能。在这项工作中，我们专注于探索从单目视频中提取时间信息以进行深度估计。具体来说，我们利用了卷积长短期记忆（CLSTM）并提出了一个新颖的时空CSLTM（ST-CLSTM）结构。我们的ST-CLSTM结构不仅能够捕捉空间特征，还能捕捉连续视频帧之间的时间相关性/一致性，而计算成本几乎不增加。此外，为了保持估计深度帧之间的时间一致性，我们应用了生成对抗学习方案并设计了一个时间一致性损失。时间一致性损失与空间损失结合，以端到端的方式更新模型。通过利用时间信息，我们构建了一个实时运行的视频深度估计框架，并生成了视觉上令人愉悦的结果。此外，我们的方法灵活，可以推广到大多数现有的深度估计框架。代码可在https://tinyurl.com/STCLSTM获取。",
        "领域": "视频深度估计/时间序列分析/生成对抗网络",
        "问题": "提高视频深度估计的准确性和实时性",
        "动机": "利用视频帧间的时间信息来提高深度估计的性能",
        "方法": "提出了一种新颖的时空CSLTM（ST-CLSTM）结构，结合生成对抗学习方案和时间一致性损失，以端到端的方式更新模型",
        "关键词": [
            "视频深度估计",
            "时间一致性",
            "生成对抗网络",
            "卷积长短期记忆"
        ],
        "涉及的技术概念": "深度卷积神经网络（CNNs）、卷积长短期记忆（CLSTM）、时空CSLTM（ST-CLSTM）结构、生成对抗学习、时间一致性损失、空间损失、端到端学习"
    },
    {
        "order": 582,
        "title": "Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Most existing person re-identification(Re-ID) approaches achieve superior results based on the assumption that a large amount of pre-labelled data is usually available and can be put into training phrase all at once. However, this assumption is not applicable to most real-world deployment of the Re-ID task. In this work, we propose an alternative reinforcement learning based human-in-the-loop model which releases the restriction of pre-labelling and keeps model upgrading with progressively collected data. The goal is to minimize human annotation efforts while maximizing Re-ID performance. It works in an iteratively updating framework by refining the RL policy and CNN parameters alternately. In particular, we formulate a Deep Reinforcement Active Learning (DRAL) method to guide an agent (a model in a reinforcement learning process) in selecting training samples on-the-fly by a human user/annotator. The reinforcement learning reward is the uncertainty value of each human selected sample. A binary feedback (positive or negative) labelled by the human annotator is used to select the samples of which are used to fine-tune a pre-trained CNN Re-ID model. Extensive experiments demonstrate the superiority of our DRAL method for deep reinforcement learning based human-in-the-loop person Re-ID when compared to existing unsupervised and transfer learning models as well as active learning models.",
        "中文标题": "深度强化主动学习用于人在回路中的行人重识别",
        "摘要翻译": "大多数现有的行人重识别（Re-ID）方法基于一个假设，即通常有大量预先标记的数据可用，并且可以一次性投入训练阶段。然而，这一假设并不适用于大多数现实世界中的Re-ID任务部署。在这项工作中，我们提出了一种基于强化学习的人在回路模型，该模型释放了预先标记的限制，并通过逐步收集的数据保持模型升级。目标是最大限度地减少人类注释工作，同时最大化Re-ID性能。它通过交替优化RL策略和CNN参数，在一个迭代更新的框架中工作。特别是，我们制定了一种深度强化主动学习（DRAL）方法，以指导代理（强化学习过程中的模型）由人类用户/注释者即时选择训练样本。强化学习的奖励是每个人类选择样本的不确定性值。由人类注释者标记的二元反馈（正面或负面）用于选择用于微调预训练CNN Re-ID模型的样本。大量实验证明了我们的DRAL方法在基于深度强化学习的人在回路行人重识别中，与现有的无监督和迁移学习模型以及主动学习模型相比的优越性。",
        "领域": "行人重识别/强化学习/主动学习",
        "问题": "减少行人重识别任务中的人类注释工作，同时保持或提高识别性能",
        "动机": "现实世界中的行人重识别任务往往无法一次性获得大量预先标记的数据，需要一种方法能够在逐步收集数据的同时保持模型性能",
        "方法": "提出了一种深度强化主动学习（DRAL）方法，通过交替优化强化学习策略和CNN参数，指导模型即时选择训练样本，以减少人类注释工作并提高性能",
        "关键词": [
            "行人重识别",
            "强化学习",
            "主动学习",
            "人在回路",
            "CNN"
        ],
        "涉及的技术概念": "深度强化主动学习（DRAL）是一种结合了强化学习和主动学习的方法，用于指导模型在人类用户的帮助下即时选择训练样本。这种方法通过交替优化强化学习策略和CNN参数，以减少人类注释工作并提高行人重识别的性能。强化学习的奖励基于每个样本的不确定性值，而人类注释者提供的二元反馈则用于选择用于微调预训练CNN Re-ID模型的样本。"
    },
    {
        "order": 583,
        "title": "Group-Wise Deep Object Co-Segmentation With Co-Attention Recurrent Neural Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Group-Wise_Deep_Object_Co-Segmentation_With_Co-Attention_Recurrent_Neural_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Group-Wise_Deep_Object_Co-Segmentation_With_Co-Attention_Recurrent_Neural_Network_ICCV_2019_paper.html",
        "abstract": "Effective feature representations which should not only express the images individual properties, but also reflect the interaction among group images are essentially crucial for real-world co-segmentation. This paper proposes a novel end-to-end deep learning approach for group-wise object co-segmentation with a recurrent network architecture. Specifically, the semantic features extracted from a pre-trained CNN of each image are first processed by single image representation branch to learn the unique properties. Meanwhile, a specially designed Co-Attention Recurrent Unit (CARU) recurrently explores all images to generate the final group representation by using the co-attention between images, and simultaneously suppresses noisy information. The group feature which contains synergetic information is broadcasted to each individual image and fused with multi-scale fine-resolution features to facilitate the inferring of co-segmentation. Moreover, we propose a groupwise training objective to utilize the co-object similarity and figure-ground distinctness as the additional supervision. The whole modules are collaboratively optimized in an end-to-end manner, further improving the robustness of the approach. Comprehensive experiments on three benchmarks can demonstrate the superiority of our approach in comparison with the state-of-the-art methods.",
        "中文标题": "基于共注意力循环神经网络的群体深度对象共分割",
        "摘要翻译": "有效的特征表示不仅应表达图像的个体属性，还应反映群体图像之间的相互作用，这对于现实世界中的共分割至关重要。本文提出了一种新颖的端到端深度学习方法，用于群体对象共分割，采用循环网络架构。具体来说，首先通过单图像表示分支处理从预训练的CNN中提取的每个图像的语义特征，以学习独特属性。同时，特别设计的共注意力循环单元（CARU）通过图像之间的共注意力循环探索所有图像，生成最终的群体表示，并同时抑制噪声信息。包含协同信息的群体特征被广播到每个单独图像，并与多尺度精细分辨率特征融合，以促进共分割的推断。此外，我们提出了一种群体训练目标，利用共对象相似性和前景-背景区分度作为额外的监督。整个模块以端到端的方式协同优化，进一步提高了方法的鲁棒性。在三个基准上的综合实验证明了我们的方法相较于最先进方法的优越性。",
        "领域": "对象共分割/群体图像分析/深度学习",
        "问题": "如何有效地进行群体图像中的对象共分割",
        "动机": "为了在现实世界的共分割任务中，不仅表达图像的个体属性，还能反映群体图像之间的相互作用",
        "方法": "提出了一种端到端的深度学习方法，采用循环网络架构，包括单图像表示分支和特别设计的共注意力循环单元（CARU），以及群体训练目标",
        "关键词": [
            "对象共分割",
            "群体图像分析",
            "共注意力循环单元",
            "深度学习"
        ],
        "涉及的技术概念": "共注意力循环单元（CARU）是一种特别设计的循环网络架构，用于探索群体图像之间的共注意力，生成最终的群体表示，并抑制噪声信息。群体训练目标利用共对象相似性和前景-背景区分度作为额外的监督，以提高共分割的准确性。"
    },
    {
        "order": 584,
        "title": "A Dual-Path Model With Adaptive Attention for Vehicle Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Khorramshahi_A_Dual-Path_Model_With_Adaptive_Attention_for_Vehicle_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Khorramshahi_A_Dual-Path_Model_With_Adaptive_Attention_for_Vehicle_Re-Identification_ICCV_2019_paper.html",
        "abstract": "In recent years, attention models have been extensively used for person and vehicle re-identification. Most re-identification methods are designed to focus attention on key-point locations. However, depending on the orientation, the contribution of each key-point varies. In this paper, we present a novel dual-path adaptive attention model for vehicle re-identification (AAVER). The global appearance path captures macroscopic vehicle features while the orientation conditioned part appearance path learns to capture localized discriminative features by focusing attention on the most informative key-points. Through extensive experimentation, we show that the proposed AAVER method is able to accurately re-identify vehicles in unconstrained scenarios, yielding state of the art results on the challenging dataset VeRi-776. As a byproduct, the proposed system is also able to accurately predict vehicle key-points and shows an improvement of more than 7% over state of the art. The code for key-point estimation model is available at https://github.com/Pirazh/Vehicle_Key_ Point_Orientation_Estimation",
        "中文标题": "一种用于车辆重识别的自适应注意力双路径模型",
        "摘要翻译": "近年来，注意力模型已被广泛用于人和车辆的重识别。大多数重识别方法旨在将注意力集中在关键点位置。然而，根据方向的不同，每个关键点的贡献也不同。在本文中，我们提出了一种新颖的双路径自适应注意力模型用于车辆重识别（AAVER）。全局外观路径捕捉宏观车辆特征，而方向条件部分外观路径通过学习将注意力集中在最具信息量的关键点上，以捕捉局部区分性特征。通过大量实验，我们展示了所提出的AAVER方法能够在不受限制的场景中准确重识别车辆，在具有挑战性的数据集VeRi-776上取得了最先进的结果。作为副产品，所提出的系统还能够准确预测车辆关键点，并显示出比现有技术超过7%的改进。关键点估计模型的代码可在https://github.com/Pirazh/Vehicle_Key_Point_Orientation_Estimation获取。",
        "领域": "车辆重识别/注意力机制/关键点检测",
        "问题": "在车辆重识别中，如何根据车辆方向自适应地关注关键点以提高识别准确率",
        "动机": "现有的车辆重识别方法大多固定关注关键点位置，而忽略了不同方向下关键点贡献的差异，这限制了识别性能的提升",
        "方法": "提出了一种双路径自适应注意力模型，包括全局外观路径和方向条件部分外观路径，前者捕捉宏观特征，后者学习关注最具信息量的关键点以捕捉局部区分性特征",
        "关键词": [
            "车辆重识别",
            "自适应注意力",
            "关键点检测"
        ],
        "涉及的技术概念": "注意力模型用于捕捉关键点信息，双路径模型结合全局和局部特征，方向条件部分外观路径根据车辆方向调整注意力，以提高车辆重识别的准确率"
    },
    {
        "order": 585,
        "title": "The Sound of Motions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_The_Sound_of_Motions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_The_Sound_of_Motions_ICCV_2019_paper.html",
        "abstract": "Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.",
        "中文标题": "运动之声",
        "摘要翻译": "声音源自物体的运动和周围空气的振动。受到人类能够通过视觉上物体的运动来解释声源这一事实的启发，我们提出了一种新颖的系统，该系统明确捕捉这些运动线索，用于声音定位和分离任务。我们的系统由一个称为深度密集轨迹（DDT）的端到端可学习模型和一个课程学习方案组成。它利用了来自大量未标记视频的音频-视觉信号的固有一致性。定量和定性评估显示，与依赖视觉外观线索的先前模型相比，我们基于运动的系统在分离乐器声音方面提高了性能。此外，它还能从同一类别乐器的二重奏中分离声音成分，这是一个以前未曾解决的挑战性问题。",
        "领域": "音频-视觉学习/声音分离/运动分析",
        "问题": "声音定位和分离，特别是从同一类别乐器的二重奏中分离声音成分",
        "动机": "受到人类能够通过视觉上物体的运动来解释声源这一事实的启发，旨在通过捕捉运动线索来改进声音定位和分离的性能",
        "方法": "提出了一种新颖的系统，包括一个称为深度密集轨迹（DDT）的端到端可学习模型和一个课程学习方案，利用来自大量未标记视频的音频-视觉信号的固有一致性",
        "关键词": [
            "声音定位",
            "声音分离",
            "运动分析",
            "音频-视觉学习"
        ],
        "涉及的技术概念": "深度密集轨迹（DDT）是一种端到端可学习模型，用于捕捉运动线索；课程学习方案是一种逐步增加难度的学习方法，用于训练模型；音频-视觉信号的固有一致性指的是音频和视觉信息之间的自然关联，有助于提高模型的性能。"
    },
    {
        "order": 586,
        "title": "Human Attention in Image Captioning: Dataset and Analysis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.html",
        "abstract": "In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human atten- tion and language constructs during perception and sen- tence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image caption- ing. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Hu- mans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between de- scribed objects and attended objects (97% of the described objects are being attended), (3) a convolutional neural net- work as feature encoder accounts for human-attended re- gions during image captioning to a great extent (around 78%), (4) soft-attention mechanism differs from human at- tention, both spatially and temporally, and there is low correlation between caption scores and attention consis- tency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model's performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/ Human-Attention-in-Image-Captioning.",
        "中文标题": "图像描述中的人类注意力：数据集与分析",
        "摘要翻译": "在这项工作中，我们提出了一个新颖的数据集，该数据集包含同步记录的图像上的眼动和口头描述。利用这些数据，我们研究了自由观看和图像描述任务中人类注意力的差异。我们探讨了感知和句子表达过程中人类注意力与语言结构之间的关系。我们还分析了被认为模仿人类在描述任务中注意力的自上而下软注意力方法中的注意力部署机制，并调查了视觉显著性是否有助于图像描述。我们的研究揭示了：(1) 人类在自由观看和图像描述任务中的注意力行为不同。在后者任务中，人类倾向于注视更多样化的区域，(2) 描述的对象和注意的对象之间存在强烈的关系（97%的描述对象被注意到），(3) 卷积神经网络作为特征编码器在很大程度上解释了图像描述过程中人类注意到的区域（约78%），(4) 软注意力机制在空间和时间上都与人类注意力不同，且描述分数与注意力一致性分数之间的相关性较低。这些表明在自上而下注意力方面，人类与机器之间存在较大差距，(5) 通过将软注意力模型与图像显著性结合，我们可以显著提高模型在Flickr30k和MSCOCO基准测试上的性能。数据集可在以下网址找到：https://github.com/SenHe/Human-Attention-in-Image-Captioning。",
        "领域": "图像描述/注意力机制/视觉显著性",
        "问题": "研究人类在图像描述任务中的注意力行为与机器注意力机制的差异",
        "动机": "探索人类在图像描述任务中的注意力行为，以及如何改进机器在图像描述任务中的注意力机制",
        "方法": "通过分析眼动和口头描述数据，研究人类注意力行为，并比较软注意力机制与人类注意力的差异，最后通过结合图像显著性改进模型性能",
        "关键词": [
            "图像描述",
            "注意力机制",
            "视觉显著性",
            "卷积神经网络"
        ],
        "涉及的技术概念": {
            "眼动和口头描述数据": "用于研究人类在图像描述任务中的注意力行为的数据",
            "软注意力机制": "一种模仿人类在图像描述任务中注意力的机制",
            "视觉显著性": "图像中吸引人类注意力的区域",
            "卷积神经网络": "用于特征编码的深度学习模型"
        }
    },
    {
        "order": 587,
        "title": "Bayesian Loss for Crowd Count Estimation With Point Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Bayesian_Loss_for_Crowd_Count_Estimation_With_Point_Supervision_ICCV_2019_paper.html",
        "abstract": "In crowd counting datasets, each person is annotated by a point, which is usually the center of the head. And the task is to estimate the total count in a crowd scene. Most of the state-of-the-art methods are based on density map estimation, which convert the sparse point annotations into a \"ground truth\" density map through a Gaussian kernel, and then use it as the learning target to train a density map estimator. However, such a \"ground-truth\" density map is imperfect due to occlusions, perspective effects, variations in object shapes, etc. On the contrary, we propose Bayesian loss, a novel loss function which constructs a density contribution probability model from the point annotations. Instead of constraining the value at every pixel in the density map, the proposed training loss adopts a more reliable supervision on the count expectation at each annotated point. Without bells and whistles, the loss function makes substantial improvements over the baseline loss on all tested datasets. Moreover, our proposed loss function equipped with a standard backbone network, without using any external detectors or multi-scale architectures, plays favourably against the state of the arts. Our method outperforms previous best approaches by a large margin on the latest and largest UCF-QNRF dataset.",
        "中文标题": "贝叶斯损失用于点监督的人群计数估计",
        "摘要翻译": "在人群计数数据集中，每个人通常通过头部中心的一个点进行标注。任务是估计人群场景中的总人数。大多数最先进的方法基于密度图估计，通过高斯核将稀疏的点标注转换为“真实”密度图，然后将其作为学习目标来训练密度图估计器。然而，由于遮挡、透视效果、物体形状变化等原因，这种“真实”密度图并不完美。相反，我们提出了贝叶斯损失，这是一种新颖的损失函数，它从点标注构建密度贡献概率模型。与约束密度图中每个像素的值不同，所提出的训练损失在每个标注点上采用更可靠的计数期望监督。无需任何花哨的技巧，该损失函数在所有测试数据集上均比基线损失有显著改进。此外，我们提出的损失函数配备标准骨干网络，无需使用任何外部检测器或多尺度架构，就能与最先进的技术相媲美。我们的方法在最新且最大的UCF-QNRF数据集上大幅超越了之前的最佳方法。",
        "领域": "人群计数/密度图估计/损失函数设计",
        "问题": "解决在人群计数中由于遮挡、透视效果和物体形状变化导致的密度图不准确问题",
        "动机": "提高人群计数的准确性和鲁棒性，通过更可靠的监督方法改进密度图估计",
        "方法": "提出贝叶斯损失函数，从点标注构建密度贡献概率模型，采用计数期望作为监督信号",
        "关键词": [
            "人群计数",
            "密度图估计",
            "贝叶斯损失",
            "点监督",
            "UCF-QNRF数据集"
        ],
        "涉及的技术概念": "贝叶斯损失是一种新的损失函数，它通过构建密度贡献概率模型来改进人群计数的准确性。这种方法不直接约束密度图中每个像素的值，而是通过点标注提供更可靠的计数期望监督。这种方法在不需要复杂架构或外部检测器的情况下，就能显著提高人群计数的性能。"
    },
    {
        "order": 588,
        "title": "SC-FEGAN: Face Editing Generative Adversarial Network With User's Sketch and Color",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jo_SC-FEGAN_Face_Editing_Generative_Adversarial_Network_With_Users_Sketch_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jo_SC-FEGAN_Face_Editing_Generative_Adversarial_Network_With_Users_Sketch_and_ICCV_2019_paper.html",
        "abstract": "We present a novel image editing system that generates images as the user provides free-form masks, sketches and color as inputs. Our system consists of an end-to-end trainable convolutional network. In contrast to the existing methods, our system utilizes entirely free-form user input in terms of color and shape. This allows the system to respond to the user's sketch and color inputs, using them as guidelines to generate an image. In this work, we trained the network with an additional style loss, which made it possible to generate realistic results despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited for generating high-quality synthetic images using intuitive user inputs.",
        "中文标题": "SC-FEGAN: 基于用户草图和颜色的面部编辑生成对抗网络",
        "摘要翻译": "我们提出了一种新颖的图像编辑系统，该系统能够根据用户提供的自由形式的面具、草图和颜色作为输入生成图像。我们的系统由一个端到端可训练的卷积网络组成。与现有方法相比，我们的系统完全利用了用户在颜色和形状方面的自由形式输入。这使得系统能够响应用户的草图和颜色输入，将它们作为生成图像的指南。在这项工作中，我们通过额外的风格损失训练网络，这使得即使在图像的大部分被移除的情况下也能生成逼真的结果。我们提出的网络架构SC-FEGAN非常适合使用直观的用户输入生成高质量的合成图像。",
        "领域": "图像生成/面部编辑/生成对抗网络",
        "问题": "如何根据用户的自由形式输入（如草图、颜色）生成高质量的面部图像",
        "动机": "现有方法在处理用户自由形式输入时存在限制，需要一种能够更灵活响应用户输入并生成高质量图像的系统",
        "方法": "采用端到端可训练的卷积网络，结合额外的风格损失训练，以响应用户的草图和颜色输入，生成图像",
        "关键词": [
            "图像生成",
            "面部编辑",
            "生成对抗网络",
            "用户输入",
            "风格损失"
        ],
        "涉及的技术概念": "SC-FEGAN是一种生成对抗网络架构，专门设计用于处理用户提供的自由形式输入（如草图和颜色），以生成高质量的面部图像。通过引入风格损失，即使在图像的大部分被移除的情况下，也能生成逼真的结果。"
    },
    {
        "order": 589,
        "title": "Variational Uncalibrated Photometric Stereo Under General Lighting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Haefner_Variational_Uncalibrated_Photometric_Stereo_Under_General_Lighting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Haefner_Variational_Uncalibrated_Photometric_Stereo_Under_General_Lighting_ICCV_2019_paper.html",
        "abstract": "Photometric stereo (PS) techniques nowadays remain constrained to an ideal laboratory setup where modeling and calibration of lighting is amenable. To eliminate such restrictions, we propose an efficient principled variational approach to uncalibrated PS under general illumination. To this end, the Lambertian reflectance model is approximated through a spherical harmonic expansion, which preserves the spatial invariance of the lighting. The joint recovery of shape, reflectance and illumination is then formulated as a single variational problem. There the shape estimation is carried out directly in terms of the underlying perspective depth map, thus implicitly ensuring integrability and bypassing the need for a subsequent normal integration. To tackle the resulting nonconvex problem numerically, we undertake a two-phase procedure to initialize a balloon-like perspective depth map, followed by a \"lagged\" block coordinate descent scheme. The experiments validate efficiency and robustness of this approach. Across a variety of evaluations, we are able to reduce the mean angular error consistently by a factor of 2-3 compared to the state-of-the-art.",
        "中文标题": "一般光照下的无校准光度立体变分方法",
        "摘要翻译": "现今的光度立体（PS）技术仍然局限于理想的实验室设置，其中光照的建模和校准是可管理的。为了消除这些限制，我们提出了一种在一般光照下无校准PS的高效原则性变分方法。为此，通过球谐展开近似朗伯反射模型，这保留了光照的空间不变性。然后，将形状、反射率和光照的联合恢复表述为一个单一的变分问题。在那里，形状估计直接基于底层透视深度图进行，从而隐式地确保了可积性并绕过了后续法线积分的需要。为了数值上解决由此产生的非凸问题，我们采取了一个两阶段程序来初始化一个类似气球的透视深度图，随后采用“滞后”块坐标下降方案。实验验证了该方法的效率和鲁棒性。在各种评估中，我们能够将平均角度误差持续减少2-3倍，与最先进的技术相比。",
        "领域": "光度立体/三维重建/光照估计",
        "问题": "解决在一般光照条件下无校准光度立体技术中的形状、反射率和光照的联合恢复问题",
        "动机": "消除光度立体技术对理想实验室设置和光照校准的依赖，提高其在一般光照条件下的应用效率和鲁棒性",
        "方法": "提出了一种基于球谐展开的朗伯反射模型近似方法，将形状、反射率和光照的联合恢复问题表述为单一变分问题，并采用两阶段程序初始化透视深度图，随后使用“滞后”块坐标下降方案进行优化",
        "关键词": [
            "光度立体",
            "三维重建",
            "光照估计",
            "变分方法",
            "球谐展开"
        ],
        "涉及的技术概念": {
            "光度立体": "一种通过分析物体在不同光照条件下的图像来恢复其三维形状的技术",
            "球谐展开": "一种数学方法，用于近似复杂的光照模型，保留光照的空间不变性",
            "变分方法": "一种数学优化技术，用于求解形状、反射率和光照的联合恢复问题",
            "透视深度图": "一种表示物体表面到相机距离的二维图像，用于直接估计形状",
            "滞后块坐标下降": "一种优化算法，用于解决非凸问题，通过分阶段和分块的方式逐步优化解"
        }
    },
    {
        "order": 590,
        "title": "Learning Spatial Awareness to Improve Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Learning_Spatial_Awareness_to_Improve_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., L_2 loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by [??] to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.",
        "中文标题": "学习空间意识以改进人群计数",
        "摘要翻译": "人群计数的目标是通过利用行人头部中心位置的注释来估计图像中的人数。随着深度卷积神经网络的普及，已经取得了令人鼓舞的进展。现有方法广泛采用欧几里得距离（即L_2损失）来优化模型，然而，这种方法有两个主要缺点：（1）由于难以保留密度图中的高频变化，损失在学习空间意识（即头部位置）方面存在困难；（2）损失对人群计数中的各种噪声高度敏感，如零均值噪声、头部大小变化和遮挡。尽管之前提出了最大子数组超额（MESA）损失通过找到预测密度图与真实值差异最大的矩形子区域来解决上述问题，但它无法通过梯度下降解决，因此很难集成到深度学习框架中。在本文中，我们提出了一种称为空间意识网络（SPANet）的新架构，以整合空间上下文进行人群计数。提出了最大像素超额（MEP）损失，通过找到与真实值差异较大的像素级子区域来实现这一点。为此，我们设计了一种弱监督学习方案，通过多分支架构生成这样的区域。所提出的框架可以集成到现有的深度人群计数方法中，并且是端到端可训练的。在四个具有挑战性的基准上进行的大量实验表明，我们的方法可以显著提高基线的性能。更值得注意的是，我们的方法在所有基准数据集上都优于最先进的方法。",
        "领域": "人群计数/深度学习/空间意识",
        "问题": "提高人群计数的准确性和鲁棒性",
        "动机": "现有方法在保留密度图中的高频变化和学习空间意识方面存在困难，以及对各种噪声高度敏感",
        "方法": "提出了一种称为空间意识网络（SPANet）的新架构，并引入了最大像素超额（MEP）损失，通过弱监督学习方案和多分支架构生成与真实值差异较大的像素级子区域",
        "关键词": [
            "人群计数",
            "空间意识",
            "深度学习",
            "弱监督学习",
            "多分支架构"
        ],
        "涉及的技术概念": "深度卷积神经网络、欧几里得距离（L_2损失）、最大子数组超额（MESA）损失、最大像素超额（MEP）损失、弱监督学习、多分支架构"
    },
    {
        "order": 591,
        "title": "Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ge_Exploring_Overall_Contextual_Information_for_Image_Captioning_in_Human-Like_Cognitive_ICCV_2019_paper.html",
        "abstract": "Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results.",
        "中文标题": "探索整体上下文信息以人类认知风格进行图像描述",
        "摘要翻译": "图像描述是一个研究热点，其中结合卷积神经网络（CNN）和长短期记忆网络（LSTM）的编码器-解码器模型取得了令人瞩目的成果。尽管取得了显著进展，这些模型生成句子的方式与人类认知风格不同。现有模型通常从第一个词到最后一个词生成完整句子，而不考虑后续词汇对整个句子生成的影响。在本文中，我们探索了利用人类认知风格，即对要描述的图像和要构建的句子建立整体认知，以增强计算机图像理解。本文首先提出了一种带有双向LSTM的互助网络结构（MaBi-LSTMs），用于获取整体上下文信息。在训练过程中，前向和后向LSTM通过同时以互补方式构建整个句子，将后续和前面的词汇编码到各自的隐藏状态中。在描述过程中，LSTM隐式地利用其隐藏状态中包含的后续语义信息。实际上，MaBi-LSTMs可以生成两个方向的句子。为了弥合跨领域模型之间的差距并生成更高质量的句子，我们进一步开发了一种跨模态注意力机制，通过融合两个句子的显著部分以及图像的显著区域来修饰这两个句子。在Microsoft COCO数据集上的实验结果表明，所提出的模型提高了编码器-解码器模型的性能，并达到了最先进的结果。",
        "领域": "图像描述/自然语言处理/深度学习",
        "问题": "现有图像描述模型生成句子的方式与人类认知风格不同，未考虑后续词汇对整个句子生成的影响",
        "动机": "增强计算机图像理解，通过模仿人类认知风格生成更自然的图像描述",
        "方法": "提出了一种带有双向LSTM的互助网络结构（MaBi-LSTMs）获取整体上下文信息，并开发了一种跨模态注意力机制来修饰生成的句子",
        "关键词": [
            "图像描述",
            "双向LSTM",
            "跨模态注意力机制"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "长短期记忆网络（LSTM）": "一种特殊的递归神经网络（RNN），能够学习长期依赖信息",
            "双向LSTM": "一种LSTM的变体，能够同时考虑序列的前向和后向信息",
            "跨模态注意力机制": "一种机制，用于在不同模态（如文本和图像）之间建立联系，以增强模型的理解和生成能力"
        }
    },
    {
        "order": 592,
        "title": "SPLINE-Net: Sparse Photometric Stereo Through Lighting Interpolation and Normal Estimation Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_SPLINE-Net_Sparse_Photometric_Stereo_Through_Lighting_Interpolation_and_Normal_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zheng_SPLINE-Net_Sparse_Photometric_Stereo_Through_Lighting_Interpolation_and_Normal_Estimation_ICCV_2019_paper.html",
        "abstract": "This paper solves the Sparse Photometric stereo through Lighting Interpolation and Normal Estimation using a generative Network (SPLINE-Net). SPLINE-Net contains a lighting interpolation network to generate dense lighting observations given a sparse set of lights as inputs followed by a normal estimation network to estimate surface normals. Both networks are jointly constrained by the proposed symmetric and asymmetric loss functions to enforce isotropic constrain and perform outlier rejection of global illumination effects. SPLINE-Net is verified to outperform existing methods for photometric stereo of general BRDFs by using only ten images of different lights instead of using nearly one hundred images.",
        "中文标题": "SPLINE-Net: 通过光照插值和法线估计网络实现稀疏光度立体视觉",
        "摘要翻译": "本文通过使用生成网络（SPLINE-Net）解决稀疏光度立体视觉问题，该网络包含光照插值网络和法线估计网络。SPLINE-Net首先通过光照插值网络生成密集的光照观测数据，输入为稀疏的光照集，随后通过法线估计网络估计表面法线。两个网络通过提出的对称和非对称损失函数共同约束，以强制执行各向同性约束并执行全局光照效应的异常值排除。SPLINE-Net被验证在使用仅十张不同光照的图像而非近一百张图像的情况下，对于一般BRDF的光度立体视觉优于现有方法。",
        "领域": "光度立体视觉/表面重建/光照估计",
        "问题": "解决稀疏光度立体视觉问题，即从稀疏的光照图像中恢复物体的三维形状和表面法线",
        "动机": "减少光度立体视觉所需的光照图像数量，提高处理效率和准确性",
        "方法": "使用包含光照插值网络和法线估计网络的生成网络（SPLINE-Net），并通过对称和非对称损失函数共同约束这两个网络",
        "关键词": [
            "光度立体视觉",
            "表面重建",
            "光照估计",
            "法线估计",
            "光照插值"
        ],
        "涉及的技术概念": "SPLINE-Net是一个生成网络，包含两个主要部分：光照插值网络和法线估计网络。光照插值网络用于从稀疏的光照输入生成密集的光照观测数据，而法线估计网络则用于估计物体的表面法线。这两个网络通过对称和非对称损失函数进行联合训练，以强制执行各向同性约束并排除全局光照效应中的异常值。这种方法显著减少了进行光度立体视觉所需的光照图像数量，从近一百张减少到仅十张。"
    },
    {
        "order": 593,
        "title": "GradNet: Gradient-Guided Network for Visual Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_GradNet_Gradient-Guided_Network_for_Visual_Object_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_GradNet_Gradient-Guided_Network_for_Visual_Object_Tracking_ICCV_2019_paper.html",
        "abstract": "The fully-convolutional siamese network based on template matching has shown great potentials in visual tracking. During testing, the template is fixed with the initial target feature and the performance totally relies on the general matching ability of the siamese network. However, this manner cannot capture the temporal variations of targets or background clutter. In this work, we propose a novel gradient-guided network to exploit the discriminative information in gradients and update the template in the siamese network through feed-forward and backward operations. To be specific, the algorithm can utilize the information from the gradient to update the template in the current frame. In addition, a template generalization training method is proposed to better use gradient information and avoid overfitting. To our knowledge, this work is the first attempt to exploit the information in the gradient for template update in siamese-based trackers. Extensive experiments on recent benchmarks demonstrate that our method achieves better performance than other state-of-the-art trackers.",
        "中文标题": "GradNet: 梯度引导的视觉目标跟踪网络",
        "摘要翻译": "基于模板匹配的全卷积孪生网络在视觉跟踪中显示出了巨大的潜力。在测试过程中，模板固定为初始目标特征，性能完全依赖于孪生网络的通用匹配能力。然而，这种方式无法捕捉目标的时间变化或背景杂波。在这项工作中，我们提出了一种新颖的梯度引导网络，以利用梯度中的判别信息，并通过前向和后向操作更新孪生网络中的模板。具体来说，该算法可以利用来自梯度的信息来更新当前帧中的模板。此外，提出了一种模板泛化训练方法，以更好地利用梯度信息并避免过拟合。据我们所知，这项工作是首次尝试在基于孪生的跟踪器中利用梯度信息进行模板更新。在最近的基准测试上的大量实验表明，我们的方法比其他最先进的跟踪器实现了更好的性能。",
        "领域": "视觉目标跟踪/梯度信息利用/模板更新",
        "问题": "解决视觉目标跟踪中模板固定无法适应目标时间变化和背景杂波的问题",
        "动机": "提高视觉目标跟踪的准确性和鲁棒性，通过利用梯度信息动态更新模板以适应目标的变化",
        "方法": "提出了一种梯度引导网络，通过前向和后向操作更新孪生网络中的模板，并引入模板泛化训练方法以避免过拟合",
        "关键词": [
            "视觉目标跟踪",
            "梯度信息",
            "模板更新",
            "孪生网络",
            "模板泛化训练"
        ],
        "涉及的技术概念": "全卷积孪生网络：一种用于视觉跟踪的深度学习模型，通过模板匹配来跟踪目标。梯度引导网络：一种新颖的网络结构，利用梯度信息来更新跟踪模板。模板泛化训练：一种训练方法，旨在提高模型的泛化能力，避免过拟合。"
    },
    {
        "order": 594,
        "title": "Order-Aware Generative Modeling Using the 3D-Craft Dataset",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Order-Aware_Generative_Modeling_Using_the_3D-Craft_Dataset_ICCV_2019_paper.html",
        "abstract": "In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available.",
        "中文标题": "使用3D-Craft数据集进行顺序感知生成建模",
        "摘要翻译": "在本文中，我们研究了在Minecraft游戏中顺序建造房屋的问题，并证明了学习顺序可以使自回归模型更加有效。给定由人类玩家部分建造的房屋，我们的系统尝试以类似人类的方式放置额外的块来完成房屋。我们为这一新任务引入了一个新的数据集，HouseCraft。HouseCraft包含了2500个Minecraft房屋由人类从头开始建造的顺序。人类动作序列使我们能够学习一个称为Voxel-CNN的顺序感知生成模型。与许多生成模型相比，其中顺序生成顺序要么不重要（例如，使用GANs的整体生成），要么通过简单规则手动/任意设置（例如，光栅扫描顺序），我们的重点是模仿人类的顺序生成。为了评估生成模型是否能够准确预测类似人类的动作，我们提出了几种新颖的定量指标。我们证明了我们的Voxel-CNN模型在这一创造性任务中既简单又有效，并且可以作为未来研究方向的强基线。HouseCraft数据集和带有基线模型的代码将公开提供。",
        "领域": "生成模型/顺序建模/游戏AI",
        "问题": "在Minecraft游戏中顺序建造房屋的问题",
        "动机": "学习顺序可以使自回归模型更加有效，模仿人类的顺序生成",
        "方法": "引入HouseCraft数据集，学习顺序感知生成模型Voxel-CNN，提出定量指标评估模型",
        "关键词": [
            "顺序建模",
            "生成模型",
            "游戏AI"
        ],
        "涉及的技术概念": "自回归模型、GANs（生成对抗网络）、Voxel-CNN（体素卷积神经网络）、顺序生成、定量指标"
    },
    {
        "order": 595,
        "title": "Hyperspectral Image Reconstruction Using Deep External and Internal Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learning_ICCV_2019_paper.html",
        "abstract": "To solve the low spatial and/or temporal resolution problem which the conventional hypelrspectral cameras often suffer from, coded snapshot hyperspectral imaging systems have attracted more attention recently. Recovering a hyperspectral image (HSI) from its corresponding coded image is an ill-posed inverse problem, and learning accurate prior of HSI is essential to solve this inverse problem. In this paper, we present an effective convolutional neural network (CNN) based method for coded HSI reconstruction, which learns the deep prior from the external dataset as well as the internal information of input coded image with spatial-spectral constraint. Our method can effectively exploit spatial-spectral correlation and sufficiently represent the variety nature of HSIs. Experimental results show our method outperforms the state-of-the-art methods under both comprehensive quantitative metrics and perceptive quality.",
        "中文标题": "使用深度外部和内部学习的高光谱图像重建",
        "摘要翻译": "为了解决传统高光谱相机常遭受的低空间和/或时间分辨率问题，编码快照高光谱成像系统最近吸引了更多关注。从其对应的编码图像中恢复高光谱图像（HSI）是一个不适定的逆问题，学习准确的HSI先验对于解决这个逆问题至关重要。在本文中，我们提出了一种有效的基于卷积神经网络（CNN）的编码HSI重建方法，该方法从外部数据集以及输入编码图像的空间-光谱约束中学习深度先验。我们的方法能够有效利用空间-光谱相关性，并充分表示HSI的多样性。实验结果表明，我们的方法在综合定量指标和感知质量方面均优于最先进的方法。",
        "领域": "高光谱成像/图像重建/卷积神经网络",
        "问题": "解决传统高光谱相机低空间和/或时间分辨率问题",
        "动机": "学习准确的HSI先验以解决从编码图像中恢复高光谱图像的逆问题",
        "方法": "提出了一种基于卷积神经网络（CNN）的编码HSI重建方法，该方法从外部数据集以及输入编码图像的空间-光谱约束中学习深度先验",
        "关键词": [
            "高光谱成像",
            "图像重建",
            "卷积神经网络"
        ],
        "涉及的技术概念": "高光谱图像（HSI）、编码快照高光谱成像系统、卷积神经网络（CNN）、空间-光谱约束、深度先验"
    },
    {
        "order": 596,
        "title": "FAMNet: Joint Learning of Feature, Affinity and Multi-Dimensional Assignment for Online Multiple Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chu_FAMNet_Joint_Learning_of_Feature_Affinity_and_Multi-Dimensional_Assignment_for_ICCV_2019_paper.html",
        "abstract": "Data association-based multiple object tracking (MOT) involves multiple separated modules processed or optimized differently, which results in complex method design and requires non-trivial tuning of parameters. In this paper, we present an end-to-end model, named FAMNet, where Feature extraction, Affinity estimation and Multi-dimensional assignment are refined in a single network. All layers in FAMNet are designed differentiable thus can be optimized jointly to learn the discriminative features and higher-order affinity model for robust MOT, which is supervised by the loss directly from the assignment ground truth. In addition, we integrate single object tracking technique and a dedicated target management scheme into the FAMNet-based tracking system to further recover false negatives and inhibit noisy target candidates generated by the external detector. The proposed method is evaluated on a diverse set of benchmarks including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising performance on all of them in comparison with state-of-the-arts.",
        "中文标题": "FAMNet: 在线多目标跟踪中特征、亲和力和多维分配的联合学习",
        "摘要翻译": "基于数据关联的多目标跟踪（MOT）涉及多个分离的模块，这些模块以不同的方式处理或优化，导致方法设计复杂且需要非平凡的参数调整。在本文中，我们提出了一个名为FAMNet的端到端模型，其中特征提取、亲和力估计和多维分配在一个单一的网络中被精炼。FAMNet中的所有层都被设计为可微分的，因此可以联合优化以学习用于鲁棒MOT的判别特征和高阶亲和力模型，这是由直接从分配真实值中得到的损失监督的。此外，我们将单目标跟踪技术和专门的目标管理方案集成到基于FAMNet的跟踪系统中，以进一步恢复假阴性并抑制由外部检测器生成的噪声目标候选。所提出的方法在包括MOT2015、MOT2017、KITTI-Car和UA-DETRAC在内的多样化基准集上进行了评估，并在与最先进技术的比较中在所有基准上实现了有希望的性能。",
        "领域": "多目标跟踪/特征学习/亲和力估计",
        "问题": "多目标跟踪中模块分离导致的复杂方法设计和参数调整问题",
        "动机": "简化多目标跟踪方法设计，减少参数调整的复杂性",
        "方法": "提出一个端到端的FAMNet模型，联合优化特征提取、亲和力估计和多维分配，集成单目标跟踪技术和目标管理方案",
        "关键词": [
            "多目标跟踪",
            "特征学习",
            "亲和力估计",
            "多维分配",
            "端到端模型"
        ],
        "涉及的技术概念": {
            "FAMNet": "一个端到端的模型，用于联合学习特征、亲和力和多维分配",
            "多目标跟踪（MOT）": "一种跟踪多个目标的技术，涉及数据关联",
            "特征提取": "从原始数据中提取有用信息的过程",
            "亲和力估计": "估计目标之间相似度的过程",
            "多维分配": "在多维空间中分配目标的过程",
            "单目标跟踪技术": "专注于跟踪单个目标的技术",
            "目标管理方案": "管理和优化跟踪目标的策略"
        }
    },
    {
        "order": 597,
        "title": "Gravity as a Reference for Estimating a Person's Height From Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bieler_Gravity_as_a_Reference_for_Estimating_a_Persons_Height_From_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bieler_Gravity_as_a_Reference_for_Estimating_a_Persons_Height_From_ICCV_2019_paper.html",
        "abstract": "Estimating the metric height of a person from monocular imagery without additional assumptions is ill-posed. Existing solutions either require manual calibration of ground plane and camera geometry, special cameras, or reference objects of known size. We focus on motion cues and exploit gravity on earth as an omnipresent reference 'object' to translate acceleration, and subsequently height, measured in image-pixels to values in meters. We require videos of motion as input, where gravity is the only external force. This limitation is different to those of existing solutions that recover a person's height and, therefore, our method opens up new application fields. We show theoretically and empirically that a simple motion trajectory analysis suffices to translate from pixel measurements to the person's metric height, reaching a MAE of up to 3.9 cm on jumping motions, and that this works without camera and ground plane calibration.",
        "中文标题": "利用重力作为参考从视频中估计人的身高",
        "摘要翻译": "在没有额外假设的情况下，从单目图像中估计人的度量高度是不适定的。现有的解决方案要么需要手动校准地面平面和相机几何，要么需要特殊相机，或者已知大小的参考物体。我们专注于运动线索，并利用地球上的重力作为一个无处不在的参考'物体'，将加速度以及随后以图像像素为单位测量的高度转换为以米为单位的值。我们需要运动视频作为输入，其中重力是唯一的外力。这一限制与现有解决方案恢复人的高度的方法不同，因此，我们的方法开辟了新的应用领域。我们从理论上和经验上证明，一个简单的运动轨迹分析足以将像素测量转换为人的度量高度，在跳跃动作上达到高达3.9厘米的平均绝对误差，而且这不需要相机和地面平面校准。",
        "领域": "人体姿态估计/视频分析/运动分析",
        "问题": "从单目视频中准确估计人的度量高度",
        "动机": "现有方法需要手动校准或特殊设备，限制了应用范围，因此探索一种无需额外假设或设备的方法来估计人的高度",
        "方法": "利用重力作为参考，通过分析运动轨迹将像素测量转换为度量高度",
        "关键词": [
            "人体姿态估计",
            "视频分析",
            "运动分析"
        ],
        "涉及的技术概念": "单目图像、度量高度、运动轨迹分析、重力作为参考、像素到米的转换、平均绝对误差（MAE）"
    },
    {
        "order": 598,
        "title": "Crowd Counting With Deep Structured Scale Integration Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Crowd_Counting_With_Deep_Structured_Scale_Integration_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Crowd_Counting_With_Deep_Structured_Scale_Integration_Network_ICCV_2019_paper.html",
        "abstract": "Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. Specifically, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. In particular, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.",
        "中文标题": "使用深度结构化尺度集成网络进行人群计数",
        "摘要翻译": "在无约束的拥挤场景中自动估计人数是一个具有挑战性的任务，其中一个主要困难源于人的巨大尺度变化。在本文中，我们提出了一种新颖的深度结构化尺度集成网络（DSSINet）用于人群计数，该网络通过使用结构化特征表示学习和分层结构化损失函数优化来解决人的尺度变化问题。与直接通过加权平均或连接融合多个特征的传统方法不同，我们首先引入了一个基于条件随机场（CRFs）的结构化特征增强模块，通过消息传递机制相互细化多尺度特征。具体来说，每个尺度特定的特征被视为一个连续随机变量，并传递互补信息以细化其他尺度的特征。其次，我们利用扩张多尺度结构相似性损失来强制我们的DSSINet学习各种大小区域内人的尺度的局部相关性，从而产生高质量的密度图。在四个具有挑战性的基准上进行的大量实验很好地证明了我们方法的有效性。特别是，我们的DSSINet在Shanghaitech数据集上实现了9.5%的错误减少，在UCF-QNRF数据集上实现了24.9%的错误减少，相对于最先进的方法。",
        "领域": "人群计数/尺度变化/密度图估计",
        "问题": "解决在无约束拥挤场景中自动估计人数时，由于人的尺度变化巨大而带来的挑战",
        "动机": "为了更准确地估计拥挤场景中的人数，需要解决由于人的尺度变化巨大而带来的问题",
        "方法": "提出了一种深度结构化尺度集成网络（DSSINet），通过结构化特征表示学习和分层结构化损失函数优化来解决人的尺度变化问题，并引入基于条件随机场的结构化特征增强模块和扩张多尺度结构相似性损失",
        "关键词": [
            "人群计数",
            "尺度变化",
            "密度图估计",
            "结构化特征增强",
            "条件随机场",
            "扩张多尺度结构相似性损失"
        ],
        "涉及的技术概念": {
            "深度结构化尺度集成网络（DSSINet）": "一种用于人群计数的网络，通过结构化特征表示学习和分层结构化损失函数优化来解决人的尺度变化问题",
            "结构化特征增强模块": "基于条件随机场（CRFs）的模块，通过消息传递机制相互细化多尺度特征",
            "扩张多尺度结构相似性损失": "一种损失函数，用于强制网络学习各种大小区域内人的尺度的局部相关性，从而产生高质量的密度图"
        }
    },
    {
        "order": 599,
        "title": "Shadow Removal via Shadow Image Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Le_Shadow_Removal_via_Shadow_Image_Decomposition_ICCV_2019_paper.html",
        "abstract": "We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects on the images. We train and test our framework on the most challenging shadow removal dataset (ISTD). Compared to the state-of-the-art method, our model achieves a 40% error reduction in terms of root mean square error (RMSE) for the shadow area, reducing RMSE from 13.3 to 7.9. Moreover, we create an augmented ISTD dataset based on an image decomposition system by modifying the shadow parameters to generate new synthetic shadow images. Training our model on this new augmented ISTD dataset further lowers the RMSE on the shadow area to 7.4.",
        "中文标题": "通过阴影图像分解进行阴影去除",
        "摘要翻译": "我们提出了一种新颖的深度学习方法来去除阴影。受阴影形成的物理模型启发，我们使用线性光照变换来模拟图像中的阴影效果，这使得阴影图像可以表示为无阴影图像、阴影参数和遮罩层的组合。我们使用两个深度网络，即SP-Net和M-Net，分别预测阴影参数和阴影遮罩。该系统使我们能够去除图像上的阴影效果。我们在最具挑战性的阴影去除数据集（ISTD）上训练和测试我们的框架。与最先进的方法相比，我们的模型在阴影区域的均方根误差（RMSE）方面实现了40%的误差减少，将RMSE从13.3降低到7.9。此外，我们通过修改阴影参数生成新的合成阴影图像，基于图像分解系统创建了一个增强的ISTD数据集。在这个新的增强ISTD数据集上训练我们的模型，进一步将阴影区域的RMSE降低到7.4。",
        "领域": "阴影去除/图像增强/深度学习应用",
        "问题": "图像中的阴影效果去除",
        "动机": "提高图像质量，通过去除阴影来改善图像分析和处理的效果",
        "方法": "使用线性光照变换模拟阴影效果，结合两个深度网络SP-Net和M-Net预测阴影参数和遮罩，实现阴影去除",
        "关键词": [
            "阴影去除",
            "图像分解",
            "深度学习"
        ],
        "涉及的技术概念": {
            "线性光照变换": "一种模拟图像中阴影效果的技术，通过变换光照条件来模拟阴影的形成",
            "SP-Net": "用于预测阴影参数的深度网络",
            "M-Net": "用于预测阴影遮罩的深度网络",
            "均方根误差（RMSE）": "衡量预测值与实际值之间差异的统计量，用于评估阴影去除效果",
            "ISTD数据集": "一个用于阴影去除研究的数据集，包含具有挑战性的阴影图像"
        }
    },
    {
        "order": 600,
        "title": "Learning Discriminative Model Prediction for Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bhat_Learning_Discriminative_Model_Prediction_for_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bhat_Learning_Discriminative_Model_Prediction_for_Tracking_ICCV_2019_paper.html",
        "abstract": "The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking.",
        "中文标题": "学习判别模型预测用于跟踪",
        "摘要翻译": "当前向端到端可训练的计算机视觉系统的努力对视觉跟踪任务提出了重大挑战。与大多数其他视觉问题相比，跟踪需要在推理阶段在线学习一个鲁棒的目标特定外观模型。为了端到端可训练，目标模型的在线学习因此需要嵌入到跟踪架构本身中。由于面临的挑战，流行的Siamese范式仅预测目标特征模板，而在推理过程中忽略了背景外观信息。因此，预测的模型具有有限的目标-背景区分能力。我们开发了一个端到端的跟踪架构，能够充分利用目标和背景外观信息进行目标模型预测。我们的架构源自于通过设计一个专门的优化过程来实现的判别学习损失，该过程能够在仅几次迭代中预测出一个强大的模型。此外，我们的方法能够学习判别损失本身的关键方面。所提出的跟踪器在6个跟踪基准上设定了新的最先进水平，在VOT2018上实现了0.440的EAO分数，同时运行速度超过40 FPS。代码和模型可在https://github.com/visionml/pytracking获取。",
        "领域": "视觉跟踪/目标检测/模型优化",
        "问题": "如何在视觉跟踪任务中实现端到端可训练的系统，并提高目标与背景的区分能力",
        "动机": "当前视觉跟踪系统在端到端训练方面面临挑战，特别是在推理阶段需要在线学习目标特定外观模型，而现有方法如Siamese范式忽略了背景信息，导致目标-背景区分能力有限",
        "方法": "开发了一个端到端的跟踪架构，通过设计专门的优化过程来预测强大的目标模型，并能够学习判别损失的关键方面",
        "关键词": [
            "视觉跟踪",
            "目标检测",
            "模型优化",
            "端到端学习",
            "判别学习"
        ],
        "涉及的技术概念": "Siamese范式是一种流行的视觉跟踪方法，它通过比较目标特征模板来进行跟踪，但这种方法在推理过程中忽略了背景信息。本文提出的方法通过设计一个专门的优化过程，能够在仅几次迭代中预测出一个强大的目标模型，并且能够学习判别损失的关键方面，从而提高目标与背景的区分能力。"
    },
    {
        "order": 601,
        "title": "Bidirectional One-Shot Unsupervised Domain Mapping",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cohen_Bidirectional_One-Shot_Unsupervised_Domain_Mapping_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cohen_Bidirectional_One-Shot_Unsupervised_Domain_Mapping_ICCV_2019_paper.html",
        "abstract": "We study the problem of mapping between a domain A, in which there is a single training sample and a domain B, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain B. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST.",
        "中文标题": "双向一次性无监督域映射",
        "摘要翻译": "我们研究了在域A（其中只有一个训练样本）和域B（我们有更丰富的训练集）之间进行映射的问题。我们提出的方法能够在这两个方向上进行这种映射。例如，我们可以将所有MNIST图像转移到由单个SVHN图像捕获的视觉域，并将SVHN图像转换为MNIST图像的域。我们的方法基于为每个域使用一个编码器和一个解码器，而不利用权重共享。单个样本域的自动编码器被训练以匹配该样本和域B的潜在空间。我们的结果展示了域之间的令人信服的映射，其中源域或目标域由单个样本定义，远远超过了现有的解决方案。我们的代码已在https://github.com/tomercohen11/BiOST公开。",
        "领域": "域适应/图像生成/无监督学习",
        "问题": "在只有一个训练样本的域A和拥有丰富训练集的域B之间进行双向映射",
        "动机": "探索在极少量样本条件下，实现不同域之间有效映射的可能性，以扩展无监督学习的应用范围",
        "方法": "为每个域使用独立的编码器和解码器，不共享权重，训练单个样本域的自动编码器以匹配该样本和另一个域的潜在空间",
        "关键词": [
            "域适应",
            "图像生成",
            "无监督学习"
        ],
        "涉及的技术概念": "自动编码器是一种神经网络，用于学习数据的有效编码（潜在空间表示），通过编码器和解码器的组合实现。在本文中，自动编码器被用来匹配单个样本和另一个域的潜在空间，从而实现域之间的映射。"
    },
    {
        "order": 602,
        "title": "DynamoNet: Dynamic Action and Motion Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Diba_DynamoNet_Dynamic_Action_and_Motion_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Diba_DynamoNet_Dynamic_Action_and_Motion_Network_ICCV_2019_paper.html",
        "abstract": "In this paper, we are interested in self-supervised learning the motion cues in videos using dynamic motion filters for a better motion representation to finally boost human action recognition in particular. Thus far, the vision community has focused on spatio-temporal approaches using standard filters, rather we here propose dynamic filters that adaptively learn the video-specific internal motion representation by predicting the short-term future frames. We name this new motion representation, as dynamic motion representation (DMR) and is embedded inside of 3D convolutional network as a new layer, which captures the visual appearance and motion dynamics throughout entire video clip via end-to-end network learning. Simultaneously, we utilize these motion representation to enrich video classification. We have designed the frame prediction task as an auxiliary task to empower the classification problem. With these overall objectives, to this end, we introduce a novel unified spatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the video classification and learning motion representation by predicting future frames as a multi-task learning problem. We conduct experiments on challenging human action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the proposed DynamoNet show promising results on all the datasets.",
        "中文标题": "DynamoNet: 动态动作与运动网络",
        "摘要翻译": "在本文中，我们感兴趣的是使用动态运动滤波器自监督学习视频中的运动线索，以获得更好的运动表示，最终特别提升人类动作识别。迄今为止，视觉社区主要关注使用标准滤波器的时空方法，而我们在这里提出了动态滤波器，通过预测短期未来帧来自适应地学习视频特定的内部运动表示。我们将这种新的运动表示命名为动态运动表示（DMR），并将其嵌入到3D卷积网络中作为一个新层，通过端到端网络学习捕捉整个视频片段中的视觉外观和运动动态。同时，我们利用这些运动表示来丰富视频分类。我们将帧预测任务设计为辅助任务，以增强分类问题。基于这些总体目标，我们引入了一种新颖的统一时空3D-CNN架构（DynamoNet），通过预测未来帧作为多任务学习问题，联合优化视频分类和学习运动表示。我们在具有挑战性的人类动作数据集上进行了实验：Kinetics 400、UCF101、HMDB51。使用提出的DynamoNet进行的实验在所有数据集上都显示出有希望的结果。",
        "领域": "视频理解/动作识别/自监督学习",
        "问题": "提升视频中人类动作识别的准确性和效率",
        "动机": "现有的时空方法主要使用标准滤波器，缺乏对视频特定内部运动表示的自适应学习能力，因此需要一种新的方法来更好地表示运动，从而提升动作识别的性能。",
        "方法": "提出了一种动态运动表示（DMR），并将其嵌入到3D卷积网络中作为一个新层，通过预测短期未来帧来自适应地学习视频特定的内部运动表示，同时利用这些运动表示来丰富视频分类。设计了一种新颖的统一时空3D-CNN架构（DynamoNet），通过预测未来帧作为多任务学习问题，联合优化视频分类和学习运动表示。",
        "关键词": [
            "动态运动表示",
            "3D卷积网络",
            "视频分类",
            "自监督学习",
            "动作识别"
        ],
        "涉及的技术概念": "动态运动表示（DMR）是一种新的运动表示方法，通过预测短期未来帧来自适应地学习视频特定的内部运动表示。3D卷积网络（3D-CNN）是一种能够捕捉视频中时空特征的网络架构。自监督学习是一种不需要大量标注数据的学习方法，通过设计辅助任务来学习有用的表示。动作识别是指从视频中识别出人类的动作或行为。"
    },
    {
        "order": 603,
        "title": "OperatorNet: Recovering 3D Shapes From Difference Operators",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_OperatorNet_Recovering_3D_Shapes_From_Difference_Operators_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_OperatorNet_Recovering_3D_Shapes_From_Difference_Operators_ICCV_2019_paper.html",
        "abstract": "This paper proposes a learning-based framework for reconstructing 3D shapes from functional operators, compactly encoded as small-sized matrices. To this end we introduce a novel neural architecture, called OperatorNet, which takes as input a set of linear operators representing a shape and produces its 3D embedding. We demonstrate that this approach significantly outperforms previous purely geometric methods for the same problem. Furthermore, we introduce a novel functional operator, which encodes the extrinsic or pose-dependent shape information, and thus complements purely intrinsic pose-oblivious operators, such as the classical Laplacian. Coupled with this novel operator, our reconstruction network achieves very high reconstruction accuracy, even in the presence of incomplete information about a shape, given a soft or functional map expressed in a reduced basis. Finally, we demonstrate that the multiplicative functional algebra enjoyed by these operators can be used to synthesize entirely new unseen shapes, in the context of shape interpolation and shape analogy applications.",
        "中文标题": "OperatorNet: 从差分算子恢复3D形状",
        "摘要翻译": "本文提出了一种基于学习的框架，用于从功能算子重建3D形状，这些算子被紧凑地编码为小尺寸矩阵。为此，我们引入了一种新颖的神经架构，称为OperatorNet，它以表示形状的一组线性算子作为输入，并生成其3D嵌入。我们证明，这种方法在相同问题上显著优于以前的纯几何方法。此外，我们引入了一种新的功能算子，它编码了外在或姿态依赖的形状信息，从而补充了纯内在的姿态无关算子，如经典的拉普拉斯算子。与这种新算子相结合，我们的重建网络即使在形状信息不完整的情况下，也能实现非常高的重建精度，前提是在简化基中表达的软或功能映射。最后，我们证明了这些算子所享有的乘法功能代数可以用于在形状插值和形状类比应用中合成全新的未见形状。",
        "领域": "3D形状重建/功能算子/神经架构",
        "问题": "从功能算子重建3D形状",
        "动机": "提高从功能算子重建3D形状的精度和效率，特别是在信息不完整的情况下",
        "方法": "引入OperatorNet神经架构，结合新的功能算子进行3D形状重建",
        "关键词": [
            "3D形状重建",
            "功能算子",
            "神经架构",
            "形状插值",
            "形状类比"
        ],
        "涉及的技术概念": "OperatorNet是一种新颖的神经架构，用于从线性算子重建3D形状。新的功能算子编码了形状的外在信息，与内在的姿态无关算子（如拉普拉斯算子）互补。这种方法在形状信息不完整的情况下也能实现高精度的重建，并可用于形状插值和类比应用中的新形状合成。"
    },
    {
        "order": 604,
        "title": "Evolving Space-Time Neural Architectures for Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Piergiovanni_Evolving_Space-Time_Neural_Architectures_for_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Piergiovanni_Evolving_Space-Time_Neural_Architectures_for_Videos_ICCV_2019_paper.html",
        "abstract": "We present a new method for finding video CNN architectures that more optimally capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing CNN video architectures. We here develop a novel evolutionary algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new diverse and interesting video architectures that were unknown previously. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on four datasets: Kinetics, Charades, Moments in Time and HMDB. We will open source the code and models, to encourage future model development.",
        "中文标题": "进化时空神经架构用于视频处理",
        "摘要翻译": "我们提出了一种新方法，用于寻找能更优地捕捉视频中丰富时空信息的视频CNN架构。之前的工作利用3D卷积，通过手动设计CNN视频架构获得了有希望的结果。我们这里开发了一种新颖的进化算法，自动探索具有不同类型和组合层的模型，以共同学习视频表示中空间和时间方面的交互。我们通过将该算法应用于两种元架构来证明其通用性。此外，我们提出了一个新组件，即iTGM层，它更有效地利用其参数，以允许在更长的时间范围内学习时空交互。iTGM层经常被进化算法优选，并允许构建成本效益高的网络。所提出的方法发现了以前未知的多样且有趣的视频架构。更重要的是，它们既比以前的模型更准确，也更快，并且在四个数据集上超越了最先进的结果：Kinetics、Charades、Moments in Time和HMDB。我们将开源代码和模型，以鼓励未来的模型开发。",
        "领域": "视频分析/神经网络架构/进化算法",
        "问题": "如何更优地捕捉视频中的时空信息",
        "动机": "为了自动探索并发现能更有效捕捉视频时空信息的神经网络架构，减少手动设计的需求",
        "方法": "开发了一种新颖的进化算法，自动探索具有不同类型和组合层的模型，并提出了iTGM层以更有效地学习时空交互",
        "关键词": [
            "视频CNN架构",
            "进化算法",
            "iTGM层",
            "时空信息"
        ],
        "涉及的技术概念": "3D卷积用于捕捉视频中的时空信息；进化算法用于自动探索和优化神经网络架构；iTGM层是一种新提出的组件，用于更有效地学习视频中的时空交互。"
    },
    {
        "order": 605,
        "title": "Neural Inverse Rendering of an Indoor Scene From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sengupta_Neural_Inverse_Rendering_of_an_Indoor_Scene_From_a_Single_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sengupta_Neural_Inverse_Rendering_of_an_Indoor_Scene_From_a_Single_ICCV_2019_paper.html",
        "abstract": "Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for single objects or with methods that solve for only one of the scene attributes. We propose the first learning based approach that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects (e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise. This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing the input image from the estimated components. We finetune with real data after pretraining with synthetic data. To this end, we use physically-based rendering to create a large-scale synthetic dataset, named SUNCG-PBR, which is a significant improvement over prior datasets. Experimental results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.",
        "中文标题": "从单张图像进行室内场景的神经逆向渲染",
        "摘要翻译": "逆向渲染旨在从图像中估计场景的物理属性，例如反射率、几何形状和光照。逆向渲染主要针对单个对象或仅解决场景属性之一的方法进行了研究。我们提出了第一个基于学习的方法，可以从单张图像联合估计室内场景的反照率、法线和光照。我们的关键贡献是残差外观渲染器（RAR），它可以被训练来合成复杂的外观效果（例如，相互反射、投射阴影、近场照明和逼真的阴影），这些效果在其他情况下会被忽略。这使得我们能够使用重建损失在真实数据上进行自监督学习，基于从估计的组件重新合成输入图像。我们在使用合成数据进行预训练后，用真实数据进行微调。为此，我们使用基于物理的渲染创建了一个名为SUNCG-PBR的大规模合成数据集，这是对先前数据集的显著改进。实验结果表明，我们的方法在估计一个或多个场景属性方面优于最先进的方法。",
        "领域": "逆向渲染/室内场景重建/物理属性估计",
        "问题": "从单张图像联合估计室内场景的反照率、法线和光照",
        "动机": "解决现有逆向渲染方法主要针对单个对象或仅解决场景属性之一的问题，提出一种能够联合估计多个场景属性的方法",
        "方法": "提出残差外观渲染器（RAR）进行复杂外观效果的合成，使用重建损失在真实数据上进行自监督学习，并通过基于物理的渲染创建大规模合成数据集进行预训练和微调",
        "关键词": [
            "逆向渲染",
            "室内场景重建",
            "物理属性估计",
            "残差外观渲染器",
            "自监督学习"
        ],
        "涉及的技术概念": "逆向渲染是一种从图像中估计场景物理属性的技术，包括反射率、几何形状和光照等。残差外观渲染器（RAR）是一种能够合成复杂外观效果的神经网络模型。自监督学习是一种不需要标注数据的学习方法，通过重建损失来训练模型。基于物理的渲染是一种模拟光线与物体交互的渲染技术，用于生成逼真的图像。"
    },
    {
        "order": 606,
        "title": "SlowFast Networks for Video Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Feichtenhofer_SlowFast_Networks_for_Video_Recognition_ICCV_2019_paper.html",
        "abstract": "We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. Code has been made available at: https://github.com/facebookresearch/SlowFast.",
        "中文标题": "用于视频识别的SlowFast网络",
        "摘要翻译": "我们提出了用于视频识别的SlowFast网络。我们的模型包括：(i) 一个以低帧率运行的Slow路径，用于捕捉空间语义，以及(ii) 一个以高帧率运行的Fast路径，用于捕捉精细时间分辨率下的运动。通过减少其通道容量，Fast路径可以变得非常轻量级，但仍能学习到对视频识别有用的时间信息。我们的模型在视频中的动作分类和检测方面都取得了强劲的性能，并且我们的SlowFast概念的贡献被明确指出为带来了大幅度的改进。我们在主要的视频识别基准测试Kinetics、Charades和AVA上报告了最先进的准确率。代码已在https://github.com/facebookresearch/SlowFast上提供。",
        "领域": "视频识别/动作分类/动作检测",
        "问题": "如何在视频中有效地捕捉空间语义和精细时间分辨率下的运动信息",
        "动机": "提高视频识别中动作分类和检测的准确率和效率",
        "方法": "提出SlowFast网络，包括一个低帧率的Slow路径用于捕捉空间语义，和一个高帧率的Fast路径用于捕捉精细时间分辨率下的运动信息，通过减少Fast路径的通道容量使其轻量化",
        "关键词": [
            "视频识别",
            "动作分类",
            "动作检测",
            "SlowFast网络",
            "空间语义",
            "时间分辨率"
        ],
        "涉及的技术概念": "SlowFast网络是一种用于视频识别的深度学习模型，它通过两个路径（Slow路径和Fast路径）分别处理视频的空间语义和时间动态信息。Slow路径以低帧率运行，专注于捕捉视频帧中的空间信息；Fast路径以高帧率运行，专注于捕捉视频中的快速运动信息。通过这种方式，SlowFast网络能够有效地结合视频的空间和时间信息，提高视频识别的准确率。"
    },
    {
        "order": 607,
        "title": "Universally Slimmable Networks and Improved Training Techniques",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Universally_Slimmable_Networks_and_Improved_Training_Techniques_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Universally_Slimmable_Networks_and_Improved_Training_Techniques_ICCV_2019_paper.html",
        "abstract": "Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks.",
        "中文标题": "通用可瘦身网络及改进的训练技术",
        "摘要翻译": "可瘦身网络是一类可以即时调整运行时宽度的神经网络。宽度可以从预定义的宽度集合中选择，以在运行时自适应地优化准确性与效率之间的权衡。在这项工作中，我们提出了一种系统的方法来训练通用可瘦身网络（US-Nets），将可瘦身网络扩展到可以在任意宽度下执行，并推广到包含和不包含批量归一化层的网络。我们进一步提出了两种改进的训练技术，名为三明治规则和原地蒸馏，以增强训练过程并提高测试准确性。我们展示了通用可瘦身MobileNet v1和MobileNet v2在ImageNet分类任务上的改进性能，与单独训练的网络和4切换可瘦身网络基线相比。我们还在图像超分辨率和深度强化学习任务上评估了提出的US-Nets和改进的训练技术。这些代表性任务上的广泛消融实验证明了我们提出方法的有效性。我们的发现开启了直接评估网络架构的FLOPs-准确性谱的可能性。代码和模型可在https://github.com/JiahuiYu/slimmable_networks获取。",
        "领域": "神经网络优化/模型压缩/自适应计算",
        "问题": "如何在运行时自适应地优化神经网络的准确性与效率之间的权衡",
        "动机": "提高神经网络在不同计算资源下的适应性和效率，同时保持或提高准确性",
        "方法": "提出通用可瘦身网络（US-Nets）和两种改进的训练技术（三明治规则和原地蒸馏）",
        "关键词": [
            "可瘦身网络",
            "模型压缩",
            "自适应计算",
            "训练技术",
            "图像分类",
            "图像超分辨率",
            "深度强化学习"
        ],
        "涉及的技术概念": {
            "可瘦身网络": "一类可以即时调整运行时宽度的神经网络，以在运行时自适应地优化准确性与效率之间的权衡。",
            "通用可瘦身网络（US-Nets）": "扩展了可瘦身网络的概念，使其可以在任意宽度下执行，并适用于包含和不包含批量归一化层的网络。",
            "三明治规则": "一种改进的训练技术，旨在增强训练过程。",
            "原地蒸馏": "一种改进的训练技术，旨在提高测试准确性。",
            "FLOPs-准确性谱": "评估网络架构在不同计算复杂度下的性能表现。"
        }
    },
    {
        "order": 608,
        "title": "Generative Multi-View Human Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Generative_Multi-View_Human_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Generative_Multi-View_Human_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "Multi-view action recognition targets to integrate complementary information from different views to improve classification performance. It is a challenging task due to the distinct gap between heterogeneous feature domains. Moreover, most existing methods neglect to consider the incomplete multi-view data, which limits their potential compatibility in real-world applications. In this work, we propose a Generative Multi-View Action Recognition (GMVAR) framework to address the challenges above. The adversarial generative network is leveraged to generate one view conditioning on the other view, which fully explores the latent connections in both intra-view and cross-view aspects. Our approach enhances the model robustness by employing adversarial training, and naturally handles the incomplete view case by imputing the missing data. Moreover, an effective View Correlation Discovery Network (VCDN) is proposed to further fuse the multi-view information in a higher-level label space. Extensive experiments demonstrate the effectiveness of our proposed approach by comparing with state-of-the-art algorithms.",
        "中文标题": "生成式多视角人类动作识别",
        "摘要翻译": "多视角动作识别旨在整合来自不同视角的互补信息以提高分类性能。由于异构特征域之间的显著差异，这是一项具有挑战性的任务。此外，大多数现有方法忽视了考虑不完整的多视角数据，这限制了它们在实际应用中的潜在兼容性。在本工作中，我们提出了一个生成式多视角动作识别（GMVAR）框架来解决上述挑战。利用对抗生成网络在一个视角的条件下生成另一个视角，这充分探索了视角内和跨视角的潜在联系。我们的方法通过采用对抗训练增强了模型的鲁棒性，并通过填补缺失数据自然地处理了不完整视角的情况。此外，提出了一个有效的视角相关发现网络（VCDN）以在更高层次的标签空间中进一步融合多视角信息。大量实验通过比较最先进的算法证明了我们提出方法的有效性。",
        "领域": "动作识别/多视角学习/对抗生成网络",
        "问题": "解决多视角动作识别中异构特征域差异和不完整多视角数据的问题",
        "动机": "提高多视角动作识别的分类性能和实际应用中的兼容性",
        "方法": "提出生成式多视角动作识别框架，利用对抗生成网络生成视角，采用对抗训练增强模型鲁棒性，提出视角相关发现网络融合多视角信息",
        "关键词": [
            "动作识别",
            "多视角学习",
            "对抗生成网络",
            "视角相关发现网络"
        ],
        "涉及的技术概念": {
            "对抗生成网络": "一种通过对抗过程估计生成模型的框架，用于生成数据",
            "对抗训练": "一种训练方法，通过引入对抗样本来提高模型的鲁棒性",
            "视角相关发现网络": "一种网络结构，用于在更高层次的标签空间中融合多视角信息"
        }
    },
    {
        "order": 609,
        "title": "ForkNet: Multi-Branch Volumetric Semantic Completion From a Single Depth Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_ForkNet_Multi-Branch_Volumetric_Semantic_Completion_From_a_Single_Depth_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_ForkNet_Multi-Branch_Volumetric_Semantic_Completion_From_a_Single_Depth_Image_ICCV_2019_paper.html",
        "abstract": "We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise. We build the new dataset by sampling the features directly from latent space which generates a pair of partial volumetric surface and completed volumetric semantic surface. Moreover, we utilize multiple discriminators to increase the accuracy and realism of the reconstructions. We demonstrate the benefits of our approach on standard benchmarks for the two most common completion tasks: semantic 3D scene completion and 3D object completion.",
        "中文标题": "ForkNet: 从单一深度图像进行多分支体积语义补全",
        "摘要翻译": "我们提出了一种新颖的模型，用于从单一深度图像进行3D语义补全，该模型基于单一编码器和三个独立的生成器，用于重建原始场景和补全场景的不同几何和语义表示，所有这些都共享相同的潜在空间。为了在网络中的几何和语义分支之间传递信息，我们引入了在相应网络层连接特征的路径。由于来自真实场景的训练样本数量有限，我们架构的一个有趣属性是能够通过生成一个包含遮挡和真实噪声的高质量、逼真场景的新训练数据集来补充现有数据集。我们通过直接从潜在空间采样特征来构建新数据集，这些特征生成了一对部分体积表面和补全的体积语义表面。此外，我们利用多个判别器来提高重建的准确性和真实感。我们在两个最常见的补全任务的标准基准上展示了我们方法的好处：语义3D场景补全和3D对象补全。",
        "领域": "3D重建/语义分割/生成对抗网络",
        "问题": "从单一深度图像进行3D语义补全",
        "动机": "由于来自真实场景的训练样本数量有限，需要一种方法来补充现有数据集，以提高3D语义补全的准确性和真实感。",
        "方法": "基于单一编码器和三个独立生成器的模型，通过引入路径在几何和语义分支之间传递信息，利用多个判别器提高重建的准确性和真实感，并通过直接从潜在空间采样特征生成新训练数据集。",
        "关键词": [
            "3D语义补全",
            "深度图像",
            "生成对抗网络"
        ],
        "涉及的技术概念": "3D语义补全指的是从单一深度图像中恢复出完整的3D场景或对象的几何和语义信息。生成对抗网络（GANs）是一种深度学习模型，通过生成器和判别器的对抗训练来生成高质量的数据。潜在空间是指数据在某种变换或编码后的表示空间，可以用于生成新的数据样本。"
    },
    {
        "order": 610,
        "title": "AutoDispNet: Improving Disparity Estimation With AutoML",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saikia_AutoDispNet_Improving_Disparity_Estimation_With_AutoML_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saikia_AutoDispNet_Improving_Disparity_Estimation_With_AutoML_ICCV_2019_paper.html",
        "abstract": "Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.",
        "中文标题": "AutoDispNet: 使用AutoML改进视差估计",
        "摘要翻译": "在计算机视觉领域，许多研究工作都致力于优化现有网络架构，以在基准测试中获得更多的百分点。最近的AutoML方法承诺将我们从这种努力中解放出来。然而，它们主要是为相对小规模的分类任务设计的。在这项工作中，我们展示了如何使用和扩展现有的AutoML技术来有效优化大规模U-Net类编码器-解码器架构。特别是，我们利用基于梯度的神经架构搜索和贝叶斯优化进行超参数搜索。这种优化不需要大规模的计算集群。我们在视差估计上的结果显示，明显优于手动优化的基线，并达到了最先进的性能。",
        "领域": "视差估计/神经架构搜索/超参数优化",
        "问题": "如何有效优化大规模U-Net类编码器-解码器架构以提高视差估计的性能",
        "动机": "减少在优化现有网络架构上的努力，同时提高视差估计的性能",
        "方法": "使用和扩展现有的AutoML技术，特别是基于梯度的神经架构搜索和贝叶斯优化进行超参数搜索",
        "关键词": [
            "AutoML",
            "视差估计",
            "神经架构搜索",
            "贝叶斯优化",
            "U-Net"
        ],
        "涉及的技术概念": "AutoML（自动机器学习）是一种自动化机器学习模型选择、超参数优化和模型训练的技术。神经架构搜索（NAS）是一种自动化设计神经网络架构的方法。贝叶斯优化是一种用于全局优化的概率模型，特别适用于优化昂贵的黑箱函数。U-Net是一种常用于图像分割的卷积神经网络架构。"
    },
    {
        "order": 611,
        "title": "Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.html",
        "abstract": "Video Recognition has drawn great research interest and great progress has been made. A suitable frame sampling strategy can improve the accuracy and efficiency of recognition. However, mainstream solutions generally adopt hand-crafted frame sampling strategies for recognition. It could degrade the performance, especially in untrimmed videos, due to the variation of frame-level saliency. To this end, we concentrate on improving untrimmed video classification via developing a learning-based frame sampling strategy. We intuitively formulate the frame sampling procedure as multiple parallel Markov decision processes, each of which aims at picking out a frame/clip by gradually adjusting an initial sampling. Then we propose to solve the problems with multi-agent reinforcement learning (MARL). Our MARL framework is composed of a novel RNN-based context-aware observation network which jointly models context information among nearby agents and historical states of a specific agent, a policy network which generates the probability distribution over a predefined action space at each step and a classification network for reward calculation as well as final recognition. Extensive experimental results show that our MARL-based scheme remarkably outperforms hand-crafted strategies with various 2D and 3D baseline methods. Our single RGB model achieves a comparable performance of ActivityNet v1.3 champion submission with multi-modal multi-model fusion and new state-of-the-art results on YouTube Birds and YouTube Cars.",
        "中文标题": "基于多智能体强化学习的帧采样用于有效的未剪辑视频识别",
        "摘要翻译": "视频识别已经引起了极大的研究兴趣，并取得了巨大进展。一个合适的帧采样策略可以提高识别的准确性和效率。然而，主流解决方案通常采用手工制作的帧采样策略进行识别。由于帧级显著性的变化，这可能会降低性能，尤其是在未剪辑的视频中。为此，我们专注于通过开发基于学习的帧采样策略来改进未剪辑视频分类。我们直观地将帧采样过程表述为多个并行的马尔可夫决策过程，每个过程旨在通过逐步调整初始采样来挑选出一个帧/片段。然后，我们提出使用多智能体强化学习（MARL）来解决这些问题。我们的MARL框架由一个新颖的基于RNN的上下文感知观察网络组成，该网络联合建模附近智能体之间的上下文信息和特定智能体的历史状态，一个策略网络，它在每一步生成预定义动作空间上的概率分布，以及一个用于奖励计算和最终识别的分类网络。大量的实验结果表明，我们的基于MARL的方案显著优于各种2D和3D基线方法的手工制作策略。我们的单一RGB模型在ActivityNet v1.3冠军提交中实现了与多模态多模型融合相当的性能，并在YouTube Birds和YouTube Cars上取得了新的最先进结果。",
        "领域": "视频识别/强化学习/帧采样",
        "问题": "提高未剪辑视频分类的准确性和效率",
        "动机": "主流解决方案采用的手工制作的帧采样策略可能会降低性能，尤其是在未剪辑的视频中",
        "方法": "开发基于学习的帧采样策略，使用多智能体强化学习（MARL）来解决帧采样问题",
        "关键词": [
            "帧采样",
            "多智能体强化学习",
            "未剪辑视频分类"
        ],
        "涉及的技术概念": "马尔可夫决策过程、多智能体强化学习（MARL）、基于RNN的上下文感知观察网络、策略网络、分类网络"
    },
    {
        "order": 612,
        "title": "Moving Indoor: Unsupervised Video Depth Learning in Challenging Environments",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Moving_Indoor_Unsupervised_Video_Depth_Learning_in_Challenging_Environments_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Moving_Indoor_Unsupervised_Video_Depth_Learning_in_Challenging_Environments_ICCV_2019_paper.html",
        "abstract": "Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",
        "中文标题": "移动室内：在挑战性环境中进行无监督视频深度学习",
        "摘要翻译": "最近，从视频中进行无监督深度学习的进展显著，其结果在户外场景如KITTI中与完全监督的方法相当。然而，当直接在室内环境中应用这项技术时，仍然存在巨大挑战，例如大面积的非纹理区域如白墙、手持相机更复杂的自我运动、透明玻璃和闪亮物体。为了克服这些问题，我们提出了一种新的基于光流的训练范式，通过提供更清晰的训练目标来减少无监督学习的难度，并处理非纹理区域。我们的实验评估表明，我们的方法的结果在NYU Depth V2基准测试中与完全监督的方法相当。据我们所知，这是在室内数据集上报告的第一个纯无监督学习方法的定量结果。",
        "领域": "深度估计/光流/无监督学习",
        "问题": "在室内环境中进行无监督视频深度学习的挑战",
        "动机": "解决在室内环境中应用无监督视频深度学习方法时遇到的大面积非纹理区域、复杂自我运动、透明和闪亮物体等问题",
        "方法": "提出了一种新的基于光流的训练范式，通过提供更清晰的训练目标来减少无监督学习的难度，并处理非纹理区域",
        "关键词": [
            "深度估计",
            "光流",
            "无监督学习",
            "室内环境",
            "非纹理区域"
        ],
        "涉及的技术概念": {
            "无监督学习": "一种机器学习方法，不需要标注数据来训练模型",
            "光流": "一种估计图像序列中物体运动的技术",
            "深度估计": "从图像或视频中估计场景中物体的距离或深度信息",
            "非纹理区域": "图像中缺乏明显纹理或细节的区域，如白墙",
            "NYU Depth V2": "一个广泛使用的室内场景深度估计基准数据集"
        }
    },
    {
        "order": 613,
        "title": "Deep Meta Functionals for Shape Representation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Littwin_Deep_Meta_Functionals_for_Shape_Representation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Littwin_Deep_Meta_Functionals_for_Shape_Representation_ICCV_2019_paper.html",
        "abstract": "We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network parametrized by these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code will be available at: https: //github.com/gidilittwin/Deep-Meta.",
        "中文标题": "深度元函数用于形状表示",
        "摘要翻译": "我们提出了一种从单张图像进行3D形状重建的新方法，其中深度神经网络直接将图像映射到网络权重的向量。由这些权重参数化的网络通过将体积中的每个点分类为形状内部或外部来表示3D形状。这种新的表示方法具有几乎无限的容量和分辨率，并且可以具有任意的拓扑结构。我们的实验表明，与现有的方法相比，包括基于体素、轮廓和网格的方法，它从2D投影中推断形状的准确性更高。代码将可在https://github.com/gidilittwin/Deep-Meta获取。",
        "领域": "3D重建/形状表示/深度学习",
        "问题": "从单张图像进行3D形状重建",
        "动机": "提高从2D投影推断3D形状的准确性",
        "方法": "使用深度神经网络将图像直接映射到网络权重的向量，通过分类体积中的每个点来表示3D形状",
        "关键词": [
            "3D重建",
            "形状表示",
            "深度神经网络"
        ],
        "涉及的技术概念": "深度神经网络用于直接从图像映射到网络权重的向量，通过分类体积中的每个点来表示3D形状，这种方法具有几乎无限的容量和分辨率，并且可以具有任意的拓扑结构。"
    },
    {
        "order": 614,
        "title": "Differentiable Kernel Evolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Differentiable_Kernel_Evolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Differentiable_Kernel_Evolution_ICCV_2019_paper.html",
        "abstract": "This paper proposes a differentiable kernel evolution (DKE) algorithm to find a better layer-operator for the convolutional neural network. Unlike most of the other neural architecture searching (NAS) technologies, we consider the searching space in a fundamental scope: kernel space, which encodes the assembly of basic multiply-accumulate (MAC) operations into a conv-kernel. We first deduce a strict form of the generalized convolutional operator by some necessary constraints and construct a continuous searching space for its extra freedom-of-degree, namely, the connection of each MAC. Then a novel unsupervised greedy evolution algorithm called gradient agreement guided searching (GAGS) is proposed to learn the optimal location for each MAC in the spatially continuous searching space. We leverage DKE on multiple kinds of tasks such as object classification, face/object detection, large-scale fine-grained and recognition, with various kinds of backbone architecture. Not to mention the consistent performance gain, we found the proposed DKE can further act as an auto-dilated operator, which makes it easy to boost the performance of miniaturized neural networks in multiple tasks.",
        "中文标题": "可微分核进化",
        "摘要翻译": "本文提出了一种可微分核进化（DKE）算法，旨在为卷积神经网络寻找更好的层操作符。与大多数其他神经架构搜索（NAS）技术不同，我们在一个基本范围内考虑搜索空间：核空间，它将基本乘加（MAC）操作的集合编码为卷积核。我们首先通过一些必要的约束推导出广义卷积算子的严格形式，并为其额外的自由度（即每个MAC的连接）构建一个连续的搜索空间。然后，提出了一种称为梯度一致性引导搜索（GAGS）的新型无监督贪婪进化算法，以学习每个MAC在空间连续搜索空间中的最佳位置。我们在多种任务上利用DKE，如对象分类、面部/对象检测、大规模细粒度和识别，以及各种骨干架构。除了性能的持续提升外，我们发现提出的DKE还可以作为自动扩张操作符，这使得在多种任务中提升小型化神经网络的性能变得容易。",
        "领域": "卷积神经网络/神经架构搜索/自动扩张操作符",
        "问题": "寻找卷积神经网络中更好的层操作符",
        "动机": "在卷积神经网络中，通过探索核空间来寻找更有效的层操作符，以提升网络性能",
        "方法": "提出了一种可微分核进化（DKE）算法，包括推导广义卷积算子的严格形式、构建连续搜索空间以及使用梯度一致性引导搜索（GAGS）算法学习最佳MAC位置",
        "关键词": [
            "可微分核进化",
            "卷积神经网络",
            "神经架构搜索",
            "自动扩张操作符"
        ],
        "涉及的技术概念": "可微分核进化（DKE）算法是一种旨在优化卷积神经网络层操作符的技术，通过探索核空间并利用梯度一致性引导搜索（GAGS）算法来学习最佳MAC位置，从而提升网络性能。此外，DKE还可以作为自动扩张操作符，用于提升小型化神经网络在多种任务中的表现。"
    },
    {
        "order": 615,
        "title": "SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight \"clip-sampling\" model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost.",
        "中文标题": "SCSampler：从视频中采样显著片段以进行高效动作识别",
        "摘要翻译": "虽然许多动作识别数据集由一系列简短的、经过剪辑的视频组成，每个视频都包含一个相关动作，但现实世界中的视频（例如，在YouTube上）表现出非常不同的特性：它们通常长达几分钟，其中简短的、相关的片段经常与包含很少变化的长时间段交替出现。将动作识别系统密集地应用于此类视频中的每个时间片段是非常昂贵的。此外，正如我们在实验中所展示的，这会导致识别准确率不理想，因为来自相关片段的有信息预测被长时间无信息视频片段的毫无意义的分类输出所淹没。在本文中，我们引入了一个轻量级的“片段采样”模型，可以有效地识别长视频中最显著的时间片段。我们证明，通过仅在这些最显著的片段上调用识别，可以显著降低未剪辑视频上动作识别的计算成本。此外，我们展示了与所有片段或随机选择片段的分析相比，这显著提高了识别准确率。在Sports1M上，我们的片段采样方案将已经是最先进的动作分类器的准确率提高了7%，并将其计算成本减少了15倍以上。",
        "领域": "动作识别/视频分析/计算效率",
        "问题": "在长视频中高效识别动作片段的问题",
        "动机": "提高动作识别的计算效率和准确率",
        "方法": "引入一个轻量级的“片段采样”模型，仅在最显著的片段上调用动作识别",
        "关键词": [
            "动作识别",
            "视频分析",
            "计算效率"
        ],
        "涉及的技术概念": "动作识别系统、片段采样模型、计算成本、识别准确率"
    },
    {
        "order": 616,
        "title": "GraphX-Convolution for Point Cloud Deformation in 2D-to-3D Conversion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen_GraphX-Convolution_for_Point_Cloud_Deformation_in_2D-to-3D_Conversion_ICCV_2019_paper.html",
        "abstract": "In this paper, we present a novel deep method to reconstruct a point cloud of an object from a single still image. Prior arts in the field struggle to reconstruct an accurate and scalable 3D model due to either the inefficient and expensive 3D representations, the dependency between the output and number of model parameters or the lack of a suitable computing operation. We propose to overcome these by deforming a random point cloud to the object shape through two steps: feature blending and deformation. In the first step, the global and point-specific shape features extracted from a 2D object image are blended with the encoded feature of a randomly generated point cloud, and then this mixture is sent to the deformation step to produce the final representative point set of the object. In the deformation process, we introduce a new layer termed as GraphX that considers the inter-relationship between points like common graph convolutions but operates on unordered sets. Moreover, with a simple trick, the proposed model can generate an arbitrary-sized point cloud, which is the first deep method to do so. Extensive experiments verify that we outperform existing models and halve the state-of-the-art distance score in single image 3D reconstruction.",
        "中文标题": "GraphX卷积在2D到3D转换中点云变形的应用",
        "摘要翻译": "在本文中，我们提出了一种新颖的深度方法，用于从单一静止图像重建物体的点云。该领域的先前技术由于低效且昂贵的3D表示、输出与模型参数数量之间的依赖关系或缺乏合适的计算操作，难以重建准确且可扩展的3D模型。我们提出通过两个步骤来克服这些问题：特征混合和变形。在第一步中，从2D物体图像中提取的全局和点特定形状特征与随机生成的点云的编码特征混合，然后将这种混合物发送到变形步骤以生成物体的最终代表性点集。在变形过程中，我们引入了一个称为GraphX的新层，该层考虑了像常见图卷积那样的点之间的相互关系，但在无序集上操作。此外，通过一个简单的技巧，所提出的模型可以生成任意大小的点云，这是第一个这样做的深度方法。大量实验验证了我们优于现有模型，并在单图像3D重建中将最先进的距离分数减半。",
        "领域": "点云处理/3D重建/深度学习",
        "问题": "从单一静止图像重建准确且可扩展的3D模型",
        "动机": "克服现有技术在3D表示效率、模型参数依赖和计算操作方面的限制",
        "方法": "通过特征混合和变形步骤，使用GraphX层考虑点间关系，在无序集上操作，生成任意大小的点云",
        "关键词": [
            "点云变形",
            "3D重建",
            "GraphX卷积"
        ],
        "涉及的技术概念": "GraphX层是一种新的计算层，它在处理点云数据时考虑了点之间的相互关系，类似于图卷积网络中的图卷积操作，但适用于无序的点集。这种方法允许模型从单一2D图像生成任意大小的3D点云，这在之前的深度学习方法中是无法实现的。"
    },
    {
        "order": 617,
        "title": "Weakly Supervised Energy-Based Learning for Action Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Weakly_Supervised_Energy-Based_Learning_for_Action_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Weakly_Supervised_Energy-Based_Learning_for_Action_Segmentation_ICCV_2019_paper.html",
        "abstract": "This paper is about labeling video frames with action classes under weak supervision in training, where we have access to a temporal ordering of actions, but their start and end frames in training videos are unknown. Following prior work, we use an HMM grounded on a Gated Recurrent Unit (GRU) for frame labeling. Our key contribution is a new constrained discriminative forward loss (CDFL) that we use for training the HMM and GRU under weak supervision. While prior work typically estimates the loss on a single, inferred video segmentation, our CDFL discriminates between the energy of all valid and invalid frame labelings of a training video. A valid frame labeling satisfies the ground-truth temporal ordering of actions, whereas an invalid one violates the ground truth. We specify an efficient recursive algorithm for computing the CDFL in terms of the logadd function of the segmentation energy. Our evaluation on action segmentation and alignment gives superior results to those of the state of the art on the benchmark Breakfast Action, Hollywood Extended, and 50Salads datasets.",
        "中文标题": "弱监督能量基学习用于动作分割",
        "摘要翻译": "本文讨论了在训练过程中，在弱监督条件下为视频帧标记动作类别的问题，其中我们可以访问动作的时间顺序，但在训练视频中动作的开始和结束帧是未知的。遵循先前的工作，我们使用基于门控循环单元（GRU）的隐马尔可夫模型（HMM）进行帧标记。我们的关键贡献是提出了一种新的约束判别前向损失（CDFL），用于在弱监督下训练HMM和GRU。虽然先前的工作通常估计单个推断的视频分割的损失，但我们的CDFL区分了训练视频所有有效和无效帧标记的能量。有效的帧标记满足动作的真实时间顺序，而无效的则违反真实情况。我们指定了一种有效的递归算法，用于根据分割能量的logadd函数计算CDFL。我们在动作分割和对齐上的评估在Breakfast Action、Hollywood Extended和50Salads数据集上给出了优于现有技术的结果。",
        "领域": "动作识别/视频分析/时间序列分析",
        "问题": "在弱监督条件下为视频帧标记动作类别",
        "动机": "解决在训练视频中动作的开始和结束帧未知的情况下，如何有效地标记视频帧的问题",
        "方法": "使用基于门控循环单元（GRU）的隐马尔可夫模型（HMM）进行帧标记，并提出了一种新的约束判别前向损失（CDFL）用于训练",
        "关键词": [
            "动作分割",
            "弱监督学习",
            "能量基学习"
        ],
        "涉及的技术概念": {
            "弱监督学习": "一种机器学习方法，其中训练数据的标签不完全或不精确",
            "隐马尔可夫模型（HMM）": "一种统计模型，用于描述一个含有隐含未知参数的马尔可夫过程",
            "门控循环单元（GRU）": "一种循环神经网络（RNN）的变体，用于处理序列数据",
            "约束判别前向损失（CDFL）": "一种新的损失函数，用于在弱监督下训练模型，通过区分有效和无效帧标记的能量来提高模型性能",
            "logadd函数": "一种数学函数，用于在计算分割能量时有效地处理数值"
        }
    },
    {
        "order": 618,
        "title": "Batch Weight for Domain Adaptation With Mass Shift",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Binkowski_Batch_Weight_for_Domain_Adaptation_With_Mass_Shift_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Binkowski_Batch_Weight_for_Domain_Adaptation_With_Mass_Shift_ICCV_2019_paper.html",
        "abstract": "Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks.",
        "中文标题": "批量权重用于质量转移的领域适应",
        "摘要翻译": "无监督领域转移是将样本从源分布转移到不同目标分布的任务。当前的无监督领域转移解决方案通常操作于分布模式匹配良好的数据上，例如源分布和目标分布之间具有相同的类别频率。然而，当模式不匹配时，这些模型表现不佳，就像从两个不同但相关的领域独立抽取样本时的情况。这种模式不平衡是有问题的，因为生成对抗网络（GANs）在这种设置中是一种成功的方法，但对模式频率敏感，这导致源样本和生成的目标分布样本之间的语义不匹配。我们提出了一种原则性的方法，通过重新加权训练样本来纠正转移分布之间的质量转移，我们称之为批量权重。我们还为领域转移提供了严格的概率设置，并为训练转移网络提供了新的简化目标，这是对当前最先进的图像到图像翻译模型中使用的复杂、多组件损失函数的替代。新目标源于联合分布的区分，并在抽象、高层次的而非像素级的意义上强制执行循环一致性。最后，我们通过实验展示了所提出方法在几个图像到图像翻译任务中的有效性。",
        "领域": "图像翻译/生成对抗网络/领域适应",
        "问题": "解决无监督领域转移中模式不平衡导致的语义不匹配问题",
        "动机": "当前的无监督领域转移方法在模式不匹配时表现不佳，需要一种新的方法来纠正分布之间的质量转移",
        "方法": "提出了一种通过重新加权训练样本来纠正质量转移的原则性方法，称为批量权重，并提供了新的简化目标来训练转移网络",
        "关键词": [
            "无监督领域转移",
            "生成对抗网络",
            "图像到图像翻译",
            "批量权重",
            "循环一致性"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种用于无监督领域转移的成功方法，但对模式频率敏感，可能导致源样本和生成的目标分布样本之间的语义不匹配。批量权重是一种通过重新加权训练样本来纠正转移分布之间质量转移的方法。循环一致性是在抽象、高层次的意义上强制执行的一种性质，用于确保领域转移的正确性。"
    },
    {
        "order": 619,
        "title": "Holistic++ Scene Understanding: Single-View 3D Holistic Scene Parsing and Human Pose Estimation With Human-Object Interaction and Physical Commonsense",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Holistic_Scene_Understanding_Single-View_3D_Holistic_Scene_Parsing_and_Human_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Holistic_Scene_Understanding_Single-View_3D_Holistic_Scene_Parsing_and_Human_ICCV_2019_paper.html",
        "abstract": "We propose a new 3D holistic++ scene understanding problem, which jointly tackles two tasks from a single-view image: (i) holistic scene parsing and reconstruction---3D estimations of object bounding boxes, camera pose, and room layout, and (ii) 3D human pose estimation. The intuition behind is to leverage the coupled nature of these two tasks to improve the granularity and performance of scene understanding. We propose to exploit two critical and essential connections between these two tasks: (i) human-object interaction (HOI) to model the fine-grained relations between agents and objects in the scene, and (ii) physical commonsense to model the physical plausibility of the reconstructed scene. The optimal configuration of the 3D scene, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable joint solution space. Experimental results demonstrate that the proposed algorithm significantly improves the performance of the two tasks on three datasets, showing an improved generalization ability.",
        "中文标题": "全面++场景理解：单视图3D全面场景解析与人体姿态估计，包含人-物交互和物理常识",
        "摘要翻译": "我们提出了一个新的3D全面++场景理解问题，该问题从单视图图像中联合解决两个任务：(i) 全面场景解析和重建---对象边界框、相机姿态和房间布局的3D估计，以及(ii) 3D人体姿态估计。背后的直觉是利用这两个任务的耦合性质来提高场景理解的粒度和性能。我们提出利用这两个任务之间的两个关键和本质的联系：(i) 人-物交互（HOI）来建模场景中代理和对象之间的细粒度关系，以及(ii) 物理常识来建模重建场景的物理合理性。3D场景的最优配置，由解析图表示，使用马尔可夫链蒙特卡洛（MCMC）推断，该方法有效地遍历了不可微分的联合解空间。实验结果表明，所提出的算法在三个数据集上显著提高了这两个任务的性能，显示出改进的泛化能力。",
        "领域": "3D场景理解/人体姿态估计/人-物交互",
        "问题": "从单视图图像中联合解决3D全面场景解析和重建以及3D人体姿态估计的问题",
        "动机": "利用3D全面场景解析和3D人体姿态估计这两个任务的耦合性质，提高场景理解的粒度和性能",
        "方法": "利用人-物交互（HOI）建模场景中代理和对象之间的细粒度关系，利用物理常识建模重建场景的物理合理性，使用马尔可夫链蒙特卡洛（MCMC）推断3D场景的最优配置",
        "关键词": [
            "3D场景理解",
            "人体姿态估计",
            "人-物交互",
            "物理常识",
            "马尔可夫链蒙特卡洛"
        ],
        "涉及的技术概念": "3D全面场景解析和重建涉及对象边界框、相机姿态和房间布局的3D估计；3D人体姿态估计涉及从单视图图像中估计人体的3D姿态；人-物交互（HOI）用于建模场景中代理和对象之间的细粒度关系；物理常识用于确保重建场景的物理合理性；马尔可夫链蒙特卡洛（MCMC）用于推断3D场景的最优配置。"
    },
    {
        "order": 620,
        "title": "SRM: A Style-Based Recalibration Module for Convolutional Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_SRM_A_Style-Based_Recalibration_Module_for_Convolutional_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_SRM_A_Style-Based_Recalibration_Module_for_Convolutional_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties.",
        "中文标题": "SRM：一种基于风格的重校准模块用于卷积神经网络",
        "摘要翻译": "随着卷积神经网络（CNNs）在风格迁移方面的进步，风格在CNNs中的作用从更广泛的角度引起了越来越多的关注。在本文中，我们旨在充分利用风格的潜力，以提高CNNs在一般视觉任务中的表现。我们提出了一种基于风格的重校准模块（SRM），这是一个简单但有效的架构单元，它通过利用特征图的风格来自适应地重校准中间特征图。SRM首先通过风格池化从特征图的每个通道中提取风格信息，然后通过通道独立的风格集成估计每个通道的重校准权重。通过将各个风格的相对重要性纳入特征图，SRM有效地增强了CNN的表示能力。所提出的模块可以直接嵌入现有的CNN架构中，且开销可忽略不计。我们在一般图像识别以及与风格相关的任务上进行了全面的实验，验证了SRM相对于最近的方法（如Squeeze-and-Excitation（SE））的优势。为了解释SRM和SE之间的固有差异，我们提供了它们表示属性的深入比较。",
        "领域": "风格迁移/图像识别/特征表示",
        "问题": "如何利用风格信息提高卷积神经网络在视觉任务中的表现",
        "动机": "充分利用风格的潜力，以提高卷积神经网络在一般视觉任务中的表现",
        "方法": "提出了一种基于风格的重校准模块（SRM），通过风格池化和通道独立的风格集成来重校准中间特征图",
        "关键词": [
            "风格迁移",
            "图像识别",
            "特征表示"
        ],
        "涉及的技术概念": "风格池化是一种从特征图的每个通道中提取风格信息的技术。通道独立的风格集成是一种估计每个通道重校准权重的方法。Squeeze-and-Excitation（SE）是一种通过显式建模通道间的依赖关系来增强网络表示能力的方法。"
    },
    {
        "order": 621,
        "title": "What Would You Expect? Anticipating Egocentric Actions With Rolling-Unrolling LSTMs and Modality Attention",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Furnari_What_Would_You_Expect_Anticipating_Egocentric_Actions_With_Rolling-Unrolling_LSTMs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Furnari_What_Would_You_Expect_Anticipating_Egocentric_Actions_With_Rolling-Unrolling_LSTMs_ICCV_2019_paper.html",
        "abstract": "Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see the project web page for code and additional details: http://iplab.dmi.unict.it/rulstm.",
        "中文标题": "你会期待什么？使用滚动-展开LSTM和模态注意力预测自我中心行为",
        "摘要翻译": "自我中心行为预测包括理解相机佩戴者将在不久的将来与哪些对象互动以及他们将执行哪些动作。我们通过提出一种能够在多个时间尺度上预测行为的架构来解决这个问题，该架构使用两个LSTM来1)总结过去，和2)制定关于未来的预测。输入视频通过考虑三种互补的模态进行处理：外观（RGB）、运动（光流）和对象（基于对象的特征）。使用一种新颖的模态注意力（MATT）机制融合特定模态的预测，该机制学会以自适应方式权衡模态。在两个大规模基准数据集上的广泛评估显示，我们的方法在包括超过2500个动作的挑战性EPIC-Kitchens数据集上优于现有技术高达+7%，并且能够推广到EGTEA Gaze+。我们的方法也被证明能够推广到早期行为识别和行为识别的任务。我们的方法在2019年EPIC-Kitchens自我中心行为预测挑战的公共排行榜上排名第一。请访问项目网页以获取代码和更多详情：http://iplab.dmi.unict.it/rulstm。",
        "领域": "行为预测/视频理解/注意力机制",
        "问题": "预测相机佩戴者将与之互动的对象及将执行的动作",
        "动机": "提高自我中心行为预测的准确性和效率，以更好地理解和预测人类行为",
        "方法": "使用两个LSTM架构总结过去并预测未来，结合RGB、光流和对象特征三种模态，通过模态注意力机制自适应地融合模态特定预测",
        "关键词": [
            "行为预测",
            "LSTM",
            "模态注意力",
            "视频理解",
            "自我中心视角"
        ],
        "涉及的技术概念": "LSTM（长短期记忆网络）用于时间序列数据的处理，模态注意力机制用于不同数据模态的融合，RGB、光流和对象特征作为视频处理的三种互补模态。"
    },
    {
        "order": 622,
        "title": "MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.html",
        "abstract": "Unlike vision modalities, body-worn sensors or passive sensing can avoid the failure of action understanding in vision related challenges, e.g. occlusion and appearance variation. However, a standard large-scale dataset does not exist, in which different types of modalities across vision and sensors are integrated. To address the disadvantage of vision-based modalities and push towards multi/cross modal action understanding, this paper introduces a new large-scale dataset recorded from 20 distinct subjects with seven different types of modalities: RGB videos, keypoints, acceleration, gyroscope, orientation, Wi-Fi and pressure signal. The dataset consists of more than 36k video clips for 37 action classes covering a wide range of daily life activities such as desktop-related and check-in-based ones in four different distinct scenarios. On the basis of our dataset, we propose a novel multi modality distillation model with attention mechanism to realize an adaptive knowledge transfer from sensor-based modalities to vision-based modalities. The proposed model significantly improves performance of action recognition compared to models trained with only RGB information. The experimental results confirm the effectiveness of our model on cross-subject, -view, -scene and -session evaluation criteria. We believe that this new large-scale multimodal dataset will contribute the community of multimodal based action understanding.",
        "中文标题": "MMAct: 用于跨模态人类动作理解的大规模数据集",
        "摘要翻译": "与视觉模态不同，佩戴在身体上的传感器或被动传感可以避免在视觉相关挑战中动作理解的失败，例如遮挡和外观变化。然而，目前尚不存在一个标准的大规模数据集，其中整合了跨越视觉和传感器的不同类型模态。为了解决基于视觉的模态的缺点并推动多/跨模态动作理解，本文引入了一个新的大规模数据集，该数据集记录了20个不同主体的七种不同类型模态：RGB视频、关键点、加速度、陀螺仪、方向、Wi-Fi和压力信号。该数据集包含超过36k个视频片段，涵盖37个动作类别，覆盖了四种不同场景中的广泛日常生活活动，如桌面相关和基于签到活动。基于我们的数据集，我们提出了一种新颖的多模态蒸馏模型，带有注意力机制，以实现从基于传感器的模态到基于视觉的模态的自适应知识转移。与仅使用RGB信息训练的模型相比，所提出的模型显著提高了动作识别的性能。实验结果证实了我们的模型在跨主体、视角、场景和会话评估标准上的有效性。我们相信，这个新的大规模多模态数据集将为基于多模态的动作理解社区做出贡献。",
        "领域": "动作识别/多模态学习/传感器融合",
        "问题": "缺乏整合视觉和传感器模态的大规模数据集，以及基于视觉的模态在动作理解中的局限性",
        "动机": "解决基于视觉的模态在动作理解中的局限性，推动多/跨模态动作理解的发展",
        "方法": "引入一个新的大规模多模态数据集，并提出一种带有注意力机制的多模态蒸馏模型，实现从传感器模态到视觉模态的自适应知识转移",
        "关键词": [
            "动作识别",
            "多模态学习",
            "传感器融合",
            "注意力机制",
            "知识转移"
        ],
        "涉及的技术概念": "RGB视频、关键点、加速度、陀螺仪、方向、Wi-Fi、压力信号、多模态蒸馏模型、注意力机制、自适应知识转移"
    },
    {
        "order": 623,
        "title": "Switchable Whitening for Deep Representation Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.html",
        "abstract": "Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset.",
        "中文标题": "可切换白化用于深度表示学习",
        "摘要翻译": "归一化方法是卷积神经网络（CNNs）中不可或缺的组成部分。它们使用在预定义的像素集中估计的统计数据来标准化或白化数据。与现有工作设计特定任务的归一化技术不同，我们提出了可切换白化（SW），它提供了一种统一不同白化方法以及标准化方法的通用形式。SW以端到端的方式学习在这些操作之间切换。它有几个优点。首先，SW为不同任务自适应地选择适当的白化或标准化统计数据（见图1），使其适用于广泛的任务而无需手动设计。其次，通过整合不同归一化器的优点，SW在各种具有挑战性的基准测试中显示出对其对应方法的持续改进。第三，SW作为理解白化和标准化技术特性的有用工具。我们展示了SW在图像分类（CIFAR-10/100，ImageNet）、语义分割（ADE20K，Cityscapes）、域适应（GTA5，Cityscapes）和图像风格转换（COCO）上优于其他替代方案。例如，在没有额外技巧的情况下，我们在ADE20K数据集上实现了45.33% mIoU的最先进性能。",
        "领域": "图像分类/语义分割/域适应",
        "问题": "如何设计一种通用的归一化方法，以适应不同的深度学习任务",
        "动机": "现有归一化技术通常针对特定任务设计，缺乏通用性和灵活性",
        "方法": "提出可切换白化（SW），一种统一不同白化方法和标准化方法的通用形式，能够自适应地选择适当的归一化策略",
        "关键词": [
            "归一化方法",
            "卷积神经网络",
            "白化",
            "标准化",
            "图像分类",
            "语义分割",
            "域适应",
            "图像风格转换"
        ],
        "涉及的技术概念": "归一化方法是指在卷积神经网络中用于标准化或白化数据的技术，目的是通过调整数据的分布来加速训练过程和提高模型性能。白化是一种特殊的归一化方法，旨在去除数据中的冗余信息，使得特征之间不相关且具有相同的方差。标准化则是将数据转换为均值为0，方差为1的分布。可切换白化（SW）是一种新的归一化技术，它能够根据任务的不同，自适应地选择最合适的归一化策略，从而提高模型在各种任务上的表现。"
    },
    {
        "order": 624,
        "title": "PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.html",
        "abstract": "Pedestrian behavior anticipation is a key challenge in the design of assistive and autonomous driving systems suitable for urban environments. An intelligent system should be able to understand the intentions or underlying motives of pedestrians and to predict their forthcoming actions. To date, only a few public datasets were proposed for the purpose of studying pedestrian behavior prediction in the context of intelligent driving. To this end, we propose a novel large-scale dataset designed for pedestrian intention estimation (PIE). We conducted a large-scale human experiment to establish human reference data for pedestrian intention in traffic scenes. We propose models for estimating pedestrian crossing intention and predicting their future trajectory. Our intention estimation model achieves 79% accuracy and our trajectory prediction algorithm outperforms state-of-the-art by 26% on the proposed dataset. We further show that combining pedestrian intention with observed motion improves trajectory prediction. The dataset and models are available at http://data.nvision2.eecs.yorku.ca/PIE_dataset/.",
        "中文标题": "PIE: 用于行人意图估计和轨迹预测的大规模数据集和模型",
        "摘要翻译": "行人行为预测是设计适用于城市环境的辅助和自动驾驶系统的关键挑战。一个智能系统应该能够理解行人的意图或潜在动机，并预测他们即将采取的行动。迄今为止，只有少数公开的数据集被提出用于研究智能驾驶背景下的行人行为预测。为此，我们提出了一个新颖的大规模数据集，专为行人意图估计（PIE）设计。我们进行了一项大规模的人类实验，以建立交通场景中行人意图的人类参考数据。我们提出了用于估计行人过街意图和预测他们未来轨迹的模型。我们的意图估计模型达到了79%的准确率，我们的轨迹预测算法在提出的数据集上比现有技术高出26%。我们进一步展示了将行人意图与观察到的运动相结合可以改善轨迹预测。数据集和模型可在http://data.nvision2.eecs.yorku.ca/PIE_dataset/获取。",
        "领域": "自动驾驶/行人行为分析/轨迹预测",
        "问题": "行人意图估计和轨迹预测",
        "动机": "设计适用于城市环境的辅助和自动驾驶系统需要准确预测行人行为",
        "方法": "提出一个大规模数据集PIE，并开发了用于估计行人过街意图和预测未来轨迹的模型",
        "关键词": [
            "行人意图估计",
            "轨迹预测",
            "自动驾驶"
        ],
        "涉及的技术概念": "行人意图估计模型、轨迹预测算法、大规模人类实验、交通场景分析"
    },
    {
        "order": 625,
        "title": "HACS: Human Action Clips and Segments Dataset for Recognition and Temporal Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_HACS_Human_Action_Clips_and_Segments_Dataset_for_Recognition_and_ICCV_2019_paper.html",
        "abstract": "This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments). We leverage consensus and disagreement among visual classifiers to automatically mine candidate short clips from unlabeled videos, which are subsequently validated by human annotators. The resulting dataset is dubbed HACS Clips. Through a separate process we also collect annotations defining action segment boundaries. This resulting dataset is called HACS Segments. Overall, HACS Clips consists of 1.5M annotated clips sampled from 504K untrimmed videos, and HACS Segments contains 139K action segments densely annotated in 50K untrimmed videos spanning 200 action categories. HACS Clips contains more labeled examples than any existing video benchmark. This renders our dataset both a large-scale action recognition benchmark and an excellent source for spatiotemporal feature learning. In our transfer learning experiments on three target datasets, HACS Clips outperforms Kinetics-600, Moments-In-Time and Sports1M as a pretraining source. On HACS Segments, we evaluate state-of-the-art methods of action proposal generation and action localization, and highlight the new challenges posed by our dense temporal annotations.",
        "中文标题": "HACS：用于识别和时间定位的人类动作片段和片段数据集",
        "摘要翻译": "本文介绍了一个新的大规模数据集，用于从网络视频中收集的人类动作的识别和时间定位。我们称之为HACS（人类动作片段和片段）。我们利用视觉分类器之间的共识和分歧，从未标记的视频中自动挖掘候选短片，随后由人类注释者验证。由此产生的数据集被称为HACS片段。通过一个单独的过程，我们还收集了定义动作片段边界的注释。这个结果数据集被称为HACS片段。总体而言，HACS片段由从504K未修剪视频中采样的1.5M注释片段组成，HACS片段包含在50K未修剪视频中密集注释的139K动作片段，跨越200个动作类别。HACS片段包含比任何现有视频基准更多的标记示例。这使得我们的数据集既是一个大规模动作识别基准，也是一个优秀的时空特征学习来源。在我们对三个目标数据集的转移学习实验中，HACS片段作为预训练源优于Kinetics-600、Moments-In-Time和Sports1M。在HACS片段上，我们评估了动作提案生成和动作定位的最先进方法，并强调了我们的密集时间注释带来的新挑战。",
        "领域": "动作识别/时间定位/时空特征学习",
        "问题": "如何从大量未标记的网络视频中自动挖掘并验证人类动作片段，以及如何定义动作片段边界",
        "动机": "为了创建一个大规模的人类动作识别和时间定位数据集，以支持动作识别和时空特征学习的研究",
        "方法": "利用视觉分类器之间的共识和分歧自动挖掘候选短片，并由人类注释者验证；通过单独的过程收集动作片段边界的注释",
        "关键词": [
            "动作识别",
            "时间定位",
            "时空特征学习"
        ],
        "涉及的技术概念": "HACS数据集包括HACS片段和HACS片段两个部分，分别包含从大量未修剪视频中自动挖掘并验证的1.5M注释片段和139K动作片段，跨越200个动作类别。该数据集旨在支持动作识别和时空特征学习的研究，通过转移学习实验证明了其作为预训练源的有效性。"
    },
    {
        "order": 626,
        "title": "Adaptative Inference Cost With Convolutional Neural Mixture Models",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ruiz_Adaptative_Inference_Cost_With_Convolutional_Neural_Mixture_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ruiz_Adaptative_Inference_Cost_With_Convolutional_Neural_Mixture_Models_ICCV_2019_paper.html",
        "abstract": "Despite the outstanding performance of convolutional neural networks (CNNs) for many vision tasks, the required computational cost during inference is problematic when resources are limited. In this context, we propose Convolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a large number of CNNs that can be jointly trained and evaluated in an efficient manner. Within the proposed framework, we present different mechanisms to prune subsets of CNNs from the mixture, allowing to easily adapt the computational cost required for inference. Image classification and semantic segmentation experiments show that our method achieve excellent accuracy-compute trade-offs. Moreover, unlike most of previous approaches, a single CNMM provides a large range of operating points along this trade-off, without any re-training.",
        "中文标题": "自适应推理成本的卷积神经混合模型",
        "摘要翻译": "尽管卷积神经网络（CNNs）在许多视觉任务中表现出色，但在资源有限的情况下，推理过程中所需的计算成本是一个问题。在此背景下，我们提出了卷积神经混合模型（CNMMs），这是一种概率模型，嵌入了大量可以联合训练和高效评估的CNNs。在提出的框架内，我们展示了从混合模型中剪枝CNNs子集的不同机制，使得推理所需的计算成本能够轻松适应。图像分类和语义分割实验表明，我们的方法实现了优秀的精度-计算权衡。此外，与大多数先前的方法不同，单个CNMM提供了沿着这一权衡的大量操作点，而无需任何重新训练。",
        "领域": "图像分类/语义分割/模型优化",
        "问题": "在资源有限的情况下，卷积神经网络推理过程中所需的计算成本问题",
        "动机": "为了解决卷积神经网络在资源有限环境下的高计算成本问题，提出一种能够自适应调整推理成本的模型",
        "方法": "提出卷积神经混合模型（CNMMs），通过嵌入大量CNNs并联合训练，采用剪枝机制自适应调整推理成本",
        "关键词": [
            "卷积神经网络",
            "模型剪枝",
            "计算成本优化"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNNs）": "一种深度学习模型，特别适用于处理图像数据，通过卷积层提取特征",
            "卷积神经混合模型（CNMMs）": "一种概率模型，包含多个卷积神经网络，能够联合训练和评估，以适应不同的计算需求",
            "模型剪枝": "一种减少模型复杂度和计算成本的技术，通过移除对模型性能影响较小的部分来实现"
        }
    },
    {
        "order": 627,
        "title": "3C-Net: Category Count and Center Loss for Weakly-Supervised Action Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Narayan_3C-Net_Category_Count_and_Center_Loss_for_Weakly-Supervised_Action_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Narayan_3C-Net_Category_Count_and_Center_Loss_for_Weakly-Supervised_Action_Localization_ICCV_2019_paper.html",
        "abstract": "Temporal action localization is a challenging computer vision problem with numerous real-world applications. Most existing methods require laborious frame-level supervision to train action localization models. In this work, we propose a framework, called 3C-Net, which only requires video-level supervision (weak supervision) in the form of action category labels and the corresponding count. We introduce a novel formulation to learn discriminative action features with enhanced localization capabilities. Our joint formulation has three terms: a classification term to ensure the separability of learned action features, an adapted multi-label center loss term to enhance the action feature discriminability and a counting loss term to delineate adjacent action sequences, leading to improved localization. Comprehensive experiments are performed on two challenging benchmarks: THUMOS14 and ActivityNet 1.2. Our approach sets a new state-of-the-art for weakly-supervised temporal action localization on both datasets. On the THUMOS14 dataset, the proposed method achieves an absolute gain of 4.6% in terms of mean average precision (mAP), compared to the state-of-the-art. Source code is available at https://github.com/naraysa/3c-net.",
        "中文标题": "3C-Net：用于弱监督动作定位的类别计数和中心损失",
        "摘要翻译": "时间动作定位是一个具有众多实际应用场景的挑战性计算机视觉问题。大多数现有方法需要繁琐的帧级监督来训练动作定位模型。在这项工作中，我们提出了一个名为3C-Net的框架，该框架仅需要以动作类别标签和相应计数的形式进行视频级监督（弱监督）。我们引入了一种新颖的公式来学习具有增强定位能力的区分性动作特征。我们的联合公式包含三个部分：一个分类项以确保学习到的动作特征的可分离性，一个适应的多标签中心损失项以增强动作特征的区分能力，以及一个计数损失项以描绘相邻动作序列，从而改善定位。在两个具有挑战性的基准测试：THUMOS14和ActivityNet 1.2上进行了全面的实验。我们的方法在这两个数据集上为弱监督时间动作定位设定了新的最先进水平。在THUMOS14数据集上，与最先进的方法相比，所提出的方法在平均精度（mAP）方面实现了4.6%的绝对增益。源代码可在https://github.com/naraysa/3c-net获取。",
        "领域": "动作识别/视频分析/弱监督学习",
        "问题": "时间动作定位",
        "动机": "减少对帧级监督的依赖，仅使用视频级监督（弱监督）来训练动作定位模型",
        "方法": "提出了3C-Net框架，通过分类项、适应的多标签中心损失项和计数损失项联合学习区分性动作特征，以增强动作定位能力",
        "关键词": [
            "时间动作定位",
            "弱监督学习",
            "动作特征学习"
        ],
        "涉及的技术概念": {
            "时间动作定位": "指在视频中确定动作发生的具体时间范围",
            "弱监督学习": "一种机器学习方法，其中训练数据的标签信息不完全，例如仅知道视频中发生了哪些动作，而不知道具体发生的时间",
            "动作特征学习": "通过学习视频中的特征来识别和定位动作",
            "多标签中心损失": "一种损失函数，用于增强特征的区分能力，特别是在多标签分类问题中",
            "计数损失": "一种损失函数，用于处理视频中动作的计数问题，帮助区分相邻的动作序列"
        }
    },
    {
        "order": 628,
        "title": "STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_STGAT_Modeling_Spatial-Temporal_Interactions_for_Human_Trajectory_Prediction_ICCV_2019_paper.html",
        "abstract": "Human trajectory prediction is challenging and critical in various applications (e.g., autonomous vehicles and social robots). Because of the continuity and foresight of the pedestrian movements, the moving pedestrians in crowded spaces will consider both spatial and temporal interactions to avoid future collisions. However, most of the existing methods ignore the temporal correlations of interactions with other pedestrians involved in a scene. In this work, we propose a Spatial-Temporal Graph Attention network (STGAT), based on a sequence-to-sequence architecture to predict future trajectories of pedestrians. Besides the spatial interactions captured by the graph attention mechanism at each time-step, we adopt an extra LSTM to encode the temporal correlations of interactions. Through comparisons with state-of-the-art methods, our model achieves superior performance on two publicly available crowd datasets (ETH and UCY) and produces more \"socially\" plausible trajectories for pedestrians.",
        "中文标题": "STGAT: 为人类轨迹预测建模空间-时间交互",
        "摘要翻译": "人类轨迹预测在各种应用（例如，自动驾驶车辆和社交机器人）中既具有挑战性又至关重要。由于行人运动的连续性和前瞻性，拥挤空间中的移动行人会考虑空间和时间交互以避免未来的碰撞。然而，大多数现有方法忽略了与场景中其他行人交互的时间相关性。在这项工作中，我们提出了一种基于序列到序列架构的空间-时间图注意力网络（STGAT），以预测行人的未来轨迹。除了通过图注意力机制在每个时间步捕获的空间交互外，我们还采用了一个额外的LSTM来编码交互的时间相关性。通过与最先进方法的比较，我们的模型在两个公开的人群数据集（ETH和UCY）上实现了卓越的性能，并为行人产生了更“社交”合理的轨迹。",
        "领域": "行人轨迹预测/自动驾驶/社交机器人",
        "问题": "预测行人在拥挤空间中的未来轨迹",
        "动机": "现有方法忽略了与场景中其他行人交互的时间相关性，导致预测的轨迹不够准确",
        "方法": "提出了一种基于序列到序列架构的空间-时间图注意力网络（STGAT），通过图注意力机制捕获空间交互，并采用LSTM编码时间相关性",
        "关键词": [
            "行人轨迹预测",
            "空间-时间交互",
            "图注意力网络",
            "LSTM"
        ],
        "涉及的技术概念": {
            "空间-时间图注意力网络（STGAT）": "一种基于序列到序列架构的网络，用于预测行人的未来轨迹，通过图注意力机制捕获空间交互，并采用LSTM编码时间相关性",
            "图注意力机制": "一种机制，用于在每个时间步捕获行人之间的空间交互",
            "LSTM": "长短期记忆网络，用于编码行人交互的时间相关性"
        }
    },
    {
        "order": 629,
        "title": "On Network Design Spaces for Visual Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Radosavovic_On_Network_Design_Spaces_for_Visual_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Radosavovic_On_Network_Design_Spaces_for_Visual_Recognition_ICCV_2019_paper.html",
        "abstract": "Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.",
        "中文标题": "关于视觉识别网络设计空间的探讨",
        "摘要翻译": "过去几年中，在设计更好的视觉识别神经网络架构方面取得了显著进展。为了帮助维持这一进展速度，我们在这项工作中提出重新审视比较网络架构的方法。特别是，我们引入了一种新的分布估计比较范式，其中通过应用统计技术对采样模型的群体进行比较网络设计空间，同时控制网络复杂性等混杂因素。与当前比较模型家族点和曲线估计的方法相比，分布估计提供了整个设计景观的更完整图景。作为案例研究，我们检查了用于神经架构搜索（NAS）的设计空间。我们发现最近NAS设计空间变体之间存在显著的统计差异，这些差异在很大程度上被忽视了。此外，我们的分析表明，像ResNeXt这样的标准模型家族的设计空间可以与最近NAS工作中使用的更复杂的设计空间相媲美。我们希望这些关于分布分析的见解将有助于在发现更好的视觉识别网络方面取得更稳健的进展。",
        "领域": "神经架构搜索/视觉识别/网络设计",
        "问题": "如何更有效地比较和评估不同神经网络架构的设计空间",
        "动机": "为了维持视觉识别神经网络架构设计的进展速度，需要一种更全面的方法来比较不同的网络设计空间",
        "方法": "引入了一种新的分布估计比较范式，通过应用统计技术对采样模型的群体进行比较，同时控制网络复杂性等混杂因素",
        "关键词": [
            "神经架构搜索",
            "视觉识别",
            "网络设计空间",
            "分布估计",
            "统计技术"
        ],
        "涉及的技术概念": "神经架构搜索（NAS）是一种自动化设计神经网络架构的方法，旨在发现最优的网络结构。分布估计是一种统计方法，用于估计和比较不同模型或设计空间的性能分布。ResNeXt是一种改进的卷积神经网络架构，通过增加网络的宽度来提高性能。"
    },
    {
        "order": 630,
        "title": "Learning Motion in Feature Space: Locally-Consistent Deformable Convolution Networks for Fine-Grained Action Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mac_Learning_Motion_in_Feature_Space_Locally-Consistent_Deformable_Convolution_Networks_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mac_Learning_Motion_in_Feature_Space_Locally-Consistent_Deformable_Convolution_Networks_for_ICCV_2019_paper.html",
        "abstract": "Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.",
        "中文标题": "在特征空间中学习运动：局部一致可变形卷积网络用于细粒度动作检测",
        "摘要翻译": "细粒度动作检测是一项重要任务，在机器人和人机交互中有广泛应用。现有方法通常采用两阶段方法，包括提取局部时空特征，然后进行时间建模以捕捉长期依赖关系。虽然最近的论文大多关注后者（长时间建模），但在这里，我们专注于更有效地建模细粒度运动的特征。我们提出了一种新颖的局部一致可变形卷积，它利用感受野的变化并实施局部一致性约束，以有效捕捉运动信息。我们的模型联合学习时空特征（而不是使用独立的空间和时间流）。时间组件是从特征空间而不是像素空间（例如光流）学习的。生成的特征可以灵活地与其他长时间建模网络（例如ST-CNN、DilatedTCN和ED-TCN）结合使用。总体而言，我们提出的方法在两个细粒度动作数据集上稳健地优于原始长时间模型：50 Salads和GTEA，分别实现了80.22%和75.39%的F1分数。",
        "领域": "动作识别/视频分析/卷积神经网络",
        "问题": "细粒度动作检测",
        "动机": "提高细粒度动作检测的效率和准确性",
        "方法": "提出局部一致可变形卷积，联合学习时空特征，从特征空间学习时间组件",
        "关键词": [
            "细粒度动作检测",
            "局部一致可变形卷积",
            "时空特征学习"
        ],
        "涉及的技术概念": {
            "局部一致可变形卷积": "一种新颖的卷积方法，通过改变感受野并实施局部一致性约束来捕捉运动信息",
            "时空特征学习": "模型联合学习空间和时间特征，而不是独立处理",
            "特征空间": "时间组件从特征空间学习，而不是传统的像素空间（如光流）",
            "长时间建模网络": "如ST-CNN、DilatedTCN和ED-TCN，用于捕捉长期依赖关系"
        }
    },
    {
        "order": 631,
        "title": "Grounded Human-Object Interaction Hotspots From Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nagarajan_Grounded_Human-Object_Interaction_Hotspots_From_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nagarajan_Grounded_Human-Object_Interaction_Hotspots_From_Video_ICCV_2019_paper.html",
        "abstract": "Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction \"hotspots\" directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating where an object would be manipulated in a potential interaction even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories. Project page: http://vision.cs.utexas.edu/projects/interaction-hotspots/",
        "中文标题": "从视频中学习基于真实人类-物体交互的热点",
        "摘要翻译": "学习如何与物体交互是实现具身视觉智能的重要一步，但现有技术受到繁重的监督或感知需求的限制。我们提出了一种直接从视频中学习人类-物体交互“热点”的方法。与将可供性视为手动监督的语义分割任务不同，我们的方法通过观看真实人类行为的视频并预测可供性动作来学习交互。给定一个新的图像或视频，我们的模型推断出一个空间热点图，指示在潜在交互中物体将被操纵的位置，即使物体当前处于静止状态。通过第一人称和第三人称视频的结果，我们展示了在真实人类-物体交互中基于可供性的价值。我们的弱监督热点不仅与强监督的可供性方法相竞争，而且还能预测新物体类别的物体交互。项目页面：http://vision.cs.utexas.edu/projects/interaction-hotspots/",
        "领域": "具身视觉智能/人类-物体交互/视频理解",
        "问题": "如何从视频中学习人类与物体的交互热点，减少对繁重监督或感知需求的依赖",
        "动机": "实现具身视觉智能需要理解人类如何与物体交互，现有方法需要大量监督或复杂的感知设备，限制了其应用范围和效率",
        "方法": "提出一种直接从视频中学习人类-物体交互热点的方法，通过观看真实人类行为的视频并预测可供性动作，推断出在潜在交互中物体将被操纵的空间热点图",
        "关键词": [
            "具身视觉智能",
            "人类-物体交互",
            "视频理解",
            "可供性预测",
            "空间热点图"
        ],
        "涉及的技术概念": {
            "可供性": "指物体提供的潜在交互可能性，如一个杯子可供抓握",
            "语义分割": "一种图像处理技术，用于将图像分割成多个区域，每个区域对应一个特定的类别",
            "弱监督学习": "一种机器学习方法，使用不完全或间接的监督信息来训练模型",
            "空间热点图": "一种表示图像或视频中特定区域重要性的视觉表示，用于指示潜在的交互位置"
        }
    },
    {
        "order": 632,
        "title": "Improved Techniques for Training Adaptive Deep Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Improved_Techniques_for_Training_Adaptive_Deep_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Improved_Techniques_for_Training_Adaptive_Deep_Networks_ICCV_2019_paper.html",
        "abstract": "Adaptive inference is a promising technique to improve the computational efficiency of deep models at test time. In contrast to static models which use the same computation graph for all instances, adaptive networks can dynamically adjust their structure conditioned on each input. While existing research on adaptive inference mainly focuses on designing more advanced architectures, this paper investigates how to train such networks more effectively. Specifically, we consider a typical adaptive deep network with multiple intermediate classifiers. We present three techniques to improve its training efficacy from two aspects: 1) a Gradient Equilibrium algorithm to resolve the conflict of learning of different classifiers; 2) an Inline Subnetwork Collaboration approach and a One-for-all Knowledge Distillation algorithm to enhance the collaboration among classifiers. On multiple datasets (CIFAR-10, CIFAR-100 and ImageNet), we show that the proposed approach consistently leads to further improved efficiency on top of state-of-the-art adaptive deep networks.",
        "中文标题": "改进训练自适应深度网络的技术",
        "摘要翻译": "自适应推理是一种有前景的技术，可以在测试时提高深度模型的计算效率。与对所有实例使用相同计算图的静态模型不同，自适应网络可以根据每个输入动态调整其结构。虽然现有的自适应推理研究主要集中在设计更先进的架构上，但本文研究了如何更有效地训练这些网络。具体来说，我们考虑了一个具有多个中间分类器的典型自适应深度网络。我们提出了三种技术，从两个方面提高其训练效率：1）一种梯度均衡算法，以解决不同分类器学习之间的冲突；2）一种内联子网络协作方法和一种全合一知识蒸馏算法，以增强分类器之间的协作。在多个数据集（CIFAR-10、CIFAR-100和ImageNet）上，我们展示了所提出的方法在现有最先进的自适应深度网络基础上，持续带来进一步的效率提升。",
        "领域": "自适应网络/深度学习优化/计算效率",
        "问题": "如何更有效地训练自适应深度网络",
        "动机": "提高自适应深度网络在测试时的计算效率",
        "方法": "提出了梯度均衡算法、内联子网络协作方法和全合一知识蒸馏算法",
        "关键词": [
            "自适应推理",
            "计算效率",
            "梯度均衡",
            "知识蒸馏"
        ],
        "涉及的技术概念": "自适应推理允许网络根据输入动态调整结构，提高计算效率。梯度均衡算法解决不同分类器学习冲突，内联子网络协作方法和全合一知识蒸馏算法增强分类器间协作。"
    },
    {
        "order": 633,
        "title": "Hallucinating IDT Descriptors and I3D Optical Flow Features for Action Recognition With CNNs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Hallucinating_IDT_Descriptors_and_I3D_Optical_Flow_Features_for_Action_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Hallucinating_IDT_Descriptors_and_I3D_Optical_Flow_Features_for_Action_ICCV_2019_paper.html",
        "abstract": "In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.",
        "中文标题": "使用CNN进行动作识别的IDT描述符和I3D光流特征的幻觉",
        "摘要翻译": "在本文中，我们复兴了使用传统手工制作的视频表示进行动作识别的方法，并通过基于CNN的幻觉步骤为这些技术注入新的生命。尽管使用了RGB和光流帧，I3D模型（以及其他模型）通过将其输出与改进的密集轨迹（IDT）结合，并通过Bag-of-Words（BoW）和Fisher Vectors（FV）编码其低级别视频描述符而蓬勃发展。这种CNN和手工制作的表示的融合由于预处理、描述符提取、编码和调参而耗时。因此，我们提出了一个端到端可训练的网络流，它在训练阶段学习基于IDT的BoW/FV表示，并且易于与I3D模型集成。具体来说，每个流在最后一个1D卷积层之前获取I3D特征图，并学习将这些图“翻译”成BoW/FV表示。因此，我们的模型可以在测试阶段幻觉并使用这种合成的BoW/FV表示。我们展示了即使是整个I3D光流流的特征也可以被幻觉，从而简化了流程。我们的模型节省了20-55小时的计算时间，并在四个公开可用的数据集上取得了最先进的结果。",
        "领域": "动作识别/视频分析/特征提取",
        "问题": "如何有效地结合CNN和传统手工制作的视频表示进行动作识别",
        "动机": "传统的手工制作视频表示方法虽然有效，但与CNN结合使用时存在预处理和参数调优耗时的问题",
        "方法": "提出一个端到端可训练的网络流，通过学习将I3D特征图翻译成BoW/FV表示，从而在测试阶段幻觉并使用这些表示",
        "关键词": [
            "动作识别",
            "视频表示",
            "特征提取",
            "CNN",
            "BoW",
            "FV",
            "I3D",
            "IDT"
        ],
        "涉及的技术概念": {
            "CNN": "卷积神经网络，用于图像和视频的特征提取和分类",
            "I3D": "Inflated 3D ConvNet，一种用于视频动作识别的深度学习模型",
            "IDT": "改进的密集轨迹，一种用于视频分析的特征提取方法",
            "BoW": "Bag-of-Words，一种用于图像和视频特征表示的方法",
            "FV": "Fisher Vectors，一种用于图像和视频特征编码的方法"
        }
    },
    {
        "order": 634,
        "title": "Dual Attention Matching for Audio-Visual Event Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Dual_Attention_Matching_for_Audio-Visual_Event_Localization_ICCV_2019_paper.html",
        "abstract": "In this paper, we investigate the audio-visual event localization problem. This task is to localize a visible and audible event in a video. Previous methods first divide a video into short segments, and then fuse visual and acoustic features at the segment level. The duration of these segments is usually short, making the visual and acoustic feature of each segment possibly not well aligned. Direct concatenation of the two features at the segment level can be vulnerable to a minor temporal misalignment of the two signals. We propose a Dual Attention Matching (DAM) module to cover a longer video duration for better high-level event information modeling, while the local temporal information is attained by the global cross-check mechanism. Our premise is that one should watch the whole video to understand the high-level event, while shorter segments should be checked in detail for localization. Specifically, the global feature of one modality queries the local feature in the other modality in a bi-directional way. With temporal co-occurrence encoded between auditory and visual signals, DAM can be readily applied in various audio-visual event localization tasks, e.g., cross-modality localization, supervised event localization. Experiments on the AVE dataset show our method outperforms the state-of-the-art by a large margin.",
        "中文标题": "双注意力匹配用于音视频事件定位",
        "摘要翻译": "本文研究了音视频事件定位问题。该任务旨在定位视频中可见且可听的事件。先前的方法首先将视频分割成短片段，然后在片段级别融合视觉和声学特征。这些片段的持续时间通常较短，使得每个片段的视觉和声学特征可能没有很好地对齐。在片段级别直接连接这两种特征可能对两种信号的微小时间错位敏感。我们提出了一个双注意力匹配（DAM）模块，以覆盖更长的视频持续时间，以便更好地建模高级事件信息，而局部时间信息则通过全局交叉检查机制获得。我们的前提是，应该观看整个视频以理解高级事件，而较短的片段应该详细检查以进行定位。具体来说，一种模态的全局特征以双向方式查询另一种模态的局部特征。通过编码听觉和视觉信号之间的时间共现，DAM可以轻松应用于各种音视频事件定位任务，例如跨模态定位、监督事件定位。在AVE数据集上的实验表明，我们的方法大幅优于现有技术。",
        "领域": "音视频事件定位/跨模态学习/时间序列分析",
        "问题": "音视频事件定位中的时间对齐问题",
        "动机": "解决视频片段中视觉和声学特征可能未对齐的问题，以提高音视频事件定位的准确性",
        "方法": "提出双注意力匹配（DAM）模块，通过全局交叉检查机制覆盖更长的视频持续时间，同时获取局部时间信息",
        "关键词": [
            "音视频事件定位",
            "双注意力匹配",
            "时间对齐",
            "跨模态学习"
        ],
        "涉及的技术概念": {
            "双注意力匹配（DAM）": "一种模块，用于在音视频事件定位中，通过全局特征查询局部特征的方式，解决视觉和声学特征的时间对齐问题",
            "全局交叉检查机制": "一种机制，用于在DAM模块中，通过全局特征与局部特征的交互，获取更准确的时间信息",
            "时间共现": "指听觉和视觉信号在时间上的同时发生，是音视频事件定位中的一个重要概念"
        }
    },
    {
        "order": 635,
        "title": "Uncertainty-Aware Audiovisual Activity Recognition Using Deep Bayesian Variational Inference",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Subedar_Uncertainty-Aware_Audiovisual_Activity_Recognition_Using_Deep_Bayesian_Variational_Inference_ICCV_2019_paper.html",
        "abstract": "Deep neural networks (DNNs) provide state-of-the-art results for a multitude of applications, but the approaches using DNNs for multimodal audiovisual applications do not consider predictive uncertainty associated with individual modalities. Bayesian deep learning methods provide principled confidence and quantify predictive uncertainty. Our contribution in this work is to propose an uncertainty aware multimodal Bayesian fusion framework for activity recognition. We demonstrate a novel approach that combines deterministic and variational layers to scale Bayesian DNNs to deeper architectures. Our experiments using in- and out-of-distribution samples selected from a subset of Moments-in-Time (MiT) dataset show a more reliable confidence measure as compared to the non-Bayesian baseline and the Monte Carlo dropout (MC dropout) approximate Bayesian inference. We also demonstrate the uncertainty estimates obtained from the proposed framework can identify out-of-distribution data on the UCF101 and MiT datasets. In the multimodal setting, the proposed framework improved precision-recall AUC by 10.2% on the subset of MiT dataset as compared to non-Bayesian baseline.",
        "中文标题": "使用深度贝叶斯变分推理的不确定性感知视听活动识别",
        "摘要翻译": "深度神经网络（DNNs）在众多应用中提供了最先进的结果，但使用DNNs进行多模态视听应用的方法并未考虑与单个模态相关的预测不确定性。贝叶斯深度学习方法提供了原则性的置信度和量化预测不确定性。我们在这项工作中的贡献是提出了一个不确定性感知的多模态贝叶斯融合框架用于活动识别。我们展示了一种新颖的方法，该方法结合了确定性和变分层，以将贝叶斯DNNs扩展到更深层次的架构。我们使用从Moments-in-Time（MiT）数据集的子集中选择的分布内和分布外样本进行的实验显示，与非贝叶斯基线和蒙特卡罗dropout（MC dropout）近似贝叶斯推理相比，具有更可靠的置信度测量。我们还展示了从提出的框架中获得的估计不确定性可以识别UCF101和MiT数据集上的分布外数据。在多模态设置中，与非贝叶斯基线相比，提出的框架在MiT数据集的子集上提高了精确召回AUC 10.2%。",
        "领域": "活动识别/多模态学习/不确定性估计",
        "问题": "多模态视听应用中的预测不确定性未得到充分考虑",
        "动机": "提高活动识别中预测的可靠性和置信度",
        "方法": "提出了一种结合确定性和变分层的深度贝叶斯变分推理方法，用于多模态贝叶斯融合框架",
        "关键词": [
            "活动识别",
            "多模态学习",
            "不确定性估计",
            "贝叶斯深度学习",
            "变分推理"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）、贝叶斯深度学习、变分推理、蒙特卡罗dropout（MC dropout）、精确召回AUC"
    },
    {
        "order": 636,
        "title": "Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_Resource_Constrained_Neural_Network_Architecture_Search_Will_a_Submodularity_Assumption_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiong_Resource_Constrained_Neural_Network_Architecture_Search_Will_a_Submodularity_Assumption_ICCV_2019_paper.html",
        "abstract": "The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.",
        "中文标题": "资源受限的神经网络架构搜索：子模假设会有帮助吗？",
        "摘要翻译": "神经网络架构的设计通常基于人类专家的试错和经验反馈，或者通过在不同离散架构选择上进行的大规模强化学习策略来解决。在后一种情况下，优化通常是非可微的，也不太适合无导数优化方法。目前使用的大多数方法都需要大量的计算资源。如果我们希望网络还满足资源约束，上述挑战会更加严峻，因为搜索现在必须在准确性和资源的某些预算约束之间取得平衡。我们将这个问题表述为集合函数的优化——我们发现这个集合函数的经验行为经常（但不总是）满足边际增益和单调性原则——这些是子模性概念的核心属性。基于这一观察，我们调整了离散优化中的算法，以获得神经网络架构搜索的启发式方案，其中我们对架构有资源约束。当这个简单方案应用于CIFAR-100和ImageNet时，它识别出了资源受限的架构，其性能明显优于当前为移动设备设计的最先进模型。具体来说，我们通过一种更快的搜索方法找到了具有更少参数和计算的高性能架构。",
        "领域": "神经网络架构搜索/资源优化/子模优化",
        "问题": "在资源受限的情况下，如何高效地搜索出性能优越的神经网络架构",
        "动机": "现有的神经网络架构搜索方法需要大量计算资源，且难以在资源约束下平衡准确性和资源使用",
        "方法": "将问题表述为集合函数的优化，利用子模性原理调整离散优化算法，提出启发式搜索方案",
        "关键词": [
            "神经网络架构搜索",
            "资源优化",
            "子模优化"
        ],
        "涉及的技术概念": "集合函数优化、子模性（边际增益和单调性原则）、离散优化算法、启发式搜索方案"
    },
    {
        "order": 637,
        "title": "Learning to Paint With Model-Based Deep Reinforcement Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_to_Paint_With_Model-Based_Deep_Reinforcement_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Learning_to_Paint_With_Model-Based_Deep_Reinforcement_Learning_ICCV_2019_paper.html",
        "abstract": "We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.",
        "中文标题": "学习使用基于模型的深度强化学习绘画",
        "摘要翻译": "我们展示了如何教机器像人类画家一样绘画，他们可以使用少量的笔触来创作出奇妙的画作。通过在基于模型的深度强化学习（DRL）中采用神经渲染器，我们的代理学会确定每个笔触的位置和颜色，并制定长期计划将纹理丰富的图像分解为笔触。实验证明，使用数百个笔触可以实现出色的视觉效果。训练过程不需要人类画家的经验或笔触跟踪数据。代码可在https://github.com/hzwer/ICCV2019-LearningToPaint获取。",
        "领域": "艺术生成/强化学习/神经渲染",
        "问题": "如何使机器能够使用少量笔触创作出具有丰富纹理的图像",
        "动机": "探索机器是否能够模仿人类画家的绘画技巧，使用有限的笔触创作出视觉上吸引人的艺术作品",
        "方法": "采用基于模型的深度强化学习框架，结合神经渲染器来学习笔触的位置和颜色，以及如何将复杂图像分解为一系列笔触",
        "关键词": [
            "艺术生成",
            "强化学习",
            "神经渲染",
            "笔触分解"
        ],
        "涉及的技术概念": "深度强化学习（DRL）是一种结合了深度学习和强化学习的技术，用于训练代理在复杂环境中做出决策。神经渲染器是一种使用神经网络来生成或修改图像的技术，能够模拟复杂的视觉效果。"
    },
    {
        "order": 638,
        "title": "Non-Local Recurrent Neural Memory for Supervised Sequence Modeling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Non-Local_Recurrent_Neural_Memory_for_Supervised_Sequence_Modeling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Non-Local_Recurrent_Neural_Memory_for_Supervised_Sequence_Modeling_ICCV_2019_paper.html",
        "abstract": "Typical methods for supervised sequence modeling are built upon the recurrent neural networks to capture temporal dependencies. One potential limitation of these methods is that they only model explicitly information interactions between adjacent time steps in a sequence, hence the high-order interactions between nonadjacent time steps are not fully exploited. It greatly limits the capability of modeling the long-range temporal dependencies since one-order interactions cannot be maintained for a long term due to information dilution and gradient vanishing. To tackle this limitation, we propose the Non-local Recurrent Neural Memory (NRNM) for supervised sequence modeling, which performs non-local operations to learn full-order interactions within a sliding temporal block and models the global interactions between blocks in a gated recurrent manner. Consequently, our model is able to capture the long-range dependencies. Besides, the latent high-level features contained in high-order interactions can be distilled by our model. We demonstrate the merits of our NRNM approach on two different tasks: action recognition and sentiment analysis.",
        "中文标题": "非局部循环神经记忆用于监督序列建模",
        "摘要翻译": "典型的监督序列建模方法建立在循环神经网络之上，以捕捉时间依赖性。这些方法的一个潜在限制是，它们仅明确地建模序列中相邻时间步之间的信息交互，因此非相邻时间步之间的高阶交互未得到充分利用。这极大地限制了建模长程时间依赖性的能力，因为由于信息稀释和梯度消失，一阶交互无法长期维持。为了解决这一限制，我们提出了用于监督序列建模的非局部循环神经记忆（NRNM），它执行非局部操作以学习滑动时间块内的全阶交互，并以门控循环方式建模块之间的全局交互。因此，我们的模型能够捕捉长程依赖性。此外，我们的模型可以提取高阶交互中包含的潜在高级特征。我们在两个不同的任务上展示了我们的NRNM方法的优点：动作识别和情感分析。",
        "领域": "动作识别/情感分析/序列建模",
        "问题": "如何有效捕捉序列中的长程时间依赖性",
        "动机": "现有方法仅能建模相邻时间步之间的信息交互，无法充分利用非相邻时间步之间的高阶交互，限制了模型捕捉长程时间依赖性的能力。",
        "方法": "提出了非局部循环神经记忆（NRNM），通过执行非局部操作学习滑动时间块内的全阶交互，并以门控循环方式建模块之间的全局交互。",
        "关键词": [
            "非局部循环神经记忆",
            "长程依赖性",
            "动作识别",
            "情感分析"
        ],
        "涉及的技术概念": "非局部操作：一种能够捕捉序列中非相邻时间步之间交互的技术；门控循环方式：一种通过门控机制控制信息流动的循环神经网络结构，用于建模全局交互。"
    },
    {
        "order": 639,
        "title": "ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_ACNet_Strengthening_the_Kernel_Skeletons_for_Powerful_CNN_via_Asymmetric_ICCV_2019_paper.html",
        "abstract": "As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels.",
        "中文标题": "ACNet：通过非对称卷积块增强CNN的核骨架",
        "摘要翻译": "在设计适用于特定应用的卷积神经网络（CNN）架构时，通常需要大量的人工工作或GPU时间，研究界正在寻求架构中立的CNN结构，这些结构可以轻松插入多个成熟的架构中，以提高我们在现实世界应用中的性能。我们提出了非对称卷积块（ACB），作为一种CNN构建块的架构中立结构，它使用一维非对称卷积来增强方形卷积核。对于一个现成的架构，我们用ACB替换标准的方形核卷积层，构建一个非对称卷积网络（ACNet），该网络可以被训练以达到更高的准确度。训练后，我们将ACNet等效转换为相同的原始架构，因此不再需要额外的计算。我们已经观察到，ACNet可以显著提高各种模型在CIFAR和ImageNet上的性能。通过进一步的实验，我们将ACB的有效性归因于其增强模型对旋转失真的鲁棒性和加强方形卷积核中心骨架部分的能力。",
        "领域": "卷积神经网络/图像识别/模型优化",
        "问题": "如何设计一种架构中立的CNN结构，以提高现有模型在图像识别任务上的性能",
        "动机": "减少设计CNN架构时的人工工作和GPU时间，同时提高模型在现实世界应用中的性能",
        "方法": "提出非对称卷积块（ACB），使用一维非对称卷积增强方形卷积核，构建非对称卷积网络（ACNet），并在训练后将其转换为原始架构",
        "关键词": [
            "非对称卷积",
            "卷积神经网络",
            "模型优化"
        ],
        "涉及的技术概念": "非对称卷积块（ACB）是一种CNN构建块，通过使用一维非对称卷积来增强方形卷积核，从而提高模型的性能。ACNet是通过用ACB替换标准卷积层构建的网络，训练后可以转换为原始架构，无需额外计算。这种方法增强了模型对旋转失真的鲁棒性，并加强了卷积核的中心骨架部分。"
    },
    {
        "order": 640,
        "title": "Neural Re-Simulation for Generating Bounces in Single Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Innamorati_Neural_Re-Simulation_for_Generating_Bounces_in_Single_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Innamorati_Neural_Re-Simulation_for_Generating_Bounces_in_Single_Images_ICCV_2019_paper.html",
        "abstract": "We introduce a method to generate videos of dynamic virtual objects plausibly interacting via collisions with a still image's environment. Given a starting trajectory, physically simulated with the estimated geometry of a single, static input image, we learn to 'correct' this trajectory to a visually plausible one via a neural network. The neural network can then be seen as learning to 'correct' traditional simulation output, generated with incomplete and imprecise world information, to obtain context-specific, visually plausible re-simulated output - a process we call neural re-simulation. We train our system on a set of 50k synthetic scenes where a virtual moving object (ball) has been physically simulated. We demonstrate our approach on both our synthetic dataset and a collection of real-life images depicting everyday scenes, obtaining consistent improvement over baseline alternatives throughout.",
        "中文标题": "神经重模拟：在单张图像中生成弹跳效果",
        "摘要翻译": "我们介绍了一种方法，用于生成动态虚拟对象与静止图像环境通过碰撞进行合理交互的视频。给定一个起始轨迹，该轨迹是通过估计单张静态输入图像的几何形状进行物理模拟的，我们学习通过神经网络将这个轨迹“修正”为视觉上合理的轨迹。然后，神经网络可以被视为学习“修正”传统模拟输出，这些输出是在不完整和不精确的世界信息下生成的，以获得特定于上下文的、视觉上合理的重模拟输出——我们称这一过程为神经重模拟。我们在一个包含50k个合成场景的数据集上训练我们的系统，其中虚拟移动对象（球）已经进行了物理模拟。我们在我们的合成数据集和一系列描绘日常场景的真实图像上展示了我们的方法，在整个过程中获得了相对于基线替代方案的持续改进。",
        "领域": "虚拟现实/物理模拟/神经网络",
        "问题": "如何在单张静态图像中生成动态虚拟对象与环境的合理交互视频",
        "动机": "为了在视觉上合理地将动态虚拟对象与静态图像环境进行交互，需要一种方法来修正基于不完整和不精确世界信息生成的物理模拟轨迹。",
        "方法": "通过神经网络学习修正传统物理模拟输出的轨迹，以获得特定于上下文的、视觉上合理的重模拟输出。",
        "关键词": [
            "虚拟现实",
            "物理模拟",
            "神经网络",
            "视觉修正",
            "动态对象"
        ],
        "涉及的技术概念": "神经重模拟是一种通过神经网络修正传统物理模拟输出的过程，旨在生成视觉上合理的动态虚拟对象与静态图像环境的交互视频。"
    },
    {
        "order": 641,
        "title": "Temporal Attentive Alignment for Large-Scale Video Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.html",
        "abstract": "Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over \"Source only\" from 73.9% to 81.8% on \"HMDB --> UCF\", and 10.3% gain on \"Kinetics --> Gameplay\"). The code and data are released at http://github.com/cmhungsteve/TA3N.",
        "中文标题": "大规模视频领域适应的时间注意力对齐",
        "摘要翻译": "尽管近年来提出了各种基于图像的领域适应（DA）技术，但视频中的领域转移仍未得到充分探索。大多数先前的工作仅评估在小规模数据集上的性能，这些数据集已经饱和。因此，我们首先提出了两个具有更大领域差异的大规模视频DA数据集：UCF-HMDB_full和Kinetics-Gameplay。其次，我们研究了视频的不同DA集成方法，并表明即使没有复杂的DA方法，同时对齐和学习时间动态也能实现有效的对齐。最后，我们提出了时间注意力对抗适应网络（TA3N），它明确地关注使用领域差异的时间动态，以实现更有效的领域对齐，在四个视频DA数据集上实现了最先进的性能（例如，在“HMDB --> UCF”上，准确率从73.9%提高到81.8%，提高了7.9%，在“Kinetics --> Gameplay”上提高了10.3%）。代码和数据已在http://github.com/cmhungsteve/TA3N发布。",
        "领域": "视频分析/领域适应/时间序列分析",
        "问题": "视频领域适应中的领域转移问题",
        "动机": "探索视频领域适应技术，解决现有小规模数据集性能饱和的问题",
        "方法": "提出时间注意力对抗适应网络（TA3N），通过关注时间动态和领域差异实现有效对齐",
        "关键词": [
            "视频领域适应",
            "时间注意力",
            "对抗网络"
        ],
        "涉及的技术概念": "领域适应（DA）技术、时间动态对齐、对抗适应网络（TA3N）、大规模视频数据集"
    },
    {
        "order": 642,
        "title": "A Comprehensive Overhaul of Feature Distillation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Heo_A_Comprehensive_Overhaul_of_Feature_Distillation_ICCV_2019_paper.html",
        "abstract": "We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.",
        "中文标题": "特征蒸馏的全面改革",
        "摘要翻译": "我们研究了实现网络压缩的特征蒸馏方法的设计方面，并提出了一种新颖的特征蒸馏方法，其中蒸馏损失被设计为使教师变换、学生变换、蒸馏特征位置和距离函数之间产生协同效应。我们提出的蒸馏损失包括一个带有新设计的margin ReLU的特征变换、一个新的蒸馏特征位置和一个部分L2距离函数，以跳过对学生压缩产生不利影响的冗余信息。在ImageNet上，我们提出的方法使用ResNet50实现了21.65%的top-1错误率，优于教师网络ResNet152的性能。我们提出的方法在图像分类、目标检测和语义分割等各种任务上进行了评估，并在所有任务中实现了显著的性能提升。代码可在项目页面获取。",
        "领域": "网络压缩/特征蒸馏/图像分类",
        "问题": "如何设计一种有效的特征蒸馏方法以实现网络压缩",
        "动机": "探索特征蒸馏方法的设计，以实现网络压缩并提升性能",
        "方法": "提出了一种新颖的特征蒸馏方法，包括带有margin ReLU的特征变换、新的蒸馏特征位置和部分L2距离函数",
        "关键词": [
            "网络压缩",
            "特征蒸馏",
            "图像分类",
            "目标检测",
            "语义分割"
        ],
        "涉及的技术概念": {
            "特征蒸馏": "一种技术，通过将知识从较大的教师网络转移到较小的学生网络来实现网络压缩",
            "margin ReLU": "一种新设计的激活函数，用于特征变换",
            "部分L2距离函数": "一种距离函数，用于跳过对学生压缩产生不利影响的冗余信息",
            "ResNet50/ResNet152": "深度残差网络，用于图像分类等任务"
        }
    },
    {
        "order": 643,
        "title": "Deep Appearance Maps",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Maximov_Deep_Appearance_Maps_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Maximov_Deep_Appearance_Maps_ICCV_2019_paper.html",
        "abstract": "We propose a deep representation of appearance, i.e. the relation of color, surface orientation, viewer position, material and illumination. Previous approaches have used deep learning to extract classic appearance representations relating to reflectance model parameters (e.g. Phong) or illumination (e.g. HDR environment maps). We suggest to directly represent appearance itself as a network we call a deep appearance map (DAM). This is a 4D generalization over 2D reflectance maps, which held the view direction fixed. First, we show how a DAM can be learned from images or video frames and later be used to synthesize appearance, given new surface orientations and viewer positions. Second, we demonstrate how another network can be used to map from an image or video frames to a DAM network to reproduce this appearance, without using a lengthy optimization such as stochastic gradient descent (learning-to-learn). Finally, we show the example of an appearance estimation-and-segmentation task, mapping from an image showing multiple materials to multiple deep appearance maps.",
        "中文标题": "深度外观映射",
        "摘要翻译": "我们提出了一种深度的外观表示方法，即颜色、表面方向、观察者位置、材质和光照之间的关系。以往的方法使用深度学习来提取与反射模型参数（如Phong模型）或光照（如HDR环境映射）相关的经典外观表示。我们建议直接将外观本身表示为一个网络，我们称之为深度外观映射（DAM）。这是对2D反射映射的4D泛化，其中视图方向是固定的。首先，我们展示了如何从图像或视频帧中学习DAM，并在给定新的表面方向和观察者位置时用于合成外观。其次，我们展示了如何使用另一个网络从图像或视频帧映射到DAM网络以重现此外观，而无需使用如随机梯度下降（学习到学习）等耗时的优化过程。最后，我们展示了一个外观估计和分割任务的例子，从显示多种材质的图像映射到多个深度外观映射。",
        "领域": "外观建模/材质识别/光照估计",
        "问题": "如何直接从图像或视频中学习并合成外观，以及如何高效地从图像映射到外观表示",
        "动机": "为了更直接和高效地表示和合成外观，避免传统方法中耗时的优化过程",
        "方法": "提出深度外观映射（DAM）作为外观的直接表示，并展示如何从图像或视频中学习DAM以及如何映射图像到DAM以重现外观",
        "关键词": [
            "外观建模",
            "材质识别",
            "光照估计",
            "深度外观映射",
            "图像合成"
        ],
        "涉及的技术概念": "深度外观映射（DAM）是一种4D泛化的外观表示方法，它扩展了传统的2D反射映射，允许视图方向变化。通过深度学习技术，DAM可以从图像或视频中学习，并用于合成新的外观。此外，通过另一个网络，可以直接从图像映射到DAM，实现外观的高效重现，避免了传统优化方法的耗时过程。"
    },
    {
        "order": 644,
        "title": "Action Assessment by Joint Relation Graphs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Action_Assessment_by_Joint_Relation_Graphs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Action_Assessment_by_Joint_Relation_Graphs_ICCV_2019_paper.html",
        "abstract": "We present a new model to assess the performance of actions from videos, through graph-based joint relation modelling. Previous works mainly focused on the whole scene including the performer's body and background, yet they ignored the detailed joint interactions. This is insufficient for fine-grained, accurate action assessment, because the action quality of each joint is dependent of its neighbouring joints. Therefore, we propose to learn the detailed joint motion based on the joint relations. We build trainable Joint Relation Graphs, and analyze joint motion on them. We propose two novel modules, the Joint Commonality Module and the Joint Difference Module, for joint motion learning. The Joint Commonality Module models the general motion for certain body parts, and the Joint Difference Module models the motion differences within body parts. We evaluate our method on six public Olympic actions for performance assessment. Our method outperforms previous approaches (+0.0912) and the whole-scene analysis (+0.0623) in the Spearman's Rank Correlation. We also demonstrate our model's ability to interpret the action assessment process.",
        "中文标题": "通过联合关系图进行动作评估",
        "摘要翻译": "我们提出了一个新模型，通过基于图的联合关系建模来评估视频中的动作表现。以往的工作主要集中在包括表演者身体和背景在内的整个场景，却忽略了详细的关节互动。这对于细粒度、准确的动作评估是不够的，因为每个关节的动作质量都依赖于其邻近关节。因此，我们提出基于关节关系学习详细的关节运动。我们构建了可训练的联合关系图，并在其上分析关节运动。我们提出了两个新模块，即关节共性模块和关节差异模块，用于关节运动学习。关节共性模块模拟某些身体部位的一般运动，而关节差异模块模拟身体部位内的运动差异。我们在六个公共奥林匹克动作上评估了我们的方法以进行表现评估。我们的方法在斯皮尔曼等级相关系数上优于以前的方法（+0.0912）和整个场景分析（+0.0623）。我们还展示了我们模型解释动作评估过程的能力。",
        "领域": "动作识别/运动分析/视频分析",
        "问题": "如何从视频中准确评估动作表现",
        "动机": "以往的方法忽略了关节间的详细互动，这对于细粒度和准确的动作评估是不够的",
        "方法": "构建可训练的联合关系图，并提出关节共性模块和关节差异模块来学习关节运动",
        "关键词": [
            "动作评估",
            "联合关系图",
            "关节运动学习"
        ],
        "涉及的技术概念": "联合关系图是一种基于图的结构，用于建模和分析关节间的运动关系。关节共性模块和关节差异模块是专门设计用于学习关节运动的两个模块，前者模拟身体部位的一般运动，后者模拟身体部位内的运动差异。斯皮尔曼等级相关系数是一种用于评估两个变量之间相关性的非参数统计方法。"
    },
    {
        "order": 645,
        "title": "Transferable Semi-Supervised 3D Object Detection From RGB-D Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tang_Transferable_Semi-Supervised_3D_Object_Detection_From_RGB-D_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tang_Transferable_Semi-Supervised_3D_Object_Detection_From_RGB-D_Data_ICCV_2019_paper.html",
        "abstract": "We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.",
        "中文标题": "可转移的半监督3D目标检测从RGB-D数据",
        "摘要翻译": "我们研究了从仅有的新类别2D边界框标签中训练3D目标检测器以检测新类别对象的方向，同时从现有类别的3D边界框标签中转移信息。为此，我们提出了一种可转移的半监督3D目标检测模型，该模型从具有两个不相交对象类别集的训练数据中学习3D目标检测网络——一组具有2D和3D框标签的强类别，以及另一组仅具有2D框标签的弱类别。特别是，我们提出了一种宽松的重投影损失、框先验损失和框到点云拟合网络，这些允许我们在训练期间有效地将有用的3D信息从强类别转移到弱类别，从而使得网络在推理期间能够检测弱类别中的3D对象。实验结果表明，我们提出的算法在SUN-RGBD和KITTI数据集上优于基线方法，并与全监督方法相比取得了有希望的结果。此外，我们展示了我们的框到点云拟合网络提高了全监督方法在两个数据集上的性能。",
        "领域": "3D目标检测/半监督学习/信息转移",
        "问题": "如何从仅有的新类别2D边界框标签中训练3D目标检测器，同时利用现有类别的3D边界框标签进行信息转移。",
        "动机": "为了在缺乏大量3D标注数据的情况下，提高3D目标检测的准确性和效率，通过半监督学习的方式，利用现有类别的3D信息来辅助新类别的检测。",
        "方法": "提出了一种可转移的半监督3D目标检测模型，包括宽松的重投影损失、框先验损失和框到点云拟合网络，以有效地从强类别向弱类别转移3D信息。",
        "关键词": [
            "3D目标检测",
            "半监督学习",
            "信息转移",
            "重投影损失",
            "框先验损失",
            "框到点云拟合网络"
        ],
        "涉及的技术概念": {
            "3D目标检测": "从图像或点云中检测和定位三维空间中的对象。",
            "半监督学习": "一种机器学习方法，它使用少量标注数据和大量未标注数据进行训练。",
            "信息转移": "从一个任务或类别中学习到的知识被应用到另一个相关任务或类别中。",
            "重投影损失": "一种损失函数，用于衡量3D预测与2D投影之间的一致性。",
            "框先验损失": "一种损失函数，用于确保预测的3D边界框符合物理世界的先验知识。",
            "框到点云拟合网络": "一种网络结构，用于将预测的3D边界框与点云数据进行拟合，以提高检测的准确性。"
        }
    },
    {
        "order": 646,
        "title": "GarNet: A Two-Stream Network for Fast and Accurate 3D Cloth Draping",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gundogdu_GarNet_A_Two-Stream_Network_for_Fast_and_Accurate_3D_Cloth_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gundogdu_GarNet_A_Two-Stream_Network_for_Fast_and_Accurate_3D_Cloth_ICCV_2019_paper.html",
        "abstract": "While Physics-Based Simulation (PBS) can accurately drape a 3D garment on a 3D body, it remains too costly for real-time applications, such as virtual try-on. By contrast, inference in a deep network, requiring a single forward pass, is much faster. Taking advantage of this, we propose a novel architecture to fit a 3D garment template to a 3D body. Specifically, we build upon the recent progress in 3D point cloud processing with deep networks to extract garment features at varying levels of detail, including point-wise, patch-wise and global features. We fuse these features with those extracted in parallel from the 3D body, so as to model the cloth-body interactions. The resulting two-stream architecture, which we call as GarNet, is trained using a loss function inspired by physics-based modeling, and delivers visually plausible garment shapes whose 3D points are, on average, less than 1 cm away from those of a PBS method, while running 100 times faster. Moreover, the proposed method can model various garment types with different cutting patterns when parameters of those patterns are given as input to the network.",
        "中文标题": "GarNet：一种用于快速准确3D布料悬挂的双流网络",
        "摘要翻译": "虽然基于物理的模拟（PBS）可以准确地将3D服装悬挂在3D身体上，但对于实时应用（如虚拟试穿）来说，它仍然过于昂贵。相比之下，深度网络中的推理只需要一次前向传递，速度要快得多。利用这一点，我们提出了一种新颖的架构，用于将3D服装模板适配到3D身体上。具体来说，我们基于最近在3D点云处理方面的进展，使用深度网络提取不同细节级别的服装特征，包括点级、块级和全局特征。我们将这些特征与从3D身体并行提取的特征融合，以模拟布料与身体的交互。我们称之为GarNet的最终双流架构，使用受基于物理建模启发的损失函数进行训练，并提供视觉上合理的服装形状，其3D点平均距离PBS方法的点不到1厘米，同时运行速度快100倍。此外，当这些图案的参数作为网络输入时，所提出的方法可以模拟具有不同裁剪图案的各种服装类型。",
        "领域": "3D建模/虚拟现实/服装设计",
        "问题": "实时3D布料悬挂在3D身体上的准确性和速度问题",
        "动机": "提高虚拟试穿等实时应用中3D布料悬挂的效率和准确性",
        "方法": "提出一种新颖的双流网络架构GarNet，利用深度网络提取和融合3D服装和身体的特征，以模拟布料与身体的交互，并通过受基于物理建模启发的损失函数进行训练",
        "关键词": [
            "3D建模",
            "虚拟现实",
            "服装设计",
            "双流网络",
            "3D点云处理"
        ],
        "涉及的技术概念": "基于物理的模拟（PBS）、深度网络、3D点云处理、点级特征、块级特征、全局特征、布料与身体交互、双流网络架构、损失函数"
    },
    {
        "order": 647,
        "title": "DPOD: 6D Pose Object Detector and Refiner",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.html",
        "abstract": "In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.",
        "中文标题": "DPOD: 6D姿态物体检测器与优化器",
        "摘要翻译": "本文提出了一种新颖的深度学习方法，用于从RGB图像中进行3D物体检测和6D姿态估计。我们的方法名为DPOD（密集姿态物体检测器），它估计输入图像与可用3D模型之间的密集多类2D-3D对应图。给定这些对应关系，通过PnP和RANSAC计算6自由度姿态。使用定制的基于深度学习的优化方案对初始姿态估计进行额外的RGB姿态优化。我们的结果和与大量相关工作的比较表明，大量的对应关系对于在优化前后获得高质量的6D姿态是有益的。与其他主要使用真实数据进行训练且不在合成渲染上训练的方法不同，我们在合成和真实训练数据上都进行了评估，展示了与所有最近检测器相比在优化前后的优越结果。尽管精确，所提出的方法仍然能够实时运行。",
        "领域": "3D物体检测/6D姿态估计/深度学习",
        "问题": "从RGB图像中进行3D物体检测和6D姿态估计",
        "动机": "提高6D姿态估计的精确度和实时性，通过大量对应关系优化姿态估计结果",
        "方法": "使用DPOD方法估计密集多类2D-3D对应图，通过PnP和RANSAC计算6自由度姿态，并采用深度学习方案进行RGB姿态优化",
        "关键词": [
            "3D物体检测",
            "6D姿态估计",
            "深度学习",
            "PnP",
            "RANSAC",
            "RGB姿态优化"
        ],
        "涉及的技术概念": {
            "DPOD": "密集姿态物体检测器，用于估计输入图像与3D模型之间的密集多类2D-3D对应图",
            "PnP": "Perspective-n-Point，一种用于从2D图像点估计3D物体姿态的算法",
            "RANSAC": "随机抽样一致算法，用于从一组包含异常值的数据中估计数学模型参数",
            "6自由度姿态": "物体在三维空间中的位置和方向，包括三个平移自由度和三个旋转自由度",
            "RGB姿态优化": "使用RGB图像信息对初始姿态估计进行进一步优化的过程"
        }
    },
    {
        "order": 648,
        "title": "Unsupervised Procedure Learning via Joint Dynamic Summarization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Elhamifar_Unsupervised_Procedure_Learning_via_Joint_Dynamic_Summarization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Elhamifar_Unsupervised_Procedure_Learning_via_Joint_Dynamic_Summarization_ICCV_2019_paper.html",
        "abstract": "We address the problem of unsupervised procedure learning from unconstrained instructional videos. Our goal is to produce a summary of the procedure key-steps and their ordering needed to perform a given task, as well as localization of the key-steps in videos. We develop a collaborative sequential subset selection framework, where we build a dynamic model on videos by learning states and transitions between them, where states correspond to different subactivities, including background and procedure steps. To extract procedure key-steps, we develop an optimization framework that finds a sequence of a small number of states that well represents all videos and is compatible with the state transition model. Given that our proposed optimization is non-convex and NP-hard, we develop a fast greedy algorithm whose complexity is linear in the length of the videos and the number of states of the dynamic model, hence, scales to large datasets. Under appropriate conditions on the transition model, our proposed formulation is approximately submodular, hence, comes with performance guarantees. We also present ProceL, a new multimodal dataset of 47.3 hours of videos and their transcripts from diverse tasks, for procedure learning evaluation. By extensive experiments, we show that our framework significantly improves the state of the art performance.",
        "中文标题": "通过联合动态摘要的无监督程序学习",
        "摘要翻译": "我们解决了从无约束的教学视频中进行无监督程序学习的问题。我们的目标是生成执行给定任务所需的关键步骤及其顺序的摘要，以及视频中关键步骤的定位。我们开发了一个协作顺序子集选择框架，在该框架中，我们通过学习状态和它们之间的转换来构建视频的动态模型，其中状态对应于不同的子活动，包括背景和程序步骤。为了提取程序关键步骤，我们开发了一个优化框架，该框架找到了一小部分状态的序列，这些状态很好地代表了所有视频，并且与状态转换模型兼容。鉴于我们提出的优化是非凸且NP难的，我们开发了一种快速的贪婪算法，其复杂度与视频长度和动态模型的状态数量成线性关系，因此可以扩展到大型数据集。在转换模型的适当条件下，我们提出的公式是近似子模的，因此具有性能保证。我们还介绍了ProceL，一个新的多模态数据集，包含47.3小时的视频及其来自不同任务的转录本，用于程序学习评估。通过大量实验，我们展示了我们的框架显著提高了最先进的性能。",
        "领域": "视频理解/程序学习/动态模型",
        "问题": "从无约束的教学视频中无监督学习程序关键步骤及其顺序，并定位视频中的关键步骤",
        "动机": "为了生成执行给定任务所需的关键步骤及其顺序的摘要，以及视频中关键步骤的定位",
        "方法": "开发了一个协作顺序子集选择框架，通过学习状态和它们之间的转换来构建视频的动态模型，并开发了一个优化框架来提取程序关键步骤",
        "关键词": [
            "无监督学习",
            "程序学习",
            "视频摘要",
            "动态模型",
            "优化框架"
        ],
        "涉及的技术概念": {
            "无监督学习": "不依赖于标注数据的学习方法",
            "程序学习": "从教学视频中学习执行任务的步骤",
            "视频摘要": "生成视频内容的简短概述",
            "动态模型": "描述系统状态随时间变化的模型",
            "优化框架": "用于寻找最优解的数学框架",
            "子模性": "一种数学性质，用于保证优化问题的近似解的质量"
        }
    },
    {
        "order": 649,
        "title": "Joint Embedding of 3D Scan and CAD Objects",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dahnert_Joint_Embedding_of_3D_Scan_and_CAD_Objects_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dahnert_Joint_Embedding_of_3D_Scan_and_CAD_Objects_ICCV_2019_paper.html",
        "abstract": "3D scan geometry and CAD models often contain complementary information towards understanding environments, which could be leveraged through establishing a mapping between the two domains. However, this is a challenging task due to strong, lower-level differences between scan and CAD geometry. We propose a novel approach to learn a joint embedding space between scan and CAD geometry, where semantically similar objects from both domains lie close together. To achieve this, we introduce a new 3D CNN-based approach to learn a joint embedding space representing object similarities across these domains. To learn a shared space where scan objects and CAD models can interlace, we propose a stacked hourglass approach to separate foreground and background from a scan object, and transform it to a complete, CAD-like representation to produce a shared embedding space. This embedding space can then be used for CAD model retrieval; to further enable this task, we introduce a new dataset of ranked scan-CAD similarity annotations, enabling new, fine-grained evaluation of CAD model retrieval to cluttered, noisy, partial scans. Our learned joint embedding outperforms current state of the art for CAD model retrieval by 12% in instance retrieval accuracy.",
        "中文标题": "3D扫描与CAD对象的联合嵌入",
        "摘要翻译": "3D扫描几何和CAD模型通常包含理解环境的互补信息，这可以通过在两个领域之间建立映射来利用。然而，由于扫描和CAD几何之间的强烈、低级差异，这是一项具有挑战性的任务。我们提出了一种新颖的方法来学习扫描和CAD几何之间的联合嵌入空间，其中来自两个领域的语义相似对象紧密地靠在一起。为了实现这一点，我们引入了一种新的基于3D CNN的方法来学习一个表示这些领域之间对象相似性的联合嵌入空间。为了学习一个扫描对象和CAD模型可以交织在一起的共享空间，我们提出了一种堆叠的沙漏方法来从扫描对象中分离前景和背景，并将其转换为完整的、类似CAD的表示，以产生共享的嵌入空间。然后，这个嵌入空间可以用于CAD模型检索；为了进一步实现这一任务，我们引入了一个新的排名扫描-CAD相似性注释数据集，使得对杂乱、噪声、部分扫描的CAD模型检索进行新的、细粒度的评估成为可能。我们学习的联合嵌入在实例检索准确率上比当前最先进的CAD模型检索方法高出12%。",
        "领域": "3D建模/计算机辅助设计/3D视觉",
        "问题": "如何有效地在3D扫描几何和CAD模型之间建立映射，以利用它们之间的互补信息",
        "动机": "利用3D扫描和CAD模型之间的互补信息来更好地理解环境",
        "方法": "提出了一种基于3D CNN的联合嵌入空间学习方法，采用堆叠的沙漏方法分离前景和背景，并将扫描对象转换为类似CAD的表示",
        "关键词": [
            "3D建模",
            "计算机辅助设计",
            "3D视觉",
            "联合嵌入",
            "3D CNN",
            "沙漏方法"
        ],
        "涉及的技术概念": {
            "3D CNN": "一种用于处理三维数据的卷积神经网络，能够捕捉3D空间中的特征",
            "联合嵌入空间": "一个共享的空间，其中来自不同领域的语义相似对象被映射到相近的位置",
            "堆叠的沙漏方法": "一种网络结构，通过重复的上下采样过程来捕捉不同尺度的特征，常用于图像分割任务"
        }
    },
    {
        "order": 650,
        "title": "STD: Sparse-to-Dense 3D Object Detector for Point Cloud",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_STD_Sparse-to-Dense_3D_Object_Detector_for_Point_Cloud_ICCV_2019_paper.html",
        "abstract": "We propose a two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point clouds as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a higher recall with less computation compared with prior works. Then, PointsPool is applied for proposal feature generation by transforming interior point features from sparse expression to compact representation, which saves even more computation. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method on 3D object and Bird's Eye View (BEV) detection. Our method outperforms other methods by a large margin, especially on the hard set, with 10+ FPS inference speed.",
        "中文标题": "STD: 用于点云的稀疏到密集三维物体检测器",
        "摘要翻译": "我们提出了一个名为稀疏到密集三维物体检测器（STD）的两阶段三维物体检测框架。第一阶段是一个自下而上的提案生成网络，它使用原始点云作为输入，通过为每个点播种一个新的球形锚点来生成准确的提案。与之前的工作相比，它以更少的计算实现了更高的召回率。然后，应用PointsPool通过将内部点特征从稀疏表达转换为紧凑表示来生成提案特征，这进一步节省了计算。在第二阶段，即框预测中，我们实现了一个平行的交并比（IoU）分支，以提高对定位准确性的认识，从而进一步提高了性能。我们在KITTI数据集上进行了实验，并在三维物体和鸟瞰图（BEV）检测上评估了我们的方法。我们的方法在性能上大幅超越其他方法，特别是在困难集上，推理速度达到10+ FPS。",
        "领域": "三维物体检测/点云处理/自动驾驶",
        "问题": "提高三维物体检测的准确性和效率",
        "动机": "为了在自动驾驶等领域中更准确地检测三维物体，同时减少计算资源的消耗",
        "方法": "采用两阶段框架，第一阶段通过自下而上的提案生成网络生成准确提案，第二阶段通过平行IoU分支提高定位准确性",
        "关键词": [
            "三维物体检测",
            "点云处理",
            "自动驾驶"
        ],
        "涉及的技术概念": {
            "稀疏到密集三维物体检测器（STD）": "一种两阶段的三维物体检测框架，旨在提高检测的准确性和效率",
            "自下而上的提案生成网络": "第一阶段的方法，使用原始点云生成准确提案",
            "PointsPool": "用于将内部点特征从稀疏表达转换为紧凑表示，以节省计算",
            "平行交并比（IoU）分支": "第二阶段的方法，用于提高定位准确性",
            "KITTI数据集": "用于实验和评估的数据集",
            "鸟瞰图（BEV）检测": "评估方法之一，用于检测物体在鸟瞰图中的位置"
        }
    },
    {
        "order": 651,
        "title": "ViSiL: Fine-Grained Spatio-Temporal Video Similarity Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kordopatis-Zilos_ViSiL_Fine-Grained_Spatio-Temporal_Video_Similarity_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kordopatis-Zilos_ViSiL_Fine-Grained_Spatio-Temporal_Video_Similarity_Learning_ICCV_2019_paper.html",
        "abstract": "In this paper we introduce ViSiL, a Video Similarity Learning architecture that considers fine-grained Spatio-Temporal relations between pairs of videos -- such relations are typically lost in previous video retrieval approaches that embed the whole frame or even the whole video into a vector descriptor before the similarity estimation. By contrast, our Convolutional Neural Network (CNN)-based approach is trained to calculate video-to-video similarity from refined frame-to-frame similarity matrices, so as to consider both intra- and inter-frame relations. In the proposed method, pairwise frame similarity is estimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on regional CNN frame features - this avoids feature aggregation before the similarity calculation between frames. Subsequently, the similarity matrix between all video frames is fed to a four-layer CNN, and then summarized using Chamfer Similarity (CS) into a video-to-video similarity score -- this avoids feature aggregation before the similarity calculation between videos and captures the temporal similarity patterns between matching frame sequences. We train the proposed network using a triplet loss scheme and evaluate it on five public benchmark datasets on four different video retrieval problems where we demonstrate large improvements in comparison to the state of the art. The implementation of ViSiL is publicly available.",
        "中文标题": "ViSiL: 细粒度时空视频相似性学习",
        "摘要翻译": "本文介绍了ViSiL，一种视频相似性学习架构，它考虑视频对之间的细粒度时空关系——这种关系通常在之前的视频检索方法中丢失，这些方法在相似性估计之前将整个帧甚至整个视频嵌入到向量描述符中。相比之下，我们基于卷积神经网络（CNN）的方法被训练来从精炼的帧到帧相似性矩阵中计算视频到视频的相似性，以便同时考虑帧内和帧间关系。在所提出的方法中，通过在区域CNN帧特征上应用张量点积（TD）和Chamfer相似性（CS）来估计成对帧相似性——这避免了在帧间相似性计算之前的特征聚合。随后，所有视频帧之间的相似性矩阵被输入到一个四层CNN中，然后使用Chamfer相似性（CS）总结为视频到视频的相似性分数——这避免了在视频间相似性计算之前的特征聚合，并捕捉了匹配帧序列之间的时间相似性模式。我们使用三重态损失方案训练所提出的网络，并在五个公共基准数据集上评估其在四个不同的视频检索问题上的表现，展示了与现有技术相比的显著改进。ViSiL的实现已公开可用。",
        "领域": "视频检索/时空关系分析/相似性学习",
        "问题": "解决视频检索中细粒度时空关系丢失的问题",
        "动机": "提高视频检索的准确性和效率，通过考虑视频对之间的细粒度时空关系",
        "方法": "使用基于卷积神经网络的方法，从精炼的帧到帧相似性矩阵中计算视频到视频的相似性，避免在相似性计算之前的特征聚合",
        "关键词": [
            "视频检索",
            "时空关系",
            "相似性学习",
            "卷积神经网络",
            "Chamfer相似性"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像和视频数据",
            "张量点积（TD）": "一种数学运算，用于计算两个张量之间的点积",
            "Chamfer相似性（CS）": "一种用于比较两个序列或集合之间相似性的方法",
            "三重态损失方案": "一种用于训练相似性学习模型的损失函数，通过比较正样本、负样本和锚点样本来优化模型"
        }
    },
    {
        "order": 652,
        "title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Schor_CompoNet_Learning_to_Generate_the_Unseen_by_Part_Synthesis_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Schor_CompoNet_Learning_to_Generate_the_Unseen_by_Part_Synthesis_and_ICCV_2019_paper.html",
        "abstract": "Data-driven generative modeling has made remarkable progress by leveraging the power of deep neural networks. A reoccurring challenge is how to enable a model to generate a rich variety of samples from the entire target distribution, rather than only from a distribution confined to the training data. In other words, we would like the generative model to go beyond the observed samples and learn to generate \"unseen\", yet still plausible, data. In our work, we present CompoNet, a generative neural network for 2D or 3D shapes that is based on a part-based prior, where the key idea is for the network to synthesize shapes by varying both the shape parts and their compositions. Treating a shape not as an unstructured whole, but as a (re-)composable set of deformable parts, adds a combinatorial dimension to the generative process to enrich the diversity of the output, encouraging the generator to venture more into the \"unseen\". We show that our part-based model generates richer variety of plausible shapes compared with baseline generative models. To this end, we introduce two quantitative metrics to evaluate the diversity of a generative model and assess how well the generated data covers both the training data and unseen data from the same target distribution.",
        "中文标题": "CompoNet：通过学习部分合成与组合生成未见内容",
        "摘要翻译": "数据驱动的生成建模通过利用深度神经网络的力量取得了显著进展。一个反复出现的挑战是如何使模型能够从整个目标分布中生成丰富的样本，而不仅仅是从局限于训练数据的分布中生成。换句话说，我们希望生成模型能够超越观察到的样本，学会生成“未见”的、但仍然合理的数据。在我们的工作中，我们提出了CompoNet，这是一个基于部分先验的生成神经网络，适用于2D或3D形状，其关键思想是让网络通过改变形状部分及其组合来合成形状。将形状视为可（重新）组合的可变形部分集合，而不是无结构的整体，为生成过程增加了一个组合维度，以丰富输出的多样性，鼓励生成器更多地探索“未见”的内容。我们展示了我们的基于部分的模型与基线生成模型相比，能够生成更多样化的合理形状。为此，我们引入了两个定量指标来评估生成模型的多样性，并评估生成的数据在多大程度上覆盖了训练数据和来自同一目标分布的未见数据。",
        "领域": "生成模型/形状合成/组合优化",
        "问题": "如何使生成模型能够生成超出训练数据范围的、多样化的合理样本",
        "动机": "为了克服生成模型局限于训练数据分布的问题，探索生成未见但合理的数据的方法",
        "方法": "提出CompoNet，一个基于部分先验的生成神经网络，通过改变形状部分及其组合来合成形状，增加生成过程的组合维度",
        "关键词": [
            "生成模型",
            "形状合成",
            "组合优化"
        ],
        "涉及的技术概念": "CompoNet是一个生成神经网络，专注于2D或3D形状的生成，通过部分合成和组合的方式增加生成样本的多样性。该方法通过将形状视为可变形部分的组合，而不是单一整体，引入组合维度，从而鼓励生成器探索并生成未见但合理的数据。此外，引入了两个定量指标来评估生成模型的多样性和覆盖范围。"
    },
    {
        "order": 653,
        "title": "DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_DUP-Net_Denoiser_and_Upsampler_Network_for_3D_Adversarial_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_DUP-Net_Denoiser_and_Upsampler_Network_for_3D_Adversarial_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classification, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a data-driven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet.",
        "中文标题": "DUP-Net: 用于3D对抗点云防御的去噪和上采样网络",
        "摘要翻译": "神经网络容易受到对抗样本的攻击，这对其在安全敏感系统中的应用构成了威胁。我们提出了一种去噪和上采样网络（DUP-Net）结构，作为3D对抗点云分类的防御手段，其中两个模块通过删除或添加点来重建表面平滑度。在本文中，统计异常值去除（SOR）和数据驱动的上采样网络分别被视为去噪器和上采样器。与基线防御相比，DUP-Net具有三个优势。首先，使用DUP-Net作为防御手段，目标模型对白盒对抗攻击更加鲁棒。其次，统计异常值去除提供了额外的鲁棒性，因为它是一种不可微的去噪操作。第三，上采样网络可以在小数据集上训练，并且能够很好地防御来自其他点云数据集的对抗攻击。我们进行了各种实验，以验证DUP-Net在实践中作为防御手段非常有效。我们的最佳防御在PointNet上消除了83.8%的C&W和基于l2损失的攻击（点移动），50.0%的C&W和基于Hausdorff距离损失的攻击（点添加）以及9.0%的基于显著性图的攻击（点删除），在200个删除点的情况下。",
        "领域": "3D点云处理/对抗防御/深度学习安全",
        "问题": "3D点云分类模型对对抗样本的脆弱性问题",
        "动机": "提高3D点云分类模型在安全敏感应用中的鲁棒性，防御对抗攻击",
        "方法": "提出DUP-Net结构，结合统计异常值去除（SOR）和数据驱动的上采样网络，通过删除或添加点来重建表面平滑度，增强模型对对抗攻击的防御能力",
        "关键词": [
            "3D点云",
            "对抗防御",
            "去噪",
            "上采样",
            "统计异常值去除"
        ],
        "涉及的技术概念": "DUP-Net是一种结合了去噪和上采样功能的网络结构，旨在提高3D点云分类模型对对抗样本的鲁棒性。统计异常值去除（SOR）是一种非可微的去噪操作，用于去除点云中的异常点，而数据驱动的上采样网络则用于增加点云的点数，以重建表面平滑度。这些技术共同作用，使得模型能够更好地防御各种对抗攻击。"
    },
    {
        "order": 654,
        "title": "DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_DDSL_Deep_Differentiable_Simplex_Layer_for_Learning_Geometric_Signals_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_DDSL_Deep_Differentiable_Simplex_Layer_for_Learning_Geometric_Signals_ICCV_2019_paper.html",
        "abstract": "We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for geometric deep learning. The DDSL is a differentiable layer compatible with deep neural networks for bridging simplex mesh-based geometry representations (point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images (e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to perform differentiable, efficient, anti- aliased rasterization of simplex-based signals. We present a complete theoretical framework for the process as well as an efficient backpropagation algorithm. Compared to previous differentiable renderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees and dimensions. In particular, we explore its applications to 2D shapes and illustrate two applications of this method: (1) mesh editing and optimization guided by neural network outputs, and (2) using DDSL for a differentiable rasterization loss to facilitate end-to-end training of polygon generators. We are able to validate the effectiveness of gradient-based shape optimization with the example of airfoil optimization, and using the differentiable rasterization loss to facilitate end-to-end training, we surpass state of the art for polygonal image segmentation given ground-truth bounding boxes.",
        "中文标题": "DDSL：用于学习几何信号的深度可微分单纯形层",
        "摘要翻译": "我们提出了一种用于几何深度学习的深度可微分单纯形层（DDSL）。DDSL是一种与深度神经网络兼容的可微分层，用于桥接基于单纯形网格的几何表示（点云、线网格、三角网格、四面体网格）与光栅图像（例如，2D/3D网格）。DDSL使用非均匀傅里叶变换（NUFT）来执行基于单纯形信号的可微分、高效、抗锯齿的光栅化。我们为该过程提供了一个完整的理论框架以及一个高效的反向传播算法。与之前的可微分渲染器和光栅化器相比，DDSL推广到了任意单纯形度和维度。特别是，我们探索了其在2D形状上的应用，并展示了该方法的两个应用：（1）由神经网络输出指导的网格编辑和优化，（2）使用DDSL进行可微分光栅化损失，以促进多边形生成器的端到端训练。我们能够通过翼型优化的例子验证基于梯度的形状优化的有效性，并且使用可微分光栅化损失来促进端到端训练，我们在给定真实边界框的情况下超越了多边形图像分割的最新技术。",
        "领域": "几何深度学习/可微分渲染/光栅化",
        "问题": "如何桥接基于单纯形网格的几何表示与光栅图像，并实现高效、可微分的渲染和优化",
        "动机": "为了在几何深度学习中实现更高效的形状优化和图像分割，需要一种能够桥接不同几何表示与光栅图像的可微分方法",
        "方法": "提出了一种深度可微分单纯形层（DDSL），使用非均匀傅里叶变换（NUFT）进行可微分、高效、抗锯齿的光栅化，并提供了完整的理论框架和高效的反向传播算法",
        "关键词": [
            "单纯形网格",
            "非均匀傅里叶变换",
            "可微分渲染",
            "形状优化",
            "图像分割"
        ],
        "涉及的技术概念": {
            "单纯形网格": "一种几何表示方法，包括点云、线网格、三角网格、四面体网格等",
            "非均匀傅里叶变换（NUFT）": "一种用于信号处理的数学工具，特别适用于非均匀采样数据的傅里叶变换",
            "可微分渲染": "一种允许梯度通过渲染过程传播的技术，使得渲染过程可以集成到深度学习模型中",
            "光栅化": "将矢量图形转换为光栅图像（像素图像）的过程",
            "反向传播算法": "一种用于训练神经网络的算法，通过计算损失函数相对于网络参数的梯度来更新参数"
        }
    },
    {
        "order": 655,
        "title": "Unsupervised Learning of Landmarks by Descriptor Vector Exchange",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Thewlis_Unsupervised_Learning_of_Landmarks_by_Descriptor_Vector_Exchange_ICCV_2019_paper.html",
        "abstract": "Equivariance to random image transformations is an effective method to learn landmarks of object categories, such as the eyes and the nose in faces, without manual supervision. However, this method does not explicitly guarantee that the learned landmarks are consistent with changes between different instances of the same object, such as different facial identities. In this paper, we develop a new perspective on the equivariance approach by noting that dense landmark detectors can be interpreted as local image descriptors equipped with invariance to intra-category variations. We then propose a direct method to enforce such an invariance in the standard equivariant loss. We do so by exchanging descriptor vectors between images of different object instances prior to matching them geometrically. In this manner, the same vectors must work regardless of the specific object identity considered. We use this approach to learn vectors that can simultaneously be interpreted as local descriptors and dense landmarks, combining the advantages of both. Experiments on standard benchmarks show that this approach can match, and in some cases surpass state-of-the-art performance amongst existing methods that learn landmarks without supervision. Code is available at www.robots.ox.ac.uk/ vgg/research/DVE/.",
        "中文标题": "通过描述符向量交换进行地标无监督学习",
        "摘要翻译": "对随机图像变换的等变性是一种有效的方法，用于学习对象类别的地标，例如面部中的眼睛和鼻子，而无需手动监督。然而，这种方法并不明确保证学习到的地标与同一对象不同实例之间的变化一致，例如不同的面部身份。在本文中，我们通过注意到密集地标检测器可以被解释为具有类别内变化不变性的局部图像描述符，从而对等变性方法提出了新的视角。然后，我们提出了一种直接的方法，在标准的等变损失中强制执行这种不变性。我们通过在几何匹配之前交换不同对象实例图像之间的描述符向量来实现这一点。通过这种方式，无论考虑的具体对象身份如何，相同的向量都必须有效。我们使用这种方法来学习可以同时被解释为局部描述符和密集地标的向量，结合了两者的优点。在标准基准上的实验表明，这种方法可以与现有无监督学习地标的方法相媲美，在某些情况下甚至超越了现有方法的性能。代码可在www.robots.ox.ac.uk/vgg/research/DVE/获取。",
        "领域": "地标检测/图像描述符/等变性学习",
        "问题": "学习到的地标与同一对象不同实例之间的变化一致性问题",
        "动机": "提高无监督学习地标的准确性和一致性，特别是在处理不同对象实例时",
        "方法": "通过在几何匹配之前交换不同对象实例图像之间的描述符向量，强制执行类别内变化的不变性",
        "关键词": [
            "地标检测",
            "图像描述符",
            "等变性学习",
            "无监督学习"
        ],
        "涉及的技术概念": "等变性学习是一种方法，它允许模型对输入数据的某些变换保持不变，例如图像的旋转或缩放。在本文中，等变性被用来学习对象的地标，而无需手动标注。通过交换描述符向量，模型被训练以识别不同对象实例之间的共同特征，从而提高地标检测的准确性和一致性。"
    },
    {
        "order": 656,
        "title": "Learning Rich Features at High-Speed for Single-Shot Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Single-stage object detection methods have received significant attention recently due to their characteristic realtime capabilities and high detection accuracies. Generally, most existing single-stage detectors follow two common practices: they employ a network backbone that is pretrained on ImageNet for the classification task and use a top-down feature pyramid representation for handling scale variations. Contrary to common pre-training strategy, recent works have demonstrated the benefits of training from scratch to reduce the task gap between classification and localization, especially at high overlap thresholds. However, detection models trained from scratch require significantly longer training time compared to their typical finetuning based counterparts. We introduce a single-stage detection framework that combines the advantages of both fine-tuning pretrained models and training from scratch. Our framework constitutes a standard network that uses a pre-trained backbone and a parallel light-weight auxiliary network trained from scratch. Further, we argue that the commonly used top-down pyramid representation only focuses on passing high-level semantics from the top layers to bottom layers. We introduce a bi-directional network that efficiently circulates both low-/mid-level and high-level semantic information in the detection framework. Experiments are performed on MS COCO and UAVDT datasets. Compared to the baseline, our detector achieives an absolute gain of 7.4% and 4.2% in average precision (AP) on MS COCO and UAVDT datasets, respectively using VGG backbone. For a 300x300 input on the MS COCO test set, our detector with ResNet backbone surpasses existing single-stage detection methods for single-scale inference achieving 34.3 AP, while operating at an inference time of 19 milliseconds on a single Titan X GPU. Code is avail- able at https://github.com/vaesl/LRF-Net.",
        "中文标题": "高速学习丰富特征以实现单次目标检测",
        "摘要翻译": "单阶段目标检测方法因其实时能力和高检测精度而受到广泛关注。通常，大多数现有的单阶段检测器遵循两种常见做法：它们使用在ImageNet上预训练的网络骨干进行分类任务，并使用自上而下的特征金字塔表示来处理尺度变化。与常见的预训练策略相反，最近的研究表明，从零开始训练可以减少分类和定位之间的任务差距，特别是在高重叠阈值时。然而，与典型的基于微调的模型相比，从零开始训练的检测模型需要显著更长的训练时间。我们引入了一个单阶段检测框架，该框架结合了微调预训练模型和从零开始训练的优势。我们的框架包括一个使用预训练骨干的标准网络和一个从零开始训练的并行轻量级辅助网络。此外，我们认为常用的自上而下的金字塔表示仅专注于将高层语义从顶层传递到底层。我们引入了一个双向网络，该网络有效地在检测框架中循环低/中层和高层语义信息。实验在MS COCO和UAVDT数据集上进行。与基线相比，我们的检测器在MS COCO和UAVDT数据集上分别使用VGG骨干实现了7.4%和4.2%的平均精度（AP）绝对增益。对于MS COCO测试集上的300x300输入，我们的检测器使用ResNet骨干在单尺度推理上超越了现有的单阶段检测方法，实现了34.3 AP，同时在单个Titan X GPU上的推理时间为19毫秒。代码可在https://github.com/vaesl/LRF-Net获取。",
        "领域": "目标检测/特征学习/网络架构",
        "问题": "如何在不显著增加训练时间的情况下，提高单阶段目标检测器的检测精度",
        "动机": "减少分类和定位任务之间的差距，特别是在高重叠阈值时，同时保持或减少训练时间",
        "方法": "引入一个结合微调预训练模型和从零开始训练优势的单阶段检测框架，包括一个使用预训练骨干的标准网络和一个从零开始训练的并行轻量级辅助网络，以及一个双向网络以循环低/中层和高层语义信息",
        "关键词": [
            "单阶段目标检测",
            "特征学习",
            "网络架构",
            "双向网络",
            "平均精度"
        ],
        "涉及的技术概念": "单阶段目标检测方法通常使用预训练的网络骨干和自上而下的特征金字塔表示。本文提出的方法结合了微调预训练模型和从零开始训练的优势，通过引入一个双向网络来循环低/中层和高层语义信息，以提高检测精度而不显著增加训练时间。"
    },
    {
        "order": 657,
        "title": "EGNet: Edge Guidance Network for Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_EGNet_Edge_Guidance_Network_for_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_EGNet_Edge_Guidance_Network_for_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Fully convolutional neural networks (FCNs) have shown their advantages in the salient object detection task. However, most existing FCNs-based methods still suffer from coarse object boundaries. In this paper, to solve this problem, we focus on the complementarity between salient edge information and salient object information. Accordingly, we present an edge guidance network (EGNet) for salient object detection with three steps to simultaneously model these two kinds of complementary information in a single network. In the first step, we extract the salient object features by a progressive fusion way. In the second step, we integrate the local edge information and global location information to obtain the salient edge features. Finally, to sufficiently leverage these complementary features, we couple the same salient edge features with salient object features at various resolutions. Benefiting from the rich edge information and location information in salient edge features, the fused features can help locate salient objects, especially their boundaries more accurately. Experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods on six widely used datasets without any pre-processing and post-processing. The source code is available at http: //mmcheng.net/egnet/.",
        "中文标题": "EGNet：用于显著目标检测的边缘引导网络",
        "摘要翻译": "全卷积神经网络（FCNs）在显著目标检测任务中已经展示了它们的优势。然而，大多数现有的基于FCNs的方法仍然受到粗糙目标边界的困扰。在本文中，为了解决这个问题，我们专注于显著边缘信息和显著目标信息之间的互补性。因此，我们提出了一个边缘引导网络（EGNet），用于显著目标检测，通过三个步骤在一个网络中同时建模这两种互补信息。在第一步中，我们通过渐进融合的方式提取显著目标特征。在第二步中，我们整合局部边缘信息和全局位置信息以获得显著边缘特征。最后，为了充分利用这些互补特征，我们将相同的显著边缘特征与不同分辨率的显著目标特征耦合。得益于显著边缘特征中丰富的边缘信息和位置信息，融合后的特征可以帮助更准确地定位显著目标，特别是它们的边界。实验结果表明，所提出的方法在六个广泛使用的数据集上无需任何预处理和后处理就能与最先进的方法相媲美。源代码可在http://mmcheng.net/egnet/获取。",
        "领域": "显著目标检测/边缘检测/特征融合",
        "问题": "解决显著目标检测中目标边界粗糙的问题",
        "动机": "利用显著边缘信息和显著目标信息之间的互补性来提高显著目标检测的准确性，特别是目标边界的准确性",
        "方法": "提出边缘引导网络（EGNet），通过三个步骤在一个网络中同时建模显著边缘信息和显著目标信息，包括提取显著目标特征、整合局部边缘信息和全局位置信息以获得显著边缘特征，以及将显著边缘特征与显著目标特征耦合",
        "关键词": [
            "显著目标检测",
            "边缘检测",
            "特征融合"
        ],
        "涉及的技术概念": "全卷积神经网络（FCNs）用于显著目标检测，显著边缘信息和显著目标信息的互补性，渐进融合方式提取特征，局部边缘信息和全局位置信息的整合，特征耦合"
    },
    {
        "order": 658,
        "title": "Learning Compositional Representations for Few-Shot Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tokmakov_Learning_Compositional_Representations_for_Few-Shot_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tokmakov_Learning_Compositional_Representations_for_Few-Shot_Recognition_ICCV_2019_paper.html",
        "abstract": "One of the key limitations of modern deep learning approaches lies in the amount of data required to train them. Humans, by contrast, can learn to recognize novel categories from just a few examples. Instrumental to this rapid learning ability is the compositional structure of concept representations in the human brain --- something that deep learning models are lacking. In this work, we make a step towards bridging this gap between human and machine learning by introducing a simple regularization technique that allows the learned representation to be decomposable into parts. Our method uses category-level attribute annotations to disentangle the feature space of a network into subspaces corresponding to the attributes. These attributes can be either purely visual, like object parts, or more abstract, like openness and symmetry. We demonstrate the value of compositional representations on three datasets: CUB-200-2011, SUN397, and ImageNet, and show that they require fewer examples to learn classifiers for novel categories.",
        "中文标题": "学习组合表示用于少样本识别",
        "摘要翻译": "现代深度学习方法的一个关键限制在于训练它们所需的数据量。相比之下，人类可以从仅有的几个例子中学习识别新类别。人类大脑中概念表示的组合结构对于这种快速学习能力至关重要——这是深度学习模型所缺乏的。在这项工作中，我们通过引入一种简单的正则化技术，使学习到的表示可以分解为部分，从而向缩小人类与机器学习之间的差距迈出了一步。我们的方法使用类别级属性注释将网络的特征空间分解为对应于属性的子空间。这些属性可以是纯粹的视觉属性，如对象部分，也可以是更抽象的属性，如开放性和对称性。我们在三个数据集上展示了组合表示的价值：CUB-200-2011、SUN397和ImageNet，并表明它们需要更少的例子来学习新类别的分类器。",
        "领域": "少样本学习/组合学习/特征分解",
        "问题": "现代深度学习方法需要大量数据训练，而人类可以从少量例子中学习识别新类别",
        "动机": "缩小人类与机器学习在快速学习能力上的差距",
        "方法": "引入一种简单的正则化技术，使学习到的表示可以分解为部分，使用类别级属性注释将网络的特征空间分解为对应于属性的子空间",
        "关键词": [
            "少样本学习",
            "组合学习",
            "特征分解",
            "正则化技术",
            "类别级属性注释"
        ],
        "涉及的技术概念": "组合表示、正则化技术、类别级属性注释、特征空间分解、少样本学习"
    },
    {
        "order": 659,
        "title": "Detecting Unseen Visual Relations Using Analogies",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peyre_Detecting_Unseen_Visual_Relations_Using_Analogies_ICCV_2019_paper.html",
        "abstract": "We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as \"person riding dog\", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset.",
        "中文标题": "使用类比检测未见过的视觉关系",
        "摘要翻译": "我们旨在检测图像中的视觉关系，形式为三元组t = (主体, 谓词, 对象)，例如“人骑狗”，其中单个实体的训练示例可用，但它们的组合在训练时未见。这是一个重要的设置，因为视觉关系的组合性质：为所有可能的三元组收集足够的训练数据将非常困难。这项工作的贡献有三方面。首先，我们学习了一种视觉关系的表示，它结合了(i)主体、对象和谓词的单独嵌入与(ii)表示关系三元组的视觉短语嵌入。其次，我们学习了如何利用涉及相似对象的关系之间的类比，将视觉短语嵌入从现有的训练三元组转移到未见过的测试三元组。第三，我们在三个具有挑战性的数据集上展示了我们方法的好处：在HICO-DET上，我们的模型在频繁和未见过的三元组上都实现了对强基线的显著改进，并且我们在COCO-a数据集上对具有词汇外谓词的未见过的三元组的检索以及在UnRel数据集中的挑战性不寻常三元组上也观察到了类似的改进。",
        "领域": "视觉关系检测/视觉短语嵌入/关系类比",
        "问题": "检测未见过的视觉关系三元组",
        "动机": "由于视觉关系的组合性质，为所有可能的三元组收集足够的训练数据非常困难，因此需要一种方法来检测未见过的视觉关系。",
        "方法": "结合主体、对象和谓词的单独嵌入与视觉短语嵌入来表示视觉关系，并利用关系之间的类比将视觉短语嵌入从现有的训练三元组转移到未见过的测试三元组。",
        "关键词": [
            "视觉关系检测",
            "视觉短语嵌入",
            "关系类比"
        ],
        "涉及的技术概念": "视觉关系检测涉及识别图像中的主体、谓词和对象之间的关系。视觉短语嵌入是一种将视觉关系表示为向量的方法，使得可以计算不同关系之间的相似度。关系类比是一种利用已知关系之间的相似性来推断未知关系的方法。"
    },
    {
        "order": 660,
        "title": "SID4VAM: A Benchmark Dataset With Synthetic Images for Visual Attention Modeling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Berga_SID4VAM_A_Benchmark_Dataset_With_Synthetic_Images_for_Visual_Attention_ICCV_2019_paper.html",
        "abstract": "A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets.",
        "中文标题": "SID4VAM: 用于视觉注意力建模的合成图像基准数据集",
        "摘要翻译": "提供了一个使用合成图像数据集对显著性模型性能进行基准测试的研究。通过显著性指标以及模型灵感的影响和与人类心理物理学的一致性来评估模型性能。SID4VAM由230张合成图像组成，这些图像具有已知的显著区域。图像是通过15种不同类型的低级特征（例如方向、亮度、颜色、大小等）生成的，采用了目标-干扰物弹出类型的合成模式。我们使用了自由观看和视觉搜索任务指令，并为每个特征类别使用了7种特征对比。我们的研究表明，最先进的深度学习显著性模型在处理合成模式图像时表现不佳，相反，具有光谱/傅里叶灵感的模型在显著性指标上表现更好，并且与人类心理物理实验更加一致。本研究提出了一种新的方法来评估即将到来的文献中的显著性模型，考虑到具有独特低级特征背景的合成图像，这与之前的眼动追踪图像数据集不同。",
        "领域": "视觉注意力建模/显著性检测/心理物理学",
        "问题": "评估显著性模型在合成图像上的性能",
        "动机": "探索显著性模型在处理具有特定低级特征的合成图像时的表现，并与人类心理物理实验进行比较",
        "方法": "使用包含230张合成图像的SID4VAM数据集，通过自由观看和视觉搜索任务指令，以及7种特征对比来评估显著性模型的性能",
        "关键词": [
            "显著性模型",
            "合成图像",
            "心理物理学",
            "低级特征",
            "视觉搜索"
        ],
        "涉及的技术概念": "显著性模型性能评估、合成图像生成、低级特征（如方向、亮度、颜色、大小等）、自由观看和视觉搜索任务、光谱/傅里叶灵感模型、人类心理物理实验"
    },
    {
        "order": 661,
        "title": "Spectral Regularization for Combating Mode Collapse in GANs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Spectral_Regularization_for_Combating_Mode_Collapse_in_GANs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Spectral_Regularization_for_Combating_Mode_Collapse_in_GANs_ICCV_2019_paper.html",
        "abstract": "Despite excellent progress in recent years, mode collapse remains a major unsolved problem in generative adversarial networks (GANs). In this paper, we present spectral regularization for GANs (SR-GANs), a new and robust method for combating the mode collapse problem in GANs. Theoretical analysis shows that the optimal solution to the discriminator has a strong relationship to the spectral distributions of the weight matrix. Therefore, we monitor the spectral distribution in the discriminator of spectral normalized GANs (SN-GANs), and discover a phenomenon which we refer to as spectral collapse, where a large number of singular values of the weight matrices drop dramatically when mode collapse occurs. We show that there are strong evidence linking mode collapse to spectral collapse; and based on this link, we set out to tackle spectral collapse as a surrogate of mode collapse. We have developed a spectral regularization method where we compensate the spectral distributions of the weight matrices to prevent them from collapsing, which in turn successfully prevents mode collapse in GANs. We provide theoretical explanations for why SR-GANs are more stable and can provide better performances than SN-GANs. We also present extensive experimental results and analysis to show that SR-GANs not only always outperform SN-GANs but also always succeed in combating mode collapse where SN-GANs fail.",
        "中文标题": "对抗生成网络中模式崩溃问题的谱正则化方法",
        "摘要翻译": "尽管近年来取得了显著进展，模式崩溃仍然是生成对抗网络（GANs）中一个未解决的主要问题。在本文中，我们提出了用于GANs的谱正则化方法（SR-GANs），这是一种新的、鲁棒的方法，用于对抗GANs中的模式崩溃问题。理论分析表明，判别器的最优解与权重矩阵的谱分布有很强的关系。因此，我们监测了谱归一化GANs（SN-GANs）判别器中的谱分布，并发现了一种我们称之为谱崩溃的现象，即当模式崩溃发生时，权重矩阵的大量奇异值急剧下降。我们展示了将模式崩溃与谱崩溃联系起来的强有力证据；基于这一联系，我们着手解决谱崩溃作为模式崩溃的替代问题。我们开发了一种谱正则化方法，通过补偿权重矩阵的谱分布来防止它们崩溃，从而成功防止了GANs中的模式崩溃。我们提供了理论解释，说明为什么SR-GANs比SN-GANs更稳定且能提供更好的性能。我们还展示了广泛的实验结果和分析，表明SR-GANs不仅总是优于SN-GANs，而且在SN-GANs失败的情况下总是成功对抗模式崩溃。",
        "领域": "生成对抗网络/谱分析/正则化技术",
        "问题": "生成对抗网络中的模式崩溃问题",
        "动机": "解决生成对抗网络中模式崩溃这一未解决的主要问题，提高生成模型的稳定性和性能",
        "方法": "提出了一种新的谱正则化方法（SR-GANs），通过监测和补偿判别器中权重矩阵的谱分布来防止谱崩溃，从而防止模式崩溃",
        "关键词": [
            "生成对抗网络",
            "模式崩溃",
            "谱正则化",
            "谱崩溃",
            "权重矩阵"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，由生成器和判别器组成，用于生成新的数据样本。模式崩溃是GANs训练中的一个问题，指的是生成器开始生成非常相似或重复的样本，导致多样性丧失。谱正则化是一种技术，用于通过控制权重矩阵的谱分布来稳定训练过程。谱归一化GANs（SN-GANs）是一种使用谱归一化技术来稳定GANs训练的方法。"
    },
    {
        "order": 662,
        "title": "Disentangling Monocular 3D Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Simonelli_Disentangling_Monocular_3D_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Simonelli_Disentangling_Monocular_3D_Object_Detection_ICCV_2019_paper.html",
        "abstract": "In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies and set a new state-of-the-art on the KITTI3D Car class.",
        "中文标题": "解耦单目3D物体检测",
        "摘要翻译": "本文提出了一种从单一RGB图像进行单目3D物体检测的方法，该方法利用了一种新颖的解耦变换来处理2D和3D检测损失，并提出了一种新颖的、自监督的3D边界框置信度评分。我们提出的损失解耦具有双重优势：简化了在存在复杂参数交互损失时的训练动态，并绕过了平衡独立回归项的问题。我们的解决方案通过隔离参数组对给定损失的贡献来克服这些问题，而不改变其本质。我们进一步将损失解耦应用于另一种新颖的、由符号交并比准则驱动的损失，以改善2D检测结果。除了我们的方法创新之外，我们还批判性地审查了KITTI3D中使用的AP度量，KITTI3D已成为比较3D检测结果的最重要数据集。我们发现并解决了11点插值AP度量中的一个缺陷，该缺陷影响了所有先前发布的检测结果，特别是对单目3D检测结果产生了偏见。我们提供了广泛的实验评估和消融研究，并在KITTI3D汽车类别上设定了新的最先进水平。",
        "领域": "3D物体检测/单目视觉/自监督学习",
        "问题": "单目3D物体检测中的损失函数复杂交互和独立回归项平衡问题",
        "动机": "简化训练动态，提高3D物体检测的准确性和效率",
        "方法": "提出了一种新颖的损失解耦变换和自监督的3D边界框置信度评分，以及一种由符号交并比准则驱动的损失",
        "关键词": [
            "3D物体检测",
            "单目视觉",
            "自监督学习",
            "损失解耦",
            "置信度评分",
            "交并比"
        ],
        "涉及的技术概念": "损失解耦变换是一种技术，用于隔离不同参数组对特定损失的贡献，从而简化训练过程。自监督的3D边界框置信度评分是一种无需外部标注即可评估3D检测结果置信度的方法。符号交并比准则驱动的损失是一种基于交并比（IoU）的损失函数，用于提高2D检测的准确性。AP（Average Precision）度量是评估检测算法性能的一种常用指标，11点插值AP是计算AP的一种方法。"
    },
    {
        "order": 663,
        "title": "Two-Stream Action Recognition-Oriented Video Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Two-Stream_Action_Recognition-Oriented_Video_Super-Resolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Two-Stream_Action_Recognition-Oriented_Video_Super-Resolution_ICCV_2019_paper.html",
        "abstract": "We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",
        "中文标题": "面向双流动作识别的视频超分辨率",
        "摘要翻译": "我们研究视频超分辨率（SR）问题，以促进视频分析任务，例如动作识别，而不是为了视觉质量。基于卷积网络的流行动作识别方法，以双流网络为例，不能直接应用于低空间分辨率的视频。这可以通过在识别之前执行视频SR来补救，这激励我们改进SR程序以提高识别准确性。针对双流动作识别网络，我们分别提出了两种视频SR方法，分别针对空间流和时间流。一方面，我们观察到具有动作的区域对识别更为重要，我们为我们的空间导向SR（SoSR）网络提出了一个光流引导的加权均方误差损失，以强调移动物体的重建。另一方面，我们观察到现有的视频SR方法会导致帧之间的时间不连续性，这也降低了识别准确性，我们为我们的时间导向SR（ToSR）训练提出了一个连体网络，强调连续帧之间的时间连续性。我们使用两种最先进的动作识别网络和两个知名数据集——UCF101和HMDB51进行实验。结果证明了我们提出的SoSR和ToSR在提高识别准确性方面的有效性。",
        "领域": "视频分析/动作识别/视频超分辨率",
        "问题": "低空间分辨率视频在动作识别任务中的应用问题",
        "动机": "提高动作识别任务中视频超分辨率的识别准确性",
        "方法": "提出了两种视频超分辨率方法，分别针对空间流和时间流，包括光流引导的加权均方误差损失和连体网络训练",
        "关键词": [
            "视频超分辨率",
            "动作识别",
            "双流网络",
            "光流",
            "连体网络"
        ],
        "涉及的技术概念": "视频超分辨率（SR）是一种提高视频分辨率的技术，旨在从低分辨率视频中恢复高分辨率视频。动作识别是指从视频中识别出特定的动作或行为。双流网络是一种结合了空间流和时间流的深度学习模型，用于视频分析。光流是描述视频中物体运动的技术。连体网络是一种特殊的神经网络结构，用于处理成对输入，以强调它们之间的关系。"
    },
    {
        "order": 664,
        "title": "Scaling and Benchmarking Self-Supervised Visual Representation Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Goyal_Scaling_and_Benchmarking_Self-Supervised_Visual_Representation_Learning_ICCV_2019_paper.html",
        "abstract": "Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular self-supervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem 'hardness'), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not 'hard' enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/fair_self_supervision_benchmark.",
        "中文标题": "扩展和基准测试自监督视觉表示学习",
        "摘要翻译": "自监督学习旨在无需显式手动监督的情况下从数据本身学习表示。现有的努力忽略了自监督学习的一个关键方面——扩展到大量数据的能力，因为自监督不需要手动标签。在这项工作中，我们重新审视了这一原则，并将两种流行的自监督方法扩展到1亿张图像。我们展示了通过在多个轴（包括数据大小和问题'难度'）上进行扩展，可以在各种任务（如对象检测、表面法线估计（3D）和使用强化学习的视觉导航）上很大程度上匹配甚至超过监督预训练的性能。扩展这些方法还提供了许多关于当前自监督技术和评估局限性的有趣见解。我们得出结论，当前的自监督方法不够'难'，无法充分利用大规模数据，并且似乎没有学习到有效的高级语义表示。我们还引入了跨越9个不同数据集和任务的广泛基准测试。我们相信，这样的基准测试以及可比较的评估设置对于取得有意义的进展是必要的。代码位于：https://github.com/facebookresearch/fair_self_supervision_benchmark。",
        "领域": "自监督学习/视觉表示学习/基准测试",
        "问题": "如何有效地扩展自监督学习方法以处理大规模数据，并评估其在各种视觉任务上的性能",
        "动机": "探索自监督学习在大规模数据上的扩展能力，以及其对视觉任务性能的影响，旨在提高自监督学习方法的效率和效果",
        "方法": "扩展两种流行的自监督学习方法至1亿张图像，通过在不同轴（如数据大小和问题难度）上进行扩展，评估其在对象检测、表面法线估计（3D）和视觉导航等任务上的性能",
        "关键词": [
            "自监督学习",
            "视觉表示学习",
            "基准测试"
        ],
        "涉及的技术概念": "自监督学习是一种无需手动标签即可从数据中学习表示的方法。视觉表示学习涉及从视觉数据中提取有用的特征或表示。基准测试是指通过一系列标准化的测试来评估和比较不同方法或系统的性能。"
    },
    {
        "order": 665,
        "title": "STM: SpatioTemporal and Motion Encoding for Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.",
        "中文标题": "STM: 时空和运动编码用于动作识别",
        "摘要翻译": "时空和运动特征是视频动作识别中两个互补且至关重要的信息。最近的最先进方法采用3D CNN流来学习时空特征，另一个流来学习运动特征。在这项工作中，我们旨在在一个统一的2D框架中高效地编码这两种特征。为此，我们首先提出了一个STM块，它包含一个通道级时空模块（CSTM）来呈现时空特征和一个通道级运动模块（CMM）来高效编码运动特征。然后，我们通过引入非常有限的额外计算成本，将ResNet架构中的原始残差块替换为STM块，形成一个简单而有效的STM网络。大量实验证明，所提出的STM网络在编码时空和运动特征的帮助下，在时间相关数据集（即Something-Something v1 & v2和Jester）和场景相关数据集（即Kinetics-400、UCF-101和HMDB-51）上均优于最先进的方法。",
        "领域": "动作识别/视频分析/深度学习",
        "问题": "如何在统一的2D框架中高效编码时空和运动特征以提升视频动作识别的性能",
        "动机": "为了在视频动作识别中更有效地利用时空和运动这两种互补且至关重要的信息",
        "方法": "提出了一个包含通道级时空模块（CSTM）和通道级运动模块（CMM）的STM块，并将ResNet架构中的原始残差块替换为STM块，形成一个简单而有效的STM网络",
        "关键词": [
            "动作识别",
            "时空特征",
            "运动特征",
            "2D框架",
            "STM网络"
        ],
        "涉及的技术概念": {
            "3D CNN": "一种用于处理视频数据，能够同时捕捉空间和时间特征的卷积神经网络",
            "ResNet": "一种深度残差网络，通过引入残差学习解决了深层网络中的退化问题",
            "CSTM": "通道级时空模块，用于在STM块中呈现时空特征",
            "CMM": "通道级运动模块，用于在STM块中高效编码运动特征"
        }
    },
    {
        "order": 666,
        "title": "Where Is My Mirror?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Where_Is_My_Mirror_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Where_Is_My_Mirror_ICCV_2019_paper.html",
        "abstract": "Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods.",
        "中文标题": "我的镜子在哪里？",
        "摘要翻译": "镜子在我们的日常生活中无处不在。现有的计算机视觉系统没有考虑镜子，因此可能会被镜子内部的反射内容所迷惑，导致性能严重下降。然而，将镜子外部的真实内容与镜子内部的反射内容分开并非易事。关键的挑战在于，镜子通常会反射与其周围环境相似的内容，这使得区分两者变得非常困难。在本文中，我们提出了一种新颖的方法来从输入图像中分割镜子。据我们所知，这是第一个用计算方法解决镜子分割问题的工作。我们做出了以下贡献。首先，我们构建了一个大规模的镜子数据集，该数据集包含带有相应手动注释掩码的镜子图像。这个数据集涵盖了各种日常生活场景，并将公开供未来研究使用。其次，我们提出了一种名为MirrorNet的新网络，用于镜子分割，通过建模镜子内外内容之间的语义和低级别颜色/纹理不连续性。第三，我们进行了广泛的实验来评估所提出的方法，并表明它优于从最先进的检测和分割方法中精心挑选的基线。",
        "领域": "图像分割/反射处理/场景理解",
        "问题": "如何从图像中准确分割出镜子及其反射内容",
        "动机": "现有计算机视觉系统在处理包含镜子的图像时性能下降，因为无法区分镜子内外的内容",
        "方法": "构建大规模镜子数据集，提出MirrorNet网络通过建模语义和低级别颜色/纹理不连续性进行镜子分割",
        "关键词": [
            "图像分割",
            "反射处理",
            "场景理解",
            "MirrorNet",
            "数据集"
        ],
        "涉及的技术概念": {
            "图像分割": "从图像中分离出特定对象或区域的技术",
            "反射处理": "处理图像中由反射引起的内容，特别是区分真实内容和反射内容",
            "场景理解": "理解图像中的场景和对象之间的关系",
            "MirrorNet": "一种专门用于镜子分割的神经网络，通过分析镜子内外内容的语义和视觉特征差异来实现分割",
            "数据集": "包含大量图像和相应注释的数据集合，用于训练和评估计算机视觉模型"
        }
    },
    {
        "order": 667,
        "title": "Learning an Effective Equivariant 3D Descriptor Without Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Spezialetti_Learning_an_Effective_Equivariant_3D_Descriptor_Without_Supervision_ICCV_2019_paper.html",
        "abstract": "Establishing correspondences between 3D shapes is a fundamental task in 3D Computer Vision, typically ad- dressed by matching local descriptors. Recently, a few at- tempts at applying the deep learning paradigm to the task have shown promising results. Yet, the only explored way to learn rotation invariant descriptors has been to feed neural networks with highly engineered and invariant representa- tions provided by existing hand-crafted descriptors, a path that goes in the opposite direction of end-to-end learning from raw data so successfully deployed for 2D images. In this paper, we explore the benefits of taking a step back in the direction of end-to-end learning of 3D descrip- tors by disentangling the creation of a robust and distinctive rotation equivariant representation, which can be learned from unoriented input data, and the definition of a good canonical orientation, required only at test time to obtain an invariant descriptor. To this end, we leverage two re- cent innovations: spherical convolutional neural networks to learn an equivariant descriptor and plane folding de- coders to learn without supervision. The effectiveness of the proposed approach is experimentally validated by out- performing hand-crafted and learned descriptors on a stan- dard benchmark.",
        "中文标题": "学习一种无需监督的有效等变3D描述符",
        "摘要翻译": "在3D计算机视觉中，建立3D形状之间的对应关系是一个基本任务，通常通过匹配局部描述符来解决。最近，一些尝试将深度学习范式应用于此任务显示出有希望的结果。然而，学习旋转不变描述符的唯一探索方式是向神经网络提供由现有手工描述符提供的高度工程化和不变的表示，这条路径与从原始数据成功部署的端到端学习方向相反，这在2D图像中非常成功。在本文中，我们探讨了在3D描述符的端到端学习方向上退一步的好处，通过解耦创建鲁棒和独特的旋转等变表示（可以从无定向输入数据中学习）和定义良好的规范方向（仅在测试时需要以获得不变描述符）。为此，我们利用了两项最近的创新：球形卷积神经网络来学习等变描述符和平面折叠解码器来进行无监督学习。通过在标准基准上优于手工和学习的描述符，实验验证了所提出方法的有效性。",
        "领域": "3D视觉/深度学习/几何处理",
        "问题": "如何在无需监督的情况下学习有效的等变3D描述符",
        "动机": "探索从原始数据端到端学习3D描述符的方法，以克服现有方法依赖于高度工程化的手工描述符的局限性",
        "方法": "利用球形卷积神经网络学习等变描述符和平面折叠解码器进行无监督学习，解耦创建旋转等变表示和定义规范方向",
        "关键词": [
            "3D描述符",
            "旋转等变",
            "无监督学习"
        ],
        "涉及的技术概念": "球形卷积神经网络用于学习等变描述符，平面折叠解码器用于无监督学习，以及如何通过解耦创建旋转等变表示和定义规范方向来实现端到端学习。"
    },
    {
        "order": 668,
        "title": "Dynamic Context Correspondence Network for Semantic Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Dynamic_Context_Correspondence_Network_for_Semantic_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Dynamic_Context_Correspondence_Network_for_Semantic_Alignment_ICCV_2019_paper.html",
        "abstract": "Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality.",
        "中文标题": "动态上下文对应网络用于语义对齐",
        "摘要翻译": "建立语义对应是计算机视觉中的一个核心问题，由于类内差异大和缺乏标注数据，这一问题仍然具有挑战性。在本文中，我们旨在以灵活的方式融入全局语义上下文，以克服依赖局部语义表示的先前工作的局限性。为此，我们首先提出了一种上下文感知的语义表示，该表示结合了空间布局，以针对局部模糊性进行鲁棒匹配。然后，我们开发了一种基于注意力机制的新颖动态融合策略，通过整合来自多个尺度的语义线索，将局部和上下文特征的优点编织在一起。我们通过设计一个名为动态上下文对应网络（DCCNet）的端到端可学习深度网络来实例化我们的策略。为了训练网络，我们采用了多辅助任务损失来提高我们弱监督学习过程的效率。我们的方法在几个具有挑战性的数据集上实现了优于或竞争性的性能，包括PF-Pascal、PF-Willow和TSS，证明了其有效性和通用性。",
        "领域": "语义对应/注意力机制/弱监督学习",
        "问题": "解决由于类内差异大和缺乏标注数据导致的语义对应建立问题",
        "动机": "克服依赖局部语义表示的先前工作的局限性，通过融入全局语义上下文来提高语义对应的准确性和鲁棒性",
        "方法": "提出了一种上下文感知的语义表示，结合空间布局进行鲁棒匹配；开发了一种基于注意力机制的动态融合策略，整合多尺度语义线索；设计了一个端到端可学习的深度网络DCCNet，并采用多辅助任务损失进行训练",
        "关键词": [
            "语义对应",
            "注意力机制",
            "弱监督学习",
            "动态融合策略",
            "上下文感知"
        ],
        "涉及的技术概念": "上下文感知的语义表示：一种结合空间布局的语义表示方法，旨在提高对局部模糊性的鲁棒性。动态融合策略：基于注意力机制，整合来自多个尺度的语义线索，以结合局部和上下文特征的优点。DCCNet：一个端到端可学习的深度网络，用于实现上述策略。多辅助任务损失：一种训练方法，通过引入多个辅助任务来提高弱监督学习过程的效率。"
    },
    {
        "order": 669,
        "title": "Disentangled Image Matting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Disentangled_Image_Matting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Disentangled_Image_Matting_ICCV_2019_paper.html",
        "abstract": "Most previous image matting methods require a roughly-specificed trimap as input, and estimate fractional alpha values for all pixels that are in the unknown region of the trimap. In this paper, we argue that directly estimating the alpha matte from a coarse trimap is a major limitation of previous methods, as this practice tries to address two difficult and inherently different problems at the same time: identifying true blending pixels inside the trimap region, and estimate accurate alpha values for them. We propose AdaMatting, a new end-to-end matting framework that disentangles this problem into two sub-tasks: trimap adaptation and alpha estimation. Trimap adaptation is a pixel-wise classification problem that infers the global structure of the input image by identifying definite foreground, background, and semi-transparent image regions. Alpha estimation is a regression problem that calculates the opacity value of each blended pixel. Our method separately handles these two sub-tasks within a single deep convolutional neural network (CNN). Extensive experiments show that AdaMatting has additional structure awareness and trimap fault-tolerance. Our method achieves the state-of-the-art performance on Adobe Composition-1k dataset both qualitatively and quantitatively. It is also the current best-performing method on the alphamatting.com online evaluation for all commonly-used metrics.",
        "中文标题": "解耦图像抠图",
        "摘要翻译": "大多数先前的图像抠图方法需要一个大致的trimap作为输入，并估计trimap未知区域内所有像素的分数alpha值。在本文中，我们认为直接从粗糙的trimap估计alpha遮罩是先前方法的一个主要限制，因为这种做法试图同时解决两个困难且本质上不同的问题：识别trimap区域内的真实混合像素，并为它们估计准确的alpha值。我们提出了AdaMatting，一个新的端到端抠图框架，将这个问题解耦为两个子任务：trimap适应和alpha估计。Trimap适应是一个像素级分类问题，通过识别确定的背景、前景和半透明图像区域来推断输入图像的全局结构。Alpha估计是一个回归问题，计算每个混合像素的不透明度值。我们的方法在一个深度卷积神经网络（CNN）内分别处理这两个子任务。大量实验表明，AdaMatting具有额外的结构意识和trimap容错能力。我们的方法在Adobe Composition-1k数据集上实现了最先进的性能，无论是定性还是定量。它也是alphamatting.com在线评估中所有常用指标的最佳表现方法。",
        "领域": "图像抠图/深度学习/卷积神经网络",
        "问题": "直接从粗糙的trimap估计alpha遮罩的限制",
        "动机": "解决识别trimap区域内的真实混合像素和估计准确alpha值这两个困难且本质上不同的问题",
        "方法": "提出AdaMatting框架，将问题解耦为trimap适应和alpha估计两个子任务，并在一个深度卷积神经网络内分别处理",
        "关键词": [
            "图像抠图",
            "trimap适应",
            "alpha估计"
        ],
        "涉及的技术概念": {
            "trimap": "一种图像分割技术，用于将图像分为前景、背景和未知区域",
            "alpha遮罩": "用于表示图像中每个像素的透明度",
            "深度卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据"
        }
    },
    {
        "order": 670,
        "title": "KPConv: Flexible and Deformable Convolution for Point Clouds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Thomas_KPConv_Flexible_and_Deformable_Convolution_for_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "We present Kernel Point Convolution (KPConv), a new design of point convolution, i.e. that operates on point clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. Its capacity to use any number of kernel points gives KPConv more flexibility than fixed grid convolutions. Furthermore, these locations are continuous in space and can be learned by the network. Therefore, KPConv can be extended to deformable convolutions that learn to adapt kernel points to local geometry. Thanks to a regular subsampling strategy, KPConv is also efficient and robust to varying densities. Whether they use deformable KPConv for complex tasks, or rigid KPconv for simpler tasks, our networks outperform state-of-the-art classification and segmentation approaches on several datasets. We also offer ablation studies and visualizations to provide understanding of what has been learned by KPConv and to validate the descriptive power of deformable KPConv.",
        "中文标题": "KPConv：点云的灵活和可变形卷积",
        "摘要翻译": "我们提出了核点卷积（KPConv），这是一种新的点卷积设计，即直接在点云上操作而无需任何中间表示。KPConv的卷积权重通过核点定位在欧几里得空间中，并应用于靠近它们的输入点。KPConv能够使用任意数量的核点，这使其比固定网格卷积具有更大的灵活性。此外，这些位置在空间中是连续的，并且可以由网络学习。因此，KPConv可以扩展到可变形卷积，学习使核点适应局部几何形状。得益于一种规则的子采样策略，KPConv在处理不同密度时也非常高效和鲁棒。无论是使用可变形KPConv处理复杂任务，还是使用刚性KPConv处理更简单的任务，我们的网络在多个数据集上的分类和分割方法都优于最先进的技术。我们还提供了消融研究和可视化，以帮助理解KPConv学习到的内容，并验证可变形KPConv的描述能力。",
        "领域": "点云处理/三维视觉/卷积神经网络",
        "问题": "如何在点云数据上实现灵活且可变的卷积操作",
        "动机": "为了在点云数据上实现更有效的特征提取和处理，需要一种能够直接操作点云且具有高度灵活性的卷积方法",
        "方法": "提出了核点卷积（KPConv），通过核点在欧几里得空间中的定位和应用，以及可变形卷积的扩展，实现了对点云数据的灵活处理",
        "关键词": [
            "核点卷积",
            "点云",
            "可变形卷积",
            "三维视觉",
            "卷积神经网络"
        ],
        "涉及的技术概念": "KPConv是一种直接在点云上操作的卷积方法，通过核点在欧几里得空间中的定位和应用，实现了对点云数据的灵活处理。该方法支持任意数量的核点，使得卷积操作具有更高的灵活性。此外，KPConv可以扩展到可变形卷积，通过网络的自我学习，使核点能够适应局部几何形状。KPConv还采用了一种规则的子采样策略，使其在处理不同密度的点云数据时既高效又鲁棒。"
    },
    {
        "order": 671,
        "title": "Fooling Network Interpretation in Image Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Subramanya_Fooling_Network_Interpretation_in_Image_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Subramanya_Fooling_Network_Interpretation_in_Image_Classification_ICCV_2019_paper.html",
        "abstract": "Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.",
        "中文标题": "愚弄图像分类中的网络解释",
        "摘要翻译": "深度神经网络已被证明使用对抗攻击算法很容易被愚弄。诸如对抗补丁之类的实用方法已被证明在导致错误分类方面极为有效。然而，这些补丁使用标准的网络解释算法被突出显示，从而揭示了攻击者的身份。我们展示了不仅可以愚弄预测，还可以改变我们对预测原因的解释的对抗补丁的创建是可能的。此外，我们引入我们的攻击作为一个受控设置来测量解释算法的准确性。我们通过广泛的实验展示了这一点，这些实验适用于Grad-CAM解释，也适用于遮挡补丁解释。我们相信我们的算法可以促进开发更强大的网络解释工具，这些工具真正解释了网络的底层决策过程。",
        "领域": "对抗性攻击/网络解释/图像分类",
        "问题": "如何创建既能愚弄预测又能改变预测原因解释的对抗补丁",
        "动机": "揭示并改进网络解释算法的准确性，以更好地理解网络的决策过程",
        "方法": "引入一种新的对抗攻击方法，通过创建特定的对抗补丁来测试和测量网络解释算法的准确性",
        "关键词": [
            "对抗性攻击",
            "网络解释",
            "图像分类",
            "Grad-CAM",
            "对抗补丁"
        ],
        "涉及的技术概念": "对抗性攻击算法用于愚弄深度神经网络，对抗补丁是一种实用方法，用于导致错误分类。Grad-CAM是一种网络解释算法，用于突出显示影响网络决策的关键区域。"
    },
    {
        "order": 672,
        "title": "Guided Super-Resolution As Pixel-to-Pixel Transformation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/de_Lutio_Guided_Super-Resolution_As_Pixel-to-Pixel_Transformation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/de_Lutio_Guided_Super-Resolution_As_Pixel-to-Pixel_Transformation_ICCV_2019_paper.html",
        "abstract": "Guided super-resolution is a unifying framework for several computer vision tasks where the inputs are a low-resolution source image of some target quantity (e.g., perspective depth acquired with a time-of-flight camera) and a high-resolution guide image from a different domain (e.g., a grey-scale image from a conventional camera); and the target output is a high-resolution version of the source (in our example, a high-res depth map). The standard way of looking at this problem is to formulate it as a super-resolution task, i.e., the source image is upsampled to the target resolution, while transferring the missing high-frequency details from the guide. Here, we propose to turn that interpretation on its head and instead see it as a pixel-to-pixel mapping of the guide image to the domain of the source image. The pixel-wise mapping is parametrised as a multi-layer perceptron, whose weights are learned by minimising the discrepancies between the source image and the downsampled target image. Importantly, our formulation makes it possible to regularise only the mapping function, while avoiding regularisation of the outputs; thus producing crisp, natural-looking images. The proposed method is unsupervised, using only the specific source and guide images to fit the mapping. We evaluate our method on two different tasks, super-resolution of depth maps and of tree height maps. In both cases, we clearly outperform recent baselines in quantitative comparisons, while delivering visually much sharper outputs.",
        "中文标题": "引导超分辨率作为像素到像素的转换",
        "摘要翻译": "引导超分辨率是一个统一的框架，适用于多个计算机视觉任务，其中输入是某个目标量的低分辨率源图像（例如，使用飞行时间相机获取的透视深度）和来自不同域的高分辨率引导图像（例如，来自传统相机的灰度图像）；目标输出是源图像的高分辨率版本（在我们的例子中，是高分辨率深度图）。看待这个问题的标准方法是将其表述为超分辨率任务，即源图像被上采样到目标分辨率，同时从引导图像转移缺失的高频细节。在这里，我们提出将这种解释颠倒过来，而是将其视为引导图像到源图像域的像素到像素映射。像素级映射被参数化为多层感知器，其权重通过最小化源图像和下采样目标图像之间的差异来学习。重要的是，我们的公式使得仅对映射函数进行正则化成为可能，同时避免对输出进行正则化；从而产生清晰、自然的图像。所提出的方法是无监督的，仅使用特定的源图像和引导图像来拟合映射。我们在两个不同的任务上评估了我们的方法，深度图的超分辨率和树高图的超分辨率。在这两种情况下，我们在定量比较中明显优于最近的基线，同时提供了视觉上更清晰的输出。",
        "领域": "超分辨率/深度估计/图像处理",
        "问题": "如何从低分辨率源图像和高分辨率引导图像生成高分辨率的目标图像",
        "动机": "探索一种新的视角，将引导超分辨率问题视为像素到像素的映射，以提高图像质量和清晰度",
        "方法": "提出将引导超分辨率问题视为像素到像素的映射，使用多层感知器参数化映射，并通过最小化源图像和下采样目标图像之间的差异来学习权重，避免对输出进行正则化",
        "关键词": [
            "超分辨率",
            "深度估计",
            "图像处理"
        ],
        "涉及的技术概念": "引导超分辨率、多层感知器、像素到像素映射、无监督学习、图像正则化"
    },
    {
        "order": 673,
        "title": "Neural Inter-Frame Compression for Video Coding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Djelouah_Neural_Inter-Frame_Compression_for_Video_Coding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Djelouah_Neural_Inter-Frame_Compression_for_Video_Coding_ICCV_2019_paper.html",
        "abstract": "While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.",
        "中文标题": "神经帧间压缩用于视频编码",
        "摘要翻译": "尽管有许多基于深度学习的单图像压缩方法，端到端学习的视频编码领域仍然较少被探索。因此，在这项工作中，我们提出了一种用于神经视频编码的帧间压缩方法，它可以无缝地建立在不同的现有神经图像编解码器之上。我们的端到端解决方案通过在像素空间中进行基于光流的运动补偿来执行时间预测。关键的见解是，我们可以通过将所需信息编码到直接解码为运动和混合系数的潜在表示中，来提高解码效率和重建质量。为了考虑剩余的预测误差，需要在原始图像和插值帧之间计算残差信息。我们建议直接在潜在空间中计算残差，而不是在像素空间中，因为这允许对关键帧和中间帧重用相同的图像压缩网络。我们在不同数据集和分辨率上的扩展评估表明，我们的方法的率失真性能与现有的最先进编解码器具有竞争力。",
        "领域": "视频编码/神经网络/压缩技术",
        "问题": "如何提高视频编码的效率和重建质量",
        "动机": "探索端到端学习的视频编码方法，以解决现有方法在效率和重建质量上的不足",
        "方法": "提出了一种帧间压缩方法，通过在像素空间中进行基于光流的运动补偿来执行时间预测，并将信息编码到潜在表示中以提高解码效率和重建质量",
        "关键词": [
            "视频编码",
            "神经网络",
            "压缩技术",
            "光流",
            "潜在表示"
        ],
        "涉及的技术概念": "端到端学习、神经图像编解码器、光流、运动补偿、潜在表示、率失真性能"
    },
    {
        "order": 674,
        "title": "Unconstrained Foreground Object Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Unconstrained_Foreground_Object_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Unconstrained_Foreground_Object_Search_ICCV_2019_paper.html",
        "abstract": "Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines.",
        "中文标题": "无约束前景对象搜索",
        "摘要翻译": "许多人在编辑图像时搜索前景对象。虽然现有方法可以检索候选对象以帮助完成这一任务，但它们仅限于返回属于预先指定的语义类别的对象。我们提出了一个新颖的无约束前景对象（UFO）搜索问题，并介绍了一种解决方案，该方案通过在相同的潜在空间中编码背景图像和候选前景对象来支持高效搜索。我们工作的一个关键贡献是提出了一种无成本、可扩展的方法，用于创建大规模训练数据集，该数据集包含每个图像位置不同语义类别的多种前景对象。使用两个不同数据集进行的定量和人类感知实验证明了我们的UFO搜索解决方案相对于相关基线的优势。",
        "领域": "图像编辑/对象检索/语义分割",
        "问题": "如何在图像编辑中高效搜索无约束的前景对象",
        "动机": "现有方法在检索前景对象时受限于预定义的语义类别，无法满足用户对无约束前景对象搜索的需求",
        "方法": "提出了一种无约束前景对象搜索解决方案，通过在相同的潜在空间中编码背景图像和候选前景对象来支持高效搜索，并开发了一种无成本、可扩展的方法来创建大规模训练数据集",
        "关键词": [
            "前景对象",
            "图像编辑",
            "语义类别",
            "潜在空间",
            "训练数据集"
        ],
        "涉及的技术概念": "无约束前景对象搜索（UFO）是一种新颖的问题，旨在解决在图像编辑中搜索不受预定义语义类别限制的前景对象的需求。通过将背景图像和候选前景对象编码到相同的潜在空间中，实现了高效搜索。此外，提出了一种无成本、可扩展的方法来创建包含多种语义类别前景对象的大规模训练数据集，以支持模型的训练和优化。"
    },
    {
        "order": 675,
        "title": "Deep Learning for Light Field Saliency Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Learning_for_Light_Field_Saliency_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Learning_for_Light_Field_Saliency_Detection_ICCV_2019_paper.html",
        "abstract": "Recent research in 4D saliency detection is limited by the deficiency of a large-scale 4D light field dataset. To address this, we introduce a new dataset to assist the subsequent research in 4D light field saliency detection. To the best of our knowledge, this is to date the largest light field dataset in which the dataset provides 1465 all-focus images with human-labeled ground truth masks and the corresponding focal stacks for every light field image. To verify the effectiveness of the light field data, we first introduce a fusion framework which includes two CNN streams where the focal stacks and all-focus images serve as the input. The focal stack stream utilizes a recurrent attention mechanism to adaptively learn to integrate every slice in the focal stack, which benefits from the extracted features of the good slices. Then it is incorporated with the output map generated by the all-focus stream to make the saliency prediction. In addition, we introduce adversarial examples by adding noise intentionally into images to help train the deep network, which can improve the robustness of the proposed network. The noise is designed by users, which is imperceptible but can fool the CNNs to make the wrong prediction. Extensive experiments show the effectiveness and superiority of the proposed model on the popular evaluation metrics. The proposed method performs favorably compared with the existing 2D, 3D and 4D saliency detection methods on the proposed dataset and existing LFSD light field dataset. The code and results can be found at https://github.com/OIPLab-DUT/ ICCV2019_Deeplightfield_Saliency. Moreover, to facilitate research in this field, all images we collected are shared in a ready-to-use manner.",
        "中文标题": "深度学习用于光场显著性检测",
        "摘要翻译": "最近的4D显著性检测研究受限于缺乏大规模4D光场数据集。为了解决这个问题，我们引入了一个新的数据集以辅助后续的4D光场显著性检测研究。据我们所知，这是迄今为止最大的光场数据集，其中提供了1465张全焦图像，每张光场图像都有人工标注的真实掩码和相应的焦堆栈。为了验证光场数据的有效性，我们首先引入了一个融合框架，该框架包括两个CNN流，其中焦堆栈和全焦图像作为输入。焦堆栈流利用循环注意力机制自适应地学习整合焦堆栈中的每一片，这得益于从良好切片中提取的特征。然后，它与全焦流生成的输出图结合以进行显著性预测。此外，我们通过故意向图像中添加噪声来引入对抗样本，以帮助训练深度网络，这可以提高所提出网络的鲁棒性。噪声由用户设计，虽然不易察觉但可以欺骗CNN做出错误预测。大量实验表明，所提出的模型在流行的评估指标上具有有效性和优越性。所提出的方法在提出的数据集和现有的LFSD光场数据集上与现有的2D、3D和4D显著性检测方法相比表现良好。代码和结果可以在https://github.com/OIPLab-DUT/ICCV2019_Deeplightfield_Saliency找到。此外，为了促进这一领域的研究，我们收集的所有图像都以即用型方式共享。",
        "领域": "光场成像/显著性检测/对抗学习",
        "问题": "缺乏大规模4D光场数据集限制了4D显著性检测的研究",
        "动机": "为了解决4D显著性检测研究中缺乏大规模数据集的问题，并验证光场数据的有效性",
        "方法": "引入一个新的4D光场数据集，并提出一个融合框架，该框架包括两个CNN流，利用循环注意力机制整合焦堆栈中的每一片，并与全焦流生成的输出图结合进行显著性预测。此外，通过故意向图像中添加噪声来引入对抗样本，以提高网络的鲁棒性",
        "关键词": [
            "光场成像",
            "显著性检测",
            "对抗学习",
            "循环注意力机制",
            "CNN"
        ],
        "涉及的技术概念": {
            "4D光场数据集": "一个包含1465张全焦图像的数据集，每张图像都有人工标注的真实掩码和相应的焦堆栈",
            "融合框架": "一个包括两个CNN流的框架，用于处理焦堆栈和全焦图像",
            "循环注意力机制": "一种机制，用于自适应地学习整合焦堆栈中的每一片",
            "对抗样本": "通过故意向图像中添加噪声生成的样本，用于提高深度网络的鲁棒性"
        }
    },
    {
        "order": 676,
        "title": "Task2Vec: Task Embedding for Meta-Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Achille_Task2Vec_Task_Embedding_for_Meta-Learning_ICCV_2019_paper.html",
        "abstract": "We introduce a method to generate vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function, we process images through a \"probe network\" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and requires no understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks. We demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a novel task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well on which task. Selecting a feature extractor with task embedding yields performance close to the best available feature extractor, with substantially less computational effort than exhaustively training and evaluating all available models.",
        "中文标题": "Task2Vec: 元学习的任务嵌入",
        "摘要翻译": "我们介绍了一种生成视觉分类任务的向量表示的方法，这些表示可以用来推理这些任务的性质及其关系。给定一个带有真实标签的数据集和一个损失函数，我们通过一个'探针网络'处理图像，并基于与探针网络参数相关的费舍尔信息矩阵的估计计算嵌入。这提供了一个任务的固定维度嵌入，该嵌入独立于诸如类别数量等细节，并且不需要理解类别标签的语义。我们证明了这种嵌入能够预测任务相似性，这些相似性与我们对不同视觉任务之间语义和分类关系的直觉相匹配。我们展示了这个框架在选择预训练特征提取器用于新任务的元任务中的实际价值。我们提出了一个简单的元学习框架，用于学习嵌入上的度量，该度量能够预测哪些特征提取器在哪些任务上表现良好。使用任务嵌入选择特征提取器，其性能接近最佳可用特征提取器，且计算工作量大大少于详尽训练和评估所有可用模型。",
        "领域": "元学习/视觉分类/特征提取",
        "问题": "如何生成视觉分类任务的向量表示以推理任务性质及其关系",
        "动机": "为了更有效地选择预训练特征提取器用于新任务，减少计算工作量",
        "方法": "通过探针网络处理图像，基于费舍尔信息矩阵的估计计算任务嵌入，提出元学习框架学习嵌入上的度量",
        "关键词": [
            "任务嵌入",
            "元学习",
            "特征提取",
            "视觉分类",
            "费舍尔信息矩阵"
        ],
        "涉及的技术概念": {
            "探针网络": "用于处理图像并计算任务嵌入的网络",
            "费舍尔信息矩阵": "用于估计与探针网络参数相关的信息，从而计算任务嵌入",
            "元学习框架": "用于学习嵌入上的度量，预测特征提取器在任务上的表现"
        }
    },
    {
        "order": 677,
        "title": "Embodied Amodal Recognition: Learning to Move to Perceive Objects",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Embodied_Amodal_Recognition_Learning_to_Move_to_Perceive_Objects_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Embodied_Amodal_Recognition_Learning_to_Move_to_Perceive_Objects_ICCV_2019_paper.html",
        "abstract": "Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this problem, we develop a new model called Embodied Mask R-CNN for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using a simulator for indoor environments. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths.",
        "中文标题": "具身非模态识别：学习移动以感知物体",
        "摘要翻译": "被动视觉系统通常在物体被严重遮挡的非模态设置中无法识别物体。相比之下，人类和其他具身代理有能力在环境中移动并主动控制视角，以更好地理解物体的形状和语义。在这项工作中，我们引入了具身非模态识别（EAR）任务：一个代理被实例化在一个靠近被遮挡目标物体的3D环境中，并且可以自由移动以执行物体分类、非模态物体定位和非模态物体分割。为了解决这个问题，我们开发了一个名为具身Mask R-CNN的新模型，使代理能够学习战略性地移动以提高其视觉识别能力。我们使用室内环境模拟器进行实验。实验结果表明：1）具有具身性（移动）的代理比被动代理实现了更好的视觉识别性能；2）为了提高视觉识别能力，代理可以学习与最短路径不同的战略路径。",
        "领域": "具身智能/3D视觉/物体识别",
        "问题": "在物体被严重遮挡的情况下，被动视觉系统无法有效识别物体的问题",
        "动机": "探索具身代理通过移动和调整视角来改善视觉识别能力的可能性",
        "方法": "开发具身Mask R-CNN模型，使代理能够在3D环境中自由移动，以执行物体分类、定位和分割任务",
        "关键词": [
            "具身智能",
            "3D视觉",
            "物体识别",
            "非模态识别",
            "Mask R-CNN"
        ],
        "涉及的技术概念": {
            "具身非模态识别（EAR）": "一种任务，要求代理在3D环境中移动以识别被遮挡的物体",
            "具身Mask R-CNN": "一种改进的Mask R-CNN模型，使代理能够通过移动来提高视觉识别能力",
            "3D环境模拟器": "用于模拟室内环境，以测试和验证具身代理的视觉识别能力"
        }
    },
    {
        "order": 678,
        "title": "Optimizing the F-Measure for Threshold-Free Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Optimizing_the_F-Measure_for_Threshold-Free_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Optimizing_the_F-Measure_for_Threshold-Free_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Current CNN-based solutions to salient object detection (SOD) mainly rely on the optimization of cross-entropy loss (CELoss). Then the quality of detected saliency maps is often evaluated in terms of F-measure. In this paper, we investigate an interesting issue: can we consistently use the F-measure formulation in both training and evaluation for SOD? By reformulating the standard F-measure we propose the relaxed F-measure which is differentiable w.r.t the posterior and can be easily appended to the back of CNNs as the loss function. Compared to the conventional cross-entropy loss of which the gradients decrease dramatically in the saturated area, our loss function, named FLoss, holds considerable gradients even when the activation approaches the target. Consequently, the FLoss can continuously force the network to produce polarized activations. Comprehensive benchmarks on several popular datasets show that FLoss outperforms the state-of-the-art with a considerable margin. More specifically, due to the polarized predictions, our method is able to obtain high-quality saliency maps without carefully tuning the optimal threshold, showing significant advantages in real-world applications.",
        "中文标题": "优化无阈值显著目标检测的F-度量",
        "摘要翻译": "当前基于CNN的显著目标检测（SOD）解决方案主要依赖于交叉熵损失（CELoss）的优化。然后，检测到的显著性图的质量通常以F-度量来评估。在本文中，我们研究了一个有趣的问题：我们能否在SOD的训练和评估中一致地使用F-度量公式？通过重新制定标准F-度量，我们提出了松弛F-度量，它对后验是可微的，并且可以轻松地附加到CNN的后面作为损失函数。与传统的交叉熵损失相比，在饱和区域梯度急剧下降，我们的损失函数（命名为FLoss）即使在激活接近目标时也保持相当大的梯度。因此，FLoss可以持续迫使网络产生极化激活。在几个流行数据集上的综合基准测试表明，FLoss以相当大的优势超越了最先进的技术。更具体地说，由于极化预测，我们的方法能够在不需要仔细调整最佳阈值的情况下获得高质量的显著性图，在现实世界的应用中显示出显著的优势。",
        "领域": "显著目标检测/深度学习/图像分析",
        "问题": "在显著目标检测中，如何在训练和评估中一致地使用F-度量",
        "动机": "探索在显著目标检测的训练和评估过程中一致使用F-度量的可能性，以提高检测质量",
        "方法": "提出了一种松弛F-度量，作为可微的损失函数附加到CNN后面，以持续迫使网络产生极化激活",
        "关键词": [
            "显著目标检测",
            "F-度量",
            "CNN",
            "损失函数",
            "极化激活"
        ],
        "涉及的技术概念": "交叉熵损失（CELoss）：一种常用的损失函数，用于衡量模型预测值与真实值之间的差异。F-度量：一种结合了精确率和召回率的评估指标，用于评估分类模型的性能。松弛F-度量：本文提出的一种改进的F-度量，使其对后验可微，能够作为损失函数使用。极化激活：指网络输出倾向于极端值（如0或1）的激活状态，有助于提高显著性图的质量。"
    },
    {
        "order": 679,
        "title": "Deep Clustering by Gaussian Mixture Variational Autoencoders With Graph Embedding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Deep_Clustering_by_Gaussian_Mixture_Variational_Autoencoders_With_Graph_Embedding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Deep_Clustering_by_Gaussian_Mixture_Variational_Autoencoders_With_Graph_Embedding_ICCV_2019_paper.html",
        "abstract": "We propose DGG:  D eep clustering via a  G aussian-mixture variational autoencoder (VAE) with  G raph embedding. To facilitate clustering, we apply Gaussian mixture model (GMM) as the prior in VAE. To handle data with complex spread, we apply graph embedding. Our idea is that graph information which captures local data structures is an excellent complement to deep GMM. Combining them facilitates the network to learn powerful representations that follow global model and local structural constraints. Therefore, our method unifies model-based and similarity-based approaches for clustering. To combine graph embedding with probabilistic deep GMM, we propose a novel stochastic extension of graph embedding: we treat samples as nodes on a graph and minimize the weighted distance between their posterior distributions. We apply Jenson-Shannon divergence as the distance. We combine the divergence minimization with the log-likelihood maximization of the deep GMM. We derive formulations to obtain an unified objective that enables simultaneous deep representation learning and clustering. Our experimental results show that our proposed DGG outperforms recent deep Gaussian mixture methods (model-based) and deep spectral clustering (similarity-based). Our results highlight advantages of combining model-based and similarity-based clustering as proposed in this work. Our code is published here: https://github.com/dodoyang0929/DGG.git",
        "中文标题": "通过具有图嵌入的高斯混合变分自编码器进行深度聚类",
        "摘要翻译": "我们提出了DGG：通过具有图嵌入的高斯混合变分自编码器（VAE）进行深度聚类。为了促进聚类，我们在VAE中应用高斯混合模型（GMM）作为先验。为了处理具有复杂分布的数据，我们应用了图嵌入。我们的想法是，捕捉局部数据结构的图信息是对深度GMM的极好补充。将它们结合起来有助于网络学习遵循全局模型和局部结构约束的强大表示。因此，我们的方法统一了基于模型和基于相似性的聚类方法。为了将图嵌入与概率深度GMM结合起来，我们提出了一种新的图嵌入的随机扩展：我们将样本视为图上的节点，并最小化它们后验分布之间的加权距离。我们应用Jenson-Shannon散度作为距离。我们将散度最小化与深度GMM的对数似然最大化结合起来。我们推导了公式以获得一个统一的目标，该目标能够同时进行深度表示学习和聚类。我们的实验结果表明，我们提出的DGG优于最近的深度高斯混合方法（基于模型）和深度谱聚类（基于相似性）。我们的结果突出了结合基于模型和基于相似性的聚类方法的优势，如本工作所提出的。我们的代码发布在这里：https://github.com/dodoyang0929/DGG.git",
        "领域": "深度聚类/图嵌入/高斯混合模型",
        "问题": "如何在复杂数据分布的情况下进行有效的深度聚类",
        "动机": "为了结合基于模型和基于相似性的聚类方法的优势，提高聚类的效果",
        "方法": "提出了一种新的图嵌入的随机扩展方法，结合高斯混合变分自编码器进行深度聚类",
        "关键词": [
            "深度聚类",
            "图嵌入",
            "高斯混合模型",
            "变分自编码器",
            "Jenson-Shannon散度"
        ],
        "涉及的技术概念": "高斯混合模型（GMM）用于变分自编码器（VAE）的先验，图嵌入用于捕捉局部数据结构，Jenson-Shannon散度用于最小化样本后验分布之间的距离，结合深度表示学习和聚类。"
    },
    {
        "order": 680,
        "title": "SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SpatialSense_An_Adversarially_Crowdsourced_Benchmark_for_Spatial_Relation_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_SpatialSense_An_Adversarially_Crowdsourced_Benchmark_for_Spatial_Relation_Recognition_ICCV_2019_paper.html",
        "abstract": "Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be \"behind\" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be \"next to\" each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at https://github.com/princeton-vl/SpatialSense.",
        "中文标题": "空间感知：一个对抗性众包的空间关系识别基准",
        "摘要翻译": "理解图像中物体之间的空间关系是一项出奇地具有挑战性的任务。一把椅子可能在一个人“后面”，即使它在图像中出现在人的左侧（取决于人面向的方向）。在图像中看起来彼此接近的两个学生，如果中间有第三个学生，实际上可能并不“相邻”。我们介绍了SpatialSense，一个专门用于空间关系识别的数据集，它捕捉了广泛的此类挑战，使得计算机视觉技术能够得到适当的基准测试。SpatialSense通过对抗性众包构建，其中人类注释者被赋予寻找难以通过简单线索（如2D空间配置或语言先验）预测的空间关系的任务。与现有数据集相比，对抗性众包显著减少了数据集偏差，并采样了更多有趣的长尾关系。在SpatialSense上，最先进的识别模型的表现与简单基线相当，表明它们依赖于直接的线索，而不是完全推理这一复杂任务。SpatialSense基准为推进计算机视觉系统的空间推理能力提供了前进的道路。数据集和代码可在https://github.com/princeton-vl/SpatialSense获取。",
        "领域": "空间关系识别/对抗性学习/众包数据收集",
        "问题": "图像中物体间空间关系的准确识别",
        "动机": "现有的空间关系识别方法依赖于简单的线索，缺乏对复杂空间关系的深入理解，需要更高质量的数据集来推动技术进步。",
        "方法": "通过对抗性众包构建SpatialSense数据集，专门捕捉难以通过简单线索预测的空间关系，减少数据集偏差，并采样更多有趣的长尾关系。",
        "关键词": [
            "空间关系识别",
            "对抗性众包",
            "数据集偏差",
            "长尾关系"
        ],
        "涉及的技术概念": "对抗性众包是一种数据收集方法，通过让人类注释者寻找难以通过简单线索预测的空间关系，来构建更高质量的数据集。这种方法旨在减少数据集偏差，并采样更多有趣的长尾关系，从而推动计算机视觉系统在空间关系识别方面的进步。"
    },
    {
        "order": 681,
        "title": "Image Inpainting With Learnable Bidirectional Attention Maps",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Image_Inpainting_With_Learnable_Bidirectional_Attention_Maps_ICCV_2019_paper.html",
        "abstract": "Most convolutional network (CNN)-based inpainting methods adopt standard convolution to indistinguishably treat valid pixels and holes, making them limited in handling irregular holes and more likely to generate inpainting results with color discrepancy and blurriness. Partial convolution has been suggested to address this issue, but it adopts handcrafted feature re-normalization, and only considers forward mask-updating. In this paper, we present a learnable attention map module for learning feature re-normalization and mask-updating in an end-to-end manner, which is effective in adapting to irregular holes and propagation of convolution layers. Furthermore, learnable reverse attention maps are introduced to allow the decoder of U-Net to concentrate on filling in irregular holes instead of reconstructing both holes and known regions, resulting in our learnable bidirectional attention maps. Qualitative and quantitative experiments show that our method performs favorably against state-of-the-arts in generating sharper, more coherent and visually plausible inpainting results. The source code and pre-trained models will be available at: https://github.com/Vious/LBAM_inpainting/.",
        "中文标题": "使用可学习的双向注意力映射进行图像修复",
        "摘要翻译": "大多数基于卷积网络（CNN）的修复方法采用标准卷积来不加区分地处理有效像素和孔洞，这使得它们在处理不规则孔洞时受限，并且更可能生成颜色不一致和模糊的修复结果。部分卷积已被提出以解决这个问题，但它采用了手工制作的特征重新归一化，并且只考虑前向掩码更新。在本文中，我们提出了一个可学习的注意力映射模块，用于以端到端的方式学习特征重新归一化和掩码更新，这在适应不规则孔洞和卷积层传播方面是有效的。此外，引入了可学习的反向注意力映射，使U-Net的解码器能够专注于填充不规则孔洞，而不是同时重建孔洞和已知区域，从而形成了我们的可学习双向注意力映射。定性和定量实验表明，我们的方法在生成更清晰、更连贯和视觉上更可信的修复结果方面优于现有技术。源代码和预训练模型将在以下网址提供：https://github.com/Vious/LBAM_inpainting/。",
        "领域": "图像修复/卷积神经网络/注意力机制",
        "问题": "处理不规则孔洞的图像修复问题，以及生成结果中的颜色不一致和模糊问题",
        "动机": "现有的基于CNN的图像修复方法在处理不规则孔洞时效果有限，且容易产生颜色不一致和模糊的修复结果，需要一种更有效的方法来改进这些问题",
        "方法": "提出了一种可学习的注意力映射模块，用于以端到端的方式学习特征重新归一化和掩码更新，并引入了可学习的反向注意力映射，使U-Net的解码器能够专注于填充不规则孔洞",
        "关键词": [
            "图像修复",
            "卷积神经网络",
            "注意力机制",
            "不规则孔洞",
            "特征重新归一化",
            "掩码更新"
        ],
        "涉及的技术概念": "标准卷积、部分卷积、特征重新归一化、掩码更新、可学习的注意力映射、U-Net、反向注意力映射"
    },
    {
        "order": 682,
        "title": "SoftTriple Loss: Deep Metric Learning Without Triplet Sampling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.html",
        "abstract": "Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.",
        "中文标题": "SoftTriple损失：无需三元组采样的深度度量学习",
        "摘要翻译": "距离度量学习（DML）旨在学习嵌入，使得来自同一类的样本比来自不同类的样本更接近。它可以被转化为一个带有三元组约束的优化问题。由于三元组约束的数量庞大，采样策略对于DML至关重要。随着深度学习在分类中的巨大成功，它已被应用于DML。当使用深度神经网络（DNNs）学习嵌入时，每次迭代只能获得一小批数据。三元组约束集必须在小批量内采样。由于小批量无法很好地捕捉原始集中的邻居，这使得学习的嵌入次优。相反，使用DNN优化SoftMax损失（一种分类损失）在某些DML任务中表现出优越的性能。这激励我们研究SoftMax的公式。我们的分析表明，SoftMax损失等同于一种平滑的三元组损失，其中每个类都有一个中心。在现实世界的数据中，一个类可能包含几个局部集群而不是一个，例如不同姿势的鸟类。因此，我们提出了SoftTriple损失，以扩展SoftMax损失，为每个类提供多个中心。与传统的深度度量学习算法相比，通过适度增加最后一个全连接层的大小，优化SoftTriple损失可以在不进行采样阶段的情况下学习嵌入。在基准细粒度数据集上的实验证明了所提出损失函数的有效性。",
        "领域": "度量学习/细粒度分类/嵌入学习",
        "问题": "如何在无需三元组采样的条件下进行有效的深度度量学习",
        "动机": "传统的深度度量学习方法需要采样三元组，这在小批量数据中难以捕捉原始数据集的邻居关系，导致学习到的嵌入次优。而SoftMax损失在某些DML任务中表现出色，激励我们探索其公式并扩展以适应现实世界数据的复杂性。",
        "方法": "提出了SoftTriple损失，通过为每个类引入多个中心来扩展SoftMax损失，从而在无需采样阶段的情况下学习嵌入。",
        "关键词": [
            "度量学习",
            "细粒度分类",
            "嵌入学习",
            "SoftTriple损失",
            "SoftMax损失"
        ],
        "涉及的技术概念": "距离度量学习（DML）旨在通过优化嵌入来使同类样本比异类样本更接近。传统的DML方法依赖于三元组采样，而本文提出的SoftTriple损失通过扩展SoftMax损失，为每个类引入多个中心，从而避免了采样阶段，提高了嵌入学习的效率和效果。"
    },
    {
        "order": 683,
        "title": "TensorMask: A Foundation for Dense Object Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.html",
        "abstract": "Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.",
        "中文标题": "TensorMask: 密集目标分割的基础",
        "摘要翻译": "在密集、规则的网格上生成边界框目标预测的滑动窗口目标检测器发展迅速且广受欢迎。相比之下，现代实例分割方法主要由首先检测目标边界框，然后裁剪和分割这些区域的方法主导，如Mask R-CNN所推广的。在这项工作中，我们研究了密集滑动窗口实例分割的范式，这一范式出人意料地未被充分探索。我们的核心观察是，这项任务与其他密集预测任务（如语义分割或边界框目标检测）根本不同，因为每个空间位置的输出本身就是一个具有自己空间维度的几何结构。为了形式化这一点，我们将密集实例分割视为对4D张量的预测任务，并提出了一个名为TensorMask的通用框架，该框架明确捕捉了这种几何结构，并启用了4D张量上的新操作符。我们证明了张量视图相对于忽略这种结构的基线带来了巨大的增益，并得出了与Mask R-CNN相当的结果。这些有希望的结果表明，TensorMask可以作为密集掩码预测新进展的基础，以及对任务更全面理解的基础。代码将公开提供。",
        "领域": "实例分割/目标检测/几何结构预测",
        "问题": "密集滑动窗口实例分割的范式未被充分探索，且与其它密集预测任务有根本不同",
        "动机": "探索密集滑动窗口实例分割的范式，以捕捉每个空间位置的输出作为具有自己空间维度的几何结构",
        "方法": "将密集实例分割视为对4D张量的预测任务，提出TensorMask框架，明确捕捉几何结构并启用4D张量上的新操作符",
        "关键词": [
            "实例分割",
            "目标检测",
            "几何结构预测",
            "4D张量",
            "TensorMask"
        ],
        "涉及的技术概念": {
            "密集滑动窗口实例分割": "一种在密集、规则的网格上进行实例分割的方法，每个空间位置的输出是一个具有自己空间维度的几何结构",
            "4D张量": "在TensorMask框架中，用于表示密集实例分割任务的输出，每个位置包含一个几何结构",
            "TensorMask": "一个通用框架，用于处理密集实例分割任务，通过捕捉几何结构和启用4D张量上的新操作符来提高性能"
        }
    },
    {
        "order": 684,
        "title": "Joint Demosaicking and Denoising by Fine-Tuning of Bursts of Raw Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ehret_Joint_Demosaicking_and_Denoising_by_Fine-Tuning_of_Bursts_of_Raw_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ehret_Joint_Demosaicking_and_Denoising_by_Fine-Tuning_of_Bursts_of_Raw_ICCV_2019_paper.html",
        "abstract": "Demosaicking and denoising are the first steps of any camera image processing pipeline and are key for obtaining high quality RGB images. A promising current research trend aims at solving these two problems jointly using convolutional neural networks. Due to the unavailability of ground truth data these networks cannot be currently trained using real RAW images. Instead, they resort to simulated data. In this paper we present a method to learn demosaicking directly from mosaicked images, without requiring ground truth RGB data. We apply this to learn joint demosaicking and denoising only from RAW images, thus enabling the use of real data. In addition we show that for this application fine-tuning a network to a specific burst improves the quality of restoration for both demosaicking and denoising.",
        "中文标题": "通过微调原始图像序列实现联合去马赛克和去噪",
        "摘要翻译": "去马赛克和去噪是任何相机图像处理流程的第一步，对于获得高质量的RGB图像至关重要。当前一个有前景的研究趋势旨在使用卷积神经网络联合解决这两个问题。由于缺乏真实数据，这些网络目前无法使用真实的RAW图像进行训练。相反，它们依赖于模拟数据。在本文中，我们提出了一种直接从马赛克图像中学习去马赛克的方法，而不需要真实的RGB数据作为基础。我们将这种方法应用于仅从RAW图像中学习联合去马赛克和去噪，从而使得使用真实数据成为可能。此外，我们展示了对于这种应用，对特定图像序列微调网络可以提高去马赛克和去噪的恢复质量。",
        "领域": "图像恢复/卷积神经网络/RAW图像处理",
        "问题": "解决在缺乏真实RGB数据的情况下，直接从RAW图像中联合进行去马赛克和去噪的问题",
        "动机": "提高相机图像处理流程中第一步的质量，以获得更高质量的RGB图像",
        "方法": "提出了一种直接从马赛克图像中学习去马赛克的方法，并应用于仅从RAW图像中学习联合去马赛克和去噪，通过微调网络到特定图像序列来提高恢复质量",
        "关键词": [
            "去马赛克",
            "去噪",
            "卷积神经网络",
            "RAW图像",
            "图像恢复"
        ],
        "涉及的技术概念": "去马赛克是指从马赛克图像中恢复出全彩图像的过程；去噪是指减少图像中的噪声，提高图像质量；卷积神经网络是一种深度学习模型，特别适用于处理图像数据；RAW图像是相机直接输出的未经处理的图像数据，包含了丰富的原始信息。"
    },
    {
        "order": 685,
        "title": "A Weakly Supervised Fine Label Classifier Enhanced by Coarse Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Taherkhani_A_Weakly_Supervised_Fine_Label_Classifier_Enhanced_by_Coarse_Supervision_ICCV_2019_paper.html",
        "abstract": "Objects are usually organized in a hierarchical structure in which each coarse category (e.g., big cat) corresponds to a superclass of several fine categories (e.g., cheetah, leopard). The objects grouped within the same coarse category, but in different fine categories, usually share a set of global visual features; however, these objects have distinctive local properties that characterize them at a fine level. This paper addresses the challenge of fine image classification in a weakly supervised fashion, whereby a subset of images is tagged by fine labels, while the remaining are tagged by coarse labels. We propose a new deep model that leverages coarse images to improve the classification performance of fine images within the coarse category. Our model is an end to end framework consisting of a Convolutional Neural Network (CNN) which uses both fine and coarse images to tune its parameters. The CNN outputs are then fanned out into two separate branches such that the first branch uses a supervised low rank self expressive layer to project the CNN outputs to the low rank subspaces to capture the global structures for the coarse classification, while the other branch uses a supervised sparse self expressive layer to project them to the sparse subspaces to capture the local structures for the fine classification. Our deep model uses coarse images in conjunction with fine images to jointly explore the low rank and sparse subspaces by sharing the parameters during the training which causes the data points obtained by the CNN to be well-projected to both sparse and low rank subspaces for classification.",
        "中文标题": "通过粗监督增强的弱监督细标签分类器",
        "摘要翻译": "对象通常以层次结构组织，其中每个粗类别（例如，大猫）对应于几个细类别（例如，猎豹、豹子）的超类。同一粗类别但不同细类别中的对象通常共享一组全局视觉特征；然而，这些对象具有独特的局部属性，这些属性在细级别上表征它们。本文以弱监督的方式解决了细图像分类的挑战，其中一部分图像被细标签标记，而其余图像被粗标签标记。我们提出了一种新的深度模型，该模型利用粗图像来提高粗类别内细图像的分类性能。我们的模型是一个端到端的框架，由卷积神经网络（CNN）组成，该网络使用细图像和粗图像来调整其参数。然后，CNN的输出被分成两个独立的分支，使得第一个分支使用监督的低秩自表达层将CNN输出投影到低秩子空间以捕获粗分类的全局结构，而另一个分支使用监督的稀疏自表达层将它们投影到稀疏子空间以捕获细分类的局部结构。我们的深度模型在训练期间通过共享参数，将粗图像与细图像结合使用，共同探索低秩和稀疏子空间，这使得CNN获得的数据点能够很好地投影到稀疏和低秩子空间进行分类。",
        "领域": "图像分类/弱监督学习/深度学习",
        "问题": "在弱监督条件下进行细粒度图像分类",
        "动机": "利用粗标签图像提高细标签图像的分类性能",
        "方法": "提出一种新的深度模型，该模型通过共享参数在训练期间共同探索低秩和稀疏子空间，以改善细粒度图像分类",
        "关键词": [
            "细粒度图像分类",
            "弱监督学习",
            "卷积神经网络",
            "低秩子空间",
            "稀疏子空间"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，用于处理图像数据，通过卷积层提取特征。",
            "低秩自表达层": "一种用于捕捉数据全局结构的层，通过将数据投影到低秩子空间来实现。",
            "稀疏自表达层": "一种用于捕捉数据局部结构的层，通过将数据投影到稀疏子空间来实现。",
            "弱监督学习": "一种机器学习方法，其中训练数据的标签不完全准确或完整。"
        }
    },
    {
        "order": 686,
        "title": "DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kupyn_DeblurGAN-v2_Deblurring_Orders-of-Magnitude_Faster_and_Better_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kupyn_DeblurGAN-v2_Deblurring_Orders-of-Magnitude_Faster_and_Better_ICCV_2019_paper.html",
        "abstract": "We present a new end-to-end generative adversarial network (GAN) for single image motion deblurring, named DeblurGAN-V2, which considerably boosts state-of-the-art deblurring performance while being much more flexible and efficient. DeblurGAN-V2 is based on a relativistic conditional GAN with a double-scale discriminator. For the first time, we introduce the Feature Pyramid Network into deblurring, as a core building block in the generator of DeblurGAN-V2. It can flexibly work with a wide range of backbones, to navigate the balance between performance and efficiency. The plug-in of sophisticated backbones (e.g. Inception ResNet v2) can lead to solid state-of-the-art performance. Meanwhile, with light-weight backbones (e.g. MobileNet and its variants), DeblurGAN-V2 becomes 10-100 times faster than the nearest competitors, while maintaining close to state-of-the-art results, implying the option of real-time video deblurring. We demonstrate that DeblurGAN-V2 has very competitive performance on several popular benchmarks, in terms of deblurring quality (both objective and subjective), as well as efficiency. In addition, we show the architecture to be effective for general image restoration tasks too. Our models and codes will be made available upon acceptance.",
        "中文标题": "DeblurGAN-v2: 更快更好的去模糊（数量级）",
        "摘要翻译": "我们提出了一种新的端到端生成对抗网络（GAN），用于单图像运动去模糊，名为DeblurGAN-V2，它显著提升了最先进的去模糊性能，同时更加灵活和高效。DeblurGAN-V2基于一个相对条件GAN，带有双尺度判别器。我们首次将特征金字塔网络引入去模糊，作为DeblurGAN-V2生成器的核心构建块。它可以灵活地与多种骨干网络配合使用，以导航性能与效率之间的平衡。复杂骨干网络（如Inception ResNet v2）的插件可以带来稳固的最先进性能。同时，使用轻量级骨干网络（如MobileNet及其变体），DeblurGAN-V2比最近的竞争对手快10-100倍，同时保持接近最先进的结果，这意味着实时视频去模糊的选项。我们证明DeblurGAN-V2在几个流行基准上具有非常竞争力的性能，无论是在去模糊质量（客观和主观）还是效率方面。此外，我们展示了该架构对于一般图像恢复任务也是有效的。我们的模型和代码将在接受后提供。",
        "领域": "图像去模糊/生成对抗网络/特征金字塔网络",
        "问题": "单图像运动去模糊",
        "动机": "提升去模糊性能，同时增加灵活性和效率",
        "方法": "基于相对条件GAN和双尺度判别器的DeblurGAN-V2，首次引入特征金字塔网络作为生成器的核心构建块，灵活配合多种骨干网络使用",
        "关键词": [
            "图像去模糊",
            "生成对抗网络",
            "特征金字塔网络",
            "相对条件GAN",
            "双尺度判别器"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）是一种深度学习模型，通过生成器和判别器的对抗过程来生成数据。特征金字塔网络是一种用于处理不同尺度特征的网络结构，能够有效地提取和融合多尺度特征。相对条件GAN是一种改进的GAN，通过引入相对判别器来提高生成图像的质量。双尺度判别器则是指同时处理不同尺度的输入，以提高模型的判别能力。"
    },
    {
        "order": 687,
        "title": "Integral Object Mining via Online Attention Accumulation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Integral_Object_Mining_via_Online_Attention_Accumulation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Integral_Object_Mining_via_Online_Attention_Accumulation_ICCV_2019_paper.html",
        "abstract": "Object attention maps generated by image classifiers are usually used as priors for weakly-supervised segmentation approaches. However, normal image classifiers produce attention only at the most discriminative object parts, which limits the performance of weakly-supervised segmentation task. Therefore, how to effectively identify entire object regions in a weakly-supervised manner has always been a challenging and meaningful problem. We observe that the attention maps produced by a classification network continuously focus on different object parts during training. In order to accumulate the discovered different object parts, we propose an online attention accumulation (OAA) strategy which maintains a cumulative attention map for each target category in each training image so that the integral object regions can be gradually promoted as the training goes. These cumulative attention maps, in turn, serve as the pixel-level supervision, which can further assist the network in discovering more integral object regions. Our method (OAA) can be plugged into any classification network and progressively accumulate the discriminative regions into integral objects as the training process goes. Despite its simplicity, when applying the resulting attention maps to the weakly-supervised semantic segmentation task, our approach improves the existing state-of-the-art methods on the PASCAL VOC 2012 segmentation benchmark, achieving a mIoU score of 66.4% on the test set. Code is available at https://mmcheng.net/oaa/.",
        "中文标题": "通过在线注意力积累进行整体对象挖掘",
        "摘要翻译": "由图像分类器生成的对象注意力图通常被用作弱监督分割方法的先验。然而，普通的图像分类器仅在最具区分性的对象部分产生注意力，这限制了弱监督分割任务的性能。因此，如何有效地以弱监督方式识别整个对象区域一直是一个具有挑战性和意义的问题。我们观察到，在训练过程中，分类网络产生的注意力图持续关注不同的对象部分。为了积累发现的不同对象部分，我们提出了一种在线注意力积累（OAA）策略，该策略为每个训练图像中的每个目标类别维护一个累积注意力图，以便随着训练的进行，整体对象区域可以逐渐提升。这些累积注意力图反过来作为像素级监督，可以进一步帮助网络发现更完整的对象区域。我们的方法（OAA）可以插入到任何分类网络中，并随着训练过程的进行，逐步将区分性区域积累为整体对象。尽管方法简单，但当将生成的注意力图应用于弱监督语义分割任务时，我们的方法在PASCAL VOC 2012分割基准上改进了现有的最先进方法，在测试集上达到了66.4%的mIoU分数。代码可在https://mmcheng.net/oaa/获取。",
        "领域": "语义分割/注意力机制/弱监督学习",
        "问题": "如何有效地以弱监督方式识别整个对象区域",
        "动机": "普通图像分类器仅在最具区分性的对象部分产生注意力，限制了弱监督分割任务的性能",
        "方法": "提出在线注意力积累（OAA）策略，通过维护每个训练图像中每个目标类别的累积注意力图，逐步提升整体对象区域",
        "关键词": [
            "语义分割",
            "注意力机制",
            "弱监督学习"
        ],
        "涉及的技术概念": "在线注意力积累（OAA）策略是一种通过维护累积注意力图来逐步提升整体对象区域的方法，这些累积注意力图作为像素级监督，帮助网络发现更完整的对象区域。"
    },
    {
        "order": 688,
        "title": "Gaussian Affinity for Max-Margin Class Imbalanced Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hayat_Gaussian_Affinity_for_Max-Margin_Class_Imbalanced_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hayat_Gaussian_Affinity_for_Max-Margin_Class_Imbalanced_Learning_ICCV_2019_paper.html",
        "abstract": "Real-world object classes appear in imbalanced ratios. This poses a significant challenge for classifiers which get biased towards frequent classes. We hypothesize that improving the generalization capability of a classifier should improve learning on imbalanced datasets. Here, we introduce the first hybrid loss function that jointly performs classification and clustering in a single formulation. Our approach is based on an `affinity measure' in Euclidean space that leads to the following benefits: (1) direct enforcement of maximum margin constraints on classification boundaries, (2) a tractable way to ensure uniformly spaced and equidistant cluster centers, (3) flexibility to learn multiple class prototypes to support diversity and discriminability in feature space. Our extensive experiments demonstrate the significant performance improvements on visual classification and verification tasks on multiple imbalanced datasets. The proposed loss can easily be plugged in any deep architecture as a differentiable block and demonstrates robustness against different levels of data imbalance and corrupted labels.",
        "中文标题": "高斯亲和力用于最大边距类别不平衡学习",
        "摘要翻译": "现实世界中的对象类别以不平衡的比例出现。这对分类器构成了重大挑战，分类器偏向于频繁出现的类别。我们假设提高分类器的泛化能力应该能改善在不平衡数据集上的学习。在这里，我们引入了第一个混合损失函数，它在单一公式中同时执行分类和聚类。我们的方法基于欧几里得空间中的“亲和力度量”，带来了以下好处：（1）直接对分类边界施加最大边距约束，（2）确保均匀间隔和等距聚类中心的可追踪方法，（3）灵活性以学习多个类别原型，以支持特征空间中的多样性和可区分性。我们的大量实验证明了在多个不平衡数据集上的视觉分类和验证任务的显著性能改进。所提出的损失可以轻松地作为可微分块插入任何深度架构中，并展示了对不同级别的数据不平衡和损坏标签的鲁棒性。",
        "领域": "视觉分类/不平衡学习/特征空间优化",
        "问题": "解决现实世界中对象类别出现不平衡比例导致的分类器偏向频繁类别的问题",
        "动机": "提高分类器的泛化能力以改善在不平衡数据集上的学习",
        "方法": "引入混合损失函数，在单一公式中同时执行分类和聚类，基于欧几里得空间中的亲和力度量，直接对分类边界施加最大边距约束，确保均匀间隔和等距聚类中心，学习多个类别原型以支持特征空间中的多样性和可区分性",
        "关键词": [
            "视觉分类",
            "不平衡学习",
            "特征空间优化",
            "混合损失函数",
            "亲和力度量"
        ],
        "涉及的技术概念": "混合损失函数：一种同时执行分类和聚类的损失函数；亲和力度量：在欧几里得空间中用于衡量样本之间相似性的方法；最大边距约束：一种优化分类边界的方法，旨在最大化不同类别之间的边距；类别原型：用于表示类别特征的样本或特征向量。"
    },
    {
        "order": 689,
        "title": "Reflective Decoding Network for Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions.",
        "中文标题": "用于图像描述的反射解码网络",
        "摘要翻译": "最先进的图像描述方法大多专注于改进视觉特征，较少关注利用语言的固有属性来提升描述性能。在本文中，我们展示了词汇之间的连贯性和句子的句法范式对于生成高质量图像描述同样重要。遵循传统的编码器-解码器框架，我们提出了用于图像描述的反射解码网络（RDN），该网络增强了描述解码器中单词的长序列依赖性和位置感知。我们的模型学会同时关注视觉和文本特征，并感知句子中每个单词的相对位置，以最大化生成描述中传递的信息。我们在COCO图像描述数据集上评估了RDN的有效性，并取得了优于之前方法的性能。进一步的实验表明，我们的方法特别适用于描述复杂场景的困难案例。",
        "领域": "图像描述/自然语言处理/深度学习",
        "问题": "如何利用语言的固有属性提升图像描述的性能",
        "动机": "现有图像描述方法大多只关注视觉特征的改进，忽视了语言属性对提升描述质量的重要性",
        "方法": "提出了反射解码网络（RDN），该网络通过增强描述解码器中单词的长序列依赖性和位置感知，同时关注视觉和文本特征，以最大化生成描述中传递的信息",
        "关键词": [
            "图像描述",
            "反射解码网络",
            "长序列依赖性",
            "位置感知"
        ],
        "涉及的技术概念": "反射解码网络（RDN）是一种改进的图像描述方法，它通过增强解码器中单词的长序列依赖性和位置感知，同时关注视觉和文本特征，以提升描述的质量。这种方法特别适用于处理复杂场景的描述任务。"
    },
    {
        "order": 690,
        "title": "Accelerated Gravitational Point Set Alignment With Altered Physical Laws",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Golyanik_Accelerated_Gravitational_Point_Set_Alignment_With_Altered_Physical_Laws_ICCV_2019_paper.html",
        "abstract": "This work describes Barnes-Hut Rigid Gravitational Approach (BH-RGA) -- a new rigid point set registration method relying on principles of particle dynamics. Interpreting the inputs as two interacting particle swarms, we directly minimise the gravitational potential energy of the system using non-linear least squares. Compared to solutions obtained by solving systems of second-order ordinary differential equations, our approach is more robust and less dependent on the parameter choice. We accelerate otherwise exhaustive particle interactions with a Barnes-Hut tree and efficiently handle massive point sets in quasilinear time while preserving the globally multiply-linked character of interactions. Among the advantages of BH-RGA is the possibility to define boundary conditions or additional alignment cues through varying point masses. Systematic experiments demonstrate that BH-RGA surpasses performances of baseline methods in terms of the convergence basin and accuracy when handling incomplete, noisy and perturbed data. The proposed approach also positively compares to the competing method for the alignment with prior matches.",
        "中文标题": "加速的引力点集对齐与改变的物理定律",
        "摘要翻译": "本工作描述了Barnes-Hut刚性引力方法（BH-RGA）——一种新的刚性点集配准方法，依赖于粒子动力学的原理。将输入解释为两个相互作用的粒子群，我们使用非线性最小二乘法直接最小化系统的引力势能。与通过求解二阶常微分方程系统获得的解决方案相比，我们的方法更加鲁棒，且对参数选择的依赖性较小。我们通过Barnes-Hut树加速了原本耗尽的粒子相互作用，并在保持全局多重链接相互作用特性的同时，有效地处理了大规模点集，实现了准线性时间。BH-RGA的优势之一是通过改变点质量来定义边界条件或额外的对齐线索。系统实验表明，在处理不完整、噪声和扰动数据时，BH-RGA在收敛盆地和准确性方面超越了基线方法的性能。所提出的方法在与先前匹配的对齐方面也与竞争方法相比具有优势。",
        "领域": "点集配准/粒子动力学/非线性优化",
        "问题": "解决刚性点集配准问题，特别是在处理大规模、不完整、噪声和扰动数据时的效率和准确性。",
        "动机": "提高点集配准的鲁棒性和效率，减少对参数选择的依赖，同时保持全局多重链接相互作用的特性。",
        "方法": "采用Barnes-Hut刚性引力方法（BH-RGA），通过非线性最小二乘法最小化系统的引力势能，利用Barnes-Hut树加速粒子相互作用，实现准线性时间处理大规模点集。",
        "关键词": [
            "点集配准",
            "粒子动力学",
            "非线性优化",
            "Barnes-Hut树",
            "引力势能"
        ],
        "涉及的技术概念": {
            "Barnes-Hut刚性引力方法（BH-RGA）": "一种新的刚性点集配准方法，依赖于粒子动力学的原理，通过非线性最小二乘法直接最小化系统的引力势能。",
            "非线性最小二乘法": "一种数学优化技术，用于最小化目标函数，这里用于最小化系统的引力势能。",
            "Barnes-Hut树": "一种用于加速N体问题计算的数据结构，通过将空间划分为不同区域来减少计算量。",
            "引力势能": "在物理学中，指由于物体之间的引力相互作用而具有的能量，这里用于描述点集之间的相互作用。"
        }
    },
    {
        "order": 691,
        "title": "Joint Optimization for Cooperative Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Vered_Joint_Optimization_for_Cooperative_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "When describing images with natural language, descriptions can be made more informative if tuned for downstream tasks. This can be achieved by training two networks: a \"speaker\" that generates sentences given an image and a \"listener\" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language. To address these challenges, we present an effective optimization technique based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. We then show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the COCO benchmark show that PSST improve the recall@10 from 60% to 86% maintaining comparable language naturalness. Human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions.",
        "中文标题": "联合优化合作式图像描述",
        "摘要翻译": "当用自然语言描述图像时，如果针对下游任务进行调整，描述可以变得更加信息丰富。这可以通过训练两个网络来实现：一个“说话者”网络根据图像生成句子，一个“听者”网络使用这些句子来执行任务。不幸的是，联合训练多个网络以进行通信面临两个主要挑战。首先，说话者网络生成的描述是离散且随机的，这使得优化非常困难且效率低下。其次，联合训练通常会导致通信中使用的词汇偏离自然语言。为了解决这些挑战，我们提出了一种基于从多项分布中部分采样结合直通梯度更新的有效优化技术，我们将其命名为PSST（Partial-Sampling Straight-Through）。然后我们展示了通过约束生成的描述与人类描述相似，可以保持其接近自然语言。这种方法共同创造了比以往方法更具区分性和更自然的描述。在COCO基准上的评估显示，PSST将recall@10从60%提高到86%，同时保持了可比较的语言自然度。人类评估显示，它在保持生成描述区分能力的同时也增加了自然度。",
        "领域": "图像描述/自然语言生成/优化技术",
        "问题": "联合训练多个网络以进行通信时，描述生成的离散性和随机性导致的优化困难，以及通信中使用的词汇偏离自然语言的问题。",
        "动机": "为了提高图像描述的区分性和自然度，使其更适合下游任务。",
        "方法": "提出了一种基于从多项分布中部分采样结合直通梯度更新的优化技术PSST，并通过约束生成的描述与人类描述相似来保持其接近自然语言。",
        "关键词": [
            "图像描述",
            "自然语言生成",
            "优化技术"
        ],
        "涉及的技术概念": {
            "PSST": "Partial-Sampling Straight-Through，一种基于从多项分布中部分采样结合直通梯度更新的优化技术。",
            "recall@10": "一种评估指标，用于衡量在前10个结果中正确检索到的相关项目的比例。",
            "COCO基准": "一个广泛使用的图像描述和对象检测的基准数据集。"
        }
    },
    {
        "order": 692,
        "title": "Domain Adaptation for Semantic Segmentation With Maximum Squares Loss",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.html",
        "abstract": "Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.",
        "中文标题": "使用最大平方损失进行语义分割的领域适应",
        "摘要翻译": "深度神经网络在语义分割方面总是需要大量带有像素级标签的样本，这成为其在实际应用中的主要困难。为了减少标注成本，提出了无监督领域适应（UDA）方法，以从标注的合成数据集向未标注的现实世界数据集转移知识。最近，一些半监督学习方法被应用于UDA，并取得了最先进的性能。半监督学习中最流行的方法之一是熵最小化方法。然而，当将熵最小化应用于语义分割的UDA时，熵的梯度偏向于易于转移的样本。为了平衡分类良好的目标样本的梯度，我们提出了最大平方损失。我们的最大平方损失防止了训练过程被目标域中易于转移的样本所主导。此外，我们引入了图像级加权比，以缓解未标注目标域中的类别不平衡问题。无论是从合成到真实还是跨城市的适应实验，都证明了我们提出的方法的有效性。代码已发布在https://github.com/ZJULearning/MaxSquareLoss。",
        "领域": "语义分割/领域适应/半监督学习",
        "问题": "减少语义分割中像素级标签样本的需求，降低标注成本",
        "动机": "解决深度神经网络在语义分割应用中需要大量标注样本的问题，通过无监督领域适应方法减少标注成本",
        "方法": "提出最大平方损失方法，防止训练过程被目标域中易于转移的样本所主导，并引入图像级加权比缓解类别不平衡问题",
        "关键词": [
            "语义分割",
            "领域适应",
            "半监督学习",
            "最大平方损失",
            "图像级加权比"
        ],
        "涉及的技术概念": "深度神经网络、无监督领域适应（UDA）、熵最小化方法、最大平方损失、图像级加权比"
    },
    {
        "order": 693,
        "title": "AttPool: Towards Hierarchical Feature Representation in Graph Convolutional Networks via Attention Mechanism",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_AttPool_Towards_Hierarchical_Feature_Representation_in_Graph_Convolutional_Networks_via_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_AttPool_Towards_Hierarchical_Feature_Representation_in_Graph_Convolutional_Networks_via_ICCV_2019_paper.html",
        "abstract": "Graph convolutional networks (GCNs) are potentially short of the ability to learn hierarchical representation for graph embedding, which holds them back in the graph classification task. Here, we propose AttPool, which is a novel graph pooling module based on attention mechanism, to remedy the problem. It is able to select nodes that are significant for graph representation adaptively, and generate hierarchical features via aggregating the attention-weighted information in nodes. Additionally, we devise a hierarchical prediction architecture to sufficiently leverage the hierarchical representation and facilitate the model learning. The AttPool module together with the entire training structure can be integrated into existing GCNs, and is trained in an end-to-end fashion conveniently. The experimental results on several graph-classification benchmark datasets with various scales demonstrate the effectiveness of our method.",
        "中文标题": "AttPool：通过注意力机制在图卷积网络中实现层次特征表示",
        "摘要翻译": "图卷积网络（GCNs）可能缺乏学习图嵌入的层次表示的能力，这限制了它们在图分类任务中的表现。在此，我们提出了AttPool，这是一个基于注意力机制的新型图池化模块，旨在解决这一问题。它能够自适应地选择对图表示重要的节点，并通过聚合节点中的注意力加权信息生成层次特征。此外，我们设计了一个层次预测架构，以充分利用层次表示并促进模型学习。AttPool模块与整个训练结构可以集成到现有的GCNs中，并且可以方便地进行端到端训练。在多个不同规模的图分类基准数据集上的实验结果证明了我们方法的有效性。",
        "领域": "图神经网络/注意力机制/图分类",
        "问题": "图卷积网络在图分类任务中缺乏有效的层次特征表示能力",
        "动机": "提高图卷积网络在图分类任务中的表现，通过引入层次特征表示来增强模型的学习能力",
        "方法": "提出了基于注意力机制的图池化模块AttPool，设计了一个层次预测架构，以充分利用层次表示并促进模型学习",
        "关键词": [
            "图卷积网络",
            "注意力机制",
            "图池化",
            "层次特征表示",
            "图分类"
        ],
        "涉及的技术概念": "图卷积网络（GCNs）是一种用于图结构数据的深度学习模型，能够捕捉节点间的复杂关系。注意力机制是一种使模型能够专注于输入数据中重要部分的技术。图池化是一种减少图数据规模并保留重要信息的技术，有助于提高模型的效率和性能。层次特征表示指的是在不同层次上捕捉和表示数据的特征，这对于理解复杂数据结构非常重要。"
    },
    {
        "order": 694,
        "title": "Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rahman_Watch_Listen_and_Tell_Multi-Modal_Weakly_Supervised_Dense_Event_Captioning_ICCV_2019_paper.html",
        "abstract": "Multi-modal learning, particularly among imaging and linguistic modalities, has made amazing strides in many high-level fundamental visual understanding problems, ranging from language grounding to dense event captioning. However, much of the research has been limited to approaches that either do not take audio corresponding to video into account at all, or those that model the audio-visual correlations in service of sound or sound source localization. In this paper, we present the evidence, that audio signals can carry surprising amount of information when it comes to high-level visual-lingual tasks. Specifically, we focus on the problem of weakly-supervised dense event captioning in videos and show that audio on its own can nearly rival performance of a state-of-the-art visual model and, combined with video, can improve on the state-of-the-art performance. Extensive experiments on the ActivityNet Captions dataset show that our proposed multi-modal approach outperforms state-of-the-art unimodal methods, as well as validate specific feature representation and architecture design choices.",
        "中文标题": "观看、聆听与讲述：多模态弱监督密集事件描述",
        "摘要翻译": "多模态学习，特别是在图像和语言模态之间，在从语言基础到密集事件描述的许多高级基础视觉理解问题上取得了惊人的进展。然而，许多研究仅限于那些完全不考虑与视频对应的音频的方法，或者那些为了声音或声源定位而建模音频-视觉相关性的方法。在本文中，我们提出了证据，表明音频信号在高级视觉-语言任务中可以携带惊人的信息量。具体来说，我们专注于视频中的弱监督密集事件描述问题，并表明仅音频就可以几乎媲美最先进的视觉模型的性能，并且与视频结合可以超越最先进的性能。在ActivityNet Captions数据集上的大量实验表明，我们提出的多模态方法优于最先进的单模态方法，并验证了特定的特征表示和架构设计选择。",
        "领域": "密集事件描述/多模态学习/音频-视觉分析",
        "问题": "视频中的弱监督密集事件描述",
        "动机": "探索音频信号在高级视觉-语言任务中的信息量，以及如何利用音频和视频结合来提升密集事件描述的性能。",
        "方法": "提出了一种多模态方法，结合音频和视频信息，通过特定的特征表示和架构设计，用于弱监督密集事件描述。",
        "关键词": [
            "密集事件描述",
            "多模态学习",
            "音频-视觉分析"
        ],
        "涉及的技术概念": "多模态学习涉及图像和语言模态的结合，用于解决高级视觉理解问题。弱监督学习指的是在训练过程中使用不完全标注的数据。密集事件描述是指在视频中为每个事件生成详细的文字描述。音频-视觉分析是指同时利用音频和视频信息来理解和描述事件。"
    },
    {
        "order": 695,
        "title": "Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.html",
        "abstract": "We propose to harness the potential of simulation for semantic segmentation of real-world self-driving scenes in a domain generalization fashion. The segmentation network is trained without any information about target domains and tested on the unseen target domains. To this end, we propose a new approach of domain randomization and pyramid consistency to learn a model with high generalizability. First, we propose to randomize the synthetic images with styles of real images in terms of visual appearances using auxiliary datasets, in order to effectively learn domain-invariant representations. Second, we further enforce pyramid consistency across different \"stylized\" images and within an image, in order to learn domain-invariant and scale-invariant features, respectively. Extensive experiments are conducted on generalization from GTA and SYNTHIA to Cityscapes, BDDS, and Mapillary; and our method achieves superior results over the state-of-the-art techniques. Remarkably, our generalization results are on par with or even better than those obtained by state-of-the-art simulation-to-real domain adaptation methods, which access the target domain data at training time.",
        "中文标题": "领域随机化和金字塔一致性：无需访问目标域数据的仿真到现实的泛化",
        "摘要翻译": "我们提出利用仿真的潜力，以领域泛化的方式对现实世界自动驾驶场景进行语义分割。分割网络在没有任何关于目标域信息的情况下进行训练，并在未见过的目标域上进行测试。为此，我们提出了一种新的领域随机化和金字塔一致性的方法，以学习具有高泛化能力的模型。首先，我们提出使用辅助数据集随机化合成图像的视觉外观风格，以有效学习领域不变表示。其次，我们进一步在不同“风格化”图像之间和图像内部强制执行金字塔一致性，以分别学习领域不变和尺度不变特征。在从GTA和SYNTHIA到Cityscapes、BDDS和Mapillary的泛化实验中，我们的方法取得了优于现有技术的结果。值得注意的是，我们的泛化结果与那些在训练时访问目标域数据的最先进的仿真到现实领域适应方法相当甚至更好。",
        "领域": "自动驾驶/语义分割/领域泛化",
        "问题": "在无需访问目标域数据的情况下，提高语义分割模型在未见过的目标域上的泛化能力",
        "动机": "利用仿真数据训练模型，以解决现实世界自动驾驶场景中的语义分割问题，同时避免对目标域数据的依赖",
        "方法": "提出领域随机化和金字塔一致性的方法，通过随机化合成图像的视觉外观风格和强制执行金字塔一致性，学习领域不变和尺度不变特征",
        "关键词": [
            "领域随机化",
            "金字塔一致性",
            "语义分割",
            "自动驾驶",
            "领域泛化"
        ],
        "涉及的技术概念": "领域随机化是一种通过随机化合成数据的视觉外观风格来学习领域不变表示的技术。金字塔一致性是一种在不同“风格化”图像之间和图像内部强制执行一致性的方法，用于学习领域不变和尺度不变特征。这些技术概念被应用于提高语义分割模型在未见过的目标域上的泛化能力。"
    },
    {
        "order": 696,
        "title": "Deep Metric Learning With Tuplet Margin Loss",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.html",
        "abstract": "Deep metric learning, in which the loss function plays a key role, has proven to be extremely useful in visual recognition tasks. However, existing deep metric learning loss functions such as contrastive loss and triplet loss usually rely on delicately selected samples (pairs or triplets) for fast convergence. In this paper, we propose a new deep metric learning loss function, tuplet margin loss, using randomly selected samples from each mini-batch. Specifically, the proposed tuplet margin loss implicitly up-weights hard samples and down-weights easy samples, while a slack margin in angular space is introduced to mitigate the problem of overfitting on the hardest sample. Furthermore, we address the problem of intra-pair variation by disentangling class-specific information to improve the generalizability of tuplet margin loss. Experimental results on three widely used deep metric learning datasets, CARS196, CUB200-2011, and Stanford Online Products, demonstrate significant improvements over existing deep metric learning methods.",
        "中文标题": "使用元组边际损失进行深度度量学习",
        "摘要翻译": "深度度量学习，其中损失函数起着关键作用，已被证明在视觉识别任务中极为有用。然而，现有的深度度量学习损失函数，如对比损失和三元组损失，通常依赖于精心选择的样本（对或三元组）以实现快速收敛。在本文中，我们提出了一种新的深度度量学习损失函数，元组边际损失，使用从每个小批量中随机选择的样本。具体来说，提出的元组边际损失隐式地增加了难样本的权重并减少了易样本的权重，同时在角度空间中引入了松弛边际以缓解对最难样本的过拟合问题。此外，我们通过解耦类特定信息来解决对内变异问题，以提高元组边际损失的泛化能力。在三个广泛使用的深度度量学习数据集CARS196、CUB200-2011和Stanford Online Products上的实验结果表明，与现有的深度度量学习方法相比，有显著的改进。",
        "领域": "视觉识别/度量学习/损失函数优化",
        "问题": "现有深度度量学习损失函数依赖于精心选择的样本以实现快速收敛，可能导致过拟合和对内变异问题",
        "动机": "提高深度度量学习的泛化能力和效率，减少对样本选择的依赖",
        "方法": "提出了一种新的深度度量学习损失函数，元组边际损失，通过随机选择样本并引入松弛边际来优化损失函数",
        "关键词": [
            "元组边际损失",
            "深度度量学习",
            "损失函数优化"
        ],
        "涉及的技术概念": "深度度量学习是一种通过优化损失函数来提高模型在视觉识别任务中性能的技术。本文提出的元组边际损失是一种新的损失函数，它通过随机选择样本并引入松弛边际来优化模型，旨在提高模型的泛化能力和减少过拟合。"
    },
    {
        "order": 697,
        "title": "Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Joint_Syntax_Representation_Learning_and_Visual_Cue_Translation_for_Video_ICCV_2019_paper.html",
        "abstract": "Video captioning is a challenging task that involves not only visual perception but also syntax representation learning. Recent progress in video captioning has been achieved through visual perception, but syntax representation learning is still under-explored. We propose a novel video captioning approach that takes into account both visual perception and syntax representation learning to generate accurate descriptions of videos. Specifically, we use sentence templates composed of Part-of-Speech (POS) tags to represent the syntax structure of captions, and accordingly, syntax representation learning is performed by directly inferring POS tags from videos. The visual perception is implemented by a mixture model which translates visual cues into lexical words that are conditional on the learned syntactic structure of sentences. Thus, a video captioning task consists of two sub-tasks: video POS tagging and visual cue translation, which are jointly modeled and trained in an end-to-end fashion. Evaluations on three public benchmark datasets demonstrate that our proposed method achieves substantially better performance than the state-of-the-art methods, which validates the superiority of joint modeling of syntax representation learning and visual perception for video captioning.",
        "中文标题": "联合语法表示学习和视觉线索翻译用于视频字幕生成",
        "摘要翻译": "视频字幕生成是一项具有挑战性的任务，它不仅涉及视觉感知，还包括语法表示学习。最近在视频字幕生成方面的进展主要是通过视觉感知实现的，但语法表示学习仍然未被充分探索。我们提出了一种新颖的视频字幕生成方法，该方法同时考虑了视觉感知和语法表示学习，以生成准确的视频描述。具体来说，我们使用由词性（POS）标签组成的句子模板来表示字幕的语法结构，并相应地通过直接从视频推断POS标签来进行语法表示学习。视觉感知通过一个混合模型实现，该模型将视觉线索翻译成依赖于学习到的句子语法结构的词汇。因此，视频字幕生成任务包括两个子任务：视频POS标记和视觉线索翻译，这两个子任务以端到端的方式联合建模和训练。在三个公共基准数据集上的评估表明，我们提出的方法比最先进的方法实现了显著更好的性能，这验证了联合建模语法表示学习和视觉感知在视频字幕生成中的优越性。",
        "领域": "视频字幕生成/语法表示学习/视觉感知",
        "问题": "视频字幕生成中的语法表示学习未被充分探索",
        "动机": "提高视频字幕生成的准确性，通过联合考虑视觉感知和语法表示学习",
        "方法": "使用由词性标签组成的句子模板表示语法结构，通过混合模型将视觉线索翻译成词汇，联合建模和训练视频POS标记和视觉线索翻译",
        "关键词": [
            "视频字幕生成",
            "语法表示学习",
            "视觉感知",
            "词性标签",
            "混合模型"
        ],
        "涉及的技术概念": {
            "语法表示学习": "通过直接从视频推断词性（POS）标签来学习字幕的语法结构",
            "视觉感知": "通过混合模型将视觉线索翻译成依赖于学习到的句子语法结构的词汇",
            "视频POS标记": "从视频中直接推断出词性标签，用于表示字幕的语法结构",
            "视觉线索翻译": "将视频中的视觉信息转换为文本词汇，依赖于句子的语法结构"
        }
    },
    {
        "order": 698,
        "title": "Semi-Supervised Skin Detection by Network With Mutual Guidance",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Semi-Supervised_Skin_Detection_by_Network_With_Mutual_Guidance_ICCV_2019_paper.html",
        "abstract": "We present a new data-driven method for robust skin detection from a single human portrait image. Unlike previous methods, we incorporate human body as a weak semantic guidance into this task, considering acquiring large-scale of human labeled skin data is commonly expensive and time-consuming. To be specific, we propose a dual-task neural network for joint detection of skin and body via a semi-supervised learning strategy. The dual-task network contains a shared encoder but two decoders for skin and body separately. For each decoder, its output also serves as a guidance for its counterpart, making both decoders mutually guided. Extensive experiments were conducted to demonstrate the effectiveness of our network with mutual guidance, and experimental results show our network outperforms the state-of-the-art in skin detection.",
        "中文标题": "半监督皮肤检测：通过网络进行相互指导",
        "摘要翻译": "我们提出了一种新的数据驱动方法，用于从单张人像图像中进行鲁棒的皮肤检测。与之前的方法不同，我们将人体作为弱语义指导纳入此任务，考虑到获取大规模人工标注的皮肤数据通常既昂贵又耗时。具体来说，我们提出了一种双任务神经网络，通过半监督学习策略联合检测皮肤和身体。该双任务网络包含一个共享编码器，但有两个分别用于皮肤和身体的解码器。对于每个解码器，其输出也作为其对应解码器的指导，使得两个解码器相互指导。我们进行了大量实验来证明我们具有相互指导的网络的有效性，实验结果表明我们的网络在皮肤检测方面优于现有技术。",
        "领域": "皮肤检测/半监督学习/神经网络",
        "问题": "从单张人像图像中进行鲁棒的皮肤检测",
        "动机": "获取大规模人工标注的皮肤数据既昂贵又耗时",
        "方法": "提出了一种双任务神经网络，通过半监督学习策略联合检测皮肤和身体，网络包含一个共享编码器和两个分别用于皮肤和身体的解码器，两个解码器相互指导",
        "关键词": [
            "皮肤检测",
            "半监督学习",
            "神经网络",
            "双任务网络",
            "相互指导"
        ],
        "涉及的技术概念": "双任务神经网络：一种包含共享编码器和两个分别用于不同任务（此处为皮肤和身体检测）的解码器的网络结构。半监督学习策略：一种利用少量标注数据和大量未标注数据进行学习的方法。相互指导：两个解码器的输出相互作为对方的指导，以提高检测的准确性。"
    },
    {
        "order": 699,
        "title": "Normalized Wasserstein for Mixture Distributions With Applications in Adversarial Learning and Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.html",
        "abstract": "Understanding proper distance measures between distributions is at the core of several learning tasks such as generative models, domain adaptation, clustering, etc. In this work, we focus on mixture distributions that arise naturally in several application domains where the data contains different sub-populations. For mixture distributions, established distance measures such as the Wasserstein distance do not take into account imbalanced mixture proportions. Thus, even if two mixture distributions have identical mixture components but different mixture proportions, the Wasserstein distance between them will be large. This often leads to undesired results in distance-based learning methods for mixture distributions. In this paper, we resolve this issue by introducing the Normalized Wasserstein measure. The key idea is to introduce mixture proportions as optimization variables, effectively normalizing mixture proportions in the Wasserstein formulation. Using the proposed normalized Wasserstein measure leads to significant performance gains for mixture distributions with imbalanced mixture proportions compared to the vanilla Wasserstein distance. We demonstrate the effectiveness of the proposed measure in GANs, domain adaptation and adversarial clustering in several benchmark datasets.",
        "中文标题": "混合分布的归一化Wasserstein距离及其在对抗学习和领域适应中的应用",
        "摘要翻译": "理解分布之间的适当距离度量是生成模型、领域适应、聚类等学习任务的核心。在本研究中，我们专注于在多个应用领域中自然出现的混合分布，这些数据包含不同的子群体。对于混合分布，已建立的距离度量如Wasserstein距离没有考虑到混合比例的不平衡。因此，即使两个混合分布具有相同的混合成分但不同的混合比例，它们之间的Wasserstein距离也会很大。这通常会导致基于距离的学习方法在处理混合分布时产生不理想的结果。在本文中，我们通过引入归一化Wasserstein度量来解决这个问题。关键思想是将混合比例作为优化变量引入，有效地在Wasserstein公式中归一化混合比例。使用提出的归一化Wasserstein度量，与普通的Wasserstein距离相比，在处理混合比例不平衡的混合分布时，可以显著提高性能。我们在多个基准数据集上的GANs、领域适应和对抗聚类中证明了所提出度量的有效性。",
        "领域": "生成对抗网络/领域适应/对抗聚类",
        "问题": "处理混合分布中混合比例不平衡导致的Wasserstein距离度量不准确问题",
        "动机": "为了改进基于距离的学习方法在处理混合分布时的性能，特别是在混合比例不平衡的情况下",
        "方法": "引入归一化Wasserstein度量，将混合比例作为优化变量，在Wasserstein公式中归一化混合比例",
        "关键词": [
            "归一化Wasserstein距离",
            "混合分布",
            "对抗学习",
            "领域适应",
            "对抗聚类"
        ],
        "涉及的技术概念": "Wasserstein距离是一种衡量两个概率分布之间差异的度量。在本文中，针对混合分布中混合比例不平衡的问题，提出了归一化Wasserstein度量，通过将混合比例作为优化变量引入，来改进距离度量的准确性。这种方法在生成对抗网络（GANs）、领域适应和对抗聚类等应用中显示出显著的性能提升。"
    },
    {
        "order": 700,
        "title": "Entangled Transformer for Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "In image captioning, the typical attention mechanisms are arduous to identify the equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as the semantic gap between vision and language. This problem can be overcome by providing semantic attributes that are homologous to language. Thanks to the inherent recurrent nature and gated operating mechanism, Recurrent Neural Network (RNN) and its variants are the dominating architectures in image captioning. However, when designing elaborate attention mechanisms to integrate visual inputs and semantic attributes, RNN-like variants become unflexible due to their complexities. In this paper, we investigate a Transformer-based sequence modeling framework, built only with attention layers and feedforward layers. To bridge the semantic gap, we introduce EnTangled Attention (ETA) that enables the Transformer to exploit semantic and visual information simultaneously. Furthermore, Gated Bilateral Controller (GBC) is proposed to guide the interactions between the multimodal information. We name our model as ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on the MSCOCO image captioning dataset. The ablation studies validate the improvements of our proposed modules.",
        "中文标题": "用于图像描述的纠缠Transformer",
        "摘要翻译": "在图像描述中，典型的注意力机制难以识别等效的视觉信号，尤其是在预测高度抽象的词汇时。这种现象被称为视觉与语言之间的语义鸿沟。通过提供与语言同源的语义属性，可以克服这一问题。由于循环神经网络（RNN）及其变体固有的循环特性和门控操作机制，它们成为图像描述中的主导架构。然而，在设计复杂的注意力机制以整合视觉输入和语义属性时，类似RNN的变体由于其复杂性而变得不灵活。在本文中，我们研究了一种基于Transformer的序列建模框架，该框架仅由注意力层和前馈层构建。为了弥合语义鸿沟，我们引入了纠缠注意力（ETA），使Transformer能够同时利用语义和视觉信息。此外，提出了门控双边控制器（GBC）来指导多模态信息之间的交互。我们将我们的模型命名为ETA-Transformer。值得注意的是，ETA-Transformer在MSCOCO图像描述数据集上实现了最先进的性能。消融研究验证了我们提出的模块的改进。",
        "领域": "图像描述/自然语言处理/序列建模",
        "问题": "解决图像描述中视觉与语言之间的语义鸿沟问题",
        "动机": "由于现有注意力机制在预测高度抽象词汇时难以识别等效的视觉信号，且RNN及其变体在处理复杂注意力机制时不够灵活，因此需要一种新的方法来更有效地整合视觉和语义信息。",
        "方法": "提出了一种基于Transformer的序列建模框架，引入了纠缠注意力（ETA）和门控双边控制器（GBC）来同时利用语义和视觉信息，并指导多模态信息之间的交互。",
        "关键词": [
            "图像描述",
            "Transformer",
            "注意力机制",
            "语义鸿沟",
            "门控双边控制器"
        ],
        "涉及的技术概念": {
            "注意力机制": "一种用于增强模型对输入数据特定部分关注的技术，广泛应用于图像描述等任务中。",
            "Transformer": "一种基于自注意力机制的序列建模框架，适用于处理序列数据，如文本或图像描述。",
            "语义鸿沟": "指视觉信息与语言描述之间的差异，导致模型难以准确描述图像内容。",
            "门控双边控制器": "一种用于控制多模态信息交互的机制，通过门控机制调节信息流。"
        }
    },
    {
        "order": 701,
        "title": "ACE: Adapting to Changing Environments for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_ACE_Adapting_to_Changing_Environments_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_ACE_Adapting_to_Changing_Environments_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Deep neural networks exhibit exceptional accuracy when they are trained and tested on the same data distributions. However, neural classifiers are often extremely brittle when confronted with domain shift---changes in the input distribution that occur over time. We present ACE, a framework for semantic segmentation that dynamically adapts to changing environments over time. By aligning the distribution of labeled training data from the original source domain with the distribution of incoming data in a shifted domain, ACE synthesizes labeled training data for environments as it sees them. This stylized data is then used to update a segmentation model so that it performs well in new environments. To avoid forgetting knowledge from past environments, we introduce a memory that stores feature statistics from previously seen domains. These statistics can be used to replay images in any of the previously observed domains, thus preventing catastrophic forgetting. In addition to standard batch training using stochastic gradient decent (SGD), we also experiment with fast adaptation methods based on adaptive meta-learning. Extensive experiments are conducted on two datasets from SYNTHIA, the results demonstrate the effectiveness of the proposed approach when adapting to a number of tasks.",
        "中文标题": "ACE: 适应变化环境的语义分割",
        "摘要翻译": "深度神经网络在训练和测试数据分布相同时表现出极高的准确性。然而，当面对领域转移——输入分布随时间变化时，神经分类器往往极其脆弱。我们提出了ACE，一个能够动态适应随时间变化环境的语义分割框架。通过将原始源域的标记训练数据分布与转移域中的输入数据分布对齐，ACE为所见环境合成了标记训练数据。这种风格化的数据随后被用于更新分割模型，使其在新环境中表现良好。为了避免忘记过去环境的知识，我们引入了一个存储先前所见领域特征统计的记忆。这些统计可以用于重现在任何先前观察到的领域的图像，从而防止灾难性遗忘。除了使用随机梯度下降（SGD）的标准批量训练外，我们还尝试了基于自适应元学习的快速适应方法。在SYNTHIA的两个数据集上进行了广泛的实验，结果证明了所提出方法在适应多个任务时的有效性。",
        "领域": "语义分割/领域适应/元学习",
        "问题": "深度神经网络在面对领域转移时的脆弱性问题",
        "动机": "提高深度神经网络在输入分布随时间变化的环境中的适应性和准确性",
        "方法": "通过对齐原始源域和转移域的数据分布，合成标记训练数据，并引入记忆机制存储特征统计，使用自适应元学习方法进行快速适应",
        "关键词": [
            "语义分割",
            "领域适应",
            "元学习",
            "灾难性遗忘",
            "自适应学习"
        ],
        "涉及的技术概念": "领域转移指的是输入数据分布随时间变化的现象，可能导致模型性能下降。ACE框架通过合成标记训练数据来适应这种变化，同时引入记忆机制防止模型忘记过去环境的知识。自适应元学习是一种快速适应新任务的方法，通过少量数据即可调整模型参数。"
    },
    {
        "order": 702,
        "title": "Fast and Practical Neural Architecture Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cui_Fast_and_Practical_Neural_Architecture_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cui_Fast_and_Practical_Neural_Architecture_Search_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a fast and practical neural architecture search (FPNAS) framework for automatic network design. FPNAS aims to discover extremely efficient networks with less than 300M FLOPs. Different from previous NAS methods, our approach searches for the whole network architecture to guarantee block diversity instead of stacking a set of similar blocks repeatedly. We model the search process as a bi-level optimization problem and propose an approximation solution. On CIFAR-10, our approach is capable of design networks with comparable performance to state-of-the-arts while using orders of magnitude less computational resource with only 20 GPU hours. Experimental results on ImageNet and ADE20K datasets further demonstrate transferability of the searched networks.",
        "中文标题": "快速实用的神经架构搜索",
        "摘要翻译": "在本文中，我们提出了一个快速实用的神经架构搜索（FPNAS）框架，用于自动网络设计。FPNAS旨在发现计算量少于300M FLOPs的极其高效的网络。与之前的NAS方法不同，我们的方法搜索整个网络架构以保证块的多样性，而不是重复堆叠一组相似的块。我们将搜索过程建模为一个双层优化问题，并提出了一个近似解决方案。在CIFAR-10上，我们的方法能够设计出与最先进技术性能相当的网络，同时仅使用20 GPU小时的计算资源，计算量减少了几个数量级。在ImageNet和ADE20K数据集上的实验结果进一步证明了搜索网络的迁移能力。",
        "领域": "神经架构搜索/自动网络设计/计算效率",
        "问题": "如何在保证网络性能的同时，减少神经架构搜索的计算资源消耗",
        "动机": "为了设计出计算效率高且性能优越的网络架构，减少神经架构搜索过程中的计算资源消耗",
        "方法": "提出一个快速实用的神经架构搜索框架，通过建模搜索过程为双层优化问题并采用近似解决方案，搜索整个网络架构以保证块的多样性",
        "关键词": [
            "神经架构搜索",
            "自动网络设计",
            "计算效率",
            "双层优化",
            "块多样性"
        ],
        "涉及的技术概念": {
            "神经架构搜索（NAS）": "一种自动化设计神经网络架构的技术，旨在发现最优的网络结构。",
            "FLOPs": "浮点运算次数，用于衡量模型的计算复杂度。",
            "双层优化问题": "一种优化问题，其中包含两个层次的决策变量，通常用于处理具有层次结构的优化问题。",
            "CIFAR-10": "一个常用的图像分类数据集，包含10个类别的60000张32x32彩色图像。",
            "ImageNet": "一个大规模视觉识别挑战赛使用的数据集，包含超过1400万张标注图像。",
            "ADE20K": "一个用于场景解析的数据集，包含超过20000张图像，涵盖150个类别。"
        }
    },
    {
        "order": 703,
        "title": "Shapeglot: Learning Language for Shape Differentiation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Achlioptas_Shapeglot_Learning_Language_for_Shape_Differentiation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Achlioptas_Shapeglot_Learning_Language_for_Shape_Differentiation_ICCV_2019_paper.html",
        "abstract": "In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on 2D and/or 3D object representations. We first build a large scale, carefully controlled dataset of human utterances each of which refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are capable of zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit supervision on such parts), and that transfer to novel classes is most successful when known part-related words are available. This work illustrates a practical approach to language grounding, and provides a novel case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",
        "中文标题": "Shapeglot: 学习用于形状区分的语言",
        "摘要翻译": "在这项工作中，我们探索了常见物体形状之间的细微差别如何在语言中表达，基于2D和/或3D物体表示。我们首先构建了一个大规模、精心控制的人类话语数据集，每个话语都指代一个3D CAD模型的2D渲染，以便将其与一组形状相似的替代品区分开来。使用这个数据集，我们开发了神经语言理解（听）和生成（说）模型，这些模型在其基础（通过点云的纯3D形式与渲染的2D图像）、捕捉的实用推理程度（例如，推理听众的说话者与否）以及神经架构（例如，有或没有注意力）方面有所不同。我们发现这些模型在与合成和人类伙伴以及保留的话语和物体上都表现良好。我们还发现，这些模型能够进行零样本转移学习到新的物体类别（例如，从椅子的训练转移到灯的测试），以及到从家具目录中提取的真实世界图像。病变研究表明，神经听众严重依赖于与部分相关的词汇，并将这些词汇正确地与物体的视觉部分相关联（没有任何关于这些部分的明确监督），并且当已知的部分相关词汇可用时，转移到新类别最为成功。这项工作说明了语言基础的实际方法，并提供了一个关于物体形状和语言结构之间关系的新案例研究，当涉及到物体区分时。",
        "领域": "自然语言处理/计算语言学/认知科学",
        "问题": "如何通过语言表达常见物体形状之间的细微差别",
        "动机": "探索物体形状与语言结构之间的关系，以及如何通过语言基础实现物体区分",
        "方法": "构建大规模数据集，开发神经语言理解和生成模型，考虑不同的基础、实用推理程度和神经架构",
        "关键词": [
            "语言基础",
            "形状区分",
            "零样本转移学习"
        ],
        "涉及的技术概念": "神经语言理解模型、神经语言生成模型、点云、2D渲染图像、实用推理、注意力机制、零样本转移学习、病变研究"
    },
    {
        "order": 704,
        "title": "Efficient Segmentation: Learning Downsampling Near Semantic Boundaries",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Marin_Efficient_Segmentation_Learning_Downsampling_Near_Semantic_Boundaries_ICCV_2019_paper.html",
        "abstract": "Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.",
        "中文标题": "高效分割：在语义边界附近学习下采样",
        "摘要翻译": "许多自动化过程，如自动驾驶，依赖于良好的语义分割作为关键组件。为了提高性能，通常会对输入帧进行下采样。然而，这会以错过小物体和降低语义边界处的准确性为代价。为了解决这个问题，我们提出了一种新的内容自适应下采样技术，该技术学习倾向于在目标类别的语义边界附近进行采样。成本性能分析表明，我们的方法在准确性和计算效率之间的平衡上始终优于均匀采样。我们的自适应采样提供了边界质量更好的分割，并对较小尺寸的物体提供了更可靠的支持。",
        "领域": "语义分割/自动驾驶/图像分析",
        "问题": "下采样过程中小物体的丢失和语义边界处准确性的降低",
        "动机": "提高语义分割的准确性和计算效率，特别是在自动驾驶等自动化过程中",
        "方法": "提出了一种新的内容自适应下采样技术，该技术学习在目标类别的语义边界附近进行采样",
        "关键词": [
            "语义分割",
            "下采样",
            "自动驾驶",
            "图像分析"
        ],
        "涉及的技术概念": "内容自适应下采样技术是一种改进的下采样方法，它通过在学习过程中倾向于在目标类别的语义边界附近进行采样，以提高分割的准确性和计算效率。这种方法特别适用于需要高精度语义分割的应用场景，如自动驾驶。"
    },
    {
        "order": 705,
        "title": "Symmetric Graph Convolutional Autoencoder for Unsupervised Graph Representation Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Symmetric_Graph_Convolutional_Autoencoder_for_Unsupervised_Graph_Representation_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Symmetric_Graph_Convolutional_Autoencoder_for_Unsupervised_Graph_Representation_Learning_ICCV_2019_paper.html",
        "abstract": "We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.",
        "中文标题": "对称图卷积自编码器用于无监督图表示学习",
        "摘要翻译": "我们提出了一种对称图卷积自编码器，它能够从图中生成低维潜在表示。与现有的具有非对称解码器部分的图自编码器相比，所提出的自编码器具有新设计的解码器，构建了一个完全对称的自编码器形式。为了重建节点特征，解码器基于拉普拉斯锐化设计，作为编码器拉普拉斯平滑的对应物，这使得在所提出的自编码器架构的整个过程中都能利用图结构。为了防止由于引入拉普拉斯锐化而导致的网络数值不稳定性，我们进一步提出了一种新的数值稳定的拉普拉斯锐化形式，通过结合有符号图来实现。此外，为了提升图像聚类任务的性能，设计了一种新的成本函数，该函数同时寻找潜在表示和潜在亲和矩阵。在聚类、链接预测和可视化任务上的实验结果强烈支持所提出的模型是稳定的，并且优于各种最先进的算法。",
        "领域": "图神经网络/自编码器/图像聚类",
        "问题": "如何在无监督学习环境中有效地从图中生成低维潜在表示",
        "动机": "现有的图自编码器通常具有非对称的解码器部分，这限制了它们在利用图结构方面的能力。为了克服这一限制，提出了一种新的对称图卷积自编码器。",
        "方法": "设计了一种新的对称图卷积自编码器，其解码器基于拉普拉斯锐化，与编码器的拉普拉斯平滑相对应。为了防止数值不稳定性，提出了一种新的数值稳定的拉普拉斯锐化形式。此外，设计了一种新的成本函数，用于同时寻找潜在表示和潜在亲和矩阵。",
        "关键词": [
            "图卷积自编码器",
            "无监督学习",
            "图表示学习",
            "拉普拉斯锐化",
            "图像聚类"
        ],
        "涉及的技术概念": "拉普拉斯锐化是一种用于增强图像或图结构中高频成分的技术，与拉普拉斯平滑相对。有符号图是一种图结构，其中边可以表示正或负的关系。潜在亲和矩阵是指在学习过程中自动发现的节点间的关系矩阵，用于表示节点间的相似性或连接强度。"
    },
    {
        "order": 706,
        "title": "nocaps: novel object captioning at scale",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Agrawal_nocaps_novel_object_captioning_at_scale_ICCV_2019_paper.html",
        "abstract": "Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed 'nocaps', for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images image-level labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work.",
        "中文标题": "大规模新物体描述：nocaps",
        "摘要翻译": "图像描述模型在包含有限视觉概念和大量配对图像-描述训练数据的数据集上取得了令人印象深刻的成果。然而，如果这些模型要在实际环境中发挥作用，必须学习到更多种类的视觉概念，理想情况下是从较少的监督中学习。为了鼓励开发能够从替代数据源（如物体检测数据集）学习视觉概念的图像描述模型，我们提出了第一个用于此任务的大规模基准。被称为'nocaps'，即大规模新物体描述，我们的基准由166,100个人类生成的描述组成，描述了来自Open Images验证和测试集的15,100张图像。相关的训练数据包括COCO图像-描述对，加上Open Images图像级标签和物体边界框。由于Open Images包含比COCO更多的类别，测试图像中看到的近400个物体类别没有或很少有相关的训练描述（因此，nocaps）。我们扩展了现有的新物体描述模型，为此基准建立了强大的基线，并提供了分析以指导未来的工作。",
        "领域": "图像描述/物体检测/数据集构建",
        "问题": "图像描述模型在实际应用中需要学习更多种类的视觉概念，而现有数据集提供的视觉概念有限。",
        "动机": "鼓励开发能够从替代数据源（如物体检测数据集）学习视觉概念的图像描述模型，以应对实际环境中的挑战。",
        "方法": "提出了一个名为'nocaps'的大规模基准，包含166,100个人类生成的描述，描述了15,100张图像，并扩展了现有的新物体描述模型来建立基线。",
        "关键词": [
            "图像描述",
            "物体检测",
            "数据集构建"
        ],
        "涉及的技术概念": "图像描述模型、视觉概念学习、物体检测数据集、COCO图像-描述对、Open Images图像级标签和物体边界框、新物体描述模型"
    },
    {
        "order": 707,
        "title": "Recurrent U-Net for Resource-Constrained Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Recurrent_U-Net_for_Resource-Constrained_Segmentation_ICCV_2019_paper.html",
        "abstract": "State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation.",
        "中文标题": "用于资源受限分割的循环U-Net",
        "摘要翻译": "最先进的分割方法依赖于非常深的网络，这些网络在没有非常大的训练数据集的情况下并不总是容易训练，并且往往在标准GPU上运行相对较慢。在本文中，我们介绍了一种新颖的循环U-Net架构，该架构保留了原始U-Net的紧凑性，同时显著提高了其性能，使其在多个基准测试中超越了最先进的技术。我们将展示其在多个任务中的有效性，包括手部分割、视网膜血管分割和道路分割。我们还引入了一个用于手部分割的大规模数据集。",
        "领域": "医学图像分割/自动驾驶/遥感图像分析",
        "问题": "解决在资源受限环境下进行高效图像分割的问题",
        "动机": "现有的深度网络分割方法需要大量数据和计算资源，难以在资源受限的环境下有效应用",
        "方法": "提出了一种新颖的循环U-Net架构，该架构在保持原始U-Net紧凑性的同时，显著提高了性能",
        "关键词": [
            "循环U-Net",
            "图像分割",
            "资源受限",
            "手部分割",
            "视网膜血管分割",
            "道路分割"
        ],
        "涉及的技术概念": {
            "循环U-Net": "一种改进的U-Net架构，通过引入循环机制来提高分割性能",
            "图像分割": "将图像分割成多个部分或对象的技术",
            "资源受限": "指在计算资源（如内存、处理能力）有限的环境下进行图像处理"
        }
    },
    {
        "order": 708,
        "title": "Deep Elastic Networks With Model Selection for Multi-Task Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ahn_Deep_Elastic_Networks_With_Model_Selection_for_Multi-Task_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ahn_Deep_Elastic_Networks_With_Model_Selection_for_Multi-Task_Learning_ICCV_2019_paper.html",
        "abstract": "In this work, we consider the problem of instance-wise dynamic network model selection for multi-task learning. To this end, we propose an efficient approach to exploit a compact but accurate model in a backbone architecture for each instance of all tasks. The proposed method consists of an estimator and a selector. The estimator is based on a backbone architecture and structured hierarchically. It can produce multiple different network models of different configurations in a hierarchical structure. The selector chooses a model dynamically from a pool of candidate models given an input instance. The selector is a relatively small-size network consisting of a few layers, which estimates a probability distribution over the candidate models when an input instance of a task is given. Both estimator and selector are jointly trained in a unified learning framework in conjunction with a sampling-based learning strategy, without additional computation steps. We demonstrate the proposed approach for several image classification tasks compared to existing approaches performing model selection or learning multiple tasks. Experimental results show that our approach gives not only outstanding performance compared to other competitors but also the versatility to perform instance-wise model selection for multiple tasks.",
        "中文标题": "深度弹性网络与模型选择用于多任务学习",
        "摘要翻译": "在这项工作中，我们考虑了多任务学习中实例级动态网络模型选择的问题。为此，我们提出了一种高效的方法，在所有任务的每个实例中利用骨干架构中的紧凑但准确的模型。所提出的方法包括一个估计器和一个选择器。估计器基于一个骨干架构，并以分层结构组织。它可以在分层结构中生成多个不同配置的网络模型。选择器根据输入实例从候选模型池中动态选择一个模型。选择器是一个相对较小的网络，由几层组成，当给定任务的输入实例时，它估计候选模型的概率分布。估计器和选择器在一个统一的学习框架中与基于采样的学习策略联合训练，无需额外的计算步骤。我们展示了所提出的方法在几个图像分类任务中与执行模型选择或学习多个任务的现有方法相比的优势。实验结果表明，我们的方法不仅与其他竞争者相比表现出色，而且具有为多个任务执行实例级模型选择的多样性。",
        "领域": "多任务学习/图像分类/模型选择",
        "问题": "多任务学习中的实例级动态网络模型选择",
        "动机": "为了提高多任务学习中模型选择的效率和准确性，提出一种能够为每个任务实例动态选择最合适模型的方法。",
        "方法": "提出了一种包括估计器和选择器的方法，估计器基于骨干架构生成多个不同配置的网络模型，选择器根据输入实例动态选择模型，两者在统一的学习框架中联合训练。",
        "关键词": [
            "多任务学习",
            "图像分类",
            "模型选择",
            "动态网络",
            "实例级选择"
        ],
        "涉及的技术概念": "估计器是基于骨干架构的分层结构，能够生成多种配置的网络模型；选择器是一个小型网络，用于根据输入实例估计候选模型的概率分布；两者通过基于采样的学习策略联合训练，无需额外计算步骤。"
    },
    {
        "order": 709,
        "title": "Fully Convolutional Geometric Features",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choy_Fully_Convolutional_Geometric_Features_ICCV_2019_paper.html",
        "abstract": "Extracting geometric features from 3D scans or point clouds is the first step in applications such as registration, reconstruction, and tracking. State-of-the-art methods require computing low-level features as input or extracting patch-based features with limited receptive field. In this work, we present fully-convolutional geometric features, computed in a single pass by a 3D fully-convolutional network. We also present new metric learning losses that dramatically improve performance. Fully-convolutional geometric features are compact, capture broad spatial context, and scale to large scenes. We experimentally validate our approach on both indoor and outdoor datasets. Fully-convolutional geometric features achieve state-of-the-art accuracy without requiring prepossessing, are compact (32 dimensions), and are 290 times faster than the most accurate prior method.",
        "中文标题": "全卷积几何特征",
        "摘要翻译": "从3D扫描或点云中提取几何特征是注册、重建和跟踪等应用的第一步。最先进的方法需要计算低级特征作为输入或提取具有有限感受野的基于补丁的特征。在这项工作中，我们提出了全卷积几何特征，这些特征通过3D全卷积网络一次性计算得出。我们还提出了新的度量学习损失，显著提高了性能。全卷积几何特征紧凑，捕捉广泛的上下文空间，并能扩展到大型场景。我们在室内和室外数据集上实验验证了我们的方法。全卷积几何特征在不需预处理的情况下实现了最先进的准确性，紧凑（32维），并且比最准确的现有方法快290倍。",
        "领域": "3D重建/点云处理/度量学习",
        "问题": "如何高效地从3D扫描或点云中提取几何特征",
        "动机": "提高3D扫描或点云中几何特征提取的效率和准确性",
        "方法": "使用3D全卷积网络一次性计算全卷积几何特征，并引入新的度量学习损失以提高性能",
        "关键词": [
            "3D重建",
            "点云处理",
            "度量学习"
        ],
        "涉及的技术概念": "全卷积几何特征是通过3D全卷积网络一次性计算得出的特征，能够捕捉广泛的上下文空间，并且能够扩展到大型场景。新的度量学习损失被引入以显著提高性能。这种方法不需要预处理，特征紧凑（32维），并且比现有最准确的方法快290倍。"
    },
    {
        "order": 710,
        "title": "Detecting the Unexpected via Image Resynthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lis_Detecting_the_Unexpected_via_Image_Resynthesis_ICCV_2019_paper.html",
        "abstract": "Classical semantic segmentation methods, including the recent deep learning ones, assume that all classes observed at test time have been seen during training. In this paper, we tackle the more realistic scenario where unexpected objects of unknown classes can appear at test time. The main trends in this area either leverage the notion of prediction uncertainty to flag the regions with low confidence as unknown, or rely on autoencoders and highlight poorly-decoded regions. Having observed that, in both cases, the detected regions typically do not correspond to unexpected objects, in this paper, we introduce a drastically different strategy: It relies on the intuition that the network will produce spurious labels in regions depicting unexpected objects. Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image. In other words, we translate the problem of detecting unknown classes to one of identifying poorly-resynthesized image regions. We show that this outperforms both uncertainty- and autoencoder-based methods.",
        "中文标题": "通过图像重合成检测意外情况",
        "摘要翻译": "传统的语义分割方法，包括最近的深度学习方法，都假设在测试时观察到的所有类别在训练时都已经见过。在本文中，我们解决了一个更现实的场景，即在测试时可能会出现未知类别的意外对象。该领域的主要趋势要么是利用预测不确定性的概念来标记低置信度区域为未知，要么依赖于自动编码器并突出显示解码不良的区域。观察到在这两种情况下，检测到的区域通常不对应于意外对象，本文引入了一种截然不同的策略：它依赖于网络在描绘意外对象的区域中会产生虚假标签的直觉。因此，从结果语义图重合成图像将产生与输入图像显著的外观差异。换句话说，我们将检测未知类别的问题转化为识别重合成不良图像区域的问题。我们展示了这种方法优于基于不确定性和自动编码器的方法。",
        "领域": "语义分割/图像重合成/未知类别检测",
        "问题": "在测试时检测未知类别的意外对象",
        "动机": "解决现有方法在检测未知类别对象时的不足，提高检测的准确性和可靠性",
        "方法": "通过图像重合成技术，将检测未知类别的问题转化为识别重合成不良图像区域的问题",
        "关键词": [
            "语义分割",
            "图像重合成",
            "未知类别检测"
        ],
        "涉及的技术概念": "语义分割是一种将图像分割成多个区域或对象的技术，每个区域或对象被赋予一个类别标签。图像重合成是指从语义图或其他形式的图像表示中重新生成图像的过程。未知类别检测是指在测试时识别出训练数据中未出现过的对象类别。"
    },
    {
        "order": 711,
        "title": "Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jacob_Metric_Learning_With_HORDE_High-Order_Regularizer_for_Deep_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jacob_Metric_Learning_With_HORDE_High-Order_Regularizer_for_Deep_Embeddings_ICCV_2019_paper.html",
        "abstract": "Learning an effective similarity measure between image representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval).",
        "中文标题": "使用HORDE进行度量学习：深度嵌入的高阶正则化",
        "摘要翻译": "学习图像表示之间的有效相似性度量是视觉搜索任务（例如验证或零样本学习）最近进展的关键。尽管度量学习部分得到了很好的解决，但这种度量通常是在提取的深度特征的平均值上计算的。然后，这种表示被训练为具有区分性。然而，这些深度特征往往分散在特征空间中。因此，这些表示对异常值、物体遮挡、背景变化等不具有鲁棒性。在本文中，我们通过一种名为HORDE的分布感知正则化来解决这种分散问题。这种正则化器强制视觉上接近的图像具有相同分布的深度特征，这些特征在特征空间中定位良好。我们提供了支持这种正则化效果的理论分析。我们还通过在4个知名数据集（Cub-200-2011、Cars-196、Stanford Online Products和Inshop Clothes Retrieval）上获得最先进的结果来展示我们方法的有效性。",
        "领域": "视觉搜索/度量学习/特征表示",
        "问题": "深度特征在特征空间中的分散问题，导致表示对异常值、物体遮挡、背景变化等不具有鲁棒性",
        "动机": "提高视觉搜索任务中图像表示之间的相似性度量的有效性，增强表示的鲁棒性",
        "方法": "采用一种名为HORDE的分布感知正则化方法，强制视觉上接近的图像具有相同分布的深度特征，这些特征在特征空间中定位良好",
        "关键词": [
            "视觉搜索",
            "度量学习",
            "特征表示",
            "正则化",
            "深度特征"
        ],
        "涉及的技术概念": "HORDE（High-Order Regularizer for Deep Embeddings）是一种分布感知的正则化方法，用于解决深度特征在特征空间中的分散问题，通过强制视觉上接近的图像具有相同分布的深度特征，增强表示的鲁棒性。"
    },
    {
        "order": 712,
        "title": "Adversarial Learning With Margin-Based Triplet Embedding Regularization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhong_Adversarial_Learning_With_Margin-Based_Triplet_Embedding_Regularization_ICCV_2019_paper.html",
        "abstract": "The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classification objective, so that the obtained models learn to resist adversarial examples. The regularization term consists of two steps optimizations which find potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classification and deep face recognition.",
        "中文标题": "基于边距的三重嵌入正则化的对抗学习",
        "摘要翻译": "深度神经网络（DNNs）在各种计算机视觉任务上取得了巨大成功，然而，它们极易受到对抗攻击的影响。为了解决这个问题，我们提出通过将基于边距的三重嵌入正则化项整合到分类目标中，来提高表示空间的局部平滑性，从而使获得的模型学会抵抗对抗样本。正则化项包括两个步骤的优化，这些步骤以迭代的方式找到潜在的扰动并通过大边距惩罚它们。在MNIST、CASIA-WebFace、VGGFace2和MS-Celeb-1M上的实验结果表明，我们的方法在简单对象分类和深度人脸识别中提高了网络对特征和标签对抗攻击的鲁棒性。",
        "领域": "对抗学习/人脸识别/鲁棒性",
        "问题": "提高深度神经网络对对抗攻击的鲁棒性",
        "动机": "深度神经网络在计算机视觉任务中表现出色，但容易受到对抗攻击的影响，因此需要提高其鲁棒性。",
        "方法": "通过将基于边距的三重嵌入正则化项整合到分类目标中，提高表示空间的局部平滑性，使模型学会抵抗对抗样本。",
        "关键词": [
            "对抗学习",
            "三重嵌入",
            "正则化",
            "鲁棒性",
            "人脸识别"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）、对抗攻击、局部平滑性、基于边距的三重嵌入正则化、特征和标签对抗攻击、简单对象分类、深度人脸识别"
    },
    {
        "order": 713,
        "title": "Simultaneous Multi-View Instance Detection With Learned Geometric Soft-Constraints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nassar_Simultaneous_Multi-View_Instance_Detection_With_Learned_Geometric_Soft-Constraints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nassar_Simultaneous_Multi-View_Instance_Detection_With_Learned_Geometric_Soft-Constraints_ICCV_2019_paper.html",
        "abstract": "We propose to jointly learn multi-view geometry and warping between views of the same object instances for robust cross-view object detection. What makes multi-view object instance detection difficult are strong changes in viewpoint, lighting conditions, high similarity of neighbouring objects, and strong variability in scale. By turning object detection and instance re-identification in different views into a joint learning task, we are able to incorporate both image appearance and geometric soft constraints into a single, multi-view detection process that is learnable end-to-end. We validate our method on a new, large data set of street-level panoramas of urban objects and show superior performance compared to various baselines. Our contribution is threefold: a large-scale, publicly available data set for multi-view instance detection and re-identification; an annotation tool custom-tailored for multi-view instance detection; and a novel, holistic multi-view instance detection and re-identification method that jointly models geometry and appearance across views.",
        "中文标题": "通过学习的几何软约束实现同步多视角实例检测",
        "摘要翻译": "我们提出了一种联合学习多视角几何和同一对象实例不同视角之间的扭曲的方法，以实现鲁棒的跨视角对象检测。多视角对象实例检测的难点在于视角、光照条件的强烈变化，邻近对象的高度相似性，以及尺度的强烈变化。通过将不同视角中的对象检测和实例重新识别转化为一个联合学习任务，我们能够将图像外观和几何软约束整合到一个可端到端学习的单一多视角检测过程中。我们在一个新的、大规模的街景全景城市对象数据集上验证了我们的方法，并展示了与各种基线相比的优越性能。我们的贡献有三方面：一个用于多视角实例检测和重新识别的大规模公开数据集；一个为多视角实例检测量身定制的注释工具；以及一种新颖的、整体的多视角实例检测和重新识别方法，该方法联合建模了跨视角的几何和外观。",
        "领域": "多视角学习/实例检测/几何建模",
        "问题": "解决多视角对象实例检测中的视角、光照条件变化、邻近对象相似性及尺度变化问题",
        "动机": "提高跨视角对象检测的鲁棒性和准确性",
        "方法": "联合学习多视角几何和视角间扭曲，整合图像外观和几何软约束到一个端到端可学习的多视角检测过程",
        "关键词": [
            "多视角学习",
            "实例检测",
            "几何建模",
            "跨视角检测",
            "对象重新识别"
        ],
        "涉及的技术概念": "多视角几何学习涉及从不同视角捕捉同一对象的几何关系；视角间扭曲指的是调整或变换图像以匹配不同视角下的对象表示；几何软约束是指在模型训练过程中引入的、基于几何关系的软性限制条件，以提高模型的泛化能力和鲁棒性。"
    },
    {
        "order": 714,
        "title": "Learning Local RGB-to-CAD Correspondences for Object Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Georgakis_Learning_Local_RGB-to-CAD_Correspondences_for_Object_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "We consider the problem of 3D object pose estimation. While much recent work has focused on the RGB domain, the reliance on accurately annotated images limits generalizability and scalability. On the other hand, the easily available object CAD models are rich sources of data, providing a large number of synthetically rendered images. In this paper, we solve this key problem of existing methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. Our key innovations compared to existing work include removing the need for either real-world textures for CAD models or explicit 3D pose annotations for RGB images. We achieve this through a series of objectives that learn how to select keypoints and enforce viewpoint and modality invariance across RGB images and CAD model renderings. Our experiments demonstrate that the proposed method can reliably estimate object pose in RGB images and generalize to object instances not seen during training.",
        "中文标题": "学习局部RGB到CAD的对应关系以进行物体姿态估计",
        "摘要翻译": "我们考虑3D物体姿态估计的问题。尽管最近的工作主要集中在RGB领域，但对精确注释图像的依赖限制了其泛化能力和可扩展性。另一方面，易于获取的物体CAD模型是丰富的数据源，提供了大量合成渲染的图像。在本文中，我们通过提出一种新方法来解决现有方法需要昂贵的3D姿态注释的关键问题，该方法将RGB图像与CAD模型匹配以进行物体姿态估计。与现有工作相比，我们的关键创新包括消除了对CAD模型的真实世界纹理或RGB图像的显式3D姿态注释的需求。我们通过一系列目标实现这一点，这些目标学习如何选择关键点并在RGB图像和CAD模型渲染之间强制执行视角和模态不变性。我们的实验表明，所提出的方法可以可靠地估计RGB图像中的物体姿态，并泛化到训练期间未见过的物体实例。",
        "领域": "3D物体姿态估计/合成数据渲染/视角不变性",
        "问题": "解决现有3D物体姿态估计方法对昂贵3D姿态注释的依赖问题",
        "动机": "提高3D物体姿态估计的泛化能力和可扩展性，通过利用易于获取的CAD模型数据",
        "方法": "提出一种新方法，通过匹配RGB图像与CAD模型进行物体姿态估计，无需真实世界纹理或显式3D姿态注释，通过学习选择关键点和强制执行视角和模态不变性",
        "关键词": [
            "3D物体姿态估计",
            "合成数据渲染",
            "视角不变性"
        ],
        "涉及的技术概念": "3D物体姿态估计涉及从2D图像中估计物体的3D位置和方向。合成数据渲染指的是使用计算机生成的图像来模拟真实世界场景。视角不变性指的是算法能够识别物体，无论从哪个角度观察。"
    },
    {
        "order": 715,
        "title": "Self-Supervised Monocular Depth Hints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Watson_Self-Supervised_Monocular_Depth_Hints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Watson_Self-Supervised_Monocular_Depth_Hints_ICCV_2019_paper.html",
        "abstract": "Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser-scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with self-supervised learning typically have multiple local minima. These plausible-looking alternatives to ground-truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods. Here, we study the problem of ambiguous reprojections in depth-prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth-suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark.",
        "中文标题": "自监督单目深度提示",
        "摘要翻译": "单目深度估计器可以通过从双目立体数据中获得的各种形式的自监督进行训练，以避免需要高质量的激光扫描或其他地面真实数据。然而，缺点是自监督学习中使用的光度重投影损失通常具有多个局部最小值。这些看起来合理的地面真实替代品可能会限制回归网络学习的内容，导致其预测的深度图质量有限。作为一个突出的例子，当前最先进的方法经常错误估计薄结构周围的深度不连续性。在这里，我们研究了基于立体的自监督深度预测中的模糊重投影问题，并引入了深度提示来缓解其影响。深度提示是从简单的现成立体算法中获得的互补深度建议。这些提示增强了现有的光度损失函数，并用于指导网络学习更好的权重。它们不需要额外的数据，并且假设只在某些时候是正确的。我们展示了使用我们的深度提示在训练几个领先的基于立体的自监督模型时提供了显著的提升，而不仅仅是我们自己的模型。此外，结合其他良好实践，我们在KITTI基准上产生了最先进的深度预测。",
        "领域": "深度估计/自监督学习/立体视觉",
        "问题": "自监督学习中的光度重投影损失具有多个局部最小值，导致深度图质量有限，特别是薄结构周围的深度不连续性经常被错误估计。",
        "动机": "解决自监督深度预测中的模糊重投影问题，提高深度图的质量，特别是在薄结构周围的深度不连续性估计。",
        "方法": "引入深度提示，这些提示是从简单的现成立体算法中获得的互补深度建议，用于增强现有的光度损失函数，并指导网络学习更好的权重。",
        "关键词": [
            "深度估计",
            "自监督学习",
            "立体视觉",
            "光度重投影损失",
            "深度提示"
        ],
        "涉及的技术概念": {
            "自监督学习": "一种不需要大量标注数据的机器学习方法，通过从数据本身生成监督信号来训练模型。",
            "光度重投影损失": "在自监督深度估计中使用的一种损失函数，通过比较原始图像和重投影图像之间的差异来训练模型。",
            "深度提示": "从现成立体算法中获得的深度建议，用于增强光度损失函数，指导网络学习更好的权重。",
            "KITTI基准": "一个广泛使用的计算机视觉基准，用于评估和比较不同算法在自动驾驶等任务中的性能。"
        }
    },
    {
        "order": 716,
        "title": "CenterNet: Keypoint Triplets for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet.",
        "中文标题": "CenterNet: 用于目标检测的关键点三元组",
        "摘要翻译": "在目标检测中，基于关键点的方法经常遇到大量错误目标边界框的问题，这可能是由于在裁剪区域内缺乏额外的评估。本文提出了一种高效的解决方案，以最小的成本探索单个裁剪区域内的视觉模式。我们的框架建立在一个名为CornerNet的代表性一阶段关键点检测器之上。我们的方法，名为CenterNet，将每个目标检测为关键点的三元组，而不是一对，这提高了精度和召回率。因此，我们设计了两个定制模块，级联角点池化和中心池化，丰富了由左上角和右下角收集的信息，并提供了来自中心区域的更多可识别信息。在MS-COCO数据集上，CenterNet实现了47.0%的AP，比所有现有的一阶段检测器至少高出4.9%。此外，CenterNet的推理速度比排名最高的两阶段检测器更快，展示了与这些检测器相当的性能。代码可在https://github.com/Duankaiwen/CenterNet获取。",
        "领域": "目标检测/关键点检测/深度学习",
        "问题": "解决基于关键点的目标检测方法中错误目标边界框数量多的问题",
        "动机": "提高目标检测的精度和召回率，减少错误边界框的数量",
        "方法": "提出CenterNet方法，通过检测关键点的三元组而非一对，并设计级联角点池化和中心池化模块来丰富信息",
        "关键词": [
            "目标检测",
            "关键点检测",
            "深度学习"
        ],
        "涉及的技术概念": "关键点检测是一种在图像中识别特定点（如物体的角点）的技术，用于定位和识别物体。级联角点池化和中心池化是两种技术，用于从图像的特定区域收集和丰富信息，以提高目标检测的准确性。"
    },
    {
        "order": 717,
        "title": "Depth From Videos in the Wild: Unsupervised Monocular Depth Learning From Unknown Cameras",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gordon_Depth_From_Videos_in_the_Wild_Unsupervised_Monocular_Depth_Learning_ICCV_2019_paper.html",
        "abstract": "We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos. The code will be open sourced once anonymity is lifted.",
        "中文标题": "野外视频中的深度：从未知相机中进行无监督单目深度学习",
        "摘要翻译": "我们提出了一种新颖的方法，用于从单目视频中同时学习深度、自我运动、物体运动和相机内参，仅使用相邻视频帧之间的一致性作为监督信号。与之前的工作类似，我们的方法通过对帧应用可微分扭曲并将结果与相邻帧进行比较来学习，但它提供了几项改进：我们几何地和可微分地处理遮挡，直接使用训练期间预测的深度图。我们引入了随机层归一化，一种新颖且强大的正则化器，并且我们考虑了物体相对于场景的运动。据我们所知，我们的工作是第一个以无监督方式从视频中学习相机内参（包括镜头畸变）的工作，从而使我们能够从任意来源的未知视频中大规模提取准确的深度和运动。我们在Cityscapes、KITTI和EuRoC数据集上评估了我们的结果，在深度预测和里程计方面建立了新的技术状态，并定性地证明了可以从YouTube视频集合中学习深度预测。一旦匿名性解除，代码将开源。",
        "领域": "深度估计/自我运动估计/相机内参估计",
        "问题": "从单目视频中无监督学习深度、自我运动、物体运动和相机内参",
        "动机": "为了从任意来源的未知视频中大规模提取准确的深度和运动，需要一种无需监督信号的方法来学习这些参数。",
        "方法": "通过应用可微分扭曲到视频帧并比较结果与相邻帧来学习，同时引入随机层归一化作为正则化器，并考虑物体相对于场景的运动。",
        "关键词": [
            "深度估计",
            "自我运动估计",
            "相机内参估计",
            "无监督学习",
            "单目视频"
        ],
        "涉及的技术概念": "可微分扭曲、随机层归一化、几何处理遮挡、物体运动估计、相机内参学习"
    },
    {
        "order": 718,
        "title": "3D Scene Reconstruction With Multi-Layer Depth and Epipolar Transformers",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shin_3D_Scene_Reconstruction_With_Multi-Layer_Depth_and_Epipolar_Transformers_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shin_3D_Scene_Reconstruction_With_Multi-Layer_Depth_and_Epipolar_Transformers_ICCV_2019_paper.html",
        "abstract": "We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel \"Epipolar Feature Transformer\" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.",
        "中文标题": "使用多层深度和极线变换器进行3D场景重建",
        "摘要翻译": "我们解决了从单一RGB图像自动重建场景完整3D模型的问题。这一挑战性任务需要推断可见和被遮挡表面的形状。我们的方法利用了从最近单物体形状补全方法中改编的场景几何的多层表示。为了提高复杂场景中视图中心表示的准确性，我们引入了一种新颖的“极线特征变换器”，它将卷积网络特征从输入视图转移到其他虚拟相机视点，从而更好地覆盖3D场景几何。与现有方法不同，现有方法首先在3D中检测和定位物体，然后使用类别特定模型推断物体形状，我们的方法是完全卷积的，端到端可微分的，并避免了体素表示的分辨率和内存限制。我们展示了多层深度表示和极线特征变换器在重建大型室内场景数据库中的优势。",
        "领域": "3D重建/场景理解/几何处理",
        "问题": "从单一RGB图像自动重建场景的完整3D模型",
        "动机": "需要推断可见和被遮挡表面的形状，以提高复杂场景中视图中心表示的准确性",
        "方法": "利用多层深度表示和引入极线特征变换器，将卷积网络特征从输入视图转移到其他虚拟相机视点",
        "关键词": [
            "3D重建",
            "场景理解",
            "几何处理"
        ],
        "涉及的技术概念": "多层深度表示、极线特征变换器、卷积网络、端到端可微分、体素表示"
    },
    {
        "order": 719,
        "title": "Online Hyper-Parameter Learning for Auto-Augmentation Strategy",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_Online_Hyper-Parameter_Learning_for_Auto-Augmentation_Strategy_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_Online_Hyper-Parameter_Learning_for_Auto-Augmentation_Strategy_ICCV_2019_paper.html",
        "abstract": "Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.",
        "中文标题": "在线超参数学习用于自动增强策略",
        "摘要翻译": "数据增强对于现代深度学习技术的成功至关重要。在本文中，我们提出了在线超参数学习用于自动增强（OHL-Auto-Aug），这是一种经济的解决方案，它学习增强策略分布与网络训练一起进行。与之前以离线方式搜索增强策略的自动增强方法不同，我们的方法将增强策略制定为参数化的概率分布，从而允许其参数与网络参数一起优化。我们提出的OHL-Auto-Aug消除了重新训练的需要，并显著降低了整个搜索过程的成本，同时相对于基线模型建立了显著的准确性改进。在CIFAR-10和ImageNet上，我们的方法在搜索准确性方面取得了显著成就，CIFAR-10上快了60倍，ImageNet上快了24倍，同时保持了竞争力的准确性。",
        "领域": "数据增强/超参数优化/深度学习",
        "问题": "如何高效地在线学习数据增强策略的超参数",
        "动机": "为了减少自动增强策略搜索过程中的成本和时间，同时提高模型的准确性",
        "方法": "将增强策略制定为参数化的概率分布，并在线优化其参数与网络参数",
        "关键词": [
            "数据增强",
            "超参数优化",
            "在线学习"
        ],
        "涉及的技术概念": "数据增强是一种通过创建训练数据的修改版本来增加训练数据集大小的技术，以提高模型的泛化能力。超参数优化是指调整模型训练过程中使用的参数，以改善模型性能。在线学习指的是模型在接收新数据时不断更新其参数，而不是在固定数据集上进行一次性训练。"
    },
    {
        "order": 720,
        "title": "OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Won_OmniMVS_End-to-End_Learning_for_Omnidirectional_Stereo_Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Won_OmniMVS_End-to-End_Learning_for_Omnidirectional_Stereo_Matching_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel end-to-end deep neural network model for omnidirectional depth estimation from a wide-baseline multi-view stereo setup. The images captured with ultra wide field-of-view (FOV) cameras on an omnidirectional rig are processed by the feature extraction module, and then the deep feature maps are warped onto the concentric spheres swept through all candidate depths using the calibrated camera parameters. The 3D encoder-decoder block takes the aligned feature volume to produce the omnidirectional depth estimate with regularization on uncertain regions utilizing the global context information. In addition, we present large-scale synthetic datasets for training and testing omnidirectional multi-view stereo algorithms. Our datasets consist of 11K ground-truth depth maps and 45K fisheye images in four orthogonal directions with various objects and environments. Experimental results show that the proposed method generates excellent results in both synthetic and real-world environments, and it outperforms the prior art and the omnidirectional versions of the state-of-the-art conventional stereo algorithms.",
        "中文标题": "OmniMVS：全向立体匹配的端到端学习",
        "摘要翻译": "在本文中，我们提出了一种新颖的端到端深度神经网络模型，用于从宽基线多视图立体设置中进行全向深度估计。使用超宽视野（FOV）相机在全向装置上捕获的图像通过特征提取模块进行处理，然后使用校准的相机参数将深度特征图扭曲到通过所有候选深度的同心球体上。3D编码器-解码器块采用对齐的特征体积来生成全向深度估计，并利用全局上下文信息对不确定区域进行正则化。此外，我们提出了大规模合成数据集，用于训练和测试全向多视图立体算法。我们的数据集包括11K地面真实深度图和45K鱼眼图像，涵盖四个正交方向的各种物体和环境。实验结果表明，所提出的方法在合成和现实世界环境中都产生了优异的结果，并且它优于现有技术和最先进的传统立体算法的全向版本。",
        "领域": "立体视觉/深度估计/全向视觉",
        "问题": "从宽基线多视图立体设置中进行全向深度估计",
        "动机": "提高全向深度估计的准确性和效率，特别是在使用超宽视野相机捕获的图像中",
        "方法": "提出了一种端到端的深度神经网络模型，包括特征提取、深度特征图扭曲、3D编码器-解码器块处理和对齐的特征体积生成全向深度估计",
        "关键词": [
            "全向深度估计",
            "宽基线多视图立体",
            "端到端学习"
        ],
        "涉及的技术概念": "端到端深度神经网络模型、特征提取模块、深度特征图扭曲、3D编码器-解码器块、全向深度估计、全局上下文信息、大规模合成数据集"
    },
    {
        "order": 721,
        "title": "How Do Neural Networks See Depth in Single Images?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.html",
        "abstract": "Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles - even those not appearing in the training set - but may depend on features that are not universally present.",
        "中文标题": "神经网络如何从单张图像中看到深度？",
        "摘要翻译": "深度神经网络在从单张图像进行深度估计方面取得了突破性进展。最近的研究表明，这些估计的质量正在迅速提高。显然，神经网络可以从单张图像中看到深度。然而，据我们所知，目前还没有工作分析这些网络学到了什么。在这项工作中，我们选取了四个先前发布的网络，并研究了它们利用了哪些深度线索。我们发现，所有网络都忽略了已知障碍物的表观大小，而倾向于它们在图像中的垂直位置。使用垂直位置需要知道相机姿态；然而，我们发现这些网络仅部分识别相机俯仰和滚动角度的变化。相机俯仰的小变化被证明会干扰对障碍物距离的估计。使用图像的垂直位置允许网络估计对任意障碍物的深度——即使是那些没有出现在训练集中的障碍物——但可能依赖于并非普遍存在的特征。",
        "领域": "深度估计/神经网络/计算机视觉",
        "问题": "分析神经网络在单张图像深度估计中利用了哪些深度线索",
        "动机": "了解神经网络在单张图像深度估计中的学习机制和利用的深度线索",
        "方法": "选取四个先前发布的网络，研究它们利用了哪些深度线索，特别是对图像中障碍物的垂直位置的利用",
        "关键词": [
            "深度估计",
            "神经网络",
            "单张图像",
            "深度线索",
            "相机姿态"
        ],
        "涉及的技术概念": "深度估计是指从单张图像中估计场景的深度信息。神经网络通过学习大量数据来识别图像中的深度线索，如物体的垂直位置，从而估计深度。相机姿态指的是相机的位置和方向，包括俯仰和滚动角度，这些因素会影响深度估计的准确性。"
    },
    {
        "order": 722,
        "title": "DANet: Divergent Activation for Weakly Supervised Object Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_DANet_Divergent_Activation_for_Weakly_Supervised_Object_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xue_DANet_Divergent_Activation_for_Weakly_Supervised_Object_Localization_ICCV_2019_paper.html",
        "abstract": "Weakly supervised object localization remains a challenge when learning object localization models from image category labels. Optimizing image classification tends to activate object parts and ignore the full object extent, while expanding object parts into full object extent could deteriorate the performance of image classification. In this paper, we propose a divergent activation (DA) approach, and target at learning complementary and discriminative visual patterns for image classification and weakly supervised object localization from the perspective of discrepancy. To this end, we design hierarchical divergent activation (HDA), which leverages the semantic discrepancy to spread feature activation, implicitly. We also propose discrepant divergent activation (DDA), which pursues object extent by learning mutually exclusive visual patterns, explicitly. Deep networks implemented with HDA and DDA, referred to as DANets, diverge and fuse discrepant yet discriminative features for image classification and object localization in an end-to-end manner. Experiments validate that DANets advance the performance of object localization while maintaining high performance of image classification on CUB-200 and ILSVRC datasets",
        "中文标题": "DANet：用于弱监督目标定位的差异激活",
        "摘要翻译": "从图像类别标签中学习目标定位模型时，弱监督目标定位仍然是一个挑战。优化图像分类往往会激活目标部分而忽略目标的完整范围，而将目标部分扩展到完整范围可能会降低图像分类的性能。在本文中，我们提出了一种差异激活（DA）方法，旨在从差异的角度学习用于图像分类和弱监督目标定位的互补和区分性视觉模式。为此，我们设计了层次差异激活（HDA），它利用语义差异隐式地扩展特征激活。我们还提出了差异差异激活（DDA），它通过学习相互排斥的视觉模式明确地追求目标范围。采用HDA和DDA实现的深度网络，称为DANets，以端到端的方式分散和融合差异但区分性的特征用于图像分类和目标定位。实验验证了DANets在CUB-200和ILSVRC数据集上提高了目标定位的性能，同时保持了图像分类的高性能。",
        "领域": "目标定位/图像分类/特征学习",
        "问题": "弱监督目标定位中，优化图像分类往往只激活目标部分而忽略完整目标范围，同时扩展目标部分到完整范围可能会降低图像分类性能。",
        "动机": "从差异的角度学习互补和区分性视觉模式，以同时提高图像分类和目标定位的性能。",
        "方法": "提出了差异激活（DA）方法，包括层次差异激活（HDA）和差异差异激活（DDA），通过利用语义差异隐式扩展特征激活和学习相互排斥的视觉模式明确追求目标范围。",
        "关键词": [
            "弱监督学习",
            "目标定位",
            "图像分类",
            "差异激活",
            "特征学习"
        ],
        "涉及的技术概念": "差异激活（DA）方法包括层次差异激活（HDA）和差异差异激活（DDA），分别通过利用语义差异隐式扩展特征激活和学习相互排斥的视觉模式明确追求目标范围。DANets是采用HDA和DDA实现的深度网络，用于图像分类和目标定位。"
    },
    {
        "order": 723,
        "title": "On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_On_Boosting_Single-Frame_3D_Human_Pose_Estimation_via_Monocular_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_On_Boosting_Single-Frame_3D_Human_Pose_Estimation_via_Monocular_Videos_ICCV_2019_paper.html",
        "abstract": "The premise of training an accurate 3D human pose estimation network is the possession of huge amount of richly annotated training data. Nonetheless, manually obtaining rich and accurate annotations is, even not impossible, tedious and slow. In this paper, we propose to exploit monocular videos to complement the training dataset for the single-image 3D human pose estimation tasks. At the beginning, a baseline model is trained with a small set of annotations. By fixing some reliable estimations produced by the resulting model, our method automatically collects the annotations across the entire video as solving the 3D trajectory completion problem. Then, the baseline model is further trained with the collected annotations to learn the new poses. We evaluate our method on the broadly-adopted Human3.6M and MPI-INF-3DHP datasets. As illustrated in experiments, given only a small set of annotations, our method successfully makes the model to learn new poses from unlabelled monocular videos, promoting the accuracies of the baseline model by about 10%. By contrast with previous approaches, our method does not rely on either multi-view imagery or any explicit 2D keypoint annotations.",
        "中文标题": "通过单目视频提升单帧3D人体姿态估计",
        "摘要翻译": "训练一个准确的3D人体姿态估计网络的前提是拥有大量丰富注释的训练数据。然而，手动获取丰富且准确的注释即使不是不可能，也是繁琐且缓慢的。在本文中，我们提出利用单目视频来补充单图像3D人体姿态估计任务的训练数据集。首先，使用一小部分注释训练一个基线模型。通过固定由该模型产生的一些可靠估计，我们的方法自动收集整个视频中的注释，作为解决3D轨迹完成问题。然后，基线模型进一步使用收集到的注释进行训练，以学习新的姿态。我们在广泛采用的人类3.6M和MPI-INF-3DHP数据集上评估了我们的方法。如实验所示，仅给定一小部分注释，我们的方法成功使模型从未标记的单目视频中学习新的姿态，将基线模型的准确率提高了约10%。与之前的方法相比，我们的方法既不依赖于多视角图像，也不依赖于任何显式的2D关键点注释。",
        "领域": "3D人体姿态估计/单目视频分析/深度学习",
        "问题": "如何有效利用单目视频补充训练数据，以提高单帧3D人体姿态估计的准确性",
        "动机": "手动获取大量准确注释的3D人体姿态数据既繁琐又缓慢，需要一种自动化的方法来补充训练数据",
        "方法": "首先训练一个基线模型，然后通过固定模型产生的可靠估计自动收集视频中的注释，最后使用这些注释进一步训练模型以学习新的姿态",
        "关键词": [
            "3D人体姿态估计",
            "单目视频",
            "轨迹完成"
        ],
        "涉及的技术概念": "3D人体姿态估计网络、单目视频、3D轨迹完成问题、基线模型、Human3.6M数据集、MPI-INF-3DHP数据集"
    },
    {
        "order": 724,
        "title": "On the Over-Smoothing Problem of CNN Based Disparity Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_On_the_Over-Smoothing_Problem_of_CNN_Based_Disparity_Estimation_ICCV_2019_paper.html",
        "abstract": "Currently, most deep learning based disparity estimation methods have the problem of over-smoothing at boundaries, which is unfavorable for some applications such as point cloud segmentation, mapping, etc. To address this problem, we first analyze the potential causes and observe that the estimated disparity at edge boundary pixels usually follows multimodal distributions, causing over-smoothing estimation. Based on this observation, we propose a single-modal weighted average operation on the probability distribution during inference, which can alleviate the problem effectively. To integrate the constraint of this inference method into training stage, we further analyze the characteristics of different loss functions and found that using cross entropy with gaussian distribution consistently further improves the performance. For quantitative evaluation, we propose a novel metric that measures the disparity error in the local structure of edge boundaries. Experiments on various datasets using various networks show our method's effectiveness and general applicability. Code will be available at https://github.com/chenchr/otosp.",
        "中文标题": "基于CNN的视差估计中的过平滑问题",
        "摘要翻译": "目前，大多数基于深度学习的视差估计方法在边界处存在过平滑问题，这对于点云分割、地图绘制等应用是不利的。为了解决这个问题，我们首先分析了潜在的原因，并观察到边缘边界像素处的估计视差通常遵循多模态分布，导致过平滑估计。基于这一观察，我们在推理过程中提出了对概率分布进行单模态加权平均操作，这可以有效缓解问题。为了将这种推理方法的约束整合到训练阶段，我们进一步分析了不同损失函数的特性，发现使用高斯分布的交叉熵可以进一步提高性能。为了进行定量评估，我们提出了一种新的度量标准，用于测量边缘边界局部结构中的视差误差。使用各种网络在各种数据集上的实验显示了我们方法的有效性和普遍适用性。代码将在https://github.com/chenchr/otosp提供。",
        "领域": "视差估计/点云处理/边界检测",
        "问题": "基于深度学习的视差估计方法在边界处存在过平滑问题",
        "动机": "过平滑问题对于点云分割、地图绘制等应用是不利的",
        "方法": "在推理过程中对概率分布进行单模态加权平均操作，并在训练阶段使用高斯分布的交叉熵损失函数",
        "关键词": [
            "视差估计",
            "过平滑问题",
            "边界检测",
            "点云处理"
        ],
        "涉及的技术概念": {
            "过平滑问题": "指在视差估计中，边界处的视差估计过于平滑，导致细节丢失的问题。",
            "单模态加权平均操作": "一种在推理过程中对概率分布进行处理的方法，旨在减少过平滑问题。",
            "高斯分布的交叉熵": "一种损失函数，用于在训练阶段提高模型的性能。",
            "视差误差度量标准": "一种新的度量标准，用于评估边缘边界局部结构中的视差误差。"
        }
    },
    {
        "order": 725,
        "title": "Canonical Surface Mapping via Geometric Cycle Consistency",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kulkarni_Canonical_Surface_Mapping_via_Geometric_Cycle_Consistency_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kulkarni_Canonical_Surface_Mapping_via_Geometric_Cycle_Consistency_ICCV_2019_paper.html",
        "abstract": "We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.",
        "中文标题": "通过几何循环一致性进行规范表面映射",
        "摘要翻译": "我们探索了规范表面映射（CSM）的任务。具体来说，给定一张图像，我们学习将对象上的像素映射到该类别的抽象3D模型上的对应位置。但我们如何学习这样的映射呢？监督方法需要大量的手动标注，这在少数精选类别之外是不可扩展的。我们的关键见解是，CSM任务（像素到3D）与3D投影（3D到像素）结合时，完成了一个循环。因此，我们可以利用几何循环一致性损失，从而允许我们放弃密集的手动监督。我们的方法使我们能够为多样化的类别训练CSM模型，而无需稀疏或密集的关键点注释，仅利用前景掩码标签进行训练。我们展示了我们的预测还允许我们推断两幅图像之间的密集对应关系，并将我们的方法与几种利用不同监督量预测对应关系的方法进行了性能比较。",
        "领域": "3D重建/图像对应/几何学习",
        "问题": "如何在没有密集手动监督的情况下学习将图像像素映射到抽象3D模型的对应位置",
        "动机": "减少对密集手动标注的依赖，提高规范表面映射任务的可扩展性",
        "方法": "利用几何循环一致性损失，结合CSM任务和3D投影，完成一个循环，从而无需密集的手动监督",
        "关键词": [
            "规范表面映射",
            "几何循环一致性",
            "3D投影"
        ],
        "涉及的技术概念": "规范表面映射（CSM）是指将图像中的像素映射到抽象3D模型上的对应位置的任务。几何循环一致性损失是一种利用任务和其逆任务之间的循环一致性来减少对监督数据依赖的方法。3D投影是将3D模型投影回2D图像空间的过程。"
    },
    {
        "order": 726,
        "title": "Selective Sparse Sampling for Fine-Grained Image Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Selective_Sparse_Sampling_for_Fine-Grained_Image_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_Selective_Sparse_Sampling_for_Fine-Grained_Image_Recognition_ICCV_2019_paper.html",
        "abstract": "Fine-grained recognition poses the unique challenge of capturing subtle inter-class differences under considerable intra-class variances (e.g., beaks for bird species). Conventional approaches crop local regions and learn detailed representation from those regions, but suffer from the fixed number of parts and missing of surrounding context. In this paper, we propose a simple yet effective framework, called Selective Sparse Sampling, to capture diverse and fine-grained details. The framework is implemented using Convolutional Neural Networks, referred to as Selective Sparse Sampling Networks (S3Ns). With image-level supervision, S3Ns collect peaks, i.e., local maximums, from class response maps to estimate informative, receptive fields and learn a set of sparse attention for capturing fine-detailed visual evidence as well as preserving context. The evidence is selectively sampled to extract discriminative and complementary features, which significantly enrich the learned representation and guide the network to discover more subtle cues. Extensive experiments and ablation studies show that the proposed method consistently outperforms the state-of-the-art methods on challenging benchmarks including CUB-200-2011, FGVC-Aircraft, and Stanford Cars.",
        "中文标题": "选择性稀疏采样用于细粒度图像识别",
        "摘要翻译": "细粒度识别面临在相当大的类内差异下捕捉细微类间差异的独特挑战（例如，鸟类的喙）。传统方法裁剪局部区域并从这些区域学习详细表示，但存在固定数量的部分和周围上下文缺失的问题。在本文中，我们提出了一个简单而有效的框架，称为选择性稀疏采样，以捕捉多样化和细粒度的细节。该框架使用卷积神经网络实现，称为选择性稀疏采样网络（S3Ns）。在图像级监督下，S3Ns从类响应图中收集峰值，即局部最大值，以估计信息丰富的感受野，并学习一组稀疏注意力以捕捉精细的视觉证据以及保留上下文。证据被选择性地采样以提取区分性和互补性特征，这显著丰富了学习到的表示并指导网络发现更细微的线索。广泛的实验和消融研究表明，所提出的方法在包括CUB-200-2011、FGVC-Aircraft和Stanford Cars在内的挑战性基准上始终优于最先进的方法。",
        "领域": "细粒度图像识别/卷积神经网络/注意力机制",
        "问题": "在细粒度图像识别中捕捉细微的类间差异，同时处理类内差异",
        "动机": "传统方法在捕捉细粒度细节时存在固定数量的部分和周围上下文缺失的问题，需要一种更有效的方法来捕捉这些细节",
        "方法": "提出选择性稀疏采样框架，使用卷积神经网络实现，通过收集类响应图中的峰值来估计信息丰富的感受野，并学习一组稀疏注意力以捕捉精细的视觉证据及保留上下文",
        "关键词": [
            "细粒度图像识别",
            "卷积神经网络",
            "注意力机制",
            "选择性稀疏采样"
        ],
        "涉及的技术概念": "选择性稀疏采样网络（S3Ns）是一种使用卷积神经网络实现的框架，旨在通过收集类响应图中的峰值来估计信息丰富的感受野，并学习一组稀疏注意力以捕捉精细的视觉证据及保留上下文。这种方法能够提取区分性和互补性特征，丰富学习到的表示，并指导网络发现更细微的线索。"
    },
    {
        "order": 727,
        "title": "Disentangling Propagation and Generation for Video Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Disentangling_Propagation_and_Generation_for_Video_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_Disentangling_Propagation_and_Generation_for_Video_Prediction_ICCV_2019_paper.html",
        "abstract": "A dynamic scene has two types of elements: those that move fluidly and can be predicted from previous frames, and those which are disoccluded (exposed) and cannot be extrapolated. Prior approaches to video prediction typically learn either to warp or to hallucinate future pixels, but not both. In this paper, we describe a computational model for high-fidelity video prediction which disentangles motion-specific propagation from motion-agnostic generation. We introduce a confidence-aware warping operator which gates the output of pixel predictions from a flow predictor for non-occluded regions and from a context encoder for occluded regions. Moreover, in contrast to prior works where confidence is jointly learned with flow and appearance using a single network, we compute confidence after a warping step, and employ a separate network to inpaint exposed regions. Empirical results on both synthetic and real datasets show that our disentangling approach provides better occlusion maps and produces both sharper and more realistic predictions compared to strong baselines.",
        "中文标题": "解耦传播与生成以进行视频预测",
        "摘要翻译": "动态场景包含两种类型的元素：那些流畅移动且可以从先前帧预测的元素，以及那些被遮挡（暴露）且无法外推的元素。以往的视频预测方法通常学习要么扭曲要么幻觉未来像素，但不同时进行。在本文中，我们描述了一种高保真视频预测的计算模型，该模型将特定于运动的传播与不依赖于运动的生成解耦。我们引入了一种置信度感知的扭曲操作符，该操作符根据流预测器对非遮挡区域的像素预测和上下文编码器对遮挡区域的像素预测进行门控输出。此外，与之前的工作相比，在那些工作中置信度是与流和外观通过单一网络联合学习的，我们在扭曲步骤后计算置信度，并采用一个单独的网络来修复暴露区域。在合成和真实数据集上的实证结果表明，与强基线相比，我们的解耦方法提供了更好的遮挡图，并产生了更清晰和更真实的预测。",
        "领域": "视频预测/动态场景分析/像素级预测",
        "问题": "解决视频预测中如何同时处理流畅移动元素和被遮挡元素的问题",
        "动机": "以往的视频预测方法通常只能处理流畅移动元素或被遮挡元素中的一种，无法同时处理两种元素，导致预测结果不够准确和真实",
        "方法": "引入置信度感知的扭曲操作符，将特定于运动的传播与不依赖于运动的生成解耦，采用单独的网络来修复暴露区域",
        "关键词": [
            "视频预测",
            "动态场景分析",
            "像素级预测",
            "置信度感知",
            "遮挡处理"
        ],
        "涉及的技术概念": "置信度感知的扭曲操作符是一种技术，用于根据流预测器和上下文编码器的输出对像素预测进行门控。解耦传播与生成意味着将特定于运动的元素（可以通过先前帧预测）与不依赖于运动的元素（被遮挡且无法外推）分开处理。"
    },
    {
        "order": 728,
        "title": "GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Grabner_GP2C_Geometric_Projection_Parameter_Consensus_for_Joint_3D_Pose_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Grabner_GP2C_Geometric_Projection_Parameter_Consensus_for_Joint_3D_Pose_and_ICCV_2019_paper.html",
        "abstract": "We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics.",
        "中文标题": "GP2C：用于野外联合3D姿态和焦距估计的几何投影参数共识",
        "摘要翻译": "我们提出了一种用于野外物体类别的联合3D姿态和焦距估计方法。与之前独立于焦距预测3D姿态或假设恒定焦距的方法不同，我们明确估计并将焦距整合到3D姿态估计中。为此，我们结合深度学习技术和几何算法，采用两阶段方法：首先，我们使用深度网络从单个RGB图像估计初始焦距并建立2D-3D对应关系。其次，我们通过最小化预测对应关系的重投影误差来恢复3D姿态并优化焦距。通过这种方式，我们利用焦距提供的几何先验进行3D姿态估计。这带来了两个优势：首先，与现有方法相比，我们显著提高了3D平移和3D姿态的准确性。其次，我们的方法在个体投影参数之间找到了几何共识，这对于精确的2D-3D对齐是必需的。我们在三个具有挑战性的真实世界数据集（Pix3D、Comp和Stanford）上评估了我们提出的方法，这些数据集包含不同的物体类别，并在多个不同指标上显著优于最先进的方法，绝对提升高达20%。",
        "领域": "3D视觉/几何计算/深度学习",
        "问题": "在野外环境中联合估计物体的3D姿态和焦距",
        "动机": "提高3D姿态估计的准确性，通过整合焦距信息来优化2D-3D对齐",
        "方法": "结合深度学习技术和几何算法的两阶段方法，首先估计初始焦距并建立2D-3D对应关系，然后通过最小化重投影误差恢复3D姿态并优化焦距",
        "关键词": [
            "3D姿态估计",
            "焦距估计",
            "几何投影",
            "2D-3D对齐",
            "重投影误差"
        ],
        "涉及的技术概念": "深度学习技术用于从单个RGB图像估计初始焦距和建立2D-3D对应关系；几何算法用于通过最小化重投影误差来恢复3D姿态和优化焦距；几何投影参数共识用于提高3D姿态估计的准确性。"
    },
    {
        "order": 729,
        "title": "Dynamic Anchor Feature Selection for Single-Shot Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Dynamic_Anchor_Feature_Selection_for_Single-Shot_Object_Detection_ICCV_2019_paper.html",
        "abstract": "The design of anchors is critical to the performance of one-stage detectors. Recently, the anchor refinement module (ARM) has been proposed to adjust the initialization of default anchors, providing the detector a better anchor reference. However, this module brings another problem: all pixels at a feature map have the same receptive field while the anchors associated with each pixel have different positions and sizes. This discordance may lead to a less effective detector. In this paper, we present a dynamic feature selection operation to select new pixels in a feature map for each refined anchor received from the ARM. The pixels are selected based on the new anchor position and size so that the receptive filed of these pixels can fit the anchor areas well, which makes the detector, especially the regression part, much easier to optimize. Furthermore, to enhance the representation ability of selected feature pixels, we design a bidirectional feature fusion module by combining features from early and deep layers. Extensive experiments on both PASCAL VOC and COCO demonstrate the effectiveness of our dynamic anchor feature selection (DAFS) operation. For the case of high IoU threshold, our DAFS can improve the mAP by a large margin.",
        "中文标题": "动态锚点特征选择用于单次目标检测",
        "摘要翻译": "锚点的设计对于一阶段检测器的性能至关重要。最近，提出了锚点细化模块（ARM）来调整默认锚点的初始化，为检测器提供更好的锚点参考。然而，这个模块带来了另一个问题：特征图上的所有像素具有相同的感受野，而与每个像素相关联的锚点具有不同的位置和大小。这种不一致可能导致检测器的效果不佳。在本文中，我们提出了一种动态特征选择操作，为从ARM接收到的每个细化锚点选择特征图中的新像素。这些像素是根据新锚点的位置和大小选择的，以便这些像素的感受野能够很好地适应锚点区域，这使得检测器，特别是回归部分，更容易优化。此外，为了增强所选特征像素的表示能力，我们设计了一个双向特征融合模块，通过结合早期和深层特征。在PASCAL VOC和COCO上的大量实验证明了我们的动态锚点特征选择（DAFS）操作的有效性。在高IoU阈值的情况下，我们的DAFS可以大幅提高mAP。",
        "领域": "目标检测/特征选择/特征融合",
        "问题": "一阶段检测器中锚点与特征图像素感受野不一致的问题",
        "动机": "提高检测器，特别是回归部分的优化效率",
        "方法": "提出动态特征选择操作和双向特征融合模块",
        "关键词": [
            "动态特征选择",
            "锚点细化模块",
            "双向特征融合"
        ],
        "涉及的技术概念": "锚点细化模块（ARM）用于调整默认锚点的初始化，动态特征选择操作根据新锚点的位置和大小选择特征图中的新像素，双向特征融合模块通过结合早期和深层特征来增强所选特征像素的表示能力。"
    },
    {
        "order": 730,
        "title": "Guided Image-to-Image Translation With Bi-Directional Feature Transformation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/AlBahar_Guided_Image-to-Image_Translation_With_Bi-Directional_Feature_Transformation_ICCV_2019_paper.html",
        "abstract": "We address the problem of guided image-to-image translation where we translate an input image into another while respecting the constraints provided by an external, user-provided guidance image. Various types of conditioning mechanisms for leveraging the given guidance image have been explored, including input concatenation, feature concatenation, and conditional affine transformation of feature activations. All these conditioning mechanisms, however, are uni-directional, i.e., no information flow from the input image back to the guidance. To better utilize the constraints of the guidance image, we present a bi-directional feature transformation (bFT) scheme. We show that our novel bFT scheme outperforms other conditioning schemes and has comparable results to state-of-the-art methods on different tasks.",
        "中文标题": "双向特征变换引导的图像到图像翻译",
        "摘要翻译": "我们解决了引导图像到图像翻译的问题，即在翻译输入图像为另一图像时，尊重由外部用户提供的引导图像所施加的约束。已经探索了各种利用给定引导图像的条件机制，包括输入连接、特征连接和特征激活的条件仿射变换。然而，所有这些条件机制都是单向的，即没有信息从输入图像回流到引导图像。为了更好地利用引导图像的约束，我们提出了一种双向特征变换（bFT）方案。我们展示了我们新颖的bFT方案优于其他条件机制，并在不同任务上取得了与最先进方法相当的结果。",
        "领域": "图像翻译/特征变换/条件生成",
        "问题": "如何在图像到图像的翻译过程中有效利用外部引导图像的约束",
        "动机": "现有的条件机制在利用引导图像时是单向的，无法充分利用引导图像的信息",
        "方法": "提出了一种双向特征变换（bFT）方案，通过允许信息在输入图像和引导图像之间双向流动，以更好地利用引导图像的约束",
        "关键词": [
            "图像翻译",
            "特征变换",
            "条件生成"
        ],
        "涉及的技术概念": "双向特征变换（bFT）是一种新颖的条件机制，它允许信息在输入图像和引导图像之间双向流动，从而更有效地利用引导图像的约束进行图像到图像的翻译。"
    },
    {
        "order": 731,
        "title": "Moulding Humans: Non-Parametric 3D Human Shape Estimation From Single Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gabeur_Moulding_Humans_Non-Parametric_3D_Human_Shape_Estimation_From_Single_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gabeur_Moulding_Humans_Non-Parametric_3D_Human_Shape_Estimation_From_Single_Images_ICCV_2019_paper.html",
        "abstract": "In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: a visible depth map and a \"hidden\" depth map are estimated and combined, to reconstruct the human 3D shape as done with a \"mould\". This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and \"humanness\" of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces.",
        "中文标题": "塑造人类：从单张图像进行非参数化3D人体形状估计",
        "摘要翻译": "在本文中，我们解决了从单张RGB图像进行3D人体形状估计的问题。尽管卷积神经网络的最新进展在3D人体姿态估计方面取得了令人印象深刻的成果，但估计一个人的完整3D形状仍然是一个未解决的问题。基于模型的方法可以输出精确的无衣物人体网格，但无法估计细节和未建模的元素，如头发或衣物。另一方面，非参数化体积方法有可能估计完整的形状，但实际上，它们受到输出网格分辨率的限制，无法产生详细的估计。在这项工作中，我们提出了一种非参数化方法，该方法采用双深度图来表示一个人的3D形状：估计并组合一个可见深度图和一个“隐藏”深度图，以“模具”的方式重建人类的3D形状。通过2D深度图的这种表示允许比基于体素的体积表示更高的分辨率输出和更低的维度。此外，我们完全可导的基于深度的模型使我们能够以对抗的方式有效地结合一个鉴别器，以提高3D输出的准确性和“人性化”。我们在SURREAL和3D-HUMANS上训练并定量验证了我们的方法，3D-HUMANS是一个新的照片级真实感数据集，由半合成的内部视频组成，并带有3D地面真实表面注释。",
        "领域": "3D重建/人体形状估计/深度图",
        "问题": "从单张RGB图像估计完整3D人体形状",
        "动机": "现有方法在估计3D人体形状时无法准确捕捉细节和未建模元素，如头发或衣物，且非参数化体积方法受限于输出网格的分辨率。",
        "方法": "提出一种非参数化方法，使用双深度图（可见深度图和隐藏深度图）来表示3D人体形状，并通过2D深度图实现高分辨率输出。此外，采用基于深度的模型结合对抗性鉴别器以提高3D输出的准确性和“人性化”。",
        "关键词": [
            "3D人体形状估计",
            "非参数化方法",
            "双深度图",
            "对抗性鉴别器"
        ],
        "涉及的技术概念": "卷积神经网络用于3D人体姿态估计，基于模型的方法输出精确的无衣物人体网格，非参数化体积方法估计完整形状但受限于分辨率，双深度图表示3D形状，对抗性鉴别器提高3D输出的准确性和“人性化”。"
    },
    {
        "order": 732,
        "title": "Incremental Learning Using Conditional Adversarial Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiang_Incremental_Learning_Using_Conditional_Adversarial_Networks_ICCV_2019_paper.html",
        "abstract": "Incremental learning using Deep Neural Networks (DNNs) suffers from catastrophic forgetting. Existing methods mitigate it by either storing old image examples or only updating a few fully connected layers of DNNs, which, however, requires large memory footprints or hurts the plasticity of models. In this paper, we propose a new incremental learning strategy based on conditional adversarial networks. Our new strategy allows us to use memory-efficient statistical information to store old knowledge, and fine-tune both convolutional layers and fully connected layers to consolidate new knowledge. Specifically, we propose a model consisting of three parts, i.e., a base sub-net, a generator, and a discriminator. The base sub-net works as a feature extractor which can be pre-trained on large scale datasets and shared across multiple image recognition tasks. The generator conditioned on labeled embeddings aims to construct pseudo-examples with the same distribution as the old data. The discriminator combines real-examples from new data and pseudo-examples generated from the old data distribution to learn representation for both old and new classes. Through adversarial training of the discriminator and generator, we accomplish the multiple continuous incremental learning. Comparison with the state-of-the-arts on public CIFAR-100 and CUB-200 datasets shows that our method achieves the best accuracies on both old and new classes while requiring relatively less memory storage.",
        "中文标题": "使用条件对抗网络进行增量学习",
        "摘要翻译": "使用深度神经网络（DNNs）进行增量学习时，会遇到灾难性遗忘的问题。现有的方法通过存储旧的图像样本或仅更新DNNs的几个全连接层来缓解这一问题，但这需要大量的内存占用或损害模型的塑性。在本文中，我们提出了一种基于条件对抗网络的新增量学习策略。我们的新策略允许我们使用内存效率高的统计信息来存储旧知识，并微调卷积层和全连接层以巩固新知识。具体来说，我们提出了一个由三部分组成的模型，即基础子网、生成器和判别器。基础子网作为特征提取器，可以在大规模数据集上进行预训练，并在多个图像识别任务中共享。生成器以标记嵌入为条件，旨在构建与旧数据分布相同的伪样本。判别器结合来自新数据的真实样本和从旧数据分布生成的伪样本来学习新旧类别的表示。通过判别器和生成器的对抗训练，我们完成了多个连续的增量学习。与公开的CIFAR-100和CUB-200数据集上的最新技术相比，我们的方法在旧类别和新类别上都达到了最佳准确率，同时需要相对较少的内存存储。",
        "领域": "增量学习/对抗网络/图像识别",
        "问题": "解决深度神经网络在增量学习中的灾难性遗忘问题",
        "动机": "现有方法需要大量内存或损害模型塑性，需要一种更高效的方法来存储旧知识并学习新知识",
        "方法": "提出基于条件对抗网络的增量学习策略，通过基础子网、生成器和判别器的对抗训练，实现内存效率高的增量学习",
        "关键词": [
            "增量学习",
            "条件对抗网络",
            "图像识别"
        ],
        "涉及的技术概念": "深度神经网络（DNNs）、条件对抗网络、特征提取器、伪样本、对抗训练"
    },
    {
        "order": 733,
        "title": "Towards Multi-Pose Guided Virtual Try-On Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Towards_Multi-Pose_Guided_Virtual_Try-On_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Towards_Multi-Pose_Guided_Virtual_Try-On_Network_ICCV_2019_paper.html",
        "abstract": "Virtual try-on systems under arbitrary human poses have significant application potential, yet also raise extensive challenges, such as self-occlusions, heavy misalignment among different poses, and complex clothes textures. Existing virtual try-on methods can only transfer clothes given a fixed human pose, and still show unsatisfactory performances, often failing to preserve person identity or texture details, and with limited pose diversity. This paper makes the first attempt towards a multi-pose guided virtual try-on system, which enables clothes to transfer onto a person with diverse poses. Given an input person image, a desired clothes image, and a desired pose, the proposed Multi-pose Guided Virtual Try-On Network (MG-VTON) generates a new person image after fitting the desired clothes into the person and manipulating the pose. MG-VTON is constructed with three stages: 1) a conditional human parsing network is proposed that matches both the desired pose and the desired clothes shape; 2) a deep Warping Generative Adversarial Network (Warp-GAN) that warps the desired clothes appearance into the synthesized human parsing map and alleviates the misalignment problem between the input human pose and the desired one; 3) a refinement render network recovers the texture details of clothes and removes artifacts, based on multi-pose composition masks. Extensive experiments on commonly-used datasets and our newly-collected largest virtual try-on benchmark demonstrate that our MG-VTON significantly outperforms all state-of-the-art methods both qualitatively and quantitatively, showing promising virtual try-on performances.",
        "中文标题": "面向多姿态引导的虚拟试穿网络",
        "摘要翻译": "任意人体姿态下的虚拟试穿系统具有显著的应用潜力，但也带来了广泛的挑战，如自遮挡、不同姿态之间的严重不对齐以及复杂的衣物纹理。现有的虚拟试穿方法只能在给定固定人体姿态的情况下转移衣物，并且表现仍然不尽如人意，常常无法保持人物身份或纹理细节，且姿态多样性有限。本文首次尝试开发一个多姿态引导的虚拟试穿系统，该系统能够将衣物转移到具有多样姿态的人物上。给定输入的人物图像、期望的衣物图像和期望的姿态，提出的多姿态引导虚拟试穿网络（MG-VTON）在将期望的衣物适配到人物并操纵姿态后生成一个新的人物图像。MG-VTON由三个阶段构建：1）提出了一个条件人体解析网络，该网络匹配期望的姿态和期望的衣物形状；2）一个深度变形生成对抗网络（Warp-GAN），将期望的衣物外观变形到合成的人体解析图中，并缓解输入人体姿态与期望姿态之间的不对齐问题；3）一个细化渲染网络，基于多姿态合成掩码恢复衣物的纹理细节并去除伪影。在常用数据集和我们新收集的最大虚拟试穿基准上的大量实验表明，我们的MG-VTON在质量和数量上均显著优于所有最先进的方法，显示出有前景的虚拟试穿性能。",
        "领域": "虚拟试穿/人体姿态估计/生成对抗网络",
        "问题": "解决在任意人体姿态下进行虚拟试穿的挑战，包括自遮挡、姿态不对齐和复杂衣物纹理的保持。",
        "动机": "现有虚拟试穿方法在固定姿态下转移衣物时表现不佳，无法保持人物身份或纹理细节，且姿态多样性有限。",
        "方法": "提出多姿态引导虚拟试穿网络（MG-VTON），通过条件人体解析网络、深度变形生成对抗网络（Warp-GAN）和细化渲染网络三个阶段，实现衣物在多样姿态下的转移和纹理细节的恢复。",
        "关键词": [
            "虚拟试穿",
            "人体姿态估计",
            "生成对抗网络"
        ],
        "涉及的技术概念": "条件人体解析网络用于匹配期望的姿态和衣物形状；深度变形生成对抗网络（Warp-GAN）用于衣物外观的变形和对齐；细化渲染网络用于恢复衣物纹理细节和去除伪影。"
    },
    {
        "order": 734,
        "title": "3DPeople: Modeling the Geometry of Dressed Humans",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pumarola_3DPeople_Modeling_the_Geometry_of_Dressed_Humans_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pumarola_3DPeople_Modeling_the_Geometry_of_Dressed_Humans_ICCV_2019_paper.html",
        "abstract": "Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape. First, we present 3DPeople, a large-scale synthetic dataset with 2 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body we annotated the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks. We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete. Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images.",
        "中文标题": "3DPeople: 着装人体几何建模",
        "摘要翻译": "近年来，3D人体形状估计的进展建立在参数化表示的基础上，这些表示很好地模拟了裸体的形状，但不适合表示服装的几何形状。在本文中，我们提出了一种方法来模拟着装人体并从单张图像预测其几何形状。我们在问题的三个基本方面做出了贡献，即一个新的数据集、一种新颖的形状参数化算法和一个用于预测形状的端到端深度生成网络。首先，我们介绍了3DPeople，这是一个大规模合成数据集，包含80个主体进行70项活动并穿着不同服装的200万张逼真图像。除了提供服装和身体的纹理3D网格外，我们还用分割掩码、骨架、深度、法线贴图和光流注释了数据集。所有这些使得3DPeople适用于众多任务。然后，我们使用2D几何图像表示3D形状。为了构建这些图像，我们提出了一种基于最优质量传输方法的新颖球面面积保持参数化算法。我们展示了这种方法改进了现有的球面映射，这些映射往往会缩小全身模型的细长部分，如手臂和腿，使得几何图像不完整。最后，我们设计了一个多分辨率深度生成网络，给定一张着装人体的输入图像，以端到端的方式预测其几何图像（即着装的身体形状）。我们在合成验证和野外图像中，在同时捕捉身体姿势和服装形状方面取得了非常有希望的结果。",
        "领域": "3D建模/人体形状估计/服装几何",
        "问题": "如何从单张图像中准确预测着装人体的几何形状",
        "动机": "现有的参数化表示方法虽然能很好地模拟裸体形状，但不适合表示服装的几何形状，因此需要一种新的方法来模拟着装人体并预测其几何形状。",
        "方法": "提出了一种新的数据集3DPeople，一种新颖的球面面积保持参数化算法，以及一个多分辨率深度生成网络，用于从单张图像预测着装人体的几何形状。",
        "关键词": [
            "3D建模",
            "人体形状估计",
            "服装几何",
            "深度生成网络",
            "球面参数化"
        ],
        "涉及的技术概念": {
            "3D人体形状估计": "通过参数化表示来估计人体的3D形状。",
            "参数化表示": "一种用于模拟和表示3D形状的方法。",
            "球面面积保持参数化算法": "一种基于最优质量传输方法的算法，用于在球面上保持面积不变地参数化3D形状。",
            "深度生成网络": "一种深度学习模型，用于生成新的数据样本，如几何图像。",
            "多分辨率": "在处理图像或数据时，考虑不同尺度的细节。"
        }
    },
    {
        "order": 735,
        "title": "Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Bilateral_Adversarial_Training_Towards_Fast_Training_of_More_Robust_Models_ICCV_2019_paper.html",
        "abstract": "In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [??], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we first show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7% versus 47.2%; on SVHN, we achieve 59.1% versus 42.1%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.",
        "中文标题": "双边对抗训练：面向快速训练更鲁棒的模型以抵御对抗攻击",
        "摘要翻译": "在本文中，我们研究了快速训练对抗鲁棒模型的方法。通过对最先进的防御方法，即多步对抗训练的分析，我们假设梯度大小与模型的鲁棒性有关。基于此，我们提出在训练过程中同时扰动图像和标签，我们称之为双边对抗训练（BAT）。为了生成对抗标签，我们推导出了一个封闭式的启发式解决方案。为了生成对抗图像，我们使用一步目标攻击，目标标签是最令人困惑的类别。在实验中，我们首先展示了随机开始和最令人困惑的目标攻击有效地防止了标签泄露和梯度掩蔽问题。然后，结合对抗标签部分，我们的模型显著提高了最先进的结果。例如，在使用交叉熵损失的PGD100白盒攻击下，在CIFAR10上，我们实现了63.7%对47.2%；在SVHN上，我们实现了59.1%对42.1%。最后，在计算上极具挑战性的ImageNet数据集上的实验进一步证明了我们快速方法的有效性。",
        "领域": "对抗学习/模型鲁棒性/快速训练",
        "问题": "如何快速训练出能够抵御对抗攻击的鲁棒模型",
        "动机": "通过分析现有最先进的防御方法，发现梯度大小与模型鲁棒性有关，从而提出双边对抗训练方法以提高模型鲁棒性",
        "方法": "提出双边对抗训练（BAT），在训练过程中同时扰动图像和标签，使用一步目标攻击生成对抗图像，并推导出封闭式启发式解决方案生成对抗标签",
        "关键词": [
            "对抗训练",
            "模型鲁棒性",
            "快速训练",
            "对抗攻击",
            "梯度大小"
        ],
        "涉及的技术概念": "多步对抗训练、梯度大小、双边对抗训练（BAT）、一步目标攻击、对抗标签、对抗图像、标签泄露、梯度掩蔽问题、PGD100白盒攻击、交叉熵损失、CIFAR10、SVHN、ImageNet"
    },
    {
        "order": 736,
        "title": "Photorealistic Style Transfer via Wavelet Transforms",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.html",
        "abstract": "Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT2) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a 1024x1024 resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, pre-trained models and supplementary documents are all available at https://github.com/ClovaAI/WCT2.",
        "中文标题": "通过小波变换实现照片级真实感风格迁移",
        "摘要翻译": "最近的风格迁移模型提供了有前景的艺术效果。然而，当以照片作为参考风格时，现有方法受到空间扭曲或不真实伪影的限制，这在真实照片中是不应该发生的。我们引入了一种理论上合理的网络架构修正，显著增强了照片真实感并忠实地迁移了风格。我们方法的关键成分是小波变换，它自然地适应于深度网络。我们提出了一种基于白化和着色变换（WCT2）的小波修正迁移，允许特征在风格化过程中保留其结构信息和VGG特征空间的统计属性。这是第一个也是唯一一个能够在4.7秒内对1024x1024分辨率图像进行风格化的端到端模型，无需任何后处理即可提供令人愉悦且照片级真实感的质量。最后但同样重要的是，我们的模型提供了无需时间约束的稳定视频风格化。我们的代码、生成的图像、预训练模型和补充文档都可以在https://github.com/ClovaAI/WCT2找到。",
        "领域": "风格迁移/图像生成/视频处理",
        "问题": "现有风格迁移方法在处理照片作为参考风格时，存在空间扭曲和不真实伪影的问题",
        "动机": "增强照片真实感并忠实地迁移风格，解决现有方法的局限性",
        "方法": "引入小波变换作为网络架构的修正，提出基于白化和着色变换（WCT2）的小波修正迁移方法",
        "关键词": [
            "小波变换",
            "风格迁移",
            "照片真实感",
            "VGG特征空间",
            "视频风格化"
        ],
        "涉及的技术概念": "小波变换是一种数学工具，用于将信号分解成不同尺度的成分，适用于图像处理中的多分辨率分析。白化和着色变换（WCT2）是一种用于风格迁移的技术，通过调整特征空间的统计属性来实现风格转换。VGG特征空间指的是由VGG网络提取的图像特征表示，这些特征在风格迁移中被用来捕捉和转移风格信息。"
    },
    {
        "order": 737,
        "title": "Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.html",
        "abstract": "Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/ nkolot/projects/spin.",
        "中文标题": "通过模型拟合循环学习重建3D人体姿态和形状",
        "摘要翻译": "基于模型的人体姿态估计目前通过两种不同的范式进行。基于优化的方法以迭代方式将参数化身体模型拟合到2D观察结果，导致精确的图像模型对齐，但通常较慢且对初始化敏感。相比之下，基于回归的方法使用深度网络直接从像素估计模型参数，倾向于提供合理但不精确到像素的结果，同时需要大量的监督。在这项工作中，我们的关键见解是，这两种范式可以形成强有力的合作，而不是探讨哪种方法更好。网络直接回归的合理估计可以初始化迭代优化，使拟合更快更准确。同样，迭代优化的像素级精确拟合可以作为网络的强监督。这是我们提出的方法SPIN（SMPL优化在循环中）的核心。深度网络初始化一个迭代优化例程，该例程在训练循环中将身体模型拟合到2D关节，随后使用拟合的估计来监督网络。我们的方法本质上是自我改进的，因为更好的网络估计可以引导优化找到更好的解决方案，而更准确的优化拟合为网络提供了更好的监督。我们在不同设置中展示了我们方法的有效性，其中3D地面实况稀缺或不可用，并且我们始终以显著优势超越最先进的基于模型的姿态估计方法。项目网站包含视频、结果和代码，可在https://seas.upenn.edu/nkolot/projects/spin找到。",
        "领域": "3D人体姿态估计/参数化身体模型/深度学习",
        "问题": "如何有效地结合基于优化和基于回归的方法来提高3D人体姿态估计的准确性和效率",
        "动机": "现有的基于优化和基于回归的方法各有优缺点，探索两者结合的可能性以提高姿态估计的性能",
        "方法": "提出SPIN方法，通过深度网络初始化迭代优化过程，并在训练循环中使用拟合结果监督网络，实现自我改进",
        "关键词": [
            "3D人体姿态估计",
            "参数化身体模型",
            "深度学习",
            "迭代优化",
            "网络监督"
        ],
        "涉及的技术概念": "SPIN方法结合了深度学习和迭代优化技术，通过深度网络提供初始估计，然后通过迭代优化过程精确化，最后使用优化结果作为网络的监督信号，实现自我改进的循环。"
    },
    {
        "order": 738,
        "title": "View Confusion Feature Learning for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_View_Confusion_Feature_Learning_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Person re-identification is an important task in video surveillance that aims to associate people across camera views at different locations and time. View variability is always a challenging problem seriously degrading person re-identification performance. Most of the existing methods either focus on how to learn view invariant feature or how to combine viewwise features. In this paper, we mainly focus on how to learn view-independent features by getting rid of view specific information through a view confusion learning mechanism. Specifically, we propose an end-to-end trainable framework, called View Confusion Feature Learning (VCFL), for person Re-ID across cameras. To the best of our knowledge, VCFL is originally proposed to learn view-independent identity-wise features, and it's a kind of combination of view-generic and view-specific methods. Furthermore, we extract sift-guided features by using bag-of-words model to help supervise the training of deep networks and enhance the view invariance of features. In experiments, our approach is validated on three benchmark datasets including CUHK01, CUHK03, and MARKET1501, which show the superiority of the proposed method over several state-of-the-art approaches.",
        "中文标题": "视角混淆特征学习用于行人重识别",
        "摘要翻译": "行人重识别是视频监控中的一项重要任务，旨在关联不同地点和时间摄像机视角下的人。视角变化一直是一个严重降低行人重识别性能的挑战性问题。大多数现有方法要么专注于如何学习视角不变特征，要么专注于如何结合视角特征。在本文中，我们主要关注如何通过视角混淆学习机制去除视角特定信息来学习视角无关特征。具体来说，我们提出了一个端到端可训练的框架，称为视角混淆特征学习（VCFL），用于跨摄像头的行人重识别。据我们所知，VCFL最初是为了学习视角无关的身份特征而提出的，它是一种视角通用和视角特定方法的结合。此外，我们通过使用词袋模型提取sift引导的特征，以帮助监督深度网络的训练并增强特征的视角不变性。在实验中，我们的方法在包括CUHK01、CUHK03和MARKET1501在内的三个基准数据集上进行了验证，结果显示所提出的方法优于几种最先进的方法。",
        "领域": "行人重识别/特征学习/视角不变性",
        "问题": "解决视角变化对行人重识别性能的影响",
        "动机": "视角变化严重降低行人重识别的性能，需要一种方法来学习视角无关的特征",
        "方法": "提出了一种端到端可训练的视角混淆特征学习（VCFL）框架，通过视角混淆学习机制去除视角特定信息，并结合词袋模型提取sift引导的特征来增强特征的视角不变性",
        "关键词": [
            "行人重识别",
            "视角混淆特征学习",
            "视角不变性",
            "词袋模型",
            "sift特征"
        ],
        "涉及的技术概念": {
            "视角混淆特征学习（VCFL）": "一种端到端可训练的框架，旨在通过去除视角特定信息来学习视角无关的特征",
            "词袋模型": "一种用于图像特征提取的技术，通过统计图像中视觉词汇的出现频率来表示图像",
            "sift特征": "尺度不变特征变换，一种用于检测和描述图像局部特征的算法"
        }
    },
    {
        "order": 739,
        "title": "Personalized Fashion Design",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Personalized_Fashion_Design_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Personalized_Fashion_Design_ICCV_2019_paper.html",
        "abstract": "Fashion recommendation is the task of suggesting a fashion item that fits well with a given item. In this work, we propose to automatically synthesis new items for recommendation. We jointly consider the two key issues for the task, i.e., compatibility and personalization. We propose a personalized fashion design framework with the help of generative adversarial training. A convolutional network is first used to map the query image into a latent vector representation. This latent representation, together with another vector which characterizes user's style preference, are taken as the input to the generator network to generate the target item image. Two discriminator networks are built to guide the generation process. One is the classic real/fake discriminator. The other is a matching network which simultaneously models the compatibility between fashion items and learns users' preference representations. The performance of the proposed method is evaluated on thousands of outfits composited by online users. The experiments show that the items generated by our model are quite realistic. They have better visual quality and higher matching degree than those generated by alternative methods.",
        "中文标题": "个性化时尚设计",
        "摘要翻译": "时尚推荐是建议一件与给定物品搭配良好的时尚物品的任务。在这项工作中，我们提出自动合成新物品以进行推荐。我们共同考虑了任务的两个关键问题，即兼容性和个性化。我们提出了一个借助生成对抗训练的个性化时尚设计框架。首先使用卷积网络将查询图像映射到潜在向量表示。这个潜在表示，连同另一个表征用户风格偏好的向量，作为生成器网络的输入，以生成目标物品图像。构建了两个判别器网络来指导生成过程。一个是经典的真/假判别器。另一个是匹配网络，它同时建模时尚物品之间的兼容性并学习用户的偏好表示。所提出方法的性能在由在线用户组合的数千套服装上进行了评估。实验表明，我们的模型生成的物品非常逼真。它们具有比替代方法生成的物品更好的视觉质量和更高的匹配度。",
        "领域": "时尚推荐/生成对抗网络/个性化推荐",
        "问题": "如何自动合成新的时尚物品以进行个性化推荐",
        "动机": "提高时尚推荐的兼容性和个性化，以生成更符合用户风格偏好的时尚物品",
        "方法": "提出一个个性化时尚设计框架，利用生成对抗训练，通过卷积网络将查询图像映射到潜在向量表示，并结合用户风格偏好向量生成目标物品图像，使用两个判别器网络指导生成过程",
        "关键词": [
            "时尚推荐",
            "生成对抗网络",
            "个性化推荐"
        ],
        "涉及的技术概念": "生成对抗训练是一种通过同时训练生成器和判别器网络来生成数据的方法。卷积网络用于从图像中提取特征。潜在向量表示是指将数据映射到低维空间中的表示，以便于处理和分析。匹配网络用于评估物品之间的兼容性和学习用户的偏好表示。"
    },
    {
        "order": 740,
        "title": "Optimizing Network Structure for 3D Human Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.",
        "中文标题": "优化网络结构以进行3D人体姿态估计",
        "摘要翻译": "人体姿态自然地表示为图，其中关节是节点，骨骼是边。因此，应用图卷积网络（GCN）从2D姿态估计3D姿态是自然的。在这项工作中，我们提出了一个通用公式，其中GCN和全连接网络（FCN）都是其特例。从这个公式中，我们发现GCN在用于估计3D姿态时具有有限的表示能力。我们通过引入局部连接网络（LCN）来克服这一限制，该网络自然地由这个通用公式实现。它显著提高了GCN的表示能力。此外，由于每个关节仅连接到其邻近的几个关节，因此它具有很强的泛化能力。在公共数据集上的实验表明：（1）它优于最先进的技术；（2）比其他模型更少依赖数据；（3）对未见过的动作和数据集具有良好的泛化能力。",
        "领域": "3D人体姿态估计/图卷积网络/局部连接网络",
        "问题": "如何提高从2D姿态估计3D姿态的准确性和泛化能力",
        "动机": "图卷积网络（GCN）在3D人体姿态估计中的表示能力有限，需要一种新的网络结构来提高估计的准确性和泛化能力",
        "方法": "提出了一种通用公式，其中图卷积网络（GCN）和全连接网络（FCN）是其特例，并引入局部连接网络（LCN）来提高表示能力和泛化能力",
        "关键词": [
            "3D人体姿态估计",
            "图卷积网络",
            "局部连接网络"
        ],
        "涉及的技术概念": {
            "图卷积网络（GCN）": "一种用于处理图结构数据的神经网络，通过卷积操作在图中的节点间传递信息",
            "全连接网络（FCN）": "一种传统的神经网络结构，其中每一层的每个神经元都与下一层的每个神经元相连",
            "局部连接网络（LCN）": "一种网络结构，其中每个节点仅与其邻近的节点相连，旨在提高网络的表示能力和泛化能力"
        }
    },
    {
        "order": 741,
        "title": "Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Auto-FPN_Automatic_Network_Architecture_Adaptation_for_Object_Detection_Beyond_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Auto-FPN_Automatic_Network_Architecture_Adaptation_for_Object_Detection_Beyond_Classification_ICCV_2019_paper.html",
        "abstract": "Abstract Neural architecture search (NAS) has shown great potential in automating the manual process of designing a good CNN architecture for image classification. In this paper, we study NAS for object detection, a core computer vision task that classifies and localizes object instances in an image. Existing works focus on transferring the searched architecture from classification task (ImageNet) to the detector backbone, while the rest of the architecture of the detector remains unchanged. However, this pipeline is not task-specific or data-oriented network search which cannot guarantee optimal adaptation to any dataset. Therefore, we propose an architecture search framework named Auto-FPN specifically designed for detection beyond simply searching a classification backbone. Specifically, we propose two auto search modules for detection: Auto-fusion to search a better fusion of the multi-level features; Auto-head to search a better structure for classification and bounding-box(bbox) regression. Instead of searching for one repeatable cell structure, we relax the constraint and allow different cells. The search space of both modules covers many popular designs of detectors and allows efficient gradient-based architecture search with resource constraint (2 days for COCO on 8 GPU cards). Extensive experiments on Pascal VOC, COCO, BDD, VisualGenome and ADE demonstrate the effectiveness of the proposed method, e.g. achieving around 5% improvement than FPN in terms of mAP while requiring around 50% fewer parameters on the searched modules.",
        "中文标题": "Auto-FPN: 超越分类的自动网络架构适应用于目标检测",
        "摘要翻译": "神经架构搜索（NAS）在自动化设计良好CNN架构以进行图像分类的过程中显示出巨大潜力。在本文中，我们研究了用于目标检测的NAS，这是一个核心计算机视觉任务，用于分类和定位图像中的对象实例。现有工作集中于将从分类任务（ImageNet）搜索到的架构转移到检测器的主干，而检测器的其余架构保持不变。然而，这种流程不是任务特定或数据导向的网络搜索，不能保证对任何数据集的最佳适应。因此，我们提出了一个名为Auto-FPN的架构搜索框架，专门设计用于检测，而不仅仅是搜索分类主干。具体来说，我们提出了两个自动搜索模块用于检测：Auto-fusion用于搜索多级特征的更好融合；Auto-head用于搜索分类和边界框（bbox）回归的更好结构。我们放宽了约束，允许不同的单元，而不是搜索一个可重复的单元结构。这两个模块的搜索空间涵盖了许多流行的检测器设计，并允许在资源约束下进行高效的基于梯度的架构搜索（在8个GPU卡上对COCO进行2天的搜索）。在Pascal VOC、COCO、BDD、VisualGenome和ADE上的广泛实验证明了所提出方法的有效性，例如，在mAP方面比FPN提高了约5%，同时在搜索的模块上需要的参数减少了约50%。",
        "领域": "目标检测/神经架构搜索/特征融合",
        "问题": "如何自动设计一个既适合分类又适合目标检测的CNN架构",
        "动机": "现有方法主要关注从分类任务转移架构到检测器主干，而检测器的其余架构保持不变，这不能保证对任何数据集的最佳适应",
        "方法": "提出了一个名为Auto-FPN的架构搜索框架，包括两个自动搜索模块：Auto-fusion用于搜索多级特征的更好融合；Auto-head用于搜索分类和边界框回归的更好结构",
        "关键词": [
            "目标检测",
            "神经架构搜索",
            "特征融合",
            "边界框回归"
        ],
        "涉及的技术概念": {
            "神经架构搜索（NAS）": "一种自动化设计神经网络架构的技术，旨在减少人工设计架构的需求",
            "CNN架构": "卷积神经网络架构，用于图像识别和处理",
            "目标检测": "计算机视觉中的一项任务，旨在识别图像中的对象并确定它们的位置",
            "特征融合": "将来自不同层次或来源的特征结合起来，以提高模型的性能",
            "边界框回归": "用于预测对象在图像中的精确位置的技术"
        }
    },
    {
        "order": 742,
        "title": "Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing Loss",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_Tag2Pix_Line_Art_Colorization_Using_Text_Tag_With_SECat_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_Tag2Pix_Line_Art_Colorization_Using_Text_Tag_With_SECat_and_ICCV_2019_paper.html",
        "abstract": "Line art colorization is expensive and challenging to automate. A GAN approach is proposed, called Tag2Pix, of line art colorization which takes as input a grayscale line art and color tag information and produces a quality colored image. First, we present the Tag2Pix line art colorization dataset. A generator network is proposed which consists of convolutional layers to transform the input line art, a pre-trained semantic extraction network, and an encoder for input color information. The discriminator is based on an auxiliary classifier GAN to classify the tag information as well as genuineness. In addition, we propose a novel network structure called SECat, which makes the generator properly colorize even small features such as eyes, and also suggest a novel two-step training method where the generator and discriminator first learn the notion of object and shape and then, based on the learned notion, learn colorization, such as where and how to place which color. We present both quantitative and qualitative evaluations which prove the effectiveness of the proposed method.",
        "中文标题": "Tag2Pix: 使用文本标签与SECat和变化损失进行线稿上色",
        "摘要翻译": "线稿上色既昂贵又难以自动化。提出了一种名为Tag2Pix的GAN方法，用于线稿上色，该方法以灰度线稿和颜色标签信息作为输入，生成高质量的彩色图像。首先，我们介绍了Tag2Pix线稿上色数据集。提出了一个生成器网络，该网络由卷积层组成，用于转换输入的线稿，一个预训练的语义提取网络，以及一个用于输入颜色信息的编码器。判别器基于辅助分类器GAN，用于分类标签信息以及真实性。此外，我们提出了一种名为SECat的新网络结构，它使生成器能够正确地为小特征（如眼睛）上色，并提出了一种新颖的两步训练方法，其中生成器和判别器首先学习对象和形状的概念，然后基于学习到的概念学习上色，如在哪里以及如何放置哪种颜色。我们提供了定量和定性的评估，证明了所提出方法的有效性。",
        "领域": "图像生成/图像上色/生成对抗网络",
        "问题": "自动化线稿上色的挑战和成本问题",
        "动机": "为了降低线稿上色的成本并提高自动化水平，提出了一种新的GAN方法。",
        "方法": "提出了一种名为Tag2Pix的GAN方法，包括一个生成器网络和一个基于辅助分类器GAN的判别器，以及一种名为SECat的新网络结构和一种新颖的两步训练方法。",
        "关键词": [
            "线稿上色",
            "生成对抗网络",
            "SECat",
            "两步训练方法"
        ],
        "涉及的技术概念": {
            "GAN": "生成对抗网络，一种深度学习模型，由生成器和判别器组成，用于生成新的数据样本。",
            "卷积层": "一种深度学习中的层类型，用于提取输入数据的特征。",
            "语义提取网络": "一种预训练的网络，用于从输入数据中提取语义信息。",
            "辅助分类器GAN": "一种GAN的变体，除了生成数据外，还能对数据进行分类。",
            "SECat": "一种新的网络结构，用于提高生成器对小特征（如眼睛）的上色能力。",
            "两步训练方法": "一种训练策略，首先学习对象和形状的概念，然后基于这些概念学习上色。"
        }
    },
    {
        "order": 743,
        "title": "Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.html",
        "abstract": "Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.",
        "中文标题": "利用时空关系通过图卷积网络进行3D姿态估计",
        "摘要翻译": "尽管从单视角图像或视频中进行3D姿态估计取得了巨大进展，但由于显著的深度模糊性和严重的自遮挡，这仍然是一个具有挑战性的任务。受到结合空间依赖性和时间一致性以缓解这些问题的有效性的启发，我们提出了一种新颖的基于图的方法，以解决从2D关节检测的短序列中进行3D人体和3D手部姿态估计的问题。特别是，关于人手（身体）配置的领域知识被明确地融入到图卷积操作中，以满足3D姿态估计的特定需求。此外，我们引入了一种从局部到全局的网络架构，该架构能够学习基于图表示的多尺度特征。我们在具有挑战性的基准数据集上评估了所提出的方法，用于3D手部姿态估计和3D身体姿态估计。实验结果表明，我们的方法在这两项任务上都达到了最先进的性能。",
        "领域": "3D姿态估计/图卷积网络/人体姿态分析",
        "问题": "解决从2D关节检测的短序列中进行3D人体和3D手部姿态估计的问题",
        "动机": "结合空间依赖性和时间一致性以缓解深度模糊性和自遮挡问题",
        "方法": "提出了一种新颖的基于图的方法，明确融入关于人手（身体）配置的领域知识，并引入从局部到全局的网络架构学习多尺度特征",
        "关键词": [
            "3D姿态估计",
            "图卷积网络",
            "人体姿态分析",
            "手部姿态估计",
            "多尺度特征学习"
        ],
        "涉及的技术概念": "图卷积操作、局部到全局网络架构、多尺度特征学习"
    },
    {
        "order": 744,
        "title": "PARN: Position-Aware Relation Networks for Few-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_PARN_Position-Aware_Relation_Networks_for_Few-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_PARN_Position-Aware_Relation_Networks_for_Few-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Few-shot learning presents a challenge that a classifier must quickly adapt to new classes that do not appear in the training set, given only a few labeled examples of each new class. This paper proposes a position-aware relation network (PARN) to learn a more flexible and robust metric ability for few-shot learning. Relation networks (RNs), a kind of architectures for relational reasoning, can acquire a deep metric ability for images by just being designed as a simple convolutional neural network (CNN)[23]. However, due to the inherent local connectivity of CNN, the CNN-based relation network (RN) can be sensitive to the spatial position relationship of semantic objects in two compared images. To address this problem, we introduce a deformable feature extractor (DFE) to extract more efficient features, and design a dual correlation attention mechanism (DCA) to deal with its inherent local connectivity. Successfully, our proposed approach extents the potential of RN to be position-aware of semantic objects by introducing only a small number of parameters. We evaluate our approach on two major benchmark datasets, i.e., Omniglot and Mini-Imagenet, and on both of the datasets our approach achieves state-of-the-art performance. It's worth noting that our 5-way 1-shot result on Omniglot even outperforms the previous 5-way 5-shot results.",
        "中文标题": "PARN: 用于少样本学习的位置感知关系网络",
        "摘要翻译": "少样本学习提出了一个挑战，即分类器必须快速适应训练集中未出现的新类别，且每个新类别只有少量标记示例。本文提出了一种位置感知关系网络（PARN），以学习更灵活和鲁棒的少样本学习度量能力。关系网络（RNs）是一种用于关系推理的架构，仅通过设计为简单的卷积神经网络（CNN）即可获得图像的深度度量能力[23]。然而，由于CNN固有的局部连接性，基于CNN的关系网络（RN）可能对两个比较图像中语义对象的空间位置关系敏感。为了解决这个问题，我们引入了一个可变形特征提取器（DFE）来提取更有效的特征，并设计了一种双相关注意力机制（DCA）来处理其固有的局部连接性。成功的是，我们提出的方法通过引入少量参数扩展了RN的潜力，使其能够感知语义对象的位置。我们在两个主要基准数据集上评估了我们的方法，即Omniglot和Mini-Imagenet，在这两个数据集上我们的方法都达到了最先进的性能。值得注意的是，我们在Omniglot上的5-way 1-shot结果甚至超过了之前的5-way 5-shot结果。",
        "领域": "少样本学习/关系推理/图像分类",
        "问题": "解决少样本学习中分类器对新类别快速适应的问题，特别是处理语义对象在图像中的空间位置关系敏感性问题。",
        "动机": "提高少样本学习中的度量能力，使其更加灵活和鲁棒，特别是针对语义对象在图像中的空间位置关系敏感性问题。",
        "方法": "提出了一种位置感知关系网络（PARN），包括引入可变形特征提取器（DFE）来提取更有效的特征，以及设计双相关注意力机制（DCA）来处理CNN固有的局部连接性问题。",
        "关键词": [
            "少样本学习",
            "关系推理",
            "图像分类",
            "位置感知",
            "可变形特征提取器",
            "双相关注意力机制"
        ],
        "涉及的技术概念": "关系网络（RNs）是一种用于关系推理的架构，通过设计为简单的卷积神经网络（CNN）来获得图像的深度度量能力。可变形特征提取器（DFE）用于提取更有效的特征，双相关注意力机制（DCA）用于处理CNN固有的局部连接性问题。"
    },
    {
        "order": 745,
        "title": "Free-Form Video Inpainting With 3D Gated Convolution and Temporal PatchGAN",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Free-Form_Video_Inpainting_With_3D_Gated_Convolution_and_Temporal_PatchGAN_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Free-Form_Video_Inpainting_With_3D_Gated_Convolution_and_Temporal_PatchGAN_ICCV_2019_paper.html",
        "abstract": "Free-form video inpainting is a very challenging task that could be widely used for video editing such as text removal. Existing patch-based methods could not handle non-repetitive structures such as faces, while directly applying image-based inpainting models to videos will result in temporal inconsistency (see this https://www.youtube.com/watch?v=BuTYfo4bO2I&list=PLnEeMdoBDCISRm0EZYFcQuaJ5ITUaaEIb&index=1). In this paper, we introduce a deep learning based free-form video inpainting model, with proposed 3D gated convolutions to tackle the uncertainty of free-form masks and a novel Temporal PatchGAN loss to enhance temporal consistency. In addition, we collect videos and design a free-form mask generation algorithm to build the free-form video inpainting (FVI) dataset for training and evaluation of video inpainting models. We demonstrate the benefits of these components and experiments on both the FaceForensics and our FVI dataset suggest that our method is superior to existing ones. Related source code, full-resolution result videos and the FVI dataset could be found on Github: https://github.com/amjltc295/Free-Form-Video-Inpainting",
        "中文标题": "使用3D门控卷积和时间PatchGAN进行自由形式视频修复",
        "摘要翻译": "自由形式视频修复是一项非常具有挑战性的任务，可以广泛用于视频编辑，如文本去除。现有的基于补丁的方法无法处理非重复结构，如面部，而直接将基于图像的修复模型应用于视频会导致时间不一致性（参见此链接：https://www.youtube.com/watch?v=BuTYfo4bO2I&list=PLnEeMdoBDCISRm0EZYFcQuaJ5ITUaaEIb&index=1）。在本文中，我们介绍了一种基于深度学习的自由形式视频修复模型，提出了3D门控卷积来解决自由形式掩码的不确定性，并提出了一种新颖的时间PatchGAN损失来增强时间一致性。此外，我们收集了视频并设计了一种自由形式掩码生成算法，以构建自由形式视频修复（FVI）数据集，用于视频修复模型的训练和评估。我们展示了这些组件的优势，并在FaceForensics和我们的FVI数据集上的实验表明，我们的方法优于现有方法。相关源代码、全分辨率结果视频和FVI数据集可以在Github上找到：https://github.com/amjltc295/Free-Form-Video-Inpainting",
        "领域": "视频修复/深度学习/计算机视觉",
        "问题": "解决自由形式视频修复中的时间不一致性和非重复结构处理问题",
        "动机": "现有的视频修复方法在处理非重复结构（如面部）时存在困难，且直接应用图像修复模型到视频中会导致时间不一致性，因此需要一种新的方法来解决这些问题。",
        "方法": "提出了一种基于深度学习的自由形式视频修复模型，使用3D门控卷积处理自由形式掩码的不确定性，并引入时间PatchGAN损失来增强时间一致性。此外，还构建了一个自由形式视频修复（FVI）数据集用于模型的训练和评估。",
        "关键词": [
            "视频修复",
            "3D门控卷积",
            "时间PatchGAN",
            "自由形式掩码",
            "时间一致性"
        ],
        "涉及的技术概念": "3D门控卷积是一种用于处理视频中自由形式掩码不确定性的技术，通过引入额外的门控机制来控制信息的流动。时间PatchGAN损失是一种用于增强视频修复结果时间一致性的损失函数，通过考虑视频帧之间的时间关系来优化模型。自由形式视频修复（FVI）数据集是一个专门为视频修复任务设计的数据集，包含了自由形式掩码的视频，用于训练和评估视频修复模型。"
    },
    {
        "order": 746,
        "title": "Resolving 3D Human Pose Ambiguities With 3D Scene Constraints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hassan_Resolving_3D_Human_Pose_Ambiguities_With_3D_Scene_Constraints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hassan_Resolving_3D_Human_Pose_Ambiguities_With_3D_Scene_Constraints_ICCV_2019_paper.html",
        "abstract": "To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.",
        "中文标题": "利用3D场景约束解决3D人体姿态模糊问题",
        "摘要翻译": "为了理解和分析人类行为，我们需要捕捉人类在世界中的移动和互动。大多数现有方法在进行3D人体姿态估计时没有明确考虑场景。然而，我们观察到世界对身体有约束，反之亦然。为了证明这一点，我们展示了当前的3D人体姿态估计方法产生的结果与3D场景不一致。我们的关键贡献是利用静态3D场景结构来更好地从单目图像中估计人体姿态。该方法通过对象排除的近端关系强制执行，称为PROX。为了测试这一点，我们收集了一个新的数据集，由12个不同的3D场景和20个主体在这些场景中移动和互动的RGB序列组成。我们使用3D人体模型SMPL-X来表示人体姿态，并扩展SMPLify-X以使用场景约束来估计身体姿态。我们通过制定两个主要约束来利用3D场景信息。穿透约束惩罚身体模型与周围3D场景之间的交叉。接触约束鼓励身体的特定部分如果距离和方向足够接近，则与场景表面接触。为了定量评估，我们捕获了一个单独的数据集，其中包含180个RGB帧，其中使用运动捕捉系统估计了真实的身体姿态。我们定量地展示了引入场景约束显著减少了3D关节误差和顶点误差。我们的代码和数据可在https://prox.is.tue.mpg.de上用于研究。",
        "领域": "3D人体姿态估计/场景理解/单目图像分析",
        "问题": "3D人体姿态估计与3D场景不一致的问题",
        "动机": "为了更准确地理解和分析人类行为，需要捕捉人类在3D场景中的移动和互动，而现有方法未充分考虑场景约束。",
        "方法": "利用静态3D场景结构，通过对象排除的近端关系（PROX）强制执行，结合穿透约束和接触约束，从单目图像中估计人体姿态。",
        "关键词": [
            "3D人体姿态估计",
            "场景约束",
            "单目图像分析"
        ],
        "涉及的技术概念": "3D人体姿态估计是指从图像或视频中估计人体的三维姿态。SMPL-X是一个3D人体模型，用于表示人体姿态。SMPLify-X是SMPL-X的扩展，用于估计身体姿态。穿透约束和接触约束是利用3D场景信息来改进姿态估计的两种方法。"
    },
    {
        "order": 747,
        "title": "Multi-Adversarial Faster-RCNN for Unrestricted Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Multi-Adversarial_Faster-RCNN_for_Unrestricted_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Multi-Adversarial_Faster-RCNN_for_Unrestricted_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Conventional object detection methods essentially suppose that the training and testing data are collected from a restricted target domain with expensive labeling cost. For alleviating the problem of domain dependency and cumbersome labeling, this paper proposes to detect objects in unrestricted environment by leveraging domain knowledge trained from an auxiliary source domain with sufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN (MAF) framework for unrestricted object detection, which inherently addresses domain disparity minimization for domain adaptation in feature representation. The paper merits are in three-fold: 1) With the idea that object detectors often becomes domain incompatible when image distribution resulted domain disparity appears, we propose a hierarchical domain feature alignment module, in which multiple adversarial domain classifier submodules for layer-wise domain feature confusion are designed; 2) An information invariant scale reduction module (SRM) for hierarchical feature map resizing is proposed for promoting the training efficiency of adversarial domain adaptation; 3) In order to improve the domain adaptability, the aggregated proposal features with detection results are feed into a proposed weighted gradient reversal layer (WGRL) for characterizing hard confused domain samples. We evaluate our MAF on unrestricted tasks including Cityscapes, KITTI, Sim10k, etc. and the experiments show the state-of-the-art performance over the existing detectors.",
        "中文标题": "多对抗Faster-RCNN用于无限制目标检测",
        "摘要翻译": "传统的目标检测方法基本上假设训练和测试数据是从一个有限的目标域收集的，且标注成本昂贵。为了缓解领域依赖性和繁琐标注的问题，本文提出通过利用从具有足够标签的辅助源域训练的领域知识来检测无限制环境中的目标。具体来说，我们提出了一个多对抗Faster-RCNN（MAF）框架用于无限制目标检测，该框架本质上解决了特征表示中领域适应性的领域差异最小化问题。本文的优点有三方面：1）基于目标检测器在图像分布导致的领域差异出现时往往变得领域不兼容的想法，我们提出了一个层次化领域特征对齐模块，其中设计了多个对抗领域分类器子模块用于层次化领域特征混淆；2）提出了一个信息不变尺度缩减模块（SRM）用于层次化特征图调整，以提高对抗领域适应的训练效率；3）为了提高领域适应性，将检测结果的聚合提议特征输入到一个提出的加权梯度反转层（WGRL）中，以表征难以混淆的领域样本。我们在包括Cityscapes、KITTI、Sim10k等无限制任务上评估了我们的MAF，实验显示其在现有检测器上的最先进性能。",
        "领域": "目标检测/领域适应/对抗学习",
        "问题": "解决目标检测中的领域依赖性和标注成本高的问题",
        "动机": "为了在无限制环境中更有效地检测目标，减少对特定领域数据的依赖和降低标注成本",
        "方法": "提出了一个多对抗Faster-RCNN框架，包括层次化领域特征对齐模块、信息不变尺度缩减模块和加权梯度反转层，以实现领域适应性",
        "关键词": [
            "目标检测",
            "领域适应",
            "对抗学习",
            "特征对齐",
            "尺度缩减"
        ],
        "涉及的技术概念": "多对抗Faster-RCNN（MAF）框架、层次化领域特征对齐模块、信息不变尺度缩减模块（SRM）、加权梯度反转层（WGRL）"
    },
    {
        "order": 748,
        "title": "TextDragon: An End-to-End Framework for Arbitrary Shaped Text Spotting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Feng_TextDragon_An_End-to-End_Framework_for_Arbitrary_Shaped_Text_Spotting_ICCV_2019_paper.html",
        "abstract": "Most existing text spotting methods either focus on horizontal/oriented texts or perform arbitrary shaped text spotting with character-level annotations. In this paper, we propose a novel text spotting framework to detect and recognize text of arbitrary shapes in an end-to-end manner, using only word/line-level annotations for training. Motivated from the name of TextSnake, which is only a detection model, we call the proposed text spotting framework TextDragon. In TextDragon, a text detector is designed to describe the shape of text with a series of quadrangles, which can handle text of arbitrary shapes. To extract arbitrary text regions from feature maps, we propose a new differentiable operator named RoISlide, which is the key to connect arbitrary shaped text detection and recognition. Based on the extracted features through RoISlide, a CNN and CTC based text recognizer is introduced to make the framework free from labeling the location of characters. The proposed method achieves state-of-the-art performance on two curved text benchmarks CTW1500 and Total-Text, and competitive results on the ICDAR 2015 Dataset.",
        "中文标题": "TextDragon：一个用于任意形状文本检测的端到端框架",
        "摘要翻译": "大多数现有的文本检测方法要么专注于水平/定向文本，要么使用字符级注释执行任意形状的文本检测。在本文中，我们提出了一种新颖的文本检测框架，以端到端的方式检测和识别任意形状的文本，仅使用单词/行级注释进行训练。受到TextSnake名称的启发，它仅是一个检测模型，我们将提出的文本检测框架称为TextDragon。在TextDragon中，设计了一个文本检测器，用一系列四边形描述文本的形状，可以处理任意形状的文本。为了从特征图中提取任意文本区域，我们提出了一种新的可微分操作符RoISlide，它是连接任意形状文本检测和识别的关键。基于通过RoISlide提取的特征，引入了一个基于CNN和CTC的文本识别器，使框架无需标注字符的位置。所提出的方法在两个弯曲文本基准CTW1500和Total-Text上实现了最先进的性能，并在ICDAR 2015数据集上取得了竞争性的结果。",
        "领域": "文本检测/文本识别/端到端学习",
        "问题": "解决任意形状文本的检测和识别问题",
        "动机": "现有方法大多局限于水平或定向文本，或需要字符级注释，限制了应用范围和效率",
        "方法": "提出TextDragon框架，包括一个描述文本形状的文本检测器和一个基于RoISlide操作符和CNN、CTC的文本识别器，实现端到端的任意形状文本检测和识别",
        "关键词": [
            "文本检测",
            "文本识别",
            "端到端学习",
            "RoISlide",
            "CNN",
            "CTC"
        ],
        "涉及的技术概念": {
            "TextSnake": "一个仅用于文本检测的模型，TextDragon框架的灵感来源",
            "RoISlide": "一种新的可微分操作符，用于从特征图中提取任意文本区域，是连接文本检测和识别的关键",
            "CNN": "卷积神经网络，用于文本识别",
            "CTC": "连接时序分类，用于文本识别，无需标注字符的位置"
        }
    },
    {
        "order": 749,
        "title": "Tex2Shape: Detailed Full Human Body Geometry From a Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Alldieck_Tex2Shape_Detailed_Full_Human_Body_Geometry_From_a_Single_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Alldieck_Tex2Shape_Detailed_Full_Human_Body_Geometry_From_a_Single_Image_ICCV_2019_paper.html",
        "abstract": "We present a simple yet effective method to infer detailed full human body shape from only a single photograph. Our model can infer full-body shape including face, hair, and clothing including wrinkles at interactive frame-rates. Results feature details even on parts that are occluded in the input image. Our main idea is to turn shape regression into an aligned image-to-image translation problem. The input to our method is a partial texture map of the visible region obtained from off-the-shelf methods. From a partial texture, we estimate detailed normal and vector displacement maps, which can be applied to a low-resolution smooth body model to add detail and clothing. Despite being trained purely with synthetic data, our model generalizes well to real-world photographs. Numerous results demonstrate the versatility and robustness of our method.",
        "中文标题": "Tex2Shape: 从单张图像获取详细全身几何形状",
        "摘要翻译": "我们提出了一种简单而有效的方法，仅从一张照片推断出详细的全身形状。我们的模型能够以交互式帧率推断包括面部、头发和衣物（包括皱纹）在内的全身形状。结果即使在输入图像中被遮挡的部分也显示出细节。我们的主要思想是将形状回归转化为一个对齐的图像到图像翻译问题。我们方法的输入是从现成方法获得的可见区域的部分纹理图。从部分纹理中，我们估计详细的法线和矢量位移图，这些图可以应用于低分辨率平滑身体模型以增加细节和衣物。尽管我们的模型完全使用合成数据进行训练，但它能够很好地泛化到真实世界的照片。大量结果展示了我们方法的多样性和鲁棒性。",
        "领域": "三维重建/人体建模/图像翻译",
        "问题": "从单张图像中推断出详细的全身几何形状",
        "动机": "为了从单张照片中获取包括面部、头发和衣物在内的详细全身形状，即使在输入图像中被遮挡的部分也能显示出细节。",
        "方法": "将形状回归转化为一个对齐的图像到图像翻译问题，从部分纹理中估计详细的法线和矢量位移图，应用于低分辨率平滑身体模型以增加细节和衣物。",
        "关键词": [
            "三维重建",
            "人体建模",
            "图像翻译"
        ],
        "涉及的技术概念": "图像到图像翻译、法线图、矢量位移图、低分辨率平滑身体模型"
    },
    {
        "order": 750,
        "title": "Object Guided External Memory Network for Video Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Object_Guided_External_Memory_Network_for_Video_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Object_Guided_External_Memory_Network_for_Video_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Video object detection is more challenging than image object detection because of the deteriorated frame quality. To enhance the feature representation, state-of-the-art methods propagate temporal information into the deteriorated frame by aligning and aggregating entire feature maps from multiple nearby frames. However, restricted by feature map's low storage-efficiency and vulnerable content-address allocation, long-term temporal information is not fully stressed by these methods. In this work, we propose the first object guided external memory network for online video object detection. Storage-efficiency is handled by object guided hard-attention to selectively store valuable features, and long-term information is protected when stored in an addressable external data matrix. A set of read/write operations are designed to accurately propagate/allocate and delete multi-level memory feature under object guidance. We evaluate our method on the ImageNet VID dataset and achieve state-of-the-art performance as well as good speed-accuracy tradeoff. Furthermore, by visualizing the external memory, we show the detailed object-level reasoning process across frames.",
        "中文标题": "对象引导的外部记忆网络用于视频对象检测",
        "摘要翻译": "视频对象检测比图像对象检测更具挑战性，因为帧质量下降。为了增强特征表示，最先进的方法通过对齐和聚合来自多个附近帧的整个特征图，将时间信息传播到质量下降的帧中。然而，受限于特征图的低存储效率和脆弱的内容地址分配，这些方法未能充分利用长期时间信息。在这项工作中，我们提出了第一个用于在线视频对象检测的对象引导外部记忆网络。通过对象引导的硬注意力选择性地存储有价值的特征来处理存储效率问题，并且当长期信息存储在可寻址的外部数据矩阵中时，它受到保护。设计了一组读/写操作，以在对象引导下准确传播/分配和删除多级记忆特征。我们在ImageNet VID数据集上评估了我们的方法，并实现了最先进的性能以及良好的速度-准确性权衡。此外，通过可视化外部记忆，我们展示了跨帧的详细对象级推理过程。",
        "领域": "视频对象检测/特征表示/时间信息处理",
        "问题": "视频对象检测中由于帧质量下降导致的特征表示不足和长期时间信息利用不充分的问题",
        "动机": "提高视频对象检测的性能，通过更有效地利用长期时间信息和增强特征表示",
        "方法": "提出了对象引导的外部记忆网络，通过对象引导的硬注意力选择性地存储有价值的特征，并设计读/写操作来准确传播/分配和删除多级记忆特征",
        "关键词": [
            "视频对象检测",
            "外部记忆网络",
            "时间信息处理",
            "特征表示"
        ],
        "涉及的技术概念": "对象引导的硬注意力、可寻址的外部数据矩阵、多级记忆特征、读/写操作"
    },
    {
        "order": 751,
        "title": "Chinese Street View Text: Large-Scale Chinese Text Reading With Partially Supervised Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Chinese_Street_View_Text_Large-Scale_Chinese_Text_Reading_With_Partially_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Chinese_Street_View_Text_Large-Scale_Chinese_Text_Reading_With_Partially_ICCV_2019_paper.html",
        "abstract": "Most existing text reading benchmarks make it difficult to evaluate the performance of more advanced deep learning models in large vocabularies due to the limited amount of training data. To address this issue, we introduce a new large-scale text reading benchmark dataset named Chinese Street View Text (C-SVT) with 430,000 street view images, which is at least 14 times as large as the existing Chinese text reading benchmarks. To recognize Chinese text in the wild while keeping large-scale datasets labeling cost-effective, we propose to annotate one part of the C-SVT dataset (30,000 images) in locations and text labels as full annotations and add 400,000 more images, where only the corresponding text-of-interest in the regions is given as weak annotations. To exploit the rich information from the weakly annotated data, we design a text reading network in a partially supervised learning framework, which enables to localize and recognize text, learn from fully and weakly annotated data simultaneously. To localize the best matched text proposals from weakly labeled images, we propose an online proposal matching module incorporated in the whole model, spotting the keyword regions by sharing parameters for end-to-end training. Compared with fully supervised training algorithms, this model can improve the end-to-end recognition performance remarkably by 4.03% in F-score at the same labeling cost. The proposed model can also achieve state-of-the-art results on the ICDAR 2017-RCTW dataset, which demonstrates the effectiveness of the proposed partially supervised learning framework.",
        "中文标题": "中国街景文本：通过部分监督学习进行大规模中文文本阅读",
        "摘要翻译": "大多数现有的文本阅读基准由于训练数据量的限制，难以评估更先进的深度学习模型在大词汇量下的性能。为了解决这个问题，我们引入了一个新的大规模文本阅读基准数据集，名为中国街景文本（C-SVT），包含430,000张街景图像，这至少是现有中文文本阅读基准的14倍。为了在保持大规模数据集标注成本效益的同时识别野外的中文文本，我们提议对C-SVT数据集的一部分（30,000张图像）进行位置和文本标签的完整标注，并添加400,000张图像，其中仅给出感兴趣区域的对应文本作为弱标注。为了利用弱标注数据中的丰富信息，我们在部分监督学习框架中设计了一个文本阅读网络，该网络能够定位和识别文本，同时从完全和弱标注数据中学习。为了从弱标注图像中定位最佳匹配的文本提议，我们提出了一个在线提议匹配模块，该模块通过共享参数进行端到端训练，以发现关键词区域。与完全监督的训练算法相比，该模型在相同的标注成本下，可以将端到端识别性能显著提高4.03%的F分数。所提出的模型还可以在ICDAR 2017-RCTW数据集上实现最先进的结果，这证明了所提出的部分监督学习框架的有效性。",
        "领域": "文本识别/街景分析/部分监督学习",
        "问题": "大规模中文文本阅读的性能评估",
        "动机": "现有文本阅读基准由于训练数据量的限制，难以评估更先进的深度学习模型在大词汇量下的性能",
        "方法": "设计了一个文本阅读网络在部分监督学习框架中，利用弱标注数据中的丰富信息，同时从完全和弱标注数据中学习，并提出了一个在线提议匹配模块以定位最佳匹配的文本提议",
        "关键词": [
            "文本识别",
            "街景分析",
            "部分监督学习"
        ],
        "涉及的技术概念": "部分监督学习框架、文本阅读网络、在线提议匹配模块、端到端训练、F分数"
    },
    {
        "order": 752,
        "title": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Saito_PIFu_Pixel-Aligned_Implicit_Function_for_High-Resolution_Clothed_Human_Digitization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Saito_PIFu_Pixel-Aligned_Implicit_Function_for_High-Resolution_Clothed_Human_Digitization_ICCV_2019_paper.html",
        "abstract": "We introduce Pixel-aligned Implicit Function (PIFu), an implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu produces high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.",
        "中文标题": "PIFu：用于高分辨率穿衣人体数字化的像素对齐隐式函数",
        "摘要翻译": "我们介绍了像素对齐隐式函数（PIFu），这是一种隐式表示，它将2D图像的像素与其对应的3D对象的全局上下文局部对齐。使用PIFu，我们提出了一种端到端的深度学习方法，用于数字化高度详细的穿衣人体，可以从单张图像推断出3D表面和纹理，并且可以选择性地从多张输入图像中推断。高度复杂的形状，如发型、服装及其变化和变形，可以以统一的方式进行数字化。与用于3D深度学习的现有表示相比，PIFu生成高分辨率表面，包括大部分未见的区域，如人的背部。特别是，它不像体素表示那样占用大量内存，可以处理任意拓扑结构，并且生成的表面与输入图像在空间上对齐。此外，虽然以前的技术设计用于处理单张图像或多个视图，但PIFu自然地扩展到任意数量的视图。我们在DeepFashion数据集的真实世界图像上展示了高分辨率和鲁棒的重建，该数据集包含各种具有挑战性的服装类型。我们的方法在公共基准上实现了最先进的性能，并且在从单张图像进行穿衣人体数字化方面优于之前的工作。",
        "领域": "3D重建/人体数字化/深度学习",
        "问题": "从单张或多张图像中高分辨率地数字化穿衣人体，包括复杂的形状和纹理",
        "动机": "现有的3D深度学习表示在数字化穿衣人体时存在分辨率低、内存消耗大、无法处理任意拓扑结构等问题，需要一种更高效、更灵活的方法",
        "方法": "提出了一种端到端的深度学习方法，使用像素对齐隐式函数（PIFu）来局部对齐2D图像的像素与其对应的3D对象的全局上下文，从而推断出3D表面和纹理",
        "关键词": [
            "3D重建",
            "人体数字化",
            "隐式函数",
            "高分辨率",
            "深度学习"
        ],
        "涉及的技术概念": "像素对齐隐式函数（PIFu）是一种隐式表示，用于将2D图像的像素与其对应的3D对象的全局上下文局部对齐。这种方法能够处理任意拓扑结构，并且生成的表面与输入图像在空间上对齐，同时保持内存效率。"
    },
    {
        "order": 753,
        "title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_An_Empirical_Study_of_Spatial_Attention_Mechanisms_in_Deep_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_An_Empirical_Study_of_Spatial_Attention_Mechanisms_in_Deep_Networks_ICCV_2019_paper.html",
        "abstract": "Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.",
        "中文标题": "深度网络中空间注意力机制的实证研究",
        "摘要翻译": "注意力机制已成为深度神经网络中的一个流行组件，然而，关于不同影响因素及从这些因素计算注意力的方法如何影响性能的研究却很少。为了更好地理解注意力机制，我们进行了一项实证研究，该研究在广义注意力公式中消融了各种空间注意力元素，包括主流的Transformer注意力以及流行的可变形卷积和动态卷积模块。该研究在多种应用中进行，得出了关于深度网络中空间注意力的重要发现，其中一些发现与常规理解相悖。例如，我们发现Transformer注意力中的查询和关键内容比较对于自注意力来说可以忽略不计，但对于编码器-解码器注意力至关重要。将可变形卷积与仅关键内容的显著性适当结合，可以在自注意力中实现最佳的准确性与效率的权衡。我们的结果表明，在注意力机制的设计上还有很大的改进空间。",
        "领域": "注意力机制/深度学习/神经网络",
        "问题": "不同影响因素及从这些因素计算注意力的方法如何影响性能",
        "动机": "为了更好地理解注意力机制",
        "方法": "在广义注意力公式中消融了各种空间注意力元素，包括主流的Transformer注意力以及流行的可变形卷积和动态卷积模块",
        "关键词": [
            "空间注意力",
            "Transformer",
            "可变形卷积",
            "动态卷积"
        ],
        "涉及的技术概念": {
            "注意力机制": "一种使神经网络能够专注于输入数据的特定部分的机制。",
            "Transformer": "一种基于注意力机制的深度学习模型，广泛应用于自然语言处理等领域。",
            "可变形卷积": "一种卷积神经网络中的卷积操作，能够根据输入数据动态调整卷积核的形状。",
            "动态卷积": "一种卷积操作，其卷积核的参数可以根据输入数据动态变化。"
        }
    },
    {
        "order": 754,
        "title": "Deep Floor Plan Recognition Using a Multi-Task Network With Room-Boundary-Guided Attention",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Deep_Floor_Plan_Recognition_Using_a_Multi-Task_Network_With_Room-Boundary-Guided_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Deep_Floor_Plan_Recognition_Using_a_Multi-Task_Network_With_Room-Boundary-Guided_ICCV_2019_paper.html",
        "abstract": "This paper presents a new approach to recognize elements in floor plan layouts. Besides walls and rooms, we aim to recognize diverse floor plan elements, such as doors, windows and different types of rooms, in the floor layouts. To this end, we model a hierarchy of floor plan elements and design a deep multi-task neural network with two tasks: one to learn to predict room-boundary elements, and the other to predict rooms with types. More importantly, we formulate the room-boundary-guided attention mechanism in our spatial contextual module to carefully take room-boundary features into account to enhance the room-type predictions. Furthermore, we design a cross-and-within-task weighted loss to balance the multi-label tasks and prepare two new datasets for floor plan recognition. Experimental results demonstrate the superiority and effectiveness of our network over the state-of-the-art methods.",
        "中文标题": "使用具有房间边界引导注意力的多任务网络进行深度平面图识别",
        "摘要翻译": "本文提出了一种新的方法来识别平面图布局中的元素。除了墙壁和房间，我们的目标是识别平面布局中的各种元素，如门、窗和不同类型的房间。为此，我们建模了平面图元素的层次结构，并设计了一个深度多任务神经网络，该网络有两个任务：一个任务是学习预测房间边界元素，另一个任务是预测带有类型的房间。更重要的是，我们在空间上下文模块中制定了房间边界引导的注意力机制，以仔细考虑房间边界特征，从而增强房间类型的预测。此外，我们设计了一个跨任务和任务内加权损失来平衡多标签任务，并准备了两个新的平面图识别数据集。实验结果表明，我们的网络在最新方法上的优越性和有效性。",
        "领域": "平面图识别/深度学习/注意力机制",
        "问题": "识别平面图布局中的多种元素，包括门、窗和不同类型的房间",
        "动机": "提高平面图元素识别的准确性和效率，特别是对于复杂布局中的房间类型预测",
        "方法": "设计了一个深度多任务神经网络，包括预测房间边界元素和房间类型的任务，并引入了房间边界引导的注意力机制来增强预测效果",
        "关键词": [
            "平面图识别",
            "多任务学习",
            "注意力机制"
        ],
        "涉及的技术概念": "深度多任务神经网络、房间边界引导的注意力机制、跨任务和任务内加权损失、平面图元素层次结构建模"
    },
    {
        "order": 755,
        "title": "DF2Net: A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_DF2Net_A_Dense-Fine-Finer_Network_for_Detailed_3D_Face_Reconstruction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_DF2Net_A_Dense-Fine-Finer_Network_for_Detailed_3D_Face_Reconstruction_ICCV_2019_paper.html",
        "abstract": "Reconstructing the detailed geometric structure from a single face image is a challenging problem due to its ill-posed nature and the fine 3D structures to be recovered. This paper proposes a deep Dense-Fine-Finer Network (DF2Net) to address this challenging problem. DF2Net decomposes the reconstruction process into three stages, each of which is processed by an elaborately-designed network, namely D-Net, F-Net, and Fr-Net. D-Net exploits a U-net architecture to map the input image to a dense depth image. F-Net refines the output of D-Net by integrating features from depth and RGB domains, whose output is further enhanced by Fr-Net with a novel multi-resolution hypercolumn architecture. In addition, we introduce three types of data to train these networks, including 3D model synthetic data, 2D image reconstructed data, and fine facial images. We elaborately exploit different datasets (or combination) together with well-designed losses to train different networks. Qualitative evaluation indicates that our DF2Net can effectively reconstruct subtle facial details such as small crow's feet and wrinkles. Our DF2Net achieves performance superior or comparable to state-of-the-art algorithms in qualitative and quantitative analyses on real-world images and the BU-3DFE dataset. Code and the collected 70K image-depth data will be publicly available.",
        "中文标题": "DF2Net: 用于详细3D面部重建的密集-精细-更精细网络",
        "摘要翻译": "从单张面部图像重建详细的几何结构是一个具有挑战性的问题，由于其不适定性质以及需要恢复的精细3D结构。本文提出了一种深度密集-精细-更精细网络（DF2Net）来解决这一挑战性问题。DF2Net将重建过程分解为三个阶段，每个阶段由一个精心设计的网络处理，即D-Net、F-Net和Fr-Net。D-Net利用U-net架构将输入图像映射到密集深度图像。F-Net通过整合深度和RGB域的特征来细化D-Net的输出，其输出由Fr-Net通过一种新颖的多分辨率超列架构进一步增强。此外，我们引入了三种类型的数据来训练这些网络，包括3D模型合成数据、2D图像重建数据和精细面部图像。我们精心利用不同的数据集（或组合）以及精心设计的损失来训练不同的网络。定性评估表明，我们的DF2Net能够有效地重建细微的面部细节，如小皱纹和皱纹。我们的DF2Net在真实世界图像和BU-3DFE数据集上的定性和定量分析中实现了优于或可与最先进算法相媲美的性能。代码和收集的70K图像深度数据将公开提供。",
        "领域": "3D面部重建/深度学习/图像处理",
        "问题": "从单张面部图像重建详细的3D几何结构",
        "动机": "解决由于不适定性质和需要恢复的精细3D结构带来的挑战",
        "方法": "提出DF2Net，将重建过程分解为三个阶段，每个阶段由精心设计的网络处理，并引入三种类型的数据来训练网络",
        "关键词": [
            "3D面部重建",
            "密集深度图像",
            "多分辨率超列架构"
        ],
        "涉及的技术概念": {
            "DF2Net": "一种深度密集-精细-更精细网络，用于从单张面部图像重建详细的3D几何结构",
            "D-Net": "利用U-net架构将输入图像映射到密集深度图像的网络",
            "F-Net": "通过整合深度和RGB域的特征来细化D-Net输出的网络",
            "Fr-Net": "通过一种新颖的多分辨率超列架构进一步增强F-Net输出的网络",
            "3D模型合成数据": "用于训练网络的3D模型数据",
            "2D图像重建数据": "用于训练网络的2D图像数据",
            "精细面部图像": "用于训练网络的精细面部图像数据",
            "BU-3DFE数据集": "用于评估3D面部重建算法性能的数据集"
        }
    },
    {
        "order": 756,
        "title": "Attribute Attention for Semantic Disambiguation in Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Attribute_Attention_for_Semantic_Disambiguation_in_Zero-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Attribute_Attention_for_Semantic_Disambiguation_in_Zero-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Zero-shot learning (ZSL) aims to accurately recognize unseen objects by learning mapping matrices that bridge the gap between visual information and semantic attributes. Previous works implicitly treat attributes equally in compatibility score while ignoring that they have different importance for discrimination, which leads to severe semantic ambiguity. Considering both low-level visual information and global class-level features that relate to this ambiguity, we propose a practical Latent Feature Guided Attribute Attention (LFGAA) framework to perform object-based attribute attention for semantic disambiguation. By distracting semantic activation in dimensions that cause ambiguity, our method outperforms existing state-of-the-art methods on AwA2, CUB and SUN datasets in both inductive and transductive settings.",
        "中文标题": "属性注意力用于零样本学习中的语义消歧",
        "摘要翻译": "零样本学习（ZSL）旨在通过学习映射矩阵来准确识别未见过的对象，这些映射矩阵弥合了视觉信息和语义属性之间的差距。以往的工作在兼容性评分中隐含地平等对待属性，而忽略了它们对于区分的重要性不同，这导致了严重的语义歧义。考虑到与这种歧义相关的低级视觉信息和全局类级特征，我们提出了一个实用的潜在特征引导属性注意力（LFGAA）框架，以执行基于对象的属性注意力进行语义消歧。通过在引起歧义的维度上分散语义激活，我们的方法在AwA2、CUB和SUN数据集上的归纳和转导设置中均优于现有的最先进方法。",
        "领域": "零样本学习/语义消歧/属性注意力",
        "问题": "解决零样本学习中的语义歧义问题",
        "动机": "以往的工作在兼容性评分中隐含地平等对待属性，忽略了它们对于区分的重要性不同，导致严重的语义歧义。",
        "方法": "提出了一个潜在特征引导属性注意力（LFGAA）框架，通过分散语义激活在引起歧义的维度上进行语义消歧。",
        "关键词": [
            "零样本学习",
            "语义消歧",
            "属性注意力"
        ],
        "涉及的技术概念": "零样本学习（ZSL）是一种机器学习方法，旨在识别训练数据中未出现过的类别。语义消歧是指消除语言表达中的歧义，确保信息的准确传递。属性注意力机制是一种通过关注对象的不同属性来提高识别准确性的技术。"
    },
    {
        "order": 757,
        "title": "GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.html",
        "abstract": "Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning structure is designed which can convert a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency and greatly improves the concurrent learning in both appearance and geometry spaces. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training.",
        "中文标题": "GA-DAN: 几何感知的域适应网络用于场景文本检测与识别",
        "摘要翻译": "最近的对抗学习研究在建模外观空间的跨域数据转移方面取得了非常显著的进展，但其在建模几何空间的跨域转移方面的对应研究却远远落后。本文提出了一种创新的几何感知域适应网络（GA-DAN），该网络能够同时在几何空间和外观空间中建模跨域转移，并能够真实地将图像转换为具有非常不同特征的跨域图像。在提出的GA-DAN中，设计了一种新颖的多模态空间学习结构，该结构可以将源域图像转换为目标域中的多个不同空间视图的图像。引入了一种新的解耦循环一致性损失，该损失平衡了循环一致性，并大大提高了在外观和几何空间中的并发学习。所提出的GA-DAN已经在经典的场景文本检测和识别任务中进行了评估，实验表明，域适应图像在网络训练中应用时，实现了卓越的场景文本检测和识别性能。",
        "领域": "场景文本检测/场景文本识别/域适应",
        "问题": "解决跨域场景文本检测和识别中的几何空间和外观空间的转移问题",
        "动机": "提高场景文本检测和识别在跨域情况下的性能",
        "方法": "设计了一种几何感知的域适应网络（GA-DAN），包括多模态空间学习结构和解耦循环一致性损失，以同时在外观和几何空间中进行跨域转移建模",
        "关键词": [
            "场景文本检测",
            "场景文本识别",
            "域适应",
            "几何空间",
            "外观空间",
            "多模态空间学习",
            "解耦循环一致性损失"
        ],
        "涉及的技术概念": "对抗学习用于建模跨域数据转移，几何感知域适应网络（GA-DAN）用于同时在外观和几何空间中进行跨域转移建模，多模态空间学习结构用于转换源域图像到目标域的不同空间视图，解耦循环一致性损失用于平衡循环一致性并提高并发学习效果。"
    },
    {
        "order": 758,
        "title": "Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.html",
        "abstract": "Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ssfootball04/generative_pose.",
        "中文标题": "单目3D人体姿态估计通过生成和顺序排序",
        "摘要翻译": "从静态图像中进行单目3D人体姿态估计是一个具有挑战性的问题，这主要是由于维度灾难和2D到3D提升的不适定性。在本文中，我们提出了一种基于深度条件变分自编码器的模型，该模型能够根据估计的2D姿态合成多种解剖学上合理的3D姿态样本。我们展示了基于CVAE的3D姿态样本集与2D姿态一致，并有助于解决2D到3D提升中的固有模糊性。我们提出了两种策略来获得最终的3D姿态：(a) 深度排序/顺序关系来评分和加权平均候选的3D姿态，称为OrdinalScore，和(b) 通过Oracle的监督。我们报告了使用OrdinalScore在两个基准数据集上接近最先进的结果，以及使用Oracle的最先进结果。我们还展示了我们的管道在没有配对的图像到3D注释的情况下也能产生竞争性的结果。训练和评估代码可在https://github.com/ssfootball04/generative_pose获取。",
        "领域": "3D人体姿态估计/变分自编码器/深度排序",
        "问题": "解决从单目图像中进行3D人体姿态估计的挑战，特别是2D到3D提升的模糊性问题。",
        "动机": "由于维度灾难和2D到3D提升的不适定性，从静态图像中进行单目3D人体姿态估计是一个具有挑战性的问题。",
        "方法": "提出了一种基于深度条件变分自编码器的模型，通过合成多种解剖学上合理的3D姿态样本来解决2D到3D提升中的固有模糊性，并提出了两种策略来获得最终的3D姿态。",
        "关键词": [
            "3D人体姿态估计",
            "变分自编码器",
            "深度排序"
        ],
        "涉及的技术概念": "深度条件变分自编码器（Deep Conditional Variational Autoencoder, CVAE）是一种生成模型，能够根据条件输入生成多样化的输出。在本研究中，CVAE被用来根据2D姿态估计生成多种解剖学上合理的3D姿态样本。深度排序/顺序关系（OrdinalScore）是一种评分和加权平均候选3D姿态的策略，用于解决2D到3D提升中的模糊性问题。Oracle监督是一种通过外部信息源（Oracle）提供的监督信号来指导模型训练的方法。"
    },
    {
        "order": 759,
        "title": "CIIDefence: Defeating Adversarial Attacks by Fusing Class-Specific Image Inpainting and Image Denoising",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class-Specific_Image_Inpainting_and_ICCV_2019_paper.html",
        "abstract": "This paper presents a novel approach for protecting deep neural networks from adversarial attacks, i.e., methods that add well-crafted imperceptible modifications to the original inputs such that they are incorrectly classified with high confidence. The proposed defence mechanism is inspired by the recent works mitigating the adversarial disturbances by the means of image reconstruction and denoising. However, unlike the previous works, we apply the reconstruction only for small and carefully selected image areas that are most influential to the current classification outcome. The selection process is guided by the class activation map responses obtained for multiple top-ranking class labels. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially more tractable than the full image reconstruction, while still being able to prevent the adversarial attacks. Furthermore, we combine the selective image inpainting with wavelet based image denoising to produce a non differentiable layer that prevents attacker from using gradient backpropagation. Moreover, the proposed nonlinearity cannot be easily approximated with simple differentiable alternative as demonstrated in the experiments with Backward Pass Differentiable Approximation (BPDA) attack. Finally, we experimentally show that the proposed Class-specific Image Inpainting Defence (CIIDefence) is able to withstand several powerful adversarial attacks including the BPDA. The obtained results are consistently better compared to the other recent defence approaches.",
        "中文标题": "CIIDefence：通过融合特定类别图像修复和图像去噪来击败对抗性攻击",
        "摘要翻译": "本文提出了一种新颖的方法，用于保护深度神经网络免受对抗性攻击，即那些对原始输入添加精心制作且难以察觉的修改，使其以高置信度被错误分类的方法。所提出的防御机制受到最近通过图像重建和去噪减轻对抗性干扰的工作的启发。然而，与之前的工作不同，我们仅对当前分类结果影响最大的小且精心选择的图像区域应用重建。选择过程由针对多个顶级类别标签获得的类别激活图响应指导。这些区域也是对抗性扰动最显著的区域，因此是最需要净化的区域。由此产生的修复任务比全图像重建更易处理，同时仍能防止对抗性攻击。此外，我们将选择性图像修复与基于小波的图像去噪相结合，产生一个不可微分的层，防止攻击者使用梯度反向传播。此外，所提出的非线性不能轻易地用简单的可微分替代品近似，如在后向传递可微分近似（BPDA）攻击的实验中所证明的。最后，我们通过实验证明，所提出的特定类别图像修复防御（CIIDefence）能够抵御包括BPDA在内的几种强大的对抗性攻击。获得的结果与其他最近的防御方法相比始终更好。",
        "领域": "对抗性防御/图像修复/图像去噪",
        "问题": "如何有效保护深度神经网络免受对抗性攻击",
        "动机": "对抗性攻击通过添加难以察觉的修改使深度神经网络错误分类，需要有效的防御机制来保护模型",
        "方法": "结合特定类别图像修复和基于小波的图像去噪，创建一个不可微分的防御层，防止梯度反向传播",
        "关键词": [
            "对抗性防御",
            "图像修复",
            "图像去噪"
        ],
        "涉及的技术概念": "类别激活图用于指导选择对分类结果影响最大的图像区域进行修复，结合小波变换进行图像去噪，创建一个不可微分的层以防止对抗性攻击。"
    },
    {
        "order": 760,
        "title": "Large-Scale Tag-Based Font Retrieval With Generative Feature Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Large-Scale_Tag-Based_Font_Retrieval_With_Generative_Feature_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Large-Scale_Tag-Based_Font_Retrieval_With_Generative_Feature_Learning_ICCV_2019_paper.html",
        "abstract": "Font selection is one of the most important steps in a design workflow. Traditional methods rely on ordered lists which require significant domain knowledge and are often difficult to use even for trained professionals. In this paper, we address the problem of large-scale tag-based font retrieval which aims to bring semantics to the font selection process and enable people without expert knowledge to use fonts effectively. We collect a large-scale font tagging dataset of high-quality professional fonts. The dataset contains nearly 20,000 fonts, 2,000 tags, and hundreds of thousands of font-tag relations. We propose a novel generative feature learning algorithm that leverages the unique characteristics of fonts. The key idea is that font images are synthetic and can therefore be controlled by the learning algorithm. We design an integrated rendering and learning process so that the visual feature from one image can be used to reconstruct another image with different text. The resulting feature captures important font design details while is robust to nuisance factors such as text. We propose a novel attention mechanism to re-weight the visual feature for joint visual-text modeling. We combine the feature and the attention mechanism in a novel recognition-retrieval model. Experimental results show that our method significantly outperforms the state-of-the-art for the important problem of large-scale tag-based font retrieval.",
        "中文标题": "基于生成特征学习的大规模标签字体检索",
        "摘要翻译": "字体选择是设计工作流程中最重要的步骤之一。传统方法依赖于需要大量领域知识的排序列表，即使对于训练有素的专业人士来说也常常难以使用。在本文中，我们解决了大规模基于标签的字体检索问题，旨在为字体选择过程带来语义，并使没有专业知识的人能够有效地使用字体。我们收集了一个大规模的高质量专业字体标签数据集。该数据集包含近20,000种字体，2,000个标签，以及数十万个字体-标签关系。我们提出了一种新颖的生成特征学习算法，该算法利用了字体的独特特性。关键思想是字体图像是合成的，因此可以由学习算法控制。我们设计了一个集成的渲染和学习过程，以便从一个图像的视觉特征可以用来重建具有不同文本的另一个图像。生成的特征捕捉了重要的字体设计细节，同时对诸如文本等干扰因素具有鲁棒性。我们提出了一种新颖的注意力机制来重新加权视觉特征，用于联合视觉-文本建模。我们将特征和注意力机制结合在一个新颖的识别-检索模型中。实验结果表明，我们的方法在大规模基于标签的字体检索这一重要问题上显著优于现有技术。",
        "领域": "字体设计/信息检索/生成模型",
        "问题": "大规模基于标签的字体检索",
        "动机": "使没有专业知识的人能够有效地使用字体，为字体选择过程带来语义",
        "方法": "提出了一种新颖的生成特征学习算法，设计了一个集成的渲染和学习过程，并提出了一种新颖的注意力机制来重新加权视觉特征，用于联合视觉-文本建模",
        "关键词": [
            "字体检索",
            "生成特征学习",
            "注意力机制",
            "视觉-文本建模"
        ],
        "涉及的技术概念": "生成特征学习算法利用字体的独特特性，通过集成的渲染和学习过程，从一个图像的视觉特征重建具有不同文本的另一个图像。注意力机制用于重新加权视觉特征，以进行联合视觉-文本建模。"
    },
    {
        "order": 761,
        "title": "Aligning Latent Spaces for 3D Hand Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Aligning_Latent_Spaces_for_3D_Hand_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Aligning_Latent_Spaces_for_3D_Hand_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "Hand pose estimation from monocular RGB inputs is a highly challenging task. Many previous works for monocular settings only used RGB information for training despite the availability of corresponding data in other modalities such as depth maps. In this work, we propose to learn a joint latent representation that leverages other modalities as weak labels to boost the RGB-based hand pose estimator. By design, our architecture is highly flexible in embedding various diverse modalities such as heat maps, depth maps and point clouds. In particular, we find that encoding and decoding the point cloud of the hand surface can improve the quality of the joint latent representation. Experiments show that with the aid of other modalities during training, our proposed method boosts the accuracy of RGB-based hand pose estimation systems and significantly outperforms state-of-the-art on two public benchmarks.",
        "中文标题": "对齐潜在空间用于3D手部姿态估计",
        "摘要翻译": "从单目RGB输入中估计手部姿态是一项极具挑战性的任务。许多先前的工作在单目设置下仅使用RGB信息进行训练，尽管在其他模态（如深度图）中也有相应的数据可用。在这项工作中，我们提出学习一个联合潜在表示，该表示利用其他模态作为弱标签来增强基于RGB的手部姿态估计器。通过设计，我们的架构在嵌入各种多样模态（如热图、深度图和点云）方面具有高度灵活性。特别是，我们发现编码和解码手部表面的点云可以提高联合潜在表示的质量。实验表明，在训练过程中借助其他模态，我们提出的方法提高了基于RGB的手部姿态估计系统的准确性，并在两个公共基准上显著优于最先进的技术。",
        "领域": "3D视觉/姿态估计/多模态学习",
        "问题": "从单目RGB输入中准确估计3D手部姿态",
        "动机": "提高基于RGB的手部姿态估计的准确性，通过利用其他模态的数据作为弱标签",
        "方法": "学习一个联合潜在表示，该表示利用其他模态（如热图、深度图和点云）作为弱标签来增强基于RGB的手部姿态估计器",
        "关键词": [
            "3D手部姿态估计",
            "联合潜在表示",
            "多模态学习"
        ],
        "涉及的技术概念": "联合潜在表示是一种将不同模态的数据（如RGB图像、深度图、点云等）映射到一个共享的潜在空间中的方法，以便于跨模态的信息融合和增强特定任务（如手部姿态估计）的性能。"
    },
    {
        "order": 762,
        "title": "ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qin_ThunderNet_Towards_Real-Time_Generic_Object_Detection_on_Mobile_Devices_ICCV_2019_paper.html",
        "abstract": "Real-time generic object detection on mobile platforms is a crucial but challenging computer vision task. Prior lightweight CNN-based detectors are inclined to use one-stage pipeline. In this paper, we investigate the effectiveness of two-stage detectors in real-time generic detection and propose a lightweight two-stage detector named ThunderNet. In the backbone part, we analyze the drawbacks in previous lightweight backbones and present a lightweight backbone designed for object detection. In the detection part, we exploit an extremely efficient RPN and detection head design. To generate more discriminative feature representation, we design two efficient architecture blocks, Context Enhancement Module and Spatial Attention Module. At last, we investigate the balance between the input resolution, the backbone, and the detection head. Benefit from the highly efficient backbone and detection part design, ThunderNet surpasses previous lightweight one-stage detectors with only 40% of the computational cost on PASCAL VOC and COCO benchmarks. Without bells and whistles, ThunderNet runs at 24.1 fps on an ARM-based device with 19.2 AP on COCO. To the best of our knowledge, this is the first real-time detector reported on ARM platforms. Code will be released for paper reproduction.",
        "中文标题": "ThunderNet: 面向移动设备的实时通用目标检测",
        "摘要翻译": "在移动平台上实现实时通用目标检测是一项关键但具有挑战性的计算机视觉任务。之前的轻量级基于CNN的检测器倾向于使用单阶段流程。在本文中，我们研究了两阶段检测器在实时通用检测中的有效性，并提出了一种名为ThunderNet的轻量级两阶段检测器。在骨干网络部分，我们分析了之前轻量级骨干网络的缺点，并提出了一种专为目标检测设计的轻量级骨干网络。在检测部分，我们采用了一种极其高效的RPN和检测头设计。为了生成更具区分性的特征表示，我们设计了两个高效的架构模块，即上下文增强模块和空间注意力模块。最后，我们研究了输入分辨率、骨干网络和检测头之间的平衡。得益于高效的骨干网络和检测部分设计，ThunderNet在PASCAL VOC和COCO基准测试中以仅40%的计算成本超越了之前的轻量级单阶段检测器。无需任何花哨的技巧，ThunderNet在基于ARM的设备上以24.1 fps的速度运行，在COCO上达到19.2 AP。据我们所知，这是首次在ARM平台上报告的实时检测器。代码将被发布以供论文复现。",
        "领域": "目标检测/移动计算/实时系统",
        "问题": "在移动设备上实现实时通用目标检测",
        "动机": "探索两阶段检测器在实时通用检测中的有效性，并提出一种轻量级解决方案以适应移动设备的计算限制",
        "方法": "提出了一种名为ThunderNet的轻量级两阶段检测器，包括专为目标检测设计的轻量级骨干网络、高效的RPN和检测头设计，以及上下文增强模块和空间注意力模块",
        "关键词": [
            "目标检测",
            "移动计算",
            "实时系统",
            "轻量级网络",
            "两阶段检测器"
        ],
        "涉及的技术概念": {
            "CNN": "卷积神经网络，一种深度学习模型，特别适用于处理图像数据",
            "RPN": "区域提议网络，用于生成可能包含目标的区域提议",
            "PASCAL VOC": "一个广泛使用的图像识别、目标检测和分割的基准数据集",
            "COCO": "Common Objects in Context，另一个广泛使用的图像识别、分割和字幕生成的基准数据集",
            "ARM": "一种处理器架构，广泛用于移动设备",
            "fps": "帧每秒，衡量视频或动画流畅度的单位",
            "AP": "平均精度，用于衡量目标检测模型的性能"
        }
    },
    {
        "order": 763,
        "title": "Convolutional Character Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xing_Convolutional_Character_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xing_Convolutional_Character_Networks_ICCV_2019_paper.html",
        "abstract": "Recent progress has been made on developing a unified framework for joint text detection and recognition in natural images, but existing joint models were mostly built on two-stage framework by involving ROI pooling, which can degrade the performance on recognition task. In this work, we propose convolutional character networks, referred as CharNet, which is an one-stage model that can process two tasks simultaneously in one pass. CharNet directly outputs bounding boxes of words and characters, with corresponding character labels. We utilize character as basic element, allowing us to overcome the main difficulty of existing approaches that attempted to optimize text detection jointly with a RNN-based recognition branch. In addition, we develop an iterative character detection approach able to transform the ability of character detection learned from synthetic data to real-world images. These technical improvements result in a simple, compact, yet powerful one-stage model that works reliably on multi-orientation and curved text. We evaluate CharNet on three standard benchmarks, where it consistently outperforms the state-of-the-art approaches [25, 24] by a large margin, e.g., with improvements of 65.33%->71.08% (with generic lexicon) on ICDAR 2015, and 54.0%->69.23% on Total-Text, on end-to-end text recognition. Code is available at: https://github.com/MalongTech/research-charnet.",
        "中文标题": "卷积字符网络",
        "摘要翻译": "最近在开发用于自然图像中联合文本检测和识别的统一框架方面取得了进展，但现有的联合模型大多建立在涉及ROI池化的两阶段框架上，这可能会降低识别任务的性能。在这项工作中，我们提出了卷积字符网络，简称CharNet，这是一个可以在一遍处理中同时处理两个任务的单阶段模型。CharNet直接输出单词和字符的边界框，以及相应的字符标签。我们利用字符作为基本元素，使我们能够克服现有方法试图与基于RNN的识别分支联合优化文本检测的主要困难。此外，我们开发了一种迭代字符检测方法，能够将从合成数据中学到的字符检测能力转换到真实世界的图像中。这些技术改进产生了一个简单、紧凑但功能强大的单阶段模型，该模型在多方向和弯曲文本上可靠地工作。我们在三个标准基准上评估了CharNet，在这些基准上，它始终以较大的优势优于最先进的方法[25, 24]，例如，在ICDAR 2015上，使用通用词典的改进为65.33%->71.08%，在Total-Text上为54.0%->69.23%，在端到端文本识别上。代码可在https://github.com/MalongTech/research-charnet获取。",
        "领域": "文本检测/文本识别/自然图像处理",
        "问题": "现有联合模型在文本检测和识别任务中性能下降的问题",
        "动机": "克服现有方法在联合优化文本检测和基于RNN的识别分支时的困难，提高识别性能",
        "方法": "提出卷积字符网络（CharNet），一个单阶段模型，直接输出单词和字符的边界框及字符标签，并开发迭代字符检测方法",
        "关键词": [
            "文本检测",
            "文本识别",
            "卷积字符网络",
            "迭代字符检测"
        ],
        "涉及的技术概念": {
            "卷积字符网络（CharNet）": "一个单阶段模型，能够同时处理文本检测和识别任务，直接输出单词和字符的边界框及字符标签。",
            "迭代字符检测": "一种方法，能够将从合成数据中学到的字符检测能力转换到真实世界的图像中。",
            "ROI池化": "一种在深度学习模型中用于提取感兴趣区域特征的技术，常用于两阶段框架中。",
            "RNN": "循环神经网络，一种用于处理序列数据的神经网络结构。"
        }
    },
    {
        "order": 764,
        "title": "HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_HEMlets_Pose_Learning_Part-Centric_Heatmap_Triplets_for_Accurate_3D_Human_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_HEMlets_Pose_Learning_Part-Centric_Heatmap_Triplets_for_Accurate_3D_Human_ICCV_2019_paper.html",
        "abstract": "Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network(ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with \"in-the-wild\" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.",
        "中文标题": "HEMlets姿势：学习部分中心热图三元组以进行准确的3D人体姿势估计",
        "摘要翻译": "从单张图像估计3D人体姿势是一项具有挑战性的任务。本工作试图通过引入一个中间状态——部分中心热图三元组（HEMlets）来解决将检测到的2D关节提升到3D空间的不确定性，这缩短了2D观察和3D解释之间的差距。HEMlets利用三个关节热图来表示每个骨骼身体部位的末端关节的相对深度信息。在我们的方法中，首先训练一个卷积网络（ConvNet）从输入图像预测HEMlets，然后进行体积关节热图回归。我们利用积分操作从体积热图中提取关节位置，保证端到端学习。尽管网络设计简单，定量比较显示，与最佳方法相比，性能有显著提升（在Human3.6M上提高了20%）。所提出的方法自然支持使用“野外”图像进行训练，其中只有骨骼关节的弱注释相对深度信息可用。这进一步提高了我们模型的泛化能力，如户外图像的定性比较所验证。",
        "领域": "3D人体姿势估计/卷积网络/热图回归",
        "问题": "从单张图像准确估计3D人体姿势",
        "动机": "解决将检测到的2D关节提升到3D空间的不确定性，缩短2D观察和3D解释之间的差距",
        "方法": "引入部分中心热图三元组（HEMlets），利用卷积网络预测HEMlets，进行体积关节热图回归，并利用积分操作从体积热图中提取关节位置",
        "关键词": [
            "3D人体姿势估计",
            "卷积网络",
            "热图回归",
            "HEMlets",
            "体积热图"
        ],
        "涉及的技术概念": {
            "HEMlets": "部分中心热图三元组，用于表示每个骨骼身体部位的末端关节的相对深度信息",
            "卷积网络（ConvNet）": "用于从输入图像预测HEMlets的神经网络",
            "体积关节热图回归": "一种回归方法，用于从体积热图中提取关节位置",
            "积分操作": "用于从体积热图中提取关节位置的操作，保证端到端学习"
        }
    },
    {
        "order": 765,
        "title": "Dual Student: Breaking the Limits of the Teacher in Semi-Supervised Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Dual_Student_Breaking_the_Limits_of_the_Teacher_in_Semi-Supervised_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ke_Dual_Student_Breaking_the_Limits_of_the_Teacher_in_Semi-Supervised_ICCV_2019_paper.html",
        "abstract": "Recently, consistency-based methods have achieved state-of-the-art results in semi-supervised learning (SSL). These methods always involve two roles, an explicit or implicit teacher model and a student model, and penalize predictions under different perturbations by a consistency constraint. However, the weights of these two roles are tightly coupled since the teacher is essentially an exponential moving average (EMA) of the student. In this work, we show that the coupled EMA teacher causes a performance bottleneck. To address this problem, we introduce Dual Student, which replaces the teacher with another student. We also define a novel concept, stable sample, following which a stabilization constraint is designed for our structure to be trainable. Further, we discuss two variants of our method, which produce even higher performance. Extensive experiments show that our method improves the classification performance significantly on several main SSL benchmarks. Specifically, it reduces the error rate of the 13-layer CNN from 16.84% to 12.39% on CIFAR-10 with 1k labels and from 34.10% to 31.56% on CIFAR-100 with 10k labels. In addition, our method also achieves a clear improvement in domain adaptation.",
        "中文标题": "双学生：突破半监督学习中教师的限制",
        "摘要翻译": "最近，基于一致性的方法在半监督学习（SSL）中取得了最先进的结果。这些方法通常涉及两个角色，一个显式或隐式的教师模型和一个学生模型，并通过一致性约束来惩罚不同扰动下的预测。然而，这两个角色的权重是紧密耦合的，因为教师本质上是学生的指数移动平均（EMA）。在这项工作中，我们展示了耦合的EMA教师会导致性能瓶颈。为了解决这个问题，我们引入了双学生，用另一个学生代替教师。我们还定义了一个新概念，稳定样本，随后为我们的结构设计了一个稳定化约束，使其可训练。此外，我们讨论了我们的方法的两个变体，它们产生了更高的性能。大量实验表明，我们的方法在几个主要的SSL基准上显著提高了分类性能。具体来说，它在CIFAR-10上使用1k标签将13层CNN的错误率从16.84%降低到12.39%，在CIFAR-100上使用10k标签从34.10%降低到31.56%。此外，我们的方法在领域适应方面也取得了明显的改进。",
        "领域": "半监督学习/深度学习/模型优化",
        "问题": "解决半监督学习中教师模型与学生模型权重紧密耦合导致的性能瓶颈问题",
        "动机": "提高半监督学习模型的分类性能，特别是在有限的标签数据情况下",
        "方法": "引入双学生模型替代传统的教师模型，定义稳定样本概念并设计稳定化约束，讨论并实现两种变体以提高性能",
        "关键词": [
            "半监督学习",
            "双学生模型",
            "稳定样本",
            "一致性约束",
            "领域适应"
        ],
        "涉及的技术概念": {
            "一致性约束": "一种通过惩罚不同扰动下的预测来增强模型一致性的方法",
            "指数移动平均（EMA）": "一种计算平均值的方法，其中最近的观测值被赋予更高的权重",
            "稳定样本": "新定义的概念，用于设计稳定化约束，使模型结构可训练",
            "领域适应": "指模型在一个领域上训练后，能够适应并有效应用于另一个相关领域的能力"
        }
    },
    {
        "order": 766,
        "title": "Geometry Normalization Networks for Accurate Scene Text Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Geometry_Normalization_Networks_for_Accurate_Scene_Text_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Geometry_Normalization_Networks_for_Accurate_Scene_Text_Detection_ICCV_2019_paper.html",
        "abstract": "Large geometry (e.g., orientation) variances are the key challenges in the scene text detection. In this work, we first conduct experiments to investigate the capacity of networks for learning geometry variances on detecting scene texts, and find that networks can handle only limited text geometry variances. Then, we put forward a novel Geometry Normalization Module (GNM) with multiple branches, each of which is composed of one Scale Normalization Unit and one Orientation Normalization Unit, to normalize each text instance to one desired canonical geometry range through at least one branch. The GNM is general and readily plugged into existing convolutional neural network based text detectors to construct end-to-end Geometry Normalization Networks (GNNets). Moreover, we propose a geometry-aware training scheme to effectively train the GNNets by sampling and augmenting text instances from a uniform geometry variance distribution. Finally, experiments on popular benchmarks of ICDAR 2015 and ICDAR 2017 MLT validate that our method outperforms all the state-of-the-art approaches remarkably by obtaining one-forward test F-scores of 88.52 and 74.54 respectively.",
        "中文标题": "几何归一化网络用于精确场景文本检测",
        "摘要翻译": "大几何（例如，方向）变化是场景文本检测中的关键挑战。在这项工作中，我们首先进行实验，以研究网络在学习几何变化以检测场景文本方面的能力，并发现网络只能处理有限的文本几何变化。然后，我们提出了一种新颖的几何归一化模块（GNM），该模块具有多个分支，每个分支由一个尺度归一化单元和一个方向归一化单元组成，通过至少一个分支将每个文本实例归一化到所需的规范几何范围。GNM是通用的，可以轻松地插入到现有的基于卷积神经网络的文本检测器中，以构建端到端的几何归一化网络（GNNets）。此外，我们提出了一种几何感知训练方案，通过从均匀几何变化分布中采样和增强文本实例来有效训练GNNets。最后，在ICDAR 2015和ICDAR 2017 MLT的流行基准上的实验验证了我们的方法通过分别获得88.52和74.54的一次测试F分数，显著优于所有最先进的方法。",
        "领域": "场景文本检测/几何归一化/卷积神经网络",
        "问题": "处理场景文本检测中的大几何变化问题",
        "动机": "发现现有网络在处理文本几何变化方面的能力有限，需要一种新的方法来提高检测的准确性和鲁棒性",
        "方法": "提出几何归一化模块（GNM）和几何归一化网络（GNNets），以及几何感知训练方案",
        "关键词": [
            "几何归一化",
            "场景文本检测",
            "卷积神经网络"
        ],
        "涉及的技术概念": "几何归一化模块（GNM）通过多个分支，每个分支包含尺度归一化单元和方向归一化单元，用于将文本实例归一化到规范几何范围。几何归一化网络（GNNets）是将GNM插入现有卷积神经网络文本检测器中构建的端到端网络。几何感知训练方案通过从均匀几何变化分布中采样和增强文本实例来训练GNNets。"
    },
    {
        "order": 767,
        "title": "End-to-End Hand Mesh Recovery From a Monocular RGB Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_End-to-End_Hand_Mesh_Recovery_From_a_Monocular_RGB_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_End-to-End_Hand_Mesh_Recovery_From_a_Monocular_RGB_Image_ICCV_2019_paper.html",
        "abstract": "In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the problem of reconstructing the full 3D mesh of a human hand from a single RGB image. In contrast to existing research on 2D or 3D hand pose estimation from RGB or/and depth image data, HAMR can provide a more expressive and useful mesh representation for monocular hand image understanding. In particular, the mesh representation is achieved by parameterizing a generic 3D hand model with shape and relative 3D joint angles. By utilizing this mesh representation, we can easily compute the 3D joint locations via linear interpolations between the vertexes of the mesh, while obtain the 2D joint locations with a projection of the 3D joints. To this end, a differentiable re-projection loss can be defined in terms of the derived representations and the ground-truth labels, thus making our framework end-to-end trainable. Qualitative experiments show that our framework is capable of recovering appealing 3D hand mesh even in the presence of severe occlusions. Quantitatively, our approach also outperforms the state-of-the-art methods for both 2D and 3D hand pose estimation from a monocular RGB image on several benchmark datasets.",
        "中文标题": "从单目RGB图像端到端恢复手部网格",
        "摘要翻译": "本文提出了一种手部网格恢复（HAMR）框架，旨在解决从单张RGB图像重建人手完整3D网格的问题。与现有的从RGB或/和深度图像数据进行2D或3D手部姿态估计的研究相比，HAMR能够为单目手部图像理解提供更具表现力和实用性的网格表示。特别是，通过参数化一个通用的3D手部模型，包括形状和相对3D关节角度，实现了网格表示。利用这种网格表示，我们可以通过网格顶点之间的线性插值轻松计算3D关节位置，同时通过3D关节的投影获得2D关节位置。为此，可以根据导出的表示和真实标签定义一个可微分的重投影损失，从而使我们的框架能够端到端训练。定性实验表明，即使在存在严重遮挡的情况下，我们的框架也能够恢复出吸引人的3D手部网格。定量上，我们的方法在多个基准数据集上对从单目RGB图像进行2D和3D手部姿态估计的最先进方法也表现出色。",
        "领域": "3D重建/手部姿态估计/单目视觉",
        "问题": "从单张RGB图像重建人手的完整3D网格",
        "动机": "为单目手部图像理解提供更具表现力和实用性的网格表示",
        "方法": "通过参数化一个通用的3D手部模型，包括形状和相对3D关节角度，实现网格表示，并利用这种表示计算3D和2D关节位置，定义一个可微分的重投影损失进行端到端训练",
        "关键词": [
            "3D手部网格",
            "单目RGB图像",
            "手部姿态估计"
        ],
        "涉及的技术概念": "HAMR框架通过参数化3D手部模型实现网格表示，利用线性插值计算3D关节位置，通过投影获得2D关节位置，并定义可微分的重投影损失进行端到端训练。"
    },
    {
        "order": 768,
        "title": "MVP Matching: A Maximum-Value Perfect Matching for Mining Hard Samples, With Application to Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_MVP_Matching_A_Maximum-Value_Perfect_Matching_for_Mining_Hard_Samples_ICCV_2019_paper.html",
        "abstract": "How to correctly stress hard samples in metric learning is critical for visual recognition tasks, especially in challenging person re-ID applications. Pedestrians across cameras with significant appearance variations are easily confused, which could bias the learned metric and slow down the convergence rate. In this paper, we propose a novel weighted complete bipartite graph based maximum-value perfect (MVP) matching for mining the hard samples from a batch of samples. It can emphasize the hard positive and negative sample pairs respectively, and thus relieve adverse optimization and sample imbalance problems. We then develop a new batch-wise MVP matching based loss objective and combine it in an end-to-end deep metric learning manner. It leads to significant improvements in both convergence rate and recognition performance. Extensive empirical results on five person re-ID benchmark datasets, i.e., Market-1501, CUHK03-Detected, CUHK03-Labeled, Duke-MTMC, and MSMT17, demonstrate the superiority of the proposed method. It can accelerate the convergence rate significantly while achieving state-of-the-art performance. The source code of our method is available at https://github.com/IAAI-CVResearchGroup/MVP-metric.",
        "中文标题": "MVP匹配：一种用于挖掘困难样本的最大值完美匹配方法，及其在行人重识别中的应用",
        "摘要翻译": "在度量学习中如何正确强调困难样本对于视觉识别任务至关重要，尤其是在具有挑战性的行人重识别应用中。跨摄像头行人的显著外观变化容易导致混淆，这可能会使学习的度量产生偏差并减慢收敛速度。在本文中，我们提出了一种基于加权完全二分图的最大值完美（MVP）匹配方法，用于从一批样本中挖掘困难样本。它能够分别强调困难的正样本对和负样本对，从而缓解不利的优化和样本不平衡问题。然后，我们开发了一种新的基于批量MVP匹配的损失目标，并以端到端的深度度量学习方式将其结合。这显著提高了收敛速度和识别性能。在五个行人重识别基准数据集（即Market-1501、CUHK03-Detected、CUHK03-Labeled、Duke-MTMC和MSMT17）上的广泛实证结果证明了所提出方法的优越性。它能够显著加速收敛速度，同时实现最先进的性能。我们方法的源代码可在https://github.com/IAAI-CVResearchGroup/MVP-metric获取。",
        "领域": "行人重识别/度量学习/深度学习",
        "问题": "在度量学习中正确强调困难样本，以改善行人重识别任务的性能",
        "动机": "跨摄像头行人的显著外观变化容易导致混淆，影响学习度量的准确性和收敛速度",
        "方法": "提出了一种基于加权完全二分图的最大值完美（MVP）匹配方法，用于挖掘困难样本，并开发了一种新的批量MVP匹配损失目标，结合端到端的深度度量学习方式",
        "关键词": [
            "行人重识别",
            "度量学习",
            "困难样本挖掘",
            "最大值完美匹配",
            "深度学习"
        ],
        "涉及的技术概念": "加权完全二分图、最大值完美匹配（MVP）、深度度量学习、批量MVP匹配损失目标、端到端学习"
    },
    {
        "order": 769,
        "title": "Symmetry-Constrained Rectification Network for Scene Text Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Symmetry-Constrained_Rectification_Network_for_Scene_Text_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Symmetry-Constrained_Rectification_Network_for_Scene_Text_Recognition_ICCV_2019_paper.html",
        "abstract": "Reading text in the wild is a very challenging task due to the diversity of text instances and the complexity of natural scenes. Recently, the community has paid increasing attention to the problem of recognizing text instances with irregular shapes. One intuitive and effective way to handle this problem is to rectify irregular text to a canonical form before recognition. However, these methods might struggle when dealing with highly curved or distorted text instances. To tackle this issue, we propose in this paper a Symmetry-constrained Rectification Network (ScRN) based on local attributes of text instances, such as center line, scale and orientation. Such constraints with an accurate description of text shape enable ScRN to generate better rectification results than existing methods and thus lead to higher recognition accuracy. Our method achieves state-of-the-art performance on text with both regular and irregular shapes. Specifically, the system outperforms existing algorithms by a large margin on datasets that contain quite a proportion of irregular text instances, e.g., ICDAR 2015, SVT-Perspective and CUTE80.",
        "中文标题": "对称约束矫正网络用于场景文本识别",
        "摘要翻译": "在自然场景中阅读文本是一项极具挑战性的任务，这归因于文本实例的多样性和自然场景的复杂性。最近，社区越来越关注识别不规则形状文本实例的问题。处理这一问题的一个直观且有效的方法是在识别之前将不规则文本矫正为规范形式。然而，这些方法在处理高度弯曲或扭曲的文本实例时可能会遇到困难。为了解决这个问题，我们在本文中提出了一种基于文本实例局部属性（如中心线、尺度和方向）的对称约束矫正网络（ScRN）。这些约束与文本形状的准确描述相结合，使ScRN能够生成比现有方法更好的矫正结果，从而提高识别准确率。我们的方法在规则和不规则形状的文本上都达到了最先进的性能。具体来说，该系统在包含大量不规则文本实例的数据集上，如ICDAR 2015、SVT-Perspective和CUTE80，大幅超越了现有算法。",
        "领域": "场景文本识别/文本矫正/不规则文本处理",
        "问题": "识别自然场景中不规则形状的文本实例",
        "动机": "提高不规则形状文本实例的识别准确率",
        "方法": "提出了一种基于文本实例局部属性的对称约束矫正网络（ScRN），通过准确描述文本形状来生成更好的矫正结果",
        "关键词": [
            "场景文本识别",
            "文本矫正",
            "不规则文本处理"
        ],
        "涉及的技术概念": "对称约束矫正网络（ScRN）是一种基于文本实例局部属性（如中心线、尺度和方向）的方法，旨在通过准确描述文本形状来生成更好的矫正结果，从而提高不规则形状文本实例的识别准确率。"
    },
    {
        "order": 770,
        "title": "Adaptive Context Network for Scene Parsing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_Adaptive_Context_Network_for_Scene_Parsing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_Adaptive_Context_Network_for_Scene_Parsing_ICCV_2019_paper.html",
        "abstract": "Recent works attempt to improve scene parsing performance by exploring different levels of contexts, and typically train a well-designed convolutional network to exploit useful contexts across all pixels equally. However, in this paper, we find that the context demands are varying from different pixels or regions in each image. Based on this observation, we propose an Adaptive Context Network (ACNet) to capture the pixel-aware contexts by a competitive fusion of global context and local context according to different per-pixel demands. Specifically, when given a pixel, the global context demand is measured by the similarity between the global feature and its local feature, whose reverse value can also be used to measure the local context demand. We model the two demanding measurements by the proposed global context module and local context module, respectively, to generate their adaptive contextual features. Furthermore, we import multiple such modules to build several adaptive context blocks in different levels of network to obtain a coarse-to-fine result. Finally, comprehensive experimental evaluations demonstrate the effectiveness of the proposed ACNet, and new state-of-the-arts performances are achieved on all four public datasets, i.e. Cityscapes, ADE20K, PASCAL Context, and COCO Stuff.",
        "中文标题": "自适应上下文网络用于场景解析",
        "摘要翻译": "最近的工作尝试通过探索不同层次的上下文来提高场景解析的性能，通常训练一个设计良好的卷积网络来平等地利用所有像素的有用上下文。然而，在本文中，我们发现每个图像中不同像素或区域的上下文需求是不同的。基于这一观察，我们提出了一个自适应上下文网络（ACNet），通过根据不同的每像素需求，对全局上下文和局部上下文进行竞争性融合，以捕捉像素感知的上下文。具体来说，当给定一个像素时，全局上下文需求通过全局特征与其局部特征之间的相似性来衡量，其反向值也可以用来衡量局部上下文需求。我们分别通过提出的全局上下文模块和局部上下文模块来建模这两种需求测量，以生成它们的自适应上下文特征。此外，我们引入了多个这样的模块，在网络的不同层次构建几个自适应上下文块，以获得从粗到细的结果。最后，全面的实验评估证明了所提出的ACNet的有效性，并且在所有四个公共数据集上实现了新的最先进性能，即Cityscapes、ADE20K、PASCAL Context和COCO Stuff。",
        "领域": "场景解析/上下文建模/卷积网络",
        "问题": "如何根据不同像素或区域的上下文需求，有效地进行场景解析",
        "动机": "现有方法平等地利用所有像素的上下文，未能考虑到不同像素或区域的上下文需求差异",
        "方法": "提出自适应上下文网络（ACNet），通过竞争性融合全局上下文和局部上下文，根据不同的每像素需求捕捉像素感知的上下文",
        "关键词": [
            "场景解析",
            "上下文建模",
            "卷积网络",
            "自适应上下文网络",
            "像素感知"
        ],
        "涉及的技术概念": {
            "全局上下文": "指的是整个图像或大区域的上下文信息，用于理解图像的整体结构和内容",
            "局部上下文": "指的是图像中特定像素或小区域的上下文信息，用于理解局部细节",
            "自适应上下文网络（ACNet）": "一种新型网络架构，能够根据每个像素的需求动态调整全局和局部上下文的使用",
            "卷积网络": "一种深度学习模型，特别适用于处理图像数据，通过卷积操作提取特征"
        }
    },
    {
        "order": 771,
        "title": "Robust Multi-Modality Multi-Object Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Robust_Multi-Modality_Multi-Object_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Robust_Multi-Modality_Multi-Object_Tracking_ICCV_2019_paper.html",
        "abstract": "Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT.",
        "中文标题": "鲁棒的多模态多目标跟踪",
        "摘要翻译": "多传感器感知对于确保自动驾驶系统的可靠性和准确性至关重要，而多目标跟踪（MOT）通过追踪动态对象的连续运动来进一步提高这一点。目前大多数多传感器多目标跟踪方法要么因为过于依赖单一输入源（例如，中心摄像头）而缺乏可靠性，要么因为在后处理中融合多个传感器的结果而没有充分利用固有信息而准确性不足。在本研究中，我们设计了一个通用的传感器无关的多模态MOT框架（mmMOT），其中每种模态（即传感器）能够独立执行其角色以保持可靠性，并可以通过一种新颖的多模态融合模块进一步提高其准确性。我们的mmMOT可以以端到端的方式进行训练，使得每种模态的基础特征提取器和跨模态的邻接估计器能够联合优化。我们的mmMOT还首次尝试在MOT的数据关联过程中编码点云的深度表示。我们在具有挑战性的KITTI基准上进行了广泛的实验，以评估所提出框架的有效性，并报告了最先进的性能。代码和模型可在https://github.com/ZwwWayne/mmMOT获取。",
        "领域": "自动驾驶/多目标跟踪/多模态融合",
        "问题": "提高多传感器多目标跟踪的可靠性和准确性",
        "动机": "当前多传感器多目标跟踪方法在可靠性和准确性方面存在不足，需要一种新的方法来充分利用多模态信息",
        "方法": "设计了一个通用的传感器无关的多模态MOT框架（mmMOT），包括独立执行角色的每种模态和一种新颖的多模态融合模块，支持端到端训练和联合优化",
        "关键词": [
            "多目标跟踪",
            "多模态融合",
            "自动驾驶"
        ],
        "涉及的技术概念": "多传感器感知、多目标跟踪（MOT）、多模态融合模块、端到端训练、联合优化、点云深度表示、KITTI基准"
    },
    {
        "order": 772,
        "title": "YOLACT: Real-Time Instance Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bolya_YOLACT_Real-Time_Instance_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bolya_YOLACT_Real-Time_Instance_Segmentation_ICCV_2019_paper.html",
        "abstract": "We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn't depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.",
        "中文标题": "YOLACT: 实时实例分割",
        "摘要翻译": "我们提出了一个简单、全卷积的模型用于实时实例分割，该模型在单个Titan Xp上评估时，在MS COCO上达到了29.8 mAP，速度为33.5 fps，这比之前任何竞争方法都要快得多。此外，我们仅在一个GPU上训练就获得了这一结果。我们通过将实例分割分解为两个并行的子任务来实现这一点：(1)生成一组原型掩码和(2)预测每个实例的掩码系数。然后，我们通过将原型与掩码系数线性组合来生成实例掩码。我们发现，由于这个过程不依赖于重新池化，这种方法产生了非常高质量的掩码，并且免费展示了时间稳定性。此外，我们分析了我们原型的涌现行为，并展示了它们以平移变体的方式自行学习定位实例，尽管是全卷积的。最后，我们还提出了Fast NMS，这是一个比标准NMS快12毫秒的替代方案，只有边际性能损失。",
        "领域": "实例分割/目标检测/卷积神经网络",
        "问题": "实时实例分割的速度和质量问题",
        "动机": "提高实例分割的速度和质量，使其能够在实时应用中有效使用",
        "方法": "将实例分割分解为生成原型掩码和预测掩码系数两个并行子任务，并通过线性组合生成实例掩码",
        "关键词": [
            "实例分割",
            "全卷积网络",
            "实时处理"
        ],
        "涉及的技术概念": {
            "实例分割": "一种计算机视觉任务，旨在识别图像中的每个对象实例并为每个实例生成像素级的分割掩码。",
            "全卷积网络": "一种神经网络架构，其中所有层都是卷积层，适用于像素级预测任务。",
            "原型掩码": "在实例分割中，用于生成最终实例掩码的基础掩码。",
            "掩码系数": "用于调整原型掩码以生成特定实例掩码的系数。",
            "Fast NMS": "一种快速非极大值抑制算法，用于在目标检测中减少重叠框的数量，提高处理速度。"
        }
    },
    {
        "order": 773,
        "title": "Constructing Self-Motivated Pyramid Curriculums for Cross-Domain Semantic Segmentation: A Non-Adversarial Approach",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lian_Constructing_Self-Motivated_Pyramid_Curriculums_for_Cross-Domain_Semantic_Segmentation_A_Non-Adversarial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lian_Constructing_Self-Motivated_Pyramid_Curriculums_for_Cross-Domain_Semantic_Segmentation_A_Non-Adversarial_ICCV_2019_paper.html",
        "abstract": "We propose a new approach, called self-motivated pyramid curriculum domain adaptation (PyCDA), to facilitate the adaptation of semantic segmentation neural networks from synthetic source domains to real target domains. Our approach draws on an insight connecting two existing works: curriculum domain adaptation and self-training. Inspired by the former, PyCDA constructs a pyramid curriculum which contains various properties about the target domain. Those properties are mainly about the desired label distributions over the target domain images, image regions, and pixels. By enforcing the segmentation neural network to observe those properties, we can improve the network's generalization capability to the target domain. Motivated by the self-training, we infer this pyramid of properties by resorting to the semantic segmentation network itself. Unlike prior work, we do not need to maintain any additional models (e.g., logistic regression or discriminator networks) or to solve minmax problems which are often difficult to optimize. We report state-of-the-art results for the adaptation from both GTAV and SYNTHIA to Cityscapes, two popular settings in unsupervised domain adaptation for semantic segmentation.",
        "中文标题": "构建自我激励的金字塔课程用于跨域语义分割：一种非对抗性方法",
        "摘要翻译": "我们提出了一种新方法，称为自我激励的金字塔课程领域适应（PyCDA），以促进语义分割神经网络从合成源域到真实目标域的适应。我们的方法借鉴了两个现有工作的见解：课程领域适应和自我训练。受前者的启发，PyCDA构建了一个包含目标域各种属性的金字塔课程。这些属性主要是关于目标域图像、图像区域和像素的期望标签分布。通过强制分割神经网络观察这些属性，我们可以提高网络对目标域的泛化能力。受自我训练的启发，我们通过依赖语义分割网络本身来推断这些属性的金字塔。与之前的工作不同，我们不需要维护任何额外的模型（例如，逻辑回归或判别器网络）或解决通常难以优化的最小最大问题。我们报告了从GTAV和SYNTHIA到Cityscapes适应的最新结果，这是语义分割无监督领域适应中的两个流行设置。",
        "领域": "语义分割/领域适应/神经网络",
        "问题": "如何提高语义分割神经网络从合成源域到真实目标域的适应能力",
        "动机": "提高神经网络对目标域的泛化能力，无需维护额外模型或解决复杂的优化问题",
        "方法": "构建包含目标域属性的金字塔课程，通过自我训练推断这些属性，强制神经网络观察这些属性以提高泛化能力",
        "关键词": [
            "语义分割",
            "领域适应",
            "神经网络",
            "自我训练",
            "金字塔课程"
        ],
        "涉及的技术概念": "PyCDA方法结合了课程领域适应和自我训练的概念，通过构建一个包含目标域属性的金字塔课程，强制语义分割神经网络观察这些属性，从而提高其对目标域的泛化能力。这种方法避免了维护额外模型或解决复杂的优化问题，实现了从合成源域到真实目标域的有效适应。"
    },
    {
        "order": 774,
        "title": "The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ivanovic_The_Trajectron_Probabilistic_Multi-Agent_Trajectory_Modeling_With_Dynamic_Spatiotemporal_Graphs_ICCV_2019_paper.html",
        "abstract": "Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.",
        "中文标题": "轨迹预测器：基于动态时空图的概率多智能体轨迹建模",
        "摘要翻译": "开发安全的人机交互系统是实现自主智能体在社会中广泛集成的必要步骤。这类系统的一个关键组成部分是能够推理场景中其他智能体的多种潜在未来（例如轨迹）。为此，我们提出了轨迹预测器，一种图结构模型，能够在高度动态和多模态场景中（即场景中的智能体数量随时间变化，并且每个智能体有许多可能的高度不同的未来）同时预测多个智能体的多种潜在未来轨迹。它结合了循环序列建模和变分深度生成建模的工具，为场景中的每个智能体生成未来轨迹的分布。我们在多个数据集上展示了我们模型的性能，在标准轨迹预测指标上获得了最先进的结果，并引入了一个新的指标来比较输出分布的模型。",
        "领域": "自主智能体/人机交互/轨迹预测",
        "问题": "如何在高度动态和多模态场景中预测多个智能体的多种潜在未来轨迹",
        "动机": "开发安全的人机交互系统，实现自主智能体在社会中的广泛集成",
        "方法": "结合循环序列建模和变分深度生成建模的工具，构建图结构模型预测未来轨迹分布",
        "关键词": [
            "自主智能体",
            "人机交互",
            "轨迹预测"
        ],
        "涉及的技术概念": "循环序列建模是一种处理序列数据的技术，能够捕捉时间序列中的依赖关系；变分深度生成建模是一种生成模型，能够学习数据的潜在分布并生成新的数据样本；图结构模型是一种利用图结构表示和处理数据的方法，适用于处理具有复杂关系的数据。"
    },
    {
        "order": 775,
        "title": "Expectation-Maximization Attention Networks for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context, and COCO Stuff, on which we set new records.",
        "中文标题": "期望最大化注意力网络用于语义分割",
        "摘要翻译": "自注意力机制已被广泛用于各种任务。它旨在通过所有位置特征的加权和来计算每个位置的表示。因此，它能够捕捉计算机视觉任务中的长距离关系。然而，由于注意力图是相对于所有其他位置计算的，它计算量大。在本文中，我们将注意力机制形式化为期望最大化的方式，并迭代估计一组更为紧凑的基，基于这些基计算注意力图。通过对这些基的加权求和，得到的表示是低秩的，并且去除了输入中的噪声信息。所提出的期望最大化注意力（EMA）模块对输入的变化具有鲁棒性，并且在内存和计算上也更为友好。此外，我们建立了基的维护和归一化方法以稳定其训练过程。我们在包括PASCAL VOC、PASCAL Context和COCO Stuff在内的流行语义分割基准上进行了广泛的实验，并在这些基准上创造了新的记录。",
        "领域": "语义分割/注意力机制/低秩表示",
        "问题": "自注意力机制在计算上消耗大，难以有效捕捉长距离关系",
        "动机": "为了减少自注意力机制的计算消耗，同时保持其捕捉长距离关系的能力",
        "方法": "将注意力机制形式化为期望最大化的方式，迭代估计一组更为紧凑的基，基于这些基计算注意力图，并通过加权求和得到低秩表示",
        "关键词": [
            "语义分割",
            "注意力机制",
            "低秩表示"
        ],
        "涉及的技术概念": {
            "自注意力机制": "一种通过所有位置特征的加权和来计算每个位置表示的机制，能够捕捉长距离关系",
            "期望最大化": "一种迭代算法，用于在统计模型中寻找参数的最大似然估计",
            "低秩表示": "通过减少矩阵的秩来近似表示数据，可以去除噪声信息并减少计算量"
        }
    },
    {
        "order": 776,
        "title": "'Skimming-Perusal' Tracking: A Framework for Real-Time and Robust Long-Term Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Skimming-Perusal_Tracking_A_Framework_for_Real-Time_and_Robust_Long-Term_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Skimming-Perusal_Tracking_A_Framework_for_Real-Time_and_Robust_Long-Term_Tracking_ICCV_2019_paper.html",
        "abstract": "Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.",
        "中文标题": "'略读-细读'跟踪：一个实时且鲁棒的长期跟踪框架",
        "摘要翻译": "与传统的短期跟踪相比，长期跟踪提出了更多挑战，并且更接近实际应用。然而，相关研究较少，且其性能也有限。在本工作中，我们提出了一个基于提出的略读和细读模块的新颖、鲁棒且实时的长期跟踪框架。细读模块包括一个有效的边界框回归器，用于生成一系列候选提案，以及一个鲁棒的目标验证器，用于推断出最优候选及其置信度分数。基于这个分数，我们的跟踪器确定被跟踪对象是否存在，然后在下一帧中选择局部搜索或全局搜索的跟踪策略。为了加速图像范围内的全局搜索，设计了一个新颖的略读模块，以高效地从大量滑动窗口中选择最可能的区域。在VOT-2018长期和OxUvA长期基准上的大量实验结果表明，所提出的方法实现了最佳性能，并且能够实时运行。源代码可在https://github.com/iiau-tracker/SPLT获取。",
        "领域": "目标跟踪/实时系统/鲁棒性分析",
        "问题": "解决长期跟踪中的实时性和鲁棒性问题",
        "动机": "长期跟踪在实际应用中面临更多挑战，现有研究较少且性能有限，需要一种新的方法来提高跟踪的实时性和鲁棒性",
        "方法": "提出了一个基于略读和细读模块的长期跟踪框架，其中细读模块用于生成候选提案和验证最优候选，略读模块用于加速全局搜索",
        "关键词": [
            "长期跟踪",
            "实时系统",
            "鲁棒性分析",
            "目标验证",
            "滑动窗口"
        ],
        "涉及的技术概念": "边界框回归器用于生成候选提案，目标验证器用于推断最优候选及其置信度分数，略读模块用于从大量滑动窗口中高效选择最可能的区域"
    },
    {
        "order": 777,
        "title": "SparseMask: Differentiable Connectivity Learning for Dense Image Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_SparseMask_Differentiable_Connectivity_Learning_for_Dense_Image_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_SparseMask_Differentiable_Connectivity_Learning_for_Dense_Image_Prediction_ICCV_2019_paper.html",
        "abstract": "In this paper, we aim at automatically searching an efficient network architecture for dense image prediction. Particularly, we follow the encoder-decoder style and focus on designing a connectivity structure for the decoder. To achieve that, we design a densely connected network with learnable connections, named Fully Dense Network, which contains a large set of possible final connectivity structures. We then employ gradient descent to search the optimal connectivity from the dense connections. The search process is guided by a novel loss function, which pushes the weight of each connection to be binary and the connections to be sparse. The discovered connectivity achieves competitive results on two segmentation datasets, while runs more than three times faster and requires less than half parameters compared to the state-of-the-art methods. An extensive experiment shows that the discovered connectivity is compatible with various backbones and generalizes well to other dense image prediction tasks.",
        "中文标题": "SparseMask: 可微分连接性学习用于密集图像预测",
        "摘要翻译": "在本文中，我们旨在自动搜索一种高效的网络架构用于密集图像预测。特别是，我们遵循编码器-解码器风格，并专注于设计解码器的连接结构。为此，我们设计了一个具有可学习连接的密集连接网络，称为全密集网络，它包含大量可能的最终连接结构。然后，我们采用梯度下降法从密集连接中搜索最优连接。搜索过程由一种新颖的损失函数引导，该函数推动每个连接的权重为二进制，并使连接稀疏。发现的连接在两个分割数据集上取得了竞争性的结果，同时运行速度比最先进的方法快三倍以上，并且需要的参数不到一半。广泛的实验表明，发现的连接与各种骨干网络兼容，并且能够很好地推广到其他密集图像预测任务。",
        "领域": "密集图像预测/网络架构搜索/图像分割",
        "问题": "自动搜索高效的网络架构用于密集图像预测",
        "动机": "设计一种能够自动搜索并优化密集图像预测任务中网络架构的方法，以提高效率和性能",
        "方法": "设计了一个具有可学习连接的密集连接网络（全密集网络），并采用梯度下降法搜索最优连接，搜索过程由一种新颖的损失函数引导",
        "关键词": [
            "密集图像预测",
            "网络架构搜索",
            "图像分割"
        ],
        "涉及的技术概念": "密集连接网络（Fully Dense Network）、梯度下降法、损失函数、二进制权重、稀疏连接"
    },
    {
        "order": 778,
        "title": "Multi-Class Part Parsing With Joint Boundary-Semantic Awareness",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Multi-Class_Part_Parsing_With_Joint_Boundary-Semantic_Awareness_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Multi-Class_Part_Parsing_With_Joint_Boundary-Semantic_Awareness_ICCV_2019_paper.html",
        "abstract": "Object part parsing in the wild, which requires to simultaneously detect multiple object classes in the scene and accurately segments semantic parts within each class, is challenging for the joint presence of class-level and part-level ambiguities. Despite its importance, however, this problem is not sufficiently explored in existing works. In this paper, we propose a joint parsing framework with boundary and semantic awareness to address this challenging problem. To handle part-level ambiguity, a boundary awareness module is proposed to make mid-level features at multiple scales attend to part boundaries for accurate part localization, which are then fused with high-level features for effective part recognition. For class-level ambiguity, we further present a semantic awareness module that selects discriminative part features relevant to a category to prevent irrelevant features being merged together. The proposed modules are lightweight and implementation friendly, improving the performance substantially when plugged into various baseline architectures. Without bells and whistles, the full model sets new state-of-the-art results on the Pascal-Part dataset, in both multi-class and the conventional single-class setting, while running substantially faster than recent high-performance approaches.",
        "中文标题": "多类别部分解析与联合边界-语义感知",
        "摘要翻译": "在野外进行对象部分解析，这需要同时检测场景中的多个对象类别并准确分割每个类别内的语义部分，对于类别级别和部分级别的模糊性共同存在的情况来说是一个挑战。尽管其重要性，然而，这个问题在现有工作中并未得到充分探索。在本文中，我们提出了一个具有边界和语义感知的联合解析框架来解决这一挑战性问题。为了处理部分级别的模糊性，提出了一个边界感知模块，使多尺度的中级特征关注部分边界以实现准确的部分定位，然后与高级特征融合以实现有效的部分识别。对于类别级别的模糊性，我们进一步提出了一个语义感知模块，选择与类别相关的判别性部分特征，以防止不相关的特征被合并在一起。所提出的模块轻量级且实现友好，当插入到各种基线架构中时，显著提高了性能。无需花哨的技巧，完整模型在Pascal-Part数据集上，无论是多类别还是传统的单类别设置，都设定了新的最先进结果，同时运行速度显著快于最近的高性能方法。",
        "领域": "对象检测/语义分割/特征融合",
        "问题": "同时检测场景中的多个对象类别并准确分割每个类别内的语义部分",
        "动机": "解决类别级别和部分级别的模糊性共同存在的情况下的对象部分解析问题",
        "方法": "提出了一个具有边界和语义感知的联合解析框架，包括边界感知模块和语义感知模块",
        "关键词": [
            "对象部分解析",
            "边界感知",
            "语义感知"
        ],
        "涉及的技术概念": {
            "边界感知模块": "使多尺度的中级特征关注部分边界以实现准确的部分定位",
            "语义感知模块": "选择与类别相关的判别性部分特征，以防止不相关的特征被合并在一起",
            "Pascal-Part数据集": "用于评估对象部分解析性能的数据集"
        }
    },
    {
        "order": 779,
        "title": "TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Min_TASED-Net_Temporally-Aggregating_Spatial_Encoder-Decoder_Network_for_Video_Saliency_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Min_TASED-Net_Temporally-Aggregating_Spatial_Encoder-Decoder_Network_for_Video_Saliency_Detection_ICCV_2019_paper.html",
        "abstract": "TASED-Net is a 3D fully-convolutional network architecture for video saliency detection. It consists of two building blocks: first, the encoder network extracts low-resolution spatiotemporal features from an input clip of several consecutive frames, and then the following prediction network decodes the encoded features spatially while aggregating all the temporal information. As a result, a single prediction map is produced from an input clip of multiple frames. Frame-wise saliency maps can be predicted by applying TASED-Net in a sliding-window fashion to a video. The proposed approach assumes that the saliency map of any frame can be predicted by considering a limited number of past frames. The results of our extensive experiments on video saliency detection validate this assumption and demonstrate that our fully-convolutional model with temporal aggregation method is effective. TASED-Net significantly outperforms previous state-of-the-art approaches on all three major large-scale datasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After analyzing the results qualitatively, we observe that our model is especially better at attending to salient moving objects.",
        "中文标题": "TASED-Net: 用于视频显著性检测的时间聚合空间编码-解码网络",
        "摘要翻译": "TASED-Net是一种用于视频显著性检测的3D全卷积网络架构。它由两个构建块组成：首先，编码器网络从输入的几个连续帧的剪辑中提取低分辨率的时空特征，然后接下来的预测网络在空间上解码编码的特征，同时聚合所有的时间信息。结果，从多个帧的输入剪辑中产生一个单一的预测图。通过以滑动窗口的方式将TASED-Net应用于视频，可以预测逐帧的显著性图。所提出的方法假设任何帧的显著性图都可以通过考虑有限数量的过去帧来预测。我们在视频显著性检测上的大量实验结果验证了这一假设，并证明了我们的全卷积模型与时间聚合方法的有效性。TASED-Net在所有三个主要的视频显著性检测大规模数据集上显著优于之前的最先进方法：DHF1K、Hollywood2和UCFSports。在定性分析结果后，我们观察到我们的模型在关注显著移动物体方面尤其出色。",
        "领域": "视频显著性检测/时空特征提取/全卷积网络",
        "问题": "如何有效地从视频中检测显著性区域",
        "动机": "提高视频显著性检测的准确性和效率，特别是在关注显著移动物体方面",
        "方法": "采用3D全卷积网络架构，结合编码器网络提取时空特征和预测网络解码并聚合时间信息，以生成显著性图",
        "关键词": [
            "视频显著性检测",
            "时空特征",
            "全卷积网络",
            "时间聚合"
        ],
        "涉及的技术概念": "3D全卷积网络架构用于提取视频中的时空特征，通过编码器网络和预测网络的结合，实现对视频显著性区域的检测。时间聚合方法用于整合时间信息，提高显著性检测的准确性。"
    },
    {
        "order": 780,
        "title": "Significance-Aware Information Bottleneck for Domain Adaptive Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Significance-Aware_Information_Bottleneck_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_Significance-Aware_Information_Bottleneck_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "For unsupervised domain adaptation problems, the strategy of aligning the two domains in latent feature space through adversarial learning has achieved much progress in image classification, but usually fails in semantic segmentation tasks in which the latent representations are overcomplex. In this work, we equip the adversarial network with a \"significance-aware information bottleneck (SIB)\", to address the above problem. The new network structure, called SIBAN, enables a significance-aware feature purification before the adversarial adaptation, which eases the feature alignment and stabilizes the adversarial training course. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method can yield leading results compared with other feature-space alternatives. Moreover, SIBAN can even match the state-of-the-art output-space methods in segmentation accuracy, while the latter are often considered to be better choices for domain adaptive segmentation task.",
        "中文标题": "领域自适应语义分割中的显著性感知信息瓶颈",
        "摘要翻译": "对于无监督领域适应问题，通过对抗学习在潜在特征空间中对齐两个领域的策略在图像分类中取得了很大进展，但在语义分割任务中通常失败，因为潜在表示过于复杂。在这项工作中，我们为对抗网络装备了一个“显著性感知信息瓶颈（SIB）”，以解决上述问题。这种新的网络结构，称为SIBAN，在对抗适应之前实现了显著性感知的特征净化，这简化了特征对齐并稳定了对抗训练过程。在两个领域适应任务中，即GTA5 -> Cityscapes和SYNTHIA -> Cityscapes，我们验证了所提出的方法可以产生领先的结果，与其他特征空间替代方案相比。此外，SIBAN在分割精度上甚至可以匹配最先进的输出空间方法，而后者通常被认为是领域自适应分割任务的更好选择。",
        "领域": "语义分割/领域适应/对抗学习",
        "问题": "解决在语义分割任务中，由于潜在表示过于复杂，导致通过对抗学习在潜在特征空间中对齐两个领域通常失败的问题。",
        "动机": "提高无监督领域适应在语义分割任务中的性能，通过引入显著性感知信息瓶颈来简化特征对齐并稳定对抗训练过程。",
        "方法": "提出了一种新的网络结构SIBAN，该结构在对抗适应之前实现了显著性感知的特征净化。",
        "关键词": [
            "语义分割",
            "领域适应",
            "对抗学习",
            "信息瓶颈"
        ],
        "涉及的技术概念": "显著性感知信息瓶颈（SIB）是一种技术，用于在对抗学习过程中对特征进行净化，以简化特征对齐并稳定训练过程。SIBAN是应用了SIB的网络结构，旨在提高领域自适应语义分割任务的性能。"
    },
    {
        "order": 781,
        "title": "Explaining Neural Networks Semantically and Quantitatively",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.html",
        "abstract": "This paper presents a method to pursue a semantic and quantitative explanation for the knowledge encoded in a convolutional neural network (CNN). The estimation of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, and it is of significant values in real applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, which explains the CNN prediction quantitatively. We discuss the problem of the biased interpretation of CNN predictions. To overcome the biased interpretation, we develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.",
        "中文标题": "从语义和定量角度解释神经网络",
        "摘要翻译": "本文提出了一种方法，旨在对卷积神经网络（CNN）中编码的知识进行语义和定量解释。估计CNN每次预测的具体理由，是理解神经网络的一个关键问题，在实际应用中具有重要价值。在本研究中，我们提出将CNN的知识提炼到一个可解释的加性模型中，该模型定量地解释了CNN的预测。我们讨论了CNN预测的偏见解释问题。为了克服偏见解释，我们开发了先验损失来指导可解释加性模型的学习。实验结果证明了我们方法的有效性。",
        "领域": "神经网络解释/模型可解释性/深度学习应用",
        "问题": "如何对卷积神经网络的预测进行语义和定量解释",
        "动机": "理解卷积神经网络每次预测的具体理由，对实际应用有重要价值",
        "方法": "将卷积神经网络的知识提炼到可解释的加性模型中，开发先验损失来指导模型学习",
        "关键词": [
            "神经网络解释",
            "模型可解释性",
            "加性模型",
            "先验损失"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据。",
            "加性模型": "一种模型，通过将多个简单模型的输出相加来预测结果，易于解释。",
            "先验损失": "在模型训练过程中引入的额外损失项，用于引导模型学习特定的特性或行为。"
        }
    },
    {
        "order": 782,
        "title": "Attacking Optical Flow",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ranjan_Attacking_Optical_Flow_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ranjan_Attacking_Optical_Flow_ICCV_2019_paper.html",
        "abstract": "Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",
        "中文标题": "攻击光流",
        "摘要翻译": "深度神经网络在光流估计问题上达到了最先进的性能。由于光流被用于多个安全关键应用，如自动驾驶汽车，因此了解这些技术的鲁棒性非常重要。最近，已经证明对抗性攻击可以轻易地欺骗深度神经网络错误分类对象。然而，到目前为止，光流网络对对抗性攻击的鲁棒性尚未被研究。在本文中，我们将对抗性补丁攻击扩展到光流网络，并展示这种攻击可以损害其性能。我们展示了破坏图像大小不到1%的小补丁可以显著影响光流估计。我们的攻击导致噪声流估计，这些估计显著超出了攻击区域，在许多情况下甚至完全擦除了场景中物体的运动。虽然使用编码器-解码器架构的网络对这些攻击非常敏感，但我们发现使用空间金字塔架构的网络受影响较小。我们通过可视化它们的特征图并将它们与对这些攻击具有鲁棒性的经典光流技术进行比较，分析了攻击这两种架构的成功和失败。我们还通过将打印的图案放置到真实场景中，证明了这种攻击的实用性。",
        "领域": "光流估计/对抗性攻击/自动驾驶",
        "问题": "光流网络对对抗性攻击的鲁棒性",
        "动机": "了解光流估计技术在安全关键应用中的鲁棒性",
        "方法": "扩展对抗性补丁攻击到光流网络，分析不同网络架构的敏感性和鲁棒性",
        "关键词": [
            "光流估计",
            "对抗性攻击",
            "自动驾驶",
            "编码器-解码器架构",
            "空间金字塔架构"
        ],
        "涉及的技术概念": "光流估计是一种计算图像序列中物体运动的技术，对抗性攻击是指通过添加特定噪声或图案来欺骗神经网络的技术，编码器-解码器架构和空间金字塔架构是两种不同的神经网络架构，用于处理图像和视频数据。"
    },
    {
        "order": 783,
        "title": "Relational Attention Network for Crowd Counting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Relational_Attention_Network_for_Crowd_Counting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Relational_Attention_Network_for_Crowd_Counting_ICCV_2019_paper.html",
        "abstract": "Crowd counting is receiving rapidly growing research interests due to its potential application value in numerous real-world scenarios. However, due to various challenges such as occlusion, insufficient resolution and dynamic backgrounds, crowd counting remains an unsolved problem in computer vision. Density estimation is a popular strategy for crowd counting, where conventional density estimation methods perform pixel-wise regression without explicitly accounting the interdependence of pixels. As a result, independent pixel-wise predictions can be noisy and inconsistent. In order to address such an issue, we propose a Relational Attention Network (RANet) with a self-attention mechanism for capturing interdependence of pixels. The RANet enhances the self-attention mechanism by accounting both short-range and long-range interdependence of pixels, where we respectively denote these implementations as local self-attention (LSA) and global self-attention (GSA). We further introduce a relation module to fuse LSA and GSA to achieve more informative aggregated feature representations. We conduct extensive experiments on four public datasets, including ShanghaiTech A, ShanghaiTech B, UCF-CC-50 and UCF-QNRF. Experimental results on all datasets suggest RANet consistently reduces estimation errors and surpasses the state-of-the-art approaches by large margins.",
        "中文标题": "关系注意力网络用于人群计数",
        "摘要翻译": "人群计数因其在众多现实世界场景中的潜在应用价值而迅速增长研究兴趣。然而，由于遮挡、分辨率不足和动态背景等各种挑战，人群计数仍然是计算机视觉中一个未解决的问题。密度估计是人群计数的一种流行策略，传统的密度估计方法执行像素级回归，而没有明确考虑像素的相互依赖性。因此，独立的像素级预测可能会产生噪声和不一致。为了解决这个问题，我们提出了一种关系注意力网络（RANet），该网络具有自注意力机制，用于捕捉像素的相互依赖性。RANet通过考虑像素的短程和长程相互依赖性来增强自注意力机制，我们分别将这些实现称为局部自注意力（LSA）和全局自注意力（GSA）。我们进一步引入了一个关系模块来融合LSA和GSA，以实现更具信息量的聚合特征表示。我们在四个公共数据集上进行了广泛的实验，包括ShanghaiTech A、ShanghaiTech B、UCF-CC-50和UCF-QNRF。所有数据集的实验结果表明，RANet一致地减少了估计误差，并大幅超越了最先进的方法。",
        "领域": "人群计数/密度估计/自注意力机制",
        "问题": "解决人群计数中的遮挡、分辨率不足和动态背景等挑战",
        "动机": "传统的密度估计方法没有明确考虑像素的相互依赖性，导致预测结果噪声大且不一致",
        "方法": "提出了一种关系注意力网络（RANet），通过自注意力机制捕捉像素的相互依赖性，并引入关系模块融合局部自注意力和全局自注意力",
        "关键词": [
            "人群计数",
            "密度估计",
            "自注意力机制",
            "关系注意力网络",
            "局部自注意力",
            "全局自注意力"
        ],
        "涉及的技术概念": "自注意力机制是一种用于捕捉序列或图像中元素相互依赖性的技术，局部自注意力（LSA）和全局自注意力（GSA）分别指代考虑像素短程和长程相互依赖性的实现方式。关系模块用于融合LSA和GSA，以生成更具信息量的特征表示。"
    },
    {
        "order": 784,
        "title": "PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.html",
        "abstract": "Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%.",
        "中文标题": "PANet：基于原型对齐的少样本图像语义分割",
        "摘要翻译": "尽管深度CNN在图像语义分割方面取得了巨大进展，但它们通常需要大量密集标注的图像进行训练，并且难以泛化到未见过的对象类别。因此，少样本分割被开发出来，以学习仅从少量标注示例中执行分割。在本文中，我们从度量学习的角度解决了具有挑战性的少样本分割问题，并提出了PANet，一种新颖的原型对齐网络，以更好地利用支持集的信息。我们的PANet在嵌入空间中从少量支持图像中学习特定类别的原型表示，然后通过将每个像素与学习到的原型匹配来对查询图像进行分割。通过非参数度量学习，PANet提供了高质量的原型，这些原型对每个语义类别具有代表性，同时对不同类别具有区分性。此外，PANet在支持和查询之间引入了原型对齐正则化。通过这一点，PANet充分利用了支持集的知识，并在少样本分割上提供了更好的泛化能力。值得注意的是，我们的模型在PASCAL-5i上实现了48.1%和55.7%的mIoU分数，分别用于1-shot和5-shot设置，超过了最先进的方法1.8%和8.6%。",
        "领域": "少样本学习/语义分割/度量学习",
        "问题": "解决少样本图像语义分割问题",
        "动机": "深度CNN在图像语义分割上需要大量标注数据且难以泛化到新类别，少样本分割旨在从少量标注示例中学习分割",
        "方法": "提出PANet，一种原型对齐网络，通过非参数度量学习在嵌入空间中学习特定类别的原型表示，并引入原型对齐正则化以充分利用支持集知识",
        "关键词": [
            "少样本学习",
            "语义分割",
            "度量学习",
            "原型对齐",
            "非参数学习"
        ],
        "涉及的技术概念": "深度CNN（深度卷积神经网络）、少样本分割（Few-shot segmentation）、度量学习（Metric learning）、原型对齐（Prototype alignment）、非参数度量学习（Non-parametric metric learning）、mIoU（平均交并比）"
    },
    {
        "order": 785,
        "title": "Pro-Cam SSfM: Projector-Camera System for Structure and Spectral Reflectance From Motion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Pro-Cam_SSfM_Projector-Camera_System_for_Structure_and_Spectral_Reflectance_From_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Pro-Cam_SSfM_Projector-Camera_System_for_Structure_and_Spectral_Reflectance_From_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. In our system, we use a standard RGB camera and leverage an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first reconstruct the 3D points while estimating the poses of the camera and the projector, which are alternately moved around the object, by combining multi-view structured light and structure-from-motion (SfM) techniques. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation model considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on several real objects demonstrate that our system can precisely acquire a dense 3D model with the full spectral reflectance property using off-the-shelf devices.",
        "中文标题": "Pro-Cam SSfM：用于从运动中获取结构和光谱反射率的投影仪-相机系统",
        "摘要翻译": "在本文中，我们提出了一种新颖的投影仪-相机系统，用于实用且低成本地获取具有光谱反射特性的密集物体3D模型。在我们的系统中，我们使用标准的RGB相机，并利用现成的投影仪作为主动照明，用于3D重建和光谱反射率估计。我们首先通过结合多视图结构光和从运动中恢复结构（SfM）技术，重建3D点同时估计相机和投影仪的姿势，它们交替围绕物体移动。然后，我们利用投影仪进行多光谱成像，并基于考虑重建的3D点与估计的投影仪位置之间几何关系的新光谱反射率估计模型，估计每个3D点的光谱反射率。在几个真实物体上的实验结果表明，我们的系统可以精确地获取具有完整光谱反射特性的密集3D模型，使用现成的设备。",
        "领域": "3D重建/光谱成像/多视图几何",
        "问题": "如何实用且低成本地获取具有光谱反射特性的密集物体3D模型",
        "动机": "为了实现对物体3D模型及其光谱反射特性的精确获取，同时降低成本和复杂性",
        "方法": "结合多视图结构光和从运动中恢复结构（SfM）技术进行3D重建和姿势估计，利用投影仪进行多光谱成像，并基于新光谱反射率估计模型估计每个3D点的光谱反射率",
        "关键词": [
            "3D重建",
            "光谱成像",
            "多视图几何",
            "结构光",
            "从运动中恢复结构"
        ],
        "涉及的技术概念": "多视图结构光技术用于3D重建，从运动中恢复结构（SfM）技术用于估计相机和投影仪的姿势，多光谱成像技术用于估计物体的光谱反射率，新光谱反射率估计模型考虑了3D点与投影仪位置之间的几何关系。"
    },
    {
        "order": 786,
        "title": "ACFNet: Attentional Class Feature Network for Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ACFNet_Attentional_Class_Feature_Network_for_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_ACFNet_Attentional_Class_Feature_Network_for_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Recent works have made great progress in semantic segmentation by exploiting richer context, most of which are designed from a spatial perspective. In contrast to previous works, we present the concept of class center which extracts the global context from a categorical perspective. This class-level context describes the overall representation of each class in an image. We further propose a novel module, named Attentional Class Feature (ACF) module, to calculate and adaptively combine different class centers according to each pixel. Based on the ACF module, we introduce a coarse-to-fine segmentation network, called Attentional Class Feature Network (ACFNet), which can be composed of an ACF module and any off-the-shell segmentation network (base network). In this paper, we use two types of base networks to evaluate the effectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85% mIoU on Cityscapes dataset with only finely annotated data used for training.",
        "中文标题": "ACFNet: 用于语义分割的注意力类别特征网络",
        "摘要翻译": "最近的工作通过利用更丰富的上下文在语义分割方面取得了巨大进展，其中大多数是从空间角度设计的。与之前的工作相比，我们提出了类别中心的概念，从分类的角度提取全局上下文。这种类别级别的上下文描述了图像中每个类别的整体表示。我们进一步提出了一个新颖的模块，称为注意力类别特征（ACF）模块，用于根据每个像素计算并自适应地结合不同的类别中心。基于ACF模块，我们引入了一个从粗到细的分割网络，称为注意力类别特征网络（ACFNet），它可以由ACF模块和任何现成的分割网络（基础网络）组成。在本文中，我们使用两种类型的基础网络来评估ACFNet的有效性。我们仅使用精细标注的数据进行训练，在Cityscapes数据集上实现了81.85% mIoU的最新性能。",
        "领域": "语义分割/图像理解/上下文建模",
        "问题": "如何从分类角度提取全局上下文以改进语义分割",
        "动机": "现有的语义分割方法主要从空间角度设计，缺乏从分类角度提取全局上下文的能力，这限制了分割性能的进一步提升。",
        "方法": "提出了类别中心的概念和注意力类别特征（ACF）模块，用于从分类角度提取全局上下文，并自适应地结合不同的类别中心。基于ACF模块，构建了一个从粗到细的分割网络ACFNet。",
        "关键词": [
            "语义分割",
            "注意力机制",
            "类别特征"
        ],
        "涉及的技术概念": {
            "类别中心": "从分类角度提取的全局上下文，描述图像中每个类别的整体表示。",
            "注意力类别特征（ACF）模块": "用于计算并自适应地结合不同类别中心的模块，根据每个像素的需求调整类别中心的影响。",
            "ACFNet": "一个从粗到细的分割网络，结合了ACF模块和任何现成的分割网络，用于提高语义分割的性能。",
            "mIoU": "平均交并比，是评估语义分割性能的常用指标。"
        }
    },
    {
        "order": 787,
        "title": "ShapeMask: Learning to Segment Novel Objects by Refining Shape Priors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kuo_ShapeMask_Learning_to_Segment_Novel_Objects_by_Refining_Shape_Priors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kuo_ShapeMask_Learning_to_Segment_Novel_Objects_by_Refining_Shape_Priors_ICCV_2019_paper.html",
        "abstract": "Instance segmentation aims to detect and segment individual objects in a scene. Most existing methods rely on precise mask annotations of every category. However, it is difficult and costly to segment objects in novel categories because a large number of mask annotations is required. We introduce ShapeMask, which learns the intermediate concept of object shape to address the problem of generalization in instance segmentation to novel categories. ShapeMask starts with a bounding box detection and gradually refines it by first estimating the shape of the detected object through a collection of shape priors. Next, ShapeMask refines the coarse shape into an instance level mask by learning instance embeddings. The shape priors provide a strong cue for object-like prediction, and the instance embeddings model the instance specific appearance information. ShapeMask significantly outperforms the state-of-the-art by 6.4 and 3.8 AP when learning across categories, and obtains competitive performance in the fully supervised setting. It is also robust to inaccurate detections, decreased model capacity, and small training data. Moreover, it runs efficiently with 150ms inference time on a GPU and trains within 11 hours on TPUs. With a larger backbone model, ShapeMask increases the gap with state-of-the-art to 9.4 and 6.2 AP across categories. Code will be publicly available at: https://sites.google.com/view/shapemask/home.",
        "中文标题": "ShapeMask: 通过精炼形状先验学习分割新物体",
        "摘要翻译": "实例分割旨在检测和分割场景中的单个物体。大多数现有方法依赖于每个类别的精确掩码注释。然而，由于需要大量的掩码注释，分割新类别的物体既困难又成本高昂。我们引入了ShapeMask，它学习物体形状的中间概念，以解决实例分割在新类别上的泛化问题。ShapeMask从边界框检测开始，通过一系列形状先验首先估计检测到的物体的形状，然后通过学习实例嵌入将粗略形状精炼为实例级别的掩码。形状先验为物体类预测提供了强有力的线索，而实例嵌入则建模了实例特定的外观信息。ShapeMask在跨类别学习时显著优于最先进的方法，分别提高了6.4和3.8 AP，并在完全监督的设置下获得了竞争性的性能。它对不准确的检测、降低的模型容量和小的训练数据也具有鲁棒性。此外，它在GPU上的推理时间为150ms，在TPU上的训练时间在11小时内。使用更大的骨干模型，ShapeMask在跨类别上将与最先进方法的差距扩大到9.4和6.2 AP。代码将公开发布在：https://sites.google.com/view/shapemask/home。",
        "领域": "实例分割/物体检测/形状估计",
        "问题": "解决实例分割在新类别上的泛化问题",
        "动机": "由于需要大量的掩码注释，分割新类别的物体既困难又成本高昂",
        "方法": "通过一系列形状先验估计检测到的物体的形状，然后通过学习实例嵌入将粗略形状精炼为实例级别的掩码",
        "关键词": [
            "实例分割",
            "形状先验",
            "实例嵌入"
        ],
        "涉及的技术概念": {
            "实例分割": "检测和分割场景中的单个物体",
            "形状先验": "为物体类预测提供强有力的线索",
            "实例嵌入": "建模实例特定的外观信息"
        }
    },
    {
        "order": 788,
        "title": "Mop Moire Patterns Using MopNet",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Mop_Moire_Patterns_Using_MopNet_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Mop_Moire_Patterns_Using_MopNet_ICCV_2019_paper.html",
        "abstract": "Moire pattern is a common image quality degradation caused by frequency aliasing between monitors and cameras when taking screen-shot photos. The complex frequency distribution, imbalanced magnitude in colour channels, and diverse appearance attributes of moire pattern make its removal a challenging problem. In this paper, we propose a Moire pattern Removal Neural Network (MopNet) to solve this problem. All core components of MopNet are specially designed for unique properties of moire patterns, including the multi-scale feature aggregation addressing complex frequency, the channel-wise target edge predictor to exploit imbalanced magnitude among colour channels, and the attribute-aware classifier to characterize the diverse appearance for better modelling Moire patterns. Quantitative and qualitative experimental comparison validate the state-of-the-art performance of MopNet.",
        "中文标题": "使用MopNet处理摩尔纹",
        "摘要翻译": "摩尔纹是一种常见的图像质量退化现象，当拍摄屏幕截图照片时，由显示器和相机之间的频率混叠引起。摩尔纹的复杂频率分布、颜色通道中不平衡的幅度以及多样的外观属性使得其去除成为一个具有挑战性的问题。在本文中，我们提出了一种摩尔纹去除神经网络（MopNet）来解决这个问题。MopNet的所有核心组件都是专门为摩尔纹的独特属性设计的，包括解决复杂频率的多尺度特征聚合、利用颜色通道间不平衡幅度的通道目标边缘预测器，以及表征多样外观以更好建模摩尔纹的属性感知分类器。定量和定性的实验比较验证了MopNet的最先进性能。",
        "领域": "图像去噪/图像恢复/神经网络应用",
        "问题": "去除由频率混叠引起的摩尔纹图像质量退化",
        "动机": "摩尔纹的复杂频率分布、颜色通道中不平衡的幅度以及多样的外观属性使得其去除成为一个具有挑战性的问题",
        "方法": "提出了一种摩尔纹去除神经网络（MopNet），包括多尺度特征聚合、通道目标边缘预测器和属性感知分类器",
        "关键词": [
            "摩尔纹",
            "图像去噪",
            "神经网络"
        ],
        "涉及的技术概念": "摩尔纹是一种由频率混叠引起的图像质量退化现象。MopNet是一种专门设计的神经网络，用于去除摩尔纹，其核心组件包括多尺度特征聚合、通道目标边缘预测器和属性感知分类器，这些组件分别针对摩尔纹的复杂频率分布、颜色通道中不平衡的幅度以及多样的外观属性进行优化。"
    },
    {
        "order": 789,
        "title": "Frame-to-Frame Aggregation of Active Regions in Web Videos for Weakly Supervised Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Frame-to-Frame_Aggregation_of_Active_Regions_in_Web_Videos_for_Weakly_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Frame-to-Frame_Aggregation_of_Active_Regions_in_Web_Videos_for_Weakly_ICCV_2019_paper.html",
        "abstract": "When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",
        "中文标题": "网络视频中活动区域的帧间聚合用于弱监督语义分割",
        "摘要翻译": "当深度神经网络仅在具有图像级标签的数据上训练时，每张图像中激活的区域往往只能识别目标对象的一小部分。我们提出了一种方法，利用从网络自动收集的视频，通过使用静态图像中不存在的时间信息来识别目标对象的更大区域。视频中的时间变化允许目标对象的不同区域被激活。我们获得视频每一帧中的激活区域，然后使用基于光流的扭曲技术将连续帧中的区域聚合到单个图像中。生成的定位图覆盖了更多的目标对象，然后可以用作训练分割网络的代理真实值。这种简单的方法在相同监督水平下优于现有方法，甚至接近依赖额外注释的方法。基于VGG-16和ResNet 101骨干网络，我们的方法在PASCAL VOC 2012测试图像上分别达到了65.0和67.4的mIoU，代表了新的最先进水平。",
        "领域": "语义分割/视频分析/弱监督学习",
        "问题": "在仅使用图像级标签的数据上训练深度神经网络时，激活的区域往往只能识别目标对象的一小部分。",
        "动机": "利用视频中的时间信息来识别目标对象的更大区域，以克服静态图像中信息的局限性。",
        "方法": "提出了一种方法，通过从网络自动收集的视频中获取激活区域，并使用基于光流的扭曲技术将连续帧中的区域聚合到单个图像中，以生成覆盖更多目标对象的定位图，用于训练分割网络。",
        "关键词": [
            "语义分割",
            "视频分析",
            "弱监督学习",
            "光流",
            "VGG-16",
            "ResNet 101"
        ],
        "涉及的技术概念": {
            "图像级标签": "仅对整张图像进行标签标注，而不对图像中的具体对象或区域进行详细标注。",
            "时间信息": "视频中随时间变化的信息，可以捕捉到目标对象在不同时间点的不同状态或视角。",
            "光流": "一种用于估计视频中连续帧之间像素运动的技术，常用于视频分析和运动估计。",
            "mIoU": "平均交并比（Mean Intersection over Union），是语义分割任务中常用的评估指标，用于衡量预测分割区域与真实分割区域的重叠程度。"
        }
    },
    {
        "order": 790,
        "title": "Sequence Level Semantics Aggregation for Video Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Sequence_Level_Semantics_Aggregation_for_Video_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Sequence_Level_Semantics_Aggregation_for_Video_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",
        "中文标题": "序列级语义聚合用于视频目标检测",
        "摘要翻译": "近年来，视频目标检测（VID）已成为一个新兴的研究方向。VID的一个核心问题是由快速运动引起的视频帧外观退化。对于单帧来说，这个问题本质上是病态的。因此，从其他帧聚合特征成为一个自然的选择。现有方法严重依赖光流或循环神经网络进行特征聚合。然而，这些方法更强调时间上接近的帧。在这项工作中，我们认为在全序列级别上聚合特征将为视频目标检测带来更具区分性和鲁棒性的特征。为了实现这一目标，我们设计了一个新颖的序列级语义聚合（SELSA）模块。我们进一步展示了所提出方法与经典谱聚类方法之间的密切关系，为理解VID问题提供了一个新的视角。我们在ImageNet VID和EPIC KITCHENS数据集上测试了所提出的方法，并取得了新的最先进的结果。我们的方法不需要复杂的后处理方法，如Seq-NMS或Tubelet重新评分，这使得流程简单而干净。",
        "领域": "视频目标检测/特征聚合/序列分析",
        "问题": "解决视频目标检测中由于快速运动导致的视频帧外观退化问题",
        "动机": "现有方法主要依赖光流或循环神经网络进行特征聚合，但更强调时间上接近的帧，忽视了全序列级别的特征聚合潜力",
        "方法": "设计了一个序列级语义聚合（SELSA）模块，通过全序列级别的特征聚合来提高视频目标检测的区分性和鲁棒性",
        "关键词": [
            "视频目标检测",
            "特征聚合",
            "序列分析"
        ],
        "涉及的技术概念": "序列级语义聚合（SELSA）模块是一种新颖的特征聚合方法，旨在通过全序列级别的特征聚合来提高视频目标检测的性能。该方法与经典的谱聚类方法有密切关系，为理解视频目标检测问题提供了新的视角。"
    },
    {
        "order": 791,
        "title": "Kernel Modeling Super-Resolution on Real Low-Resolution Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Kernel_Modeling_Super-Resolution_on_Real_Low-Resolution_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Kernel_Modeling_Super-Resolution_on_Real_Low-Resolution_Images_ICCV_2019_paper.html",
        "abstract": "Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels.",
        "中文标题": "真实低分辨率图像上的核建模超分辨率",
        "摘要翻译": "深度卷积神经网络（CNNs）在对应的高分辨率和低分辨率图像对上训练，在单图像超分辨率方面实现了最先进的性能，并超越了之前基于信号处理的方法。然而，当应用于真实照片时，它们的性能受到限制。原因在于它们的训练数据：低分辨率（LR）图像是通过相应高分辨率（HR）图像的双三次插值获得的。应用的卷积核与现实世界中的相机模糊显著不同。因此，虽然当前的CNNs能够很好地超分辨率双三次下采样的LR图像，但它们通常在相机捕获的LR图像上失败。为了提高深度超分辨率CNNs在真实照片上的泛化能力和鲁棒性，我们提出了一种核建模超分辨率网络（KMSR），该网络在训练中融入了模糊核建模。我们提出的KMSR包括两个阶段：首先，我们使用生成对抗网络（GAN）构建一个现实模糊核的池，然后我们使用生成的核构建的HR和相应的LR图像训练一个超分辨率网络。我们广泛的实验验证证明了我们的单图像超分辨率方法在具有未知模糊核的照片上的有效性。",
        "领域": "超分辨率/图像恢复/生成对抗网络",
        "问题": "提高深度卷积神经网络在真实照片上的超分辨率性能",
        "动机": "现有的深度卷积神经网络在真实照片上的超分辨率性能受限，主要因为训练数据中的低分辨率图像是通过双三次插值获得的，这与现实世界中的相机模糊不同。",
        "方法": "提出了一种核建模超分辨率网络（KMSR），该网络在训练中融入了模糊核建模，包括使用生成对抗网络（GAN）构建现实模糊核的池，然后使用生成的核构建的HR和相应的LR图像训练超分辨率网络。",
        "关键词": [
            "超分辨率",
            "图像恢复",
            "生成对抗网络"
        ],
        "涉及的技术概念": "深度卷积神经网络（CNNs）、生成对抗网络（GAN）、模糊核建模、双三次插值、单图像超分辨率"
    },
    {
        "order": 792,
        "title": "Boundary-Aware Feature Propagation for Scene Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_Boundary-Aware_Feature_Propagation_for_Scene_Segmentation_ICCV_2019_paper.html",
        "abstract": "In this work, we address the challenging issue of scene segmentation. To increase the feature similarity of the same object while keeping the feature discrimination of different objects, we explore to propagate information throughout the image under the control of objects' boundaries. To this end, we first propose to learn the boundary as an additional semantic class to enable the network to be aware of the boundary layout. Then, we propose unidirectional acyclic graphs (UAGs) to model the function of undirected cyclic graphs (UCGs), which structurize the image via building graphic pixel-by-pixel connections, in an efficient and effective way. Furthermore, we propose a boundary-aware feature propagation (BFP) module to harvest and propagate the local features within their regions isolated by the learned boundaries in the UAG-structured image. The proposed BFP is capable of splitting the feature propagation into a set of semantic groups via building strong connections among the same segment region but weak connections between different segment regions. Without bells and whistles, our approach achieves new state-of-the-art segmentation performance on three challenging semantic segmentation datasets, i.e., PASCAL-Context, CamVid, and Cityscapes.",
        "中文标题": "边界感知特征传播用于场景分割",
        "摘要翻译": "在这项工作中，我们解决了场景分割这一具有挑战性的问题。为了提高同一对象的特征相似性，同时保持不同对象的特征区分度，我们探索在对象边界的控制下在整个图像中传播信息。为此，我们首先提出将边界学习为额外的语义类别，使网络能够意识到边界布局。然后，我们提出使用单向无环图（UAGs）来模拟无向环图（UCGs）的功能，通过建立像素间的图形连接来结构化图像，这是一种高效且有效的方式。此外，我们提出了一个边界感知特征传播（BFP）模块，以在UAG结构化的图像中，通过学习的边界隔离的区域内部收获和传播局部特征。所提出的BFP能够通过在同一分割区域之间建立强连接，而在不同分割区域之间建立弱连接，将特征传播分割成一组语义组。无需任何花哨的技巧，我们的方法在三个具有挑战性的语义分割数据集上实现了新的最先进的分割性能，即PASCAL-Context、CamVid和Cityscapes。",
        "领域": "场景理解/语义分割/图像分析",
        "问题": "提高场景分割中同一对象的特征相似性，同时保持不同对象的特征区分度",
        "动机": "探索在对象边界的控制下在整个图像中传播信息，以提高分割性能",
        "方法": "提出将边界学习为额外的语义类别，使用单向无环图（UAGs）模拟无向环图（UCGs）的功能，并提出边界感知特征传播（BFP）模块",
        "关键词": [
            "场景分割",
            "特征传播",
            "边界感知"
        ],
        "涉及的技术概念": "单向无环图（UAGs）用于模拟无向环图（UCGs）的功能，边界感知特征传播（BFP）模块用于在图像中传播局部特征，通过学习的边界隔离的区域内部进行特征传播。"
    },
    {
        "order": 793,
        "title": "Video Object Segmentation Using Space-Time Memory Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.html",
        "abstract": "We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, J of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set).",
        "中文标题": "使用时空记忆网络的视频对象分割",
        "摘要翻译": "我们提出了一种新颖的半监督视频对象分割解决方案。由于问题的性质，可用的线索（例如带有对象掩码的视频帧）随着中间预测而变得更加丰富。然而，现有方法无法充分利用这一丰富的信息源。我们通过利用记忆网络解决了这个问题，并学会了从所有可用来源读取相关信息。在我们的框架中，带有对象掩码的过去帧形成了一个外部记忆，而当前帧作为查询则使用记忆中的掩码信息进行分割。具体来说，查询和记忆在特征空间中密集匹配，以前馈方式覆盖所有时空像素位置。与之前的方法相比，大量使用引导信息使我们能够更好地处理外观变化和遮挡等挑战。我们在最新的基准测试集上验证了我们的方法，并实现了最先进的性能（在Youtube-VOS验证集上的总得分为79.4，在DAVIS 2016/2017验证集上的J分别为88.7和79.2），同时具有快速的运行时间（在DAVIS 2016验证集上为0.16秒/帧）。",
        "领域": "视频对象分割/记忆网络/半监督学习",
        "问题": "半监督视频对象分割中现有方法无法充分利用丰富的中间预测信息",
        "动机": "解决现有方法在半监督视频对象分割中无法充分利用中间预测信息的问题",
        "方法": "利用记忆网络，将带有对象掩码的过去帧作为外部记忆，当前帧作为查询，在特征空间中密集匹配，使用记忆中的掩码信息进行分割",
        "关键词": [
            "视频对象分割",
            "记忆网络",
            "半监督学习",
            "特征空间匹配",
            "外观变化",
            "遮挡处理"
        ],
        "涉及的技术概念": "记忆网络：一种能够存储和检索过去信息的网络结构，用于处理序列数据。特征空间匹配：在特征空间中比较和匹配不同数据点的方法，用于识别相似性。半监督学习：一种结合了少量标注数据和大量未标注数据进行学习的方法。"
    },
    {
        "order": 794,
        "title": "Learning to Jointly Generate and Separate Reflections",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Learning_to_Jointly_Generate_and_Separate_Reflections_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Learning_to_Jointly_Generate_and_Separate_Reflections_ICCV_2019_paper.html",
        "abstract": "Existing learning-based single image reflection removal methods using paired training data have fundamental limitations about the generalization capability on real-world reflections due to the limited variations in training pairs. In this work, we propose to jointly generate and separate reflections within a weakly-supervised learning framework, aiming to model the reflection image formation more comprehensively with abundant unpaired supervision. By imposing the adversarial losses and combinable mapping mechanism in a multi-task structure, the proposed framework elegantly integrates the two separate stages of reflection generation and separation into a unified model. The gradient constraint is incorporated into the concurrent training process of the multi-task learning as well. In particular, we built up an unpaired reflection dataset with 4,027 images, which is useful for facilitating the weakly-supervised learning of reflection removal model. Extensive experiments on a public benchmark dataset show that our framework performs favorably against state-of-the-art methods and consistently produces visually appealing results.",
        "中文标题": "学习联合生成和分离反射",
        "摘要翻译": "现有的基于学习的单图像反射去除方法使用配对训练数据，由于训练对中的变化有限，对现实世界反射的泛化能力存在根本限制。在这项工作中，我们提出在一个弱监督学习框架内联合生成和分离反射，旨在通过丰富的未配对监督更全面地建模反射图像的形成。通过在多任务结构中施加对抗性损失和可组合映射机制，所提出的框架优雅地将反射生成和分离的两个独立阶段集成到一个统一的模型中。梯度约束也被纳入多任务学习的并发训练过程中。特别是，我们建立了一个包含4,027张图像的未配对反射数据集，这对于促进反射去除模型的弱监督学习非常有用。在公共基准数据集上的大量实验表明，我们的框架在性能上优于最先进的方法，并始终产生视觉上吸引人的结果。",
        "领域": "图像反射去除/弱监督学习/多任务学习",
        "问题": "现有单图像反射去除方法在现实世界反射上的泛化能力有限",
        "动机": "通过弱监督学习框架联合生成和分离反射，以更全面地建模反射图像的形成",
        "方法": "在多任务结构中施加对抗性损失和可组合映射机制，集成反射生成和分离阶段，并纳入梯度约束",
        "关键词": [
            "图像反射去除",
            "弱监督学习",
            "多任务学习",
            "对抗性损失",
            "可组合映射机制"
        ],
        "涉及的技术概念": "对抗性损失是一种用于生成对抗网络（GANs）中的损失函数，旨在通过对抗过程提高生成模型的质量。可组合映射机制指的是在模型中设计能够灵活组合不同功能模块的机制，以适应多任务学习的需求。梯度约束是在训练过程中对模型参数的梯度进行限制，以防止过拟合或提高模型的泛化能力。"
    },
    {
        "order": 795,
        "title": "Self-Ensembling With GAN-Based Data Augmentation for Domain Adaptation in Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Self-Ensembling_With_GAN-Based_Data_Augmentation_for_Domain_Adaptation_in_Semantic_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Self-Ensembling_With_GAN-Based_Data_Augmentation_for_Domain_Adaptation_in_Semantic_ICCV_2019_paper.html",
        "abstract": "Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks.",
        "中文标题": "基于GAN数据增强的自集成用于语义分割中的领域适应",
        "摘要翻译": "基于深度学习的语义分割方法存在一个内在限制，即训练模型需要大量带有像素级注释的数据。为了解决这一挑战性问题，许多研究者关注于语义分割的无监督领域适应。无监督领域适应旨在将源领域上训练的模型适应到目标领域。在本文中，我们引入了一种自集成技术，这是分类中领域适应的一种成功方法。然而，将自集成应用于语义分割非常困难，因为自集成中使用的经过大量调整的手动数据增强对于减少语义分割中的大领域差距并不有效。为了克服这一限制，我们提出了一个由两个互补组件组成的新框架。首先，我们提出了一种基于生成对抗网络（GANs）的数据增强方法，这种方法计算效率高且有效，有助于促进领域对齐。给定这些增强的图像，我们应用自集成来提高分割网络在目标领域上的性能。所提出的方法在无监督领域适应基准上优于最先进的语义分割方法。",
        "领域": "语义分割/领域适应/生成对抗网络",
        "问题": "训练语义分割模型需要大量带有像素级注释的数据，这是一个挑战性问题。",
        "动机": "为了解决训练语义分割模型需要大量标注数据的问题，研究者关注于无监督领域适应，旨在将源领域上训练的模型适应到目标领域。",
        "方法": "提出了一个由基于生成对抗网络（GANs）的数据增强方法和自集成技术组成的新框架，以提高分割网络在目标领域上的性能。",
        "关键词": [
            "语义分割",
            "领域适应",
            "生成对抗网络",
            "自集成",
            "数据增强"
        ],
        "涉及的技术概念": {
            "自集成技术": "一种在分类中用于领域适应的成功方法，通过集成多个模型的预测来提高性能。",
            "生成对抗网络（GANs）": "一种深度学习模型，通过生成器和判别器的对抗过程来生成新的数据样本。",
            "无监督领域适应": "一种技术，旨在将源领域上训练的模型适应到目标领域，而无需目标领域的标注数据。"
        }
    },
    {
        "order": 796,
        "title": "Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Zero-Shot_Video_Object_Segmentation_via_Attentive_Graph_Neural_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Zero-Shot_Video_Object_Segmentation_via_Attentive_Graph_Neural_Networks_ICCV_2019_paper.html",
        "abstract": "This work proposes a novel attentive graph neural network (AGNN) for zero-shot video object segmentation (ZVOS). The suggested AGNN recasts this task as a process of iterative information fusion over video graphs. Specifically, AGNN builds a fully connected graph to efficiently represent frames as nodes, and relations between arbitrary frame pairs as edges. The underlying pair-wise relations are described by a differentiable attention mechanism. Through parametric message passing, AGNN is able to efficiently capture and mine much richer and higher-order relations between video frames, thus enabling a more complete understanding of video content and more accurate foreground estimation. Experimental results on three video segmentation datasets show that AGNN sets a new state-of-the-art in each case. To further demonstrate the generalizability of our framework, we extend AGNN to an additional task: image object co-segmentation (IOCS). We perform experiments on two famous IOCS datasets and observe again the superiority of our AGNN model. The extensive experiments verify that AGNN is able to learn the underlying semantic/appearance relationships among video frames or related images, and discover the common objects.",
        "中文标题": "通过注意力图神经网络进行零样本视频对象分割",
        "摘要翻译": "本工作提出了一种新颖的注意力图神经网络（AGNN）用于零样本视频对象分割（ZVOS）。所提出的AGNN将此任务重新定义为视频图上迭代信息融合的过程。具体来说，AGNN构建了一个全连接图，以有效地将帧表示为节点，将任意帧对之间的关系表示为边。底层成对关系由可微分注意力机制描述。通过参数化消息传递，AGNN能够有效地捕捉和挖掘视频帧之间更丰富和更高阶的关系，从而实现对视频内容的更完整理解和更准确的前景估计。在三个视频分割数据集上的实验结果表明，AGNN在每种情况下都设定了新的最先进水平。为了进一步证明我们框架的通用性，我们将AGNN扩展到另一个任务：图像对象共分割（IOCS）。我们在两个著名的IOCS数据集上进行了实验，并再次观察到我们的AGNN模型的优越性。广泛的实验验证了AGNN能够学习视频帧或相关图像之间的潜在语义/外观关系，并发现共同的对象。",
        "领域": "视频对象分割/图像共分割/图神经网络",
        "问题": "零样本视频对象分割和图像对象共分割",
        "动机": "提高视频对象分割和图像对象共分割的准确性和效率，通过捕捉和挖掘视频帧或相关图像之间更丰富和更高阶的关系",
        "方法": "提出了一种新颖的注意力图神经网络（AGNN），通过构建全连接图来表示视频帧和它们之间的关系，利用可微分注意力机制描述成对关系，并通过参数化消息传递捕捉和挖掘视频帧之间更丰富和更高阶的关系",
        "关键词": [
            "零样本学习",
            "视频对象分割",
            "图像共分割",
            "图神经网络",
            "注意力机制"
        ],
        "涉及的技术概念": "注意力图神经网络（AGNN）是一种结合了图神经网络和注意力机制的模型，用于处理视频对象分割和图像对象共分割任务。通过构建全连接图来表示视频帧和它们之间的关系，并利用可微分注意力机制来描述这些关系，AGNN能够有效地捕捉和挖掘视频帧或相关图像之间的潜在语义/外观关系，从而实现更准确的分割。"
    },
    {
        "order": 797,
        "title": "Deep Multi-Model Fusion for Single-Image Dehazing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Deep_Multi-Model_Fusion_for_Single-Image_Dehazing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Deep_Multi-Model_Fusion_for_Single-Image_Dehazing_ICCV_2019_paper.html",
        "abstract": "This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts.",
        "中文标题": "深度多模型融合用于单图像去雾",
        "摘要翻译": "本文提出了一种深度多模型融合网络，以注意力机制整合多个模型到不同的层，并提升单图像去雾的性能。为此，我们首先制定了注意力特征整合模块，以最大化整合不同卷积神经网络（CNN）层的特征，并生成注意力多层次整合特征（AMLIF）。然后，从AMLIF中，我们进一步预测了大气散射模型的无雾结果，以及四个雾层分离模型的结果，然后将这些结果融合在一起，生成最终的无雾图像。为了评估我们方法的有效性，我们在两个广泛使用的去雾基准数据集以及两组真实世界的雾霾图像上，将我们的网络与几种最先进的方法进行了比较。实验结果表明，我们的方法在定量和定性上均优于最先进的方法。",
        "领域": "图像去雾/卷积神经网络/注意力机制",
        "问题": "单图像去雾",
        "动机": "提升单图像去雾的性能",
        "方法": "提出了一种深度多模型融合网络，通过注意力机制整合多个模型到不同的层，生成注意力多层次整合特征（AMLIF），并进一步预测无雾结果，最终融合生成无雾图像。",
        "关键词": [
            "图像去雾",
            "卷积神经网络",
            "注意力机制",
            "多模型融合"
        ],
        "涉及的技术概念": {
            "深度多模型融合网络": "一种网络结构，用于整合多个模型以提升性能。",
            "注意力特征整合模块": "用于最大化整合不同CNN层特征的模块。",
            "注意力多层次整合特征（AMLIF）": "通过注意力机制整合不同CNN层特征后生成的特征。",
            "大气散射模型": "用于描述雾霾对图像影响的模型。",
            "雾层分离模型": "用于从雾霾图像中分离出不同雾层的模型。"
        }
    },
    {
        "order": 798,
        "title": "Explaining the Ambiguity of Object Detection and 6D Pose From Visual Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Manhardt_Explaining_the_Ambiguity_of_Object_Detection_and_6D_Pose_From_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Manhardt_Explaining_the_Ambiguity_of_Object_Detection_and_6D_Pose_From_ICCV_2019_paper.html",
        "abstract": "3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with these ambiguities. For each object instance we predict multiple 6D pose outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.",
        "中文标题": "解释视觉数据中物体检测和6D姿态的模糊性",
        "摘要翻译": "从单张图像进行3D物体检测和姿态估计是两个本质上模糊的问题。由于形状对称性、遮挡和重复纹理，物体从不同视角看往往相似。这种在检测和姿态估计中的模糊性意味着一个物体实例可以由多个不同的姿态甚至类别完美描述。在这项工作中，我们提出明确处理这些模糊性。对于每个物体实例，我们预测多个6D姿态结果，以估计由对称性和重复纹理生成的特定姿态分布。当视觉外观唯一识别出一个有效姿态时，分布会坍缩为单一结果。我们展示了我们方法的好处，不仅为姿态模糊性提供了更好的解释，而且在姿态估计方面也提供了更高的准确性。",
        "领域": "3D物体检测/姿态估计/视觉模糊性处理",
        "问题": "处理3D物体检测和姿态估计中的模糊性问题",
        "动机": "由于形状对称性、遮挡和重复纹理，物体从不同视角看往往相似，导致检测和姿态估计存在模糊性",
        "方法": "预测每个物体实例的多个6D姿态结果，估计由对称性和重复纹理生成的特定姿态分布，当视觉外观唯一识别出一个有效姿态时，分布坍缩为单一结果",
        "关键词": [
            "3D物体检测",
            "姿态估计",
            "视觉模糊性"
        ],
        "涉及的技术概念": "6D姿态估计指的是在三维空间中物体的位置（3D坐标）和方向（3D旋转）的估计。视觉模糊性处理指的是在视觉数据中处理由于物体形状对称性、遮挡和重复纹理等因素导致的识别和定位模糊性问题。"
    },
    {
        "order": 799,
        "title": "MeteorNet: Deep Learning on Dynamic 3D Point Cloud Sequences",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_MeteorNet_Deep_Learning_on_Dynamic_3D_Point_Cloud_Sequences_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_MeteorNet_Deep_Learning_on_Dynamic_3D_Point_Cloud_Sequences_ICCV_2019_paper.html",
        "abstract": "Understanding dynamic 3D environment is crucial for robotic agents and many other applications. We propose a novel neural network architecture called MeteorNet for learning representations for dynamic 3D point cloud sequences. Different from previous work that adopts a grid-based representation and applies 3D or 4D convolutions, our network directly processes point clouds. We propose two ways to construct spatiotemporal neighborhoods for each point in the point cloud sequence. Information from these neighborhoods is aggregated to learn features per point. We benchmark our network on a variety of 3D recognition tasks including action recognition, semantic segmentation and scene flow estimation. MeteorNet shows stronger performance than previous grid-based methods while achieving state-of-the-art performance on Synthia. MeteorNet also outperforms previous baseline methods that are able to process at most two consecutive point clouds. To the best of our knowledge, this is the first work on deep learning for dynamic raw point cloud sequences.",
        "中文标题": "MeteorNet: 动态3D点云序列的深度学习",
        "摘要翻译": "理解动态3D环境对于机器人代理和许多其他应用至关重要。我们提出了一种名为MeteorNet的新型神经网络架构，用于学习动态3D点云序列的表示。与之前采用基于网格的表示并应用3D或4D卷积的工作不同，我们的网络直接处理点云。我们提出了两种方法来为点云序列中的每个点构建时空邻域。从这些邻域中聚合信息以学习每个点的特征。我们在包括动作识别、语义分割和场景流估计在内的多种3D识别任务上对我们的网络进行了基准测试。MeteorNet显示出比之前基于网格的方法更强的性能，同时在Synthia上实现了最先进的性能。MeteorNet还优于之前最多只能处理两个连续点云的基线方法。据我们所知，这是首次关于动态原始点云序列深度学习的工作。",
        "领域": "3D视觉/机器人视觉/动态场景理解",
        "问题": "如何有效地从动态3D点云序列中学习表示",
        "动机": "为了提升机器人代理和其他应用对动态3D环境的理解能力",
        "方法": "提出了一种新型神经网络架构MeteorNet，直接处理点云，并提出了两种构建时空邻域的方法来聚合信息学习特征",
        "关键词": [
            "3D点云",
            "动态场景",
            "时空邻域"
        ],
        "涉及的技术概念": {
            "动态3D点云序列": "指随时间变化的3D点云数据，用于表示动态3D环境",
            "时空邻域": "在点云序列中，为每个点构建的包含时间和空间信息的邻域",
            "3D识别任务": "包括动作识别、语义分割和场景流估计等任务，旨在从3D数据中提取有用信息"
        }
    },
    {
        "order": 800,
        "title": "Deep Learning for Seeing Through Window With Raindrops",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Quan_Deep_Learning_for_Seeing_Through_Window_With_Raindrops_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Quan_Deep_Learning_for_Seeing_Through_Window_With_Raindrops_ICCV_2019_paper.html",
        "abstract": "When taking pictures through glass window in rainy day, the images are comprised and corrupted by the raindrops adhered to glass surfaces. It is a challenging problem to remove the effect of raindrops from an image. The key task is how to accurately and robustly identify the raindrop regions in an image. This paper develops a convolutional neural network (CNN) for removing the effect of raindrops from an image. In the proposed CNN, we introduce a double attention mechanism that concurrently guides the CNN using shape-driven attention and channel re-calibration. The shape-driven attention exploits physical shape priors of raindrops, i.e. convexness and contour closedness, to accurately locate raindrops, and the channel re-calibration improves the robustness when processing raindrops with varying appearances. The experimental results show that the proposed CNN outperforms the state-of-the-art approaches in terms of both quantitative metrics and visual quality.",
        "中文标题": "深度学习用于透过带雨滴的窗户看世界",
        "摘要翻译": "在雨天透过玻璃窗拍摄照片时，图像会受到附着在玻璃表面的雨滴的影响而受损。从图像中去除雨滴的影响是一个具有挑战性的问题。关键任务是如何准确且鲁棒地识别图像中的雨滴区域。本文开发了一种卷积神经网络（CNN），用于从图像中去除雨滴的影响。在所提出的CNN中，我们引入了一种双重注意力机制，该机制同时使用形状驱动的注意力和通道重新校准来指导CNN。形状驱动的注意力利用雨滴的物理形状先验，即凸性和轮廓闭合性，来准确定位雨滴，而通道重新校准则提高了处理具有不同外观的雨滴时的鲁棒性。实验结果表明，所提出的CNN在定量指标和视觉质量方面均优于最先进的方法。",
        "领域": "图像去雨/图像修复/卷积神经网络",
        "问题": "如何从图像中准确且鲁棒地去除雨滴的影响",
        "动机": "雨天透过玻璃窗拍摄的照片因雨滴附着而受损，需要一种有效的方法去除雨滴影响",
        "方法": "开发了一种卷积神经网络（CNN），并引入双重注意力机制，包括形状驱动的注意力和通道重新校准，以提高雨滴定位的准确性和处理不同外观雨滴的鲁棒性",
        "关键词": [
            "图像去雨",
            "卷积神经网络",
            "双重注意力机制"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "双重注意力机制": "一种同时使用形状驱动的注意力和通道重新校准的机制，用于提高模型性能",
            "形状驱动的注意力": "利用雨滴的物理形状先验（如凸性和轮廓闭合性）来准确定位雨滴",
            "通道重新校准": "一种提高模型在处理具有不同外观的雨滴时的鲁棒性的技术"
        }
    },
    {
        "order": 801,
        "title": "Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Accurate_Monocular_3D_Object_Detection_via_Color-Embedded_3D_Reconstruction_for_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a monocular 3D object detection framework in the domain of autonomous driving. Unlike previous image-based methods which focus on RGB feature extracted from 2D images, our method solves this problem in the reconstructed 3D space in order to exploit 3D contexts explicitly. To this end, we first leverage a stand-alone module to transform the input data from 2D image plane to 3D point clouds space for a better input representation, then we perform the 3D detection using PointNet backbone net to obtain objects' 3D locations, dimensions and orientations. To enhance the discriminative capability of point clouds, we propose a multi-modal feature fusion module to embed the complementary RGB cue into the generated point clouds representation. We argue that it is more effective to infer the 3D bounding boxes from the generated 3D scene space (i.e., X,Y, Z space) compared to the image plane (i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows that our approach boosts the performance of state-of-the-art monocular approach by a large margin.",
        "中文标题": "通过颜色嵌入的3D重建实现精确的单目3D物体检测用于自动驾驶",
        "摘要翻译": "本文中，我们提出了一个在自动驾驶领域内的单目3D物体检测框架。与之前基于图像的方法主要关注从2D图像中提取的RGB特征不同，我们的方法在重建的3D空间中解决这个问题，以便显式地利用3D上下文。为此，我们首先利用一个独立的模块将输入数据从2D图像平面转换到3D点云空间，以获得更好的输入表示，然后我们使用PointNet骨干网络进行3D检测，以获取物体的3D位置、尺寸和方向。为了增强点云的区分能力，我们提出了一个多模态特征融合模块，将互补的RGB线索嵌入到生成的点云表示中。我们认为，与图像平面（即R,G,B图像平面）相比，从生成的3D场景空间（即X,Y,Z空间）推断3D边界框更为有效。在具有挑战性的KITTI数据集上的评估显示，我们的方法大幅提升了最先进的单目方法的性能。",
        "领域": "自动驾驶/3D重建/物体检测",
        "问题": "如何在自动驾驶中实现精确的单目3D物体检测",
        "动机": "为了在自动驾驶中更有效地利用3D上下文信息，提高物体检测的准确性",
        "方法": "首先将2D图像数据转换为3D点云空间，然后使用PointNet进行3D检测，并通过多模态特征融合模块将RGB信息嵌入点云表示中",
        "关键词": [
            "单目3D物体检测",
            "3D重建",
            "多模态特征融合"
        ],
        "涉及的技术概念": {
            "单目3D物体检测": "使用单个摄像头进行3D空间中的物体检测",
            "3D重建": "从2D图像中重建3D场景或物体的过程",
            "PointNet": "一种用于处理点云数据的深度学习网络",
            "多模态特征融合": "将来自不同模态（如RGB图像和点云）的特征结合起来，以提高模型的性能"
        }
    },
    {
        "order": 802,
        "title": "3D Instance Segmentation via Multi-Task Metric Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lahoud_3D_Instance_Segmentation_via_Multi-Task_Metric_Learning_ICCV_2019_paper.html",
        "abstract": "We propose a novel method for instance label segmentation of dense 3D voxel grids. We target volumetric scene representations, which have been acquired with depth sensors or multi-view stereo methods and which have been processed with semantic 3D reconstruction or scene completion methods. The main task is to learn shape information about individual object instances in order to accurately separate them, including connected and incompletely scanned objects. We solve the 3D instance-labeling problem with a multi-task learning strategy. The first goal is to learn an abstract feature embedding, which groups voxels with the same instance label close to each other while separating clusters with different instance labels from each other. The second goal is to learn instance information by densely estimating directional information of the instance's center of mass for each voxel. This is particularly useful to find instance boundaries in the clustering post-processing step, as well as, for scoring the segmentation quality for the first goal. Both synthetic and real-world experiments demonstrate the viability and merits of our approach. In fact, it achieves state-of-the-art performance on the ScanNet 3D instance segmentation benchmark.",
        "中文标题": "通过多任务度量学习进行3D实例分割",
        "摘要翻译": "我们提出了一种新颖的方法，用于密集3D体素网格的实例标签分割。我们针对通过深度传感器或多视角立体方法获取的体素场景表示，这些表示已经通过语义3D重建或场景完成方法处理。主要任务是学习关于单个对象实例的形状信息，以便准确地将它们分离，包括连接的和不完全扫描的对象。我们通过多任务学习策略解决了3D实例标记问题。第一个目标是学习一个抽象的特征嵌入，它将具有相同实例标签的体素分组在一起，同时将具有不同实例标签的集群分开。第二个目标是通过密集估计每个体素的实例质心方向信息来学习实例信息。这对于在聚类后处理步骤中找到实例边界以及为第一个目标评分分割质量特别有用。合成和真实世界的实验都证明了我们方法的可行性和优点。事实上，它在ScanNet 3D实例分割基准测试中达到了最先进的性能。",
        "领域": "3D视觉/实例分割/体素网格处理",
        "问题": "解决密集3D体素网格中的实例标签分割问题，包括连接和不完全扫描对象的准确分离",
        "动机": "为了准确分离3D场景中的对象实例，包括那些连接和不完全扫描的对象，需要学习关于单个对象实例的形状信息",
        "方法": "采用多任务学习策略，首先学习一个抽象的特征嵌入以分组相同实例标签的体素并分离不同实例标签的集群，其次通过密集估计每个体素的实例质心方向信息来学习实例信息",
        "关键词": [
            "3D实例分割",
            "多任务学习",
            "体素网格",
            "特征嵌入",
            "实例质心方向"
        ],
        "涉及的技术概念": "多任务学习策略用于3D实例分割，通过特征嵌入和实例质心方向信息的学习来提高分割的准确性和质量。"
    },
    {
        "order": 803,
        "title": "Mask-ShadowGAN: Learning to Remove Shadows From Unpaired Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Mask-ShadowGAN_Learning_to_Remove_Shadows_From_Unpaired_Data_ICCV_2019_paper.html",
        "abstract": "This paper presents a new method for shadow removal using unpaired data, enabling us to avoid tedious annotations and obtain more diverse training samples. However, directly employing adversarial learning and cycle-consistency constraints is insufficient to learn the underlying relationship between the shadow and shadow-free domains, since the mapping between shadow and shadow-free images is not simply one-to-one. To address the problem, we formulate Mask-ShadowGAN, a new deep framework that automatically learns to produce a shadow mask from the input shadow image and then takes the mask to guide the shadow generation via re-formulated cycle-consistency constraints. Particularly, the framework simultaneously learns to produce shadow masks and learns to remove shadows, to maximize the overall performance. Also, we prepared an unpaired dataset for shadow removal and demonstrated the effectiveness of Mask-ShadowGAN on various experiments, even it was trained on unpaired data.",
        "中文标题": "Mask-ShadowGAN：从非配对数据中学习去除阴影",
        "摘要翻译": "本文提出了一种使用非配对数据进行阴影去除的新方法，使我们能够避免繁琐的注释并获得更多样化的训练样本。然而，直接采用对抗性学习和循环一致性约束不足以学习阴影和无阴影域之间的潜在关系，因为阴影和无阴影图像之间的映射不是简单的一对一。为了解决这个问题，我们提出了Mask-ShadowGAN，一个新的深度框架，它自动学习从输入的阴影图像中生成阴影掩码，然后通过重新制定的循环一致性约束来指导阴影生成。特别是，该框架同时学习生成阴影掩码和学习去除阴影，以最大化整体性能。此外，我们准备了一个用于阴影去除的非配对数据集，并在各种实验中证明了Mask-ShadowGAN的有效性，即使它是在非配对数据上训练的。",
        "领域": "图像修复/生成对抗网络/阴影去除",
        "问题": "如何从非配对数据中有效去除图像中的阴影",
        "动机": "避免繁琐的注释工作，获得更多样化的训练样本，以及解决阴影和无阴影图像之间复杂映射关系的问题",
        "方法": "提出Mask-ShadowGAN框架，通过自动生成阴影掩码并利用重新制定的循环一致性约束来指导阴影生成，同时学习生成阴影掩码和去除阴影",
        "关键词": [
            "阴影去除",
            "生成对抗网络",
            "非配对数据",
            "循环一致性约束",
            "阴影掩码"
        ],
        "涉及的技术概念": "对抗性学习、循环一致性约束、阴影掩码、非配对数据、图像生成"
    },
    {
        "order": 804,
        "title": "MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bertoni_MonoLoco_Monocular_3D_Pedestrian_Localization_and_Uncertainty_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bertoni_MonoLoco_Monocular_3D_Pedestrian_Localization_and_Uncertainty_Estimation_ICCV_2019_paper.html",
        "abstract": "We tackle the fundamentally ill-posed problem of 3D human localization from monocular RGB images. Driven by the limitation of neural networks outputting point estimates, we address the ambiguity in the task by predicting confidence intervals through a loss function based on the Laplace distribution. Our architecture is a light-weight feed-forward neural network that predicts 3D locations and corresponding confidence intervals given 2D human poses. The design is particularly well suited for small training data, cross-dataset generalization, and real-time applications. Our experiments show that we (i) outperform state-of-the-art results on KITTI and nuScenes datasets, (ii) even outperform a stereo-based method for far-away pedestrians, and (iii) estimate meaningful confidence intervals. We further share insights on our model of uncertainty in cases of limited observations and out-of-distribution samples.",
        "中文标题": "MonoLoco: 单目3D行人定位与不确定性估计",
        "摘要翻译": "我们解决了从单目RGB图像中进行3D人体定位这一本质上不适定问题。受限于神经网络输出点估计的局限性，我们通过基于拉普拉斯分布的损失函数预测置信区间，以解决任务中的模糊性问题。我们的架构是一个轻量级的前馈神经网络，它在给定2D人体姿态的情况下预测3D位置和相应的置信区间。该设计特别适合小规模训练数据、跨数据集泛化和实时应用。我们的实验表明，我们（i）在KITTI和nuScenes数据集上超越了最先进的结果，（ii）甚至对于远处的行人超越了基于立体视觉的方法，以及（iii）估计了有意义的置信区间。我们进一步分享了在有限观测和分布外样本情况下我们不确定性模型的见解。",
        "领域": "3D定位/不确定性估计/行人检测",
        "问题": "从单目RGB图像中进行3D人体定位",
        "动机": "解决神经网络输出点估计的局限性，通过预测置信区间来处理任务中的模糊性问题",
        "方法": "使用基于拉普拉斯分布的损失函数预测置信区间的轻量级前馈神经网络",
        "关键词": [
            "3D定位",
            "不确定性估计",
            "行人检测",
            "单目视觉",
            "置信区间"
        ],
        "涉及的技术概念": "单目RGB图像、3D人体定位、置信区间、拉普拉斯分布、前馈神经网络、2D人体姿态、KITTI数据集、nuScenes数据集、立体视觉方法、分布外样本"
    },
    {
        "order": 805,
        "title": "DeepGCNs: Can GCNs Go As Deep As CNNs?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.html",
        "abstract": "Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.",
        "中文标题": "DeepGCNs: GCNs能像CNNs一样深入吗？",
        "摘要翻译": "卷积神经网络（CNNs）在多个领域取得了令人印象深刻的性能。当能够可靠地训练非常深的CNN模型时，它们的成功得到了巨大的提升。尽管CNN有其优点，但它们无法正确处理非欧几里得数据的问题。为了克服这一挑战，图卷积网络（GCNs）构建图来表示非欧几里得数据，借用CNN的概念，并在训练中应用它们。GCNs显示出有希望的结果，但由于梯度消失问题，它们通常仅限于非常浅的模型。因此，大多数最先进的GCN模型的深度不超过3或4层。在这项工作中，我们提出了成功训练非常深的GCNs的新方法。我们通过借用CNN的概念，特别是残差/密集连接和扩张卷积，并将它们适应于GCN架构来实现这一点。大量实验显示了这些深度GCN框架的积极效果。最后，我们使用这些新概念构建了一个非常深的56层GCN，并展示了它如何显著提高点云语义分割任务的性能（比最先进的技术高出+3.7% mIoU）。我们相信，这项工作可以为社区带来巨大的好处，因为它为推进基于GCN的研究开辟了许多机会。",
        "领域": "图卷积网络/点云处理/语义分割",
        "问题": "如何训练非常深的图卷积网络（GCNs）以处理非欧几里得数据",
        "动机": "解决现有GCN模型由于梯度消失问题而仅限于浅层模型的问题，以及提高点云语义分割任务的性能",
        "方法": "借用CNN中的残差/密集连接和扩张卷积概念，并将其适应于GCN架构，以成功训练非常深的GCNs",
        "关键词": [
            "图卷积网络",
            "点云处理",
            "语义分割"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）用于处理欧几里得数据，图卷积网络（GCNs）用于处理非欧几里得数据，残差/密集连接和扩张卷积是CNN中的技术，用于解决梯度消失问题并提高网络深度。点云语义分割是计算机视觉中的一个任务，旨在为点云数据中的每个点分配语义标签。"
    },
    {
        "order": 806,
        "title": "Spatio-Temporal Filter Adaptive Network for Video Deblurring",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Spatio-Temporal_Filter_Adaptive_Network_for_Video_Deblurring_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Spatio-Temporal_Filter_Adaptive_Network_for_Video_Deblurring_ICCV_2019_paper.html",
        "abstract": "Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.",
        "中文标题": "时空滤波自适应网络用于视频去模糊",
        "摘要翻译": "视频去模糊是一项具有挑战性的任务，因为相机抖动、物体运动和深度变化等引起的空间变化模糊。现有方法通常估计模糊视频中的光流以对齐连续帧或近似模糊核。然而，当估计的光流不准确时，它们往往会产生伪影或无法有效去除模糊。为了克服单独光流估计的限制，我们提出了一个时空滤波自适应网络（STFAN），用于在统一框架中进行对齐和去模糊。提出的STFAN将前一帧的模糊和恢复图像以及当前帧的模糊图像作为输入，并动态生成空间自适应滤波器以进行对齐和去模糊。然后，我们提出了新的滤波自适应卷积（FAC）层，以将前一帧的去模糊特征与当前帧对齐，并从当前帧的特征中去除空间变化的模糊。最后，我们开发了一个重建网络，该网络采用两个变换特征的融合来恢复清晰的帧。在基准数据集和真实世界视频上的定量和定性评估结果表明，所提出的算法在准确性、速度以及模型大小方面均优于最先进的方法。",
        "领域": "视频处理/图像恢复/深度学习",
        "问题": "视频去模糊，特别是处理由相机抖动、物体运动和深度变化引起的空间变化模糊",
        "动机": "现有方法在光流估计不准确时会产生伪影或无法有效去除模糊，需要一种更有效的方法来处理视频去模糊问题",
        "方法": "提出了一个时空滤波自适应网络（STFAN），在统一框架中进行对齐和去模糊，包括动态生成空间自适应滤波器、使用滤波自适应卷积（FAC）层对齐和去模糊特征，以及开发重建网络恢复清晰帧",
        "关键词": [
            "视频去模糊",
            "时空滤波",
            "自适应网络",
            "光流估计",
            "图像恢复"
        ],
        "涉及的技术概念": "时空滤波自适应网络（STFAN）是一种深度学习模型，用于视频去模糊。它通过动态生成空间自适应滤波器来处理由相机抖动、物体运动和深度变化引起的空间变化模糊。滤波自适应卷积（FAC）层是该网络的一个关键组成部分，用于对齐和去模糊特征。重建网络则用于融合变换后的特征，以恢复清晰的视频帧。"
    },
    {
        "order": 807,
        "title": "Unsupervised High-Resolution Depth Learning From Videos With Dual Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Unsupervised_High-Resolution_Depth_Learning_From_Videos_With_Dual_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Unsupervised_High-Resolution_Depth_Learning_From_Videos_With_Dual_Networks_ICCV_2019_paper.html",
        "abstract": "Unsupervised depth learning takes the appearance difference between a target view and a view synthesized from its adjacent frame as supervisory signal. Since the supervisory signal only comes from images themselves, the resolution of training data significantly impacts the performance. High-resolution images contain more fine-grained details and provide more accurate supervisory signal. However, due to the limitation of memory and computation power, the original images are typically down-sampled during training, which suffers heavy loss of details and disparity accuracy. In order to fully explore the information contained in high-resolution data, we propose a simple yet effective dual networks architecture, which can directly take high-resolution images as input and generate high-resolution and high-accuracy depth map efficiently. We also propose a Self-assembled Attention (SA-Attention) module to handle low-texture region. The evaluation on the benchmark KITTI and Make3D datasets demonstrates that our method achieves state-of-the-art results in the monocular depth estimation task.",
        "中文标题": "从视频中通过双网络进行无监督高分辨率深度学习",
        "摘要翻译": "无监督深度学习将目标视图与从其相邻帧合成的视图之间的外观差异作为监督信号。由于监督信号仅来自图像本身，训练数据的分辨率显著影响性能。高分辨率图像包含更多细粒度的细节，并提供更准确的监督信号。然而，由于内存和计算能力的限制，原始图像在训练过程中通常会被下采样，这会导致细节和视差精度的严重损失。为了充分利用高分辨率数据中包含的信息，我们提出了一种简单而有效的双网络架构，可以直接将高分辨率图像作为输入，并高效地生成高分辨率和高精度的深度图。我们还提出了一种自组装注意力（SA-Attention）模块来处理低纹理区域。在KITTI和Make3D基准数据集上的评估表明，我们的方法在单目深度估计任务中达到了最先进的结果。",
        "领域": "深度估计/视频分析/注意力机制",
        "问题": "如何在内存和计算能力有限的情况下，利用高分辨率图像进行无监督深度学习，以提高深度估计的精度和细节保留。",
        "动机": "高分辨率图像包含更多细节，能提供更准确的监督信号，但由于硬件限制，通常需要下采样处理，导致细节和精度损失。",
        "方法": "提出了一种双网络架构，可以直接处理高分辨率图像，并引入自组装注意力模块以改善低纹理区域的深度估计。",
        "关键词": [
            "无监督学习",
            "高分辨率",
            "深度估计",
            "双网络架构",
            "自组装注意力"
        ],
        "涉及的技术概念": {
            "无监督深度学习": "一种不依赖于标注数据，通过图像自身信息进行学习的方法。",
            "双网络架构": "一种网络设计，能够处理高分辨率输入并生成高精度输出。",
            "自组装注意力（SA-Attention）": "一种注意力机制，用于改善网络在低纹理区域的性能。",
            "单目深度估计": "从单一图像中估计场景深度信息的技术。"
        }
    },
    {
        "order": 808,
        "title": "Learning Deep Priors for Image Dehazing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Learning_Deep_Priors_for_Image_Dehazing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Learning_Deep_Priors_for_Image_Dehazing_ICCV_2019_paper.html",
        "abstract": "Image dehazing is a well-known ill-posed problem, which usually requires some image priors to make the problem well-posed. We propose an effective iteration algorithm with deep CNNs to learn haze-relevant priors for image dehazing. We formulate the image dehazing problem as the minimization of a variational model with favorable data fidelity terms and prior terms to regularize the model. We solve the variational model based on the classical gradient descent method with built-in deep CNNs so that iteration-wise image priors for the atmospheric light, transmission map and clear image can be well estimated. Our method combines the properties of both the physical formation of image dehazing as well as deep learning approaches. We show that it is able to generate clear images as well as accurate atmospheric light and transmission maps. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in both benchmark datasets and real-world images.",
        "中文标题": "学习用于图像去雾的深度先验",
        "摘要翻译": "图像去雾是一个众所周知的不适定问题，通常需要一些图像先验来使问题适定。我们提出了一种有效的迭代算法，利用深度卷积神经网络来学习与雾相关的先验，用于图像去雾。我们将图像去雾问题表述为具有良好数据保真项和先验项的变分模型的最小化，以正则化模型。我们基于经典的梯度下降方法，结合内置的深度卷积神经网络来解决这个变分模型，从而能够很好地估计大气光、透射图和清晰图像的迭代先验。我们的方法结合了图像去雾的物理形成特性和深度学习方法。我们展示了它能够生成清晰的图像以及准确的大气光和透射图。大量的实验结果表明，所提出的算法在基准数据集和真实世界图像中均优于最先进的方法。",
        "领域": "图像去雾/深度学习/变分模型",
        "问题": "图像去雾问题的不适定性",
        "动机": "为了解决图像去雾问题的不适定性，需要有效的先验知识来使问题适定",
        "方法": "提出了一种结合深度卷积神经网络和变分模型的迭代算法，用于学习与雾相关的先验",
        "关键词": [
            "图像去雾",
            "深度卷积神经网络",
            "变分模型",
            "大气光",
            "透射图"
        ],
        "涉及的技术概念": "深度卷积神经网络（Deep CNNs）用于学习图像去雾的先验知识；变分模型（Variational Model）用于正则化图像去雾问题；梯度下降方法（Gradient Descent Method）用于解决变分模型；大气光（Atmospheric Light）和透射图（Transmission Map）是图像去雾中的关键参数。"
    },
    {
        "order": 809,
        "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qi_Deep_Hough_Voting_for_3D_Object_Detection_in_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qi_Deep_Hough_Voting_for_3D_Object_Detection_in_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",
        "中文标题": "深度霍夫投票用于点云中的3D物体检测",
        "摘要翻译": "当前的3D物体检测方法深受2D检测器的影响。为了利用2D检测器中的架构，它们通常将3D点云转换为规则网格（即体素网格或鸟瞰图图像），或依赖于2D图像中的检测来提出3D框。很少有工作尝试直接在点云中检测物体。在这项工作中，我们回归基本原理，为点云数据构建一个尽可能通用的3D检测管道。然而，由于数据的稀疏性——3D空间中2D流形的样本——我们在直接从场景点预测边界框参数时面临一个主要挑战：3D物体的质心可能远离任何表面点，因此难以一步准确回归。为了解决这一挑战，我们提出了VoteNet，一个基于深度点集网络和霍夫投票协同作用的端到端3D物体检测网络。我们的模型在两个大型真实3D扫描数据集ScanNet和SUN RGB-D上实现了最先进的3D检测，具有简单的设计、紧凑的模型尺寸和高效率。值得注意的是，VoteNet通过仅使用几何信息而不依赖彩色图像，超越了以前的方法。",
        "领域": "3D物体检测/点云处理/深度学习",
        "问题": "直接在点云中进行3D物体检测",
        "动机": "现有方法多受2D检测器影响，缺乏直接在点云中检测物体的有效方法",
        "方法": "提出VoteNet，一个基于深度点集网络和霍夫投票协同作用的端到端3D物体检测网络",
        "关键词": [
            "3D物体检测",
            "点云处理",
            "霍夫投票",
            "深度点集网络"
        ],
        "涉及的技术概念": "3D物体检测指的是在三维空间中识别和定位物体的过程。点云处理涉及对由大量点组成的三维数据进行操作和分析。霍夫投票是一种用于检测图像中特定形状的技术，通过投票机制确定最可能的形状参数。深度点集网络是一种处理点云数据的深度学习架构，能够直接从点云中学习特征。"
    },
    {
        "order": 810,
        "title": "Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html",
        "abstract": "We propose a framework for recognizing human actions from skeleton data by modeling the underlying dynamic process that generates the motion pattern. We capture three major factors that contribute to the complexity of the motion pattern including spatial dependencies among body joints, temporal dependencies of body poses, and variation among subjects in action execution. We utilize graph convolution to extract structure-aware feature representation from pose data by exploiting the skeleton anatomy. Long short-term memory (LSTM) network is then used to capture the temporal dynamics of the data. Finally, the whole model is extended under the Bayesian framework to a probabilistic model in order to better capture the stochasticity and variation in the data. An adversarial prior is developed to regularize the model parameters to improve the generalization of the model. A Bayesian inference problem is formulated to solve the classification task. We demonstrate the benefit of this framework in several benchmark datasets with recognition under various generalization conditions.",
        "中文标题": "基于贝叶斯图卷积LSTM的骨架动作识别",
        "摘要翻译": "我们提出了一个框架，通过建模生成运动模式的基础动态过程来从骨架数据中识别人类动作。我们捕捉了导致运动模式复杂性的三个主要因素，包括身体关节之间的空间依赖性、身体姿势的时间依赖性以及动作执行中受试者之间的变化。我们利用图卷积通过利用骨架解剖学从姿势数据中提取结构感知的特征表示。然后使用长短期记忆（LSTM）网络捕捉数据的时间动态。最后，整个模型在贝叶斯框架下扩展为概率模型，以更好地捕捉数据中的随机性和变化。开发了一个对抗性先验来正则化模型参数，以提高模型的泛化能力。制定了一个贝叶斯推理问题来解决分类任务。我们在几个基准数据集上展示了该框架在各种泛化条件下识别的优势。",
        "领域": "动作识别/骨架分析/贝叶斯学习",
        "问题": "从骨架数据中识别人类动作",
        "动机": "捕捉运动模式的复杂性，包括空间依赖性、时间依赖性和受试者之间的变化",
        "方法": "利用图卷积提取结构感知特征，使用LSTM捕捉时间动态，扩展为贝叶斯概率模型，开发对抗性先验正则化模型参数",
        "关键词": [
            "骨架动作识别",
            "图卷积",
            "LSTM",
            "贝叶斯学习",
            "对抗性先验"
        ],
        "涉及的技术概念": "图卷积用于从骨架数据中提取特征，LSTM网络用于捕捉时间序列数据中的动态，贝叶斯框架用于建模数据中的随机性和变化，对抗性先验用于正则化模型参数以提高泛化能力。"
    },
    {
        "order": 811,
        "title": "JPEG Artifacts Reduction via Deep Convolutional Sparse Coding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Fu_JPEG_Artifacts_Reduction_via_Deep_Convolutional_Sparse_Coding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Fu_JPEG_Artifacts_Reduction_via_Deep_Convolutional_Sparse_Coding_ICCV_2019_paper.html",
        "abstract": "To effectively reduce JPEG compression artifacts, we propose a deep convolutional sparse coding (DCSC) network architecture. We design our DCSC in the framework of classic learned iterative shrinkage-threshold algorithm. To focus on recognizing and separating artifacts only, we sparsely code the feature maps instead of the raw image. The final de-blocked image is directly reconstructed from the coded features. We use dilated convolution to extract multi-scale image features, which allows our single model to simultaneously handle multiple JPEG compression levels. Since our method integrates model-based convolutional sparse coding with a learning-based deep neural network, the entire network structure is compact and more explainable. The resulting lightweight model generates comparable or better de-blocking results when compared with state-of-the-art methods.",
        "中文标题": "通过深度卷积稀疏编码减少JPEG伪影",
        "摘要翻译": "为了有效减少JPEG压缩伪影，我们提出了一种深度卷积稀疏编码（DCSC）网络架构。我们在经典的学习迭代收缩阈值算法框架内设计了我们的DCSC。为了专注于识别和分离伪影，我们对特征图而不是原始图像进行稀疏编码。最终的去块图像直接从编码特征重建。我们使用扩张卷积来提取多尺度图像特征，这使得我们的单一模型能够同时处理多个JPEG压缩级别。由于我们的方法将基于模型的卷积稀疏编码与基于学习的深度神经网络相结合，整个网络结构紧凑且更易于解释。与最先进的方法相比，生成的轻量级模型产生了可比或更好的去块效果。",
        "领域": "图像压缩/图像恢复/卷积神经网络",
        "问题": "减少JPEG压缩伪影",
        "动机": "JPEG压缩在图像存储和传输中广泛使用，但会引入伪影，影响图像质量。",
        "方法": "提出了一种深度卷积稀疏编码（DCSC）网络架构，结合了基于模型的卷积稀疏编码和基于学习的深度神经网络，使用扩张卷积提取多尺度图像特征。",
        "关键词": [
            "JPEG伪影",
            "深度卷积稀疏编码",
            "扩张卷积",
            "图像去块"
        ],
        "涉及的技术概念": "深度卷积稀疏编码（DCSC）是一种结合了稀疏编码和深度学习的图像处理技术，旨在通过稀疏表示来减少图像中的伪影。扩张卷积是一种卷积神经网络中的技术，用于在不增加参数数量的情况下增加感受野，从而捕捉到更广泛的上下文信息。"
    },
    {
        "order": 812,
        "title": "DeCaFA: Deep Convolutional Cascade for Face Alignment in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dapogny_DeCaFA_Deep_Convolutional_Cascade_for_Face_Alignment_in_the_Wild_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dapogny_DeCaFA_Deep_Convolutional_Cascade_for_Face_Alignment_in_the_Wild_ICCV_2019_paper.html",
        "abstract": "Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce an end-to-end deep convolutional cascade (DeCaFA) architecture for face alignment. Face Alignment is an active computer vision domain, that consists in localizing a number of facial landmarks that vary across datasets. State-of-the-art face alignment methods either consist in end-to-end regression, or in refining the shape in a cascaded manner, starting from an initial guess. In this paper, we introduce DeCaFA, an end-to-end deep convolutional cascade architecture for face alignment. DeCaFA uses fully-convolutional stages to keep full spatial resolution throughout the cascade. Between each cascade stage, DeCaFA uses multiple chained transfer layers with spatial softmax to produce landmark-wise attention maps for each of several landmark alignment tasks. Weighted intermediate supervision, as well as efficient feature fusion between the stages allow to learn to progressively refine the attention maps in an end-to-end manner. We show experimentally that DeCaFA significantly outperforms existing approaches on 300W, CelebA and WFLW databases. In addition, we show that DeCaFA can learn fine alignment with reasonable accuracy from very few images using coarsely annotated data.",
        "中文标题": "DeCaFA: 用于野外人脸对齐的深度卷积级联",
        "摘要翻译": "人脸对齐是一个活跃的计算机视觉领域，其目的是定位数据集中变化的面部标志点。最先进的人脸对齐方法要么是端到端的回归，要么是从初始猜测开始，以级联方式细化形状。在本文中，我们介绍了一种用于人脸对齐的端到端深度卷积级联（DeCaFA）架构。DeCaFA使用全卷积阶段在整个级联过程中保持全空间分辨率。在每个级联阶段之间，DeCaFA使用多个链式传输层与空间softmax相结合，为多个标志点对齐任务中的每一个生成标志点注意图。加权中间监督以及阶段之间的有效特征融合允许以端到端的方式学习逐步细化注意图。我们通过实验证明，DeCaFA在300W、CelebA和WFLW数据库上显著优于现有方法。此外，我们还展示了DeCaFA可以从粗标注的数据中学习到精细对齐，且仅需很少的图像即可达到合理的准确度。",
        "领域": "人脸对齐/深度学习/卷积神经网络",
        "问题": "如何在野外环境下准确地进行人脸对齐",
        "动机": "提高人脸对齐的准确性和效率，特别是在复杂或变化的环境中",
        "方法": "提出了一种端到端的深度卷积级联架构（DeCaFA），通过全卷积阶段保持空间分辨率，使用链式传输层和空间softmax生成注意图，并通过加权中间监督和特征融合逐步细化注意图",
        "关键词": [
            "人脸对齐",
            "深度卷积级联",
            "注意图"
        ],
        "涉及的技术概念": {
            "端到端回归": "一种直接从输入到输出进行预测的方法，无需手动设计中间步骤",
            "级联方式": "一种逐步细化预测结果的方法，每个阶段都基于前一个阶段的结果进行改进",
            "全卷积阶段": "使用卷积层处理输入数据，保持输入的空间维度",
            "空间softmax": "一种在空间维度上应用softmax函数的方法，用于生成注意图",
            "加权中间监督": "在训练过程中对中间层的输出进行监督，以帮助模型更好地学习",
            "特征融合": "将不同阶段的特征结合起来，以提高模型的性能"
        }
    },
    {
        "order": 813,
        "title": "M3D-RPN: Monocular 3D Region Proposal Network for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brazil_M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brazil_M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Understanding the world in 3D is a critical component of urban autonomous driving. Generally, the combination of expensive LiDAR sensors and stereo RGB imaging has been paramount for successful 3D object detection algorithms, whereas monocular image-only methods experience drastically reduced performance. We propose to reduce the gap by reformulating the monocular 3D detection problem as a standalone 3D region proposal network. We leverage the geometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize well-known and powerful convolutional features generated in the image-space. To help address the strenuous 3D parameter estimations, we further design depth-aware convolutional layers which enable location specific feature development and in consequence improved 3D scene understanding. Compared to prior work in monocular 3D detection, our method consists of only the proposed 3D region proposal network rather than relying on external networks, data, or multiple stages. M3D-RPN is able to significantly improve the performance of both monocular 3D Object Detection and Bird's Eye View tasks within the KITTI urban autonomous driving dataset, while efficiently using a shared multi-class model.",
        "中文标题": "M3D-RPN: 用于物体检测的单目3D区域提议网络",
        "摘要翻译": "理解三维世界是城市自动驾驶的关键组成部分。通常，昂贵的LiDAR传感器和立体RGB成像的结合对于成功的3D物体检测算法至关重要，而仅使用单目图像的方法性能大幅下降。我们提出通过将单目3D检测问题重新表述为独立的3D区域提议网络来缩小这一差距。我们利用2D和3D视角的几何关系，使3D框能够利用图像空间中生成的知名且强大的卷积特征。为了帮助解决艰巨的3D参数估计问题，我们进一步设计了深度感知卷积层，这些层能够实现特定位置的特征发展，从而改善3D场景理解。与之前的工作相比，我们的方法仅包含提出的3D区域提议网络，而不依赖于外部网络、数据或多个阶段。M3D-RPN能够在KITTI城市自动驾驶数据集中显著提高单目3D物体检测和鸟瞰图任务的性能，同时高效地使用共享的多类模型。",
        "领域": "自动驾驶/3D物体检测/卷积神经网络",
        "问题": "单目图像在3D物体检测中的性能大幅下降",
        "动机": "缩小单目3D检测与使用LiDAR和立体RGB成像的3D检测之间的性能差距",
        "方法": "提出一个独立的3D区域提议网络，利用2D和3D视角的几何关系，并设计深度感知卷积层以改善3D场景理解",
        "关键词": [
            "3D物体检测",
            "单目图像",
            "深度感知卷积层"
        ],
        "涉及的技术概念": "3D区域提议网络是一种用于3D物体检测的深度学习模型，它通过分析2D图像中的特征来预测3D空间中的物体位置和大小。深度感知卷积层是一种特殊的卷积层，它能够根据图像中物体的深度信息调整其特征提取过程，从而提高3D场景理解的准确性。"
    },
    {
        "order": 814,
        "title": "Self-Guided Network for Fast Image Denoising",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Self-Guided_Network_for_Fast_Image_Denoising_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Self-Guided_Network_for_Fast_Image_Denoising_ICCV_2019_paper.html",
        "abstract": "During the past years, tremendous advances in image restoration tasks have been achieved using highly complex neural networks. Despite their good restoration performance, the heavy computational burden hinders the deployment of these networks on constrained devices, e.g. smart phones and consumer electronic products. To tackle this problem, we propose a self-guided network (SGN), which adopts a top-down self-guidance architecture to better exploit image multi-scale information. SGN directly generates multi-resolution inputs with the shuffling operation. Large-scale contextual information extracted at low resolution is gradually propagated into the higher resolution sub-networks to guide the feature extraction processes at these scales. Such a self-guidance strategy enables SGN to efficiently incorporate multi-scale information and extract good local features to recover noisy images. We validate the effectiveness of SGN through extensive experiments. The experimental results demonstrate that SGN greatly improves the memory and runtime efficiency over state-of-the-art efficient methods, without trading off PSNR accuracy.",
        "中文标题": "自引导网络用于快速图像去噪",
        "摘要翻译": "过去几年中，使用高度复杂的神经网络在图像恢复任务中取得了巨大进展。尽管这些网络具有良好的恢复性能，但沉重的计算负担阻碍了它们在受限设备上的部署，例如智能手机和消费电子产品。为了解决这个问题，我们提出了一种自引导网络（SGN），它采用自上而下的自引导架构，以更好地利用图像的多尺度信息。SGN通过洗牌操作直接生成多分辨率输入。在低分辨率下提取的大规模上下文信息逐渐传播到更高分辨率的子网络中，以指导这些尺度上的特征提取过程。这种自引导策略使SGN能够有效地整合多尺度信息并提取良好的局部特征以恢复噪声图像。我们通过大量实验验证了SGN的有效性。实验结果表明，SGN在不牺牲PSNR准确性的情况下，大大提高了内存和运行时的效率。",
        "领域": "图像去噪/神经网络优化/计算效率",
        "问题": "解决复杂神经网络在图像去噪任务中计算负担重，难以在受限设备上部署的问题",
        "动机": "提高图像去噪网络的计算效率，使其能够在智能手机等受限设备上有效运行",
        "方法": "提出一种自引导网络（SGN），采用自上而下的自引导架构，通过洗牌操作生成多分辨率输入，并利用低分辨率提取的上下文信息指导高分辨率子网络的特征提取",
        "关键词": [
            "图像去噪",
            "自引导网络",
            "计算效率",
            "多尺度信息",
            "神经网络优化"
        ],
        "涉及的技术概念": {
            "自引导网络（SGN）": "一种采用自上而下自引导架构的神经网络，旨在通过多尺度信息整合提高图像去噪效率",
            "洗牌操作": "一种生成多分辨率输入的技术，用于自引导网络中",
            "多尺度信息": "指图像在不同分辨率下的信息，自引导网络通过整合这些信息来提高去噪效果",
            "PSNR": "峰值信噪比，用于衡量图像恢复质量的一个指标"
        }
    },
    {
        "order": 815,
        "title": "Probabilistic Face Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Probabilistic_Face_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shi_Probabilistic_Face_Embeddings_ICCV_2019_paper.html",
        "abstract": "Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.",
        "中文标题": "概率面部嵌入",
        "摘要翻译": "嵌入方法通过比较潜在语义空间中的面部特征，在面部识别中取得了成功。然而，在一个完全不受约束的面部设置中，嵌入模型学习到的面部特征可能是模糊的，甚至可能不在输入的面部中出现，导致表示带有噪声。我们提出了概率面部嵌入（PFEs），它将每个面部图像表示为潜在空间中的高斯分布。分布的均值估计最可能的特征值，而方差显示特征值的不确定性。然后，可以自然地利用不确定性信息为匹配和融合PFEs推导出概率解决方案。在不同基线模型、训练数据集和基准上的实证评估表明，通过将它们转换为PFEs，所提出的方法可以提高确定性嵌入的面部识别性能。PFEs估计的不确定性也作为潜在匹配准确性的良好指标，这对于风险控制的识别系统非常重要。",
        "领域": "面部识别/不确定性估计/特征嵌入",
        "问题": "在完全不受约束的面部设置中，嵌入模型学习到的面部特征可能是模糊的，甚至可能不在输入的面部中出现，导致表示带有噪声。",
        "动机": "提高面部识别性能，特别是在完全不受约束的面部设置中，通过引入不确定性信息来改进特征表示。",
        "方法": "提出概率面部嵌入（PFEs），将每个面部图像表示为潜在空间中的高斯分布，利用分布的均值和方差来估计特征值和其不确定性，进而推导出匹配和融合PFEs的概率解决方案。",
        "关键词": [
            "面部识别",
            "不确定性估计",
            "特征嵌入"
        ],
        "涉及的技术概念": "概率面部嵌入（PFEs）是一种将面部图像表示为潜在空间中高斯分布的方法，其中分布的均值用于估计最可能的特征值，方差用于表示特征值的不确定性。这种方法允许在面部识别中引入不确定性信息，从而提高识别性能，并为风险控制的识别系统提供潜在匹配准确性的指标。"
    },
    {
        "order": 816,
        "title": "SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Behley_SemanticKITTI_A_Dataset_for_Semantic_Scene_Understanding_of_LiDAR_Sequences_ICCV_2019_paper.html",
        "abstract": "Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete 360-degree field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.",
        "中文标题": "SemanticKITTI：用于LiDAR序列语义场景理解的数据集",
        "摘要翻译": "语义场景理解对于各种应用非常重要。特别是，自动驾驶汽车需要对其周围环境和物体有细致的理解。光检测和测距（LiDAR）提供了关于环境的精确几何信息，因此是几乎所有自动驾驶汽车传感器套件的一部分。尽管语义场景理解对于这一应用具有重要意义，但缺乏基于汽车LiDAR的大型数据集。在本文中，我们引入了一个大型数据集，以推动基于激光的语义分割研究。我们注释了KITTI视觉里程计基准的所有序列，并为所使用的汽车LiDAR的完整360度视野提供了密集的点级注释。我们基于此数据集提出了三个基准任务：（i）使用单次扫描的点云语义分割，（ii）使用多次过去扫描的语义分割，以及（iii）语义场景完成，这需要预测未来的语义场景。我们提供了基线实验，并表明需要更复杂的模型来有效解决这些任务。我们的数据集为开发更先进的方法打开了大门，同时也提供了丰富的数据来探索新的研究方向。",
        "领域": "自动驾驶/激光雷达/语义分割",
        "问题": "缺乏基于汽车LiDAR的大型数据集用于语义场景理解",
        "动机": "推动基于激光的语义分割研究，为自动驾驶汽车提供细致的环境理解",
        "方法": "引入一个大型数据集，注释KITTI视觉里程计基准的所有序列，提供密集的点级注释，并提出三个基准任务",
        "关键词": [
            "自动驾驶",
            "激光雷达",
            "语义分割",
            "语义场景理解",
            "点云"
        ],
        "涉及的技术概念": "LiDAR（光检测和测距）技术用于提供环境的精确几何信息，语义分割是指对图像或点云中的每个点或像素进行分类，以识别其所属的物体或表面类型。"
    },
    {
        "order": 817,
        "title": "Non-Local Intrinsic Decomposition With Near-Infrared Priors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Non-Local_Intrinsic_Decomposition_With_Near-Infrared_Priors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Non-Local_Intrinsic_Decomposition_With_Near-Infrared_Priors_ICCV_2019_paper.html",
        "abstract": "Intrinsic image decomposition is a highly under-constrained problem that has been extensively studied by computer vision researchers. Previous methods impose additional constraints by exploiting either empirical or data-driven priors. In this paper, we revisit intrinsic image decomposition with the aid of near-infrared (NIR) imagery. We show that NIR band is considerably less sensitive to textures and can be exploited to reduce ambiguity caused by reflectance variation, promoting a simple yet powerful prior for shading smoothness. With this observation, we formulate intrinsic decomposition as an energy minimisation problem. Unlike existing methods, our energy formulation decouples reflectance and shading estimation, into a convex local shading component based on NIR-RGB image pair, and a reflectance component that encourages reflectance homogeneity both locally and globally. We further show the minimisation process can be approached by a series of multi-dimensional kernel convolutions, each within linear time complexity. To validate the proposed algorithm, a NIR-RGB dataset is captured over real-world objects, where our NIR-assisted approach demonstrates clear superiority over RGB methods.",
        "中文标题": "基于近红外先验的非局部本征分解",
        "摘要翻译": "本征图像分解是一个高度欠约束的问题，已被计算机视觉研究者广泛研究。先前的方法通过利用经验或数据驱动的先验来施加额外的约束。在本文中，我们借助近红外（NIR）图像重新审视本征图像分解。我们展示了NIR波段对纹理的敏感性显著较低，并且可以利用它来减少由反射率变化引起的歧义，从而促进一个简单而强大的阴影平滑先验。基于这一观察，我们将本征分解表述为一个能量最小化问题。与现有方法不同，我们的能量公式将反射率和阴影估计解耦，分为基于NIR-RGB图像对的凸局部阴影组件，以及鼓励局部和全局反射率均匀性的反射率组件。我们进一步展示了最小化过程可以通过一系列多维核卷积来接近，每个卷积都在线性时间复杂度内。为了验证所提出的算法，我们在真实世界的物体上捕获了一个NIR-RGB数据集，其中我们的NIR辅助方法展示了相对于RGB方法的明显优势。",
        "领域": "本征图像分解/近红外成像/能量最小化",
        "问题": "解决本征图像分解中的高度欠约束问题",
        "动机": "利用近红外图像减少由反射率变化引起的歧义，促进阴影平滑先验",
        "方法": "将本征分解表述为能量最小化问题，解耦反射率和阴影估计，采用多维核卷积进行最小化",
        "关键词": [
            "本征图像分解",
            "近红外成像",
            "能量最小化",
            "阴影平滑",
            "反射率均匀性"
        ],
        "涉及的技术概念": "本征图像分解是指将图像分解为反射率和阴影两个本征成分的过程。近红外（NIR）成像利用近红外波段对纹理的敏感性较低的特性，减少反射率变化引起的歧义。能量最小化是一种优化方法，通过最小化能量函数来求解问题。多维核卷积是一种在多个维度上进行的卷积操作，用于图像处理中的滤波和特征提取。"
    },
    {
        "order": 818,
        "title": "Gaze360: Physically Unconstrained Gaze Estimation in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kellnhofer_Gaze360_Physically_Unconstrained_Gaze_Estimation_in_the_Wild_ICCV_2019_paper.html",
        "abstract": "Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu.",
        "中文标题": "Gaze360：野外物理无约束的视线估计",
        "摘要翻译": "理解人们正在看哪里是一个信息丰富的社交线索。在这项工作中，我们提出了Gaze360，一个大规模远程视线跟踪数据集和方法，用于在无约束图像中进行鲁棒的3D视线估计。我们的数据集包括238名在室内和室外环境中的受试者，具有广泛头部姿势和距离的标记3D视线。它是同类中最大且种类最多的公开可用数据集，这得益于一个简单而高效的收集方法。我们提出的3D视线模型扩展了现有模型，以包括时间信息并直接输出视线不确定性的估计。我们通过消融研究展示了我们模型的优势，并通过与其他最近的视线基准数据集的跨数据集评估展示了其泛化性能。此外，我们提出了一种简单的自监督方法，以改进跨数据集领域适应。最后，我们展示了我们的模型在超市环境中估计顾客注意力的应用。我们的数据集和模型将在http://gaze360.csail.mit.edu上提供。",
        "领域": "视线估计/3D视觉/社交信号处理",
        "问题": "在无约束环境中进行鲁棒的3D视线估计",
        "动机": "理解人们的视线方向对于社交互动和商业应用（如顾客注意力分析）非常重要",
        "方法": "提出了一个大规模远程视线跟踪数据集Gaze360，并开发了一个扩展现有模型以包括时间信息和直接输出视线不确定性估计的3D视线模型，以及一种改进跨数据集领域适应的自监督方法",
        "关键词": [
            "视线估计",
            "3D视觉",
            "社交信号处理",
            "数据集",
            "自监督学习"
        ],
        "涉及的技术概念": "3D视线估计模型、时间信息、视线不确定性估计、消融研究、跨数据集评估、自监督学习、领域适应"
    },
    {
        "order": 819,
        "title": "WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yogamani_WoodScape_A_Multi-Task_Multi-Camera_Fisheye_Dataset_for_Autonomous_Driving_ICCV_2019_paper.html",
        "abstract": "Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.",
        "中文标题": "WoodScape: 用于自动驾驶的多任务、多摄像头鱼眼数据集",
        "摘要翻译": "鱼眼摄像头通常用于在监控、增强现实特别是汽车应用中获取大视野。尽管它们很普遍，但很少有公开的数据集用于详细评估鱼眼图像上的计算机视觉算法。我们发布了首个广泛的鱼眼汽车数据集WoodScape，以1906年发明鱼眼摄像头的Robert Wood命名。WoodScape包括四个环绕视图摄像头和九个任务，包括分割、深度估计、3D边界框检测和污染检测。提供了超过10,000张图像的40个类别的实例级语义注释，以及超过100,000张图像的其他任务注释。通过WoodScape，我们希望鼓励社区适应鱼眼摄像头的计算机视觉模型，而不是使用简单的校正。",
        "领域": "自动驾驶/鱼眼图像处理/计算机视觉",
        "问题": "缺乏用于详细评估鱼眼图像上计算机视觉算法的公开数据集",
        "动机": "鼓励社区适应鱼眼摄像头的计算机视觉模型，而不是使用简单的校正",
        "方法": "发布首个广泛的鱼眼汽车数据集WoodScape，包括四个环绕视图摄像头和九个任务，提供大量图像的详细注释",
        "关键词": [
            "鱼眼摄像头",
            "自动驾驶",
            "数据集",
            "计算机视觉模型",
            "语义注释"
        ],
        "涉及的技术概念": {
            "鱼眼摄像头": "一种能够提供大视野的摄像头，常用于监控、增强现实和汽车应用",
            "自动驾驶": "利用计算机视觉和其他技术实现车辆自主导航的技术",
            "数据集": "用于训练和评估计算机视觉算法的图像集合",
            "计算机视觉模型": "用于处理和分析视觉信息的算法和模型",
            "语义注释": "对图像中的对象进行类别标记，以便于计算机视觉算法理解和处理"
        }
    },
    {
        "order": 820,
        "title": "VideoMem: Constructing, Analyzing, Predicting Short-Term and Long-Term Video Memorability",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cohendet_VideoMem_Constructing_Analyzing_Predicting_Short-Term_and_Long-Term_Video_Memorability_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cohendet_VideoMem_Constructing_Analyzing_Predicting_Short-Term_and_Long-Term_Video_Memorability_ICCV_2019_paper.html",
        "abstract": "Humans share a strong tendency to memorize/forget some of the visual information they encounter. This paper focuses on understanding the intrinsic memorability of visual content. To address this challenge, we introduce a large scale dataset (VideoMem) composed of 10,000 videos with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes and again 24-72 hours after memorization. Hence, the dataset comes with short-term and long-term memorability annotations. After an in-depth analysis of the dataset, we investigate various deep neural network-based models for the prediction of video memorability. Our best model using a ranking loss achieves a Spearman's rank correlation of 0.494 (respectively 0.256) for short-term (resp. long-term) memorability prediction, while our model with attention mechanism provides insights of what makes a content memorable. The VideoMem dataset with pre-extracted features is publicly available.",
        "中文标题": "VideoMem：构建、分析、预测短期和长期视频记忆性",
        "摘要翻译": "人类有一种强烈的倾向，即记住/忘记他们遇到的一些视觉信息。本文专注于理解视觉内容的内在记忆性。为了应对这一挑战，我们引入了一个大规模数据集（VideoMem），该数据集由10,000个带有记忆性评分的视频组成。与之前关于图像记忆性的工作相比——在记忆后几分钟测量记忆性——记忆性能被测量两次：记忆后几分钟和24-72小时。因此，该数据集带有短期和长期记忆性注释。在对数据集进行深入分析后，我们研究了各种基于深度神经网络的模型用于预测视频记忆性。我们使用排名损失的最佳模型在短期（分别为长期）记忆性预测中达到了0.494（分别为0.256）的斯皮尔曼等级相关性，而我们的带有注意力机制的模型提供了使内容难忘的见解。带有预提取特征的VideoMem数据集已公开可用。",
        "领域": "视频记忆性分析/深度学习应用/注意力机制",
        "问题": "如何预测视频的短期和长期记忆性",
        "动机": "理解视觉内容的内在记忆性，探索影响视频记忆性的因素",
        "方法": "引入大规模数据集VideoMem，使用基于深度神经网络的模型进行记忆性预测，特别是采用排名损失和注意力机制的模型",
        "关键词": [
            "视频记忆性",
            "深度学习",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "VideoMem数据集": "一个包含10,000个视频的大规模数据集，每个视频都有短期和长期记忆性评分。",
            "深度神经网络": "用于预测视频记忆性的模型，包括使用排名损失和注意力机制的模型。",
            "注意力机制": "一种技术，用于提供关于什么使内容难忘的见解。",
            "斯皮尔曼等级相关性": "用于衡量预测模型性能的统计方法，特别是在记忆性预测中的应用。"
        }
    },
    {
        "order": 821,
        "title": "Unsupervised Person Re-Identification by Camera-Aware Similarity Consistency Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Unsupervised_Person_Re-Identification_by_Camera-Aware_Similarity_Consistency_Learning_ICCV_2019_paper.html",
        "abstract": "For matching pedestrians across disjoint camera views in surveillance, person re-identification (Re-ID) has made great progress in supervised learning. However, it is infeasible to label data in a number of new scenes when extending a Re-ID system. Thus, studying unsupervised learning for Re-ID is important for saving labelling cost. Yet, cross-camera scene variation is a key challenge for unsupervised Re-ID, such as illumination, background and viewpoint variations, which cause domain shift in the feature space and result in inconsistent pairwise similarity distributions that degrade matching performance. To alleviate the effect of cross-camera scene variation, we propose a Camera-Aware Similarity Consistency Loss to learn consistent pairwise similarity distributions for intra-camera matching and cross-camera matching. To avoid learning ineffective knowledge in consistency learning, we preserve the prior common knowledge of intra-camera matching in the pretrained model as reliable guiding information, which does not suffer from cross-camera scene variation as cross-camera matching. To learn similarity consistency more effectively, we further develop a coarse-to-fine consistency learning scheme to learn consistency globally and locally in two steps. Experiments show that our method outperformed the state-of-the-art unsupervised Re-ID methods.",
        "中文标题": "通过相机感知相似性一致性学习进行无监督行人重识别",
        "摘要翻译": "为了在监控中跨不连续相机视图匹配行人，行人重识别（Re-ID）在监督学习方面取得了巨大进展。然而，在扩展Re-ID系统时，在许多新场景中标记数据是不可行的。因此，研究无监督学习对于Re-ID来说，对于节省标记成本非常重要。然而，跨相机场景变化是无监督Re-ID的一个关键挑战，如光照、背景和视角变化，这些变化会导致特征空间中的域偏移，并导致成对相似性分布不一致，从而降低匹配性能。为了减轻跨相机场景变化的影响，我们提出了一种相机感知相似性一致性损失，以学习用于相机内匹配和跨相机匹配的一致成对相似性分布。为了避免在一致性学习中学习无效知识，我们在预训练模型中保留了相机内匹配的先验共同知识作为可靠的指导信息，这些信息不会像跨相机匹配那样受到跨相机场景变化的影响。为了更有效地学习相似性一致性，我们进一步开发了一种从粗到细的一致性学习方案，以两步学习全局和局部的一致性。实验表明，我们的方法优于最先进的无监督Re-ID方法。",
        "领域": "行人重识别/无监督学习/跨相机匹配",
        "问题": "解决跨相机场景变化导致的行人重识别性能下降问题",
        "动机": "节省在新场景中标记数据的成本，提高无监督行人重识别的性能",
        "方法": "提出相机感知相似性一致性损失，保留相机内匹配的先验共同知识，开发从粗到细的一致性学习方案",
        "关键词": [
            "行人重识别",
            "无监督学习",
            "跨相机匹配",
            "相似性一致性",
            "相机感知"
        ],
        "涉及的技术概念": "相机感知相似性一致性损失是一种用于无监督行人重识别的技术，旨在通过保持相机内和跨相机匹配的相似性分布一致性来减轻跨相机场景变化的影响。从粗到细的一致性学习方案则是一种分步骤学习全局和局部一致性的方法，以提高学习效率和效果。"
    },
    {
        "order": 822,
        "title": "Scalable Place Recognition Under Appearance Change for Autonomous Driving",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Doan_Scalable_Place_Recognition_Under_Appearance_Change_for_Autonomous_Driving_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Doan_Scalable_Place_Recognition_Under_Appearance_Change_for_Autonomous_Driving_ICCV_2019_paper.html",
        "abstract": "A major challenge in place recognition for autonomous driving is to be robust against appearance changes due to short-term (e.g., weather, lighting) and long-term (seasons, vegetation growth, etc.) environmental variations. A promising solution is to continuously accumulate images to maintain an adequate sample of the conditions and incorporate new changes into the place recognition decision. However, this demands a place recognition technique that is scalable on an ever growing dataset. To this end, we propose a novel place recognition technique that can be efficiently retrained and compressed, such that the recognition of new queries can exploit all available data (including recent changes) without suffering from visible growth in computational cost. Underpinning our method is a novel temporal image matching technique based on Hidden Markov Models. Our experiments show that, compared to state-of-the-art techniques, our method has much greater potential for large-scale place recognition for autonomous driving.",
        "中文标题": "自动驾驶中外观变化下的可扩展地点识别",
        "摘要翻译": "自动驾驶中的地点识别面临的一个主要挑战是对抗由于短期（如天气、光照）和长期（季节、植被生长等）环境变化引起的外观变化的鲁棒性。一个有前景的解决方案是持续积累图像以保持条件的充分样本，并将新的变化纳入地点识别决策中。然而，这要求地点识别技术能够在不断增长的数据集上扩展。为此，我们提出了一种新颖的地点识别技术，该技术可以有效地重新训练和压缩，使得新查询的识别可以利用所有可用数据（包括最近的变化），而不会遭受计算成本显著增长的困扰。我们方法的基础是一种基于隐马尔可夫模型的新型时间图像匹配技术。我们的实验表明，与最先进的技术相比，我们的方法在自动驾驶的大规模地点识别方面具有更大的潜力。",
        "领域": "自动驾驶/地点识别/图像匹配",
        "问题": "自动驾驶中地点识别对外观变化的鲁棒性问题",
        "动机": "解决自动驾驶中由于环境变化引起的地点识别挑战，提高识别的准确性和效率",
        "方法": "提出了一种基于隐马尔可夫模型的新型时间图像匹配技术，该技术可以有效地重新训练和压缩，以利用所有可用数据进行地点识别",
        "关键词": [
            "自动驾驶",
            "地点识别",
            "图像匹配",
            "隐马尔可夫模型"
        ],
        "涉及的技术概念": "隐马尔可夫模型（Hidden Markov Models, HMMs）是一种统计模型，用于描述一个含有隐含未知参数的马尔可夫过程。在本研究中，HMMs被用于时间图像匹配，以处理自动驾驶中地点识别面临的外观变化问题。"
    },
    {
        "order": 823,
        "title": "Rescan: Inductive Instance Segmentation for Indoor RGBD Scans",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Halber_Rescan_Inductive_Instance_Segmentation_for_Indoor_RGBD_Scans_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Halber_Rescan_Inductive_Instance_Segmentation_for_Indoor_RGBD_Scans_ICCV_2019_paper.html",
        "abstract": "In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these \"rescans\" to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation.",
        "中文标题": "Rescan: 室内RGBD扫描的归纳实例分割",
        "摘要翻译": "在从家庭机器人到AR/VR的深度感知应用中，以稀疏时间间隔（例如，作为日常使用的一部分）重复获取室内空间的3D扫描将变得常见。我们提出了一种算法，分析这些“重新扫描”以推断具有语义实例信息的场景时间模型。我们的算法通过使用过去观察结果产生的时间模型来推断新扫描的实例分割，然后用于更新时间模型。该模型包含跨时间的对象实例关联，因此可以用于跟踪单个对象，即使只有稀疏的观察。在为新任务的新基准测试中，我们的算法优于基于最先进的语义实例分割网络的替代方法。",
        "领域": "3D重建/实例分割/时间序列分析",
        "问题": "如何在稀疏时间间隔的3D扫描中，有效地进行实例分割并跟踪单个对象",
        "动机": "在家庭机器人到AR/VR的深度感知应用中，需要一种方法来分析重复获取的3D扫描，以推断场景的时间模型并跟踪对象",
        "方法": "提出了一种归纳算法，利用过去观察结果产生的时间模型来推断新扫描的实例分割，并更新时间模型以跟踪对象",
        "关键词": [
            "3D扫描",
            "实例分割",
            "时间模型",
            "对象跟踪"
        ],
        "涉及的技术概念": "3D扫描技术用于获取室内空间的深度信息，实例分割技术用于识别和分割场景中的对象实例，时间模型用于记录和更新对象实例的关联信息，以便在稀疏观察下跟踪对象。"
    },
    {
        "order": 824,
        "title": "Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_Photo-Realistic_Monocular_Gaze_Redirection_Using_Generative_Adversarial_Networks_ICCV_2019_paper.html",
        "abstract": "Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.",
        "中文标题": "使用生成对抗网络实现照片级真实感的单目视线重定向",
        "摘要翻译": "视线重定向是指对于给定的单目眼睛图像，将视线改变到期望的方向。许多应用，如视频会议、电影、游戏以及视线估计训练数据的生成，都需要在不扭曲眼睛周围区域外观的同时，生成照片级真实感的图像来重定向视线。现有方法缺乏生成感知上可信图像的能力。在这项工作中，我们提出了一种新方法，通过利用生成对抗训练来合成基于目标视线方向的眼睛图像，从而缓解这一问题。我们的方法确保了合成图像与真实图像在感知上的相似性和一致性。此外，使用视线估计损失来精确控制视线方向。为了获得高质量的图像，我们将感知和循环一致性损失纳入我们的架构中。在广泛的评估中，我们展示了所提出的方法在图像质量和重定向精度方面均优于最先进的方法。最后，我们展示了如果使用生成的图像来增强真实训练数据，可以为视线估计任务带来显著的改进。",
        "领域": "生成对抗网络/视线估计/图像合成",
        "问题": "现有方法在生成感知上可信的视线重定向图像方面存在不足",
        "动机": "为了在不扭曲眼睛周围区域外观的同时，生成照片级真实感的图像来重定向视线，满足视频会议、电影、游戏等应用的需求",
        "方法": "利用生成对抗训练合成基于目标视线方向的眼睛图像，并引入感知和循环一致性损失以提高图像质量",
        "关键词": [
            "视线重定向",
            "生成对抗网络",
            "图像合成",
            "视线估计"
        ],
        "涉及的技术概念": "生成对抗训练是一种通过同时训练生成器和判别器来生成新数据的方法，其中生成器尝试生成尽可能真实的图像，而判别器则尝试区分真实图像和生成图像。感知和循环一致性损失是用于确保生成图像在视觉上与真实图像相似，并且在循环过程中保持一致的损失函数。视线估计损失用于精确控制生成图像的视线方向。"
    },
    {
        "order": 825,
        "title": "Exploring the Limitations of Behavior Cloning for Autonomous Driving",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Codevilla_Exploring_the_Limitations_of_Behavior_Cloning_for_Autonomous_Driving_ICCV_2019_paper.html",
        "abstract": "Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-of-the-art results, executing complex lateral and longitudinal maneuvers, even in unseen environments, without being explicitly programmed to do so. However, we confirm some limitations of the behavior cloning approach: some well-known limitations (e.g., dataset bias and overfitting), new generalization issues (e.g., dynamic objects and the lack of a causal modeling), and training instabilities, all requiring further research before behavior cloning can graduate to real-world driving. The code, dataset, benchmark, and agent studied in this paper can be found at github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md",
        "中文标题": "探索行为克隆在自动驾驶中的局限性",
        "摘要翻译": "驾驶需要对各种复杂的环境条件和代理行为做出反应。明确地建模每一种可能的场景是不现实的。相比之下，模仿学习理论上可以利用来自大量人类驾驶汽车的数据。特别是行为克隆已成功用于端到端地学习简单的视觉运动策略，但扩展到全方位的驾驶行为仍然是一个未解决的问题。在本文中，我们提出了一个新的基准，以实验性地研究行为克隆的可扩展性和局限性。我们展示了行为克隆能够实现最先进的结果，执行复杂的横向和纵向操作，即使在未见过的环境中，也不需要明确编程。然而，我们确认了行为克隆方法的一些局限性：一些众所周知的局限性（例如，数据集偏差和过拟合），新的泛化问题（例如，动态对象和缺乏因果建模），以及训练不稳定性，所有这些都需要进一步研究，才能使行为克隆应用于现实世界的驾驶。本文研究的代码、数据集、基准和代理可以在github.com/felipecode/coiltraine/blob/master/docs/exploring_limitations.md找到。",
        "领域": "自动驾驶/行为克隆/模仿学习",
        "问题": "行为克隆在自动驾驶中的应用局限性和可扩展性问题",
        "动机": "探索行为克隆方法在自动驾驶中的应用潜力及其局限性，以推动该技术向实际应用迈进",
        "方法": "提出新的基准来实验性地研究行为克隆的可扩展性和局限性，并通过实验展示其效果和存在的问题",
        "关键词": [
            "自动驾驶",
            "行为克隆",
            "模仿学习",
            "数据集偏差",
            "过拟合",
            "泛化问题",
            "训练不稳定性"
        ],
        "涉及的技术概念": {
            "行为克隆": "一种模仿学习技术，通过模仿专家的行为来学习策略",
            "模仿学习": "一种机器学习方法，通过模仿专家的行为来学习完成任务",
            "数据集偏差": "训练数据与真实世界数据之间的不一致，导致模型泛化能力下降",
            "过拟合": "模型在训练数据上表现良好，但在未见过的数据上表现不佳",
            "泛化问题": "模型在处理未见过的数据或情况时的表现问题",
            "训练不稳定性": "在训练过程中，模型性能波动大，难以稳定提升"
        }
    },
    {
        "order": 826,
        "title": "Dynamic Kernel Distillation for Efficient Pose Estimation in Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.html",
        "abstract": "Existing video-based human pose estimation methods extensively apply large networks onto every frame in the video to localize body joints, which suffer high computational cost and hardly meet the low-latency requirement in realistic applications. To address this issue, we propose a novel Dynamic Kernel Distillation (DKD) model to facilitate small networks for estimating human poses in videos, thus significantly lifting the efficiency. In particular, DKD introduces a light-weight distillator to online distill pose kernels via leveraging temporal cues from the previous frame in a one-shot feed-forward manner. Then, DKD simplifies body joint localization into a matching procedure between the pose kernels and the current frame, which can be efficiently computed via simple convolution. In this way, DKD fast transfers pose knowledge from one frame to provide compact guidance for body joint localization in the following frame, which enables utilization of small networks in video-based pose estimation. To facilitate the training process, DKD exploits a temporally adversarial training strategy that introduces a temporal discriminator to help generate temporally coherent pose kernels and pose estimation results within a long range. Experiments on Penn Action and Sub-JHMDB benchmarks demonstrate outperforming efficiency of DKD, specifically, 10x flops reduction and 2x speedup over previous best model, and its state-of-the-art accuracy.",
        "中文标题": "动态核蒸馏用于视频中的高效姿态估计",
        "摘要翻译": "现有的基于视频的人体姿态估计方法广泛地将大型网络应用于视频中的每一帧以定位身体关节，这种方法计算成本高，难以满足实际应用中的低延迟要求。为了解决这个问题，我们提出了一种新颖的动态核蒸馏（DKD）模型，以促进小型网络在视频中估计人体姿态，从而显著提高效率。特别是，DKD引入了一个轻量级的蒸馏器，通过利用前一帧的时间线索以一次性前馈方式在线蒸馏姿态核。然后，DKD将身体关节定位简化为姿态核与当前帧之间的匹配过程，这可以通过简单的卷积高效计算。通过这种方式，DKD快速将姿态知识从一帧转移到下一帧，为后续帧中的身体关节定位提供紧凑的指导，这使得在基于视频的姿态估计中能够利用小型网络。为了促进训练过程，DKD采用了一种时间对抗训练策略，该策略引入了一个时间判别器，以帮助生成长范围内时间一致的姿态核和姿态估计结果。在Penn Action和Sub-JHMDB基准测试上的实验证明了DKD的卓越效率，具体来说，与之前的最佳模型相比，计算量减少了10倍，速度提高了2倍，并且其准确性达到了最先进的水平。",
        "领域": "人体姿态估计/视频分析/深度学习",
        "问题": "视频中人体姿态估计的高计算成本和低延迟要求",
        "动机": "解决现有方法在视频中应用大型网络导致的高计算成本和难以满足低延迟要求的问题",
        "方法": "提出动态核蒸馏（DKD）模型，通过轻量级蒸馏器在线蒸馏姿态核，简化身体关节定位为姿态核与当前帧的匹配过程，采用时间对抗训练策略促进训练",
        "关键词": [
            "动态核蒸馏",
            "人体姿态估计",
            "视频分析",
            "时间对抗训练"
        ],
        "涉及的技术概念": "动态核蒸馏（DKD）是一种新颖的模型，旨在通过在线蒸馏姿态核和简化身体关节定位过程来提高视频中人体姿态估计的效率。时间对抗训练策略用于生成长范围内时间一致的姿态核和姿态估计结果。"
    },
    {
        "order": 827,
        "title": "End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Avetisyan_End-to-End_CAD_Model_Retrieval_and_9DoF_Alignment_in_3D_Scans_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Avetisyan_End-to-End_CAD_Model_Retrieval_and_9DoF_Alignment_in_3D_Scans_ICCV_2019_paper.html",
        "abstract": "We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by 19.04% for CAD model alignment to scans, with approximately 250x faster runtime than previous data-driven approaches.",
        "中文标题": "端到端的CAD模型检索与3D扫描中的9自由度对齐",
        "摘要翻译": "我们提出了一种新颖的端到端方法，用于将CAD模型与场景的3D扫描对齐，从而能够将噪声大、不完整的3D扫描转换为具有干净、完整对象几何的紧凑CAD重建。我们的主要贡献在于制定了一种可微分的Procrustes对齐方法，该方法与对称感知的密集对象对应预测相结合。为了同时将CAD模型与扫描场景中的所有对象对齐，我们的方法检测对象位置，然后在统一的对象空间中预测扫描与CAD几何之间的对称感知密集对象对应关系，以及最近的CAD模型，这两者随后用于通知可微分的Procrustes对齐。我们的方法以全卷积方式操作，使得在单次前向传递中即可将CAD模型与扫描对象对齐。这使得我们的方法在CAD模型与扫描对齐方面优于最先进的方法19.04%，运行时间比之前的数据驱动方法快约250倍。",
        "领域": "3D重建/CAD模型对齐/对称感知",
        "问题": "如何将CAD模型与3D扫描中的对象进行高效准确的对齐",
        "动机": "为了将噪声大、不完整的3D扫描转换为具有干净、完整对象几何的紧凑CAD重建",
        "方法": "采用了一种可微分的Procrustes对齐方法，结合对称感知的密集对象对应预测，以全卷积方式操作，实现单次前向传递中的对齐",
        "关键词": [
            "3D重建",
            "CAD模型对齐",
            "对称感知",
            "Procrustes对齐",
            "密集对象对应预测"
        ],
        "涉及的技术概念": {
            "Procrustes对齐": "一种用于形状分析的统计方法，通过旋转、缩放和平移来最小化两个形状之间的差异",
            "对称感知的密集对象对应预测": "一种预测方法，考虑到对象的对称性，预测扫描与CAD几何之间的密集对应关系",
            "全卷积方式": "一种神经网络架构，能够处理任意大小的输入图像，常用于图像分割等任务"
        }
    },
    {
        "order": 828,
        "title": "Habitat: A Platform for Embodied AI Research",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html",
        "abstract": "We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments  train, test  x  Matterport3D, Gibson  for multiple sensors  blind, RGB, RGBD, D  and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",
        "中文标题": "Habitat：一个用于具身人工智能研究的平台",
        "摘要翻译": "我们介绍了Habitat，一个用于具身人工智能（AI）研究的平台。Habitat能够在高效的逼真3D模拟中训练具身代理（虚拟机器人）。具体来说，Habitat包括：（i）Habitat-Sim：一个灵活、高性能的3D模拟器，具有可配置的代理、传感器和通用3D数据集处理能力。Habitat-Sim速度极快——在渲染Matterport3D的场景时，单线程运行可达到每秒数千帧（fps），在单个GPU上多进程运行可超过10,000 fps。（ii）Habitat-API：一个模块化的高级库，用于具身AI算法的端到端开发——定义任务（例如，导航、指令跟随、问答）、配置、训练和基准测试具身代理。这些大规模的工程贡献使我们能够回答需要实验的科学问题，这些实验直到现在都是不可行的或“仅仅”是不切实际的。具体来说，在点目标导航的背景下：（1）我们重新审视了最近两项工作中学习和SLAM方法的比较，并发现了相反结论的证据——如果扩展到比之前研究多一个数量级的经验，学习优于SLAM，（2）我们进行了首次跨数据集泛化实验——在Matterport3D和Gibson上训练、测试——针对多个传感器（盲、RGB、RGBD、D），发现只有带有深度（D）传感器的代理能够跨数据集泛化。我们希望我们的开源平台和这些发现能够推动具身AI的研究。",
        "领域": "具身人工智能/3D模拟/机器人导航",
        "问题": "如何高效训练具身代理在逼真的3D模拟环境中执行任务",
        "动机": "推动具身人工智能研究，通过提供一个高效的平台来训练和测试具身代理，解决之前实验不可行或不切实际的问题",
        "方法": "开发了Habitat平台，包括Habitat-Sim（高性能3D模拟器）和Habitat-API（模块化高级库），用于定义任务、配置、训练和基准测试具身代理",
        "关键词": [
            "具身人工智能",
            "3D模拟",
            "机器人导航",
            "跨数据集泛化"
        ],
        "涉及的技术概念": {
            "Habitat-Sim": "一个灵活、高性能的3D模拟器，支持可配置的代理、传感器和通用3D数据集处理，能够以极高的帧率渲染场景",
            "Habitat-API": "一个模块化的高级库，用于具身AI算法的端到端开发，包括任务定义、代理配置、训练和基准测试",
            "点目标导航": "在具身AI中的一个任务，代理需要在环境中导航到指定的点",
            "SLAM": "同时定位与地图构建，一种让机器人在未知环境中建立地图并同时确定自身位置的技术",
            "跨数据集泛化": "指模型或代理在一个数据集上训练后，能够在另一个未见过的数据集上有效执行任务的能力"
        }
    },
    {
        "order": 829,
        "title": "Single-Stage Multi-Person Pose Machines",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.html",
        "abstract": "Multi-person pose estimation is a challenging problem. Existing methods are mostly two-stage based-one stage for proposal generation and the other for allocating poses to corresponding persons. However, such two-stage methods generally suffer low efficiency. In this work, we present the first single-stage model, Single-stage multi-person Pose Machine (SPM), to simplify the pipeline and lift the efficiency for multi-person pose estimation. To achieve this, we propose a novel Structured Pose Representation (SPR) that unifies person instance and body joint position representations. Based on SPR, we develop the SPM model that can directly predict structured poses for multiple persons in a single stage, and thus offer a more compact pipeline and attractive efficiency advantage over two-stage methods. In particular, SPR introduces the root joints to indicate different person instances and human body joint positions are encoded into their displacements w.r.t. the roots. To better predict long-range displacements for some joints, SPR is further extended to hierarchical representations. Based on SPR, SPM can efficiently perform multi-person poses estimation by simultaneously predicting root joints (location of instances) and body joint displacements via CNNs. Moreover, to demonstrate the generality of SPM, we also apply it to multi-person 3D pose estimation. Comprehensive experiments on benchmarks MPII, extended PASCAL-Person-Part, MSCOCO and CMU Panoptic clearly demonstrate the state-of-the-art efficiency of SPM for multi-person 2D/3D pose estimation, together with outstanding accuracy.",
        "中文标题": "单阶段多人姿态机",
        "摘要翻译": "多人姿态估计是一个具有挑战性的问题。现有的方法大多基于两阶段——一个阶段用于生成提议，另一个阶段用于将姿态分配给相应的人。然而，这样的两阶段方法通常效率较低。在这项工作中，我们提出了第一个单阶段模型，单阶段多人姿态机（SPM），以简化流程并提高多人姿态估计的效率。为了实现这一点，我们提出了一种新颖的结构化姿态表示（SPR），它统一了人物实例和身体关节位置的表示。基于SPR，我们开发了SPM模型，该模型可以在单阶段直接预测多个人的结构化姿态，从而提供了一个更紧凑的流程和比两阶段方法更具吸引力的效率优势。特别是，SPR引入了根关节来指示不同的人物实例，并将人体关节位置编码为它们相对于根的位移。为了更好地预测某些关节的长距离位移，SPR进一步扩展到分层表示。基于SPR，SPM可以通过同时预测根关节（实例的位置）和通过CNN预测身体关节位移来高效地执行多人姿态估计。此外，为了展示SPM的通用性，我们还将其应用于多人3D姿态估计。在MPII、扩展的PASCAL-Person-Part、MSCOCO和CMU Panoptic基准上的综合实验清楚地展示了SPM在多人2D/3D姿态估计中的最先进效率，以及出色的准确性。",
        "领域": "姿态估计/人体姿态分析/3D重建",
        "问题": "提高多人姿态估计的效率和简化流程",
        "动机": "现有的两阶段方法在多人姿态估计中效率较低，需要简化流程并提高效率",
        "方法": "提出了一种新颖的结构化姿态表示（SPR），并基于此开发了单阶段多人姿态机（SPM）模型，该模型可以直接在单阶段预测多个人的结构化姿态",
        "关键词": [
            "姿态估计",
            "结构化姿态表示",
            "单阶段模型"
        ],
        "涉及的技术概念": "结构化姿态表示（SPR）是一种统一人物实例和身体关节位置表示的方法，通过引入根关节来指示不同的人物实例，并将人体关节位置编码为它们相对于根的位移。为了处理长距离位移，SPR被扩展到分层表示。单阶段多人姿态机（SPM）模型基于SPR，通过同时预测根关节和身体关节位移来高效执行多人姿态估计。"
    },
    {
        "order": 830,
        "title": "Making History Matter: History-Advantage Sequence Training for Visual Dialog",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Making_History_Matter_History-Advantage_Sequence_Training_for_Visual_Dialog_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Making_History_Matter_History-Advantage_Sequence_Training_for_Visual_Dialog_ICCV_2019_paper.html",
        "abstract": "We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage -- a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.",
        "中文标题": "让历史重要：视觉对话中的历史优势序列训练",
        "摘要翻译": "我们研究了视觉对话中的多轮响应生成，其中响应是根据视觉基础的对话历史生成的。给定一个三元组：图像、问答历史和当前问题，所有流行的方法都遵循监督学习范式中的编解码器（即编码器-解码器）方式：多模态编码器将三元组编码为特征向量，然后将其输入解码器以生成当前答案，由真实答案监督。然而，这种传统的监督学习并未考虑到不完美历史的影响，违反了视觉对话的对话性质，从而使编解码器更倾向于学习历史偏见而非上下文推理。为此，受强化学习中演员-评论家策略梯度的启发，我们提出了一种新的训练范式，称为历史优势序列训练（HAST）。具体来说，我们故意在历史中引入错误答案，获得一个不利的评论家，并通过历史优势——从真实历史奖励中减去不利评论家得到的量——来观察历史错误如何影响编解码器的未来行为。此外，为了使编解码器对历史更加敏感，我们提出了一种新的注意力网络，称为历史感知共同注意力网络（HACAN），它可以通过使用HAST有效训练。在三个基准测试上的实验结果：VisDial v0.9&v1.0和GuessWhat?!，显示所提出的HAST策略始终优于最先进的监督对应物。",
        "领域": "视觉对话/多模态学习/强化学习",
        "问题": "解决视觉对话中多轮响应生成的问题，特别是如何处理不完美的对话历史以提高模型的上下文推理能力",
        "动机": "传统的监督学习方法在视觉对话中未能考虑不完美历史的影响，导致模型倾向于学习历史偏见而非进行有效的上下文推理",
        "方法": "提出了一种新的训练范式HAST，通过故意引入历史错误答案来观察其对模型未来行为的影响，并开发了历史感知共同注意力网络（HACAN）以提高模型对历史的敏感性",
        "关键词": [
            "视觉对话",
            "多模态学习",
            "强化学习",
            "历史优势序列训练",
            "历史感知共同注意力网络"
        ],
        "涉及的技术概念": {
            "视觉对话": "一种结合视觉信息和文本对话的交互式任务，旨在生成与图像相关的对话响应",
            "多模态学习": "涉及处理和理解来自不同模态（如视觉和文本）信息的学习方法",
            "强化学习": "一种机器学习方法，通过奖励和惩罚机制来训练模型做出决策",
            "历史优势序列训练（HAST）": "一种训练范式，通过引入历史错误答案来评估和改善模型对历史信息的处理能力",
            "历史感知共同注意力网络（HACAN）": "一种注意力网络，旨在提高模型对对话历史的敏感性和理解能力"
        }
    },
    {
        "order": 831,
        "title": "Towards Interpretable Face Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Towards_Interpretable_Face_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yin_Towards_Interpretable_Face_Recognition_ICCV_2019_paper.html",
        "abstract": "Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.",
        "中文标题": "迈向可解释的人脸识别",
        "摘要翻译": "深度卷积神经网络（CNNs）在过去几年中一直在推动视觉识别的前沿。除了识别准确性外，研究社区对理解深度CNNs的强烈需求推动了工具的发展，以剖析预训练模型，可视化它们如何做出预测。最近的工作进一步推动了网络学习阶段的可解释性，以学习更有意义的表示。在这项工作中，我们专注于视觉识别的一个特定领域，报告了我们迈向可解释人脸识别的努力。我们提出了一种空间激活多样性损失，以学习更有结构的人脸表示。通过利用这种结构，我们进一步设计了一种特征激活多样性损失，以推动可解释的表示具有区分性并对遮挡具有鲁棒性。我们在三个人脸识别基准上证明，我们提出的方法能够实现最先进的人脸识别准确性，同时提供易于解释的人脸表示。",
        "领域": "人脸识别/深度学习/神经网络",
        "问题": "提高人脸识别的可解释性",
        "动机": "研究社区对理解深度CNNs的强烈需求，以及提高人脸识别模型的可解释性",
        "方法": "提出空间激活多样性损失和特征激活多样性损失，以学习更有结构的人脸表示，并推动可解释的表示具有区分性并对遮挡具有鲁棒性",
        "关键词": [
            "人脸识别",
            "可解释性",
            "深度学习",
            "神经网络",
            "空间激活多样性损失",
            "特征激活多样性损失"
        ],
        "涉及的技术概念": {
            "深度卷积神经网络（CNNs）": "一种深度学习模型，用于处理图像等视觉数据",
            "空间激活多样性损失": "一种损失函数，用于学习更有结构的人脸表示",
            "特征激活多样性损失": "一种损失函数，用于推动可解释的表示具有区分性并对遮挡具有鲁棒性",
            "预训练模型": "在大量数据上预先训练好的模型，可以用于特定任务的微调"
        }
    },
    {
        "order": 832,
        "title": "SO-HandNet: Self-Organizing Network for 3D Hand Pose Estimation With Semi-Supervised Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_SO-HandNet_Self-Organizing_Network_for_3D_Hand_Pose_Estimation_With_Semi-Supervised_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_SO-HandNet_Self-Organizing_Network_for_3D_Hand_Pose_Estimation_With_Semi-Supervised_ICCV_2019_paper.html",
        "abstract": "3D hand pose estimation has made significant progress recently, where Convolutional Neural Networks (CNNs) play a critical role. However, most of the existing CNN-based hand pose estimation methods depend much on the training set, while labeling 3D hand pose on training data is laborious and time-consuming. Inspired by the point cloud autoencoder presented in self-organizing network (SO-Net), our proposed SO-HandNet aims at making use of the unannotated data to obtain accurate 3D hand pose estimation in a semi-supervised manner. We exploit hand feature encoder (HFE) to extract multi-level features from hand point cloud and then fuse them to regress 3D hand pose by a hand pose estimator (HPE). We design a hand feature decoder (HFD) to recover the input point cloud from the encoded feature. Since the HFE and the HFD can be trained without 3D hand pose annotation, the proposed method is able to make the best of unannotated data during the training phase. Experiments on four challenging benchmark datasets validate that our proposed SO-HandNet can achieve superior performance for 3D hand pose estimation via semi-supervised learning.",
        "中文标题": "SO-HandNet：用于半监督学习的三维手部姿态估计的自组织网络",
        "摘要翻译": "近年来，三维手部姿态估计取得了显著进展，其中卷积神经网络（CNNs）扮演了关键角色。然而，大多数现有的基于CNN的手部姿态估计方法在很大程度上依赖于训练集，而在训练数据上标注三维手部姿态既费力又耗时。受自组织网络（SO-Net）中提出的点云自动编码器的启发，我们提出的SO-HandNet旨在利用未标注的数据以半监督的方式获得准确的三维手部姿态估计。我们利用手部特征编码器（HFE）从手部点云中提取多层次特征，然后通过手部姿态估计器（HPE）融合这些特征以回归三维手部姿态。我们设计了一个手部特征解码器（HFD）来从编码的特征中恢复输入点云。由于HFE和HFD可以在没有三维手部姿态标注的情况下进行训练，所提出的方法能够在训练阶段充分利用未标注的数据。在四个具有挑战性的基准数据集上的实验验证了我们提出的SO-HandNet通过半监督学习可以实现卓越的三维手部姿态估计性能。",
        "领域": "三维视觉/姿态估计/半监督学习",
        "问题": "三维手部姿态估计的训练数据标注既费力又耗时",
        "动机": "利用未标注的数据以半监督的方式获得准确的三维手部姿态估计",
        "方法": "利用手部特征编码器（HFE）提取多层次特征，通过手部姿态估计器（HPE）融合特征以回归三维手部姿态，设计手部特征解码器（HFD）恢复输入点云",
        "关键词": [
            "三维手部姿态估计",
            "半监督学习",
            "自组织网络",
            "点云自动编码器"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNNs）": "一种深度学习模型，用于处理图像数据，能够自动提取图像特征。",
            "自组织网络（SO-Net）": "一种网络结构，能够自动组织数据，用于点云数据的处理。",
            "点云自动编码器": "一种用于点云数据的编码和解码技术，能够从点云数据中提取特征并重建点云。",
            "手部特征编码器（HFE）": "用于从手部点云中提取多层次特征的编码器。",
            "手部姿态估计器（HPE）": "用于从提取的特征中回归三维手部姿态的估计器。",
            "手部特征解码器（HFD）": "用于从编码的特征中恢复输入点云的解码器。"
        }
    },
    {
        "order": 833,
        "title": "Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Stochastic_Attraction-Repulsion_Embedding_for_Large_Scale_Image_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Stochastic_Attraction-Repulsion_Embedding_for_Large_Scale_Image_Localization_ICCV_2019_paper.html",
        "abstract": "This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.",
        "中文标题": "随机吸引-排斥嵌入用于大规模图像定位",
        "摘要翻译": "本文解决了大规模基于图像的定位（IBL）问题，其中通过在大数据库中找出最相似的参考图像来确定查询图像的空间位置。为了解决这个问题，一个关键任务是学习能够捕捉与定位相关的信息性信息的判别性图像表示。我们提出了一种具有更高位置区分能力的新颖表示学习方法。它提供了以下贡献：1）我们将一个地点（位置）表示为一组描绘相同地标的示例图像，并旨在最大化地点内图像之间的相似性，同时最小化地点间图像之间的相似性；2）我们将相似性度量建模为地点内和地点间图像表示之间的L_2度量距离上的概率分布；3）我们提出了一种新的随机吸引和排斥嵌入（SARE）损失函数，最小化学习到的和实际概率分布之间的KL散度；4）我们通过分析梯度，提供了SARE、三重排序和对比损失之间的理论比较，深入探讨了为什么SARE更好。我们的SARE损失易于实现并可插入任何CNN。实验表明，我们提出的方法在标准基准上大幅提高了定位性能。展示了我们方法的广泛适用性，我们在2018年谷歌地标检索挑战赛中从209个团队中获得了第三名。我们的代码和模型可在https://github.com/Liumouliu/deepIBL获取。",
        "领域": "图像定位/表示学习/地标识别",
        "问题": "大规模基于图像的定位问题",
        "动机": "提高图像定位的准确性和效率，通过增强图像表示的判别能力来更好地解决大规模图像定位问题。",
        "方法": "提出了一种新的随机吸引和排斥嵌入（SARE）损失函数，通过最小化学习到的和实际概率分布之间的KL散度来学习具有更高位置区分能力的图像表示。",
        "关键词": [
            "图像定位",
            "表示学习",
            "地标识别",
            "SARE损失函数",
            "KL散度"
        ],
        "涉及的技术概念": {
            "大规模基于图像的定位（IBL）": "通过在大数据库中找出最相似的参考图像来确定查询图像的空间位置。",
            "判别性图像表示": "能够捕捉与定位相关的信息性信息的图像表示。",
            "随机吸引和排斥嵌入（SARE）损失函数": "一种新的损失函数，通过最小化学习到的和实际概率分布之间的KL散度来学习图像表示。",
            "KL散度": "衡量两个概率分布之间差异的指标。",
            "CNN": "卷积神经网络，一种深度学习模型，广泛用于图像识别和处理。"
        }
    },
    {
        "order": 834,
        "title": "Co-Mining: Deep Face Recognition With Noisy Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.html",
        "abstract": "Face recognition has achieved significant progress with the growing scale of collected datasets, which empowers us to train strong convolutional neural networks (CNNs). While a variety of CNN architectures and loss functions have been devised recently, we still have a limited understanding of how to train the CNN models with the label noise inherent in existing face recognition datasets. To address this issue, this paper develops a novel co-mining strategy to effectively train on the datasets with noisy labels. Specifically, we simultaneously use the loss values as the cue to detect noisy labels, exchange the high-confidence clean faces to alleviate the errors accumulated issue caused by the sample-selection bias, and re-weight the predicted clean faces to make them dominate the discriminative model training in a mini-batch fashion. Extensive experiments by training on three popular datasets (i.e., CASIA-WebFace, MS-Celeb-1M and VggFace2) and testing on several benchmarks, including LFW, AgeDB, CFP, CALFW, CPLFW, RFW, and MegaFace, have demonstrated the effectiveness of our new approach over the state-of-the-art alternatives.",
        "中文标题": "协同挖掘：带有噪声标签的深度人脸识别",
        "摘要翻译": "随着收集数据集的规模不断扩大，人脸识别技术取得了显著进展，这使我们能够训练出强大的卷积神经网络（CNNs）。尽管最近设计了多种CNN架构和损失函数，但对于如何利用现有的人脸识别数据集中固有的标签噪声来训练CNN模型，我们的理解仍然有限。为了解决这个问题，本文开发了一种新颖的协同挖掘策略，以有效地在带有噪声标签的数据集上进行训练。具体来说，我们同时使用损失值作为检测噪声标签的线索，交换高置信度的干净人脸以缓解由样本选择偏差引起的错误累积问题，并以小批量方式重新加权预测的干净人脸，使它们在判别模型训练中占据主导地位。通过在三个流行数据集（即CASIA-WebFace、MS-Celeb-1M和VggFace2）上训练并在包括LFW、AgeDB、CFP、CALFW、CPLFW、RFW和MegaFace在内的多个基准上进行测试的广泛实验，证明了我们的新方法相对于最先进替代方案的有效性。",
        "领域": "人脸识别/深度学习/卷积神经网络",
        "问题": "如何在带有噪声标签的人脸识别数据集上有效训练卷积神经网络模型",
        "动机": "现有的人脸识别数据集中存在标签噪声，这限制了CNN模型的训练效果，需要开发新的方法来提高训练效率和模型性能",
        "方法": "开发了一种协同挖掘策略，通过使用损失值检测噪声标签，交换高置信度的干净人脸以缓解样本选择偏差引起的错误累积，并以小批量方式重新加权预测的干净人脸来主导判别模型训练",
        "关键词": [
            "人脸识别",
            "卷积神经网络",
            "噪声标签",
            "协同挖掘",
            "样本选择偏差"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）是一种深度学习模型，特别适用于处理图像数据。损失函数用于评估模型预测与真实标签之间的差异。噪声标签指的是数据集中错误或不准确的标签。协同挖掘策略是一种通过交换和重新加权高置信度样本来提高模型训练效果的方法。样本选择偏差是指在选择训练样本时可能引入的偏差，导致模型训练效果不佳。"
    },
    {
        "order": 835,
        "title": "Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Adaptive_Wing_Loss_for_Robust_Face_Alignment_via_Heatmap_Regression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Adaptive_Wing_Loss_for_Robust_Face_Alignment_via_Heatmap_Regression_ICCV_2019_paper.html",
        "abstract": "Heatmap regression with a deep network has become one of the mainstream approaches to localize facial landmarks. However, the loss function for heatmap regression is rarely studied. In this paper, we analyze the ideal loss function properties for heatmap regression in face alignment problems. Then we propose a novel loss function, named Adaptive Wing loss, that is able to adapt its shape to different types of ground truth heatmap pixels. This adaptability penalizes loss more on foreground pixels while less on background pixels. To address the imbalance between foreground and background pixels, we also propose Weighted Loss Map, which assigns high weights on foreground and difficult background pixels to help training process focus more on pixels that are crucial to landmark localization. To further improve face alignment accuracy, we introduce boundary prediction and CoordConv with boundary coordinates. Extensive experiments on different benchmarks, including COFW, 300W and WFLW, show our approach outperforms the state-of-the-art by a significant margin on various evaluation metrics. Besides, the Adaptive Wing loss also helps other heatmap regression tasks.",
        "中文标题": "通过热图回归实现鲁棒面部对齐的自适应翼损失",
        "摘要翻译": "使用深度网络进行热图回归已成为定位面部标志的主流方法之一。然而，热图回归的损失函数很少被研究。在本文中，我们分析了面部对齐问题中热图回归的理想损失函数特性。然后，我们提出了一种新的损失函数，名为自适应翼损失，它能够根据不同类型的地面真实热图像素调整其形状。这种适应性在前景像素上施加更大的损失惩罚，而在背景像素上施加较小的损失。为了解决前景和背景像素之间的不平衡问题，我们还提出了加权损失图，它在前景和困难的背景像素上分配高权重，以帮助训练过程更多地关注对标志定位至关重要的像素。为了进一步提高面部对齐的准确性，我们引入了边界预测和带有边界坐标的CoordConv。在不同基准上的广泛实验，包括COFW、300W和WFLW，显示我们的方法在各种评估指标上显著优于现有技术。此外，自适应翼损失也有助于其他热图回归任务。",
        "领域": "面部标志定位/热图回归/损失函数优化",
        "问题": "热图回归中损失函数的研究不足，以及前景与背景像素之间的不平衡问题",
        "动机": "提高面部对齐的准确性和鲁棒性，通过优化损失函数来更好地处理热图回归中的像素差异",
        "方法": "提出自适应翼损失函数和加权损失图，引入边界预测和CoordConv技术",
        "关键词": [
            "面部标志定位",
            "热图回归",
            "损失函数优化",
            "自适应翼损失",
            "加权损失图",
            "边界预测",
            "CoordConv"
        ],
        "涉及的技术概念": "热图回归是一种通过深度网络预测图像中特定点（如面部标志）位置的方法。损失函数用于衡量预测值与真实值之间的差异，优化损失函数可以提高模型的预测准确性。自适应翼损失是一种能够根据像素类型调整损失形状的损失函数，旨在更有效地处理前景和背景像素之间的差异。加权损失图通过为关键像素分配更高的权重来优化训练过程。边界预测和CoordConv是用于提高面部对齐准确性的技术，其中CoordConv通过在卷积层中引入坐标信息来增强模型的空间感知能力。"
    },
    {
        "order": 836,
        "title": "Scene Graph Prediction With Limited Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Scene_Graph_Prediction_With_Limited_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Scene_Graph_Prediction_With_Limited_Labels_ICCV_2019_paper.html",
        "abstract": "Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.",
        "中文标题": "有限标签下的场景图预测",
        "摘要翻译": "视觉知识库，如Visual Genome，在计算机视觉中推动了众多应用，包括视觉问答和字幕生成，但存在关系稀疏、不完整的问题。迄今为止，所有场景图模型都仅限于在每类关系有数千个训练标签的小型视觉关系集上进行训练。雇佣人类注释者成本高昂，而使用文本知识库补全方法与视觉数据不兼容。在本文中，我们介绍了一种半监督方法，该方法使用少量标注示例为大量未标注图像分配概率关系标签。我们分析视觉关系，提出了两种与图像无关的特征类型，用于生成噪声启发式，其输出通过基于因子图的生成模型进行聚合。每个关系仅需10个标注示例，生成模型就能创建足够的训练数据来训练任何现有的最先进的场景图模型。我们证明了我们的方法在场景图预测上优于所有基线方法，PREDCLS的recall@100提高了5.16。在我们的有限标签设置中，我们定义了一个关系复杂度指标，作为我们的方法在哪些条件下优于迁移学习（有限标签训练的事实标准方法）的指标（R^2 = 0.778）。",
        "领域": "场景理解/视觉关系检测/半监督学习",
        "问题": "解决视觉知识库中关系稀疏、不完整的问题，以及在有限标签下训练场景图模型的挑战",
        "动机": "由于雇佣人类注释者成本高昂，且文本知识库补全方法与视觉数据不兼容，需要一种有效的方法在有限标签下训练场景图模型",
        "方法": "提出了一种半监督方法，通过分析视觉关系生成两种与图像无关的特征，使用这些特征生成噪声启发式，并通过基于因子图的生成模型聚合输出，从而在少量标注示例下生成足够的训练数据",
        "关键词": [
            "场景图预测",
            "半监督学习",
            "视觉关系检测",
            "因子图模型"
        ],
        "涉及的技术概念": {
            "半监督方法": "一种结合少量标注数据和大量未标注数据进行学习的方法",
            "因子图模型": "一种用于表示变量之间关系的图形模型，常用于概率推理和机器学习",
            "视觉关系检测": "识别图像中对象之间的视觉关系，如空间关系、动作关系等",
            "场景图预测": "预测图像中对象及其关系的结构化表示，用于场景理解"
        }
    },
    {
        "order": 837,
        "title": "Few-Shot Adaptive Gaze Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Park_Few-Shot_Adaptive_Gaze_Estimation_ICCV_2019_paper.html",
        "abstract": "Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (<= 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze",
        "中文标题": "少样本自适应视线估计",
        "摘要翻译": "人际间的解剖学差异限制了独立于个人的视线估计网络的准确性。然而，为了支持需要更高质量的应用，有必要进一步降低视线误差。通过个性化视线网络，理想情况下使用少量校准样本，可以进一步获得增益。然而，过度参数化的神经网络不适合从少量样本中学习，因为它们可能会迅速过拟合。我们接受这些挑战，并提出了一个新颖的框架——少样本自适应视线估计（Faze），用于学习个人特定的视线网络，使用非常少（<= 9）的校准样本。Faze通过解耦的编码器-解码器架构学习旋转感知的视线潜在表示，以及使用元学习训练的高度适应性视线估计器。它能够适应任何新的人，仅用3个样本就能产生显著的性能提升，在GazeCapture上实现了3.18度的最先进性能，比现有技术提高了19%。我们在https://github.com/NVlabs/few_shot_gaze开源了我们的代码。",
        "领域": "视线估计/元学习/个性化学习",
        "问题": "解决人际间解剖学差异导致的独立于个人的视线估计网络准确性问题",
        "动机": "为了支持需要更高质量视线估计的应用，降低视线误差",
        "方法": "提出了一个新颖的框架——少样本自适应视线估计（Faze），通过解耦的编码器-解码器架构学习旋转感知的视线潜在表示，以及使用元学习训练的高度适应性视线估计器",
        "关键词": [
            "视线估计",
            "元学习",
            "个性化学习",
            "编码器-解码器架构"
        ],
        "涉及的技术概念": "Faze框架通过解耦的编码器-解码器架构学习旋转感知的视线潜在表示，以及使用元学习训练的高度适应性视线估计器，能够适应任何新的人，仅用3个样本就能产生显著的性能提升。"
    },
    {
        "order": 838,
        "title": "Single-Network Whole-Body Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "We present the first single-network approach for 2D whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in the image. The network is trained in a single stage using multi-task learning, through an improved architecture which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves upon OpenPose [??], the only work so far capable of whole-body pose estimation, both in terms of speed and global accuracy. Unlike OpenPose, our method does not need to run an additional network for each hand and face candidate, making it substantially faster for multi-person scenarios. This work directly results in a reduction of computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands. For code, trained models, and validation benchmarks, visit our project page: https://github.com/CMU-Perceptual-Computing-Lab/openpose_train.",
        "中文标题": "单网络全身姿态估计",
        "摘要翻译": "我们提出了首个用于2D全身姿态估计的单网络方法，该方法能够同时定位身体、面部、手部和脚部的关键点。由于采用了自下而上的方法，我们的方法无论图像中有多少人，都能保持恒定的实时性能。网络通过改进的架构进行单阶段训练，使用多任务学习，能够处理身体/脚部和面部/手部关键点之间的尺度差异。我们的方法在速度和全局准确性方面显著优于迄今为止唯一能够进行全身姿态估计的OpenPose [??]。与OpenPose不同，我们的方法不需要为每个手和面部候选运行额外的网络，使得在多人物场景中速度显著加快。这项工作直接减少了需要2D全身信息的应用（例如，VR/AR，重定向）的计算复杂度。此外，它提供了更高的准确性，特别是对于被遮挡、模糊和低分辨率的面部和手部。有关代码、训练模型和验证基准，请访问我们的项目页面：https://github.com/CMU-Perceptual-Computing-Lab/openpose_train。",
        "领域": "姿态估计/实时处理/多任务学习",
        "问题": "2D全身姿态估计中的关键点定位问题",
        "动机": "提高全身姿态估计的速度和准确性，减少计算复杂度",
        "方法": "采用自下而上的单网络方法，通过改进的架构处理不同关键点之间的尺度差异，使用多任务学习进行单阶段训练",
        "关键词": [
            "姿态估计",
            "实时处理",
            "多任务学习",
            "关键点定位",
            "计算复杂度"
        ],
        "涉及的技术概念": "自下而上的方法指的是从局部到整体的处理方式，多任务学习是指同时学习多个相关任务以提高模型的泛化能力，改进的架构指的是网络结构上的优化以适应不同尺度关键点的处理。"
    },
    {
        "order": 839,
        "title": "Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.html",
        "abstract": "Many vision and language models suffer from poor visual grounding -- often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances -- ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data.",
        "中文标题": "采取提示：利用解释使视觉和语言模型更加接地",
        "摘要翻译": "许多视觉和语言模型存在视觉接地性差的问题——往往依赖于易于学习的语言先验，而不是基于图像中的视觉概念做出决策。在这项工作中，我们提出了一种称为人类重要性感知网络调优（HINT）的通用方法，该方法有效利用人类示范来提高视觉接地性。HINT鼓励深度网络对人类关注的相同输入区域敏感。我们的方法优化了人类注意力图和基于梯度的网络重要性之间的对齐——确保模型不仅学会看，而且在做出预测时依赖于人类认为对任务相关的视觉概念。我们将HINT应用于视觉问答和图像字幕任务，在仅使用6%的训练数据的人类注意力示范的情况下，在惩罚过度依赖语言先验的分割（VQA-CP和鲁棒字幕）上超越了顶级方法。",
        "领域": "视觉问答/图像字幕/视觉接地性",
        "问题": "提高视觉和语言模型的视觉接地性，减少对语言先验的过度依赖",
        "动机": "许多视觉和语言模型在做出决策时过度依赖语言先验，而不是基于图像中的视觉概念，导致视觉接地性差",
        "方法": "提出人类重要性感知网络调优（HINT）方法，通过优化人类注意力图和基于梯度的网络重要性之间的对齐，鼓励模型关注人类认为对任务相关的视觉概念",
        "关键词": [
            "视觉问答",
            "图像字幕",
            "视觉接地性",
            "人类注意力",
            "网络调优"
        ],
        "涉及的技术概念": "视觉接地性指的是模型在做出决策时基于图像中的视觉概念而非语言先验的能力。人类重要性感知网络调优（HINT）是一种方法，通过利用人类示范来优化模型，使其关注与人类相同的视觉区域，从而提高视觉接地性。"
    },
    {
        "order": 840,
        "title": "Live Face De-Identification in Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gafni_Live_Face_De-Identification_in_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gafni_Live_Face_De-Identification_in_Video_ICCV_2019_paper.html",
        "abstract": "We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time.",
        "中文标题": "视频中的实时人脸去识别",
        "摘要翻译": "我们提出了一种人脸去识别方法，能够以高帧率实现全自动视频修改。目标是最大限度地去除身份相关性，同时保持感知（姿势、光照和表情）不变。我们通过一种新颖的前馈编码器-解码器网络架构实现这一点，该架构以人物面部图像的高级表示为条件。该网络是全局的，意味着它不需要为特定视频或特定身份重新训练，并且它创建了看起来自然的图像序列，时间上的失真很小。",
        "领域": "人脸识别/视频处理/隐私保护",
        "问题": "如何在视频中实现实时人脸去识别，同时保持图像的自然性和时间一致性",
        "动机": "为了保护个人隐私，在视频中自动去除或替换人脸信息，同时保持视频的自然流畅",
        "方法": "采用一种新颖的前馈编码器-解码器网络架构，该架构以人物面部图像的高级表示为条件，实现全局的去识别处理",
        "关键词": [
            "人脸去识别",
            "视频修改",
            "隐私保护"
        ],
        "涉及的技术概念": "前馈编码器-解码器网络架构是一种深度学习模型，用于处理和转换图像数据。在这种架构中，编码器部分负责将输入图像转换为一种高级的、紧凑的表示，而解码器部分则负责从这种表示中重建或修改图像。这种方法允许在不重新训练模型的情况下，对不同的视频或身份进行处理，从而实现高效且自然的图像序列生成。"
    },
    {
        "order": 841,
        "title": "Face Alignment With Kernel Density Deep Neural Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Face_Alignment_With_Kernel_Density_Deep_Neural_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Face_Alignment_With_Kernel_Density_Deep_Neural_Network_ICCV_2019_paper.html",
        "abstract": "Deep neural networks achieve good performance in many computer vision problems such as face alignment. However, when the testing image is challenging due to low resolution, occlusion or adversarial attacks, the accuracy of a deep neural network suffers greatly. Therefore, it is important to quantify the uncertainty in its predictions. A probabilistic neural network with Gaussian distribution over the target is typically used to quantify uncertainty for regression problems. However, in real-world problems especially computer vision tasks, the Gaussian assumption is too strong. To model more general distributions, such as multi-modal or asymmetric distributions, we propose to develop a kernel density deep neural network. Specifically, for face alignment, we adapt state-of-the-art hourglass neural network into a probabilistic neural network framework with landmark probability map as its output. The model is trained by maximizing the conditional log likelihood. To exploit the output probability map, we extend the model to multi-stage so that the logits map from the previous stage can feed into the next stage to progressively improve the landmark detection accuracy. Extensive experiments on benchmark datasets against state-of-the-art unconstrained deep learning method demonstrate that the proposed kernel density network achieves comparable or superior performance in terms of prediction accuracy. It further provides aleatoric uncertainty estimation in predictions.",
        "中文标题": "使用核密度深度神经网络进行面部对齐",
        "摘要翻译": "深度神经网络在许多计算机视觉问题中表现出色，例如面部对齐。然而，当测试图像由于低分辨率、遮挡或对抗性攻击而具有挑战性时，深度神经网络的准确性会大大降低。因此，量化其预测中的不确定性非常重要。通常使用具有高斯分布的概率神经网络来量化回归问题的不确定性。然而，在现实世界的问题中，特别是计算机视觉任务中，高斯假设过于强烈。为了建模更一般的分布，如多模态或非对称分布，我们提出开发一种核密度深度神经网络。具体来说，对于面部对齐，我们将最先进的沙漏神经网络适应到概率神经网络框架中，以地标概率图作为其输出。通过最大化条件对数似然来训练模型。为了利用输出概率图，我们将模型扩展到多阶段，以便前一阶段的对数图可以馈入下一阶段，从而逐步提高地标检测的准确性。在基准数据集上对最先进的非约束深度学习方法进行的大量实验表明，所提出的核密度网络在预测准确性方面实现了可比或更优的性能。它进一步提供了预测中的偶然不确定性估计。",
        "领域": "面部对齐/不确定性量化/概率神经网络",
        "问题": "在低分辨率、遮挡或对抗性攻击等挑战性条件下，深度神经网络在面部对齐任务中的准确性下降",
        "动机": "量化深度神经网络预测中的不确定性，特别是在面部对齐等计算机视觉任务中，以提高模型的鲁棒性和准确性",
        "方法": "开发一种核密度深度神经网络，将沙漏神经网络适应到概率神经网络框架中，以地标概率图作为输出，并通过最大化条件对数似然进行训练，同时扩展模型到多阶段以利用输出概率图",
        "关键词": [
            "核密度深度神经网络",
            "面部对齐",
            "不确定性量化",
            "概率神经网络",
            "多阶段模型"
        ],
        "涉及的技术概念": "核密度深度神经网络是一种用于建模更一般分布（如多模态或非对称分布）的神经网络。沙漏神经网络是一种用于面部对齐的先进神经网络结构。概率神经网络框架允许模型输出地标概率图，从而量化预测的不确定性。多阶段模型通过将前一阶段的对数图馈入下一阶段，逐步提高地标检测的准确性。"
    },
    {
        "order": 842,
        "title": "Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Datta_Align2Ground_Weakly_Supervised_Phrase_Grounding_Guided_by_Image-Caption_Alignment_ICCV_2019_paper.html",
        "abstract": "We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a downstream task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In the subsequent step, this learned representation is aligned with the caption. Our key contribution lies in building this \"caption-conditioned\" image encoding, which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% and 1.3% (absolute) over the prior state-of-the-art on the VisualGenome and Flickr30k Entities datasets. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets.",
        "中文标题": "Align2Ground: 基于图像-标题对齐的弱监督短语定位",
        "摘要翻译": "我们解决了通过使用图像-标题对的弱监督来定位自由形式文本短语的问题。我们提出了一种新颖的端到端模型，该模型使用标题到图像检索作为下游任务来指导短语定位过程。我们的方法首先推断出感兴趣区域（RoIs）与标题中短语之间的潜在对应关系，并使用这些匹配的RoIs创建了一个区分性的图像表示。在随后的步骤中，这种学习到的表示与标题对齐。我们的关键贡献在于构建这种“标题条件”图像编码，它紧密耦合了这两个任务，并允许弱监督有效地指导视觉定位。我们提供了广泛的实证和定性分析，以研究我们提出的模型的不同组件，并将其与竞争基线进行比较。对于短语定位，我们在VisualGenome和Flickr30k Entities数据集上报告了比先前最先进技术提高了4.9%和1.3%（绝对值）的改进。我们还报告了在COCO和Flickr30k数据集上的下游标题到图像检索任务中与最先进技术相当的结果。",
        "领域": "视觉定位/图像检索/自然语言处理",
        "问题": "如何通过弱监督从图像-标题对中定位自由形式的文本短语",
        "动机": "提高短语定位的准确性，通过利用图像-标题对的弱监督来指导视觉定位过程",
        "方法": "提出了一种端到端模型，使用标题到图像检索作为下游任务来指导短语定位，通过推断感兴趣区域与标题中短语的潜在对应关系，并创建区分性的图像表示，然后与标题对齐",
        "关键词": [
            "视觉定位",
            "图像检索",
            "自然语言处理"
        ],
        "涉及的技术概念": "感兴趣区域（RoIs）、标题到图像检索、弱监督、视觉定位、图像表示、标题条件图像编码"
    },
    {
        "order": 843,
        "title": "Face Video Deblurring Using 3D Facial Priors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ren_Face_Video_Deblurring_Using_3D_Facial_Priors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ren_Face_Video_Deblurring_Using_3D_Facial_Priors_ICCV_2019_paper.html",
        "abstract": "Existing face deblurring methods only consider single frames and do not account for facial structure and identity information. These methods struggle to deblur face videos that exhibit significant pose variations and misalignment. In this paper we propose a novel face video deblurring network capitalizing on 3D facial priors. The model consists of two main branches: i) a face video deblurring sub-network based on an encoder-decoder architecture, and ii) a 3D face reconstruction and rendering branch for predicting 3D priors of salient facial structures and identity knowledge. These structures encourage the deblurring branch to generate sharp faces with detailed structures. Our method not only uses low-level information (i.e., image intensity), but also middle-level information (i.e., 3D facial structure) and high-level knowledge (i.e., identity content) to further explore spatial constraints of facial components from blurry face frames. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.",
        "中文标题": "使用3D面部先验进行面部视频去模糊",
        "摘要翻译": "现有的面部去模糊方法仅考虑单帧图像，并未考虑面部结构和身份信息。这些方法在处理表现出显著姿态变化和不对齐的面部视频时存在困难。本文提出了一种新颖的面部视频去模糊网络，该网络利用3D面部先验。该模型由两个主要分支组成：i) 基于编码器-解码器架构的面部视频去模糊子网络，和ii) 用于预测显著面部结构和身份知识的3D面部重建和渲染分支。这些结构鼓励去模糊分支生成具有详细结构的清晰面部。我们的方法不仅使用低级信息（即图像强度），还使用中级信息（即3D面部结构）和高级知识（即身份内容）来进一步探索模糊面部帧中面部组件的空间约束。大量实验结果表明，所提出的算法在性能上优于最先进的方法。",
        "领域": "面部去模糊/3D重建/视频处理",
        "问题": "现有面部去模糊方法在处理具有显著姿态变化和不对齐的面部视频时效果不佳",
        "动机": "提高面部视频去模糊的效果，通过利用3D面部先验来考虑面部结构和身份信息",
        "方法": "提出了一种新颖的面部视频去模糊网络，该网络包括一个基于编码器-解码器架构的去模糊子网络和一个用于预测3D面部先验的重建和渲染分支",
        "关键词": [
            "面部去模糊",
            "3D面部先验",
            "视频处理"
        ],
        "涉及的技术概念": "3D面部先验、编码器-解码器架构、面部视频去模糊、3D面部重建和渲染"
    },
    {
        "order": 844,
        "title": "Spatiotemporal Feature Residual Propagation for Action Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Spatiotemporal_Feature_Residual_Propagation_for_Action_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Spatiotemporal_Feature_Residual_Propagation_for_Action_Prediction_ICCV_2019_paper.html",
        "abstract": "Recognizing actions from limited preliminary video observations has seen considerable recent progress. Typically, however, such progress has been had without explicitly modeling fine-grained motion evolution as a potentially valuable information source. In this study, we address this task by investigating how action patterns evolve over time in a spatial feature space. There are three key components to our system. First, we work with intermediate-layer ConvNet features, which allow for abstraction from raw data, while retaining spatial layout, which is sacrificed in approaches that rely on vectorized global representations. Second, instead of propagating features per se, we propagate their residuals across time, which allows for a compact representation that reduces redundancy while retaining essential information about evolution over time. Third, we employ a Kalman filter to combat error build-up and unify across prediction start times. Extensive experimental results on the JHMDB21, UCF101 and BIT datasets show that our approach leads to a new state-of-the-art in action prediction.",
        "中文标题": "时空特征残差传播用于动作预测",
        "摘要翻译": "从有限的初步视频观察中识别动作已经取得了相当大的进展。然而，通常这种进展并没有明确地将细粒度运动演化建模为潜在的有价值的信息源。在本研究中，我们通过研究动作模式在空间特征空间中如何随时间演化来解决这一任务。我们的系统有三个关键组成部分。首先，我们使用中间层卷积网络特征，这些特征允许从原始数据中抽象出来，同时保留空间布局，这在依赖向量化全局表示的方法中被牺牲了。其次，我们不是传播特征本身，而是传播它们的残差，这允许一个紧凑的表示，减少了冗余，同时保留了关于时间演化的基本信息。第三，我们采用卡尔曼滤波器来对抗误差积累，并在预测开始时间上统一。在JHMDB21、UCF101和BIT数据集上的大量实验结果表明，我们的方法在动作预测方面达到了新的最先进水平。",
        "领域": "动作识别/视频分析/时空特征学习",
        "问题": "如何从有限的初步视频观察中准确预测动作",
        "动机": "现有方法未充分利用细粒度运动演化作为有价值的信息源",
        "方法": "使用中间层卷积网络特征，传播特征残差，并采用卡尔曼滤波器对抗误差积累",
        "关键词": [
            "动作预测",
            "时空特征",
            "残差传播",
            "卡尔曼滤波器"
        ],
        "涉及的技术概念": "中间层卷积网络特征：从原始数据中抽象出来，同时保留空间布局的特征。残差传播：传播特征的变化量，以减少冗余并保留时间演化的信息。卡尔曼滤波器：用于估计动态系统的状态，减少预测误差。"
    },
    {
        "order": 845,
        "title": "Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_Expression_Grounding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_Expression_Grounding_ICCV_2019_paper.html",
        "abstract": "Weakly supervised referring expression grounding aims at localizing the referential object in an image according to the linguistic query, where the mapping between the referential object and query is unknown in the training stage. To address this problem, we propose a novel end-to-end adaptive reconstruction network (ARN). It builds the correspondence between image region proposal and query in an adaptive manner: adaptive grounding and collaborative reconstruction. Specifically, we first extract the subject, location and context features to represent the proposals and the query respectively. Then, we design the adaptive grounding module to compute the matching score between each proposal and query by a hierarchical attention model. Finally, based on attention score and proposal features, we reconstruct the input query with a collaborative loss of language reconstruction loss, adaptive reconstruction loss, and attribute classification loss. This adaptive mechanism helps our model to alleviate the variance of different referring expressions. Experiments on four large-scale datasets show ARN outperforms existing state-of-the-art methods by a large margin. Qualitative results demonstrate that the proposed ARN can better handle the situation where multiple objects of a particular category situated together.",
        "中文标题": "自适应重建网络用于弱监督指代表达定位",
        "摘要翻译": "弱监督指代表达定位旨在根据语言查询定位图像中的参考对象，其中在训练阶段参考对象与查询之间的映射是未知的。为了解决这个问题，我们提出了一种新颖的端到端自适应重建网络（ARN）。它以自适应方式建立图像区域提议与查询之间的对应关系：自适应定位和协作重建。具体来说，我们首先提取主题、位置和上下文特征来分别表示提议和查询。然后，我们设计了自适应定位模块，通过分层注意力模型计算每个提议与查询之间的匹配分数。最后，基于注意力分数和提议特征，我们通过语言重建损失、自适应重建损失和属性分类损失的协作损失重建输入查询。这种自适应机制帮助我们的模型减轻了不同指代表达的差异。在四个大规模数据集上的实验表明，ARN大大优于现有的最先进方法。定性结果表明，所提出的ARN可以更好地处理特定类别的多个对象位于一起的情况。",
        "领域": "指代表达定位/图像理解/自然语言处理",
        "问题": "在训练阶段参考对象与查询之间的映射未知的情况下，如何根据语言查询定位图像中的参考对象",
        "动机": "解决弱监督指代表达定位问题，即在不明确知道参考对象与查询之间映射的情况下，通过语言查询定位图像中的参考对象",
        "方法": "提出了一种端到端自适应重建网络（ARN），通过自适应定位和协作重建建立图像区域提议与查询之间的对应关系，包括提取特征、设计自适应定位模块计算匹配分数，以及基于注意力分数和提议特征重建输入查询",
        "关键词": [
            "指代表达定位",
            "自适应重建网络",
            "分层注意力模型"
        ],
        "涉及的技术概念": "自适应重建网络（ARN）是一种端到端的网络，用于弱监督指代表达定位。它通过自适应定位和协作重建建立图像区域提议与查询之间的对应关系。自适应定位模块使用分层注意力模型计算匹配分数，而协作重建则涉及语言重建损失、自适应重建损失和属性分类损失。这种方法旨在减轻不同指代表达的差异，提高定位的准确性。"
    },
    {
        "order": 846,
        "title": "Semi-Supervised Monocular 3D Face Reconstruction With End-to-End Shape-Preserved Domain Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Semi-Supervised_Monocular_3D_Face_Reconstruction_With_End-to-End_Shape-Preserved_Domain_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Piao_Semi-Supervised_Monocular_3D_Face_Reconstruction_With_End-to-End_Shape-Preserved_Domain_Transfer_ICCV_2019_paper.html",
        "abstract": "Monocular face reconstruction is a challenging task in computer vision, which aims to recover 3D face geometry from a single RGB face image. Recently, deep learning based methods have achieved great improvements on monocular face reconstruction. However, for deep learning-based methods to reach optimal performance, it is paramount to have large-scale training images with ground-truth 3D face geometry, which is generally difficult for human to annotate. To tackle this problem, we propose a semi-supervised monocular reconstruction method, which jointly optimizes a shape-preserved domain-transfer CycleGAN and a shape estimation network. The framework is semi-supervised trained with 3D rendered images with ground-truth shapes and in-the-wild face images without any extra annotation. The CycleGAN network transforms all realistic images to have the rendered style and is end-to-end trained within the overall framework. This is the key difference compared with existing CycleGAN-based learning methods, which just used CycleGAN as a separate training sample generator. Novel landmark consistency loss and edge-aware shape estimation loss are proposed for our two networks to jointly solve the challenging face reconstruction problem. Extensive experiments on public face reconstruction datasets demonstrate the effectiveness of our overall method as well as the individual components.",
        "中文标题": "半监督单目3D人脸重建与端到端形状保持域转移",
        "摘要翻译": "单目人脸重建是计算机视觉中的一项挑战性任务，旨在从单个RGB人脸图像中恢复3D人脸几何。最近，基于深度学习的方法在单目人脸重建方面取得了巨大进步。然而，为了使基于深度学习的方法达到最佳性能，拥有带有真实3D人脸几何的大规模训练图像至关重要，这对于人类来说通常难以标注。为了解决这个问题，我们提出了一种半监督单目重建方法，该方法联合优化了一个形状保持域转移的CycleGAN和一个形状估计网络。该框架通过带有真实形状的3D渲染图像和没有任何额外标注的野外人脸图像进行半监督训练。CycleGAN网络将所有真实图像转换为具有渲染风格，并在整个框架内进行端到端训练。这是与现有的基于CycleGAN的学习方法的关键区别，后者仅将CycleGAN用作单独的训练样本生成器。我们为两个网络提出了新的地标一致性损失和边缘感知形状估计损失，以共同解决具有挑战性的人脸重建问题。在公共人脸重建数据集上的大量实验证明了我们整体方法以及各个组成部分的有效性。",
        "领域": "3D人脸重建/半监督学习/域适应",
        "问题": "从单个RGB人脸图像中恢复3D人脸几何",
        "动机": "解决大规模训练图像标注困难的问题，提高单目人脸重建的准确性和效率",
        "方法": "提出了一种半监督单目重建方法，联合优化形状保持域转移的CycleGAN和形状估计网络，通过3D渲染图像和野外人脸图像进行训练",
        "关键词": [
            "3D人脸重建",
            "半监督学习",
            "域适应",
            "CycleGAN",
            "形状估计"
        ],
        "涉及的技术概念": {
            "CycleGAN": "一种用于图像到图像转换的生成对抗网络，能够实现两个不同域之间的图像风格转换",
            "半监督学习": "一种机器学习方法，它使用少量标注数据和大量未标注数据进行训练",
            "域适应": "一种技术，旨在使模型能够将在源域上学到的知识应用到目标域上，即使两个域的分布不同",
            "形状估计网络": "一种用于从图像中估计物体形状的神经网络",
            "地标一致性损失": "一种损失函数，用于确保重建的3D人脸与输入图像中的关键点位置一致",
            "边缘感知形状估计损失": "一种损失函数，用于在形状估计过程中考虑图像的边缘信息，以提高重建的准确性"
        }
    },
    {
        "order": 847,
        "title": "Identity From Here, Pose From There: Self-Supervised Disentanglement and Generation of Objects Using Unlabeled Videos",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiao_Identity_From_Here_Pose_From_There_Self-Supervised_Disentanglement_and_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xiao_Identity_From_Here_Pose_From_There_Self-Supervised_Disentanglement_and_Generation_ICCV_2019_paper.html",
        "abstract": "We propose a novel approach that disentangles the identity and pose of objects for image generation. Our model takes as input an ID image and a pose image, and generates an output image with the identity of the ID image and the pose of the pose image. Unlike most previous unsupervised work which rely on cyclic constraints, which can often be brittle, we instead propose to learn this in a self-supervised way. Specifically, we leverage unlabeled videos to automatically construct pseudo ground-truth targets to directly supervise our model. To enforce disentanglement, we propose a novel disentanglement loss, and to improve realism, we propose a pixel-verification loss in which the generated image's pixels must trace back to the ID input. We conduct extensive experiments on both synthetic and real images to demonstrate improved realism, diversity, and ID/pose disentanglement compared to existing methods.",
        "中文标题": "身份从此处，姿态从彼处：利用未标记视频进行对象的自监督解耦与生成",
        "摘要翻译": "我们提出了一种新颖的方法，用于解耦对象的身份和姿态以进行图像生成。我们的模型以身份图像和姿态图像作为输入，并生成一个输出图像，该图像具有身份图像的身份和姿态图像的姿态。与大多数以前依赖循环约束的无监督工作不同，我们提出以自监督的方式学习这一点。具体来说，我们利用未标记的视频自动构建伪地面真实目标，直接监督我们的模型。为了强制解耦，我们提出了一种新颖的解耦损失，并为了提高真实感，我们提出了一种像素验证损失，其中生成的图像的像素必须追溯到身份输入。我们在合成图像和真实图像上进行了广泛的实验，以证明与现有方法相比，我们的方法在真实感、多样性和身份/姿态解耦方面有所改进。",
        "领域": "图像生成/自监督学习/视频分析",
        "问题": "如何有效地解耦对象的身份和姿态以进行图像生成",
        "动机": "现有的无监督方法主要依赖循环约束，这些方法往往脆弱，因此需要一种更稳健的方法来解耦对象的身份和姿态",
        "方法": "利用未标记的视频自动构建伪地面真实目标，直接监督模型，并提出解耦损失和像素验证损失以提高解耦效果和生成图像的真实感",
        "关键词": [
            "图像生成",
            "自监督学习",
            "视频分析",
            "解耦",
            "像素验证"
        ],
        "涉及的技术概念": "自监督学习是一种不依赖标注数据的学习方法，通过设计特定的任务让模型自我学习。解耦是指将对象的身份和姿态等特征分离，以便于单独控制。像素验证损失是一种确保生成图像的像素与输入图像相对应的技术，用于提高生成图像的真实感。"
    },
    {
        "order": 848,
        "title": "Hierarchy Parsing for Image Captioning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.html",
        "abstract": "It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",
        "中文标题": "图像描述中的层次解析",
        "摘要翻译": "人们一直相信，将图像解析为构成视觉模式有助于理解和表示图像。然而，对于用自然语言描述图像这一想法，尚未有证据支持。在本文中，我们引入了一种新的设计，从实例级别（分割）、区域级别（检测）到整个图像建模层次结构，以深入进行图像理解以进行描述。具体来说，我们提出了一种层次解析（HIP）架构，该架构新颖地将层次结构集成到图像编码器中。技术上，图像被分解为一组区域，其中一些区域被解析为更细的区域。然后，每个区域回归到一个实例，即区域的前景。这样的过程自然地构建了一个层次树。然后，采用树结构的长短期记忆（Tree-LSTM）网络来解释层次结构并增强所有实例级别、区域级别和图像级别的特征。我们的HIP在可插入任何神经描述模型方面具有吸引力。在COCO图像描述数据集上的大量实验证明了HIP的优越性。更值得注意的是，HIP加上基于自上而下注意力的LSTM解码器，将COCO Karpathy测试分割的CIDEr-D性能从120.1%提高到127.2%。当进一步通过图卷积网络（GCN）学习到的语义关系赋予HIP的实例级别和区域级别特征时，CIDEr-D提升至130.6%。",
        "领域": "图像描述/层次解析/自然语言生成",
        "问题": "如何通过层次解析提高图像描述的准确性和丰富性",
        "动机": "探索将图像解析为视觉模式层次结构以增强图像理解和描述的自然语言表达",
        "方法": "提出了一种层次解析（HIP）架构，该架构将层次结构集成到图像编码器中，并使用树结构的长短期记忆（Tree-LSTM）网络来解释层次结构并增强特征",
        "关键词": [
            "层次解析",
            "图像描述",
            "Tree-LSTM",
            "图卷积网络"
        ],
        "涉及的技术概念": "层次解析（HIP）架构、树结构的长短期记忆（Tree-LSTM）网络、图卷积网络（GCN）"
    },
    {
        "order": 849,
        "title": "3D Face Modeling From Diverse Raw Scan Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_3D_Face_Modeling_From_Diverse_Raw_Scan_Data_ICCV_2019_paper.html",
        "abstract": "Traditional 3D face models learn a latent representation of faces using linear subspaces from limited scans of a single database. The main roadblock of building a large-scale face model from diverse 3D databases lies in the lack of dense correspondence among raw scans. To address these problems, this paper proposes an innovative framework to jointly learn a nonlinear face model from a diverse set of raw 3D scan databases and establish dense point-to-point correspondence among their scans. Specifically, by treating input scans as unorganized point clouds, we explore the use of PointNet architectures for converting point clouds to identity and expression feature representations, from which the decoder networks recover their 3D face shapes. Further, we propose a weakly supervised learning approach that does not require correspondence label for the scans. We demonstrate the superior dense correspondence and representation power of our proposed method, and its contribution to single-image 3D face reconstruction.",
        "中文标题": "从多样化的原始扫描数据中进行3D面部建模",
        "摘要翻译": "传统的3D面部模型使用线性子空间从单一数据库的有限扫描中学习面部的潜在表示。从多样化的3D数据库中构建大规模面部模型的主要障碍在于原始扫描之间缺乏密集的对应关系。为了解决这些问题，本文提出了一个创新框架，共同学习来自多样化原始3D扫描数据库的非线性面部模型，并在它们的扫描之间建立密集的点对点对应关系。具体来说，通过将输入扫描视为无组织的点云，我们探索了使用PointNet架构将点云转换为身份和表情特征表示，解码器网络从中恢复它们的3D面部形状。此外，我们提出了一种弱监督学习方法，该方法不需要扫描的对应标签。我们展示了我们提出的方法在密集对应和表示能力上的优越性，以及它对单图像3D面部重建的贡献。",
        "领域": "3D面部建模/点云处理/弱监督学习",
        "问题": "从多样化的3D数据库中构建大规模面部模型的主要障碍在于原始扫描之间缺乏密集的对应关系",
        "动机": "解决传统3D面部模型在从单一数据库的有限扫描中学习面部潜在表示的局限性，以及从多样化3D数据库中构建大规模面部模型的挑战",
        "方法": "提出一个创新框架，共同学习来自多样化原始3D扫描数据库的非线性面部模型，并在它们的扫描之间建立密集的点对点对应关系。使用PointNet架构将点云转换为身份和表情特征表示，并提出一种弱监督学习方法",
        "关键词": [
            "3D面部建模",
            "点云处理",
            "弱监督学习"
        ],
        "涉及的技术概念": "PointNet架构用于将无组织的点云转换为身份和表情特征表示，解码器网络用于从这些特征表示中恢复3D面部形状，弱监督学习方法不需要扫描的对应标签"
    },
    {
        "order": 850,
        "title": "Relation Distillation Networks for Video Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Relation_Distillation_Networks_for_Video_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Relation_Distillation_Networks_for_Video_Object_Detection_ICCV_2019_paper.html",
        "abstract": "It has been well recognized that modeling object-to-object relations would be helpful for object detection. Nevertheless, the problem is not trivial especially when exploring the interactions between objects to boost video object detectors. The difficulty originates from the aspect that reliable object relations in a video should depend on not only the objects in the present frame but also all the supportive objects extracted over a long range span of the video. In this paper, we introduce a new design to capture the interactions across the objects in spatio-temporal context. Specifically, we present Relation Distillation Networks (RDN) --- a new architecture that novelly aggregates and propagates object relation to augment object features for detection. Technically, object proposals are first generated via Region Proposal Networks (RPN). RDN then, on one hand, models object relation via multi-stage reasoning, and on the other, progressively distills relation through refining supportive object proposals with high objectness scores in a cascaded manner. The learnt relation verifies the efficacy on both improving object detection in each frame and box linking across frames. Extensive experiments are conducted on ImageNet VID dataset, and superior results are reported when comparing to state-of-the-art methods. More remarkably, our RDN achieves 81.8% and 83.2% mAP with ResNet-101 and ResNeXt-101, respectively. When further equipped with linking and rescoring, we obtain to-date the best reported mAP of 83.8% and 84.7%.",
        "中文标题": "关系蒸馏网络用于视频目标检测",
        "摘要翻译": "人们普遍认为，建模对象与对象之间的关系有助于目标检测。然而，这个问题并不简单，尤其是在探索对象之间的相互作用以提升视频目标检测器时。困难源于视频中可靠的对象关系不仅应依赖于当前帧中的对象，还应依赖于视频长时间跨度内提取的所有支持对象。在本文中，我们引入了一种新设计来捕捉时空背景中对象之间的相互作用。具体来说，我们提出了关系蒸馏网络（RDN）——一种新颖地聚合和传播对象关系以增强检测对象特征的新架构。技术上，首先通过区域提议网络（RPN）生成对象提议。然后，RDN一方面通过多阶段推理建模对象关系，另一方面通过级联方式精炼具有高对象性分数的支持对象提议来逐步蒸馏关系。学习到的关系验证了在提高每帧中的目标检测和跨帧的框链接方面的有效性。在ImageNet VID数据集上进行了广泛的实验，与最先进的方法相比，报告了优越的结果。更值得注意的是，我们的RDN在使用ResNet-101和ResNeXt-101时分别达到了81.8%和83.2%的mAP。当进一步配备链接和重新评分时，我们获得了迄今为止最佳报告的mAP，分别为83.8%和84.7%。",
        "领域": "视频目标检测/对象关系建模/时空背景分析",
        "问题": "如何在视频中有效地建模对象与对象之间的关系以提升目标检测性能",
        "动机": "探索对象之间的相互作用以提升视频目标检测器的性能",
        "方法": "提出关系蒸馏网络（RDN），通过多阶段推理建模对象关系，并通过级联方式精炼支持对象提议来逐步蒸馏关系",
        "关键词": [
            "视频目标检测",
            "对象关系建模",
            "时空背景分析",
            "关系蒸馏网络",
            "区域提议网络"
        ],
        "涉及的技术概念": {
            "关系蒸馏网络（RDN）": "一种新颖地聚合和传播对象关系以增强检测对象特征的新架构",
            "区域提议网络（RPN）": "用于生成对象提议的网络",
            "多阶段推理": "一种通过多个阶段逐步推理来建模对象关系的方法",
            "级联方式": "一种逐步精炼支持对象提议的方法",
            "mAP": "平均精度均值，用于评估目标检测性能的指标"
        }
    },
    {
        "order": 851,
        "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Miech_HowTo100M_Learning_a_Text-Video_Embedding_by_Watching_Hundred_Million_Narrated_ICCV_2019_paper.html",
        "abstract": "Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available.",
        "中文标题": "HowTo100M：通过观看一亿个带旁白的视频片段学习文本-视频嵌入",
        "摘要翻译": "学习文本-视频嵌入通常需要一个带有手动提供字幕的视频片段数据集。然而，这样的数据集创建起来既昂贵又耗时，因此难以大规模获取。在这项工作中，我们提出从带有现成自然语言注释的视频数据中学习这种嵌入，这些注释以自动转录的旁白形式存在。这项工作的贡献有三方面。首先，我们介绍了HowTo100M：一个包含1.36亿个视频片段的大规模数据集，这些片段来源于122万个带旁白的教学网络视频，描绘了人类执行和描述超过23,000种不同的视觉任务。我们的数据收集过程快速、可扩展，并且不需要任何额外的手动注释。其次，我们证明了在这种数据上训练的文本-视频嵌入在YouCook2或CrossTask等教学视频数据集上的文本到视频检索和动作定位方面达到了最先进的结果。最后，我们展示了这种嵌入能够很好地转移到其他领域：在通用YouTube视频（MSR-VTT数据集）和电影（LSMDC数据集）上的微调表现优于仅在这些数据集上训练的模型。我们的数据集、代码和模型都是公开可用的。",
        "领域": "视频理解/文本-视频嵌入/动作识别",
        "问题": "如何有效地学习文本-视频嵌入，以改进文本到视频检索和动作定位的性能",
        "动机": "由于手动注释的视频数据集创建成本高且耗时，难以大规模获取，因此需要一种更高效的方法来学习文本-视频嵌入",
        "方法": "利用自动转录的旁白作为自然语言注释，从大规模的教学网络视频中学习文本-视频嵌入，并在多个视频数据集上验证其有效性",
        "关键词": [
            "文本-视频嵌入",
            "视频检索",
            "动作定位"
        ],
        "涉及的技术概念": {
            "文本-视频嵌入": "一种将文本和视频内容映射到同一嵌入空间的技术，以便于文本到视频的检索和视频内容的分析",
            "自动转录的旁白": "通过语音识别技术将视频中的语音内容自动转换为文本，作为视频的自然语言注释",
            "动作定位": "在视频中识别和定位特定动作或活动的过程"
        }
    },
    {
        "order": 852,
        "title": "A Decoupled 3D Facial Shape Model by Adversarial Training",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Abrevaya_A_Decoupled_3D_Facial_Shape_Model_by_Adversarial_Training_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Abrevaya_A_Decoupled_3D_Facial_Shape_Model_by_Adversarial_Training_ICCV_2019_paper.html",
        "abstract": "Data-driven generative 3D face models are used to compactly encode facial shape data into meaningful parametric representations. A desirable property of these models is their ability to effectively decouple natural sources of variation, in particular identity and expression. While factorized representations have been proposed for that purpose, they are still limited in the variability they can capture and may present modeling artifacts when applied to tasks such as expression transfer. In this work, we explore a new direction with Generative Adversarial Networks and show that they contribute to better face modeling performances, especially in decoupling natural factors, while also achieving more diverse samples. To train the model we introduce a novel architecture that combines a 3D generator with a 2D discriminator that leverages conventional CNNs, where the two components are bridged by a geometry mapping layer. We further present a training scheme, based on auxiliary classifiers, to explicitly disentangle identity and expression attributes. Through quantitative and qualitative results on standard face datasets, we illustrate the benefits of our model and demonstrate that it outperforms competing state of the art methods in terms of decoupling and diversity.",
        "中文标题": "通过对抗训练解耦的3D面部形状模型",
        "摘要翻译": "数据驱动的生成式3D面部模型用于将面部形状数据紧凑地编码为有意义的参数表示。这些模型的一个理想特性是它们能够有效地解耦自然变化的来源，特别是身份和表情。虽然已经提出了因子化表示来实现这一目的，但它们在捕捉变异性方面仍然有限，并且在应用于表情转移等任务时可能会出现建模伪影。在这项工作中，我们探索了生成对抗网络（GANs）的新方向，并展示了它们有助于提高面部建模性能，特别是在解耦自然因素方面，同时也实现了更多样化的样本。为了训练模型，我们引入了一种新颖的架构，该架构结合了3D生成器和利用传统卷积神经网络（CNNs）的2D判别器，其中两个组件通过几何映射层连接。我们进一步提出了一种基于辅助分类器的训练方案，以明确解耦身份和表情属性。通过在标准面部数据集上的定量和定性结果，我们展示了我们模型的优势，并证明它在解耦和多样性方面优于竞争的最先进方法。",
        "领域": "面部建模/生成对抗网络/3D形状分析",
        "问题": "如何有效地解耦面部形状数据中的身份和表情因素",
        "动机": "现有的因子化表示在捕捉面部形状数据的变异性方面存在限制，且在表情转移等任务中可能出现建模伪影",
        "方法": "引入结合3D生成器和2D判别器的新颖架构，通过几何映射层连接，并采用基于辅助分类器的训练方案来明确解耦身份和表情属性",
        "关键词": [
            "面部建模",
            "生成对抗网络",
            "3D形状分析",
            "表情转移",
            "身份解耦"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）用于提高面部建模性能，特别是解耦自然因素；3D生成器和2D判别器结合，通过几何映射层连接；辅助分类器用于明确解耦身份和表情属性。"
    },
    {
        "order": 853,
        "title": "Video Compression With Rate-Distortion Autoencoders",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Habibian_Video_Compression_With_Rate-Distortion_Autoencoders_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Habibian_Video_Compression_With_Rate-Distortion_Autoencoders_ICCV_2019_paper.html",
        "abstract": "In this paper we present a a deep generative model for lossy video compression. We employ a model that consists of a 3D autoencoder with a discrete latent space and an autoregressive prior used for entropy coding. Both autoencoder and prior are trained jointly to minimize a rate-distortion loss, which is closely related to the ELBO used in variational autoencoders. Despite its simplicity, we find that our method outperforms the state-of-the-art learned video compression networks based on motion compensation or interpolation. We systematically evaluate various design choices, such as the use of frame-based or spatio-temporal autoencoders, and the type of autoregressive prior. In addition, we present three extensions of the basic method that demonstrate the benefits over classical approaches to compression. First, we introduce semantic compression, where the model is trained to allocate more bits to objects of interest. Second, we study adaptive compression, where the model is adapted to a domain with limited variability, e.g. videos taken from an autonomous car, to achieve superior compression on that domain. Finally, we introduce multimodal compression, where we demonstrate the effectiveness of our model in joint compression of multiple modalities captured by non-standard imaging sensors, such as quad cameras. We believe that this opens up novel video compression applications, which have not been feasible with classical codecs.",
        "中文标题": "使用率失真自编码器进行视频压缩",
        "摘要翻译": "本文提出了一种用于有损视频压缩的深度生成模型。我们采用了一个由具有离散潜在空间的三维自编码器和用于熵编码的自回归先验组成的模型。自编码器和先验共同训练，以最小化与变分自编码器中使用的ELBO密切相关的率失真损失。尽管方法简单，但我们发现我们的方法优于基于运动补偿或插值的最先进的视频压缩网络。我们系统地评估了各种设计选择，如使用基于帧的或时空自编码器，以及自回归先验的类型。此外，我们提出了基本方法的三个扩展，展示了相对于传统压缩方法的优势。首先，我们引入了语义压缩，其中模型被训练为对感兴趣的对象分配更多比特。其次，我们研究了自适应压缩，其中模型适应于变异性有限的领域，例如从自动驾驶汽车拍摄的视频，以在该领域实现卓越的压缩。最后，我们引入了多模态压缩，展示了我们的模型在联合压缩由非标准成像传感器（如四摄像头）捕获的多种模态方面的有效性。我们相信这开辟了新的视频压缩应用，这些应用在传统编解码器中是不可行的。",
        "领域": "视频压缩/生成模型/熵编码",
        "问题": "有损视频压缩",
        "动机": "探索深度生成模型在视频压缩中的应用，以超越基于运动补偿或插值的现有技术",
        "方法": "采用具有离散潜在空间的三维自编码器和自回归先验的模型，共同训练以最小化率失真损失",
        "关键词": [
            "视频压缩",
            "深度生成模型",
            "自编码器",
            "熵编码",
            "语义压缩",
            "自适应压缩",
            "多模态压缩"
        ],
        "涉及的技术概念": {
            "3D自编码器": "一种能够处理三维数据（如视频帧）的自编码器，用于特征提取和数据压缩",
            "离散潜在空间": "自编码器中的潜在表示被离散化，有助于熵编码和压缩效率",
            "自回归先验": "用于熵编码的先验模型，能够根据前面的数据预测当前数据的概率分布",
            "率失真损失": "衡量压缩效率和重建质量之间权衡的损失函数，与变分自编码器中的ELBO密切相关",
            "语义压缩": "一种压缩策略，通过分配更多比特给感兴趣的对象来提高压缩效率",
            "自适应压缩": "根据特定领域（如自动驾驶汽车视频）的特性调整压缩模型，以提高压缩效率",
            "多模态压缩": "联合压缩由多种传感器捕获的不同类型数据（如视频和深度信息）的方法"
        }
    },
    {
        "order": 854,
        "title": "Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Controllable_Video_Captioning_With_POS_Sequence_Guidance_Based_on_Gated_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose to guide the video caption generation with Part-of-Speech (POS) information, based on a gated fusion of multiple representations of input videos. We construct a novel gated fusion network, with one particularly designed cross-gating (CG) block, to effectively encode and fuse different types of representations, e.g., the motion and content features of an input video. One POS sequence generator relies on this fused representation to predict the global syntactic structure, which is thereafter leveraged to guide the video captioning generation and control the syntax of the generated sentence. Specifically, a gating strategy is proposed to dynamically and adaptively incorporate the global syntactic POS information into the decoder for generating each word. Experimental results on two benchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed model can well exploit complementary information from multiple representations, resulting in improved performances. Moreover, the generated global POS information can well capture the global syntactic structure of the sentence, and thus be exploited to control the syntactic structure of the description. Such POS information not only boosts the video captioning performance but also improves the diversity of the generated captions. Our code is at: https://github.com/vsislab/Controllable_XGating.",
        "中文标题": "基于门控融合网络的POS序列引导可控视频字幕生成",
        "摘要翻译": "本文提出了一种基于输入视频的多种表示的门控融合，利用词性（POS）信息来指导视频字幕的生成。我们构建了一个新颖的门控融合网络，其中特别设计了一个交叉门控（CG）块，以有效地编码和融合不同类型的表示，例如输入视频的运动和内容特征。一个POS序列生成器依赖于这种融合表示来预测全局句法结构，随后利用该结构来指导视频字幕的生成并控制生成句子的句法。具体来说，提出了一种门控策略，以动态和自适应的方式将全局句法POS信息纳入解码器，以生成每个单词。在两个基准数据集（即MSR-VTT和MSVD）上的实验结果表明，所提出的模型能够很好地利用来自多种表示的互补信息，从而提高了性能。此外，生成的全局POS信息能够很好地捕捉句子的全局句法结构，因此可以用来控制描述的句法结构。这种POS信息不仅提高了视频字幕的性能，还提高了生成字幕的多样性。我们的代码位于：https://github.com/vsislab/Controllable_XGating。",
        "领域": "视频字幕生成/自然语言处理/深度学习",
        "问题": "如何有效地利用视频的多种表示来生成具有控制句法结构的视频字幕",
        "动机": "提高视频字幕生成的性能和质量，同时增加生成字幕的多样性",
        "方法": "构建了一个门控融合网络，特别设计了一个交叉门控（CG）块来编码和融合视频的多种表示，利用POS序列生成器预测全局句法结构，并通过门控策略动态地将全局句法POS信息纳入解码器",
        "关键词": [
            "视频字幕生成",
            "门控融合网络",
            "POS序列",
            "交叉门控",
            "句法结构"
        ],
        "涉及的技术概念": "门控融合网络是一种用于融合多种输入表示的神经网络结构，通过门控机制动态调整信息流。交叉门控（CG）块是门控融合网络中的一个组件，用于有效地编码和融合不同类型的表示。POS序列生成器是一种基于融合表示预测全局句法结构的模型。门控策略是一种动态和自适应地将全局句法POS信息纳入解码器的方法，用于生成每个单词。"
    },
    {
        "order": 855,
        "title": "Photo-Realistic Facial Details Synthesis From Single Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Photo-Realistic_Facial_Details_Synthesis_From_Single_Image_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Photo-Realistic_Facial_Details_Synthesis_From_Single_Image_ICCV_2019_paper.html",
        "abstract": "We present a single-image 3D face synthesis technique that can handle challenging facial expressions while recovering fine geometric details. Our technique employs expression analysis for proxy face geometry generation and combines supervised and unsupervised learning for facial detail synthesis. On proxy generation, we conduct emotion prediction to determine a new expression-informed proxy. On detail synthesis, we present a Deep Facial Detail Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs both geometry and appearance loss functions. For geometry, we capture 366 high-quality 3D scans from 122 different subjects under 3 facial expressions. For appearance, we use additional 163K in-the-wild face images and apply image-based rendering to accommodate lighting variations. Comprehensive experiments demonstrate that our framework can produce high-quality 3D faces with realistic details under challenging facial expressions.",
        "中文标题": "从单张图像合成照片级真实感面部细节",
        "摘要翻译": "我们提出了一种单图像3D面部合成技术，该技术能够处理具有挑战性的面部表情，同时恢复精细的几何细节。我们的技术采用表情分析来生成代理面部几何，并结合监督和无监督学习进行面部细节合成。在代理生成方面，我们进行情绪预测以确定一个新的表情信息代理。在细节合成方面，我们提出了一个基于条件生成对抗网络（CGAN）的深度面部细节网络（DFDN），该网络采用了几何和外观损失函数。对于几何，我们从122个不同对象在3种面部表情下捕获了366个高质量的3D扫描。对于外观，我们使用了额外的163K野外面部图像，并应用基于图像的渲染以适应光照变化。综合实验表明，我们的框架能够在具有挑战性的面部表情下生成具有真实细节的高质量3D面部。",
        "领域": "3D面部重建/表情分析/图像合成",
        "问题": "从单张图像合成具有挑战性面部表情的高质量3D面部细节",
        "动机": "为了在具有挑战性的面部表情下恢复和合成精细的几何细节，提高3D面部重建的真实感和细节表现",
        "方法": "采用表情分析生成代理面部几何，结合监督和无监督学习进行面部细节合成，使用深度面部细节网络（DFDN）基于条件生成对抗网络（CGAN），并采用几何和外观损失函数",
        "关键词": [
            "3D面部重建",
            "表情分析",
            "图像合成"
        ],
        "涉及的技术概念": {
            "表情分析": "用于生成代理面部几何的技术",
            "监督和无监督学习": "用于面部细节合成的学习方法",
            "深度面部细节网络（DFDN）": "基于条件生成对抗网络（CGAN）的网络，用于面部细节合成",
            "条件生成对抗网络（CGAN）": "一种生成对抗网络，用于在给定条件下生成数据",
            "几何和外观损失函数": "用于训练网络以优化几何和外观细节的损失函数",
            "图像基于渲染": "用于适应光照变化的技术"
        }
    },
    {
        "order": 856,
        "title": "Non-Local ConvLSTM for Video Compression Artifact Reduction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Non-Local_ConvLSTM_for_Video_Compression_Artifact_Reduction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Non-Local_ConvLSTM_for_Video_Compression_Artifact_Reduction_ICCV_2019_paper.html",
        "abstract": "Video compression artifact reduction aims to recover high-quality videos from low-quality compressed videos. Most existing approaches use a single neighboring frame or a pair of neighboring frames (preceding and/or following the target frame) for this task. Furthermore, as frames of high quality overall may contain low-quality patches, and high-quality patches may exist in frames of low quality overall, current methods focusing on nearby peak-quality frames (PQFs) may miss high-quality details in low-quality frames. To remedy these shortcomings, in this paper we propose a novel end-to-end deep neural network called non-local ConvLSTM (NL-ConvLSTM in short) that exploits multiple consecutive frames. An approximate non-local strategy is introduced in NL-ConvLSTM to capture global motion patterns and trace the spatiotemporal dependency in a video sequence. This approximate strategy makes the non-local module work in a fast and low space-cost way. Our method uses the preceding and following frames of the target frame to generate a residual, from which a higher quality frame is reconstructed. Experiments on two datasets show that NL-ConvLSTM outperforms the existing methods.",
        "中文标题": "非局部ConvLSTM用于视频压缩伪影减少",
        "摘要翻译": "视频压缩伪影减少旨在从低质量压缩视频中恢复高质量视频。大多数现有方法使用单个相邻帧或一对相邻帧（目标帧的前后帧）来完成此任务。此外，由于整体高质量的帧可能包含低质量的片段，而高质量的片段可能存在于整体低质量的帧中，当前专注于附近峰值质量帧（PQFs）的方法可能会错过低质量帧中的高质量细节。为了弥补这些不足，本文提出了一种新颖的端到端深度神经网络，称为非局部ConvLSTM（简称NL-ConvLSTM），它利用多个连续帧。在NL-ConvLSTM中引入了一种近似非局部策略，以捕捉全局运动模式并追踪视频序列中的时空依赖性。这种近似策略使得非局部模块能够以快速且低空间成本的方式工作。我们的方法使用目标帧的前后帧生成残差，从中重建出更高质量的帧。在两个数据集上的实验表明，NL-ConvLSTM优于现有方法。",
        "领域": "视频处理/深度学习/神经网络",
        "问题": "从低质量压缩视频中恢复高质量视频",
        "动机": "现有方法可能错过低质量帧中的高质量细节，需要一种更有效的方法来捕捉全局运动模式和时空依赖性",
        "方法": "提出了一种新颖的端到端深度神经网络NL-ConvLSTM，利用多个连续帧和近似非局部策略来捕捉全局运动模式和时空依赖性",
        "关键词": [
            "视频压缩",
            "伪影减少",
            "ConvLSTM",
            "非局部策略",
            "时空依赖性"
        ],
        "涉及的技术概念": "非局部ConvLSTM（NL-ConvLSTM）是一种深度神经网络，用于视频压缩伪影减少。它通过利用多个连续帧和引入近似非局部策略来捕捉全局运动模式和追踪视频序列中的时空依赖性，从而有效地从低质量压缩视频中恢复高质量视频。"
    },
    {
        "order": 857,
        "title": "Multi-View Stereo by Temporal Nonparametric Fusion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hou_Multi-View_Stereo_by_Temporal_Nonparametric_Fusion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hou_Multi-View_Stereo_by_Temporal_Nonparametric_Fusion_ICCV_2019_paper.html",
        "abstract": "We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.",
        "中文标题": "通过时间非参数融合的多视图立体视觉",
        "摘要翻译": "我们提出了一种新颖的想法，用于从多视图图像-姿态对中进行深度估计，该模型能够利用场景先前潜在空间编码的信息。该模型使用图像和姿态对，通过编码器-解码器模型进行视差估计。新颖之处在于通过非参数高斯过程先验对瓶颈层进行软约束。我们提出了一种姿态核结构，鼓励相似姿态具有相似的潜在空间。高斯过程（GP）先验的灵活性提供了适应记忆，用于融合来自附近视图的信息。我们联合训练编码器-解码器和GP超参数，端到端。除了批量方法外，我们还推导了一种轻量级估计方案，避免了高斯过程推理扩展中的标准陷阱，并展示了我们的方案如何在智能设备上实时运行。",
        "领域": "立体视觉/深度估计/高斯过程",
        "问题": "从多视图图像-姿态对中进行深度估计",
        "动机": "利用场景先前潜在空间编码的信息，提高深度估计的准确性和效率",
        "方法": "使用编码器-解码器模型进行视差估计，通过非参数高斯过程先验对瓶颈层进行软约束，提出姿态核结构鼓励相似姿态具有相似的潜在空间，联合训练编码器-解码器和GP超参数，端到端，并推导了一种轻量级估计方案",
        "关键词": [
            "多视图立体视觉",
            "深度估计",
            "高斯过程",
            "编码器-解码器模型",
            "姿态核结构"
        ],
        "涉及的技术概念": "多视图图像-姿态对、潜在空间编码、视差估计、非参数高斯过程先验、姿态核结构、高斯过程（GP）先验、编码器-解码器模型、轻量级估计方案"
    },
    {
        "order": 858,
        "title": "S2GAN: Share Aging Factors Across Ages and Share Aging Trends Among Individuals",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_S2GAN_Share_Aging_Factors_Across_Ages_and_Share_Aging_Trends_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_S2GAN_Share_Aging_Factors_Across_Ages_and_Share_Aging_Trends_ICCV_2019_paper.html",
        "abstract": "Generally, we human follow the roughly common aging trends, e.g., the wrinkles only tend to be more, longer or deeper. However, the aging process of each individual is more dominated by his/her personalized factors, including the invariant factors such as identity and mole, as well as the personalized aging patterns, e.g., one may age by graying hair while another may age by receding hairline. Following this biological principle, in this work, we propose an effective and efficient method to simulate natural aging. Specifically, a personalized aging basis is established for each individual to depict his/her own aging factors. Then different ages share this basis, being derived through age-specific transforms. The age-specific transforms represent the aging trends which are shared among all individuals. The proposed method can achieve continuous face aging with favorable aging accuracy, identity preservation, and fidelity. Furthermore, befitted from the effective design, a unique model is capable of all ages and the prediction time is significantly saved.",
        "中文标题": "S2GAN: 跨年龄共享老化因素和个体间共享老化趋势",
        "摘要翻译": "通常，我们人类遵循大致相同的老化趋势，例如，皱纹只会变得更多、更长或更深。然而，每个人的老化过程更多地受到其个性化因素的支配，包括不变的因素如身份和痣，以及个性化的老化模式，例如，一个人可能通过头发变白而老化，而另一个人可能通过发际线后退而老化。遵循这一生物学原理，在这项工作中，我们提出了一种有效且高效的方法来模拟自然老化。具体来说，为每个个体建立了一个个性化的老化基础，以描述其自身的老化因素。然后，不同的年龄共享这一基础，通过特定年龄的变换得出。特定年龄的变换代表了所有个体共享的老化趋势。所提出的方法可以实现连续的面部老化，具有良好的老化准确性、身份保持和保真度。此外，得益于有效的设计，一个独特的模型能够适用于所有年龄，并且显著节省了预测时间。",
        "领域": "面部老化模拟/个性化老化模式/年龄变换",
        "问题": "如何有效且高效地模拟自然老化过程，同时保持个体的身份特征和老化模式的个性化。",
        "动机": "基于人类老化过程中存在的共同趋势和个性化因素，探索一种能够准确模拟自然老化并保持个体特征的方法。",
        "方法": "为每个个体建立个性化的老化基础，通过特定年龄的变换共享老化趋势，实现连续的面部老化。",
        "关键词": [
            "面部老化",
            "个性化老化模式",
            "年龄变换"
        ],
        "涉及的技术概念": "个性化老化基础：为每个个体建立的老化因素描述；特定年龄变换：根据不同年龄对老化基础进行变换，以共享老化趋势；连续面部老化：通过上述方法实现的面部老化过程，保持身份特征和老化模式的个性化。"
    },
    {
        "order": 859,
        "title": "Self-Supervised Moving Vehicle Tracking With Stereo Sound",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gan_Self-Supervised_Moving_Vehicle_Tracking_With_Stereo_Sound_ICCV_2019_paper.html",
        "abstract": "Humans are able to localize objects in the environment using both visual and auditory cues, integrating information from multiple modalities into a common reference frame. We introduce a system that can leverage unlabeled audiovisual data to learn to localize objects (moving vehicles) in a visual reference frame, purely using stereo sound at inference time. Since it is labor-intensive to manually annotate the correspondences between audio and object bounding boxes, we achieve this goal by using the co-occurrence of visual and audio streams in unlabeled videos as a form of self-supervision, without resorting to the collection of ground truth annotations. In particular, we propose a framework that consists of a vision \"teacher\" network and a stereo-sound \"student\" network. During training, knowledge embodied in a well-established visual vehicle detection model is transferred to the audio domain using unlabeled videos as a bridge. At test time, the stereo-sound student network can work independently to perform object localization using just stereo audio and camera meta-data, without any visual input. Experimental results on a newly collected Auditory Vehicles Tracking dataset verify that our proposed approach outperforms several baseline approaches. We also demonstrate that our cross-modal auditory localization approach can assist in the visual localization of moving vehicles under poor lighting conditions.",
        "中文标题": "自监督立体声移动车辆跟踪",
        "摘要翻译": "人类能够利用视觉和听觉线索在环境中定位物体，将来自多种模态的信息整合到一个共同的参考框架中。我们引入了一个系统，该系统能够利用未标记的视听数据来学习在视觉参考框架中定位物体（移动车辆），在推理时仅使用立体声。由于手动注释音频和物体边界框之间的对应关系是劳动密集型的，我们通过使用未标记视频中视觉和音频流的共现作为一种自我监督的形式来实现这一目标，而无需收集地面真实注释。特别是，我们提出了一个框架，该框架由一个视觉“教师”网络和一个立体声“学生”网络组成。在训练期间，通过使用未标记的视频作为桥梁，将成熟的视觉车辆检测模型中的知识转移到音频领域。在测试时，立体声学生网络可以独立工作，仅使用立体声和相机元数据进行物体定位，无需任何视觉输入。在新收集的听觉车辆跟踪数据集上的实验结果验证了我们提出的方法优于几种基线方法。我们还证明了我们的跨模态听觉定位方法可以在光线不佳的条件下辅助移动车辆的视觉定位。",
        "领域": "听觉定位/跨模态学习/自监督学习",
        "问题": "如何在仅使用立体声和相机元数据的情况下，实现移动车辆的定位",
        "动机": "减少对视觉输入的依赖，特别是在光线不佳的条件下，通过利用未标记的视听数据实现自监督学习",
        "方法": "提出一个由视觉“教师”网络和立体声“学生”网络组成的框架，通过未标记视频作为桥梁，将视觉车辆检测模型的知识转移到音频领域，实现仅使用立体声和相机元数据进行物体定位",
        "关键词": [
            "听觉定位",
            "跨模态学习",
            "自监督学习",
            "立体声",
            "移动车辆跟踪"
        ],
        "涉及的技术概念": {
            "视觉“教师”网络": "一个成熟的视觉车辆检测模型，用于在训练期间提供知识",
            "立体声“学生”网络": "在测试时独立工作，仅使用立体声和相机元数据进行物体定位的网络",
            "自监督学习": "利用未标记的视听数据中的视觉和音频流的共现作为监督信号，无需地面真实注释",
            "跨模态学习": "将视觉和听觉信息整合到一个共同的参考框架中，实现物体定位"
        }
    },
    {
        "order": 860,
        "title": "Floor-SP: Inverse CAD for Floorplans by Sequential Room-Wise Shortest Path",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Floor-SP_Inverse_CAD_for_Floorplans_by_Sequential_Room-Wise_Shortest_Path_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Floor-SP_Inverse_CAD_for_Floorplans_by_Sequential_Room-Wise_Shortest_Path_ICCV_2019_paper.html",
        "abstract": "This paper proposes a new approach for automated floorplan reconstruction from RGBD scans, a major milestone in indoor mapping research. The approach, dubbed Floor-SP, formulates a novel optimization problem, where room-wise coordinate descent sequentially solves shortest path problems to optimize the floorplan graph structure. The objective function consists of data terms guided by deep neural networks, consistency terms encouraging adjacent rooms to share corners and walls, and the model complexity term. The approach does not require corner/edge primitive extraction unlike most other methods. We have evaluated our system on production-quality RGBD scans of 527 apartments or houses, including many units with non-Manhattan structures. Qualitative and quantitative evaluations demonstrate a significant performance boost over the current state-of-the-art. Please refer to our project website http://jcchen.me/floor-sp/ for code and data.",
        "中文标题": "Floor-SP：通过顺序房间最短路径进行平面图逆向CAD",
        "摘要翻译": "本文提出了一种从RGBD扫描自动重建平面图的新方法，这是室内地图研究的一个重要里程碑。该方法名为Floor-SP，制定了一个新颖的优化问题，其中房间坐标下降顺序解决最短路径问题以优化平面图结构。目标函数包括由深度神经网络指导的数据项、鼓励相邻房间共享角落和墙壁的一致性项以及模型复杂性项。与大多数其他方法不同，该方法不需要角落/边缘基元提取。我们已经在527套公寓或房屋的生产质量RGBD扫描上评估了我们的系统，包括许多具有非曼哈顿结构的单元。定性和定量评估表明，与当前最先进的技术相比，性能有显著提升。请访问我们的项目网站http://jcchen.me/floor-sp/获取代码和数据。",
        "领域": "室内地图重建/平面图优化/逆向CAD",
        "问题": "从RGBD扫描自动重建平面图",
        "动机": "提高室内地图研究的自动化水平，优化平面图结构",
        "方法": "通过顺序房间最短路径解决优化问题，结合深度神经网络指导的数据项、一致性项和模型复杂性项",
        "关键词": [
            "平面图重建",
            "RGBD扫描",
            "最短路径",
            "深度神经网络",
            "优化问题"
        ],
        "涉及的技术概念": "RGBD扫描是一种结合了RGB彩色图像和深度信息的扫描技术，用于捕捉室内环境的详细三维信息。深度神经网络是一种模仿人脑结构和功能的计算模型，用于处理和分析复杂的数据。最短路径问题是在图论中寻找两点之间最短路径的问题，常用于优化和规划问题。逆向CAD是指从物理对象或扫描数据中重建计算机辅助设计（CAD）模型的过程。"
    },
    {
        "order": 861,
        "title": "PuppetGAN: Cross-Domain Image Manipulation by Demonstration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Usman_PuppetGAN_Cross-Domain_Image_Manipulation_by_Demonstration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Usman_PuppetGAN_Cross-Domain_Image_Manipulation_by_Demonstration_ICCV_2019_paper.html",
        "abstract": "In this work we propose a model that can manipulate individual visual attributes of objects in a real scene using examples of how respective attribute manipulations affect the output of a simulation. As an example, we train our model to manipulate the expression of a human face using nonphotorealistic 3D renders of a face with varied expression. Our model manages to preserve all other visual attributes of a real face, such as head orientation, even though this and other attributes are not labeled in either real or synthetic domain. Since our model learns to manipulate a specific property in isolation using only \"synthetic demonstrations\" of such manipulations without explicitly provided labels, it can be applied to shape, texture, lighting, and other properties that are difficult to measure or represent as real-valued vectors. We measure the degree to which our model preserves other attributes of a real image when a single specific attribute is manipulated. We use digit datasets to analyze how discrepancy in attribute distributions affects the performance of our model, and demonstrate results in a far more difficult setting: learning to manipulate real human faces using nonphotorealistic 3D renders.",
        "中文标题": "PuppetGAN：通过演示进行跨域图像操作",
        "摘要翻译": "在这项工作中，我们提出了一种模型，该模型可以使用相应属性操作如何影响模拟输出的示例来操作真实场景中对象的各个视觉属性。作为一个例子，我们训练我们的模型使用具有不同表情的非真实感3D面部渲染来操作人脸的表达。我们的模型设法保留了真实面部的所有其他视觉属性，如头部方向，尽管这些和其他属性在真实或合成域中都没有被标记。由于我们的模型仅使用此类操作的“合成演示”来学习单独操作特定属性，而无需明确提供的标签，因此它可以应用于形状、纹理、光照和其他难以测量或表示为实值向量的属性。我们测量了当单个特定属性被操作时，我们的模型保留真实图像其他属性的程度。我们使用数字数据集来分析属性分布的差异如何影响我们模型的性能，并在一个更困难的环境中展示结果：学习使用非真实感3D渲染来操作真实的人脸。",
        "领域": "图像生成/面部表情合成/3D渲染",
        "问题": "如何在保留图像其他视觉属性的同时，操作特定视觉属性",
        "动机": "为了实现对真实场景中对象特定视觉属性的精确操作，同时保持其他属性不变，尤其是在缺乏明确标签的情况下",
        "方法": "提出了一种模型，该模型通过使用合成演示来学习单独操作特定属性，而无需明确提供的标签，并应用于形状、纹理、光照等属性",
        "关键词": [
            "图像生成",
            "面部表情合成",
            "3D渲染"
        ],
        "涉及的技术概念": "该研究涉及的技术概念包括图像生成技术、面部表情的合成、3D渲染技术，以及如何在没有明确标签的情况下通过合成演示来操作特定视觉属性。"
    },
    {
        "order": 862,
        "title": "Self-Supervised Learning With Geometric Constraints in Monocular Video: Connecting Flow, Depth, and Camera",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Self-Supervised_Learning_With_Geometric_Constraints_in_Monocular_Video_Connecting_Flow_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Supervised_Learning_With_Geometric_Constraints_in_Monocular_Video_Connecting_Flow_ICCV_2019_paper.html",
        "abstract": "We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video -- addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning.",
        "中文标题": "自监督学习在单目视频中的几何约束：连接光流、深度和相机",
        "摘要翻译": "我们提出了GLNet，一个自监督框架，用于从单目视频中学习深度、光流、相机姿态和内在参数——解决了为这些任务获取真实地面实况的困难。我们提出了三个贡献：1）我们设计了新的损失函数，捕捉多种几何约束（例如，极线几何）以及支持多个移动物体（刚性和非刚性）的自适应光度损失，2）我们扩展了模型，使其能够预测相机内在参数，使其适用于未校准的视频，3）我们提出了几种在线优化策略，这些策略依赖于我们自监督损失在训练和测试中的对称性，特别是优化模型参数和/或不同任务的输出，利用它们的相互影响。在所有几何和光度约束下联合优化系统输出的想法可以被视为经典捆绑调整的密集泛化。我们在KITTI和Cityscapes上展示了我们方法的有效性，在多个任务上超越了之前的自监督方法。我们还展示了良好的迁移学习泛化能力。",
        "领域": "深度估计/光流估计/相机姿态估计",
        "问题": "从单目视频中学习深度、光流、相机姿态和内在参数的困难",
        "动机": "解决为这些任务获取真实地面实况的困难",
        "方法": "设计了新的损失函数捕捉几何约束和自适应光度损失，扩展模型预测相机内在参数，提出在线优化策略",
        "关键词": [
            "自监督学习",
            "几何约束",
            "单目视频",
            "深度估计",
            "光流估计",
            "相机姿态估计"
        ],
        "涉及的技术概念": "自监督学习是一种不需要大量标注数据的学习方法，通过设计特定的任务和损失函数来指导模型学习。几何约束指的是在视觉任务中利用几何关系（如极线几何）来约束模型的输出，提高预测的准确性。自适应光度损失是一种能够根据场景中物体的运动特性（刚性和非刚性）调整的损失函数，用于提高光流估计的准确性。在线优化策略指的是在模型训练和测试过程中，根据模型输出的对称性和任务间的相互影响，动态调整模型参数和任务输出，以提高整体性能。"
    },
    {
        "order": 863,
        "title": "Polarimetric Relative Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cui_Polarimetric_Relative_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cui_Polarimetric_Relative_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline.",
        "中文标题": "偏振相对姿态估计",
        "摘要翻译": "在本文中，我们考虑了从具有每像素偏振信息的两幅图像中进行相对姿态估计的问题。利用这些额外的测量，我们推导出了一个简单的本质矩阵最小解算器，该解算器仅需要两个点对应。偏振约束使我们能够逐点恢复三维表面法线，直到漫反射的双重模糊性。由于这种模糊性存在于每个点，因此存在可能性的组合爆炸。然而，由于我们的解算器仅需要两个点对应，因此在解决相对姿态时我们只需要考虑16种配置。一旦恢复了相对方向，我们展示了解决剩余点的模糊性是微不足道的。为了鲁棒性，我们还提出了相对姿态和折射率之间的联合优化，以处理折射失真。在实验中，无论是在合成数据还是真实数据上，我们都证明了通过利用偏振相机提供的额外信息，我们可以改进仅依赖二维点位置来估计几何形状的经典方法。最后，我们通过将其集成到最先进的全局运动结构管道中，展示了我们方法的实际适用性。",
        "领域": "三维重建/姿态估计/偏振成像",
        "问题": "从具有每像素偏振信息的两幅图像中进行相对姿态估计",
        "动机": "利用偏振相机提供的额外信息改进仅依赖二维点位置来估计几何形状的经典方法",
        "方法": "推导出一个简单的本质矩阵最小解算器，仅需要两个点对应，并通过偏振约束逐点恢复三维表面法线，提出相对姿态和折射率之间的联合优化以处理折射失真",
        "关键词": [
            "偏振成像",
            "相对姿态估计",
            "三维重建"
        ],
        "涉及的技术概念": "本质矩阵最小解算器、偏振约束、三维表面法线恢复、折射率联合优化、全局运动结构管道"
    },
    {
        "order": 864,
        "title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zakharov_Few-Shot_Adversarial_Learning_of_Realistic_Neural_Talking_Head_Models_ICCV_2019_paper.html",
        "abstract": "Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.",
        "中文标题": "少样本对抗学习生成逼真的神经说话头模型",
        "摘要翻译": "最近的几项研究表明，通过训练卷积神经网络生成高度逼真的人头图像是可能的。为了创建个性化的说话头模型，这些研究需要在单个人的大量图像数据集上进行训练。然而，在许多实际场景中，这种个性化的说话头模型需要从一个人的少量图像视图中学习，甚至可能仅从一张图像中学习。在这里，我们提出了一个具有这种少样本能力的系统。它在一个大型视频数据集上进行了长时间的元学习，之后能够将未见过的人的神经说话头模型的少样本和单样本学习框架化为具有高容量生成器和判别器的对抗训练问题。关键的是，该系统能够以特定于人的方式初始化生成器和判别器的参数，因此尽管需要调整数千万个参数，训练可以基于仅有的几张图像快速完成。我们展示了这种方法能够学习到新人的高度逼真和个性化的说话头模型，甚至是肖像画。",
        "领域": "生成对抗网络/元学习/个性化建模",
        "问题": "如何从少量图像中学习生成个性化的逼真说话头模型",
        "动机": "在实际应用中，个性化说话头模型需要从一个人的少量图像视图中快速学习",
        "方法": "通过在大规模视频数据集上进行元学习，将少样本和单样本学习框架化为对抗训练问题，并采用高容量生成器和判别器",
        "关键词": [
            "少样本学习",
            "对抗训练",
            "个性化建模",
            "元学习"
        ],
        "涉及的技术概念": "卷积神经网络用于生成逼真的人头图像；元学习用于在大规模视频数据集上预训练；对抗训练框架用于少样本和单样本学习；高容量生成器和判别器用于提高模型的逼真度和个性化能力"
    },
    {
        "order": 865,
        "title": "Learning Temporal Action Proposals With Fewer Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Learning_Temporal_Action_Proposals_With_Fewer_Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Learning_Temporal_Action_Proposals_With_Fewer_Labels_ICCV_2019_paper.html",
        "abstract": "Temporal action proposals are a common module in action detection pipelines today. Most current methods for training action proposal modules rely on fully supervised approaches that require large amounts of annotated temporal action intervals in long video sequences. The large cost and effort in annotation that this entails motivate us to study the problem of training proposal modules with less supervision. In this work, we propose a semi-supervised learning algorithm specifically designed for training temporal action proposal networks. When only a small number of labels are available, our semi-supervised method generates significantly better proposals than the fully-supervised counterpart and other strong semi-supervised baselines. We validate our method on two challenging action detection video datasets, ActivityNet v1.3 and THUMOS14. We show that our semi-supervised approach consistently matches or outperforms the fully supervised state-of-the-art approaches.",
        "中文标题": "用更少的标签学习时间动作提案",
        "摘要翻译": "时间动作提案是当今动作检测流程中的一个常见模块。目前大多数训练动作提案模块的方法依赖于完全监督的方法，这些方法需要大量的长视频序列中的注释时间动作间隔。这种注释所需的高成本和高努力激励我们研究用更少的监督训练提案模块的问题。在这项工作中，我们提出了一种专门为训练时间动作提案网络设计的半监督学习算法。当只有少量标签可用时，我们的半监督方法生成的提案显著优于完全监督的对应方法和其他强大的半监督基线。我们在两个具有挑战性的动作检测视频数据集ActivityNet v1.3和THUMOS14上验证了我们的方法。我们展示了我们的半监督方法始终匹配或优于完全监督的最先进方法。",
        "领域": "动作检测/视频分析/半监督学习",
        "问题": "如何在少量标签的情况下训练时间动作提案网络",
        "动机": "减少训练时间动作提案模块所需的注释成本和工作量",
        "方法": "提出了一种专门为训练时间动作提案网络设计的半监督学习算法",
        "关键词": [
            "时间动作提案",
            "半监督学习",
            "动作检测"
        ],
        "涉及的技术概念": "时间动作提案是指在视频中识别出可能包含动作的时间段。半监督学习是一种机器学习方法，它使用少量标注数据和大量未标注数据来训练模型。动作检测是指在视频中识别和定位特定动作的任务。"
    },
    {
        "order": 866,
        "title": "Closed-Form Optimal Two-View Triangulation Based on Angular Errors",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Closed-Form_Optimal_Two-View_Triangulation_Based_on_Angular_Errors_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Closed-Form_Optimal_Two-View_Triangulation_Based_on_Angular_Errors_ICCV_2019_paper.html",
        "abstract": "In this paper, we study closed-form optimal solutions to two-view triangulation with known internal calibration and pose. By formulating the triangulation problem as L-1 and L-infinity minimization of angular reprojection errors, we derive the exact closed-form solutions that guarantee global optimality under respective cost functions. To the best of our knowledge, we are the first to present such solutions. Since the angular error is rotationally invariant, our solutions can be applied for any type of central cameras, be it perspective, fisheye or omnidirectional. Our methods also require significantly less computation than the existing optimal methods. Experimental results on synthetic and real datasets validate our theoretical derivations.",
        "中文标题": "基于角度误差的闭式最优双视图三角测量",
        "摘要翻译": "本文研究了已知内部校准和姿态的双视图三角测量的闭式最优解。通过将三角测量问题表述为角度重投影误差的L-1和L-无穷最小化，我们推导出了在各自成本函数下保证全局最优性的精确闭式解。据我们所知，我们是第一个提出此类解决方案的。由于角度误差是旋转不变的，我们的解决方案可以应用于任何类型的中心相机，无论是透视、鱼眼还是全方位相机。我们的方法也比现有的最优方法需要显著更少的计算。在合成和真实数据集上的实验结果验证了我们的理论推导。",
        "领域": "三维重建/相机校准/几何优化",
        "问题": "双视图三角测量的最优解问题",
        "动机": "为了解决双视图三角测量中角度重投影误差的最小化问题，并找到保证全局最优性的闭式解",
        "方法": "通过将三角测量问题表述为角度重投影误差的L-1和L-无穷最小化，推导出精确的闭式解",
        "关键词": [
            "三角测量",
            "角度误差",
            "闭式解",
            "L-1最小化",
            "L-无穷最小化"
        ],
        "涉及的技术概念": {
            "双视图三角测量": "一种从两个不同视角的图像中恢复三维点位置的技术",
            "角度重投影误差": "指在三维重建过程中，将三维点投影回二维图像平面时，与原始图像点之间的角度差异",
            "L-1和L-无穷最小化": "分别指最小化误差的绝对值和最大误差的优化方法",
            "闭式解": "指可以直接通过公式计算得到的解，不需要迭代或搜索过程",
            "全局最优性": "指在给定的优化问题中，找到的解是所有可能解中最优的"
        }
    },
    {
        "order": 867,
        "title": "Pose-Aware Multi-Level Feature Network for Human Object Interaction Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wan_Pose-Aware_Multi-Level_Feature_Network_for_Human_Object_Interaction_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wan_Pose-Aware_Multi-Level_Feature_Network_for_Human_Object_Interaction_Detection_ICCV_2019_paper.html",
        "abstract": "Reasoning human object interactions is a core problem in human-centric scene understanding and detecting such relations poses a unique challenge to vision systems due to large variations in human-object configurations, multiple co-occurring relation instances and subtle visual difference between relation categories. To address those challenges, we propose a multi-level relation detection strategy that utilizes human pose cues to capture global spatial configurations of relations and as an attention mechanism to dynamically zoom into relevant regions at human part level. We develop a multi-branch deep network to learn a pose-augmented relation representation at three semantic levels, incorporating interaction context, object features and detailed semantic part cues. As a result, our approach is capable of generating robust predictions on fine-grained human object interactions with interpretable outputs. Extensive experimental evaluations on public benchmarks show that our model outperforms prior methods by a considerable margin, demonstrating its efficacy in handling complex scenes.",
        "中文标题": "姿态感知的多层次特征网络用于人物交互检测",
        "摘要翻译": "推理人物交互是以人为中心的场景理解中的一个核心问题，由于人物配置的大变化、多个同时发生的关系实例以及关系类别之间细微的视觉差异，检测这种关系对视觉系统提出了独特的挑战。为了应对这些挑战，我们提出了一种多层次关系检测策略，该策略利用人体姿态线索来捕捉关系的全局空间配置，并作为一种注意力机制动态放大到人体部位级别的相关区域。我们开发了一个多分支深度网络，以在三个语义层次上学习姿态增强的关系表示，包括交互上下文、物体特征和详细的语义部位线索。因此，我们的方法能够生成具有可解释输出的细粒度人物交互的稳健预测。在公共基准上的广泛实验评估显示，我们的模型以相当大的优势优于先前的方法，证明了其在处理复杂场景中的有效性。",
        "领域": "人物交互检测/场景理解/姿态估计",
        "问题": "检测和理解人物与物体之间的交互关系",
        "动机": "由于人物配置的大变化、多个同时发生的关系实例以及关系类别之间细微的视觉差异，检测这种关系对视觉系统提出了独特的挑战",
        "方法": "提出了一种多层次关系检测策略，利用人体姿态线索来捕捉关系的全局空间配置，并作为一种注意力机制动态放大到人体部位级别的相关区域。开发了一个多分支深度网络，以在三个语义层次上学习姿态增强的关系表示。",
        "关键词": [
            "人物交互检测",
            "姿态估计",
            "场景理解"
        ],
        "涉及的技术概念": "多层次关系检测策略、人体姿态线索、注意力机制、多分支深度网络、姿态增强的关系表示、交互上下文、物体特征、语义部位线索"
    },
    {
        "order": 868,
        "title": "TSM: Temporal Shift Module for Efficient Video Understanding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lin_TSM_Temporal_Shift_Module_for_Efficient_Video_Understanding_ICCV_2019_paper.html",
        "abstract": "The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module.",
        "中文标题": "TSM：用于高效视频理解的时间移位模块",
        "摘要翻译": "视频流的爆炸性增长带来了在高准确性和低计算成本下进行视频理解的挑战。传统的2D CNN计算成本低，但无法捕捉时间关系；基于3D CNN的方法可以实现良好的性能，但计算密集，部署成本高。在本文中，我们提出了一种通用且有效的时间移位模块（TSM），它既高效又高性能。具体来说，它可以实现3D CNN的性能，但保持2D CNN的复杂性。TSM沿时间维度移动部分通道；从而促进相邻帧之间的信息交换。它可以插入到2D CNN中，以零计算和零参数实现时间建模。我们还将TSM扩展到在线设置，这使得实时低延迟的在线视频识别和视频对象检测成为可能。TSM准确且高效：它在发布时在Something-Something排行榜上排名第一；在Jetson Nano和Galaxy Note8上，它实现了13ms和35ms的低延迟在线视频识别。代码可在https://github.com/mit-han-lab/temporal-shift-module获取。",
        "领域": "视频理解/实时处理/低延迟计算",
        "问题": "在保持低计算成本的同时，提高视频理解的准确性和效率",
        "动机": "解决视频流增长带来的高准确性和低计算成本视频理解的挑战",
        "方法": "提出时间移位模块（TSM），通过沿时间维度移动部分通道来促进信息交换，实现高效的时间建模",
        "关键词": [
            "视频理解",
            "时间移位模块",
            "低延迟计算"
        ],
        "涉及的技术概念": "2D CNN（二维卷积神经网络）、3D CNN（三维卷积神经网络）、时间移位模块（TSM）、在线视频识别、视频对象检测"
    },
    {
        "order": 869,
        "title": "Pix2Vox: Context-Aware 3D Reconstruction From Single and Multi-View Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.html",
        "abstract": "Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.",
        "中文标题": "Pix2Vox: 从单视图和多视图图像中进行上下文感知的3D重建",
        "摘要翻译": "在过去的几年中，通过深度神经网络从单视图或多视图RGB图像中恢复物体的3D表示引起了越来越多的关注。一些主流工作（例如，3D-R2N2）使用循环神经网络（RNNs）顺序融合从输入图像中提取的多个特征图。然而，当给定相同的一组输入图像但顺序不同时，基于RNN的方法无法产生一致的重建结果。此外，由于长期记忆丢失，RNNs无法充分利用输入图像来优化重建结果。为了解决这些问题，我们提出了一种新颖的单视图和多视图3D重建框架，名为Pix2Vox。通过使用一个精心设计的编码器-解码器，它从每个输入图像生成一个粗糙的3D体积。然后，引入一个上下文感知融合模块，从不同的粗糙3D体积中自适应地选择每个部分（例如，桌腿）的高质量重建，以获得融合的3D体积。最后，一个细化器进一步细化融合的3D体积以生成最终输出。在ShapeNet和Pix3D基准测试上的实验结果表明，所提出的Pix2Vox大幅优于现有技术。此外，所提出的方法在反向推理时间上比3D-R2N2快24倍。在ShapeNet未见过的3D类别上的实验显示了我们的方法的优越泛化能力。",
        "领域": "3D重建/深度学习/计算机视觉",
        "问题": "解决从单视图或多视图RGB图像中恢复物体的3D表示时，基于RNN的方法无法产生一致的重建结果和无法充分利用输入图像优化重建结果的问题",
        "动机": "提高3D重建的准确性和效率，解决现有方法在输入图像顺序不同时重建结果不一致和长期记忆丢失的问题",
        "方法": "提出Pix2Vox框架，通过编码器-解码器生成粗糙的3D体积，引入上下文感知融合模块自适应选择高质量重建，最后通过细化器生成最终输出",
        "关键词": [
            "3D重建",
            "上下文感知",
            "编码器-解码器",
            "融合模块",
            "细化器"
        ],
        "涉及的技术概念": "Pix2Vox框架包括一个编码器-解码器用于从输入图像生成粗糙的3D体积，一个上下文感知融合模块用于从不同的粗糙3D体积中选择高质量重建，以及一个细化器用于进一步优化融合的3D体积以生成最终输出。"
    },
    {
        "order": 870,
        "title": "TRB: A Novel Triplet Representation for Understanding 2D Human Body",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_TRB_A_Novel_Triplet_Representation_for_Understanding_2D_Human_Body_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Duan_TRB_A_Novel_Triplet_Representation_for_Understanding_2D_Human_Body_ICCV_2019_paper.html",
        "abstract": "Human pose and shape are two important components of 2D human body. However, how to efficiently represent both of them in images is still an open question. In this paper, we propose the Triplet Representation for Body (TRB) --- a compact 2D human body representation, with skeleton keypoints capturing human pose information and contour keypoints containing human shape information. TRB not only preserves the flexibility of skeleton keypoint representation, but also contains rich pose and human shape information. Therefore, it promises broader application areas, such as human shape editing and conditional image generation. We further introduce the challenging problem of TRB estimation, where joint learning of human pose and shape is required. We construct several large-scale TRB estimation datasets, based on the popular 2D pose datasets LSP, MPII and COCO. To effectively solve TRB estimation, we propose a two-branch network (TRB-net) with three novel techniques, namely X-structure (Xs), Directional Convolution (DC) and Pairwise mapping (PM), to enforce multi-level message passing for joint feature learning. We evaluate our proposed TRB-net and several leading approaches on our proposed TRB datasets, and demonstrate the superiority of our method through extensive evaluations.",
        "中文标题": "TRB：一种用于理解2D人体的新颖三元表示",
        "摘要翻译": "人体姿态和形状是2D人体的两个重要组成部分。然而，如何在图像中有效地表示这两者仍然是一个未解决的问题。在本文中，我们提出了身体的三元表示（TRB）——一种紧凑的2D人体表示，其中骨架关键点捕捉人体姿态信息，轮廓关键点包含人体形状信息。TRB不仅保留了骨架关键点表示的灵活性，还包含了丰富的姿态和人体形状信息。因此，它承诺有更广泛的应用领域，如人体形状编辑和条件图像生成。我们进一步介绍了TRB估计的挑战性问题，其中需要联合学习人体姿态和形状。我们基于流行的2D姿态数据集LSP、MPII和COCO构建了几个大规模的TRB估计数据集。为了有效解决TRB估计问题，我们提出了一个两分支网络（TRB-net），并采用了三种新技术，即X结构（Xs）、方向卷积（DC）和成对映射（PM），以加强多级消息传递以进行联合特征学习。我们在我们提出的TRB数据集上评估了我们提出的TRB-net和几种领先的方法，并通过广泛的评估证明了我们方法的优越性。",
        "领域": "人体姿态估计/人体形状编辑/条件图像生成",
        "问题": "如何在图像中有效地表示人体姿态和形状",
        "动机": "解决2D人体姿态和形状表示的问题，以支持更广泛的应用，如人体形状编辑和条件图像生成",
        "方法": "提出了一种紧凑的2D人体表示TRB，并开发了一个两分支网络TRB-net，采用X结构、方向卷积和成对映射技术进行联合特征学习",
        "关键词": [
            "人体姿态估计",
            "人体形状编辑",
            "条件图像生成"
        ],
        "涉及的技术概念": {
            "TRB": "一种紧凑的2D人体表示，包含骨架关键点和轮廓关键点，分别捕捉人体姿态和形状信息",
            "TRB-net": "一个两分支网络，用于TRB估计，采用X结构、方向卷积和成对映射技术",
            "X-structure (Xs)": "一种用于加强多级消息传递的技术",
            "Directional Convolution (DC)": "一种卷积技术，用于增强特征学习的方向性",
            "Pairwise mapping (PM)": "一种映射技术，用于成对特征学习"
        }
    },
    {
        "order": 871,
        "title": "Graph Convolutional Networks for Temporal Action Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Graph_Convolutional_Networks_for_Temporal_Action_Localization_ICCV_2019_paper.html",
        "abstract": "Most state-of-the-art action localization systems process each action proposal individually, without explicitly exploiting their relations during learning. However, the relations between proposals actually play an important role in action localization, since a meaningful action always consists of multiple proposals in a video. In this paper, we propose to exploit the proposal-proposal relations using GraphConvolutional Networks (GCNs). First, we construct an action proposal graph, where each proposal is represented as a node and their relations between two proposals as an edge. Here, we use two types of relations, one for capturing the context information for each proposal and the other one for characterizing the correlations between distinct actions. Then we apply the GCNs over the graph to model the relations among different proposals and learn powerful representations for the action classification and localization. Experimental results show that our approach significantly outperforms the state-of-the-art on THUMOS14(49.1% versus 42.8%). Moreover, augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships.",
        "中文标题": "图卷积网络在时序动作定位中的应用",
        "摘要翻译": "大多数最先进的动作定位系统在处理每个动作提案时都是单独进行的，在学习过程中没有明确利用它们之间的关系。然而，提案之间的关系实际上在动作定位中扮演着重要角色，因为一个有意义的动作总是由视频中的多个提案组成。在本文中，我们提出利用图卷积网络（GCNs）来开发提案-提案关系。首先，我们构建了一个动作提案图，其中每个提案被表示为一个节点，两个提案之间的关系被表示为边。这里，我们使用了两种类型的关系，一种用于捕捉每个提案的上下文信息，另一种用于表征不同动作之间的相关性。然后，我们在图上应用GCNs来建模不同提案之间的关系，并学习用于动作分类和定位的强大表示。实验结果表明，我们的方法在THUMOS14上显著优于最先进的技术（49.1%对42.8%）。此外，在ActivityNet上的增强实验也验证了建模动作提案关系的有效性。",
        "领域": "时序动作定位/图卷积网络/视频分析",
        "问题": "如何有效利用动作提案之间的关系来提高时序动作定位的准确性",
        "动机": "现有的动作定位系统在处理动作提案时忽视了它们之间的关系，而这些关系对于准确识别和定位视频中的动作至关重要",
        "方法": "构建动作提案图，利用图卷积网络（GCNs）建模提案之间的关系，并学习用于动作分类和定位的表示",
        "关键词": [
            "时序动作定位",
            "图卷积网络",
            "视频分析"
        ],
        "涉及的技术概念": "图卷积网络（GCNs）是一种深度学习模型，专门用于处理图结构数据。在本研究中，GCNs被用来建模视频中动作提案之间的关系，通过学习这些关系来提高动作分类和定位的准确性。动作提案图是一种将视频中的动作提案表示为图中的节点，并将提案之间的关系表示为边的数据结构。"
    },
    {
        "order": 872,
        "title": "Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Esser_Unsupervised_Robust_Disentangling_of_Latent_Characteristics_for_Image_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Esser_Unsupervised_Robust_Disentangling_of_Latent_Characteristics_for_Image_Synthesis_ICCV_2019_paper.html",
        "abstract": "Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available.",
        "中文标题": "无监督鲁棒解耦潜在特征用于图像合成",
        "摘要翻译": "深度生成模型承诺学习视觉对象的可解释表示，允许图像采样、合成和选择性修改。主要挑战是学习如何正确建模对象的独立潜在特征，特别是其外观和姿态。我们提出了一种新方法，学习这些特征的解耦表示并分别解释它们。训练仅需要描绘相同对象外观的图像对，而不需要姿态注释。我们提出了一个额外的分类器，估计强制执行解耦所需的最小正则化量。因此，这两种表示可以完全解释图像，同时彼此独立。基于对抗方法的前方法未能强制执行这种独立性，而基于变分方法的方法导致无信息的表示。在多样化的对象类别实验中，该方法成功地重新组合姿态和外观以重建和重定向新合成的图像。我们在使用相同监督水平的最先进方法上取得了显著改进，并达到了与姿态监督方法相当的性能。然而，我们可以处理没有姿态模型/注释的大量关节对象类别。",
        "领域": "图像合成/特征解耦/无监督学习",
        "问题": "如何无监督地学习并解耦图像中的独立潜在特征，特别是外观和姿态，以进行图像合成和修改。",
        "动机": "为了克服现有方法在解耦图像特征时无法保证特征独立性的问题，以及减少对姿态注释的依赖，提出一种新的无监督学习方法。",
        "方法": "提出了一种新方法，通过训练仅需要相同对象外观的图像对，无需姿态注释，引入一个额外的分类器估计最小正则化量以强制执行特征解耦，从而实现外观和姿态的独立表示。",
        "关键词": [
            "图像合成",
            "特征解耦",
            "无监督学习"
        ],
        "涉及的技术概念": "深度生成模型、解耦表示、正则化、对抗方法、变分方法、图像采样、图像重建、图像重定向"
    },
    {
        "order": 873,
        "title": "Learning Trajectory Dependencies for Human Motion Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mao_Learning_Trajectory_Dependencies_for_Human_Motion_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mao_Learning_Trajectory_Dependencies_for_Human_Motion_Prediction_ICCV_2019_paper.html",
        "abstract": "Human motion prediction, i.e., forecasting future body poses given observed pose sequence, has typically been tackled with recurrent neural networks (RNNs). However, as evidenced by prior work, the resulted RNN models suffer from prediction errors accumulation, leading to undesired discontinuities in motion prediction. In this paper, we propose a simple feed-forward deep network for motion prediction, which takes into account both temporal smoothness and spatial dependencies among human body joints. In this context, we then propose to encode temporal information by working in trajectory space, instead of the traditionally-used pose space. This alleviates us from manually defining the range of temporal dependencies (or temporal convolutional filter size, as done in previous work). Moreover, spatial dependency of human pose is encoded by treating a human pose as a generic graph (rather than a human skeletal kinematic tree) formed by links between every pair of body joints. Instead of using a pre-defined graph structure, we design a new graph convolutional network to learn graph connectivity automatically. This allows the network to capture long range dependencies beyond that of human kinematic tree. We evaluate our approach on several standard benchmark datasets for motion prediction, including Human3.6M, the CMU motion capture dataset and 3DPW. Our experiments clearly demonstrate that the proposed approach achieves state of the art performance, and is applicable to both angle-based and position-based pose representations. The code is available at https://github.com/wei-mao-2019/LearnTrajDep",
        "中文标题": "学习轨迹依赖以进行人体运动预测",
        "摘要翻译": "人体运动预测，即在给定观察到的姿态序列的情况下预测未来的身体姿态，通常使用循环神经网络（RNNs）来解决。然而，正如先前的工作所证明的那样，由此产生的RNN模型遭受预测误差累积的问题，导致运动预测中出现不希望的不连续性。在本文中，我们提出了一个简单的前馈深度网络用于运动预测，该网络考虑了人体关节之间的时间平滑性和空间依赖性。在此背景下，我们提出通过在轨迹空间中工作来编码时间信息，而不是传统上使用的姿态空间。这使我们免于手动定义时间依赖性的范围（或时间卷积滤波器大小，如先前的工作中所做的那样）。此外，通过将人体姿态视为由每对身体关节之间的链接形成的通用图（而不是人体骨骼运动学树）来编码人体姿态的空间依赖性。我们设计了一个新的图卷积网络来自动学习图连接性，而不是使用预定义的图结构。这使得网络能够捕捉到超出人体运动学树的长距离依赖性。我们在几个标准的运动预测基准数据集上评估了我们的方法，包括Human3.6M、CMU运动捕捉数据集和3DPW。我们的实验清楚地表明，所提出的方法达到了最先进的性能，并且适用于基于角度和基于位置的姿态表示。代码可在https://github.com/wei-mao-2019/LearnTrajDep获取。",
        "领域": "运动预测/图卷积网络/时间序列分析",
        "问题": "解决人体运动预测中的预测误差累积问题",
        "动机": "为了克服循环神经网络在人体运动预测中因预测误差累积导致的不连续性问题",
        "方法": "提出了一种考虑时间平滑性和空间依赖性的前馈深度网络，通过在轨迹空间编码时间信息，并设计新的图卷积网络自动学习图连接性",
        "关键词": [
            "运动预测",
            "图卷积网络",
            "时间序列分析",
            "前馈深度网络",
            "轨迹空间"
        ],
        "涉及的技术概念": "循环神经网络（RNNs）、前馈深度网络、图卷积网络、时间序列分析、轨迹空间、人体运动学树"
    },
    {
        "order": 874,
        "title": "Fast Object Detection in Compressed Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Fast_Object_Detection_in_Compressed_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Fast_Object_Detection_in_Compressed_Video_ICCV_2019_paper.html",
        "abstract": "Object detection in videos has drawn increasing attention since it is more practical in real scenarios. Most of the deep learning methods use CNNs to process each decoded frame in a video stream individually. However, the free of charge yet valuable motion information already embedded in the video compression format is usually overlooked. In this paper, we propose a fast object detection method by taking advantage of this with a novel Motion aided Memory Network (MMNet). The MMNet has two major advantages: 1) It significantly accelerates the procedure of feature extraction for compressed videos. It only need to run a complete recognition network for I-frames, i.e. a few reference frames in a video, and it produces the features for the following P frames (predictive frames) with a light weight memory network, which runs fast; 2) Unlike existing methods that establish an additional network to model motion of frames, we take full advantage of both motion vectors and residual errors that are freely available in video streams. To our best knowledge, the MMNet is the first work that investigates a deep convolutional detector on compressed videos. Our method is evaluated on the large-scale ImageNet VID dataset, and the results show that it is 3x times faster than single image detector R-FCN and 10x times faster than high-performance detector MANet at a minor accuracy loss.",
        "中文标题": "压缩视频中的快速目标检测",
        "摘要翻译": "视频中的目标检测因其在实际场景中的实用性而受到越来越多的关注。大多数深度学习方法使用卷积神经网络（CNNs）单独处理视频流中的每个解码帧。然而，视频压缩格式中已经嵌入的免费且有价值的运动信息通常被忽视。在本文中，我们提出了一种快速目标检测方法，通过利用这些信息与一种新颖的运动辅助记忆网络（MMNet）。MMNet有两个主要优点：1）它显著加快了压缩视频特征提取的过程。它只需要为I帧（即视频中的一些参考帧）运行一个完整的识别网络，并为随后的P帧（预测帧）生成特征，使用一个轻量级的记忆网络，运行速度快；2）与现有方法不同，现有方法建立一个额外的网络来建模帧的运动，我们充分利用了视频流中免费可用的运动矢量和残差错误。据我们所知，MMNet是第一个研究压缩视频上深度卷积检测器的工作。我们的方法在大规模ImageNet VID数据集上进行了评估，结果显示，它在精度损失较小的情况下，比单图像检测器R-FCN快3倍，比高性能检测器MANet快10倍。",
        "领域": "视频分析/目标检测/压缩视频处理",
        "问题": "如何在压缩视频中快速有效地进行目标检测",
        "动机": "利用视频压缩格式中已经嵌入的运动信息，提高目标检测的速度和效率",
        "方法": "提出了一种新颖的运动辅助记忆网络（MMNet），通过仅对I帧运行完整的识别网络，并为P帧使用轻量级记忆网络生成特征，以及充分利用视频流中的运动矢量和残差错误",
        "关键词": [
            "视频分析",
            "目标检测",
            "压缩视频处理"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）、运动辅助记忆网络（MMNet）、I帧、P帧、运动矢量、残差错误、ImageNet VID数据集"
    },
    {
        "order": 875,
        "title": "SROBB: Targeted Perceptual Loss for Single Image Super-Resolution",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rad_SROBB_Targeted_Perceptual_Loss_for_Single_Image_Super-Resolution_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rad_SROBB_Targeted_Perceptual_Loss_for_Single_Image_Super-Resolution_ICCV_2019_paper.html",
        "abstract": "By benefiting from perceptual losses, recent studies have improved significantly the performance of the super-resolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objective functions generate near-photorealistic results, their capability is limited, since they estimate the reconstruction error for an entire image in the same way, without considering any semantic information. In this paper, we propose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based decoder with a targeted objective function that penalizes images at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed approach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and results of extensive user studies.",
        "中文标题": "SROBB：用于单图像超分辨率的定向感知损失",
        "摘要翻译": "得益于感知损失，最近的研究显著提高了超分辨率任务的性能，其中高分辨率图像是从其低分辨率对应物中解析出来的。尽管这样的目标函数生成了接近照片真实感的结果，但它们的能力是有限的，因为它们以相同的方式估计整个图像的重建误差，而没有考虑任何语义信息。在本文中，我们提出了一种新方法，以更客观的方式从感知损失中受益。我们优化了一个基于深度网络的解码器，使用定向目标函数，该函数使用相应的术语在不同语义级别上惩罚图像。特别是，所提出的方法利用我们从分割标签生成的OBB（对象、背景和边界）标签，来估计适合边界的感知损失，同时考虑背景的纹理相似性。我们展示了我们提出的方法在标准基准上的定性结果和广泛的用户研究结果方面，都产生了更真实的纹理和更锐利的边缘，并且优于其他最先进的算法。",
        "领域": "图像超分辨率/语义分割/感知损失",
        "问题": "如何更有效地利用感知损失来提高单图像超分辨率的性能",
        "动机": "现有的感知损失方法在估计整个图像的重建误差时没有考虑语义信息，限制了其性能",
        "方法": "提出了一种新方法，通过优化基于深度网络的解码器，并使用定向目标函数在不同语义级别上惩罚图像，特别是利用OBB标签来估计适合边界的感知损失，同时考虑背景的纹理相似性",
        "关键词": [
            "图像超分辨率",
            "语义分割",
            "感知损失",
            "OBB标签",
            "纹理相似性"
        ],
        "涉及的技术概念": {
            "感知损失": "一种用于图像处理的目标函数，旨在使生成的图像在感知上更接近真实图像",
            "OBB标签": "从分割标签生成的标签，用于区分图像中的对象、背景和边界",
            "纹理相似性": "在图像处理中，用于评估图像背景区域之间纹理的相似程度"
        }
    },
    {
        "order": 876,
        "title": "Cross-Domain Adaptation for Animal Pose Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.html",
        "abstract": "In this paper, we are interested in pose estimation of animals. Animals usually exhibit a wide range of variations on poses and there is no available animal pose dataset for training and testing. To address this problem, we build an animal pose dataset to facilitate training and evaluation. Considering the heavy labor needed to label dataset and it is impossible to label data for all concerned animal species, we, therefore, proposed a novel cross-domain adaptation method to transform the animal pose knowledge from labeled animal classes to unlabeled animal classes. We use the modest animal pose dataset to adapt learned knowledge to multiple animals species. Moreover, humans also share skeleton similarities with some animals (especially four-footed mammals). Therefore, the easily available human pose dataset, which is of a much larger scale than our labeled animal dataset, provides important prior knowledge to boost up the performance on animal pose estimation. Experiments show that our proposed method leverages these pieces of prior knowledge well and achieves convincing results on animal pose estimation.",
        "中文标题": "跨领域适应用于动物姿态估计",
        "摘要翻译": "在本文中，我们对动物的姿态估计感兴趣。动物通常展现出广泛的姿态变化，并且没有可用的动物姿态数据集用于训练和测试。为了解决这个问题，我们构建了一个动物姿态数据集以促进训练和评估。考虑到标注数据集需要大量劳动，并且不可能为所有相关动物物种标注数据，因此我们提出了一种新颖的跨领域适应方法，将动物姿态知识从已标注的动物类别转移到未标注的动物类别。我们使用适度的动物姿态数据集将学习到的知识适应到多种动物物种。此外，人类也与某些动物（尤其是四足哺乳动物）共享骨骼相似性。因此，易于获得的人类姿态数据集，其规模远大于我们标注的动物数据集，为提高动物姿态估计的性能提供了重要的先验知识。实验表明，我们提出的方法很好地利用了这些先验知识，并在动物姿态估计上取得了令人信服的结果。",
        "领域": "姿态估计/跨领域学习/数据集构建",
        "问题": "动物姿态估计中缺乏标注数据集的问题",
        "动机": "解决动物姿态估计中缺乏标注数据的问题，并利用人类姿态数据集提高动物姿态估计的性能",
        "方法": "提出了一种跨领域适应方法，将已标注动物类别的知识转移到未标注动物类别，并利用人类姿态数据集作为先验知识",
        "关键词": [
            "姿态估计",
            "跨领域适应",
            "数据集构建"
        ],
        "涉及的技术概念": "跨领域适应方法是一种技术，它允许将从一个领域（如人类姿态估计）学到的知识应用到另一个领域（如动物姿态估计）。这种方法特别适用于目标领域（动物姿态估计）缺乏标注数据的情况。通过利用源领域（人类姿态估计）的大量标注数据，可以提高目标领域的模型性能。此外，构建特定领域的数据集（如动物姿态数据集）是解决特定问题（如动物姿态估计）的关键步骤。"
    },
    {
        "order": 877,
        "title": "Predicting 3D Human Dynamics From Video",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Predicting_3D_Human_Dynamics_From_Video_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Predicting_3D_Human_Dynamics_From_Video_ICCV_2019_paper.html",
        "abstract": "Given a video of a person in action, we can easily guess the 3D future motion of the person. In this work, we present perhaps the first approach for predicting a future 3D mesh model sequence of a person from past video input. We do this for periodic motions such as walking and also actions like bowling and squatting seen in sports or workout videos. While there has been a surge of future prediction problems in computer vision, most approaches predict 3D future from 3D past or 2D future from 2D past inputs. In this work, we focus on the problem of predicting 3D future motion from past image sequences, which has a plethora of practical applications in autonomous systems that must operate safely around people from visual inputs. Inspired by the success of autoregressive models in language modeling tasks, we learn an intermediate latent space on which we predict the future. This effectively facilitates autoregressive predictions when the input differs from the output domain. Our approach can be trained on video sequences obtained in-the-wild without 3D ground truth labels. The project website with videos can be found at https://jasonyzhang.com/phd.",
        "中文标题": "从视频预测3D人体动态",
        "摘要翻译": "给定一个人行动的视频，我们可以很容易地猜测该人的3D未来动作。在这项工作中，我们提出了可能是第一个从过去的视频输入预测人的未来3D网格模型序列的方法。我们对周期性动作（如行走）以及体育或锻炼视频中看到的动作（如保龄球和下蹲）进行了这样的预测。尽管计算机视觉中未来预测问题激增，但大多数方法都是从3D过去预测3D未来或从2D过去输入预测2D未来。在这项工作中，我们专注于从过去的图像序列预测3D未来动作的问题，这在必须从视觉输入安全地围绕人操作的自主系统中具有大量实际应用。受到自回归模型在语言建模任务中成功的启发，我们学习了一个中间潜在空间，在该空间上我们预测未来。这有效地促进了当输入与输出域不同时的自回归预测。我们的方法可以在没有3D地面真实标签的情况下对野外获得的视频序列进行训练。项目网站上的视频可以在https://jasonyzhang.com/phd找到。",
        "领域": "3D人体动态预测/视频分析/自回归模型",
        "问题": "从过去的图像序列预测3D未来动作",
        "动机": "在必须从视觉输入安全地围绕人操作的自主系统中具有大量实际应用",
        "方法": "学习一个中间潜在空间，在该空间上预测未来，促进当输入与输出域不同时的自回归预测",
        "关键词": [
            "3D人体动态预测",
            "视频分析",
            "自回归模型"
        ],
        "涉及的技术概念": "自回归模型是一种统计模型，用于预测时间序列数据中的未来值，基于其自身的过去值。在这项工作中，自回归模型被用来预测3D未来动作，通过从视频中学习一个中间潜在空间。这种方法允许在没有3D地面真实标签的情况下对视频序列进行训练，从而扩展了其在自主系统等实际应用中的可用性。"
    },
    {
        "order": 878,
        "title": "An Internal Learning Approach to Video Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_An_Internal_Learning_Approach_to_Video_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_An_Internal_Learning_Approach_to_Video_Inpainting_ICCV_2019_paper.html",
        "abstract": "We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.",
        "中文标题": "一种内部学习方法的视频修复",
        "摘要翻译": "我们提出了一种新颖的视频修复算法，该算法同时幻觉缺失的外观和运动（光流）信息，基于最近提出的'深度图像先验'（DIP），该先验利用卷积网络架构在静态图像中强制执行合理的纹理。在将DIP扩展到视频时，我们做出了两个重要贡献。首先，我们展示了在没有先验训练的情况下，连贯的视频修复是可能的。我们采用了一种基于内部（视频内）学习的生成方法进行修复，不依赖于外部视觉数据语料库来训练一个适用于一般视频大空间的一刀切模型。其次，我们展示了这样的框架可以联合生成外观和流，同时利用这些互补的模态来确保相互一致性。我们展示了利用每个视频特定的外观统计数据可以实现视觉上合理的结果，同时处理长期一致性的挑战性问题。",
        "领域": "视频修复/光流估计/生成模型",
        "问题": "视频中缺失信息的修复，包括外观和运动信息",
        "动机": "探索在没有先验训练的情况下实现连贯视频修复的可能性，以及通过内部学习生成外观和流信息，确保修复结果的视觉合理性和长期一致性",
        "方法": "基于深度图像先验（DIP）的扩展，采用内部学习方法，不依赖外部数据训练，联合生成视频的外观和光流信息，并利用这些信息的互补性确保修复结果的一致性",
        "关键词": [
            "视频修复",
            "光流估计",
            "生成模型",
            "内部学习",
            "深度图像先验"
        ],
        "涉及的技术概念": {
            "深度图像先验（DIP）": "一种利用卷积网络架构在静态图像中强制执行合理纹理的技术",
            "内部学习": "一种不依赖外部数据，仅利用视频内部信息进行学习的方法",
            "光流": "视频中物体运动的信息，用于描述视频帧之间的运动"
        }
    },
    {
        "order": 879,
        "title": "NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_NOTE-RCNN_NOise_Tolerant_Ensemble_RCNN_for_Semi-Supervised_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_NOTE-RCNN_NOise_Tolerant_Ensemble_RCNN_for_Semi-Supervised_Object_Detection_ICCV_2019_paper.html",
        "abstract": "The labeling cost of large number of bounding boxes is one of the main challenges for training modern object detectors. To reduce the dependence on expensive bounding box annotations, we propose a new semi-supervised object detection formulation, in which a few seed box level annotations and a large scale of image level annotations are used to train the detector. We adopt a training-mining framework, which is widely used in weakly supervised object detection tasks. However, the mining process inherently introduces various kinds of labelling noises: false negatives, false positives and inaccurate boundaries, which can be harmful for training the standard object detectors (e.g. Faster RCNN). We propose a novel NOise Tolerant Ensemble RCNN (NOTE-RCNN) object detector to handle such noisy labels. Comparing to standard Faster RCNN, it contains three highlights: an ensemble of two classification heads and a distillation head to avoid overfitting on noisy labels and improve the mining precision, masking the negative sample loss in box predictor to avoid the harm of false negative labels, and training box regression head only on seed annotations to eliminate the harm from inaccurate boundaries of mined bounding boxes. We evaluate the methods on ILSVRC 2013 and MSCOCO 2017 dataset; we observe that the detection accuracy consistently improves as we iterate between mining and training steps, and state-of-the-art performance is achieved.",
        "中文标题": "NOTE-RCNN: 用于半监督目标检测的噪声容忍集成RCNN",
        "摘要翻译": "大量边界框的标注成本是训练现代目标检测器的主要挑战之一。为了减少对昂贵的边界框标注的依赖，我们提出了一种新的半监督目标检测方法，其中使用少量种子框级别的标注和大量的图像级别标注来训练检测器。我们采用了在弱监督目标检测任务中广泛使用的训练-挖掘框架。然而，挖掘过程本质上引入了各种类型的标注噪声：假阴性、假阳性和不准确的边界，这些噪声可能对训练标准目标检测器（例如Faster RCNN）有害。我们提出了一种新颖的噪声容忍集成RCNN（NOTE-RCNN）目标检测器来处理这种噪声标签。与标准的Faster RCNN相比，它包含三个亮点：两个分类头和一个蒸馏头的集成，以避免在噪声标签上过拟合并提高挖掘精度，在框预测器中屏蔽负样本损失以避免假阴性标签的伤害，以及仅在种子标注上训练框回归头以消除挖掘边界框不准确边界带来的伤害。我们在ILSVRC 2013和MSCOCO 2017数据集上评估了这些方法；我们观察到，随着我们在挖掘和训练步骤之间迭代，检测精度持续提高，并达到了最先进的性能。",
        "领域": "目标检测/半监督学习/噪声容忍学习",
        "问题": "减少对昂贵边界框标注的依赖，同时处理标注噪声问题",
        "动机": "为了降低目标检测任务中的标注成本，并解决标注噪声对检测器训练的影响",
        "方法": "提出了一种新的半监督目标检测方法，采用训练-挖掘框架，并引入了噪声容忍集成RCNN（NOTE-RCNN）目标检测器，通过集成分类头、蒸馏头、屏蔽负样本损失和仅在种子标注上训练框回归头等策略来处理噪声标签",
        "关键词": [
            "半监督学习",
            "噪声容忍",
            "目标检测"
        ],
        "涉及的技术概念": "半监督学习是一种利用少量标注数据和大量未标注数据进行学习的方法。噪声容忍学习是指在学习过程中能够容忍和处理数据中的噪声。目标检测是计算机视觉中的一个任务，旨在识别图像中的对象并确定它们的位置。"
    },
    {
        "order": 880,
        "title": "Imitation Learning for Human Pose Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Imitation_Learning_for_Human_Pose_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Imitation_Learning_for_Human_Pose_Prediction_ICCV_2019_paper.html",
        "abstract": "Modeling and prediction of human motion dynamics has long been a challenging problem in computer vision, and most existing methods rely on the end-to-end supervised training of various architectures of recurrent neural networks. Inspired by the recent success of deep reinforcement learning methods, in this paper we propose a new reinforcement learning formulation for the problem of human pose prediction, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning. Our experiments show that our proposed method outperforms all existing state-of-the-art baseline models by large margins on the task of human pose prediction in both short-term predictions and long-term predictions, while also enjoying huge advantage in training speed.",
        "中文标题": "模仿学习用于人体姿态预测",
        "摘要翻译": "人体运动动态的建模和预测长期以来一直是计算机视觉中的一个具有挑战性的问题，大多数现有方法依赖于各种循环神经网络架构的端到端监督训练。受到深度强化学习方法最近成功的启发，本文提出了一种新的强化学习公式来解决人体姿态预测问题，并通过行为克隆和生成对抗模仿学习的结合，开发了一种模仿学习算法来预测未来姿态。我们的实验表明，我们提出的方法在人体姿态预测任务中，无论是在短期预测还是长期预测上，都大幅超越了所有现有的最先进基线模型，同时在训练速度上也享有巨大优势。",
        "领域": "人体姿态预测/强化学习/模仿学习",
        "问题": "人体运动动态的建模和预测",
        "动机": "受到深度强化学习方法最近成功的启发，提出新的强化学习公式来解决人体姿态预测问题",
        "方法": "通过行为克隆和生成对抗模仿学习的结合，开发了一种模仿学习算法来预测未来姿态",
        "关键词": [
            "人体姿态预测",
            "强化学习",
            "模仿学习",
            "行为克隆",
            "生成对抗模仿学习"
        ],
        "涉及的技术概念": {
            "行为克隆": "一种模仿学习技术，通过观察专家的行为来学习策略",
            "生成对抗模仿学习": "结合生成对抗网络和模仿学习的技术，用于生成与专家行为相似的数据"
        }
    },
    {
        "order": 881,
        "title": "Deep CG2Real: Synthetic-to-Real Translation via Image Disentanglement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Deep_CG2Real_Synthetic-to-Real_Translation_via_Image_Disentanglement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Deep_CG2Real_Synthetic-to-Real_Translation_via_Image_Disentanglement_ICCV_2019_paper.html",
        "abstract": "We present a method to improve the visual realism of low-quality, synthetic images, e.g. OpenGL renderings. Training an unpaired synthetic-to-real translation network in image space is severely under-constrained and produces visible artifacts. Instead, we propose a semi-supervised approach that operates on the disentangled shading and albedo layers of the image. Our two-stage pipeline first learns to predict accurate shading in a supervised fashion using physically-based renderings as targets, and further increases the realism of the textures and shading with an improved CycleGAN network. Extensive evaluations on the SUNCG indoor scene dataset demonstrate that our approach yields more realistic images compared to other state-of-the-art approaches. Furthermore, networks trained on our generated \"real\" images predict more accurate depth and normals than domain adaptation approaches, suggesting that improving the visual realism of the images can be more effective than imposing task-specific losses.",
        "中文标题": "深度CG2Real：通过图像解缠实现合成到真实的转换",
        "摘要翻译": "我们提出了一种方法来提高低质量合成图像（例如OpenGL渲染）的视觉真实感。在图像空间中训练一个无配对的合成到真实转换网络是严重不足的，并且会产生可见的伪影。相反，我们提出了一种半监督方法，该方法操作于图像的分离的阴影和反照率层。我们的两阶段管道首先学习以监督方式使用基于物理的渲染作为目标来预测准确的阴影，并通过改进的CycleGAN网络进一步提高纹理和阴影的真实感。在SUNCG室内场景数据集上的广泛评估表明，与其他最先进的方法相比，我们的方法产生了更真实的图像。此外，使用我们生成的“真实”图像训练的网络比领域适应方法预测的深度和法线更准确，这表明提高图像的视觉真实感可能比施加特定任务的损失更有效。",
        "领域": "图像渲染/图像增强/图像转换",
        "问题": "提高低质量合成图像的视觉真实感",
        "动机": "解决在图像空间中训练无配对的合成到真实转换网络时产生的可见伪影问题",
        "方法": "提出一种半监督方法，操作于图像的分离的阴影和反照率层，通过两阶段管道学习预测准确的阴影，并通过改进的CycleGAN网络提高纹理和阴影的真实感",
        "关键词": [
            "图像渲染",
            "图像增强",
            "图像转换"
        ],
        "涉及的技术概念": "图像解缠、阴影和反照率层、半监督学习、CycleGAN网络、物理基于渲染、SUNCG室内场景数据集"
    },
    {
        "order": 882,
        "title": "Unsupervised Out-of-Distribution Detection by Maximum Classifier Discrepancy",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Unsupervised_Out-of-Distribution_Detection_by_Maximum_Classifier_Discrepancy_ICCV_2019_paper.html",
        "abstract": "Since deep learning models have been implemented in many commercial applications, it is important to detect out-of-distribution (OOD) inputs correctly to maintain the performance of the models, ensure the quality of the collected data, and prevent the applications from being used for other-than-intended purposes. In this work, we propose a two-head deep convolutional neural network (CNN) and maximize the discrepancy between the two classifiers to detect OOD inputs. We train a two-head CNN consisting of one common feature extractor and two classifiers which have different decision boundaries but can classify in-distribution (ID) samples correctly. Unlike previous methods, we also utilize unlabeled data for unsupervised training and we use these unlabeled data to maximize the discrepancy between the decision boundaries of two classifiers to push OOD samples outside the manifold of the in-distribution (ID) samples, which enables us to detect OOD samples that are far from the support of the ID samples. Overall, our approach significantly outperforms other state-of-the-art methods on several OOD detection benchmarks and two cases of real-world simulation.",
        "中文标题": "通过最大分类器差异进行无监督分布外检测",
        "摘要翻译": "由于深度学习模型已在许多商业应用中得到实施，正确检测分布外（OOD）输入对于维持模型性能、确保收集数据的质量以及防止应用被用于非预期目的至关重要。在这项工作中，我们提出了一个双头深度卷积神经网络（CNN），并通过最大化两个分类器之间的差异来检测OOD输入。我们训练了一个由共同特征提取器和两个分类器组成的双头CNN，这两个分类器具有不同的决策边界但能正确分类分布内（ID）样本。与之前的方法不同，我们还利用未标记数据进行无监督训练，并使用这些未标记数据来最大化两个分类器决策边界之间的差异，以将OOD样本推离ID样本的流形，这使我们能够检测远离ID样本支持的OOD样本。总体而言，我们的方法在多个OOD检测基准和两个真实世界模拟案例中显著优于其他最先进的方法。",
        "领域": "异常检测/无监督学习/卷积神经网络",
        "问题": "检测深度学习模型中的分布外（OOD）输入",
        "动机": "维持模型性能、确保数据质量、防止应用被用于非预期目的",
        "方法": "提出一个双头深度卷积神经网络（CNN），通过最大化两个分类器之间的差异来检测OOD输入，并利用未标记数据进行无监督训练",
        "关键词": [
            "异常检测",
            "无监督学习",
            "卷积神经网络",
            "分布外检测"
        ],
        "涉及的技术概念": "双头深度卷积神经网络（CNN）、分布外（OOD）输入检测、无监督训练、决策边界差异最大化"
    },
    {
        "order": 883,
        "title": "Adversarial Defense via Learning to Generate Diverse Attacks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jang_Adversarial_Defense_via_Learning_to_Generate_Diverse_Attacks_ICCV_2019_paper.html",
        "abstract": "With the remarkable success of deep learning, Deep Neural Networks (DNNs) have been applied as dominant tools to various machine learning domains. Despite this success, however, it has been found that DNNs are surprisingly vulnerable to malicious attacks; adding a small, perceptually indistinguishable perturbations to the data can easily degrade classification performance. Adversarial training is an effective defense strategy to train a robust classifier. In this work, we propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbation by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier. Our experiment results on MNIST and CIFAR-10 datasets show that the classifier adversarially trained with our method yields more robust performance over various white-box and black-box attacks.",
        "中文标题": "通过学会生成多样化攻击进行对抗防御",
        "摘要翻译": "随着深度学习的显著成功，深度神经网络（DNNs）已被作为主导工具应用于各种机器学习领域。然而，尽管取得了这样的成功，人们发现DNNs对恶意攻击出奇地脆弱；向数据添加小的、感知上不可区分的扰动可以轻易降低分类性能。对抗训练是一种有效的防御策略，用于训练一个鲁棒的分类器。在这项工作中，我们提出利用生成器学习如何创建对抗样本。与现有方法通过确定性生成器创建一次性扰动不同，我们提出了一种递归和随机的生成器，它产生更强和更多样化的扰动，全面揭示了目标分类器的脆弱性。我们在MNIST和CIFAR-10数据集上的实验结果表明，使用我们的方法进行对抗训练的分类器在各种白盒和黑盒攻击下表现出更鲁棒的性能。",
        "领域": "对抗学习/网络安全/模型鲁棒性",
        "问题": "深度神经网络对恶意攻击的脆弱性问题",
        "动机": "提高深度神经网络在面对恶意攻击时的鲁棒性",
        "方法": "提出一种递归和随机的生成器，用于产生更强和更多样化的扰动，以全面揭示目标分类器的脆弱性，并通过对抗训练提高分类器的鲁棒性",
        "关键词": [
            "对抗样本",
            "对抗训练",
            "模型鲁棒性",
            "递归生成器",
            "随机生成器"
        ],
        "涉及的技术概念": {
            "对抗样本": "通过添加小的、感知上不可区分的扰动到数据上，以降低分类性能的样本",
            "对抗训练": "一种训练策略，通过将对抗样本纳入训练过程，提高模型的鲁棒性",
            "递归生成器": "一种生成器，能够递归地产生扰动，以增强扰动的强度和多样性",
            "随机生成器": "一种生成器，通过引入随机性来产生多样化的扰动",
            "白盒攻击": "攻击者完全了解目标模型的内部结构和参数的攻击方式",
            "黑盒攻击": "攻击者不了解目标模型的内部结构和参数，仅通过输入输出对模型进行攻击的方式"
        }
    },
    {
        "order": 884,
        "title": "Human Motion Prediction via Spatio-Temporal Inpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hernandez_Human_Motion_Prediction_via_Spatio-Temporal_Inpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hernandez_Human_Motion_Prediction_via_Spatio-Temporal_Inpainting_ICCV_2019_paper.html",
        "abstract": "We propose a Generative Adversarial Network (GAN) to forecast 3D human motion given a sequence of past 3D skeleton poses. While recent GANs have shown promising results, they can only forecast plausible motion over relatively short periods of time (few hundred milliseconds) and typically ignore the absolute position of the skeleton w.r.t. the camera. Our scheme provides long term predictions (two seconds or more) for both the body pose and its absolute position. Our approach builds upon three main contributions. First, we represent the data using a spatio-temporal tensor of 3D skeleton coordinates which allows formulating the prediction problem as an inpainting one, for which GANs work particularly well. Secondly, we design an architecture to learn the joint distribution of body poses and global motion, capable to hypothesize large chunks of the input 3D tensor with missing data. And finally, we argue that the L2 metric, considered so far by most approaches, fails to capture the actual distribution of long-term human motion. We propose two alternative metrics, based on the distribution of frequencies, that are able to capture more realistic motion patterns. Extensive experiments demonstrate our approach to significantly improve the state of the art, while also handling situations in which past observations are corrupted by occlusions, noise and missing frames.",
        "中文标题": "通过时空修复进行人体运动预测",
        "摘要翻译": "我们提出了一种生成对抗网络（GAN），用于根据过去的三维骨架姿势序列预测三维人体运动。尽管最近的GAN显示出有希望的结果，但它们只能在相对较短的时间内（几百毫秒）预测合理的运动，并且通常忽略了骨架相对于摄像机的绝对位置。我们的方案提供了对身体姿势及其绝对位置的长期预测（两秒或更长时间）。我们的方法基于三个主要贡献。首先，我们使用三维骨架坐标的时空张量来表示数据，这使得预测问题可以表述为一个修复问题，对于这种问题，GAN特别有效。其次，我们设计了一种架构来学习身体姿势和全局运动的联合分布，能够假设输入的三维张量中缺失数据的大块。最后，我们认为，到目前为止大多数方法所考虑的L2度量无法捕捉长期人体运动的实际分布。我们提出了两种基于频率分布的替代度量，能够捕捉更真实的运动模式。大量实验证明，我们的方法显著提高了现有技术水平，同时还能处理过去观察被遮挡、噪声和缺失帧所破坏的情况。",
        "领域": "人体运动预测/三维重建/生成对抗网络",
        "问题": "预测三维人体运动，特别是在长期预测和考虑骨架绝对位置方面",
        "动机": "现有的GAN方法在长期预测和考虑骨架绝对位置方面存在不足，需要一种能够提供更长期、更准确预测的方法",
        "方法": "使用三维骨架坐标的时空张量表示数据，设计一种架构学习身体姿势和全局运动的联合分布，并提出两种基于频率分布的替代度量",
        "关键词": [
            "人体运动预测",
            "三维重建",
            "生成对抗网络",
            "时空张量",
            "长期预测"
        ],
        "涉及的技术概念": {
            "生成对抗网络（GAN）": "一种深度学习模型，通过生成器和判别器的对抗过程来生成数据",
            "三维骨架坐标": "用于表示人体姿势的三维空间中的点集",
            "时空张量": "一种数据结构，用于表示在时间和空间上变化的数据",
            "L2度量": "一种常用的距离度量方法，用于计算两个向量之间的欧氏距离",
            "频率分布": "数据在频率域上的分布，用于分析数据的周期性特征"
        }
    },
    {
        "order": 885,
        "title": "SBSGAN: Suppression of Inter-Domain Background Shift for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_SBSGAN_Suppression_of_Inter-Domain_Background_Shift_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "Cross-domain person re-identification (re-ID) is challenging due to the bias between training and testing domains. We observe that if backgrounds in the training and testing datasets are very different, it dramatically introduces difficulties to extract robust pedestrian features, and thus compromises the cross-domain person re-ID performance. In this paper, we formulate such problems as a background shift problem. A Suppression of Background Shift Generative Adversarial Network (SBSGAN) is proposed to generate images with suppressed backgrounds. Unlike simply removing backgrounds using binary masks, SBSGAN allows the generator to decide whether pixels should be preserved or suppressed to reduce segmentation errors caused by noisy foreground masks. Additionally, we take ID-related cues, such as vehicles and companions into consideration. With high-quality generated images, a Densely Associated 2-Stream (DA-2S) network is introduced with Inter Stream Densely Connection (ISDC) modules to strengthen the complementarity of the generated data and ID-related cues. The experiments show that the proposed method achieves competitive performance on three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and CUHK03, under the cross-domain person re-ID scenario.",
        "中文标题": "SBSGAN: 抑制跨域背景偏移以实现行人重识别",
        "摘要翻译": "跨域行人重识别（re-ID）由于训练和测试域之间的偏差而具有挑战性。我们观察到，如果训练和测试数据集中的背景差异很大，这将极大地增加提取鲁棒行人特征的难度，从而影响跨域行人重识别的性能。在本文中，我们将此类问题表述为背景偏移问题。提出了一种抑制背景偏移生成对抗网络（SBSGAN）来生成具有抑制背景的图像。与简单地使用二值掩码去除背景不同，SBSGAN允许生成器决定是否应保留或抑制像素，以减少由噪声前景掩码引起的分割错误。此外，我们还考虑了与ID相关的线索，如车辆和同伴。利用高质量生成的图像，引入了具有流间密集连接（ISDC）模块的密集关联双流（DA-2S）网络，以增强生成数据和ID相关线索的互补性。实验表明，所提出的方法在跨域行人重识别场景下，在三个re-ID数据集（即Market-1501、DukeMTMC-reID和CUHK03）上实现了竞争性能。",
        "领域": "行人重识别/生成对抗网络/图像生成",
        "问题": "跨域行人重识别中由于训练和测试数据集背景差异大导致的性能下降问题",
        "动机": "提高跨域行人重识别的性能，通过抑制背景偏移来提取更鲁棒的行人特征",
        "方法": "提出抑制背景偏移生成对抗网络（SBSGAN）生成具有抑制背景的图像，并引入密集关联双流（DA-2S）网络增强生成数据和ID相关线索的互补性",
        "关键词": [
            "行人重识别",
            "生成对抗网络",
            "背景偏移",
            "图像生成",
            "密集关联双流网络"
        ],
        "涉及的技术概念": {
            "SBSGAN": "抑制背景偏移生成对抗网络，用于生成具有抑制背景的图像，减少由噪声前景掩码引起的分割错误",
            "DA-2S网络": "密集关联双流网络，通过流间密集连接（ISDC）模块增强生成数据和ID相关线索的互补性",
            "ISDC模块": "流间密集连接模块，用于在DA-2S网络中增强数据流的互补性"
        }
    },
    {
        "order": 886,
        "title": "Image Generation From Small Datasets via Batch Statistics Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Noguchi_Image_Generation_From_Small_Datasets_via_Batch_Statistics_Adaptation_ICCV_2019_paper.html",
        "abstract": "Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small ( 100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain. Code is available at github.com/nogu-atsu/small-dataset-image-generation",
        "中文标题": "通过批量统计适应从小数据集中生成图像",
        "摘要翻译": "得益于深度生成模型的最新发展，生成既保真又多样化的高质量图像正变得可能。然而，这类生成模型的训练需要大量数据集。为了减少所需的数据量，我们提出了一种新方法，用于将预训练生成器的先验知识（该生成器是用大数据集训练的）转移到不同领域的小数据集中。利用这种先验知识，模型可以生成利用一些从小数据集中无法获得的常识的图像。在这项工作中，我们提出了一种专注于生成器隐藏层的批量统计参数、比例和偏移的新方法。通过仅以监督方式训练这些参数，我们实现了生成器的稳定训练，并且我们的方法可以在数据集较小（100）的情况下，与之前的方法相比生成更高质量的图像而不会崩溃。我们的结果表明，预训练生成器中获得的滤波器多样性对于目标域的性能至关重要。我们的方法使得在不干扰原始域性能的情况下，向预训练生成器添加新类别或领域成为可能。代码可在github.com/nogu-atsu/small-dataset-image-generation获取。",
        "领域": "图像生成/迁移学习/小样本学习",
        "问题": "减少生成高质量图像所需的数据量",
        "动机": "深度生成模型训练需要大量数据，但在许多情况下，获取大量数据是不现实的。因此，研究如何减少所需数据量，同时保持或提高生成图像的质量和多样性，具有重要的实际意义。",
        "方法": "提出了一种新方法，通过仅以监督方式训练生成器隐藏层的批量统计参数、比例和偏移，利用预训练生成器的先验知识来生成高质量图像。",
        "关键词": [
            "图像生成",
            "迁移学习",
            "小样本学习",
            "批量统计适应"
        ],
        "涉及的技术概念": "深度生成模型、预训练生成器、批量统计参数、监督学习、图像质量、数据集大小、滤波器多样性"
    },
    {
        "order": 887,
        "title": "Structured Prediction Helps 3D Human Motion Modelling",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Aksan_Structured_Prediction_Helps_3D_Human_Motion_Modelling_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Aksan_Structured_Prediction_Helps_3D_Human_Motion_Modelling_ICCV_2019_paper.html",
        "abstract": "Human motion prediction is a challenging and important task in many computer vision application domains. Existing work only implicitly models the spatial structure of the human skeleton. In this paper, we propose a novel approach that decomposes the prediction into individual joints by means of a structured prediction layer that explicitly models the joint dependencies. This is implemented via a hierarchy of small-sized neural networks connected analogously to the kinematic chains in the human body as well as a joint-wise decomposition in the loss function. The proposed layer is agnostic to the underlying network and can be used with existing architectures for motion modelling. Prior work typically leverages the H3.6M dataset. We show that some state-of-the-art techniques do not perform well when trained and tested on AMASS, a recently released dataset 14 times the size of H3.6M. Our experiments indicate that the proposed layer increases the performance of motion forecasting irrespective of the base network, joint-angle representation, and prediction horizon. We furthermore show that the layer also improves motion predictions qualitatively. We make code and models publicly available at https://ait.ethz.ch/projects/2019/spl.",
        "中文标题": "结构化预测助力3D人体运动建模",
        "摘要翻译": "人体运动预测在许多计算机视觉应用领域中是一个具有挑战性且重要的任务。现有的工作仅隐式地模拟了人体骨骼的空间结构。在本文中，我们提出了一种新颖的方法，通过一个结构化预测层将预测分解为各个关节，该层显式地模拟了关节之间的依赖关系。这是通过一系列小型神经网络的层次结构实现的，这些网络以类似于人体运动链的方式连接，并在损失函数中进行关节级分解。所提出的层对底层网络是不可知的，可以与现有的运动建模架构一起使用。先前的工作通常利用H3.6M数据集。我们展示了一些最先进的技术在AMASS（一个最近发布的、大小是H3.6M 14倍的数据集）上训练和测试时表现不佳。我们的实验表明，无论基础网络、关节角度表示和预测范围如何，所提出的层都能提高运动预测的性能。我们还展示了该层在质量上也改善了运动预测。我们在https://ait.ethz.ch/projects/2019/spl上公开了代码和模型。",
        "领域": "3D人体运动建模/运动预测/神经网络",
        "问题": "如何提高3D人体运动预测的准确性和质量",
        "动机": "现有的3D人体运动预测方法未能充分模拟人体骨骼的空间结构，限制了预测的准确性和质量",
        "方法": "提出了一种新颖的结构化预测层，通过显式模拟关节依赖关系，将预测分解为各个关节，并采用一系列小型神经网络的层次结构和关节级分解的损失函数来实现",
        "关键词": [
            "3D人体运动建模",
            "运动预测",
            "神经网络",
            "结构化预测",
            "关节依赖"
        ],
        "涉及的技术概念": "结构化预测层是一种新颖的方法，用于显式模拟人体关节之间的依赖关系，通过小型神经网络的层次结构和关节级分解的损失函数来实现。这种方法不依赖于特定的底层网络架构，可以与现有的运动建模技术结合使用。"
    },
    {
        "order": 888,
        "title": "Enriched Feature Guided Refinement Network for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Enriched_Feature_Guided_Refinement_Network_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Enriched_Feature_Guided_Refinement_Network_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "We propose a single-stage detection framework that jointly tackles the problem of multi-scale object detection and class imbalance. Rather than designing deeper networks, we introduce a simple yet effective feature enrichment scheme to produce multi-scale contextual features. We further introduce a cascaded refinement scheme which first instills multi-scale contextual features into the prediction layers of the single-stage detector in order to enrich their discriminative power for multi-scale detection. Second, the cascaded refinement scheme counters the class imbalance problem by refining the anchors and enriched features to improve classification and regression. Experiments are performed on two benchmarks: PASCAL VOC and MS COCO. For a 320x320 input on the MS COCO test-dev, our detector achieves state-of-the-art single-stage detection accuracy with a COCO AP of 33.2 in the case of single-scale inference, while operating at 21 milliseconds on a Titan XP GPU. For a 512x512 input on the MS COCO test-dev, our approach obtains an absolute gain of 1.6% in terms of COCO AP, compared to the best reported single-stage results[5]. Source code and models are available at: https://github.com/Ranchentx/EFGRNet.",
        "中文标题": "用于目标检测的丰富特征引导精炼网络",
        "摘要翻译": "我们提出了一个单阶段检测框架，该框架共同解决了多尺度目标检测和类别不平衡的问题。我们没有设计更深的网络，而是引入了一种简单但有效的特征丰富方案，以产生多尺度上下文特征。我们进一步引入了一种级联精炼方案，该方案首先将多尺度上下文特征注入到单阶段检测器的预测层中，以增强其对多尺度检测的判别能力。其次，级联精炼方案通过精炼锚点和丰富特征来改善分类和回归，从而对抗类别不平衡问题。实验在两个基准上进行：PASCAL VOC和MS COCO。对于MS COCO测试开发集上的320x320输入，我们的检测器在单尺度推理的情况下实现了最先进的单阶段检测精度，COCO AP为33.2，同时在Titan XP GPU上运行时间为21毫秒。对于MS COCO测试开发集上的512x512输入，我们的方法在COCO AP方面获得了1.6%的绝对增益，与最佳报告的单阶段结果相比[5]。源代码和模型可在以下网址获取：https://github.com/Ranchentx/EFGRNet。",
        "领域": "目标检测/特征提取/类别不平衡",
        "问题": "多尺度目标检测和类别不平衡",
        "动机": "提高单阶段检测器在多尺度目标检测和类别不平衡问题上的性能",
        "方法": "引入特征丰富方案和级联精炼方案，通过精炼锚点和丰富特征来改善分类和回归",
        "关键词": [
            "单阶段检测",
            "多尺度检测",
            "类别不平衡",
            "特征丰富",
            "级联精炼"
        ],
        "涉及的技术概念": {
            "单阶段检测框架": "一种目标检测方法，直接在输入图像上预测目标类别和位置，无需区域提议阶段。",
            "多尺度上下文特征": "通过特征丰富方案产生的，能够捕捉不同尺度目标信息的特征。",
            "级联精炼方案": "一种通过逐步精炼特征和锚点来改善检测性能的方法。",
            "COCO AP": "在MS COCO数据集上评估目标检测模型性能的指标，表示平均精度。"
        }
    },
    {
        "order": 889,
        "title": "Lifelong GAN: Continual Learning for Conditional Image Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhai_Lifelong_GAN_Continual_Learning_for_Conditional_Image_Generation_ICCV_2019_paper.html",
        "abstract": "Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.",
        "中文标题": "终身GAN：条件图像生成的持续学习",
        "摘要翻译": "终身学习对于深度神经网络来说是一个挑战，因为它们容易遭受灾难性遗忘。灾难性遗忘发生在训练好的网络在学习新任务时无法保持完成先前学习任务的能力。我们研究了生成模型的终身学习问题，扩展了一个训练好的网络以执行新的条件生成任务而不忘记之前的任务，同时假设只能访问当前任务的训练数据。与仅限于标签条件图像生成任务的最先进的基于记忆重放的方法相比，本文提出了一个更通用的框架，用于在不同条件图像生成设置下生成模型的持续学习。终身GAN采用知识蒸馏将学习到的知识从先前的网络转移到新网络。这使得在终身学习设置下执行图像条件生成任务成为可能。我们验证了终身GAN在图像条件和标签条件生成任务中的有效性，并提供了定性和定量结果来展示我们方法的通用性和有效性。",
        "领域": "生成对抗网络/持续学习/条件图像生成",
        "问题": "深度神经网络在终身学习过程中容易遭受灾难性遗忘，导致无法保持完成先前学习任务的能力。",
        "动机": "研究生成模型的终身学习问题，旨在扩展训练好的网络以执行新的条件生成任务而不忘记之前的任务。",
        "方法": "提出了一种终身GAN框架，采用知识蒸馏技术将学习到的知识从先前的网络转移到新网络，以支持图像条件和标签条件生成任务的持续学习。",
        "关键词": [
            "终身学习",
            "生成对抗网络",
            "条件图像生成",
            "知识蒸馏"
        ],
        "涉及的技术概念": "灾难性遗忘指的是神经网络在学习新任务时忘记旧任务的现象。知识蒸馏是一种技术，用于将一个网络的知识转移到另一个网络，以减少灾难性遗忘的影响。"
    },
    {
        "order": 890,
        "title": "Learning Shape Templates With Structured Implicit Functions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Genova_Learning_Shape_Templates_With_Structured_Implicit_Functions_ICCV_2019_paper.html",
        "abstract": "Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image.",
        "中文标题": "学习具有结构化隐函数的形状模板",
        "摘要翻译": "模板3D形状对于图形和视觉中的许多任务非常有用，包括拟合观测数据、分析形状集合和转移形状属性。由于现实世界形状的几何和拓扑多样性，以前的方法通常使用手工制作的模板库。在本文中，我们研究了从数据中学习通用形状模板。为了适应广泛变化的几何和拓扑，我们选择了基于局部形状元素组合的隐式表面表示。虽然这种表示在计算机图形学中早已为人所知，但在视觉的机器学习背景下尚未被探索。我们展示了结构化隐函数适合学习，并允许网络平滑且同时拟合多类形状。学习的形状模板支持从RGB图像进行形状探索、对应、抽象、插值和语义分割等应用。",
        "领域": "3D形状建模/隐式表面表示/形状分析",
        "问题": "如何从数据中学习一个通用的3D形状模板以适应现实世界形状的几何和拓扑多样性",
        "动机": "现实世界形状的几何和拓扑多样性要求一种能够适应这种多样性的通用形状模板学习方法，以支持图形和视觉中的多种应用",
        "方法": "采用基于局部形状元素组合的隐式表面表示，探索结构化隐函数在视觉机器学习中的应用，以学习能够平滑且同时拟合多类形状的通用形状模板",
        "关键词": [
            "3D形状建模",
            "隐式表面表示",
            "形状分析"
        ],
        "涉及的技术概念": "结构化隐函数是一种用于表示3D形状的技术，它通过组合局部形状元素来适应广泛变化的几何和拓扑。这种方法允许从数据中学习通用形状模板，支持多种图形和视觉应用，如形状探索、对应、抽象、插值和从RGB图像进行语义分割。"
    },
    {
        "order": 891,
        "title": "Deep Meta Metric Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Meta_Metric_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Deep_Meta_Metric_Learning_ICCV_2019_paper.html",
        "abstract": "In this paper, we present a deep meta metric learning (DMML) approach for visual recognition. Unlike most existing deep metric learning methods formulating the learning process by an overall objective, our DMML formulates the metric learning in a meta way, and proves that softmax and triplet loss are consistent in the meta space. Specifically, we sample some subsets from the original training set and learn metrics across different subsets. In each sampled sub-task, we split the training data into a support set as well as a query set, and learn the set-based distance, instead of sample-based one, to verify the query cell from multiple support cells. In addition, we introduce hard sample mining for set-based distance to encourage the intra-class compactness. Experimental results on three visual recognition applications including person re-identification, vehicle re-identification and face verification show that the proposed DMML method outperforms most existing approaches.",
        "中文标题": "深度元度量学习",
        "摘要翻译": "在本文中，我们提出了一种用于视觉识别的深度元度量学习（DMML）方法。与大多数现有的深度度量学习方法通过一个总体目标来制定学习过程不同，我们的DMML以元方式制定度量学习，并证明在元空间中softmax和三元组损失是一致的。具体来说，我们从原始训练集中采样一些子集，并在不同子集之间学习度量。在每个采样的子任务中，我们将训练数据分为支持集和查询集，并学习基于集合的距离，而不是基于样本的距离，以从多个支持单元验证查询单元。此外，我们引入了基于集合距离的难样本挖掘，以鼓励类内紧凑性。在包括行人重识别、车辆重识别和面部验证在内的三个视觉识别应用上的实验结果表明，所提出的DMML方法优于大多数现有方法。",
        "领域": "视觉识别/度量学习/元学习",
        "问题": "如何更有效地进行视觉识别任务中的度量学习",
        "动机": "现有的深度度量学习方法通常通过一个总体目标来制定学习过程，这限制了其在复杂视觉识别任务中的表现。",
        "方法": "提出了一种深度元度量学习方法，通过从原始训练集中采样子集并在不同子集之间学习度量，以及引入基于集合距离的难样本挖掘来提高类内紧凑性。",
        "关键词": [
            "视觉识别",
            "度量学习",
            "元学习",
            "难样本挖掘"
        ],
        "涉及的技术概念": {
            "深度元度量学习（DMML）": "一种以元方式制定度量学习的方法，通过在元空间中证明softmax和三元组损失的一致性来提高视觉识别的效果。",
            "基于集合的距离": "与传统的基于样本的距离不同，这种方法学习的是基于集合的距离，以从多个支持单元验证查询单元。",
            "难样本挖掘": "一种技术，用于在训练过程中识别和处理难以分类的样本，以提高模型的性能。"
        }
    },
    {
        "order": 892,
        "title": "Bayesian Relational Memory for Semantic Visual Navigation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Bayesian_Relational_Memory_for_Semantic_Visual_Navigation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Bayesian_Relational_Memory_for_Semantic_Visual_Navigation_ICCV_2019_paper.html",
        "abstract": "We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.",
        "中文标题": "贝叶斯关系记忆用于语义视觉导航",
        "摘要翻译": "我们引入了一种新的记忆架构——贝叶斯关系记忆（BRM），以提高语义视觉导航代理在未见环境中的泛化能力，其中代理被赋予一个语义目标以导航。BRM采用语义实体（例如，房间类型）上的概率关系图形式，允许（1）从训练环境中捕捉布局先验，即先验知识，（2）在测试时估计后验布局，即记忆更新，以及（3）高效规划导航，三者合一。我们开发了一个BRM代理，包括一个用于生成子目标的BRM模块和一个用于控制的目标条件运动模块。在未见环境中测试时，BRM代理的表现优于那些未明确利用概率关系记忆结构的基线。",
        "领域": "语义视觉导航/概率图模型/智能体控制",
        "问题": "提高语义视觉导航代理在未见环境中的泛化能力",
        "动机": "为了在未见环境中更有效地导航，需要一种能够捕捉和利用环境布局先验知识的方法",
        "方法": "开发了一种新的记忆架构——贝叶斯关系记忆（BRM），通过概率关系图捕捉语义实体间的关系，并结合目标条件运动模块进行控制",
        "关键词": [
            "语义视觉导航",
            "贝叶斯关系记忆",
            "概率关系图",
            "智能体控制"
        ],
        "涉及的技术概念": "贝叶斯关系记忆（BRM）是一种新的记忆架构，用于捕捉语义实体间的概率关系，以提高导航代理在未见环境中的泛化能力。它包括从训练环境中捕捉布局先验、在测试时估计后验布局以及高效规划导航。BRM代理由BRM模块和目标条件运动模块组成，前者用于生成子目标，后者用于控制。"
    },
    {
        "order": 893,
        "title": "CompenNet++: End-to-End Full Projector Compensation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CompenNet_End-to-End_Full_Projector_Compensation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_CompenNet_End-to-End_Full_Projector_Compensation_ICCV_2019_paper.html",
        "abstract": "Full projector compensation aims to modify a projector input image such that it can compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately, although they are known to correlate with each other. In this paper, we propose the first end-to-end solution, named CompenNet++, to solve the two problems jointly. Our work non-trivially extends CompenNet, which was recently proposed for photometric compensation with promising performance. First, we propose a novel geometric correction subnet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from photometric sampling images. Second, by concatenating the geometric correction subset with CompenNet, CompenNet++ accomplishes full projector compensation and is end-to-end trainable. Third, after training, we significantly simplify both geometric and photometric compensation parts, and hence largely improves the running time efficiency. Moreover, we construct the first setup-independent full compensation benchmark to facilitate the study on this topic. In our thorough experiments, our method shows clear advantages over previous arts with promising compensation quality and meanwhile being practically convenient.",
        "中文标题": "CompenNet++: 端到端的全投影仪补偿",
        "摘要翻译": "全投影仪补偿旨在修改投影仪的输入图像，使其能够补偿投影表面的几何和光度干扰。传统方法通常分别解决这两个部分，尽管已知它们是相互关联的。在本文中，我们提出了第一个端到端解决方案，名为CompenNet++，以联合解决这两个问题。我们的工作非平凡地扩展了CompenNet，这是最近提出的用于光度补偿的方法，具有有前途的性能。首先，我们提出了一个新颖的几何校正子网，它设计有级联的粗到细结构，以直接从光度采样图像中学习采样网格。其次，通过将几何校正子集与CompenNet连接，CompenNet++实现了全投影仪补偿，并且是端到端可训练的。第三，训练后，我们显著简化了几何和光度补偿部分，从而大大提高了运行时间效率。此外，我们构建了第一个独立于设置的全补偿基准，以促进这一主题的研究。在我们的全面实验中，我们的方法显示出明显的优势，具有有前途的补偿质量，同时在实际应用中非常方便。",
        "领域": "投影技术/图像校正/光度补偿",
        "问题": "解决投影表面的几何和光度干扰问题",
        "动机": "传统方法分别处理几何和光度补偿，而这两者是相互关联的，需要一个联合解决方案",
        "方法": "提出了CompenNet++，一个端到端的解决方案，包括一个新颖的几何校正子网和与CompenNet的连接，实现了全投影仪补偿",
        "关键词": [
            "投影技术",
            "图像校正",
            "光度补偿"
        ],
        "涉及的技术概念": "CompenNet++是一个端到端的解决方案，用于全投影仪补偿，包括几何校正和光度补偿。几何校正子网采用级联的粗到细结构，直接从光度采样图像中学习采样网格。通过将几何校正子集与CompenNet连接，实现了全投影仪补偿，并且是端到端可训练的。训练后，几何和光度补偿部分被显著简化，提高了运行时间效率。"
    },
    {
        "order": 894,
        "title": "Discriminative Feature Transformation for Occluded Pedestrian Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Discriminative_Feature_Transformation_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Discriminative_Feature_Transformation_for_Occluded_Pedestrian_Detection_ICCV_2019_paper.html",
        "abstract": "Despite promising performance achieved by deep con- volutional neural networks for non-occluded pedestrian de- tection, it remains a great challenge to detect partially oc- cluded pedestrians. Compared with non-occluded pedes- trian examples, it is generally more difficult to distinguish occluded pedestrian examples from background in featue space due to the missing of occluded parts. In this paper, we propose a discriminative feature transformation which en- forces feature separability of pedestrian and non-pedestrian examples to handle occlusions for pedestrian detection. Specifically, in feature space it makes pedestrian exam- ples approach the centroid of easily classified non-occluded pedestrian examples and pushes non-pedestrian examples close to the centroid of easily classified non-pedestrian ex- amples. Such a feature transformation partially compen- sates the missing contribution of occluded parts in feature space, therefore improving the performance for occluded pedestrian detection. We implement our approach in the Fast R-CNN framework by adding one transformation net- work branch. We validate the proposed approach on two widely used pedestrian detection datasets: Caltech and CityPersons. Experimental results show that our approach achieves promising performance for both non-occluded and occluded pedestrian detection.",
        "中文标题": "用于遮挡行人检测的判别特征变换",
        "摘要翻译": "尽管深度卷积神经网络在非遮挡行人检测方面取得了令人鼓舞的性能，但检测部分遮挡的行人仍然是一个巨大的挑战。与非遮挡行人样本相比，由于遮挡部分的缺失，通常更难在特征空间中区分遮挡行人样本与背景。在本文中，我们提出了一种判别特征变换，该变换增强了行人和非行人样本在特征空间中的可分离性，以处理行人检测中的遮挡问题。具体来说，在特征空间中，它使行人样本接近易于分类的非遮挡行人样本的质心，并将非行人样本推向易于分类的非行人样本的质心。这种特征变换部分补偿了特征空间中遮挡部分缺失的贡献，从而提高了遮挡行人检测的性能。我们通过在Fast R-CNN框架中添加一个变换网络分支来实现我们的方法。我们在两个广泛使用的行人检测数据集：Caltech和CityPersons上验证了所提出的方法。实验结果表明，我们的方法在非遮挡和遮挡行人检测方面都取得了令人鼓舞的性能。",
        "领域": "行人检测/特征变换/遮挡处理",
        "问题": "检测部分遮挡的行人",
        "动机": "由于遮挡部分的缺失，通常更难在特征空间中区分遮挡行人样本与背景",
        "方法": "提出了一种判别特征变换，增强了行人和非行人样本在特征空间中的可分离性",
        "关键词": [
            "行人检测",
            "特征变换",
            "遮挡处理"
        ],
        "涉及的技术概念": "深度卷积神经网络、特征空间、判别特征变换、Fast R-CNN框架、质心"
    },
    {
        "order": 895,
        "title": "Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brickwedde_Mono-SF_Multi-View_Geometry_Meets_Single-View_Depth_for_Monocular_Scene_Flow_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brickwedde_Mono-SF_Multi-View_Geometry_Meets_Single-View_Depth_for_Monocular_Scene_Flow_ICCV_2019_paper.html",
        "abstract": "Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design.",
        "中文标题": "Mono-SF：多视图几何与单视图深度相结合用于动态交通场景的单目场景流估计",
        "摘要翻译": "现有的3D场景流估计方法提供了场景的3D几何和3D运动，并在自动驾驶等领域引起了广泛关注。这些方法传统上基于一系列时间上的立体图像。在本文中，我们提出了一种新颖的单目3D场景流估计方法，称为Mono-SF。Mono-SF通过结合多视图几何和单视图深度信息，共同估计场景的3D结构和运动。Mono-SF认为，基于多视图几何原理，场景流在将参考图像扭曲到连续图像中时应保持一致。为了以统计方式整合单视图深度，提出了一个称为ProbDepthNet的卷积神经网络。ProbDepthNet从单张图像估计像素级深度分布，而不是单一的深度值。此外，作为ProbDepthNet的一部分，提出了一种新颖的回归问题重新校准技术，以确保分布的良好校准。我们的实验表明，Mono-SF优于最先进的单目基线，消融研究支持Mono-SF方法和ProbDepthNet设计。",
        "领域": "场景流估计/自动驾驶/深度估计",
        "问题": "单目3D场景流估计",
        "动机": "提高动态交通场景中3D场景流估计的准确性和效率",
        "方法": "结合多视图几何和单视图深度信息，使用卷积神经网络ProbDepthNet估计像素级深度分布，并提出回归问题重新校准技术",
        "关键词": [
            "场景流估计",
            "自动驾驶",
            "深度估计"
        ],
        "涉及的技术概念": "多视图几何、单视图深度信息、卷积神经网络、像素级深度分布、回归问题重新校准技术"
    },
    {
        "order": 896,
        "title": "Deep Parametric Indoor Lighting Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gardner_Deep_Parametric_Indoor_Lighting_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gardner_Deep_Parametric_Indoor_Lighting_Estimation_ICCV_2019_paper.html",
        "abstract": "We present a method to estimate lighting from a single image of an indoor scene. Previous work has used an environment map representation that does not account for the localized nature of indoor lighting. Instead, we represent lighting as a set of discrete 3D lights with geometric and photometric parameters. We train a deep neural network to regress these parameters from a single image, on a dataset of environment maps annotated with depth. We propose a differentiable layer to convert these parameters to an environment map to compute our loss; this bypasses the challenge of establishing correspondences between estimated and ground truth lights. We demonstrate, via quantitative and qualitative evaluations, that our representation and training scheme lead to more accurate results compared to previous work, while allowing for more realistic 3D object compositing with spatially-varying lighting.",
        "中文标题": "深度参数化室内光照估计",
        "摘要翻译": "我们提出了一种从室内场景的单张图像中估计光照的方法。之前的工作使用了环境贴图表示法，这种方法没有考虑到室内光照的局部特性。相反，我们将光照表示为一组具有几何和光度参数的离散3D光源。我们在一个带有深度注释的环境贴图数据集上训练了一个深度神经网络，以从单张图像中回归这些参数。我们提出了一个可微分层，将这些参数转换为环境贴图以计算我们的损失；这绕过了在估计光源和真实光源之间建立对应关系的挑战。我们通过定量和定性评估证明，与之前的工作相比，我们的表示和训练方案导致了更准确的结果，同时允许在空间变化的光照下进行更真实的3D对象合成。",
        "领域": "光照估计/3D重建/图像合成",
        "问题": "从单张室内场景图像中准确估计光照",
        "动机": "解决现有环境贴图表示法无法准确反映室内光照局部特性的问题",
        "方法": "使用深度神经网络回归离散3D光源的几何和光度参数，并通过可微分层将这些参数转换为环境贴图来计算损失",
        "关键词": [
            "光照估计",
            "3D光源",
            "深度神经网络",
            "环境贴图",
            "可微分层"
        ],
        "涉及的技术概念": "环境贴图表示法是一种用于模拟光照的技术，它通过捕捉场景周围的光照信息来生成图像。深度神经网络是一种模仿人脑结构和功能的计算模型，用于处理和分析大量数据。可微分层是深度学习中的一种技术，允许模型在训练过程中自动计算梯度，从而优化模型参数。"
    },
    {
        "order": 897,
        "title": "Contextual Attention for Hand Detection in the Wild",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Narasimhaswamy_Contextual_Attention_for_Hand_Detection_in_the_Wild_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Narasimhaswamy_Contextual_Attention_for_Hand_Detection_in_the_Wild_ICCV_2019_paper.html",
        "abstract": "We present Hand-CNN, a novel convolutional network architecture for detecting hand masks and predicting hand orientations in unconstrained images. Hand-CNN extends MaskRCNN with a novel attention mechanism to incorporate contextual cues in the detection process. This attention mechanism can be implemented as an efficient network module that captures non-local dependencies between features. This network module can be inserted at different stages of an object detection network, and the entire detector can be trained end-to-end. We also introduce large-scale annotated hand datasets containing hands in unconstrained images for training and evaluation. We show that Hand-CNN outperforms existing methods on the newly collected datasets and the publicly available PASCAL VOC human layout dataset. Data and code: https://www3.cs.stonybrook.edu/ cvl/projects/hand_det_attention/",
        "中文标题": "野外手部检测的上下文注意力机制",
        "摘要翻译": "我们提出了Hand-CNN，一种新颖的卷积网络架构，用于在无约束图像中检测手部掩码并预测手部方向。Hand-CNN通过一种新颖的注意力机制扩展了MaskRCNN，以在检测过程中融入上下文线索。这种注意力机制可以作为一个高效的网络模块实现，捕捉特征之间的非局部依赖关系。这个网络模块可以插入到对象检测网络的不同阶段，整个检测器可以端到端训练。我们还引入了包含无约束图像中手部的大规模注释手部数据集，用于训练和评估。我们展示了Hand-CNN在新收集的数据集和公开可用的PASCAL VOC人体布局数据集上优于现有方法。数据和代码：https://www3.cs.stonybrook.edu/cvl/projects/hand_det_attention/",
        "领域": "手部检测/注意力机制/卷积神经网络",
        "问题": "在无约束图像中准确检测手部掩码和预测手部方向",
        "动机": "提高在复杂背景下手部检测的准确性和鲁棒性",
        "方法": "扩展MaskRCNN，引入一种新颖的注意力机制来捕捉特征间的非局部依赖关系，实现端到端训练",
        "关键词": [
            "手部检测",
            "注意力机制",
            "卷积神经网络",
            "MaskRCNN",
            "非局部依赖"
        ],
        "涉及的技术概念": "Hand-CNN是一种基于卷积神经网络的架构，专门设计用于在无约束环境中检测手部。它通过引入一种新的注意力机制来增强MaskRCNN，这种机制能够有效地捕捉图像特征之间的非局部依赖关系，从而提高检测的准确性。此外，该研究还创建了一个大规模的手部数据集，用于训练和评估模型，展示了Hand-CNN在多个数据集上的优越性能。"
    },
    {
        "order": 898,
        "title": "Prior Guided Dropout for Robust Visual Localization in Dynamic Environments",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Prior_Guided_Dropout_for_Robust_Visual_Localization_in_Dynamic_Environments_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Prior_Guided_Dropout_for_Robust_Visual_Localization_in_Dynamic_Environments_ICCV_2019_paper.html",
        "abstract": "Camera localization from monocular images has been a long-standing problem, but its robustness in dynamic environments is still not adequately addressed. Compared with classic geometric approaches, modern CNN-based methods (e.g. PoseNet) have manifested the reliability against illumination or viewpoint variations, but they still have the following limitations. First, foreground moving objects are not explicitly handled, which results in poor performance and instability in dynamic environments. Second, the output for each image is a point estimate without uncertainty quantification. In this paper, we propose a framework which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. The key idea is a prior guided dropout module coupled with a self-attention module which can guide CNNs to ignore foreground objects during both training and inference. Additionally, the dropout module enables the pose regressor to output multiple hypotheses from which the uncertainty of pose estimates can be quantified and leveraged in the following uncertainty-aware pose-graph optimization to improve the robustness further. We achieve an average accuracy of 9.98m/3.63deg on RobotCar dataset, which outperforms the state-of-the-art method by 62.97%/47.08%. The source code of our implementation is available at https://github.com/zju3dv/RVL-dynamic.",
        "中文标题": "先验引导的Dropout用于动态环境中的鲁棒视觉定位",
        "摘要翻译": "从单目图像进行相机定位是一个长期存在的问题，但其在动态环境中的鲁棒性仍未得到充分解决。与经典的几何方法相比，基于现代CNN的方法（例如PoseNet）已经显示出对光照或视角变化的可靠性，但它们仍然存在以下局限性。首先，前景移动物体没有被明确处理，这导致在动态环境中性能差和不稳定。其次，每张图像的输出是一个点估计，没有不确定性量化。在本文中，我们提出了一个框架，该框架可以普遍应用于现有的基于CNN的姿态回归器，以提高其在动态环境中的鲁棒性。关键思想是一个先验引导的dropout模块与一个自注意力模块相结合，可以在训练和推理过程中引导CNN忽略前景物体。此外，dropout模块使姿态回归器能够输出多个假设，从中可以量化姿态估计的不确定性，并在后续的不确定性感知的姿态图优化中利用，以进一步提高鲁棒性。我们在RobotCar数据集上实现了9.98米/3.63度的平均准确度，这比最先进的方法提高了62.97%/47.08%。我们实现的源代码可在https://github.com/zju3dv/RVL-dynamic获取。",
        "领域": "视觉定位/动态环境处理/不确定性量化",
        "问题": "提高在动态环境中基于CNN的相机定位方法的鲁棒性",
        "动机": "解决现有基于CNN的相机定位方法在动态环境中对前景移动物体处理不足和缺乏不确定性量化的问题",
        "方法": "提出一个先验引导的dropout模块与自注意力模块相结合的框架，用于在训练和推理过程中引导CNN忽略前景物体，并通过输出多个假设来量化姿态估计的不确定性，进而利用不确定性感知的姿态图优化提高鲁棒性",
        "关键词": [
            "视觉定位",
            "动态环境",
            "不确定性量化",
            "自注意力机制",
            "dropout"
        ],
        "涉及的技术概念": "先验引导的dropout模块用于在训练和推理过程中忽略前景物体，自注意力模块用于增强模型对重要特征的关注，不确定性量化通过输出多个假设实现，不确定性感知的姿态图优化用于进一步提高定位的鲁棒性。"
    },
    {
        "order": 899,
        "title": "FSGAN: Subject Agnostic Face Swapping and Reenactment",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_and_Reenactment_ICCV_2019_paper.html",
        "abstract": "We present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)-based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.",
        "中文标题": "FSGAN：主题无关的面部交换和重演",
        "摘要翻译": "我们提出了用于面部交换和重演的面部交换生成对抗网络（FSGAN）。与之前的工作不同，FSGAN是主题无关的，可以应用于面部对，而无需对这些面部进行训练。为此，我们描述了一些技术贡献。我们推导出了一种基于循环神经网络（RNN）的新方法，用于面部重演，该方法可以调整姿势和表情变化，并且可以应用于单个图像或视频序列。对于视频序列，我们引入了基于重演、Delaunay三角剖分和重心坐标的面部视图的连续插值。被遮挡的面部区域由面部完成网络处理。最后，我们使用面部混合网络进行无缝混合，同时保留目标肤色和光照条件。该网络使用了一种新颖的泊松混合损失，将泊松优化与感知损失结合起来。我们将我们的方法与现有的最先进系统进行了比较，并展示了我们的结果在质量和数量上都更优。",
        "领域": "面部交换/面部重演/生成对抗网络",
        "问题": "如何实现主题无关的面部交换和重演",
        "动机": "为了开发一种无需特定面部训练即可应用于任何面部对的面部交换和重演技术",
        "方法": "采用基于循环神经网络的面部重演方法，结合Delaunay三角剖分和重心坐标进行面部视图的连续插值，使用面部完成网络处理遮挡区域，以及使用面部混合网络进行无缝混合",
        "关键词": [
            "面部交换",
            "面部重演",
            "生成对抗网络",
            "循环神经网络",
            "Delaunay三角剖分",
            "重心坐标",
            "面部完成网络",
            "面部混合网络",
            "泊松混合损失"
        ],
        "涉及的技术概念": {
            "循环神经网络（RNN）": "一种用于处理序列数据的神经网络，能够捕捉时间序列中的依赖关系。",
            "Delaunay三角剖分": "一种将点集分割成三角形的方法，确保没有点在三角形的外接圆内。",
            "重心坐标": "在三角形中，用于表示点相对于三角形顶点的位置的一种坐标系统。",
            "泊松混合损失": "结合泊松优化和感知损失的混合损失函数，用于图像混合以保持细节和结构。"
        }
    },
    {
        "order": 900,
        "title": "Meta R-CNN: Towards General Solver for Instance-Level Low-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Meta_R-CNN_Towards_General_Solver_for_Instance-Level_Low-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Meta_R-CNN_Towards_General_Solver_for_Instance-Level_Low-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Resembling the rapid learning capability of human, low-shot learning empowers vision systems to understand new concepts by training with few samples. Leading approaches derived from meta-learning on images with a single visual object. Obfuscated by a complex background and multiple objects in one image, they are hard to promote the research of low-shot object detection/segmentation. In this work, we present a flexible and general methodology to achieve these tasks. Our work extends Faster /Mask R-CNN by proposing meta-learning over RoI (Region-of-Interest) features instead of a full image feature. This simple spirit disentangles multi-object information merged with the background, without bells and whistles, enabling Faster /Mask R-CNN turn into a meta-learner to achieve the tasks. Specifically, we introduce a Predictor-head Remodeling Network (PRN) that shares its main backbone with Faster /Mask R-CNN. PRN receives images containing low-shot objects with their bounding boxes or masks to infer their class attentive vectors. The vectors take channel-wise soft-attention on RoI features, remodeling those R-CNN predictor heads to detect or segment the objects consistent with the classes these vectors represent. In our experiments, Meta R-CNN yields the new state of the art in low-shot object detection and improves low-shot object segmentation by Mask R-CNN. Code: https://yanxp.github.io/metarcnn.html.",
        "中文标题": "Meta R-CNN：面向实例级少样本学习的通用解决方案",
        "摘要翻译": "类似于人类的快速学习能力，少样本学习通过少量样本训练使视觉系统能够理解新概念。领先的方法源自于对单一视觉对象图像的元学习。由于复杂背景和图像中多个对象的混淆，这些方法难以推动少样本对象检测/分割的研究。在这项工作中，我们提出了一种灵活且通用的方法来实现这些任务。我们的工作通过提出在RoI（感兴趣区域）特征上进行元学习而不是全图像特征，扩展了Faster /Mask R-CNN。这种简单的精神解开了与背景合并的多对象信息，无需花哨的技巧，使Faster /Mask R-CNN转变为元学习者以实现任务。具体来说，我们引入了一个预测头重塑网络（PRN），它与Faster /Mask R-CNN共享其主要骨干。PRN接收包含少样本对象的图像及其边界框或掩码，以推断它们的类别注意力向量。这些向量在RoI特征上采取通道级软注意力，重塑那些R-CNN预测头以检测或分割与这些向量代表的类别一致的对象。在我们的实验中，Meta R-CNN在少样本对象检测中取得了新的最先进成果，并通过Mask R-CNN改进了少样本对象分割。代码：https://yanxp.github.io/metarcnn.html。",
        "领域": "少样本学习/对象检测/对象分割",
        "问题": "解决在复杂背景和多个对象存在的情况下，少样本对象检测和分割的挑战",
        "动机": "推动少样本对象检测和分割的研究，通过元学习提高视觉系统对新概念的理解能力",
        "方法": "提出在RoI特征上进行元学习，引入预测头重塑网络（PRN）来重塑R-CNN预测头，以检测或分割与类别注意力向量代表的对象",
        "关键词": [
            "少样本学习",
            "对象检测",
            "对象分割",
            "元学习",
            "RoI特征",
            "预测头重塑网络"
        ],
        "涉及的技术概念": "元学习（Meta-learning）是一种让模型通过少量样本快速学习新任务的技术。RoI（Region-of-Interest）特征指的是从图像中提取的感兴趣区域的特征。预测头重塑网络（PRN）是一种网络结构，用于根据输入的少样本对象图像及其边界框或掩码，推断出类别注意力向量，进而重塑R-CNN的预测头，以实现对象的检测或分割。"
    },
    {
        "order": 901,
        "title": "Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Martin_DriveAct_A_Multi-Modal_Dataset_for_Fine-Grained_Driver_Behavior_Recognition_in_ICCV_2019_paper.html",
        "abstract": "We introduce the novel domain-specific Drive&Act benchmark for fine-grained categorization of driver behavior. Our dataset features twelve hours and over 9.6 million frames of people engaged in distractive activities during both, manual and automated driving. We capture color, infrared, depth and 3D body pose information from six views and densely label the videos with a hierarchical annotation scheme, resulting in 83 categories. The key challenges of our dataset are: (1) recognition of fine-grained behavior inside the vehicle cabin; (2) multi-modal activity recognition, focusing on diverse data streams; and (3) a cross view recognition benchmark, where a model handles data from an unfamiliar domain, as sensor type and placement in the cabin can change between vehicles. Finally, we provide challenging benchmarks by adopting prominent methods for video- and body pose-based action recognition.",
        "中文标题": "Drive&Act: 自动驾驶汽车中细粒度驾驶员行为识别的多模态数据集",
        "摘要翻译": "我们介绍了新颖的领域特定Drive&Act基准，用于细粒度分类驾驶员行为。我们的数据集包含十二小时的视频和超过960万帧的画面，记录了手动和自动驾驶期间人们从事分散注意力的活动。我们从六个视角捕捉了颜色、红外、深度和3D身体姿态信息，并使用分层注释方案对视频进行了密集标注，共产生了83个类别。我们数据集的关键挑战包括：(1) 车内细粒度行为的识别；(2) 多模态活动识别，专注于多样化的数据流；(3) 跨视角识别基准，模型需要处理来自不熟悉领域的数据，因为传感器类型和车内位置可能因车辆而异。最后，我们通过采用视频和基于身体姿态的动作识别的突出方法，提供了具有挑战性的基准。",
        "领域": "自动驾驶/行为识别/多模态学习",
        "问题": "细粒度驾驶员行为识别",
        "动机": "为了在自动驾驶汽车中准确识别和理解驾驶员的行为，特别是在手动和自动驾驶模式下，驾驶员可能从事的分散注意力的活动。",
        "方法": "采用多模态数据（颜色、红外、深度和3D身体姿态信息）和分层注释方案，创建了一个包含83个类别的细粒度驾驶员行为数据集，并提出了跨视角识别和多模态活动识别的挑战。",
        "关键词": [
            "自动驾驶",
            "行为识别",
            "多模态学习",
            "细粒度分类",
            "跨视角识别"
        ],
        "涉及的技术概念": "多模态数据包括颜色、红外、深度和3D身体姿态信息；分层注释方案用于视频的密集标注；跨视角识别和多模态活动识别是数据集的关键挑战。"
    },
    {
        "order": 902,
        "title": "Pyramid Graph Networks With Connection Attentions for Region-Based One-Shot Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.html",
        "abstract": "One-shot image segmentation aims to undertake the segmentation task of a novel class with only one training image available. The difficulty lies in that image segmentation has structured data representations, which yields a many-to-many message passing problem. Previous methods often simplify it to a one-to-many problem by squeezing support data to a global descriptor. However, a mixed global representation drops the data structure and information of individual elements. In this paper, we propose to model structured segmentation data with graphs and apply attentive graph reasoning to propagate label information from support data to query data. The graph attention mechanism could establish the element-to-element correspondence across structured data by learning attention weights between connected graph nodes. To capture correspondence at different semantic levels, we further propose a pyramid-like structure that models different sizes of image regions as graph nodes and undertakes graph reasoning at different levels. Experiments on PASCAL VOC 2012 dataset demonstrate that our proposed network significantly outperforms the baseline method and leads to new state-of-the-art performance on 1-shot and 5-shot segmentation benchmarks.",
        "中文标题": "基于连接注意力的金字塔图网络用于区域一次性语义分割",
        "摘要翻译": "一次性图像分割旨在仅使用一张训练图像来完成新类别的分割任务。难点在于图像分割具有结构化的数据表示，这导致了一个多对多的消息传递问题。以前的方法通常通过将支持数据压缩为全局描述符来将其简化为一个一对多的问题。然而，混合的全局表示丢弃了数据结构和个人元素的信息。在本文中，我们提出用图来建模结构化分割数据，并应用注意力图推理将标签信息从支持数据传播到查询数据。图注意力机制可以通过学习连接图节点之间的注意力权重来建立跨结构化数据的元素到元素的对应关系。为了捕捉不同语义层次的对应关系，我们进一步提出了一种金字塔状结构，该结构将不同大小的图像区域建模为图节点，并在不同层次上进行图推理。在PASCAL VOC 2012数据集上的实验表明，我们提出的网络显著优于基线方法，并在1-shot和5-shot分割基准上取得了新的最先进性能。",
        "领域": "语义分割/图神经网络/注意力机制",
        "问题": "解决一次性图像分割任务中的多对多消息传递问题",
        "动机": "为了克服现有方法在一次性图像分割任务中因简化数据结构而丢失信息的问题",
        "方法": "提出使用图来建模结构化分割数据，并应用注意力图推理来传播标签信息，同时引入金字塔状结构以捕捉不同语义层次的对应关系",
        "关键词": [
            "语义分割",
            "图神经网络",
            "注意力机制",
            "一次性学习",
            "图像区域"
        ],
        "涉及的技术概念": "图注意力机制用于建立元素到元素的对应关系，金字塔状结构用于在不同层次上进行图推理，以捕捉不同语义层次的对应关系。"
    },
    {
        "order": 903,
        "title": "Deep Single-Image Portrait Relighting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Deep_Single-Image_Portrait_Relighting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Deep_Single-Image_Portrait_Relighting_ICCV_2019_paper.html",
        "abstract": "Conventional physically-based methods for relighting portrait images need to solve an inverse rendering problem, estimating face geometry, reflectance and lighting. However, the inaccurate estimation of face components can cause strong artifacts in relighting, leading to unsatisfactory results. In this work, we apply a physically-based portrait relighting method to generate a large scale, high quality, \"in the wild\" portrait relighting dataset (DPR). A deep Convolutional Neural Network (CNN) is then trained using this dataset to generate a relit portrait image by using a source image and a target lighting as input. The training procedure regularizes the generated results, removing the artifacts caused by physically-based relighting methods. A GAN loss is further applied to improve the quality of the relit portrait image. Our trained network can relight portrait images with resolutions as high as 1024 x 1024. We evaluate the proposed method on the proposed DPR datset, Flickr portrait dataset and Multi-PIE dataset both qualitatively and quantitatively. Our experiments demonstrate that the proposed method achieves state-of-the-art results. Please refer to https://zhhoper.github.io/dpr.html for dataset and code.",
        "中文标题": "深度单图像肖像重照明",
        "摘要翻译": "传统的基于物理的肖像图像重照明方法需要解决一个逆渲染问题，估计面部几何、反射率和光照。然而，面部组件的不准确估计可能导致重照明中的强烈伪影，导致不满意的结果。在这项工作中，我们应用了一种基于物理的肖像重照明方法来生成一个大规模、高质量、“野外”肖像重照明数据集（DPR）。然后使用这个数据集训练一个深度卷积神经网络（CNN），通过使用源图像和目标光照作为输入来生成重照明的肖像图像。训练过程规范了生成的结果，消除了基于物理的重照明方法引起的伪影。进一步应用GAN损失来提高重照明肖像图像的质量。我们训练的网络可以重照明分辨率高达1024 x 1024的肖像图像。我们在提出的DPR数据集、Flickr肖像数据集和Multi-PIE数据集上定性和定量地评估了所提出的方法。我们的实验表明，所提出的方法达到了最先进的结果。请访问https://zhhoper.github.io/dpr.html获取数据集和代码。",
        "领域": "图像重照明/深度学习/计算机图形学",
        "问题": "解决传统基于物理的肖像重照明方法中因面部组件估计不准确导致的伪影问题",
        "动机": "提高肖像图像重照明的质量和准确性，消除伪影，生成更自然的重照明效果",
        "方法": "应用基于物理的肖像重照明方法生成大规模高质量数据集，训练深度卷积神经网络进行图像重照明，并使用GAN损失提高图像质量",
        "关键词": [
            "图像重照明",
            "卷积神经网络",
            "GAN损失"
        ],
        "涉及的技术概念": "逆渲染问题涉及估计面部几何、反射率和光照，以解决肖像图像重照明中的伪影问题。深度卷积神经网络（CNN）用于通过学习大规模数据集来生成重照明的肖像图像。GAN（生成对抗网络）损失用于进一步提高重照明图像的质量，使其更加自然和真实。"
    },
    {
        "order": 904,
        "title": "Depth Completion From Sparse LiDAR Data With Depth-Normal Constraints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Depth_Completion_From_Sparse_LiDAR_Data_With_Depth-Normal_Constraints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_Depth_Completion_From_Sparse_LiDAR_Data_With_Depth-Normal_Constraints_ICCV_2019_paper.html",
        "abstract": "Depth completion aims to recover dense depth maps from sparse depth measurements. It is of increasing importance for autonomous driving and draws increasing attention from the vision community. Most of the current competitive methods directly train a network to learn a mapping from sparse depth inputs to dense depth maps, which has difficulties in utilizing the 3D geometric constraints and handling the practical sensor noises. In this paper, to regularize the depth completion and improve the robustness against noise, we propose a unified CNN framework that 1) models the geometric constraints between depth and surface normal in a diffusion module and 2) predicts the confidence of sparse LiDAR measurements to mitigate the impact of noise. Specifically, our encoder-decoder backbone predicts the surface normal, coarse depth and confidence of LiDAR inputs simultaneously, which are subsequently inputted into our diffusion refinement module to obtain the final completion results. Extensive experiments on KITTI depth completion dataset and NYU-Depth-V2 dataset demonstrate that our method achieves state-of-the-art performance. Further ablation study and analysis give more insights into the proposed components and demonstrate the generalization capability and stability of our model.",
        "中文标题": "基于深度-法线约束的稀疏LiDAR数据深度补全",
        "摘要翻译": "深度补全旨在从稀疏的深度测量中恢复密集的深度图。这对于自动驾驶越来越重要，并引起了视觉社区的越来越多的关注。当前大多数竞争方法直接训练网络以学习从稀疏深度输入到密集深度图的映射，这种方法在利用3D几何约束和处理实际传感器噪声方面存在困难。在本文中，为了规范化深度补全并提高对噪声的鲁棒性，我们提出了一个统一的CNN框架，该框架1)在扩散模块中建模深度和表面法线之间的几何约束，2)预测稀疏LiDAR测量的置信度以减轻噪声的影响。具体来说，我们的编码器-解码器骨干网络同时预测表面法线、粗略深度和LiDAR输入的置信度，这些随后被输入到我们的扩散细化模块中以获得最终的补全结果。在KITTI深度补全数据集和NYU-Depth-V2数据集上的大量实验表明，我们的方法实现了最先进的性能。进一步的消融研究和分析为我们提出的组件提供了更多见解，并展示了我们模型的泛化能力和稳定性。",
        "领域": "自动驾驶/三维重建/传感器融合",
        "问题": "从稀疏的深度测量中恢复密集的深度图",
        "动机": "提高深度补全的规范化程度和对噪声的鲁棒性",
        "方法": "提出一个统一的CNN框架，包括建模深度和表面法线之间的几何约束的扩散模块和预测稀疏LiDAR测量置信度的方法",
        "关键词": [
            "深度补全",
            "LiDAR",
            "CNN",
            "扩散模块",
            "表面法线",
            "置信度预测"
        ],
        "涉及的技术概念": "深度补全是指从稀疏的深度测量中恢复密集的深度图的过程。LiDAR是一种通过激光测量距离的技术，常用于自动驾驶等领域。CNN（卷积神经网络）是一种深度学习模型，擅长处理图像数据。扩散模块是一种用于建模几何约束的技术。表面法线是指垂直于物体表面的向量，用于描述物体的几何形状。置信度预测是指评估测量数据的可靠性，以减少噪声对结果的影响。"
    },
    {
        "order": 905,
        "title": "PU-GAN: A Point Cloud Upsampling Adversarial Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_PU-GAN_A_Point_Cloud_Upsampling_Adversarial_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_PU-GAN_A_Point_Cloud_Upsampling_Adversarial_Network_ICCV_2019_paper.html",
        "abstract": "Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality.",
        "中文标题": "PU-GAN: 点云上采样对抗网络",
        "摘要翻译": "从范围扫描获得的点云通常是稀疏的、有噪声的且不均匀的。本文提出了一种新的点云上采样网络，称为PU-GAN，它基于生成对抗网络（GAN）构建，旨在从潜在空间学习丰富的点分布，并在物体表面的补丁上进行点上采样。为了实现一个有效的GAN网络，我们在生成器中构建了一个上下上扩展单元，用于上采样点特征，并带有误差反馈和自我校正，同时制定了一个自注意力单元来增强特征整合。此外，我们设计了一个包含对抗性、均匀性和重建项的复合损失，以鼓励判别器学习更多的潜在模式并增强输出点分布的均匀性。定性和定量评估表明，我们的结果在分布均匀性、表面接近度和3D重建质量方面优于现有技术。",
        "领域": "点云处理/3D重建/生成对抗网络",
        "问题": "点云的稀疏性、噪声和不均匀性",
        "动机": "提高点云上采样的质量，使其更加均匀且接近真实表面",
        "方法": "构建基于GAN的点云上采样网络PU-GAN，包括上下上扩展单元和自注意力单元，以及设计复合损失函数",
        "关键词": [
            "点云上采样",
            "生成对抗网络",
            "3D重建"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）用于点云上采样，上下上扩展单元用于特征上采样，自注意力单元用于特征整合，复合损失函数包括对抗性、均匀性和重建项。"
    },
    {
        "order": 906,
        "title": "Presence-Only Geographical Priors for Fine-Grained Image Classification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Aodha_Presence-Only_Geographical_Priors_for_Fine-Grained_Image_Classification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Aodha_Presence-Only_Geographical_Priors_for_Fine-Grained_Image_Classification_ICCV_2019_paper.html",
        "abstract": "Appearance information alone is often not sufficient to accurately differentiate between fine-grained visual categories. Human experts make use of additional cues such as where, and when, a given image was taken in order to inform their final decision. This contextual information is readily available in many online image collections but has been underutilized by existing image classifiers that focus solely on making predictions based on the image contents. We propose an efficient spatio-temporal prior, that when conditioned on a geographical location and time, estimates the probability that a given object category occurs at that location. Our prior is trained from presence-only observation data and jointly models object categories, their spatio-temporal distributions, and photographer biases. Experiments performed on multiple challenging image classification datasets show that combining our prior with the predictions from image classifiers results in a large improvement in final classification performance.",
        "中文标题": "仅存在的地理先验用于细粒度图像分类",
        "摘要翻译": "仅凭外观信息通常不足以准确区分细粒度的视觉类别。人类专家利用额外的线索，如图片的拍摄地点和时间，来辅助他们的最终决策。这种上下文信息在许多在线图片集合中很容易获得，但现有的图像分类器主要基于图像内容进行预测，未能充分利用这些信息。我们提出了一种高效的时空先验，当基于地理位置和时间进行条件化时，能够估计给定对象类别在该位置出现的概率。我们的先验是从仅存在的观测数据中训练出来的，并共同建模对象类别、它们的时空分布以及摄影师的偏见。在多个具有挑战性的图像分类数据集上进行的实验表明，将我们的先验与图像分类器的预测结果结合，可以显著提高最终的分类性能。",
        "领域": "细粒度图像分类/时空数据分析/地理信息系统",
        "问题": "如何利用地理位置和时间等上下文信息提高细粒度图像分类的准确性",
        "动机": "现有的图像分类器主要依赖图像内容进行预测，未能充分利用地理位置和时间等上下文信息，这些信息对于提高细粒度图像分类的准确性至关重要。",
        "方法": "提出了一种高效的时空先验，该先验基于地理位置和时间进行条件化，估计给定对象类别在该位置出现的概率，并与图像分类器的预测结果结合以提高分类性能。",
        "关键词": [
            "细粒度图像分类",
            "时空先验",
            "地理信息系统"
        ],
        "涉及的技术概念": "时空先验是一种基于地理位置和时间条件化的概率估计方法，用于预测特定对象类别在给定位置出现的可能性。这种方法结合了对象类别的时空分布和摄影师的偏见，以提高图像分类的准确性。"
    },
    {
        "order": 907,
        "title": "PRECOG: PREdiction Conditioned on Goals in Visual Multi-Agent Settings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Rhinehart_PRECOG_PREdiction_Conditioned_on_Goals_in_Visual_Multi-Agent_Settings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Rhinehart_PRECOG_PREdiction_Conditioned_on_Goals_in_Visual_Multi-Agent_Settings_ICCV_2019_paper.html",
        "abstract": "For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",
        "中文标题": "PRECOG：视觉多智能体设置中基于目标的预测",
        "摘要翻译": "为了让自动驾驶车辆（AVs）在由人类驾驶的车辆组成的道路上表现得当，它们必须能够从丰富的感知信息中推理出其他驾驶员的不确定意图和决策。为了实现这些能力，我们提出了一个关于未来交互的概率预测模型，该模型适用于可变数量的智能体。我们既执行了标准的预测任务，也执行了条件预测这一新任务，后者推理所有智能体如何可能响应受控智能体（在这里是AV）的目标。我们在真实和模拟数据上训练模型，以根据过去的位置和激光雷达（LIDAR）预测车辆轨迹。我们的评估显示，在多智能体驾驶场景中，我们的模型比现有的最先进技术准确得多。除了执行条件预测查询的一般能力外，我们还展示了当基于AV目标的知识进行条件预测时，我们模型对所有智能体的预测都有所改善，进一步说明了其建模智能体交互的能力。",
        "领域": "自动驾驶/智能体交互/轨迹预测",
        "问题": "自动驾驶车辆在多智能体环境中准确预测其他车辆轨迹的问题",
        "动机": "提高自动驾驶车辆在复杂交通环境中的决策能力和安全性",
        "方法": "提出了一种概率预测模型，该模型能够进行标准预测和条件预测，通过训练在真实和模拟数据上，利用过去的位置和激光雷达数据来预测车辆轨迹",
        "关键词": [
            "自动驾驶",
            "智能体交互",
            "轨迹预测",
            "条件预测",
            "激光雷达"
        ],
        "涉及的技术概念": "概率预测模型、条件预测、智能体交互、激光雷达（LIDAR）"
    },
    {
        "order": 908,
        "title": "Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape Representation Learning and Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bouritsas_Neural_3D_Morphable_Models_Spiral_Convolutional_Networks_for_3D_Shape_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bouritsas_Neural_3D_Morphable_Models_Spiral_Convolutional_Networks_for_3D_Shape_ICCV_2019_paper.html",
        "abstract": "Generative models for 3D geometric data arise in many important applications in 3D computer vision and graphics. In this paper, we focus on 3D deformable shapes that share a common topological structure, such as human faces and bodies. Morphable Models and their variants, despite their linear formulation, have been widely used for shape representation, while most of the recently proposed nonlinear approaches resort to intermediate representations, such as 3D voxel grids or 2D views. In this work, we introduce a novel graph convolutional operator, acting directly on the 3D mesh, that explicitly models the inductive bias of the fixed underlying graph. This is achieved by enforcing consistent local orderings of the vertices of the graph, through the spiral operator, thus breaking the permutation invariance property that is adopted by all the prior work on Graph Neural Networks. Our operator comes by construction with desirable properties (anisotropic, topology-aware, lightweight, easy-to-optimise), and by using it as a building block for traditional deep generative architectures, we demonstrate state-of-the-art results on a variety of 3D shape datasets compared to the linear Morphable Model and other graph convolutional operators.",
        "中文标题": "神经3D可变形模型：用于3D形状表示学习和生成的螺旋卷积网络",
        "摘要翻译": "3D几何数据的生成模型在3D计算机视觉和图形学的许多重要应用中产生。在本文中，我们专注于共享共同拓扑结构的3D可变形形状，如人脸和人体。尽管可变形模型及其变体具有线性公式，但它们已被广泛用于形状表示，而最近提出的大多数非线性方法则依赖于中间表示，如3D体素网格或2D视图。在这项工作中，我们引入了一种新颖的图卷积操作符，直接作用于3D网格，明确地模拟了固定底层图的归纳偏差。这是通过螺旋操作符强制图的顶点的一致局部排序来实现的，从而打破了所有先前关于图神经网络的工作所采用的排列不变性属性。我们的操作符通过构建具有理想的属性（各向异性、拓扑感知、轻量级、易于优化），并通过将其用作传统深度生成架构的构建块，我们在各种3D形状数据集上展示了与线性可变形模型和其他图卷积操作符相比的最先进结果。",
        "领域": "3D形状表示/3D几何数据处理/图卷积网络",
        "问题": "如何有效地表示和生成共享共同拓扑结构的3D可变形形状",
        "动机": "现有的线性可变形模型虽然广泛使用，但非线性方法多依赖于中间表示，存在局限性",
        "方法": "引入一种新颖的图卷积操作符，直接作用于3D网格，通过螺旋操作符强制图的顶点的一致局部排序，打破排列不变性属性",
        "关键词": [
            "3D形状表示",
            "图卷积网络",
            "螺旋操作符"
        ],
        "涉及的技术概念": {
            "3D几何数据": "指三维空间中的几何形状数据，如3D模型、点云等",
            "可变形模型": "一种能够表示和生成可变形状的模型，常用于人脸和人体等形状的建模",
            "图卷积网络": "一种在图结构数据上操作的神经网络，能够处理非欧几里得数据",
            "螺旋操作符": "一种用于图卷积网络的操作符，通过强制图的顶点的一致局部排序来模拟固定底层图的归纳偏差"
        }
    },
    {
        "order": 909,
        "title": "POD: Practical Object Detection With Scale-Sensitive Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_POD_Practical_Object_Detection_With_Scale-Sensitive_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peng_POD_Practical_Object_Detection_With_Scale-Sensitive_Network_ICCV_2019_paper.html",
        "abstract": "Scale-sensitive object detection remains a challenging task, where most of the existing methods not learn it explicitly and not robust to scale variance. In addition, the most existing methods are less efficient during training or slow during inference, which are not friendly to real-time application. In this paper, we propose a practical object detection with scale-sensitive network.Our method first predicts a global continuous scale ,which shared by all position, for each convolution filter of each network stage. To effectively learn the scale, we average the spatial features and distill the scale from channels. For fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into combinations of fixed integral scales for each convolution filter, which exploit the dilated convolution. We demonstrate it on one-stage and two-stage algorithm under almost different configure. For practical application, training of our method is of efficiency and simplicity which gets rid of complex data sampling or optimize strategy. During testing, the proposed method requires no extra operation and is very friendly to hardware acceleration like TensorRT and TVM.On the COCO test-dev, our model could achieve a 41.5mAP on one-stage detector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming baselines by 2.4 and 2.1 respectively without extra FLOPS.",
        "中文标题": "POD：具有尺度敏感网络的实用目标检测",
        "摘要翻译": "尺度敏感的目标检测仍然是一个具有挑战性的任务，大多数现有方法没有明确学习它，并且对尺度变化不鲁棒。此外，大多数现有方法在训练时效率较低或在推理时速度较慢，这对实时应用不友好。在本文中，我们提出了一种具有尺度敏感网络的实用目标检测方法。我们的方法首先为每个网络阶段的每个卷积滤波器预测一个全局连续尺度，该尺度由所有位置共享。为了有效学习尺度，我们对空间特征进行平均并从通道中提取尺度。为了快速部署，我们提出了一种尺度分解方法，将鲁棒的小数尺度转换为每个卷积滤波器的固定整数尺度的组合，这利用了扩张卷积。我们在几乎不同的配置下的一阶段和二阶段算法上进行了演示。对于实际应用，我们的方法训练效率高且简单，摆脱了复杂的数据采样或优化策略。在测试期间，所提出的方法不需要额外的操作，并且对硬件加速如TensorRT和TVM非常友好。在COCO测试开发集上，我们的模型在一阶段检测器上可以达到41.5mAP，在基于ResNet-101的二阶段检测器上可以达到42.1mAP，分别比基线高出2.4和2.1，而无需额外的FLOPS。",
        "领域": "目标检测/尺度敏感网络/硬件加速",
        "问题": "解决尺度敏感目标检测中的尺度变化鲁棒性和效率问题",
        "动机": "提高目标检测方法在尺度变化下的鲁棒性和效率，使其更适合实时应用",
        "方法": "提出了一种尺度敏感网络，通过预测全局连续尺度并利用尺度分解方法将小数尺度转换为固定整数尺度的组合，以提高训练效率和推理速度",
        "关键词": [
            "目标检测",
            "尺度敏感网络",
            "硬件加速"
        ],
        "涉及的技术概念": {
            "尺度敏感网络": "一种能够有效处理不同尺度目标的网络结构",
            "扩张卷积": "一种卷积操作，通过增加卷积核之间的间隔来扩大感受野，而不增加参数数量",
            "TensorRT": "NVIDIA的深度学习推理优化器和运行时库，用于加速深度学习模型的推理",
            "TVM": "一个开源的深度学习编译器堆栈，旨在将深度学习模型高效地部署到各种硬件后端"
        }
    },
    {
        "order": 910,
        "title": "LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_LPD-Net_3D_Point_Cloud_Learning_for_Large-Scale_Place_Recognition_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_LPD-Net_3D_Point_Cloud_Learning_for_Large-Scale_Place_Recognition_and_ICCV_2019_paper.html",
        "abstract": "Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions.",
        "中文标题": "LPD-Net: 用于大规模地点识别和环境分析的3D点云学习",
        "摘要翻译": "基于点云的地点识别仍然是一个开放性问题，原因在于从原始3D点云中提取局部特征和生成全局描述符的难度，在大规模动态环境中更是如此。本文中，我们开发了一种名为LPD-Net（大规模地点描述网络）的新型深度神经网络，它能够从原始3D点云中提取具有区分性和泛化能力的全局描述符。提出了两个模块，即自适应局部特征提取模块和基于图的邻域聚合模块，这两个模块有助于提取局部结构并揭示大规模点云中局部特征的空间分布，采用端到端的方式。我们在解决基于点云的检索任务中实现了所提出的全局描述符，以实现大规模地点识别。比较结果表明，我们的LPD-Net比PointNetVLAD要好得多，并达到了最先进的水平。我们还比较了我们的LPD-Net与基于视觉的解决方案，以展示我们的方法对不同天气和光照条件的鲁棒性。",
        "领域": "地点识别/环境分析/3D点云处理",
        "问题": "从原始3D点云中提取局部特征和生成全局描述符的难度，以及在大规模动态环境中实现地点识别",
        "动机": "解决基于点云的地点识别在大规模动态环境中的挑战，提高地点识别的准确性和鲁棒性",
        "方法": "开发了一种名为LPD-Net的深度神经网络，包括自适应局部特征提取模块和基于图的邻域聚合模块，采用端到端的方式提取局部结构并揭示局部特征的空间分布",
        "关键词": [
            "地点识别",
            "环境分析",
            "3D点云处理",
            "全局描述符",
            "局部特征提取"
        ],
        "涉及的技术概念": "LPD-Net是一种深度神经网络，用于从3D点云中提取全局描述符。自适应局部特征提取模块和基于图的邻域聚合模块是该网络的两个关键组成部分，分别负责提取局部特征和揭示这些特征在大规模点云中的空间分布。这种方法旨在提高地点识别的准确性和对不同环境条件的鲁棒性。"
    },
    {
        "order": 911,
        "title": "Joint Learning of Saliency Detection and Weakly Supervised Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Joint_Learning_of_Saliency_Detection_and_Weakly_Supervised_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Joint_Learning_of_Saliency_Detection_and_Weakly_Supervised_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Existing weakly supervised semantic segmentation (WSSS) methods usually utilize the results of pre-trained saliency detection (SD) models without explicitly modelling the connections between the two tasks, which is not the most efficient configuration. Here we propose a unified multi-task learning framework to jointly solve WSSS and SD using a single network, i.e. saliency and segmentation network (SSNet). SSNet consists of a segmentation network (SN) and a saliency aggregation module (SAM). For an input image, SN generates the segmentation result and, SAM predicts the saliency of each category and aggregating the segmentation masks of all categories into a saliency map. The proposed network is trained end-to-end with image-level category labels and class-agnostic pixel-level saliency labels. Experiments on PASCAL VOC 2012 segmentation dataset and four saliency benchmark datasets show the performance of our method compares favorably against state-of-the-art weakly supervised segmentation methods and fully supervised saliency detection methods.",
        "中文标题": "显著性检测与弱监督语义分割的联合学习",
        "摘要翻译": "现有的弱监督语义分割（WSSS）方法通常利用预训练的显著性检测（SD）模型的结果，而没有明确建模这两个任务之间的联系，这不是最有效的配置。在这里，我们提出了一个统一的多任务学习框架，使用单一网络联合解决WSSS和SD，即显著性和分割网络（SSNet）。SSNet由分割网络（SN）和显著性聚合模块（SAM）组成。对于输入图像，SN生成分割结果，SAM预测每个类别的显著性，并将所有类别的分割掩码聚合为显著性图。所提出的网络使用图像级类别标签和类无关的像素级显著性标签进行端到端训练。在PASCAL VOC 2012分割数据集和四个显著性基准数据集上的实验表明，我们的方法在性能上优于最先进的弱监督分割方法和全监督显著性检测方法。",
        "领域": "显著性检测/语义分割/多任务学习",
        "问题": "如何有效地联合解决显著性检测和弱监督语义分割问题",
        "动机": "现有的方法没有明确建模显著性检测和弱监督语义分割之间的联系，导致效率不高",
        "方法": "提出了一个统一的多任务学习框架，使用单一网络联合解决显著性检测和弱监督语义分割问题，包括分割网络和显著性聚合模块",
        "关键词": [
            "显著性检测",
            "弱监督语义分割",
            "多任务学习",
            "端到端训练"
        ],
        "涉及的技术概念": {
            "弱监督语义分割（WSSS）": "一种使用弱标签（如图像级标签）进行语义分割的方法",
            "显著性检测（SD）": "识别图像中最显著或最吸引注意力的区域",
            "多任务学习": "同时学习多个相关任务以提高学习效率和性能",
            "端到端训练": "一种训练方法，其中模型直接从输入到输出进行训练，无需手动设计中间步骤"
        }
    },
    {
        "order": 912,
        "title": "Human Uncertainty Makes Classification More Robust",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.html",
        "abstract": "The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.",
        "中文标题": "人类的不确定性使分类更加稳健",
        "摘要翻译": "深度神经网络的分类性能已经开始在接近完美的水平上趋于稳定。然而，它们在训练集之外的泛化能力以及对对抗性攻击的鲁棒性尚未达到。在本文中，我们通过使用反映人类感知不确定性的完整标签分布进行训练，在这一问题上取得了进展。我们首先提出了一个新的基准数据集，我们称之为CIFAR10H，它包含了CIFAR10测试集中每张图像的完整人类标签分布。然后我们展示，虽然当代分类器本身未能表现出类似人类的不确定性，但在我们的数据集上进行显式训练可以缩小这一差距，支持对越来越超出训练分布的测试数据集的改进泛化，并赋予对抗性攻击的鲁棒性。",
        "领域": "对抗性机器学习/泛化能力/人类感知",
        "问题": "提高深度神经网络在训练集之外的泛化能力和对抗性攻击的鲁棒性",
        "动机": "深度神经网络在分类性能上已达到接近完美的水平，但在泛化能力和对抗性攻击的鲁棒性方面仍有不足，希望通过反映人类感知不确定性的训练方法来改进这些问题。",
        "方法": "使用包含人类标签分布的CIFAR10H数据集进行训练，以提高分类器的泛化能力和对抗性攻击的鲁棒性。",
        "关键词": [
            "对抗性攻击",
            "泛化能力",
            "人类感知不确定性"
        ],
        "涉及的技术概念": "深度神经网络、对抗性攻击、泛化能力、人类感知不确定性、CIFAR10H数据集"
    },
    {
        "order": 913,
        "title": "Local Supports Global: Deep Camera Relocalization With Sequence Enhancement",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_Local_Supports_Global_Deep_Camera_Relocalization_With_Sequence_Enhancement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xue_Local_Supports_Global_Deep_Camera_Relocalization_With_Sequence_Enhancement_ICCV_2019_paper.html",
        "abstract": "We propose to leverage the local information in a image sequence to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure.",
        "中文标题": "局部支持全局：通过序列增强实现深度相机重定位",
        "摘要翻译": "我们提出利用图像序列中的局部信息来支持全局相机重定位。与之前从单张图像回归全局姿态的方法不同，我们通过结合视觉里程计（VO）组件，利用序列图像中的时空一致性来减轻由于视觉模糊性带来的不确定性。具体来说，我们引入了两个有效步骤，称为内容增强的姿态估计和基于运动的优化。内容增强步骤侧重于通过基于VO流构建的局部地图中的共视性来增强观测，从而减轻姿态估计的不确定性。此外，基于运动的优化被表述为姿态图，其中通过采用VO组件提供的相对姿态作为额外的运动约束来进一步优化相机姿态。因此，可以保证全局一致性。在公开的室内7-Scenes和室外Oxford RobotCar基准数据集上的实验表明，受益于序列中固有的局部信息，我们的方法在性能上优于最先进的方法，特别是在一些具有挑战性的情况下，例如纹理不足、高度重复的纹理、相似的外观和过度曝光。",
        "领域": "相机重定位/视觉里程计/姿态估计",
        "问题": "解决在视觉模糊性条件下，从单张图像回归全局姿态的不确定性问题",
        "动机": "通过利用图像序列中的局部信息来增强全局相机重定位的准确性和鲁棒性",
        "方法": "引入内容增强的姿态估计和基于运动的优化两个步骤，结合视觉里程计组件，利用序列图像中的时空一致性来减轻不确定性",
        "关键词": [
            "相机重定位",
            "视觉里程计",
            "姿态估计",
            "内容增强",
            "运动优化"
        ],
        "涉及的技术概念": "视觉里程计（VO）是一种通过分析连续图像帧之间的视觉信息来估计相机运动的技术。姿态图是一种用于表示和优化相机姿态之间关系的图结构，其中节点代表相机姿态，边代表姿态之间的相对变换。共视性指的是在多个图像中观察到的相同场景点，这对于建立局部地图和增强姿态估计的准确性至关重要。"
    },
    {
        "order": 914,
        "title": "Towards High-Resolution Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zeng_Towards_High-Resolution_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zeng_Towards_High-Resolution_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Deep neural network based methods have made a significant breakthrough in salient object detection. However, they are typically limited to input images with low resolutions (400x400 pixels or less). Little effort has been made to train neural networks to directly handle salient object segmentation in high-resolution images. This paper pushes forward high-resolution saliency detection, and contributes a new dataset, named High-Resolution Salient Object Detection (HRSOD) dataset. To our best knowledge, HRSOD is the first high-resolution saliency detection dataset to date. As another contribution, we also propose a novel approach, which incorporates both global semantic information and local high-resolution details, to address this challenging task. More specifically, our approach consists of a Global Semantic Network (GSN), a Local Refinement Network (LRN) and a Global-Local Fusion Network (GLFN). The GSN extracts the global semantic information based on downsampled entire image. Guided by the results of GSN, the LRN focuses on some local regions and progressively produces high-resolution predictions. The GLFN is further proposed to enforce spatial consistency and boost performance. Experiments illustrate that our method outperforms existing state-of-the-art methods on high-resolution saliency datasets by a large margin, and achieves comparable or even better performance than them on some widely used saliency benchmarks.",
        "中文标题": "迈向高分辨率显著目标检测",
        "摘要翻译": "基于深度神经网络的方法在显著目标检测方面取得了显著突破。然而，这些方法通常仅限于处理低分辨率（400x400像素或更小）的输入图像。在训练神经网络直接处理高分辨率图像中的显著目标分割方面，几乎没有做出任何努力。本文推动了高分辨率显著性检测的发展，并贡献了一个新的数据集，命名为高分辨率显著目标检测（HRSOD）数据集。据我们所知，HRSOD是迄今为止第一个高分辨率显著性检测数据集。作为另一个贡献，我们还提出了一种新方法，该方法结合了全局语义信息和局部高分辨率细节，以解决这一具有挑战性的任务。更具体地说，我们的方法包括一个全局语义网络（GSN）、一个局部细化网络（LRN）和一个全局-局部融合网络（GLFN）。GSN基于下采样的整个图像提取全局语义信息。在GSN结果的指导下，LRN专注于一些局部区域，并逐步产生高分辨率预测。进一步提出了GLFN以加强空间一致性并提升性能。实验表明，我们的方法在高分辨率显著性数据集上大幅优于现有的最先进方法，并在一些广泛使用的显著性基准测试中实现了可比甚至更好的性能。",
        "领域": "显著目标检测/高分辨率图像处理/深度学习",
        "问题": "现有显著目标检测方法在处理高分辨率图像时存在限制",
        "动机": "推动高分辨率显著性检测的发展，解决现有方法在处理高分辨率图像时的不足",
        "方法": "提出了一种结合全局语义信息和局部高分辨率细节的新方法，包括全局语义网络（GSN）、局部细化网络（LRN）和全局-局部融合网络（GLFN）",
        "关键词": [
            "显著目标检测",
            "高分辨率图像",
            "全局语义网络",
            "局部细化网络",
            "全局-局部融合网络"
        ],
        "涉及的技术概念": {
            "显著目标检测": "识别图像中最吸引人注意的目标或区域",
            "高分辨率图像": "具有较高像素密度的图像，能够提供更多细节信息",
            "全局语义网络（GSN）": "用于从下采样的整个图像中提取全局语义信息的网络",
            "局部细化网络（LRN）": "专注于局部区域并逐步产生高分辨率预测的网络",
            "全局-局部融合网络（GLFN）": "用于加强空间一致性并提升性能的网络"
        }
    },
    {
        "order": 915,
        "title": "FCOS: Fully Convolutional One-Stage Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.html",
        "abstract": "We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1",
        "中文标题": "FCOS: 全卷积一阶段目标检测",
        "摘要翻译": "我们提出了一种全卷积一阶段目标检测器（FCOS），以逐像素预测的方式解决目标检测问题，类似于语义分割。几乎所有最先进的目标检测器，如RetinaNet、SSD、YOLOv3和Faster R-CNN，都依赖于预定义的锚框。相比之下，我们提出的检测器FCOS无需锚框，也无需提案。通过消除预定义的锚框集，FCOS完全避免了与锚框相关的复杂计算，如训练期间的重叠计算。更重要的是，我们还避免了所有与锚框相关的超参数，这些超参数通常对最终检测性能非常敏感。仅通过后处理非极大值抑制（NMS），使用ResNeXt-64x4d-101的FCOS在单模型和单尺度测试中实现了44.7%的AP，超越了之前的一阶段检测器，具有更简单的优势。我们首次展示了一个更简单、更灵活的检测框架，实现了改进的检测精度。我们希望提出的FCOS框架可以作为许多其他实例级任务的简单而强大的替代方案。代码可在https://tinyurl.com/FCOSv1获取。",
        "领域": "目标检测/语义分割/实例级任务",
        "问题": "解决目标检测中的复杂性和对预定义锚框的依赖",
        "动机": "简化目标检测框架，提高检测精度，避免与锚框相关的复杂计算和敏感超参数",
        "方法": "提出一种无需锚框和提案的全卷积一阶段目标检测器（FCOS），采用逐像素预测方式，仅通过非极大值抑制（NMS）进行后处理",
        "关键词": [
            "目标检测",
            "全卷积网络",
            "非极大值抑制"
        ],
        "涉及的技术概念": "FCOS是一种无需预定义锚框的目标检测方法，它通过逐像素预测来定位和分类目标，避免了传统方法中与锚框相关的复杂计算和超参数调整。该方法通过非极大值抑制（NMS）进行后处理，以提高检测精度。"
    },
    {
        "order": 916,
        "title": "Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Sequential_Adversarial_Learning_for_Self-Supervised_Deep_Visual_Odometry_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Sequential_Adversarial_Learning_for_Self-Supervised_Deep_Visual_Odometry_ICCV_2019_paper.html",
        "abstract": "We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.",
        "中文标题": "序列对抗学习用于自监督深度视觉里程计",
        "摘要翻译": "我们提出了一个自监督学习框架，用于视觉里程计（VO），该框架结合了连续帧的相关性，并利用了对抗学习的优势。之前的方法将自监督VO视为一个从运动中恢复局部结构（SfM）的问题，通过最小化扭曲图像和捕获图像之间的光度损失，从单张图像恢复深度，从图像对恢复相对姿态。由于单视图深度估计是一个不适定问题，且光度损失无法区分扭曲图像的失真伪影，因此估计的深度模糊，姿态不准确。与之前的方法相比，我们的框架学习了一个帧到帧相关性的紧凑表示，该表示通过结合序列信息进行更新。更新后的表示用于深度估计。此外，我们将VO视为一个自监督图像生成任务，并利用生成对抗网络（GAN）。生成器学习估计深度和姿态以生成扭曲的目标图像。判别器通过高级结构感知评估生成图像的质量，克服了之前方法中像素级损失的问题。在KITTI和Cityscapes数据集上的实验表明，我们的方法获得了更准确的深度，细节得以保留，预测的姿态显著优于最先进的自监督方法。",
        "领域": "视觉里程计/自监督学习/生成对抗网络",
        "问题": "解决自监督视觉里程计中深度估计模糊和姿态不准确的问题",
        "动机": "由于单视图深度估计的不适定性和光度损失无法有效区分图像失真，导致现有方法在深度和姿态估计上存在不足",
        "方法": "提出一个结合连续帧相关性和对抗学习的自监督学习框架，利用生成对抗网络进行深度和姿态估计，通过高级结构感知评估图像质量",
        "关键词": [
            "视觉里程计",
            "自监督学习",
            "生成对抗网络",
            "深度估计",
            "姿态估计"
        ],
        "涉及的技术概念": "视觉里程计（VO）是一种通过分析连续图像帧来估计相机运动的技术。自监督学习是一种不需要人工标注数据的学习方法。生成对抗网络（GAN）由生成器和判别器组成，生成器尝试生成逼真的数据，而判别器则尝试区分真实数据和生成数据。光度损失是指通过比较图像像素值来评估图像相似度的损失函数。"
    },
    {
        "order": 917,
        "title": "Event-Based Motion Segmentation by Motion Compensation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Stoffregen_Event-Based_Motion_Segmentation_by_Motion_Compensation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Stoffregen_Event-Based_Motion_Segmentation_by_Motion_Compensation_ICCV_2019_paper.html",
        "abstract": "In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes (called \"events\"), with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations (i.e., segmentation) and the motion parameters of the objects (or the background) by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement.",
        "中文标题": "基于事件的运动补偿运动分割",
        "摘要翻译": "与传统相机相比，其像素具有共同的曝光时间，基于事件的相机是一种新型的生物启发传感器，其像素独立工作并异步输出强度变化（称为“事件”），具有微秒级分辨率。由于事件是由物体的明显运动引起的，基于事件的相机根据场景动态采样视觉信息，因此比传统相机更适合获取运动，特别是在高速情况下，传统相机会受到运动模糊的影响。然而，区分由不同移动物体和相机自身运动引起的事件是一项具有挑战性的任务。我们提出了第一个逐事件分割方法，用于将场景分割为独立移动的物体。我们的方法通过最大化目标函数联合估计事件-物体关联（即分割）和物体（或背景）的运动参数，该目标函数建立在基于事件的运动补偿的最新结果之上。我们在公共数据集上对我们的方法进行了全面评估，比现有技术高出多达10%。我们还展示了事件相机分割算法的首次定量评估，在4像素相对位移时产生约90%的准确率。",
        "领域": "运动分割/事件相机/运动补偿",
        "问题": "区分由不同移动物体和相机自身运动引起的事件",
        "动机": "基于事件的相机更适合获取运动，特别是在高速情况下，传统相机会受到运动模糊的影响",
        "方法": "通过最大化目标函数联合估计事件-物体关联和物体（或背景）的运动参数",
        "关键词": [
            "运动分割",
            "事件相机",
            "运动补偿"
        ],
        "涉及的技术概念": "基于事件的相机是一种新型的生物启发传感器，其像素独立工作并异步输出强度变化（称为“事件”），具有微秒级分辨率。事件是由物体的明显运动引起的，基于事件的相机根据场景动态采样视觉信息。"
    },
    {
        "order": 918,
        "title": "Self-Critical Attention Learning for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Self-Critical_Attention_Learning_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a self-critical attention learning method for person re-identification. Unlike most existing methods which train the attention mechanism in a weakly-supervised manner and ignore the attention confidence level, we learn the attention with a critic which measures the attention quality and provides a powerful supervisory signal to guide the learning process. Moreover, the critic model facilitates the interpretation of the effectiveness of the attention mechanism during the learning process, by estimating the quality of the attention maps. Specifically, we jointly train our attention agent and critic in a reinforcement learning manner, where the agent produces the visual attention while the critic analyzes the gain from the attention and guides the agent to maximize this gain. We design spatial- and channel-wise attention models with our critic module and evaluate them on three popular benchmarks including Market-1501, DukeMTMC-ReID, and CUHK03. The experimental results demonstrate the superiority of our method, which outperforms the state-of-the-art methods by a large margin of 5.9%/2.1%, 6.3%/3.0%, and 10.5%/9.5% on mAP/Rank-1, respectively.",
        "中文标题": "自批判注意力学习用于行人重识别",
        "摘要翻译": "在本文中，我们提出了一种自批判注意力学习方法用于行人重识别。与大多数现有方法不同，这些方法以弱监督方式训练注意力机制并忽略注意力置信度，我们通过一个批判者来学习注意力，该批判者测量注意力质量并提供强大的监督信号来指导学习过程。此外，批判者模型通过估计注意力图的质量，促进了学习过程中注意力机制有效性的解释。具体来说，我们以强化学习的方式联合训练我们的注意力代理和批判者，其中代理产生视觉注意力，而批判者分析注意力带来的增益并指导代理最大化这一增益。我们设计了带有批判者模块的空间和通道注意力模型，并在包括Market-1501、DukeMTMC-ReID和CUHK03在内的三个流行基准上进行了评估。实验结果表明，我们的方法在mAP/Rank-1上分别以5.9%/2.1%、6.3%/3.0%和10.5%/9.5%的大幅度优于最先进的方法。",
        "领域": "行人重识别/注意力机制/强化学习",
        "问题": "如何提高行人重识别的准确性和效率",
        "动机": "现有方法在训练注意力机制时往往忽略注意力置信度，且缺乏对注意力机制有效性的解释",
        "方法": "提出了一种自批判注意力学习方法，通过一个批判者来测量注意力质量并提供监督信号，以强化学习的方式联合训练注意力代理和批判者",
        "关键词": [
            "行人重识别",
            "注意力机制",
            "强化学习",
            "自批判学习"
        ],
        "涉及的技术概念": "注意力机制是一种使模型能够专注于输入数据的特定部分的技术。在本文中，通过引入批判者模型来评估注意力图的质量，从而提供反馈以优化注意力机制。强化学习是一种学习方法，其中代理通过与环境交互来学习策略，以最大化某种累积奖励。本文中，注意力代理和批判者以强化学习的方式联合训练，以提高行人重识别的性能。"
    },
    {
        "order": 919,
        "title": "TextPlace: Visual Place Recognition and Topological Localization Through Reading Scene Texts",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hong_TextPlace_Visual_Place_Recognition_and_Topological_Localization_Through_Reading_Scene_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hong_TextPlace_Visual_Place_Recognition_and_Topological_Localization_Through_Reading_Scene_ICCV_2019_paper.html",
        "abstract": "Visual place recognition is a fundamental problem for many vision based applications. Sparse feature and deep learning based methods have been successful and dominant over the decade. However, most of them do not explicitly leverage high-level semantic information to deal with challenging scenarios where they may fail. This paper proposes a novel visual place recognition algorithm, termed TextPlace, based on scene texts in the wild. Since scene texts are high-level information invariant to illumination changes and very distinct for different places when considering spatial correlation, it is beneficial for visual place recognition tasks under extreme appearance changes and perceptual aliasing. It also takes spatial-temporal dependence between scene texts into account for topological localization. Extensive experiments show that TextPlace achieves state-of-the-art performance, verifying the effectiveness of using high-level scene texts for robust visual place recognition in urban areas.",
        "中文标题": "TextPlace: 通过阅读场景文本进行视觉地点识别和拓扑定位",
        "摘要翻译": "视觉地点识别是许多基于视觉的应用中的一个基本问题。稀疏特征和基于深度学习的方法在过去十年中取得了成功并占据了主导地位。然而，大多数方法并未明确利用高级语义信息来处理它们可能失败的挑战性场景。本文提出了一种新颖的视觉地点识别算法，称为TextPlace，基于野外场景中的文本。由于场景文本是高级信息，对光照变化不变，并且在考虑空间相关性时对不同地点非常独特，因此它对于极端外观变化和感知混淆下的视觉地点识别任务非常有益。它还考虑了场景文本之间的时空依赖性，用于拓扑定位。大量实验表明，TextPlace实现了最先进的性能，验证了使用高级场景文本在城市地区进行鲁棒视觉地点识别的有效性。",
        "领域": "视觉地点识别/拓扑定位/场景文本分析",
        "问题": "在极端外观变化和感知混淆下进行鲁棒的视觉地点识别",
        "动机": "现有方法大多未利用高级语义信息处理挑战性场景，导致在特定情况下可能失败",
        "方法": "提出了一种基于场景文本的视觉地点识别算法TextPlace，利用场景文本的高级信息不变性和空间相关性，同时考虑场景文本之间的时空依赖性进行拓扑定位",
        "关键词": [
            "视觉地点识别",
            "拓扑定位",
            "场景文本",
            "高级语义信息",
            "时空依赖性"
        ],
        "涉及的技术概念": "TextPlace算法通过分析场景中的文本信息，利用这些文本的高级语义信息（对光照变化不变且具有空间相关性）来增强视觉地点识别的鲁棒性。此外，该算法还考虑了场景文本之间的时空依赖性，以支持拓扑定位任务。"
    },
    {
        "order": 920,
        "title": "Depth-Induced Multi-Scale Recurrent Attention Network for Saliency Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.html",
        "abstract": "In this work, we propose a novel depth-induced multi-scale recurrent attention network for saliency detection. It achieves dramatic performance especially in complex scenarios. There are three main contributions of our network that are experimentally demonstrated to have significant practical merits. First, we design an effective depth refinement block using residual connections to fully extract and fuse multi-level paired complementary cues from RGB and depth streams. Second, depth cues with abundant spatial information are innovatively combined with multi-scale context features for accurately locating salient objects. Third, we boost our model's performance by a novel recurrent attention module inspired by Internal Generative Mechanism of human brain. This module can generate more accurate saliency results via comprehensively learning the internal semantic relation of the fused feature and progressively optimizing local details with memory-oriented scene understanding. In addition, we create a large scale RGB-D dataset containing more complex scenarios, which can contribute to comprehensively evaluating saliency models. Extensive experiments on six public datasets and ours demonstrate that our method can accurately identify salient objects and achieve consistently superior performance over 16 state-of-the-art RGB and RGB-D approaches.",
        "中文标题": "深度诱导的多尺度循环注意力网络用于显著性检测",
        "摘要翻译": "在本工作中，我们提出了一种新颖的深度诱导多尺度循环注意力网络用于显著性检测。它在复杂场景中尤其表现出色。我们的网络有三个主要贡献，这些贡献通过实验证明具有显著的实用价值。首先，我们设计了一个有效的深度细化块，使用残差连接从RGB和深度流中充分提取并融合多层次的配对互补线索。其次，具有丰富空间信息的深度线索与多尺度上下文特征创新性地结合，以准确定位显著对象。第三，我们通过一个受人类大脑内部生成机制启发的新型循环注意力模块提升了我们模型的性能。该模块通过全面学习融合特征的内部语义关系，并通过面向记忆的场景理解逐步优化局部细节，从而生成更准确的显著性结果。此外，我们创建了一个包含更复杂场景的大规模RGB-D数据集，这有助于全面评估显著性模型。在六个公共数据集和我们的数据集上进行的大量实验表明，我们的方法能够准确识别显著对象，并在16种最先进的RGB和RGB-D方法中始终表现出色。",
        "领域": "显著性检测/深度估计/场景理解",
        "问题": "在复杂场景中准确检测显著对象",
        "动机": "提高显著性检测在复杂场景中的准确性和鲁棒性",
        "方法": "设计深度细化块提取和融合RGB和深度流的互补线索，结合深度线索与多尺度上下文特征，引入循环注意力模块优化局部细节",
        "关键词": [
            "显著性检测",
            "深度估计",
            "场景理解",
            "循环注意力模块",
            "RGB-D数据集"
        ],
        "涉及的技术概念": "深度细化块用于从RGB和深度流中提取和融合多层次的配对互补线索；多尺度上下文特征与深度线索结合以准确定位显著对象；循环注意力模块通过全面学习融合特征的内部语义关系，并通过面向记忆的场景理解逐步优化局部细节，以生成更准确的显著性结果。"
    },
    {
        "order": 921,
        "title": "Temporal Knowledge Propagation for Image-to-Video Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Temporal_Knowledge_Propagation_for_Image-to-Video_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "In many scenarios of Person Re-identification (Re-ID), the gallery set consists of lots of surveillance videos and the query is just an image, thus Re-ID has to be conducted between image and videos. Compared with videos, still person images lack temporal information. Besides, the information asymmetry between image and video features increases the difficulty in matching images and videos. To solve this problem, we propose a novel Temporal Knowledge Propagation (TKP) method which propagates the temporal knowledge learned by the video representation network to the image representation network. Specifically, given the input videos, we enforce the image representation network to fit the outputs of video representation network in a shared feature space. With back propagation, temporal knowledge can be transferred to enhance the image features and the information asymmetry problem can be alleviated. With additional classification and integrated triplet losses, our model can learn expressive and discriminative image and video features for image-to-video re-identification. Extensive experiments demonstrate the effectiveness of our method and the overall results on two widely used datasets surpass the state-of-the-art methods by a large margin.",
        "中文标题": "时间知识传播用于图像到视频的人员再识别",
        "摘要翻译": "在许多人员再识别（Re-ID）场景中，图库集由大量监控视频组成，而查询仅是一张图像，因此必须在图像和视频之间进行Re-ID。与视频相比，静止的人员图像缺乏时间信息。此外，图像和视频特征之间的信息不对称增加了匹配图像和视频的难度。为了解决这个问题，我们提出了一种新颖的时间知识传播（TKP）方法，该方法将视频表示网络学习到的时间知识传播到图像表示网络。具体来说，给定输入视频，我们强制图像表示网络在共享特征空间中适应视频表示网络的输出。通过反向传播，时间知识可以被转移以增强图像特征，从而缓解信息不对称问题。通过额外的分类和集成三元组损失，我们的模型可以学习到表达性和区分性的图像和视频特征，用于图像到视频的再识别。大量实验证明了我们方法的有效性，并且在两个广泛使用的数据集上的总体结果大大超过了最先进的方法。",
        "领域": "人员再识别/视频分析/特征学习",
        "问题": "解决图像和视频之间人员再识别时的时间信息缺乏和信息不对称问题",
        "动机": "静止的人员图像缺乏时间信息，且图像和视频特征之间存在信息不对称，增加了匹配难度",
        "方法": "提出时间知识传播（TKP）方法，通过视频表示网络向图像表示网络传播时间知识，增强图像特征，缓解信息不对称问题",
        "关键词": [
            "时间知识传播",
            "人员再识别",
            "特征学习",
            "视频分析"
        ],
        "涉及的技术概念": "时间知识传播（TKP）方法通过视频表示网络向图像表示网络传播时间知识，利用反向传播增强图像特征，通过分类和集成三元组损失学习表达性和区分性的图像和视频特征。"
    },
    {
        "order": 922,
        "title": "CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.html",
        "abstract": "Camera re-localization is an important but challenging task in applications like robotics and autonomous driving. Recently, retrieval-based methods have been considered as a promising direction as they can be easily generalized to novel scenes. Despite significant progress has been made, we observe that the performance bottleneck of previous methods actually lies in the retrieval module. These methods use the same features for both retrieval and relative pose regression tasks which have potential conflicts in learning. To this end, here we present a coarse-to-fine retrieval-based deep learning framework, which includes three steps, i.e., image-based coarse retrieval, pose-based fine retrieval and precise relative pose regression. With our carefully designed retrieval module, the relative pose regression task can be surprisingly simpler. We design novel retrieval losses with batch hard sampling criterion and two-stage retrieval to locate samples that adapt to the relative pose regression task. Extensive experiments show that our model (CamNet) outperforms the state-of-the-art methods by a large margin on both indoor and outdoor datasets.",
        "中文标题": "CamNet: 从粗到细的检索用于相机重定位",
        "摘要翻译": "相机重定位在机器人和自动驾驶等应用中是一个重要但具有挑战性的任务。最近，基于检索的方法被认为是一个有前景的方向，因为它们可以很容易地推广到新场景。尽管已经取得了显著进展，但我们观察到以前方法的性能瓶颈实际上在于检索模块。这些方法在检索和相对姿态回归任务中使用相同的特征，这在学习中可能存在冲突。为此，我们提出了一个从粗到细的基于检索的深度学习框架，包括三个步骤，即基于图像的粗检索、基于姿态的细检索和精确的相对姿态回归。通过我们精心设计的检索模块，相对姿态回归任务可以变得异常简单。我们设计了新的检索损失，采用批量硬采样标准和两阶段检索来定位适应相对姿态回归任务的样本。大量实验表明，我们的模型（CamNet）在室内和室外数据集上都大幅超越了最先进的方法。",
        "领域": "相机重定位/深度学习/机器人视觉",
        "问题": "解决相机重定位任务中的性能瓶颈问题",
        "动机": "观察到以前的方法在检索和相对姿态回归任务中使用相同特征可能导致学习冲突，因此提出新的框架以提高性能",
        "方法": "提出了一个从粗到细的基于检索的深度学习框架，包括图像粗检索、姿态细检索和精确相对姿态回归三个步骤，并设计了新的检索损失和两阶段检索方法",
        "关键词": [
            "相机重定位",
            "深度学习",
            "检索模块",
            "相对姿态回归"
        ],
        "涉及的技术概念": "基于检索的方法、深度学习框架、图像粗检索、姿态细检索、相对姿态回归、检索损失、批量硬采样、两阶段检索"
    },
    {
        "order": 923,
        "title": "Stacked Cross Refinement Network for Edge-Aware Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Salient object detection is a fundamental computer vision task. The majority of existing algorithms focus on aggregating multi-level features of pre-trained convolutional neural networks. Moreover, some researchers attempt to utilize edge information for auxiliary training. However, existing edge-aware models design unidirectional frameworks which only use edge features to improve the segmentation features. Motivated by the logical interrelations between binary segmentation and edge maps, we propose a novel Stacked Cross Refinement Network (SCRN) for salient object detection in this paper. Our framework aims to simultaneously refine multi-level features of salient object detection and edge detection by stacking Cross Refinement Unit (CRU). According to the logical interrelations, the CRU designs two direction-specific integration operations, and bidirectionally passes messages between the two tasks. Incorporating the refined edge-preserving features with the typical U-Net, our model detects salient objects accurately. Extensive experiments conducted on six benchmark datasets demonstrate that our method outperforms existing state-of-the-art algorithms in both accuracy and efficiency. Besides, the attribute-based performance on the SOC dataset show that the proposed model ranks first in the majority of challenging scenes. Code can be found at https://github.com/wuzhe71/SCAN.",
        "中文标题": "堆叠交叉精炼网络用于边缘感知的显著目标检测",
        "摘要翻译": "显著目标检测是一项基础的计算机视觉任务。大多数现有算法集中于聚合预训练卷积神经网络的多层次特征。此外，一些研究者尝试利用边缘信息进行辅助训练。然而，现有的边缘感知模型设计了单向框架，仅使用边缘特征来改进分割特征。受到二值分割和边缘图之间逻辑相互关系的启发，我们在本文中提出了一种新颖的堆叠交叉精炼网络（SCRN）用于显著目标检测。我们的框架旨在通过堆叠交叉精炼单元（CRU）同时精炼显著目标检测和边缘检测的多层次特征。根据逻辑相互关系，CRU设计了两个方向特定的集成操作，并在两个任务之间双向传递信息。通过将精炼的边缘保留特征与典型的U-Net结合，我们的模型能够准确地检测显著目标。在六个基准数据集上进行的大量实验表明，我们的方法在准确性和效率上都优于现有的最先进算法。此外，在SOC数据集上的基于属性的性能显示，所提出的模型在大多数挑战性场景中排名第一。代码可以在https://github.com/wuzhe71/SCAN找到。",
        "领域": "显著目标检测/边缘检测/特征精炼",
        "问题": "如何同时精炼显著目标检测和边缘检测的多层次特征",
        "动机": "受到二值分割和边缘图之间逻辑相互关系的启发，旨在改进现有边缘感知模型仅使用边缘特征来改进分割特征的单向框架",
        "方法": "提出了一种新颖的堆叠交叉精炼网络（SCRN），通过堆叠交叉精炼单元（CRU）同时精炼显著目标检测和边缘检测的多层次特征，并设计两个方向特定的集成操作，在显著目标检测和边缘检测之间双向传递信息",
        "关键词": [
            "显著目标检测",
            "边缘检测",
            "特征精炼",
            "堆叠交叉精炼网络",
            "交叉精炼单元"
        ],
        "涉及的技术概念": "显著目标检测是一项基础的计算机视觉任务，旨在从图像中识别出最吸引人注意的目标。边缘检测是识别图像中物体边界的过程。特征精炼是指通过特定方法改进或优化特征表示的过程。堆叠交叉精炼网络（SCRN）是一种新颖的网络架构，通过交叉精炼单元（CRU）在显著目标检测和边缘检测之间实现多层次特征的精炼和双向信息传递。"
    },
    {
        "order": 924,
        "title": "RepPoints: Point Set Representation for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present RepPoints (representative points), a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 AP_ 50  on the COCO test-dev detection benchmark, using ResNet-101 model. Code is available at  https://github.com/microsoft/RepPoints  \\color cyan  https://github.com/microsoft/RepPoints  .",
        "中文标题": "RepPoints: 用于目标检测的点集表示",
        "摘要翻译": "现代目标检测器在各个识别阶段严重依赖矩形边界框，如锚点、提议和最终预测，来表示对象。边界框使用方便，但仅提供对象的粗略定位，并导致对象特征的相应粗略提取。在本文中，我们提出了RepPoints（代表性点），一种新的更精细的对象表示方法，作为一组样本点，对定位和识别都有用。给定用于训练的地面真实定位和识别目标，RepPoints学会自动排列自己，以限定对象的空间范围并指示语义上重要的局部区域。此外，它们不需要使用锚点来采样边界框的空间。我们展示了基于RepPoints的无锚点目标检测器可以与最先进的基于锚点的检测方法一样有效，在COCO测试开发检测基准上使用ResNet-101模型达到了46.5 AP和67.4 AP_50。代码可在https://github.com/microsoft/RepPoints获取。",
        "领域": "目标检测/图像识别/特征提取",
        "问题": "传统目标检测方法依赖矩形边界框进行对象表示，导致对象定位和特征提取的精度不高。",
        "动机": "提高目标检测中对象定位和特征提取的精度，通过引入更精细的对象表示方法。",
        "方法": "提出RepPoints，一种新的对象表示方法，通过一组样本点来表示对象，这些点能够自动排列以限定对象空间范围并指示语义上重要的局部区域，无需使用锚点。",
        "关键词": [
            "目标检测",
            "对象表示",
            "特征提取"
        ],
        "涉及的技术概念": "RepPoints是一种新的对象表示方法，通过一组样本点来表示对象，这些点能够自动排列以限定对象空间范围并指示语义上重要的局部区域。这种方法不需要使用传统的锚点来采样边界框的空间，从而提高了对象定位和特征提取的精度。"
    },
    {
        "order": 925,
        "title": "Situational Fusion of Visual Representation for Visual Navigation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Situational_Fusion_of_Visual_Representation_for_Visual_Navigation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Situational_Fusion_of_Visual_Representation_for_Visual_Navigation_ICCV_2019_paper.html",
        "abstract": "A complex visual navigation task puts an agent in different situations which call for a diverse range of visual perception abilities. For example, to \"go to the nearest chair\", the agent might need to identify a chair in a living room using semantics, follow along a hallway using vanishing point cues, and avoid obstacles using depth. Therefore, utilizing the appropriate visual perception abilities based on a situational understanding of the visual environment can empower these navigation models in unseen visual environments. We propose to train an agent to fuse a large set of visual representations that correspond to diverse visual perception abilities. To fully utilize each representation, we develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action. Furthermore, we employ a data-driven inter-task affinity regularization to reduce redundancies and improve generalization. Our approach leads to a significantly improved performance in novel environments over ImageNet-pretrained baseline and other fusion methods.",
        "中文标题": "视觉导航中视觉表示的情境融合",
        "摘要翻译": "复杂的视觉导航任务将代理置于不同的情境中，这些情境需要多样化的视觉感知能力。例如，为了“去最近的椅子”，代理可能需要使用语义在客厅中识别椅子，使用消失点线索沿着走廊前进，并使用深度避免障碍物。因此，基于对视觉环境的情境理解，利用适当的视觉感知能力可以增强这些导航模型在未见过的视觉环境中的能力。我们提出训练一个代理来融合对应于多样化视觉感知能力的大量视觉表示。为了充分利用每个表示，我们开发了一种动作级别的表示融合方案，该方案从每个表示中预测一个动作候选，并自适应地将这些动作候选整合为最终动作。此外，我们采用数据驱动的任务间亲和性正则化来减少冗余并提高泛化能力。我们的方法在新环境中相比ImageNet预训练的基线和其他融合方法显著提高了性能。",
        "领域": "视觉导航/视觉感知/情境理解",
        "问题": "如何在复杂视觉导航任务中有效融合多样化的视觉表示以提高导航模型在未见过的视觉环境中的性能",
        "动机": "复杂的视觉导航任务需要代理在不同的情境中利用多样化的视觉感知能力，而现有的导航模型在未见过的视觉环境中表现不佳，因此需要一种方法来有效融合这些视觉表示以提高性能。",
        "方法": "提出了一种动作级别的视觉表示融合方案，该方案从每个视觉表示中预测动作候选，并自适应地整合这些候选为最终动作。同时，采用数据驱动的任务间亲和性正则化来减少冗余并提高泛化能力。",
        "关键词": [
            "视觉导航",
            "视觉感知",
            "情境理解",
            "表示融合",
            "动作预测",
            "任务间亲和性正则化"
        ],
        "涉及的技术概念": "视觉表示融合指的是将不同的视觉感知能力（如语义识别、消失点线索、深度感知）的表示结合起来，以提高视觉导航任务的性能。动作级别的表示融合方案是一种从每个视觉表示中预测动作候选，并将这些候选整合为最终动作的方法。数据驱动的任务间亲和性正则化是一种减少模型冗余并提高其在新环境中泛化能力的技术。"
    },
    {
        "order": 926,
        "title": "Motion Guided Attention for Video Salient Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Motion_Guided_Attention_for_Video_Salient_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Motion_Guided_Attention_for_Video_Salient_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.",
        "中文标题": "运动引导注意力用于视频显著目标检测",
        "摘要翻译": "视频显著目标检测旨在发现视频中最具视觉显著性的目标。在视频显著目标检测过程中，如何有效地考虑目标运动是一个关键问题。现有的最先进方法要么没有明确地建模和利用运动线索，要么忽略了光流图像中的空间上下文。在本文中，我们开发了一个多任务运动引导视频显著目标检测网络，该网络学习使用两个子网络完成两个子任务，一个子网络用于静态图像中的显著目标检测，另一个用于光流图像中的运动显著性检测。我们进一步引入了一系列新颖的运动引导注意力模块，这些模块利用运动显著性子网络来关注并增强静态图像子网络。这两个子网络通过端到端训练学习相互适应。实验结果表明，所提出的方法在广泛的基准测试中显著优于现有的最先进算法。我们希望我们简单而有效的方法能作为一个坚实的基线，并有助于简化未来在视频显著目标检测中的研究。代码和模型将公开提供。",
        "领域": "视频分析/显著目标检测/运动分析",
        "问题": "在视频显著目标检测中有效考虑目标运动",
        "动机": "现有方法未能有效利用运动线索或忽略光流图像中的空间上下文",
        "方法": "开发了一个多任务运动引导视频显著目标检测网络，包括两个子网络分别处理静态图像和光流图像，并引入运动引导注意力模块增强静态图像子网络",
        "关键词": [
            "视频显著目标检测",
            "运动引导注意力",
            "多任务学习"
        ],
        "涉及的技术概念": "显著目标检测是指在图像或视频中识别出最吸引人注意的目标或区域。光流图像是通过分析视频帧之间的像素移动来捕捉运动信息的图像。多任务学习是一种机器学习方法，通过同时学习多个相关任务来提高模型的泛化能力。端到端训练指的是模型从输入到输出的整个流程都是可训练的，无需手动设计中间步骤。"
    },
    {
        "order": 927,
        "title": "SegEQA: Video Segmentation Based Visual Attention for Embodied Question Answering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_SegEQA_Video_Segmentation_Based_Visual_Attention_for_Embodied_Question_Answering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_SegEQA_Video_Segmentation_Based_Visual_Attention_for_Embodied_Question_Answering_ICCV_2019_paper.html",
        "abstract": "Embodied Question Answering (EQA) is a newly defined research area where an agent is required to answer the user's questions by exploring the real world environment. It has attracted increasing research interests due to its broad applications in automatic driving system, in-home robots, and personal assistants. Most of the existing methods perform poorly in terms of answering and navigation accuracy due to the absence of local details and vulnerability to the ambiguity caused by complicated vision conditions. To tackle these problems, we propose a segmentation based visual attention mechanism for Embodied Question Answering. Firstly, We extract the local semantic features by introducing a novel high-speed video segmentation framework. Then by the guide of extracted semantic features, a bottom-up visual attention mechanism is proposed for the Visual Question Answering (VQA) sub-task. Further, a feature fusion strategy is proposed to guide the training of the navigator without much additional computational cost. The ablation experiments show that our method boosts the performance of VQA module by 4.2% (68.99% vs 64.73%) and leads to 3.6% (48.59% vs 44.98%) overall improvement in EQA accuracy.",
        "中文标题": "SegEQA: 基于视频分割的视觉注意力机制用于具身问答",
        "摘要翻译": "具身问答（EQA）是一个新定义的研究领域，要求代理通过探索现实世界环境来回答用户的问题。由于其在自动驾驶系统、家用机器人和个人助理中的广泛应用，它吸引了越来越多的研究兴趣。大多数现有方法在回答和导航准确性方面表现不佳，原因是缺乏局部细节以及对复杂视觉条件引起的模糊性的脆弱性。为了解决这些问题，我们提出了一种基于分割的视觉注意力机制用于具身问答。首先，我们通过引入一种新颖的高速视频分割框架来提取局部语义特征。然后，在提取的语义特征的指导下，提出了一种自下而上的视觉注意力机制用于视觉问答（VQA）子任务。此外，提出了一种特征融合策略，以指导导航器的训练，而无需太多额外的计算成本。消融实验表明，我们的方法将VQA模块的性能提高了4.2%（68.99%对64.73%），并在EQA准确性上带来了3.6%（48.59%对44.98%）的整体提升。",
        "领域": "具身问答/视觉问答/视频分割",
        "问题": "提高具身问答中的回答和导航准确性",
        "动机": "现有方法在回答和导航准确性方面表现不佳，原因是缺乏局部细节以及对复杂视觉条件引起的模糊性的脆弱性",
        "方法": "提出了一种基于分割的视觉注意力机制，包括引入高速视频分割框架提取局部语义特征，提出自下而上的视觉注意力机制用于视觉问答子任务，以及提出特征融合策略指导导航器的训练",
        "关键词": [
            "具身问答",
            "视觉问答",
            "视频分割",
            "视觉注意力机制",
            "特征融合"
        ],
        "涉及的技术概念": "具身问答（EQA）是一种要求代理通过探索现实世界环境来回答用户问题的研究领域。视觉问答（VQA）是EQA的一个子任务，涉及通过视觉信息回答问题。视频分割是一种技术，用于从视频中提取局部语义特征。视觉注意力机制是一种方法，用于指导模型关注图像或视频中的重要区域。特征融合是一种策略，用于结合不同来源的特征以提高模型性能。"
    },
    {
        "order": 928,
        "title": "Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.html",
        "abstract": "Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.",
        "中文标题": "学习异常抑制相关滤波器用于实时无人机跟踪",
        "摘要翻译": "传统的判别相关滤波器（DCF）框架经常受到不希望的边界效应的影响。过去几年中，已经提出了几种扩大搜索区域的方法来弥补这一不足。然而，随着背景信息的增加，也引入了更多的背景噪声，判别滤波器容易从环境中学习而不是从目标中学习。这种情况，加上由完全/部分遮挡、光照变化和其他原因引起的目标外观变化，使得检测过程中更有可能出现异常，这可能会大大降低其结果的可信度。因此，在这项工作中，提出了一种新的方法来抑制检测过程中发生的异常，即异常抑制相关滤波器（ARCF）。通过在检测阶段生成的响应图中对变化率施加限制，ARCF跟踪器可以明显抑制异常，从而更稳健和准确地跟踪目标。在不同的无人机数据集上进行了大量实验，以从空中视角执行目标跟踪，即UAV123、UAVDT和DTB70，包含超过90K帧的243个具有挑战性的图像序列，以验证ARCF跟踪器的性能，并证明其优于其他20种基于DCF和深度学习框架的最先进跟踪器，具有足够的实时应用速度。",
        "领域": "无人机跟踪/目标跟踪/异常检测",
        "问题": "传统判别相关滤波器框架中的边界效应和背景噪声问题，以及目标外观变化导致的检测异常",
        "动机": "提高无人机跟踪的准确性和鲁棒性，减少检测过程中的异常，提升跟踪结果的可信度",
        "方法": "提出异常抑制相关滤波器（ARCF），通过在检测阶段生成的响应图中对变化率施加限制来抑制异常",
        "关键词": [
            "无人机跟踪",
            "目标跟踪",
            "异常检测",
            "判别相关滤波器",
            "响应图"
        ],
        "涉及的技术概念": "判别相关滤波器（DCF）是一种用于目标跟踪的技术，通过计算目标与背景的相关性来区分目标。异常抑制相关滤波器（ARCF）是一种改进的DCF方法，通过在检测阶段对响应图的变化率施加限制来抑制检测过程中的异常，从而提高跟踪的准确性和鲁棒性。"
    },
    {
        "order": 929,
        "title": "Semi-Supervised Video Salient Object Detection Using Pseudo-Labels",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_ICCV_2019_paper.html",
        "abstract": "Deep learning-based video salient object detection has recently achieved great success with its performance significantly outperforming any other unsupervised methods. However, existing data-driven approaches heavily rely on a large quantity of pixel-wise annotated video frames to deliver such promising results. In this paper, we address the semi-supervised video salient object detection task using pseudo-labels. Specifically, we present an effective video saliency detector that consists of a spatial refinement network and a spatiotemporal module. Based on the same refinement network and motion information in terms of optical flow, we further propose a novel method for generating pixel-level pseudo-labels from sparsely annotated frames. By utilizing the generated pseudo-labels together with a part of manual annotations, our video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement, thus producing accurate saliency maps. Experimental results demonstrate that our proposed semi-supervised method even greatly outperforms all the state-of-the-art fully supervised methods across three public benchmarks of VOS, DAVIS, and FBMS.",
        "中文标题": "使用伪标签的半监督视频显著目标检测",
        "摘要翻译": "基于深度学习的视频显著目标检测最近取得了巨大成功，其性能显著优于任何其他无监督方法。然而，现有的数据驱动方法在很大程度上依赖于大量像素级注释的视频帧来提供如此有希望的结果。在本文中，我们使用伪标签解决了半监督视频显著目标检测任务。具体来说，我们提出了一个有效的视频显著性检测器，它由一个空间细化网络和一个时空模块组成。基于相同的细化网络和光流形式的运动信息，我们进一步提出了一种从稀疏注释帧生成像素级伪标签的新方法。通过利用生成的伪标签和一部分手动注释，我们的视频显著性检测器学习了对比推理和一致性增强的空间和时间线索，从而产生准确的显著性图。实验结果表明，我们提出的半监督方法在VOS、DAVIS和FBMS三个公共基准测试中甚至大大优于所有最先进的完全监督方法。",
        "领域": "视频分析/显著性检测/半监督学习",
        "问题": "解决视频显著目标检测任务中需要大量像素级注释视频帧的问题",
        "动机": "减少对大量像素级注释视频帧的依赖，提高视频显著目标检测的性能",
        "方法": "提出了一种有效的视频显著性检测器，包括空间细化网络和时空模块，并提出了一种从稀疏注释帧生成像素级伪标签的新方法",
        "关键词": [
            "视频显著性检测",
            "伪标签",
            "半监督学习"
        ],
        "涉及的技术概念": "空间细化网络用于提高显著性检测的空间精度，时空模块用于捕捉视频中的时间动态，光流用于提取运动信息，伪标签用于减少对大量手动注释的依赖。"
    },
    {
        "order": 930,
        "title": "No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_No-Frills_Human-Object_Interaction_Detection_Factorization_Layout_Encodings_and_Training_Techniques_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_No-Frills_Human-Object_Interaction_Detection_Factorization_Layout_Encodings_and_Training_Techniques_ICCV_2019_paper.html",
        "abstract": "We show that for human-object interaction detection a relatively simple factorized model with appearance and layout encodings constructed from pre-trained object detectors outperforms more sophisticated approaches. Our model includes factors for detection scores, human and object appearance, and coarse (box-pair configuration) and optionally fine-grained layout (human pose). We also develop training techniques that improve learning efficiency by: (1) eliminating a train-inference mismatch; (2) rejecting easy negatives during mini-batch training; and (3) using a ratio of negatives to positives that is two orders of magnitude larger than existing approaches. We conduct a thorough ablation study to understand the importance of different factors and training techniques using the challenging HICO-Det dataset.",
        "中文标题": "无修饰的人-物交互检测：因子分解、布局编码和训练技术",
        "摘要翻译": "我们展示了在人-物交互检测中，一个相对简单的因子分解模型，结合从预训练的对象检测器构建的外观和布局编码，能够超越更复杂的方法。我们的模型包括检测分数、人和对象的外观、以及粗略（框对配置）和可选的细粒度布局（人体姿态）的因子。我们还开发了训练技术，通过以下方式提高学习效率：（1）消除训练-推理不匹配；（2）在迷你批次训练中拒绝容易的负样本；（3）使用比现有方法大两个数量级的负样本与正样本比例。我们使用具有挑战性的HICO-Det数据集进行了彻底的消融研究，以了解不同因子和训练技术的重要性。",
        "领域": "人-物交互检测/深度学习/计算机视觉",
        "问题": "提高人-物交互检测的准确性和效率",
        "动机": "探索更简单但有效的方法来超越复杂的人-物交互检测方法",
        "方法": "采用因子分解模型结合外观和布局编码，并开发新的训练技术以提高学习效率",
        "关键词": [
            "人-物交互检测",
            "因子分解模型",
            "布局编码",
            "训练技术",
            "HICO-Det数据集"
        ],
        "涉及的技术概念": "因子分解模型通过将检测分数、人和对象的外观、以及布局（包括粗略的框对配置和细粒度的人体姿态）作为因子，来简化人-物交互检测。新的训练技术包括消除训练-推理不匹配、在迷你批次训练中拒绝容易的负样本、以及使用更大的负样本与正样本比例，以提高学习效率和检测准确性。"
    },
    {
        "order": 931,
        "title": "6-DOF GraspNet: Variational Grasp Generation for Object Manipulation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mousavian_6-DOF_GraspNet_Variational_Grasp_Generation_for_Object_Manipulation_ICCV_2019_paper.html",
        "abstract": "Generating grasp poses is a crucial component for any robot object manipulation task. In this work, we formulate the problem of grasp generation as sampling a set of grasps using a variational autoencoder and assess and refine the sampled grasps using a grasp evaluator model. Both Grasp Sampler and Grasp Refinement networks take 3D point clouds observed by a depth camera as input. We evaluate our approach in simulation and real-world robot experiments. Our approach achieves 88% success rate on various commonly used objects with diverse appearances, scales, and weights. Our model is trained purely in simulation and works in the real-world without any extra steps.",
        "中文标题": "6自由度抓取网络：用于物体操作的变分抓取生成",
        "摘要翻译": "生成抓取姿势是任何机器人物体操作任务的关键组成部分。在这项工作中，我们将抓取生成问题表述为使用变分自编码器采样一组抓取，并使用抓取评估模型评估和优化采样的抓取。抓取采样器和抓取优化网络都采用深度相机观察到的3D点云作为输入。我们在模拟和真实世界的机器人实验中评估了我们的方法。我们的方法在各种常用物体上实现了88%的成功率，这些物体具有不同的外观、尺寸和重量。我们的模型仅在模拟中训练，无需任何额外步骤即可在现实世界中工作。",
        "领域": "机器人操作/3D视觉/变分自编码器",
        "问题": "生成有效的机器人抓取姿势",
        "动机": "提高机器人对多样化物体的操作成功率",
        "方法": "使用变分自编码器采样抓取姿势，并通过抓取评估模型进行优化",
        "关键词": [
            "机器人操作",
            "3D点云",
            "变分自编码器",
            "抓取评估"
        ],
        "涉及的技术概念": "变分自编码器用于生成抓取姿势的采样，抓取评估模型用于评估和优化这些姿势，3D点云作为输入数据源。"
    },
    {
        "order": 932,
        "title": "Joint Learning of Semantic Alignment and Object Landmark Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html",
        "abstract": "Convolutional neural networks (CNNs) based approaches for semantic alignment and object landmark detection have improved their performance significantly. Current efforts for the two tasks focus on addressing the lack of massive training data through weakly- or unsupervised learning frameworks. In this paper, we present a joint learning approach for obtaining dense correspondences and discovering object landmarks from semantically similar images. Based on the key insight that the two tasks can mutually provide supervisions to each other, our networks accomplish this through a joint loss function that alternatively imposes a consistency constraint between the two tasks, thereby boosting the performance and addressing the lack of training data in a principled manner. To the best of our knowledge, this is the first attempt to address the lack of training data for the two tasks through the joint learning. To further improve the robustness of our framework, we introduce a probabilistic learning formulation that allows only reliable matches to be used in the joint learning process. With the proposed method, state-of-the-art performance is attained on several benchmarks for semantic matching and landmark detection.",
        "中文标题": "语义对齐与物体地标检测的联合学习",
        "摘要翻译": "基于卷积神经网络（CNNs）的语义对齐和物体地标检测方法已经显著提高了其性能。当前对于这两个任务的研究重点是通过弱监督或无监督学习框架来解决缺乏大量训练数据的问题。在本文中，我们提出了一种联合学习方法，用于从语义相似的图像中获取密集对应关系并发现物体地标。基于这两个任务可以相互提供监督的关键见解，我们的网络通过一个联合损失函数来实现这一点，该函数交替地对两个任务之间施加一致性约束，从而提升性能并以一种原则性的方式解决训练数据缺乏的问题。据我们所知，这是首次尝试通过联合学习来解决这两个任务训练数据缺乏的问题。为了进一步提高我们框架的鲁棒性，我们引入了一种概率学习公式，该公式只允许在联合学习过程中使用可靠的匹配。通过所提出的方法，在语义匹配和地标检测的几个基准测试中达到了最先进的性能。",
        "领域": "语义对齐/物体地标检测/联合学习",
        "问题": "解决语义对齐和物体地标检测任务中缺乏大量训练数据的问题",
        "动机": "通过联合学习框架，利用两个任务之间的相互监督，以原则性的方式解决训练数据缺乏的问题",
        "方法": "提出了一种联合学习方法，通过一个联合损失函数交替地对语义对齐和物体地标检测任务之间施加一致性约束，并引入概率学习公式以提高框架的鲁棒性",
        "关键词": [
            "语义对齐",
            "物体地标检测",
            "联合学习",
            "卷积神经网络",
            "弱监督学习",
            "无监督学习"
        ],
        "涉及的技术概念": "卷积神经网络（CNNs）用于语义对齐和物体地标检测，联合学习框架通过联合损失函数实现任务间的相互监督，概率学习公式用于提高学习过程的鲁棒性。"
    },
    {
        "order": 933,
        "title": "Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ye_Cap2Det_Learning_to_Amplify_Weak_Caption_Supervision_for_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Learning to localize and name object instances is a fundamental problem in vision, but state-of-the-art approaches rely on expensive bounding box supervision. While weakly supervised detection (WSOD) methods relax the need for boxes to that of image-level annotations, even cheaper supervision is naturally available in the form of unstructured textual descriptions that users may freely provide when uploading image content. However, straightforward approaches to using such data for WSOD wastefully discard captions that do not exactly match object names. Instead, we show how to squeeze the most information out of these captions by training a text-only classifier that generalizes beyond dataset boundaries. Our discovery provides an opportunity for learning detection models from noisy but more abundant and freely-available caption data. We also validate our model on three classic object detection benchmarks and achieve state-of-the-art WSOD performance. Our code is available at https://github.com/yekeren/Cap2Det.",
        "中文标题": "Cap2Det: 学习放大弱标注监督以进行目标检测",
        "摘要翻译": "学习定位和命名对象实例是视觉中的一个基本问题，但最先进的方法依赖于昂贵的边界框监督。虽然弱监督检测（WSOD）方法将框的需求放宽到图像级注释，但更便宜的监督自然以用户在上传图像内容时可能自由提供的非结构化文本描述的形式存在。然而，直接使用此类数据进行WSOD的方法会浪费那些不完全匹配对象名称的标注。相反，我们展示了如何通过训练一个仅文本的分类器来最大限度地利用这些标注，该分类器能够泛化超出数据集的边界。我们的发现为从噪声但更丰富且自由可用的标注数据中学习检测模型提供了机会。我们还在三个经典的目标检测基准上验证了我们的模型，并实现了最先进的WSOD性能。我们的代码可在https://github.com/yekeren/Cap2Det获取。",
        "领域": "目标检测/弱监督学习/文本分类",
        "问题": "如何利用非结构化文本描述进行目标检测，减少对昂贵边界框监督的依赖",
        "动机": "减少目标检测中对昂贵边界框监督的依赖，利用更便宜且自由可用的非结构化文本描述进行学习",
        "方法": "训练一个仅文本的分类器，该分类器能够泛化超出数据集的边界，从而最大限度地利用非结构化文本描述进行目标检测",
        "关键词": [
            "目标检测",
            "弱监督学习",
            "文本分类"
        ],
        "涉及的技术概念": "弱监督检测（WSOD）方法通过放宽对边界框的需求，转而使用图像级注释进行监督。本文提出了一种新的方法，通过训练一个仅文本的分类器来利用非结构化文本描述进行目标检测，这种方法能够泛化超出数据集的边界，从而有效地利用噪声但更丰富且自由可用的标注数据。"
    },
    {
        "order": 934,
        "title": "DAGMapper: Learning to Map by Discovering Lane Topology",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Homayounfar_DAGMapper_Learning_to_Map_by_Discovering_Lane_Topology_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Homayounfar_DAGMapper_Learning_to_Map_by_Discovering_Lane_Topology_ICCV_2019_paper.html",
        "abstract": "One of the fundamental challenges to scale self-driving is being able to create accurate high definition maps (HD maps) with low cost. Current attempts to automate this pro- cess typically focus on simple scenarios, estimate independent maps per frame or do not have the level of precision required by modern self driving vehicles. In contrast, in this paper we focus on drawing the lane boundaries of complex highways with many lanes that contain topology changes due to forks and merges. Towards this goal, we formulate the problem as inference in a directed acyclic graphical model (DAG), where the nodes of the graph encode geo- metric and topological properties of the local regions of the lane boundaries. Since we do not know a priori the topology of the lanes, we also infer the DAG topology (i.e., nodes and edges) for each region. We demonstrate the effectiveness of our approach on two major North American Highways in two different states and show high precision and recall as well as 89% correct topology.",
        "中文标题": "DAGMapper：通过发现车道拓扑学习映射",
        "摘要翻译": "扩展自动驾驶的一个基本挑战是能够以低成本创建精确的高清地图（HD地图）。当前自动化这一过程的尝试通常集中在简单场景上，每帧估计独立的地图，或者没有现代自动驾驶车辆所需的精度水平。相比之下，在本文中，我们专注于绘制包含由于分叉和合并而导致拓扑变化的复杂高速公路的车道边界。为了实现这一目标，我们将问题表述为在有向无环图模型（DAG）中的推理，其中图的节点编码车道边界局部区域的几何和拓扑属性。由于我们事先不知道车道的拓扑结构，我们还为每个区域推断DAG拓扑（即节点和边）。我们在两个不同州的北美主要高速公路上展示了我们方法的有效性，并展示了高精度和高召回率以及89%的正确拓扑。",
        "领域": "自动驾驶/地图构建/车道检测",
        "问题": "创建精确且成本效益高的高清地图（HD地图）",
        "动机": "为了扩展自动驾驶技术，需要一种能够高效且准确地创建高清地图的方法，特别是在复杂的高速公路环境中。",
        "方法": "将问题建模为在有向无环图（DAG）中的推理，其中图的节点表示车道边界的局部区域的几何和拓扑属性，并推断每个区域的DAG拓扑。",
        "关键词": [
            "自动驾驶",
            "高清地图",
            "车道检测",
            "有向无环图",
            "拓扑推断"
        ],
        "涉及的技术概念": "有向无环图（DAG）：一种图结构，用于表示具有方向且不形成环的节点间关系。在本研究中，DAG用于编码车道边界的几何和拓扑属性。拓扑推断：指在未知拓扑结构的情况下，通过分析数据来推断出节点和边的连接方式。"
    },
    {
        "order": 935,
        "title": "RainFlow: Optical Flow Under Rain Streaks and Rain Veiling Effect",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_RainFlow_Optical_Flow_Under_Rain_Streaks_and_Rain_Veiling_Effect_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_RainFlow_Optical_Flow_Under_Rain_Streaks_and_Rain_Veiling_Effect_ICCV_2019_paper.html",
        "abstract": "Optical flow in heavy rainy scenes is challenging due to the presence of both rain steaks and rain veiling effect, which break the existing optical flow constraints. Concerning this, we propose a deep-learning based optical flow method designed to handle heavy rain. We introduce a feature multiplier in our network that transforms the features of an image affected by the rain veiling effect into features that are less affected by it, which we call veiling-invariant features. We establish a new mapping operation in the feature space to produce streak-invariant features. The operation is based on a feature pyramid structure of the input images, and the basic idea is to preserve the chromatic features of the background scenes while canceling the rain-streak patterns. Both the veiling-invariant and streak-invariant features are computed and optimized automatically based on the the accuracy of our optical flow estimation. Our network is end-to-end, and handles both rain streaks and the veiling effect in an integrated framework. Extensive experiments show the effectiveness of our method, which outperforms the state of the art method and other baseline methods. We also show that our network can robustly maintain good performance on clean (no rain) images even though it is trained under rain image data.",
        "中文标题": "RainFlow: 雨条纹和雨幕效应下的光流",
        "摘要翻译": "在强雨场景中，由于雨条纹和雨幕效应的存在，光流估计变得具有挑战性，因为它们破坏了现有的光流约束。针对这一问题，我们提出了一种基于深度学习的光流方法，旨在处理强雨情况。我们在网络中引入了一个特征乘数，将受雨幕效应影响的图像特征转换为较少受其影响的特征，我们称之为抗雨幕特征。我们在特征空间中建立了一个新的映射操作，以产生抗条纹特征。该操作基于输入图像的特征金字塔结构，其基本思想是在消除雨条纹模式的同时保留背景场景的色度特征。抗雨幕和抗条纹特征都是基于我们光流估计的准确性自动计算和优化的。我们的网络是端到端的，并在一个集成框架中处理雨条纹和雨幕效应。大量实验证明了我们方法的有效性，它优于最先进的方法和其他基线方法。我们还展示了我们的网络即使在雨图像数据下训练，也能在干净（无雨）图像上稳健地保持良好的性能。",
        "领域": "光流估计/雨天图像处理/深度学习应用",
        "问题": "在强雨场景中准确估计光流",
        "动机": "强雨场景中的雨条纹和雨幕效应破坏了现有的光流约束，使得光流估计变得困难",
        "方法": "提出了一种基于深度学习的光流方法，通过引入特征乘数和建立新的特征空间映射操作来处理雨条纹和雨幕效应",
        "关键词": [
            "光流估计",
            "雨天图像处理",
            "深度学习"
        ],
        "涉及的技术概念": "特征乘数用于转换受雨幕效应影响的图像特征，特征空间中的映射操作基于输入图像的特征金字塔结构，旨在消除雨条纹模式的同时保留背景场景的色度特征。"
    },
    {
        "order": 936,
        "title": "No Fear of the Dark: Image Retrieval Under Varying Illumination Conditions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jenicek_No_Fear_of_the_Dark_Image_Retrieval_Under_Varying_Illumination_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jenicek_No_Fear_of_the_Dark_Image_Retrieval_Under_Varying_Illumination_ICCV_2019_paper.html",
        "abstract": "Image retrieval under varying illumination conditions, such as day and night images, is addressed by image preprocessing, both hand-crafted and learned. Prior to extracting image descriptors by a convolutional neural network, images are photometrically normalised in order to reduce the descriptor sensitivity to illumination changes. We propose a learnable normalisation based on the U-Net architecture, which is trained on a combination of single-camera multi-exposure images and a newly constructed collection of similar views of landmarks during day and night. We experimentally show that both hand-crafted normalisation based on local histogram equalisation and the learnable normalisation outperform standard approaches in varying illumination conditions, while staying on par with the state-of-the-art methods on daylight illumination benchmarks, such as Oxford or Paris datasets.",
        "中文标题": "无惧黑暗：变化光照条件下的图像检索",
        "摘要翻译": "针对变化光照条件下的图像检索问题，如日夜图像，本文通过图像预处理方法，包括手工制作和学习的，来解决。在通过卷积神经网络提取图像描述符之前，图像会进行光度归一化处理，以减少描述符对光照变化的敏感性。我们提出了一种基于U-Net架构的可学习归一化方法，该方法在单相机多曝光图像和新构建的日夜地标相似视图集合上进行训练。实验表明，基于局部直方图均衡化的手工归一化和可学习归一化在变化光照条件下均优于标准方法，同时在日光光照基准测试（如牛津或巴黎数据集）上与最先进的方法保持同等水平。",
        "领域": "图像检索/光度归一化/卷积神经网络",
        "问题": "解决在变化光照条件下（如日夜交替）图像检索的准确性问题",
        "动机": "提高图像检索系统在不同光照条件下的鲁棒性和准确性",
        "方法": "提出了一种基于U-Net架构的可学习归一化方法，结合手工归一化和可学习归一化，对图像进行光度归一化处理，以减少描述符对光照变化的敏感性",
        "关键词": [
            "图像检索",
            "光度归一化",
            "U-Net架构",
            "卷积神经网络",
            "局部直方图均衡化"
        ],
        "涉及的技术概念": "图像预处理、光度归一化、卷积神经网络、U-Net架构、局部直方图均衡化、图像描述符、单相机多曝光图像、日夜地标相似视图集合"
    },
    {
        "order": 937,
        "title": "3D-LaneNet: End-to-End 3D Multiple Lane Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Garnett_3D-LaneNet_End-to-End_3D_Multiple_Lane_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Garnett_3D-LaneNet_End-to-End_3D_Multiple_Lane_Detection_ICCV_2019_paper.html",
        "abstract": "We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art.",
        "中文标题": "3D-LaneNet: 端到端的3D多车道检测",
        "摘要翻译": "我们介绍了一种网络，该网络直接从单张图像预测道路场景中的车道3D布局。这项工作标志着首次尝试在车载传感中解决这一任务，而不假设已知的恒定车道宽度或依赖预先映射的环境。我们的网络架构，3D-LaneNet，应用了两个新概念：网络内逆透视映射（IPM）和基于锚点的车道表示。网络内IPM投影促进了在常规图像视图和俯视图中的双表示信息流。每列锚点输出表示使我们的端到端方法取代了常见的启发式方法，如聚类和异常值拒绝，将车道估计视为对象检测问题。此外，我们的方法明确处理了复杂情况，如车道合并和分裂。结果在两个新的3D车道数据集上展示，一个是合成的，一个是真实的。为了与现有方法进行比较，我们在仅图像的tuSimple车道检测基准上测试了我们的方法，实现了与最先进技术竞争的性能。",
        "领域": "自动驾驶/车道检测/3D场景理解",
        "问题": "从单张图像直接预测道路场景中的车道3D布局",
        "动机": "在车载传感中解决车道检测任务，不假设已知的恒定车道宽度或依赖预先映射的环境",
        "方法": "应用网络内逆透视映射（IPM）和基于锚点的车道表示，实现端到端的车道检测",
        "关键词": [
            "3D车道检测",
            "逆透视映射",
            "基于锚点的表示"
        ],
        "涉及的技术概念": "网络内逆透视映射（IPM）是一种技术，用于在图像处理和计算机视觉中，将图像从透视视图转换为俯视图，以便于分析和理解场景。基于锚点的车道表示是一种方法，通过在图像中预定义的位置（锚点）来检测和表示车道，这种方法可以提高检测的准确性和效率。"
    },
    {
        "order": 938,
        "title": "GridDehazeNet: Attention-Based Multi-Scale Network for Image Dehazing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_GridDehazeNet_Attention-Based_Multi-Scale_Network_for_Image_Dehazing_ICCV_2019_paper.html",
        "abstract": "We propose an end-to-end trainable Convolutional Neural Network (CNN), named GridDehazeNet, for single image dehazing. The GridDehazeNet consists of three modules: pre-processing, backbone, and post-processing. The trainable pre-processing module can generate learned inputs with better diversity and more pertinent features as compared to those derived inputs produced by hand-selected pre-processing methods. The backbone module implements a novel attention-based multi-scale estimation on a grid network, which can effectively alleviate the bottleneck issue often encountered in the conventional multi-scale approach. The post-processing module helps to reduce the artifacts in the final output. Experimental results indicate that the GridDehazeNet outperforms the state-of-the-arts on both synthetic and real-world images. The proposed hazing method does not rely on the atmosphere scattering model, and we provide an explanation as to why it is not necessarily beneficial to take advantage of the dimension reduction offered by the atmosphere scattering model for image dehazing, even if only the dehazing results on synthetic images are concerned.",
        "中文标题": "GridDehazeNet: 基于注意力的多尺度网络用于图像去雾",
        "摘要翻译": "我们提出了一种端到端可训练的卷积神经网络（CNN），名为GridDehazeNet，用于单幅图像去雾。GridDehazeNet由三个模块组成：预处理、骨干和后处理。可训练的预处理模块可以生成具有更好多样性和更相关特征的学习输入，与手工选择的预处理方法产生的输入相比。骨干模块在网格网络上实现了一种新颖的基于注意力的多尺度估计，可以有效缓解传统多尺度方法中经常遇到的瓶颈问题。后处理模块有助于减少最终输出中的伪影。实验结果表明，GridDehazeNet在合成图像和真实世界图像上均优于现有技术。所提出的去雾方法不依赖于大气散射模型，我们提供了一个解释，说明为什么即使只关注合成图像上的去雾结果，利用大气散射模型提供的降维也不一定有益。",
        "领域": "图像去雾/卷积神经网络/注意力机制",
        "问题": "单幅图像去雾",
        "动机": "解决传统多尺度方法中的瓶颈问题，提高去雾效果",
        "方法": "提出了一种端到端可训练的卷积神经网络GridDehazeNet，包括预处理、基于注意力的多尺度估计的骨干模块和后处理模块",
        "关键词": [
            "图像去雾",
            "卷积神经网络",
            "注意力机制",
            "多尺度估计"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "注意力机制": "一种让模型能够关注输入数据中重要部分的技术",
            "多尺度估计": "在不同尺度上分析图像特征，以捕捉更丰富的信息",
            "大气散射模型": "传统图像去雾方法中常用的一种物理模型，用于描述雾霾对图像的影响"
        }
    },
    {
        "order": 939,
        "title": "Hierarchical Shot Detector",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cao_Hierarchical_Shot_Detector_ICCV_2019_paper.html",
        "abstract": "Single shot detector simultaneously predicts object categories and regression offsets of the default boxes. Despite of high efficiency, this structure has some inappropriate designs: (1) The classification result of the default box is improperly assigned to that of the regressed box during inference, (2) Only regression once is not good enough for accurate object detection. To solve the first problem, a novel reg-offset-cls (ROC) module is proposed. It contains three hierarchical steps: box regression, the feature sampling location predication, and the regressed box classification with the features of offset locations. To further solve the second problem, a hierarchical shot detector (HSD) is proposed, which stacks two ROC modules and one feature enhanced module. The second ROC treats the regressed boxes and the feature sampling locations of features in the first ROC as the inputs. Meanwhile, the feature enhanced module injected between two ROCs aims to extract the local and non-local context. Experiments on the MS COCO and PASCAL VOC datasets demonstrate the superiority of proposed HSD. Without the bells or whistles, HSD outperforms all one-stage methods at real-time speed.",
        "中文标题": "层次化单次检测器",
        "摘要翻译": "单次检测器同时预测默认框的对象类别和回归偏移。尽管效率高，但这种结构存在一些不恰当的设计：（1）在推理过程中，默认框的分类结果被不适当地分配给回归框的分类结果，（2）仅回归一次对于准确的对象检测来说是不够的。为了解决第一个问题，提出了一种新颖的回归偏移分类（ROC）模块。它包含三个层次化步骤：框回归、特征采样位置预测和偏移位置特征的回归框分类。为了进一步解决第二个问题，提出了层次化单次检测器（HSD），它堆叠了两个ROC模块和一个特征增强模块。第二个ROC将第一个ROC中的回归框和特征采样位置作为输入。同时，插入在两个ROC之间的特征增强模块旨在提取局部和非局部上下文。在MS COCO和PASCAL VOC数据集上的实验证明了所提出的HSD的优越性。在没有花哨技巧的情况下，HSD在实时速度下优于所有一阶段方法。",
        "领域": "对象检测/特征提取/上下文理解",
        "问题": "单次检测器在推理过程中默认框分类结果分配不当和仅回归一次不足以进行准确的对象检测",
        "动机": "提高对象检测的准确性和效率",
        "方法": "提出了一种新颖的回归偏移分类（ROC）模块和层次化单次检测器（HSD），通过层次化步骤和特征增强模块来提高检测准确性",
        "关键词": [
            "对象检测",
            "回归偏移分类",
            "特征增强"
        ],
        "涉及的技术概念": "单次检测器是一种同时预测对象类别和回归偏移的方法，回归偏移分类（ROC）模块通过框回归、特征采样位置预测和回归框分类三个步骤来提高检测准确性，层次化单次检测器（HSD）通过堆叠ROC模块和特征增强模块来进一步提高检测性能。"
    },
    {
        "order": 940,
        "title": "Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Once_a_MAN_Towards_Multi-Target_Attack_via_Learning_Multi-Target_Adversarial_ICCV_2019_paper.html",
        "abstract": "Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods.",
        "中文标题": "一次MAN：通过学习多目标对抗网络实现多目标攻击",
        "摘要翻译": "现代深度神经网络往往容易受到对抗样本的攻击。基于第一种基于优化的攻击方法，许多后续方法被提出来以提高攻击性能和速度。最近，基于生成的方法受到了广泛关注，因为它们直接使用前馈网络生成对抗样本，从而避免了基于优化和基于梯度的方法中耗时的迭代攻击过程。然而，当前的基于生成的方法只能攻击一个模型中的一个特定目标（类别），这使得它们不适用于通常有数百/数千个类别的真实分类系统。在本文中，我们提出了第一个多目标对抗网络（MAN），它可以使用单一模型生成多目标对抗样本。通过将指定的类别信息整合到中间特征中，它可以在运行时攻击目标分类模型的任何类别。实验表明，所提出的MAN在多目标攻击任务和单目标攻击任务中都能产生更强的攻击结果，并且比之前的最先进方法具有更好的可转移性。我们进一步使用我们的MAN生成的对抗样本来提高分类模型的鲁棒性。当受到各种方法的攻击时，它也能比其他方法实现更好的分类准确率。",
        "领域": "对抗样本生成/模型鲁棒性/分类系统",
        "问题": "当前基于生成的方法只能攻击一个模型中的一个特定目标，不适用于有数百/数千个类别的真实分类系统。",
        "动机": "提高对抗样本的攻击性能和速度，同时增强分类模型的鲁棒性。",
        "方法": "提出了第一个多目标对抗网络（MAN），通过将指定的类别信息整合到中间特征中，使用单一模型生成多目标对抗样本。",
        "关键词": [
            "对抗样本",
            "多目标攻击",
            "模型鲁棒性",
            "分类准确率"
        ],
        "涉及的技术概念": "对抗样本是指那些经过特殊设计，能够欺骗深度神经网络的输入样本。多目标攻击指的是能够同时针对多个目标类别生成对抗样本的攻击方法。模型鲁棒性指的是模型在面对各种攻击或异常输入时，仍能保持其性能的能力。分类准确率是衡量分类模型性能的一个重要指标，表示模型正确分类样本的比例。"
    },
    {
        "order": 941,
        "title": "Learning to See Moving Objects in the Dark",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Learning_to_See_Moving_Objects_in_the_Dark_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Learning_to_See_Moving_Objects_in_the_Dark_ICCV_2019_paper.html",
        "abstract": "Video surveillance systems have wide range of utilities, yet easily suffer from great quality degeneration under dim light circumstances. Industrial solutions mainly use extra near-infrared illuminations, even though it doesn't preserve color and texture information. A variety of researches enhanced low-light videos shot by visible light cameras, while they either relied on task specific preconditions or trained with synthetic datasets. We propose a novel optical system to capture bright and dark videos of the exact same scenes, generating training and groud truth pairs for authentic low-light video dataset. A fully convolutional network with 3D and 2D miscellaneous operations is utilized to learn an enhancement mapping with proper spatial-temporal transformation from raw camera sensor data to bright RGB videos. Experiments show promising results by our method, and it outperforms state-of-the-art low-light image/video enhancement algorithms.",
        "中文标题": "学习在黑暗中看到移动物体",
        "摘要翻译": "视频监控系统具有广泛的实用性，但在昏暗的光线条件下容易遭受严重的质量退化。工业解决方案主要使用额外的近红外照明，尽管它不保留颜色和纹理信息。各种研究增强了由可见光相机拍摄的低光视频，但它们要么依赖于特定任务的前提条件，要么使用合成数据集进行训练。我们提出了一种新颖的光学系统，用于捕捉完全相同场景的明亮和黑暗视频，为真实的低光视频数据集生成训练和地面真实对。利用具有3D和2D杂项操作的完全卷积网络，从原始相机传感器数据到明亮的RGB视频学习具有适当时空变换的增强映射。实验显示我们的方法取得了有希望的结果，并且它优于最先进的低光图像/视频增强算法。",
        "领域": "低光视频增强/视频监控/光学系统",
        "问题": "在昏暗光线条件下视频监控系统的质量退化问题",
        "动机": "提高在低光环境下视频监控系统的性能，保留颜色和纹理信息",
        "方法": "提出一种新颖的光学系统捕捉明亮和黑暗视频，使用完全卷积网络学习从原始相机传感器数据到明亮RGB视频的增强映射",
        "关键词": [
            "低光视频增强",
            "视频监控",
            "光学系统",
            "完全卷积网络",
            "时空变换"
        ],
        "涉及的技术概念": "完全卷积网络（FCN）是一种深度学习模型，用于图像和视频处理，能够处理任意大小的输入并输出相应大小的结果。3D和2D杂项操作指的是在视频处理中同时考虑时间和空间维度的操作，以及仅考虑空间维度的操作。时空变换是指在视频处理中同时考虑时间和空间的变化，以捕捉视频中的动态信息。"
    },
    {
        "order": 942,
        "title": "Few-Shot Learning With Global Class Representations",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Few-Shot_Learning_With_Global_Class_Representations_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Few-Shot_Learning_With_Global_Class_Representations_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose to tackle the challenging few-shot learning (FSL) problem by learning global class representations using both base and novel class training samples. In each training episode, an episodic class mean computed from a support set is registered with the global representation via a registration module. This produces a registered global class representation for computing the classification loss using a query set. Though following a similar episodic training pipeline as existing meta learning based approaches, our method differs significantly in that novel class training samples are involved in the training from the beginning. To compensate for the lack of novel class training samples, an effective sample synthesis strategy is developed to avoid overfitting. Importantly, by joint base-novel class training, our approach can be easily extended to a more practical yet challenging FSL setting, i.e., generalized FSL, where the label space of test data is extended to both base and novel classes. Extensive experiments show that our approach is effective for both of the two FSL settings.",
        "中文标题": "使用全局类表示的少样本学习",
        "摘要翻译": "在本文中，我们提出通过使用基础和新型类训练样本来学习全局类表示，以解决具有挑战性的少样本学习（FSL）问题。在每个训练周期中，从支持集计算出的周期性类均值通过注册模块与全局表示进行注册。这产生了一个注册的全局类表示，用于使用查询集计算分类损失。尽管遵循与现有基于元学习的方法相似的周期性训练流程，我们的方法在新型类训练样本从一开始就参与训练这一点上显著不同。为了弥补新型类训练样本的不足，开发了一种有效的样本合成策略以避免过拟合。重要的是，通过联合基础-新型类训练，我们的方法可以轻松扩展到更实用但更具挑战性的FSL设置，即广义FSL，其中测试数据的标签空间扩展到基础和新型类。大量实验表明，我们的方法对这两种FSL设置都是有效的。",
        "领域": "少样本学习/元学习/图像分类",
        "问题": "解决少样本学习中的分类问题，特别是在新型类训练样本有限的情况下。",
        "动机": "提高少样本学习模型在新型类上的泛化能力，避免过拟合，并扩展到广义FSL设置。",
        "方法": "通过联合基础-新型类训练和样本合成策略，学习全局类表示，以改进少样本学习的分类性能。",
        "关键词": [
            "少样本学习",
            "全局类表示",
            "样本合成",
            "广义FSL"
        ],
        "涉及的技术概念": "少样本学习（FSL）是一种在只有少量样本的情况下学习新类别的技术。全局类表示指的是在整个训练过程中维护的类别的统一表示。样本合成策略是为了增加训练样本的多样性，以避免模型过拟合。广义FSL是指测试数据的标签空间不仅包括新型类，还包括基础类。"
    },
    {
        "order": 943,
        "title": "SegSort: Segmentation by Discriminative Sorting of Segments",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.html",
        "abstract": "Almost all existing deep learning approaches for semantic segmentation tackle this task as a pixel-wise classification problem. Yet humans understand a scene not in terms of pixels, but by decomposing it into perceptual groups and structures that are the basic building blocks of recognition. This motivates us to propose an end-to-end pixel-wise metric learning approach that mimics this process. In our approach, the optimal visual representation determines the right segmentation within individual images and associates segments with the same semantic classes across images. The core visual learning problem is therefore to maximize the similarity within segments and minimize the similarity between segments. Given a model trained this way, inference is performed consistently by extracting pixel-wise embeddings and clustering, with the semantic label determined by the majority vote of its nearest neighbors from an annotated set. As a result, we present the SegSort, as a first attempt using deep learning for unsupervised semantic segmentation, achieving 76% performance of its supervised counterpart. When supervision is available, SegSort shows consistent improvements over conventional approaches based on pixel-wise softmax training. Additionally, our approach produces more precise boundaries and consistent region predictions. The proposed SegSort further produces an interpretable result, as each choice of label can be easily understood from the retrieved nearest segments.",
        "中文标题": "SegSort: 通过区分性排序片段进行分割",
        "摘要翻译": "几乎所有现有的深度学习语义分割方法都将此任务视为像素级分类问题。然而，人类理解场景不是通过像素，而是通过将其分解为感知组和结构，这些是识别的基本构建块。这激励我们提出一种端到端的像素级度量学习方法，模仿这一过程。在我们的方法中，最优视觉表示确定单个图像内的正确分割，并将具有相同语义类别的片段跨图像关联。因此，核心视觉学习问题是最大化片段内的相似性并最小化片段间的相似性。给定这样训练的模型，通过提取像素级嵌入和聚类一致地执行推理，语义标签由来自注释集的最近邻的多数投票确定。因此，我们提出了SegSort，作为使用深度学习进行无监督语义分割的首次尝试，达到了其监督对应物的76%性能。当有监督时，SegSort显示出基于像素级softmax训练的常规方法的持续改进。此外，我们的方法产生更精确的边界和一致的区域预测。所提出的SegSort进一步产生可解释的结果，因为每个标签的选择都可以从检索到的最近片段中轻松理解。",
        "领域": "语义分割/度量学习/无监督学习",
        "问题": "如何通过模仿人类理解场景的方式，即通过分解为感知组和结构，来进行语义分割",
        "动机": "人类理解场景的方式不是通过像素，而是通过分解为感知组和结构，这激励我们提出一种模仿这一过程的端到端像素级度量学习方法",
        "方法": "提出了一种端到端的像素级度量学习方法，通过最大化片段内的相似性并最小化片段间的相似性来确定最优视觉表示，并通过提取像素级嵌入和聚类进行推理",
        "关键词": [
            "语义分割",
            "度量学习",
            "无监督学习"
        ],
        "涉及的技术概念": "像素级分类问题、端到端像素级度量学习、最优视觉表示、像素级嵌入、聚类、语义标签、最近邻多数投票、无监督语义分割、像素级softmax训练"
    },
    {
        "order": 944,
        "title": "Attention Bridging Network for Knowledge Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.html",
        "abstract": "The attention of a deep neural network obtained by back-propagating gradients can effectively explain the decision of the network. They can further be used to explicitly access to the network response to a specific pattern. Considering objects of the same category but from different domains share similar visual patterns, we propose to treat the network attention as a bridge to connect objects across domains. In this paper, we use knowledge from the source domain to guide the network's response to categories shared with the target domain. With weights sharing and domain adversary training, this knowledge can be successfully transferred by regularizing the network's response to the same category in the target domain. Specifically, we transfer the foreground prior from a simple single-label dataset to another complex multi-label dataset, leading to improvement of attention maps. Experiments about the weakly-supervised semantic segmentation task show the effectiveness of our method. Besides, we further explore and validate that the proposed method is able to improve the generalization ability of a classification network in domain adaptation and domain generalization settings.",
        "中文标题": "注意力桥接网络用于知识转移",
        "摘要翻译": "通过反向传播梯度获得的深度神经网络的注意力可以有效地解释网络的决策。它们还可以进一步用于明确访问网络对特定模式的响应。考虑到来自不同领域但属于同一类别的对象共享相似的视觉模式，我们提出将网络注意力视为连接跨领域对象的桥梁。在本文中，我们使用源领域的知识来指导网络对与目标领域共享类别的响应。通过权重共享和领域对抗训练，这种知识可以通过规范网络对目标领域中同一类别的响应而成功转移。具体来说，我们将前景先验从一个简单的单标签数据集转移到另一个复杂的多标签数据集，从而提高了注意力图的质量。关于弱监督语义分割任务的实验显示了我们方法的有效性。此外，我们进一步探索并验证了所提出的方法能够提高分类网络在领域适应和领域泛化设置中的泛化能力。",
        "领域": "语义分割/领域适应/领域泛化",
        "问题": "如何有效地将知识从一个领域转移到另一个领域，以提高注意力图的质量和分类网络的泛化能力",
        "动机": "考虑到来自不同领域但属于同一类别的对象共享相似的视觉模式，利用网络注意力作为桥梁连接跨领域对象，以提高模型的性能和泛化能力",
        "方法": "使用源领域的知识指导网络对与目标领域共享类别的响应，通过权重共享和领域对抗训练实现知识的成功转移，并提高注意力图的质量",
        "关键词": [
            "注意力机制",
            "知识转移",
            "领域对抗训练",
            "语义分割",
            "领域适应",
            "领域泛化"
        ],
        "涉及的技术概念": "深度神经网络的注意力机制、反向传播梯度、权重共享、领域对抗训练、弱监督语义分割、领域适应、领域泛化"
    },
    {
        "order": 945,
        "title": "Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution for Small Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Noh_Better_to_Follow_Follow_to_Be_Better_Towards_Precise_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Noh_Better_to_Follow_Follow_to_Be_Better_Towards_Precise_Supervision_ICCV_2019_paper.html",
        "abstract": "In spite of recent success of proposal-based CNN models for object detection, it is still difficult to detect small objects due to the limited and distorted information that small region of interests (RoI) contain. One way to alleviate this issue is to enhance the features of small RoIs using a super-resolution (SR) technique. We investigate how to improve feature-level super-resolution especially for small object detection, and discover its performance can be significantly improved by (i) utilizing proper high-resolution target features as supervision signals for training of a SR model and (ii) matching the relative receptive fields of training pairs of input low-resolution features and target high-resolution features. We propose a novel feature-level super-resolution approach that not only correctly addresses these two desiderata but also is integrable with any proposal-based detectors with feature pooling. In our experiments, our approach significantly improves the performance of Faster R-CNN on three benchmarks of Tsinghua-Tencent 100K, PASCAL VOC and MS COCO. The improvement for small objects is remarkably large, and encouragingly, those for medium and large objects are nontrivial too. As a result, we achieve new state-of-the-art performance on Tsinghua-Tencent 100K and highly competitive results on both PASCAL VOC and MS COCO.",
        "中文标题": "更好追随，追随更好：面向小目标检测的特征超分辨率精确监督",
        "摘要翻译": "尽管基于提议的CNN模型在目标检测方面取得了近期的成功，但由于小感兴趣区域（RoI）包含的信息有限且失真，检测小目标仍然困难。缓解这一问题的一种方法是使用超分辨率（SR）技术增强小RoI的特征。我们研究了如何改进特征级超分辨率，特别是针对小目标检测，并发现通过（i）利用适当的高分辨率目标特征作为SR模型训练的监督信号和（ii）匹配输入低分辨率特征和目标高分辨率特征的训练对的相对感受野，其性能可以显著提高。我们提出了一种新颖的特征级超分辨率方法，不仅正确解决了这两个需求，而且可以与任何具有特征池化的基于提议的检测器集成。在我们的实验中，我们的方法显著提高了Faster R-CNN在清华-腾讯100K、PASCAL VOC和MS COCO三个基准上的性能。对小目标的改进尤为显著，令人鼓舞的是，对中等和大目标的改进也非常显著。因此，我们在清华-腾讯100K上实现了新的最先进性能，并在PASCAL VOC和MS COCO上都取得了极具竞争力的结果。",
        "领域": "目标检测/超分辨率/特征增强",
        "问题": "小目标检测中的信息有限和失真问题",
        "动机": "提高小目标检测的性能，通过增强小感兴趣区域的特征",
        "方法": "提出了一种新颖的特征级超分辨率方法，利用高分辨率目标特征作为监督信号，并匹配输入低分辨率特征和目标高分辨率特征的相对感受野",
        "关键词": [
            "小目标检测",
            "超分辨率",
            "特征增强"
        ],
        "涉及的技术概念": "超分辨率（SR）技术用于增强小感兴趣区域（RoI）的特征，通过利用高分辨率目标特征作为监督信号，并匹配输入低分辨率特征和目标高分辨率特征的相对感受野，以提高小目标检测的性能。"
    },
    {
        "order": 946,
        "title": "Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Recover_and_Identify_A_Generative_Dual_Model_for_Cross-Resolution_Person_ICCV_2019_paper.html",
        "abstract": "Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.",
        "中文标题": "恢复与识别：一种用于跨分辨率行人重识别的生成对抗双模型",
        "摘要翻译": "行人重识别（re-ID）旨在跨摄像头视角匹配同一身份的图像。由于摄像头与感兴趣行人之间的距离不同，可能会出现分辨率不匹配的情况，这在实际场景中会降低行人重识别的性能。为了解决这个问题，我们提出了一种新颖的生成对抗网络来处理跨分辨率行人重识别，允许查询图像具有不同的分辨率。通过推进对抗学习技术，我们提出的模型学习分辨率不变的图像表示，同时能够恢复低分辨率输入图像中缺失的细节。由于保持了分辨率不变性并恢复了面向重识别的判别细节，所得到的特征可以共同应用于提高行人重识别的性能。我们在五个基准数据集上的实验证实了我们方法的有效性及其相对于最先进方法的优越性，特别是在训练期间未见过的输入分辨率情况下。",
        "领域": "行人重识别/生成对抗网络/图像恢复",
        "问题": "解决跨分辨率行人重识别中的分辨率不匹配问题",
        "动机": "由于摄像头与行人之间的距离不同，导致分辨率不匹配，影响行人重识别的性能",
        "方法": "提出一种新颖的生成对抗网络，通过对抗学习技术学习分辨率不变的图像表示，并恢复低分辨率图像中的缺失细节",
        "关键词": [
            "行人重识别",
            "生成对抗网络",
            "图像恢复",
            "分辨率不变性",
            "判别细节"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）是一种深度学习模型，通过对抗过程学习生成数据。在本文中，GAN被用来学习分辨率不变的图像表示，并恢复低分辨率图像中的细节，以提高行人重识别的性能。分辨率不变性指的是模型能够处理不同分辨率的输入图像，而判别细节是指那些有助于区分不同行人身份的细节特征。"
    },
    {
        "order": 947,
        "title": "Weakly Supervised Object Detection With Segmentation Collaboration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Weakly_Supervised_Object_Detection_With_Segmentation_Collaboration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Weakly_Supervised_Object_Detection_With_Segmentation_Collaboration_ICCV_2019_paper.html",
        "abstract": "Weakly supervised object detection aims at learning precise object detectors, given image category labels. In recent prevailing works, this problem is generally formulated as a multiple instance learning module guided by an image classification loss. The object bounding box is assumed to be the one contributing most to the classification among all proposals. However, the region contributing most is also likely to be a crucial part or the supporting context of an object. To obtain a more accurate detector, in this work we propose a novel end-to-end weakly supervised detection approach, where a newly introduced generative adversarial segmentation module interacts with the conventional detection module in a collaborative loop. The collaboration mechanism takes full advantages of the complementary interpretations of the weakly supervised localization task, namely detection and segmentation tasks, forming a more comprehensive solution. Consequently, our method obtains more precise object bounding boxes, rather than parts or irrelevant surroundings. Expectedly, the proposed method achieves an accuracy of 53.7% on the PASCAL VOC 2007 dataset, outperforming the state-of-the-arts and demonstrating its superiority for weakly supervised object detection.",
        "中文标题": "弱监督目标检测与分割协作",
        "摘要翻译": "弱监督目标检测旨在给定图像类别标签的情况下学习精确的目标检测器。在最近的主流工作中，这个问题通常被表述为由图像分类损失指导的多实例学习模块。假设目标边界框是所有提案中对分类贡献最大的一个。然而，贡献最大的区域也可能是目标的关键部分或支持上下文。为了获得更准确的检测器，在这项工作中，我们提出了一种新颖的端到端弱监督检测方法，其中新引入的生成对抗分割模块与传统检测模块在协作循环中相互作用。协作机制充分利用了弱监督定位任务的互补解释，即检测和分割任务，形成了更全面的解决方案。因此，我们的方法获得了更精确的目标边界框，而不是部分或无关的周围环境。预期地，所提出的方法在PASCAL VOC 2007数据集上达到了53.7%的准确率，优于现有技术，并展示了其在弱监督目标检测中的优越性。",
        "领域": "目标检测/图像分割/生成对抗网络",
        "问题": "在仅给定图像类别标签的情况下，如何学习精确的目标检测器",
        "动机": "现有的弱监督目标检测方法通常假设贡献最大的区域是目标边界框，但这可能是目标的关键部分或支持上下文，导致检测不准确",
        "方法": "提出了一种新颖的端到端弱监督检测方法，通过引入生成对抗分割模块与传统检测模块协作，利用检测和分割任务的互补性，形成更全面的解决方案",
        "关键词": [
            "弱监督学习",
            "目标检测",
            "图像分割",
            "生成对抗网络"
        ],
        "涉及的技术概念": "多实例学习模块、图像分类损失、生成对抗分割模块、协作循环、PASCAL VOC 2007数据集"
    },
    {
        "order": 948,
        "title": "What Synthesis Is Missing: Depth Adaptation Integrated With Weak Supervision for Indoor Scene Parsing",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_What_Synthesis_Is_Missing_Depth_Adaptation_Integrated_With_Weak_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_What_Synthesis_Is_Missing_Depth_Adaptation_Integrated_With_Weak_Supervision_ICCV_2019_paper.html",
        "abstract": "Scene Parsing is a crucial step to enable autonomous systems to understand and interact with their surroundings. Supervised deep learning methods have made great progress in solving scene parsing problems, however, come at the cost of laborious manual pixel-level annotation. Synthetic data as well as weak supervision have been investigated to alleviate this effort. Nonetheless, synthetically generated data still suffers from severe domain shift while weak labels often lack precision. Moreover, most existing works for weakly supervised scene parsing are limited to salient foreground objects. The aim of this work is hence twofold: Exploit synthetic data where feasible and integrate weak supervision where necessary. More concretely, we address this goal by utilizing depth as transfer domain because its synthetic-to-real discrepancy is much lower than for color. At the same time, we perform weak localization from easily obtainable image level labels and integrate both using a novel contour-based scheme. Our approach is implemented as a teacher-student learning framework to solve the transfer learning problem by generating a pseudo ground truth. Using only depth-based adaptation, this approach already outperforms previous transfer learning approaches on the popular indoor scene parsing SUN RGB-D dataset. Our proposed two-stage integration more than halves the gap towards fully supervised methods when compared to previous state-of-the-art in transfer learning.",
        "中文标题": "合成数据缺少什么：深度适应与弱监督相结合用于室内场景解析",
        "摘要翻译": "场景解析是使自主系统能够理解并与其周围环境互动的关键步骤。监督深度学习方法在解决场景解析问题上取得了巨大进展，但这是以繁琐的手动像素级注释为代价的。为了减轻这一负担，已经研究了合成数据以及弱监督。然而，合成生成的数据仍然遭受严重的领域转移，而弱标签往往缺乏精确性。此外，大多数现有的弱监督场景解析工作仅限于显著的前景对象。因此，这项工作的目标有两个：在可行的情况下利用合成数据，在必要时整合弱监督。更具体地说，我们通过利用深度作为转移领域来实现这一目标，因为其合成到真实的差异远低于颜色。同时，我们从易于获得的图像级别标签执行弱定位，并使用一种新颖的基于轮廓的方案将两者整合。我们的方法被实现为一个教师-学生学习框架，通过生成伪地面实况来解决转移学习问题。仅使用基于深度的适应，这种方法在流行的室内场景解析SUN RGB-D数据集上已经优于以前的转移学习方法。与转移学习中的先前最先进技术相比，我们提出的两阶段整合将差距缩小了一半以上，接近完全监督方法。",
        "领域": "场景解析/深度适应/弱监督学习",
        "问题": "解决室内场景解析中合成数据与真实数据之间的领域转移问题以及弱标签的精确性问题",
        "动机": "减少场景解析中对手动像素级注释的依赖，同时提高解析的准确性和效率",
        "方法": "利用深度作为转移领域，结合弱监督和合成数据，采用基于轮廓的方案和教师-学生学习框架进行两阶段整合",
        "关键词": [
            "场景解析",
            "深度适应",
            "弱监督学习",
            "合成数据",
            "转移学习"
        ],
        "涉及的技术概念": "深度适应指的是利用深度信息来减少合成数据与真实数据之间的差异；弱监督学习指的是使用不完全或不精确的标签进行学习；教师-学生学习框架是一种通过教师模型生成伪标签来指导学生模型学习的转移学习方法。"
    },
    {
        "order": 949,
        "title": "Aggregation via Separation: Boosting Facial Landmark Detector With Semi-Supervised Style Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_Aggregation_via_Separation_Boosting_Facial_Landmark_Detector_With_Semi-Supervised_Style_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_Aggregation_via_Separation_Boosting_Facial_Landmark_Detector_With_Semi-Supervised_Style_ICCV_2019_paper.html",
        "abstract": "Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign.",
        "中文标题": "通过分离进行聚合：利用半监督风格转换提升面部标志检测器",
        "摘要翻译": "面部标志检测，或面部对齐，是一个已被广泛研究的基础任务。在本文中，我们探讨了面部标志检测的一个新视角，并证明它带来了进一步的显著改进。鉴于任何面部图像都可以分解为捕捉光照、纹理和图像环境的风格空间和风格不变的结构空间，我们的关键思想是利用每个个体的解耦风格和形状空间，通过风格转换来增强现有结构。通过这些增强的合成样本，我们的半监督模型意外地大幅超越了全监督模型。大量实验验证了我们的想法的有效性，在WFLW、300W、COFW和AFLW数据集上取得了最先进的结果。我们提出的结构是通用的，可以组装到任何面部对齐框架中。代码已在https://github.com/thesouthfrog/stylealign公开。",
        "领域": "面部标志检测/风格转换/半监督学习",
        "问题": "如何通过风格转换增强面部标志检测的准确性和鲁棒性",
        "动机": "探索面部标志检测的新视角，通过解耦风格和形状空间来增强现有结构，以提高检测性能",
        "方法": "利用解耦的风格和形状空间，通过风格转换生成增强的合成样本，采用半监督学习方法训练模型",
        "关键词": [
            "面部标志检测",
            "风格转换",
            "半监督学习",
            "解耦空间"
        ],
        "涉及的技术概念": "本文涉及的技术概念包括面部标志检测（Facial Landmark Detection）、风格转换（Style Translation）、解耦空间（Disentangled Space）、半监督学习（Semi-Supervised Learning）以及合成样本生成（Synthetic Sample Generation）。"
    },
    {
        "order": 950,
        "title": "AdaptIS: Adaptive Instance Selection Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sofiiuk_AdaptIS_Adaptive_Instance_Selection_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sofiiuk_AdaptIS_Adaptive_Instance_Selection_Network_ICCV_2019_paper.html",
        "abstract": "We present Adaptive Instance Selection network architecture for class-agnostic instance segmentation. Given an input image and a point (x, y), it generates a mask for the object located at (x, y). The network adapts to the input point with a help of AdaIN layers [??], thus producing different masks for different objects on the same image. AdaptIS generates pixel-accurate object masks, therefore it accurately segments objects of complex shape or severely occluded ones. AdaptIS can be easily combined with standard semantic segmentation pipeline to perform panoptic segmentation. To illustrate the idea, we perform experiments on a challenging toy problem with difficult occlusions. Then we extensively evaluate the method on panoptic segmentation benchmarks. We obtain state-of-the-art results on Cityscapes and Mapillary even without pretraining on COCO, and show competitive results on a challenging COCO dataset. The source code of the method and the trained models are available at  https://github.com/saic-vul/adaptis  https://github.com/saic-vul/adaptis .",
        "中文标题": "AdaptIS: 自适应实例选择网络",
        "摘要翻译": "我们提出了一种用于类别无关实例分割的自适应实例选择网络架构。给定输入图像和一个点(x, y)，它为位于(x, y)的对象生成一个掩码。网络通过AdaIN层的帮助适应输入点，从而在同一图像上为不同对象生成不同的掩码。AdaptIS生成像素级精确的对象掩码，因此它能够准确分割形状复杂或严重遮挡的对象。AdaptIS可以轻松地与标准语义分割流程结合，以执行全景分割。为了说明这一想法，我们在一个具有挑战性的玩具问题上进行了实验，该问题涉及困难的遮挡。然后，我们在全景分割基准上广泛评估了该方法。我们在Cityscapes和Mapillary上获得了最先进的结果，即使没有在COCO上进行预训练，也在具有挑战性的COCO数据集上展示了竞争性的结果。该方法的源代码和训练模型可在https://github.com/saic-vul/adaptis获取。",
        "领域": "实例分割/全景分割/自适应网络",
        "问题": "解决类别无关的实例分割问题，特别是在处理复杂形状或严重遮挡的对象时。",
        "动机": "为了提高实例分割的准确性和适应性，特别是在面对复杂形状或严重遮挡的对象时。",
        "方法": "采用自适应实例选择网络架构，通过AdaIN层适应输入点，生成像素级精确的对象掩码，并与标准语义分割流程结合进行全景分割。",
        "关键词": [
            "实例分割",
            "全景分割",
            "自适应网络",
            "AdaIN层",
            "像素级精确"
        ],
        "涉及的技术概念": "AdaIN层（自适应实例归一化层）用于使网络适应不同的输入点，从而为同一图像上的不同对象生成不同的掩码。这种方法能够处理复杂形状或严重遮挡的对象，实现像素级精确的分割。"
    },
    {
        "order": 951,
        "title": "AutoFocus: Efficient Multi-Scale Inference",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Najibi_AutoFocus_Efficient_Multi-Scale_Inference_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Najibi_AutoFocus_Efficient_Multi-Scale_Inference_ICCV_2019_paper.html",
        "abstract": "This paper describes AutoFocus, an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions which are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is only applied inside FocusChips, which reduces computation while processing finer scales. Different types of error can arise when detections from FocusChips of multiple scales are combined, hence techniques to correct them are proposed. AutoFocus obtains an mAP of 47.9% (68.3% at 50% overlap) on the COCO test-dev set while processing 6.4 images per second on a Titan X (Pascal) GPU. This is 2.5X faster than our multi-scale baseline detector and matches its mAP. The number of pixels processed in the pyramid can be reduced by 5X with a 1% drop in mAP. AutoFocus obtains more than 10% mAP gain compared to RetinaNet but runs at the same speed with the same ResNet-101 backbone.",
        "中文标题": "AutoFocus: 高效多尺度推理",
        "摘要翻译": "本文介绍了AutoFocus，一种用于基于深度学习的目标检测器的高效多尺度推理算法。AutoFocus采用从粗到细的方法，而不是处理整个图像金字塔，仅处理在更细尺度上可能包含小物体的区域。这是通过在较粗尺度上预测小物体的类别无关分割图（称为FocusPixels）来实现的。FocusPixels可以以高召回率预测，在许多情况下，它们仅覆盖整个图像的一小部分。为了有效利用FocusPixels，提出了一种算法，该算法生成包含FocusPixels的紧凑矩形FocusChips。检测器仅在FocusChips内部应用，这减少了处理更细尺度时的计算量。当结合来自多个尺度的FocusChips的检测结果时，可能会出现不同类型的错误，因此提出了纠正这些错误的技术。AutoFocus在COCO测试开发集上获得了47.9%的mAP（在50%重叠时为68.3%），同时在Titan X（Pascal）GPU上每秒处理6.4张图像。这比我们的多尺度基线检测器快2.5倍，并且匹配其mAP。金字塔中处理的像素数可以减少5倍，而mAP仅下降1%。与RetinaNet相比，AutoFocus获得了超过10%的mAP增益，但在相同的ResNet-101骨干网络上以相同的速度运行。",
        "领域": "目标检测/图像分割/深度学习优化",
        "问题": "提高基于深度学习的目标检测器在多尺度推理中的效率",
        "动机": "为了减少处理整个图像金字塔的计算量，同时保持或提高目标检测的准确率",
        "方法": "采用从粗到细的方法，通过预测FocusPixels和生成FocusChips来减少计算量，同时提出技术纠正多尺度检测中的错误",
        "关键词": [
            "多尺度推理",
            "目标检测",
            "图像分割",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "FocusPixels": "在较粗尺度上预测的小物体的类别无关分割图",
            "FocusChips": "包含FocusPixels的紧凑矩形区域，用于减少计算量",
            "mAP": "平均精度均值，用于评估目标检测器的性能",
            "COCO": "一个广泛使用的图像识别、分割和字幕数据集",
            "ResNet-101": "一种深度残差网络，用于图像识别任务"
        }
    },
    {
        "order": 952,
        "title": "DADA: Depth-Aware Domain Adaptation in Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Vu_DADA_Depth-Aware_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Vu_DADA_Depth-Aware_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Unsupervised domain adaptation (UDA) is important for applications where large scale annotation of representative data is challenging. For semantic segmentation in particular, it helps deploy on real \"target domain\" data models that are trained on annotated images from a different \"source domain\", notably a virtual environment. To this end, most previous works consider semantic segmentation as the only mode of supervision for source domain data, while ignoring other, possibly available, information like depth. In this work, we aim at exploiting at best such a privileged information while training the UDA model. We propose a unified depth-aware UDA framework that leverages in several complementary ways the knowledge of dense depth in the source domain. As a result, the performance of the trained semantic segmentation model on the target domain is boosted. Our novel approach indeed achieves state-of-the-art performance on different challenging synthetic-2-real benchmarks.",
        "中文标题": "DADA: 语义分割中的深度感知域适应",
        "摘要翻译": "无监督域适应（UDA）对于大规模标注代表性数据具有挑战性的应用非常重要。特别是对于语义分割，它有助于在真实“目标域”数据上部署模型，这些模型是在来自不同“源域”的标注图像上训练的，尤其是虚拟环境。为此，大多数先前的工作将语义分割视为源域数据的唯一监督模式，而忽略了其他可能可用的信息，如深度。在这项工作中，我们旨在在训练UDA模型时充分利用这种特权信息。我们提出了一个统一的深度感知UDA框架，该框架以几种互补的方式利用源域中密集深度的知识。结果，训练后的语义分割模型在目标域上的性能得到了提升。我们的新方法确实在不同的具有挑战性的合成到真实基准测试中实现了最先进的性能。",
        "领域": "语义分割/域适应/深度感知",
        "问题": "如何在语义分割中有效利用源域中的深度信息进行无监督域适应",
        "动机": "提高语义分割模型在目标域上的性能，特别是在源域和目标域之间存在差异时",
        "方法": "提出一个统一的深度感知UDA框架，通过多种方式利用源域中的深度信息",
        "关键词": [
            "语义分割",
            "无监督域适应",
            "深度感知"
        ],
        "涉及的技术概念": "无监督域适应（UDA）是一种技术，旨在在没有目标域标注数据的情况下，将模型从源域适应到目标域。语义分割是计算机视觉中的一项任务，旨在为图像中的每个像素分配一个类别标签。深度感知指的是利用图像中的深度信息来增强模型的性能。"
    },
    {
        "order": 953,
        "title": "Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shvets_Leveraging_Long-Range_Temporal_Relationships_Between_Proposals_for_Video_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shvets_Leveraging_Long-Range_Temporal_Relationships_Between_Proposals_for_Video_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Single-frame object detectors perform well on videos sometimes, even without temporal context. However, challenges such as occlusion, motion blur, and rare poses of objects are hard to resolve without temporal awareness. Thus, there is a strong need to improve video object detection by considering long-range temporal dependencies. In this paper, we present a light-weight modification to a single-frame detector that accounts for arbitrary long dependencies in a video. It improves the accuracy of a single-frame detector significantly with negligible compute overhead. The key component of our approach is a novel temporal relation module, operating on object proposals, that learns the similarities between proposals from different frames and selects proposals from past and/or future to support current proposals. Our final \"causal\" model, without any offline post-processing steps, runs at a similar speed as a single-frame detector and achieves state-of-the-art video object detection on ImageNet VID dataset.",
        "中文标题": "利用提案之间的长程时间关系进行视频目标检测",
        "摘要翻译": "单帧目标检测器有时在视频上表现良好，即使没有时间上下文。然而，诸如遮挡、运动模糊和物体的罕见姿态等挑战，在没有时间意识的情况下难以解决。因此，迫切需要通过考虑长程时间依赖性来改进视频目标检测。在本文中，我们提出了一种对单帧检测器的轻量级修改，该修改考虑了视频中的任意长依赖性。它以可忽略的计算开销显著提高了单帧检测器的准确性。我们方法的关键组件是一个新颖的时间关系模块，该模块在目标提案上操作，学习不同帧之间提案的相似性，并选择过去和/或未来的提案以支持当前提案。我们的最终“因果”模型，没有任何离线后处理步骤，以与单帧检测器相似的速度运行，并在ImageNet VID数据集上实现了最先进的视频目标检测。",
        "领域": "视频目标检测/时间关系学习/目标提案",
        "问题": "解决视频目标检测中的遮挡、运动模糊和物体罕见姿态问题",
        "动机": "提高视频目标检测的准确性，通过考虑长程时间依赖性来解决单帧检测器难以处理的挑战",
        "方法": "提出了一种轻量级修改的单帧检测器，引入了一个新颖的时间关系模块，该模块学习不同帧之间提案的相似性，并选择过去和/或未来的提案以支持当前提案",
        "关键词": [
            "视频目标检测",
            "时间关系模块",
            "目标提案"
        ],
        "涉及的技术概念": "单帧检测器、时间上下文、长程时间依赖性、时间关系模块、目标提案、ImageNet VID数据集"
    },
    {
        "order": 954,
        "title": "Transferable Contrastive Network for Generalized Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Transferable_Contrastive_Network_for_Generalized_Zero-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Transferable_Contrastive_Network_for_Generalized_Zero-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "Zero-shot learning (ZSL) is a challenging problem that aims to recognize the target categories without seen data, where semantic information is leveraged to transfer knowledge from some source classes. Although ZSL has made great progress in recent years, most existing approaches are easy to overfit the sources classes in generalized zero-shot learning (GZSL) task, which indicates that they learn little knowledge about target classes. To tackle such problem, we propose a novel Transferable Contrastive Network (TCN) that explicitly transfers knowledge from the source classes to the target classes. It automatically contrasts one image with different classes to judge whether they are consistent or not. By exploiting the class similarities to make knowledge transfer from source images to similar target classes, our approach is more robust to recognize the target images. Experiments on five benchmark datasets show the superiority of our approach for GZSL.",
        "中文标题": "可转移对比网络用于广义零样本学习",
        "摘要翻译": "零样本学习（ZSL）是一个旨在无需看到数据的情况下识别目标类别的挑战性问题，其中利用语义信息从一些源类别转移知识。尽管近年来ZSL取得了很大进展，但在广义零样本学习（GZSL）任务中，大多数现有方法容易过拟合源类别，这表明它们对目标类别的知识学习很少。为了解决这个问题，我们提出了一种新颖的可转移对比网络（TCN），它明确地将知识从源类别转移到目标类别。它自动将一幅图像与不同类别进行对比，以判断它们是否一致。通过利用类别相似性使知识从源图像转移到相似的目标类别，我们的方法在识别目标图像方面更加鲁棒。在五个基准数据集上的实验显示了我们的方法在GZSL中的优越性。",
        "领域": "零样本学习/知识转移/图像识别",
        "问题": "解决广义零样本学习任务中现有方法容易过拟合源类别，对目标类别知识学习不足的问题",
        "动机": "提高广义零样本学习任务中对目标类别的识别能力，减少对源类别的过拟合",
        "方法": "提出一种可转移对比网络（TCN），通过自动对比图像与不同类别的一致性，利用类别相似性实现知识从源图像到相似目标类别的转移",
        "关键词": [
            "零样本学习",
            "知识转移",
            "图像识别",
            "类别相似性",
            "对比网络"
        ],
        "涉及的技术概念": "零样本学习（ZSL）是一种在没有看到目标类别数据的情况下，利用语义信息从源类别转移知识来识别目标类别的技术。广义零样本学习（GZSL）是ZSL的一个扩展，旨在同时识别源类别和目标类别。可转移对比网络（TCN）是一种新颖的网络结构，通过对比图像与不同类别的一致性，利用类别相似性实现知识从源图像到相似目标类别的转移，从而提高对目标类别的识别能力。"
    },
    {
        "order": 955,
        "title": "Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.html",
        "abstract": "Most progress in semantic segmentation reports on daytime images taken under favorable illumination conditions. We instead address the problem of semantic segmentation of nighttime images and improve the state-of-the-art, by adapting daytime models to nighttime without using nighttime annotations. Moreover, we design a new evaluation framework to address the substantial uncertainty of semantics in nighttime images. Our central contributions are: 1) a curriculum framework to gradually adapt semantic segmentation models from day to night via labeled synthetic images and unlabeled real images, both for progressively darker times of day, which exploits cross-time-of-day correspondences for the real images to guide the inference of their labels; 2) a novel uncertainty-aware annotation and evaluation framework and metric for semantic segmentation, designed for adverse conditions and including image regions beyond human recognition capability in the evaluation in a principled fashion; 3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920 unlabeled twilight images with correspondences to their daytime counterparts plus a set of 151 nighttime images with fine pixel-level annotations created with our protocol, which serves as a first benchmark to perform our novel evaluation. Experiments show that our guided curriculum adaptation significantly outperforms state-of-the-art methods on real nighttime sets both for standard metrics and our uncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals that selective invalidation of predictions can lead to better results on data with ambiguous content such as our nighttime benchmark and profit safety-oriented applications which involve invalid inputs.",
        "中文标题": "引导课程模型适应与不确定性感知评估用于语义夜间图像分割",
        "摘要翻译": "语义分割的大多数进展报告都是在良好光照条件下拍摄的日间图像。我们则解决了夜间图像的语义分割问题，并通过将日间模型适应到夜间而不使用夜间注释来改进现有技术。此外，我们设计了一个新的评估框架来解决夜间图像中语义的显著不确定性。我们的核心贡献是：1）一个课程框架，通过标记的合成图像和未标记的真实图像，逐步将语义分割模型从日间适应到夜间，利用真实图像的跨时间对应关系来指导其标签的推断；2）一个新的不确定性感知注释和评估框架及度量，专为恶劣条件设计，并以原则性的方式包括超出人类识别能力的图像区域在评估中；3）Dark Zurich数据集，包括2416张未标记的夜间图像和2920张未标记的黄昏图像，以及它们与日间对应图像的对应关系，加上一组151张夜间图像，这些图像使用我们的协议创建了精细的像素级注释，作为执行我们新评估的第一个基准。实验表明，我们的引导课程适应在真实夜间数据集上显著优于现有技术方法，无论是标准度量还是我们的不确定性感知度量。此外，我们的不确定性感知评估显示，预测的选择性无效化可以导致在具有模糊内容的数据（如我们的夜间基准）上获得更好的结果，并有利于涉及无效输入的安全导向应用。",
        "领域": "语义分割/夜间图像处理/模型适应",
        "问题": "夜间图像的语义分割",
        "动机": "改进现有技术，通过将日间模型适应到夜间而不使用夜间注释来解决夜间图像的语义分割问题",
        "方法": "1）使用课程框架逐步将语义分割模型从日间适应到夜间；2）设计新的不确定性感知注释和评估框架及度量；3）创建Dark Zurich数据集作为评估基准",
        "关键词": [
            "语义分割",
            "夜间图像",
            "模型适应",
            "不确定性评估",
            "数据集"
        ],
        "涉及的技术概念": "课程学习框架用于模型适应，不确定性感知评估框架用于处理夜间图像中的语义不确定性，Dark Zurich数据集作为评估基准。"
    },
    {
        "order": 956,
        "title": "Fast Point R-CNN",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Fast_Point_R-CNN_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Fast_Point_R-CNN_ICCV_2019_paper.html",
        "abstract": "We present a unified, efficient and effective framework for point-cloud based 3D object detection. Our two-stage approach utilizes both voxel representation and raw point cloud data to exploit respective advantages. The first stage network, with voxel representation as input, only consists of light convolutional operations, producing a small number of high-quality initial predictions. Coordinate and indexed convolutional feature of each point in initial prediction are effectively fused with the attention mechanism, preserving both accurate localization and context information. The second stage works on interior points with their fused feature for further refining the prediction. Our method is evaluated on KITTI dataset, in terms of both 3D and Bird's Eye View (BEV) detection, and achieves state-of-the-arts with a 15FPS detection rate.",
        "中文标题": "快速点R-CNN",
        "摘要翻译": "我们提出了一个统一、高效且有效的基于点云的3D物体检测框架。我们的两阶段方法利用体素表示和原始点云数据来发挥各自的优势。第一阶段网络以体素表示为输入，仅包含轻量级卷积操作，产生少量高质量的初始预测。初始预测中每个点的坐标和索引卷积特征通过注意力机制有效融合，保留了精确定位和上下文信息。第二阶段对内部点及其融合特征进行处理，以进一步细化预测。我们的方法在KITTI数据集上进行了评估，无论是3D还是鸟瞰图（BEV）检测，均达到了最先进的水平，检测率为15FPS。",
        "领域": "3D物体检测/点云处理/卷积神经网络",
        "问题": "基于点云的3D物体检测",
        "动机": "为了开发一个统一、高效且有效的框架，以利用点云数据进行3D物体检测，同时保留精确定位和上下文信息。",
        "方法": "采用两阶段方法，第一阶段使用体素表示和轻量级卷积操作生成高质量初始预测，第二阶段通过注意力机制融合点特征以细化预测。",
        "关键词": [
            "3D物体检测",
            "点云处理",
            "卷积神经网络",
            "注意力机制"
        ],
        "涉及的技术概念": "体素表示、卷积操作、注意力机制、点云数据、3D物体检测、鸟瞰图（BEV）检测"
    },
    {
        "order": 957,
        "title": "SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_SceneGraphNet_Neural_Message_Passing_for_3D_Indoor_Scene_Augmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_SceneGraphNet_Neural_Message_Passing_for_3D_Indoor_Scene_Augmentation_ICCV_2019_paper.html",
        "abstract": "In this paper we propose a neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings. Given an input, potentially incomplete, 3D scene and a query location, our method predicts a probability distribution over object types that fit well in that location. Our distribution is predicted though passing learned messages in a dense graph whose nodes represent objects in the input scene and edges represent spatial and structural relationships. By weighting messages through an attention mechanism, our method learns to focus on the most relevant surrounding scene context to predict new scene objects. We found that our method significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene based on our experiments in the SUNCG dataset. We also demonstrate other applications of our method, including context-based 3D object recognition and iterative scene generation.",
        "中文标题": "SceneGraphNet: 用于3D室内场景增强的神经消息传递",
        "摘要翻译": "在本文中，我们提出了一种神经消息传递方法，用于通过匹配周围环境的新对象来增强输入的3D室内场景。给定一个可能不完整的3D场景和一个查询位置，我们的方法预测适合该位置的对象类型的概率分布。我们的分布是通过在密集图中传递学习到的消息来预测的，图中的节点代表输入场景中的对象，边代表空间和结构关系。通过注意力机制加权消息，我们的方法学会关注最相关的周围场景上下文以预测新的场景对象。我们发现，基于我们在SUNCG数据集上的实验，我们的方法在正确预测场景中缺失的对象方面显著优于最先进的方法。我们还展示了我们方法的其他应用，包括基于上下文的3D对象识别和迭代场景生成。",
        "领域": "3D场景理解/室内场景增强/神经消息传递",
        "问题": "如何通过匹配周围环境的新对象来增强输入的3D室内场景",
        "动机": "提高3D室内场景中对象预测的准确性，以增强场景的完整性和实用性",
        "方法": "使用神经消息传递方法，在密集图中传递学习到的消息，通过注意力机制加权消息，预测适合特定位置的对象类型的概率分布",
        "关键词": [
            "3D场景理解",
            "室内场景增强",
            "神经消息传递",
            "注意力机制",
            "对象预测"
        ],
        "涉及的技术概念": "神经消息传递是一种在图中传递信息的方法，用于处理图结构数据。注意力机制是一种使模型能够关注输入数据中最相关部分的技术。3D场景理解涉及从3D数据中提取和理解场景的结构和内容。室内场景增强是指通过添加或修改对象来改进或扩展室内场景的视觉表示。"
    },
    {
        "order": 958,
        "title": "SkyScapes  Fine-Grained Semantic Understanding of Aerial Scenes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Azimi_SkyScapes__Fine-Grained_Semantic_Understanding_of_Aerial_Scenes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Azimi_SkyScapes__Fine-Grained_Semantic_Understanding_of_Aerial_Scenes_ICCV_2019_paper.html",
        "abstract": "Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks.",
        "中文标题": "SkyScapes 精细粒度语义理解的航空场景",
        "摘要翻译": "理解厘米级精度的复杂城市基础设施对于从自动驾驶到地图绘制、基础设施监控和城市管理的许多应用至关重要。航空图像即时提供了大面积的有价值信息；然而，目前没有数据集能够捕捉到现实世界应用所需粒度级别的航空场景复杂性。为了解决这个问题，我们引入了SkyScapes，一个具有高精度、细粒度注释的航空图像数据集，用于像素级语义标注。SkyScapes提供了31个语义类别的注释，从大型结构（如建筑物、道路和植被）到精细细节（如12个（子）类别的车道标记）。我们在这个数据集上定义了两个主要任务：密集语义分割和多类车道标记预测。我们进行了广泛的实验，以评估在SkyScapes上最先进的分割方法。现有方法难以处理广泛的类别、物体大小、比例和精细细节。因此，我们提出了一种新颖的多任务模型，该模型结合了语义边缘检测，并更好地调整了从广泛比例中提取特征。该模型在两个任务上的区域轮廓和细节水平上均实现了显著的改进。",
        "领域": "航空图像分析/语义分割/多任务学习",
        "问题": "现有数据集无法满足现实世界应用对航空场景复杂性和粒度级别的需求",
        "动机": "为了提供高精度、细粒度的航空图像数据集，以支持像素级语义标注，从而更好地理解和分析复杂城市基础设施",
        "方法": "引入SkyScapes数据集，并提出一种结合语义边缘检测的多任务模型，以改进特征提取和处理广泛类别、物体大小、比例和精细细节的能力",
        "关键词": [
            "航空图像",
            "语义分割",
            "多任务学习",
            "语义边缘检测",
            "特征提取"
        ],
        "涉及的技术概念": "SkyScapes数据集提供了31个语义类别的注释，包括大型结构和精细细节。提出的多任务模型结合了语义边缘检测，旨在从广泛的比例中提取特征，以改进区域轮廓和细节水平的处理。"
    },
    {
        "order": 959,
        "title": "Mesh R-CNN",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gkioxari_Mesh_R-CNN_ICCV_2019_paper.html",
        "abstract": "Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",
        "中文标题": "Mesh R-CNN",
        "摘要翻译": "2D感知的快速发展已经导致了能够准确检测现实世界图像中物体的系统。然而，这些系统在2D中进行预测，忽略了世界的3D结构。同时，3D形状预测的进展主要集中在合成基准和孤立物体上。我们将这两个领域的进展统一起来。我们提出了一个系统，该系统能够检测现实世界图像中的物体，并生成一个三角形网格，给出每个检测到的物体的完整3D形状。我们的系统，称为Mesh R-CNN，通过增加一个网格预测分支来增强Mask R-CNN，该分支通过首先预测粗糙的体素表示来输出具有不同拓扑结构的网格，这些体素表示被转换为网格，并通过在网格的顶点和边上操作的图卷积网络进行细化。我们在ShapeNet上验证了我们的网格预测分支，在那里我们在单图像形状预测上优于之前的工作。然后，我们将完整的Mesh R-CNN系统部署在Pix3D上，在那里我们联合检测物体并预测它们的3D形状。",
        "领域": "3D形状预测/物体检测/图卷积网络",
        "问题": "如何在现实世界图像中检测物体并预测其完整的3D形状",
        "动机": "现有的2D物体检测系统忽略了物体的3D结构，而3D形状预测的研究主要集中在合成数据和孤立物体上，缺乏对现实世界图像中物体3D形状的预测能力。",
        "方法": "提出Mesh R-CNN系统，通过增加一个网格预测分支来增强Mask R-CNN，该分支首先预测粗糙的体素表示，然后将其转换为网格，并通过图卷积网络进行细化。",
        "关键词": [
            "3D形状预测",
            "物体检测",
            "图卷积网络",
            "Mesh R-CNN",
            "Mask R-CNN"
        ],
        "涉及的技术概念": {
            "2D感知": "指能够识别和理解2D图像中物体的技术。",
            "3D结构": "指物体在三维空间中的形状和布局。",
            "三角形网格": "一种用于表示3D形状的图形数据结构，由顶点、边和面组成。",
            "体素表示": "一种将3D空间分割成小立方体（体素）来表示3D形状的方法。",
            "图卷积网络": "一种在图结构数据上操作的神经网络，用于处理非欧几里得数据。",
            "ShapeNet": "一个包含大量3D形状的数据集，用于3D形状识别和预测的研究。",
            "Pix3D": "一个包含现实世界图像及其对应3D模型的数据集，用于3D物体检测和形状预测的研究。"
        }
    },
    {
        "order": 960,
        "title": "Transferable Representation Learning in Vision-and-Language Navigation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Transferable_Representation_Learning_in_Vision-and-Language_Navigation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_Transferable_Representation_Learning_in_Vision-and-Language_Navigation_ICCV_2019_paper.html",
        "abstract": "Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",
        "中文标题": "视觉与语言导航中的可迁移表示学习",
        "摘要翻译": "视觉与语言导航（VLN）任务，如房间到房间（R2R），要求机器代理解释自然语言指令并学习在视觉上逼真的环境中行动以实现导航目标。整个任务需要解决几个感知问题：成功的代理结合时空、视觉和语言理解来产生适当的行动序列。我们的方法将预训练的视觉和语言表示适应于相关的领域内任务，使它们对VLN更有效。具体来说，这些表示被适应以解决跨模态序列对齐和序列一致性任务。在序列对齐任务中，模型确定指令是否对应于一系列视觉帧。在序列一致性任务中，模型确定感知序列在指令条件下的潜在空间中是否顺序预测。通过转移领域适应的表示，我们提高了R2R中竞争代理的性能，这是通过路径长度加权的成功率（SPL）指标来衡量的。",
        "领域": "视觉与语言导航/跨模态学习/序列预测",
        "问题": "如何使机器代理在视觉与语言导航任务中更有效地解释自然语言指令并学习在视觉上逼真的环境中行动",
        "动机": "提高视觉与语言导航任务中机器代理的性能，通过适应预训练的视觉和语言表示来解决跨模态序列对齐和序列一致性问题",
        "方法": "将预训练的视觉和语言表示适应于相关的领域内任务，解决跨模态序列对齐和序列一致性任务，并通过转移领域适应的表示来提高代理性能",
        "关键词": [
            "视觉与语言导航",
            "跨模态学习",
            "序列预测"
        ],
        "涉及的技术概念": "预训练的视觉和语言表示、跨模态序列对齐、序列一致性任务、指令条件下的潜在空间、路径长度加权的成功率（SPL）指标"
    },
    {
        "order": 961,
        "title": "Deep Supervised Hashing With Anchor Graph",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Deep_Supervised_Hashing_With_Anchor_Graph_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Deep_Supervised_Hashing_With_Anchor_Graph_ICCV_2019_paper.html",
        "abstract": "Recently, a series of deep supervised hashing methods were proposed for binary code learning. However, due to the high computation cost and the limited hardware's memory, these methods will first select a subset from the training set, and then form a mini-batch data to update the network in each iteration. Therefore, the remaining labeled data cannot be fully utilized and the model cannot directly obtain the binary codes of the entire training set for retrieval. To address these problems, this paper proposes an interesting regularized deep model to seamlessly integrate the advantages of deep hashing and efficient binary code learning by using the anchor graph. As such, the deep features and label matrix can be jointly used to optimize the binary codes, and the network can obtain more discriminative feedback from the linear combinations of the learned bits. Moreover, we also reveal the algorithm mechanism and its computation essence. Experiments on three large-scale datasets indicate that the proposed method achieves better retrieval performance with less training time compared to previous deep hashing methods.",
        "中文标题": "基于锚点图的深度监督哈希",
        "摘要翻译": "最近，一系列深度监督哈希方法被提出用于二进制码学习。然而，由于高计算成本和硬件内存的限制，这些方法首先会从训练集中选择一个子集，然后在每次迭代中形成一个小批量数据来更新网络。因此，剩余的标记数据无法被充分利用，模型无法直接获取整个训练集的二进制码进行检索。为了解决这些问题，本文提出了一个有趣的正则化深度模型，通过使用锚点图无缝整合深度哈希和高效二进制码学习的优势。这样，深度特征和标签矩阵可以联合用于优化二进制码，网络可以从学习到的位的线性组合中获得更多区分性反馈。此外，我们还揭示了算法机制及其计算本质。在三个大规模数据集上的实验表明，与之前的深度哈希方法相比，所提出的方法在更短的训练时间内实现了更好的检索性能。",
        "领域": "图像检索/哈希学习/二进制码学习",
        "问题": "高计算成本和硬件内存限制导致无法充分利用所有标记数据和直接获取整个训练集的二进制码进行检索",
        "动机": "解决现有深度监督哈希方法在计算成本和硬件内存限制下无法充分利用所有标记数据和直接获取整个训练集二进制码的问题",
        "方法": "提出一个正则化深度模型，通过使用锚点图无缝整合深度哈希和高效二进制码学习的优势，联合使用深度特征和标签矩阵优化二进制码，并从学习到的位的线性组合中获得更多区分性反馈",
        "关键词": [
            "图像检索",
            "哈希学习",
            "二进制码学习"
        ],
        "涉及的技术概念": "深度监督哈希方法、二进制码学习、锚点图、正则化深度模型、深度特征、标签矩阵、线性组合、区分性反馈"
    },
    {
        "order": 962,
        "title": "Towards Unsupervised Image Captioning With Shared Multimodal Embeddings",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.html",
        "abstract": "Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work.",
        "中文标题": "迈向共享多模态嵌入的无监督图像描述",
        "摘要翻译": "在没有显式监督的情况下理解图像已成为计算机视觉中的一个重要问题。在本文中，我们通过生成场景的语言描述来解决图像描述问题，而无需从注释的图像和其描述对中学习。我们方法的核心组件是一个由视觉概念构建的共享潜在空间。在这个空间中，两种模态应该是不可区分的。首先训练一个语言模型将句子编码为语义结构化的嵌入。被翻译到这个嵌入空间中的图像特征可以通过相同的语言模型解码为描述，类似于句子嵌入。这种翻译是从弱配对的图像和文本中学习的，使用了对噪声分配鲁棒的损失和条件对抗组件。我们的方法允许利用注释的图像/描述数据分布之外的大型文本语料库。我们的实验表明，所提出的领域对齐学习了一种语义上有意义的表示，其性能优于以前的工作。",
        "领域": "图像描述/自然语言处理/多模态学习",
        "问题": "在没有显式监督的情况下生成图像的语言描述",
        "动机": "解决在没有注释的图像和描述对的情况下理解图像的问题，以利用大型文本语料库",
        "方法": "构建一个由视觉概念构建的共享潜在空间，训练语言模型将句子编码为语义结构化的嵌入，并通过相同的语言模型将图像特征解码为描述，使用对噪声分配鲁棒的损失和条件对抗组件学习翻译",
        "关键词": [
            "无监督学习",
            "图像描述",
            "多模态嵌入"
        ],
        "涉及的技术概念": "共享潜在空间、视觉概念、语义结构化嵌入、语言模型、条件对抗组件、领域对齐"
    },
    {
        "order": 963,
        "title": "Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Detecting_11K_Classes_Large_Scale_Object_Detection_Without_Fine-Grained_Bounding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Detecting_11K_Classes_Large_Scale_Object_Detection_Without_Fine-Grained_Bounding_ICCV_2019_paper.html",
        "abstract": "Recent advances in deep learning greatly boost the performance of object detection. State-of-the-art methods such as Faster-RCNN, FPN and R-FCN have achieved high accuracy in challenging benchmark datasets. However, these methods require fully annotated object bounding boxes for training, which are incredibly hard to scale up due to the high annotation cost. Weakly-supervised methods, on the other hand, only require image-level labels for training, but the performance is far below their fully-supervised counterparts. In this paper, we propose a semi-supervised large scale fine-grained detection method, which only needs bounding box annotations of a smaller number of coarse-grained classes and image-level labels of large scale fine-grained classes, and can detect all classes at nearly fully-supervised accuracy. We achieve this by utilizing the correlations between coarse-grained and fine-grained classes with shared backbone, soft-attention based proposal re-ranking, and a dual-level memory module. Experiment results show that our methods can achieve close accuracy on object detection to state-of-the-art fully-supervised methods on two large scale datasets, ImageNet and OpenImages, with only a small fraction of fully annotated classes.",
        "中文标题": "检测11K类别：无需细粒度边界框的大规模目标检测",
        "摘要翻译": "深度学习的最新进展极大地提升了目标检测的性能。诸如Faster-RCNN、FPN和R-FCN等最先进的方法在具有挑战性的基准数据集上实现了高准确率。然而，这些方法需要完全标注的目标边界框进行训练，由于高昂的标注成本，这些方法难以扩展。另一方面，弱监督方法仅需要图像级标签进行训练，但其性能远低于完全监督的对应方法。在本文中，我们提出了一种半监督的大规模细粒度检测方法，该方法仅需要少量粗粒度类别的边界框注释和大规模细粒度类别的图像级标签，并且可以以接近完全监督的准确率检测所有类别。我们通过利用粗粒度和细粒度类别之间的相关性，共享骨干网络，基于软注意力的提议重排序，以及双级记忆模块来实现这一点。实验结果表明，我们的方法在两个大规模数据集ImageNet和OpenImages上，仅使用一小部分完全注释的类别，就能实现接近最先进完全监督方法的目标检测准确率。",
        "领域": "目标检测/细粒度分类/半监督学习",
        "问题": "如何在减少标注成本的同时，实现大规模细粒度目标检测的高准确率",
        "动机": "完全监督的目标检测方法需要大量精细标注的边界框，成本高昂且难以扩展；弱监督方法虽然降低了标注成本，但性能远不及完全监督方法。因此，研究一种既能减少标注成本又能保持高检测准确率的方法具有重要意义。",
        "方法": "提出了一种半监督的大规模细粒度检测方法，通过利用粗粒度和细粒度类别之间的相关性，共享骨干网络，基于软注意力的提议重排序，以及双级记忆模块，仅需少量粗粒度类别的边界框注释和大规模细粒度类别的图像级标签，就能实现接近完全监督方法的目标检测准确率。",
        "关键词": [
            "目标检测",
            "细粒度分类",
            "半监督学习",
            "软注意力",
            "双级记忆模块"
        ],
        "涉及的技术概念": {
            "Faster-RCNN": "一种流行的目标检测框架，通过区域提议网络（RPN）生成候选区域，然后对这些区域进行分类和边界框回归。",
            "FPN": "特征金字塔网络，用于在不同尺度上提取特征，以提高目标检测的性能。",
            "R-FCN": "区域全卷积网络，一种用于目标检测的框架，通过位置敏感的得分图来提高检测效率。",
            "软注意力": "一种机制，用于在神经网络中动态地聚焦于输入的不同部分，以提高模型对重要特征的关注。",
            "双级记忆模块": "一种用于存储和检索信息的模块，通过两个级别的记忆机制来增强模型的学习能力。"
        }
    },
    {
        "order": 964,
        "title": "ViCo: Word Embeddings From Visual Co-Occurrences",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gupta_ViCo_Word_Embeddings_From_Visual_Co-Occurrences_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gupta_ViCo_Word_Embeddings_From_Visual_Co-Occurrences_ICCV_2019_paper.html",
        "abstract": "We propose to learn word embeddings from visual co-occurrences. Two words co-occur visually if both words apply to the same image or image region. Specifically, we extract four types of visual co-occurrences between object and attribute words from large-scale, textually-annotated visual databases like VisualGenome and ImageNet. We then train a multi-task log-bilinear model that compactly encodes word \"meanings\" represented by each co-occurrence type into a single visual word-vector. Through unsupervised clustering, supervised partitioning, and a zero-shot-like generalization analysis we show that our word embeddings complement text-only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone. We further evaluate our embeddings on five downstream applications, four of which are vision-language tasks. Augmenting GloVe with our embeddings yields gains on all tasks. We also find that random embeddings perform comparably to learned embeddings on all supervised vision-language tasks, contrary to conventional wisdom.",
        "中文标题": "ViCo: 从视觉共现中学习词嵌入",
        "摘要翻译": "我们提出从视觉共现中学习词嵌入。如果两个词适用于同一图像或图像区域，则这两个词在视觉上共现。具体来说，我们从大规模文本注释的视觉数据库（如VisualGenome和ImageNet）中提取对象和属性词之间的四种类型的视觉共现。然后，我们训练一个多任务对数双线性模型，该模型将每种共现类型所代表的词“意义”紧凑地编码为单个视觉词向量。通过无监督聚类、有监督分区和零样本式泛化分析，我们展示了我们的词嵌入通过更好地表示视觉概念之间的相似性和差异性来补充仅文本嵌入（如GloVe），这些相似性和差异性很难仅从文本语料库中获得。我们进一步在五个下游应用上评估了我们的嵌入，其中四个是视觉-语言任务。将我们的嵌入与GloVe结合使用，在所有任务上都取得了增益。我们还发现，与常规智慧相反，在所有有监督的视觉-语言任务上，随机嵌入的表现与学习嵌入相当。",
        "领域": "视觉-语言理解/词嵌入/多模态学习",
        "问题": "如何从视觉共现中学习词嵌入，以补充和增强仅文本嵌入的表示能力",
        "动机": "现有的词嵌入方法主要依赖于文本语料库，难以捕捉视觉概念之间的相似性和差异性，因此需要一种能够从视觉信息中学习词嵌入的方法",
        "方法": "从大规模文本注释的视觉数据库中提取对象和属性词之间的四种类型的视觉共现，并训练一个多任务对数双线性模型来紧凑地编码这些共现类型所代表的词“意义”为单个视觉词向量",
        "关键词": [
            "视觉共现",
            "词嵌入",
            "多任务学习",
            "视觉-语言任务"
        ],
        "涉及的技术概念": {
            "视觉共现": "两个词适用于同一图像或图像区域的情况",
            "词嵌入": "将词表示为向量空间中的点，以捕捉词之间的语义关系",
            "多任务对数双线性模型": "一种能够同时处理多个任务的模型，通过线性变换和双线性交互来学习特征表示",
            "无监督聚类": "不使用标签信息，仅根据数据本身的特征将数据分组",
            "有监督分区": "使用标签信息来指导数据的分组过程",
            "零样本式泛化分析": "评估模型在未见过的类别上的泛化能力",
            "视觉-语言任务": "涉及视觉信息和语言信息的任务，如图像标注、视觉问答等"
        }
    },
    {
        "order": 965,
        "title": "Re-ID Driven Localization Refinement for Person Search",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Re-ID_Driven_Localization_Refinement_for_Person_Search_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Re-ID_Driven_Localization_Refinement_for_Person_Search_ICCV_2019_paper.html",
        "abstract": "Person search aims at localizing and identifying a query person from a gallery of uncropped scene images. Different from person re-identification (re-ID), its performance also depends on the localization accuracy of a pedestrian detector. The state-of-the-art methods train the detector individually, and the detected bounding boxes may be sub-optimal for the following re-ID task. To alleviate this issue, we propose a re-ID driven localization refinement framework for providing the refined detection boxes for person search. Specifically, we develop a differentiable ROI transform layer to effectively transform the bounding boxes from the original images. Thus, the box coordinates can be supervised by the re-ID training other than the original detection task. With this supervision, the detector can generate more reliable bounding boxes, and the downstream re-ID model can produce more discriminative embeddings based on the refined person localizations. Extensive experimental results on the widely used benchmarks demonstrate that our proposed method performs favorably against the state-of-the-art person search methods.",
        "中文标题": "基于重识别驱动定位优化的人员搜索",
        "摘要翻译": "人员搜索旨在从未裁剪的场景图像库中定位并识别查询人员。与人员重识别（re-ID）不同，其性能还取决于行人检测器的定位准确性。目前最先进的方法单独训练检测器，检测到的边界框对于后续的重识别任务可能不是最优的。为了缓解这一问题，我们提出了一个重识别驱动的定位优化框架，为人员搜索提供优化的检测框。具体来说，我们开发了一个可微分的ROI变换层，以有效地从原始图像中变换边界框。因此，除了原始检测任务外，框坐标还可以通过重识别训练进行监督。在这种监督下，检测器可以生成更可靠的边界框，而下游的重识别模型可以基于优化的人员定位生成更具区分性的嵌入。在广泛使用的基准上的大量实验结果表明，我们提出的方法在人员搜索方面优于最先进的方法。",
        "领域": "行人检测/人员重识别/目标检测",
        "问题": "提高人员搜索中行人检测器的定位准确性",
        "动机": "当前方法单独训练检测器，导致检测到的边界框对于后续的重识别任务可能不是最优的，需要一种方法来优化检测框以提高重识别性能。",
        "方法": "提出了一个重识别驱动的定位优化框架，包括开发一个可微分的ROI变换层来变换边界框，并通过重识别训练对框坐标进行监督。",
        "关键词": [
            "行人检测",
            "人员重识别",
            "目标检测"
        ],
        "涉及的技术概念": {
            "人员搜索": "从未裁剪的场景图像库中定位并识别查询人员。",
            "行人检测器": "用于检测图像中行人的位置。",
            "重识别（re-ID）": "识别不同图像中的同一人员。",
            "ROI变换层": "一种可微分的层，用于从原始图像中变换边界框。",
            "边界框": "用于定位图像中目标的矩形框。"
        }
    },
    {
        "order": 966,
        "title": "Seq-SG2SL: Inferring Semantic Layout From Scene Graph Through Sequence to Sequence Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Seq-SG2SL_Inferring_Semantic_Layout_From_Scene_Graph_Through_Sequence_to_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Seq-SG2SL_Inferring_Semantic_Layout_From_Scene_Graph_Through_Sequence_to_ICCV_2019_paper.html",
        "abstract": "Generating semantic layout from scene graph is a crucial intermediate task connecting text to image. We present a conceptually simple, flexible and general framework using sequence to sequence (seq-to-seq) learning for this task. The framework, called Seq-SG2SL, derives sequence proxies for the two modality and a Transformer-based seq-to-seq model learns to transduce one into the other. A scene graph is decomposed into a sequence of semantic fragments (SF), one for each relationship. A semantic layout is represented as the consequence from a series of brick-action code segments (BACS), dictating the position and scale of each object bounding box in the layout. Viewing the two building blocks, SF and BACS, as corresponding terms in two different vocabularies, a seq-to-seq model is fittingly used to translate. A new metric, semantic layout evaluation understudy (SLEU), is devised to evaluate the task of semantic layout prediction inspired by BLEU. SLEU defines relationships within a layout as unigrams and looks at the spatial distribution for n-grams. Unlike the binary precision of BLEU, SLEU allows for some tolerances spatially through thresholding the Jaccard Index and is consequently more adapted to the task. Experimental results on the challenging Visual Genome dataset show improvement over a non-sequential approach based on graph convolution.",
        "中文标题": "Seq-SG2SL: 通过序列到序列学习从场景图推断语义布局",
        "摘要翻译": "从场景图生成语义布局是连接文本到图像的关键中间任务。我们提出了一个概念上简单、灵活且通用的框架，使用序列到序列（seq-to-seq）学习来完成这一任务。该框架名为Seq-SG2SL，它为两种模态派生序列代理，并基于Transformer的seq-to-seq模型学习将一个转换为另一个。场景图被分解为一系列语义片段（SF），每个关系一个。语义布局表示为一连串砖块动作代码段（BACS）的结果，这些代码段规定了布局中每个对象边界框的位置和比例。将这两个构建块SF和BACS视为两种不同词汇中的对应术语，seq-to-seq模型被恰当地用于翻译。受BLEU启发，设计了一种新的度量标准，语义布局评估研究（SLEU），用于评估语义布局预测任务。SLEU将布局内的关系定义为单字，并查看n-gram的空间分布。与BLEU的二进制精度不同，SLEU通过阈值化Jaccard指数在空间上允许一定的容忍度，因此更适应于该任务。在具有挑战性的Visual Genome数据集上的实验结果显示，与基于图卷积的非序列方法相比有所改进。",
        "领域": "语义布局预测/场景理解/序列到序列学习",
        "问题": "如何从场景图生成语义布局",
        "动机": "为了更有效地连接文本到图像，需要一个中间任务来生成语义布局，这有助于提高图像生成的准确性和相关性。",
        "方法": "使用序列到序列学习框架Seq-SG2SL，将场景图分解为语义片段（SF），并将语义布局表示为砖块动作代码段（BACS），通过Transformer模型进行转换。",
        "关键词": [
            "语义布局",
            "场景图",
            "序列到序列学习",
            "Transformer模型",
            "语义布局评估研究"
        ],
        "涉及的技术概念": {
            "序列到序列学习": "一种模型架构，用于将一个序列转换为另一个序列，常用于机器翻译等任务。",
            "Transformer模型": "一种基于自注意力机制的深度学习模型，适用于处理序列数据。",
            "语义片段（SF）": "场景图中每个关系的表示，用于构建语义布局。",
            "砖块动作代码段（BACS）": "描述语义布局中对象边界框位置和比例的代码段。",
            "语义布局评估研究（SLEU）": "一种新的度量标准，用于评估语义布局预测任务，通过考虑空间分布和Jaccard指数的阈值化来提高评估的适应性。"
        }
    },
    {
        "order": 967,
        "title": "Hierarchical Encoding of Sequential Data With Compact and Sub-Linear Storage Cost",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Hierarchical_Encoding_of_Sequential_Data_With_Compact_and_Sub-Linear_Storage_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Le_Hierarchical_Encoding_of_Sequential_Data_With_Compact_and_Sub-Linear_Storage_ICCV_2019_paper.html",
        "abstract": "Snapshot-based visual localization is an important problem in several computer vision and robotics applications such as Simultaneous Localization And Mapping (SLAM). To achieve real-time performance in very large-scale environments with massive amounts of training and map data, techniques such as approximate nearest neighbor search (ANN) algorithms are used. While several state-of-the-art variants of quantization and indexing techniques have demonstrated to be efficient in practice, their theoretical memory cost still scales at least linearly with the training data (i.e., O(n) where n is the number of instances in the database), since each data point must be associated with at least one code vector. To address these limitations, in this paper we present a totally new hierarchical encoding approach that enables a sub-linear storage scale. The algorithm exploits the widespread sequential nature of sensor information streams in robotics and autonomous vehicle applications and achieves, both theoretically and experimentally, sub-linear scalability in storage required for a given environment size. Furthermore, the associated query time of our algorithm is also of sub-linear complexity. We benchmark the performance of the proposed algorithm on several real-world benchmark datasets and experimentally validate the theoretical sub-linearity of our approach, while also showing that our approach yields competitive absolute storage performance as well.",
        "中文标题": "具有紧凑和次线性存储成本的序列数据分层编码",
        "摘要翻译": "基于快照的视觉定位是几个计算机视觉和机器人应用中的重要问题，如同时定位与地图构建（SLAM）。为了在具有大量训练和地图数据的非常大规模环境中实现实时性能，使用了近似最近邻搜索（ANN）算法等技术。虽然几种最先进的量化和索引技术变体在实践中已被证明是有效的，但它们的理论内存成本仍然至少与训练数据线性扩展（即O(n)，其中n是数据库中的实例数量），因为每个数据点必须与至少一个代码向量相关联。为了解决这些限制，本文提出了一种全新的分层编码方法，实现了次线性存储规模。该算法利用了机器人和自动驾驶车辆应用中传感器信息流的广泛序列性质，并在理论上和实验上实现了给定环境大小所需的存储次线性扩展。此外，我们算法的相关查询时间也具有次线性复杂度。我们在几个真实世界的基准数据集上对提出的算法进行了性能基准测试，并实验验证了我们方法的理论次线性，同时也展示了我们的方法在绝对存储性能上也具有竞争力。",
        "领域": "视觉定位/机器人/自动驾驶",
        "问题": "在大规模环境中实现实时视觉定位的存储成本问题",
        "动机": "解决现有技术在处理大规模训练和地图数据时存储成本线性增长的问题",
        "方法": "提出了一种全新的分层编码方法，利用传感器信息流的序列性质，实现次线性存储规模",
        "关键词": [
            "视觉定位",
            "分层编码",
            "次线性存储"
        ],
        "涉及的技术概念": "近似最近邻搜索（ANN）算法、量化和索引技术、分层编码方法、次线性存储规模"
    },
    {
        "order": 968,
        "title": "U-CAM: Visual Explanation Using Uncertainty Based Class Activation Maps",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.html",
        "abstract": "Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods.",
        "中文标题": "U-CAM: 使用基于不确定性的类激活图进行视觉解释",
        "摘要翻译": "理解和解释深度学习模型是一项迫切的任务。为此，我们提出了一种方法，该方法获得基于梯度的确定性估计，同时也提供视觉注意力图。特别是，我们解决了视觉问答任务。我们结合了现代概率深度学习方法，通过使用这些估计的梯度进一步改进。这有两个好处：a) 在获得与错误分类样本更好相关的确定性估计方面有所改进，b) 提供了与人类注意力区域相关性方面最先进结果的改进注意力图。改进的注意力图导致各种视觉问答方法的持续改进。因此，所提出的技术可以被视为获得改进的确定性估计和深度学习模型解释的配方。我们提供了对所有标准基准上的视觉问答任务的详细实证分析，并与最先进的方法进行了比较。",
        "领域": "视觉问答/深度学习解释性/注意力机制",
        "问题": "如何提高深度学习模型在视觉问答任务中的解释性和准确性",
        "动机": "为了理解和解释深度学习模型，提高模型在视觉问答任务中的表现和解释性",
        "方法": "提出了一种基于梯度的确定性估计方法，结合现代概率深度学习方法，通过使用梯度改进估计，从而获得改进的确定性估计和注意力图",
        "关键词": [
            "视觉问答",
            "深度学习解释性",
            "注意力机制",
            "概率深度学习",
            "梯度估计"
        ],
        "涉及的技术概念": "类激活图（CAM）是一种用于解释深度学习模型决策的技术，通过可视化模型在做出决策时关注的图像区域。基于不确定性的类激活图（U-CAM）进一步引入了不确定性估计，以提高解释的准确性和可靠性。视觉问答任务要求模型根据图像内容回答自然语言问题，这需要模型不仅理解图像内容，还要理解问题的语义。注意力机制是一种使模型能够专注于输入数据中重要部分的技术，对于提高模型性能至关重要。概率深度学习方法涉及在模型预测中引入概率分布，以处理不确定性和提高模型的鲁棒性。"
    },
    {
        "order": 969,
        "title": "C-MIDN: Coupled Multiple Instance Detection Network With Segmentation Guidance for Weakly Supervised Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_C-MIDN_Coupled_Multiple_Instance_Detection_Network_With_Segmentation_Guidance_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gao_C-MIDN_Coupled_Multiple_Instance_Detection_Network_With_Segmentation_Guidance_for_ICCV_2019_paper.html",
        "abstract": "Weakly supervised object detection (WSOD) that only needs image-level annotations has obtained much attention recently. By combining convolutional neural network with multiple instance learning method, Multiple Instance Detection Network (MIDN) has become the most popular method to address the WSOD problem and been adopted as the initial model in many works. We argue that MIDN inclines to converge to the most discriminative object parts, which limits the performance of methods based on it. In this paper, we propose a novel Coupled Multiple Instance Detection Network (C-MIDN) to address this problem. Specifically, we use a pair of MIDNs, which work in a complementary manner with proposal removal. The localization information of the MIDNs is further coupled to obtain tighter bounding boxes and localize multiple objects. We also introduce a Segmentation Guided Proposal Removal (SGPR) algorithm to guarantee the MIL constraint after the removal and ensure the robustness of C-MIDN. Through a simple implementation of the C-MIDN with online detector refinement, we obtain 53.6% and 50.3% mAP on the challenging PASCAL VOC 2007 and 2012 benchmarks respectively, which significantly outperform the previous state-of-the-arts.",
        "中文标题": "C-MIDN：具有分割指导的耦合多实例检测网络用于弱监督目标检测",
        "摘要翻译": "仅需要图像级注释的弱监督目标检测（WSOD）最近获得了大量关注。通过将卷积神经网络与多实例学习方法结合，多实例检测网络（MIDN）已成为解决WSOD问题的最流行方法，并被许多工作采用为初始模型。我们认为MIDN倾向于收敛到最具区分性的对象部分，这限制了基于它的方法的性能。在本文中，我们提出了一种新颖的耦合多实例检测网络（C-MIDN）来解决这个问题。具体来说，我们使用一对MIDN，它们以互补的方式工作，并带有提议移除。MIDN的定位信息进一步耦合以获得更紧密的边界框并定位多个对象。我们还引入了一种分割指导的提议移除（SGPR）算法，以确保移除后的MIL约束并确保C-MIDN的鲁棒性。通过对C-MIDN的简单实现和在线检测器细化，我们在具有挑战性的PASCAL VOC 2007和2012基准测试中分别获得了53.6%和50.3%的mAP，显著优于之前的最先进技术。",
        "领域": "目标检测/弱监督学习/图像分割",
        "问题": "解决弱监督目标检测中MIDN倾向于收敛到最具区分性的对象部分，从而限制性能的问题",
        "动机": "提高弱监督目标检测的性能，通过解决MIDN的局限性",
        "方法": "提出了一种新颖的耦合多实例检测网络（C-MIDN），使用一对互补的MIDN，并引入分割指导的提议移除（SGPR）算法",
        "关键词": [
            "弱监督目标检测",
            "多实例检测网络",
            "分割指导提议移除"
        ],
        "涉及的技术概念": {
            "弱监督目标检测（WSOD）": "一种仅需要图像级注释的目标检测方法",
            "多实例检测网络（MIDN）": "结合卷积神经网络与多实例学习方法，用于解决WSOD问题",
            "分割指导的提议移除（SGPR）算法": "一种算法，用于确保移除提议后的MIL约束，并提高C-MIDN的鲁棒性",
            "PASCAL VOC": "一个广泛使用的计算机视觉数据集，用于目标检测等任务"
        }
    },
    {
        "order": 970,
        "title": "See-Through-Text Grouping for Referring Image Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_See-Through-Text_Grouping_for_Referring_Image_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_See-Through-Text_Grouping_for_Referring_Image_Segmentation_ICCV_2019_paper.html",
        "abstract": "Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments.",
        "中文标题": "透视文本分组用于参考图像分割",
        "摘要翻译": "受到传统图像分割分组技术的启发，我们开发了其深度神经网络（DNN）对应物以解决参考变体。所提出的方法由卷积循环神经网络（ConvRNN）驱动，该网络迭代地执行自下而上的分割线索的顶部处理。给定一个自然语言参考表达式，我们的方法学习预测其与每个像素的相关性，并得出一个透视文本嵌入像素级（STEP）热图，该热图通过学习的视觉-文本共嵌入揭示了像素级的分割线索。ConvRNN通过将STEP热图转换为一个精炼的热图来执行顶部近似，而改进则来自于用来自真实值的分类损失训练网络。通过精炼的热图，我们通过重新评估其注意力分布来更新参考表达式的文本表示，然后计算一个新的STEP热图作为ConvRNN的下一个输入。通过这种协作学习的推动，该框架可以逐步并同时产生所需的参考分割和参考句子上的合理注意力分布。我们的方法是通用的，不依赖于其他DNN模型的对象检测结果，同时在实验中的所有四个数据集中实现了最先进的性能。",
        "领域": "图像分割/自然语言处理/卷积循环神经网络",
        "问题": "解决参考图像分割中的透视文本分组问题",
        "动机": "受到传统图像分割分组技术的启发，开发深度神经网络对应物以解决参考变体",
        "方法": "采用卷积循环神经网络（ConvRNN）迭代处理自下而上的分割线索，通过学习的视觉-文本共嵌入揭示像素级的分割线索，并通过精炼热图和更新文本表示来逐步改进分割结果",
        "关键词": [
            "图像分割",
            "自然语言处理",
            "卷积循环神经网络"
        ],
        "涉及的技术概念": {
            "卷积循环神经网络（ConvRNN）": "一种结合卷积神经网络和循环神经网络的网络结构，用于处理序列数据并捕捉空间特征",
            "透视文本嵌入像素级（STEP）热图": "通过视觉-文本共嵌入学习得到的像素级分割线索表示",
            "注意力分布": "在自然语言处理中，用于表示模型在处理文本时对不同部分的关注程度"
        }
    },
    {
        "order": 971,
        "title": "Learning Feature-to-Feature Translator by Alternating Back-Propagation for Generative Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Learning_Feature-to-Feature_Translator_by_Alternating_Back-Propagation_for_Generative_Zero-Shot_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhu_Learning_Feature-to-Feature_Translator_by_Alternating_Back-Propagation_for_Generative_Zero-Shot_Learning_ICCV_2019_paper.html",
        "abstract": "We investigate learning feature-to-feature translator networks by alternating back-propagation as a general-purpose solution to zero-shot learning (ZSL) problems. It is a generative model-based ZSL framework. In contrast to models based on generative adversarial networks (GAN) or variational autoencoders (VAE) that require auxiliary networks to assist the training, our model consists of a single conditional generator that maps class-level semantic features and Gaussian white noise vectors accounting for instance-level latent factors to visual features, and is trained by maximum likelihood estimation. The training process is a simple yet effective alternating back-propagation process that iterates the following two steps: (i) the inferential back-propagation to infer the latent noise vector of each observed example, and (ii) the learning back-propagation to update the model parameters. We show that, with slight modifications, our model is capable of learning from incomplete visual features for ZSL. We conduct extensive comparisons with existing generative ZSL methods on five benchmarks, demonstrating the superiority of our method in not only ZSL performance but also convergence speed and computational cost. Specifically, our model outperforms the existing state-of-the-art methods by a remarkable margin up to 3.1% and 4.0% in ZSL and generalized ZSL settings, respectively.",
        "中文标题": "通过交替反向传播学习特征到特征翻译器用于生成零样本学习",
        "摘要翻译": "我们研究了通过交替反向传播学习特征到特征翻译器网络，作为零样本学习（ZSL）问题的通用解决方案。这是一个基于生成模型的ZSL框架。与基于生成对抗网络（GAN）或变分自编码器（VAE）的模型相比，这些模型需要辅助网络来协助训练，而我们的模型由一个单一的条件生成器组成，该生成器将类级语义特征和用于实例级潜在因素的高斯白噪声向量映射到视觉特征，并通过最大似然估计进行训练。训练过程是一个简单而有效的交替反向传播过程，它迭代以下两个步骤：（i）推断反向传播以推断每个观察到的例子的潜在噪声向量，和（ii）学习反向传播以更新模型参数。我们展示了，通过轻微修改，我们的模型能够从不完整的视觉特征中学习用于ZSL。我们在五个基准上与现有的生成ZSL方法进行了广泛的比较，证明了我们的方法不仅在ZSL性能上优越，而且在收敛速度和计算成本上也表现出色。具体来说，我们的模型在ZSL和广义ZSL设置中分别以高达3.1%和4.0%的显著优势超越了现有的最先进方法。",
        "领域": "零样本学习/生成模型/特征翻译",
        "问题": "解决零样本学习中的特征到特征翻译问题",
        "动机": "开发一种无需辅助网络即可训练的生成模型，以提高零样本学习的性能和效率",
        "方法": "采用交替反向传播方法训练一个单一的条件生成器，该生成器将类级语义特征和实例级潜在因素映射到视觉特征",
        "关键词": [
            "零样本学习",
            "生成模型",
            "特征翻译",
            "交替反向传播",
            "条件生成器"
        ],
        "涉及的技术概念": "交替反向传播是一种训练方法，通过交替执行推断反向传播和学习反向传播来优化模型参数。条件生成器是一种能够根据特定条件生成数据的模型，这里用于将语义特征和噪声向量映射到视觉特征。最大似然估计是一种参数估计方法，用于找到使观察数据概率最大化的模型参数。"
    },
    {
        "order": 972,
        "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.html",
        "abstract": "Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",
        "中文标题": "VideoBERT：视频和语言表示学习的联合模型",
        "摘要翻译": "自监督学习在利用如YouTube等平台上大量未标记数据方面变得越来越重要。尽管大多数现有方法学习低级表示，我们提出了一种联合视觉-语言模型，以在没有任何显式监督的情况下学习高级特征。特别是，受到其在语言建模中最近成功的启发，我们基于BERT模型，学习视觉和语言标记序列的双向联合分布，这些标记分别来自视频数据的矢量量化和现成的语音识别输出。我们在多项任务中使用VideoBERT，包括动作分类和视频字幕生成。我们展示了它可以直接应用于开放词汇分类，并确认大量训练数据和跨模态信息对性能至关重要。此外，我们在视频字幕生成上超越了现有技术，定量结果验证了模型学习到的高级语义特征。",
        "领域": "视频理解/自然语言处理/自监督学习",
        "问题": "如何在没有显式监督的情况下学习视频和语言的高级特征表示",
        "动机": "利用YouTube等平台上大量未标记数据进行自监督学习，以学习视频和语言的高级特征表示",
        "方法": "基于BERT模型，学习视觉和语言标记序列的双向联合分布，这些标记分别来自视频数据的矢量量化和现成的语音识别输出",
        "关键词": [
            "自监督学习",
            "视频理解",
            "自然语言处理"
        ],
        "涉及的技术概念": "自监督学习是一种利用未标记数据的学习方法，BERT模型是一种基于变换器的深度学习模型，用于自然语言处理任务，矢量量化是一种将连续信号转换为离散信号的过程，语音识别是将语音转换为文本的技术。"
    },
    {
        "order": 973,
        "title": "Deep Constrained Dominant Sets for Person Re-Identification",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Alemu_Deep_Constrained_Dominant_Sets_for_Person_Re-Identification_ICCV_2019_paper.html",
        "abstract": "In this work, we propose an end-to-end constrained clustering scheme to tackle the person re-identification (re-id) problem. Deep neural networks (DNN) have recently proven to be effective on person re-identification task. In particular, rather than leveraging solely a probe-gallery similarity, diffusing the similarities among the gallery images in an end-to-end manner has proven to be effective in yielding a robust probe-gallery affinity. However, existing methods do not apply probe image as a constraint, and are prone to noise propagation during the similarity diffusion process. To overcome this, we propose an intriguing scheme which treats person-image retrieval problem as a constrained clustering optimization problem, called deep constrained dominant sets (DCDS). Given a probe and gallery images, we re-formulate person re-id problem as finding a constrained cluster, where the probe image is taken as a constraint (seed) and each cluster corresponds to a set of images corresponding to the same person. By optimizing the constrained clustering in an end-to-end manner, we naturally leverage the contextual knowledge of a set of images corresponding to the given person-images. We further enhance the performance by integrating an auxiliary net alongside DCDS, which employs a multi-scale ResNet. To validate the effectiveness of our method we present experiments on several benchmark datasets and show that the proposed method can outperform state-of-the-art methods.",
        "中文标题": "深度约束主导集用于行人重识别",
        "摘要翻译": "在本工作中，我们提出了一种端到端的约束聚类方案来解决行人重识别（re-id）问题。深度神经网络（DNN）最近已被证明在行人重识别任务中是有效的。特别是，与仅利用探针-画廊相似性相比，以端到端的方式在画廊图像之间扩散相似性已被证明在产生稳健的探针-画廊亲和力方面是有效的。然而，现有方法并未将探针图像作为约束应用，并且在相似性扩散过程中容易受到噪声传播的影响。为了克服这一点，我们提出了一种有趣的方案，将行人图像检索问题视为一个约束聚类优化问题，称为深度约束主导集（DCDS）。给定探针和画廊图像，我们将行人重识别问题重新表述为寻找一个约束聚类，其中探针图像被视为约束（种子），每个聚类对应于与同一人对应的一组图像。通过以端到端的方式优化约束聚类，我们自然地利用了与给定行人图像对应的一组图像的上下文知识。我们通过集成一个辅助网络与DCDS一起进一步提高了性能，该辅助网络采用了多尺度ResNet。为了验证我们方法的有效性，我们在几个基准数据集上进行了实验，并展示了所提出的方法可以超越最先进的方法。",
        "领域": "行人重识别/聚类优化/图像检索",
        "问题": "解决行人重识别中的噪声传播和探针图像约束应用问题",
        "动机": "现有方法在行人重识别中未将探针图像作为约束应用，并且在相似性扩散过程中容易受到噪声传播的影响，需要一种新的方法来克服这些问题。",
        "方法": "提出了一种端到端的约束聚类方案，称为深度约束主导集（DCDS），将行人图像检索问题视为一个约束聚类优化问题，并集成一个采用多尺度ResNet的辅助网络以提高性能。",
        "关键词": [
            "行人重识别",
            "约束聚类",
            "图像检索"
        ],
        "涉及的技术概念": "深度神经网络（DNN）、端到端学习、约束聚类优化、多尺度ResNet、探针-画廊相似性、噪声传播、上下文知识利用"
    },
    {
        "order": 974,
        "title": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ji_Invariant_Information_Clustering_for_Unsupervised_Image_Classification_and_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ji_Invariant_Information_Clustering_for_Unsupervised_Image_Classification_and_Segmentation_ICCV_2019_paper.html",
        "abstract": "We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC",
        "中文标题": "不变信息聚类用于无监督图像分类和分割",
        "摘要翻译": "我们提出了一种新颖的聚类目标，该目标仅给定未标记的数据样本，从头开始学习神经网络分类器。该模型发现的聚类准确匹配语义类别，在涵盖图像分类和分割的八个无监督聚类基准测试中取得了最先进的结果。这些包括STL10、ImageNet的无监督变体和CIFAR10，在这些测试中，我们分别显著超越了最接近的竞争对手的准确率6.6和9.5个百分点。该方法不仅限于计算机视觉，可以操作于任何配对的数据集样本；在我们的实验中，我们使用随机变换从每张图像中获取一对。训练后的网络直接输出语义标签，而不是需要外部处理才能用于语义聚类的高维表示。目标仅仅是最大化每对类别分配之间的互信息。它易于实现，并且在信息论中有严格的基础，意味着我们轻松避免了其他聚类方法容易出现的退化解决方案。除了完全无监督的模式外，我们还测试了两种半监督设置。第一种在STL10分类上达到了88.8%的准确率，为所有现有方法（无论是监督、半监督还是无监督）设定了新的全球最先进水平。第二种显示了对标签覆盖率减少90%的鲁棒性，这对于希望利用少量标签的应用具有重要意义。github.com/xu-ji/IIC",
        "领域": "无监督学习/图像分类/图像分割",
        "问题": "如何在无监督的情况下准确匹配语义类别进行图像分类和分割",
        "动机": "为了在无监督学习环境中实现图像分类和分割，需要一种能够从头开始学习神经网络分类器的方法，该方法能够准确匹配语义类别，并在多个基准测试中取得最先进的结果。",
        "方法": "提出了一种新颖的聚类目标，通过最大化每对类别分配之间的互信息来学习神经网络分类器，该方法易于实现，并且在信息论中有严格的基础。",
        "关键词": [
            "无监督学习",
            "图像分类",
            "图像分割",
            "互信息",
            "神经网络"
        ],
        "涉及的技术概念": "互信息用于衡量两个变量之间的相互依赖程度，在这里用于最大化每对类别分配之间的互信息，以实现准确的语义类别匹配。神经网络分类器从头开始学习，直接输出语义标签，而不是需要进一步处理的高维表示。"
    },
    {
        "order": 975,
        "title": "Language Features Matter: Effective Language Representations for Vision-Language Tasks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Burns_Language_Features_Matter_Effective_Language_Representations_for_Vision-Language_Tasks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Burns_Language_Features_Matter_Effective_Language_Representations_for_Vision-Language_Tasks_ICCV_2019_paper.html",
        "abstract": "Shouldn't language and vision features be treated equally in vision-language (VL) tasks? Many VL approaches treat the language component as an afterthought, using simple language models that are either built upon fixed word embeddings trained on text-only data or are learned from scratch. We conclude that language features deserve more attention, which has been informed by experiments which compare different word embeddings, language models, and embedding augmentation steps on five common VL tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. Our experiments provide some striking results; an average embedding language model outperforms a LSTM on retrieval-style tasks; state-of-the-art representations such as BERT perform relatively poorly on vision-language tasks. From this comprehensive set of experiments we can propose a set of best practices for incorporating the language component of vision-language tasks. To further elevate language features, we also show that knowledge in vision-language problems can be transferred across tasks to gain performance with multi-task training. This multi-task training is applied to a new Graph Oriented Vision-Language Embedding (GrOVLE), which we adapt from Word2Vec using WordNet and an original visual-language graph built from Visual Genome, providing a ready-to-use vision-language embedding: http://ai.bu.edu/grovle.",
        "中文标题": "语言特征的重要性：视觉-语言任务中的有效语言表示",
        "摘要翻译": "在视觉-语言（VL）任务中，语言和视觉特征是否应该被平等对待？许多VL方法将语言组件视为事后考虑，使用简单的语言模型，这些模型要么基于仅在文本数据上训练的固定词嵌入，要么是从头开始学习的。我们通过实验得出结论，语言特征值得更多关注，这些实验比较了五种常见VL任务中的不同词嵌入、语言模型和嵌入增强步骤：图像-句子检索、图像描述、视觉问答、短语定位和文本到剪辑检索。我们的实验提供了一些引人注目的结果；在检索式任务中，平均嵌入语言模型优于LSTM；像BERT这样的最先进表示在视觉-语言任务中表现相对较差。从这一系列全面的实验中，我们可以提出一套最佳实践，用于整合视觉-语言任务的语言组件。为了进一步提升语言特征，我们还展示了视觉-语言问题中的知识可以通过多任务训练跨任务转移以获得性能提升。这种多任务训练应用于一种新的图导向视觉-语言嵌入（GrOVLE），我们使用WordNet和从Visual Genome构建的原始视觉-语言图从Word2Vec改编而来，提供了一个即用型视觉-语言嵌入：http://ai.bu.edu/grovle。",
        "领域": "视觉-语言理解/多模态学习/自然语言处理",
        "问题": "在视觉-语言任务中，如何更有效地利用语言特征",
        "动机": "许多现有的视觉-语言方法未能充分重视语言组件，导致性能受限",
        "方法": "比较不同词嵌入、语言模型和嵌入增强步骤的效果，提出最佳实践，并通过多任务训练提升语言特征",
        "关键词": [
            "视觉-语言理解",
            "多模态学习",
            "自然语言处理"
        ],
        "涉及的技术概念": {
            "视觉-语言任务": "涉及视觉和语言信息的任务，如图像-句子检索、图像描述、视觉问答等",
            "词嵌入": "将词汇转换为固定大小的向量表示，以便于计算机处理",
            "语言模型": "用于预测或生成语言序列的模型",
            "多任务训练": "同时训练模型以解决多个相关任务，以提高模型的泛化能力",
            "图导向视觉-语言嵌入（GrOVLE）": "一种新的视觉-语言嵌入方法，通过结合WordNet和Visual Genome构建的视觉-语言图来增强语言特征"
        }
    },
    {
        "order": 976,
        "title": "Subspace Structure-Aware Spectral Clustering for Robust Subspace Clustering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yamaguchi_Subspace_Structure-Aware_Spectral_Clustering_for_Robust_Subspace_Clustering_ICCV_2019_paper.html",
        "abstract": "Subspace clustering is the problem of partitioning data drawn from a union of multiple subspaces. The most popular subspace clustering framework in recent years is the graph clustering-based approach, which performs subspace clustering in two steps: graph construction and graph clustering. Although both steps are equally important for accurate clustering, the vast majority of work has focused on improving the graph construction step rather than the graph clustering step. In this paper, we propose a novel graph clustering framework for robust subspace clustering. By incorporating a geometry-aware term with the spectral clustering objective, we encourage our framework to be robust to noise and outliers in given affinity matrices. We also develop an efficient expectation-maximization-based algorithm for optimization. Through extensive experiments on four real-world datasets, we demonstrate that the proposed method outperforms existing methods.",
        "中文标题": "子空间结构感知的谱聚类用于鲁棒子空间聚类",
        "摘要翻译": "子空间聚类是将来自多个子空间的数据进行划分的问题。近年来最流行的子空间聚类框架是基于图聚类的方法，该方法通过两个步骤执行子空间聚类：图构建和图聚类。尽管这两个步骤对于准确聚类同等重要，但绝大多数工作都集中在改进图构建步骤而非图聚类步骤。在本文中，我们提出了一种新颖的图聚类框架用于鲁棒子空间聚类。通过将几何感知项与谱聚类目标相结合，我们鼓励我们的框架对给定亲和矩阵中的噪声和异常值具有鲁棒性。我们还开发了一种基于期望最大化的高效算法进行优化。通过在四个真实世界数据集上的广泛实验，我们证明了所提出的方法优于现有方法。",
        "领域": "子空间聚类/谱聚类/图聚类",
        "问题": "提高子空间聚类的鲁棒性，特别是在图聚类步骤中",
        "动机": "现有方法大多关注图构建步骤的改进，而忽视了图聚类步骤的重要性，导致对噪声和异常值的鲁棒性不足",
        "方法": "提出了一种新颖的图聚类框架，通过将几何感知项与谱聚类目标相结合，并开发了一种基于期望最大化的高效算法进行优化",
        "关键词": [
            "子空间聚类",
            "谱聚类",
            "图聚类",
            "鲁棒性",
            "几何感知",
            "期望最大化"
        ],
        "涉及的技术概念": "子空间聚类是一种将数据划分为来自多个子空间的技术。谱聚类是一种基于图论的聚类方法，通过图的谱（即图的拉普拉斯矩阵的特征值）来进行聚类。图聚类是指将图中的节点划分为若干组，使得组内节点之间的连接比组间节点的连接更紧密。几何感知项是指在聚类过程中考虑数据的几何结构，以提高聚类的准确性和鲁棒性。期望最大化算法是一种迭代优化算法，用于寻找统计模型参数的最大似然估计。"
    },
    {
        "order": 977,
        "title": "Semantic Stereo Matching With Pyramid Cost Volumes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Semantic_Stereo_Matching_With_Pyramid_Cost_Volumes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Semantic_Stereo_Matching_With_Pyramid_Cost_Volumes_ICCV_2019_paper.html",
        "abstract": "The accuracy of stereo matching has been greatly improved by using deep learning with convolutional neural networks. To further capture the details of disparity maps, in this paper, we propose a novel semantic stereo network named SSPCV-Net, which includes newly designed pyramid cost volumes for describing semantic and spatial information on multiple levels. The semantic features are inferred by a semantic segmentation subnetwork while the spatial features are derived by hierarchical spatial pooling. In the end, we design a 3D multi-cost aggregation module to integrate the extracted multilevel features and perform regression for accurate disparity maps. We conduct comprehensive experiments and comparisons with some recent stereo matching networks on Scene Flow, KITTI 2015 and 2012, and Cityscapes benchmark datasets, and the results show that the proposed SSPCV-Net significantly promotes the state-of-the-art stereo-matching performance.",
        "中文标题": "使用金字塔成本体积的语义立体匹配",
        "摘要翻译": "通过使用深度学习和卷积神经网络，立体匹配的准确性得到了极大的提高。为了进一步捕捉视差图的细节，本文提出了一种名为SSPCV-Net的新型语义立体网络，它包括新设计的金字塔成本体积，用于描述多个层次上的语义和空间信息。语义特征由语义分割子网络推断，而空间特征则通过分层空间池化得出。最后，我们设计了一个3D多成本聚合模块，以整合提取的多层次特征，并执行回归以生成准确的视差图。我们在Scene Flow、KITTI 2015和2012以及Cityscapes基准数据集上进行了全面的实验和与一些最近的立体匹配网络的比较，结果表明，所提出的SSPCV-Net显著提升了立体匹配的最新技术水平。",
        "领域": "立体视觉/语义分割/视差估计",
        "问题": "提高立体匹配的准确性，捕捉视差图的细节",
        "动机": "为了进一步捕捉视差图的细节，提高立体匹配的准确性",
        "方法": "提出了一种新型语义立体网络SSPCV-Net，包括新设计的金字塔成本体积，用于描述多个层次上的语义和空间信息，并设计了一个3D多成本聚合模块，以整合提取的多层次特征，执行回归以生成准确的视差图",
        "关键词": [
            "立体匹配",
            "语义分割",
            "视差估计",
            "金字塔成本体积",
            "3D多成本聚合"
        ],
        "涉及的技术概念": {
            "金字塔成本体积": "用于描述多个层次上的语义和空间信息的技术",
            "语义分割子网络": "用于推断语义特征的网络部分",
            "分层空间池化": "用于得出空间特征的技术",
            "3D多成本聚合模块": "用于整合提取的多层次特征并执行回归以生成准确视差图的模块"
        }
    },
    {
        "order": 978,
        "title": "Learning Relationships for Multi-View 3D Object Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_Relationships_for_Multi-View_3D_Object_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Learning_Relationships_for_Multi-View_3D_Object_Recognition_ICCV_2019_paper.html",
        "abstract": "Recognizing 3D object has attracted plenty of attention recently, and view-based methods have achieved best results until now. However, previous view-based methods ignore the region-to-region and view-to-view relationships between different view images, which are crucial for multi-view 3D object representation. To tackle this problem, we propose a Relation Network to effectively connect corresponding regions from different viewpoints, and therefore reinforce the information of individual view image. In addition, the Relation Network exploits the inter-relationships over a group of views, and integrates those views to obtain a discriminative 3D object representation. Systematic experiments conducted on ModelNet dataset demonstrate the effectiveness of our proposed methods for both 3D object recognition and retrieval tasks.",
        "中文标题": "学习多视图3D物体识别的关系",
        "摘要翻译": "近年来，3D物体识别吸引了大量关注，基于视图的方法迄今为止取得了最佳结果。然而，以往的基于视图的方法忽略了不同视图图像之间的区域到区域和视图到视图的关系，这些关系对于多视图3D物体表示至关重要。为了解决这个问题，我们提出了一个关系网络，以有效地连接来自不同视角的相应区域，从而增强单个视图图像的信息。此外，关系网络利用了一组视图之间的相互关系，并整合这些视图以获得一个具有区分性的3D物体表示。在ModelNet数据集上进行的系统实验证明了我们提出的方法在3D物体识别和检索任务中的有效性。",
        "领域": "3D物体识别/多视图学习/关系网络",
        "问题": "如何有效地利用不同视图图像之间的区域到区域和视图到视图的关系来增强3D物体识别和检索的性能",
        "动机": "以往的基于视图的方法忽略了不同视图图像之间的关键关系，这些关系对于多视图3D物体表示至关重要",
        "方法": "提出了一个关系网络，以有效地连接来自不同视角的相应区域，并利用一组视图之间的相互关系来整合这些视图，从而获得一个具有区分性的3D物体表示",
        "关键词": [
            "3D物体识别",
            "多视图学习",
            "关系网络"
        ],
        "涉及的技术概念": "关系网络是一种用于连接不同视角的相应区域并利用视图间相互关系的技术，旨在增强3D物体识别和检索的性能。ModelNet数据集是用于3D物体识别和检索任务的一个常用数据集。"
    },
    {
        "order": 979,
        "title": "Order-Preserving Wasserstein Discriminant Analysis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Su_Order-Preserving_Wasserstein_Discriminant_Analysis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Su_Order-Preserving_Wasserstein_Discriminant_Analysis_ICCV_2019_paper.html",
        "abstract": "Supervised dimensionality reduction for sequence data projects the observations in sequences onto a low-dimensional subspace to better separate different sequence classes. It is typically more challenging than conventional dimensionality reduction for static data, because measuring the separability of sequences involves non-linear procedures to manipulate the temporal structures. This paper presents a linear method, namely Order-preserving Wasserstein Discriminant Analysis (OWDA), which learns the projection by maximizing the inter-class distance and minimizing the intra-class scatter. For each class, OWDA extracts the order-preserving Wasserstein barycenter and constructs the intra-class scatter as the dispersion of the training sequences around the barycenter. The inter-class distance is measured as the order-preserving Wasserstein distance between the corresponding barycenters. OWDA is able to concentrate on the distinctive differences among classes by lifting the geometric relations with temporal constraints. Experiments show that OWDA achieves competitive results on three 3D action recognition datasets.",
        "中文标题": "保序Wasserstein判别分析",
        "摘要翻译": "序列数据的监督降维将序列中的观测投影到低维子空间，以更好地分离不同的序列类别。这通常比静态数据的常规降维更具挑战性，因为测量序列的可分性涉及操作时间结构的非线性过程。本文提出了一种线性方法，即保序Wasserstein判别分析（OWDA），该方法通过最大化类间距离和最小化类内散度来学习投影。对于每个类别，OWDA提取保序Wasserstein重心，并构建类内散度作为训练序列围绕重心的分散度。类间距离被测量为相应重心之间的保序Wasserstein距离。OWDA能够通过提升具有时间约束的几何关系来集中关注类别间的显著差异。实验表明，OWDA在三个3D动作识别数据集上取得了竞争性的结果。",
        "领域": "动作识别/序列数据分析/降维技术",
        "问题": "如何在保持序列数据时间结构的同时，有效地进行监督降维以分离不同的序列类别",
        "动机": "序列数据的监督降维比静态数据更具挑战性，因为需要操作时间结构的非线性过程来测量序列的可分性",
        "方法": "提出了一种线性方法，即保序Wasserstein判别分析（OWDA），通过最大化类间距离和最小化类内散度来学习投影，并利用保序Wasserstein距离和重心来测量类间和类内关系",
        "关键词": [
            "监督降维",
            "序列数据",
            "保序Wasserstein距离",
            "动作识别"
        ],
        "涉及的技术概念": "保序Wasserstein判别分析（OWDA）是一种线性降维方法，它通过最大化类间距离和最小化类内散度来学习投影。该方法利用保序Wasserstein距离来测量类间距离，并提取每个类别的保序Wasserstein重心来构建类内散度。这种方法特别适用于处理具有时间结构的序列数据，如3D动作识别。"
    },
    {
        "order": 980,
        "title": "View N-Gram Network for 3D Object Retrieval",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/He_View_N-Gram_Network_for_3D_Object_Retrieval_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/He_View_N-Gram_Network_for_3D_Object_Retrieval_ICCV_2019_paper.html",
        "abstract": "How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.",
        "中文标题": "视图N-Gram网络用于3D物体检索",
        "摘要翻译": "如何将3D物体的多视图表示聚合成一个信息丰富且具有区分性的表示，仍然是多视图3D物体检索的一个关键挑战。现有方法要么使用视图级池化策略，忽略了不同视图之间的空间信息，要么采用循环神经网络，可能面临效率问题。为了解决这些问题，我们提出了一个有效且高效的框架，称为视图N-Gram网络（VNN）。受自然语言处理中n-gram模型的启发，VNN将视图序列划分为一组视觉n-gram，这些n-gram涉及重叠的连续视图子序列。通过这样做，捕获了多个视图之间的空间信息，有助于为每个3D物体学习一个具有区分性的全局嵌入。在3D形状检索基准测试上的实验，包括ModelNet10、ModelNet40和ShapeNetCore55数据集，证明了我们提出方法的优越性。",
        "领域": "3D物体检索/多视图表示/空间信息捕获",
        "问题": "如何有效地聚合3D物体的多视图表示，以生成信息丰富且具有区分性的表示",
        "动机": "现有方法在聚合多视图表示时，要么忽略了视图间的空间信息，要么效率低下，需要一种新的方法来解决这些问题",
        "方法": "提出视图N-Gram网络（VNN），通过将视图序列划分为重叠的连续视图子序列（视觉n-gram），捕获视图间的空间信息，从而学习到具有区分性的全局嵌入",
        "关键词": [
            "3D物体检索",
            "多视图表示",
            "空间信息捕获",
            "视图N-Gram网络",
            "全局嵌入"
        ],
        "涉及的技术概念": "视图N-Gram网络（VNN）是一种受自然语言处理中n-gram模型启发的框架，用于3D物体检索。它通过将视图序列划分为重叠的连续视图子序列（称为视觉n-gram），来捕获多个视图之间的空间信息，从而为每个3D物体学习一个具有区分性的全局嵌入。这种方法旨在解决现有方法在聚合多视图表示时忽略空间信息或效率低下的问题。"
    },
    {
        "order": 981,
        "title": "LayoutVAE: Stochastic Scene Layout Generation From a Label Set",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jyothi_LayoutVAE_Stochastic_Scene_Layout_Generation_From_a_Label_Set_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jyothi_LayoutVAE_Stochastic_Scene_Layout_Generation_From_a_Label_Set_ICCV_2019_paper.html",
        "abstract": "Recently there is an increasing interest in scene generation within the research community. However, models used for generating scene layouts from textual description largely ignore plausible visual variations within the structure dictated by the text. We propose LayoutVAE, a variational autoencoder based framework for generating stochastic scene layouts. LayoutVAE is a versatile modeling framework that allows for generating full image layouts given a label set, or per label layouts for an existing image given a new label. In addition, it is also capable of detecting unusual layouts, potentially providing a way to evaluate layout generation problem. Extensive experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset verifies the effectiveness of our proposed framework.",
        "中文标题": "LayoutVAE: 从标签集生成随机场景布局",
        "摘要翻译": "最近，研究社区对场景生成的兴趣日益增加。然而，用于从文本描述生成场景布局的模型大多忽略了文本所指示结构内的合理视觉变化。我们提出了LayoutVAE，一个基于变分自编码器的框架，用于生成随机场景布局。LayoutVAE是一个多功能的建模框架，它允许在给定标签集的情况下生成完整的图像布局，或者在给定新标签的情况下为现有图像生成每个标签的布局。此外，它还能够检测不寻常的布局，可能提供一种评估布局生成问题的方法。在MNIST-Layouts和具有挑战性的COCO 2017全景数据集上的大量实验验证了我们提出框架的有效性。",
        "领域": "场景生成/变分自编码器/图像布局",
        "问题": "从文本描述生成场景布局时忽略合理视觉变化的问题",
        "动机": "提高场景布局生成的多样性和合理性，以及评估布局生成问题的能力",
        "方法": "提出了基于变分自编码器的LayoutVAE框架，用于生成随机场景布局，并能够检测不寻常的布局",
        "关键词": [
            "场景生成",
            "变分自编码器",
            "图像布局",
            "文本描述",
            "视觉变化"
        ],
        "涉及的技术概念": {
            "变分自编码器": "一种生成模型，通过学习数据的潜在表示来生成新的数据样本",
            "MNIST-Layouts": "一个用于布局生成研究的合成数据集，基于MNIST手写数字数据集",
            "COCO 2017 Panoptic数据集": "一个包含丰富标注的图像数据集，用于图像分割和场景理解等任务"
        }
    },
    {
        "order": 982,
        "title": "Robust Variational Bayesian Point Set Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Robust_Variational_Bayesian_Point_Set_Registration_ICCV_2019_paper.html",
        "abstract": "In this work, we propose a hierarchical Bayesian network based point set registration method to solve missing correspondences and various massive outliers. We construct this network first using the finite Student s t latent mixture model (TLMM), in which distributions of latent variables are estimated by a tree-structured variational inference (VI) so that to obtain a tighter lower bound under the Bayesian framework. We then divide the TLMM into two different mixtures with isotropic and anisotropic covariances for correspondences recovering and outliers identification, respectively. Finally, the parameters of mixing proportion and covariances are both taken as latent variables, which benefits explaining of missing correspondences and heteroscedastic outliers. In addition, a cooling schedule is adopted to anneal prior on covariances and scale variables within designed two phases of transformation, it anneal priors on global and local variables to perform a coarse-to- fine registration. In experiments, our method outperforms five state-of-the-art methods in synthetic point set and realistic imaging registrations.",
        "中文标题": "鲁棒的变分贝叶斯点集配准",
        "摘要翻译": "在本工作中，我们提出了一种基于层次贝叶斯网络的点集配准方法，以解决缺失对应点和各种大量异常值的问题。我们首先使用有限学生t潜在混合模型（TLMM）构建这个网络，其中潜在变量的分布通过树结构变分推断（VI）估计，以便在贝叶斯框架下获得更紧密的下界。然后，我们将TLMM分为具有各向同性和各向异性协方差的两种不同混合，分别用于对应点恢复和异常值识别。最后，混合比例和协方差的参数都被视为潜在变量，这有助于解释缺失对应点和异方差异常值。此外，采用冷却计划在设计的两个变换阶段内对协方差和尺度变量的先验进行退火，它对全局和局部变量的先验进行退火以执行从粗到细的配准。在实验中，我们的方法在合成点集和实际成像配准中优于五种最先进的方法。",
        "领域": "点集配准/异常值检测/变分推断",
        "问题": "解决点集配准中的缺失对应点和大量异常值问题",
        "动机": "为了提高点集配准的准确性和鲁棒性，特别是在存在缺失对应点和异常值的情况下",
        "方法": "使用有限学生t潜在混合模型（TLMM）构建层次贝叶斯网络，通过树结构变分推断估计潜在变量分布，将TLMM分为具有各向同性和各向异性协方差的两种混合，分别用于对应点恢复和异常值识别，并采用冷却计划对协方差和尺度变量的先验进行退火",
        "关键词": [
            "点集配准",
            "异常值检测",
            "变分推断",
            "贝叶斯网络",
            "冷却计划"
        ],
        "涉及的技术概念": "有限学生t潜在混合模型（TLMM）是一种统计模型，用于处理具有厚尾分布的数据，适用于异常值检测。树结构变分推断（VI）是一种近似推断方法，用于估计潜在变量的分布。各向同性和各向异性协方差分别指协方差矩阵在所有方向上相同和不同的情况，用于区分不同类型的点集特征。冷却计划是一种优化技术，通过逐渐减少温度参数来控制优化过程，以实现从全局到局部的精细调整。"
    },
    {
        "order": 983,
        "title": "Expert Sample Consensus Applied to Camera Re-Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Brachmann_Expert_Sample_Consensus_Applied_to_Camera_Re-Localization_ICCV_2019_paper.html",
        "abstract": "Fitting model parameters to a set of noisy data points is a common problem in computer vision. In this work, we fit the 6D camera pose to a set of noisy correspondences between the 2D input image and a known 3D environment. We estimate these correspondences from the image using a neural network. Since the correspondences often contain outliers, we utilize a robust estimator such as Random Sample Consensus (RANSAC) or Differentiable RANSAC (DSAC) to fit the pose parameters. When the problem domain, e.g. the space of all 2D-3D correspondences, is large or ambiguous, a single network does not cover the domain well. Mixture of Experts (MoE) is a popular strategy to divide a problem domain among an ensemble of specialized networks, so called experts, where a gating network decides which expert is responsible for a given input. In this work, we introduce Expert Sample Consensus (ESAC), which integrates DSAC in a MoE. Our main technical contribution is an efficient method to train ESAC jointly and end-to-end. We demonstrate experimentally that ESAC handles two real-world problems better than competing methods, i.e. scalability and ambiguity. We apply ESAC to fitting simple geometric models to synthetic images, and to camera re-localization for difficult, real datasets.",
        "中文标题": "专家样本一致性应用于相机重定位",
        "摘要翻译": "在计算机视觉中，将模型参数拟合到一组噪声数据点是一个常见问题。在这项工作中，我们将6D相机姿态拟合到2D输入图像与已知3D环境之间的一组噪声对应关系中。我们使用神经网络从图像中估计这些对应关系。由于对应关系经常包含异常值，我们利用鲁棒估计器，如随机样本一致性（RANSAC）或可微分RANSAC（DSAC）来拟合姿态参数。当问题领域，例如所有2D-3D对应关系的空间，很大或模糊时，单个网络不能很好地覆盖该领域。专家混合（MoE）是一种流行的策略，用于在专门网络的集合中划分问题领域，这些网络被称为专家，其中门控网络决定哪个专家负责给定的输入。在这项工作中，我们引入了专家样本一致性（ESAC），它将DSAC集成到MoE中。我们的主要技术贡献是一种联合和端到端训练ESAC的有效方法。我们通过实验证明，ESAC比竞争方法更好地处理了两个现实世界的问题，即可扩展性和模糊性。我们将ESAC应用于将简单几何模型拟合到合成图像，以及用于困难、真实数据集的相机重定位。",
        "领域": "相机重定位/几何模型拟合/神经网络",
        "问题": "在噪声数据中准确估计6D相机姿态",
        "动机": "解决在大量或模糊的2D-3D对应关系中准确估计相机姿态的问题",
        "方法": "引入专家样本一致性（ESAC），将可微分RANSAC（DSAC）集成到专家混合（MoE）中，并开发了一种联合和端到端训练ESAC的有效方法",
        "关键词": [
            "相机重定位",
            "几何模型拟合",
            "神经网络",
            "专家样本一致性",
            "可微分RANSAC"
        ],
        "涉及的技术概念": {
            "6D相机姿态": "相机在三维空间中的位置和方向",
            "2D-3D对应关系": "二维图像点与三维空间点之间的匹配关系",
            "随机样本一致性（RANSAC）": "一种鲁棒的参数估计方法，用于从包含异常值的数据中估计数学模型参数",
            "可微分RANSAC（DSAC）": "RANSAC的一种变体，允许通过神经网络进行端到端训练",
            "专家混合（MoE）": "一种模型架构，通过多个专家网络和一个门控网络来处理复杂或模糊的问题领域",
            "专家样本一致性（ESAC）": "本文提出的方法，结合了DSAC和MoE的优点，用于更准确地估计相机姿态"
        }
    },
    {
        "order": 984,
        "title": "Is an Affine Constraint Needed for Affine Subspace Clustering?",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/You_Is_an_Affine_Constraint_Needed_for_Affine_Subspace_Clustering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/You_Is_an_Affine_Constraint_Needed_for_Affine_Subspace_Clustering_ICCV_2019_paper.html",
        "abstract": "Subspace clustering methods based on expressing each data point as a linear combination of other data points have achieved great success in computer vision applications such as motion segmentation, face and digit clustering. In face clustering, the subspaces are linear and subspace clustering methods can be applied directly. In motion segmentation, the subspaces are affine and an additional affine constraint on the coefficients is often enforced. However, since affine subspaces can always be embedded into linear subspaces of one extra dimension, it is unclear if the affine constraint is really necessary. This paper shows, both theoretically and empirically, that when the dimension of the ambient space is high relative to the sum of the dimensions of the affine subspaces, the affine constraint has a negligible effect on clustering performance. Specifically, our analysis provides conditions that guarantee the correctness of affine subspace clustering methods both with and without the affine constraint, and shows that these conditions are satisfied for high-dimensional data. Underlying our analysis is the notion of affinely independent subspaces, which not only provides geometrically interpretable correctness conditions, but also clarifies the relationships between existing results for affine subspace clustering.",
        "中文标题": "仿射子空间聚类是否需要仿射约束？",
        "摘要翻译": "基于将每个数据点表示为其他数据点的线性组合的子空间聚类方法在计算机视觉应用中取得了巨大成功，如运动分割、面部和数字聚类。在面部聚类中，子空间是线性的，子空间聚类方法可以直接应用。在运动分割中，子空间是仿射的，通常会对系数施加额外的仿射约束。然而，由于仿射子空间总是可以嵌入到额外一维的线性子空间中，因此尚不清楚仿射约束是否真的必要。本文从理论和实证两方面表明，当环境空间的维度相对于仿射子空间维度的总和较高时，仿射约束对聚类性能的影响可以忽略不计。具体来说，我们的分析提供了保证仿射子空间聚类方法正确性的条件，无论是否施加仿射约束，并表明这些条件对于高维数据是满足的。我们分析的基础是仿射独立子空间的概念，这不仅提供了几何上可解释的正确性条件，还阐明了现有仿射子空间聚类结果之间的关系。",
        "领域": "运动分割/面部聚类/数字聚类",
        "问题": "仿射子空间聚类中仿射约束的必要性",
        "动机": "探讨在仿射子空间聚类中，当环境空间的维度相对于仿射子空间维度的总和较高时，仿射约束对聚类性能的影响是否可以忽略不计。",
        "方法": "通过理论和实证分析，提供保证仿射子空间聚类方法正确性的条件，并引入仿射独立子空间的概念来阐明现有结果之间的关系。",
        "关键词": [
            "仿射子空间",
            "聚类",
            "高维数据"
        ],
        "涉及的技术概念": "仿射子空间聚类是一种将数据点表示为其他数据点的线性组合的方法，用于计算机视觉应用如运动分割、面部和数字聚类。仿射约束是在运动分割中对系数施加的额外约束，以确保子空间的仿射性质。仿射独立子空间的概念用于提供几何上可解释的正确性条件，并阐明现有仿射子空间聚类结果之间的关系。"
    },
    {
        "order": 985,
        "title": "Semantic Part Detection via Matching: Learning to Generalize to Novel Viewpoints From Limited Training Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bai_Semantic_Part_Detection_via_Matching_Learning_to_Generalize_to_Novel_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bai_Semantic_Part_Detection_via_Matching_Learning_to_Generalize_to_Novel_ICCV_2019_paper.html",
        "abstract": "Detecting semantic parts of an object is a challenging task, particularly because it is hard to annotate semantic parts and construct large datasets. In this paper, we present an approach which can learn from a small annotated dataset containing a limited range of viewpoints and generalize to detect semantic parts for a much larger range of viewpoints. The approach is based on our matching algorithm, which is used for finding accurate spatial correspondence between two images and transplanting semantic parts annotated on one image to the other. Images in the training set are matched to synthetic images rendered from a 3D CAD model, following which a clustering algorithm is used to automatically annotate semantic parts of the CAD model. During the testing period, this CAD model can synthesize annotated images under every viewpoint. These synthesized images are matched to images in the testing set to detect semantic parts in novel viewpoints. Our algorithm is simple, intuitive, and contains very few parameters. Experiments show our method outperforms standard deep learning approaches and, in particular, performs much better on novel viewpoints. For facilitating the future research, code is available: https://github.com/ytongbai/SemanticPartDetection",
        "中文标题": "通过匹配进行语义部分检测：从有限训练数据中学习以泛化到新视角",
        "摘要翻译": "检测对象的语义部分是一项具有挑战性的任务，特别是因为很难注释语义部分并构建大型数据集。在本文中，我们提出了一种方法，可以从包含有限视角范围的小型注释数据集中学习，并泛化以检测更大视角范围内的语义部分。该方法基于我们的匹配算法，该算法用于找到两幅图像之间的准确空间对应关系，并将一幅图像上注释的语义部分移植到另一幅图像上。训练集中的图像与从3D CAD模型渲染的合成图像进行匹配，随后使用聚类算法自动注释CAD模型的语义部分。在测试期间，该CAD模型可以在每个视角下合成注释图像。这些合成图像与测试集中的图像进行匹配，以检测新视角中的语义部分。我们的算法简单、直观，并且包含非常少的参数。实验表明，我们的方法优于标准的深度学习方法，特别是在新视角上表现更好。为了促进未来的研究，代码已公开：https://github.com/ytongbai/SemanticPartDetection",
        "领域": "语义分割/3D视觉/图像匹配",
        "问题": "如何从有限的训练数据中学习并泛化到新视角以检测对象的语义部分",
        "动机": "由于注释语义部分和构建大型数据集的困难，需要一种能够从小型注释数据集中学习并泛化到新视角的方法",
        "方法": "基于匹配算法找到两幅图像之间的准确空间对应关系，并将语义部分从一幅图像移植到另一幅图像；使用聚类算法自动注释3D CAD模型的语义部分；在测试期间，通过匹配合成图像与测试图像来检测新视角中的语义部分",
        "关键词": [
            "语义分割",
            "3D视觉",
            "图像匹配"
        ],
        "涉及的技术概念": "匹配算法用于图像间的空间对应关系建立，聚类算法用于自动注释3D模型的语义部分，3D CAD模型用于生成不同视角下的合成图像"
    },
    {
        "order": 986,
        "title": "Dynamic Points Agglomeration for Hierarchical Point Sets Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Dynamic_Points_Agglomeration_for_Hierarchical_Point_Sets_Learning_ICCV_2019_paper.html",
        "abstract": "Many previous works on point sets learning achieve excellent performance with hierarchical architecture. Their strategies towards points agglomeration, however, only perform points sampling and grouping in original Euclidean space in a fixed way. These heuristic and task-irrelevant strategies severely limit their ability to adapt to more varied scenarios. To this end, we develop a novel hierarchical point sets learning architecture, with dynamic points agglomeration. By exploiting the relation of points in semantic space, a module based on graph convolution network is designed to learn a soft points cluster agglomeration. We construct a hierarchical architecture that gradually agglomerates points by stacking this learnable and lightweight module. In contrast to fixed points agglomeration strategy, our method can handle more diverse situations robustly and efficiently. Moreover, we propose a parameter sharing scheme for reducing memory usage and computational burden induced by the agglomeration module. Extensive experimental results on several point cloud analytic tasks, including classification and segmentation, well demonstrate the superior performance of our dynamic hierarchical learning framework over current state-of-the-art methods.",
        "中文标题": "动态点聚合用于层次点集学习",
        "摘要翻译": "许多先前关于点集学习的工作通过层次架构实现了卓越的性能。然而，它们对点聚合的策略仅在原始欧几里得空间中以固定方式执行点采样和分组。这些启发式且与任务无关的策略严重限制了它们适应更多样化场景的能力。为此，我们开发了一种新颖的层次点集学习架构，具有动态点聚合。通过利用语义空间中点的关系，设计了一个基于图卷积网络的模块来学习软点集群聚合。我们构建了一个层次架构，通过堆叠这个可学习和轻量级的模块逐渐聚合点。与固定点聚合策略相比，我们的方法能够更稳健和高效地处理更多样化的情况。此外，我们提出了一种参数共享方案，以减少由聚合模块引起的内存使用和计算负担。在包括分类和分割在内的多个点云分析任务上的广泛实验结果，很好地证明了我们的动态层次学习框架相对于当前最先进方法的优越性能。",
        "领域": "点云处理/图卷积网络/语义分析",
        "问题": "固定点聚合策略在处理多样化场景时的适应性问题",
        "动机": "提高点集学习架构在处理多样化场景时的适应性和效率",
        "方法": "开发了一种基于图卷积网络的动态点聚合模块，构建层次架构逐渐聚合点，并提出参数共享方案以减少内存和计算负担",
        "关键词": [
            "点云处理",
            "图卷积网络",
            "语义分析",
            "动态聚合",
            "层次学习"
        ],
        "涉及的技术概念": {
            "点集学习": "一种处理点云数据的学习方法，点云是由大量点构成的三维数据表示形式。",
            "层次架构": "一种分层的网络结构，通过逐层处理数据来提取特征。",
            "图卷积网络": "一种处理图结构数据的神经网络，能够利用图中节点之间的关系进行信息传递和特征提取。",
            "动态点聚合": "一种根据数据特性动态调整点聚合策略的方法，以提高模型对不同场景的适应性。",
            "参数共享": "一种减少模型参数数量，从而降低内存使用和计算负担的技术。"
        }
    },
    {
        "order": 987,
        "title": "Meta-Learning to Detect Rare Objects",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.html",
        "abstract": "Few-shot learning, i.e., learning novel concepts from few examples, is fundamental to practical visual recognition systems. While most of existing work has focused on few-shot classification, we make a step towards few-shot object detection, a more challenging yet under-explored task. We develop a conceptually simple but powerful meta-learning based framework that simultaneously tackles few-shot classification and few-shot localization in a unified, coherent way. This framework leverages meta-level knowledge about \"model parameter generation\" from base classes with abundant data to facilitate the generation of a detector for novel classes. Our key insight is to disentangle the learning of category-agnostic and category-specific components in a CNN based detection model. In particular, we introduce a weight prediction meta-model that enables predicting the parameters of category-specific components from few examples. We systematically benchmark the performance of modern detectors in the small-sample size regime. Experiments in a variety of realistic scenarios, including within-domain, cross-domain, and long-tailed settings, demonstrate the effectiveness and generality of our approach under different notions of novel classes.",
        "中文标题": "元学习检测稀有物体",
        "摘要翻译": "少样本学习，即从少量例子中学习新概念，对于实际的视觉识别系统至关重要。虽然大多数现有工作集中在少样本分类上，但我们向少样本物体检测迈出了一步，这是一个更具挑战性但尚未充分探索的任务。我们开发了一个概念上简单但功能强大的基于元学习的框架，该框架以统一、连贯的方式同时解决少样本分类和少样本定位问题。该框架利用来自基础类的关于“模型参数生成”的元级知识，这些基础类有丰富的数据，以促进新类别的检测器的生成。我们的关键见解是在基于CNN的检测模型中解耦类别无关和类别特定组件的学习。特别是，我们引入了一个权重预测元模型，该模型能够从少量例子中预测类别特定组件的参数。我们系统地评估了现代检测器在小样本量情况下的性能。在各种现实场景中的实验，包括域内、跨域和长尾设置，证明了我们的方法在不同新类别概念下的有效性和普遍性。",
        "领域": "少样本学习/物体检测/元学习",
        "问题": "解决少样本物体检测的挑战",
        "动机": "少样本学习对于实际视觉识别系统至关重要，而少样本物体检测是一个更具挑战性但尚未充分探索的任务。",
        "方法": "开发了一个基于元学习的框架，该框架利用来自基础类的元级知识，解耦类别无关和类别特定组件的学习，并引入权重预测元模型来预测类别特定组件的参数。",
        "关键词": [
            "少样本学习",
            "物体检测",
            "元学习",
            "权重预测元模型"
        ],
        "涉及的技术概念": "少样本学习指的是从少量例子中学习新概念的能力；物体检测是指在图像中识别和定位特定物体的任务；元学习是一种学习如何学习的方法，旨在提高模型在新任务上的学习效率和效果；权重预测元模型是一种能够从少量例子中预测模型参数的模型。"
    },
    {
        "order": 988,
        "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Attributing_Fake_Images_to_GANs_Learning_and_Analyzing_GAN_Fingerprints_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_Attributing_Fake_Images_to_GANs_Learning_and_Analyzing_GAN_Fingerprints_ICCV_2019_paper.html",
        "abstract": "Recent advances in Generative Adversarial Networks (GANs) have shown increasing success in generating photorealistic images. But they also raise challenges to visual forensics and model attribution. We present the first study of learning GAN fingerprints towards image attribution and using them to classify an image as real or GAN-generated. For GAN-generated images, we further identify their sources. Our experiments show that (1) GANs carry distinct model fingerprints and leave stable fingerprints in their generated images, which support image attribution; (2) even minor differences in GAN training can result in different fingerprints, which enables fine-grained model authentication; (3) fingerprints persist across different image frequencies and patches and are not biased by GAN artifacts; (4) fingerprint finetuning is effective in immunizing against five types of adversarial image perturbations; and (5) comparisons also show our learned fingerprints consistently outperform several baselines in a variety of setups.",
        "中文标题": "将假图像归因于GANs：学习和分析GAN指纹",
        "摘要翻译": "生成对抗网络（GANs）的最新进展在生成逼真图像方面显示出越来越大的成功。但它们也给视觉取证和模型归因带来了挑战。我们提出了第一个关于学习GAN指纹以实现图像归因并使用它们将图像分类为真实或GAN生成的研究。对于GAN生成的图像，我们进一步识别它们的来源。我们的实验表明：（1）GANs携带独特的模型指纹并在其生成的图像中留下稳定的指纹，这支持图像归因；（2）即使GAN训练中的微小差异也会导致不同的指纹，这使得细粒度的模型认证成为可能；（3）指纹在不同的图像频率和补丁中持续存在，并且不受GAN伪影的偏见；（4）指纹微调在免疫五种类型的对抗性图像扰动方面是有效的；（5）比较还显示，在各种设置中，我们学习的指纹始终优于几个基线。",
        "领域": "图像取证/模型归因/对抗性学习",
        "问题": "如何将假图像归因于特定的GAN模型，并区分真实图像与GAN生成的图像",
        "动机": "随着GANs在生成逼真图像方面的成功，如何对这些图像进行有效的视觉取证和模型归因成为了一个挑战",
        "方法": "学习GAN指纹以实现图像归因，并使用这些指纹将图像分类为真实或GAN生成，进一步识别GAN生成图像的来源",
        "关键词": [
            "GAN指纹",
            "图像归因",
            "模型认证",
            "对抗性图像扰动"
        ],
        "涉及的技术概念": "GAN指纹指的是生成对抗网络（GANs）在生成图像时留下的独特标记，这些标记可以用于识别图像的来源。对抗性图像扰动是指对图像进行微小修改以欺骗机器学习模型的技术。"
    },
    {
        "order": 989,
        "title": "New Convex Relaxations for MRF Inference With Unknown Graphs",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_New_Convex_Relaxations_for_MRF_Inference_With_Unknown_Graphs_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_New_Convex_Relaxations_for_MRF_Inference_With_Unknown_Graphs_ICCV_2019_paper.html",
        "abstract": "Treating graph structures of Markov random fields as unknown and estimating them jointly with labels have been shown to be useful for modeling human activity recognition and other related tasks. We propose two novel relaxations for solving this problem. The first is a linear programming (LP) relaxation, which is provably tighter than the existing LP relaxation. The second is a non-convex quadratic programming (QP) relaxation, which admits an efficient concave-convex procedure (CCCP). The CCCP algorithm is initialized by solving a convex QP relaxation of the problem, which is obtained by modifying the diagonal of the matrix that specifies the non-convex QP relaxation. We show that our convex QP relaxation is optimal in the sense that it minimizes the L1 norm of the diagonal modification vector. While the convex QP relaxation is not as tight as the existing and the new LP relaxations, when used in conjunction with the CCCP algorithm for the non-convex QP relaxation, it provides accurate solutions. We demonstrate the efficacy of our new relaxations for both synthetic data and human activity recognition.",
        "中文标题": "用于未知图MRF推理的新凸松弛方法",
        "摘要翻译": "将马尔可夫随机场的图结构视为未知，并与标签联合估计，已被证明对于建模人类活动识别及其他相关任务非常有用。我们提出了两种新的松弛方法来解决这个问题。第一种是线性规划（LP）松弛，它被证明比现有的LP松弛更紧密。第二种是非凸二次规划（QP）松弛，它允许使用高效的凹凸过程（CCCP）。CCCP算法通过解决问题的凸QP松弛来初始化，这是通过修改指定非凸QP松弛的矩阵的对角线获得的。我们证明了我们的凸QP松弛在最小化对角线修改向量的L1范数的意义上是最优的。虽然凸QP松弛不如现有的和新提出的LP松弛紧密，但当与用于非凸QP松弛的CCCP算法结合使用时，它提供了准确的解决方案。我们展示了我们的新松弛方法在合成数据和人类活动识别上的有效性。",
        "领域": "马尔可夫随机场/人类活动识别/优化算法",
        "问题": "在未知图结构的情况下，联合估计马尔可夫随机场的图结构和标签",
        "动机": "为了提高人类活动识别等任务的建模效果",
        "方法": "提出了两种新的松弛方法：一种更紧密的线性规划（LP）松弛和一种非凸二次规划（QP）松弛，后者采用凹凸过程（CCCP）算法进行优化",
        "关键词": [
            "马尔可夫随机场",
            "人类活动识别",
            "优化算法",
            "线性规划",
            "二次规划",
            "凹凸过程"
        ],
        "涉及的技术概念": "线性规划（LP）松弛、非凸二次规划（QP）松弛、凹凸过程（CCCP）算法、L1范数"
    },
    {
        "order": 990,
        "title": "Dual Adversarial Inference for Text-to-Image Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lao_Dual_Adversarial_Inference_for_Text-to-Image_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lao_Dual_Adversarial_Inference_for_Text-to-Image_Synthesis_ICCV_2019_paper.html",
        "abstract": "Synthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated on Oxford-102, CUB and COCO datasets.",
        "中文标题": "文本到图像合成的双重对抗推理",
        "摘要翻译": "从给定的文本描述合成图像涉及两种类型的信息：内容，包括文本中明确描述的信息（例如，颜色、构图等），以及风格，这通常在文本中没有很好地描述（例如，位置、数量、大小等）。然而，在以前的工作中，它通常被视为仅从内容生成图像的过程，即不考虑学习有意义的风格表示。在本文中，我们旨在学习在潜在空间中解耦的两个变量，分别代表内容和风格。我们通过在当前文本到图像合成框架中增加双重对抗推理机制来实现这一点。通过大量实验，我们展示了我们的模型以无监督的方式学习风格表示，这些表示对应于图像中存在的某些有意义的信息，而这些信息在文本中没有很好地描述。新框架在Oxford-102、CUB和COCO数据集上评估时也提高了合成图像的质量。",
        "领域": "图像合成/文本到图像转换/对抗学习",
        "问题": "如何从文本描述中合成包含明确内容和隐含风格的图像",
        "动机": "现有方法通常只从内容生成图像，忽略了学习有意义的风格表示",
        "方法": "通过增加双重对抗推理机制来学习内容和风格的解耦表示",
        "关键词": [
            "图像合成",
            "文本到图像转换",
            "对抗学习",
            "无监督学习"
        ],
        "涉及的技术概念": "双重对抗推理机制是一种在潜在空间中解耦内容和风格表示的方法，通过这种方式，模型能够以无监督的方式学习到图像中存在的、文本中未明确描述的有意义信息。"
    },
    {
        "order": 991,
        "title": "Cluster Alignment With a Teacher for Unsupervised Domain Adaptation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Cluster_Alignment_With_a_Teacher_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Deng_Cluster_Alignment_With_a_Teacher_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.html",
        "abstract": "Deep learning methods have shown promise in unsupervised domain adaptation, which aims to leverage a labeled source domain to learn a classifier for the unlabeled target domain with a different distribution. However, such methods typically learn a domain-invariant representation space to match the marginal distributions of the source and target domains, while ignoring their fine-level structures. In this paper, we propose Cluster Alignment with a Teacher (CAT) for unsupervised domain adaptation, which can effectively incorporate the discriminative clustering structures in both domains for better adaptation. Technically, CAT leverages an implicit ensembling teacher model to reliably discover the class-conditional structure in the feature space for the unlabeled target domain. Then CAT forces the features of both the source and the target domains to form discriminative class-conditional clusters and aligns the corresponding clusters across domains. Empirical results demonstrate that CAT achieves state-of-the-art results in several unsupervised domain adaptation scenarios.",
        "中文标题": "使用教师模型进行聚类对齐的无监督域适应",
        "摘要翻译": "深度学习方法在无监督域适应方面显示出了潜力，其目的是利用标记的源域来学习一个分类器，用于具有不同分布的未标记目标域。然而，这些方法通常学习一个域不变表示空间来匹配源域和目标域的边际分布，而忽略了它们的细粒度结构。在本文中，我们提出了使用教师模型进行聚类对齐（CAT）的无监督域适应方法，该方法能够有效地结合两个域中的判别性聚类结构以实现更好的适应。技术上，CAT利用一个隐式集成教师模型来可靠地发现未标记目标域特征空间中的类条件结构。然后，CAT强制源域和目标域的特征形成判别性类条件聚类，并在域间对齐相应的聚类。实证结果表明，CAT在几种无监督域适应场景中实现了最先进的结果。",
        "领域": "无监督学习/域适应/特征学习",
        "问题": "在无监督域适应中，如何有效地结合源域和目标域的判别性聚类结构以实现更好的适应。",
        "动机": "现有的无监督域适应方法通常只匹配源域和目标域的边际分布，而忽略了它们的细粒度结构，这限制了域适应的效果。",
        "方法": "提出了一种使用教师模型进行聚类对齐（CAT）的方法，通过隐式集成教师模型发现未标记目标域的类条件结构，并强制源域和目标域的特征形成判别性类条件聚类，在域间对齐相应的聚类。",
        "关键词": [
            "无监督学习",
            "域适应",
            "特征学习",
            "聚类对齐",
            "教师模型"
        ],
        "涉及的技术概念": "无监督域适应是一种技术，旨在利用一个标记的源域来学习一个分类器，用于具有不同分布的未标记目标域。聚类对齐是一种方法，通过强制源域和目标域的特征形成判别性类条件聚类，并在域间对齐相应的聚类，以实现更好的域适应。教师模型是一种隐式集成模型，用于可靠地发现未标记目标域特征空间中的类条件结构。"
    },
    {
        "order": 992,
        "title": "View-LSTM: Novel-View Video Synthesis Through View Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lakhal_View-LSTM_Novel-View_Video_Synthesis_Through_View_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lakhal_View-LSTM_Novel-View_Video_Synthesis_Through_View_Decomposition_ICCV_2019_paper.html",
        "abstract": "We tackle the problem of synthesizing a video of multiple moving people as seen from a novel view, given only an input video and depth information or human poses of the novel view as prior. This problem requires a model that learns to transform input features into target features while maintaining temporal consistency. To this end, we learn an invariant feature from the input video that is shared across all viewpoints of the same scene and a view-dependent feature obtained using the target priors. The proposed approach, View-LSTM, is a recurrent neural network structure that accounts for the temporal consistency and target feature approximation constraints. We validate View-LSTM by designing an end-to-end generator for novel-view video synthesis. Experiments on a large multi-view action recognition dataset validate the proposed model.",
        "中文标题": "View-LSTM: 通过视图分解的新视角视频合成",
        "摘要翻译": "我们解决了从新视角合成多个移动人物的视频的问题，仅给定输入视频和深度信息或新视角的人体姿态作为先验。这个问题需要一个模型，该模型能够学习将输入特征转换为目标特征，同时保持时间一致性。为此，我们从输入视频中学习一个在所有相同场景的视点之间共享的不变特征，以及使用目标先验获得的视图依赖特征。提出的方法View-LSTM是一个考虑时间一致性和目标特征近似约束的循环神经网络结构。我们通过设计一个端到端的新视角视频合成生成器来验证View-LSTM。在一个大型多视角动作识别数据集上的实验验证了所提出的模型。",
        "领域": "视频合成/动作识别/多视角学习",
        "问题": "从新视角合成多个移动人物的视频",
        "动机": "需要从输入视频和深度信息或新视角的人体姿态作为先验，合成新视角的视频，同时保持时间一致性",
        "方法": "提出View-LSTM，一个循环神经网络结构，学习不变特征和视图依赖特征，考虑时间一致性和目标特征近似约束",
        "关键词": [
            "视频合成",
            "动作识别",
            "多视角学习"
        ],
        "涉及的技术概念": "View-LSTM是一种循环神经网络结构，用于从输入视频中学习不变特征和视图依赖特征，以合成新视角的视频。不变特征是在所有相同场景的视点之间共享的，而视图依赖特征是通过目标先验获得的。这种方法考虑了时间一致性和目标特征近似约束，通过设计一个端到端的新视角视频合成生成器来验证其有效性。"
    },
    {
        "order": 993,
        "title": "Analyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Thiede_Analyzing_the_Variety_Loss_in_the_Context_of_Probabilistic_Trajectory_ICCV_2019_paper.html",
        "abstract": "Trajectory or behavior prediction of traffic agents is an important component of autonomous driving and robot planning in general. It can be framed as a probabilistic future sequence generation problem and recent literature has studied the applicability of generative models in this context. The variety or Minimum over N (MoN) loss, which tries to minimize the error between the ground truth and the closest of N output predictions, has been used in these recent learning models to improve the diversity of predictions. In this work, we present a proof to show that the MoN loss does not lead to the ground truth probability density function, but approximately to its square root instead. We validate this finding with extensive experiments on both simulated toy as well as real world datasets. We also propose multiple solutions to compensate for the dilation to show improvement of log likelihood of the ground truth samples in the corrected probability density function.",
        "中文标题": "分析概率轨迹预测中的多样性损失",
        "摘要翻译": "交通代理的轨迹或行为预测是自动驾驶和机器人规划的重要组成部分。它可以被框架化为一个概率未来序列生成问题，最近的文献研究了生成模型在此背景下的适用性。多样性或N个最小（MoN）损失，试图最小化地面实况与N个输出预测中最接近的一个之间的误差，已在最近的学习模型中被用于提高预测的多样性。在这项工作中，我们提出了一个证明，表明MoN损失不会导致地面实况概率密度函数，而是近似于其平方根。我们通过在模拟玩具以及真实世界数据集上的广泛实验验证了这一发现。我们还提出了多种解决方案来补偿这种扩张，以显示在校正后的概率密度函数中地面实况样本的对数似然的改进。",
        "领域": "自动驾驶/机器人规划/概率模型",
        "问题": "提高交通代理轨迹或行为预测的多样性",
        "动机": "研究MoN损失在概率轨迹预测中的应用及其对预测多样性的影响",
        "方法": "提出证明MoN损失不会导致地面实况概率密度函数，并通过实验验证，提出解决方案以补偿扩张",
        "关键词": [
            "轨迹预测",
            "行为预测",
            "概率模型",
            "多样性损失",
            "自动驾驶"
        ],
        "涉及的技术概念": "MoN损失（Minimum over N loss）是一种用于提高预测多样性的损失函数，通过最小化地面实况与N个输出预测中最接近的一个之间的误差来实现。概率密度函数是描述随机变量在某个确定值点附近的可能性的函数。对数似然是评估统计模型拟合度的常用方法，通过计算模型预测值与实际观测值之间的对数概率来衡量。"
    },
    {
        "order": 994,
        "title": "HoloGAN: Unsupervised Learning of 3D Representations From Natural Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nguyen-Phuoc_HoloGAN_Unsupervised_Learning_of_3D_Representations_From_Natural_Images_ICCV_2019_paper.html",
        "abstract": "We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. Particularly, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.",
        "中文标题": "HoloGAN：从自然图像中无监督学习3D表示",
        "摘要翻译": "我们提出了一种新颖的生成对抗网络（GAN），用于从自然图像中无监督学习3D表示的任务。大多数生成模型依赖于2D核来生成图像，并且对3D世界做出很少的假设。因此，这些模型在需要强烈3D理解的任务中，如新视角合成，往往会产生模糊的图像或伪影。HoloGAN则学习世界的3D表示，并以逼真的方式渲染这种表示。与其他GAN不同，HoloGAN通过对学习的3D特征进行刚体变换，提供了对生成对象姿态的显式控制。我们的实验表明，使用显式的3D特征使HoloGAN能够解耦3D姿态和身份，这进一步分解为形状和外观，同时仍然能够生成与其他生成模型相似或更高视觉质量的图像。HoloGAN可以仅从未标记的2D图像端到端训练。特别是，我们不需要姿态标签、3D形状或同一对象的多个视图。这表明HoloGAN是第一个以完全无监督的方式从自然图像中学习3D表示的生成模型。",
        "领域": "3D视觉/生成模型/无监督学习",
        "问题": "从自然图像中无监督学习3D表示",
        "动机": "解决现有生成模型在需要强烈3D理解的任务中产生模糊图像或伪影的问题",
        "方法": "提出一种新颖的生成对抗网络（HoloGAN），通过学习世界的3D表示并以逼真的方式渲染这种表示，提供对生成对象姿态的显式控制",
        "关键词": [
            "3D视觉",
            "生成模型",
            "无监督学习"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）、3D表示、刚体变换、新视角合成、无监督学习"
    },
    {
        "order": 995,
        "title": "Deep Mesh Reconstruction From Single RGB Images via Topology Modification Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Pan_Deep_Mesh_Reconstruction_From_Single_RGB_Images_via_Topology_Modification_ICCV_2019_paper.html",
        "abstract": "Reconstructing the 3D mesh of a general object from a single image is now possible thanks to the latest advances of deep learning technologies. However, due to the nontrivial difficulty of generating a feasible mesh structure, the state-of-the-art approaches often simplify the problem by learning the displacements of a template mesh that deforms it to the target surface. Though reconstructing a 3D shape with complex topology can be achieved by deforming multiple mesh patches, it remains difficult to stitch the results to ensure a high meshing quality. In this paper, we present an end-to-end single-view mesh reconstruction framework that is able to generate high-quality meshes with complex topologies from a single genus-0 template mesh. The key to our approach is a novel progressive shaping framework that alternates between mesh deformation and topology modification. While a deformation network predicts the per-vertex translations that reduce the gap between the reconstructed mesh and the ground truth, a novel topology modification network is employed to prune the error-prone faces, enabling the evolution of topology. By iterating over the two procedures, one can progressively modify the mesh topology while achieving higher reconstruction accuracy. Moreover, a boundary refinement network is designed to refine the boundary conditions to further improve the visual quality of the reconstructed mesh. Extensive experiments demonstrate that our approach outperforms the current state-of-the-art methods both qualitatively and quantitatively, especially for the shapes with complex topologies.",
        "中文标题": "通过拓扑修改网络从单张RGB图像进行深度网格重建",
        "摘要翻译": "由于深度学习技术的最新进展，现在可以从单张图像重建一般物体的3D网格。然而，由于生成可行网格结构的非平凡难度，最先进的方法通常通过学习模板网格的位移来简化问题，使其变形为目标表面。尽管通过变形多个网格补丁可以实现具有复杂拓扑的3D形状重建，但确保高质量的网格缝合仍然困难。在本文中，我们提出了一个端到端的单视图网格重建框架，能够从单个genus-0模板网格生成具有复杂拓扑的高质量网格。我们方法的关键是一个新颖的渐进成形框架，它在网格变形和拓扑修改之间交替进行。虽然变形网络预测每个顶点的平移以减少重建网格与地面实况之间的差距，但采用了一种新颖的拓扑修改网络来修剪易出错的面，从而实现拓扑的进化。通过迭代这两个过程，可以逐步修改网格拓扑，同时实现更高的重建精度。此外，设计了一个边界细化网络来细化边界条件，以进一步提高重建网格的视觉质量。大量实验证明，我们的方法在质量和数量上都优于当前的最先进方法，特别是对于具有复杂拓扑的形状。",
        "领域": "3D重建/网格处理/深度学习",
        "问题": "从单张RGB图像重建具有复杂拓扑的高质量3D网格",
        "动机": "解决现有方法在生成高质量网格和确保网格缝合质量方面的困难",
        "方法": "采用一个端到端的单视图网格重建框架，结合变形网络和拓扑修改网络，通过迭代过程逐步修改网格拓扑并提高重建精度",
        "关键词": [
            "3D重建",
            "网格处理",
            "深度学习"
        ],
        "涉及的技术概念": {
            "深度学习技术": "用于从单张图像重建3D网格",
            "变形网络": "预测每个顶点的平移以减少重建网格与地面实况之间的差距",
            "拓扑修改网络": "修剪易出错的面，实现拓扑的进化",
            "边界细化网络": "细化边界条件，提高重建网格的视觉质量"
        }
    },
    {
        "order": 996,
        "title": "Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Unpaired_Image-to-Speech_Synthesis_With_Multimodal_Information_Bottleneck_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Unpaired_Image-to-Speech_Synthesis_With_Multimodal_Information_Bottleneck_ICCV_2019_paper.html",
        "abstract": "Deep generative models have led to significant advances in cross-modal generation such as text-to-image synthesis. Training these models typically requires paired data with direct correspondence between modalities. We introduce the novel problem of translating instances from one modality to another without paired data by leveraging an intermediate modality shared by the two other modalities. To demonstrate this, we take the problem of translating images to speech. In this case, one could leverage disjoint datasets with one shared modality, e.g., image-text pairs and text-speech pairs, with text as the shared modality. We call this problem \"skip-modal generation\" because the shared modality is skipped during the generation process. We propose a multimodal information bottleneck approach that learns the correspondence between modalities from unpaired data (image and speech) by leveraging the shared modality (text). We address fundamental challenges of skip-modal generation: 1) learning multimodal representations using a single model, 2) bridging the domain gap between two unrelated datasets, and 3) learning the correspondence between modalities from unpaired data. We show qualitative results on image-to-speech synthesis; this is the first time such results have been reported in the literature. We also show that our approach improves performance on traditional cross-modal generation, suggesting that it improves data efficiency in solving individual tasks.",
        "中文标题": "无配对图像到语音合成与多模态信息瓶颈",
        "摘要翻译": "深度生成模型在跨模态生成（如文本到图像合成）方面取得了显著进展。训练这些模型通常需要具有模态间直接对应关系的配对数据。我们引入了一个新颖的问题，即通过利用两个其他模态共享的中间模态，在没有配对数据的情况下将一个模态的实例翻译到另一个模态。为了证明这一点，我们以图像到语音的翻译问题为例。在这种情况下，可以利用具有一个共享模态的不相交数据集，例如图像-文本对和文本-语音对，其中文本作为共享模态。我们称这个问题为“跳过模态生成”，因为在生成过程中跳过了共享模态。我们提出了一种多模态信息瓶颈方法，通过利用共享模态（文本）从未配对数据（图像和语音）中学习模态间的对应关系。我们解决了跳过模态生成的基本挑战：1）使用单一模型学习多模态表示，2）桥接两个不相关数据集之间的领域差距，以及3）从未配对数据中学习模态间的对应关系。我们展示了图像到语音合成的定性结果；这是文献中首次报告此类结果。我们还展示了我们的方法在传统跨模态生成上的性能提升，表明它在解决个别任务时提高了数据效率。",
        "领域": "跨模态生成/语音合成/图像理解",
        "问题": "在没有配对数据的情况下实现图像到语音的跨模态翻译",
        "动机": "探索利用共享中间模态（如文本）从未配对数据中学习模态间对应关系，以解决跨模态生成中的配对数据需求问题",
        "方法": "提出了一种多模态信息瓶颈方法，通过利用共享模态（文本）从未配对数据（图像和语音）中学习模态间的对应关系，解决了跳过模态生成的基本挑战",
        "关键词": [
            "跨模态生成",
            "语音合成",
            "图像理解",
            "信息瓶颈",
            "跳过模态生成"
        ],
        "涉及的技术概念": "多模态信息瓶颈方法是一种利用共享模态（如文本）从未配对数据中学习模态间对应关系的技术，旨在解决跨模态生成中的配对数据需求问题。该方法通过桥接不相关数据集之间的领域差距，使用单一模型学习多模态表示，并从未配对数据中学习模态间的对应关系，从而实现跳过共享模态的生成过程。"
    },
    {
        "order": 997,
        "title": "UprightNet: Geometry-Aware Camera Orientation Estimation From Single Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xian_UprightNet_Geometry-Aware_Camera_Orientation_Estimation_From_Single_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xian_UprightNet_Geometry-Aware_Camera_Orientation_Estimation_From_Single_Images_ICCV_2019_paper.html",
        "abstract": "We introduce UprightNet, a learning-based approach for estimating 2DoF camera orientation from a single RGB image of an indoor scene. Unlike recent methods that leverage deep learning to perform black-box regression from image to orientation parameters, we propose an end-to-end framework that incorporates explicit geometric reasoning. In particular, we design a network that predicts two representations of scene geometry, in both the local camera and global reference coordinate systems, and solves for the camera orientation as the rotation that best aligns these two predictions via a differentiable least squares module. This network can be trained end-to-end, and can be supervised with both ground truth camera poses and intermediate representations of surface geometry. We evaluate UprightNet on the single-image camera orientation task on synthetic and real datasets, and show significant improvements over prior state-of-the-art approaches.",
        "中文标题": "UprightNet: 从单张图像进行几何感知的相机方向估计",
        "摘要翻译": "我们介绍了UprightNet，一种基于学习的方法，用于从室内场景的单个RGB图像估计2DoF相机方向。与最近利用深度学习从图像到方向参数执行黑盒回归的方法不同，我们提出了一个端到端的框架，该框架结合了显式的几何推理。特别是，我们设计了一个网络，该网络预测场景几何的两种表示，分别在局部相机和全局参考坐标系中，并通过可微分的最小二乘模块求解相机方向，作为最佳对齐这两种预测的旋转。该网络可以端到端训练，并且可以用地面真实相机姿态和表面几何的中间表示进行监督。我们在合成和真实数据集上的单图像相机方向任务上评估了UprightNet，并显示出相对于先前最先进方法的显著改进。",
        "领域": "室内场景理解/相机姿态估计/几何推理",
        "问题": "从单个RGB图像估计2DoF相机方向",
        "动机": "现有方法多采用深度学习进行黑盒回归，缺乏显式的几何推理，UprightNet旨在通过结合显式几何推理来提高相机方向估计的准确性和可解释性。",
        "方法": "设计了一个端到端的网络框架，该框架预测场景几何的两种表示，并通过可微分的最小二乘模块求解相机方向，作为最佳对齐这两种预测的旋转。",
        "关键词": [
            "2DoF相机方向估计",
            "几何推理",
            "端到端学习"
        ],
        "涉及的技术概念": "2DoF相机方向估计指的是估计相机在空间中的两个自由度方向，通常包括俯仰角和偏航角。几何推理涉及使用几何原理和算法来理解和处理图像中的几何信息。端到端学习指的是从输入到输出直接学习映射关系，无需手动设计中间步骤或特征提取。"
    },
    {
        "order": 998,
        "title": "Improved Conditional VRNNs for Video Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Castrejon_Improved_Conditional_VRNNs_for_Video_Prediction_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Castrejon_Improved_Conditional_VRNNs_for_Video_Prediction_ICCV_2019_paper.html",
        "abstract": "Predicting future frames for a video sequence is a challenging generative modeling task. Promising approaches include probabilistic latent variable models such as the Variational Auto-Encoder. While VAEs can handle uncertainty and model multiple possible future outcomes, they have a tendency to produce blurry predictions. In this work we argue that this is a sign of underfitting. To address this issue, we propose to increase the expressiveness of the latent distributions and to use higher capacity likelihood models. Our approach relies on a hierarchy of latent variables, which defines a family of flexible prior and posterior distributions in order to better model the probability of future sequences. We validate our proposal through a series of ablation experiments and compare our approach to current state-of-the-art latent variable models. Our method performs favorably under several metrics in three different datasets.",
        "中文标题": "改进的条件VRNNs用于视频预测",
        "摘要翻译": "预测视频序列的未来帧是一项具有挑战性的生成建模任务。有前景的方法包括概率潜在变量模型，如变分自编码器。虽然VAEs能够处理不确定性并建模多种可能的未来结果，但它们倾向于产生模糊的预测。在这项工作中，我们认为这是欠拟合的迹象。为了解决这个问题，我们提出增加潜在分布的表达能力并使用更高容量的似然模型。我们的方法依赖于潜在变量的层次结构，这定义了一族灵活的先验和后验分布，以更好地建模未来序列的概率。我们通过一系列消融实验验证了我们的提议，并将我们的方法与当前最先进的潜在变量模型进行了比较。我们的方法在三个不同数据集的多个指标下表现优异。",
        "领域": "视频预测/生成模型/变分自编码器",
        "问题": "视频序列未来帧的预测模糊问题",
        "动机": "解决变分自编码器在视频预测中产生的模糊预测问题，提高预测的清晰度和准确性",
        "方法": "增加潜在分布的表达能力，使用更高容量的似然模型，以及依赖于潜在变量的层次结构来定义灵活的先验和后验分布",
        "关键词": [
            "视频预测",
            "变分自编码器",
            "潜在变量模型"
        ],
        "涉及的技术概念": {
            "变分自编码器（VAE）": "一种概率潜在变量模型，能够处理不确定性并建模多种可能的未来结果",
            "潜在变量": "在模型中用于表示数据的潜在特征或结构的变量",
            "先验和后验分布": "在贝叶斯统计中，先验分布是在观察到数据之前对参数的假设分布，而后验分布是在观察到数据之后对参数的更新分布"
        }
    },
    {
        "order": 999,
        "title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Henzler_Escaping_Platos_Cave_3D_Shape_From_Adversarial_Rendering_ICCV_2019_paper.html",
        "abstract": "We introduce PlatonicGAN to discover the 3D structure of an object class from an unstructured collection of 2D images, i.e., where no relation between photos is known, except that they are showing instances of the same category. The key idea is to train a deep neural network to generate 3D shapes which, when rendered to images, are indistinguishable from ground truth images (for a discriminator) under various camera poses. Discriminating 2D images instead of 3D shapes allows tapping into unstructured 2D photo collections instead of relying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish constraints between 2D image observation and their 3D interpretation, we suggest a family of rendering layers that are effectively differentiable. This family includes visual hull, absorption-only (akin to x-ray), and emission-absorption. We can successfully reconstruct 3D shapes from unstructured 2D images and extensively evaluate PlatonicGAN on a range of synthetic and real data sets achieving consistent improvements over baseline methods. We further show that PlatonicGAN can be combined with 3D supervision to improve on and in some cases even surpass the quality of 3D-supervised methods.",
        "中文标题": "逃离柏拉图的洞穴：基于对抗渲染的3D形状",
        "摘要翻译": "我们引入了PlatonicGAN，以从无结构的2D图像集合中发现对象类别的3D结构，即除了它们显示的是同一类别的实例外，照片之间没有已知的关系。关键思想是训练一个深度神经网络生成3D形状，这些形状在各种相机姿态下渲染成图像时，与真实图像（对于判别器）无法区分。通过判别2D图像而不是3D形状，可以利用无结构的2D照片集合，而不是依赖于经过整理的（例如，对齐的，注释的等）3D数据集。为了在2D图像观察和它们的3D解释之间建立约束，我们提出了一系列有效可微的渲染层。这个系列包括视觉外壳、仅吸收（类似于X射线）和发射-吸收。我们能够成功地从无结构的2D图像重建3D形状，并在广泛的合成和真实数据集上对PlatonicGAN进行了广泛评估，实现了对基线方法的持续改进。我们进一步展示了PlatonicGAN可以与3D监督结合使用，以改进并在某些情况下甚至超越3D监督方法的质量。",
        "领域": "3D重建/生成对抗网络/图像渲染",
        "问题": "从无结构的2D图像集合中发现对象类别的3D结构",
        "动机": "利用无结构的2D照片集合，而不是依赖于经过整理的3D数据集，以发现对象类别的3D结构",
        "方法": "训练一个深度神经网络生成3D形状，这些形状在各种相机姿态下渲染成图像时，与真实图像无法区分。提出了一系列有效可微的渲染层，包括视觉外壳、仅吸收和发射-吸收。",
        "关键词": [
            "3D重建",
            "生成对抗网络",
            "图像渲染"
        ],
        "涉及的技术概念": "PlatonicGAN是一种生成对抗网络，用于从无结构的2D图像集合中生成3D形状。通过训练深度神经网络，使得生成的3D形状在渲染成图像时与真实图像无法区分。提出的渲染层包括视觉外壳、仅吸收（类似于X射线）和发射-吸收，这些层都是有效可微的，用于在2D图像观察和它们的3D解释之间建立约束。"
    },
    {
        "order": 1000,
        "title": "Visualizing the Invisible: Occluded Vehicle Segmentation and Recovery",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Visualizing_the_Invisible_Occluded_Vehicle_Segmentation_and_Recovery_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Visualizing_the_Invisible_Occluded_Vehicle_Segmentation_and_Recovery_ICCV_2019_paper.html",
        "abstract": "In this paper, we propose a novel iterative multi-task framework to complete the segmentation mask of an occluded vehicle and recover the appearance of its invisible parts. In particular, firstly, to improve the quality of the segmentation completion, we present two coupled discriminators that introduce an auxiliary 3D model pool for sampling authentic silhouettes as adversarial samples. In addition, we propose a two-path structure with a shared network to enhance the appearance recovery capability. By iteratively performing the segmentation completion and the appearance recovery, the results will be progressively refined. To evaluate our method, we present a dataset, Occluded Vehicle dataset, containing synthetic and real-world occluded vehicle images. Based on this dataset, we conduct comparison experiments and demonstrate that our model outperforms the state-of-the-arts in both tasks of recovering segmentation mask and appearance for occluded vehicles. Moreover, we also demonstrate that our appearance recovery approach can benefit the occluded vehicle tracking in real-world videos.",
        "中文标题": "可视化不可见：遮挡车辆分割与恢复",
        "摘要翻译": "在本文中，我们提出了一种新颖的迭代多任务框架，用于完成遮挡车辆的分割掩码并恢复其不可见部分的外观。特别是，首先，为了提高分割完成的质量，我们提出了两个耦合的判别器，它们引入了一个辅助的3D模型池，用于采样真实的轮廓作为对抗样本。此外，我们提出了一种具有共享网络的双路径结构，以增强外观恢复能力。通过迭代执行分割完成和外观恢复，结果将逐步细化。为了评估我们的方法，我们提出了一个包含合成和真实世界遮挡车辆图像的数据集，遮挡车辆数据集。基于这个数据集，我们进行了比较实验，并证明我们的模型在恢复遮挡车辆的分割掩码和外观两项任务上均优于现有技术。此外，我们还证明了我们的外观恢复方法可以有益于真实世界视频中的遮挡车辆跟踪。",
        "领域": "遮挡处理/车辆识别/视频分析",
        "问题": "遮挡车辆的分割掩码完成和不可见部分的外观恢复",
        "动机": "提高遮挡车辆分割和外观恢复的质量，以支持遮挡车辆跟踪等应用",
        "方法": "提出了一种迭代多任务框架，包括使用耦合判别器和辅助3D模型池来提高分割完成质量，以及采用双路径结构增强外观恢复能力",
        "关键词": [
            "遮挡处理",
            "车辆识别",
            "视频分析",
            "分割掩码",
            "外观恢复"
        ],
        "涉及的技术概念": "迭代多任务框架、耦合判别器、辅助3D模型池、双路径结构、分割掩码完成、外观恢复、遮挡车辆数据集"
    },
    {
        "order": 1001,
        "title": "Deep End-to-End Alignment and Refinement for Time-of-Flight RGB-D Module",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiu_Deep_End-to-End_Alignment_and_Refinement_for_Time-of-Flight_RGB-D_Module_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qiu_Deep_End-to-End_Alignment_and_Refinement_for_Time-of-Flight_RGB-D_Module_ICCV_2019_paper.html",
        "abstract": "Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.",
        "中文标题": "深度端到端对齐与优化用于飞行时间RGB-D模块",
        "摘要翻译": "近年来，为移动RGB相机配备飞行时间（ToF）传感器以进行主动深度感知变得越来越流行。然而，对于现成的ToF传感器，为了获得相对于RGB相机的高质量深度，必须解决两个问题，即1）在线校准和对齐；以及2）ToF深度感知的复杂错误校正。在这项工作中，我们提出了一个通过深度学习联合对齐和优化的框架。首先，估计RGB图像和ToF幅度图像之间的跨模态光流以进行对齐。然后，通过改进的核预测网络对对齐的深度进行优化，该网络执行核归一化并在动态卷积之前应用偏差先验。为了丰富我们的端到端训练数据，我们还使用计算机图形学工具合成了一个数据集。实验结果证明了我们方法的有效性，在ToF优化方面达到了最先进的水平。",
        "领域": "深度感知/传感器融合/光流估计",
        "问题": "解决ToF传感器与RGB相机之间的在线校准、对齐以及ToF深度感知的复杂错误校正问题",
        "动机": "提高移动RGB相机配备ToF传感器进行深度感知的质量",
        "方法": "提出一个通过深度学习联合对齐和优化的框架，包括跨模态光流估计和核预测网络优化",
        "关键词": [
            "深度感知",
            "传感器融合",
            "光流估计",
            "核预测网络",
            "动态卷积"
        ],
        "涉及的技术概念": "飞行时间（ToF）传感器、RGB相机、跨模态光流、核预测网络、动态卷积、计算机图形学"
    },
    {
        "order": 1002,
        "title": "FrameNet: Learning Local Canonical Frames of 3D Surfaces From a Single RGB Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_FrameNet_Learning_Local_Canonical_Frames_of_3D_Surfaces_From_a_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Huang_FrameNet_Learning_Local_Canonical_Frames_of_3D_Surfaces_From_a_ICCV_2019_paper.html",
        "abstract": "In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image corresponds to a surface in the underlying 3D geometry, where a canonical frame can be identified as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB. Our first insight is that canonical frames computed automatically with recently introduced direction field synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3Dcanonical frames through projective geometry and orthogonality constraints. In our experiments, we find that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality.",
        "中文标题": "FrameNet：从单一RGB图像学习3D表面的局部规范框架",
        "摘要翻译": "在这项工作中，我们提出了从单一RGB图像识别密集的规范3D坐标框架的新问题。我们观察到，图像中的每个像素对应于底层3D几何中的一个表面，其中规范框架可以被识别为由三个正交轴表示，一个沿着其法线方向，两个在其切平面内。我们提出了一种从RGB预测这些轴的算法。我们的第一个见解是，使用最近引入的方向场合成方法自动计算的规范框架可以为该任务提供训练数据。我们的第二个见解是，当联合训练以预测规范框架时，设计用于表面法线预测的网络提供了更好的结果，并且在训练以预测规范框架的2D投影时效果更佳。我们推测这是因为规范切线方向的投影通常与图像中的局部梯度对齐，并且这些方向通过投影几何和正交性约束与3D规范框架紧密相连。在我们的实验中，我们发现我们的方法预测的3D规范框架可以用于从表面法线估计、特征匹配到增强现实的应用。",
        "领域": "3D重建/增强现实/图像分析",
        "问题": "从单一RGB图像识别密集的规范3D坐标框架",
        "动机": "探索如何从单一RGB图像中自动识别3D表面的局部规范框架，以支持表面法线估计、特征匹配和增强现实等应用",
        "方法": "提出了一种算法，利用方向场合成方法自动计算的规范框架作为训练数据，并通过联合训练网络预测规范框架及其2D投影来提高预测精度",
        "关键词": [
            "3D重建",
            "增强现实",
            "图像分析"
        ],
        "涉及的技术概念": "规范3D坐标框架、方向场合成方法、表面法线预测、2D投影、投影几何、正交性约束"
    },
    {
        "order": 1003,
        "title": "GEOBIT: A Geodesic-Based Binary Descriptor Invariant to Non-Rigid Deformations for RGB-D Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Nascimento_GEOBIT_A_Geodesic-Based_Binary_Descriptor_Invariant_to_Non-Rigid_Deformations_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Nascimento_GEOBIT_A_Geodesic-Based_Binary_Descriptor_Invariant_to_Non-Rigid_Deformations_for_ICCV_2019_paper.html",
        "abstract": "At the core of most three-dimensional alignment and tracking tasks resides the critical problem of point correspondence. In this context, the design of descriptors that efficiently and uniquely identifies keypoints, to be matched, is of central importance. Numerous descriptors have been developed for dealing with affine/perspective warps, but few can also handle non-rigid deformations. In this paper, we introduce a novel binary RGB-D descriptor invariant to isometric deformations. Our method uses geodesic isocurves on smooth textured manifolds. It combines appearance and geometric information from RGB-D images to tackle non-rigid transformations. We used our descriptor to track multiple textured depth maps and demonstrate that it produces reliable feature descriptors even in the presence of strong non-rigid deformations and depth noise. The experiments show that our descriptor outperforms different state-of-the-art descriptors in both precision-recall and recognition rate metrics. We also provide to the community a new dataset composed of annotated RGB-D images of different objects (shirts, cloths, paintings, bags), subjected to strong non-rigid deformations, to evaluate point correspondence algorithms.",
        "中文标题": "GEOBIT: 一种基于测地线的对非刚性变形不变的RGB-D图像二进制描述符",
        "摘要翻译": "在大多数三维对齐和跟踪任务的核心中，存在着点对应的关键问题。在这一背景下，设计能够高效且唯一地识别关键点以进行匹配的描述符至关重要。已经开发了许多用于处理仿射/透视扭曲的描述符，但很少有能够处理非刚性变形的。在本文中，我们介绍了一种新的对等距变形不变的二进制RGB-D描述符。我们的方法使用光滑纹理流形上的测地线等值线。它结合了RGB-D图像的外观和几何信息来处理非刚性变换。我们使用我们的描述符来跟踪多个纹理深度图，并证明即使在存在强烈非刚性变形和深度噪声的情况下，它也能产生可靠的特征描述符。实验表明，我们的描述符在精度-召回率和识别率指标上均优于不同的最先进描述符。我们还向社区提供了一个新的数据集，该数据集由不同物体（衬衫、布料、画作、包）的注释RGB-D图像组成，这些物体经历了强烈的非刚性变形，用于评估点对应算法。",
        "领域": "三维重建/特征描述/非刚性变形",
        "问题": "在存在非刚性变形和深度噪声的情况下，如何高效且唯一地识别关键点以进行匹配",
        "动机": "现有的描述符大多只能处理仿射/透视扭曲，而无法有效处理非刚性变形，因此需要开发一种新的描述符来解决这一问题",
        "方法": "使用光滑纹理流形上的测地线等值线，结合RGB-D图像的外观和几何信息来处理非刚性变换",
        "关键词": [
            "三维对齐",
            "点对应",
            "非刚性变形",
            "RGB-D描述符",
            "测地线等值线"
        ],
        "涉及的技术概念": "测地线等值线是一种在光滑纹理流形上定义的曲线，用于捕捉物体的几何和外观信息。RGB-D图像结合了颜色（RGB）和深度（D）信息，提供了物体的三维结构。非刚性变形指的是物体形状的变形，这种变形不保持物体的刚性结构，如布料或软物体的变形。"
    },
    {
        "order": 1004,
        "title": "CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lukezic_CDTB_A_Color_and_Depth_Visual_Object_Tracking_Dataset_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lukezic_CDTB_A_Color_and_Depth_Visual_Object_Tracking_Dataset_and_ICCV_2019_paper.html",
        "abstract": "We propose a new color-and-depth general visual object tracking benchmark (CDTB). CDTB is recorded by several passive and active RGB-D setups and contains indoor as well as outdoor sequences acquired in direct sunlight. The CDTB dataset is the largest and most diverse dataset in RGB-D tracking, with an order of magnitude larger number of frames than related datasets. The sequences have been carefully recorded to contain significant object pose change, clutter, occlusion, and periods of long-term target absence to enable tracker evaluation under realistic conditions. Sequences are per-frame annotated with 13 visual attributes for detailed analysis. Experiments with RGB and RGB-D trackers show that CDTB is more challenging than previous datasets. State-of-the-art RGB trackers outperform the recent RGB-D trackers, indicating a large gap between the two fields, which has not been previously detected by the prior benchmarks. Based on the results of the analysis we point out opportunities for future research in RGB-D tracker design.",
        "中文标题": "CDTB: 一个颜色和深度视觉对象跟踪数据集和基准",
        "摘要翻译": "我们提出了一个新的颜色和深度通用视觉对象跟踪基准（CDTB）。CDTB由多个被动和主动的RGB-D设置记录，包含在直射阳光下获取的室内和室外序列。CDTB数据集是RGB-D跟踪中最大且最多样化的数据集，其帧数比相关数据集大一个数量级。这些序列经过精心记录，包含显著的对象姿态变化、杂乱、遮挡和长期目标缺失的时期，以便在现实条件下进行跟踪器评估。序列每帧都标注了13个视觉属性，用于详细分析。使用RGB和RGB-D跟踪器进行的实验表明，CDTB比以前的数据集更具挑战性。最先进的RGB跟踪器优于最近的RGB-D跟踪器，表明这两个领域之间存在巨大差距，这是以前的基准测试未检测到的。基于分析结果，我们指出了RGB-D跟踪器设计未来研究的机会。",
        "领域": "视觉对象跟踪/RGB-D跟踪/基准测试",
        "问题": "解决RGB-D视觉对象跟踪的挑战，提供一个更全面、更具挑战性的数据集和基准",
        "动机": "现有的RGB-D视觉对象跟踪数据集在规模和多样性上存在不足，无法充分评估跟踪器在现实条件下的性能",
        "方法": "提出一个新的颜色和深度通用视觉对象跟踪基准（CDTB），包含室内外序列，记录时考虑对象姿态变化、杂乱、遮挡和长期目标缺失等因素，每帧标注13个视觉属性",
        "关键词": [
            "视觉对象跟踪",
            "RGB-D跟踪",
            "基准测试"
        ],
        "涉及的技术概念": {
            "RGB-D跟踪": "使用RGB颜色信息和深度信息进行视觉对象跟踪的技术",
            "视觉属性": "用于描述和分类视觉对象特征的属性，如颜色、形状、纹理等",
            "基准测试": "通过一系列标准化的测试来评估和比较不同系统或算法性能的过程"
        }
    },
    {
        "order": 1005,
        "title": "Learning Joint 2D-3D Representations for Depth Completion",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Learning_Joint_2D-3D_Representations_for_Depth_Completion_ICCV_2019_paper.html",
        "abstract": "In this paper, we tackle the problem of depth completion from RGBD data. Towards this goal, we design a simple yet effective neural network block that learns to extract joint 2D and 3D features. Specifically, the block consists of two domain-specific sub-networks that apply 2D convolution on image pixels and continuous convolution on 3D points, with their output features fused in image space. We build the depth completion network simply by stacking the proposed block, which has the advantage of learning hierarchical representations that are fully fused between 2D and 3D spaces at multiple levels. We demonstrate the effectiveness of our approach on the challenging KITTI depth completion benchmark and show that our approach outperforms the state-of-the-art.",
        "中文标题": "学习用于深度补全的联合2D-3D表示",
        "摘要翻译": "在本文中，我们解决了从RGBD数据进行深度补全的问题。为此，我们设计了一个简单而有效的神经网络块，该块学习提取联合的2D和3D特征。具体来说，该块由两个特定领域的子网络组成，这两个子网络分别在图像像素上应用2D卷积和在3D点上应用连续卷积，它们的输出特征在图像空间中融合。我们通过堆叠所提出的块来构建深度补全网络，这样做的好处是可以在多个层次上学习到2D和3D空间之间完全融合的分层表示。我们在具有挑战性的KITTI深度补全基准上展示了我们方法的有效性，并表明我们的方法优于最先进的技术。",
        "领域": "深度补全/3D视觉/卷积神经网络",
        "问题": "从RGBD数据进行深度补全",
        "动机": "解决从RGBD数据进行深度补全的问题，以提高深度补全的准确性和效率",
        "方法": "设计了一个简单而有效的神经网络块，该块由两个特定领域的子网络组成，分别在图像像素上应用2D卷积和在3D点上应用连续卷积，输出特征在图像空间中融合，通过堆叠这些块来构建深度补全网络",
        "关键词": [
            "深度补全",
            "3D视觉",
            "卷积神经网络"
        ],
        "涉及的技术概念": "RGBD数据、2D卷积、3D点、连续卷积、图像空间融合、分层表示、KITTI深度补全基准"
    },
    {
        "order": 1006,
        "title": "Make a Face: Towards Arbitrary High Fidelity Face Manipulation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_Make_a_Face_Towards_Arbitrary_High_Fidelity_Face_Manipulation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_Make_a_Face_Towards_Arbitrary_High_Fidelity_Face_Manipulation_ICCV_2019_paper.html",
        "abstract": "Recent studies have shown remarkable success in face manipulation task with the advance of GANs and VAEs paradigms, but the outputs are sometimes limited to low-resolution and lack of diversity. In this work, we propose Additive Focal Variational Auto-encoder (AF-VAE), a novel approach that can arbitrarily manipulate high-resolution face images using a simple yet effective model and only weak supervision of reconstruction and KL divergence losses. First, a novel additive Gaussian Mixture assumption is introduced with an unsupervised clustering mechanism in the structural latent space, which endows better disentanglement and boosts multi-modal representation with external memory. Second, to improve the perceptual quality of synthesized results, two simple strategies in architecture design are further tailored and discussed on the behavior of Human Visual System (HVS) for the first time, allowing for fine control over the model complexity and sample quality. Human opinion studies and new state-of-the-art Inception Score (IS) / Frechet Inception Distance (FID) demonstrate the superiority of our approach over existing algorithms, advancing both the fidelity and extremity of face manipulation task.",
        "中文标题": "制作面部：朝向任意高保真面部操作",
        "摘要翻译": "最近的研究表明，随着GANs和VAEs范式的发展，面部操作任务取得了显著的成功，但输出有时仅限于低分辨率且缺乏多样性。在这项工作中，我们提出了加性焦点变分自编码器（AF-VAE），这是一种新颖的方法，可以使用简单而有效的模型和仅重建和KL散度损失的弱监督，任意操作高分辨率面部图像。首先，在结构潜在空间中引入了一种新颖的加性高斯混合假设，并带有无监督聚类机制，这赋予了更好的解缠能力，并通过外部记忆增强了多模态表示。其次，为了提高合成结果的感知质量，首次在架构设计中针对人类视觉系统（HVS）的行为定制并讨论了两个简单策略，允许对模型复杂性和样本质量进行精细控制。人类意见研究和新的最先进的Inception Score（IS）/Frechet Inception Distance（FID）证明了我们的方法相对于现有算法的优越性，推动了面部操作任务的保真度和极端性。",
        "领域": "面部生成/图像合成/变分自编码器",
        "问题": "解决高分辨率面部图像操作中的低分辨率和缺乏多样性的问题",
        "动机": "提高面部操作任务的保真度和极端性，通过引入新的方法来解决现有技术的限制",
        "方法": "提出加性焦点变分自编码器（AF-VAE），采用加性高斯混合假设和无监督聚类机制，以及针对人类视觉系统行为的架构设计策略",
        "关键词": [
            "面部生成",
            "图像合成",
            "变分自编码器",
            "高分辨率",
            "多模态表示"
        ],
        "涉及的技术概念": "GANs（生成对抗网络）、VAEs（变分自编码器）、加性高斯混合假设、无监督聚类机制、人类视觉系统（HVS）、Inception Score（IS）、Frechet Inception Distance（FID）"
    },
    {
        "order": 1007,
        "title": "M2FPA: A Multi-Yaw Multi-Pitch High-Quality Dataset and Benchmark for Facial Pose Analysis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_M2FPA_A_Multi-Yaw_Multi-Pitch_High-Quality_Dataset_and_Benchmark_for_Facial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_M2FPA_A_Multi-Yaw_Multi-Pitch_High-Quality_Dataset_and_Benchmark_for_Facial_ICCV_2019_paper.html",
        "abstract": "Facial images in surveillance or mobile scenarios often have large view-point variations in terms of pitch and yaw angles. These jointly occurred angle variations make face recognition challenging. Current public face databases mainly consider the case of yaw variations. In this paper, a new large-scale Multi-yaw Multi-pitch high-quality database is proposed for Facial Pose Analysis (M2FPA), including face frontalization, face rotation, facial pose estimation and pose-invariant face recognition. It contains 397,544 images of 229 subjects with yaw, pitch, attribute, illumination and accessory. M2FPA is the most comprehensive multi-view face database for facial pose analysis. Further, we provide an effective benchmark for face frontalization and pose-invariant face recognition on M2FPA with several state-of-the-art methods, including DR-GAN, TP-GAN and CAPG-GAN. We believe that the new database and benchmark can significantly push forward the advance of facial pose analysis in real-world applications. Moreover, a simple yet effective parsing guided discriminator is introduced to capture the local consistency during GAN optimization. Extensive quantitative and qualitative results on M2FPA and Multi-PIE demonstrate the superiority of our face frontalization method. Baseline results for both face synthesis and face recognition from state-of-the-art methods demonstrate the challenge offered by this new database.",
        "中文标题": "M2FPA: 一个用于面部姿态分析的多偏航多俯仰高质量数据集和基准",
        "摘要翻译": "在监控或移动场景中，面部图像通常在俯仰和偏航角度上有很大的视角变化。这些共同发生的角度变化使得面部识别变得具有挑战性。当前的公共面部数据库主要考虑偏航变化的情况。本文提出了一个新的用于面部姿态分析（M2FPA）的大规模多偏航多俯仰高质量数据库，包括面部正面化、面部旋转、面部姿态估计和姿态不变的面部识别。它包含了229个主题的397,544张图像，具有偏航、俯仰、属性、光照和配饰。M2FPA是用于面部姿态分析的最全面的多视角面部数据库。此外，我们为M2FPA上的面部正面化和姿态不变的面部识别提供了一个有效的基准，包括DR-GAN、TP-GAN和CAPG-GAN等几种最先进的方法。我们相信，新的数据库和基准可以显著推动面部姿态分析在实际应用中的进步。此外，引入了一个简单但有效的解析引导判别器，以在GAN优化过程中捕捉局部一致性。在M2FPA和Multi-PIE上的广泛定量和定性结果证明了我们面部正面化方法的优越性。从最先进的方法中获得的面部合成和面部识别的基线结果展示了这个新数据库提供的挑战。",
        "领域": "面部识别/姿态估计/生成对抗网络",
        "问题": "解决在监控或移动场景中，由于俯仰和偏航角度的大变化导致的面部识别挑战",
        "动机": "推动面部姿态分析在实际应用中的进步，提供更全面的多视角面部数据库",
        "方法": "提出了一个新的用于面部姿态分析的大规模多偏航多俯仰高质量数据库M2FPA，并提供了一个有效的基准，包括DR-GAN、TP-GAN和CAPG-GAN等几种最先进的方法，以及引入了一个简单但有效的解析引导判别器",
        "关键词": [
            "面部正面化",
            "面部旋转",
            "姿态不变的面部识别"
        ],
        "涉及的技术概念": {
            "M2FPA": "一个用于面部姿态分析的大规模多偏航多俯仰高质量数据库",
            "DR-GAN": "一种用于面部正面化的生成对抗网络方法",
            "TP-GAN": "一种用于面部正面化的生成对抗网络方法",
            "CAPG-GAN": "一种用于面部正面化的生成对抗网络方法",
            "解析引导判别器": "在GAN优化过程中用于捕捉局部一致性的技术"
        }
    },
    {
        "order": 1008,
        "title": "Fair Loss: Margin-Aware Reinforcement Learning for Deep Face Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Fair_Loss_Margin-Aware_Reinforcement_Learning_for_Deep_Face_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Fair_Loss_Margin-Aware_Reinforcement_Learning_for_Deep_Face_Recognition_ICCV_2019_paper.html",
        "abstract": "Recently, large-margin softmax loss methods, such as angular softmax loss (SphereFace), large margin cosine loss (CosFace), and additive angular margin loss (ArcFace), have demonstrated impressive performance on deep face recognition. These methods incorporate a fixed additive margin to all the classes, ignoring the class imbalance problem. However, imbalanced problem widely exists in various real-world face datasets, in which samples from some classes are in a higher number than others. We argue that the number of a class would influence its demand for the additive margin. In this paper, we introduce a new margin-aware reinforcement learning based loss function, namely fair loss, in which each class will learn an appropriate adaptive margin by Deep Q-learning. Specifically, we train an agent to learn a margin adaptive strategy for each class, and make the additive margins for different classes more reasonable. Our method has better performance than present large-margin loss functions on three benchmarks, Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace, which demonstrates that our method could learn better face representation on imbalanced face datasets.",
        "中文标题": "公平损失：基于深度人脸识别的边缘感知强化学习",
        "摘要翻译": "最近，大边缘softmax损失方法，如角度softmax损失（SphereFace）、大边缘余弦损失（CosFace）和加性角度边缘损失（ArcFace），在深度人脸识别方面展示了令人印象深刻的性能。这些方法为所有类别引入了固定的加性边缘，忽略了类别不平衡问题。然而，不平衡问题广泛存在于各种现实世界的人脸数据集中，其中某些类别的样本数量远多于其他类别。我们认为，一个类别的数量会影响其对加性边缘的需求。在本文中，我们引入了一种新的基于边缘感知强化学习的损失函数，即公平损失，其中每个类别将通过深度Q学习学习一个适当的自适应边缘。具体来说，我们训练一个代理来学习每个类别的边缘自适应策略，并使不同类别的加性边缘更加合理。我们的方法在三个基准测试上，即Labeled Face in the Wild (LFW)、Youtube Faces (YTF)和MegaFace，表现优于现有的大边缘损失函数，这表明我们的方法能够在不平衡的人脸数据集上学习到更好的人脸表示。",
        "领域": "人脸识别/强化学习/损失函数优化",
        "问题": "解决深度人脸识别中的类别不平衡问题",
        "动机": "现有的大边缘softmax损失方法忽略了类别不平衡问题，导致在现实世界的人脸数据集中表现不佳",
        "方法": "引入了一种新的基于边缘感知强化学习的损失函数，即公平损失，通过深度Q学习为每个类别学习一个适当的自适应边缘",
        "关键词": [
            "人脸识别",
            "强化学习",
            "损失函数优化",
            "类别不平衡",
            "自适应边缘"
        ],
        "涉及的技术概念": {
            "大边缘softmax损失方法": "一种在深度学习中用于提高分类性能的技术，通过增加类别间的边缘来增强模型的判别能力",
            "深度Q学习": "一种强化学习算法，用于学习在给定状态下采取何种行动以最大化累积奖励",
            "类别不平衡问题": "在数据集中，某些类别的样本数量远多于其他类别，导致模型在训练过程中偏向于多数类，影响模型性能"
        }
    },
    {
        "order": 1009,
        "title": "Face De-Occlusion Using 3D Morphable Model and Generative Adversarial Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Face_De-Occlusion_Using_3D_Morphable_Model_and_Generative_Adversarial_Network_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yuan_Face_De-Occlusion_Using_3D_Morphable_Model_and_Generative_Adversarial_Network_ICCV_2019_paper.html",
        "abstract": "In recent decades, 3D morphable model (3DMM) has been commonly used in image-based photorealistic 3D face reconstruction. However, face images are often corrupted by serious occlusion by non-face objects including eyeglasses, masks, and hands. Such objects block the correct capture of landmarks and shading information. Therefore, the reconstructed 3D face model is hardly reusable. In this paper, a novel method is proposed to restore de-occluded face images based on inverse use of 3DMM and generative adversarial network. We utilize the 3DMM prior to the proposed adversarial network and combine a global and local adversarial convolutional neural network to learn face de-occlusion model. The 3DMM serves not only as geometric prior but also proposes the face region for the local discriminator. Experiment results confirm the effectiveness and robustness of the proposed algorithm in removing challenging types of occlusions with various head poses and illumination. Furthermore, the proposed method reconstructs the correct 3D face model with de-occluded textures.",
        "中文标题": "使用3D可变形模型和生成对抗网络进行人脸去遮挡",
        "摘要翻译": "近几十年来，3D可变形模型（3DMM）已被广泛用于基于图像的真实感3D人脸重建。然而，人脸图像经常被非人脸物体（如眼镜、口罩和手）严重遮挡。这些物体阻碍了地标和阴影信息的正确捕捉。因此，重建的3D人脸模型几乎无法重复使用。本文提出了一种基于3DMM和生成对抗网络逆向使用的新方法，以恢复去遮挡的人脸图像。我们利用3DMM作为先验知识，结合全局和局部对抗卷积神经网络来学习人脸去遮挡模型。3DMM不仅作为几何先验，还为局部判别器提出了人脸区域。实验结果证实了所提出算法在去除具有各种头部姿势和光照的挑战性遮挡类型方面的有效性和鲁棒性。此外，所提出的方法重建了具有去遮挡纹理的正确3D人脸模型。",
        "领域": "3D人脸重建/生成对抗网络/图像恢复",
        "问题": "人脸图像被非人脸物体严重遮挡，导致3D人脸模型重建困难",
        "动机": "解决因遮挡导致的人脸图像重建问题，提高3D人脸模型的重建质量和可用性",
        "方法": "结合3D可变形模型（3DMM）和生成对抗网络（GAN），利用3DMM作为几何先验，结合全局和局部对抗卷积神经网络学习人脸去遮挡模型",
        "关键词": [
            "3D可变形模型",
            "生成对抗网络",
            "人脸去遮挡",
            "图像恢复"
        ],
        "涉及的技术概念": {
            "3D可变形模型（3DMM）": "一种用于3D人脸重建的技术，能够根据2D图像生成3D人脸模型",
            "生成对抗网络（GAN）": "一种深度学习模型，通过生成器和判别器的对抗学习生成新的数据样本",
            "对抗卷积神经网络": "一种特殊的卷积神经网络，用于在生成对抗网络中作为判别器，区分真实数据和生成数据"
        }
    },
    {
        "order": 1010,
        "title": "Detecting Photoshopped Faces by Scripting Photoshop",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Detecting_Photoshopped_Faces_by_Scripting_Photoshop_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Detecting_Photoshopped_Faces_by_Scripting_Photoshop_ICCV_2019_paper.html",
        "abstract": "Most malicious photo manipulations are created using standard image editing tools, such as Adobe Photoshop. We present a method for detecting one very popular Photoshop manipulation -- image warping applied to human faces -- using a model trained entirely using fake images that were automatically generated by scripting Photoshop itself. We show that our model outperforms humans at the task of recognizing manipulated images, can predict the specific location of edits, and in some cases can be used to \"undo\" a manipulation to reconstruct the original, unedited image. We demonstrate that the system can be successfully applied to artist-created image manipulations.",
        "中文标题": "通过脚本化Photoshop检测Photoshop处理过的人脸",
        "摘要翻译": "大多数恶意的照片篡改都是使用标准的图像编辑工具，如Adobe Photoshop创建的。我们提出了一种检测一种非常流行的Photoshop篡改——应用于人脸的图像扭曲——的方法，使用完全由通过脚本化Photoshop本身自动生成的假图像训练的模型。我们展示了我们的模型在识别篡改图像的任务上优于人类，可以预测编辑的具体位置，并且在某些情况下可以用来“撤销”篡改以重建原始的、未编辑的图像。我们证明了该系统可以成功地应用于艺术家创作的图像篡改。",
        "领域": "图像篡改检测/人脸识别/图像恢复",
        "问题": "检测使用Photoshop对人脸进行的图像扭曲篡改",
        "动机": "为了有效识别和定位通过Photoshop等标准图像编辑工具进行的人脸图像篡改，以及探索篡改图像的恢复可能性",
        "方法": "使用通过脚本化Photoshop自动生成的假图像训练的模型进行检测和定位，并尝试恢复原始图像",
        "关键词": [
            "图像篡改检测",
            "人脸识别",
            "图像恢复"
        ],
        "涉及的技术概念": "图像扭曲篡改检测、Photoshop脚本化、假图像生成、图像恢复技术"
    },
    {
        "order": 1011,
        "title": "Ego-Pose Estimation and Forecasting As Real-Time PD Control",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.html",
        "abstract": "We propose the use of a proportional-derivative (PD) control based policy learned via reinforcement learning (RL) to estimate and forecast 3D human pose from egocentric videos. The method learns directly from unsegmented egocentric videos and motion capture data consisting of various complex human motions (e.g., crouching, hopping, bending, and motion transitions). We propose a video-conditioned recurrent control technique to forecast physically-valid and stable future motions of arbitrary length. We also introduce a value function based fail-safe mechanism which enables our method to run as a single pass algorithm over the video data. Experiments with both controlled and in-the-wild data show that our approach outperforms previous art in both quantitative metrics and visual quality of the motions, and is also robust enough to transfer directly to real-world scenarios. Additionally, our time analysis shows that the combined use of our pose estimation and forecasting can run at 30 FPS, making it suitable for real-time applications.",
        "中文标题": "自我姿态估计与预测作为实时PD控制",
        "摘要翻译": "我们提出了一种基于比例-微分（PD）控制策略的方法，通过强化学习（RL）从自我中心视频中估计和预测3D人体姿态。该方法直接从包含各种复杂人体动作（如蹲下、跳跃、弯曲和动作转换）的未分割自我中心视频和动作捕捉数据中学习。我们提出了一种视频条件递归控制技术，以预测任意长度的物理有效且稳定的未来动作。我们还引入了一种基于价值函数的故障安全机制，这使得我们的方法能够作为视频数据的单次通过算法运行。使用受控数据和野外数据的实验表明，我们的方法在定量指标和动作的视觉质量方面均优于以往的技术，并且足够稳健，可以直接转移到现实世界场景中。此外，我们的时间分析显示，结合使用我们的姿态估计和预测可以以30 FPS的速度运行，使其适用于实时应用。",
        "领域": "人体姿态估计/动作预测/实时系统",
        "问题": "从自我中心视频中实时估计和预测3D人体姿态",
        "动机": "提高从自我中心视频中估计和预测3D人体姿态的准确性和实时性，以适应实时应用需求",
        "方法": "采用基于比例-微分（PD）控制策略的强化学习方法，结合视频条件递归控制技术和基于价值函数的故障安全机制",
        "关键词": [
            "3D人体姿态估计",
            "动作预测",
            "实时系统",
            "强化学习",
            "比例-微分控制"
        ],
        "涉及的技术概念": "比例-微分（PD）控制是一种经典的控制策略，用于调整系统的输出以接近期望的目标。强化学习（RL）是一种机器学习方法，通过奖励机制来学习策略。视频条件递归控制技术是一种利用视频信息来预测未来动作的方法。基于价值函数的故障安全机制是一种确保算法稳定运行的技术。"
    },
    {
        "order": 1012,
        "title": "End-to-End Learning for Graph Decomposition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Song_End-to-End_Learning_for_Graph_Decomposition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Song_End-to-End_Learning_for_Graph_Decomposition_ICCV_2019_paper.html",
        "abstract": "Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.",
        "中文标题": "图分解的端到端学习",
        "摘要翻译": "深度神经网络为模式识别提供了强大的工具，而经典图算法广泛用于解决组合问题。在计算机视觉中，许多任务结合了模式识别和图推理的元素。在本文中，我们研究了如何将深度网络与图分解连接成一个端到端可训练的框架。更具体地说，首先将最小成本多割问题转换为一个无约束的二元三次公式，其中循环一致性约束被纳入目标函数。这个新的优化问题可以被视为一个条件随机场（CRF），其中随机变量与二元边标签相关联。循环约束作为高阶势被引入CRF。一个标准的卷积神经网络（CNN）为完全可微的CRF提供前端特征。两部分的参数都以端到端的方式进行优化。通过在MNIST图像聚类和现实世界多人姿态估计的挑战性任务上的实验，证明了所提出的学习算法的有效性。",
        "领域": "图分解/条件随机场/卷积神经网络",
        "问题": "如何将深度网络与图分解结合，形成一个端到端可训练的框架",
        "动机": "结合深度神经网络和图算法的优势，解决计算机视觉中结合模式识别和图推理的任务",
        "方法": "将最小成本多割问题转换为无约束的二元三次公式，并引入循环一致性约束到目标函数中，构建一个条件随机场（CRF），其中随机变量与二元边标签相关联，循环约束作为高阶势被引入CRF，使用卷积神经网络（CNN）为CRF提供前端特征，并以端到端的方式优化参数",
        "关键词": [
            "图分解",
            "条件随机场",
            "卷积神经网络",
            "端到端学习",
            "最小成本多割问题"
        ],
        "涉及的技术概念": {
            "深度神经网络": "用于模式识别的强大工具",
            "图算法": "用于解决组合问题的经典算法",
            "条件随机场（CRF）": "一种统计建模方法，用于结构化预测",
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "端到端学习": "一种学习方法，直接从输入到输出进行学习，无需手动设计特征"
        }
    },
    {
        "order": 1013,
        "title": "Laplace Landmark Localization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Robinson_Laplace_Landmark_Localization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Robinson_Laplace_Landmark_Localization_ICCV_2019_paper.html",
        "abstract": "Landmark localization in images and videos is a classic problem solved in various ways. Nowadays, with deep networks prevailing throughout machine learning, there are revamped interests in pushing facial landmark detectors to handle more challenging data. Most efforts use network objectives based on L1 or L2 norms, which have several disadvantages. First of all, the generated heatmaps translate to the locations of landmarks (i.e. confidence maps) from which predicted landmark locations (i.e. the means) get penalized without accounting for the spread: a high- scatter corresponds to low confidence and vice-versa. For this, we introduce a LaplaceKL objective that penalizes for low confidence. Another issue is a dependency on labeled data, which are expensive to obtain and susceptible to error. To address both issues, we propose an adversarial training framework that leverages unlabeled data to improve model performance. Our method claims state-of-the-art on all of the 300W benchmarks and ranks second-to-best on the Annotated Facial Landmarks in the Wild (AFLW) dataset. Furthermore, our model is robust with a reduced size: 1/8 the number of channels (i.e. 0.0398 MB) is comparable to the state-of-the-art in real-time on CPU. Thus, this work is of high practical value to real-life application.",
        "中文标题": "拉普拉斯地标定位",
        "摘要翻译": "图像和视频中的地标定位是一个以多种方式解决的经典问题。如今，随着深度网络在机器学习中的普及，人们对推动面部地标检测器处理更具挑战性的数据重新产生了兴趣。大多数努力使用基于L1或L2范数的网络目标，这些目标有几个缺点。首先，生成的heatmaps转化为地标的位置（即置信度图），从中预测的地标位置（即均值）在没有考虑散布的情况下受到惩罚：高散布对应于低置信度，反之亦然。为此，我们引入了一个LaplaceKL目标，对低置信度进行惩罚。另一个问题是对标记数据的依赖，这些数据获取成本高且容易出错。为了解决这两个问题，我们提出了一个对抗训练框架，利用未标记的数据来提高模型性能。我们的方法在所有300W基准测试中声称达到了最先进的水平，并在野外注释面部地标（AFLW）数据集中排名第二。此外，我们的模型在尺寸减小的情况下仍然稳健：通道数的1/8（即0.0398 MB）在CPU上实时运行时与最先进的技术相当。因此，这项工作对现实生活中的应用具有很高的实用价值。",
        "领域": "面部地标检测/对抗训练/实时处理",
        "问题": "提高面部地标检测器在更具挑战性数据上的性能",
        "动机": "现有的基于L1或L2范数的网络目标存在对地标位置预测的惩罚不考虑散布的问题，以及对标记数据的依赖问题",
        "方法": "引入LaplaceKL目标对低置信度进行惩罚，并提出一个对抗训练框架利用未标记的数据来提高模型性能",
        "关键词": [
            "面部地标检测",
            "对抗训练",
            "实时处理",
            "LaplaceKL目标",
            "未标记数据"
        ],
        "涉及的技术概念": "LaplaceKL目标是一种新的网络目标，用于对低置信度的地标位置预测进行惩罚。对抗训练框架是一种利用未标记数据来提高模型性能的方法。300W和AFLW是面部地标检测的基准测试数据集。"
    },
    {
        "order": 1014,
        "title": "Through-Wall Human Mesh Recovery Using Radio Signals",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Through-Wall_Human_Mesh_Recovery_Using_Radio_Signals_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Through-Wall_Human_Mesh_Recovery_Using_Radio_Signals_ICCV_2019_paper.html",
        "abstract": "This paper presents RF-Avatar, a neural network model that can estimate 3D meshes of the human body in the presence of occlusions, baggy clothes, and bad lighting conditions. We leverage that radio frequency (RF) signals in the WiFi range traverse clothes and occlusions and bounce off the human body. Our model parses such radio signals and recovers 3D body meshes. Our meshes are dynamic and smoothly track the movements of the corresponding people. Further, our model works both in single and multi-person scenarios. Inferring body meshes from radio signals is a highly under-constrained problem. Our model deals with this challenge using: 1) a combination of strong and weak supervision, 2) a multi-headed self-attention mechanism that attends differently to temporal information in the radio signal, and 3) an adversarially trained temporal discriminator that imposes a prior on the dynamics of human motion. Our results show that RF-Avatar accurately recovers dynamic 3D meshes in the presence of occlusions, baggy clothes, bad lighting conditions, and even through walls.",
        "中文标题": "使用无线电信号进行穿墙人体网格恢复",
        "摘要翻译": "本文介绍了RF-Avatar，一种神经网络模型，能够在存在遮挡、宽松衣物和不良光照条件下估计人体的3D网格。我们利用WiFi范围内的射频（RF）信号穿透衣物和遮挡物并从人体反射的特性。我们的模型解析这些射频信号并恢复3D人体网格。我们的网格是动态的，并能平滑地跟踪相应人员的运动。此外，我们的模型在单人和多人场景中均能工作。从射频信号推断人体网格是一个高度欠约束的问题。我们的模型通过以下方式应对这一挑战：1）结合强监督和弱监督，2）一个多头自注意力机制，该机制对射频信号中的时间信息进行不同的关注，以及3）一个对抗训练的时间判别器，该判别器对人体运动的动态施加先验。我们的结果表明，RF-Avatar在存在遮挡、宽松衣物、不良光照条件甚至穿墙的情况下，都能准确恢复动态3D网格。",
        "领域": "无线感知/3D重建/人体姿态估计",
        "问题": "在遮挡、宽松衣物和不良光照条件下，从射频信号中恢复动态3D人体网格",
        "动机": "解决在复杂环境下（如遮挡、宽松衣物、不良光照条件甚至穿墙）准确恢复动态3D人体网格的挑战",
        "方法": "结合强监督和弱监督、多头自注意力机制、对抗训练的时间判别器",
        "关键词": [
            "无线感知",
            "3D重建",
            "人体姿态估计"
        ],
        "涉及的技术概念": "射频信号（RF signals）用于穿透衣物和遮挡物并从人体反射，神经网络模型解析这些信号以恢复3D人体网格。模型采用强监督和弱监督结合、多头自注意力机制对时间信息进行不同关注，以及对抗训练的时间判别器对人体运动的动态施加先验。"
    },
    {
        "order": 1015,
        "title": "Discriminatively Learned Convex Models for Set Based Face Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Cevikalp_Discriminatively_Learned_Convex_Models_for_Set_Based_Face_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Cevikalp_Discriminatively_Learned_Convex_Models_for_Set_Based_Face_Recognition_ICCV_2019_paper.html",
        "abstract": "Majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual's face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generative convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on four of the five tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX and ESOGU video datasets.",
        "中文标题": "基于集合的人脸识别中的判别性学习凸模型",
        "摘要翻译": "大多数基于图像集合的人脸识别方法使用为每个人独立学习的生成模型，这种方法忽略了图库中其他人的信息。与这些方法不同，本文介绍了一种新颖的方法，该方法寻找最适合个人面部图像但同时尽可能远离图库中其他人图像的判别性凸模型。我们为图像集合的仿射和凸包学习判别性凸模型。在测试期间，通过使用简单的矩阵乘法有效地计算查询集图像到这些模型的距离，并将查询集分配给图库中图像集最接近查询图像的人。所提出的方法在使用生成凸模型的其他方法上在准确性和测试时间方面显著优于其他方法，并在五个测试数据集中的四个上实现了最先进的结果。特别是在具有挑战性的PaSC、COX和ESOGU视频数据集上，准确性的提高尤为显著。",
        "领域": "人脸识别/图像集合分析/视频分析",
        "问题": "解决基于图像集合的人脸识别中生成模型忽略图库中其他人信息的问题",
        "动机": "提高人脸识别的准确性和效率，特别是在具有挑战性的视频数据集上",
        "方法": "学习判别性凸模型，这些模型最适合个人面部图像同时尽可能远离图库中其他人的图像，并在测试时通过简单矩阵乘法计算距离",
        "关键词": [
            "人脸识别",
            "图像集合",
            "判别性学习",
            "凸模型",
            "视频分析"
        ],
        "涉及的技术概念": "判别性凸模型、仿射和凸包、矩阵乘法、生成模型与判别模型的对比"
    },
    {
        "order": 1016,
        "title": "Camera Distance-Aware Top-Down Approach for 3D Multi-Person Pose Estimation From a Single RGB Image",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Moon_Camera_Distance-Aware_Top-Down_Approach_for_3D_Multi-Person_Pose_Estimation_From_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Moon_Camera_Distance-Aware_Top-Down_Approach_for_3D_Multi-Person_Pose_Estimation_From_ICCV_2019_paper.html",
        "abstract": "Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in (https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE) , (https://github.com/mks0601/3DMPPE_POSENET_RELEASE).",
        "中文标题": "相机距离感知的自顶向下方法用于从单张RGB图像进行3D多人姿态估计",
        "摘要翻译": "尽管最近在3D人体姿态估计方面取得了显著进展，但大多数先前的方法仅处理单人情况。在这项工作中，我们首次提出了一种完全基于学习的、相机距离感知的自顶向下方法，用于从单张RGB图像进行3D多人姿态估计。所提出系统的流程包括人体检测、绝对3D人体根定位和根相对3D单人姿态估计模块。我们的系统在没有使用任何地面真实信息的情况下，与最先进的3D单人姿态估计模型取得了可比的结果，并在公开可用的数据集上显著优于先前的3D多人姿态估计方法。代码可在(https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE)和(https://github.com/mks0601/3DMPPE_POSENET_RELEASE)获取。",
        "领域": "3D姿态估计/人体检测/深度学习",
        "问题": "从单张RGB图像进行3D多人姿态估计",
        "动机": "解决现有方法仅处理单人情况的问题，提高3D多人姿态估计的准确性和效率",
        "方法": "提出了一种完全基于学习的、相机距离感知的自顶向下方法，包括人体检测、绝对3D人体根定位和根相对3D单人姿态估计模块",
        "关键词": [
            "3D姿态估计",
            "多人姿态估计",
            "相机距离感知",
            "自顶向下方法"
        ],
        "涉及的技术概念": "3D姿态估计是指从图像中估计人体关节的三维位置。人体检测是指在图像中定位人体的位置。绝对3D人体根定位是指确定人体在三维空间中的根位置。根相对3D单人姿态估计是指相对于根位置估计人体的姿态。"
    },
    {
        "order": 1017,
        "title": "Context-Aware Emotion Recognition Networks",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Context-Aware_Emotion_Recognition_Networks_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lee_Context-Aware_Emotion_Recognition_Networks_ICCV_2019_paper.html",
        "abstract": "Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to separately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io.",
        "中文标题": "上下文感知的情绪识别网络",
        "摘要翻译": "传统的情绪识别技术仅集中于面部表情分析，因此提供的能力有限，无法全面编码代表情绪反应的上下文。我们提出了用于上下文感知情绪识别的深度网络，称为CAER-Net，它不仅利用人类面部表情，还以联合和增强的方式利用上下文信息。关键思想是在视觉场景中隐藏人脸，并基于注意力机制寻找其他上下文。我们的网络由两个子网络组成，包括分别提取面部和上下文区域特征的双流编码网络，以及以自适应方式融合这些特征的自适应融合网络。我们还引入了一个新的上下文感知情绪识别基准，称为CAER，它在质量和数量上都比现有基准更合适。在几个基准上，CAER-Net证明了上下文对情绪识别的影响。我们的数据集可在http://caer-dataset.github.io获取。",
        "领域": "情绪识别/上下文理解/注意力机制",
        "问题": "传统情绪识别技术仅依赖面部表情分析，无法全面编码情绪反应的上下文信息。",
        "动机": "提高情绪识别的准确性和全面性，通过利用面部表情和上下文信息来更全面地理解情绪反应。",
        "方法": "提出了CAER-Net，一个深度网络，通过双流编码网络分别提取面部和上下文区域的特征，并通过自适应融合网络以自适应方式融合这些特征。",
        "关键词": [
            "情绪识别",
            "上下文理解",
            "注意力机制",
            "双流编码网络",
            "自适应融合"
        ],
        "涉及的技术概念": "CAER-Net是一种深度网络，专门设计用于情绪识别，通过结合面部表情和上下文信息来提高识别的准确性。它采用了双流编码网络来分别处理面部和上下文信息，并通过自适应融合网络来整合这些信息。此外，该研究还引入了一个新的基准CAER，用于评估上下文感知情绪识别的效果。"
    },
    {
        "order": 1018,
        "title": "Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.html",
        "abstract": "Head pose estimation aims at predicting an accurate pose from an image. Current approaches rely on supervised deep learning, which typically requires large amounts of labeled data. Manual or sensor-based annotations of head poses are prone to errors. A solution is to generate synthetic training data by rendering 3D face models. However, the differences (domain gap) between rendered (source-domain) and real-world (target-domain) images can cause low performance. Advances in visual domain adaptation allow reducing the influence of domain differences using adversarial neural networks, which match the feature spaces between domains by enforcing domain-invariant features. While previous work on visual domain adaptation generally assumes discrete and shared label spaces, these assumptions are both invalid for pose estimation tasks. We are the first to present domain adaptation for head pose estimation with a focus on partially shared and continuous label spaces. More precisely, we adapt the predominant weighting approaches to continuous label spaces by applying a weighted resampling of the source domain during training. To evaluate our approach, we revise and extend existing datasets resulting in a new benchmark for visual domain adaption. Our experiments show that our method improves the accuracy of head pose estimation for real-world images despite using only labels from synthetic images.",
        "中文标题": "使用合成图像和部分对抗域适应进行连续标签空间的深度头部姿态估计",
        "摘要翻译": "头部姿态估计旨在从图像中预测准确的姿态。当前的方法依赖于监督深度学习，这通常需要大量的标注数据。手动或基于传感器的头部姿态标注容易出错。一个解决方案是通过渲染3D面部模型生成合成训练数据。然而，渲染的（源域）和现实世界（目标域）图像之间的差异（域差距）可能导致性能低下。视觉域适应的进展允许通过使用对抗神经网络减少域差异的影响，这些网络通过强制域不变特征来匹配域之间的特征空间。虽然之前关于视觉域适应的工作通常假设离散和共享的标签空间，但这些假设对于姿态估计任务都是无效的。我们是第一个提出针对头部姿态估计的域适应方法，重点关注部分共享和连续标签空间。更准确地说，我们通过在训练期间对源域进行加权重采样，将主要的加权方法适应于连续标签空间。为了评估我们的方法，我们修订并扩展了现有数据集，从而为视觉域适应创建了一个新的基准。我们的实验表明，尽管仅使用合成图像的标签，我们的方法提高了现实世界图像的头部姿态估计的准确性。",
        "领域": "头部姿态估计/视觉域适应/对抗神经网络",
        "问题": "解决头部姿态估计中由于域差距导致的性能低下问题",
        "动机": "减少对大量标注数据的依赖，提高头部姿态估计的准确性",
        "方法": "通过渲染3D面部模型生成合成训练数据，并应用对抗神经网络进行视觉域适应，特别是在连续标签空间中进行部分共享的域适应",
        "关键词": [
            "头部姿态估计",
            "视觉域适应",
            "对抗神经网络",
            "连续标签空间",
            "3D面部模型"
        ],
        "涉及的技术概念": "监督深度学习、3D面部模型渲染、视觉域适应、对抗神经网络、连续标签空间、加权重采样"
    },
    {
        "order": 1019,
        "title": "Flare in Interference-Based Hyperspectral Cameras",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sassoon_Flare_in_Interference-Based_Hyperspectral_Cameras_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sassoon_Flare_in_Interference-Based_Hyperspectral_Cameras_ICCV_2019_paper.html",
        "abstract": "Stray light (flare) is formed inside cameras by internal reflections between optical elements. We point out a flare effect of significant magnitude and implication to snapshot hyperspectral imagers. Recent technologies enable placing interference-based filters on individual pixels in imaging sensors. These filters have narrow transmission bands around custom wavelengths and high transmission efficiency. Cameras using arrays of such filters are compact, robust and fast. However, as opposed to traditional broad-band filters, which often absorb unwanted light, narrow band-pass interference filters reflect non-transmitted light. This is a source of very significant flare which biases hyperspectral measurements. The bias in any pixel depends on spectral content in other pixels. We present a theoretical image formation model for this effect, and quantify it through simulations and experiments. In addition, we test deflaring of signals affected by such flare.",
        "中文标题": "基于干涉的高光谱相机中的光晕问题",
        "摘要翻译": "杂散光（光晕）是由光学元件之间的内部反射在相机内部形成的。我们指出了一种对快照高光谱成像器具有重大影响的光晕效应。最近的技术使得能够在成像传感器的单个像素上放置基于干涉的滤波器。这些滤波器在自定义波长附近具有窄的传输带和高传输效率。使用这种滤波器阵列的相机紧凑、坚固且快速。然而，与通常吸收不需要的光的传统宽带滤波器不同，窄带通干涉滤波器反射非传输光。这是导致高光谱测量偏差的非常显著的光晕来源。任何像素中的偏差取决于其他像素中的光谱内容。我们提出了这种效应的理论图像形成模型，并通过模拟和实验对其进行量化。此外，我们测试了受这种光晕影响的信号的去光晕。",
        "领域": "高光谱成像/光学滤波器/图像形成模型",
        "问题": "解决基于干涉的高光谱相机中由窄带通干涉滤波器反射非传输光引起的光晕问题",
        "动机": "光晕效应严重影响高光谱成像的准确性，需要理解和量化这种效应，并探索去光晕的方法",
        "方法": "提出了光晕效应的理论图像形成模型，并通过模拟和实验进行量化，同时测试了去光晕的方法",
        "关键词": [
            "高光谱成像",
            "光学滤波器",
            "图像形成模型",
            "光晕效应",
            "去光晕"
        ],
        "涉及的技术概念": "杂散光（光晕）是由光学元件之间的内部反射形成的，影响高光谱成像的准确性。基于干涉的滤波器具有窄的传输带和高传输效率，但反射非传输光，导致光晕。通过理论模型、模拟和实验量化光晕效应，并测试去光晕方法。"
    },
    {
        "order": 1020,
        "title": "Computational Hyperspectral Imaging Based on Dimension-Discriminative Low-Rank Tensor Recovery",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Computational_Hyperspectral_Imaging_Based_on_Dimension-Discriminative_Low-Rank_Tensor_Recovery_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhang_Computational_Hyperspectral_Imaging_Based_on_Dimension-Discriminative_Low-Rank_Tensor_Recovery_ICCV_2019_paper.html",
        "abstract": "Exploiting the prior information is fundamental for the image reconstruction in computational hyperspectral imaging. Existing methods usually unfold the 3D signal as a 1D vector and treat the prior information within different dimensions in an indiscriminative manner, which ignores the high-dimensionality nature of hyperspectral image (HSI) and thus results in poor quality reconstruction. In this paper, we propose to make full use of the high-dimensionality structure of the desired HSI to boost the reconstruction quality. We first build a high-order tensor by exploiting the nonlocal similarity in HSI. Then, we propose a dimension-discriminative low-rank tensor recovery (DLTR) model to characterize the structure prior adaptively in each dimension. By integrating the structure prior in DLTR with the system imaging process, we develop an optimization framework for HSI reconstruction, which is finally solved via the alternating minimization algorithm. Extensive experiments implemented with both synthetic and real data demonstrate that our method outperforms state-of-the-art methods.",
        "中文标题": "基于维度区分低秩张量恢复的计算高光谱成像",
        "摘要翻译": "利用先验信息对于计算高光谱成像中的图像重建至关重要。现有方法通常将3D信号展开为1D向量，并以无差别的方式处理不同维度的先验信息，这忽略了高光谱图像（HSI）的高维特性，从而导致重建质量较差。在本文中，我们提出充分利用所需HSI的高维结构来提高重建质量。我们首先通过利用HSI中的非局部相似性构建一个高阶张量。然后，我们提出了一种维度区分低秩张量恢复（DLTR）模型，以自适应地描述每个维度的结构先验。通过将DLTR中的结构先验与系统成像过程相结合，我们开发了一个用于HSI重建的优化框架，最终通过交替最小化算法解决。使用合成数据和真实数据进行的广泛实验表明，我们的方法优于最先进的方法。",
        "领域": "高光谱成像/张量恢复/图像重建",
        "问题": "高光谱图像重建质量差的问题",
        "动机": "现有方法在处理高光谱图像时忽略了其高维特性，导致重建质量不佳",
        "方法": "提出了一种维度区分低秩张量恢复（DLTR）模型，通过利用高光谱图像的非局部相似性构建高阶张量，并开发了一个优化框架用于图像重建",
        "关键词": [
            "高光谱成像",
            "张量恢复",
            "图像重建"
        ],
        "涉及的技术概念": "高光谱图像（HSI）的高维特性、非局部相似性、高阶张量、维度区分低秩张量恢复（DLTR）模型、交替最小化算法"
    },
    {
        "order": 1021,
        "title": "Deep Optics for Monocular Depth Estimation and 3D Object Detection",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.html",
        "abstract": "Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.",
        "中文标题": "深度光学用于单目深度估计和3D物体检测",
        "摘要翻译": "深度估计和3D物体检测对于场景理解至关重要，但由于图像捕捉过程中3D信息的丢失，使用单张图像执行这些任务仍然具有挑战性。最近使用深度神经网络的模型已经提高了单目深度估计的性能，但在预测绝对深度和泛化到标准数据集之外方面仍然存在困难。在这里，我们引入了深度光学的范式，即光学和图像处理的端到端设计，来解决单目深度估计问题，使用编码的散焦模糊作为由神经网络解码的额外深度线索。我们评估了几种光学编码策略以及用于深度估计的端到端优化方案，在包括NYU Depth v2和KITTI在内的三个数据集上进行了测试。我们发现优化的自由曲面透镜设计产生了最佳结果，但单透镜的色差也显著提高了性能。我们构建了一个物理原型，并验证了色差在现实世界结果中改善了深度估计。此外，我们在KITTI数据集上训练了物体检测网络，并展示了为深度估计优化的透镜也提高了3D物体检测的性能。",
        "领域": "单目深度估计/3D物体检测/光学设计",
        "问题": "单目深度估计和3D物体检测在单张图像上的挑战",
        "动机": "提高单目深度估计的准确性和泛化能力，以及改善3D物体检测的性能",
        "方法": "引入深度光学的范式，使用编码的散焦模糊作为额外深度线索，评估光学编码策略和端到端优化方案，构建物理原型验证色差对深度估计的改善",
        "关键词": [
            "单目深度估计",
            "3D物体检测",
            "光学编码",
            "色差",
            "自由曲面透镜"
        ],
        "涉及的技术概念": "深度光学范式涉及将光学和图像处理进行端到端设计，使用编码的散焦模糊作为深度线索，通过神经网络解码。评估了包括自由曲面透镜设计和单透镜色差在内的光学编码策略，以及端到端优化方案。构建物理原型验证了色差在现实世界中对深度估计的改善效果，并在KITTI数据集上训练物体检测网络，展示了优化透镜对3D物体检测性能的提升。"
    },
    {
        "order": 1022,
        "title": "Physics-Based Rendering for Improving Robustness to Rain",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Halder_Physics-Based_Rendering_for_Improving_Robustness_to_Rain_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Halder_Physics-Based_Rendering_for_Improving_Robustness_to_Rain_ICCV_2019_paper.html",
        "abstract": "To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.",
        "中文标题": "基于物理的渲染以提高对雨的鲁棒性",
        "摘要翻译": "为了提高对雨的鲁棒性，我们提出了一种基于物理的雨渲染管道，用于将雨真实地插入到晴朗天气的图像中。我们的渲染依赖于物理粒子模拟器、场景光照的估计和精确的雨光度建模，以增强图像中任意数量的真实雨或雾。我们通过用户研究验证了我们的渲染，证明我们的雨被认为比最先进的技术真实40%。使用我们生成的天气增强的Kitti和Cityscapes数据集，我们对深度对象检测和语义分割算法进行了全面评估，并显示它们在恶劣天气下的性能下降，对象检测下降约15%，语义分割下降约60%。此外，我们展示了使用我们的增强图像精炼现有网络可以提高对象检测和语义分割算法的鲁棒性。我们在nuScenes上进行了实验，并测量到与原始雨天性能相比，对象检测提高了15%，语义分割提高了35%。增强的数据库和代码可在项目页面上获得。",
        "领域": "渲染技术/图像增强/深度学习",
        "问题": "提高深度学习模型在恶劣天气条件下的鲁棒性",
        "动机": "现有的深度学习模型在恶劣天气条件下的性能显著下降，需要提高这些模型在雨天等恶劣天气条件下的鲁棒性。",
        "方法": "提出了一种基于物理的雨渲染管道，通过物理粒子模拟器、场景光照估计和精确的雨光度建模来增强图像，然后使用这些增强图像来精炼现有的深度学习模型。",
        "关键词": [
            "物理渲染",
            "图像增强",
            "深度学习",
            "对象检测",
            "语义分割"
        ],
        "涉及的技术概念": "物理粒子模拟器用于模拟雨滴的物理行为，场景光照估计用于确保渲染的雨或雾与原始图像的光照条件一致，雨光度建模用于精确地模拟雨或雾对图像的影响。"
    },
    {
        "order": 1023,
        "title": "ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ding_ARGAN_Attentive_Recurrent_Generative_Adversarial_Network_for_Shadow_Detection_and_ICCV_2019_paper.html",
        "abstract": "In this paper we propose an attentive recurrent generative adversarial network (ARGAN) to detect and remove shadows in an image. The generator consists of multiple progressive steps. At each step a shadow attention detector is firstly exploited to generate an attention map which specifies shadow regions in the input image. Given the attention map, a negative residual by a shadow remover encoder will recover a shadow-lighter or even a shadow-free image. The discriminator is designed to classify whether the output image in the last progressive step is real or fake. Moreover, ARGAN is suitable to be trained with a semi-supervised strategy to make full use of sufficient unsupervised data. The experiments on four public datasets have demonstrated that our ARGAN is robust to detect both simple and complex shadows and to produce more realistic shadow removal results. It outperforms the state-of-the-art methods, especially in detail of recovering shadow areas.",
        "中文标题": "ARGAN：用于阴影检测和去除的注意力循环生成对抗网络",
        "摘要翻译": "本文提出了一种注意力循环生成对抗网络（ARGAN），用于检测和去除图像中的阴影。生成器由多个渐进步骤组成。在每一步中，首先利用阴影注意力检测器生成一个注意力图，该图指定输入图像中的阴影区域。给定注意力图，阴影去除编码器产生的负残差将恢复一个阴影较轻甚至无阴影的图像。判别器被设计用于分类最后渐进步骤中的输出图像是真实的还是伪造的。此外，ARGAN适合采用半监督策略进行训练，以充分利用大量无监督数据。在四个公共数据集上的实验表明，我们的ARGAN在检测简单和复杂阴影以及产生更真实的阴影去除结果方面具有鲁棒性。它在恢复阴影区域的细节方面优于最先进的方法。",
        "领域": "阴影检测/图像恢复/生成对抗网络",
        "问题": "图像中阴影的检测和去除",
        "动机": "提高图像中阴影检测和去除的准确性和真实性，特别是在恢复阴影区域的细节方面",
        "方法": "提出了一种注意力循环生成对抗网络（ARGAN），包括一个由多个渐进步骤组成的生成器和一个用于分类图像真实性的判别器。生成器利用阴影注意力检测器生成注意力图，并通过阴影去除编码器恢复阴影较轻或无阴影的图像。",
        "关键词": [
            "阴影检测",
            "图像恢复",
            "生成对抗网络"
        ],
        "涉及的技术概念": "注意力循环生成对抗网络（ARGAN）是一种结合了注意力机制和生成对抗网络（GAN）的深度学习模型，用于图像处理任务，如阴影检测和去除。注意力机制用于指定图像中的特定区域（如阴影区域），而生成对抗网络则用于生成更真实的图像。半监督学习策略允许模型利用大量未标记的数据进行训练，提高模型的泛化能力和性能。"
    },
    {
        "order": 1024,
        "title": "Deep Tensor ADMM-Net for Snapshot Compressive Imaging",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_Deep_Tensor_ADMM-Net_for_Snapshot_Compressive_Imaging_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_Deep_Tensor_ADMM-Net_for_Snapshot_Compressive_Imaging_ICCV_2019_paper.html",
        "abstract": "Snapshot compressive imaging (SCI) systems have been developed to capture high-dimensional (> 3) signals using low-dimensional off-the-shelf sensors, i.e., mapping multiple video frames into a single measurement frame. One key module of a SCI system is an accurate decoder that recovers the original video frames. However, existing model-based decoding algorithms require exhaustive parameter tuning with prior knowledge and cannot support practical applications due to the extremely long running time. In this paper, we propose a deep tensor ADMM-Net for video SCI systems that provides high-quality decoding in seconds. Firstly, we start with a standard tensor ADMM algorithm, unfold its inference iterations into a layer-wise structure, and design a deep neural network based on tensor operations. Secondly, instead of relying on a pre-specified sparse representation domain, the network learns the domain of low-rank tensor through stochastic gradient descent. It is worth noting that the proposed deep tensor ADMM-Net has potentially mathematical interpretations. On public video data, the simulation results show the proposed  method  achieves average 0.8 ~ 2.5 dB improvement in PSNR and 0.07 ~ 0.1 in SSIM, and 1500x~ 3600 xspeedups over the state-of-the-art methods. On real data captured by SCI cameras, the experimental results show comparable visual results with the state-of-the-art methods but in much shorter running time.",
        "中文标题": "用于快照压缩成像的深度张量ADMM网络",
        "摘要翻译": "快照压缩成像（SCI）系统已被开发用于使用低维现成传感器捕获高维（>3）信号，即将多个视频帧映射到单个测量帧中。SCI系统的一个关键模块是能够恢复原始视频帧的精确解码器。然而，现有的基于模型的解码算法需要具有先验知识的详尽参数调整，并且由于极长的运行时间而无法支持实际应用。在本文中，我们提出了一种用于视频SCI系统的深度张量ADMM网络，该网络在几秒钟内提供高质量的解码。首先，我们从标准的张量ADMM算法开始，将其推理迭代展开为分层结构，并设计了一个基于张量操作的深度神经网络。其次，网络通过随机梯度下降学习低秩张量的域，而不是依赖于预先指定的稀疏表示域。值得注意的是，所提出的深度张量ADMM网络具有潜在的数学解释。在公共视频数据上，仿真结果表明，所提出的方法在PSNR上实现了0.8~2.5 dB的平均改进，在SSIM上实现了0.07~0.1的改进，并且比最先进的方法快了1500倍~3600倍。在SCI相机捕获的真实数据上，实验结果显示与最先进的方法相比具有可比性的视觉结果，但运行时间大大缩短。",
        "领域": "快照压缩成像/视频解码/张量分解",
        "问题": "现有基于模型的解码算法需要详尽参数调整且运行时间长，无法支持实际应用",
        "动机": "提高视频SCI系统的解码质量和速度，支持实际应用",
        "方法": "提出了一种深度张量ADMM网络，通过将标准张量ADMM算法的推理迭代展开为分层结构，并设计基于张量操作的深度神经网络，学习低秩张量的域",
        "关键词": [
            "快照压缩成像",
            "视频解码",
            "张量分解",
            "ADMM算法",
            "深度神经网络"
        ],
        "涉及的技术概念": {
            "快照压缩成像（SCI）": "一种使用低维传感器捕获高维信号的成像技术",
            "ADMM算法": "一种用于解决优化问题的迭代算法",
            "张量分解": "一种将高维数据分解为低维表示的技术",
            "深度神经网络": "一种模仿人脑结构和功能的计算模型，用于学习和识别数据中的模式",
            "PSNR": "峰值信噪比，用于衡量图像或视频质量的一种指标",
            "SSIM": "结构相似性，用于衡量两幅图像相似度的一种指标"
        }
    },
    {
        "order": 1025,
        "title": "Convex Relaxations for Consensus and Non-Minimal Problems in 3D Vision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Probst_Convex_Relaxations_for_Consensus_and_Non-Minimal_Problems_in_3D_Vision_ICCV_2019_paper.html",
        "abstract": "In this paper, we formulate a generic non-minimal solver using the existing tools of Polynomials Optimization Problems (POP) from computational algebraic geometry. The proposed method exploits the well known Shor's or Lasserre's relaxations, whose theoretical aspects are also discussed. Notably, we further exploit the POP formulation of non-minimal solver also for the generic consensus maximization problems in 3D vision. Our framework is simple and straightforward to implement, which is also supported by three diverse applications in 3D vision, namely rigid body transformation estimation, Non-Rigid Structure-from-Motion (NRSfM), and camera autocalibration. In all three cases, both non-minimal and consensus maximization are tested, which are also compared against the state-of-the-art methods. Our results are competitive to the compared methods, and are also coherent with our theoretical analysis. The main contribution of this paper is the claim that a good approximate solution for many polynomial problems involved in 3D vision can be obtained using the existing theory of numerical computational algebra. This claim leads us to reason about why many relaxed methods in 3D vision behave so well? And also allows us to offer a generic relaxed solver in a rather straightforward way. We further show that the convex relaxation of these polynomials can easily be used for maximizing consensus in a deterministic manner. We support our claim using several experiments for aforementioned three diverse problems in 3D vision.",
        "中文标题": "凸松弛在3D视觉中的共识和非最小问题中的应用",
        "摘要翻译": "在本文中，我们利用计算代数几何中的多项式优化问题（POP）现有工具，制定了一个通用的非最小求解器。所提出的方法利用了著名的Shor或Lasserre松弛，其理论方面也进行了讨论。值得注意的是，我们进一步利用非最小求解器的POP公式来解决3D视觉中的通用共识最大化问题。我们的框架简单直接，易于实现，并且得到了3D视觉中三个不同应用的支持，即刚体变换估计、非刚性结构从运动（NRSfM）和相机自校准。在所有三种情况下，都测试了非最小和共识最大化，并与最先进的方法进行了比较。我们的结果与比较方法相比具有竞争力，并且与我们的理论分析一致。本文的主要贡献是声称，利用现有的数值计算代数理论，可以为3D视觉中涉及的许多多项式问题获得良好的近似解。这一声明引导我们思考为什么3D视觉中的许多松弛方法表现得如此之好？并且允许我们以一种相当直接的方式提供一个通用的松弛求解器。我们进一步表明，这些多项式的凸松弛可以很容易地用于以确定性的方式最大化共识。我们通过对上述3D视觉中三个不同问题的几个实验来支持我们的声明。",
        "领域": "3D视觉/计算代数几何/多项式优化",
        "问题": "解决3D视觉中的非最小问题和共识最大化问题",
        "动机": "探索利用现有的数值计算代数理论为3D视觉中的多项式问题提供良好的近似解，并理解松弛方法在3D视觉中表现良好的原因",
        "方法": "利用多项式优化问题（POP）的现有工具，特别是Shor或Lasserre松弛，制定通用的非最小求解器，并将其应用于3D视觉中的共识最大化问题",
        "关键词": [
            "多项式优化",
            "3D视觉",
            "共识最大化",
            "非最小求解器",
            "凸松弛"
        ],
        "涉及的技术概念": {
            "多项式优化问题（POP）": "一种数学优化问题，其中目标函数和约束条件都是多项式",
            "Shor或Lasserre松弛": "用于多项式优化问题的凸松弛技术，可以将非凸问题转化为凸问题，从而更容易求解",
            "非最小求解器": "一种求解器，不要求解是最小的，但要求解是有效的",
            "共识最大化": "在3D视觉中，寻找一组数据点，使得这些点之间的某种一致性度量最大化",
            "凸松弛": "一种数学技术，通过放宽问题的约束条件，将非凸问题转化为凸问题，以便更容易找到全局最优解"
        }
    },
    {
        "order": 1026,
        "title": "Pareto Meets Huber: Efficiently Avoiding Poor Minima in Robust Estimation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zach_Pareto_Meets_Huber_Efficiently_Avoiding_Poor_Minima_in_Robust_Estimation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zach_Pareto_Meets_Huber_Efficiently_Avoiding_Poor_Minima_in_Robust_Estimation_ICCV_2019_paper.html",
        "abstract": "Robust cost optimization is the task of fitting parameters to data points containing outliers. In particular, we focus on large-scale computer vision problems, such as bundle adjustment, where Non-Linear Least Square (NLLS) solvers are the current workhorse. In this context, NLLS-based state of the art algorithms have been designed either to quickly improve the target objective and find a local minimum close to the initial value of the parameters, or to have a strong ability to escape poor local minima. In this paper, we propose a novel algorithm relying on multi-objective optimization which allows to match those two properties. We experimentally demonstrate that our algorithm has an ability to escape poor local minima that is on par with the best performing algorithms with a faster decrease of the target objective.",
        "中文标题": "帕累托遇见胡贝尔：在鲁棒估计中有效避免不良最小值",
        "摘要翻译": "鲁棒成本优化是拟合包含异常值的数据点的参数的任务。特别是，我们专注于大规模计算机视觉问题，如捆绑调整，其中非线性最小二乘（NLLS）求解器是当前的主力。在这种情况下，基于NLLS的最先进算法被设计为要么快速改进目标目标并找到接近参数初始值的局部最小值，要么具有强大的能力逃离不良的局部最小值。在本文中，我们提出了一种依赖于多目标优化的新算法，该算法能够匹配这两个属性。我们通过实验证明，我们的算法具有与最佳性能算法相当的逃离不良局部最小值的能力，同时目标目标的下降速度更快。",
        "领域": "鲁棒估计/多目标优化/非线性最小二乘",
        "问题": "在大规模计算机视觉问题中，如何有效避免在鲁棒成本优化过程中陷入不良局部最小值",
        "动机": "提高在大规模计算机视觉问题中鲁棒成本优化的效率和效果，特别是在处理包含异常值的数据时",
        "方法": "提出了一种依赖于多目标优化的新算法，该算法能够同时实现快速改进目标目标和逃离不良局部最小值的能力",
        "关键词": [
            "鲁棒估计",
            "多目标优化",
            "非线性最小二乘"
        ],
        "涉及的技术概念": "鲁棒成本优化是指在拟合包含异常值的数据点的参数时，通过特定的算法和技术手段，使得模型对这些异常值不敏感，从而提高模型的泛化能力和鲁棒性。多目标优化是一种同时考虑多个目标函数的优化方法，旨在找到一组解，使得这些目标函数之间达到一个平衡。非线性最小二乘（NLLS）是一种用于求解非线性最小二乘问题的数值优化方法，广泛应用于计算机视觉等领域。"
    },
    {
        "order": 1027,
        "title": "K-Best Transformation Synchronization",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_K-Best_Transformation_Synchronization_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_K-Best_Transformation_Synchronization_ICCV_2019_paper.html",
        "abstract": "In this paper, we introduce the problem of K-best transformation synchronization for the purpose of multiple scan matching. Given noisy pair-wise transformations computed between a subset of depth scan pairs, K-best transformation synchronization seeks to output multiple consistent relative transformations. This problem naturally arises in many geometry reconstruction applications, where the underlying object possesses self-symmetry. For approximately symmetric or even non-symmetric objects, K-best solutions offer an intermediate presentation for recovering the underlying single-best solution. We introduce a simple yet robust iterative algorithm for K-best transformation synchronization, which alternates between transformation propagation and transformation clustering. We present theoretical guarantees on the robust and exact recoveries of our algorithm. Experimental results demonstrate the advantage of our approach against state-of-the-art transformation synchronization techniques on both synthetic and real datasets.",
        "中文标题": "K最佳变换同步",
        "摘要翻译": "本文介绍了用于多扫描匹配目的的K最佳变换同步问题。给定在深度扫描对子集之间计算的噪声成对变换，K最佳变换同步旨在输出多个一致的相对变换。这个问题自然出现在许多几何重建应用中，其中基础对象具有自对称性。对于近似对称甚至非对称的对象，K最佳解决方案为恢复基础单一最佳解决方案提供了中间表示。我们引入了一种简单而鲁棒的迭代算法用于K最佳变换同步，该算法在变换传播和变换聚类之间交替进行。我们提出了关于我们算法鲁棒性和精确恢复的理论保证。实验结果证明了我们的方法在合成和真实数据集上相对于最先进的变换同步技术的优势。",
        "领域": "几何重建/自对称性分析/变换同步",
        "问题": "多扫描匹配中的K最佳变换同步问题",
        "动机": "解决在几何重建应用中，由于基础对象的自对称性导致的变换同步问题，以及为近似对称或非对称对象提供中间表示以恢复单一最佳解决方案。",
        "方法": "引入了一种简单而鲁棒的迭代算法，该算法在变换传播和变换聚类之间交替进行，以实现K最佳变换同步。",
        "关键词": [
            "几何重建",
            "自对称性",
            "变换同步"
        ],
        "涉及的技术概念": "K最佳变换同步是一种在多扫描匹配中寻找多个一致相对变换的方法，特别适用于处理具有自对称性的几何重建问题。通过迭代算法在变换传播和变换聚类之间交替进行，可以有效解决这一问题。"
    },
    {
        "order": 1028,
        "title": "Parametric Majorization for Data-Driven Energy Minimization Methods",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Geiping_Parametric_Majorization_for_Data-Driven_Energy_Minimization_Methods_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Geiping_Parametric_Majorization_for_Data-Driven_Energy_Minimization_Methods_ICCV_2019_paper.html",
        "abstract": "Energy minimization methods are a classical tool in a multitude of computer vision applications. While they are interpretable and well-studied, their regularity assumptions are difficult to design by hand. Deep learning techniques on the other hand are purely data-driven, often provide excellent results, but are very difficult to constrain to predefined physical or safety-critical models. A possible combination between the two approaches is to design a parametric energy and train the free parameters in such a way that minimizers of the energy correspond to desired solution on a set of training examples. Unfortunately, such formulations typically lead to bi-level optimization problems, on which common optimization algorithms are difficult to scale to modern requirements in data processing and efficiency. In this work, we present a new strategy to optimize these bi-level problems. We investigate surrogate single-level problems that majorize the target problems and can be implemented with existing tools, leading to efficient algorithms without collapse of the energy function. This framework of strategies enables new avenues to the training of parameterized energy minimization models from large data.",
        "中文标题": "参数化主化用于数据驱动的能量最小化方法",
        "摘要翻译": "能量最小化方法是众多计算机视觉应用中的经典工具。虽然它们可解释且被深入研究，但其正则化假设难以手工设计。另一方面，深度学习技术纯粹是数据驱动的，通常能提供出色的结果，但很难约束到预定义的物理或安全关键模型。两种方法的一个可能结合是设计一个参数化能量，并以这样的方式训练自由参数，使得能量的最小化器对应于一组训练示例上的期望解。不幸的是，这样的公式通常会导致双层优化问题，对于这些问题，常见的优化算法难以扩展到现代数据处理和效率的要求。在这项工作中，我们提出了一种新的策略来优化这些双层问题。我们研究了替代的单层问题，这些问题主化了目标问题，并且可以使用现有工具实现，从而在不导致能量函数崩溃的情况下实现高效算法。这一策略框架为从大数据中训练参数化能量最小化模型开辟了新途径。",
        "领域": "优化算法/能量最小化/深度学习",
        "问题": "如何有效地优化双层能量最小化问题",
        "动机": "结合能量最小化方法的可解释性和深度学习的数据驱动优势，同时克服双层优化问题的计算挑战",
        "方法": "提出了一种新的策略，通过研究替代的单层问题来优化双层能量最小化问题，这些单层问题主化了目标问题，并且可以使用现有工具实现",
        "关键词": [
            "能量最小化",
            "双层优化",
            "参数化模型"
        ],
        "涉及的技术概念": "能量最小化方法是一种在计算机视觉中广泛使用的技术，旨在通过最小化某个能量函数来找到最优解。双层优化问题涉及两个层次的优化任务，其中上层问题的解依赖于下层问题的解。参数化模型指的是模型的参数可以通过数据训练来调整，以适应特定的任务或数据集。"
    },
    {
        "order": 1029,
        "title": "A Bayesian Optimization Framework for Neural Network Compression",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ma_A_Bayesian_Optimization_Framework_for_Neural_Network_Compression_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ma_A_Bayesian_Optimization_Framework_for_Neural_Network_Compression_ICCV_2019_paper.html",
        "abstract": "Neural network compression is an important step for deploying neural networks where speed is of high importance, or on devices with limited memory. It is necessary to tune compression parameters in order to achieve the desired trade-off between size and performance. This is often done by optimizing the loss on a validation set of data, which should be large enough to approximate the true risk and therefore yield sufficient generalization ability. However, using a full validation set can be computationally expensive. In this work, we develop a general Bayesian optimization framework for optimizing functions that are computed based on U-statistics. We propagate Gaussian uncertainties from the statistics through the Bayesian optimization framework yielding a method that gives a probabilistic approximation certificate of the result. We then apply this to parameter selection in neural network compression. Compression objectives that can be written as U-statistics are typically based on empirical risk and knowledge distillation for deep discriminative models. We demonstrate our method on VGG and ResNet models, and the resulting system can find optimal compression parameters for relatively high-dimensional parametrizations in a matter of minutes on a standard desktop machine, orders of magnitude faster than competing methods.",
        "中文标题": "神经网络压缩的贝叶斯优化框架",
        "摘要翻译": "神经网络压缩是在速度极为重要或内存有限的设备上部署神经网络的重要步骤。为了在大小和性能之间达到理想的权衡，调整压缩参数是必要的。这通常通过优化验证数据集上的损失来完成，该数据集应足够大以近似真实风险，从而产生足够的泛化能力。然而，使用完整的验证集可能在计算上非常昂贵。在这项工作中，我们开发了一个通用的贝叶斯优化框架，用于优化基于U统计量计算的函数。我们将高斯不确定性从统计量传播通过贝叶斯优化框架，产生一种方法，该方法提供了结果的概率近似证书。然后，我们将此应用于神经网络压缩中的参数选择。可以写成U统计量的压缩目标通常基于经验风险和深度判别模型的知识蒸馏。我们在VGG和ResNet模型上展示了我们的方法，结果系统可以在标准台式机上几分钟内找到相对高维参数化的最优压缩参数，比竞争方法快几个数量级。",
        "领域": "神经网络压缩/贝叶斯优化/深度学习模型优化",
        "问题": "在神经网络压缩过程中，如何高效地调整压缩参数以达到大小和性能之间的理想权衡",
        "动机": "为了在速度极为重要或内存有限的设备上部署神经网络，需要一种高效的方法来调整压缩参数，同时减少计算成本",
        "方法": "开发了一个通用的贝叶斯优化框架，用于优化基于U统计量计算的函数，并将高斯不确定性从统计量传播通过贝叶斯优化框架，以提供结果的概率近似证书",
        "关键词": [
            "神经网络压缩",
            "贝叶斯优化",
            "U统计量",
            "高斯不确定性",
            "知识蒸馏"
        ],
        "涉及的技术概念": {
            "贝叶斯优化": "一种用于全局优化的概率模型，通过构建目标函数的概率模型来指导搜索过程，以找到最优解",
            "U统计量": "一种统计量，用于估计某些参数，基于样本数据的对称函数",
            "高斯不确定性": "指在统计估计中，由于数据的不确定性导致的估计结果的概率分布，通常假设为高斯分布",
            "知识蒸馏": "一种模型压缩技术，通过训练一个较小的模型（学生模型）来模仿一个较大的预训练模型（教师模型）的行为，以达到压缩模型大小的目的"
        }
    },
    {
        "order": 1030,
        "title": "HiPPI: Higher-Order Projected Power Iterations for Scalable Multi-Matching",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bernard_HiPPI_Higher-Order_Projected_Power_Iterations_for_Scalable_Multi-Matching_ICCV_2019_paper.html",
        "abstract": "The matching of multiple objects (e.g. shapes or images) is a fundamental problem in vision and graphics. In order to robustly handle ambiguities, noise and repetitive patterns in challenging real-world settings, it is essential to take geometric consistency between points into account. Computationally, the multi-matching problem is difficult. It can be phrased as simultaneously solving multiple (NP-hard) quadratic assignment problems (QAPs) that are coupled via cycle-consistency constraints. The main limitations of existing multi-matching methods are that they either ignore geometric consistency and thus have limited robustness, or they are restricted to small-scale problems due to their (relatively) high computational cost. We address these shortcomings by introducing a Higher-order Projected Power Iteration method, which is (i) efficient and scales to tens of thousands of points, (ii) straightforward to implement, (iii) able to incorporate geometric consistency, (iv) guarantees cycle-consistent multi-matchings, and (iv) comes with theoretical convergence guarantees. Experimentally we show that our approach is superior to existing methods.",
        "中文标题": "HiPPI: 用于可扩展多匹配的高阶投影幂迭代",
        "摘要翻译": "多个对象（例如形状或图像）的匹配是视觉和图形学中的一个基本问题。为了在具有挑战性的现实世界设置中稳健地处理模糊性、噪声和重复模式，考虑点之间的几何一致性是至关重要的。从计算上讲，多匹配问题是困难的。它可以被表述为同时解决多个（NP难）二次分配问题（QAPs），这些问题通过循环一致性约束相互耦合。现有多匹配方法的主要限制是，它们要么忽略了几何一致性，因此鲁棒性有限，要么由于（相对）较高的计算成本而仅限于小规模问题。我们通过引入一种高阶投影幂迭代方法来解决这些缺点，该方法（i）高效且可扩展到数万个点，（ii）实现简单，（iii）能够结合几何一致性，（iv）保证循环一致的多匹配，并且（iv）具有理论收敛保证。实验表明，我们的方法优于现有方法。",
        "领域": "形状匹配/图像匹配/几何一致性",
        "问题": "解决多个对象（如形状或图像）在多匹配问题中的几何一致性和计算效率问题",
        "动机": "现有方法在几何一致性和计算效率方面存在限制，需要一种既能保证几何一致性又能高效处理大规模问题的新方法",
        "方法": "引入高阶投影幂迭代方法，该方法高效、易于实现、能够结合几何一致性、保证循环一致的多匹配，并具有理论收敛保证",
        "关键词": [
            "形状匹配",
            "图像匹配",
            "几何一致性",
            "循环一致性",
            "高阶投影幂迭代"
        ],
        "涉及的技术概念": "二次分配问题（QAPs）是NP难问题，涉及在多个对象之间找到最优匹配。循环一致性约束确保匹配结果在不同对象之间是一致的。高阶投影幂迭代是一种计算技术，用于高效解决这类问题，同时保证几何一致性和循环一致性。"
    },
    {
        "order": 1031,
        "title": "Language-Conditioned Graph Networks for Relational Reasoning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Language-Conditioned_Graph_Networks_for_Relational_Reasoning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hu_Language-Conditioned_Graph_Networks_for_Relational_Reasoning_ICCV_2019_paper.html",
        "abstract": "Solving grounded language tasks often requires reasoning about relationships between objects in the context of a given task. For example, to answer the question \"What color is the mug on the plate?\" we must check the color of the specific mug that satisfies the \"on\" relationship with respect to the plate. Recent work has proposed various methods capable of complex relational reasoning. However, most of their power is in the inference structure, while the scene is represented with simple local appearance features. In this paper, we take an alternate approach and build contextualized representations for objects in a visual scene to support relational reasoning. We propose a general framework of Language-Conditioned Graph Networks (LCGN), where each node represents an object, and is described by a context-aware representation from related objects through iterative message passing conditioned on the textual input. E.g., conditioning on the \"on\" relationship to the plate, the object \"mug\" gathers messages from the object \"plate\" to update its representation to \"mug on the plate\", which can be easily consumed by a simple classifier for answer prediction. We experimentally show that our LCGN approach effectively supports relational reasoning and improves performance across several tasks and datasets. Our code is available at http://ronghanghu.com/lcgn.",
        "中文标题": "语言条件图网络用于关系推理",
        "摘要翻译": "解决基于语言的任务通常需要在给定任务的上下文中对对象之间的关系进行推理。例如，要回答“盘子上的杯子是什么颜色的？”这个问题，我们必须检查与盘子有“上”关系的特定杯子的颜色。最近的工作提出了各种能够进行复杂关系推理的方法。然而，它们的大部分能力在于推理结构，而场景是用简单的局部外观特征表示的。在本文中，我们采取了另一种方法，为视觉场景中的对象构建上下文表示以支持关系推理。我们提出了一个语言条件图网络（LCGN）的通用框架，其中每个节点代表一个对象，并通过基于文本输入的迭代消息传递从相关对象中描述为上下文感知表示。例如，基于与盘子的“上”关系，对象“杯子”从对象“盘子”收集消息，以将其表示更新为“盘子上的杯子”，这可以很容易地被简单的分类器用于答案预测。我们通过实验表明，我们的LCGN方法有效地支持关系推理，并在多个任务和数据集上提高了性能。我们的代码可在http://ronghanghu.com/lcgn获取。",
        "领域": "关系推理/视觉问答/自然语言理解",
        "问题": "如何在视觉场景中有效地进行对象间的关系推理",
        "动机": "现有的方法虽然在推理结构上表现出色，但在场景表示上仅使用简单的局部外观特征，限制了关系推理的能力",
        "方法": "提出了一种语言条件图网络（LCGN）框架，通过迭代消息传递和文本输入条件化，为视觉场景中的对象构建上下文感知表示",
        "关键词": [
            "关系推理",
            "视觉问答",
            "自然语言理解"
        ],
        "涉及的技术概念": "语言条件图网络（LCGN）是一种框架，用于在视觉场景中通过迭代消息传递和文本输入条件化来构建对象的上下文感知表示，以支持复杂的关系推理。这种方法允许对象根据与其它对象的关系动态更新其表示，从而更有效地解决基于语言的任务。"
    },
    {
        "order": 1032,
        "title": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.html",
        "abstract": "Conditional text-to-image generation is an active area of research, with many possible applications. Existing research has primarily focused on generating a single image from available conditioning information in one step. One practical extension beyond one-step generation is a system that generates an image iteratively, conditioned on ongoing linguistic input or feedback. This is significantly more challenging than one-step generation tasks, as such a system must understand the contents of its generated images with respect to the feedback history, the current feedback, as well as the interactions among concepts present in the feedback history. In this work, we present a recurrent image generation model which takes into account both the generated output up to the current step as well as all past instructions for generation. We show that our model is able to generate the background, add new objects, and apply simple transformations to existing objects. We believe our approach is an important step toward interactive generation. Code and data is available at: https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/.",
        "中文标题": "讲述、绘制与重复：基于持续语言指令的图像生成与修改",
        "摘要翻译": "条件文本到图像生成是一个活跃的研究领域，具有许多潜在的应用。现有的研究主要集中在从可用的条件信息中一步生成单个图像。一个实用的扩展是开发一个系统，该系统能够根据持续的语言输入或反馈迭代生成图像。这比一步生成任务更具挑战性，因为这样的系统必须理解其生成的图像内容与反馈历史、当前反馈以及反馈历史中概念之间的相互作用。在这项工作中，我们提出了一个循环图像生成模型，该模型考虑了到当前步骤为止的生成输出以及所有过去的生成指令。我们展示了我们的模型能够生成背景、添加新对象并对现有对象进行简单变换。我们相信我们的方法是迈向交互式生成的重要一步。代码和数据可在以下网址获取：https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/。",
        "领域": "图像生成/自然语言处理/交互式系统",
        "问题": "如何根据持续的语言指令迭代生成和修改图像",
        "动机": "开发一个能够根据持续的语言输入或反馈迭代生成图像的系统，以扩展现有的条件文本到图像生成研究",
        "方法": "提出了一个循环图像生成模型，该模型考虑了到当前步骤为止的生成输出以及所有过去的生成指令",
        "关键词": [
            "图像生成",
            "自然语言处理",
            "交互式系统"
        ],
        "涉及的技术概念": "条件文本到图像生成、循环图像生成模型、交互式生成"
    },
    {
        "order": 1033,
        "title": "Relation-Aware Graph Attention Network for Visual Question Answering",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Relation-Aware_Graph_Attention_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Relation-Aware_Graph_Attention_Network_for_Visual_Question_Answering_ICCV_2019_paper.html",
        "abstract": "In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA.",
        "中文标题": "关系感知图注意力网络用于视觉问答",
        "摘要翻译": "为了回答关于图像的语义复杂问题，视觉问答（VQA）模型需要充分理解图像中的视觉场景，特别是不同对象之间的交互动态。我们提出了一种关系感知图注意力网络（ReGAT），它将每张图像编码为一个图，并通过图注意力机制建模多类型对象间关系，以学习问题适应的关系表示。探索了两种类型的视觉对象关系：（i）显式关系，表示对象之间的几何位置和语义交互；（ii）隐式关系，捕捉图像区域之间的隐藏动态。实验表明，ReGAT在VQA 2.0和VQA-CP v2数据集上均优于之前的最先进方法。我们进一步展示了ReGAT与现有VQA架构的兼容性，并可以作为通用关系编码器来提高VQA模型的性能。",
        "领域": "视觉问答/图神经网络/注意力机制",
        "问题": "如何充分理解图像中的视觉场景，特别是不同对象之间的交互动态，以回答语义复杂的问题",
        "动机": "提高视觉问答模型对图像中对象间关系的理解和表示能力，以更准确地回答复杂问题",
        "方法": "提出关系感知图注意力网络（ReGAT），通过图注意力机制建模多类型对象间关系，学习问题适应的关系表示",
        "关键词": [
            "视觉问答",
            "图注意力网络",
            "对象关系"
        ],
        "涉及的技术概念": {
            "视觉问答（VQA）": "一种结合视觉和语言理解的技术，旨在回答关于图像内容的问题",
            "图注意力网络（GAT）": "一种利用注意力机制处理图结构数据的神经网络，能够捕捉节点间的重要性",
            "显式关系": "指对象之间可以直接观察到的关系，如几何位置和语义交互",
            "隐式关系": "指对象之间不易直接观察到的关系，需要通过模型学习捕捉的隐藏动态"
        }
    },
    {
        "order": 1034,
        "title": "Unpaired Image Captioning via Scene Graph Alignments",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_Unpaired_Image_Captioning_via_Scene_Graph_Alignments_ICCV_2019_paper.html",
        "abstract": "Most of current image captioning models heavily rely on paired image-caption datasets. However, getting large scale image-caption paired data is labor-intensive and time-consuming. In this paper, we present a scene graph-based approach for unpaired image captioning. Our framework comprises an image scene graph generator, a sentence scene graph generator, a scene graph encoder, and a sentence decoder. Specifically, we first train the scene graph encoder and the sentence decoder on the text modality. To align the scene graphs between images and sentences, we propose an unsupervised feature alignment method that maps the scene graph features from the image to the sentence modality. Experimental results show that our proposed model can generate quite promising results without using any image-caption training pairs, outperforming existing methods by a wide margin.",
        "中文标题": "通过场景图对齐实现无配对图像描述",
        "摘要翻译": "目前大多数图像描述模型严重依赖于配对的图像-描述数据集。然而，获取大规模的图像-描述配对数据既费时又费力。在本文中，我们提出了一种基于场景图的无配对图像描述方法。我们的框架包括一个图像场景图生成器、一个句子场景图生成器、一个场景图编码器和一个句子解码器。具体来说，我们首先在文本模态上训练场景图编码器和句子解码器。为了对齐图像和句子之间的场景图，我们提出了一种无监督的特征对齐方法，将场景图特征从图像模态映射到句子模态。实验结果表明，我们提出的模型可以在不使用任何图像-描述训练对的情况下生成相当有希望的结果，远远超过了现有方法。",
        "领域": "图像描述/场景理解/自然语言生成",
        "问题": "解决无配对图像描述的问题",
        "动机": "获取大规模的图像-描述配对数据既费时又费力",
        "方法": "提出了一种基于场景图的无配对图像描述方法，包括图像场景图生成器、句子场景图生成器、场景图编码器和句子解码器，并采用无监督的特征对齐方法",
        "关键词": [
            "图像描述",
            "场景图",
            "无监督学习"
        ],
        "涉及的技术概念": "场景图生成器用于从图像和句子中提取场景图，场景图编码器用于编码场景图特征，句子解码器用于生成描述，无监督特征对齐方法用于将图像场景图特征映射到句子模态。"
    },
    {
        "order": 1035,
        "title": "Modeling Inter and Intra-Class Relations in the Triplet Loss for Zero-Shot Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Le_Cacheux_Modeling_Inter_and_Intra-Class_Relations_in_the_Triplet_Loss_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Le_Cacheux_Modeling_Inter_and_Intra-Class_Relations_in_the_Triplet_Loss_for_ICCV_2019_paper.html",
        "abstract": "Recognizing visual unseen classes, i.e. for which no training data is available, is known as Zero Shot Learning (ZSL). Some of the best performing methods apply the triplet loss to seen classes to learn a mapping between visual representations of images and attribute vectors that constitute class prototypes. They nevertheless make several implicit assumptions that limit their performance on real use cases, particularly with fine-grained datasets comprising a large number of classes. We identify three of these assumptions and put forward corresponding novel contributions to address them. Our approach consists in taking into account both inter-class and intra-class relations, respectively by being more permissive with confusions between similar classes, and by penalizing visual samples which are atypical to their class. The approach is tested on four datasets, including the large-scale ImageNet, and exhibits performances significantly above recent methods, even generative methods based on more restrictive hypotheses.",
        "中文标题": "在零样本学习的三元组损失中建模类间和类内关系",
        "摘要翻译": "识别视觉未见类别，即没有训练数据可用的类别，被称为零样本学习（ZSL）。一些表现最佳的方法将三元组损失应用于已见类别，以学习图像视觉表示与构成类别原型的属性向量之间的映射。然而，它们做出了几个隐含假设，这些假设限制了它们在实际用例中的表现，特别是在包含大量类别的细粒度数据集上。我们识别了这些假设中的三个，并提出了相应的新贡献来解决它们。我们的方法包括考虑类间和类内关系，分别通过对相似类别之间的混淆更加宽容，以及对那些在其类别中非典型的视觉样本进行惩罚。该方法在四个数据集上进行了测试，包括大规模ImageNet，并展示了显著高于最近方法的表现，甚至超过了基于更严格假设的生成方法。",
        "领域": "零样本学习/细粒度分类/视觉表示学习",
        "问题": "解决零样本学习在实际用例中的表现限制，特别是在细粒度数据集上。",
        "动机": "现有的零样本学习方法在实际应用中表现受限，特别是在处理细粒度数据集时，因为它们基于一些隐含假设。",
        "方法": "通过考虑类间和类内关系，对相似类别之间的混淆更加宽容，并对非典型的视觉样本进行惩罚。",
        "关键词": [
            "零样本学习",
            "三元组损失",
            "细粒度分类",
            "视觉表示学习"
        ],
        "涉及的技术概念": {
            "零样本学习（ZSL）": "一种机器学习方法，旨在识别训练数据中未出现的类别。",
            "三元组损失": "一种用于学习图像表示的方法，通过比较正样本、负样本和锚点样本来优化模型。",
            "细粒度分类": "指在非常相似的类别之间进行分类的任务，如不同种类的鸟类或车型。",
            "视觉表示学习": "指从图像中学习有用的特征表示，以便于后续的分类或其他视觉任务。"
        }
    },
    {
        "order": 1036,
        "title": "Occlusion-Shared and Feature-Separated Network for Occlusion Relationship Reasoning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Lu_Occlusion-Shared_and_Feature-Separated_Network_for_Occlusion_Relationship_Reasoning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Lu_Occlusion-Shared_and_Feature-Separated_Network_for_Occlusion_Relationship_Reasoning_ICCV_2019_paper.html",
        "abstract": "Occlusion relationship reasoning demands closed contour to express the object, and orientation of each contour pixel to describe the order relationship between objects. Current CNN-based methods neglect two critical issues of the task: (1) simultaneous existence of the relevance and distinction for the two elements, i.e, occlusion edge and occlusion orientation; and (2) inadequate exploration to the orientation features. For the reasons above, we propose the Occlusion-shared and Feature-separated Network (OFNet). On one hand, considering the relevance between edge and orientation, two sub-networks are designed to share the occlusion cue. On the other hand, the whole network is split into two paths to learn the high semantic features separately. Moreover, a contextual feature for orientation prediction is extracted, which represents the bilateral cue of the foreground and background areas. The bilateral cue is then fused with the occlusion cue to precisely locate the object regions. Finally, a stripe convolution is designed to further aggregate features from surrounding scenes of the occlusion edge. The proposed OFNet remarkably advances the state-of-the-art approaches on PIOD and BSDS ownership dataset.",
        "中文标题": "遮挡共享与特征分离网络用于遮挡关系推理",
        "摘要翻译": "遮挡关系推理需要闭合轮廓来表达对象，以及每个轮廓像素的方向来描述对象之间的顺序关系。当前基于CNN的方法忽视了该任务的两个关键问题：(1) 遮挡边缘和遮挡方向这两个元素的相关性和区别性同时存在；(2) 对方向特征的探索不足。基于上述原因，我们提出了遮挡共享与特征分离网络（OFNet）。一方面，考虑到边缘和方向之间的相关性，设计了两个子网络来共享遮挡线索。另一方面，整个网络被分成两条路径，分别学习高语义特征。此外，提取了用于方向预测的上下文特征，该特征代表了前景和背景区域的双边线索。然后，将双边线索与遮挡线索融合，以精确定位对象区域。最后，设计了一种条纹卷积，以进一步聚合遮挡边缘周围场景的特征。所提出的OFNet在PIOD和BSDS所有权数据集上显著推进了最先进的方法。",
        "领域": "遮挡关系推理/图像分割/深度学习",
        "问题": "解决遮挡关系推理中遮挡边缘和方向特征的相关性与区别性，以及方向特征探索不足的问题",
        "动机": "当前基于CNN的方法在遮挡关系推理任务中忽视了遮挡边缘和方向特征的相关性与区别性，以及对方向特征的探索不足，因此需要一种新的方法来改进这些方面",
        "方法": "提出了遮挡共享与特征分离网络（OFNet），通过设计两个子网络共享遮挡线索，将网络分成两条路径分别学习高语义特征，提取用于方向预测的上下文特征，并将双边线索与遮挡线索融合，以及设计条纹卷积进一步聚合特征",
        "关键词": [
            "遮挡关系推理",
            "图像分割",
            "深度学习"
        ],
        "涉及的技术概念": "CNN（卷积神经网络）、遮挡线索、高语义特征、上下文特征、双边线索、条纹卷积"
    },
    {
        "order": 1037,
        "title": "Mixture-Kernel Graph Attention Network for Situation Recognition",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Suhail_Mixture-Kernel_Graph_Attention_Network_for_Situation_Recognition_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Suhail_Mixture-Kernel_Graph_Attention_Network_for_Situation_Recognition_ICCV_2019_paper.html",
        "abstract": "Understanding images beyond salient actions involves reasoning about scene context, objects, and the roles they play in the captured event. Situation recognition has recently been introduced as the task of jointly reasoning about the verbs (actions) and a set of semantic-role and entity (noun) pairs in the form of action frames. Labeling an image with an action frame requires an assignment of values (nouns) to the roles based on the observed image content. Among the inherent challenges are the rich conditional structured dependencies between the output role assignments and the overall semantic sparsity. In this paper, we propose a novel mixture-kernel attention graph neural network (GNN) architecture designed to address these challenges. Our GNN enables dynamic graph structure during training and inference, through the use of a graph attention mechanism, and context-aware interactions between role pairs. We illustrate the efficacy of our model and design choices by conducting experiments on imSitu benchmark dataset, with accuracy improvements of up to 10% over the state-of-the-art.",
        "中文标题": "混合核图注意力网络用于情境识别",
        "摘要翻译": "理解图像超越显著动作涉及对场景上下文、对象及其在捕捉事件中所扮演角色的推理。情境识别最近被引入作为一项任务，旨在联合推理动词（动作）和一组语义角色与实体（名词）对，以动作框架的形式。用动作框架标记图像需要根据观察到的图像内容为角色分配值（名词）。其中的固有挑战包括输出角色分配之间的丰富条件结构化依赖性和整体语义稀疏性。在本文中，我们提出了一种新颖的混合核注意力图神经网络（GNN）架构，旨在解决这些挑战。我们的GNN通过使用图注意力机制和角色对之间的上下文感知交互，在训练和推理过程中实现动态图结构。我们通过在imSitu基准数据集上进行实验，展示了我们模型和设计选择的有效性，准确率比现有技术提高了10%。",
        "领域": "情境理解/图神经网络/语义角色标注",
        "问题": "解决图像情境识别中的角色分配和语义稀疏性问题",
        "动机": "为了更准确地理解图像中的动作和角色，需要一种能够处理输出角色分配之间复杂依赖性和语义稀疏性的方法",
        "方法": "提出了一种混合核注意力图神经网络架构，通过图注意力机制和上下文感知的角色对交互，实现动态图结构",
        "关键词": [
            "情境识别",
            "图神经网络",
            "语义角色标注",
            "注意力机制"
        ],
        "涉及的技术概念": "图神经网络（GNN）是一种处理图结构数据的神经网络，能够捕捉节点间的关系。注意力机制允许模型在处理信息时聚焦于最重要的部分。语义角色标注是自然语言处理中的一种技术，用于识别句子中动词的语义角色。"
    },
    {
        "order": 1038,
        "title": "Learning Similarity Conditions Without Explicit Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Learning_Similarity_Conditions_Without_Explicit_Supervision_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Learning_Similarity_Conditions_Without_Explicit_Supervision_ICCV_2019_paper.html",
        "abstract": "Many real-world tasks require models to compare images along multiple similarity conditions (e.g. similarity in color, category or shape). Existing methods often reason about these complex similarity relationships by learning condition-aware embeddings. While such embeddings aid models in learning different notions of similarity, they also limit their capability to generalize to unseen categories since they require explicit labels at test time. To address this deficiency, we propose an approach that jointly learns representations for the different similarity conditions and their contributions as a latent variable without explicit supervision. Comprehensive experiments across three datasets, Polyvore-Outfits, Maryland-Polyvore and UT-Zappos50k, demonstrate the effectiveness of our approach: our model outperforms the state-of-the-art methods, even those that are strongly supervised with pre-defined similarity conditions, on fill-in-the-blank, outfit compatibility prediction and triplet prediction tasks. Finally, we show that our model learns different visually-relevant semantic sub-spaces that allow it to generalize well to unseen categories.",
        "中文标题": "无需显式监督学习相似性条件",
        "摘要翻译": "许多现实世界的任务要求模型沿着多个相似性条件（例如颜色、类别或形状的相似性）比较图像。现有方法通常通过学习条件感知嵌入来推理这些复杂的相似性关系。虽然这样的嵌入有助于模型学习不同的相似性概念，但它们也限制了模型对未见类别的泛化能力，因为它们在测试时需要显式标签。为了解决这一不足，我们提出了一种方法，该方法联合学习不同相似性条件的表示及其贡献作为潜在变量，而无需显式监督。在Polyvore-Outfits、Maryland-Polyvore和UT-Zappos50k三个数据集上的综合实验证明了我们方法的有效性：我们的模型在填空、服装兼容性预测和三重预测任务上优于最先进的方法，即使是那些使用预定义相似性条件进行强监督的方法。最后，我们展示了我们的模型学习了不同的视觉相关语义子空间，使其能够很好地泛化到未见类别。",
        "领域": "图像相似性分析/服装搭配/视觉语义理解",
        "问题": "如何在没有显式监督的情况下学习图像的多条件相似性表示",
        "动机": "现有方法需要显式标签来推理复杂相似性关系，限制了模型对未见类别的泛化能力",
        "方法": "提出一种联合学习不同相似性条件表示及其贡献作为潜在变量的方法，无需显式监督",
        "关键词": [
            "图像相似性",
            "服装搭配",
            "视觉语义理解"
        ],
        "涉及的技术概念": "条件感知嵌入：一种通过学习特定条件（如颜色、类别或形状）的嵌入来推理图像相似性的方法。潜在变量：在模型中用于表示不可直接观测但影响观测数据的变量。视觉相关语义子空间：模型学习到的能够捕捉图像视觉特征的语义表示空间，有助于模型泛化到未见类别。"
    },
    {
        "order": 1039,
        "title": "Joint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Bi_Joint_Prediction_for_Kinematic_Trajectories_in_Vehicle-Pedestrian-Mixed_Scenes_ICCV_2019_paper.html",
        "abstract": "Trajectory prediction for objects is challenging and critical for various applications (e.g., autonomous driving, and anomaly detection). Most of the existing methods focus on homogeneous pedestrian trajectories prediction, where pedestrians are treated as particles without size. However, they fall short of handling crowded vehicle-pedestrian-mixed scenes directly since vehicles, limited with kinematics in reality, should be treated as rigid, non-particle objects ideally. In this paper, we tackle this problem using separate LSTMs for heterogeneous vehicles and pedestrians. Specifically, we use an oriented bounding box to represent each vehicle, calculated based on its position and orientation, to denote its kinematic trajectories. We then propose a framework called VP-LSTM to predict the kinematic trajectories of both vehicles and pedestrians simultaneously. In order to evaluate our model, a large dataset containing the trajectories of both vehicles and pedestrians in vehicle-pedestrian-mixed scenes is specially built. Through comparisons between our method with state-of-the-art approaches, we show the effectiveness and advantages of our method on kinematic trajectories prediction in vehicle-pedestrian-mixed scenes.",
        "中文标题": "车辆-行人混合场景中的运动轨迹联合预测",
        "摘要翻译": "物体轨迹预测对于各种应用（例如，自动驾驶和异常检测）既具有挑战性又至关重要。大多数现有方法集中于同质行人轨迹预测，其中行人被视为无大小的粒子。然而，这些方法直接处理拥挤的车辆-行人混合场景时存在不足，因为现实中受运动学限制的车辆应理想地被视为刚体、非粒子对象。在本文中，我们通过为异质车辆和行人使用单独的LSTM来解决这个问题。具体来说，我们使用基于其位置和方向计算的定向边界框来表示每辆车，以表示其运动轨迹。然后，我们提出了一个名为VP-LSTM的框架，以同时预测车辆和行人的运动轨迹。为了评估我们的模型，特别构建了一个包含车辆-行人混合场景中车辆和行人轨迹的大型数据集。通过将我们的方法与最先进的方法进行比较，我们展示了我们的方法在车辆-行人混合场景中运动轨迹预测的有效性和优势。",
        "领域": "自动驾驶/异常检测/运动轨迹预测",
        "问题": "在车辆-行人混合场景中准确预测车辆和行人的运动轨迹",
        "动机": "现有方法在处理车辆-行人混合场景时存在不足，因为车辆应被视为刚体、非粒子对象，而现有方法主要集中于同质行人轨迹预测。",
        "方法": "使用单独的LSTM为异质车辆和行人预测运动轨迹，并提出VP-LSTM框架同时预测车辆和行人的运动轨迹。",
        "关键词": [
            "运动轨迹预测",
            "LSTM",
            "车辆-行人混合场景"
        ],
        "涉及的技术概念": "LSTM（长短期记忆网络）用于处理时间序列数据，定向边界框用于表示车辆的位置和方向，VP-LSTM框架用于同时预测车辆和行人的运动轨迹。"
    },
    {
        "order": 1040,
        "title": "Learning to Caption Images Through a Lifetime by Asking Questions",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Shen_Learning_to_Caption_Images_Through_a_Lifetime_by_Asking_Questions_ICCV_2019_paper.html",
        "abstract": "In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.",
        "中文标题": "通过提问终身学习为图像添加描述",
        "摘要翻译": "为了让人工智能代理融入我们的生活，我们需要超越在封闭数据集上的监督学习，拥有持续扩展知识的能力。受到学生在教室中学习的启发，我们提出了一个代理，它可以通过向人类提出自然语言问题来持续学习。我们的代理由三个相互作用的模块组成，一个执行描述生成，另一个生成问题，还有一个决策者通过隐式推理代理的不确定性和教师的专业知识来学习何时提问。与当前查询图像以获取完整描述的活动学习方法相比，我们的代理能够提出有针对性的问题来改进生成的描述。代理在改进的描述上进行训练，扩展其知识。我们展示了在具有挑战性的MSCOCO数据集上，我们的方法在使用较少人类监督的情况下实现了比基线更好的性能。",
        "领域": "图像描述生成/自然语言处理/持续学习",
        "问题": "如何使人工智能代理能够持续扩展知识，特别是在图像描述生成方面",
        "动机": "超越封闭数据集上的监督学习，实现持续知识扩展，使人工智能代理更好地融入人类生活",
        "方法": "提出一个由描述生成模块、问题生成模块和决策模块组成的代理，通过向人类提出自然语言问题来持续学习和改进图像描述",
        "关键词": [
            "图像描述生成",
            "持续学习",
            "自然语言问题"
        ],
        "涉及的技术概念": "监督学习、自然语言处理、活动学习、图像描述生成、持续学习、决策模块、不确定性推理"
    },
    {
        "order": 1041,
        "title": "VrR-VG: Refocusing Visually-Relevant Relationships",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.html",
        "abstract": "Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than \"learning\" to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. Both our VrR-VG dataset and representation-aware features will be made publicly available soon.",
        "中文标题": "VrR-VG: 重新聚焦视觉相关关系",
        "摘要翻译": "关系编码了个体实例之间的相互作用，并在深度视觉场景理解中扮演着关键角色。由于非视觉信息的高预测性，关系模型往往倾向于拟合统计偏差，而不是从图像中“学习”推断关系。为了鼓励视觉关系的进一步发展，我们提出了一种新方法，通过自动修剪视觉上不相关的关系来挖掘更有价值的关系。我们基于Visual Genome构建了一个新的场景图数据集，名为视觉相关关系数据集（VrR-VG）。与现有数据集相比，VrR-VG中可学习方法和统计方法之间的性能差距更为显著，基于频率的分析不再有效。此外，我们提出通过联合考虑实例、属性和关系来学习关系感知表示。通过在VrR-VG上学习的表示感知特征，图像描述和视觉问答的性能得到了系统性提升，这证明了我们的数据集和特征嵌入方案的有效性。我们的VrR-VG数据集和表示感知特征将很快公开。",
        "领域": "场景理解/图像描述/视觉问答",
        "问题": "解决关系模型倾向于拟合统计偏差而非从图像中学习推断关系的问题",
        "动机": "鼓励视觉关系的进一步发展，通过挖掘更有价值的关系来提升深度视觉场景理解",
        "方法": "提出一种新方法，通过自动修剪视觉上不相关的关系来挖掘更有价值的关系，并构建新的场景图数据集VrR-VG，以及学习关系感知表示",
        "关键词": [
            "视觉关系",
            "场景图",
            "关系感知表示"
        ],
        "涉及的技术概念": "关系模型、统计偏差、视觉相关关系、场景图数据集、关系感知表示、图像描述、视觉问答"
    },
    {
        "order": 1042,
        "title": "TAPA-MVS: Textureless-Aware PAtchMatch Multi-View Stereo",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Romanoni_TAPA-MVS_Textureless-Aware_PAtchMatch_Multi-View_Stereo_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Romanoni_TAPA-MVS_Textureless-Aware_PAtchMatch_Multi-View_Stereo_ICCV_2019_paper.html",
        "abstract": "One of the most successful approaches in Multi-View Stereo estimates a depth map and a normal map for each view via PatchMatch-based optimization and fuses them into a consistent 3D points cloud. This approach relies on photo-consistency to evaluate the goodness of a depth estimate. It generally produces very accurate results; however, the reconstructed model often lacks completeness, especially in correspondence of broad untextured areas where the photo-consistency metrics are unreliable. Assuming the untextured areas piecewise planar, in this paper we generate novel PatchMatch hypotheses so to expand reliable depth estimates in neighboring untextured regions. At the same time, we modify the photo-consistency measure such to favor standard or novel PatchMatch depth hypotheses depending on the textureness of the considered area. We also propose a depth refinement step to filter wrong estimates and to fill the gaps on both the depth maps and normal maps while preserving the discontinuities. The effectiveness of our new methods has been tested against several state of the art algorithms in the publicly available ETH3D dataset containing a wide variety of high and low-resolution images.",
        "中文标题": "TAPA-MVS: 无纹理感知的PatchMatch多视图立体视觉",
        "摘要翻译": "在多视图立体视觉中，最成功的方法之一是通过基于PatchMatch的优化为每个视图估计深度图和法线图，并将它们融合成一个一致的三维点云。这种方法依赖于照片一致性来评估深度估计的优劣。它通常能产生非常准确的结果；然而，重建的模型往往缺乏完整性，特别是在照片一致性度量不可靠的广泛无纹理区域对应处。假设无纹理区域是分段平面的，本文中我们生成了新的PatchMatch假设，以便在邻近的无纹理区域扩展可靠的深度估计。同时，我们修改了照片一致性度量，以便根据考虑区域的纹理性来支持标准或新的PatchMatch深度假设。我们还提出了一个深度细化步骤，以过滤错误的估计并填补深度图和法线图上的空白，同时保留不连续性。我们的新方法的有效性已经在公开可用的ETH3D数据集上对几种最先进的算法进行了测试，该数据集包含了各种高分辨率和低分辨率的图像。",
        "领域": "三维重建/立体视觉/图像处理",
        "问题": "解决多视图立体视觉中无纹理区域深度估计不准确和模型不完整的问题",
        "动机": "提高在无纹理区域深度估计的准确性和重建模型的完整性",
        "方法": "生成新的PatchMatch假设以扩展无纹理区域的深度估计，修改照片一致性度量以支持不同纹理区域的深度假设，提出深度细化步骤以过滤错误估计并填补空白",
        "关键词": [
            "三维重建",
            "立体视觉",
            "图像处理"
        ],
        "涉及的技术概念": "PatchMatch优化、照片一致性度量、深度图和法线图、三维点云、无纹理区域、深度细化"
    },
    {
        "order": 1043,
        "title": "U4D: Unsupervised 4D Dynamic Scene Understanding",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mustafa_U4D_Unsupervised_4D_Dynamic_Scene_Understanding_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mustafa_U4D_Unsupervised_4D_Dynamic_Scene_Understanding_ICCV_2019_paper.html",
        "abstract": "We introduce the first approach to solve the challenging problem of unsupervised 4D visual scene understanding for complex dynamic scenes with multiple interacting people from multi-view video. Our approach simultaneously estimates a detailed model that includes a per-pixel semantically and temporally coherent reconstruction, together with instance-level segmentation exploiting photo-consistency, semantic and motion information. We further leverage recent advances in 3D pose estimation to constrain the joint semantic instance segmentation and 4D temporally coherent reconstruction. This enables per person semantic instance segmentation of multiple interacting people in complex dynamic scenes. Extensive evaluation of the joint visual scene understanding framework against state-of-the-art methods on challenging indoor and outdoor sequences demonstrates a significant (approx 40%) improvement in semantic segmentation, reconstruction and scene flow accuracy.",
        "中文标题": "U4D：无监督4D动态场景理解",
        "摘要翻译": "我们提出了首个解决复杂动态场景中多视角视频下多个人交互的无监督4D视觉场景理解挑战的方法。我们的方法同时估计一个详细模型，该模型包括每个像素的语义和时间一致性重建，以及利用照片一致性、语义和运动信息的实例级分割。我们进一步利用3D姿态估计的最新进展来约束联合语义实例分割和4D时间一致性重建。这使得在复杂动态场景中能够对多个交互的人进行每个人的语义实例分割。对联合视觉场景理解框架在具有挑战性的室内和室外序列上与最先进方法的广泛评估表明，在语义分割、重建和场景流准确性方面有显著（约40%）的改进。",
        "领域": "动态场景理解/3D重建/实例分割",
        "问题": "解决复杂动态场景中多个人交互的无监督4D视觉场景理解",
        "动机": "为了在无需人工标注的情况下，从多视角视频中理解和重建复杂动态场景，特别是包含多个人交互的场景。",
        "方法": "同时估计包括每个像素的语义和时间一致性重建的详细模型，利用照片一致性、语义和运动信息进行实例级分割，并利用3D姿态估计的最新进展来约束联合语义实例分割和4D时间一致性重建。",
        "关键词": [
            "无监督学习",
            "4D重建",
            "实例分割",
            "3D姿态估计",
            "场景流"
        ],
        "涉及的技术概念": "无监督学习指的是在没有人工标注的情况下从数据中学习；4D重建涉及从多视角视频中重建动态场景的三维结构和时间变化；实例分割是指识别和分割图像中的每个对象实例；3D姿态估计是从图像或视频中估计人体或物体的三维姿态；场景流是指场景中物体的三维运动场。"
    },
    {
        "order": 1044,
        "title": "Hierarchical Point-Edge Interaction Network for Point Cloud Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Jiang_Hierarchical_Point-Edge_Interaction_Network_for_Point_Cloud_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "We achieve 3D semantic scene labeling by exploring semantic relation between each point and its contextual neighbors through edges. Besides an encoder-decoder branch for predicting point labels, we construct an edge branch to hierarchically integrate point features and generate edge features. To incorporate point features in the edge branch, we establish a hierarchical graph framework, where the graph is initialized from a coarse layer and gradually enriched along the point decoding process. For each edge in the final graph, we predict a label to indicate the semantic consistency of the two connected points to enhance point prediction. At different layers, edge features are also fed into the corresponding point module to integrate contextual information for message passing enhancement in local regions. The two branches interact with each other and cooperate in segmentation. Decent experimental results on several 3D semantic labeling datasets demonstrate the effectiveness of our work.",
        "中文标题": "层次点-边交互网络用于点云语义分割",
        "摘要翻译": "我们通过探索每个点与其上下文邻居之间的语义关系来实现3D语义场景标记。除了用于预测点标签的编码器-解码器分支外，我们还构建了一个边分支，以层次化地整合点特征并生成边特征。为了在边分支中融入点特征，我们建立了一个层次图框架，其中图从粗层初始化，并沿着点解码过程逐渐丰富。对于最终图中的每条边，我们预测一个标签以指示两个连接点的语义一致性，以增强点预测。在不同层次，边特征也被馈送到相应的点模块中，以整合上下文信息，从而增强局部区域中的消息传递。两个分支相互交互并在分割中合作。在几个3D语义标记数据集上的良好实验结果证明了我们工作的有效性。",
        "领域": "点云处理/语义分割/3D场景理解",
        "问题": "点云数据的语义分割",
        "动机": "探索点与其上下文邻居之间的语义关系，以提高3D语义场景标记的准确性",
        "方法": "构建了一个包含编码器-解码器分支和边分支的层次点-边交互网络，通过层次图框架整合点特征和边特征，预测边的语义一致性标签以增强点预测，并在不同层次整合边特征到点模块中以增强局部区域的消息传递",
        "关键词": [
            "点云",
            "语义分割",
            "3D场景理解",
            "层次图",
            "消息传递"
        ],
        "涉及的技术概念": {
            "点云": "点云是由大量的点组成的数据集，每个点都有其空间坐标，可能还包含颜色、强度等信息。",
            "语义分割": "语义分割是指将图像或点云中的每个像素或点分配到预定义的类别中，以实现对图像或场景的语义理解。",
            "3D场景理解": "3D场景理解是指从3D数据中提取和理解场景的语义信息，包括物体的识别、分类和场景的解析等。",
            "层次图": "层次图是一种图结构，其中节点和边按照层次组织，用于表示不同层次的信息和关系。",
            "消息传递": "消息传递是指在图结构中，节点之间通过边传递信息，以便每个节点都能获取其邻居节点的信息，从而进行更准确的预测或分类。"
        }
    },
    {
        "order": 1045,
        "title": "Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point Clouds From Multiple Angles by Joint Self-Reconstruction and Half-to-Half Prediction",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_Multi-Angle_Point_Cloud-VAE_Unsupervised_Feature_Learning_for_3D_Point_Clouds_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_Multi-Angle_Point_Cloud-VAE_Unsupervised_Feature_Learning_for_3D_Point_Clouds_ICCV_2019_paper.html",
        "abstract": "Unsupervised feature learning for point clouds has been vital for large-scale point cloud understanding. Recent deep learning based methods depend on learning global geometry from self-reconstruction. However, these methods are still suffering from ineffective learning of local geometry, which significantly limits the discriminability of learned features. To resolve this issue, we propose MAP-VAE to enable the learning of global and local geometry by jointly leveraging global and local self-supervision. To enable effective local self-supervision, we introduce multi-angle analysis for point clouds. In a multi-angle scenario, we first split a point cloud into a front half and a back half from each angle, and then, train MAP-VAE to learn to predict a back half sequence from the corresponding front half sequence. MAP-VAE performs this half-to-half prediction using RNN to simultaneously learn each local geometry and the spatial relationship among them. In addition, MAP-VAE also learns global geometry via self-reconstruction, where we employ a variational constraint to facilitate novel shape generation. The outperforming results in four shape analysis tasks show that MAP-VAE can learn more discriminative global or local features than the state-of-the-art methods.",
        "中文标题": "多角度点云-VAE：通过联合自重建和半对半预测从多角度进行3D点云的无监督特征学习",
        "摘要翻译": "点云的无监督特征学习对于大规模点云理解至关重要。最近基于深度学习的方法依赖于从自重建中学习全局几何。然而，这些方法在学习局部几何方面仍然效率低下，这显著限制了学习特征的区分能力。为了解决这个问题，我们提出了MAP-VAE，通过联合利用全局和局部自监督来学习全局和局部几何。为了实现有效的局部自监督，我们引入了点云的多角度分析。在多角度场景中，我们首先从每个角度将点云分为前半部分和后半部分，然后训练MAP-VAE从前半部分序列预测相应的后半部分序列。MAP-VAE使用RNN执行这种半对半预测，以同时学习每个局部几何及其之间的空间关系。此外，MAP-VAE还通过自重建学习全局几何，我们采用变分约束来促进新形状的生成。在四个形状分析任务中的优异结果表明，MAP-VAE可以学习到比现有方法更具区分性的全局或局部特征。",
        "领域": "3D点云处理/无监督学习/特征学习",
        "问题": "解决现有方法在学习局部几何方面的效率低下问题，提高学习特征的区分能力",
        "动机": "为了提高点云无监督特征学习中局部几何的学习效率，从而增强学习特征的区分能力",
        "方法": "提出了MAP-VAE，通过联合利用全局和局部自监督来学习全局和局部几何，并引入多角度分析进行有效的局部自监督",
        "关键词": [
            "3D点云",
            "无监督学习",
            "特征学习",
            "自监督学习",
            "多角度分析"
        ],
        "涉及的技术概念": {
            "MAP-VAE": "一种用于3D点云无监督特征学习的模型，通过联合自重建和半对半预测来学习全局和局部几何",
            "RNN": "循环神经网络，用于执行半对半预测，以同时学习每个局部几何及其之间的空间关系",
            "变分约束": "用于自重建过程中，以促进新形状的生成"
        }
    },
    {
        "order": 1046,
        "title": "P-MVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_P-MVSNet_Learning_Patch-Wise_Matching_Confidence_Aggregation_for_Multi-View_Stereo_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Luo_P-MVSNet_Learning_Patch-Wise_Matching_Confidence_Aggregation_for_Multi-View_Stereo_ICCV_2019_paper.html",
        "abstract": "Learning-based methods are demonstrating their strong competitiveness in estimating depth for multi-view stereo reconstruction in recent years. Among them the approaches that generate cost volumes based on the plane-sweeping algorithm and then use them for feature matching have shown to be very prominent recently. The plane-sweep volumes are essentially anisotropic in depth and spatial directions, but they are often approximated by isotropic cost volumes in those methods, which could be detrimental. In this paper, we propose a new end-to-end deep learning network of P-MVSNet for multi-view stereo based on isotropic and anisotropic 3D convolutions. Our P-MVSNet consists of two core modules: a patch-wise aggregation module learns to aggregate the pixel-wise correspondence information of extracted features to generate a matching confidence volume, from which a hybrid 3D U-Net then infers a depth probability distribution and predicts the depth maps. We perform extensive experiments on the DTU and Tanks & Temples benchmark datasets, and the results show that the proposed P-MVSNet achieves the state-of-the-art performance over many existing methods on multi-view stereo.",
        "中文标题": "P-MVSNet: 学习基于补丁的匹配置信度聚合用于多视图立体视觉",
        "摘要翻译": "近年来，基于学习的方法在估计多视图立体重建的深度方面展示了其强大的竞争力。其中，基于平面扫描算法生成成本体积然后用于特征匹配的方法最近显示出非常突出的表现。平面扫描体积在深度和空间方向上是本质上各向异性的，但在这些方法中通常被各向同性的成本体积所近似，这可能是有害的。在本文中，我们提出了一种新的端到端深度学习网络P-MVSNet，用于基于各向同性和各向异性3D卷积的多视图立体视觉。我们的P-MVSNet由两个核心模块组成：一个基于补丁的聚合模块学习聚合提取特征的像素级对应信息以生成匹配置信度体积，然后一个混合3D U-Net从中推断深度概率分布并预测深度图。我们在DTU和Tanks & Temples基准数据集上进行了广泛的实验，结果表明，所提出的P-MVSNet在多视图立体视觉上实现了对许多现有方法的最先进性能。",
        "领域": "多视图立体视觉/3D重建/深度估计",
        "问题": "多视图立体视觉中的深度估计问题",
        "动机": "现有的方法在处理平面扫描体积时通常使用各向同性的成本体积近似，这可能对深度估计的准确性产生不利影响。",
        "方法": "提出了一种新的端到端深度学习网络P-MVSNet，该网络结合了各向同性和各向异性3D卷积，通过一个基于补丁的聚合模块和一个混合3D U-Net来生成匹配置信度体积并预测深度图。",
        "关键词": [
            "多视图立体视觉",
            "3D卷积",
            "深度估计",
            "匹配置信度",
            "3D U-Net"
        ],
        "涉及的技术概念": {
            "平面扫描算法": "一种用于生成成本体积的技术，通过在不同深度平面上扫描图像来估计深度。",
            "各向同性成本体积": "在所有方向上具有相同性质的成本体积，与各向异性成本体积相对。",
            "各向异性3D卷积": "一种在3D卷积中考虑不同方向特性的卷积方法，能够更好地处理各向异性的数据。",
            "3D U-Net": "一种用于3D数据处理的深度学习网络结构，常用于图像分割和重建任务。"
        }
    },
    {
        "order": 1047,
        "title": "SME-Net: Sparse Motion Estimation for Parametric Video Prediction Through Reinforcement Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ho_SME-Net_Sparse_Motion_Estimation_for_Parametric_Video_Prediction_Through_Reinforcement_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ho_SME-Net_Sparse_Motion_Estimation_for_Parametric_Video_Prediction_Through_Reinforcement_ICCV_2019_paper.html",
        "abstract": "This paper leverages a classic prediction technique, known as parametric overlapped block motion compensation (POBMC), in a reinforcement learning framework for video prediction. Learning-based prediction methods with explicit motion models often suffer from having to estimate large numbers of motion parameters with artificial regularization. Inspired by the success of sparse motion-based prediction for video compression, we propose a parametric video prediction on a sparse motion field composed of few critical pixels and their motion vectors. The prediction is achieved by gradually refining the estimate of a future frame in iterative, discrete steps. Along the way, the identification of critical pixels and their motion estimation are addressed by two neural networks trained under a reinforcement learning setting. Our model achieves the state-of-the-art performance on CaltchPed, UCF101 and CIF datasets in one-step and multi-step prediction tests. It shows good generalization results and is able to learn well on small training data.",
        "中文标题": "SME-Net: 通过强化学习进行参数化视频预测的稀疏运动估计",
        "摘要翻译": "本文利用一种经典的预测技术，称为参数化重叠块运动补偿（POBMC），在强化学习框架内进行视频预测。具有显式运动模型的学习型预测方法常常需要估计大量运动参数，并伴随人为的正则化。受到基于稀疏运动的视频压缩预测成功的启发，我们提出了一种在由少量关键像素及其运动矢量组成的稀疏运动场上进行参数化视频预测的方法。预测通过逐步细化未来帧的估计，在迭代的离散步骤中实现。在此过程中，关键像素的识别及其运动估计由在强化学习设置下训练的两个神经网络处理。我们的模型在CaltchPed、UCF101和CIF数据集上的一步和多步预测测试中达到了最先进的性能。它展示了良好的泛化结果，并能够在少量训练数据上良好学习。",
        "领域": "视频预测/运动估计/强化学习",
        "问题": "视频预测中需要估计大量运动参数并伴随人为正则化的问题",
        "动机": "受到基于稀疏运动的视频压缩预测成功的启发，提出在稀疏运动场上进行参数化视频预测",
        "方法": "利用参数化重叠块运动补偿（POBMC）技术，在强化学习框架内，通过两个神经网络处理关键像素的识别及其运动估计，逐步细化未来帧的估计",
        "关键词": [
            "视频预测",
            "运动估计",
            "强化学习",
            "稀疏运动",
            "参数化重叠块运动补偿"
        ],
        "涉及的技术概念": {
            "参数化重叠块运动补偿（POBMC）": "一种经典的预测技术，用于视频预测中的运动补偿",
            "稀疏运动场": "由少量关键像素及其运动矢量组成的运动场，用于减少需要估计的运动参数数量",
            "强化学习": "一种机器学习方法，通过奖励机制来训练模型，使其能够在特定任务上做出最优决策",
            "神经网络": "用于处理关键像素的识别及其运动估计的计算模型"
        }
    },
    {
        "order": 1048,
        "title": "ClothFlow: A Flow-Based Model for Clothed Person Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Han_ClothFlow_A_Flow-Based_Model_for_Clothed_Person_Generation_ICCV_2019_paper.html",
        "abstract": "We present ClothFlow, an appearance-flow-based generative model to synthesize clothed person for posed-guided person image generation and virtual try-on. By estimating a dense flow between source and target clothing regions, ClothFlow effectively models the geometric changes and naturally transfers the appearance to synthesize novel images as shown in Figure 1. We achieve this with a three-stage framework: 1) Conditioned on a target pose, we first estimate a person semantic layout to provide richer guidance to the generation process. 2) Built on two feature pyramid networks, a cascaded flow estimation network then accurately estimates the appearance matching between corresponding clothing regions. The resulting dense flow warps the source image to flexibly account for deformations. 3) Finally, a generative network takes the warped clothing regions as inputs and renders the target view. We conduct extensive experiments on the DeepFashion dataset for pose-guided person image generation and on the VITON dataset for the virtual try-on task. Strong qualitative and quantitative results validate the effectiveness of our method.",
        "中文标题": "ClothFlow: 一种基于流的穿衣人物生成模型",
        "摘要翻译": "我们提出了ClothFlow，一种基于外观流的生成模型，用于合成穿衣人物，以支持姿势引导的人物图像生成和虚拟试穿。通过估计源和目标服装区域之间的密集流，ClothFlow有效地模拟了几何变化，并自然地转移外观以合成新图像，如图1所示。我们通过一个三阶段框架实现这一点：1）在目标姿势的条件下，我们首先估计一个人物语义布局，为生成过程提供更丰富的指导。2）基于两个特征金字塔网络，级联流估计网络然后准确估计相应服装区域之间的外观匹配。由此产生的密集流扭曲源图像，以灵活地考虑变形。3）最后，生成网络将扭曲的服装区域作为输入，并渲染目标视图。我们在DeepFashion数据集上进行了广泛的实验，用于姿势引导的人物图像生成，并在VITON数据集上进行了虚拟试穿任务。强大的定性和定量结果验证了我们方法的有效性。",
        "领域": "虚拟试穿/姿势引导图像生成/服装合成",
        "问题": "如何有效地合成穿衣人物图像以支持姿势引导的图像生成和虚拟试穿",
        "动机": "为了在虚拟试穿和姿势引导的人物图像生成中实现更自然和准确的服装外观转移",
        "方法": "采用一个三阶段框架，包括人物语义布局估计、级联流估计网络和生成网络，以实现服装区域的准确匹配和自然转移",
        "关键词": [
            "虚拟试穿",
            "姿势引导图像生成",
            "服装合成"
        ],
        "涉及的技术概念": "外观流、密集流估计、特征金字塔网络、级联流估计网络、生成网络、DeepFashion数据集、VITON数据集"
    },
    {
        "order": 1049,
        "title": "LADN: Local Adversarial Disentangling Network for Facial Makeup and De-Makeup",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_LADN_Local_Adversarial_Disentangling_Network_for_Facial_Makeup_and_De-Makeup_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gu_LADN_Local_Adversarial_Disentangling_Network_for_Facial_Makeup_and_De-Makeup_ICCV_2019_paper.html",
        "abstract": "We propose a local adversarial disentangling network (LADN) for facial makeup and de-makeup. Central to our method are multiple and overlapping local adversarial discriminators in a content-style disentangling network for achieving local detail transfer between facial images, with the use of asymmetric loss functions for dramatic makeup styles with high-frequency details. Existing techniques do not demonstrate or fail to transfer high-frequency details in a global adversarial setting, or train a single local discriminator only to ensure image structure consistency and thus work only for relatively simple styles. Unlike others, our proposed local adversarial discriminators can distinguish whether the generated local image details are consistent with the corresponding regions in the given reference image in cross-image style transfer in an unsupervised setting. Incorporating these technical contributions, we achieve not only state-of-the-art results on conventional styles but also novel results involving complex and dramatic styles with high-frequency details covering large areas across multiple facial features. A carefully designed dataset of unpaired before and after makeup images is released at https://georgegu1997.github.io/LADN-project-page.",
        "中文标题": "LADN: 用于面部化妆与卸妆的局部对抗解缠网络",
        "摘要翻译": "我们提出了一种用于面部化妆与卸妆的局部对抗解缠网络（LADN）。我们方法的核心是在内容-风格解缠网络中使用多个重叠的局部对抗判别器，以实现面部图像之间的局部细节转移，并利用非对称损失函数来处理具有高频细节的戏剧性化妆风格。现有技术要么无法在全局对抗设置中转移高频细节，要么仅训练单个局部判别器以确保图像结构一致性，因此仅适用于相对简单的风格。与其它方法不同，我们提出的局部对抗判别器能够在无监督设置下的跨图像风格转移中区分生成的局部图像细节是否与给定参考图像中的相应区域一致。结合这些技术贡献，我们不仅在传统风格上取得了最先进的结果，而且在涉及复杂和戏剧性风格、覆盖多个面部特征的大面积高频细节方面也取得了新颖的结果。一个精心设计的未配对化妆前后图像数据集已在https://georgegu1997.github.io/LADN-project-page发布。",
        "领域": "面部图像处理/风格转移/对抗网络",
        "问题": "如何在面部化妆与卸妆的图像处理中有效转移高频细节",
        "动机": "现有技术在全局对抗设置中无法有效转移高频细节，或仅适用于简单风格，需要一种新方法来处理复杂和戏剧性风格",
        "方法": "提出局部对抗解缠网络（LADN），使用多个重叠的局部对抗判别器和非对称损失函数，以实现面部图像之间的局部细节转移",
        "关键词": [
            "面部化妆",
            "卸妆",
            "局部对抗解缠网络",
            "高频细节",
            "风格转移"
        ],
        "涉及的技术概念": "局部对抗解缠网络（LADN）是一种用于面部化妆与卸妆的图像处理技术，通过使用多个重叠的局部对抗判别器和非对称损失函数，实现面部图像之间的局部细节转移，特别是在处理具有高频细节的戏剧性化妆风格时表现出色。这种方法在无监督设置下进行跨图像风格转移，能够区分生成的局部图像细节是否与给定参考图像中的相应区域一致。"
    },
    {
        "order": 1050,
        "title": "Point-to-Point Video Generation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Point-to-Point_Video_Generation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Point-to-Point_Video_Generation_ICCV_2019_paper.html",
        "abstract": "While image synthesis achieves tremendous breakthroughs (e.g., generating realistic faces), video generation is less explored and harder to control, which limits its applications in the real world. For instance, video editing requires temporal coherence across multiple clips and thus poses both start and end constraints within a video sequence. We introduce point-to-point video generation that controls the generation process with two control points: the targeted start- and end-frames. The task is challenging since the model not only generates a smooth transition of frames but also plans ahead to ensure that the generated end-frame conforms to the targeted end-frame for videos of various lengths. We propose to maximize the modified variational lower bound of conditional data likelihood under a skip-frame training strategy. Our model can generate end-frame-consistent sequences without loss of quality and diversity. We evaluate our method through extensive experiments on Stochastic Moving MNIST, Weizmann Action, Human3.6M, and BAIR Robot Pushing under a series of scenarios. The qualitative results showcase the effectiveness and merits of point-to-point generation.",
        "中文标题": "点对点视频生成",
        "摘要翻译": "尽管图像合成取得了巨大的突破（例如，生成逼真的面孔），但视频生成的研究较少且更难控制，这限制了其在现实世界中的应用。例如，视频编辑需要跨多个片段的时间一致性，因此在视频序列中既提出了开始也提出了结束的约束。我们引入了点对点视频生成，该生成过程通过两个控制点来控制：目标开始帧和结束帧。这项任务具有挑战性，因为模型不仅需要生成平滑的帧过渡，还需要提前规划，以确保生成的结束帧符合各种长度视频的目标结束帧。我们提出在跳帧训练策略下最大化条件数据可能性的修改变分下界。我们的模型可以生成结束帧一致的序列，而不会损失质量和多样性。我们通过在Stochastic Moving MNIST、Weizmann Action、Human3.6M和BAIR Robot Pushing等一系列场景下的广泛实验来评估我们的方法。定性结果展示了点对点生成的有效性和优点。",
        "领域": "视频生成/时间序列分析/生成模型",
        "问题": "视频生成过程中的时间一致性和控制问题",
        "动机": "提高视频生成的可控性，以扩展其在现实世界中的应用",
        "方法": "提出点对点视频生成方法，通过两个控制点（目标开始帧和结束帧）控制生成过程，并在跳帧训练策略下最大化条件数据可能性的修改变分下界",
        "关键词": [
            "视频生成",
            "时间一致性",
            "生成模型"
        ],
        "涉及的技术概念": "点对点视频生成是一种通过指定视频的开始帧和结束帧来控制视频生成过程的技术。这种方法需要在生成过程中考虑时间一致性，确保生成的视频序列不仅平滑过渡，而且能够准确达到预定的结束帧。通过跳帧训练策略和修改变分下界的最大化，模型能够在保持视频质量和多样性的同时，实现结束帧的一致性。"
    },
    {
        "order": 1051,
        "title": "Semantics-Enhanced Adversarial Nets for Text-to-Image Synthesis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Semantics-Enhanced_Adversarial_Nets_for_Text-to-Image_Synthesis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Tan_Semantics-Enhanced_Adversarial_Nets_for_Text-to-Image_Synthesis_ICCV_2019_paper.html",
        "abstract": "This paper presents a new model, Semantics-enhanced Generative Adversarial Network (SEGAN), for fine-grained text-to-image generation. We introduce two modules, a Semantic Consistency Module (SCM) and an Attention Competition Module (ACM), to our SEGAN. The SCM incorporates image-level semantic consistency into the training of the Generative Adversarial Network (GAN), and can diversify the generated images and improve their structural coherence. A Siamese network and two types of semantic similarities are designed to map the synthesized image and the groundtruth image to nearby points in the latent semantic feature space. The ACM constructs adaptive attention weights to differentiate keywords from unimportant words, and improves the stability and accuracy of SEGAN. Extensive experiments demonstrate that our SEGAN significantly outperforms existing state-of-the-art methods in generating photo-realistic images. All source codes and models will be released for comparative study.",
        "中文标题": "语义增强的对抗网络用于文本到图像合成",
        "摘要翻译": "本文提出了一种新模型，语义增强生成对抗网络（SEGAN），用于细粒度的文本到图像生成。我们在SEGAN中引入了两个模块，语义一致性模块（SCM）和注意力竞争模块（ACM）。SCM将图像级语义一致性纳入生成对抗网络（GAN）的训练中，可以多样化生成的图像并提高其结构一致性。设计了一个Siamese网络和两种类型的语义相似性，将合成图像和真实图像映射到潜在语义特征空间中的附近点。ACM构建了自适应注意力权重，以区分关键词和不重要的词，并提高了SEGAN的稳定性和准确性。大量实验表明，我们的SEGAN在生成逼真图像方面显著优于现有的最先进方法。所有源代码和模型将发布以供比较研究。",
        "领域": "图像生成/语义理解/对抗网络",
        "问题": "细粒度的文本到图像生成",
        "动机": "提高生成图像的结构一致性和多样性，以及生成过程的稳定性和准确性",
        "方法": "引入语义一致性模块（SCM）和注意力竞争模块（ACM）到语义增强生成对抗网络（SEGAN）中，使用Siamese网络和两种语义相似性设计，构建自适应注意力权重",
        "关键词": [
            "文本到图像生成",
            "语义一致性",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "语义增强生成对抗网络（SEGAN）": "一种用于细粒度文本到图像生成的模型，通过引入语义一致性模块和注意力竞争模块来提高生成图像的质量。",
            "语义一致性模块（SCM）": "将图像级语义一致性纳入GAN训练，以提高生成图像的结构一致性和多样性。",
            "注意力竞争模块（ACM）": "构建自适应注意力权重，以区分关键词和不重要的词，提高生成过程的稳定性和准确性。",
            "Siamese网络": "一种网络结构，用于将合成图像和真实图像映射到潜在语义特征空间中的附近点。",
            "语义相似性": "用于衡量图像之间语义相似度的指标，有助于提高生成图像的质量。"
        }
    },
    {
        "order": 1052,
        "title": "VTNFP: An Image-Based Virtual Try-On Network With Body and Clothing Feature Preservation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yu_VTNFP_An_Image-Based_Virtual_Try-On_Network_With_Body_and_Clothing_ICCV_2019_paper.html",
        "abstract": "Image-based virtual try-on systems with the goal of transferring a desired clothing item onto the corresponding region of a person have made great strides recently, but challenges remain in generating realistic looking images that preserve both body and clothing details. Here we present a new virtual try-on network, called VTNFP, to synthesize photo-realistic images given the images of a clothed person and a target clothing item. In order to better preserve clothing and body features, VTNFP follows a three-stage design strategy. First, it transforms the target clothing into a warped form compatible with the pose of the given person. Next, it predicts a body segmentation map of the person wearing the target clothing, delineating body parts as well as clothing regions. Finally, the warped clothing, body segmentation map and given person image are fused together for fine-scale image synthesis. A key innovation of VTNFP is the body segmentation map prediction module, which provides critical information to guide image synthesis in regions where body parts and clothing intersects, and is very beneficial for preventing blurry pictures and preserving clothing and body part details. Experiments on a fashion dataset demonstrate that VTNFP generates substantially better results than state-of-the-art methods.",
        "中文标题": "VTNFP: 一种基于图像的虚拟试穿网络，保留身体和服装特征",
        "摘要翻译": "基于图像的虚拟试穿系统旨在将所需的服装项目转移到人的相应区域，最近取得了巨大进展，但在生成保留身体和服装细节的逼真图像方面仍存在挑战。在这里，我们提出了一种新的虚拟试穿网络，称为VTNFP，以合成给定穿着服装的人和目标服装项目的照片级真实感图像。为了更好地保留服装和身体特征，VTNFP遵循三阶段设计策略。首先，它将目标服装转换为与给定人的姿势兼容的扭曲形式。接下来，它预测穿着目标服装的人的身体分割图，描绘身体部位以及服装区域。最后，将扭曲的服装、身体分割图和给定的人图像融合在一起进行精细的图像合成。VTNFP的一个关键创新是身体分割图预测模块，它提供了关键信息来指导身体部位和服装相交区域的图像合成，并且非常有利于防止模糊图片和保留服装和身体部位的细节。在时尚数据集上的实验表明，VTNFP生成的结果大大优于最先进的方法。",
        "领域": "虚拟试穿/图像合成/时尚技术",
        "问题": "生成保留身体和服装细节的逼真虚拟试穿图像",
        "动机": "解决现有虚拟试穿系统在保留身体和服装细节方面的不足，提高虚拟试穿图像的真实感",
        "方法": "采用三阶段设计策略，包括目标服装的扭曲、身体分割图的预测以及图像合成",
        "关键词": [
            "虚拟试穿",
            "图像合成",
            "身体分割图",
            "服装特征保留"
        ],
        "涉及的技术概念": "VTNFP网络通过三个阶段实现虚拟试穿：首先对目标服装进行扭曲以适应人体姿势，然后预测穿着目标服装的身体分割图，最后通过融合扭曲服装、身体分割图和原始人像进行精细图像合成。其中，身体分割图预测模块是关键创新，用于指导身体和服装相交区域的图像合成，有效防止图像模糊并保留细节。"
    },
    {
        "order": 1053,
        "title": "Boundless: Generative Adversarial Networks for Image Extension",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Teterwak_Boundless_Generative_Adversarial_Networks_for_Image_Extension_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Teterwak_Boundless_Generative_Adversarial_Networks_for_Image_Extension_ICCV_2019_paper.html",
        "abstract": "Image extension models have broad applications in image editing, computational photography and computer graphics. While image inpainting has been extensively studied in the literature, it is challenging to directly apply the state-of-the-art inpainting methods to image extension as they tend to generate blurry or repetitive pixels with inconsistent semantics. We introduce semantic conditioning to the discriminator of a generative adversarial network (GAN), and achieve strong results on image extension with coherent semantics and visually pleasing colors and textures. We also show promising results in extreme extensions, such as panorama generation.",
        "中文标题": "无限：用于图像扩展的生成对抗网络",
        "摘要翻译": "图像扩展模型在图像编辑、计算摄影和计算机图形学中有广泛的应用。尽管图像修复在文献中已被广泛研究，但直接将最先进的修复方法应用于图像扩展具有挑战性，因为它们往往会产生模糊或重复的像素，且语义不一致。我们在生成对抗网络（GAN）的判别器中引入了语义条件，并在图像扩展上取得了语义连贯、视觉上令人愉悦的颜色和纹理的强结果。我们还在极端扩展（如全景生成）中展示了有希望的结果。",
        "领域": "图像编辑/计算摄影/计算机图形学",
        "问题": "图像扩展时产生的模糊或重复像素及语义不一致问题",
        "动机": "解决现有图像修复方法在图像扩展应用中的局限性，提高图像扩展的质量和语义连贯性",
        "方法": "在生成对抗网络（GAN）的判别器中引入语义条件",
        "关键词": [
            "图像扩展",
            "生成对抗网络",
            "语义条件"
        ],
        "涉及的技术概念": "生成对抗网络（GAN）是一种深度学习模型，由生成器和判别器组成，通过对抗过程学习生成数据。语义条件是指在判别器中引入语义信息，以指导生成器生成语义连贯的图像。图像扩展是指通过算法增加图像的尺寸或内容，使其超出原始图像的边界。"
    },
    {
        "order": 1054,
        "title": "Image Synthesis From Reconfigurable Layout and Style",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_Image_Synthesis_From_Reconfigurable_Layout_and_Style_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_Image_Synthesis_From_Reconfigurable_Layout_and_Style_ICCV_2019_paper.html",
        "abstract": "Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long- standing problem to learn generative models that are capable of synthesizing realistic and sharp images from re- configurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at https://github.com/iVMCL/LostGANs.",
        "中文标题": "从可重构布局和风格进行图像合成",
        "摘要翻译": "尽管在无条件和条件图像合成方面取得了显著的进展，但学习能够从可重构的空间布局（即图像格子中的边界框+类别标签）和风格（即由潜在向量编码的结构和外观变化）合成逼真且清晰图像的生成模型，尤其是在高分辨率下，仍然是一个长期存在的问题。所谓可重构，意味着模型能够保留从给定布局到多个具有不同风格的合理图像的内在多对一映射，并且能够适应布局和风格潜在代码的扰动。在本文中，我们提出了一种基于布局和风格的生成对抗网络架构（称为LostGANs），该架构可以端到端训练，以从可重构的布局和风格生成图像。受原始StyleGAN的启发，提出的LostGAN包含两个新组件：（i）以弱监督方式学习细粒度掩码图，以弥合布局和图像之间的差距，以及（ii）在生成器中学习对象实例特定的布局感知特征归一化（ISLA-Norm），以实现多对象风格生成。在实验中，所提出的方法在COCO-Stuff数据集和Visual Genome数据集上进行了测试，并获得了最先进的性能。代码和预训练模型可在https://github.com/iVMCL/LostGANs获取。",
        "领域": "图像合成/生成对抗网络/风格迁移",
        "问题": "如何从可重构的空间布局和风格合成逼真且清晰的图像",
        "动机": "解决从给定布局到多个具有不同风格的合理图像的内在多对一映射问题，并适应布局和风格潜在代码的扰动",
        "方法": "提出了一种基于布局和风格的生成对抗网络架构（LostGANs），包含学习细粒度掩码图和对象实例特定的布局感知特征归一化（ISLA-Norm）两个新组件",
        "关键词": [
            "图像合成",
            "生成对抗网络",
            "风格迁移",
            "布局感知特征归一化"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）、StyleGAN、弱监督学习、细粒度掩码图、布局感知特征归一化（ISLA-Norm）、COCO-Stuff数据集、Visual Genome数据集"
    },
    {
        "order": 1055,
        "title": "Attribute Manipulation Generative Adversarial Networks for Fashion Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ak_Attribute_Manipulation_Generative_Adversarial_Networks_for_Fashion_Images_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Ak_Attribute_Manipulation_Generative_Adversarial_Networks_for_Fashion_Images_ICCV_2019_paper.html",
        "abstract": "Recent advances in Generative Adversarial Networks (GANs) have made it possible to conduct multi-domain image-to-image translation using a single generative network. While recent methods such as Ganimation and SaGAN are able to conduct translations on attribute-relevant regions using attention, they do not perform well when the number of attributes increases as the training of attention masks mostly rely on classification losses. To address this and other limitations, we introduce Attribute Manipulation Generative Adversarial Networks (AMGAN) for fashion images. While AMGAN's generator network uses class activation maps (CAMs) to empower its attention mechanism, it also exploits perceptual losses by assigning reference (target) images based on attribute similarities. AMGAN incorporates an additional discriminator network that focuses on attribute-relevant regions to detect unrealistic translations. Additionally, AMGAN can be controlled to perform attribute manipulations on specific regions such as the sleeve or torso regions. Experiments show that AMGAN outperforms state-of-the-art methods using traditional evaluation metrics as well as an alternative one that is based on image retrieval.",
        "中文标题": "属性操作生成对抗网络用于时尚图像",
        "摘要翻译": "生成对抗网络（GANs）的最新进展使得使用单一生成网络进行多领域图像到图像的转换成为可能。尽管最近的方法如Ganimation和SaGAN能够使用注意力在属性相关区域进行转换，但当属性数量增加时，它们表现不佳，因为注意力掩码的训练主要依赖于分类损失。为了解决这一问题及其他限制，我们引入了用于时尚图像的属性操作生成对抗网络（AMGAN）。AMGAN的生成器网络使用类激活映射（CAMs）来增强其注意力机制，同时通过基于属性相似性分配参考（目标）图像来利用感知损失。AMGAN还包含一个额外的鉴别器网络，该网络专注于属性相关区域以检测不真实的转换。此外，AMGAN可以被控制以在特定区域（如袖子或躯干区域）执行属性操作。实验表明，AMGAN在使用传统评估指标以及基于图像检索的替代指标上均优于最先进的方法。",
        "领域": "时尚图像处理/图像生成/图像转换",
        "问题": "当属性数量增加时，现有方法在属性相关区域的图像转换表现不佳",
        "动机": "提高在时尚图像中进行多属性操作的能力和准确性",
        "方法": "引入AMGAN，使用类激活映射增强注意力机制，利用感知损失，并增加专注于属性相关区域的鉴别器网络",
        "关键词": [
            "生成对抗网络",
            "图像转换",
            "属性操作",
            "类激活映射",
            "感知损失"
        ],
        "涉及的技术概念": "生成对抗网络（GANs）是一种深度学习模型，由生成器和鉴别器组成，用于生成逼真的图像。类激活映射（CAMs）是一种技术，用于可视化神经网络中哪些区域对特定类别的预测贡献最大。感知损失是一种损失函数，用于比较生成图像和目标图像在特征空间中的差异，以改善图像质量。"
    },
    {
        "order": 1056,
        "title": "Few-Shot Unsupervised Image-to-Image Translation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Few-Shot_Unsupervised_Image-to-Image_Translation_ICCV_2019_paper.html",
        "abstract": "Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT",
        "中文标题": "少样本无监督图像到图像翻译",
        "摘要翻译": "无监督图像到图像翻译方法学习将给定类别的图像映射到不同类别中的类似图像，利用非结构化（未注册的）图像数据集。尽管非常成功，当前的方法在训练时需要访问源类和目标类中的许多图像。我们认为这大大限制了它们的使用。受到人类能够从少量示例中捕捉新物体的本质并从中进行概括的能力的启发，我们寻求一种少样本、无监督的图像到图像翻译算法，该算法适用于在测试时仅通过少量示例图像指定的先前未见过的目标类别。我们的模型通过将对抗训练方案与新颖的网络设计相结合，实现了这种少样本生成能力。通过在基准数据集上进行广泛的实验验证和与几种基线方法的比较，我们验证了所提出框架的有效性。我们的实现和数据集可在https://github.com/NVlabs/FUNIT获取。",
        "领域": "图像翻译/少样本学习/对抗网络",
        "问题": "解决在训练时仅能访问少量源类和目标类图像的无监督图像到图像翻译问题",
        "动机": "受到人类从少量示例中学习并推广的能力启发，旨在开发一种适用于未见过的目标类别的少样本无监督图像到图像翻译算法",
        "方法": "通过将对抗训练方案与新颖的网络设计相结合，实现少样本生成能力",
        "关键词": [
            "图像翻译",
            "少样本学习",
            "对抗网络"
        ],
        "涉及的技术概念": "无监督图像到图像翻译、对抗训练、少样本学习、网络设计"
    },
    {
        "order": 1057,
        "title": "Very Long Natural Scenery Image Prediction by Outpainting",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Very_Long_Natural_Scenery_Image_Prediction_by_Outpainting_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Yang_Very_Long_Natural_Scenery_Image_Prediction_by_Outpainting_ICCV_2019_paper.html",
        "abstract": "Comparing to image inpainting, image outpainting receives less attention due to two challenges in it. The first challenge is how to keep the spatial and content consistency between generated images and original input. The second challenge is how to maintain high quality in generated results, especially for multi-step generations in which generated regions are spatially far away from the initial input. To solve the two problems, we devise some innovative modules, named Skip Horizontal Connection and Recurrent Content Transfer, and integrate them into our designed encoder-decoder structure. By this design, our network can generate highly realistic outpainting prediction effectively and efficiently. Other than that, our method can generate new images with very long sizes while keeping the same style and semantic content as the given input. To test the effectiveness of the proposed architecture, we collect a new scenery dataset with diverse, complicated natural scenes. The experimental results on this dataset have demonstrated the efficacy of our proposed network.",
        "中文标题": "通过外绘预测超长自然风景图像",
        "摘要翻译": "与图像修复相比，图像外绘由于其中的两个挑战而受到较少的关注。第一个挑战是如何保持生成图像与原始输入之间的空间和内容一致性。第二个挑战是如何在生成结果中保持高质量，特别是对于多步生成，其中生成的区域在空间上远离初始输入。为了解决这两个问题，我们设计了一些创新模块，名为跳过水平连接和循环内容转移，并将它们集成到我们设计的编码器-解码器结构中。通过这种设计，我们的网络可以有效地生成高度真实的外绘预测。除此之外，我们的方法可以生成具有非常长尺寸的新图像，同时保持与给定输入相同的风格和语义内容。为了测试所提出架构的有效性，我们收集了一个新的风景数据集，包含多样化的复杂自然场景。在该数据集上的实验结果证明了我们提出的网络的有效性。",
        "领域": "图像生成/风景图像处理/深度学习应用",
        "问题": "图像外绘中的空间和内容一致性保持以及高质量生成结果的维持",
        "动机": "解决图像外绘中存在的两个主要挑战，以生成高质量且与原始输入一致的图像",
        "方法": "设计了跳过水平连接和循环内容转移模块，并将其集成到编码器-解码器结构中",
        "关键词": [
            "图像外绘",
            "空间一致性",
            "内容一致性",
            "高质量生成",
            "编码器-解码器结构"
        ],
        "涉及的技术概念": "图像外绘是一种图像处理技术，旨在扩展图像的边界，生成超出原始图像范围的视觉内容。跳过水平连接和循环内容转移是两种创新模块，用于在图像生成过程中保持空间和内容的一致性，以及提高生成图像的质量。编码器-解码器结构是一种常用的深度学习模型架构，用于图像生成任务，通过编码输入图像的特征并解码生成新的图像。"
    },
    {
        "order": 1058,
        "title": "Scaling Recurrent Models via Orthogonal Approximations in Tensor Trains",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Mehta_Scaling_Recurrent_Models_via_Orthogonal_Approximations_in_Tensor_Trains_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Mehta_Scaling_Recurrent_Models_via_Orthogonal_Approximations_in_Tensor_Trains_ICCV_2019_paper.html",
        "abstract": "Modern deep networks have proven to be very effective for analyzing real world images. However, their application in medical imaging is still in its early stages, primarily due to the large size of three-dimensional images, requiring enormous convolutional or fully connected layers - if we treat an image (and not image patches) as a sample. These issues only compound when the focus moves towards longitudinal analysis of 3D image volumes through recurrent structures, and when a point estimate of model parameters is insufficient in scientific applications where a reliability measure is necessary. Using insights from differential geometry, we adapt the tensor train decomposition to construct networks with significantly fewer parameters, allowing us to train powerful recurrent networks on whole brain image volume sequences. We describe the \"orthogonal\" tensor train, and demonstrate its ability to express a standard network layer both theoretically and empirically. We show its ability to effectively reconstruct whole brain volumes with faster convergence and stronger confidence intervals compared to the standard tensor train decomposition. We provide code and show experiments on the ADNI dataset using image sequences to regress on a cognition related outcome.",
        "中文标题": "通过张量列车中的正交近似扩展循环模型",
        "摘要翻译": "现代深度网络已被证明在分析现实世界图像方面非常有效。然而，它们在医学成像中的应用仍处于早期阶段，主要是由于三维图像的大尺寸，需要巨大的卷积或全连接层——如果我们把整个图像（而不是图像块）作为样本。当焦点转向通过循环结构对3D图像体积进行纵向分析时，以及在科学应用中需要可靠性度量而模型参数的点估计不足时，这些问题只会更加复杂。利用微分几何的见解，我们调整了张量列车分解以构建参数显著减少的网络，使我们能够在全脑图像体积序列上训练强大的循环网络。我们描述了“正交”张量列车，并在理论上和实证上展示了其表达标准网络层的能力。我们展示了其与标准张量列车分解相比，在有效重建全脑体积方面具有更快的收敛速度和更强的置信区间。我们提供了代码，并在ADNI数据集上使用图像序列对认知相关结果进行回归实验。",
        "领域": "医学成像/张量分解/循环神经网络",
        "问题": "处理三维医学图像的大尺寸和纵向分析中的挑战",
        "动机": "解决在医学成像中应用深度网络时遇到的大尺寸图像和纵向分析问题，以及提高模型参数的可靠性度量",
        "方法": "利用微分几何的见解，调整张量列车分解以构建参数显著减少的网络，训练强大的循环网络",
        "关键词": [
            "张量列车分解",
            "循环神经网络",
            "医学成像",
            "正交近似",
            "全脑图像体积"
        ],
        "涉及的技术概念": "张量列车分解是一种用于减少深度学习模型参数数量的技术，通过将高维张量分解为一系列低维张量的乘积来实现。正交近似是在张量列车分解中引入的一种方法，旨在提高模型的表达能力和训练效率。循环神经网络（RNN）是一种处理序列数据的神经网络，特别适用于时间序列分析或任何形式的序列数据。医学成像涉及使用各种技术来创建人体内部的图像，用于临床分析和医学干预。"
    },
    {
        "order": 1059,
        "title": "A Deep Cybersickness Predictor Based on Brain Signal Analysis for Virtual Reality Contents",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Kim_A_Deep_Cybersickness_Predictor_Based_on_Brain_Signal_Analysis_for_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Kim_A_Deep_Cybersickness_Predictor_Based_on_Brain_Signal_Analysis_for_ICCV_2019_paper.html",
        "abstract": "What if we could interpret the cognitive state of a user while experiencing a virtual reality (VR) and estimate the cognitive state from a visual stimulus? In this paper, we address the above question by developing an electroencephalography (EEG) driven VR cybersickness prediction model. The EEG data has been widely utilized to learn the cognitive representation of brain activity. In the first stage, to fully exploit the advantages of the EEG data, it is transformed into the multi-channel spectrogram which enables to account for the correlation of spectral and temporal coefficient. Then, a convolutional neural network (CNN) is applied to encode the cognitive representation of the EEG spectrogram. In the second stage, we train a cybersickness prediction model on the VR video sequence by designing a Recurrent Neural Network (RNN). Here, the encoded cognitive representation is transferred to the model to train the visual and cognitive features for cybersickness prediction. Through the proposed framework, it is possible to predict the cybersickness level that reflects brain activity automatically. We use 8-channels EEG data to record brain activity while more than 200 subjects experience 44 different VR contents. After rigorous training, we demonstrate that the proposed framework reliably estimates cognitive states without the EEG data. Furthermore, it achieves state-of-the-art performance comparing to existing VR cybersickness prediction models.",
        "中文标题": "基于脑信号分析的虚拟现实内容深度晕动症预测器",
        "摘要翻译": "如果我们能在用户体验虚拟现实（VR）时解释其认知状态，并从视觉刺激中估计认知状态会怎样？在本文中，我们通过开发一个基于脑电图（EEG）的VR晕动症预测模型来解决上述问题。EEG数据已被广泛用于学习大脑活动的认知表示。在第一阶段，为了充分利用EEG数据的优势，将其转换为多通道频谱图，以便考虑频谱和时间系数的相关性。然后，应用卷积神经网络（CNN）对EEG频谱图的认知表示进行编码。在第二阶段，我们通过设计一个循环神经网络（RNN）在VR视频序列上训练晕动症预测模型。在这里，编码的认知表示被转移到模型中，以训练视觉和认知特征用于晕动症预测。通过提出的框架，可以自动预测反映大脑活动的晕动症水平。我们使用8通道EEG数据记录超过200名受试者体验44种不同VR内容时的大脑活动。经过严格的训练，我们证明了所提出的框架能够可靠地估计认知状态，而无需EEG数据。此外，与现有的VR晕动症预测模型相比，它实现了最先进的性能。",
        "领域": "虚拟现实/脑机接口/认知科学",
        "问题": "如何从视觉刺激中估计用户的认知状态并预测虚拟现实中的晕动症",
        "动机": "开发一个能够解释用户在体验虚拟现实时的认知状态，并从中预测晕动症的模型",
        "方法": "首先将EEG数据转换为多通道频谱图，然后使用CNN编码认知表示，最后通过RNN训练晕动症预测模型",
        "关键词": [
            "脑电图",
            "卷积神经网络",
            "循环神经网络",
            "晕动症预测",
            "虚拟现实"
        ],
        "涉及的技术概念": "EEG数据用于学习大脑活动的认知表示，多通道频谱图用于考虑频谱和时间系数的相关性，CNN用于编码EEG频谱图的认知表示，RNN用于训练晕动症预测模型。"
    },
    {
        "order": 1060,
        "title": "Learning With Unsure Data for Medical Image Diagnosis",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Learning_With_Unsure_Data_for_Medical_Image_Diagnosis_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Learning_With_Unsure_Data_for_Medical_Image_Diagnosis_ICCV_2019_paper.html",
        "abstract": "In image-based disease prediction, it can be hard to give certain cases a deterministic \"disease/normal\" label due to lack of enough information, e.g., at its early stage. We call such cases \"unsure\" data. Labeling such data as unsure suggests follow-up examinations so as to avoid irreversible medical accident/loss in contrast to incautious prediction. This is a common practice in clinical diagnosis, however, mostly neglected by existing methods. Learning with unsure data also interweaves with two other practical issues: (i) data imbalance issue that may incur model-bias towards the majority class, and (ii) conservative/aggressive strategy consideration, i.e., the negative (normal) samples and positive (disease) samples should NOT be treated equally \\-- the former should be detected with a high precision (conservativeness) and the latter should be detected with a high recall (aggression) to avoid missing opportunity for treatment. Mixed with these issues, learning with unsure data becomes particularly challenging. In this paper, we raise \"learning with unsure data\" problem and formulate it as an ordinal regression and propose a unified end-to-end learning framework, which also considers the aforementioned two issues: (i) incorporate cost-sensitive parameters to alleviate the data imbalance problem, and (ii) execute the conservative and aggressive strategies by introducing two parameters in the training procedure. The benefits of learning with unsure data and validity of our models are demonstrated on the prediction of Alzheimer's Disease and lung nodules.",
        "中文标题": "学习不确定数据用于医学图像诊断",
        "摘要翻译": "在基于图像的疾病预测中，由于缺乏足够的信息，例如在疾病的早期阶段，很难为某些病例确定性地标记为“疾病/正常”。我们称此类病例为“不确定”数据。将此类数据标记为不确定，建议进行后续检查，以避免与不慎预测相比不可逆的医疗事故/损失。这是临床诊断中的常见做法，然而，现有方法大多忽视了这一点。学习不确定数据还与另外两个实际问题交织在一起：（i）可能导致模型偏向多数类的数据不平衡问题，以及（ii）保守/激进策略的考虑，即阴性（正常）样本和阳性（疾病）样本不应同等对待——前者应以高精度（保守性）检测，后者应以高召回率（激进性）检测，以避免错过治疗机会。与这些问题混合在一起，学习不确定数据变得特别具有挑战性。在本文中，我们提出了“学习不确定数据”问题，并将其表述为序数回归，并提出了一个统一的端到端学习框架，该框架还考虑了上述两个问题：（i）引入成本敏感参数以缓解数据不平衡问题，以及（ii）通过在训练过程中引入两个参数来执行保守和激进策略。学习不确定数据的好处和我们模型的有效性在阿尔茨海默病和肺结节的预测中得到了证明。",
        "领域": "医学图像分析/疾病预测/数据不平衡处理",
        "问题": "处理医学图像诊断中的不确定数据问题",
        "动机": "避免因不慎预测导致的不可逆医疗事故/损失，并解决数据不平衡和保守/激进策略考虑的问题",
        "方法": "提出一个统一的端到端学习框架，通过引入成本敏感参数缓解数据不平衡问题，并在训练过程中引入两个参数执行保守和激进策略",
        "关键词": [
            "不确定数据",
            "数据不平衡",
            "保守策略",
            "激进策略",
            "序数回归"
        ],
        "涉及的技术概念": "不确定数据指的是在医学图像诊断中，由于信息不足而难以确定疾病状态的病例。数据不平衡问题指的是在数据集中，某一类样本的数量远多于其他类，可能导致模型偏向多数类。保守策略和激进策略分别指在检测阴性样本时追求高精度，在检测阳性样本时追求高召回率。序数回归是一种用于处理有序分类问题的回归方法。"
    },
    {
        "order": 1061,
        "title": "Recursive Cascaded Networks for Unsupervised Medical Image Registration",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_Recursive_Cascaded_Networks_for_Unsupervised_Medical_Image_Registration_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Recursive_Cascaded_Networks_for_Unsupervised_Medical_Image_Registration_ICCV_2019_paper.html",
        "abstract": "We present recursive cascaded networks, a general architecture that enables learning deep cascades, for deformable image registration. The proposed architecture is simple in design and can be built on any base network. The moving image is warped successively by each cascade and finally aligned to the fixed image; this procedure is recursive in a way that every cascade learns to perform a progressive deformation for the current warped image. The entire system is end-to-end and jointly trained in an unsupervised manner. In addition, enabled by the recursive architecture, one cascade can be iteratively applied for multiple times during testing, which approaches a better fit between each of the image pairs. We evaluate our method on 3D medical images, where deformable registration is most commonly applied. We demonstrate that recursive cascaded networks achieve consistent, significant gains and outperform state-of-the-art methods. The performance reveals an increasing trend as long as more cascades are trained, while the limit is not observed. Code is available at https://github.com/zsyzzsoft/Recursive-Cascaded-Networks.",
        "中文标题": "递归级联网络用于无监督医学图像配准",
        "摘要翻译": "我们提出了递归级联网络，这是一种能够学习深度级联的通用架构，用于可变形图像配准。所提出的架构设计简单，可以建立在任何基础网络之上。移动图像通过每个级联依次变形，并最终与固定图像对齐；这个过程是递归的，每个级联都学会对当前变形图像执行渐进式变形。整个系统是端到端的，并以无监督的方式联合训练。此外，由于递归架构的支持，一个级联在测试期间可以多次迭代应用，从而更好地适应每对图像。我们在3D医学图像上评估了我们的方法，这是可变形配准最常应用的领域。我们证明了递归级联网络实现了持续、显著的增益，并优于最先进的方法。随着训练级联数量的增加，性能呈现出上升趋势，而没有观察到极限。代码可在https://github.com/zsyzzsoft/Recursive-Cascaded-Networks获取。",
        "领域": "医学图像处理/图像配准/深度学习",
        "问题": "解决医学图像配准中的可变形图像对齐问题",
        "动机": "提高医学图像配准的准确性和效率，特别是在3D医学图像中的应用",
        "方法": "提出了一种递归级联网络架构，通过递归方式学习深度级联，实现端到端的无监督训练，并在测试时迭代应用级联以优化图像对齐",
        "关键词": [
            "递归级联网络",
            "无监督学习",
            "图像配准",
            "3D医学图像"
        ],
        "涉及的技术概念": "递归级联网络是一种深度学习架构，用于图像配准任务，特别是医学图像的可变形配准。它通过递归方式逐步变形移动图像，使其与固定图像对齐。该架构支持无监督学习，意味着不需要标注数据即可训练模型。此外，该架构允许在测试阶段多次迭代应用级联，以优化图像对齐效果。"
    },
    {
        "order": 1062,
        "title": "DUAL-GLOW: Conditional Flow-Based Generative Model for Modality Transfer",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_DUAL-GLOW_Conditional_Flow-Based_Generative_Model_for_Modality_Transfer_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Sun_DUAL-GLOW_Conditional_Flow-Based_Generative_Model_for_Modality_Transfer_ICCV_2019_paper.html",
        "abstract": "Positron emission tomography (PET) imaging is an imaging modality for diagnosing a number of neurological diseases. In contrast to Magnetic Resonance Imaging (MRI), PET is costly and involves injecting a radioactive substance into the patient. Motivated by developments in modality transfer in vision, we study the generation of certain types of PET images from MRI data. We derive new flow-based generative models which we show perform well in this small sample size regime (much smaller than dataset sizes available in standard vision tasks). Our formulation, DUAL-GLOW, is based on two invertible networks and a relation network that maps the latent spaces to each other. We discuss how given the prior distribution, learning the conditional distribution of PET given the MRI image reduces to obtaining the conditional distribution between the two latent codes w.r.t. the two image types. We also extend our framework to leverage \"side\" information (or attributes) when available. By controlling the PET generation through \"conditioning\" on age, our model is also able to capture brain FDG-PET (hypometabolism) changes, as a function of age. We present experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset with 826 subjects, and obtain good performance in PET image synthesis, qualitatively and quantitatively better than recent works.",
        "中文标题": "DUAL-GLOW：基于条件流的生成模型用于模态转换",
        "摘要翻译": "正电子发射断层扫描（PET）成像是一种用于诊断多种神经系统疾病的成像方式。与磁共振成像（MRI）相比，PET成本高昂且需要向患者体内注射放射性物质。受到视觉模态转换发展的启发，我们研究了从MRI数据生成特定类型PET图像的方法。我们推导了新的基于流的生成模型，这些模型在小样本量情况下表现良好（远小于标准视觉任务中可用的数据集大小）。我们的模型DUAL-GLOW基于两个可逆网络和一个将潜在空间相互映射的关系网络。我们讨论了在给定先验分布的情况下，学习给定MRI图像的PET条件分布如何简化为获得两种图像类型之间潜在代码的条件分布。我们还扩展了我们的框架，以利用可用的“侧”信息（或属性）。通过“条件”控制年龄来生成PET，我们的模型还能够捕捉大脑FDG-PET（低代谢）变化，作为年龄的函数。我们在阿尔茨海默病神经影像倡议（ADNI）数据集上进行了实验，该数据集包含826名受试者，并在PET图像合成方面取得了良好的性能，定性和定量上都优于最近的工作。",
        "领域": "医学影像分析/生成模型/神经影像",
        "问题": "如何从MRI数据生成特定类型的PET图像",
        "动机": "PET成像成本高昂且需要注射放射性物质，而MRI则更为普遍和安全，因此研究从MRI生成PET图像的方法具有重要价值。",
        "方法": "提出了基于两个可逆网络和一个关系网络的DUAL-GLOW模型，通过控制条件生成PET图像，并利用侧信息提高生成质量。",
        "关键词": [
            "模态转换",
            "生成模型",
            "医学影像",
            "神经影像",
            "条件流"
        ],
        "涉及的技术概念": "基于流的生成模型是一种利用可逆网络进行数据生成的方法，DUAL-GLOW模型通过两个可逆网络和一个关系网络实现从MRI到PET的模态转换，同时利用侧信息（如年龄）来控制生成过程，以捕捉特定条件下的图像变化。"
    },
    {
        "order": 1063,
        "title": "Dilated Convolutional Neural Networks for Sequential Manifold-Valued Data",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhen_Dilated_Convolutional_Neural_Networks_for_Sequential_Manifold-Valued_Data_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhen_Dilated_Convolutional_Neural_Networks_for_Sequential_Manifold-Valued_Data_ICCV_2019_paper.html",
        "abstract": "Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and algorithms. Motivated by neuroimaging applications, we study formulations where the data are   sequential manifold-valued measurements . This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Frechet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy.",
        "中文标题": "用于序列流形值数据的扩张卷积神经网络",
        "摘要翻译": "目前正在研究如何将深度神经网络的力量扩展到非标准数据类型，如结构化数据（例如，图）或流形值数据（例如，单位向量或特殊矩阵）。通常，当这些数据空间的几何形状被纳入模型、架构和算法的设计时，可能会实现显著的实证改进。受到神经影像应用的启发，我们研究了数据为序列流形值测量的情况。这种情况在脑成像中很常见，其中样本对应于对称正定矩阵或方向分布函数。我们不是采用会带来计算/技术问题的循环模型，而是受到最近显示扩张卷积模型在序列预测中可行性的结果的启发，为此任务开发了一种扩张卷积神经网络架构。在技术方面，我们展示了如何在明确考虑黎曼流形结构的情况下推导出我们网络中所需的模块。我们展示了如何利用已知结果来计算加权弗雷谢均值（wFM）。最后，我们展示了在阿尔茨海默病（AD）群体差异分析中的科学结果，其中群体是使用AD病理负荷得出的：在这里，模型发现了几个与AD相关的脑纤维束，即使所有受试者仍然认知健康。",
        "领域": "神经影像分析/流形学习/卷积神经网络",
        "问题": "如何将深度神经网络应用于非标准数据类型，特别是序列流形值数据",
        "动机": "受到神经影像应用的启发，研究如何将深度神经网络的力量扩展到非标准数据类型，以实现显著的实证改进",
        "方法": "开发了一种扩张卷积神经网络架构，明确考虑黎曼流形结构，并利用加权弗雷谢均值（wFM）进行计算",
        "关键词": [
            "扩张卷积神经网络",
            "序列流形值数据",
            "加权弗雷谢均值",
            "神经影像分析",
            "阿尔茨海默病"
        ],
        "涉及的技术概念": "扩张卷积神经网络是一种特殊的卷积神经网络，通过引入扩张率来增加卷积层的感受野，而不增加参数数量或计算量。序列流形值数据指的是数据点位于流形上的序列数据，如对称正定矩阵或方向分布函数。加权弗雷谢均值（wFM）是在流形上计算加权平均的一种方法，考虑了流形的几何结构。"
    },
    {
        "order": 1064,
        "title": "Align, Attend and Locate: Chest X-Ray Diagnosis via Contrast Induced Attention Network With Limited Supervision",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Align_Attend_and_Locate_Chest_X-Ray_Diagnosis_via_Contrast_Induced_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_Align_Attend_and_Locate_Chest_X-Ray_Diagnosis_via_Contrast_Induced_ICCV_2019_paper.html",
        "abstract": "Obstacles facing accurate identification and localization of diseases in chest X-ray images lie in the lack of high-quality images and annotations. In this paper, we propose a Contrast Induced Attention Network (CIA-Net), which exploits the highly structured property of chest X-ray images and localizes diseases via contrastive learning on the aligned positive and negative samples. To force the attention module to focus only on sites of abnormalities, we also introduce a learnable alignment module to adjust all the input images, which eliminates variations of scales, angles, and displacements of X-ray images generated under bad scan conditions. We show that the use of contrastive attention and alignment module allows the model to learn rich identification and localization information using only a small amount of location annotations, resulting in state-of-the-art performance in NIH chest X-ray dataset.",
        "中文标题": "对齐、关注与定位：通过对比诱导注意力网络在有限监督下进行胸部X光诊断",
        "摘要翻译": "在胸部X光图像中准确识别和定位疾病面临的障碍在于缺乏高质量的图像和注释。在本文中，我们提出了一种对比诱导注意力网络（CIA-Net），它利用胸部X光图像的高度结构化特性，并通过在对齐的正负样本上进行对比学习来定位疾病。为了迫使注意力模块仅关注异常部位，我们还引入了一个可学习的对齐模块来调整所有输入图像，这消除了在不良扫描条件下生成的X光图像的尺度、角度和位移变化。我们展示了使用对比注意力和对齐模块使模型能够仅使用少量位置注释学习丰富的识别和定位信息，从而在NIH胸部X光数据集中实现了最先进的性能。",
        "领域": "医学影像分析/对比学习/注意力机制",
        "问题": "在缺乏高质量图像和注释的情况下，准确识别和定位胸部X光图像中的疾病",
        "动机": "提高胸部X光图像中疾病识别和定位的准确性，尤其是在高质量图像和注释有限的情况下",
        "方法": "提出了一种对比诱导注意力网络（CIA-Net），利用胸部X光图像的高度结构化特性，通过对比学习定位疾病，并引入可学习的对齐模块调整输入图像，消除不良扫描条件下生成的X光图像的尺度、角度和位移变化",
        "关键词": [
            "医学影像分析",
            "对比学习",
            "注意力机制",
            "胸部X光",
            "疾病定位"
        ],
        "涉及的技术概念": "对比诱导注意力网络（CIA-Net）是一种利用对比学习来定位疾病的网络，它通过对比正负样本来学习识别和定位信息。可学习的对齐模块用于调整输入图像，以消除不良扫描条件下生成的X光图像的尺度、角度和位移变化，从而提高疾病识别的准确性。"
    },
    {
        "order": 1065,
        "title": "Joint Acne Image Grading and Counting via Label Distribution Learning",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Joint_Acne_Image_Grading_and_Counting_via_Label_Distribution_Learning_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Joint_Acne_Image_Grading_and_Counting_via_Label_Distribution_Learning_ICCV_2019_paper.html",
        "abstract": "Accurate grading of skin disease severity plays a crucial role in precise treatment for patients. Acne vulgaris, the most common skin disease in adolescence, can be graded by evidence-based lesion counting as well as experience-based global estimation in the medical field. However, due to the appearance similarity of acne with close severity, it is challenging to count and grade acne accurately. In this paper, we address the problem of acne image analysis via Label Distribution Learning (LDL) considering the ambiguous information among acne severity. Based on the professional grading criterion, we generate two acne label distributions considering the relationship between the similar number of lesions and severity of acne, respectively. We also propose a unified framework for joint acne image grading and counting, which is optimized by the multi-task learning loss. In addition, we further build the ACNE04 dataset with annotations of acne severity and lesion number of each image for evaluation. Experiments demonstrate that our proposed framework performs favorably against state-of-the-art methods. We make the code and dataset publicly available at https://github.com/xpwu95/ldl.",
        "中文标题": "通过标签分布学习联合痤疮图像分级与计数",
        "摘要翻译": "准确的皮肤病严重程度分级在患者的精确治疗中起着至关重要的作用。痤疮是青春期最常见的皮肤病，在医学领域可以通过基于证据的病变计数以及基于经验的全局估计来进行分级。然而，由于严重程度相近的痤疮在外观上的相似性，准确计数和分级痤疮具有挑战性。在本文中，我们通过考虑痤疮严重程度之间的模糊信息，使用标签分布学习（LDL）来解决痤疮图像分析的问题。基于专业分级标准，我们分别生成了两个考虑相似病变数量与痤疮严重程度之间关系的痤疮标签分布。我们还提出了一个联合痤疮图像分级与计数的统一框架，该框架通过多任务学习损失进行优化。此外，我们进一步构建了ACNE04数据集，其中包含每张图像的痤疮严重程度和病变数量的注释，用于评估。实验表明，我们提出的框架在性能上优于最先进的方法。我们在https://github.com/xpwu95/ldl上公开了代码和数据集。",
        "领域": "皮肤病学/图像分析/标签分布学习",
        "问题": "准确计数和分级痤疮图像",
        "动机": "由于严重程度相近的痤疮在外观上的相似性，准确计数和分级痤疮具有挑战性",
        "方法": "使用标签分布学习（LDL）生成痤疮标签分布，并提出一个联合痤疮图像分级与计数的统一框架，通过多任务学习损失进行优化",
        "关键词": [
            "痤疮",
            "图像分级",
            "病变计数",
            "标签分布学习",
            "多任务学习"
        ],
        "涉及的技术概念": {
            "标签分布学习（LDL）": "一种考虑标签之间模糊关系的机器学习方法，用于处理具有不确定性的分类问题",
            "多任务学习": "一种机器学习方法，通过同时学习多个相关任务来提高模型的泛化能力",
            "ACNE04数据集": "一个包含痤疮图像及其严重程度和病变数量注释的数据集，用于评估痤疮图像分析方法的性能"
        }
    },
    {
        "order": 1066,
        "title": "An Alarm System for Segmentation Algorithm Based on Shape Model",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_An_Alarm_System_for_Segmentation_Algorithm_Based_on_Shape_Model_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Liu_An_Alarm_System_for_Segmentation_Algorithm_Based_on_Shape_Model_ICCV_2019_paper.html",
        "abstract": "It is usually hard for a learning system to predict correctly on rare events that never occur in the training data, and there is no exception for segmentation algorithms. Meanwhile, manual inspection of each case to locate the failures becomes infeasible due to the trend of large data scale and limited human resource. Therefore, we build an alarm system that will set off alerts when the segmentation result is possibly unsatisfactory, assuming no corresponding ground truth mask is provided. One plausible solution is to project the segmentation results into a low dimensional feature space; then learn classifiers/regressors to predict their qualities. Motivated by this, in this paper, we learn a feature space using the shape information which is a strong prior shared among different datasets and robust to the appearance variation of input data. The shape feature is captured using a Variational Auto-Encoder (VAE) network that trained with only the ground truth masks. During testing, the segmentation results with bad shapes shall not fit the shape prior well, resulting in large loss values. Thus, the VAE is able to evaluate the quality of segmentation result on unseen data, without using ground truth. Finally, we learn a regressor in the one-dimensional feature space to predict the qualities of segmentation results. Our alarm system is evaluated on several recent state-of-art segmentation algorithms for 3D medical segmentation tasks. Compared with other standard quality assessment methods, our system consistently provides more reliable prediction on the qualities of segmentation results.",
        "中文标题": "基于形状模型的分割算法报警系统",
        "摘要翻译": "对于学习系统来说，正确预测训练数据中从未出现的罕见事件通常很困难，分割算法也不例外。同时，由于数据规模大和人力资源有限的趋势，手动检查每个案例以定位失败变得不可行。因此，我们构建了一个报警系统，当分割结果可能不令人满意时，该系统将发出警报，假设没有提供相应的地面真实掩码。一个可行的解决方案是将分割结果投影到低维特征空间中；然后学习分类器/回归器来预测它们的质量。受此启发，在本文中，我们利用形状信息学习了一个特征空间，这是不同数据集之间共享的强大先验，并且对输入数据的外观变化具有鲁棒性。形状特征是通过使用仅用地面真实掩码训练的变分自编码器（VAE）网络捕获的。在测试期间，形状不佳的分割结果将不能很好地适应形状先验，导致较大的损失值。因此，VAE能够在没有使用地面真实的情况下评估未见数据的分割结果的质量。最后，我们在一维特征空间中学习了一个回归器来预测分割结果的质量。我们的报警系统在几个最新的最先进的分割算法上进行了3D医学分割任务的评估。与其他标准质量评估方法相比，我们的系统在分割结果质量的预测上始终提供更可靠的预测。",
        "领域": "医学图像分割/3D图像处理/变分自编码器",
        "问题": "分割算法在处理训练数据中未出现的罕见事件时预测不准确，且在大规模数据下手动检查失败案例不可行",
        "动机": "构建一个报警系统，当分割结果可能不令人满意时发出警报，以解决大规模数据和有限人力资源下的分割质量问题",
        "方法": "利用形状信息学习特征空间，使用变分自编码器（VAE）网络捕获形状特征，并在测试时通过VAE评估分割结果的质量，最后在一维特征空间中学习回归器预测分割结果的质量",
        "关键词": [
            "分割算法",
            "报警系统",
            "形状模型",
            "变分自编码器",
            "3D医学分割"
        ],
        "涉及的技术概念": "变分自编码器（VAE）是一种生成模型，用于学习数据的潜在表示。在本研究中，VAE被用来捕获分割结果的形状特征，并通过这些特征来评估分割结果的质量，而无需地面真实掩码。这种方法允许系统在没有直接比较的情况下预测分割结果的质量，特别是在处理未见数据时。"
    },
    {
        "order": 1067,
        "title": "HistoSegNet: Semantic Segmentation of Histological Tissue Type in Whole Slide Images",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Chan_HistoSegNet_Semantic_Segmentation_of_Histological_Tissue_Type_in_Whole_Slide_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Chan_HistoSegNet_Semantic_Segmentation_of_Histological_Tissue_Type_in_Whole_Slide_ICCV_2019_paper.html",
        "abstract": "In digital pathology, tissue slides are scanned into Whole Slide Images (WSI) and pathologists first screen for diagnostically-relevant Regions of Interest (ROIs) before reviewing them. Screening for ROIs is a tedious and time-consuming visual recognition task which can be exhausting. The cognitive workload could be reduced by developing a visual aid to narrow down the visual search area by highlighting (or segmenting) regions of diagnostic relevance, enabling pathologists to spend more time diagnosing relevant ROIs. In this paper, we propose HistoSegNet, a method for semantic segmentation of histological tissue type (HTT). Using the HTT-annotated Atlas of Digital Pathology (ADP) database, we train a Convolutional Neural Network on the patch annotations, infer Gradient-Weighted Class Activation Maps, average overlapping predictions, and post-process the segmentation with a fully-connected Conditional Random Field. Our method out-performs more complicated weakly-supervised semantic segmentation methods and can generalize to other datasets without retraining.",
        "中文标题": "HistoSegNet：全切片图像中组织类型的语义分割",
        "摘要翻译": "在数字病理学中，组织切片被扫描成全切片图像（WSI），病理学家首先筛选出诊断相关的感兴趣区域（ROIs），然后再进行审查。筛选ROIs是一项繁琐且耗时的视觉识别任务，可能会让人感到疲惫。通过开发一种视觉辅助工具，通过突出显示（或分割）诊断相关区域来缩小视觉搜索范围，可以减少认知负荷，使病理学家能够花更多时间诊断相关的ROIs。在本文中，我们提出了HistoSegNet，一种用于组织类型（HTT）语义分割的方法。使用HTT注释的数字病理学图谱（ADP）数据库，我们在补丁注释上训练卷积神经网络，推断梯度加权类激活图，平均重叠预测，并使用全连接条件随机场对分割进行后处理。我们的方法优于更复杂的弱监督语义分割方法，并且可以在不重新训练的情况下推广到其他数据集。",
        "领域": "数字病理学/语义分割/卷积神经网络",
        "问题": "在全切片图像中自动识别和分割出诊断相关的组织类型区域",
        "动机": "减少病理学家在筛选诊断相关区域时的认知负荷，提高诊断效率",
        "方法": "使用卷积神经网络训练于HTT注释的数字病理学图谱数据库，推断梯度加权类激活图，平均重叠预测，并使用全连接条件随机场进行后处理",
        "关键词": [
            "数字病理学",
            "语义分割",
            "卷积神经网络",
            "条件随机场"
        ],
        "涉及的技术概念": {
            "全切片图像（WSI）": "数字病理学中，组织切片被扫描成的高分辨率图像",
            "感兴趣区域（ROIs）": "在图像中具有诊断价值的特定区域",
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据",
            "梯度加权类激活图（Grad-CAM）": "一种可视化技术，用于理解CNN的决策过程，通过梯度信息来强调图像中对分类决策最重要的区域",
            "条件随机场（CRF）": "一种统计建模方法，常用于图像分割任务中，以提高分割的准确性和一致性"
        }
    },
    {
        "order": 1068,
        "title": "Prior-Aware Neural Network for Partially-Supervised Multi-Organ Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Prior-Aware_Neural_Network_for_Partially-Supervised_Multi-Organ_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_Prior-Aware_Neural_Network_for_Partially-Supervised_Multi-Organ_Segmentation_ICCV_2019_paper.html",
        "abstract": "Accurate multi-organ abdominal CT segmentation is essential to many clinical applications such as computer-aided intervention. As data annotation requires massive human labor from experienced radiologists, it is common that training data is usually partially-labeled. However, these background labels can be misleading in multi-organ segmentation since the \"background\" usually contains some other organs of interest. To address the background ambiguity in these partially-labeled datasets, we propose Prior-aware Neural Network (PaNN) via explicitly incorporating anatomical priors on abdominal organ sizes, guiding the training process with domain-specific knowledge. More specifically, PaNN assumes that the average organ size distributions in the abdomen should approximate their empirical distributions, a prior statistics obtained from the fully-labeled dataset. As our objective is difficult to be directly optimized using stochastic gradient descent, it is reformulated as a min-max form and optimized via the stochastic primal-dual gradient algorithm. PaNN achieves state-of-the-art performance on the MICCAI2015 challenge \"Multi-Atlas Labeling Beyond the Cranial Vault\", a competition on organ segmentation in the abdomen. We report an average Dice score of 84.97%, surpassing the prior art by a large margin of 3.27%. Code and models will be made publicly available.",
        "中文标题": "先验感知神经网络用于部分监督的多器官分割",
        "摘要翻译": "准确的多器官腹部CT分割对于许多临床应用，如计算机辅助干预，至关重要。由于数据注释需要经验丰富的放射科医生的大量人力，训练数据通常部分标注是常见的。然而，在多器官分割中，这些背景标签可能会产生误导，因为“背景”通常包含一些其他感兴趣的器官。为了解决这些部分标注数据集中的背景模糊问题，我们提出了先验感知神经网络（PaNN），通过明确地结合腹部器官大小的解剖先验，用领域特定知识指导训练过程。更具体地说，PaNN假设腹部器官的平均大小分布应接近其经验分布，这是从完全标注的数据集中获得的先验统计。由于我们的目标难以直接使用随机梯度下降优化，因此将其重新表述为最小-最大形式，并通过随机原始-对偶梯度算法进行优化。PaNN在MICCAI2015挑战“超越颅顶的多图谱标注”中实现了最先进的性能，这是一个关于腹部器官分割的竞赛。我们报告的平均Dice得分为84.97%，大大超过了之前的技术水平3.27%。代码和模型将公开提供。",
        "领域": "医学影像分析/腹部器官分割/计算机辅助诊断",
        "问题": "解决部分标注数据集中背景模糊问题，提高多器官腹部CT分割的准确性",
        "动机": "由于数据注释需要大量人力，训练数据通常部分标注，这可能导致在多器官分割中背景标签的误导",
        "方法": "提出先验感知神经网络（PaNN），通过结合腹部器官大小的解剖先验，用领域特定知识指导训练过程，并将目标重新表述为最小-最大形式，通过随机原始-对偶梯度算法进行优化",
        "关键词": [
            "医学影像分析",
            "腹部器官分割",
            "计算机辅助诊断"
        ],
        "涉及的技术概念": "先验感知神经网络（PaNN）是一种结合解剖先验知识的神经网络，用于解决部分标注数据集中的背景模糊问题。通过假设腹部器官的平均大小分布应接近其经验分布，PaNN利用从完全标注的数据集中获得的先验统计来指导训练过程。由于直接优化目标困难，PaNN将目标重新表述为最小-最大形式，并通过随机原始-对偶梯度算法进行优化。"
    },
    {
        "order": 1069,
        "title": "CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_CAMEL_A_Weakly_Supervised_Learning_Framework_for_Histopathology_Image_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_CAMEL_A_Weakly_Supervised_Learning_Framework_for_Histopathology_Image_Segmentation_ICCV_2019_paper.html",
        "abstract": "Histopathology image analysis plays a critical role in cancer diagnosis and treatment. To automatically segment the cancerous regions, fully supervised segmentation algorithms require labor-intensive and time-consuming labeling at the pixel level. In this research, we propose CAMEL, a weakly supervised learning framework for histopathology image segmentation using only image-level labels. Using multiple instance learning (MIL)-based label enrichment, CAMEL splits the image into latticed instances and automatically generates instance-level labels. After label enrichment, the instance-level labels are further assigned to the corresponding pixels, producing the approximate pixel-level labels and making fully supervised training of segmentation models possible. CAMEL achieves comparable performance with the fully supervised approaches in both instance-level classification and pixel-level segmentation on CAMELYON16 and a colorectal adenoma dataset. Moreover, the generality of the automatic labeling methodology may benefit future weakly supervised learning studies for histopathology image analysis.",
        "中文标题": "CAMEL: 一种用于病理图像分割的弱监督学习框架",
        "摘要翻译": "病理图像分析在癌症诊断和治疗中扮演着关键角色。为了自动分割癌变区域，完全监督的分割算法需要在像素级别上进行劳动密集型和耗时的标注。在本研究中，我们提出了CAMEL，一种仅使用图像级别标签的病理图像分割的弱监督学习框架。通过基于多实例学习（MIL）的标签丰富，CAMEL将图像分割成格子实例并自动生成实例级别标签。标签丰富后，实例级别标签进一步分配给相应的像素，产生近似的像素级别标签，使得分割模型的完全监督训练成为可能。CAMEL在CAMELYON16和结直肠腺瘤数据集上的实例级别分类和像素级别分割中，与完全监督方法相比，达到了可比的性能。此外，自动标注方法的通用性可能有益于未来病理图像分析的弱监督学习研究。",
        "领域": "病理图像分析/弱监督学习/图像分割",
        "问题": "如何在仅使用图像级别标签的情况下，自动分割病理图像中的癌变区域",
        "动机": "减少病理图像分割中像素级别标注的劳动密集型和耗时性",
        "方法": "提出CAMEL框架，利用多实例学习（MIL）进行标签丰富，自动生成实例级别标签，并进一步分配给像素，实现分割模型的完全监督训练",
        "关键词": [
            "病理图像分析",
            "弱监督学习",
            "图像分割",
            "多实例学习",
            "自动标注"
        ],
        "涉及的技术概念": {
            "多实例学习（MIL）": "一种机器学习方法，用于处理每个训练样本由多个实例组成的情况，其中只有部分实例被标记。",
            "标签丰富": "通过算法自动生成或增强标签信息，以减少人工标注的工作量。",
            "实例级别分类": "对图像中的每个实例进行分类，而不是整个图像。",
            "像素级别分割": "对图像中的每个像素进行分类，以识别图像中的不同区域或对象。"
        }
    },
    {
        "order": 1070,
        "title": "Conditional Recurrent Flow: Conditional Generation of Longitudinal Samples With Applications to Neuroimaging",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_Conditional_Recurrent_Flow_Conditional_Generation_of_Longitudinal_Samples_With_Applications_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Hwang_Conditional_Recurrent_Flow_Conditional_Generation_of_Longitudinal_Samples_With_Applications_ICCV_2019_paper.html",
        "abstract": "We develop a conditional generative model for longitudinal image datasets based on sequential invertible neural networks. Longitudinal image acquisitions are common in various scientific and biomedical studies where often each image sequence sample may also come together with various secondary (fixed or temporally dependent) measurements. The key goal is not only to estimate the parameters of a deep generative model for the given longitudinal data, but also to enable evaluation of how the temporal course of the generated longitudinal samples are influenced as a function of induced changes in the (secondary) temporal measurements (or events). Our proposed formulation incorporates recurrent subnetworks and temporal context gating, which provides a smooth transition in a temporal sequence of generated data that can be easily informed or modulated by secondary temporal conditioning variables. We show that the formulation works well despite the smaller sample sizes common in these applications. Our model is validated on two video datasets and a longitudinal Alzheimer's disease (AD) dataset for both quantitative and qualitative evaluations of the generated samples. Further, using our generated longitudinal image samples, we show that we can capture the pathological progressions in the brain that turn out to be consistent with the existing literature, and could facilitate various types of downstream statistical analysis.",
        "中文标题": "条件递归流：纵向样本的条件生成及其在神经影像学中的应用",
        "摘要翻译": "我们开发了一种基于序列可逆神经网络的条件生成模型，用于纵向图像数据集。纵向图像采集在各种科学和生物医学研究中很常见，其中每个图像序列样本通常还伴随着各种次要（固定或时间依赖）测量。关键目标不仅是估计给定纵向数据的深度生成模型的参数，而且还能够评估生成的纵向样本的时间过程如何作为次要时间测量（或事件）中诱导变化的函数而受到影响。我们提出的公式结合了递归子网络和时间上下文门控，这提供了生成数据时间序列中的平滑过渡，可以很容易地通过次要时间条件变量进行通知或调制。我们展示了尽管这些应用中常见的样本量较小，但该公式仍然表现良好。我们的模型在两个视频数据集和一个纵向阿尔茨海默病（AD）数据集上进行了验证，用于生成样本的定量和定性评估。此外，使用我们生成的纵向图像样本，我们展示了我们能够捕捉到与现有文献一致的大脑病理进展，并可以促进各种类型的下游统计分析。",
        "领域": "神经影像学/生物医学图像分析/时间序列分析",
        "问题": "如何在样本量较小的情况下，生成与次要时间测量或事件相关的纵向图像样本，并评估这些样本的时间过程如何受到影响",
        "动机": "为了在科学和生物医学研究中，更好地理解和分析纵向图像数据，以及这些数据如何受到次要时间测量或事件的影响",
        "方法": "开发了一种基于序列可逆神经网络的条件生成模型，结合递归子网络和时间上下文门控，以生成平滑过渡的纵向图像样本",
        "关键词": [
            "条件生成模型",
            "序列可逆神经网络",
            "递归子网络",
            "时间上下文门控",
            "纵向图像样本",
            "阿尔茨海默病"
        ],
        "涉及的技术概念": "序列可逆神经网络是一种能够处理序列数据的神经网络，通过递归子网络和时间上下文门控机制，可以在生成纵向图像样本时考虑时间序列中的平滑过渡和次要时间条件变量的影响。"
    },
    {
        "order": 1071,
        "title": "Multi-Stage Pathological Image Classification Using Semantic Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Takahama_Multi-Stage_Pathological_Image_Classification_Using_Semantic_Segmentation_ICCV_2019_paper.html",
        "abstract": "Histopathological image analysis is an essential process for the discovery of diseases such as cancer. However, it is challenging to train CNN on whole slide images (WSIs) of gigapixel resolution considering the available memory capacity. Most of the previous works divide high resolution WSIs into small image patches and separately input them into the model to classify it as a tumor or a normal tissue. However, patch-based classification uses only patch-scale local information but ignores the relationship between neighboring patches. If we consider the relationship of neighboring patches and global features, we can improve the classification performance. In this paper, we propose a new model structure combining the patch-based classification model and whole slide-scale segmentation model in order to improve the prediction performance of automatic pathological diagnosis. We extract patch features from the classification model and input them into the segmentation model to obtain a whole slide tumor probability heatmap. The classification model considers patch-scale local features, and the segmentation model can take global information into account. We also propose a new optimization method that retains gradient information and trains the model partially for end-to-end learning with limited GPU memory capacity. We apply our method to the tumor/normal prediction on WSIs and the classification performance is improved compared with the conventional patch-based method.",
        "中文标题": "使用语义分割的多阶段病理图像分类",
        "摘要翻译": "病理图像分析是发现癌症等疾病的重要过程。然而，考虑到可用内存容量，在千兆像素分辨率的全切片图像（WSIs）上训练卷积神经网络（CNN）是具有挑战性的。大多数先前的工作将高分辨率的WSIs分割成小图像块，并分别将它们输入模型以分类为肿瘤或正常组织。然而，基于图像块的分类仅使用图像块尺度的局部信息，而忽略了相邻图像块之间的关系。如果我们考虑相邻图像块的关系和全局特征，我们可以提高分类性能。在本文中，我们提出了一种新的模型结构，结合了基于图像块的分类模型和全切片尺度的分割模型，以提高自动病理诊断的预测性能。我们从分类模型中提取图像块特征，并将它们输入分割模型以获得全切片肿瘤概率热图。分类模型考虑图像块尺度的局部特征，而分割模型可以考虑全局信息。我们还提出了一种新的优化方法，该方法保留梯度信息，并在有限的GPU内存容量下部分训练模型以实现端到端学习。我们将我们的方法应用于WSIs上的肿瘤/正常预测，与传统的基于图像块的方法相比，分类性能得到了提高。",
        "领域": "病理图像分析/肿瘤诊断/图像分割",
        "问题": "在全切片图像（WSIs）上训练卷积神经网络（CNN）以进行病理图像分类时，如何有效利用局部和全局信息以提高分类性能。",
        "动机": "现有的基于图像块的分类方法仅使用局部信息，忽略了相邻图像块之间的关系和全局特征，这限制了分类性能的提升。",
        "方法": "提出了一种新的模型结构，结合了基于图像块的分类模型和全切片尺度的分割模型，通过提取图像块特征并输入分割模型以获得全切片肿瘤概率热图，同时考虑局部和全局信息。还提出了一种新的优化方法，以在有限的GPU内存容量下实现端到端学习。",
        "关键词": [
            "病理图像分析",
            "肿瘤诊断",
            "图像分割",
            "卷积神经网络",
            "全切片图像"
        ],
        "涉及的技术概念": {
            "卷积神经网络（CNN）": "一种深度学习模型，特别适用于处理图像数据。",
            "全切片图像（WSIs）": "高分辨率的病理图像，通常用于癌症等疾病的诊断。",
            "图像块": "将高分辨率图像分割成的小块，以便于处理和分析。",
            "端到端学习": "一种机器学习方法，模型直接从输入数据学习到输出结果，无需人工干预。",
            "梯度信息": "在训练深度学习模型时，用于更新模型参数的导数信息。"
        }
    },
    {
        "order": 1072,
        "title": "Semantic-Transferable Weakly-Supervised Endoscopic Lesions Segmentation",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Semantic-Transferable_Weakly-Supervised_Endoscopic_Lesions_Segmentation_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Dong_Semantic-Transferable_Weakly-Supervised_Endoscopic_Lesions_Segmentation_ICCV_2019_paper.html",
        "abstract": "Weakly-supervised learning under image-level labels supervision has been widely applied to semantic segmentation of medical lesions regions. However, 1) most existing models rely on effective constraints to explore the internal representation of lesions, which only produces inaccurate and coarse lesions regions; 2) they ignore the strong probabilistic dependencies between target lesions dataset (e.g., enteroscopy images) and well-to-annotated source diseases dataset (e.g., gastroscope images). To better utilize these dependencies, we present a new semantic lesions representation transfer model for weakly-supervised endoscopic lesions segmentation, which can exploit useful knowledge from relevant fully-labeled diseases segmentation task to enhance the performance of target weakly-labeled lesions segmentation task. More specifically, a pseudo label generator is proposed to leverage seed information to generate highly-confident pseudo pixel labels by incorporating class balance and super-pixel spatial prior. It can iteratively include more hard-to-transfer samples from weakly-labeled target dataset into training set. Afterwards, dynamically-searched feature centroids for same class among different datasets are aligned by accumulating previously-learned features. Meanwhile, adversarial learning is also employed in this paper, to narrow the gap between the lesions among different datasets in output space. Finally, we build a new medical endoscopic dataset with 3659 images collected from more than 1100 volunteers. Extensive experiments on our collected dataset and several benchmark datasets validate the effectiveness of our model.",
        "中文标题": "语义可迁移的弱监督内镜病变分割",
        "摘要翻译": "在图像级标签监督下的弱监督学习已广泛应用于医学病变区域的语义分割。然而，1）大多数现有模型依赖于有效约束来探索病变的内部表示，这仅产生不准确和粗糙的病变区域；2）它们忽略了目标病变数据集（例如，肠镜图像）和良好注释的源疾病数据集（例如，胃镜图像）之间的强概率依赖性。为了更好地利用这些依赖性，我们提出了一种新的语义病变表示转移模型，用于弱监督内镜病变分割，该模型可以利用相关全标签疾病分割任务中的有用知识来增强目标弱标签病变分割任务的性能。更具体地说，提出了一个伪标签生成器，通过结合类别平衡和超像素空间先验，利用种子信息生成高置信度的伪像素标签。它可以迭代地将更多难以转移的样本从弱标签目标数据集纳入训练集。之后，通过累积先前学习的特征，对齐不同数据集中同一类别的动态搜索特征中心。同时，本文还采用了对抗学习，以缩小不同数据集间病变在输出空间中的差距。最后，我们建立了一个新的医学内镜数据集，包含从1100多名志愿者收集的3659张图像。在我们收集的数据集和几个基准数据集上的广泛实验验证了我们模型的有效性。",
        "领域": "医学图像分析/内镜图像处理/病变分割",
        "问题": "弱监督学习在医学病变区域语义分割中的应用存在不准确和粗糙的问题，以及忽略目标病变数据集与源疾病数据集之间的强概率依赖性。",
        "动机": "为了更好地利用目标病变数据集与源疾病数据集之间的强概率依赖性，提高弱监督内镜病变分割的性能。",
        "方法": "提出了一种新的语义病变表示转移模型，包括伪标签生成器、动态搜索特征中心对齐和对抗学习。",
        "关键词": [
            "弱监督学习",
            "语义分割",
            "内镜图像",
            "病变分割",
            "伪标签生成",
            "对抗学习"
        ],
        "涉及的技术概念": "弱监督学习是指在只有图像级标签的情况下进行学习，语义分割是指将图像分割成多个语义区域，内镜图像是指通过内窥镜拍摄的医学图像，病变分割是指识别和分割图像中的病变区域，伪标签生成是指在没有真实标签的情况下生成标签，对抗学习是指通过对抗过程来缩小不同数据集间的差异。"
    },
    {
        "order": 1073,
        "title": "Unsupervised Microvascular Image Segmentation Using an Active Contours Mimicking Neural Network",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Gur_Unsupervised_Microvascular_Image_Segmentation_Using_an_Active_Contours_Mimicking_Neural_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Gur_Unsupervised_Microvascular_Image_Segmentation_Using_an_Active_Contours_Mimicking_Neural_ICCV_2019_paper.html",
        "abstract": "The task of blood vessel segmentation in microscopy images is crucial for many diagnostic and research applications. However, vessels can look vastly different, depending on the transient imaging conditions, and collecting data for supervised training is laborious. We present a novel deep learning method for unsupervised segmentation of blood vessels. The method is inspired by the field of active contours and we introduce a new loss term, which is based on the morphological Active Contours Without Edges (ACWE) optimization method. The role of the morphological operators is played by novel pooling layers that are incorporated to the network's architecture. We demonstrate the challenges that are faced by previous supervised learning solutions, when the imaging conditions shift. Our unsupervised method is able to outperform such previous methods in both the labeled dataset, and when applied to similar but different datasets. Our code, as well as efficient pytorch reimplementations of the baseline methods VesselNN and DeepVess are attached as supplementary.",
        "中文标题": "使用模仿主动轮廓的神经网络进行无监督微血管图像分割",
        "摘要翻译": "在显微镜图像中进行血管分割的任务对于许多诊断和研究应用至关重要。然而，血管的外观可能因瞬时的成像条件而有很大差异，且收集用于监督训练的数据是费力的。我们提出了一种新的深度学习方法，用于血管的无监督分割。该方法受到主动轮廓领域的启发，我们引入了一个新的损失项，该损失项基于形态学的无边缘主动轮廓（ACWE）优化方法。形态学操作的作用由新颖的池化层扮演，这些池化层被整合到网络的架构中。我们展示了当成像条件变化时，以前的监督学习解决方案所面临的挑战。我们的无监督方法在标记数据集上以及应用于相似但不同的数据集时，能够超越这些以前的方法。我们的代码，以及基线方法VesselNN和DeepVess的高效pytorch重新实现，作为补充材料附上。",
        "领域": "医学图像分析/血管分割/无监督学习",
        "问题": "在显微镜图像中进行血管分割，特别是在成像条件变化时，如何有效且无需大量标注数据进行训练。",
        "动机": "由于血管在显微镜图像中的外观可能因成像条件而有很大差异，且收集用于监督训练的数据费时费力，因此需要一种无需大量标注数据的有效血管分割方法。",
        "方法": "提出了一种新的深度学习方法，该方法受到主动轮廓领域的启发，引入了一个基于形态学的无边缘主动轮廓（ACWE）优化方法的新损失项，并通过整合新颖的池化层到网络架构中来实现形态学操作。",
        "关键词": [
            "血管分割",
            "无监督学习",
            "主动轮廓",
            "形态学操作",
            "深度学习"
        ],
        "涉及的技术概念": "主动轮廓（Active Contours）是一种图像分割技术，通过模拟物理过程来检测图像中的对象边界。无边缘主动轮廓（ACWE）是主动轮廓的一种变体，它不依赖于图像边缘信息。形态学操作包括膨胀、腐蚀等，用于图像处理中以改变图像的形状或结构。池化层是深度学习中的一种层，用于减少数据的空间尺寸，从而减少计算量和防止过拟合。"
    },
    {
        "order": 1074,
        "title": "GLAMpoints: Greedily Learned Accurate Match Points",
        "pdf": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Truong_GLAMpoints_Greedily_Learned_Accurate_Match_Points_ICCV_2019_paper.pdf",
        "html": "https://openaccess.thecvf.com/content_ICCV_2019/html/Truong_GLAMpoints_Greedily_Learned_Accurate_Match_Points_ICCV_2019_paper.html",
        "abstract": "We introduce a novel CNN-based feature point detector - Greedily Learned Accurate Match Points (GLAMpoints) - learned in a semi-supervised manner. Our detector extracts repeatable, stable interest points with a dense coverage, specifically designed to maximize the correct matching in a specific domain, which is in contrast to conventional techniques that optimize indirect metrics. In this paper, we apply our method on challenging retinal slitlamp images, for which classical detectors yield unsatisfactory results due to low image quality and insufficient amount of low-level features. We show that GLAMpoints significantly outperforms classical detectors as well as state-of-the-art CNN-based methods in matching and registration quality for retinal images.",
        "中文标题": "GLAMpoints: 贪婪学习精确匹配点",
        "摘要翻译": "我们介绍了一种新颖的基于CNN的特征点检测器——贪婪学习精确匹配点（GLAMpoints），它以半监督的方式学习。我们的检测器提取具有密集覆盖的重复、稳定的兴趣点，专门设计用于在特定领域最大化正确匹配，这与优化间接指标的传统技术形成对比。在本文中，我们将我们的方法应用于具有挑战性的视网膜裂隙灯图像，对于这些图像，由于图像质量低和低层次特征不足，经典检测器产生的结果不尽人意。我们展示了GLAMpoints在视网膜图像的匹配和注册质量上显著优于经典检测器以及最先进的基于CNN的方法。",
        "领域": "医学图像分析/特征点检测/图像匹配",
        "问题": "在低质量和低层次特征不足的视网膜裂隙灯图像中，如何提高特征点检测和匹配的准确性",
        "动机": "传统特征点检测器在低质量和低层次特征不足的视网膜裂隙灯图像上表现不佳，需要一种新的方法来提高匹配和注册的准确性",
        "方法": "开发了一种基于CNN的特征点检测器GLAMpoints，通过半监督学习方式，专门设计用于在特定领域最大化正确匹配",
        "关键词": [
            "特征点检测",
            "图像匹配",
            "视网膜图像"
        ],
        "涉及的技术概念": "CNN（卷积神经网络）、半监督学习、特征点检测、图像匹配、视网膜裂隙灯图像"
    }
]