[
    {
        "order": 0,
        "title": "12-Lead ECG Reconstruction via Koopman Operators",
        "html": "https://ICML.cc//virtual/2021/poster/10177",
        "abstract": "32% of all global deaths in the world are caused by cardiovascular diseases. Early detection, especially for patients with ischemia or cardiac arrhythmia, is crucial. To reduce the time between symptoms onset and treatment, wearable ECG sensors were developed to allow for the recording of the full 12-lead ECG signal at home.  However, if even a single lead is not correctly positioned on the body that lead becomes corrupted, making automatic diagnosis on the basis of the full signal impossible. In this work, we present a methodology to reconstruct missing or noisy leads using the theory of Koopman Operators.  Given a dataset consisting of full 12-lead ECGs, we learn a dynamical system describing the evolution of the 12 individual signals together in time.  The Koopman theory indicates that there exists a high-dimensional embedding space in which the operator which propagates from one time instant to the next is linear.  We therefore learn both the mapping to this embedding space, as well as the corresponding linear operator.  Armed with this representation, we are able to impute missing leads by solving a least squares system in the embedding space, which can be achieved efficiently due to the sparse structure of the system. We perform an empirical evaluation using 12-lead ECG signals from thousands of patients, and show that we are able to reconstruct the signals in such way that enables accurate clinical diagnosis.",
        "conference": "ICML",
        "中文标题": "通过Koopman算子重构12导联心电图",
        "摘要翻译": "全球32%的死亡是由心血管疾病引起的。早期检测，特别是对于缺血或心律失常患者，至关重要。为了缩短症状出现到治疗之间的时间，开发了可穿戴心电图传感器，允许在家记录完整的12导联心电图信号。然而，即使有一个导联没有正确放置在身体上，该导联就会损坏，使得基于完整信号的自动诊断变得不可能。在这项工作中，我们提出了一种使用Koopman算子理论来重建缺失或噪声导联的方法。给定一个包含完整12导联心电图的数据集，我们学习了一个描述12个单独信号随时间演化的动态系统。Koopman理论表明，存在一个高维嵌入空间，其中从一个时间点到下一个时间点的传播算子是线性的。因此，我们学习了到这个嵌入空间的映射，以及相应的线性算子。有了这种表示，我们能够通过在嵌入空间中解决一个最小二乘系统来填补缺失的导联，由于系统的稀疏结构，这可以高效地实现。我们使用来自数千名患者的12导联心电图信号进行了实证评估，并表明我们能够以支持准确临床诊断的方式重建信号。",
        "领域": "心电图信号处理、动态系统建模、医疗诊断辅助",
        "问题": "解决12导联心电图中因导联错位或损坏导致的信号缺失或噪声问题，以支持准确的自动诊断。",
        "动机": "心血管疾病的高死亡率要求早期准确诊断，而可穿戴设备记录的12导联心电图信号可能因导联错位或损坏而影响诊断准确性。",
        "方法": "利用Koopman算子理论学习12导联心电图信号的动态系统模型，通过高维嵌入空间中的线性算子重建缺失或噪声导联。",
        "关键词": [
            "Koopman算子",
            "12导联心电图",
            "信号重建",
            "动态系统建模",
            "医疗诊断"
        ],
        "涉及的技术概念": {
            "Koopman算子": "用于描述动态系统在高维嵌入空间中的线性演化，使得信号重建问题可以通过线性代数方法解决。",
            "高维嵌入空间": "通过将原始信号映射到更高维的空间，使得动态系统的演化可以用线性算子表示，简化了信号重建问题。",
            "最小二乘系统": "在嵌入空间中用于解决信号重建问题的数学框架，利用系统的稀疏结构实现高效计算。"
        },
        "success": true
    },
    {
        "order": 1,
        "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed",
        "html": "https://ICML.cc//virtual/2021/poster/9809",
        "abstract": "Scalable training of large models (like BERT and GPT-3) requires careful optimization rooted in model design, architecture, and system capabilities. From a system standpoint, communication has become a major bottleneck, especially on commodity systems with standard TCP interconnects that offer limited network bandwidth. Communication compression is an important technique to reduce training time on such systems. One of the most effective ways to compress communication is via error compensation compression, which offers robust convergence speed, even under 1-bit compression. However, state-of-the-art error compensation techniques only work with basic optimizers like SGD and momentum SGD, which are linearly dependent on the gradients. They do not work with non-linear gradient-based optimizers like Adam, which offer state-of-the-art convergence efficiency and accuracy for models like BERT. In this paper, we propose 1-bit Adam that reduces the communication volume by up to 5x, offers much better scalability, and provides the same convergence speed as uncompressed Adam. Our key finding is that Adam's variance becomes stable (after a warmup phase) and can be used as a fixed precondition for the rest of the training (compression phase). We performed experiments on up to 256 GPUs and show that 1-bit Adam enables up to 3.3x higher throughput for BERT-Large pre-training and up to 2.9x higher throughput for SQuAD fine-tuning. In addition, we provide theoretical analysis for 1-bit Adam.",
        "conference": "ICML",
        "中文标题": "1-bit Adam：通信高效的大规模训练与Adam的收敛速度",
        "摘要翻译": "大规模模型（如BERT和GPT-3）的可扩展训练需要基于模型设计、架构和系统能力的精心优化。从系统角度来看，通信已成为主要瓶颈，尤其是在提供有限网络带宽的标准TCP互连的商用系统上。通信压缩是减少此类系统训练时间的重要技术。最有效的通信压缩方式之一是通过误差补偿压缩，即使在1-bit压缩下也能提供稳健的收敛速度。然而，最先进的误差补偿技术仅适用于如SGD和动量SGD这样的基本优化器，这些优化器线性依赖于梯度。它们不适用于如Adam这样的基于梯度的非线性优化器，Adam为如BERT这样的模型提供了最先进的收敛效率和准确性。在本文中，我们提出了1-bit Adam，它将通信量减少了多达5倍，提供了更好的可扩展性，并且提供了与未压缩Adam相同的收敛速度。我们的关键发现是Adam的方差在预热阶段后变得稳定，并可以作为训练剩余部分（压缩阶段）的固定预处理条件。我们在多达256个GPU上进行了实验，结果表明1-bit Adam在BERT-Large预训练中实现了高达3.3倍的吞吐量提升，在SQuAD微调中实现了高达2.9倍的吞吐量提升。此外，我们还为1-bit Adam提供了理论分析。",
        "领域": "大规模深度学习训练、优化算法、通信压缩",
        "问题": "解决在大规模模型训练中通信成为瓶颈的问题，特别是在使用Adam优化器时如何有效压缩通信量而不牺牲收敛速度。",
        "动机": "为了提升大规模模型训练的效率和可扩展性，特别是在有限网络带宽的环境下，减少通信开销同时保持Adam优化器的收敛速度和准确性。",
        "方法": "提出1-bit Adam方法，通过发现Adam优化器方差在预热阶段后稳定的特性，将其作为固定预处理条件，实现通信量的有效压缩。",
        "关键词": [
            "1-bit Adam",
            "通信压缩",
            "大规模训练",
            "优化算法",
            "深度学习"
        ],
        "涉及的技术概念": {
            "误差补偿压缩": "一种通信压缩技术，即使在极端压缩条件下也能保持模型的收敛速度。",
            "Adam优化器": "一种基于梯度的非线性优化器，以其高效的收敛速度和准确性在深度学习模型中广泛应用。",
            "预热阶段": "训练初期的一个阶段，用于稳定优化器的某些参数（如方差），为后续的压缩阶段创造条件。"
        },
        "success": true
    },
    {
        "order": 2,
        "title": "A Bit More Bayesian: Domain-Invariant Learning with Uncertainty",
        "html": "https://ICML.cc//virtual/2021/poster/10665",
        "abstract": "Domain generalization is challenging due to the domain shift and the uncertainty caused by the inaccessibility of target domain data. In this paper, we address both challenges with a probabilistic framework based on variational Bayesian inference, by incorporating uncertainty into neural network weights. We couple domain invariance in a probabilistic formula with the variational Bayesian inference. This enables us to explore domain-invariant learning in a principled way. Specifically, we derive domain-invariant representations and classifiers, which are jointly established in a two-layer Bayesian neural network. We empirically demonstrate the effectiveness of our proposal on four widely used cross-domain visual recognition benchmarks. Ablation studies validate the synergistic benefits of our Bayesian treatment when jointly learning domain-invariant representations and classifiers for domain generalization. Further, our method consistently delivers state-of-the-art mean accuracy on all benchmarks.",
        "conference": "ICML",
        "中文标题": "更贝叶斯一点：基于不确定性的域不变学习",
        "摘要翻译": "由于域偏移和目标域数据不可访问性引起的不确定性，域泛化具有挑战性。在本文中，我们通过基于变分贝叶斯推断的概率框架，将不确定性纳入神经网络权重，以解决这两个挑战。我们将域不变性与变分贝叶斯推断在概率公式中耦合，这使我们能够以原则性的方式探索域不变学习。具体而言，我们推导出了域不变表示和分类器，它们在一个两层的贝叶斯神经网络中共同建立。我们在四个广泛使用的跨域视觉识别基准上实证证明了我们提议的有效性。消融研究验证了我们的贝叶斯处理在联合学习域不变表示和分类器以进行域泛化时的协同效益。此外，我们的方法在所有基准测试中始终提供最先进的平均准确率。",
        "领域": "域泛化、变分贝叶斯推断、跨域视觉识别",
        "问题": "解决域偏移和目标域数据不可访问性引起的不确定性，实现域不变学习",
        "动机": "探索在域泛化中如何有效地利用不确定性，以提高模型在未见过的域上的表现",
        "方法": "采用基于变分贝叶斯推断的概率框架，将不确定性纳入神经网络权重，耦合域不变性与变分贝叶斯推断，推导域不变表示和分类器",
        "关键词": [
            "域泛化",
            "变分贝叶斯推断",
            "域不变学习",
            "跨域视觉识别",
            "贝叶斯神经网络"
        ],
        "涉及的技术概念": {
            "变分贝叶斯推断": "用于将不确定性纳入神经网络权重的概率框架，帮助模型在域泛化中更好地处理不确定性",
            "域不变性": "通过概率公式与变分贝叶斯推断耦合，旨在学习能够跨不同域泛化的表示和分类器",
            "贝叶斯神经网络": "采用两层的贝叶斯神经网络结构，共同建立域不变表示和分类器，以提高模型在未见过的域上的表现"
        },
        "success": true
    },
    {
        "order": 3,
        "title": "Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework",
        "html": "https://ICML.cc//virtual/2021/poster/9081",
        "abstract": "Most neural network pruning methods, such as filter-level and layer-level prunings, prune the network model along one dimension (depth, width, or resolution) solely to meet a computational budget. However, such a pruning policy often leads to excessive reduction of that dimension, thus inducing a huge accuracy loss. To alleviate this issue, we argue that pruning should be conducted along three dimensions comprehensively. For this purpose, our pruning framework formulates pruning as an optimization problem. Specifically, it first casts the relationships between a certain model's accuracy and depth/width/resolution into a polynomial regression and then maximizes the polynomial to acquire the optimal values for the three dimensions. Finally, the model is pruned along the three optimal dimensions accordingly. In this framework, since collecting too much data for training the regression is very time-costly, we propose two approaches to lower the cost: 1) specializing the polynomial to ensure an accurate regression even with less training data; 2) employing iterative pruning and fine-tuning to collect the data faster. Extensive experiments show that our proposed algorithm surpasses state-of-the-art pruning algorithms and even neural architecture search-based algorithms.",
        "conference": "ICML",
        "中文标题": "从三个维度加速CNNs：一个全面的剪枝框架",
        "摘要翻译": "大多数神经网络剪枝方法，如滤波器级和层级剪枝，仅沿一个维度（深度、宽度或分辨率）剪枝网络模型以满足计算预算。然而，这样的剪枝策略往往导致该维度的过度减少，从而引起巨大的准确率损失。为了缓解这个问题，我们认为剪枝应该全面沿三个维度进行。为此，我们的剪枝框架将剪枝制定为一个优化问题。具体来说，它首先将某个模型的准确率与深度/宽度/分辨率之间的关系转化为多项式回归，然后最大化该多项式以获得这三个维度的最优值。最后，模型根据这三个最优维度进行剪枝。在这个框架中，由于收集过多数据来训练回归非常耗时，我们提出了两种方法来降低成本：1）专门化多项式以确保即使训练数据较少也能准确回归；2）采用迭代剪枝和微调以更快地收集数据。大量实验表明，我们提出的算法超越了最先进的剪枝算法，甚至超越了基于神经架构搜索的算法。",
        "领域": "模型压缩、神经网络优化、计算机视觉",
        "问题": "解决传统神经网络剪枝方法因仅沿单一维度剪枝而导致的准确率损失问题",
        "动机": "通过全面考虑深度、宽度和分辨率三个维度进行剪枝，以减少准确率损失并满足计算预算",
        "方法": "将剪枝制定为优化问题，通过多项式回归确定深度、宽度和分辨率的最优值，并采用专门化多项式和迭代剪枝微调降低数据收集成本",
        "关键词": [
            "神经网络剪枝",
            "模型优化",
            "多项式回归",
            "迭代剪枝",
            "微调"
        ],
        "涉及的技术概念": {
            "多项式回归": "用于建立模型准确率与深度/宽度/分辨率之间关系的数学模型，以确定最优剪枝维度",
            "迭代剪枝": "通过多次剪枝和微调过程，逐步优化模型结构，减少每次剪枝对模型性能的影响",
            "微调": "在剪枝后对模型进行再训练，以恢复或提升模型的性能"
        },
        "success": true
    },
    {
        "order": 4,
        "title": "Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with O(1/k^2) Rate on Squared Gradient Norm",
        "html": "https://ICML.cc//virtual/2021/poster/10227",
        "abstract": "In this work, we study the computational complexity of reducing the squared gradient magnitude for smooth minimax optimization problems. First, we present algorithms with accelerated $\\mathcal{O}(1/k^2)$ last-iterate rates, faster than the existing $\\mathcal{O}(1/k)$ or slower rates for extragradient, Popov, and gradient descent with anchoring. The acceleration mechanism combines extragradient steps with anchoring and is distinct from Nesterov's acceleration. We then establish optimality of the $\\mathcal{O}(1/k^2)$ rate through a matching lower bound.",
        "conference": "ICML",
        "success": true,
        "中文标题": "光滑凸凹极小极大问题的加速算法，在平方梯度范数上达到O(1/k^2)速率",
        "摘要翻译": "在这项工作中，我们研究了降低光滑极小极大优化问题平方梯度幅度的计算复杂性。首先，我们提出了算法，其加速的$\\mathcal{O}(1/k^2)$最后迭代速率，比现有的外梯度、Popov和带有锚定的梯度下降的$\\mathcal{O}(1/k)$或更慢的速率更快。加速机制将外梯度步骤与锚定相结合，并且不同于Nesterov的加速。然后，我们通过匹配下界建立$\\mathcal{O}(1/k^2)$速率的最优性。",
        "领域": "优化算法、凸优化、对抗学习",
        "问题": "如何设计加速算法，以更快地降低光滑凸凹极小极大优化问题的平方梯度幅度。",
        "动机": "现有的外梯度、Popov和带有锚定的梯度下降算法在解决光滑极小极大优化问题时，收敛速度较慢（O(1/k)或更慢），为了提高收敛速度，有必要设计更快的算法。",
        "方法": "结合外梯度步骤与锚定技术，设计了一种新的加速机制，并在理论上证明了该算法的收敛速度达到了最优的O(1/k^2)速率。",
        "关键词": [
            "极小极大优化",
            "加速算法",
            "外梯度法",
            "锚定",
            "收敛速度"
        ],
        "涉及的技术概念": {
            "极小极大优化": "一种优化问题，目标是找到一个点，使得一个函数在一个变量上最小化，在另一个变量上最大化。该问题广泛应用于对抗学习等领域。",
            "外梯度法": "一种用于解决极小极大问题的迭代算法，通过在原始变量和对偶变量上交替执行梯度更新来逼近最优解。"
        }
    },
    {
        "order": 5,
        "title": "Accelerating Feedforward Computation via Parallel Nonlinear Equation Solving",
        "html": "https://ICML.cc//virtual/2021/poster/9397",
        "abstract": "Feedforward computation, such as evaluating a neural network or sampling from an autoregressive model, is ubiquitous in machine learning. The sequential nature of feedforward computation, however, requires a strict order of execution and cannot be easily accelerated with parallel computing. To enable parallelization, we frame the task of feedforward computation as solving a system of nonlinear equations. We then propose to find the solution using a Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods of both. Crucially, Jacobi updates operate independently on each equation and can be executed in parallel. Our method is guaranteed to give exactly the same values as the original feedforward computation with a reduced (or equal) number of parallelizable iterations, and hence reduced time given sufficient parallel computing power. Experimentally, we demonstrate the effectiveness of our approach in accelerating (i) backpropagation of RNNs, (ii) evaluation of DenseNets, and (iii) autoregressive sampling of MADE and PixelCNN++, with speedup factors between 2.1 and 26 under various settings.",
        "conference": "ICML",
        "中文标题": "通过并行非线性方程求解加速前馈计算",
        "摘要翻译": "前馈计算，如评估神经网络或从自回归模型中采样，在机器学习中无处不在。然而，前馈计算的顺序性质要求严格的执行顺序，难以通过并行计算轻松加速。为了实现并行化，我们将前馈计算的任务框架化为求解非线性方程组。然后，我们提出使用雅可比或高斯-赛德尔定点迭代方法，以及两者的混合方法来寻找解。关键的是，雅可比更新独立地对每个方程进行操作，可以并行执行。我们的方法保证在减少（或相等）的可并行迭代次数下，给出与原始前馈计算完全相同的值，因此在有足够并行计算能力的情况下减少时间。实验上，我们证明了我们的方法在加速（i）RNN的反向传播，（ii）DenseNets的评估，以及（iii）MADE和PixelCNN++的自回归采样方面的有效性，在各种设置下速度提升因子在2.1到26之间。",
        "领域": "深度学习优化、并行计算、神经网络加速",
        "问题": "如何克服前馈计算的顺序性限制，实现并行加速",
        "动机": "前馈计算的顺序执行限制了其在并行计算环境下的加速潜力，研究旨在通过重新框架化为非线性方程求解问题来实现并行化",
        "方法": "将前馈计算视为非线性方程组求解，采用雅可比或高斯-赛德尔定点迭代方法及其混合方法进行并行求解",
        "关键词": [
            "前馈计算",
            "并行计算",
            "非线性方程求解",
            "定点迭代",
            "神经网络加速"
        ],
        "涉及的技术概念": {
            "雅可比迭代": "一种用于求解非线性方程组的迭代方法，允许独立并行处理每个方程",
            "高斯-赛德尔迭代": "另一种迭代方法，通过顺序更新变量来求解方程组，适合某些类型的并行化",
            "定点迭代": "一种求解方程的方法，通过迭代逼近不动点来找到解，适用于并行化前馈计算"
        },
        "success": true
    },
    {
        "order": 6,
        "title": "Accelerating Gossip SGD with Periodic Global Averaging",
        "html": "https://ICML.cc//virtual/2021/poster/8735",
        "abstract": "Communication overhead hinders the scalability of large-scale distributed training. Gossip SGD, where each node averages only with its neighbors, is more communication-efficient than the prevalent parallel SGD. However, its convergence rate is reversely proportional to quantity $1-\\beta$ which measures the network connectivity. On large and sparse networks where $1-\\beta \\to 0$, Gossip SGD requires more iterations to converge, which offsets against its communication benefit. This paper introduces Gossip-PGA, which adds Periodic Global Averaging to accelerate Gossip SGD. Its transient stage, i.e., the iterations required to reach asymptotic linear speedup stage, improves from $\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex problems. The influence of network topology in Gossip-PGA can be controlled by the averaging period $H$. Its transient-stage complexity is also superior to local SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale training on image classification (ResNet50) and language modeling (BERT) validate our theoretical findings.",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过周期性全局平均加速Gossip SGD",
        "摘要翻译": "通信开销阻碍了大规模分布式训练的可扩展性。Gossip SGD，其中每个节点仅与其邻居平均，比流行的并行SGD更通信高效。然而，其收敛速度与衡量网络连接性的量$1-\\beta$成反比。在大型且稀疏的网络中，其中$1-\\beta \\to 0$，Gossip SGD需要更多的迭代来收敛，这抵消了其通信优势。本文介绍了Gossip-PGA，它通过添加周期性全局平均来加速Gossip SGD。对于非凸问题，其瞬态阶段，即达到渐近线性加速阶段所需的迭代次数，从$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$改进到$\\Omega(\\beta^4 n^3 H^4)$。网络拓扑在Gossip-PGA中的影响可以通过平均周期$H$来控制。其瞬态阶段的复杂性也优于局部SGD，后者具有$\\Omega(n^3 H^4)$的阶数。在图像分类（ResNet50）和语言建模（BERT）上的大规模训练实证结果验证了我们的理论发现。",
        "领域": "分布式深度学习, 优化算法, 大规模训练",
        "问题": "解决Gossip SGD在大型稀疏网络上收敛速度慢的问题",
        "动机": "提高Gossip SGD在大型稀疏网络上的收敛速度，以充分发挥其通信效率的优势",
        "方法": "引入周期性全局平均（PGA）来加速Gossip SGD，通过控制平均周期来优化网络拓扑的影响",
        "关键词": [
            "Gossip SGD",
            "周期性全局平均",
            "分布式训练",
            "收敛速度",
            "网络拓扑"
        ],
        "涉及的技术概念": {
            "Gossip SGD": "一种分布式优化算法，节点仅与邻居交换信息以减少通信开销",
            "周期性全局平均（PGA）": "在Gossip SGD中定期进行全局平均，以加速收敛过程"
        }
    },
    {
        "order": 7,
        "title": "Accelerating Safe Reinforcement Learning with Constraint-mismatched Baseline Policies",
        "html": "https://ICML.cc//virtual/2021/poster/8451",
        "abstract": "We consider the problem of reinforcement learning when provided with (1) a baseline control policy and (2) a set of constraints that the learner must satisfy. The baseline policy can arise from demonstration data or a teacher agent and may provide useful cues for learning, but it might also be sub-optimal for the task at hand, and is not guaranteed to satisfy the specified constraints, which might encode safety, fairness or other application-specific requirements. In order to safely learn from baseline policies, we propose an iterative policy optimization algorithm that alternates between maximizing expected return on the task, minimizing distance to the baseline policy, and projecting the policy onto the constraint-satisfying set. We analyze our algorithm theoretically and provide a finite-time convergence guarantee. In our experiments on five different control tasks, our algorithm consistently outperforms several state-of-the-art baselines, achieving 10 times fewer constraint violations and 40% higher reward on average.",
        "conference": "ICML",
        "中文标题": "加速安全强化学习：利用约束不匹配的基线策略",
        "摘要翻译": "我们考虑在提供（1）一个基线控制策略和（2）学习者必须满足的一组约束条件下的强化学习问题。基线策略可能来源于演示数据或教师代理，可能为学习提供有用的线索，但它也可能对当前任务不是最优的，并且不保证满足指定的约束，这些约束可能编码安全性、公平性或其他应用特定要求。为了安全地从基线策略中学习，我们提出了一种迭代策略优化算法，该算法在最大化任务的预期回报、最小化与基线策略的距离以及将策略投影到满足约束的集合之间交替进行。我们从理论上分析了我们的算法，并提供了有限时间收敛保证。在我们对五种不同控制任务的实验中，我们的算法始终优于几种最先进的基线方法，平均实现了10倍更少的约束违反和40%更高的奖励。",
        "领域": "安全强化学习、策略优化、控制任务",
        "问题": "如何在基线策略不满足特定约束条件下，安全有效地进行强化学习。",
        "动机": "研究动机在于利用基线策略加速学习过程，同时确保学习策略满足安全性、公平性等约束条件。",
        "方法": "提出了一种迭代策略优化算法，交替进行预期回报最大化、与基线策略距离最小化和约束满足集合投影。",
        "关键词": [
            "安全强化学习",
            "基线策略",
            "约束满足",
            "策略优化",
            "控制任务"
        ],
        "涉及的技术概念": {
            "基线控制策略": "来源于演示数据或教师代理的策略，为学习提供初始线索，但可能不满足特定约束。",
            "约束满足集合投影": "将策略调整到满足所有给定约束的集合中的过程，确保学习策略的安全性、公平性等。",
            "迭代策略优化算法": "一种在多个目标间交替优化的算法，旨在平衡任务回报、策略接近度和约束满足。"
        },
        "success": true
    },
    {
        "order": 8,
        "title": "Acceleration via Fractal Learning Rate Schedules",
        "html": "https://ICML.cc//virtual/2021/poster/9173",
        "abstract": "In practical applications of iterative first-order optimization, the learning rate schedule remains notoriously difficult to understand and expensive to tune. We demonstrate the presence of these subtleties even in the innocuous case when the objective is a convex quadratic. We reinterpret an iterative algorithm from the numerical analysis literature as what we call the Chebyshev learning rate schedule for accelerating vanilla gradient descent, and show that the problem of mitigating instability leads to a fractal ordering of step sizes. We provide some experiments to challenge conventional beliefs about stable learning rates in deep learning: the fractal schedule enables training to converge with locally unstable updates which make negative progress on the objective.",
        "conference": "ICML",
        "中文标题": "通过分形学习率调度实现加速",
        "摘要翻译": "在迭代一阶优化的实际应用中，学习率调度仍然难以理解且调优成本高昂。我们展示了即使在目标为凸二次函数这种看似无害的情况下，这些微妙之处依然存在。我们将数值分析文献中的一个迭代算法重新解释为我们所称的切比雪夫学习率调度，用于加速普通梯度下降，并表明缓解不稳定性问题导致了步长的分形排序。我们提供了一些实验来挑战深度学习中对稳定学习率的传统信念：分形调度使得训练能够在局部不稳定的更新下收敛，这些更新在目标上取得了负面进展。",
        "领域": "优化算法、深度学习、梯度下降",
        "问题": "理解和调优迭代一阶优化中的学习率调度问题",
        "动机": "探索更有效的学习率调度方法，以加速梯度下降并解决传统方法中的不稳定性问题",
        "方法": "重新解释数值分析中的迭代算法为切比雪夫学习率调度，并通过实验验证分形调度在深度学习中的应用",
        "关键词": [
            "学习率调度",
            "梯度下降",
            "分形排序",
            "优化算法",
            "深度学习"
        ],
        "涉及的技术概念": {
            "切比雪夫学习率调度": "一种用于加速普通梯度下降的学习率调度方法，源自数值分析中的迭代算法",
            "分形排序": "在缓解优化算法不稳定性问题时出现的步长排序方式，具有分形特性",
            "局部不稳定更新": "在分形学习率调度下，允许训练过程中出现局部不稳定的更新，这些更新可能在短期内对目标产生负面影响，但有助于整体收敛"
        },
        "success": true
    },
    {
        "order": 9,
        "title": "Accumulated Decoupled Learning with Gradient Staleness Mitigation for Convolutional Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9061",
        "abstract": "Gradient staleness is a major side effect in decoupled learning when training convolutional neural networks asynchronously.  Existing methods that ignore this effect might result in reduced generalization and even divergence. In this paper,  we propose an accumulated decoupled learning (ADL), which includes a module-wise gradient accumulation in order to mitigate the gradient staleness. Unlike prior arts ignoring the gradient staleness, we quantify the staleness in such a way that its mitigation can be quantitatively visualized. As a new learning scheme, the proposed ADL is theoretically shown to converge to critical points in spite of its asynchronism. Extensive experiments on CIFAR-10 and ImageNet datasets are conducted, demonstrating that ADL gives promising generalization results while the state-of-the-art methods experience reduced generalization and divergence. In addition, our ADL is shown to have the fastest training speed among the compared methods.",
        "conference": "ICML",
        "中文标题": "累积解耦学习与梯度陈旧性缓解用于卷积神经网络",
        "摘要翻译": "梯度陈旧性是异步训练卷积神经网络时解耦学习的一个主要副作用。忽视这一效应的现有方法可能会导致泛化能力下降甚至发散。本文提出了一种累积解耦学习（ADL），它包括模块级梯度累积以缓解梯度陈旧性。与之前忽视梯度陈旧性的技术不同，我们以可以定量可视化其缓解的方式量化了陈旧性。作为一种新的学习方案，所提出的ADL在理论上被证明尽管存在异步性，仍能收敛到临界点。在CIFAR-10和ImageNet数据集上进行了大量实验，证明ADL提供了有希望的泛化结果，而最先进的方法则经历了泛化能力下降和发散。此外，我们的ADL在比较的方法中显示出最快的训练速度。",
        "领域": "深度学习优化、卷积神经网络、异步学习",
        "问题": "解决异步训练卷积神经网络时梯度陈旧性导致的泛化能力下降和发散问题",
        "动机": "为了缓解异步训练中梯度陈旧性对模型性能的负面影响，提高模型的泛化能力和训练效率",
        "方法": "提出累积解耦学习（ADL），通过模块级梯度累积来定量缓解梯度陈旧性，并在理论上证明其收敛性",
        "关键词": [
            "累积解耦学习",
            "梯度陈旧性",
            "卷积神经网络",
            "异步训练",
            "模块级梯度累积"
        ],
        "涉及的技术概念": {
            "梯度陈旧性": "在异步训练中，由于参数更新延迟导致的梯度信息过时现象，影响模型训练效果",
            "累积解耦学习（ADL）": "一种新的学习方案，通过模块级梯度累积来缓解梯度陈旧性，提高模型泛化能力",
            "模块级梯度累积": "ADL中的关键技术，通过累积模块级别的梯度来减少梯度陈旧性的影响"
        },
        "success": true
    },
    {
        "order": 10,
        "title": "Accuracy, Interpretability, and Differential Privacy via Explainable Boosting",
        "html": "https://ICML.cc//virtual/2021/poster/9575",
        "abstract": "We show that adding differential privacy to Explainable Boosting Machines (EBMs), a recent method for training interpretable ML models, yields state-of-the-art accuracy while protecting privacy. Our experiments on multiple classification and regression datasets show that DP-EBM models suffer surprisingly little accuracy loss even with strong differential privacy guarantees. In addition to high accuracy, two other benefits of applying DP to EBMs are: a) trained models provide exact global and local interpretability, which is often important in settings where differential privacy is needed; and b) the models can be edited after training without loss of privacy to correct errors which DP noise may have introduced.",
        "conference": "ICML",
        "中文标题": "通过可解释提升实现准确性、可解释性与差分隐私",
        "摘要翻译": "我们展示了将差分隐私加入到可解释提升机（EBMs）——一种最近用于训练可解释机器学习模型的方法中，可以在保护隐私的同时达到最先进的准确性。我们在多个分类和回归数据集上的实验表明，DP-EBM模型即使在强差分隐私保证下，准确性的损失也出人意料地小。除了高准确性之外，将DP应用于EBMs还有另外两个好处：a) 训练后的模型提供精确的全局和局部可解释性，这在需要差分隐私的环境中往往很重要；b) 模型可以在训练后进行编辑，以纠正DP噪声可能引入的错误，而不会损失隐私。",
        "领域": "可解释机器学习, 差分隐私, 模型编辑",
        "问题": "如何在保护数据隐私的同时，保持机器学习模型的高准确性和可解释性。",
        "动机": "研究动机是为了解决在需要高隐私保护的应用场景中，如何不牺牲机器学习模型的准确性和可解释性的问题。",
        "方法": "采用的方法是将差分隐私技术整合到可解释提升机（EBMs）中，通过实验验证其在保持高准确性的同时，提供模型的可解释性和隐私保护能力。",
        "关键词": [
            "可解释提升机",
            "差分隐私",
            "模型编辑",
            "机器学习",
            "隐私保护"
        ],
        "涉及的技术概念": {
            "可解释提升机（EBMs）": "一种用于训练可解释机器学习模型的方法，通过提升技术提高模型的准确性和可解释性。",
            "差分隐私（DP）": "一种隐私保护技术，通过在数据或模型输出中添加噪声来保护个体数据不被识别，同时尽量不影响整体数据的实用性。",
            "模型编辑": "指在模型训练完成后，对模型进行调整或修正以改进其性能或纠正错误，而不需要重新训练整个模型。"
        },
        "success": true
    },
    {
        "order": 11,
        "title": "Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/9243",
        "abstract": "For machine learning systems to be reliable, we must understand their performance in unseen, out- of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of- distribution performance on variants of CIFAR- 10 & ImageNet, a synthetic pose estimation task derived from YCB objects, FMoW-WILDS satellite imagery classification, and wildlife classification in iWildCam-WILDS. The correlation holds across model architectures, hyperparameters, training set size, and training duration, and is more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations.",
        "conference": "ICML",
        "中文标题": "准确性的线性关系：论分布外与分布内泛化之间的强相关性",
        "摘要翻译": "为了使机器学习系统可靠，我们必须理解它们在未见过的、分布外环境中的表现。本文中，我们通过实证展示了对于广泛的模型和分布变化，分布外的表现与分布内的表现之间存在强相关性。具体来说，我们在CIFAR-10和ImageNet的变体、源自YCB物体的合成姿态估计任务、FMoW-WILDS卫星图像分类以及iWildCam-WILDS中的野生动物分类上，证明了分布内和分布外表现之间的强相关性。这种相关性跨越了模型架构、超参数、训练集大小和训练持续时间，并且比现有领域适应理论预期的更为精确。为了全面了解，我们还研究了相关性较弱的情况，例如CIFAR-10-C中的一些合成分布变化和组织分类数据集Camelyon17-WILDS。最后，我们基于高斯数据模型提供了一个候选理论，展示了分布变化引起的数据协方差变化如何影响观察到的相关性。",
        "领域": "机器学习泛化性研究、分布偏移适应、模型性能评估",
        "问题": "探索和理解机器学习模型在分布外环境中的性能与分布内性能之间的相关性。",
        "动机": "为了提升机器学习系统在未知环境中的可靠性，需要深入理解模型在分布外和分布内环境中的表现关系。",
        "方法": "通过在不同数据集（如CIFAR-10、ImageNet变体等）和任务上实证分析模型在分布内和分布外的表现，以及基于高斯数据模型的理论分析。",
        "关键词": [
            "分布偏移",
            "泛化性",
            "模型评估",
            "机器学习可靠性",
            "高斯数据模型"
        ],
        "涉及的技术概念": {
            "分布内性能": "模型在与训练数据相同分布的数据上的表现，用于评估模型在已知环境中的有效性。",
            "分布外性能": "模型在与训练数据不同分布的数据上的表现，用于评估模型在未知环境中的泛化能力。",
            "高斯数据模型": "一种理论模型，用于解释数据协方差变化如何影响模型在分布内和分布外数据上的表现相关性。"
        },
        "success": true
    },
    {
        "order": 12,
        "title": "Accurate Post Training Quantization With Small Calibration Sets",
        "html": "https://ICML.cc//virtual/2021/poster/10173",
        "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. We suggest two flavors for our method, parallel and sequential aim for a fixed and flexible bit-width allocation.   For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but first and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code \\textit{https://github.com/papers-submission/CalibTIP}.",
        "conference": "ICML",
        "中文标题": "使用小型校准集进行精确的训练后量化",
        "摘要翻译": "最近，训练后量化方法因其简单易用且仅需少量未标记的校准集而受到广泛关注。这个小数据集在不引起显著过拟合的情况下无法用于微调模型。相反，这些方法仅使用校准集来设置激活的动态范围。然而，当用于8位以下（小型数据集除外）时，这些方法总是导致显著的精度下降。在这里，我们的目标是打破8位的限制。为此，我们通过在校准集上优化其参数，最小化每一层或块的量化误差。我们经验性地证明，这种方法：（1）比标准的微调方法更不容易过拟合，甚至可以在非常小的校准集上使用；（2）比之前仅设置激活动态范围的方法更强大。我们提出了两种方法变体，并行和顺序，分别针对固定和灵活的位宽分配。对于后者，我们通过提出一种新颖的整数规划公式，展示了如何在约束精度下降或模型压缩的情况下，为每一层最优分配位宽。最后，我们建议进行模型全局统计调整，以纠正量化过程中引入的偏差。这些方法共同为视觉和文本模型带来了最先进的结果。例如，在ResNet50上，我们获得了不到1%的精度下降——在所有层（除了第一层和最后一层）中使用4位权重和激活。所建议的方法比用于低于8位量化的传统量化感知训练方法快两个数量级。我们开源了我们的代码https://github.com/papers-submission/CalibTIP。",
        "领域": "模型量化、深度学习优化、计算机视觉",
        "问题": "在8位以下进行训练后量化时精度显著下降的问题",
        "动机": "打破8位量化的限制，实现在小型校准集上的高效量化，减少精度损失",
        "方法": "通过优化每一层或块的参数最小化量化误差，提出并行和顺序两种位宽分配方法，以及模型全局统计调整",
        "关键词": [
            "训练后量化",
            "位宽分配",
            "整数规划",
            "模型量化",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "训练后量化": "在模型训练完成后进行的量化过程，旨在减少模型大小和加速推理，而不需要重新训练",
            "位宽分配": "确定每一层或块使用的位数，以优化模型性能和资源使用",
            "整数规划": "一种数学优化方法，用于在约束条件下找到最优的位宽分配方案"
        },
        "success": true
    },
    {
        "order": 13,
        "title": "ACE: Explaining cluster from an adversarial perspective",
        "html": "https://ICML.cc//virtual/2021/poster/10155",
        "abstract": "A common workflow in single-cell RNA-seq analysis is to project the data to a latent space, cluster the cells in that space, and identify sets of marker genes that explain the differences among the discovered clusters. A primary drawback to this three-step procedure is that each step is carried out independently, thereby neglecting the effects of the nonlinear embedding and inter-gene dependencies on the selection of marker genes. Here we propose an integrated deep learning framework, Adversarial Clustering Explanation (ACE), that bundles all three steps into a single workflow. The method thus moves away from the notion of 'marker genes' to instead identify a panel of explanatory genes. This panel may include genes that are not only enriched but also depleted relative to other cell types, as well as genes that exhibit differences between closely related cell types. Empirically, we demonstrate that ACE is able to identify gene panels that are both highly discriminative and nonredundant, and we demonstrate the applicability of ACE to an image recognition task.",
        "conference": "ICML",
        "中文标题": "ACE：从对抗性视角解释聚类",
        "摘要翻译": "单细胞RNA-seq分析中的一个常见工作流程是将数据投影到一个潜在空间，在该空间中聚类细胞，并识别出能够解释已发现聚类之间差异的标记基因集。这种三步程序的一个主要缺点是每一步都是独立进行的，从而忽略了非线性嵌入和基因间依赖性对标记基因选择的影响。在这里，我们提出了一个集成的深度学习框架——对抗性聚类解释（ACE），它将所有三个步骤捆绑到一个单一的工作流程中。因此，该方法从‘标记基因’的概念转向识别一组解释性基因。这组基因可能不仅包括相对于其他细胞类型富集的基因，还包括相对于其他细胞类型耗竭的基因，以及表现出密切相关细胞类型之间差异的基因。经验上，我们证明了ACE能够识别出既高度区分又非冗余的基因面板，并且我们展示了ACE在图像识别任务中的适用性。",
        "领域": "单细胞RNA-seq分析、深度学习、图像识别",
        "问题": "解决单细胞RNA-seq分析中三步独立工作流程忽略非线性嵌入和基因间依赖性影响的问题",
        "动机": "为了提高单细胞RNA-seq分析的准确性和效率，通过集成深度学习框架将数据投影、聚类和标记基因识别整合为一个连贯的工作流程",
        "方法": "提出了对抗性聚类解释（ACE）框架，将数据投影、聚类和解释性基因识别整合到一个单一的工作流程中，识别出既高度区分又非冗余的基因面板",
        "关键词": [
            "单细胞RNA-seq分析",
            "对抗性聚类解释",
            "深度学习",
            "基因面板",
            "图像识别"
        ],
        "涉及的技术概念": {
            "对抗性聚类解释（ACE）": "一个集成的深度学习框架，用于将单细胞RNA-seq分析中的数据投影、聚类和解释性基因识别整合为一个连贯的工作流程",
            "非线性嵌入": "在单细胞RNA-seq分析中，用于将高维数据投影到低维潜在空间的技术，ACE框架考虑了其对标记基因选择的影响",
            "基因间依赖性": "指基因之间的相互作用和依赖关系，ACE框架在识别解释性基因时考虑了这种依赖性，以提高分析的准确性"
        },
        "success": true
    },
    {
        "order": 14,
        "title": "Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic and Adversarial Linear Bandits Simultaneously",
        "html": "https://ICML.cc//virtual/2021/poster/9819",
        "abstract": "In this work, we develop linear bandit algorithms that automatically adapt to different environments. By plugging a novel loss estimator into the optimization problem that characterizes the instance-optimal strategy, our first algorithm not only achieves nearly instance-optimal regret in stochastic environments, but also works in corrupted environments with additional regret being the amount of corruption, while the state-of-the-art (Li et al., 2019) achieves neither instance-optimality nor the optimal dependence on the corruption amount. Moreover, by equipping this algorithm with an adversarial component and carefully-designed testings, our second algorithm additionally enjoys minimax-optimal regret in completely adversarial environments, which is the first of this kind to our knowledge. Finally, all our guarantees hold with high probability, while existing instance-optimal guarantees only hold in expectation.",
        "conference": "ICML",
        "中文标题": "在随机和对抗性线性赌博机中同时实现接近实例最优和极小极大最优",
        "摘要翻译": "在这项工作中，我们开发了能够自动适应不同环境的线性赌博机算法。通过将一种新颖的损失估计器插入到描述实例最优策略的优化问题中，我们的第一个算法不仅在随机环境中实现了接近实例最优的遗憾，而且在具有额外遗憾（即腐败量）的腐败环境中也能工作，而现有技术（Li等人，2019年）既未实现实例最优性，也未实现对腐败量的最优依赖。此外，通过为该算法配备一个对抗性组件和精心设计的测试，我们的第二个算法在完全对抗性环境中还享有极小极大最优的遗憾，据我们所知，这是此类算法中的第一个。最后，我们所有的保证都以高概率成立，而现有的实例最优保证仅在期望中成立。",
        "领域": "强化学习、在线学习、优化算法",
        "问题": "开发能够在随机和对抗性环境中同时实现接近实例最优和极小极大最优的线性赌博机算法。",
        "动机": "现有技术无法在随机和对抗性环境中同时实现实例最优性和对腐败量的最优依赖，本研究旨在填补这一空白。",
        "方法": "通过引入新颖的损失估计器和对抗性组件，结合精心设计的测试，开发出能够自动适应不同环境的线性赌博机算法。",
        "关键词": [
            "线性赌博机",
            "实例最优性",
            "极小极大最优",
            "随机环境",
            "对抗性环境"
        ],
        "涉及的技术概念": {
            "损失估计器": "用于插入到优化问题中，帮助算法在随机和腐败环境中实现接近实例最优的遗憾。",
            "对抗性组件": "为算法配备的组件，使其在完全对抗性环境中能够实现极小极大最优的遗憾。",
            "高概率保证": "算法性能的保证不仅限于期望值，而是以高概率成立，提高了算法的可靠性。"
        },
        "success": true
    },
    {
        "order": 15,
        "title": "A Collective Learning Framework to Boost GNN Expressiveness for Node Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9337",
        "abstract": "Collective Inference (CI) is a procedure designed to boost weak relational classifiers, specially for node classification tasks. Graph Neural Networks (GNNs) are strong classifiers that have been used with great success. Unfortunately, most existing practical GNNs are not most-expressive (universal). Thus, it is an open question whether one can improve strong relational node classifiers, such as GNNs, with CI.\nIn this work, we investigate this question and propose {\\em collective learning} for GNNs ---a general collective classification approach for node representation learning that increases their representation power. We show that previous attempts to incorporate CI into GNNs fail to boost their expressiveness because they do not adapt CI's Monte Carlo sampling to representation learning.\nWe evaluate our proposed framework with a variety of state-of-the-art GNNs. Our experiments show a consistent, significant boost in node classification accuracy ---regardless of the choice of underlying GNN--- for inductive node classification in partially-labeled graphs, across five real-world network datasets. ",
        "conference": "ICML",
        "中文标题": "提升图神经网络节点分类表达能力的集体学习框架",
        "摘要翻译": "集体推理（CI）是一种旨在增强弱关系分类器性能的程序，特别适用于节点分类任务。图神经网络（GNNs）是强大的分类器，已取得巨大成功。不幸的是，大多数现有的实用GNNs并非最具表达力（通用）。因此，是否可以通过CI改进强大的关系节点分类器（如GNNs）仍是一个开放性问题。在这项工作中，我们探讨了这个问题，并提出了GNNs的集体学习——一种通用的集体分类方法，用于节点表示学习，以提高其表示能力。我们表明，之前尝试将CI融入GNNs未能提升其表达能力，因为它们没有将CI的蒙特卡洛采样适应于表示学习。我们使用多种最先进的GNNs评估了我们提出的框架。我们的实验显示，在五个真实世界的网络数据集中，对于部分标记图中的归纳节点分类，无论选择哪种基础GNN，节点分类准确率都有持续且显著的提升。",
        "领域": "图神经网络、节点分类、集体学习",
        "问题": "如何提升图神经网络在节点分类任务中的表达能力",
        "动机": "现有的图神经网络在节点分类任务中虽表现强大，但并非最具表达力，集体推理（CI）作为一种增强弱关系分类器的方法，其潜力尚未被充分挖掘。",
        "方法": "提出了一种集体学习方法，通过将集体推理（CI）的蒙特卡洛采样适应于表示学习，以提升图神经网络的表达能力。",
        "关键词": [
            "集体学习",
            "图神经网络",
            "节点分类",
            "集体推理",
            "蒙特卡洛采样"
        ],
        "涉及的技术概念": {
            "集体推理（CI）": "一种旨在增强弱关系分类器性能的程序，特别适用于节点分类任务。",
            "图神经网络（GNNs）": "强大的分类器，用于处理图结构数据，已在多种任务中取得巨大成功。",
            "蒙特卡洛采样": "一种统计方法，用于近似计算，本文中用于适应集体推理到表示学习中。"
        },
        "success": true
    },
    {
        "order": 16,
        "title": "Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills",
        "html": "https://ICML.cc//virtual/2021/poster/9743",
        "abstract": "We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting that is becoming increasingly important for scaling robot learning by reusing past robotic data. In particular, we propose the objective of learning a functional understanding of the environment by learning to reach any goal state in a given dataset. We employ goal-conditioned Q-learning with hindsight relabeling and develop several techniques that enable training in a particularly challenging offline setting. We find that our method can operate on high-dimensional camera images and learn a variety of skills on real robots that generalize to previously unseen scenes and objects. We also show that our method can learn to reach long-horizon goals across multiple episodes through goal chaining, and learn rich representations that can help with downstream tasks through pre-training or auxiliary objectives.",
        "conference": "ICML",
        "中文标题": "可操作模型：机器人技能的无监督离线强化学习",
        "摘要翻译": "我们考虑从先前收集的离线数据中学习有用的机器人技能的问题，而无需访问手动指定的奖励或额外的在线探索，这一设置对于通过重用过去的机器人数据来扩展机器人学习变得越来越重要。特别是，我们提出了通过学会到达给定数据集中的任何目标状态来学习对环境的功能性理解的目标。我们采用带有后见之明重新标记的目标条件Q学习，并开发了几种技术，使得在特别具有挑战性的离线设置中进行训练成为可能。我们发现，我们的方法可以操作高维相机图像，并在真实机器人上学习各种技能，这些技能能够泛化到以前未见过的场景和物体。我们还表明，我们的方法可以通过目标链学会跨多个情节达到长期目标，并通过预训练或辅助目标学习有助于下游任务的丰富表示。",
        "领域": "机器人学习、强化学习、计算机视觉",
        "问题": "如何从离线数据中无监督地学习有用的机器人技能，而无需手动指定的奖励或额外的在线探索。",
        "动机": "通过重用过去的机器人数据来扩展机器人学习，减少对人工干预的依赖。",
        "方法": "采用目标条件Q学习与后见之明重新标记技术，开发适用于高维相机图像和真实机器人技能学习的技术。",
        "关键词": [
            "无监督学习",
            "离线强化学习",
            "机器人技能",
            "目标条件Q学习",
            "后见之明重新标记"
        ],
        "涉及的技术概念": {
            "目标条件Q学习": "在强化学习中，通过学习到达特定目标状态的策略来指导机器人行为。",
            "后见之明重新标记": "通过重新标记经验回放中的目标，提高学习效率和泛化能力。",
            "高维相机图像处理": "处理和分析来自高维相机输入的数据，以支持复杂的视觉任务和决策。"
        },
        "success": true
    },
    {
        "order": 17,
        "title": "Active Covering",
        "html": "https://ICML.cc//virtual/2021/poster/10019",
        "abstract": "We analyze the problem of active covering, where the learner is given an unlabeled dataset and can sequentially label query examples. The objective is to label query all of the positive examples in the fewest number of total label queries. We show under standard non-parametric assumptions that a classical support estimator can be repurposed as an offline algorithm attaining an excess query cost of $\\widetilde{\\Theta}(n^{D/(D+1)})$ compared to the optimal learner, where $n$ is the number of datapoints and $D$ is the dimension. We then provide a simple active learning method that attains an improved excess query cost of $\\widetilde{O}(n^{(D-1)/D})$. Furthermore, the proposed algorithms only require access to the positive labeled examples, which in certain settings provides additional computational and privacy benefits. Finally, we show that the active learning method consistently outperforms offline methods as well as a variety of baselines on a wide range of benchmark image-based datasets.",
        "conference": "ICML",
        "success": true,
        "中文标题": "主动覆盖",
        "摘要翻译": "我们分析了主动覆盖问题，其中学习者被给予一个未标记的数据集，并且可以依次标记查询示例。目标是用最少的总标签查询数来标记所有正例。我们证明，在标准的非参数假设下，与最优学习器相比，一个经典的支持估计器可以被重新用作离线算法，获得 $\\\\widetilde{\\\\Theta}(n^{D/(D+1)})$ 的额外查询成本，其中 $n$ 是数据点的数量，$D$ 是维度。然后，我们提供了一种简单的主动学习方法，该方法可以获得 $\\\\widetilde{O}(n^{(D-1)/D})$ 的改进的额外查询成本。此外，所提出的算法只需要访问正标记的例子，这在某些设置中提供了额外的计算和隐私好处。最后，我们表明，在各种基于图像的基准数据集上，主动学习方法始终优于离线方法以及各种基线方法。",
        "领域": "主动学习, 图像分类, 数据挖掘",
        "问题": "如何通过主动查询标记数据，以最少的查询次数覆盖所有正例。",
        "动机": "在未标记数据集中，有效识别和标记所有正例需要付出高昂的代价，因此研究如何降低标记成本，提高标记效率具有重要意义。",
        "方法": "提出了一种基于支持估计器的主动学习方法，并与离线算法和多种基线方法在图像数据集上进行了比较。",
        "关键词": [
            "主动学习",
            "支持估计器",
            "标签查询",
            "图像分类",
            "非参数假设"
        ],
        "涉及的技术概念": {
            "主动学习": "通过主动选择信息量最大的样本进行标记，从而减少标注工作量，提高学习效率。",
            "支持估计器": "用于估计数据分布的支持集，从而辅助主动学习过程，指导样本选择。"
        }
    },
    {
        "order": 18,
        "title": "Active Deep Probabilistic Subsampling",
        "html": "https://ICML.cc//virtual/2021/poster/10593",
        "abstract": "Subsampling a signal of interest can reduce costly data transfer, battery drain, radiation exposure and acquisition time in a wide range of problems. The recently proposed Deep Probabilistic Subsampling (DPS) method effectively integrates subsampling in an end-to-end deep learning model, but learns a static pattern for all datapoints. We generalize DPS to a sequential method that actively picks the next sample based on the information acquired so far; dubbed Active-DPS (A-DPS). We validate that A-DPS  improves over DPS for MNIST classification at high subsampling rates. Moreover, we demonstrate strong performance in active acquisition Magnetic Resonance Image (MRI) reconstruction, outperforming DPS and other deep learning methods.",
        "conference": "ICML",
        "中文标题": "主动深度概率子采样",
        "摘要翻译": "对感兴趣信号进行子采样可以在广泛的问题中减少昂贵的数据传输、电池消耗、辐射暴露和采集时间。最近提出的深度概率子采样（DPS）方法有效地将子采样集成到一个端到端的深度学习模型中，但为所有数据点学习了一个静态模式。我们将DPS推广为一个顺序方法，该方法基于迄今为止获取的信息主动选择下一个样本；称为主动-DPS（A-DPS）。我们验证了A-DPS在高子采样率下对MNIST分类的改进优于DPS。此外，我们在主动采集磁共振图像（MRI）重建中展示了强大的性能，优于DPS和其他深度学习方法。",
        "领域": "图像重建, 信号处理, 深度学习",
        "问题": "如何在减少数据采集量的同时，保持或提高信号重建的质量和效率",
        "动机": "为了解决在信号处理中减少数据采集量带来的各种成本问题，同时提高重建质量",
        "方法": "推广深度概率子采样（DPS）方法为主动-DPS（A-DPS），基于已获取信息动态选择下一个样本",
        "关键词": [
            "主动学习",
            "信号子采样",
            "磁共振图像重建",
            "深度学习",
            "概率模型"
        ],
        "涉及的技术概念": {
            "深度概率子采样（DPS）": "一种将子采样集成到端到端深度学习模型中的方法，学习静态子采样模式",
            "主动-DPS（A-DPS）": "基于已获取信息动态选择下一个样本的顺序方法，改进DPS在高子采样率下的性能",
            "磁共振图像（MRI）重建": "应用A-DPS方法在减少采集量的同时，提高图像重建质量的技术"
        },
        "success": true
    },
    {
        "order": 19,
        "title": "Active Feature Acquisition with Generative Surrogate Models",
        "html": "https://ICML.cc//virtual/2021/poster/10533",
        "abstract": "Many real-world situations allow for the acquisition of additional relevant information when making an assessment with limited or uncertain data. However, traditional ML approaches either require all features to be acquired beforehand or regard part of them as missing data that cannot be acquired. In this work, we consider models that perform active feature acquisition (AFA) and query the environment for unobserved features to improve the prediction assessments at evaluation time. Our work reformulates the Markov decision process (MDP) that underlies the AFA problem as a generative modeling task and optimizes a policy via a novel model-based approach. We propose learning a generative surrogate model (GSM) that captures the dependencies among input features to assess potential information gain from acquisitions. The GSM is leveraged to provide intermediate rewards and auxiliary information to aid the agent navigate a complicated high-dimensional action space and sparse rewards. Furthermore, we extend AFA in a task we coin active instance recognition (AIR) for the unsupervised case where the target variables are the unobserved features themselves and the goal is to collect information for a particular instance in a cost-efficient way. Empirical results demonstrate that our approach achieves considerably better performance than previous state of the art methods on both supervised and unsupervised tasks.",
        "conference": "ICML",
        "中文标题": "基于生成替代模型的主动特征获取",
        "摘要翻译": "在许多现实世界的情况下，当使用有限或不确定的数据进行评估时，可以获取额外的相关信息。然而，传统的机器学习方法要么需要事先获取所有特征，要么将部分特征视为无法获取的缺失数据。在这项工作中，我们考虑了执行主动特征获取（AFA）的模型，并在评估时查询环境以获取未观察到的特征，以改进预测评估。我们的工作将AFA问题背后的马尔可夫决策过程（MDP）重新表述为一个生成建模任务，并通过一种新颖的基于模型的方法优化策略。我们提出学习一个生成替代模型（GSM），该模型捕获输入特征之间的依赖关系，以评估从获取中获得的潜在信息增益。GSM被用来提供中间奖励和辅助信息，以帮助代理在复杂的高维动作空间和稀疏奖励中导航。此外，我们将AFA扩展到一个我们称之为主动实例识别（AIR）的任务中，用于无监督的情况，其中目标变量是未观察到的特征本身，目标是以成本效益的方式为特定实例收集信息。实证结果表明，我们的方法在监督和无监督任务上都比之前的最先进方法实现了显著更好的性能。",
        "领域": "主动学习, 特征选择, 无监督学习",
        "问题": "解决在有限或不确定数据下如何主动获取额外相关信息以提高预测评估的问题",
        "动机": "传统机器学习方法在特征获取上的局限性促使研究如何动态获取未观察到的特征以改进预测",
        "方法": "通过将主动特征获取问题重新表述为生成建模任务，并利用生成替代模型（GSM）来评估信息增益和优化策略",
        "关键词": [
            "主动特征获取",
            "生成替代模型",
            "马尔可夫决策过程",
            "主动实例识别",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "主动特征获取（AFA）": "在评估时动态查询环境以获取未观察到的特征，以改进预测评估的技术",
            "生成替代模型（GSM）": "捕获输入特征间依赖关系的模型，用于评估从特征获取中获得的潜在信息增益",
            "马尔可夫决策过程（MDP）": "用于建模AFA问题的决策过程框架，通过策略优化来实现特征的有效获取"
        },
        "success": true
    },
    {
        "order": 20,
        "title": "Active Learning for Distributionally Robust Level-Set Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/8729",
        "abstract": "Many cases exist in which a black-box function $f$ with high evaluation cost depends on two types of variables $\\bm x$ and $\\bm w$, where $\\bm x$ is a controllable \\emph{design} variable and $\\bm w$ are uncontrollable \\emph{environmental} variables that have random variation following a certain distribution $P$. In such cases, an important task is to find the range of design variables $\\bm x$ such that the function $f(\\bm x, \\bm w)$ has the desired properties by incorporating the random variation of the environmental variables $\\bm w$. A natural measure of robustness is the probability that $f(\\bm x, \\bm w)$ exceeds a given threshold $h$, which is known as the \\emph{probability threshold robustness} (PTR) measure in the literature on robust optimization. However, this robustness measure cannot be correctly evaluated when the distribution $P$ is unknown. In this study, we addressed this problem by considering the \\textit{distributionally robust PTR} (DRPTR) measure, which considers the worst-case PTR within given candidate distributions. Specifically, we studied the problem of efficiently identifying a reliable set $H$, which is defined as a region in which the DRPTR measure exceeds a certain desired probability $\\alpha$, which can be interpreted as a level set estimation (LSE) problem for DRPTR. We propose a theoretically grounded and computationally efficient active learning method for this problem. We show that the proposed method has theoretical guarantees on convergence and accuracy, and confirmed through numerical experiments that the proposed method outperforms existing methods.",
        "conference": "ICML",
        "success": true,
        "中文标题": "分布鲁棒水平集估计的主动学习",
        "摘要翻译": "在许多情况下，一个具有高评估成本的黑箱函数f依赖于两种类型的变量x和w，其中x是可控制的设计变量，而w是不可控制的环境变量，这些环境变量遵循某种分布P具有随机变化。在这种情况下，一个重要任务是找到设计变量x的范围，使得通过纳入环境变量w的随机变化，函数f(x, w)具有所需的属性。一个自然的鲁棒性度量是f(x, w)超过给定阈值h的概率，这在鲁棒优化文献中被称为概率阈值鲁棒性(PTR)度量。然而，当分布P未知时，这个鲁棒性度量无法正确评估。在本研究中，我们通过考虑分布鲁棒PTR(DRPTR)度量来解决这个问题，该度量考虑了给定候选分布中最坏情况的PTR。具体来说，我们研究了高效识别可靠集H的问题，H被定义为DRPTR度量超过某个期望概率α的区域，这可以解释为DRPTR的水平集估计(LSE)问题。我们为这个问题提出了一种理论基础扎实且计算效率高的主动学习方法。我们展示了所提出的方法在收敛性和准确性上具有理论保证，并通过数值实验证实了所提出的方法优于现有方法。",
        "领域": "鲁棒优化, 主动学习, 水平集估计",
        "问题": "在分布P未知的情况下，如何正确评估和优化概率阈值鲁棒性(PTR)度量。",
        "动机": "为了解决在环境变量分布未知时，评估和优化设计变量的鲁棒性问题。",
        "方法": "提出了一种理论基础扎实且计算效率高的主动学习方法，用于分布鲁棒概率阈值鲁棒性(DRPTR)的水平集估计。",
        "关键词": [
            "分布鲁棒优化",
            "主动学习",
            "水平集估计",
            "概率阈值鲁棒性",
            "黑箱函数"
        ],
        "涉及的技术概念": {
            "分布鲁棒PTR(DRPTR)": "考虑了给定候选分布中最坏情况的PTR，用于在分布未知时评估鲁棒性。",
            "水平集估计(LSE)": "用于识别DRPTR度量超过某个期望概率α的区域，即可靠集H。",
            "主动学习": "一种计算效率高的学习方法，用于在有限的评估次数内高效地识别可靠集H。"
        }
    },
    {
        "order": 21,
        "title": "Active Learning of Continuous-time Bayesian Networks through Interventions",
        "html": "https://ICML.cc//virtual/2021/poster/9647",
        "abstract": "We consider the problem of learning structures and parameters of Continuous-time Bayesian Networks (CTBNs) from time-course data under minimal experimental resources. In practice, the cost of generating experimental data poses a bottleneck, especially in the natural and social sciences. A popular approach to overcome this is Bayesian optimal experimental design (BOED). However, BOED becomes infeasible in high-dimensional settings, as it involves integration over all possible experimental outcomes. We propose a novel criterion for experimental design based on a variational approximation of the expected information gain. We show that for CTBNs, a semi-analytical expression for this criterion can be calculated for structure and parameter learning. By doing so, we can replace sampling over experimental outcomes by solving the CTBNs master-equation, for which scalable approximations exist. This alleviates the computational burden of sampling possible experimental outcomes in high-dimensions. We employ this framework to recommend interventional sequences. In this context, we extend the CTBN model to conditional CTBNs to incorporate interventions. We demonstrate the performance of our criterion on synthetic and real-world data.",
        "conference": "ICML",
        "中文标题": "通过干预主动学习连续时间贝叶斯网络",
        "摘要翻译": "我们考虑在最小化实验资源的条件下，从时间序列数据中学习连续时间贝叶斯网络（CTBNs）的结构和参数的问题。在实践中，生成实验数据的成本构成了一个瓶颈，尤其是在自然科学和社会科学领域。克服这一问题的流行方法是贝叶斯最优实验设计（BOED）。然而，在高维设置下，BOED变得不可行，因为它涉及对所有可能的实验结果进行积分。我们提出了一种基于期望信息增益的变分近似的新实验设计准则。我们证明，对于CTBNs，可以为结构和参数学习计算这一准则的半解析表达式。通过这样做，我们可以通过解决CTBNs的主方程来替代对实验结果的采样，而主方程存在可扩展的近似方法。这减轻了在高维情况下采样可能实验结果的计算负担。我们采用这一框架来推荐干预序列。在此背景下，我们将CTBN模型扩展到条件CTBNs以纳入干预。我们在合成和真实世界数据上展示了我们准则的性能。",
        "领域": "贝叶斯网络学习, 实验设计优化, 高维数据分析",
        "问题": "在高维设置下，如何有效地从时间序列数据中学习连续时间贝叶斯网络的结构和参数，同时最小化实验资源的使用。",
        "动机": "解决在高维情况下，由于实验数据生成成本高和贝叶斯最优实验设计（BOED）计算不可行的问题，提出一种新的实验设计准则以减少计算负担。",
        "方法": "提出了一种基于期望信息增益的变分近似的新实验设计准则，通过解决CTBNs的主方程来替代对实验结果的采样，并扩展CTBN模型到条件CTBNs以纳入干预。",
        "关键词": [
            "连续时间贝叶斯网络",
            "实验设计",
            "变分近似",
            "高维数据",
            "干预序列"
        ],
        "涉及的技术概念": {
            "连续时间贝叶斯网络（CTBNs）": "用于建模连续时间下随机变量之间的概率依赖关系，本文中用于结构和参数学习。",
            "贝叶斯最优实验设计（BOED）": "一种实验设计方法，旨在通过贝叶斯推理优化实验设计，但在高维情况下计算不可行。",
            "变分近似": "用于近似计算期望信息增益的技术，本文中用于提出新的实验设计准则，以减少计算负担。"
        },
        "success": true
    },
    {
        "order": 22,
        "title": "Active Slices for Sliced Stein Discrepancy",
        "html": "https://ICML.cc//virtual/2021/poster/10539",
        "abstract": "Sliced Stein discrepancy (SSD) and its kernelized variants have demonstrated promising successes in goodness-of-fit tests and model learning in high dimensions. Despite the theoretical elegance, their empirical performance depends crucially on the search of the optimal slicing directions to discriminate between two distributions. Unfortunately, previous gradient-based optimisation approach returns sub-optimal results for the slicing directions: it is computationally expensive, sensitive to initialization, and it lacks theoretical guarantee for convergence. We address these issues in two steps. First, we show in theory that the requirement of using optimal slicing directions in the kernelized version of SSD can be relaxed, validating the resulting discrepancy with finite random slicing directions. Second, given that good slicing directions are crucial for practical performance, we propose a fast algorithm for finding good slicing directions based on ideas of active sub-space construction and spectral decomposition. Experiments in goodness-of-fit tests and model learning show that our approach achieves both the best performance and the fastest convergence. Especially, we demonstrate 14-80x speed-up in goodness-of-fit tests when compared with the gradient-based approach. \n",
        "conference": "ICML",
        "中文标题": "用于切片斯坦差异的主动切片方法",
        "摘要翻译": "切片斯坦差异（SSD）及其核化变体在高维度的拟合优度测试和模型学习中已显示出令人鼓舞的成功。尽管理论优雅，它们的实证性能关键取决于寻找最优切片方向以区分两种分布。不幸的是，先前基于梯度的优化方法对于切片方向的返回结果是次优的：它计算成本高，对初始化敏感，并且缺乏收敛的理论保证。我们通过两个步骤解决这些问题。首先，我们在理论上证明了在核化版本的SSD中使用最优切片方向的要求可以放宽，验证了使用有限随机切片方向产生的差异。其次，鉴于良好的切片方向对实际性能至关重要，我们提出了一种基于主动子空间构建和谱分解思想的快速算法来寻找良好的切片方向。在拟合优度测试和模型学习中的实验表明，我们的方法既实现了最佳性能，又实现了最快的收敛速度。特别是，与基于梯度的方法相比，我们在拟合优度测试中展示了14-80倍的速度提升。",
        "领域": "统计机器学习, 高维数据分析, 拟合优度测试",
        "问题": "如何高效且有效地寻找最优切片方向以提升切片斯坦差异在拟合优度测试和模型学习中的性能",
        "动机": "解决基于梯度的优化方法在寻找最优切片方向时计算成本高、对初始化敏感及缺乏收敛理论保证的问题",
        "方法": "提出了一种基于主动子空间构建和谱分解思想的快速算法来寻找良好的切片方向，并在理论上放宽了对最优切片方向的要求",
        "关键词": [
            "切片斯坦差异",
            "主动子空间",
            "谱分解",
            "拟合优度测试",
            "模型学习"
        ],
        "涉及的技术概念": {
            "切片斯坦差异": "用于衡量两个概率分布之间差异的度量，特别适用于高维数据",
            "主动子空间构建": "一种降维技术，用于识别数据中变化最大的方向，以优化切片方向的选择",
            "谱分解": "一种数学方法，用于分解矩阵以识别其主要成分，在本研究中用于高效寻找切片方向"
        },
        "success": true
    },
    {
        "order": 23,
        "title": "Active Testing: Sample-Efficient Model Evaluation",
        "html": "https://ICML.cc//virtual/2021/poster/9293",
        "abstract": "We introduce a new framework for sample-efficient model evaluation that we call active testing. While approaches like active learning reduce the number of labels needed for model training, existing literature largely ignores the cost of labeling test data, typically unrealistically assuming large test sets for model evaluation. This creates a disconnect to real applications, where test labels are important and just as expensive, e.g. for optimizing hyperparameters. Active testing addresses this by carefully selecting the test points to label, ensuring model evaluation is sample-efficient. To this end, we derive theoretically-grounded and intuitive acquisition strategies that are specifically tailored to the goals of active testing, noting these are distinct to those of active learning. As actively selecting labels introduces a bias; we further show how to remove this bias while reducing the variance of the estimator at the same time. Active testing is easy to implement and can be applied to any supervised machine learning method. We demonstrate its effectiveness on models including WideResNets and Gaussian processes on datasets including Fashion-MNIST and CIFAR-100.",
        "conference": "ICML",
        "中文标题": "主动测试：样本高效的模型评估",
        "摘要翻译": "我们引入了一种新的样本高效模型评估框架，称之为主动测试。虽然像主动学习这样的方法减少了模型训练所需的标签数量，但现有文献在很大程度上忽略了标注测试数据的成本，通常不切实际地假设有大量测试集用于模型评估。这与实际应用存在脱节，在实际应用中，测试标签同样重要且成本高昂，例如用于优化超参数。主动测试通过精心选择需要标注的测试点来解决这一问题，确保模型评估是样本高效的。为此，我们推导了理论基础和直观的获取策略，这些策略专门针对主动测试的目标定制，注意到这些目标与主动学习的目标不同。由于主动选择标签会引入偏差；我们进一步展示了如何在减少估计器方差的同时消除这种偏差。主动测试易于实现，可以应用于任何监督机器学习方法。我们在包括Fashion-MNIST和CIFAR-100的数据集上，对包括WideResNets和高斯过程在内的模型展示了其有效性。",
        "领域": "模型评估优化、超参数优化、监督学习",
        "问题": "减少模型评估过程中所需的测试样本数量，同时保持评估的准确性和效率。",
        "动机": "在实际应用中，获取大量测试标签既昂贵又不切实际，特别是在需要频繁评估模型性能的场景下，如超参数优化。主动测试旨在通过智能选择测试样本来降低评估成本。",
        "方法": "提出主动测试框架，通过精心选择测试点和消除选择偏差的方法，实现样本高效的模型评估。",
        "关键词": [
            "主动测试",
            "样本高效",
            "模型评估",
            "偏差消除",
            "监督学习"
        ],
        "涉及的技术概念": {
            "主动测试": "一种样本高效的模型评估框架，通过智能选择测试点减少评估所需的标签数量。",
            "偏差消除": "在主动选择测试标签时引入的技术，用于消除由此产生的偏差，同时减少估计器的方差。",
            "监督学习": "主动测试可以应用于任何监督学习方法，通过利用有限的标签数据高效评估模型性能。"
        },
        "success": true
    },
    {
        "order": 24,
        "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training",
        "html": "https://ICML.cc//virtual/2021/poster/9443",
        "abstract": "The increasing size of neural network models has been critical for improvements in their accuracy, but device memory is not growing at the same rate. This creates fundamental challenges for training neural networks within limited memory environments. In this work, we propose ActNN, a memory-efficient training framework that stores randomly quantized activations for back propagation. We prove the convergence of ActNN for general network architectures, and we characterize the impact of quantization on the convergence via an exact expression for the gradient variance. Using our theory, we propose novel mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers. These techniques can be readily applied to existing dynamic graph frameworks, such as PyTorch, simply by substituting the layers. We evaluate ActNN on mainstream computer vision models for classification, detection, and segmentation tasks. On all these tasks, ActNN compresses the activation to 2 bits on average, with negligible accuracy loss. ActNN reduces the memory footprint of the activation by 12x, and it enables training with a 6.6x to 14x larger batch size.",
        "conference": "ICML",
        "中文标题": "ActNN：通过2位激活压缩训练减少训练内存占用",
        "摘要翻译": "神经网络模型规模的增加对其准确性的提升至关重要，但设备内存的增长速度并未与之同步。这在有限内存环境中训练神经网络带来了根本性挑战。在本工作中，我们提出了ActNN，一个内存高效的训练框架，它存储随机量化的激活以用于反向传播。我们证明了ActNN对于一般网络架构的收敛性，并通过梯度方差的精确表达式量化了量化对收敛的影响。利用我们的理论，我们提出了新颖的混合精度量化策略，这些策略利用了激活在特征维度、样本和层之间的异质性。这些技术可以简单地通过替换层来应用于现有的动态图框架，如PyTorch。我们在主流的计算机视觉模型上评估ActNN，用于分类、检测和分割任务。在所有这些任务上，ActNN平均将激活压缩到2位，准确率损失可忽略不计。ActNN将激活的内存占用减少了12倍，并且能够训练6.6倍到14倍更大的批量大小。",
        "领域": "深度学习优化、计算机视觉、模型压缩",
        "问题": "在有限内存环境中高效训练大规模神经网络模型",
        "动机": "解决设备内存增长速度无法跟上神经网络模型规模增长带来的训练内存挑战",
        "方法": "提出ActNN框架，通过随机量化激活存储和混合精度量化策略减少内存占用",
        "关键词": [
            "激活量化",
            "内存高效训练",
            "混合精度",
            "模型压缩",
            "大规模训练"
        ],
        "涉及的技术概念": {
            "随机量化激活": "在反向传播过程中存储随机量化的激活以减少内存占用",
            "混合精度量化策略": "根据激活在特征维度、样本和层之间的异质性，采用不同精度的量化策略",
            "梯度方差分析": "通过精确表达梯度方差，量化量化策略对模型收敛性的影响"
        },
        "success": true
    },
    {
        "order": 25,
        "title": "Adapting to Delays and Data in Adversarial Multi-Armed Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9277",
        "abstract": "We consider the adversarial multi-armed bandit problem under delayed feedback. We analyze variants of the Exp3 algorithm that tune their step size using only information (about the losses and delays) available at the time of the decisions, and obtain regret guarantees that adapt to the observed (rather than the worst-case) sequences of delays and/or losses. First, through a remarkably simple proof technique, we show that with proper tuning of the step size, the algorithm achieves an optimal (up to logarithmic factors) regret of order $\\sqrt{\\log(K)(TK + D)}$ both in expectation and in high probability, where $K$ is the number of arms, $T$ is the time horizon, and $D$ is the cumulative delay. The high-probability version of the bound, which is the first high-probability delay-adaptive bound in the literature, crucially depends on the use of implicit exploration in estimating the losses. Then, following Zimmert and Seldin (2019), we extend these results so that the algorithm can ``skip'' rounds with large delays, resulting in regret bounds of order $\\sqrt{TK\\log(K)} + |R| + \\sqrt{D_{\\bar{R}}\\log(K)}$, where $R$ is an arbitrary set of rounds (which are skipped) and $D_{\\bar{R}}$ is the cumulative delay of the feedback for other rounds. Finally, we present another, data-adaptive (AdaGrad-style) version of the algorithm for which the regret adapts to the observed (delayed) losses instead of only adapting to the cumulative delay (this algorithm requires an a priori upper bound on the maximum delay, or the advance knowledge of the delay for each decision when it is made). The resulting bound can be orders of magnitude smaller on benign problems, and it can be shown that the delay only affects the regret through the loss of the best arm.",
        "conference": "ICML",
        "中文标题": "适应延迟和数据的对抗性多臂老虎机问题",
        "摘要翻译": "我们考虑了在延迟反馈下的对抗性多臂老虎机问题。我们分析了Exp3算法的变体，这些变体仅使用决策时可用的信息（关于损失和延迟）来调整其步长，并获得了适应观察到的（而非最坏情况下的）延迟和/或损失序列的遗憾保证。首先，通过一个非常简单的证明技术，我们展示了通过适当调整步长，算法在期望和高概率下都能达到最优（对数因子以内）的遗憾阶数√(log(K)(TK + D))，其中K是臂的数量，T是时间范围，D是累积延迟。该界限的高概率版本，是文献中第一个高概率延迟自适应界限，关键依赖于在估计损失时使用隐式探索。然后，遵循Zimmert和Seldin（2019）的方法，我们扩展了这些结果，使得算法可以“跳过”具有大延迟的轮次，导致遗憾界限为√(TKlog(K)) + |R| + √(D_R̅log(K))，其中R是任意一组被跳过的轮次，D_R̅是其他轮次反馈的累积延迟。最后，我们提出了算法的另一个数据自适应（AdaGrad风格）版本，其遗憾适应于观察到的（延迟的）损失，而不仅仅是适应于累积延迟（该算法需要最大延迟的先验上限，或每次决策时延迟的预先知识）。在良性问题上，由此产生的界限可以小几个数量级，并且可以证明延迟仅通过最佳臂的损失影响遗憾。",
        "领域": "对抗性学习, 在线学习, 强化学习",
        "问题": "在延迟反馈条件下优化对抗性多臂老虎机问题的决策策略",
        "动机": "研究在存在延迟反馈的情况下，如何调整算法以最小化对抗性多臂老虎机问题中的遗憾",
        "方法": "通过调整Exp3算法的步长和使用隐式探索技术，以及引入跳过机制和数据自适应方法，来适应延迟和损失",
        "关键词": [
            "对抗性多臂老虎机",
            "延迟反馈",
            "Exp3算法",
            "数据自适应",
            "遗憾最小化"
        ],
        "涉及的技术概念": {
            "Exp3算法": "一种用于对抗性多臂老虎机问题的算法，通过调整步长来适应延迟和损失",
            "隐式探索": "在估计损失时使用的一种技术，用于在高概率下获得延迟自适应的遗憾界限",
            "数据自适应": "一种算法调整策略，使得遗憾能够适应观察到的延迟损失，而不仅仅是累积延迟"
        },
        "success": true
    },
    {
        "order": 26,
        "title": "Adapting to misspecification in contextual bandits with offline regression oracles",
        "html": "https://ICML.cc//virtual/2021/poster/10169",
        "abstract": "Computationally efficient contextual bandits are often based on estimating a predictive model of rewards given contexts and arms using past data.  However, when the reward model is not well-specified, the bandit algorithm may incur unexpected regret, so recent work has focused on algorithms that are robust to misspecification. We propose a simple family of contextual bandit algorithms that adapt to misspecification error by reverting to a good safe policy when there is evidence that misspecification is causing a regret increase. Our algorithm requires only an offline regression oracle to ensure regret guarantees that gracefully degrade in terms of a measure of the average misspecification level. Compared to prior work, we attain similar regret guarantees, but we do no rely on a master algorithm, and do not require more robust oracles like online or constrained regression oracles  (e.g., Foster et al. (2020), Krishnamurthy et al. (2020)). This allows us to design algorithms for more general function approximation classes.",
        "conference": "ICML",
        "中文标题": "适应上下文赌博机中模型误设的离线回归预言机方法",
        "摘要翻译": "计算效率高的上下文赌博机通常基于使用过去数据估计给定上下文和臂的奖励预测模型。然而，当奖励模型设定不当时，赌博机算法可能会产生意外的遗憾，因此最近的研究集中在能够对模型误设具有鲁棒性的算法上。我们提出了一类简单的上下文赌博机算法，这些算法通过在有证据表明模型误设导致遗憾增加时回归到一个良好的安全策略，来适应模型误设错误。我们的算法仅需要一个离线回归预言机来确保遗憾保证，这些保证在平均模型误设水平的度量上优雅地降低。与之前的工作相比，我们获得了类似的遗憾保证，但我们不依赖于主算法，也不需要更鲁棒的预言机，如在线或约束回归预言机（例如，Foster等人（2020），Krishnamurthy等人（2020））。这使得我们能够为更一般的函数近似类设计算法。",
        "领域": "强化学习、在线学习、机器学习理论",
        "问题": "解决在奖励模型设定不当时，上下文赌博机算法可能产生的意外遗憾问题。",
        "动机": "研究动机是为了开发能够适应模型误设并减少由此产生的遗憾的上下文赌博机算法。",
        "方法": "提出了一类简单的上下文赌博机算法，这些算法在有证据表明模型误设导致遗憾增加时，回归到一个良好的安全策略。",
        "关键词": [
            "上下文赌博机",
            "模型误设",
            "离线回归预言机",
            "遗憾保证",
            "函数近似"
        ],
        "涉及的技术概念": {
            "上下文赌博机": "一种在给定上下文信息的情况下选择行动以最大化累积奖励的强化学习方法。",
            "模型误设": "指奖励模型未能准确反映真实奖励生成过程的情况。",
            "离线回归预言机": "一种能够在不需要在线交互的情况下，基于历史数据预测奖励的算法。"
        },
        "success": true
    },
    {
        "order": 27,
        "title": "Adaptive Newton Sketch: Linear-time Optimization with Quadratic Convergence and Effective Hessian Dimensionality",
        "html": "https://ICML.cc//virtual/2021/poster/9571",
        "abstract": "We propose a randomized algorithm with quadratic convergence rate for convex optimization problems with a self-concordant, composite, strongly convex objective function. Our method is based on performing an approximate Newton step using a random projection of the Hessian. Our first contribution is to show that, at each iteration, the embedding dimension (or sketch size) can be as small as the effective dimension of the Hessian matrix. Leveraging this novel fundamental result, we design an algorithm with a sketch size proportional to the effective dimension and which exhibits a quadratic rate of convergence. This result dramatically improves on the classical linear-quadratic convergence rates of state-of-the-art sub-sampled Newton methods. However, in most practical cases, the effective dimension is not known beforehand, and this raises the question of how to pick a sketch size as small as the effective dimension while preserving a quadratic convergence rate. Our second and main contribution is thus to propose an adaptive sketch size algorithm with quadratic convergence rate and which does not require prior knowledge or estimation of the effective dimension: at each iteration, it starts with a small sketch size, and increases it until quadratic progress is achieved. Importantly, we show that the embedding dimension remains proportional to the effective dimension throughout the entire path and that our method achieves state-of-the-art computational complexity for solving convex optimization programs with a strongly convex component. We discuss and illustrate applications to linear and quadratic programming, as well as logistic regression and other generalized linear models.",
        "conference": "ICML",
        "中文标题": "自适应牛顿草图：具有二次收敛和有效Hessian维度的线性时间优化",
        "摘要翻译": "我们提出了一种针对自协调、复合、强凸目标函数的凸优化问题的随机算法，该算法具有二次收敛率。我们的方法基于使用Hessian矩阵的随机投影执行近似牛顿步。我们的第一个贡献是证明了在每次迭代中，嵌入维度（或草图大小）可以小至Hessian矩阵的有效维度。利用这一新颖的基本结果，我们设计了一种草图大小与有效维度成比例的算法，并且该算法展现出二次收敛率。这一结果极大地改进了现有子采样牛顿方法的经典线性-二次收敛率。然而，在大多数实际情况下，有效维度事先未知，这就提出了一个问题：如何在保持二次收敛率的同时选择尽可能小的草图大小。因此，我们的第二个也是主要的贡献是提出了一种具有二次收敛率的自适应草图大小算法，该算法不需要事先知道或估计有效维度：在每次迭代中，它从一个小的草图大小开始，并逐步增加，直到实现二次进展。重要的是，我们证明了在整个过程中嵌入维度保持与有效维度成比例，并且我们的方法在解决具有强凸分量的凸优化程序时达到了最先进的计算复杂度。我们讨论并说明了在线性和二次规划，以及逻辑回归和其他广义线性模型中的应用。",
        "领域": "凸优化、随机算法、广义线性模型",
        "问题": "如何在不知道Hessian矩阵有效维度的情况下，设计一种具有二次收敛率的随机优化算法。",
        "动机": "现有的子采样牛顿方法在凸优化问题中通常表现出线性-二次收敛率，而本研究旨在通过自适应草图大小的方法，实现更高效的二次收敛率，无需事先知道有效维度。",
        "方法": "提出了一种基于Hessian矩阵随机投影的自适应草图大小算法，该算法通过动态调整草图大小来保证二次收敛率，同时嵌入维度与Hessian的有效维度成比例。",
        "关键词": [
            "自适应草图",
            "二次收敛",
            "Hessian有效维度",
            "凸优化",
            "随机算法"
        ],
        "涉及的技术概念": {
            "自协调目标函数": "在论文中用于描述一类特殊的凸优化问题，这类问题具有良好的数学性质，便于分析和优化。",
            "Hessian矩阵的随机投影": "用于近似计算牛顿步的技术，通过随机投影减少计算复杂度，同时保持算法的收敛性能。",
            "有效维度": "指Hessian矩阵中信息量最大的维度数量，算法通过动态调整草图大小以适应这一维度，从而提高效率。"
        },
        "success": true
    },
    {
        "order": 28,
        "title": "Adaptive Sampling for Best Policy Identification in Markov Decision Processes",
        "html": "https://ICML.cc//virtual/2021/poster/9627",
        "abstract": "We investigate the problem of best-policy identification in discounted Markov Decision Processes (MDPs) when the learner has access to a generative model. The objective is to devise a learning algorithm returning the best policy as early as possible. We first derive a problem-specific lower bound of the sample complexity satisfied by any learning algorithm. This lower bound corresponds to an optimal sample allocation that solves a non-convex program, and hence, is hard to exploit in the design of efficient algorithms. We then provide a simple and tight upper bound of the sample complexity lower bound, whose corresponding nearly-optimal sample allocation becomes explicit. The upper bound depends on specific functionals of the MDP such as the sub-optimality gaps and the variance of the next-state value function, and thus really captures the hardness of the MDP. Finally, we devise KLB-TS (KL Ball Track-and-Stop), an algorithm tracking this nearly-optimal allocation, and provide asymptotic guarantees for its sample complexity (both almost surely and in expectation). The advantages of KLB-TS against state-of-the-art algorithms are discussed and illustrated numerically.",
        "conference": "ICML",
        "中文标题": "马尔可夫决策过程中最佳策略识别的自适应采样",
        "摘要翻译": "我们研究了在折扣马尔可夫决策过程（MDPs）中，当学习者可以访问生成模型时，最佳策略识别的问题。目标是设计一种学习算法，尽可能早地返回最佳策略。我们首先推导出任何学习算法满足的样本复杂度的特定问题下界。这个下界对应于解决非凸程序的最优样本分配，因此，在设计高效算法时难以利用。然后，我们提供了一个简单且紧密的样本复杂度下界上界，其对应的近乎最优样本分配变得明确。上界依赖于MDP的特定函数，如次优差距和下一状态值函数的方差，因此真正捕捉了MDP的难度。最后，我们设计了KLB-TS（KL球跟踪停止），一种跟踪这种近乎最优分配的算法，并为其样本复杂度提供了渐近保证（几乎肯定和在期望中）。讨论了KLB-TS相对于最先进算法的优势，并通过数值进行了说明。",
        "领域": "强化学习、马尔可夫决策过程、最优控制",
        "问题": "在折扣马尔可夫决策过程中，如何设计一种学习算法以尽可能早地识别出最佳策略。",
        "动机": "研究动机是为了解决在马尔可夫决策过程中快速识别最佳策略的问题，特别是在可以访问生成模型的情况下。",
        "方法": "通过推导样本复杂度的下界和上界，设计了一种名为KLB-TS的算法来跟踪近乎最优的样本分配，以实现快速识别最佳策略。",
        "关键词": [
            "马尔可夫决策过程",
            "最佳策略识别",
            "样本复杂度",
            "KLB-TS算法",
            "自适应采样"
        ],
        "涉及的技术概念": {
            "样本复杂度": "在论文中用于衡量学习算法识别最佳策略所需的样本数量，是评估算法效率的关键指标。",
            "KLB-TS算法": "一种设计用于跟踪近乎最优样本分配的算法，旨在实现快速识别最佳策略，具有渐近保证。",
            "次优差距": "用于描述当前策略与最佳策略之间的性能差异，是影响样本复杂度的重要因素之一。"
        },
        "success": true
    },
    {
        "order": 29,
        "title": "AdaXpert: Adapting Neural Architecture for Growing Data",
        "html": "https://ICML.cc//virtual/2021/poster/8621",
        "abstract": "In real-world applications, data often come in a growing manner, where the data volume and the number of classes may increase dynamically. This will bring a critical challenge for learning: given the increasing data volume or the number of classes, one has to instantaneously adjust the neural model capacity to obtain promising performance. Existing methods either ignore the growing nature of data or seek to independently search an optimal architecture for a given dataset, and thus are incapable of promptly adjusting the architectures for the changed data.  To address this, we present a neural architecture adaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust previous architectures on the growing data. Specifically, we introduce an architecture adjuster to generate a suitable architecture for each data snapshot, based on the previous architecture and the different extent between current and previous data distributions. Furthermore, we propose an adaptation condition to determine the necessity of adjustment, thereby avoiding unnecessary and time-consuming adjustments. Extensive experiments on two growth scenarios (increasing data volume and number of classes) demonstrate the effectiveness of the proposed method.",
        "conference": "ICML",
        "中文标题": "AdaXpert：适应数据增长的神经网络架构调整",
        "摘要翻译": "在现实世界的应用中，数据往往以增长的方式出现，数据量和类别数量可能会动态增加。这给学习带来了一个关键挑战：面对不断增加的数据量或类别数量，必须即时调整神经模型的容量以获得良好的性能。现有方法要么忽视了数据的增长特性，要么试图为给定数据集独立搜索最优架构，因此无法为变化的数据及时调整架构。为了解决这个问题，我们提出了一种神经网络架构适应方法，即适应专家（AdaXpert），以有效地在增长的数据上调整先前的架构。具体来说，我们引入了一个架构调整器，基于先前的架构和当前与先前数据分布之间的差异程度，为每个数据快照生成合适的架构。此外，我们提出了一个适应条件来确定调整的必要性，从而避免不必要且耗时的调整。在两种增长场景（增加数据量和类别数量）上的大量实验证明了所提出方法的有效性。",
        "领域": "神经网络架构优化、增量学习、自适应学习",
        "问题": "如何在数据量和类别数量动态增长的情况下，即时调整神经网络架构以保持或提升模型性能。",
        "动机": "现有方法无法有效应对数据动态增长带来的挑战，需要一种能够即时调整神经网络架构以适应数据变化的方法。",
        "方法": "提出了一种名为AdaXpert的神经网络架构适应方法，通过架构调整器根据数据变化动态调整架构，并引入适应条件避免不必要的调整。",
        "关键词": [
            "神经网络架构适应",
            "增量学习",
            "自适应调整",
            "数据增长",
            "架构优化"
        ],
        "涉及的技术概念": {
            "架构调整器": "基于先前架构和数据分布差异动态生成新架构的组件，用于适应数据增长。",
            "适应条件": "用于判断是否需要进行架构调整的条件，避免不必要的调整，提高效率。",
            "数据快照": "在特定时间点的数据状态，用于描述数据增长过程中的某一阶段。"
        },
        "success": true
    },
    {
        "order": 30,
        "title": "Additive Error Guarantees for Weighted Low Rank Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/9981",
        "abstract": "Low-rank approximation is a classic tool in data analysis, where the goal is to approximate a matrix $A$ with a low-rank matrix $L$ so as to minimize the error $\\norm{A - L}_F^2$. However in many applications, approximating some entries is more important than others, which leads to the weighted low rank approximation problem. However, the addition of weights makes the low-rank approximation problem intractable. Thus many works have obtained efficient algorithms under additional structural assumptions on the weight matrix (such as low rank, and appropriate block structure). We study a natural greedy algorithm for weighted low rank approximation and develop a simple condition under which it yields bi-criteria approximation up to a small additive factor in the error. The algorithm involves iteratively computing the top singular vector of an appropriately varying matrix, and is thus easy to implement at scale. Our methods also allow us to study the problem of low rank approximation under $\\ell_p$ norm error.",
        "conference": "ICML",
        "中文标题": "加权低秩逼近的加性误差保证",
        "摘要翻译": "低秩逼近是数据分析中的经典工具，其目标是用一个低秩矩阵L来近似矩阵A，以最小化误差范数A - L的Frobenius范数的平方。然而，在许多应用中，近似某些条目比其他条目更重要，这导致了加权低秩逼近问题。然而，权重的加入使得低秩逼近问题变得难以处理。因此，许多工作已经在权重矩阵的额外结构假设（如低秩和适当的块结构）下获得了有效的算法。我们研究了一种自然的贪婪算法用于加权低秩逼近，并开发了一个简单的条件，在该条件下，它可以在误差中产生双准则近似，直至一个小的加性因子。该算法涉及迭代计算适当变化矩阵的顶部奇异向量，因此易于大规模实现。我们的方法还允许我们研究在ℓp范数误差下的低秩逼近问题。",
        "领域": "矩阵分解、优化算法、数据压缩",
        "问题": "解决加权低秩逼近问题的计算复杂性和效率问题",
        "动机": "在许多实际应用中，不同数据点的重要性不同，需要一种能够考虑这种差异的低秩逼近方法",
        "方法": "采用贪婪算法迭代计算顶部奇异向量，实现加权低秩逼近",
        "关键词": [
            "加权低秩逼近",
            "贪婪算法",
            "奇异值分解",
            "ℓp范数",
            "双准则近似"
        ],
        "涉及的技术概念": {
            "加权低秩逼近": "在低秩逼近问题中引入权重矩阵，以反映不同数据点的重要性差异",
            "贪婪算法": "通过迭代选择当前最优解来逼近全局最优解，用于解决加权低秩逼近问题",
            "奇异值分解": "用于计算矩阵的顶部奇异向量，是贪婪算法实现的关键步骤"
        },
        "success": true
    },
    {
        "order": 31,
        "title": "Addressing Catastrophic Forgetting in Few-Shot Problems",
        "html": "https://ICML.cc//virtual/2021/poster/8817",
        "abstract": "Neural networks are known to suffer from catastrophic forgetting when trained on sequential datasets. While there have been numerous attempts to solve this problem in large-scale supervised classification, little has been done to overcome catastrophic forgetting in few-shot classification problems. We demonstrate that the popular gradient-based model-agnostic meta-learning algorithm (MAML) indeed suffers from catastrophic forgetting and introduce a Bayesian online meta-learning framework that tackles this problem. Our framework utilises Bayesian online learning and meta-learning along with Laplace approximation and variational inference to overcome catastrophic forgetting in few-shot classification problems. The experimental evaluations demonstrate that our framework can effectively achieve this goal in comparison with various baselines. As an additional utility, we also demonstrate empirically that our framework is capable of meta-learning on sequentially arriving few-shot tasks from a stationary task distribution.",
        "conference": "ICML",
        "中文标题": "解决少样本问题中的灾难性遗忘",
        "摘要翻译": "神经网络在顺序数据集上训练时容易遭受灾难性遗忘。尽管在大规模监督分类问题中已有众多尝试解决这一问题，但在少样本分类问题中克服灾难性遗忘的工作却很少。我们证明了流行的基于梯度的模型无关元学习算法（MAML）确实存在灾难性遗忘问题，并引入了一个贝叶斯在线元学习框架来解决这一问题。我们的框架利用贝叶斯在线学习和元学习，结合拉普拉斯近似和变分推理，以克服少样本分类问题中的灾难性遗忘。实验评估表明，与各种基线相比，我们的框架能有效实现这一目标。作为额外的效用，我们还通过实验证明，我们的框架能够对来自静止任务分布的连续到达的少样本任务进行元学习。",
        "领域": "元学习、少样本学习、在线学习",
        "问题": "解决少样本分类问题中的灾难性遗忘",
        "动机": "在大规模监督分类中已有解决灾难性遗忘的方法，但在少样本分类问题中这一问题的解决方案较少",
        "方法": "引入贝叶斯在线元学习框架，结合拉普拉斯近似和变分推理",
        "关键词": [
            "灾难性遗忘",
            "少样本学习",
            "贝叶斯在线学习",
            "元学习",
            "变分推理"
        ],
        "涉及的技术概念": {
            "贝叶斯在线学习": "用于在连续到达的数据上更新模型，避免灾难性遗忘",
            "拉普拉斯近似": "用于近似贝叶斯推断中的后验分布",
            "变分推理": "用于高效地近似复杂分布，优化模型参数"
        },
        "success": true
    },
    {
        "order": 32,
        "title": "A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation",
        "html": "https://ICML.cc//virtual/2021/poster/9999",
        "abstract": "Marginalized importance sampling (MIS), which measures the density ratio between the state-action occupancy of a target policy and that of a sampling distribution, is a promising approach for off-policy evaluation. However, current state-of-the-art MIS methods rely on complex optimization tricks and succeed mostly on simple toy problems. We bridge the gap between MIS and deep reinforcement learning by observing that the density ratio can be computed from the successor representation of the target policy. The successor representation can be trained through deep reinforcement learning methodology and decouples the reward optimization from the dynamics of the environment, making the resulting algorithm stable and applicable to high-dimensional domains. We evaluate the empirical performance of our approach on a variety of challenging Atari and MuJoCo environments.",
        "conference": "ICML",
        "中文标题": "一种基于深度强化学习的边缘化重要性采样方法及其后继表示",
        "摘要翻译": "边缘化重要性采样（MIS）通过测量目标策略与采样分布的状态-动作占用密度比，是一种有前景的离策略评估方法。然而，当前最先进的MIS方法依赖于复杂的优化技巧，且主要在简单的玩具问题上取得成功。我们通过观察到密度比可以从目标策略的后继表示中计算出来，从而将MIS与深度强化学习联系起来。后继表示可以通过深度强化学习方法进行训练，并将奖励优化与环境动态解耦，使得所得算法稳定且适用于高维领域。我们在多种具有挑战性的Atari和MuJoCo环境中评估了我们方法的实证性能。",
        "领域": "深度强化学习、离策略评估、高维环境决策",
        "问题": "如何在高维环境中有效进行离策略评估",
        "动机": "当前边缘化重要性采样方法在复杂环境中的应用受限，需要一种更稳定且适用于高维领域的方法",
        "方法": "利用后继表示计算密度比，结合深度强化学习方法进行训练，实现奖励优化与环境动态的解耦",
        "关键词": [
            "边缘化重要性采样",
            "后继表示",
            "深度强化学习",
            "离策略评估",
            "高维环境"
        ],
        "涉及的技术概念": {
            "边缘化重要性采样": "用于测量目标策略与采样分布的状态-动作占用密度比，是离策略评估的核心方法",
            "后继表示": "用于计算密度比，通过深度强化学习方法训练，实现算法在高维环境中的稳定应用",
            "离策略评估": "在不同于数据收集策略的策略下评估其性能，是强化学习中的一个重要问题"
        },
        "success": true
    },
    {
        "order": 33,
        "title": "A Differentiable Point Process with Its Application to Spiking Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10651",
        "abstract": "This paper is concerned about a learning algorithm for a probabilistic model of spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a stochastic variational inference algorithm to train SNNs with hidden neurons. The algorithm updates the variational distribution using the score function gradient estimator, whose high variance often impedes the whole learning algorithm. This paper presents an alternative gradient estimator for SNNs based on the path-wise gradient estimator. The main technical difficulty is a lack of a general method to differentiate a realization of an arbitrary point process, which is necessary to derive the path-wise gradient estimator. We develop a differentiable point process, which is the technical highlight of this paper, and apply it to derive the path-wise gradient estimator for SNNs. We investigate the effectiveness of our gradient estimator through numerical simulation.\n",
        "conference": "ICML",
        "中文标题": "一种可微分点过程及其在脉冲神经网络中的应用",
        "摘要翻译": "本文关注于脉冲神经网络（SNNs）概率模型的学习算法。Jimenez Rezende & Gerstner（2014）提出了一种随机变分推理算法来训练带有隐藏神经元的SNNs。该算法使用得分函数梯度估计器更新变分分布，其高方差常常阻碍整个学习算法。本文提出了一种基于路径梯度估计器的SNNs替代梯度估计器。主要技术难点在于缺乏一种通用方法来微分任意点过程的实现，这是推导路径梯度估计器所必需的。我们开发了一种可微分点过程，这是本文的技术亮点，并将其应用于推导SNNs的路径梯度估计器。我们通过数值模拟研究了我们的梯度估计器的有效性。",
        "领域": "脉冲神经网络、变分推理、梯度估计",
        "问题": "解决脉冲神经网络中由于得分函数梯度估计器高方差导致的学习算法效率低下问题",
        "动机": "提高脉冲神经网络学习算法的效率和稳定性",
        "方法": "开发一种可微分点过程，并基于此提出路径梯度估计器作为替代方案",
        "关键词": [
            "脉冲神经网络",
            "可微分点过程",
            "路径梯度估计器",
            "变分推理",
            "数值模拟"
        ],
        "涉及的技术概念": {
            "可微分点过程": "本文开发的技术，允许对任意点过程的实现进行微分，是推导路径梯度估计器的关键",
            "路径梯度估计器": "替代得分函数梯度估计器的方法，旨在减少梯度估计的方差，提高学习算法的效率",
            "变分推理": "用于训练带有隐藏神经元的脉冲神经网络的随机变分推理算法，本文在此基础上进行了改进"
        },
        "success": true
    },
    {
        "order": 34,
        "title": "A Discriminative Technique for Multiple-Source Adaptation",
        "html": "https://ICML.cc//virtual/2021/poster/10189",
        "abstract": "We present a new discriminative technique for the multiple-source adaptation (MSA) problem. Unlike previous work, which relies on density estimation for each source domain, our solution only requires conditional probabilities that can be straightforwardly accurately estimated from unlabeled data from the source domains. We give a detailed analysis of our new technique, including general guarantees based on R\\'enyi divergences, and learning bounds when conditional Maxent is used for estimating conditional probabilities for a point to belong to a source domain. We show that these guarantees compare favorably to those that can be derived for the generative solution, using kernel density estimation.  Our\r\nexperiments with real-world applications further demonstrate that our new discriminative MSA algorithm outperforms the previous generative solution as well as other domain adaptation baselines.",
        "conference": "ICML",
        "中文标题": "一种多源适应的判别技术",
        "摘要翻译": "我们提出了一种新的判别技术，用于解决多源适应（MSA）问题。与之前依赖于每个源域密度估计的工作不同，我们的解决方案仅需要条件概率，这些概率可以直接从源域的无标签数据中准确估计。我们对我们的新技术进行了详细分析，包括基于Rényi散度的一般保证，以及当使用条件最大熵来估计一个点属于源域的条件概率时的学习界限。我们表明，这些保证与使用核密度估计的生成解决方案相比具有优势。我们在真实世界应用中的实验进一步证明，我们的新判别性MSA算法优于之前的生成解决方案以及其他域适应基线。",
        "领域": "多源域适应、条件概率估计、判别学习",
        "问题": "解决多源域适应问题，提高模型在多个源域数据上的适应性和性能。",
        "动机": "现有的多源域适应方法依赖于复杂的密度估计，而本研究旨在开发一种更简单、更有效的判别性技术，仅需条件概率即可实现域适应。",
        "方法": "提出了一种新的判别性多源适应技术，该技术基于条件概率的直接估计，避免了复杂的密度估计过程，并利用Rényi散度和条件最大熵进行理论保证和学习界限的分析。",
        "关键词": [
            "多源域适应",
            "判别学习",
            "条件概率估计",
            "Rényi散度",
            "条件最大熵"
        ],
        "涉及的技术概念": {
            "多源域适应": "研究如何利用多个源域的数据来提高模型在目标域上的性能。",
            "条件概率估计": "直接从无标签数据中估计样本属于特定源域的条件概率，避免了复杂的密度估计。",
            "Rényi散度": "用于衡量不同概率分布之间的差异，为多源域适应提供理论保证。"
        },
        "success": true
    },
    {
        "order": 35,
        "title": "A Distribution-dependent Analysis of Meta Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10047",
        "abstract": "A key problem in the theory of meta-learning is to understand how the task distributions influence transfer risk, the expected error of a meta-learner on a new task drawn from the unknown task distribution. In this paper, focusing on fixed design linear regression with Gaussian noise and a Gaussian task (or parameter) distribution, we give distribution-dependent lower bounds on the transfer risk of any algorithm, while we also show that a novel, weighted version of the so-called biased regularized regression method is able to match these lower bounds up to a fixed constant factor. Notably, the weighting is derived from the covariance of the Gaussian task distribution. Altogether, our results provide a precise characterization of the difficulty of meta-learning in this Gaussian setting. While this problem setting may appear simple, we show that it is rich enough to unify the “parameter sharing” and “representation learning” streams of meta-learning; in particular, representation learning is obtained as the special case when the covariance matrix of the task distribution is unknown. For this case we propose to adopt the EM method, which is shown to enjoy efficient updates in our case. The paper is completed by an empirical study of EM. In particular, our experimental results show that the EM algorithm can attain the lower bound as the number of tasks grows, while the algorithm is also successful in competing with its alternatives when used in a representation learning context.\n",
        "conference": "ICML",
        "中文标题": "元学习的分布依赖性分析",
        "摘要翻译": "元学习理论中的一个关键问题是理解任务分布如何影响转移风险，即元学习器在从未知任务分布中抽取的新任务上的预期误差。本文聚焦于具有高斯噪声和高斯任务（或参数）分布的固定设计线性回归，我们给出了任何算法在转移风险上的分布依赖性下界，同时展示了所谓的偏置正则化回归方法的一个新颖加权版本能够将这些下界匹配到一个固定的常数因子。值得注意的是，加权是从高斯任务分布的协方差中导出的。总之，我们的结果提供了在这个高斯设置下元学习难度的精确刻画。虽然这个问题设置可能看起来简单，但我们展示了它足够丰富以统一元学习的“参数共享”和“表示学习”流派；特别是，表示学习是作为任务分布的协方差矩阵未知时的特殊情况获得的。对于这种情况，我们建议采用EM方法，该方法在我们的案例中被证明享有高效的更新。本文通过EM的实证研究完成。特别是，我们的实验结果表明，随着任务数量的增长，EM算法可以达到下界，同时当用于表示学习上下文时，该算法也能成功地与其替代方案竞争。",
        "领域": "元学习、表示学习、参数共享",
        "问题": "理解任务分布如何影响元学习中的转移风险",
        "动机": "研究任务分布对元学习性能的影响，特别是在高斯噪声和高斯任务分布下的固定设计线性回归场景中",
        "方法": "提出了一个加权版本的偏置正则化回归方法，并采用EM方法处理表示学习中的未知协方差矩阵情况",
        "关键词": [
            "元学习",
            "转移风险",
            "高斯分布",
            "EM算法",
            "表示学习"
        ],
        "涉及的技术概念": {
            "转移风险": "元学习器在从未知任务分布中抽取的新任务上的预期误差",
            "偏置正则化回归": "一种用于匹配转移风险下界的加权回归方法，权重来源于任务分布的协方差",
            "EM算法": "用于处理任务分布协方差矩阵未知情况下的表示学习问题，能够高效更新模型参数"
        },
        "success": true
    },
    {
        "order": 36,
        "title": "ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10125",
        "abstract": "We propose ADOM -- an accelerated method for smooth and strongly convex decentralized optimization over time-varying networks. ADOM uses a dual oracle, i.e., we assume access to the gradient of the Fenchel conjugate of the individual loss functions. Up to a constant factor, which depends on the network structure only, its communication complexity is the same as that of accelerated Nesterov gradient method. To the best of our knowledge,  only the algorithm of Rogozin et al. (2019) has a  convergence rate with similar properties. However, their algorithm converges under the very restrictive assumption that the number of network changes can not be greater than a tiny percentage of the number of iterations. This assumption is hard to satisfy in practice, as the network topology changes usually can not be controlled. In contrast, ADOM merely requires the network to stay connected throughout time. ",
        "conference": "ICML",
        "中文标题": "ADOM：时变网络加速去中心化优化方法",
        "摘要翻译": "我们提出了ADOM——一种针对时变网络的平滑且强凸去中心化优化的加速方法。ADOM使用了对偶预言机，即我们假设可以访问个体损失函数的Fenchel共轭梯度。除了一个仅依赖于网络结构的常数因子外，其通信复杂度与加速Nesterov梯度方法相同。据我们所知，只有Rogozin等人（2019）的算法具有类似性质的收敛速率。然而，他们的算法在非常严格的假设下收敛，即网络变化的数量不能大于迭代次数的极小百分比。这一假设在实践中难以满足，因为网络拓扑的变化通常无法控制。相比之下，ADOM仅要求网络在整个时间内保持连接。",
        "领域": "去中心化优化, 时变网络, 加速优化方法",
        "问题": "解决时变网络环境下平滑且强凸去中心化优化的加速问题",
        "动机": "现有算法在时变网络环境下收敛条件过于严格，难以满足实际应用需求，ADOM旨在放宽这些条件，仅需网络保持连接即可实现高效优化",
        "方法": "采用对偶预言机技术，结合Fenchel共轭梯度，实现与加速Nesterov梯度方法相当的通信复杂度",
        "关键词": [
            "去中心化优化",
            "时变网络",
            "加速方法",
            "Fenchel共轭",
            "对偶预言机"
        ],
        "涉及的技术概念": {
            "对偶预言机": "用于访问个体损失函数的Fenchel共轭梯度，是实现去中心化优化的关键技术",
            "Fenchel共轭": "在优化问题中用于转换原问题到对偶空间，便于处理复杂的优化目标",
            "加速Nesterov梯度方法": "一种高效的优化技术，ADOM在通信复杂度上与之相当，适用于时变网络环境"
        },
        "success": true
    },
    {
        "order": 37,
        "title": "Adversarial Combinatorial Bandits with General Non-linear Reward Functions",
        "html": "https://ICML.cc//virtual/2021/poster/8419",
        "abstract": "In this paper we study the adversarial combinatorial bandit with a known non-linear reward function, extending existing work on adversarial linear combinatorial bandit. {The adversarial combinatorial bandit with general non-linear reward is an important open problem in bandit literature, and it is still unclear whether there is a significant gap from the case of linear reward, stochastic bandit, or semi-bandit feedback.} We show that, with $N$ arms and subsets of $K$ arms being chosen at each of $T$ time periods, the minimax optimal regret is $\\widetilde\\Theta_{d}(\\sqrt{N^d T})$ if the reward function is a $d$-degree polynomial with $d< K$, and $\\Theta_K(\\sqrt{N^K T})$ if the reward function is not a low-degree polynomial. {Both bounds are significantly different from the bound $O(\\sqrt{\\mathrm{poly}(N,K)T})$ for the linear case, which suggests that there is a fundamental gap between the linear and non-linear reward structures.} Our result also finds applications to adversarial assortment optimization problem in online recommendation. We show that in the worst-case of adversarial assortment problem, the optimal algorithm must treat each individual $\\binom{N}{K}$ assortment as independent.",
        "conference": "ICML",
        "success": true,
        "中文标题": "具有一般非线性奖励函数的对抗性组合赌博机",
        "摘要翻译": "本文研究了已知非线性奖励函数的对抗性组合赌博机，扩展了对抗性线性组合赌博机的现有工作。具有一般非线性奖励的对抗性组合赌博机是赌博机文献中的一个重要开放问题，目前尚不清楚是否存在与线性奖励、随机赌博机或半赌博机反馈情况的显著差距。我们表明，在每T个时间段选择N个臂和K个臂的子集的情况下，如果奖励函数是一个d次多项式且d<K，则极小极大最优遗憾为Θ̃d(√(N^d T))；如果奖励函数不是低次多项式，则为ΘK(√(N^K T))。这两个界限与线性情况的O(√(poly(N,K)T))界限显著不同，这表明线性和非线性奖励结构之间存在根本性差距。我们的结果也适用于在线推荐中的对抗性分类优化问题。我们表明，在对抗性分类问题的最坏情况下，最优算法必须将每个独立的C(N,K)分类视为独立。",
        "领域": "对抗性学习, 组合优化, 在线推荐系统",
        "问题": "研究对抗性组合赌博机在非线性奖励函数下的最优遗憾界限",
        "动机": "探索非线性奖励结构在对抗性组合赌博机中的影响，填补现有研究的空白",
        "方法": "通过理论分析，确定了不同非线性奖励函数下的极小极大最优遗憾界限",
        "关键词": [
            "对抗性组合赌博机",
            "非线性奖励函数",
            "极小极大遗憾",
            "在线推荐",
            "组合优化"
        ],
        "涉及的技术概念": {
            "对抗性组合赌博机": "研究在对抗性环境下选择最优臂组合的问题，扩展了传统赌博机理论",
            "非线性奖励函数": "论文中用于描述奖励与选择臂之间非线性关系的函数，区别于传统的线性模型",
            "极小极大遗憾": "用于衡量算法在最坏情况下的性能差距，是评估对抗性环境下算法性能的关键指标"
        }
    },
    {
        "order": 38,
        "title": "Adversarial Dueling Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/10433",
        "abstract": "We introduce the problem of regret minimization in Adversarial Dueling Bandits. As in classic Dueling Bandits, the learner has to repeatedly choose a pair of items and observe only a relative binary `win-loss' feedback for this pair, but here this feedback is generated from an arbitrary preference matrix, possibly chosen adversarially. \nOur main result is an algorithm whose $T$-round regret compared to the \\emph{Borda-winner} from a set of $K$ items is $\\tilde{O}(K^{1/3}T^{2/3})$, as well as a matching $\\Omega(K^{1/3}T^{2/3})$ lower bound. We also prove a similar high probability regret bound.\nWe further consider a simpler \\emph{fixed-gap} adversarial setup, which bridges between two extreme preference feedback models for dueling bandits: stationary preferences and an arbitrary sequence of preferences. For the fixed-gap adversarial setup we give an $\\smash{ \\tilde{O}((K/\\Delta^2)\\log{T}) }$ regret algorithm, where $\\Delta$ is the gap in Borda scores between the best item and all other items, and show a lower bound of $\\Omega(K/\\Delta^2)$ indicating that our dependence on the main problem parameters $K$ and $\\Delta$ is tight (up to logarithmic factors). Finally, we corroborate the theoretical results with empirical evaluations.",
        "conference": "ICML",
        "success": true,
        "中文标题": "对抗性决斗赌博机",
        "摘要翻译": "我们介绍了在对抗性决斗赌博机中的遗憾最小化问题。与经典的决斗赌博机一样，学习者必须重复选择一对物品，并仅观察这对物品的相对二元‘胜负’反馈，但这里的反馈是由任意偏好矩阵生成的，可能是对抗性选择的。我们的主要结果是一个算法，其与来自一组K个物品的Borda胜者相比的T轮遗憾为O~(K^(1/3)T^(2/3)，以及一个匹配的Ω(K^(1/3)T^(2/3))下界。我们还证明了一个类似的高概率遗憾界。我们进一步考虑了一个更简单的固定间隙对抗性设置，它桥接了决斗赌博机的两种极端偏好反馈模型：静态偏好和任意序列偏好。对于固定间隙对抗性设置，我们给出了一个O~((K/Δ^2)logT)遗憾算法，其中Δ是Borda分数中最佳物品与所有其他物品之间的差距，并展示了一个Ω(K/Δ^2)的下界，表明我们对主要问题参数K和Δ的依赖是紧的（达到对数因子）。最后，我们用实证评估验证了理论结果。",
        "领域": "强化学习, 在线学习, 对抗性机器学习",
        "问题": "在对抗性环境中最小化决斗赌博机的遗憾",
        "动机": "研究在对抗性偏好反馈下，如何有效地最小化学习过程中的遗憾",
        "方法": "提出了一种算法，用于在对抗性决斗赌博机中最小化遗憾，并提供了理论上的上下界证明",
        "关键词": [
            "对抗性决斗赌博机",
            "遗憾最小化",
            "Borda胜者",
            "固定间隙",
            "在线学习"
        ],
        "涉及的技术概念": {
            "对抗性决斗赌博机": "一种在对抗性环境中研究决斗赌博机问题的框架，其中反馈由可能对抗性选择的偏好矩阵生成",
            "Borda胜者": "在决斗赌博机中，根据Borda分数确定的最佳物品",
            "固定间隙对抗性设置": "一种简化的对抗性设置，用于桥接静态偏好和任意序列偏好两种极端反馈模型"
        }
    },
    {
        "order": 39,
        "title": "Adversarial Multi Class Learning under Weak Supervision with Performance Guarantees",
        "html": "https://ICML.cc//virtual/2021/poster/9605",
        "abstract": "We develop a rigorous approach for using a set of arbitrarily correlated weak supervision sources in order to solve a multiclass classification task when only a very small set of labeled data is available. Our learning algorithm provably converges to a model that has minimum empirical risk with respect to an adversarial choice over feasible labelings for a set of unlabeled data, where the feasibility of a labeling is computed through constraints defined by rigorously estimated statistics of the weak supervision sources. We show theoretical guarantees for this approach that depend on the information provided by the  weak supervision sources. Notably, this method does not require the weak supervision sources to have the same labeling space as the multiclass classification task. We demonstrate the effectiveness of our approach with experiments on various image classification tasks.",
        "conference": "ICML",
        "中文标题": "弱监督下具有性能保证的对抗性多类学习",
        "摘要翻译": "我们开发了一种严格的方法，用于利用一组任意相关的弱监督源来解决多类分类任务，当只有非常少量的标记数据可用时。我们的学习算法可证明地收敛到一个模型，该模型对于一组未标记数据的可行标记的对抗性选择具有最小经验风险，其中标记的可行性是通过严格估计的弱监督源统计定义的约束来计算的。我们展示了这种方法依赖于弱监督源提供的信息的理论保证。值得注意的是，该方法不要求弱监督源与多类分类任务具有相同的标记空间。我们通过在各种图像分类任务上的实验证明了我们方法的有效性。",
        "领域": "弱监督学习、多类分类、图像分类",
        "问题": "在仅有少量标记数据的情况下，利用任意相关的弱监督源解决多类分类问题。",
        "动机": "研究动机是为了在标记数据稀缺的情况下，通过利用弱监督源的信息，有效地解决多类分类问题。",
        "方法": "开发了一种学习算法，该算法通过严格估计弱监督源的统计信息，定义约束条件，确保模型在对抗性选择的可行标记上具有最小经验风险。",
        "关键词": [
            "弱监督学习",
            "多类分类",
            "对抗性学习",
            "性能保证",
            "图像分类"
        ],
        "涉及的技术概念": {
            "弱监督源": "在标记数据稀缺的情况下，利用不完全或噪声标记的数据源来辅助学习。",
            "对抗性选择": "在模型训练过程中，通过对抗性方法选择最不利于当前模型的标记，以提高模型的鲁棒性。",
            "经验风险最小化": "通过最小化模型在训练数据上的预测错误，来优化模型性能。"
        },
        "success": true
    },
    {
        "order": 40,
        "title": "Adversarial Option-Aware Hierarchical Imitation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8589",
        "abstract": "It has been a challenge to learning skills for an agent from long-horizon unannotated demonstrations. Existing approaches like Hierarchical Imitation Learning(HIL) are prone to compounding errors or suboptimal solutions. In this paper, we propose Option-GAIL, a novel method to learn skills at long horizon. The key idea of Option-GAIL is modeling the task hierarchy by options and train the policy via generative adversarial optimization. In particular, we propose an Expectation-Maximization(EM)-style algorithm: an E-step that samples the options of expert conditioned on the current learned policy, and an M-step that updates the low- and high-level policies of agent simultaneously to minimize the newly proposed option-occupancy measurement between the expert and the agent. We theoretically prove the convergence of the proposed algorithm. Experiments show that Option-GAIL outperforms other counterparts consistently across a variety of tasks.",
        "conference": "ICML",
        "中文标题": "对抗性选项感知的层次模仿学习",
        "摘要翻译": "从长期未标注的演示中学习技能对智能体来说一直是一个挑战。现有的方法，如层次模仿学习（HIL），容易产生复合错误或次优解。在本文中，我们提出了Option-GAIL，一种学习长期技能的新方法。Option-GAIL的核心思想是通过选项建模任务层次，并通过生成对抗优化训练策略。特别是，我们提出了一种期望最大化（EM）风格的算法：E步骤根据当前学习到的策略采样专家的选项，M步骤同时更新智能体的低级和高级策略，以最小化专家和智能体之间新提出的选项占用测量。我们从理论上证明了所提出算法的收敛性。实验表明，Option-GAIL在各种任务中始终优于其他对应方法。",
        "领域": "模仿学习、强化学习、生成对抗网络",
        "问题": "解决从长期未标注演示中学习技能时产生的复合错误或次优解问题",
        "动机": "为了更有效地从长期未标注的演示中学习技能，避免现有方法的局限性",
        "方法": "提出Option-GAIL方法，通过选项建模任务层次，并使用生成对抗优化训练策略，包括一个EM风格的算法来更新策略",
        "关键词": [
            "模仿学习",
            "生成对抗网络",
            "选项建模",
            "长期技能学习",
            "层次策略"
        ],
        "涉及的技术概念": {
            "选项建模": "用于建模任务层次，帮助智能体理解长期技能的结构",
            "生成对抗优化": "用于训练策略，通过对抗过程提高策略的质量",
            "期望最大化算法": "用于交替采样专家选项和更新策略，以优化选项占用测量"
        },
        "success": true
    },
    {
        "order": 41,
        "title": "Adversarial Policy Learning in Two-player Competitive Games",
        "html": "https://ICML.cc//virtual/2021/poster/9779",
        "abstract": "In a two-player deep reinforcement learning task, recent work shows an attacker could learn an adversarial policy that triggers a target agent to perform poorly and even react in an undesired way. However, its efficacy heavily relies upon the zero-sum assumption made in the two-player game. In this work, we propose a new adversarial learning algorithm. It addresses the problem by resetting the optimization goal in the learning process and designing a new surrogate optimization function. Our experiments show that our method significantly improves adversarial agents' exploitability compared with the state-of-art attack. Besides, we also discover that our method could augment an agent with the ability to abuse the target game's unfairness. Finally, we show that agents adversarially re-trained against our adversarial agents could obtain stronger adversary-resistance.",
        "conference": "ICML",
        "中文标题": "双人竞争游戏中的对抗策略学习",
        "摘要翻译": "在双人深度强化学习任务中，最近的研究表明，攻击者可以学习一种对抗策略，触发目标代理表现不佳，甚至以不希望的方式反应。然而，其效果在很大程度上依赖于双人游戏中做出的零和假设。在这项工作中，我们提出了一种新的对抗学习算法。它通过在学习过程中重置优化目标和设计一个新的替代优化函数来解决这个问题。我们的实验表明，与最先进的攻击相比，我们的方法显著提高了对抗代理的可利用性。此外，我们还发现，我们的方法可以增强代理利用目标游戏不公平性的能力。最后，我们展示了针对我们的对抗代理进行对抗性重新训练的代理可以获得更强的对抗抵抗能力。",
        "领域": "对抗性学习、强化学习、游戏AI",
        "问题": "解决在双人深度强化学习任务中，对抗策略学习依赖于零和假设的问题",
        "动机": "提高对抗代理的可利用性，并探索代理利用游戏不公平性的能力",
        "方法": "提出了一种新的对抗学习算法，包括重置优化目标和设计新的替代优化函数",
        "关键词": [
            "对抗策略学习",
            "深度强化学习",
            "双人游戏",
            "对抗性学习算法",
            "游戏不公平性"
        ],
        "涉及的技术概念": {
            "对抗策略学习": "在双人游戏中，攻击者学习策略以触发目标代理表现不佳或反应异常",
            "替代优化函数": "新设计的优化函数，用于在学习过程中提高对抗代理的可利用性",
            "对抗性重新训练": "针对对抗代理进行训练，以增强代理的对抗抵抗能力"
        },
        "success": true
    },
    {
        "order": 42,
        "title": "Adversarial Purification with Score-based Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/10745",
        "abstract": "While adversarial training is considered as a standard defense method against adversarial attacks for image classifiers, adversarial purification, which purifies attacked images into clean images with a standalone purification, model has shown promises as an alternative defense method. Recently, an EBM trained with MCMC has been highlighted as a purification model, where an attacked image is purified by running a long Markov-chain using the gradients of the EBM. Yet, the practicality of the adversarial purification using an EBM remains questionable because the number of MCMC steps required for such purification is too large. In this paper, we propose a novel adversarial purification method based on an EBM trained with DSM. We show that an EBM trained with DSM can quickly purify attacked images within a few steps. We further introduce a simple yet effective randomized purification scheme that injects random noises into images before purification. This process screens the adversarial perturbations imposed on images by the random noises and brings the images to the regime where the EBM can denoise well. We show that our purification method is robust against various attacks and demonstrate its state-of-the-art performances.",
        "conference": "ICML",
        "中文标题": "基于分数生成模型的对抗净化",
        "摘要翻译": "虽然对抗训练被视为图像分类器对抗攻击的标准防御方法，但对抗净化作为一种替代防御方法，通过独立的净化模型将受攻击的图像净化成干净图像，已显示出其潜力。最近，使用MCMC训练的EBM作为一种净化模型受到关注，其中通过利用EBM的梯度运行长马尔可夫链来净化受攻击的图像。然而，由于这种净化所需的MCMC步骤数量过大，使用EBM进行对抗净化的实用性仍然存疑。在本文中，我们提出了一种基于DSM训练的EBM的新型对抗净化方法。我们展示了使用DSM训练的EBM可以在几步之内快速净化受攻击的图像。我们进一步引入了一种简单而有效的随机净化方案，该方案在净化前将随机噪声注入图像中。这一过程通过随机噪声筛选出图像上的对抗性扰动，并将图像带到EBM能够良好去噪的区域。我们展示了我们的净化方法对各种攻击具有鲁棒性，并证明了其最先进的性能。",
        "领域": "对抗性防御、图像分类、生成模型",
        "问题": "解决对抗性攻击下图像分类器的防御问题，特别是减少对抗净化所需的计算步骤",
        "动机": "提高对抗净化的效率和实用性，使其在实际应用中更加可行",
        "方法": "提出了一种基于DSM训练的EBM的新型对抗净化方法，并引入随机噪声注入方案以提高净化效率",
        "关键词": [
            "对抗净化",
            "分数生成模型",
            "EBM",
            "DSM",
            "随机噪声注入"
        ],
        "涉及的技术概念": {
            "EBM（能量基模型）": "用于构建净化模型，通过梯度信息指导图像净化过程",
            "DSM（去噪分数匹配）": "训练EBM的方法，使得模型能够快速有效地净化受攻击的图像",
            "随机噪声注入": "在净化前向图像添加随机噪声，以筛选和削弱对抗性扰动，提高净化效果"
        },
        "success": true
    },
    {
        "order": 43,
        "title": "Adversarial Robustness Guarantees for Random Deep Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8649",
        "abstract": "The reliability of deep learning algorithms is fundamentally challenged by the existence of adversarial examples, which are incorrectly classified inputs that are extremely close to a correctly classified input. We explore the properties of adversarial examples for deep neural networks with random weights and biases, and prove that for any p≥1, the ℓ^p distance of any given input from the classification boundary scales as one over the square root of the dimension of the input times the ℓ^p norm of the input. The results are based on the recently proved equivalence between Gaussian processes and deep neural networks in the limit of infinite width of the hidden layers, and are validated with experiments on both random deep neural networks and deep neural networks trained on the MNIST and CIFAR10 datasets. The results constitute a fundamental advance in the theoretical understanding of adversarial examples, and open the way to a thorough theoretical characterization of the relation between network architecture and robustness to adversarial perturbations.",
        "conference": "ICML",
        "中文标题": "随机深度神经网络的对抗鲁棒性保证",
        "摘要翻译": "深度学习算法的可靠性从根本上受到对抗样本存在的挑战，这些对抗样本是被错误分类的输入，与正确分类的输入极为接近。我们探索了具有随机权重和偏置的深度神经网络对抗样本的性质，并证明对于任何p≥1，任何给定输入与分类边界的ℓ^p距离与输入维度的平方根成反比，乘以输入的ℓ^p范数。这些结果基于最近证明的高斯过程与无限宽度隐藏层的深度神经网络之间的等价性，并通过在随机深度神经网络和在MNIST及CIFAR10数据集上训练的深度神经网络的实验得到验证。这些结果构成了对抗样本理论理解的根本进展，并为网络架构与对抗扰动鲁棒性之间关系的彻底理论表征开辟了道路。",
        "领域": "对抗性机器学习、深度神经网络理论、图像分类",
        "问题": "探索和证明随机深度神经网络对抗样本的性质及其与输入维度和范数的关系。",
        "动机": "对抗样本的存在挑战了深度学习算法的可靠性，理解其性质有助于提升模型的鲁棒性。",
        "方法": "基于高斯过程与无限宽度深度神经网络的等价性理论，通过理论分析和实验验证对抗样本的性质。",
        "关键词": [
            "对抗样本",
            "深度神经网络",
            "高斯过程",
            "鲁棒性",
            "理论分析"
        ],
        "涉及的技术概念": {
            "对抗样本": "被错误分类的输入，与正确分类的输入极为接近，挑战深度学习算法的可靠性。",
            "高斯过程": "用于理论分析的工具，与无限宽度深度神经网络在数学上等价。",
            "ℓ^p距离": "衡量输入与分类边界距离的度量，用于分析对抗样本的性质。"
        },
        "success": true
    },
    {
        "order": 44,
        "title": "Affine Invariant Analysis of Frank-Wolfe on Strongly Convex Sets",
        "html": "https://ICML.cc//virtual/2021/poster/9815",
        "abstract": "It is known that the Frank-Wolfe (FW) algorithm, which is affine covariant, enjoys faster convergence rates than $\\mathcal{O}\\left(1/K\\right)$ when the constraint set is strongly convex. However, these results rely on norm-dependent assumptions, usually incurring non-affine invariant bounds, in contradiction with FW's affine covariant property. In this work, we introduce new structural assumptions on the problem (such as the directional smoothness) and derive an affine invariant, norm-independent analysis of Frank-Wolfe. We show that our rates are better than any other known convergence rates of FW in this setting. Based on our analysis, we propose an affine invariant backtracking line-search. Interestingly, we show that typical backtracking line-searches using smoothness of the objective function present similar performances than its affine invariant counterpart, despite using affine dependent norms in the step size's computation.",
        "conference": "ICML",
        "success": true,
        "中文标题": "强凸集上Frank-Wolfe算法的仿射不变性分析",
        "摘要翻译": "已知Frank-Wolfe（FW）算法具有仿射协变性，当约束集为强凸时，其收敛速度优于$\\mathcal{O}\\left(1/K\\right)$。然而，这些结果依赖于范数相关的假设，通常会产生非仿射不变的界限，与FW的仿射协变性质相矛盾。在这项工作中，我们引入了问题上的新结构假设（如方向平滑性），并推导出了Frank-Wolfe的仿射不变、范数独立分析。我们表明，在此设置下，我们的速率优于FW的任何其他已知收敛速率。基于我们的分析，我们提出了一种仿射不变的回溯线搜索。有趣的是，我们发现，尽管在步长计算中使用了仿射依赖的范数，但使用目标函数平滑性的典型回溯线搜索与其仿射不变对应物表现出相似的性能。",
        "领域": "优化算法, 凸优化, 机器学习理论",
        "问题": "解决Frank-Wolfe算法在强凸约束集上的收敛速率分析中存在的范数依赖性问题，实现仿射不变的分析。",
        "动机": "Frank-Wolfe算法的仿射协变性质与其收敛速率分析中的范数依赖性之间存在矛盾，本研究旨在消除这一矛盾，提供更准确的收敛速率分析。",
        "方法": "引入新的结构假设（如方向平滑性），进行仿射不变、范数独立的Frank-Wolfe算法分析，并提出仿射不变的回溯线搜索方法。",
        "关键词": [
            "Frank-Wolfe算法",
            "仿射不变性",
            "强凸集",
            "收敛速率",
            "回溯线搜索"
        ],
        "涉及的技术概念": {
            "仿射协变性": "Frank-Wolfe算法的一种性质，意味着算法的性能不受仿射变换的影响。",
            "方向平滑性": "本研究引入的新结构假设，用于描述目标函数在特定方向上的平滑程度。"
        }
    },
    {
        "order": 45,
        "title": "A Framework for Private Matrix Analysis in Sliding Window Model",
        "html": "https://ICML.cc//virtual/2021/poster/8999",
        "abstract": "We perform a rigorous study of private matrix analysis when only the last $W$ updates to matrices are considered useful for analysis. We show the existing framework in the non-private setting is not robust to noise required for privacy. We then propose a framework robust to noise and use it to give first efficient $o(W)$ space differentially private algorithms for spectral approximation, principal component analysis (PCA), multi-response linear regression, sparse PCA, and non-negative PCA. Prior to our work, no such result was known for sparse and non-negative differentially private PCA even in the static data setting. We also give a lower bound to demonstrate the cost of privacy in the sliding window model. \n",
        "conference": "ICML",
        "中文标题": "滑动窗口模型中私有矩阵分析的框架",
        "摘要翻译": "我们对私有矩阵分析进行了严格研究，仅考虑矩阵的最后$W$次更新对分析有用。我们展示了在非私有设置中现有框架对隐私所需的噪声不具备鲁棒性。接着，我们提出了一个对噪声具有鲁棒性的框架，并利用它首次为谱近似、主成分分析（PCA）、多响应线性回归、稀疏PCA和非负PCA提供了高效的$o(W)$空间差分隐私算法。在我们的工作之前，即使在静态数据设置中，稀疏和非负差分隐私PCA也没有已知的此类结果。我们还给出了一个下界，以展示滑动窗口模型中隐私的成本。",
        "领域": "差分隐私、主成分分析、线性回归",
        "问题": "在滑动窗口模型下，如何高效地进行私有矩阵分析，特别是针对谱近似、PCA、线性回归等问题。",
        "动机": "现有非私有框架对隐私保护所需的噪声不具备鲁棒性，需要开发新的框架以支持高效的差分隐私算法。",
        "方法": "提出了一种对噪声具有鲁棒性的框架，并基于此框架开发了高效的差分隐私算法，用于解决多种矩阵分析问题。",
        "关键词": [
            "差分隐私",
            "滑动窗口模型",
            "主成分分析",
            "稀疏PCA",
            "非负PCA"
        ],
        "涉及的技术概念": {
            "差分隐私": "用于保护数据隐私的技术，确保算法的输出不会泄露个体数据的信息。",
            "滑动窗口模型": "一种数据处理模型，仅考虑数据流中最近的$W$个更新，适用于动态数据分析。",
            "主成分分析（PCA）": "一种统计方法，通过线性变换将数据转换到新的坐标系中，使得任何投影数据的第一大方差在第一个坐标上，第二大方差在第二个坐标上，依此类推。"
        },
        "success": true
    },
    {
        "order": 46,
        "title": "A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration",
        "html": "https://ICML.cc//virtual/2021/poster/9041",
        "abstract": "Spiking Neural Network (SNN) has been recognized as one of the next generation of neural networks. Conventionally, SNN can be converted from a pre-trained ANN by only replacing the ReLU activation to spike activation while keeping the parameters intact. Perhaps surprisingly, in this work we show that a proper way to calibrate the parameters during the conversion of ANN to SNN can bring significant improvements. We introduce SNN Calibration, a cheap but extraordinarily effective method by leveraging the knowledge within a pre-trained Artificial Neural Network (ANN). Starting by analyzing the conversion error and its propagation through layers theoretically, we propose the calibration algorithm that can correct the error layer-by-layer. The calibration only takes a handful number of training data and several minutes to finish. Moreover, our calibration algorithm can produce SNN with state-of-the-art architecture on the large-scale ImageNet dataset, including MobileNet and RegNet. Extensive experiments demonstrate the effectiveness and efficiency of our algorithm. For example, our advanced pipeline can increase up to 69% top-1 accuracy when converting MobileNet on ImageNet compared to baselines. Codes are released at https://github.com/yhhhli/SNN_Calibration.",
        "conference": "ICML",
        "中文标题": "来自人工神经网络的免费午餐：迈向高效、精确的脉冲神经网络校准",
        "摘要翻译": "脉冲神经网络（SNN）已被认为是下一代神经网络之一。传统上，SNN可以通过仅将ReLU激活替换为脉冲激活，同时保持参数不变，从预训练的人工神经网络（ANN）转换而来。或许令人惊讶的是，在这项工作中，我们展示了在ANN到SNN的转换过程中正确校准参数可以带来显著的改进。我们引入了SNN校准，这是一种廉价但极其有效的方法，通过利用预训练的人工神经网络（ANN）中的知识。从理论上分析转换误差及其在层间的传播开始，我们提出了可以逐层校正误差的校准算法。校准仅需少量训练数据和几分钟即可完成。此外，我们的校准算法可以在大规模ImageNet数据集上生成具有最先进架构的SNN，包括MobileNet和RegNet。大量实验证明了我们算法的有效性和效率。例如，与基线相比，我们的高级流程在ImageNet上转换MobileNet时可以将top-1准确率提高多达69%。代码发布于https://github.com/yhhhli/SNN_Calibration。",
        "领域": "脉冲神经网络、模型转换、图像分类",
        "问题": "如何高效且准确地将预训练的人工神经网络转换为脉冲神经网络",
        "动机": "探索在ANN到SNN转换过程中参数校准的重要性，以提高转换后SNN的性能",
        "方法": "提出了一种名为SNN校准的算法，通过分析转换误差及其传播，逐层校正误差，利用少量训练数据快速完成校准",
        "关键词": [
            "脉冲神经网络",
            "模型转换",
            "参数校准",
            "ImageNet",
            "MobileNet"
        ],
        "涉及的技术概念": {
            "脉冲神经网络（SNN）": "被认为是下一代神经网络之一，使用脉冲激活函数模拟生物神经元的行为",
            "模型转换": "将预训练的人工神经网络转换为脉冲神经网络的过程，旨在保持或提升模型性能",
            "参数校准": "在模型转换过程中调整参数以减少误差，提高转换后模型的准确率"
        },
        "success": true
    },
    {
        "order": 47,
        "title": "A Functional Perspective on Learning Symmetric Functions with Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10631",
        "abstract": "Symmetric functions, which take as input an unordered, fixed-size set, are known to be universally representable by neural networks that enforce permutation invariance.  These architectures only give guarantees for fixed input sizes, yet in many practical applications, including point clouds and particle physics, a relevant notion of generalization should include varying the input size.  In this work we treat symmetric functions (of any size) as functions over probability measures, and study the learning and representation of neural networks defined on measures.  By focusing on shallow architectures, we establish approximation and generalization bounds under different choices of regularization (such as RKHS and variation norms), that capture a hierarchy of functional spaces with increasing degree of non-linear learning. The resulting models can be learned efficiently and enjoy generalization guarantees that extend across input sizes, as we verify empirically. ",
        "conference": "ICML",
        "中文标题": "从功能视角探讨神经网络学习对称函数",
        "摘要翻译": "对称函数以无序、固定大小的集合作为输入，已知可以通过强制排列不变性的神经网络普遍表示。这些架构仅对固定输入大小提供保证，然而在许多实际应用中，包括点云和粒子物理学，泛化的相关概念应包括改变输入大小。在这项工作中，我们将（任何大小的）对称函数视为概率度量上的函数，并研究定义在度量上的神经网络的学习和表示。通过专注于浅层架构，我们在不同的正则化选择（如RKHS和变分范数）下建立了近似和泛化界限，这些界限捕捉了具有不断增加非线性学习程度的功能空间层次。由此产生的模型可以高效学习，并享有跨输入大小的泛化保证，正如我们通过实验验证的那样。",
        "领域": "函数逼近理论、深度学习理论、概率度量学习",
        "问题": "如何使神经网络在处理不同大小的输入时保持对称函数的泛化能力",
        "动机": "研究旨在解决神经网络在处理可变大小输入时的对称函数表示和泛化问题，特别是在点云和粒子物理学等实际应用中。",
        "方法": "通过将对称函数视为概率度量上的函数，研究浅层神经网络在特定正则化条件下的学习和表示能力，以实现在不同输入大小下的泛化。",
        "关键词": [
            "对称函数",
            "神经网络",
            "概率度量",
            "泛化保证",
            "非线性学习"
        ],
        "涉及的技术概念": {
            "排列不变性": "确保神经网络输出不因输入集合中元素的排列顺序而改变，是处理对称函数的关键特性。",
            "RKHS正则化": "使用再生核希尔伯特空间(RKHS)范数作为正则化手段，控制模型的复杂度，促进泛化。",
            "变分范数": "通过变分范数进行正则化，捕捉功能空间的非线性层次，增强模型对不同输入大小的适应能力。"
        },
        "success": true
    },
    {
        "order": 48,
        "title": "A General Framework For Detecting Anomalous Inputs to DNN Classifiers",
        "html": "https://ICML.cc//virtual/2021/poster/10151",
        "abstract": "Detecting anomalous inputs, such as adversarial and out-of-distribution (OOD) inputs, is critical for classifiers (including deep neural networks or DNNs) deployed in real-world applications. While prior works have proposed various methods to detect such anomalous samples using information from the internal layer representations of a DNN, there is a lack of consensus on a principled approach for the different components of such a detection method. As a result, often heuristic and one-off methods are applied for different aspects of this problem. We propose an unsupervised anomaly detection framework based on the internal DNN layer representations in the form of a meta-algorithm with configurable components. We proceed to propose specific instantiations for each component of the meta-algorithm based on ideas grounded in statistical testing and anomaly detection. We evaluate the proposed methods on well-known image classification datasets with strong adversarial attacks and OOD inputs, including an adaptive attack that uses the internal layer representations of the DNN (often not considered in prior work). Comparisons with five recently-proposed competing detection methods demonstrates the effectiveness of our method in detecting adversarial and OOD inputs.",
        "conference": "ICML",
        "中文标题": "深度神经网络分类器异常输入检测的通用框架",
        "摘要翻译": "检测异常输入，如对抗性和分布外（OOD）输入，对于实际应用中部署的分类器（包括深度神经网络或DNN）至关重要。虽然先前的工作提出了多种利用DNN内部层表示信息来检测此类异常样本的方法，但对于这种检测方法不同组件的原则性方法缺乏共识。因此，对于这个问题的不同方面，常常应用启发式和一次性方法。我们提出了一种基于DNN内部层表示的无监督异常检测框架，形式为一个可配置组件的元算法。接着，我们基于统计测试和异常检测的思想，为元算法的每个组件提出了具体的实例化。我们在著名的图像分类数据集上评估了所提出的方法，这些数据集包含强大的对抗攻击和OOD输入，包括一种利用DNN内部层表示的自适应攻击（在先前的工作中通常未被考虑）。与五种最近提出的竞争检测方法的比较证明了我们的方法在检测对抗性和OOD输入方面的有效性。",
        "领域": "异常检测、对抗性攻击防御、分布外检测",
        "问题": "缺乏一种原则性方法来检测深度神经网络分类器的异常输入，包括对抗性和分布外输入。",
        "动机": "为了解决现有方法在检测异常输入时缺乏共识和原则性方法的问题，提出一个通用的、可配置的检测框架。",
        "方法": "提出了一种基于DNN内部层表示的无监督异常检测框架，采用元算法形式，并基于统计测试和异常检测的思想为每个组件提供具体实例化。",
        "关键词": [
            "异常检测",
            "对抗性攻击",
            "分布外检测",
            "深度神经网络",
            "统计测试"
        ],
        "涉及的技术概念": {
            "元算法": "作为框架的基础，允许灵活配置不同的检测组件，以适应不同的检测需求。",
            "统计测试": "用于评估输入数据与正常分布的偏差，是检测异常输入的核心技术之一。",
            "无监督学习": "框架不依赖于标记的异常数据，能够自动识别和分类异常输入。"
        },
        "success": true
    },
    {
        "order": 49,
        "title": "AGENT: A Benchmark for Core Psychological Reasoning",
        "html": "https://ICML.cc//virtual/2021/poster/10211",
        "abstract": "For machine agents to successfully interact with humans in real-world settings, they will need to develop an understanding of human mental life. Intuitive psychology, the ability to reason about hidden mental variables that drive observable actions, comes naturally to people: even pre-verbal infants can tell agents from objects, expecting agents to act efficiently to achieve goals given constraints. Despite recent interest in machine agents that reason about other agents, it is not clear if such agents learn or hold the core psychology principles that drive human reasoning. Inspired by cognitive development studies on intuitive psychology, we present a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology. We validate AGENT with human-ratings, propose an evaluation protocol emphasizing generalization, and compare two strong baselines built on Bayesian inverse planning and a Theory of Mind neural network. Our results suggest that to pass the designed tests of core intuitive psychology at human levels, a model must acquire or have built-in representations of how agents plan, combining utility computations and core knowledge of objects and physics.",
        "conference": "ICML",
        "中文标题": "AGENT：核心心理推理基准",
        "摘要翻译": "为了让机器代理在现实世界中成功与人类互动，它们需要发展对人类心理生活的理解。直觉心理学，即推理驱动可观察行为的隐藏心理变量的能力，对人类来说是自然而然的：即使是前语言期的婴儿也能区分代理与对象，期望代理在给定约束条件下高效行动以实现目标。尽管最近对能够推理其他代理的机器代理产生了兴趣，但尚不清楚这些代理是否学习或持有驱动人类推理的核心心理学原则。受直觉心理学认知发展研究的启发，我们提出了一个基准，包括一个大型程序生成的3D动画数据集AGENT（行动、目标、效率、约束、效用），围绕四个场景（目标偏好、行动效率、未观察到的约束和成本-奖励权衡）构建，这些场景探究了核心直觉心理学的关键概念。我们通过人类评分验证了AGENT，提出了一个强调泛化的评估协议，并比较了基于贝叶斯逆规划和心智理论神经网络的两个强基线。我们的结果表明，要在人类水平上通过设计的核心直觉心理学测试，模型必须获得或内置代理如何规划的表征，结合效用计算和对象与物理的核心知识。",
        "领域": "心智理论、人机交互、认知建模",
        "问题": "机器代理是否能理解和应用驱动人类行为的核心心理学原则",
        "动机": "开发能够理解和预测人类行为的机器代理，以促进更自然的人机交互",
        "方法": "构建一个包含程序生成的3D动画的基准数据集AGENT，围绕四个核心心理学场景设计测试，并通过人类评分和两种基线模型（贝叶斯逆规划和心智理论神经网络）进行验证",
        "关键词": [
            "直觉心理学",
            "心智理论",
            "贝叶斯逆规划",
            "3D动画",
            "人机交互"
        ],
        "涉及的技术概念": {
            "直觉心理学": "研究人类如何自然地理解和预测他人行为背后的心理状态",
            "贝叶斯逆规划": "一种推理方法，用于从观察到的行为中推断出最可能的目标和约束条件",
            "心智理论神经网络": "一种神经网络架构，旨在模拟人类理解和预测他人心理状态的能力"
        },
        "success": true
    },
    {
        "order": 50,
        "title": "Aggregating From Multiple Target-Shifted Sources",
        "html": "https://ICML.cc//virtual/2021/poster/10163",
        "abstract": "Multi-source domain adaptation aims at leveraging the knowledge from multiple tasks for predicting a related target domain. Hence, a crucial aspect is to properly combine different sources based on their relations. In this paper, we analyzed the problem for aggregating source domains with different label distributions, where most recent source selection approaches fail. Our proposed algorithm differs from previous approaches in two key ways: the model aggregates multiple sources mainly through the similarity of semantic conditional distribution rather than marginal distribution; the model proposes a unified framework to select relevant sources for three popular scenarios, i.e., domain adaptation with limited label on target domain, unsupervised domain adaptation and label partial unsupervised domain adaption. We evaluate the proposed method through extensive experiments. The empirical results significantly outperform the baselines. ",
        "conference": "ICML",
        "中文标题": "从多个目标偏移源进行聚合",
        "摘要翻译": "多源领域适应的目的是利用来自多个任务的知识来预测相关的目标领域。因此，一个关键的方面是基于它们的关系正确地组合不同的源。在本文中，我们分析了聚合具有不同标签分布的源领域的问题，其中大多数最近的源选择方法都失败了。我们提出的算法在两个方面与之前的方法不同：模型主要通过语义条件分布的相似性而不是边缘分布来聚合多个源；模型提出了一个统一的框架，为三种流行场景选择相关源，即目标领域标签有限的领域适应、无监督领域适应和标签部分无监督领域适应。我们通过大量实验评估了所提出的方法。实证结果显著优于基线。",
        "领域": "领域适应、多源学习、语义条件分布",
        "问题": "如何有效地从具有不同标签分布的多个源领域聚合知识，以改善目标领域的预测性能。",
        "动机": "解决在多源领域适应中，现有方法在处理源领域间标签分布差异时的不足，提出一种更有效的聚合策略。",
        "方法": "通过语义条件分布的相似性聚合多个源，并提出一个统一的框架来适应不同的领域适应场景。",
        "关键词": [
            "多源领域适应",
            "语义条件分布",
            "标签分布差异",
            "统一框架",
            "领域适应场景"
        ],
        "涉及的技术概念": {
            "语义条件分布": "用于衡量源领域与目标领域在给定标签条件下的特征分布相似性，是聚合多个源的关键依据。",
            "统一框架": "一个灵活的框架，能够适应目标领域标签有限、无监督以及标签部分无监督三种不同的领域适应场景。",
            "多源领域适应": "利用多个相关但可能具有不同数据分布的源领域知识，以提高目标领域模型性能的技术。"
        },
        "success": true
    },
    {
        "order": 51,
        "title": "Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins",
        "html": "https://ICML.cc//virtual/2021/poster/10395",
        "abstract": "We analyze the properties of gradient descent on convex surrogates for the zero-one loss for the agnostic learning of halfspaces.  We show that when a quantity we refer to as the \\textit{soft margin} is well-behaved---a condition satisfied by log-concave isotropic distributions among others---minimizers of convex surrogates for the zero-one loss are approximate minimizers for the zero-one loss itself.  As standard convex optimization arguments lead to efficient guarantees for minimizing convex surrogates of the zero-one loss, our methods allow for the first positive guarantees for the classification error of halfspaces learned by gradient descent using the binary cross-entropy or hinge loss in the presence of agnostic label noise.",
        "conference": "ICML",
        "中文标题": "通过软边界的梯度下降进行半空间的不可知学习",
        "摘要翻译": "我们分析了梯度下降在零一损失凸替代上的性质，用于半空间的不可知学习。我们表明，当我们称之为软边界的量表现良好时——这一条件由对数凹各向同性分布等满足——零一损失的凸替代的最小化者也是零一损失本身的近似最小化者。由于标准的凸优化论证导致了对零一损失的凸替代最小化的有效保证，我们的方法首次为在存在不可知标签噪声的情况下，使用二元交叉熵或铰链损失通过梯度下降学习的半空间的分类错误提供了正面保证。",
        "领域": "机器学习理论、优化算法、分类算法",
        "问题": "在存在不可知标签噪声的情况下，如何有效地学习半空间分类器",
        "动机": "研究在不可知标签噪声环境下，通过梯度下降优化凸替代损失函数来学习半空间分类器的有效性",
        "方法": "分析梯度下降在零一损失的凸替代上的性质，利用软边界条件证明凸替代最小化者也是零一损失的近似最小化者",
        "关键词": [
            "梯度下降",
            "半空间学习",
            "不可知学习",
            "软边界",
            "凸优化"
        ],
        "涉及的技术概念": {
            "软边界": "用于衡量数据分布的性质，确保在满足特定条件时，凸替代损失的最小化能够有效近似零一损失的最小化",
            "零一损失的凸替代": "用于替代难以直接优化的零一损失，如二元交叉熵或铰链损失，以便于使用梯度下降等优化方法",
            "不可知标签噪声": "指在训练数据中存在的、与任何潜在模型无关的标签错误，研究在这种噪声下的学习算法鲁棒性"
        },
        "success": true
    },
    {
        "order": 52,
        "title": "A Gradient Based Strategy for Hamiltonian Monte Carlo Hyperparameter Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10755",
        "abstract": "Hamiltonian Monte Carlo (HMC) is one of the most successful sampling methods in machine learning. However, its performance is significantly affected by the choice of hyperparameter values. Existing approaches for optimizing the HMC hyperparameters either optimize a proxy for mixing speed or consider the HMC chain as an implicit variational distribution and optimize a tractable lower bound that can be very loose in practice. Instead, we propose to optimize an objective that quantifies directly the speed of convergence to the target distribution. Our objective can be easily optimized using stochastic gradient descent.  We evaluate our proposed method and compare to baselines on a variety of problems including sampling from synthetic 2D distributions, reconstructing sparse signals, learning deep latent variable models and sampling molecular configurations from the Boltzmann distribution of a 22 atom molecule. We find that our method is competitive with or improves upon alternative baselines in all these experiments.",
        "conference": "ICML",
        "中文标题": "基于梯度的哈密尔顿蒙特卡洛超参数优化策略",
        "摘要翻译": "哈密尔顿蒙特卡洛（HMC）是机器学习中最成功的采样方法之一。然而，其性能显著受到超参数值选择的影响。现有的优化HMC超参数的方法要么优化混合速度的代理，要么将HMC链视为隐式变分分布并优化一个在实践中可能非常宽松的可处理下界。相反，我们提出优化一个直接量化向目标分布收敛速度的目标。我们的目标可以很容易地使用随机梯度下降进行优化。我们评估了我们提出的方法，并在包括从合成2D分布采样、重建稀疏信号、学习深度潜变量模型以及从22原子分子的玻尔兹曼分布采样分子构型等多种问题上与基线进行了比较。我们发现，在所有实验中，我们的方法都与替代基线竞争或优于它们。",
        "领域": "贝叶斯机器学习, 概率图模型, 计算统计学",
        "问题": "优化哈密尔顿蒙特卡洛（HMC）方法的超参数选择，以提高其采样效率和收敛速度。",
        "动机": "现有的HMC超参数优化方法要么优化的是混合速度的代理，要么依赖于可能非常宽松的可处理下界，这限制了HMC方法的性能和应用范围。",
        "方法": "提出了一种新的优化目标，直接量化向目标分布的收敛速度，并使用随机梯度下降进行优化。",
        "关键词": [
            "哈密尔顿蒙特卡洛",
            "超参数优化",
            "随机梯度下降",
            "贝叶斯采样",
            "收敛速度"
        ],
        "涉及的技术概念": {
            "哈密尔顿蒙特卡洛": "一种高效的马尔可夫链蒙特卡洛方法，用于从复杂的概率分布中采样。",
            "超参数优化": "调整模型参数以改善性能的过程，这里特指HMC方法的参数优化。",
            "随机梯度下降": "一种优化算法，用于最小化目标函数，特别适用于大规模数据集。"
        },
        "success": true
    },
    {
        "order": 53,
        "title": "A Hybrid Variance-Reduced Method for Decentralized Stochastic Non-Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/8935",
        "abstract": "This paper considers decentralized stochastic optimization over a network of $n$ nodes, where each node possesses a smooth non-convex local cost function and the goal of the networked nodes is to find an $\\epsilon$-accurate first-order stationary point of the sum of the local costs. We focus on an online setting, where each node accesses its local cost only by means of a stochastic first-order oracle that returns a noisy version of the exact gradient. In this context, we propose a novel single-loop decentralized hybrid variance-reduced stochastic gradient method, called GT-HSGD, that outperforms the existing approaches in terms of both the oracle complexity and practical implementation. The GT-HSGD algorithm implements specialized local hybrid stochastic gradient estimators that are fused over the network to track the global gradient. Remarkably, GT-HSGD achieves a network topology-independent oracle complexity of $O(n^{-1}\\epsilon^{-3})$ when the required error tolerance $\\epsilon$ is small enough, leading to a linear speedup with respect to the centralized optimal online variance-reduced approaches that operate on a single node. Numerical experiments are provided to illustrate our main technical results.",
        "conference": "ICML",
        "success": true,
        "中文标题": "一种混合方差缩减方法用于去中心化随机非凸优化",
        "摘要翻译": "本文考虑在一个由n个节点组成的网络上的去中心化随机优化问题，其中每个节点拥有一个平滑的非凸局部成本函数，网络节点的目标是找到局部成本之和的一个ε精确的一阶稳定点。我们专注于在线设置，其中每个节点仅通过一个随机一阶预言机访问其局部成本，该预言机返回精确梯度的噪声版本。在此背景下，我们提出了一种新颖的单循环去中心化混合方差缩减随机梯度方法，称为GT-HSGD，该方法在预言机复杂度和实际实现方面均优于现有方法。GT-HSGD算法实现了专门的局部混合随机梯度估计器，这些估计器通过网络融合以跟踪全局梯度。值得注意的是，当所需的误差容忍度ε足够小时，GT-HSGD实现了与网络拓扑无关的预言机复杂度O(n^{-1}ϵ^{-3})，从而相对于在单个节点上运行的集中式最优在线方差缩减方法实现了线性加速。提供了数值实验以说明我们的主要技术结果。",
        "领域": "随机优化、去中心化学习、梯度下降算法",
        "问题": "解决去中心化网络中节点间协作进行随机非凸优化的问题，特别是在在线设置下，如何高效找到全局成本函数的一阶稳定点。",
        "动机": "现有的去中心化随机优化方法在预言机复杂度和实际应用效率上存在不足，需要一种更高效的方法来提升性能。",
        "方法": "提出了一种名为GT-HSGD的单循环去中心化混合方差缩减随机梯度方法，通过融合局部混合随机梯度估计器来跟踪全局梯度，实现网络拓扑无关的预言机复杂度。",
        "关键词": [
            "去中心化优化",
            "随机梯度下降",
            "方差缩减",
            "非凸优化",
            "在线学习"
        ],
        "涉及的技术概念": {
            "混合方差缩减": "结合不同的方差缩减技术以减少梯度估计的方差，提高优化效率。",
            "单循环算法": "一种简化算法结构，减少计算复杂度，提高实际应用中的执行效率。",
            "网络拓扑无关的复杂度": "算法的性能不受网络结构影响，保证了在不同网络配置下的稳定性和可扩展性。"
        }
    },
    {
        "order": 54,
        "title": "A Language for Counterfactual Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/9997",
        "abstract": "We present Omega, a probabilistic programming language with support for counterfactual inference. Counterfactual inference means to observe some fact in the present, and infer what would have happened had some past intervention been taken, e.g. ``given that medication was not effective at dose x, what is the probability that it would have been effective at dose 2x?.'' We accomplish this by introducing a new operator to probabilistic programming akin to Pearl's do, define its formal semantics, provide an implementation, and demonstrate its utility through examples in a variety of simulation models.",
        "conference": "ICML",
        "中文标题": "反事实生成模型的语言",
        "摘要翻译": "我们介绍了Omega，一种支持反事实推理的概率编程语言。反事实推理意味着观察当前的某些事实，并推断如果过去采取了某些干预措施会发生什么，例如‘给定药物在剂量x下无效，那么在剂量2x下有效的概率是多少？’。我们通过引入一种类似于Pearl的do操作符的新操作符来实现这一点，定义其形式语义，提供实现，并通过在各种模拟模型中的示例展示其实用性。",
        "领域": "概率编程语言、反事实推理、模拟模型",
        "问题": "如何在一个概率编程语言中实现反事实推理功能",
        "动机": "为了能够在一个统一的框架内进行反事实推理，从而在模拟模型中探索不同干预措施的效果",
        "方法": "引入新的操作符来支持反事实推理，定义其形式语义，并提供实现和示例展示",
        "关键词": [
            "概率编程语言",
            "反事实推理",
            "模拟模型",
            "干预措施",
            "形式语义"
        ],
        "涉及的技术概念": {
            "反事实推理": "在观察到某些事实后，推断如果过去采取了不同的干预措施会发生什么的技术",
            "概率编程语言": "一种允许开发者以概率和统计的方式表达不确定性和进行推理的编程语言",
            "形式语义": "为编程语言的操作符和行为提供严格的数学定义，确保其行为是可预测和可验证的"
        },
        "success": true
    },
    {
        "order": 55,
        "title": "A large-scale benchmark for few-shot program induction and synthesis",
        "html": "https://ICML.cc//virtual/2021/poster/9205",
        "abstract": "A landmark challenge for AI is to learn flexible, powerful\nrepresentations from small numbers of examples.\nOn an important class of tasks, hypotheses in the form of programs\nprovide extreme generalization capabilities from surprisingly few\nexamples. However, whereas large natural few-shot learning image\nbenchmarks have spurred progress in meta-learning for deep networks,\nthere is no comparably big, natural program-synthesis dataset that can\nplay a similar role. This is because, whereas images are relatively easy\nto label from internet meta-data or annotated by non-experts, generating\nmeaningful input-output examples for program induction has proven hard\nto scale. In this work, we propose a new way of leveraging unit tests\nand natural inputs for small programs as meaningful input-output\nexamples for each sub-program of the overall program. This allows us to\ncreate a large-scale naturalistic few-shot program-induction benchmark\nand propose new challenges in this domain. The evaluation of multiple program induction and synthesis algorithms points to shortcomings of current methods and suggests multiple avenues for future work.",
        "conference": "ICML",
        "中文标题": "大规模少样本程序归纳与合成的基准测试",
        "摘要翻译": "AI的一个里程碑式挑战是从少量例子中学习灵活、强大的表示。在一类重要任务上，以程序形式提出的假设能够从极少的例子中获得极端的泛化能力。然而，尽管大规模的自然少样本学习图像基准推动了深度网络元学习的进步，但缺乏一个可比的、大规模的自然程序合成数据集来发挥类似作用。这是因为，虽然图像相对容易通过互联网元数据标记或由非专家注释，但为程序归纳生成有意义的输入输出示例已被证明难以扩展。在这项工作中，我们提出了一种新方法，利用单元测试和小型程序的自然输入作为整体程序中每个子程序的有意义输入输出示例。这使我们能够创建一个大规模的自然主义少样本程序归纳基准，并提出了该领域的新挑战。对多种程序归纳和合成算法的评估指出了当前方法的不足，并为未来工作提出了多条路径。",
        "领域": "程序合成、元学习、少样本学习",
        "问题": "缺乏大规模的自然程序合成数据集来支持少样本程序归纳和合成的研究",
        "动机": "推动少样本程序归纳和合成领域的研究，通过创建大规模基准测试来评估和比较不同算法的性能",
        "方法": "利用单元测试和小型程序的自然输入作为整体程序中每个子程序的有意义输入输出示例，创建大规模的自然主义少样本程序归纳基准",
        "关键词": [
            "程序归纳",
            "程序合成",
            "少样本学习",
            "元学习",
            "基准测试"
        ],
        "涉及的技术概念": {
            "少样本学习": "从极少的例子中学习灵活、强大的表示，以解决缺乏大量标注数据的问题",
            "程序合成": "自动生成满足特定输入输出示例的程序，是AI领域的一个重要研究方向",
            "元学习": "学习如何学习，旨在提高模型在新任务上的学习效率和性能"
        },
        "success": true
    },
    {
        "order": 56,
        "title": "Align, then memorise: the dynamics of learning with feedback alignment",
        "html": "https://ICML.cc//virtual/2021/poster/8673",
        "abstract": "Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to backpropagation for training deep neural networks.  Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive.   Here,  we propose a theory of feedback alignment algorithms.  We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a net-work trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks.  Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorize process occurs sequentially from the bottom layers of the network to the top.",
        "conference": "ICML",
        "success": true,
        "中文标题": "对齐后记忆：反馈对齐学习动态",
        "摘要翻译": "直接反馈对齐（DFA）作为一种高效且生物可信的反向传播替代方案，正在被用于训练深度神经网络。尽管DFA依赖于随机反馈权重进行反向传递，它成功地训练了如Transformer等最先进的模型。然而，众所周知，它无法训练卷积网络。对于DFA内部工作机制以解释这些分歧结果的理解仍然难以捉摸。在此，我们提出了反馈对齐算法的理论。我们首先展示了在浅层网络中学习分为两个步骤：一个对齐阶段，模型调整其权重以使近似梯度与损失函数的真实梯度对齐；随后是一个记忆阶段，模型专注于拟合数据。这一两步过程具有退化打破效应：在所有的低损失解中，用DFA训练的神经网络自然地收敛于最大化梯度对齐的解。我们还确定了深度线性网络中对齐的一个关键量：对齐矩阵的条件数。后者使对数据结构对齐影响的详细理解成为可能，并为DFA无法训练卷积神经网络的众所周知失败提供了一个简单的解释。在MNIST和CIFAR10上的数值实验清楚地展示了深度非线性网络中的退化打破，并显示了对齐后记忆过程从网络的底层到顶层顺序发生。",
        "领域": "深度学习优化算法, 神经网络训练, 生物启发计算",
        "问题": "理解直接反馈对齐（DFA）算法在不同类型神经网络中表现差异的原因，特别是其在卷积神经网络中训练失败的原因。",
        "动机": "探索DFA作为一种替代反向传播的高效且生物可信的神经网络训练方法的内在机制，以解释其在某些网络架构中成功而在其他架构中失败的现象。",
        "方法": "提出反馈对齐算法的理论，通过分析浅层和深层网络中的学习过程，识别影响DFA性能的关键因素，如对齐矩阵的条件数，并通过数值实验验证理论。",
        "关键词": [
            "直接反馈对齐",
            "神经网络训练",
            "梯度对齐",
            "卷积神经网络",
            "退化打破"
        ],
        "涉及的技术概念": {
            "直接反馈对齐（DFA）": "一种替代传统反向传播的神经网络训练方法，使用随机反馈权重进行误差信号的传递。",
            "梯度对齐": "在DFA训练过程中，模型调整权重以使近似梯度与真实梯度对齐的过程，是学习的关键步骤之一。",
            "对齐矩阵条件数": "影响深度线性网络中对齐效果的关键量，决定了数据结构和网络架构对DFA训练效果的影响。"
        }
    },
    {
        "order": 57,
        "title": "Almost Optimal Anytime Algorithm for Batched Multi-Armed Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/8491",
        "abstract": "In batched multi-armed bandit problems, the learner can adaptively pull arms and adjust strategy in batches. In many real applications, not only the regret but also the batch complexity need to be optimized. Existing batched bandit algorithms usually assume that the time horizon T is known in advance. However, many applications involve an unpredictable stopping time. In this paper, we study the anytime batched multi-armed bandit problem. We propose an anytime algorithm that achieves the asymptotically optimal regret for exponential families of reward distributions with O(\\log \\log T \\ilog^{\\alpha} (T)) \\footnote{Notation \\ilog^{\\alpha} (T) is the result of iteratively applying the logarithm function on T for \\alpha times, e.g., \\ilog^{3} (T)=\\log\\log\\log T.} batches, where $\\alpha\\in O_{T}(1). Moreover, we prove that for any constant c>0, no algorithm can achieve the asymptotically optimal regret within c\\log\\log T batches.",
        "conference": "ICML",
        "中文标题": "近乎最优的批量多臂老虎机问题随时算法",
        "摘要翻译": "在批量多臂老虎机问题中，学习者可以自适应地拉动臂并在批次中调整策略。在许多实际应用中，不仅需要优化遗憾，还需要优化批次复杂度。现有的批量老虎机算法通常假设时间范围T是预先已知的。然而，许多应用涉及不可预测的停止时间。在本文中，我们研究了随时批量多臂老虎机问题。我们提出了一种随时算法，该算法以O(log log T ilog^α(T))批次实现了奖励分布指数族的渐近最优遗憾，其中α∈O_T(1)。此外，我们证明对于任何常数c>0，没有算法可以在c log log T批次内实现渐近最优遗憾。",
        "领域": "强化学习、在线学习、优化算法",
        "问题": "在未知停止时间的批量多臂老虎机问题中，如何同时优化遗憾和批次复杂度。",
        "动机": "解决现有批量老虎机算法需要预先知道时间范围T的限制，适应实际应用中不可预测的停止时间需求。",
        "方法": "提出了一种随时算法，通过自适应地调整策略和批次大小，实现了在未知停止时间情况下的渐近最优遗憾和批次复杂度。",
        "关键词": [
            "批量多臂老虎机",
            "随时算法",
            "遗憾优化",
            "批次复杂度",
            "指数族分布"
        ],
        "涉及的技术概念": {
            "批量多臂老虎机": "一种强化学习问题，学习者通过批次方式选择动作（拉动臂）来最大化累积奖励。",
            "随时算法": "能够在任何时间点提供解决方案的算法，适用于停止时间不可预测的场景。",
            "指数族分布": "一类概率分布，用于描述奖励的统计特性，算法针对这类分布实现了渐近最优的遗憾。"
        },
        "success": true
    },
    {
        "order": 58,
        "title": "A Lower Bound for the Sample Complexity of Inverse Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9833",
        "abstract": "Inverse reinforcement learning (IRL) is the task of finding a reward function that generates a desired optimal policy for a given Markov Decision Process (MDP). This paper develops an information-theoretic lower bound for the sample complexity of the finite state, finite action IRL problem. A geometric construction of $\\beta$-strict separable IRL problems using spherical codes is considered. Properties of the ensemble size as well as the Kullback-Leibler divergence between the generated trajectories are derived. The resulting ensemble is then used along with Fano's inequality to derive a sample complexity lower bound of $O(n \\log n)$, where $n$ is the number of states in the MDP.",
        "conference": "ICML",
        "中文标题": "逆强化学习样本复杂度的下界",
        "摘要翻译": "逆强化学习（IRL）的任务是为给定的马尔可夫决策过程（MDP）找到一个能生成期望最优策略的奖励函数。本文为有限状态、有限动作的IRL问题的样本复杂度开发了一个信息论下界。考虑了使用球面码构建的β-严格可分离IRL问题的几何构造。推导了集合大小以及生成轨迹之间的Kullback-Leibler散度的性质。然后，利用所得集合和Fano不等式推导出样本复杂度的下界为O(n log n)，其中n是MDP中的状态数。",
        "领域": "逆强化学习、马尔可夫决策过程、信息论",
        "问题": "为有限状态、有限动作的逆强化学习问题确定样本复杂度的下界。",
        "动机": "理解逆强化学习任务中样本复杂度的基本限制，以便更有效地设计和评估IRL算法。",
        "方法": "通过几何构造β-严格可分离IRL问题，利用球面码和Kullback-Leibler散度性质，结合Fano不等式推导样本复杂度下界。",
        "关键词": [
            "逆强化学习",
            "样本复杂度",
            "马尔可夫决策过程",
            "信息论下界",
            "Fano不等式"
        ],
        "涉及的技术概念": {
            "逆强化学习": "一种从观察到的行为中推断出潜在奖励函数的技术，用于理解或模仿智能体的决策过程。",
            "马尔可夫决策过程": "一个用于建模决策过程的数学框架，其中未来的状态仅依赖于当前状态和采取的动作。",
            "Fano不等式": "信息论中的一个不等式，用于推导估计问题中的下界，特别是在样本复杂度分析中。"
        },
        "success": true
    },
    {
        "order": 59,
        "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence",
        "html": "https://ICML.cc//virtual/2021/poster/9165",
        "abstract": "Weight-sharing neural architecture search (NAS) is an effective technique for automating efficient neural architecture design. Weight-sharing NAS builds a supernet that assembles all the architectures as its sub-networks and jointly trains the supernet with the sub-networks. The success of weight-sharing NAS heavily relies on distilling the knowledge of the supernet to the sub-networks. However, we find that the widely used distillation divergence, i.e., KL divergence, may lead to student sub-networks that over-estimate or under-estimate the uncertainty of the teacher supernet, leading to inferior performance of the sub-networks. In this work, we propose to improve the supernet training with a more generalized alpha-divergence. By adaptively selecting the alpha-divergence, we simultaneously prevent the over-estimation or under-estimation of the uncertainty of the teacher model. We apply the proposed alpha-divergence based supernets training to both slimmable neural networks and weight-sharing NAS, and demonstrate significant improvements. Specifically, our discovered model family, AlphaNet, outperforms prior-art models on a wide range of FLOPs regimes, including BigNAS, Once-for-All networks, and AttentiveNAS. We achieve ImageNet top-1 accuracy of 80.0% with only 444M FLOPs. Our code and pretrained models are available at https://github.com/facebookresearch/AlphaNet.\n",
        "conference": "ICML",
        "中文标题": "AlphaNet：使用Alpha散度改进超级网络的训练",
        "摘要翻译": "权重共享神经架构搜索（NAS）是一种自动化高效神经架构设计的有效技术。权重共享NAS构建了一个超级网络，将所有架构作为其子网络组装起来，并与子网络共同训练超级网络。权重共享NAS的成功很大程度上依赖于将超级网络的知识提炼到子网络中。然而，我们发现广泛使用的蒸馏散度，即KL散度，可能导致学生子网络高估或低估教师超级网络的不确定性，从而导致子网络的性能不佳。在这项工作中，我们提出使用更广义的alpha散度来改进超级网络的训练。通过自适应选择alpha散度，我们同时防止了对教师模型不确定性的高估或低估。我们将提出的基于alpha散度的超级网络训练应用于可瘦身神经网络和权重共享NAS，并展示了显著的改进。具体来说，我们发现的模型家族AlphaNet，在广泛的FLOPs范围内优于现有技术模型，包括BigNAS、Once-for-All网络和AttentiveNAS。我们仅用444M FLOPs就实现了ImageNet top-1准确率80.0%。我们的代码和预训练模型可在https://github.com/facebookresearch/AlphaNet获取。",
        "领域": "神经架构搜索、模型压缩、深度学习优化",
        "问题": "解决权重共享神经架构搜索中KL散度导致子网络性能不佳的问题",
        "动机": "改进超级网络训练，防止子网络对教师模型不确定性的高估或低估",
        "方法": "提出使用广义的alpha散度自适应选择，改进超级网络的训练",
        "关键词": [
            "Alpha散度",
            "权重共享NAS",
            "超级网络训练",
            "模型压缩",
            "神经架构搜索"
        ],
        "涉及的技术概念": {
            "Alpha散度": "用于改进超级网络训练，防止子网络对教师模型不确定性的高估或低估",
            "权重共享NAS": "一种自动化高效神经架构设计的技术，通过构建超级网络来共同训练所有子网络",
            "超级网络训练": "通过训练一个包含所有可能子网络的超级网络，来优化神经架构搜索过程"
        },
        "success": true
    },
    {
        "order": 60,
        "title": "Alternative Microfoundations for Strategic Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9047",
        "abstract": "When reasoning about strategic behavior in a machine learning context it is tempting to combine standard microfoundations of rational agents with the statistical decision theory underlying classification. In this work, we argue that a direct combination of these ingredients leads to brittle solution concepts of limited descriptive and prescriptive value. First, we show that rational agents with perfect information produce discontinuities in the aggregate response to a decision rule that we often do not observe empirically. Second, when any positive fraction of agents is not perfectly strategic, desirable stable points---where the classifier is optimal for the data it entails---no longer exist. Third, optimal decision rules under standard microfoundations maximize a measure of negative externality known as social burden within a broad class of assumptions about agent behavior. Recognizing these limitations we explore alternatives to standard microfoundations for binary classification. We describe desiderata that help navigate the space of possible assumptions about agent responses, and we then propose the noisy response model. Inspired by smoothed analysis and empirical observations, noisy response incorporates imperfection in the agent responses, which we show mitigates the limitations of standard microfoundations. Our model retains analytical tractability, leads to more robust insights about stable points, and imposes a lower social burden at optimality.",
        "conference": "ICML",
        "success": true,
        "中文标题": "战略分类的替代微观基础",
        "摘要翻译": "在机器学习背景下推理战略行为时，很容易将理性代理的标准微观基础与分类的统计决策理论结合起来。在这项工作中，我们认为这些要素的直接结合导致了脆弱的解决方案概念，其描述性和规范性价值有限。首先，我们展示了具有完美信息的理性代理在对决策规则的总体响应中产生了我们通常在实证中观察不到的间断性。其次，当任何正比例的代理不是完全战略性时，期望的稳定点——分类器对其所包含的数据是最优的——不再存在。第三，在标准微观基础下，最优决策规则在关于代理行为的一大类假设中最大化了一种称为社会负担的负外部性度量。认识到这些限制，我们探索了二进制分类标准微观基础的替代方案。我们描述了有助于导航关于代理响应可能假设空间的期望特性，然后我们提出了噪声响应模型。受平滑分析和实证观察的启发，噪声响应包含了代理响应中的不完美性，我们展示了这减轻了标准微观基础的局限性。我们的模型保留了分析的可操作性，导致关于稳定点的更稳健的见解，并在最优性时施加了较低的社会负担。",
        "领域": "机器学习、战略分类、决策理论",
        "问题": "解决在机器学习中结合理性代理的微观基础和统计决策理论导致的脆弱解决方案概念问题",
        "动机": "探索标准微观基础的替代方案，以克服其在战略分类中的局限性",
        "方法": "提出噪声响应模型，该模型结合了代理响应中的不完美性，以减轻标准微观基础的局限性",
        "关键词": [
            "战略分类",
            "微观基础",
            "噪声响应模型",
            "社会负担",
            "稳定点"
        ],
        "涉及的技术概念": {
            "战略分类": "研究在代理可能战略性响应的情况下进行分类的问题",
            "噪声响应模型": "一种考虑了代理响应不完美性的模型，旨在提供更稳健的解决方案",
            "社会负担": "衡量决策规则在最优性时施加的负外部性的度量"
        }
    },
    {
        "order": 61,
        "title": "A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network",
        "html": "https://ICML.cc//virtual/2021/poster/10149",
        "abstract": "Incorporating a so-called ``momentum'' dynamic in gradient descent methods is widely used in neural net training as it has been broadly observed that, at least empirically, it often leads to significantly faster convergence. At the same time, there are very few theoretical guarantees in the literature to explain this apparent acceleration effect. Even for the classical strongly convex quadratic problems, several existing results only show Polyak's momentum has an accelerated linear rate asymptotically. In this paper, we first revisit the quadratic problems and show a non-asymptotic accelerated linear rate of Polyak's momentum. Then, we provably show that Polyak's momentum achieves acceleration for training a one-layer wide ReLU network and a deep linear network, which are perhaps the two most popular canonical models for studying optimization and deep learning in the literature. Prior works (Du et al. 2019) and (Wu et al. 2019) showed that using vanilla gradient descent, and with an use of over-parameterization, the error decays as $(1- \\Theta(\\frac{1}{ \\kappa'}))^t$ after $t$ iterations, where $\\kappa'$ is the condition number of a Gram Matrix. Our result shows that with the appropriate choice of parameters Polyak's momentum has a rate of $(1-\\Theta(\\frac{1}{\\sqrt{\\kappa'}}))^t$. For the deep linear network, prior work (Hu et al. 2020) showed that  vanilla gradient descent has a rate of  $(1-\\Theta(\\frac{1}{\\kappa}))^t$, where $\\kappa$ is the condition number of a data matrix. Our result shows an acceleration rate $(1- \\Theta(\\frac{1}{\\sqrt{\\kappa}}))^t$ is achievable by Polyak's momentum. This work establishes that momentum does indeed speed up neural net training.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于Polyak动量的可证明加速的模块化分析：训练一个宽ReLU网络和一个深度线性网络",
        "摘要翻译": "在梯度下降方法中引入所谓的“动量”动态被广泛应用于神经网络训练中，因为经验观察表明，它通常能显著加快收敛速度。然而，文献中很少有理论保证能够解释这种明显的加速效应。即使对于经典的强凸二次问题，一些现有的结果也只表明Polyak动量具有渐近的加速线性收敛速率。在本文中，我们首先重新审视二次问题，并展示Polyak动量的非渐近加速线性收敛速率。然后，我们可证明地表明，Polyak动量在训练一个单层宽ReLU网络和一个深度线性网络时实现了加速，这可能是文献中研究优化和深度学习的两个最流行的典型模型。先前的工作表明，使用香草梯度下降，并且使用过度参数化，误差在t次迭代后以(1-Θ(1/κ'))^t衰减，其中κ'是格拉姆矩阵的条件数。我们的结果表明，通过适当选择参数，Polyak动量的收敛速率为(1-Θ(1/√κ'))^t。对于深度线性网络，先前的工作表明，香草梯度下降的收敛速率为(1-Θ(1/κ))^t，其中κ是数据矩阵的条件数。我们的结果表明，Polyak动量可以实现加速速率(1-Θ(1/√κ))^t。这项工作确立了动量确实可以加速神经网络训练。",
        "领域": "优化算法、神经网络训练、理论分析",
        "问题": "现有的理论分析无法充分解释动量方法在神经网络训练中的加速效果，尤其是在非渐近情况下的收敛速率。",
        "动机": "为了从理论上解释动量方法在神经网络训练中的加速效应，并提供更强的收敛性保证，尤其是在实际训练中常见的非渐近情况下。",
        "方法": "通过对强凸二次问题进行重新审视，推导出Polyak动量的非渐近加速线性收敛速率。然后，将分析扩展到单层宽ReLU网络和深度线性网络，证明Polyak动量可以实现加速训练。",
        "关键词": [
            "Polyak动量",
            "加速收敛",
            "ReLU网络",
            "深度线性网络",
            "非渐近分析"
        ],
        "涉及的技术概念": {
            "Polyak动量": "一种梯度下降优化算法的变体，通过引入动量项来加速收敛，尤其适用于病态优化问题。",
            "非渐近分析": "与渐近分析不同，非渐近分析关注算法在有限次迭代后的性能表现，提供更实际的收敛性保证。"
        }
    },
    {
        "order": 62,
        "title": "Amortized Conditional Normalized Maximum Likelihood: Reliable Out of Distribution Uncertainty Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9305",
        "abstract": "While deep neural networks provide good performance for a range of challenging tasks, calibration and uncertainty estimation remain major challenges, especially under distribution shift. In this paper, we propose the amortized conditional normalized maximum likelihood (ACNML) method as a scalable general-purpose approach for uncertainty estimation, calibration, and out-of-distribution robustness with deep networks. Our algorithm builds on the conditional normalized maximum likelihood (CNML) coding scheme, which has minimax optimal properties according to the minimum description length principle, but is computationally intractable to evaluate exactly for all but the simplest of model classes. \nWe propose to use approximate Bayesian inference technqiues to produce a tractable approximation to the CNML distribution. Our approach can be combined with any approximate inference algorithm that provides tractable posterior densities over model parameters. We demonstrate that ACNML compares favorably to a number of prior techniques for uncertainty estimation in terms of calibration when faced with distribution shift.",
        "conference": "ICML",
        "中文标题": "摊销条件归一化最大似然：可靠的分布外不确定性估计",
        "摘要翻译": "尽管深度神经网络在一系列具有挑战性的任务中提供了良好的性能，校准和不确定性估计仍然是主要挑战，尤其是在分布变化的情况下。在本文中，我们提出了摊销条件归一化最大似然（ACNML）方法，作为一种可扩展的通用方法，用于深度网络的不确定性估计、校准和分布外鲁棒性。我们的算法建立在条件归一化最大似然（CNML）编码方案的基础上，该方案根据最小描述长度原则具有极小极大最优性质，但对于除最简单的模型类别外的所有模型类别，计算上难以精确评估。我们提出使用近似贝叶斯推断技术来产生CNML分布的可处理近似。我们的方法可以与任何提供模型参数可处理后验密度的近似推断算法结合使用。我们证明，在面对分布变化时，ACNML在校准方面优于许多先前的不确定性估计技术。",
        "领域": "不确定性估计、深度学习校准、分布鲁棒性",
        "问题": "深度神经网络在分布变化下的校准和不确定性估计问题",
        "动机": "提高深度神经网络在面对分布变化时的校准能力和不确定性估计的可靠性",
        "方法": "提出摊销条件归一化最大似然（ACNML）方法，结合近似贝叶斯推断技术，实现对CNML分布的可处理近似",
        "关键词": [
            "不确定性估计",
            "深度网络校准",
            "分布鲁棒性",
            "ACNML",
            "近似贝叶斯推断"
        ],
        "涉及的技术概念": {
            "条件归一化最大似然（CNML）": "一种编码方案，根据最小描述长度原则具有极小极大最优性质，用于不确定性估计",
            "摊销条件归一化最大似然（ACNML）": "本文提出的方法，通过摊销技术使CNML计算变得可行，用于提高不确定性估计的效率和准确性",
            "近似贝叶斯推断": "用于产生CNML分布的可处理近似，使得方法可以与各种近似推断算法结合使用"
        },
        "success": true
    },
    {
        "order": 63,
        "title": "An Algorithm for Stochastic and Adversarial Bandits with Switching Costs",
        "html": "https://ICML.cc//virtual/2021/poster/9467",
        "abstract": "We propose an algorithm for stochastic and adversarial multiarmed bandits with switching costs, where the algorithm pays a price $\\lambda$ every time it switches the arm being played. Our algorithm is based on adaptation of the Tsallis-INF algorithm of Zimmert and Seldin (2021) and requires no prior knowledge of the regime or time horizon. In the oblivious adversarial setting it achieves the minimax optimal regret bound of $ O( (\\lambda K)^{1/3}T^{2/3} + \\sqrt{KT})$, where $T$ is the time horizon and $K$ is the number of arms. In the stochastically constrained adversarial regime, which includes the stochastic regime as a special case, it achieves a regret bound of $O((\\lambda K)^{2/3} T^{1/3} + \\ln T)\\sum_{i \\neq i^*} \\Delta_i^{-1})$, where $\\Delta_i$ are suboptimality gaps and $i^*$ is the unique optimal arm. In the special case of $\\lambda = 0$ (no switching costs), both bounds are minimax optimal within constants.  We also explore variants of the problem, where switching cost is allowed to change over time. We provide experimental evaluation showing competitiveness of our algorithm with the relevant baselines in the stochastic, stochastically constrained adversarial, and adversarial regimes with fixed switching cost.",
        "conference": "ICML",
        "中文标题": "一种带有切换成本的随机和对抗性多臂老虎机算法",
        "摘要翻译": "我们提出了一种用于带有切换成本的随机和对抗性多臂老虎机的算法，其中算法每次切换所玩的手臂时都需要支付价格λ。我们的算法基于对Zimmert和Seldin（2021）的Tsallis-INF算法的适应，并且不需要预先了解机制或时间范围。在无知的对抗性设置中，它实现了最小最大最优的遗憾界限O((λK)^(1/3)T^(2/3) + √(KT))，其中T是时间范围，K是手臂的数量。在随机约束的对抗性机制中，包括随机机制作为特例，它实现了遗憾界限O((λK)^(2/3)T^(1/3) + ln T)Σ_{i≠i*}Δ_i^(-1))，其中Δ_i是次优性差距，i*是唯一的最优手臂。在λ=0（无切换成本）的特例中，两个界限在常数范围内都是最小最大最优的。我们还探讨了问题的变体，其中切换成本允许随时间变化。我们提供了实验评估，显示了我们的算法在随机、随机约束对抗性和固定切换成本的对抗性机制中与相关基线的竞争力。",
        "领域": "强化学习、在线学习、多臂老虎机问题",
        "问题": "解决在带有切换成本的随机和对抗性多臂老虎机问题中实现最小最大最优遗憾界限的问题",
        "动机": "为了在不需要预先了解机制或时间范围的情况下，有效处理带有切换成本的多臂老虎机问题，提出一种适应性强的算法",
        "方法": "基于Tsallis-INF算法的适应，无需预先了解机制或时间范围，适用于随机和对抗性设置",
        "关键词": [
            "多臂老虎机",
            "切换成本",
            "Tsallis-INF算法",
            "对抗性学习",
            "随机学习"
        ],
        "涉及的技术概念": {
            "Tsallis-INF算法": "一种用于多臂老虎机问题的算法，通过适应此算法来处理带有切换成本的情况",
            "最小最大最优遗憾界限": "在对抗性和随机性设置中，算法能够达到的理论最优性能界限",
            "切换成本": "算法在切换选择的手臂时需要支付的额外成本，影响算法的整体性能"
        },
        "success": true
    },
    {
        "order": 64,
        "title": "Analysis of stochastic Lanczos quadrature for spectrum approximation",
        "html": "https://ICML.cc//virtual/2021/poster/9115",
        "abstract": "The cumulative empirical spectral measure (CESM) $\\Phi[\\mathbf{A}] : \\mathbb{R} \\to [0,1]$ of a $n\\times n$ symmetric matrix $\\mathbf{A}$ is defined as the fraction of eigenvalues of $\\mathbf{A}$ less than a given threshold, i.e., $\\Phi[\\mathbf{A}](x) := \\sum_{i=1}^{n} \\frac{1}{n} {\\large\\unicode{x1D7D9}}[ \\lambda_i[\\mathbf{A}]\\leq x]$. Spectral sums $\\operatorname{tr}(f[\\mathbf{A}])$ can be computed as the Riemann--Stieltjes integral of $f$ against $\\Phi[\\mathbf{A}]$, so the task of estimating CESM arises frequently in a number of applications, including machine learning. We present an error analysis for stochastic Lanczos quadrature (SLQ). We show that SLQ obtains an approximation to the CESM within a Wasserstein distance of $t \\: | \\lambda_{\\text{max}}[\\mathbf{A}] - \\lambda_{\\text{min}}[\\mathbf{A}] |$ with probability at least $1-\\eta$, by applying the Lanczos algorithm for $\\lceil 12 t^{-1} + \\frac{1}{2} \\rceil$ iterations to $\\lceil 4 ( n+2 )^{-1}t^{-2} \\ln(2n\\eta^{-1}) \\rceil$ vectors sampled independently and uniformly from the unit sphere. We additionally provide (matrix-dependent) a posteriori error bounds for the Wasserstein and Kolmogorov--Smirnov distances between the output of this algorithm and the true CESM. The quality of our bounds is demonstrated using numerical experiments.",
        "conference": "ICML",
        "中文标题": "随机Lanczos求积法在谱近似中的分析",
        "摘要翻译": "累积经验谱测度（CESM）Φ[A]：R→[0,1]对于一个n×n的对称矩阵A，定义为A的特征值小于给定阈值的比例，即Φ[A](x) := ∑_{i=1}^{n} 1/n 1[λ_i[A]≤x]。谱和tr(f[A])可以计算为f对Φ[A]的Riemann-Stieltjes积分，因此在包括机器学习在内的许多应用中，估计CESM的任务频繁出现。我们对随机Lanczos求积法（SLQ）进行了误差分析。我们表明，SLQ通过将Lanczos算法应用于从单位球面独立且均匀采样的⌈4(n+2)^{-1}t^{-2}ln(2nη^{-1})⌉向量，进行⌈12t^{-1}+1/2⌉次迭代，以至少1-η的概率，在Wasserstein距离t|λ_max[A]-λ_min[A]|内获得CESM的近似值。我们还提供了（矩阵依赖的）后验误差界限，用于该算法输出与真实CESM之间的Wasserstein和Kolmogorov-Smirnov距离。我们的界限质量通过数值实验得到了验证。",
        "领域": "数值线性代数, 机器学习, 谱分析",
        "问题": "如何高效准确地估计对称矩阵的累积经验谱测度（CESM）",
        "动机": "在机器学习和其它应用中，需要频繁估计对称矩阵的谱和，这需要通过CESM来实现，因此开发一个高效准确的估计方法具有重要的实际意义。",
        "方法": "采用随机Lanczos求积法（SLQ），通过Lanczos算法对从单位球面采样的向量进行迭代，以估计CESM，并提供了误差分析和后验误差界限。",
        "关键词": [
            "随机Lanczos求积法",
            "累积经验谱测度",
            "谱和",
            "Wasserstein距离",
            "Kolmogorov-Smirnov距离"
        ],
        "涉及的技术概念": {
            "随机Lanczos求积法（SLQ）": "一种用于估计对称矩阵谱和的方法，通过Lanczos算法迭代处理随机采样的向量来近似CESM。",
            "累积经验谱测度（CESM）": "描述对称矩阵特征值分布的函数，用于计算谱和。",
            "Wasserstein距离": "用于衡量两个概率分布之间差异的度量，本文中用于评估SLQ近似CESM的误差。"
        },
        "success": true
    },
    {
        "order": 65,
        "title": "Analyzing the tree-layer structure of Deep Forests",
        "html": "https://ICML.cc//virtual/2021/poster/9653",
        "abstract": "Random forests on the one hand, and neural networks on the other hand, have met great success in the machine learning community for their predictive performance. Combinations of both have been proposed in the literature, notably leading to the so-called deep forests (DF) (Zhou & Feng,2019). In this paper, our aim is not to benchmark DF performances but to investigate instead their underlying mechanisms. Additionally, DF architecture can be generally simplified into more simple and computationally efficient shallow forest networks. Despite some instability, the latter may outperform standard predictive tree-based methods. We exhibit a theoretical framework in which a shallow tree network is shown to enhance the performance of classical decision trees. In such a setting, we provide tight theoretical lower and upper bounds on its excess risk. These theoretical results show the interest of tree-network architectures for well-structured data provided that the first layer, acting as a data encoder, is rich enough.",
        "conference": "ICML",
        "中文标题": "分析深度森林的三层结构",
        "摘要翻译": "随机森林和神经网络在机器学习社区中因其预测性能而取得了巨大成功。文献中提出了两者的结合，特别是导致了所谓的深度森林（DF）（Zhou & Feng，2019）。在本文中，我们的目标不是对DF的性能进行基准测试，而是研究其潜在机制。此外，DF架构通常可以简化为更简单且计算效率更高的浅层森林网络。尽管存在一些不稳定性，后者可能优于标准的基于树的预测方法。我们展示了一个理论框架，其中浅层树网络被证明可以增强经典决策树的性能。在这样的设置中，我们提供了其超额风险的严格理论下限和上限。这些理论结果表明，只要第一层（作为数据编码器）足够丰富，树网络架构对于结构良好的数据是有意义的。",
        "领域": "机器学习模型融合、决策树算法、深度学习理论",
        "问题": "研究深度森林（DF）的潜在机制及其简化架构的性能",
        "动机": "探索深度森林架构的简化形式及其在提升经典决策树性能方面的潜力",
        "方法": "通过理论分析，展示浅层树网络如何增强决策树性能，并提供超额风险的理论界限",
        "关键词": [
            "深度森林",
            "浅层森林网络",
            "决策树",
            "超额风险",
            "理论界限"
        ],
        "涉及的技术概念": {
            "深度森林（DF）": "结合随机森林和神经网络特性的机器学习模型，用于提升预测性能",
            "浅层森林网络": "深度森林架构的简化形式，旨在提高计算效率同时保持或提升性能",
            "超额风险": "衡量模型性能与最优性能之间差异的指标，本文提供了其理论界限"
        },
        "success": true
    },
    {
        "order": 66,
        "title": "An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming",
        "html": "https://ICML.cc//virtual/2021/poster/8721",
        "abstract": "Predicting molecular conformations (or 3D structures) from molecular graphs is a fundamental problem in many applications. Most existing approaches are usually divided into two steps by first predicting the distances between atoms and then generating a 3D structure through optimizing a distance geometry problem. However, the distances predicted with such two-stage approaches may not be able to consistently preserve the geometry of local atomic neighborhoods, making the generated structures unsatisfying. In this paper, we propose an end-to-end solution for molecular conformation prediction called ConfVAE based on the conditional variational autoencoder framework. Specifically, the molecular graph is first encoded in a latent space, and then the 3D structures are generated by solving a principled bilevel optimization program. Extensive experiments on several benchmark data sets prove the effectiveness of our proposed approach over existing state-of-the-art approaches.  Code is available at \\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.",
        "conference": "ICML",
        "中文标题": "基于双层规划的分子构象生成端到端框架",
        "摘要翻译": "从分子图预测分子构象（或3D结构）是许多应用中的基本问题。大多数现有方法通常分为两个步骤，首先预测原子之间的距离，然后通过优化距离几何问题生成3D结构。然而，这种两阶段方法预测的距离可能无法一致地保留局部原子邻域的几何形状，使得生成的结构不尽如人意。在本文中，我们提出了一种基于条件变分自编码器框架的分子构象预测端到端解决方案，称为ConfVAE。具体而言，分子图首先在潜在空间中被编码，然后通过解决一个原则性的双层优化程序生成3D结构。在几个基准数据集上的大量实验证明了我们提出的方法相对于现有最先进方法的有效性。代码可在https://github.com/MinkaiXu/ConfVAE-ICML21获取。",
        "领域": "分子构象预测、化学信息学、深度学习在化学中的应用",
        "问题": "解决从分子图预测分子3D结构时，现有两阶段方法无法一致保留局部原子邻域几何形状的问题。",
        "动机": "为了提高分子构象预测的准确性和效率，减少因两阶段方法导致的几何不一致性。",
        "方法": "提出了一种基于条件变分自编码器框架的端到端解决方案ConfVAE，通过双层优化程序直接从分子图生成3D结构。",
        "关键词": [
            "分子构象预测",
            "条件变分自编码器",
            "双层优化",
            "端到端学习",
            "化学信息学"
        ],
        "涉及的技术概念": {
            "条件变分自编码器": "用于在潜在空间中编码分子图，作为生成3D结构的基础框架。",
            "双层优化程序": "用于从编码的潜在表示中生成满足几何一致性的3D分子结构。",
            "端到端学习": "直接从分子图到3D结构的端到端预测流程，避免了传统两阶段方法的几何不一致问题。"
        },
        "success": true
    },
    {
        "order": 67,
        "title": "A New Formalism, Method and Open Issues for Zero-Shot Coordination",
        "html": "https://ICML.cc//virtual/2021/poster/9983",
        "abstract": "In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible.  Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this “label-free” problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game.  We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective.  We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it is optimal in the LFC problem and an equilibrium in the LFC game. Since arbitrary tie-breaking is precisely what the ZSC setting aims to prevent, we conclude that the LFC problem does not reflect the aims of ZSC. To address this, we introduce an alternative informal operationalization of ZSC as a starting point for future work.",
        "conference": "ICML",
        "中文标题": "零样本协调的新形式化方法、方法及开放性问题",
        "摘要翻译": "在许多协调问题中，独立推理的人类能够发现相互兼容的策略。相比之下，独立训练的自我对弈策略往往相互不兼容。零样本协调（ZSC）最近被提出作为多智能体强化学习中的一个新前沿，以解决这一基本问题。先前的工作通过假设玩家可以就共享的学习算法达成一致，但不能就动作和观察的标签达成一致，来接近ZSC问题，并提出其他对弈作为最优解决方案。然而，直到现在，这个“无标签”问题只是被非正式地定义。我们通过定义无标签协调游戏，将这个设置形式化为无标签协调（LFC）问题。我们表明，其他对弈不是LFC问题的最优解决方案，因为它未能一致地打破其他对弈目标的不兼容最大化者之间的平局。我们引入了该算法的扩展，即带有平局打破的其他对弈，并证明它在LFC问题中是最优的，并且在LFC游戏中是一个均衡。由于任意平局打破正是ZSC设置旨在防止的，我们得出结论，LFC问题没有反映ZSC的目标。为了解决这个问题，我们引入了ZSC的另一种非正式操作化作为未来工作的起点。",
        "领域": "多智能体强化学习、零样本学习、协调问题",
        "问题": "解决在多智能体系统中独立训练的智能体策略相互不兼容的问题",
        "动机": "探索在多智能体环境中如何实现无需预先协调的策略兼容性",
        "方法": "提出并形式化无标签协调问题，引入带有平局打破的其他对弈算法作为解决方案",
        "关键词": [
            "零样本协调",
            "多智能体强化学习",
            "无标签协调",
            "策略兼容性",
            "其他对弈"
        ],
        "涉及的技术概念": {
            "零样本协调（ZSC）": "旨在解决多智能体系统中独立训练的策略相互不兼容的问题",
            "无标签协调（LFC）": "形式化了无需预先协调标签的协调问题，作为ZSC的一个具体实例",
            "其他对弈": "一种在多智能体强化学习中用于促进策略兼容性的算法，通过假设玩家可以共享学习算法但不能共享动作和观察的标签"
        },
        "success": true
    },
    {
        "order": 68,
        "title": "A New Representation of Successor Features for Transfer across Dissimilar Environments",
        "html": "https://ICML.cc//virtual/2021/poster/10753",
        "abstract": "Transfer in reinforcement learning is usually achieved through generalisation across tasks. Whilst many studies have investigated transferring knowledge when the reward function changes, they have assumed that the dynamics of the environments remain consistent. Many real-world RL problems require transfer among environments with different dynamics. To address this problem, we propose an approach based on successor features in which we model successor feature functions with Gaussian Processes permitting the source successor features to be treated as noisy measurements of the target successor feature function. Our theoretical analysis proves the convergence of this approach as well as the bounded error on modelling successor feature functions with Gaussian Processes in\nenvironments with both different dynamics and rewards. We demonstrate our method on benchmark datasets and show that it outperforms current baselines.",
        "conference": "ICML",
        "中文标题": "一种用于不同环境间迁移的后继特征新表示方法",
        "摘要翻译": "在强化学习中，迁移通常通过任务间的泛化实现。虽然许多研究探讨了奖励函数变化时的知识迁移，但它们假设环境动态保持一致。许多现实世界的强化学习问题需要在动态不同的环境间进行迁移。为解决这一问题，我们提出了一种基于后继特征的方法，其中我们使用高斯过程对后继特征函数建模，允许将源后继特征视为目标后继特征函数的噪声测量。我们的理论分析证明了这种方法在动态和奖励均不同的环境中的收敛性，以及使用高斯过程建模后继特征函数时的误差界限。我们在基准数据集上展示了我们的方法，并显示其优于当前基线。",
        "领域": "强化学习",
        "问题": "解决在动态不同的环境间进行知识迁移的问题",
        "动机": "现实世界的强化学习问题常常需要在动态变化的环境中进行有效的知识迁移",
        "方法": "提出了一种基于后继特征和高斯过程的方法，用于在不同动态的环境间进行知识迁移",
        "关键词": [
            "强化学习",
            "后继特征",
            "高斯过程",
            "知识迁移",
            "动态环境"
        ],
        "涉及的技术概念": {
            "后继特征": "用于表示在特定策略下状态或状态-动作对的未来累积特征，是实现知识迁移的关键",
            "高斯过程": "用于建模后继特征函数，允许将源环境的特征视为目标环境的噪声测量",
            "知识迁移": "在不同但相关的任务或环境间传递学习到的知识，以提高学习效率和性能"
        },
        "success": true
    },
    {
        "order": 69,
        "title": "An exact solver for the Weston-Watkins SVM subproblem",
        "html": "https://ICML.cc//virtual/2021/poster/10195",
        "abstract": "Recent empirical evidence suggests that the Weston-Watkins support vector machine is among the best performing multiclass extensions of the binary SVM. Current state-of-the-art solvers repeatedly solve a particular subproblem approximately using an iterative strategy. In this work, we propose an algorithm that solves the subproblem exactly using a novel reparametrization of the Weston-Watkins dual problem. For linear WW-SVMs, our solver shows significant speed-up over the state-of-the-art solver when the number of classes is large. Our exact subproblem solver also allows us to prove linear convergence of the overall solver.",
        "conference": "ICML",
        "中文标题": "Weston-Watkins SVM子问题的精确求解器",
        "摘要翻译": "最近的实证研究表明，Weston-Watkins支持向量机是二进制SVM多类扩展中性能最佳之一。当前最先进的求解器使用迭代策略反复近似解决特定子问题。在这项工作中，我们提出了一种算法，该算法通过Weston-Watkins对偶问题的新颖重新参数化精确解决了子问题。对于线性WW-SVM，当类别数量较大时，我们的求解器显示出比最先进求解器显著的速度提升。我们的精确子问题求解器还使我们能够证明整体求解器的线性收敛性。",
        "领域": "多类分类、支持向量机、优化算法",
        "问题": "解决Weston-Watkins支持向量机子问题的精确求解问题",
        "动机": "提高多类分类问题中Weston-Watkins支持向量机的求解效率和准确性",
        "方法": "通过重新参数化Weston-Watkins对偶问题，提出一种精确求解子问题的算法",
        "关键词": [
            "Weston-Watkins SVM",
            "多类分类",
            "精确求解",
            "优化算法",
            "线性收敛"
        ],
        "涉及的技术概念": {
            "Weston-Watkins SVM": "一种多类支持向量机的扩展，用于处理多类分类问题",
            "重新参数化": "通过对偶问题的重新表述，使得子问题能够被精确求解",
            "线性收敛": "算法在迭代过程中以线性速率收敛到最优解"
        },
        "success": true
    },
    {
        "order": 70,
        "title": "An Identifiable Double VAE  For Disentangled Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10249",
        "abstract": "A large part of the literature on learning disentangled representations focuses on variational autoencoders (VAEs). Recent developments demonstrate that disentanglement cannot be obtained in a fully unsupervised setting without inductive biases on models and data. However, Khemakhem et al., AISTATS, 2020 suggest that employing a particular form of factorized prior, conditionally dependent on auxiliary variables complementing input observations, can be one such bias, resulting in an identifiable model with guarantees on disentanglement. Working along this line, we propose a novel VAE-based generative model with theoretical guarantees on identifiability. We obtain our conditional prior over the latents by learning an optimal representation, which imposes an additional strength on their regularization. We also extend our method to semi-supervised settings. Experimental results indicate superior performance with respect to state-of-the-art approaches, according to several established metrics proposed in the literature on disentanglement.",
        "conference": "ICML",
        "中文标题": "可识别的双重变分自编码器用于解耦表示",
        "摘要翻译": "关于学习解耦表示的大部分文献集中在变分自编码器（VAEs）上。最近的发展表明，在没有对模型和数据引入归纳偏置的情况下，无法在完全无监督的设置中获得解耦。然而，Khemakhem等人在AISTATS 2020中提出，采用一种特定形式的因子化先验，条件依赖于补充输入观测的辅助变量，可以作为一种偏置，从而产生一个具有解耦保证的可识别模型。沿着这一思路，我们提出了一种基于VAE的新型生成模型，具有理论上的可识别性保证。我们通过学习一个最优表示来获得我们的条件先验，这在其正则化上施加了额外的强度。我们还将我们的方法扩展到半监督设置。实验结果表明，根据文献中提出的几种已建立的解耦度量，我们的方法在性能上优于最先进的方法。",
        "领域": "解耦表示学习",
        "问题": "在无监督和半监督设置下学习解耦表示",
        "动机": "探索在变分自编码器中引入特定形式的因子化先验，以提高解耦表示的可识别性和性能",
        "方法": "提出了一种基于VAE的新型生成模型，通过学习最优表示来获得条件先验，并扩展到半监督设置",
        "关键词": [
            "解耦表示",
            "变分自编码器",
            "可识别性",
            "半监督学习",
            "因子化先验"
        ],
        "涉及的技术概念": {
            "变分自编码器（VAEs）": "用于学习解耦表示的基础生成模型",
            "因子化先验": "条件依赖于辅助变量的特定形式先验，用于提高模型的可识别性和解耦性能",
            "半监督学习": "扩展方法到半监督设置，利用有限的标注数据提高解耦表示的学习效果"
        },
        "success": true
    },
    {
        "order": 71,
        "title": "An Information-Geometric Distance on the Space of Tasks",
        "html": "https://ICML.cc//virtual/2021/poster/10479",
        "abstract": "This paper prescribes a distance between learning tasks modeled as joint distributions on data and labels. Using tools in information geometry, the distance is defined to be the length of the shortest weight trajectory on a Riemannian manifold as a classifier is fitted on an interpolated task. The interpolated task evolves from the source to the target task using an optimal transport formulation. This distance, which we call the 'coupled transfer distance' can be compared across different classifier architectures. We develop an algorithm to compute the distance which iteratively transports the marginal on the data of the source task to that of the target task while updating the weights of the classifier to track this evolving data distribution. We develop theory to show that our distance captures the intuitive idea that a good transfer trajectory is the one that keeps the generalization gap small during transfer, in particular at the end on the target task. We perform thorough empirical validation and analysis across diverse image classification datasets to show that the coupled transfer distance correlates strongly with the difficulty of fine-tuning.",
        "conference": "ICML",
        "中文标题": "任务空间上的信息几何距离",
        "摘要翻译": "本文提出了一种将学习任务建模为数据和标签联合分布之间的距离度量。利用信息几何的工具，该距离被定义为在黎曼流形上，当分类器在插值任务上拟合时，最短权重轨迹的长度。插值任务通过最优传输公式从源任务演变为目标任务。我们称这种距离为‘耦合传输距离’，它可以在不同的分类器架构之间进行比较。我们开发了一种算法来计算这一距离，该算法迭代地将源任务数据的边缘传输到目标任务数据的边缘，同时更新分类器的权重以跟踪这一演变的数据分布。我们提出的理论表明，我们的距离捕捉了一个直观的想法，即一个好的传输轨迹是在传输过程中保持泛化差距较小的轨迹，特别是在目标任务结束时。我们通过多样化的图像分类数据集进行了彻底的实证验证和分析，结果表明耦合传输距离与微调的难度有很强的相关性。",
        "领域": "迁移学习, 信息几何, 图像分类",
        "问题": "如何量化不同学习任务之间的距离，以评估迁移学习的难度。",
        "动机": "为了提供一个可以跨不同分类器架构比较的任务距离度量，以更好地理解和优化迁移学习过程。",
        "方法": "利用信息几何和最优传输理论，定义一个基于黎曼流形上最短权重轨迹的任务距离，并开发算法计算这一距离。",
        "关键词": [
            "信息几何",
            "最优传输",
            "迁移学习",
            "黎曼流形",
            "任务距离"
        ],
        "涉及的技术概念": {
            "信息几何": "用于在概率分布空间上定义距离和几何结构，为任务距离提供理论基础。",
            "最优传输": "用于描述和计算从源任务到目标任务的最优演变路径，是定义耦合传输距离的关键。",
            "黎曼流形": "提供了一个几何框架，使得可以在分类器权重空间上定义和计算最短路径，即任务距离。"
        },
        "success": true
    },
    {
        "order": 72,
        "title": "An Integer Linear Programming Framework for Mining Constraints from Data",
        "html": "https://ICML.cc//virtual/2021/poster/9731",
        "abstract": " Structured output prediction problems (e.g., sequential tagging, hierarchical multi-class classification) often involve constraints over the output space. These constraints interact with the learned models to filter infeasible solutions and facilitate in building an accountable system. However, despite constraints are useful, they are often based on hand-crafted rules. This raises a question -- can we mine constraints and rules from data based on a learning algorithm?\n    \n    In this paper, we present a general framework for mining constraints from data. In particular, we consider the inference in structured output prediction as an integer linear programming (ILP) problem. Then, given the coefficients of the objective function and the corresponding solution, we mine the underlying constraints by estimating the outer and inner polytopes of the feasible set. We verify the proposed constraint mining algorithm in various synthetic and real-world applications and demonstrate that the proposed approach successfully identifies the feasible set at scale. \n    In particular, we show that our approach can learn to solve 9x9 Sudoku puzzles and minimal spanning tree problems from examples without providing the underlying rules. Our algorithm can also integrate with a neural network model to learn the hierarchical label structure of a multi-label classification task. Besides, we provide theoretical analysis about the tightness of the polytopes and the reliability of the mined constraints.",
        "conference": "ICML",
        "中文标题": "一种从数据中挖掘约束的整数线性规划框架",
        "摘要翻译": "结构化输出预测问题（如序列标注、层次多类分类）通常涉及对输出空间的约束。这些约束与学习到的模型交互，以过滤不可行的解决方案，并有助于建立一个负责任的系统。然而，尽管约束是有用的，但它们通常基于手工制定的规则。这就提出了一个问题——我们能否基于学习算法从数据中挖掘约束和规则？在本文中，我们提出了一个从数据中挖掘约束的通用框架。特别是，我们将结构化输出预测中的推理视为一个整数线性规划（ILP）问题。然后，给定目标函数的系数和相应的解，我们通过估计可行集的外部和内部多面体来挖掘潜在的约束。我们在各种合成和实际应用中验证了提出的约束挖掘算法，并证明所提出的方法能够成功地大规模识别可行集。特别是，我们展示了我们的方法可以从例子中学习解决9x9数独谜题和最小生成树问题，而无需提供底层规则。我们的算法还可以与神经网络模型集成，以学习多标签分类任务的层次标签结构。此外，我们还提供了关于多面体紧密度和挖掘约束可靠性的理论分析。",
        "领域": "结构化预测、整数线性规划、多标签分类",
        "问题": "如何从数据中自动挖掘结构化输出预测中的约束，而非依赖手工制定的规则。",
        "动机": "减少对手工制定规则的依赖，通过数据驱动的方法自动发现和利用结构化输出预测中的约束。",
        "方法": "将结构化输出预测中的推理视为整数线性规划问题，通过估计可行集的外部和内部多面体来挖掘潜在的约束。",
        "关键词": [
            "整数线性规划",
            "约束挖掘",
            "结构化预测",
            "多标签分类",
            "可行集估计"
        ],
        "涉及的技术概念": {
            "整数线性规划（ILP）": "用于将结构化输出预测中的推理问题形式化为数学优化问题，以便挖掘约束。",
            "可行集的多面体估计": "通过估计可行集的外部和内部多面体来识别和挖掘潜在的约束。",
            "层次标签结构学习": "通过与神经网络模型集成，学习多标签分类任务中的层次结构，以提高分类性能。"
        },
        "success": true
    },
    {
        "order": 73,
        "title": "Annealed Flow Transport Monte Carlo",
        "html": "https://ICML.cc//virtual/2021/poster/10537",
        "abstract": "Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC) extensions are state-of-the-art methods for estimating normalizing constants of probability distributions. We propose here a novel Monte Carlo algorithm, Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them with normalizing flows (NFs) for improved performance. This method transports a set of particles using not only importance sampling (IS), Markov chain Monte Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are learned sequentially to push particles towards the successive annealed targets. We provide limit theorems for the resulting Monte Carlo estimates of the normalizing constant and expectations with respect to the target distribution. Additionally, we show that a continuous-time scaling limit of the population version of AFT is given by a Feynman--Kac measure which simplifies to the law of a controlled diffusion for expressive NFs. We demonstrate experimentally the benefits and limitations of our methodology on a variety of applications.",
        "conference": "ICML",
        "中文标题": "退火流传输蒙特卡洛",
        "摘要翻译": "退火重要性采样（AIS）及其序列蒙特卡洛（SMC）扩展是估计概率分布归一化常数的最先进方法。本文提出了一种新颖的蒙特卡洛算法——退火流传输（AFT），该算法基于AIS和SMC，并结合了归一化流（NFs）以提高性能。该方法不仅使用了重要性采样（IS）、马尔可夫链蒙特卡洛（MCMC）和重采样步骤——如SMC中那样，还依赖于顺序学习的NFs来推动粒子向连续的退火目标前进。我们为归一化常数和目标分布期望的蒙特卡洛估计提供了极限定理。此外，我们还展示了AFT群体版本的连续时间缩放极限由Feynman-Kac测度给出，对于表达性NFs，该测度简化为受控扩散的定律。我们通过实验展示了我们方法在各种应用中的优势和局限性。",
        "领域": "概率图模型, 蒙特卡洛方法, 归一化流",
        "问题": "提高概率分布归一化常数估计的效率和准确性",
        "动机": "结合归一化流技术改进现有的退火重要性采样和序列蒙特卡洛方法，以更有效地估计复杂概率分布的归一化常数和期望",
        "方法": "提出退火流传输（AFT）算法，结合重要性采样、马尔可夫链蒙特卡洛、重采样步骤和顺序学习的归一化流，以推动粒子向目标分布前进",
        "关键词": [
            "退火重要性采样",
            "序列蒙特卡洛",
            "归一化流",
            "蒙特卡洛估计",
            "Feynman-Kac测度"
        ],
        "涉及的技术概念": {
            "退火重要性采样": "用于估计概率分布归一化常数的技术，通过一系列中间分布逐步接近目标分布",
            "归一化流": "一种可逆的神经网络架构，用于建模复杂的概率分布，能够有效地进行密度估计和采样",
            "Feynman-Kac测度": "在连续时间极限下描述粒子系统演化的数学工具，用于分析AFT算法的理论性质"
        },
        "success": true
    },
    {
        "order": 74,
        "title": "A Novel Method to Solve Neural Knapsack Problems",
        "html": "https://ICML.cc//virtual/2021/poster/10715",
        "abstract": "0-1 knapsack is of fundamental importance across many fields. In this paper, we present a game-theoretic method to solve 0-1 knapsack problems (KPs) where the number of items (products) is large and the values of items are not predetermined but decided by an external value assignment function (e.g., a neural network in our case) during the optimization process. While existing papers are interested in predicting solutions with neural networks for classical KPs whose objective functions are mostly linear functions, we are interested in solving KPs whose objective functions are neural networks. In other words, we choose a subset of items that maximize the sum of the values predicted by neural networks. Its key challenge is how to optimize the neural network-based non-linear KP objective with a budget constraint. Our solution is inspired by game-theoretic approaches in deep learning, e.g., generative adversarial networks. After formally defining our two-player game, we develop an adaptive gradient ascent method to solve it. In our experiments, our method successfully solves two neural network-based non-linear KPs and conventional linear KPs with 1 million items.",
        "conference": "ICML",
        "中文标题": "解决神经背包问题的新方法",
        "摘要翻译": "0-1背包问题在许多领域具有基础重要性。本文提出了一种博弈论方法来解决0-1背包问题（KPs），其中物品（产品）数量众多，且物品的价值不是预先确定的，而是在优化过程中由外部价值分配函数（例如，在我们的案例中是一个神经网络）决定。虽然现有论文对预测经典KPs的解决方案感兴趣，这些KPs的目标函数大多是线性函数，但我们对解决目标函数是神经网络的KPs感兴趣。换句话说，我们选择一组物品，使得神经网络预测的价值总和最大化。其关键挑战是如何在预算约束下优化基于神经网络的非线性KP目标。我们的解决方案受到深度学习中博弈论方法的启发，例如生成对抗网络。在正式定义我们的双玩家游戏后，我们开发了一种自适应梯度上升方法来解决它。在我们的实验中，我们的方法成功解决了两个基于神经网络的非线性KPs和具有100万物品的常规线性KPs。",
        "领域": "优化算法、神经网络应用、博弈论",
        "问题": "如何在预算约束下优化基于神经网络的非线性背包问题目标函数",
        "动机": "解决目标函数为神经网络的背包问题，特别是在物品数量众多且价值由外部函数动态决定的情况下",
        "方法": "采用博弈论方法，特别是受到生成对抗网络启发的双玩家游戏框架，开发自适应梯度上升方法",
        "关键词": [
            "神经背包问题",
            "博弈论方法",
            "自适应梯度上升",
            "非线性优化",
            "大规模物品"
        ],
        "涉及的技术概念": {
            "博弈论方法": "用于解决背包问题的框架，特别是通过双玩家游戏来优化目标函数",
            "自适应梯度上升": "一种优化技术，用于在博弈论框架内调整参数以最大化目标函数",
            "神经网络": "用于动态决定物品价值的外部价值分配函数，使得背包问题的目标函数非线性化"
        },
        "success": true
    },
    {
        "order": 75,
        "title": "A Novel Sequential Coreset Method for Gradient Descent Algorithms",
        "html": "https://ICML.cc//virtual/2021/poster/10767",
        "abstract": "A wide range of optimization problems arising in machine learning can be solved by gradient descent algorithms, and a central question in this area is how to efficiently compress a large-scale dataset so as to reduce the computational complexity. Coreset is a popular data compression technique that has been extensively studied before. However, most of existing coreset methods are problem-dependent and cannot be used as a general tool for a broader range of applications. A key obstacle is that they often rely on the pseudo-dimension and total sensitivity bound that can be very high or hard to obtain. In this paper, based on the ``locality'' property of gradient descent algorithms, we propose a new framework, termed ``sequential coreset'', which effectively avoids these obstacles. Moreover, our method is particularly suitable for sparse optimization whence the coreset size can be further reduced to be only poly-logarithmically dependent on the dimension. In practice, the experimental results suggest that our method can save a large amount of running time compared with the baseline algorithms.",
        "conference": "ICML",
        "中文标题": "一种新颖的梯度下降算法顺序核心集方法",
        "摘要翻译": "机器学习中出现的广泛优化问题可以通过梯度下降算法解决，这一领域的核心问题是如何高效压缩大规模数据集以降低计算复杂度。核心集是一种流行的数据压缩技术，之前已被广泛研究。然而，现有的大多数核心集方法都是问题依赖的，不能作为更广泛应用范围的通用工具。一个关键障碍是它们往往依赖于伪维度和总敏感度界限，这些可能非常高或难以获得。在本文中，基于梯度下降算法的“局部性”特性，我们提出了一个新框架，称为“顺序核心集”，有效避免了这些障碍。此外，我们的方法特别适合稀疏优化，此时核心集大小可以进一步减少到仅对维度有多对数依赖。在实践中，实验结果表明，与基线算法相比，我们的方法可以节省大量运行时间。",
        "领域": "优化算法、机器学习、稀疏优化",
        "问题": "如何高效压缩大规模数据集以降低梯度下降算法的计算复杂度",
        "动机": "现有核心集方法多为问题依赖，难以作为通用工具应用于更广泛的范围，且依赖于难以获得的高伪维度和总敏感度界限",
        "方法": "基于梯度下降算法的局部性特性，提出顺序核心集框架，避免依赖伪维度和总敏感度界限，特别适用于稀疏优化",
        "关键词": [
            "梯度下降",
            "核心集",
            "稀疏优化",
            "计算复杂度",
            "数据压缩"
        ],
        "涉及的技术概念": {
            "顺序核心集": "一种新的核心集框架，基于梯度下降算法的局部性特性，避免依赖伪维度和总敏感度界限",
            "伪维度": "在现有核心集方法中用于衡量数据集复杂度的概念，但往往难以获得或非常高",
            "总敏感度界限": "现有核心集方法中用于衡量数据点对整体解的影响的指标，计算复杂或难以准确估计"
        },
        "success": true
    },
    {
        "order": 76,
        "title": "A Nullspace Property for Subspace-Preserving Recovery",
        "html": "https://ICML.cc//virtual/2021/poster/10703",
        "abstract": "Much of the theory for classical sparse recovery is based on conditions on the dictionary that are both necessary and sufficient (e.g., nullspace property) or only sufficient (e.g., incoherence and restricted isometry). In contrast, much of the theory for subspace-preserving recovery, the theoretical underpinnings for sparse subspace classification and clustering methods, is based on conditions on the subspaces and the data that are only sufficient (e.g., subspace incoherence and data inner-radius). This paper derives a necessary and sufficient condition for subspace-preserving recovery that is inspired by the classical nullspace property.Based on this novel condition, called here the subspace nullspace property, we derive equivalent characterizations that either admit a clear geometric interpretation that relates data distribution and subspace separation to the recovery success, or can be verified using a finite set of extreme points of a properly defined set. We further exploit these characterizations to derive new sufficient conditions, based on inner-radius and outer-radius measures and dual bounds, that generalize existing conditions and preserve the geometric interpretations.  These results fill an important gap in the subspace-preserving recovery literature.",
        "conference": "ICML",
        "中文标题": "子空间保持恢复的零空间性质",
        "摘要翻译": "经典稀疏恢复的理论大多基于字典上的条件，这些条件既是必要的也是充分的（例如，零空间性质）或仅是充分的（例如，不相干性和限制等距性）。相比之下，子空间保持恢复的理论，即稀疏子空间分类和聚类方法的理论基础，大多基于子空间和数据的条件，这些条件仅是充分的（例如，子空间不相干性和数据内半径）。本文受经典零空间性质的启发，推导出了一个既必要又充分的子空间保持恢复条件。基于这一新颖的条件，我们称之为子空间零空间性质，我们推导出了等价的表征，这些表征要么允许一个清晰的几何解释，将数据分布和子空间分离与恢复成功联系起来，要么可以使用适当定义的集合的有限极端点集来验证。我们进一步利用这些表征来推导新的充分条件，基于内半径和外半径度量以及对偶界限，这些条件概括了现有条件并保留了几何解释。这些结果填补了子空间保持恢复文献中的一个重要空白。",
        "领域": "稀疏表示",
        "问题": "如何为子空间保持恢复提供一个既必要又充分的条件",
        "动机": "填补子空间保持恢复理论中缺乏必要且充分条件的空白",
        "方法": "基于经典零空间性质的启发，提出子空间零空间性质，并推导出等价的表征和新的充分条件",
        "关键词": [
            "子空间保持恢复",
            "零空间性质",
            "稀疏表示"
        ],
        "涉及的技术概念": {
            "子空间零空间性质": "本文提出的一个既必要又充分的子空间保持恢复条件",
            "内半径和外半径度量": "用于推导新的充分条件，概括现有条件并保留几何解释",
            "对偶界限": "在推导新的充分条件时使用，帮助保留几何解释"
        },
        "success": true
    },
    {
        "order": 77,
        "title": "A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10153",
        "abstract": "A fundamental challenge in multiagent reinforcement learning is to learn beneficial behaviors in a shared environment with other simultaneously learning agents. In particular, each agent perceives the environment as effectively non-stationary due to the changing policies of other agents. Moreover, each agent is itself constantly learning, leading to natural non-stationarity in the distribution of experiences encountered. In this paper, we propose a novel meta-multiagent policy gradient theorem that directly accounts for the non-stationary policy dynamics inherent to multiagent learning settings. This is achieved by modeling our gradient updates to consider both an agent’s own non-stationary policy dynamics and the non-stationary policy dynamics of other agents in the environment. We show that our theoretically grounded approach provides a general solution to the multiagent learning problem, which inherently comprises all key aspects of previous state of the art approaches on this topic. We test our method on a diverse suite of multiagent benchmarks and demonstrate a more efficient ability to adapt to new agents as they learn than baseline methods across the full spectrum of mixed incentive, competitive, and cooperative domains.",
        "conference": "ICML",
        "中文标题": "多智能体强化学习中学习学习的策略梯度算法",
        "摘要翻译": "多智能体强化学习中的一个基本挑战是在与其他同时学习的智能体共享的环境中学习有益行为。特别是，由于其他智能体策略的变化，每个智能体感知到的环境实际上是非平稳的。此外，每个智能体自身也在不断学习，导致所遇到的经验分布自然具有非平稳性。在本文中，我们提出了一种新颖的元多智能体策略梯度定理，直接考虑了多智能体学习环境中固有的非平稳策略动态。这是通过建模我们的梯度更新来实现的，该更新考虑了智能体自身的非平稳策略动态以及环境中其他智能体的非平稳策略动态。我们表明，我们的理论方法为多智能体学习问题提供了一个通用解决方案，该方案本质上包含了这一主题上先前最先进方法的所有关键方面。我们在多样化的多智能体基准测试套件上测试了我们的方法，并展示了在混合激励、竞争和合作领域的全谱范围内，比基线方法更有效地适应新智能体学习的能力。",
        "领域": "多智能体强化学习、策略梯度方法、元学习",
        "问题": "解决多智能体强化学习中的非平稳性问题，即智能体在共享环境中学习时，由于其他智能体策略的变化导致的环境非平稳性。",
        "动机": "在多智能体环境中，智能体需要适应其他智能体的学习过程，这导致环境表现出非平稳性。研究动机是开发一种能够直接考虑这种非平稳性的方法，以提高智能体在共享环境中的学习效率和适应性。",
        "方法": "提出了一种元多智能体策略梯度定理，通过建模梯度更新来考虑智能体自身和其他智能体的非平稳策略动态，从而提供了一种通用的多智能体学习解决方案。",
        "关键词": [
            "多智能体强化学习",
            "策略梯度",
            "元学习",
            "非平稳性",
            "适应性学习"
        ],
        "涉及的技术概念": {
            "元多智能体策略梯度定理": "直接考虑多智能体学习环境中固有的非平稳策略动态的策略梯度定理。",
            "非平稳策略动态": "由于智能体不断学习导致的环境策略变化，是多智能体强化学习中的核心挑战。",
            "适应性学习": "智能体能够根据环境中其他智能体的学习动态调整自身策略的能力，是本方法的关键优势。"
        },
        "success": true
    },
    {
        "order": 78,
        "title": "Approximate Group Fairness for Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/10725",
        "abstract": "We incorporate group fairness into the algorithmic centroid clustering problem, where $k$ centers are to be located to serve $n$ agents distributed in a metric space. We refine the notion of proportional fairness proposed in [Chen et al.,  ICML 2019] as {\\em core fairness}. A $k$-clustering is in the core if no coalition containing at least $n/k$ agents can strictly decrease their total distance by deviating to a new center together. Our solution concept is motivated by the situation where agents are able to coordinate and utilities are transferable. A string of existence, hardness and approximability results is provided. Particularly,  we propose two dimensions to relax core requirements: one is on the degree of distance improvement, and the other is on the size of deviating coalition. For both relaxations and their combination, we study the extent to which relaxed core fairness can be satisfied in metric spaces including line, tree and general metric space, and design approximation algorithms accordingly. We also conduct experiments on synthetic and real-world data to examine the performance of our algorithms.",
        "conference": "ICML",
        "中文标题": "聚类中的近似群体公平性",
        "摘要翻译": "我们将群体公平性引入到算法中心聚类问题中，其中需要定位k个中心来服务于分布在度量空间中的n个代理。我们细化了[Chen等人，ICML 2019]中提出的比例公平性概念，提出了核心公平性。如果一个k聚类处于核心，那么没有任何包含至少n/k个代理的联盟能够通过共同偏离到一个新中心来严格减少它们的总距离。我们的解决方案概念受到代理能够协调且效用可转移的情况的启发。我们提供了一系列关于存在性、难度和近似性的结果。特别是，我们提出了两个维度来放宽核心要求：一个是距离改进的程度，另一个是偏离联盟的大小。对于这两种放宽及其组合，我们研究了在包括线、树和一般度量空间在内的度量空间中，放宽后的核心公平性能够在多大程度上得到满足，并相应地设计了近似算法。我们还对合成和真实世界的数据进行了实验，以检验我们算法的性能。",
        "领域": "聚类算法、公平性机器学习、算法设计",
        "问题": "如何在聚类算法中实现群体公平性，确保没有任何代理群体因为聚类结果而处于不利地位。",
        "动机": "受到代理能够协调且效用可转移的情况的启发，研究如何在聚类问题中实现公平性，避免某些代理群体因为聚类结果而受到不公平对待。",
        "方法": "提出了核心公平性的概念，并通过放宽核心要求的两个维度（距离改进的程度和偏离联盟的大小）来研究在度量空间中实现放宽后的核心公平性的可能性，设计了相应的近似算法。",
        "关键词": [
            "群体公平性",
            "核心公平性",
            "近似算法",
            "度量空间",
            "聚类问题"
        ],
        "涉及的技术概念": {
            "核心公平性": "细化了比例公平性概念，确保没有任何包含至少n/k个代理的联盟能够通过共同偏离到一个新中心来严格减少它们的总距离。",
            "近似算法": "设计用于在放宽核心要求的情况下，在度量空间中实现放宽后的核心公平性的算法。",
            "度量空间": "研究在包括线、树和一般度量空间在内的度量空间中实现放宽后的核心公平性的可能性。"
        },
        "success": true
    },
    {
        "order": 79,
        "title": "Approximating a Distribution Using Weight Queries",
        "html": "https://ICML.cc//virtual/2021/poster/8457",
        "abstract": "We consider a novel challenge: approximating a distribution without the ability to randomly sample from that distribution. We study how such an approximation can be obtained using *weight queries*. Given some data set of examples, a weight query presents one of the examples to an oracle, which returns the probability, according to the target distribution, of observing examples similar to the presented example. This oracle can represent, for instance, counting queries to a database of the target population, or an interface to a search engine which returns the number of results that match a given search. \n\nWe propose an interactive algorithm that iteratively selects data set examples and performs corresponding weight queries. The algorithm finds a reweighting of the data set that approximates the weights according to the target distribution, using a limited number of weight queries. We derive an approximation bound on the total variation distance between the reweighting found by the algorithm and the best achievable reweighting. Our algorithm takes inspiration from the UCB approach common in multi-armed bandits problems, and combines it with a new discrepancy estimator and a greedy iterative procedure. In addition to our theoretical guarantees, we demonstrate in experiments the advantages of the proposed algorithm over several baselines. A python implementation of the proposed algorithm and of all the experiments can be found at https://github.com/Nadav-Barak/AWP.",
        "conference": "ICML",
        "中文标题": "使用权重查询近似分布",
        "摘要翻译": "我们考虑了一个新颖的挑战：在没有能力从该分布中随机抽样的条件下近似一个分布。我们研究了如何利用*权重查询*来获得这样的近似。给定一些示例数据集，权重查询向一个预言机提交其中一个示例，预言机返回根据目标分布观察到类似示例的概率。这个预言机可以代表，例如，对目标群体数据库的计数查询，或者一个搜索引擎接口，返回匹配给定搜索的结果数量。我们提出了一种交互式算法，该算法迭代地选择数据集示例并执行相应的权重查询。该算法使用有限数量的权重查询，找到一个重新加权的数据集，该数据集根据目标分布近似权重。我们推导了算法找到的重新加权与最佳可实现重新加权之间总变异距离的近似界限。我们的算法从多臂老虎机问题中常见的UCB方法中汲取灵感，并将其与一个新的差异估计器和一个贪婪的迭代过程相结合。除了我们的理论保证外，我们还在实验中展示了所提出算法相对于几个基线的优势。所提出算法和所有实验的Python实现可以在https://github.com/Nadav-Barak/AWP找到。",
        "领域": "概率分布近似, 机器学习算法, 数据挖掘",
        "问题": "在没有能力从目标分布中随机抽样的条件下近似该分布",
        "动机": "研究如何利用权重查询来近似分布，以解决无法直接抽样的问题",
        "方法": "提出了一种结合UCB方法、新差异估计器和贪婪迭代过程的交互式算法",
        "关键词": [
            "权重查询",
            "分布近似",
            "交互式算法",
            "UCB方法",
            "差异估计器"
        ],
        "涉及的技术概念": {
            "权重查询": "向预言机提交示例以获取目标分布下类似示例的概率",
            "UCB方法": "从多臂老虎机问题中汲取灵感，用于算法中的决策过程",
            "差异估计器": "新提出的用于评估重新加权与目标分布之间差异的工具"
        },
        "success": true
    },
    {
        "order": 80,
        "title": "Approximation Theory Based Methods for RKHS Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/8553",
        "abstract": "The RKHS bandit problem (also called kernelized multi-armed bandit problem)\nis an online optimization problem of non-linear functions with noisy feedback.\nAlthough the problem has been extensively studied,\nthere are unsatisfactory results for some problems compared to\nthe well-studied linear bandit case.\nSpecifically, there is no general algorithm for the adversarial RKHS bandit problem.\nIn addition, high computational complexity of existing algorithms hinders practical application.\nWe address these issues by considering a novel amalgamation\nof approximation theory and the misspecified linear bandit problem.\nUsing an approximation method,\nwe propose efficient algorithms for the stochastic\nRKHS bandit problem and the first general algorithm for the adversarial RKHS bandit problem.\nFurthermore,\nwe empirically show that one of our proposed methods has\ncomparable cumulative regret to IGP-UCB and its running time is much shorter.",
        "conference": "ICML",
        "中文标题": "基于逼近理论的RKHS赌博机方法",
        "摘要翻译": "RKHS赌博机问题（也称为核化多臂赌博机问题）是一个带有噪声反馈的非线性函数的在线优化问题。尽管该问题已被广泛研究，但与深入研究过的线性赌博机情况相比，对于某些问题的结果并不令人满意。具体来说，目前还没有针对对抗性RKHS赌博机问题的通用算法。此外，现有算法的高计算复杂度阻碍了实际应用。我们通过考虑逼近理论与误设线性赌博机问题的新颖结合来解决这些问题。使用一种逼近方法，我们为随机RKHS赌博机问题提出了高效算法，并为对抗性RKHS赌博机问题提出了第一个通用算法。此外，我们实证表明，我们提出的方法之一在累积遗憾方面与IGP-UCB相当，且其运行时间要短得多。",
        "领域": "强化学习、在线优化、核方法",
        "问题": "解决RKHS赌博机问题中缺乏通用对抗性算法和高计算复杂度的问题",
        "动机": "为了克服RKHS赌博机问题中现有算法的局限性，特别是在对抗性设置下的通用算法缺失和计算效率低下的问题",
        "方法": "结合逼近理论和误设线性赌博机问题，提出新的算法，包括针对随机和对抗性RKHS赌博机问题的高效算法",
        "关键词": [
            "RKHS赌博机",
            "逼近理论",
            "在线优化",
            "对抗性算法",
            "计算效率"
        ],
        "涉及的技术概念": {
            "RKHS赌博机": "一种在线优化问题，涉及在再生核希尔伯特空间中选择动作以最大化奖励",
            "逼近理论": "用于设计算法，通过近似复杂函数来提高计算效率",
            "对抗性算法": "针对对抗性环境设计的算法，能够在面对对手时保持性能"
        },
        "success": true
    },
    {
        "order": 81,
        "title": "Approximation Theory of Convolutional Architectures for Time Series Modelling",
        "html": "https://ICML.cc//virtual/2021/poster/10513",
        "abstract": "We study the approximation properties of convolutional architectures applied to time series modelling, which can be formulated mathematically as a functional approximation problem. In the recurrent setting, recent results reveal an intricate connection between approximation efficiency and memory structures in the data generation process. In this paper, we derive parallel results for convolutional architectures, with WaveNet being a prime example. Our results reveal that in this new setting, approximation efficiency is not only characterised by memory, but also additional fine structures in the target relationship. This leads to a novel definition of spectrum-based regularity that measures the complexity of temporal relationships under the convolutional approximation scheme. These analyses provide a foundation to understand the differences between architectural choices for time series modelling and can give theoretically grounded guidance for practical applications.",
        "conference": "ICML",
        "中文标题": "卷积架构在时间序列建模中的近似理论",
        "摘要翻译": "我们研究了应用于时间序列建模的卷积架构的近似性质，这可以数学上表述为一个函数近似问题。在循环设置中，最近的结果揭示了近似效率与数据生成过程中的内存结构之间错综复杂的联系。在本文中，我们为卷积架构推导了类似的结果，以WaveNet为主要例子。我们的结果表明，在这种新设置中，近似效率不仅由内存表征，还由目标关系中的额外精细结构表征。这导致了一种基于频谱的规律性的新定义，该定义衡量了卷积近似方案下时间关系的复杂性。这些分析为理解时间序列建模中架构选择的差异提供了基础，并可以为实际应用提供理论依据的指导。",
        "领域": "时间序列分析、深度学习、信号处理",
        "问题": "研究卷积架构在时间序列建模中的近似性质及其效率",
        "动机": "探索卷积架构在时间序列建模中的近似效率与内存结构及目标关系中精细结构的关系",
        "方法": "通过数学建模和理论分析，推导卷积架构在时间序列建模中的近似性质，以WaveNet为例进行研究",
        "关键词": [
            "卷积架构",
            "时间序列建模",
            "近似理论",
            "WaveNet",
            "频谱规律性"
        ],
        "涉及的技术概念": {
            "卷积架构": "用于时间序列建模的深度学习架构，能够捕捉时间序列数据中的局部依赖关系",
            "近似效率": "衡量模型在给定复杂度下逼近目标函数的能力",
            "频谱规律性": "新定义的衡量卷积近似方案下时间关系复杂性的指标"
        },
        "success": true
    },
    {
        "order": 82,
        "title": "A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix Groups",
        "html": "https://ICML.cc//virtual/2021/poster/9727",
        "abstract": " Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including  $\\mathrm{O}(1,3)$, $\\mathrm{O}(5)$, $\\mathrm{Sp}(n)$, and the Rubik's cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and modeling dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary",
        "conference": "ICML",
        "success": true,
        "中文标题": "构建任意矩阵群等变多层感知器的实用方法",
        "摘要翻译": "对称性和等变性对于神经网络在图像、图和点云等领域的泛化至关重要。现有工作主要集中在一小部分群上，如平移群、旋转群和置换群。在这项工作中，我们提供了一个完全通用的算法，用于求解矩阵群的等变层。除了作为特例恢复其他工作的解外，我们还构建了对多个以前从未处理过的群（包括O(1,3)、O(5)、Sp(n)和魔方群）等变的多层感知器。我们的方法在粒子物理和动态系统建模应用中优于非等变基线。我们发布了我们的软件库，使研究人员能够为任意群构建等变层。",
        "领域": "深度学习、计算机视觉、粒子物理",
        "问题": "如何构建适用于任意矩阵群的等变多层感知器",
        "动机": "解决现有方法局限于特定群的问题，扩展等变神经网络的应用范围",
        "方法": "提出一个通用算法，用于求解任意矩阵群的等变层，并构建相应的多层感知器",
        "关键词": [
            "等变性",
            "多层感知器",
            "矩阵群",
            "神经网络",
            "对称性"
        ],
        "涉及的技术概念": {
            "等变性": "神经网络层的一种性质，确保网络输出在输入变换下以特定方式变换",
            "矩阵群": "由满足特定条件的矩阵组成的群，是构建等变网络的基础",
            "多层感知器": "一种前馈人工神经网络模型，由多个神经元层组成，用于学习和建模复杂的数据模式"
        }
    },
    {
        "order": 83,
        "title": "A Precise Performance Analysis of Support Vector Regression",
        "html": "https://ICML.cc//virtual/2021/poster/9883",
        "abstract": "In this paper, we study the hard and soft support vector regression techniques applied to a set of $n$ linear measurements of the form $y_i=\\boldsymbol{\\beta}_\\star^{T}{\\bf x}_i +n_i$ where $\\boldsymbol{\\beta}_\\star$ is an unknown vector, $\\left\\{{\\bf x}_i\\right\\}_{i=1}^n$ are the feature vectors and $\\left\\{{n}_i\\right\\}_{i=1}^n$ model the noise. Particularly, under some plausible assumptions on the statistical distribution of the data, we characterize the feasibility condition for the hard support vector regression in the regime of high dimensions and, when feasible, derive an asymptotic approximation for its risk. Similarly, we study the test risk for the soft support vector regression as a function of its parameters. Our results are then used to optimally tune the parameters intervening in the design of hard and soft support vector regression algorithms. Based on our analysis, we illustrate that adding more samples may be harmful to the test performance of support vector regression, while it is always beneficial when the parameters are optimally selected. Such a result reminds a similar phenomenon observed in modern learning architectures according to which optimally tuned architectures present a decreasing test performance curve with respect to the number of samples. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "支持向量回归的精确性能分析",
        "摘要翻译": "在本文中，我们研究了应用于一组线性测量值的硬支持向量回归和软支持向量回归技术，测量值的形式为 $y_i=\\\\boldsymbol{\\\\beta}_\\\\star^{T}{\\\\bf x}_i +n_i$，其中 $\\\\boldsymbol{\\\\beta}_\\\\star$ 是一个未知向量，$\\\\left\\\\{{\\\\bf x}_i\\\\right\\\\}_{i=1}^n$ 是特征向量，$\\\\left\\\\{{n}_i\\\\right\\\\}_{i=1}^n$ 对噪声进行建模。特别地，在一些关于数据统计分布的合理假设下，我们刻画了高维情况下硬支持向量回归的可行性条件，并在可行时，推导出其风险的渐近近似。类似地，我们研究了软支持向量回归的测试风险，作为其参数的函数。然后，我们的结果被用于优化调整干预硬支持向量回归和软支持向量回归算法设计的参数。基于我们的分析，我们说明了添加更多样本可能对支持向量回归的测试性能有害，而当参数被最佳选择时，它总是Beneficial。这样的结果让人想起在现代学习架构中观察到的类似现象，根据该现象，经过最佳调整的架构呈现出相对于样本数量的递减测试性能曲线。",
        "领域": "机器学习理论, 高维统计, 支持向量机",
        "问题": "在高维数据下，硬支持向量回归和软支持向量回归的性能分析和参数优化。",
        "动机": "理解在高维数据情况下，增加样本数量对支持向量回归性能的影响，以及如何优化参数以避免性能下降。",
        "方法": "通过统计分布假设，推导硬支持向量回归可行性条件和风险的渐近近似，以及软支持向量回归的测试风险函数，并利用这些结果进行参数优化。",
        "关键词": [
            "支持向量回归",
            "高维数据",
            "风险分析",
            "参数优化",
            "渐近近似"
        ],
        "涉及的技术概念": {
            "支持向量回归": "一种基于支持向量机的回归方法，用于预测连续值。",
            "风险分析": "通过理论推导，对算法在高维数据下的泛化性能进行评估。"
        }
    },
    {
        "order": 84,
        "title": "A Probabilistic Approach to Neural Network Pruning",
        "html": "https://ICML.cc//virtual/2021/poster/10317",
        "abstract": "Neural network pruning techniques reduce the number of parameters without compromising predicting ability of a network. Many algorithms have been developed for pruning both over-parameterized fully-connected networks (FCN) and convolutional neural networks (CNN), but analytical studies of capabilities and compression ratios of such pruned sub-networks are lacking. We theoretically study the performance of two pruning techniques (random and magnitude-based) on FCN and CNN. Given a target network, we provide a universal approach to bound the gap between a pruned and the target network in a probabilistic sense, which is the first study of this nature. The results establish that there exist pruned networks with expressive power within any specified bound from the target network and with a significant compression ratio.\n",
        "conference": "ICML",
        "中文标题": "神经网络剪枝的概率方法",
        "摘要翻译": "神经网络剪枝技术在不影响网络预测能力的情况下减少参数数量。已经开发出许多算法用于剪枝过参数化的全连接网络（FCN）和卷积神经网络（CNN），但对于这些剪枝后子网络的能力和压缩比的分析研究却不足。我们从理论上研究了两种剪枝技术（随机和基于幅度的）在FCN和CNN上的表现。给定一个目标网络，我们提供了一种通用的方法来概率意义上界定剪枝网络与目标网络之间的差距，这是此类研究的首次尝试。结果表明，存在剪枝网络，其表达能力在目标网络的任何指定范围内，并且具有显著的压缩比。",
        "领域": "神经网络优化、模型压缩、深度学习理论",
        "问题": "如何在不显著降低网络预测能力的前提下，有效减少神经网络的参数数量。",
        "动机": "当前缺乏对剪枝后子网络能力和压缩比的深入分析研究，需要理论上的探索以指导实践。",
        "方法": "理论分析两种剪枝技术（随机和基于幅度的）在全连接网络和卷积神经网络上的表现，并提出一种概率方法来界定剪枝网络与目标网络之间的差距。",
        "关键词": [
            "神经网络剪枝",
            "模型压缩",
            "概率方法",
            "全连接网络",
            "卷积神经网络"
        ],
        "涉及的技术概念": {
            "神经网络剪枝": "通过移除神经网络中的部分连接或节点来减少模型大小和计算复杂度，同时尽量保持模型性能的技术。",
            "模型压缩": "减少模型大小和计算资源消耗的技术，旨在使模型更适合在资源受限的环境中部署。",
            "概率方法": "在本研究中，用于理论上界定剪枝网络与目标网络之间差距的方法，提供了一种量化剪枝效果的手段。"
        },
        "success": true
    },
    {
        "order": 85,
        "title": "A Proxy Variable View of Shared Confounding",
        "html": "https://ICML.cc//virtual/2021/poster/10489",
        "abstract": "Causal inference from observational data can be biased by unobserved\nconfounders. Confounders—the variables that affect both the treatments\nand the outcome—induce spurious non-causal correlations between the\ntwo. Without additional conditions, unobserved confounders generally\nmake causal quantities hard to identify. In this paper, we focus on\nthe setting where there are many treatments with shared confounding,\nand we study under what conditions is causal identification possible.\nThe key observation is that we can view subsets of treatments as\nproxies of the unobserved confounder and identify the intervention\ndistributions of the rest. Moreover, while existing identification\nformulas for proxy variables involve solving integral equations, we\nshow that one can circumvent the need for such solutions by directly\nmodeling the data. Finally, we extend these results to an expanded\nclass of causal graphs, those with other confounders and selection\nvariables.",
        "conference": "ICML",
        "中文标题": "共享混杂因素的代理变量视角",
        "摘要翻译": "从观察数据中进行因果推断可能会受到未观察到的混杂因素的偏差。混杂因素——即同时影响治疗和结果的变量——会在两者之间诱导出虚假的非因果相关性。在没有额外条件的情况下，未观察到的混杂因素通常使得因果量难以识别。在本文中，我们关注于存在许多具有共享混杂因素的治疗设置，并研究在什么条件下因果识别是可能的。关键观察是，我们可以将治疗的子集视为未观察到的混杂因素的代理，并识别其余治疗的干预分布。此外，虽然现有的代理变量识别公式涉及解决积分方程，但我们表明，通过直接建模数据可以绕过这种解决方案的需求。最后，我们将这些结果扩展到一个扩展的因果图类别，包括具有其他混杂因素和选择变量的图。",
        "领域": "因果推断、统计机器学习、数据科学",
        "问题": "在存在未观察到的共享混杂因素的情况下，如何识别因果量。",
        "动机": "解决在观察性研究中，由于未观察到的混杂因素导致的因果推断偏差问题。",
        "方法": "将治疗子集视为未观察到的混杂因素的代理，直接建模数据以绕过复杂的积分方程求解。",
        "关键词": [
            "因果推断",
            "共享混杂因素",
            "代理变量",
            "干预分布",
            "因果图"
        ],
        "涉及的技术概念": {
            "共享混杂因素": "指多个治疗变量共同受到同一未观察到的混杂因素影响的情况。",
            "代理变量": "用于代表或替代未观察到的混杂因素的变量子集。",
            "干预分布": "在特定干预下结果的概率分布，用于因果效应的识别和估计。"
        },
        "success": true
    },
    {
        "order": 86,
        "title": "APS: Active Pretraining with Successor Features",
        "html": "https://ICML.cc//virtual/2021/poster/9241",
        "abstract": "We introduce a new unsupervised pretraining objective for reinforcement learning. During the unsupervised reward-free pretraining phase, the agent maximizes mutual information between tasks and states induced by the policy. Our key contribution is a novel lower bound of this intractable quantity. We show that by reinterpreting and combining variational successor features~\\citep{Hansen2020Fast} with nonparametric entropy maximization~\\citep{liu2021behavior}, the intractable mutual information can be efficiently optimized. The proposed method Active Pretraining with Successor Feature (APS) explores the environment via nonparametric entropy maximization, and the explored data can be efficiently leveraged to learn behavior by variational successor features. APS addresses the limitations of existing mutual information maximization based and entropy maximization based unsupervised RL, and combines the best of both worlds. When evaluated on the Atari 100k data-efficiency benchmark, our approach significantly outperforms previous methods combining unsupervised pretraining with task-specific finetuning.",
        "conference": "ICML",
        "中文标题": "APS：基于后继特征的主动预训练",
        "摘要翻译": "我们为强化学习引入了一种新的无监督预训练目标。在无监督无奖励的预训练阶段，智能体通过策略最大化任务与状态之间的互信息。我们的主要贡献是提出了这一难以处理量的新下界。我们展示了通过重新解释并结合变分后继特征与非参数熵最大化，可以高效优化这一难以处理的互信息。所提出的基于后继特征的主动预训练方法（APS）通过非参数熵最大化探索环境，并且探索到的数据可以通过变分后继特征高效地用于学习行为。APS解决了现有基于互信息最大化和基于熵最大化的无监督强化学习的局限性，并集两者之长。在Atari 100k数据效率基准测试中评估时，我们的方法在结合无监督预训练与任务特定微调方面显著优于之前的方法。",
        "领域": "强化学习、无监督学习、数据效率",
        "问题": "如何在无监督预训练阶段高效地探索环境并学习行为，以提升强化学习的数据效率。",
        "动机": "解决现有基于互信息最大化和基于熵最大化的无监督强化学习方法在探索环境和学习行为方面的局限性。",
        "方法": "提出了一种新的无监督预训练目标，通过结合变分后继特征与非参数熵最大化，高效优化任务与状态之间的互信息。",
        "关键词": [
            "无监督预训练",
            "变分后继特征",
            "非参数熵最大化",
            "数据效率",
            "强化学习"
        ],
        "涉及的技术概念": {
            "变分后继特征": "用于高效地从探索到的数据中学习行为，解决了现有方法在利用探索数据方面的不足。",
            "非参数熵最大化": "用于探索环境，通过最大化策略诱导的状态熵来促进环境的广泛探索。",
            "互信息最大化": "作为无监督预训练的目标，旨在通过最大化任务与状态之间的互信息来指导策略的学习。"
        },
        "success": true
    },
    {
        "order": 87,
        "title": "A Receptor Skeleton for Capsule Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8915",
        "abstract": "In previous Capsule Neural Networks (CapsNets), routing algorithms often performed clustering processes to assemble the child capsules' representations into parent capsules. Such routing algorithms were typically implemented with iterative processes and incurred high computing complexity. This paper presents a new capsule structure, which contains a set of optimizable receptors and a transmitter is devised on the capsule's representation. Specifically, child capsules' representations are sent to the parent capsules whose receptors match well the transmitters of the child capsules' representations, avoiding applying computationally complex routing algorithms. To ensure the receptors in a CapsNet work cooperatively, we build a skeleton to organize the receptors in different capsule layers in a CapsNet. The receptor skeleton assigns a share-out objective for each receptor, making the CapsNet perform as a hierarchical agglomerative clustering process. Comprehensive experiments verify that our approach facilitates efficient clustering processes, and CapsNets with our approach significantly outperform CapsNets with previous routing algorithms on image classification, affine transformation generalization, overlapped object recognition, and representation semantic decoupling.",
        "conference": "ICML",
        "中文标题": "胶囊神经网络的受体骨架",
        "摘要翻译": "在以往的胶囊神经网络（CapsNets）中，路由算法通常执行聚类过程，将子胶囊的表示组装成父胶囊。这类路由算法通常通过迭代过程实现，计算复杂度高。本文提出了一种新的胶囊结构，包含一组可优化的受体，并在胶囊的表示上设计了一个发射器。具体来说，子胶囊的表示被发送到那些受体与子胶囊表示发射器匹配良好的父胶囊，从而避免应用计算复杂的路由算法。为了确保CapsNet中的受体协同工作，我们构建了一个骨架来组织CapsNet中不同胶囊层的受体。受体骨架为每个受体分配了一个共享目标，使CapsNet执行层次凝聚聚类过程。全面的实验验证了我们的方法促进了高效的聚类过程，并且采用我们方法的CapsNets在图像分类、仿射变换泛化、重叠物体识别和表示语义解耦方面显著优于采用以往路由算法的CapsNets。",
        "领域": "胶囊神经网络、图像分类、对象识别",
        "问题": "解决胶囊神经网络中路由算法计算复杂度高的问题",
        "动机": "提高胶囊神经网络的效率和性能，特别是在图像分类和对象识别任务中",
        "方法": "提出了一种新的胶囊结构，包含可优化的受体和发射器，构建受体骨架以组织受体，避免复杂的路由算法",
        "关键词": [
            "胶囊神经网络",
            "受体骨架",
            "层次凝聚聚类",
            "图像分类",
            "对象识别"
        ],
        "涉及的技术概念": {
            "受体": "在胶囊结构中用于匹配和接收子胶囊表示的可优化组件",
            "发射器": "设计在胶囊表示上的组件，用于与父胶囊的受体匹配",
            "层次凝聚聚类": "通过受体骨架实现的聚类过程，使胶囊网络能够高效地组织信息"
        },
        "success": true
    },
    {
        "order": 88,
        "title": "A Regret Minimization Approach to Iterative Learning Control",
        "html": "https://ICML.cc//virtual/2021/poster/9817",
        "abstract": "We consider the setting of iterative learning control, or model-based policy learning in the presence of uncertain, time-varying dynamics. In this setting, we propose a new performance metric, planning regret, which replaces the standard stochastic uncertainty assumptions with worst case regret. Based on recent advances in non-stochastic control, we design a new iterative algorithm for minimizing planning regret that is more robust to model mismatch and uncertainty. We provide theoretical and empirical evidence that the proposed algorithm outperforms existing methods on several benchmarks. ",
        "conference": "ICML",
        "中文标题": "一种基于遗憾最小化的迭代学习控制方法",
        "摘要翻译": "我们考虑在存在不确定、时变动力学的情况下进行迭代学习控制或基于模型的策略学习的设置。在此设置中，我们提出了一个新的性能指标——规划遗憾，它用最坏情况下的遗憾替代了标准的随机不确定性假设。基于非随机控制的最新进展，我们设计了一种新的迭代算法，用于最小化规划遗憾，该算法对模型不匹配和不确定性更加鲁棒。我们提供了理论和实证证据，表明所提出的算法在多个基准测试中优于现有方法。",
        "领域": "迭代学习控制、非随机控制、模型预测控制",
        "问题": "在不确定、时变动力学环境下，如何提高迭代学习控制的性能和鲁棒性",
        "动机": "为了克服标准随机不确定性假设的局限性，提出一种更鲁棒的迭代学习控制方法",
        "方法": "提出规划遗憾作为新的性能指标，并设计一种基于非随机控制的迭代算法来最小化规划遗憾",
        "关键词": [
            "迭代学习控制",
            "规划遗憾",
            "非随机控制",
            "模型不匹配",
            "鲁棒性"
        ],
        "涉及的技术概念": {
            "规划遗憾": "替代标准随机不确定性假设的新性能指标，用于评估在不确定、时变动力学环境下的控制性能",
            "非随机控制": "基于最新进展的控制方法，用于设计更鲁棒的迭代学习控制算法",
            "模型不匹配": "指模型预测与实际动力学之间的差异，算法设计需考虑其对控制性能的影响"
        },
        "success": true
    },
    {
        "order": 89,
        "title": "A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8759",
        "abstract": "An effective approach in meta-learning is to utilize multiple ``train tasks'' to learn a good initialization for model parameters that can help solve unseen ``test tasks'' with very few samples by fine-tuning from this initialization. Although successful in practice, theoretical understanding of such methods is limited. This work studies an important aspect of these methods: splitting the data from each task into train (support) and validation (query) sets during meta-training. Inspired by recent work (Raghu et al., 2020), we view such meta-learning methods through the lens of representation learning and argue that the train-validation split encourages the learned representation to be {\\em low-rank} without compromising on expressivity, as opposed to the non-splitting variant that encourages high-rank representations. Since sample efficiency benefits from low-rankness, the splitting strategy will require very few samples to solve unseen test tasks. We present theoretical results that formalize this idea for linear representation learning on a subspace meta-learning instance, and experimentally verify this practical benefit of splitting in simulations and on standard meta-learning benchmarks.",
        "conference": "ICML",
        "中文标题": "从表征学习视角看元学习中训练-验证分割的重要性",
        "摘要翻译": "元学习中的一个有效方法是利用多个‘训练任务’来学习模型参数的良好初始化，这有助于通过从该初始化进行微调，用极少样本解决未见过的‘测试任务’。尽管在实践中取得了成功，但对此类方法的理论理解仍然有限。本研究探讨了这些方法的一个重要方面：在元训练过程中将每个任务的数据分割为训练（支持）集和验证（查询）集。受近期工作（Raghu等人，2020年）的启发，我们从表征学习的角度看待此类元学习方法，并认为训练-验证分割鼓励学习到的表征在不牺牲表达能力的情况下保持低秩，与不分割的变体鼓励高秩表征形成对比。由于样本效率受益于低秩性，分割策略将需要极少样本来解决未见过的测试任务。我们提出了理论结果，将这一思想形式化为子空间元学习实例上的线性表征学习，并通过模拟和标准元学习基准实验验证了分割的实际益处。",
        "领域": "元学习、表征学习、小样本学习",
        "问题": "理解并验证在元学习过程中将数据分割为训练集和验证集对学习低秩表征的影响及其对样本效率的益处。",
        "动机": "探索元学习方法中训练-验证分割的理论基础，特别是在促进低秩表征学习方面的作用，以提高解决新任务时的样本效率。",
        "方法": "通过理论分析和实验验证，研究在元学习过程中数据分割对表征学习的影响，特别是在线性表征学习和子空间元学习实例上的应用。",
        "关键词": [
            "元学习",
            "表征学习",
            "训练-验证分割",
            "低秩表征",
            "样本效率"
        ],
        "涉及的技术概念": {
            "元学习": "研究如何利用多个训练任务来学习模型参数的初始化，以便于快速适应新任务。",
            "表征学习": "关注于学习数据的表示形式，以便于后续任务的解决，特别是在低秩表征的情况下提高样本效率。",
            "训练-验证分割": "在元训练过程中将每个任务的数据分割为训练集和验证集，以促进学习低秩表征而不牺牲表达能力。"
        },
        "success": true
    },
    {
        "order": 90,
        "title": "A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance",
        "html": "https://ICML.cc//virtual/2021/poster/8477",
        "abstract": "The Wasserstein distance has become increasingly important in machine learning and deep learning. Despite its popularity, the Wasserstein distance is hard to approximate because of the curse of dimensionality. A recently proposed approach to alleviate the curse of dimensionality is to project the sampled data from the high dimensional probability distribution onto a lower-dimensional subspace, and then compute the Wasserstein distance between the projected data. However, this approach requires to solve a max-min problem over the Stiefel manifold, which is very challenging in practice. In this paper, we propose a Riemannian block coordinate descent (RBCD) method to solve this problem, which is based on a novel reformulation of the regularized max-min problem over the Stiefel manifold. We show that the complexity of arithmetic operations for RBCD to obtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$, which is significantly better than the complexity of existing methods. Numerical results on both synthetic and real datasets demonstrate that our method is more efficient than existing methods, especially when the number of sampled data is very large. ",
        "conference": "ICML",
        "中文标题": "一种用于计算投影鲁棒Wasserstein距离的黎曼块坐标下降法",
        "摘要翻译": "Wasserstein距离在机器学习和深度学习中变得越来越重要。尽管它很受欢迎，但由于维数灾难，Wasserstein距离难以近似。最近提出的一种缓解维数灾难的方法是将从高维概率分布中采样的数据投影到低维子空间，然后计算投影数据之间的Wasserstein距离。然而，这种方法需要在Stiefel流形上解决一个最大最小问题，这在实践中非常具有挑战性。在本文中，我们提出了一种黎曼块坐标下降（RBCD）方法来解决这个问题，该方法基于对Stiefel流形上正则化最大最小问题的新颖重新表述。我们证明了RBCD获得一个ε-稳定点的算术运算复杂度为O(ε^{-3})，这显著优于现有方法的复杂度。在合成和真实数据集上的数值结果表明，我们的方法比现有方法更高效，尤其是当采样数据量非常大时。",
        "领域": "最优传输理论、机器学习优化方法、高维数据分析",
        "问题": "解决在高维空间中计算Wasserstein距离时遇到的维数灾难问题。",
        "动机": "为了更高效地近似计算高维数据之间的Wasserstein距离，特别是在数据量大的情况下。",
        "方法": "提出了一种基于黎曼块坐标下降（RBCD）的方法，通过重新表述Stiefel流形上的正则化最大最小问题来优化计算过程。",
        "关键词": [
            "Wasserstein距离",
            "黎曼优化",
            "Stiefel流形"
        ],
        "涉及的技术概念": {
            "Wasserstein距离": "用于衡量两个概率分布之间差异的度量，本文中通过投影到低维空间来近似计算。",
            "黎曼块坐标下降（RBCD）": "一种优化方法，用于在黎曼流形上高效解决最大最小问题。",
            "Stiefel流形": "本文中优化问题的约束空间，由所有正交矩阵组成。"
        },
        "success": true
    },
    {
        "order": 91,
        "title": "ARMS: Antithetic-REINFORCE-Multi-Sample Gradient for Binary Variables",
        "html": "https://ICML.cc//virtual/2021/poster/9775",
        "abstract": "Estimating the gradients for binary variables is a task that arises frequently in various domains, such as training discrete latent variable models. What has been commonly used is a REINFORCE based Monte Carlo estimation method that uses either independent samples or pairs of negatively correlated samples. To better utilize more than two samples, we propose ARMS, an Antithetic REINFORCE-based Multi-Sample gradient estimator. ARMS uses a copula to generate any number of mutually antithetic samples. It is unbiased, has low variance, and generalizes both DisARM, which we show to be ARMS with two samples, and the leave-one-out REINFORCE (LOORF) estimator, which is ARMS with uncorrelated samples. We evaluate ARMS on several datasets for training generative models, and our experimental results show that it outperforms competing methods. We also develop a version of ARMS for optimizing the multi-sample variational bound, and show that it outperforms both VIMCO and DisARM. The code is publicly available.",
        "conference": "ICML",
        "中文标题": "ARMS：针对二元变量的反相关-REINFORCE-多样本梯度方法",
        "摘要翻译": "估计二元变量的梯度是在多个领域中频繁出现的任务，例如训练离散潜在变量模型。常用的方法是基于REINFORCE的蒙特卡洛估计方法，该方法使用独立样本或负相关样本对。为了更好地利用两个以上的样本，我们提出了ARMS，一种基于反相关REINFORCE的多样本梯度估计器。ARMS使用copula生成任意数量的相互反相关样本。它是无偏的，具有低方差，并且概括了DisARM（我们展示其为两个样本的ARMS）和留一REINFORCE（LOORF）估计器（即样本不相关的ARMS）。我们在多个数据集上评估ARMS以训练生成模型，实验结果表明其性能优于竞争方法。我们还开发了一个用于优化多样本变分界限的ARMS版本，并显示其性能优于VIMCO和DisARM。代码已公开提供。",
        "领域": "离散潜在变量模型训练, 生成模型优化, 梯度估计技术",
        "问题": "如何更有效地估计二元变量的梯度，以改进离散潜在变量模型的训练",
        "动机": "现有的基于REINFORCE的梯度估计方法仅能利用独立或成对负相关样本，限制了梯度估计的效率和准确性",
        "方法": "提出ARMS方法，利用copula生成任意数量的相互反相关样本，实现无偏、低方差的梯度估计",
        "关键词": [
            "ARMS",
            "梯度估计",
            "离散潜在变量",
            "copula",
            "REINFORCE"
        ],
        "涉及的技术概念": {
            "REINFORCE": "一种基于蒙特卡洛的梯度估计方法，用于优化不可微分的损失函数",
            "copula": "用于生成具有特定相关结构的随机变量的统计工具，在ARMS中用于生成反相关样本",
            "DisARM": "一种特定的ARMS实现，仅使用两个样本进行梯度估计"
        },
        "success": true
    },
    {
        "order": 92,
        "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9261",
        "abstract": "Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.",
        "conference": "ICML",
        "中文标题": "ASAM：面向深度神经网络尺度不变学习的自适应锐度感知最小化",
        "摘要翻译": "最近，受损失表面锐度作为泛化差距有效度量的启发，学习算法已展现出最先进的性能。然而，在固定半径的刚性区域内定义的锐度，对不影响损失的参数重新缩放具有敏感性，这一缺点削弱了锐度与泛化差距之间的联系。本文中，我们引入了尺度不变的自适应锐度概念，并提出了相应的泛化界限。我们提出了一种新的学习方法——自适应锐度感知最小化（ASAM），利用所提出的泛化界限。在各种基准数据集上的实验结果表明，ASAM显著提高了模型的泛化性能。",
        "领域": "深度学习优化、模型泛化、神经网络训练",
        "问题": "解决传统锐度定义在参数重新缩放时的敏感性问题，以及锐度与泛化差距之间联系的削弱问题。",
        "动机": "为了克服固定半径锐度定义的局限性，提出一种尺度不变的自适应锐度概念，以更有效地衡量和优化模型的泛化性能。",
        "方法": "提出自适应锐度感知最小化（ASAM）方法，利用尺度不变的自适应锐度概念和相应的泛化界限来优化模型训练。",
        "关键词": [
            "自适应锐度",
            "尺度不变学习",
            "泛化性能优化",
            "深度学习优化",
            "ASAM"
        ],
        "涉及的技术概念": {
            "自适应锐度": "一种尺度不变的锐度概念，用于更有效地衡量模型的泛化性能，克服传统锐度定义的局限性。",
            "泛化界限": "提出的理论界限，用于指导ASAM方法的优化过程，确保模型具有良好的泛化能力。",
            "ASAM方法": "一种新的学习方法，通过自适应锐度感知最小化来优化模型训练，显著提高模型的泛化性能。"
        },
        "success": true
    },
    {
        "order": 93,
        "title": "A Sampling-Based Method for Tensor Ring Decomposition",
        "html": "https://ICML.cc//virtual/2021/poster/8447",
        "abstract": "We propose a sampling-based method for computing the tensor ring (TR) decomposition of a data tensor. The method uses leverage score sampled alternating least squares to fit the TR cores in an iterative fashion. By taking advantage of the special structure of TR tensors, we can efficiently estimate the leverage scores and attain a method which has complexity sublinear in the number of input tensor entries. We provide high-probability relative-error guarantees for the sampled least squares problems. We compare our proposal to existing methods in experiments on both synthetic and real data. Our method achieves substantial speedup---sometimes two or three orders of magnitude---over competing methods, while maintaining good accuracy. We also provide an example of how our method can be used for rapid feature extraction.",
        "conference": "ICML",
        "中文标题": "一种基于采样的张量环分解方法",
        "摘要翻译": "我们提出了一种基于采样的方法，用于计算数据张量的张量环（TR）分解。该方法利用杠杆得分采样交替最小二乘法以迭代方式拟合TR核心。通过利用TR张量的特殊结构，我们能够高效地估计杠杆得分，并获得一种在输入张量条目数量上次线性复杂度的方法。我们为采样最小二乘问题提供了高概率的相对误差保证。我们在合成数据和真实数据的实验中，将我们的方法与现有方法进行了比较。我们的方法在保持良好准确性的同时，实现了比竞争方法显著的速度提升——有时是两到三个数量级。我们还提供了一个示例，说明我们的方法如何用于快速特征提取。",
        "领域": "张量分解、机器学习优化、特征提取",
        "问题": "高效计算大规模张量的张量环分解",
        "动机": "为了解决传统张量环分解方法在处理大规模数据时计算复杂度高、效率低下的问题",
        "方法": "采用基于杠杆得分采样的交替最小二乘法，迭代拟合TR核心，利用TR张量的特殊结构高效估计杠杆得分",
        "关键词": [
            "张量环分解",
            "杠杆得分采样",
            "交替最小二乘法",
            "特征提取",
            "计算效率"
        ],
        "涉及的技术概念": {
            "张量环分解": "一种张量分解方法，用于将高阶张量分解为一系列低维核心张量的环状连接",
            "杠杆得分采样": "一种采样技术，用于在最小二乘问题中优先选择对解有更大影响的样本，以提高计算效率",
            "交替最小二乘法": "一种迭代优化技术，通过交替固定其他变量优化一个变量来求解多变量优化问题"
        },
        "success": true
    },
    {
        "order": 94,
        "title": "A Scalable Deterministic Global Optimization Algorithm for Clustering Problems",
        "html": "https://ICML.cc//virtual/2021/poster/10243",
        "abstract": "The minimum sum-of-squares clustering (MSSC) task, which can be treated as a Mixed Integer Second Order Cone Programming (MISOCP) problem, is rarely investigated in the literature through deterministic optimization to find its global optimal value. In this paper, we modelled the MSSC task as a two-stage optimization problem and proposed a tailed reduced-space branch and bound (BB) algorithm. We designed several approaches to construct lower and upper bounds at each node in the BB scheme, including a scenario grouping based Lagrangian decomposition approach. One key advantage of this reduced-space algorithm is that it only needs to perform branching on the centers of clusters to guarantee convergence, and the size of centers is independent of the number of data samples. Moreover, the lower bounds can be computed by solving small-scale sample subproblems, and upper bounds can be obtained trivially. These two properties enable our algorithm easy to be paralleled and can be scalable to the dataset with up to 200,000 samples for finding a global $\\epsilon$-optimal solution of the MSSC task. We performed numerical experiments on both synthetic and real-world datasets and compared our proposed algorithms with the off-the-shelf global optimal solvers and classical local optimal algorithms. The results reveal a strong performance and scalability of our algorithm.",
        "conference": "ICML",
        "success": true,
        "中文标题": "一种可扩展的确定性全局优化算法用于聚类问题",
        "摘要翻译": "最小平方和聚类（MSSC）任务，可被视为混合整数二阶锥规划（MISOCP）问题，在文献中很少通过确定性优化来寻找其全局最优值。在本文中，我们将MSSC任务建模为一个两阶段优化问题，并提出了一种尾部缩减空间分支定界（BB）算法。我们设计了几种方法来在BB方案的每个节点构建下界和上界，包括一种基于场景分组的拉格朗日分解方法。这种缩减空间算法的一个关键优势是，它只需要对聚类中心进行分支以保证收敛，且中心的大小与数据样本的数量无关。此外，下界可以通过解决小规模样本子问题来计算，而上界可以轻松获得。这两个特性使我们的算法易于并行化，并且可以扩展到包含多达200,000个样本的数据集，以找到MSSC任务的全局ε-最优解。我们在合成和真实世界的数据集上进行了数值实验，并将我们提出的算法与现成的全局最优求解器和经典的局部最优算法进行了比较。结果显示了我们的算法的强大性能和可扩展性。",
        "领域": "聚类分析, 全局优化, 混合整数规划",
        "问题": "解决最小平方和聚类（MSSC）任务的全局最优问题",
        "动机": "由于MSSC任务在确定性优化方面的研究较少，本文旨在通过提出一种新的算法来填补这一空白，以实现对大规模数据集的全局最优解的有效寻找。",
        "方法": "提出了一种尾部缩减空间分支定界算法，通过两阶段优化模型和拉格朗日分解方法来构建下界和上界，实现了算法的并行化和可扩展性。",
        "关键词": [
            "最小平方和聚类",
            "混合整数二阶锥规划",
            "分支定界算法",
            "全局优化",
            "并行计算"
        ],
        "涉及的技术概念": {
            "混合整数二阶锥规划（MISOCP）": "用于建模MSSC任务，结合整数和连续变量以及二阶锥约束，以实现全局优化。",
            "分支定界算法（BB）": "通过系统地枚举和剪枝搜索空间中的候选解，以找到全局最优解。",
            "拉格朗日分解方法": "用于在分支定界算法的每个节点构建有效的下界和上界，提高算法的效率和可扩展性。"
        }
    },
    {
        "order": 95,
        "title": "A Scalable Second Order Method for Ill-Conditioned Matrix Completion from Few Samples",
        "html": "https://ICML.cc//virtual/2021/poster/9411",
        "abstract": "We propose an iterative algorithm for low-rank matrix completion with that can be interpreted as an iteratively reweighted least squares (IRLS) algorithm, a saddle-escaping smoothing Newton method or a variable metric proximal gradient method applied to a non-convex rank surrogate. It combines the favorable data-efficiency of previous IRLS approaches with an improved scalability by several orders of magnitude. We establish the first local convergence guarantee from a minimal number of samples for that class of algorithms, showing that the method attains a local quadratic convergence rate. Furthermore, we show that the linear systems to be solved are well-conditioned even for very ill-conditioned ground truth matrices.  We provide extensive experiments, indicating that unlike many state-of-the-art approaches, our method is able to complete very ill-conditioned matrices with a condition number of up to $10^{10}$ from few samples, while being competitive in its scalability.",
        "conference": "ICML",
        "中文标题": "一种可扩展的二阶方法用于从少量样本中完成病态矩阵",
        "摘要翻译": "我们提出了一种用于低秩矩阵完成的迭代算法，该算法可以解释为迭代重加权最小二乘（IRLS）算法、鞍点逃逸平滑牛顿法或应用于非凸秩代理的变度量近端梯度法。它结合了先前IRLS方法的数据效率优势，并将可扩展性提高了几个数量级。我们首次为该类算法从最少数量的样本中建立了局部收敛保证，表明该方法达到了局部二次收敛率。此外，我们表明，即使对于非常病态的真实矩阵，需要解决的线性系统也是条件良好的。我们提供了广泛的实验，表明与许多最先进的方法不同，我们的方法能够从少量样本中完成条件数高达10^10的非常病态的矩阵，同时在可扩展性方面具有竞争力。",
        "领域": "低秩矩阵完成、优化算法、数值线性代数",
        "问题": "解决在少量样本下完成病态低秩矩阵的问题",
        "动机": "提高低秩矩阵完成算法的数据效率和可扩展性，特别是在处理非常病态的矩阵时",
        "方法": "结合迭代重加权最小二乘（IRLS）算法、鞍点逃逸平滑牛顿法和变度量近端梯度法，应用于非凸秩代理",
        "关键词": [
            "低秩矩阵完成",
            "迭代重加权最小二乘",
            "病态矩阵",
            "局部收敛",
            "可扩展性"
        ],
        "涉及的技术概念": {
            "迭代重加权最小二乘（IRLS）算法": "用于低秩矩阵完成，通过迭代调整权重来优化解",
            "鞍点逃逸平滑牛顿法": "用于避免优化过程中的鞍点，提高算法的收敛性",
            "变度量近端梯度法": "应用于非凸秩代理，优化算法的性能和可扩展性"
        },
        "success": true
    },
    {
        "order": 96,
        "title": "A Second look at Exponential and Cosine Step Sizes: Simplicity, Adaptivity, and Performance",
        "html": "https://ICML.cc//virtual/2021/poster/9527",
        "abstract": "Stochastic Gradient Descent (SGD) is a popular tool in training large-scale machine learning models. Its performance, however, is highly variable, depending crucially on the choice of the step sizes. Accordingly, a variety of strategies for tuning the step sizes have been proposed, ranging from coordinate-wise approaches (a.k.a. ``adaptive'' step sizes) to sophisticated heuristics to change the step size in each iteration. In this paper, we study two step size schedules whose power has been repeatedly confirmed in practice: the exponential and the cosine step sizes. For the first time, we provide theoretical support for them proving convergence rates for smooth non-convex functions, with and without the Polyak-\\L{}ojasiewicz (PL) condition. Moreover, we show the surprising property that these two strategies are \\emph{adaptive} to the noise level in the stochastic gradients of PL functions. That is, contrary to polynomial step sizes, they achieve almost optimal performance without needing to know the noise level nor tuning their hyperparameters based on it. Finally, we conduct a fair and comprehensive empirical evaluation of real-world datasets with deep learning architectures. Results show that, even if only requiring at most two hyperparameters to tune, these two strategies best or match the performance of various finely-tuned state-of-the-art strategies.",
        "conference": "ICML",
        "中文标题": "再探指数与余弦步长：简洁性、适应性与性能",
        "摘要翻译": "随机梯度下降（SGD）是训练大规模机器学习模型的流行工具。然而，其性能高度可变，关键取决于步长的选择。因此，人们提出了多种调整步长的策略，从坐标方式（即“自适应”步长）到复杂的启发式方法，以在每次迭代中改变步长。在本文中，我们研究了两种在实践中反复证实其效力的步长调度策略：指数步长和余弦步长。我们首次为它们提供了理论支持，证明了在平滑非凸函数上的收敛速率，无论是否满足Polyak-Łojasiewicz（PL）条件。此外，我们展示了这两种策略对PL函数随机梯度中噪声水平的惊人适应性。也就是说，与多项式步长不同，它们几乎不需要知道噪声水平，也不需要基于此调整超参数，就能实现近乎最优的性能。最后，我们在真实世界的数据集上使用深度学习架构进行了公平且全面的实证评估。结果表明，即使最多只需要调整两个超参数，这两种策略也能最佳或匹配各种精细调整的最先进策略的性能。",
        "领域": "优化算法",
        "问题": "如何在不了解噪声水平的情况下，自动适应并优化随机梯度下降中的步长选择",
        "动机": "探索和验证指数步长和余弦步长在理论上的有效性及其在实际应用中的性能",
        "方法": "理论分析指数和余弦步长在平滑非凸函数上的收敛性，并进行实证评估",
        "关键词": [
            "随机梯度下降",
            "步长调度",
            "自适应优化"
        ],
        "涉及的技术概念": {
            "指数步长": "一种步长调度策略，其值按指数规律变化，用于调整学习率",
            "余弦步长": "一种步长调度策略，其值按余弦函数规律变化，用于平滑调整学习率",
            "Polyak-Łojasiewicz条件": "一种用于分析优化算法收敛性的条件，本文中用于证明步长策略的理论有效性"
        },
        "success": true
    },
    {
        "order": 97,
        "title": "A Sharp Analysis of Model-based Reinforcement Learning with Self-Play",
        "html": "https://ICML.cc//virtual/2021/poster/8981",
        "abstract": "Model-based algorithms---algorithms that explore the environment through building and utilizing an estimated model---are widely used in reinforcement learning practice and theoretically shown to achieve optimal sample efficiency for single-agent reinforcement learning in Markov Decision Processes (MDPs). However, for multi-agent reinforcement learning in Markov games, the current best known sample complexity for model-based algorithms is rather suboptimal and compares unfavorably against recent model-free approaches. In this paper, we present a sharp analysis of model-based self-play algorithms for multi-agent Markov games. We design an algorithm \\emph{Optimistic Nash Value Iteration} (Nash-VI) for two-player zero-sum Markov games that is able to output an $\\epsilon$-approximate Nash policy in $\\tilde{\\mathcal{O}}(H^3SAB/\\epsilon^2)$ episodes of game playing, where $S$ is the number of states, $A,B$ are the number of actions for the two players respectively, and $H$ is the horizon length. This significantly improves over the best known model-based guarantee of $\\tilde{\\mathcal{O}}(H^4S^2AB/\\epsilon^2)$, and is the first that matches the information-theoretic lower bound $\\Omega(H^3S(A+B)/\\epsilon^2)$ except for a $\\min\\{A,B\\}$ factor. In addition, our guarantee compares favorably against the best known model-free algorithm if $\\min\\{A,B\\}=o(H^3)$, and outputs a single Markov policy while existing sample-efficient model-free algorithms output a nested mixture of Markov policies that is in general non-Markov and rather inconvenient to store and execute. We further adapt our analysis to designing a provably efficient task-agnostic algorithm for zero-sum Markov games, and designing the first line of provably sample-efficient algorithms for multi-player general-sum Markov games.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于模型的自博弈强化学习的精确分析",
        "摘要翻译": "基于模型的算法，即通过构建和利用估计模型来探索环境的算法，被广泛应用于强化学习实践中，并且在理论上被证明可以在马尔可夫决策过程（MDP）中实现单智能体强化学习的最佳样本效率。然而，对于马尔可夫博弈中的多智能体强化学习，当前已知的基于模型的算法的最佳样本复杂度相当不理想，并且与最近的无模型方法相比处于劣势。在本文中，我们对用于多智能体马尔可夫博弈的基于模型的自博弈算法进行了精确分析。我们为双人零和马尔可夫博弈设计了一种算法“乐观纳什值迭代”（Nash-VI），该算法能够在$\\tilde{\\mathcal{O}}(H^3SAB/\\epsilon^2)$次博弈中输出一个$\\epsilon$-近似纳什策略，其中$S$是状态的数量，$A,B$分别是两个玩家的动作数量，$H$是horizon长度。这显著优于已知的最佳基于模型的保证$\\tilde{\\mathcal{O}}(H^4S^2AB/\\epsilon^2)$，并且是第一个匹配信息论下界$\\Omega(H^3S(A+B)/\\epsilon^2)$的算法，除了一个$\\min\\{A,B\\}$因子。此外，如果$\\min\\{A,B\\}=o(H^3)$，我们的保证与已知的最佳无模型算法相比更具优势，并且输出一个单一的马尔可夫策略，而现有的样本高效无模型算法输出一个马尔可夫策略的嵌套混合，这种混合通常是非马尔可夫的，并且存储和执行起来相当不方便。我们进一步调整我们的分析，为零和马尔可夫博弈设计一种可证明有效的任务无关算法，并为多玩家一般和马尔可夫博弈设计第一批可证明样本高效的算法。",
        "领域": "多智能体强化学习, 马尔可夫博弈, 强化学习理论",
        "问题": "现有基于模型的算法在多智能体马尔可夫博弈中的样本复杂度不理想，且不如无模型方法。",
        "动机": "旨在提高基于模型的算法在多智能体马尔可夫博弈中的样本效率，使其与无模型方法相比更具竞争力，并接近信息论下界。",
        "方法": "设计了一种名为“乐观纳什值迭代”（Nash-VI）的算法，并对其进行了精确的样本复杂度分析，证明了其在双人零和马尔可夫博弈中的有效性。并推广到任务无关和一般和马尔可夫博弈。",
        "关键词": [
            "自博弈",
            "强化学习",
            "马尔可夫博弈",
            "样本复杂度",
            "纳什均衡"
        ],
        "涉及的技术概念": {
            "马尔可夫博弈": "一种多智能体环境，其中智能体的动作会影响彼此的状态和奖励，论文研究该环境下的强化学习算法。",
            "样本复杂度": "衡量算法学习到有效策略所需的样本数量，论文旨在降低基于模型的算法在该问题上的样本复杂度。"
        }
    },
    {
        "order": 98,
        "title": "A statistical perspective on distillation",
        "html": "https://ICML.cc//virtual/2021/poster/9065",
        "abstract": "Knowledge distillation is a technique for improving a ``student'' model by replacing its one-hot training labels with a label distribution obtained from a ``teacher'' model. Despite its broad success, several basic questions --- e.g., Why does distillation help? Why do more accurate teachers not necessarily distill better? --- have received limited formal study. In this paper, we present a statistical perspective on distillation which provides an answer to these questions. Our core observation is that a ``Bayes teacher'' providing the true class-probabilities can lower the variance of the student objective, and thus improve performance. We then establish a bias-variance tradeoff that quantifies the value of teachers that approximate the Bayes class-probabilities. This provides a formal criterion as to what constitutes a ``good'' teacher, namely, the quality of its probability estimates. Finally, we illustrate how our statistical perspective facilitates novel applications of distillation to bipartite ranking and multiclass retrieval. ",
        "conference": "ICML",
        "中文标题": "蒸馏的统计视角",
        "摘要翻译": "知识蒸馏是一种通过用从“教师”模型获得的标签分布替换“学生”模型的一热训练标签来改进“学生”模型的技术。尽管它取得了广泛的成功，但几个基本问题——例如，为什么蒸馏有帮助？为什么更准确的教师不一定能更好地蒸馏？——得到的正式研究有限。在本文中，我们提出了一个关于蒸馏的统计视角，为这些问题提供了答案。我们的核心观察是，提供真实类别概率的“贝叶斯教师”可以降低学生目标的方差，从而提高性能。然后，我们建立了一个偏差-方差权衡，量化了近似贝叶斯类别概率的教师的价值。这为“好”教师提供了一个正式的标准，即其概率估计的质量。最后，我们说明了我们的统计视角如何促进蒸馏在二分排名和多类检索中的新应用。",
        "领域": "模型压缩、知识蒸馏、机器学习理论",
        "问题": "理解知识蒸馏为何有效以及如何选择有效的教师模型",
        "动机": "探索知识蒸馏背后的统计原理，解决为何蒸馏有助于模型改进以及教师模型准确性与蒸馏效果之间关系的基本问题",
        "方法": "通过统计视角分析蒸馏过程，建立偏差-方差权衡模型，量化教师模型概率估计的质量对蒸馏效果的影响",
        "关键词": [
            "知识蒸馏",
            "统计视角",
            "偏差-方差权衡",
            "贝叶斯教师",
            "多类检索"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "一种模型压缩技术，通过教师模型指导学生模型的训练，以提高学生模型的性能",
            "偏差-方差权衡": "用于量化教师模型概率估计质量对蒸馏效果影响的统计模型",
            "贝叶斯教师": "提供真实类别概率的理想教师模型，用于降低学生模型训练目标的方差"
        },
        "success": true
    },
    {
        "order": 99,
        "title": "A Structured Observation Distribution for Generative Biological Sequence Prediction and Forecasting",
        "html": "https://ICML.cc//virtual/2021/poster/9725",
        "abstract": "Generative probabilistic modeling of biological sequences has widespread existing and potential application across biology and biomedicine, from evolutionary biology to epidemiology to protein design. Many standard sequence analysis methods preprocess data using a multiple sequence alignment (MSA) algorithm, one of the most widely used computational methods in all of science. However, as we show in this article, training generative probabilistic models with MSA preprocessing leads to statistical pathologies in the context of sequence prediction and forecasting. To address these problems, we propose a principled drop-in alternative to MSA preprocessing in the form of a structured observation distribution (the 'MuE' distribution). We prove theoretically that the MuE distribution comprehensively generalizes popular methods for inferring biological sequence alignments, and provide a precise characterization of how such biological models have differed from natural language latent alignment models. We show empirically that models that use the MuE as an observation distribution outperform comparable methods across a variety of datasets, and apply MuE models to a novel problem for generative probabilistic sequence models: forecasting pathogen evolution.",
        "conference": "ICML",
        "中文标题": "生成生物序列预测与预测的结构化观测分布",
        "摘要翻译": "生物序列的生成概率建模在生物学和生物医学领域有着广泛的应用，从进化生物学到流行病学再到蛋白质设计。许多标准的序列分析方法使用多序列比对（MSA）算法预处理数据，这是所有科学领域中最广泛使用的计算方法之一。然而，正如我们在本文中所示，在使用MSA预处理训练生成概率模型时，在序列预测和预测的背景下会导致统计病理学。为了解决这些问题，我们提出了一个原则性的替代MSA预处理的方法，即结构化观测分布（'MuE'分布）。我们从理论上证明了MuE分布全面概括了推断生物序列比对的流行方法，并精确描述了这些生物模型与自然语言潜在比对模型的不同之处。我们通过实证表明，使用MuE作为观测分布的模型在各种数据集上优于可比方法，并将MuE模型应用于生成概率序列模型的一个新问题：预测病原体进化。",
        "领域": "生物信息学、序列分析、病原体进化预测",
        "问题": "解决使用多序列比对（MSA）预处理训练生成概率模型时导致的统计病理学问题",
        "动机": "为了提供一个更有效的替代MSA预处理的方法，以提高生成概率模型在序列预测和预测中的性能",
        "方法": "提出结构化观测分布（MuE分布）作为MSA预处理的替代方法，理论上证明其优越性，并通过实证验证其性能",
        "关键词": [
            "结构化观测分布",
            "生成概率模型",
            "多序列比对",
            "序列预测",
            "病原体进化"
        ],
        "涉及的技术概念": {
            "结构化观测分布（MuE分布）": "作为MSA预处理的替代方法，用于提高生成概率模型在序列预测和预测中的性能",
            "多序列比对（MSA）": "一种广泛使用的序列分析方法，但在生成概率模型训练中可能导致统计病理学",
            "生成概率模型": "用于生物序列建模的方法，能够预测和预测序列的进化"
        },
        "success": true
    },
    {
        "order": 100,
        "title": "Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections",
        "html": "https://ICML.cc//virtual/2021/poster/9019",
        "abstract": "Gaussian noise injections (GNIs) are a family of simple and widely-used regularisation methods for training neural networks, where one injects additive or multiplicative Gaussian noise to the network activations at every iteration of the optimisation algorithm, which is typically chosen as stochastic gradient descent (SGD). In this paper, we focus on the so-called `implicit effect' of GNIs, which is the effect of the injected noise on the dynamics of SGD. We show that this effect induces an \\emph{asymmetric heavy-tailed noise} on SGD gradient updates. In order to model this modified dynamics, we first develop a Langevin-like stochastic differential equation that is driven by a general family of \\emph{asymmetric} heavy-tailed noise. Using this model we then formally prove that GNIs induce an `implicit bias', which varies depending on the heaviness of the tails and the level of asymmetry. Our empirical results confirm that different types of neural networks trained with GNIs are well-modelled by the proposed dynamics and that the implicit effect of these injections induces a bias that degrades the performance of networks. ",
        "conference": "ICML",
        "中文标题": "高斯噪声注入中的不对称重尾与隐式偏差",
        "摘要翻译": "高斯噪声注入（GNIs）是一类简单且广泛使用的神经网络训练正则化方法，其在优化算法（通常选择为随机梯度下降（SGD））的每次迭代中向网络激活注入加性或乘性高斯噪声。在本文中，我们专注于GNIs的所谓‘隐式效应’，即注入噪声对SGD动态的影响。我们展示了这种效应在SGD梯度更新上诱导了一种不对称重尾噪声。为了模拟这种修改后的动态，我们首先开发了一个由一般不对称重尾噪声家族驱动的朗之万型随机微分方程。利用这个模型，我们随后正式证明了GNIs诱导了一种‘隐式偏差’，这种偏差根据尾部的重度和不对称程度而变化。我们的实证结果证实，使用GNIs训练的不同类型神经网络能够很好地由所提出的动态模型所描述，并且这些注入的隐式效应诱导了一种偏差，降低了网络的性能。",
        "领域": "深度学习正则化方法、神经网络优化、随机梯度下降",
        "问题": "高斯噪声注入（GNIs）在训练神经网络时诱导的不对称重尾噪声及其对随机梯度下降（SGD）动态的隐式影响。",
        "动机": "研究GNIs对SGD动态的隐式效应，特别是其诱导的不对称重尾噪声如何影响神经网络的训练和性能。",
        "方法": "开发了一个由不对称重尾噪声驱动的朗之万型随机微分方程模型，用于模拟GNIs修改后的SGD动态，并通过实证研究验证模型的有效性。",
        "关键词": [
            "高斯噪声注入",
            "隐式偏差",
            "不对称重尾噪声",
            "随机梯度下降",
            "神经网络正则化"
        ],
        "涉及的技术概念": {
            "高斯噪声注入（GNIs）": "一种通过在神经网络训练过程中注入加性或乘性高斯噪声来正则化模型的方法。",
            "隐式偏差": "指GNIs对SGD动态的间接影响，导致训练过程中出现特定的偏差。",
            "不对称重尾噪声": "一种统计特性，表现为噪声分布的一侧比另一侧有更长的尾部，影响SGD的梯度更新动态。"
        },
        "success": true
    },
    {
        "order": 101,
        "title": "Asymmetric Loss Functions for Learning with Noisy Labels",
        "html": "https://ICML.cc//virtual/2021/poster/8595",
        "abstract": "Robust loss functions are essential for training deep neural networks with better generalization power in the presence of noisy labels. Symmetric loss functions are confirmed to be robust to label noise. However, the symmetric condition is overly restrictive. In this work, we propose a new class of loss functions, namely asymmetric loss functions, which are robust to learning from noisy labels for arbitrary noise type. Subsequently, we investigate general theoretical properties of asymmetric loss functions, including classification-calibration, excess risk bound, and noise-tolerance. Meanwhile, we introduce the asymmetry ratio to measure the asymmetry of a loss function, and the empirical results show that a higher ratio will provide better robustness. Moreover, we modify several common loss functions, and establish the necessary and sufficient conditions for them to be asymmetric. Experiments on benchmark datasets demonstrate that asymmetric loss functions can outperform state-of-the-art methods.",
        "conference": "ICML",
        "中文标题": "用于含噪声标签学习的不对称损失函数",
        "摘要翻译": "在存在噪声标签的情况下，鲁棒的损失函数对于训练具有更好泛化能力的深度神经网络至关重要。对称损失函数已被证实对标签噪声具有鲁棒性。然而，对称条件过于严格。在这项工作中，我们提出了一类新的损失函数，即不对称损失函数，它们对于任意类型的噪声标签学习都具有鲁棒性。随后，我们研究了不对称损失函数的一般理论性质，包括分类校准、超额风险界限和噪声容忍度。同时，我们引入了不对称比率来衡量损失函数的不对称性，实证结果表明，更高的比率将提供更好的鲁棒性。此外，我们修改了几种常见的损失函数，并建立了它们成为不对称损失函数的必要和充分条件。在基准数据集上的实验表明，不对称损失函数可以超越最先进的方法。",
        "领域": "深度学习优化、噪声标签学习、损失函数设计",
        "问题": "如何在存在噪声标签的情况下设计鲁棒的损失函数以提高深度神经网络的泛化能力",
        "动机": "现有的对称损失函数虽然对标签噪声具有鲁棒性，但其对称条件过于严格，限制了损失函数的灵活性和应用范围",
        "方法": "提出一类新的不对称损失函数，研究其理论性质，并通过修改常见损失函数来验证其有效性",
        "关键词": [
            "不对称损失函数",
            "噪声标签学习",
            "鲁棒性",
            "深度学习优化",
            "损失函数设计"
        ],
        "涉及的技术概念": {
            "不对称损失函数": "一类新的损失函数，对任意类型的噪声标签学习都具有鲁棒性",
            "不对称比率": "用于衡量损失函数不对称性的指标，更高的比率意味着更好的鲁棒性",
            "分类校准": "确保损失函数能够正确分类的理论性质之一"
        },
        "success": true
    },
    {
        "order": 102,
        "title": "Asymptotic Normality and Confidence Intervals for Prediction Risk of the Min-Norm Least Squares Estimator",
        "html": "https://ICML.cc//virtual/2021/poster/8631",
        "abstract": "This paper quantifies the uncertainty of prediction risk for the min-norm least squares estimator in high-dimensional linear regression models. We establish the asymptotic normality of prediction risk when both the sample size and the number of features tend to infinity. Based on the newly established central limit theorems(CLTs), we derive the confidence intervals of the prediction risk under various scenarios. Our results demonstrate the sample-wise non-monotonicity of the prediction risk and confirm ``more data hurt' phenomenon. Furthermore, the width of confidence intervals indicates that over-parameterization would enlarge the randomness of prediction performance.",
        "conference": "ICML",
        "中文标题": "最小范数最小二乘估计器预测风险的渐近正态性与置信区间",
        "摘要翻译": "本文量化了高维线性回归模型中最小范数最小二乘估计器预测风险的不确定性。当样本量和特征数量都趋向于无穷大时，我们建立了预测风险的渐近正态性。基于新建立的中心极限定理(CLTs)，我们在不同场景下推导了预测风险的置信区间。我们的结果展示了预测风险的样本非单调性，并证实了‘更多数据有害’现象。此外，置信区间的宽度表明，过度参数化会增加预测性能的随机性。",
        "领域": "高维统计、线性回归、预测模型",
        "问题": "量化高维线性回归模型中最小范数最小二乘估计器预测风险的不确定性，并建立其渐近正态性及置信区间。",
        "动机": "研究高维数据环境下预测风险的不确定性，理解样本量和特征数量对预测风险的影响，以及过度参数化对预测性能随机性的影响。",
        "方法": "建立预测风险的渐近正态性，基于中心极限定理推导不同场景下的置信区间，分析样本非单调性和过度参数化的影响。",
        "关键词": [
            "高维统计",
            "最小范数最小二乘",
            "预测风险",
            "渐近正态性",
            "置信区间"
        ],
        "涉及的技术概念": {
            "最小范数最小二乘估计器": "用于高维线性回归的参数估计，通过最小化参数向量的范数来解决过参数化问题。",
            "渐近正态性": "描述当样本量趋向于无穷大时，估计量的分布趋近于正态分布的性质，用于建立置信区间。",
            "中心极限定理(CLTs)": "在样本量足够大时，独立随机变量的和近似服从正态分布，为推导预测风险的置信区间提供理论基础。"
        },
        "success": true
    },
    {
        "order": 103,
        "title": "Asymptotics of Ridge Regression in Convolutional Models",
        "html": "https://ICML.cc//virtual/2021/poster/9899",
        "abstract": "Understanding generalization and estimation error of estimators for simple models such as linear and generalized linear models has attracted a lot of attention recently. This is in part due to an interesting observation made in machine learning community that highly over-parameterized neural networks achieve zero training error, and yet they are able to generalize well over the test samples. This phenomenon is captured by the so called double descent curve, where the generalization error starts decreasing again after the interpolation threshold. A series of recent works tried to explain such phenomenon for simple models. In this work, we analyze the asymptotics of estimation error in ridge estimators for convolutional linear models. These convolutional inverse problems, also known as deconvolution, naturally arise in different fields such as seismology, imaging, and acoustics among others. Our results hold for a large class of input distributions that include i.i.d. features as a special case. We derive exact formulae for estimation error of ridge estimators that hold in a certain high-dimensional regime. We show the double descent phenomenon in our experiments for convolutional models and show that our theoretical results match the experiments.",
        "conference": "ICML",
        "中文标题": "卷积模型中岭回归的渐近性",
        "摘要翻译": "最近，对于线性模型和广义线性模型等简单模型的估计器的泛化能力和估计误差的理解引起了广泛关注。这部分归因于机器学习社区中一个有趣的观察，即高度过参数化的神经网络能够达到零训练误差，同时它们还能在测试样本上表现出良好的泛化能力。这种现象被称为双下降曲线，其中泛化误差在插值阈值之后再次开始下降。一系列最近的工作试图为简单模型解释这种现象。在这项工作中，我们分析了卷积线性模型中岭估计器估计误差的渐近性。这些卷积逆问题，也称为解卷积，自然地出现在地震学、成像和声学等不同领域。我们的结果适用于包括独立同分布特征作为特例在内的一大类输入分布。我们推导了岭估计器估计误差的精确公式，这些公式在某种高维状态下成立。我们在卷积模型的实验中展示了双下降现象，并表明我们的理论结果与实验相符。",
        "领域": "卷积神经网络、岭回归、解卷积",
        "问题": "分析卷积线性模型中岭估计器估计误差的渐近性，以理解高度过参数化模型的泛化能力。",
        "动机": "解释高度过参数化神经网络在达到零训练误差的同时，仍能在测试样本上表现出良好泛化能力的现象。",
        "方法": "分析卷积线性模型中岭估计器的估计误差，推导在高维状态下成立的精确公式，并通过实验验证理论结果。",
        "关键词": [
            "岭回归",
            "卷积模型",
            "双下降曲线",
            "高维统计",
            "解卷积"
        ],
        "涉及的技术概念": {
            "岭回归": "一种用于处理共线性数据的回归分析方法，通过引入L2正则化项来防止过拟合。",
            "双下降曲线": "描述模型复杂度增加时，泛化误差先下降后上升再下降的现象。",
            "解卷积": "一种信号处理技术，用于从观测到的信号中恢复原始信号，广泛应用于成像和声学等领域。"
        },
        "success": true
    },
    {
        "order": 104,
        "title": "Asynchronous Decentralized Optimization With Implicit Stochastic Variance Reduction",
        "html": "https://ICML.cc//virtual/2021/poster/8957",
        "abstract": "A novel asynchronous decentralized optimization method that follows Stochastic Variance Reduction (SVR) is proposed. Average consensus algorithms, such as Decentralized Stochastic Gradient Descent (DSGD), facilitate distributed training of machine learning models. However, the gradient will drift within the local nodes due to statistical heterogeneity of the subsets of data residing on the nodes and long communication intervals. To overcome the drift problem, (i) Gradient Tracking-SVR (GT-SVR) integrates SVR into DSGD and (ii) Edge-Consensus Learning (ECL) solves a model constrained minimization problem using a primal-dual formalism. In this paper, we reformulate the update procedure of ECL such that it implicitly includes the gradient modification of SVR by optimally selecting a constraint-strength control parameter. Through  convergence analysis and experiments, we confirmed that the proposed ECL with Implicit SVR (ECL-ISVR) is stable and approximately reaches the reference performance obtained with computation on a single-node using full data set. ",
        "conference": "ICML",
        "中文标题": "具有隐式随机方差减少的异步分散优化",
        "摘要翻译": "提出了一种新颖的异步分散优化方法，该方法遵循随机方差减少（SVR）。平均共识算法，如分散随机梯度下降（DSGD），促进了机器学习模型的分布式训练。然而，由于节点上数据子集的统计异质性和长通信间隔，梯度会在本地节点内漂移。为了克服漂移问题，（i）梯度跟踪-SVR（GT-SVR）将SVR集成到DSGD中，（ii）边缘共识学习（ECL）使用原始-对偶形式主义解决模型约束最小化问题。在本文中，我们重新制定了ECL的更新过程，使其通过最优选择约束强度控制参数隐式地包括SVR的梯度修改。通过收敛分析和实验，我们确认提出的具有隐式SVR的ECL（ECL-ISVR）是稳定的，并且大约达到了使用完整数据集在单节点上计算获得的参考性能。",
        "领域": "分布式机器学习优化, 异步优化算法, 随机方差减少技术",
        "问题": "解决在分布式机器学习训练中由于数据统计异质性和长通信间隔导致的梯度漂移问题",
        "动机": "提高分布式机器学习模型训练的效率和准确性，减少梯度漂移对模型性能的影响",
        "方法": "通过将随机方差减少（SVR）技术集成到分散随机梯度下降（DSGD）中，并重新制定边缘共识学习（ECL）的更新过程，隐式地包括SVR的梯度修改",
        "关键词": [
            "异步分散优化",
            "随机方差减少",
            "边缘共识学习",
            "梯度跟踪",
            "分布式机器学习"
        ],
        "涉及的技术概念": {
            "随机方差减少（SVR）": "用于减少梯度估计的方差，提高优化算法的收敛速度和稳定性",
            "分散随机梯度下降（DSGD）": "一种分布式优化算法，用于在多个节点上并行训练机器学习模型",
            "边缘共识学习（ECL）": "一种解决模型约束最小化问题的方法，通过原始-对偶形式主义优化模型训练过程"
        },
        "success": true
    },
    {
        "order": 105,
        "title": "Asynchronous Distributed Learning : Adapting to Gradient Delays without Prior Knowledge",
        "html": "https://ICML.cc//virtual/2021/poster/10171",
        "abstract": "We consider stochastic convex optimization problems, where several machines act asynchronously in parallel while sharing a common memory. We propose a robust training method for the constrained setting and derive non asymptotic convergence guarantees that do not depend on prior knowledge of update delays, objective smoothness, and gradient variance. Conversely, existing methods for this setting crucially rely on this prior knowledge, which render them unsuitable for essentially all shared-resources computational environments, such as clouds and data centers. Concretely, existing approaches are unable to accommodate changes in the delays which result from dynamic allocation of the machines, while our method implicitly adapts to such changes. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "异步分布式学习：无需先验知识适应梯度延迟",
        "摘要翻译": "我们考虑随机凸优化问题，其中多台机器在共享公共内存的同时异步并行工作。我们提出了一种针对约束环境的鲁棒训练方法，并推导出不依赖于更新延迟、目标平滑度和梯度方差的先验知识的非渐进收敛保证。相反，针对此环境的现有方法严重依赖于这些先验知识，这使得它们基本上不适合所有共享资源的计算环境，如云和数据中心。具体来说，现有方法无法适应由机器动态分配引起的延迟变化，而我们的方法能够隐式适应这些变化。",
        "领域": "分布式机器学习、异步优化、随机凸优化",
        "问题": "在共享资源的计算环境中，如何在不依赖先验知识的情况下适应梯度延迟进行异步分布式学习。",
        "动机": "现有的异步分布式学习方法严重依赖于对更新延迟、目标平滑度和梯度方差的先验知识，这限制了它们在动态共享资源环境（如云和数据中心）中的应用。",
        "方法": "提出了一种鲁棒的训练方法，能够在不需要先验知识的情况下适应梯度延迟，并提供了非渐进收敛的理论保证。",
        "关键词": [
            "异步分布式学习",
            "梯度延迟",
            "随机凸优化",
            "鲁棒训练",
            "非渐进收敛"
        ],
        "涉及的技术概念": {
            "异步分布式学习": "在多台机器异步并行工作的环境下进行的学习方法，无需等待所有机器同步更新。",
            "梯度延迟": "在分布式学习中，由于机器间的通信和计算速度不同，梯度更新可能出现的延迟现象。",
            "随机凸优化": "在随机梯度下降框架下，针对凸优化问题的求解方法，适用于大规模数据集。"
        }
    },
    {
        "order": 106,
        "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions",
        "html": "https://ICML.cc//virtual/2021/poster/9735",
        "abstract": "Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full softmax is costly from the computational and energy perspective. There have been various sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there is no sampling scheme that is provably adaptive and samples the negative classes efficiently. Therefore, alternative heuristics like random sampling, static frequency-based sampling, or learning-based biased sampling, which primarily trade either the sampling cost or the adaptivity of samples per iteration are adopted. In this paper, we show two classes of distributions where the sampling scheme is truly adaptive and provably generates negative samples in near-constant time. Our implementation in C++ on CPU is significantly superior, both in terms of wall-clock time and accuracy, compared to the most optimized TensorFlow implementations of other popular negative sampling approaches on powerful NVIDIA V100 GPU.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "两种高效且信息丰富的负采样分布的故事",
        "摘要翻译": "在自然语言处理和信息检索等许多应用中，非常大量类别的Softmax分类器自然出现。从计算和能源的角度来看，完整Softmax的计算成本很高。为了克服这一挑战，已经出现了各种采样方法，通常称为负采样（NS）。理想情况下，NS应该从依赖于输入数据、当前参数和正确正类别的分布中采样负类别。不幸的是，由于动态更新的参数和数据样本，没有采样方案是可证明自适应的，并且能高效地采样负类别。因此，采用了替代启发式方法，如随机采样、基于静态频率的采样或基于学习的偏置采样，这些方法主要在每次迭代中牺牲采样成本或样本的自适应性。在本文中，我们展示了两类分布，其中采样方案是真正自适应的，并且可证明在近恒定时间内生成负样本。我们在CPU上的C++实现，在墙钟时间和准确性方面，都比其他流行的负采样方法在强大的NVIDIA V100 GPU上的最优TensorFlow实现显著优越。",
        "领域": "自然语言处理, 信息检索, 机器学习优化",
        "问题": "解决在大规模类别Softmax分类器中，计算和能源成本高的问题",
        "动机": "克服在大规模类别Softmax分类器中，由于动态更新的参数和数据样本，缺乏高效且自适应的负采样方法的问题",
        "方法": "提出了两类分布，其中采样方案是真正自适应的，并且能在近恒定时间内生成负样本，通过C++在CPU上实现，优化了计算效率和准确性",
        "关键词": [
            "负采样",
            "Softmax分类器",
            "自适应采样",
            "计算效率",
            "机器学习优化"
        ],
        "涉及的技术概念": {
            "负采样（NS）": "一种采样方法，用于在大规模类别Softmax分类器中减少计算和能源成本",
            "自适应采样": "根据输入数据、当前参数和正确正类别动态调整采样策略，以提高效率和准确性",
            "计算效率优化": "通过算法和实现优化，减少计算时间和能源消耗，同时保持或提高模型性能"
        }
    },
    {
        "order": 107,
        "title": "A theory of high dimensional regression with arbitrary correlations between input features and target functions: sample complexity, multiple descent curves and a hierarchy of phase transitions",
        "html": "https://ICML.cc//virtual/2021/poster/10415",
        "abstract": "The performance of neural networks depends on precise relationships between four distinct ingredients: the architecture, the loss function, the statistical structure of inputs, and the ground truth target function. \nMuch theoretical work has focused on understanding the role of the first two ingredients under highly simplified models of random uncorrelated data and target functions. \nIn contrast, performance likely relies on a conspiracy between the statistical structure of the input distribution and the structure of the function to be learned.  \nTo understand this better we revisit ridge regression in high dimensions, which corresponds to an exceedingly simple architecture and loss function, but we analyze its performance under arbitrary correlations between input features and the target function.  \nWe find a rich mathematical structure that includes: (1) a dramatic reduction in sample complexity when the target function aligns with data anisotropy; (2) the existence of multiple descent curves; (3) a sequence of phase transitions in the performance, loss landscape, and optimal regularization as a function of the amount of data that explains the first two effects.  ",
        "conference": "ICML",
        "success": true,
        "中文标题": "高维回归理论：输入特征与目标函数间任意相关性的样本复杂度、多重下降曲线及相变层次",
        "摘要翻译": "神经网络的性能取决于四个不同要素之间的精确关系：架构、损失函数、输入的统计结构以及真实目标函数。许多理论工作集中在高度简化的随机不相关数据和目标函数模型下理解前两个要素的作用。相比之下，性能很可能依赖于输入分布的统计结构与待学习函数结构之间的共谋。为了更好地理解这一点，我们重新审视高维岭回归，它对应于极其简单的架构和损失函数，但我们分析了在输入特征与目标函数之间任意相关性下的性能。我们发现了一个丰富的数学结构，包括：（1）当目标函数与数据各向异性对齐时，样本复杂度显著降低；（2）多重下降曲线的存在；（3）性能、损失景观和最优正则化作为数据量的函数的相变序列，这解释了前两个效应。",
        "领域": "高维统计学习、岭回归分析、神经网络理论",
        "问题": "理解在高维回归中，输入特征与目标函数之间的任意相关性如何影响样本复杂度、学习曲线和相变行为。",
        "动机": "探索神经网络性能背后的数学结构，特别是在输入数据统计结构与目标函数结构之间存在复杂关系时的表现。",
        "方法": "通过高维岭回归模型，分析在输入特征与目标函数间存在任意相关性时的性能表现，揭示样本复杂度、多重下降曲线及相变序列。",
        "关键词": [
            "高维回归",
            "岭回归",
            "样本复杂度",
            "多重下降曲线",
            "相变"
        ],
        "涉及的技术概念": {
            "高维岭回归": "用于分析高维数据下输入特征与目标函数间任意相关性对模型性能影响的统计方法。",
            "样本复杂度": "描述学习算法达到一定性能水平所需样本数量的度量，本文中探讨了其与数据各向异性的关系。",
            "多重下降曲线": "指在学习过程中，模型性能随样本数量增加而出现的多次下降现象，反映了学习动态的复杂性。"
        }
    },
    {
        "order": 108,
        "title": "A Theory of Label Propagation for Subpopulation Shift",
        "html": "https://ICML.cc//virtual/2021/poster/9441",
        "abstract": "One of the central problems in machine learning is domain adaptation. Different from past theoretical works, we consider a new model of subpopulation shift in the input or representation space. In this work, we propose a provably effective framework based on label propagation by using an input consistency loss. In our analysis we used a simple but realistic “expansion” assumption, which has been proposed in \\citet{wei2021theoretical}. It turns out that based on a teacher classifier on the source domain, the learned classifier can not only propagate to the target domain but also improve upon the teacher. By leveraging existing generalization bounds, we also obtain end-to-end finite-sample guarantees on deep neural networks. In addition, we extend our theoretical framework to a more general setting of source-to-target transfer based on an additional unlabeled dataset, which can be easily applied to various learning scenarios. Inspired by our theory, we adapt consistency-based semi-supervised learning methods to domain adaptation settings and gain significant improvements.",
        "conference": "ICML",
        "success": true,
        "中文标题": "亚群体偏移下的标签传播理论",
        "摘要翻译": "领域自适应是机器学习中的一个核心问题。与以往的理论工作不同，我们考虑输入或表示空间中亚群体偏移的新模型。在这项工作中，我们提出了一个基于标签传播的、可证明有效的框架，该框架利用输入一致性损失。在我们的分析中，我们使用了简单但现实的“扩张”假设，该假设已在[wei2021theoretical]中提出。结果表明，基于源域上的教师分类器，学习到的分类器不仅可以传播到目标域，而且可以优于教师。通过利用现有的泛化界限，我们还获得了关于深度神经网络的端到端有限样本保证。此外，我们将我们的理论框架扩展到基于额外的无标签数据集的更一般的源到目标迁移设置，该设置可以很容易地应用于各种学习场景。受到我们理论的启发，我们将基于一致性的半监督学习方法应用于领域自适应设置，并获得了显著的改进。",
        "领域": "领域自适应，半监督学习，泛化理论",
        "问题": "解决输入或表示空间中亚群体偏移下的领域自适应问题，提升目标域的分类性能。",
        "动机": "现有领域自适应理论工作较少关注亚群体偏移，而该现象在实际应用中普遍存在。本研究旨在通过标签传播和输入一致性损失，提出一种在该偏移下依然有效的领域自适应方法。",
        "方法": "提出基于标签传播的框架，利用输入一致性损失，结合“扩张”假设和泛化界限，推导出深度神经网络的有限样本保证。并将一致性半监督学习方法应用于领域自适应。",
        "关键词": [
            "领域自适应",
            "标签传播",
            "亚群体偏移",
            "输入一致性",
            "半监督学习"
        ],
        "涉及的技术概念": {
            "标签传播": "利用源域已标记数据的信息，通过某种机制（如相似性度量）将标签信息传递到目标域未标记数据，从而实现领域自适应。",
            "输入一致性损失": "通过约束模型对输入数据及其扰动后的数据产生一致的预测结果，从而提升模型的鲁棒性和泛化能力。"
        }
    },
    {
        "order": 109,
        "title": "Attention is not all you need: pure attention loses rank doubly exponentially with depth",
        "html": "https://ICML.cc//virtual/2021/poster/9821",
        "abstract": "Attention-based architectures have become ubiquitous in machine learning. \nYet, our understanding of the reasons for their effectiveness remains limited. \nThis work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms---or paths---each involving the operation of a sequence of attention heads across layers. \nUsing this path decomposition, we prove that self-attention possesses a strong inductive bias towards 'token uniformity'. Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.",
        "conference": "ICML",
        "中文标题": "注意力并非万能：纯注意力机制随深度增加而双指数级丧失秩",
        "摘要翻译": "基于注意力的架构在机器学习中已无处不在。然而，我们对其有效性原因的理解仍然有限。这项工作提出了一种理解自注意力网络的新方法：我们展示了它们的输出可以被分解为较小的项——或路径——的总和，每一项都涉及跨层的注意力头序列的操作。利用这种路径分解，我们证明了自注意力具有强烈的‘令牌均匀性’归纳偏置。具体来说，在没有跳跃连接或多层感知器（MLPs）的情况下，输出会双指数级收敛到一个秩-1矩阵。另一方面，跳跃连接和MLPs阻止了输出的退化。我们的实验在标准变压器架构上验证了这些收敛结果。",
        "领域": "自然语言处理与视觉结合, 深度学习模型架构, 注意力机制研究",
        "问题": "理解自注意力网络的有效性及其输出退化问题",
        "动机": "探索自注意力网络的工作原理及其在缺乏特定结构（如跳跃连接和MLPs）时的输出退化现象",
        "方法": "通过路径分解方法分析自注意力网络的输出，并研究跳跃连接和MLPs对防止输出退化的作用",
        "关键词": [
            "自注意力网络",
            "路径分解",
            "令牌均匀性",
            "跳跃连接",
            "多层感知器"
        ],
        "涉及的技术概念": {
            "自注意力网络": "论文中研究的核心架构，用于捕捉输入数据间的长距离依赖关系",
            "路径分解": "论文中提出的方法，用于分解自注意力网络的输出为多个路径的总和，以理解其工作原理",
            "令牌均匀性": "论文中证明的自注意力网络的归纳偏置，指网络倾向于使所有令牌的输出趋同"
        },
        "success": true
    },
    {
        "order": 110,
        "title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment",
        "html": "https://ICML.cc//virtual/2021/poster/10307",
        "abstract": "Reinforcement learning from large-scale offline datasets provides us with the ability to learn policies without potentially unsafe or impractical exploration. Significant progress has been made in the past few years in dealing with the challenge of correcting for differing behavior between the data collection and learned policies. However, little attention has been paid to potentially changing dynamics when transferring a policy to the online setting, where performance can be up to 90% reduced for existing methods. In this paper we address this problem with Augmented World Models (AugWM). We augment a learned dynamics model with simple transformations that seek to capture potential changes in physical properties of the robot, leading to more robust policies. We not only train our policy in this new setting, but also provide it with the sampled augmentation as a context, allowing it to adapt to changes in the environment. At test time we learn the context in a self-supervised fashion by approximating the augmentation which corresponds to the new environment. We rigorously evaluate our approach on over 100 different changed dynamics settings, and show that this simple approach can significantly improve the zero-shot generalization of a recent state-of-the-art baseline, often achieving successful policies where the baseline fails.",
        "conference": "ICML",
        "中文标题": "增强世界模型促进从单一离线环境的零样本动态泛化",
        "摘要翻译": "从大规模离线数据集中进行强化学习使我们能够在不需要可能不安全或不切实际的探索的情况下学习策略。过去几年中，在解决数据收集和学习策略之间行为差异的挑战方面取得了显著进展。然而，当将策略转移到在线设置时，对于可能变化的动态特性，现有方法的性能可能会降低多达90%，这一点却鲜少受到关注。在本文中，我们通过增强世界模型（AugWM）来解决这个问题。我们通过学习到的动态模型进行简单变换来增强，这些变换旨在捕捉机器人物理特性的潜在变化，从而产生更鲁棒的策略。我们不仅在这种新设置中训练我们的策略，而且还为策略提供了采样的增强作为上下文，使其能够适应环境的变化。在测试时，我们通过近似对应于新环境的增强，以自监督的方式学习上下文。我们在超过100种不同的动态变化设置上严格评估了我们的方法，并表明这种简单的方法可以显著提高最新基线方法的零样本泛化能力，通常在基线方法失败的情况下实现成功的策略。",
        "领域": "强化学习、机器人控制、动态模型泛化",
        "问题": "解决在将离线学习的策略转移到在线环境时，由于动态特性变化导致的性能下降问题。",
        "动机": "现有方法在动态特性变化的在线环境中性能显著下降，需要一种能够提高策略在未知动态环境下泛化能力的方法。",
        "方法": "通过增强世界模型（AugWM）学习动态模型的变换，捕捉潜在的环境动态变化，训练策略时提供增强上下文，测试时自监督学习新环境的上下文。",
        "关键词": [
            "强化学习",
            "动态模型泛化",
            "零样本学习",
            "机器人控制",
            "自监督学习"
        ],
        "涉及的技术概念": {
            "增强世界模型（AugWM）": "通过学习动态模型的变换来增强，旨在捕捉环境动态特性的潜在变化，提高策略的泛化能力。",
            "零样本泛化": "指策略在没有直接学习过的新环境或任务中表现良好的能力。",
            "自监督学习": "在测试时通过近似新环境的增强来自主学习上下文，无需外部标注。"
        },
        "success": true
    },
    {
        "order": 111,
        "title": "A Unified Generative Adversarial Network Training via Self-Labeling and Self-Attention",
        "html": "https://ICML.cc//virtual/2021/poster/9517",
        "abstract": "We propose a novel GAN training scheme that can handle any level of labeling in a unified manner.\nOur scheme introduces a form of artificial labeling that can incorporate manually defined labels, when available, and induce an alignment between them.\nTo define the artificial labels, we exploit the assumption that neural network generators can be trained more easily to map nearby latent vectors to data with semantic similarities, than across separate categories.\nWe use generated data samples and their corresponding artificial conditioning labels to train a classifier. \nThe classifier is then used to self-label real data. To boost the accuracy of the self-labeling, we also use the exponential moving average of the classifier.\nHowever, because the classifier might still make  mistakes, especially at the beginning of the training, we also refine the labels through self-attention, by using the labeling of real data samples only when the classifier outputs a high classification probability score.\nWe evaluate our approach on CIFAR-10, STL-10 and SVHN, and show that both self-labeling and self-attention consistently improve the quality of generated data. More surprisingly, we find that the proposed scheme can even outperform class-conditional GANs. ",
        "conference": "ICML",
        "中文标题": "一种通过自标记和自注意力统一的生成对抗网络训练方法",
        "摘要翻译": "我们提出了一种新颖的生成对抗网络（GAN）训练方案，该方案能够以统一的方式处理任何级别的标记。我们的方案引入了一种人工标记形式，可以在可用时合并手动定义的标记，并在它们之间诱导对齐。为了定义人工标记，我们利用了神经网络生成器可以更容易地被训练将附近的潜在向量映射到具有语义相似性的数据，而不是跨不同类别的假设。我们使用生成的数据样本及其相应的人工条件标记来训练分类器。然后，该分类器用于自标记真实数据。为了提高自标记的准确性，我们还使用了分类器的指数移动平均。然而，由于分类器可能仍然会犯错误，特别是在训练开始时，我们还通过自注意力来细化标记，仅当分类器输出高分类概率分数时，才使用真实数据样本的标记。我们在CIFAR-10、STL-10和SVHN上评估了我们的方法，并表明自标记和自注意力都一致提高了生成数据的质量。更令人惊讶的是，我们发现所提出的方案甚至可以超越类条件GANs。",
        "领域": "生成对抗网络、图像生成、半监督学习",
        "问题": "如何在标记数据不足或标记级别不一致的情况下，统一训练生成对抗网络",
        "动机": "解决生成对抗网络在标记数据不足或标记级别不一致时的训练问题，提高生成数据的质量和多样性",
        "方法": "提出了一种结合自标记和自注意力的统一训练方案，通过人工标记和分类器的自标记能力，以及自注意力机制细化标记，来优化GAN的训练过程",
        "关键词": [
            "生成对抗网络",
            "自标记",
            "自注意力",
            "半监督学习",
            "图像生成"
        ],
        "涉及的技术概念": {
            "自标记": "通过训练分类器对生成的数据进行标记，然后用于标记真实数据，以减少对大量手动标记数据的依赖",
            "自注意力": "通过自注意力机制细化分类器的标记结果，仅在分类器输出高置信度时使用标记，以提高标记的准确性",
            "指数移动平均": "用于平滑分类器的输出，减少训练初期的波动，提高自标记的稳定性"
        },
        "success": true
    },
    {
        "order": 112,
        "title": "A Unified Lottery Ticket Hypothesis for Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10503",
        "abstract": "With graphs rapidly growing in size and deeper graph neural networks (GNNs) emerging, the training and inference of GNNs become increasingly expensive. Existing network weight pruning algorithms cannot address the main space and computational bottleneck in GNNs, caused by the size and connectivity of the graph. To this end, this paper first presents a unified GNN sparsification (UGS) framework that simultaneously prunes the graph adjacency matrix and the model weights, for effectively accelerating GNN inference on large-scale graphs. Leveraging this new tool, we further generalize the recently popular lottery ticket hypothesis to  GNNs for the first time, by defining a graph lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network, which can be jointly identified from the original GNN and the full dense graph by iteratively applying UGS. Like its counterpart in convolutional neural networks, GLT can be trained in isolation to match the performance of training with the full model and graph, and can be drawn from both randomly initialized and self-supervised pre-trained GNNs. Our proposal has been experimentally verified across various GNN architectures and diverse tasks, on both small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale datasets from the challenging Open Graph Benchmark (OGB). Specifically, for node classification, our found GLTs achieve the same accuracies with 20%~98% MACs saving on small graphs and 25%~85% MACs saving on large ones. For link prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph datasets, respectively, without compromising predictive performance. Codes are at https://github.com/VITA-Group/Unified-LTH-GNN.",
        "conference": "ICML",
        "中文标题": "图神经网络的统一彩票假设",
        "摘要翻译": "随着图规模的迅速扩大和更深层次的图神经网络（GNNs）的出现，GNNs的训练和推理变得越来越昂贵。现有的网络权重剪枝算法无法解决GNNs中由图的规模和连接性引起的主要空间和计算瓶颈。为此，本文首先提出了一个统一的GNN稀疏化（UGS）框架，该框架同时剪枝图邻接矩阵和模型权重，以有效加速大规模图上的GNN推理。利用这一新工具，我们进一步首次将最近流行的彩票假设推广到GNNs，通过定义一个图彩票（GLT）作为核心子数据集和稀疏子网络的配对，可以通过迭代应用UGS从原始GNN和全密集图中联合识别。与卷积神经网络中的对应物一样，GLT可以单独训练以匹配使用完整模型和图训练的性能，并且可以从随机初始化和自监督预训练的GNNs中提取。我们的提案已经在各种GNN架构和多样化任务上进行了实验验证，包括小规模图数据集（Cora、Citeseer和PubMed）和来自具有挑战性的开放图基准（OGB）的大规模数据集。具体来说，对于节点分类，我们发现的GLT在小图上实现了相同的准确度，同时节省了20%~98%的MACs，在大图上节省了25%~85%的MACs。对于链接预测，GLT在小和大图数据集上分别节省了48%~97%和70%的MACs，而不影响预测性能。代码可在https://github.com/VITA-Group/Unified-LTH-GNN找到。",
        "领域": "图神经网络优化、图稀疏化、深度学习效率提升",
        "问题": "解决图神经网络在大规模图上的训练和推理成本高昂的问题",
        "动机": "随着图规模的扩大和GNNs的深入应用，传统的剪枝算法无法有效降低计算和存储成本，需要一种新的方法来同时优化图和模型的结构",
        "方法": "提出统一的GNN稀疏化框架（UGS），同时剪枝图邻接矩阵和模型权重，并首次将彩票假设推广到GNNs，定义图彩票（GLT）作为核心子数据集和稀疏子网络的配对",
        "关键词": [
            "图神经网络",
            "彩票假设",
            "图稀疏化",
            "计算效率",
            "模型剪枝"
        ],
        "涉及的技术概念": {
            "统一GNN稀疏化（UGS）框架": "同时剪枝图邻接矩阵和模型权重的框架，用于加速GNN推理",
            "图彩票（GLT）": "由核心子数据集和稀疏子网络组成的配对，能够单独训练以匹配完整模型的性能",
            "MACs节省": "通过GLT实现的乘加运算（MACs）的节省，衡量计算效率的提升"
        },
        "success": true
    },
    {
        "order": 113,
        "title": "AutoAttend: Automated Attention Representation Search",
        "html": "https://ICML.cc//virtual/2021/poster/8657",
        "abstract": "Self-attention mechanisms have been widely adopted in many machine learning areas, including Natural Language Processing (NLP) and Graph Representation Learning (GRL), etc. However, existing works heavily rely on hand-crafted design to obtain customized attention mechanisms. In this paper, we automate Key, Query and Value representation design, which is one of the most important steps to obtain effective self-attentions. We propose an automated self-attention representation model, AutoAttend, which can automatically search powerful attention representations for downstream tasks leveraging Neural Architecture Search (NAS). In particular, we design a tailored search space for attention representation automation, which is flexible to produce effective attention representation designs. Based on the design prior obtained from attention representations in previous works, we further regularize our search space to reduce the space complexity without the loss of expressivity. Moreover, we propose a novel context-aware parameter sharing mechanism considering special characteristics of each sub-architecture to provide more accurate architecture estimations when conducting parameter sharing in our tailored search space. Experiments show the superiority of our proposed AutoAttend model over previous state-of-the-arts on eight text classification tasks in NLP and four node classification tasks in GRL.",
        "conference": "ICML",
        "中文标题": "AutoAttend：自动化注意力表示搜索",
        "摘要翻译": "自注意力机制已被广泛应用于许多机器学习领域，包括自然语言处理（NLP）和图表示学习（GRL）等。然而，现有工作严重依赖手工设计来获得定制的注意力机制。在本文中，我们自动化了关键、查询和值表示的设计，这是获得有效自注意力最重要的一步。我们提出了一个自动化的自注意力表示模型AutoAttend，该模型能够利用神经架构搜索（NAS）自动为下游任务搜索强大的注意力表示。特别是，我们设计了一个专门用于注意力表示自动化的搜索空间，该空间灵活，能够产生有效的注意力表示设计。基于从先前工作中的注意力表示获得的设计先验，我们进一步规范化我们的搜索空间，以减少空间复杂度而不损失表达能力。此外，考虑到每个子架构的特殊特性，我们提出了一种新颖的上下文感知参数共享机制，以在我们的定制搜索空间中进行参数共享时提供更准确的架构估计。实验显示，我们提出的AutoAttend模型在NLP的八个文本分类任务和GRL的四个节点分类任务上优于之前的最先进技术。",
        "领域": "自然语言处理与视觉结合, 图表示学习, 神经架构搜索",
        "问题": "自动化设计自注意力机制中的关键、查询和值表示",
        "动机": "减少对手工设计自注意力机制的依赖，提高模型的自动化水平和效率",
        "方法": "利用神经架构搜索（NAS）自动化搜索自注意力表示，设计专门的搜索空间和上下文感知参数共享机制",
        "关键词": [
            "自注意力机制",
            "神经架构搜索",
            "自动化设计",
            "上下文感知",
            "参数共享"
        ],
        "涉及的技术概念": {
            "自注意力机制": "用于捕捉输入数据内部依赖关系的机制，是模型理解复杂模式的关键",
            "神经架构搜索": "自动化机器学习模型设计的技术，用于高效探索最优模型架构",
            "上下文感知参数共享": "在神经架构搜索中，根据子架构特性动态调整参数共享策略，以提高搜索效率和模型性能"
        },
        "success": true
    },
    {
        "order": 114,
        "title": "Autoencoder Image Interpolation by Shaping the Latent Space",
        "html": "https://ICML.cc//virtual/2021/poster/8769",
        "abstract": "One of the fascinating properties of deep learning is the ability of the network to reveal the underlying factors characterizing elements in datasets of different types. Autoencoders represent an effective approach for computing these factors. Autoencoders have been studied in the context of enabling interpolation between data points by decoding convex combinations of latent vectors. However, this interpolation often leads to artifacts or produces unrealistic results during reconstruction. We argue that these incongruities are due to the structure of the latent space and to the fact that such naively interpolated latent vectors deviate from the data manifold. In this paper, we propose a regularization technique that shapes the latent representation to follow a manifold that is consistent with the training images and that forces the manifold to be smooth and locally convex. This regularization not only enables faithful interpolation between data points, as we show herein but can also be used as a general regularization technique to avoid overfitting or to produce new samples for data augmentation.",
        "conference": "ICML",
        "中文标题": "通过塑造潜在空间实现自编码器图像插值",
        "摘要翻译": "深度学习的一个迷人特性是网络能够揭示不同类型数据集中元素的基础特征。自编码器是计算这些特征的有效方法。在通过解码潜在向量的凸组合实现数据点间插值的背景下，自编码器已被研究。然而，这种插值在重建过程中常常导致伪影或产生不切实际的结果。我们认为这些不一致是由于潜在空间的结构以及这种简单插值的潜在向量偏离了数据流形。在本文中，我们提出了一种正则化技术，该技术塑造潜在表示以遵循与训练图像一致的流形，并强制流形平滑且局部凸。这种正则化不仅能够实现数据点间的忠实插值，如我们在此所示，还可以作为一种通用的正则化技术来避免过拟合或为数据增强生成新样本。",
        "领域": "图像生成、自编码器、数据增强",
        "问题": "自编码器在潜在空间中进行简单插值时产生的伪影和不切实际结果",
        "动机": "改善自编码器在潜在空间中的插值质量，使其更忠实于原始数据分布",
        "方法": "提出一种正则化技术，塑造潜在表示以遵循与训练图像一致的平滑且局部凸的流形",
        "关键词": [
            "自编码器",
            "潜在空间",
            "图像插值",
            "正则化",
            "数据流形"
        ],
        "涉及的技术概念": {
            "自编码器": "用于学习数据的高效表示，通过编码和解码过程揭示数据的基础特征",
            "潜在空间": "自编码器将输入数据映射到的低维空间，插值操作在此空间进行",
            "正则化技术": "用于塑造潜在表示，使其遵循与训练数据一致的流形，改善插值质量和模型泛化能力"
        },
        "success": true
    },
    {
        "order": 115,
        "title": "Autoencoding Under Normalization Constraints",
        "html": "https://ICML.cc//virtual/2021/poster/9093",
        "abstract": "Likelihood is a standard estimate for outlier detection. The specific role of the normalization constraint is to ensure that the out-of-distribution (OOD) regime has a small likelihood when samples are learned using maximum likelihood. Because autoencoders do not possess such a process of normalization, they often fail to recognize outliers even when they are obviously OOD. We propose the Normalized Autoencoder (NAE), a normalized probabilistic model constructed from an autoencoder. The probability density of NAE is defined using the reconstruction error of an autoencoder, which is differently defined in the conventional energy-based model. In our model, normalization is enforced by suppressing the reconstruction of negative samples, significantly improving the outlier detection performance. Our experimental results confirm the efficacy of NAE, both in detecting outliers and in generating in-distribution samples.",
        "conference": "ICML",
        "中文标题": "归一化约束下的自动编码",
        "摘要翻译": "似然是异常检测的标准估计方法。归一化约束的具体作用是确保当使用最大似然学习样本时，分布外（OOD）区域的似然较小。由于自动编码器不具备这样的归一化过程，它们即使在明显是OOD的情况下也常常无法识别异常。我们提出了归一化自动编码器（NAE），一种从自动编码器构建的归一化概率模型。NAE的概率密度是使用自动编码器的重构误差定义的，这与传统的基于能量的模型中的定义不同。在我们的模型中，通过抑制负样本的重构来强制执行归一化，显著提高了异常检测的性能。我们的实验结果证实了NAE在检测异常和生成分布内样本方面的有效性。",
        "领域": "异常检测、自动编码器、概率模型",
        "问题": "自动编码器在缺乏归一化过程的情况下，难以有效识别分布外（OOD）的异常样本。",
        "动机": "为了解决自动编码器在异常检测中因缺乏归一化约束而性能不佳的问题。",
        "方法": "提出归一化自动编码器（NAE），通过定义基于重构误差的概率密度并强制执行归一化，以提高异常检测性能。",
        "关键词": [
            "归一化自动编码器",
            "异常检测",
            "概率模型",
            "重构误差",
            "分布外样本"
        ],
        "涉及的技术概念": {
            "归一化约束": "确保模型在最大似然学习下对分布外样本赋予低似然，提高异常检测的准确性。",
            "重构误差": "用于定义归一化自动编码器的概率密度，与传统基于能量的模型不同，是本模型的核心。",
            "负样本抑制": "通过抑制负样本的重构来强制执行归一化，显著提升模型在异常检测上的性能。"
        },
        "success": true
    },
    {
        "order": 116,
        "title": "Automatic variational inference with cascading flows",
        "html": "https://ICML.cc//virtual/2021/poster/9519",
        "abstract": "The automation of probabilistic reasoning is one of the primary aims of machine learning. Recently, the confluence of variational inference and deep learning has led to powerful and flexible automatic inference methods that can be trained by stochastic gradient descent.  In particular, normalizing flows are highly parameterized deep models that can fit arbitrarily complex posterior densities. However, normalizing flows struggle in highly structured probabilistic programs as they need to relearn the forward-pass of the program. Automatic structured variational inference (ASVI) remedies this problem by constructing variational programs that embed the forward-pass. Here, we combine the flexibility of normalizing flows and the prior-embedding property of ASVI in a new family of variational programs, which we named cascading flows. A cascading flows program interposes a newly designed highway flow architecture in between the conditional distributions of the prior program such as to steer it toward the observed data. These programs can be constructed automatically from an input probabilistic program and can also be amortized automatically. We evaluate the performance of the new variational programs in a series of structured inference problems. We find that cascading flows have much higher performance than both normalizing flows and ASVI in a large set of structured inference problems. ",
        "conference": "ICML",
        "中文标题": "级联流的自动变分推断",
        "摘要翻译": "概率推理的自动化是机器学习的主要目标之一。最近，变分推断与深度学习的结合催生了强大且灵活的自动推断方法，这些方法可以通过随机梯度下降进行训练。特别是，归一化流是高度参数化的深度模型，可以拟合任意复杂的后验密度。然而，归一化流在高度结构化的概率程序中表现不佳，因为它们需要重新学习程序的前向传递。自动结构化变分推断（ASVI）通过构建嵌入前向传递的变分程序来解决这个问题。在这里，我们将归一化流的灵活性和ASVI的先验嵌入特性结合在一个新的变分程序家族中，我们称之为级联流。级联流程序在先前程序的条件分布之间插入了一个新设计的高速流架构，以引导其朝向观测数据。这些程序可以从输入的概率程序中自动构建，也可以自动摊销。我们在一系列结构化推断问题中评估了新变分程序的性能。我们发现，在大量结构化推断问题中，级联流的性能远高于归一化流和ASVI。",
        "领域": "概率图模型, 变分推断, 深度学习",
        "问题": "解决在高度结构化的概率程序中，归一化流需要重新学习程序的前向传递的问题",
        "动机": "结合归一化流的灵活性和自动结构化变分推断（ASVI）的先验嵌入特性，提高结构化推断问题的性能",
        "方法": "提出级联流，通过在先前程序的条件分布之间插入新设计的高速流架构，自动构建和摊销变分程序",
        "关键词": [
            "级联流",
            "变分推断",
            "归一化流",
            "结构化概率程序",
            "自动摊销"
        ],
        "涉及的技术概念": {
            "归一化流": "高度参数化的深度模型，用于拟合任意复杂的后验密度",
            "自动结构化变分推断（ASVI）": "通过构建嵌入前向传递的变分程序，解决归一化流在结构化概率程序中的问题",
            "级联流": "结合归一化流和ASVI特性的新变分程序家族，通过在条件分布间插入高速流架构来引导程序朝向观测数据"
        },
        "success": true
    },
    {
        "order": 117,
        "title": "Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators",
        "html": "https://ICML.cc//virtual/2021/poster/9949",
        "abstract": "While maximizing deep neural networks' (DNNs') acceleration efficiency requires a joint search/design of three different yet highly coupled aspects, including the networks, bitwidths, and accelerators, the challenges associated with such a joint search have not yet been fully understood and addressed. The key challenges include (1) the dilemma of whether to explode the memory consumption due to the huge joint space or achieve sub-optimal designs, (2) the discrete nature of the accelerator design space that is coupled yet different from that of the networks and bitwidths, and (3) the chicken and egg problem associated with network-accelerator co-search, i.e., co-search requires operation-wise hardware cost, which is lacking during search as the optimal accelerator depending on the whole network is still unknown during search. To tackle these daunting challenges towards optimal and fast development of DNN accelerators, we propose a framework dubbed Auto-NBA to enable jointly searching for the Networks, Bitwidths, and Accelerators, by efficiently localizing the optimal design within the huge joint design space for each target dataset and acceleration specification. Our Auto-NBA integrates a heterogeneous sampling strategy to achieve unbiased search with constant memory consumption, and a novel joint-search pipeline equipped with a generic differentiable accelerator search engine. Extensive experiments and ablation studies validate that both Auto-NBA generated networks and accelerators consistently outperform state-of-the-art designs (including co-search/exploration techniques, hardware-aware NAS methods, and DNN accelerators), in terms of search time, task accuracy, and accelerator efficiency. Our codes are available at: https://github.com/RICE-EIC/Auto-NBA.",
        "conference": "ICML",
        "中文标题": "Auto-NBA：在神经网络、位宽和加速器的联合空间中进行高效有效搜索",
        "摘要翻译": "尽管最大化深度神经网络（DNNs）的加速效率需要对网络、位宽和加速器这三个不同但高度耦合的方面进行联合搜索/设计，但与这种联合搜索相关的挑战尚未被充分理解和解决。关键挑战包括：（1）由于巨大的联合空间导致的存储消耗爆炸与实现次优设计之间的两难选择，（2）加速器设计空间的离散性质，该空间与网络和位宽的设计空间既耦合又不同，（3）网络-加速器联合搜索中的鸡与蛋问题，即联合搜索需要操作级的硬件成本，而在搜索过程中由于最优加速器依赖于整个网络而未知，因此缺乏这一成本。为了应对这些挑战，实现DNN加速器的最优和快速开发，我们提出了一个名为Auto-NBA的框架，通过高效定位每个目标数据集和加速规范在巨大联合设计空间中的最优设计，实现网络、位宽和加速器的联合搜索。我们的Auto-NBA集成了一个异构采样策略以实现恒定内存消耗的无偏搜索，以及一个配备了通用可微分加速器搜索引擎的新型联合搜索流程。大量实验和消融研究验证了Auto-NBA生成的网络和加速器在搜索时间、任务准确性和加速器效率方面均优于最先进的设计（包括联合搜索/探索技术、硬件感知的NAS方法和DNN加速器）。我们的代码可在https://github.com/RICE-EIC/Auto-NBA获取。",
        "领域": "神经网络架构搜索, 硬件加速器设计, 量化深度学习",
        "问题": "解决在神经网络、位宽和加速器的联合设计空间中进行高效搜索的挑战",
        "动机": "为了最大化深度神经网络的加速效率，需要同时优化网络架构、位宽和加速器设计，但这一联合搜索过程面临存储消耗、设计空间离散性和网络-加速器联合搜索的鸡与蛋问题等挑战。",
        "方法": "提出了Auto-NBA框架，通过异构采样策略和可微分加速器搜索引擎，在联合设计空间中高效定位最优设计。",
        "关键词": [
            "神经网络架构搜索",
            "硬件加速器",
            "量化深度学习",
            "联合优化",
            "可微分搜索"
        ],
        "涉及的技术概念": {
            "异构采样策略": "用于在联合搜索过程中保持内存消耗恒定，同时实现无偏搜索的技术。",
            "可微分加速器搜索引擎": "一种新型搜索技术，允许在搜索过程中对加速器设计进行梯度优化。",
            "网络-加速器联合搜索": "指同时优化神经网络架构和加速器设计的过程，以最大化整体性能。"
        },
        "success": true
    },
    {
        "order": 118,
        "title": "Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting",
        "html": "https://ICML.cc//virtual/2021/poster/8591",
        "abstract": "In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.",
        "conference": "ICML",
        "中文标题": "自回归去噪扩散模型在多变量概率时间序列预测中的应用",
        "摘要翻译": "在本工作中，我们提出了TimeGrad，一种用于多变量概率时间序列预测的自回归模型，该模型通过估计数据分布的梯度在每个时间步进行采样。为此，我们使用了扩散概率模型，这是一类与分数匹配和基于能量的方法密切相关的潜在变量模型。我们的模型通过优化数据似然的变分边界来学习梯度，并在推理时通过使用Langevin采样的马尔可夫链将白噪声转换为感兴趣分布的样本。我们通过实验证明，所提出的自回归去噪扩散模型在具有数千个相关维度的真实世界数据集上是多变量概率预测方法的新最先进技术。我们希望这种方法对实践者来说是一个有用的工具，并为该领域的未来研究奠定基础。",
        "领域": "时间序列预测、概率模型、深度学习",
        "问题": "解决多变量时间序列的概率预测问题",
        "动机": "开发一种新的自回归模型，以提高多变量概率时间序列预测的准确性和效率",
        "方法": "使用扩散概率模型和Langevin采样技术，通过优化数据似然的变分边界来学习梯度，实现时间序列的概率预测",
        "关键词": [
            "自回归模型",
            "扩散概率模型",
            "Langevin采样",
            "多变量预测",
            "概率时间序列"
        ],
        "涉及的技术概念": {
            "扩散概率模型": "用于估计数据分布的梯度，实现从白噪声到目标分布的转换",
            "Langevin采样": "在推理阶段使用，通过马尔可夫链过程生成目标分布的样本",
            "变分边界优化": "用于学习数据分布的梯度，是模型训练的核心"
        },
        "success": true
    },
    {
        "order": 119,
        "title": "AutoSampling: Search for Effective Data Sampling Schedules",
        "html": "https://ICML.cc//virtual/2021/poster/8661",
        "abstract": "Data sampling acts as a pivotal role in training deep learning models. However, an effective sampling schedule is difficult to learn due to its inherent high-dimension as a hyper-parameter. In this paper, we propose an AutoSampling method to automatically learn sampling schedules for model training, which consists of the multi-exploitation step aiming for optimal local sampling schedules and the exploration step for the ideal sampling distribution. More specifically, we achieve sampling schedule search with shortened exploitation cycle to provide enough supervision. In addition, we periodically estimate the sampling distribution from the learned sampling schedules and perturb it to search in the distribution space. The combination of two searches allows us to learn a robust sampling schedule. We apply our AutoSampling method to a variety of image classification tasks illustrating the effectiveness of the proposed method.",
        "conference": "ICML",
        "中文标题": "自动采样：寻找有效的数据采样计划",
        "摘要翻译": "数据采样在训练深度学习模型中扮演着关键角色。然而，由于其作为超参数固有的高维度特性，学习一个有效的采样计划是困难的。在本文中，我们提出了一种自动采样方法，用于自动学习模型训练的采样计划，该方法包括旨在寻找最优局部采样计划的多重利用步骤和寻找理想采样分布的探索步骤。更具体地说，我们通过缩短利用周期来实现采样计划的搜索，以提供足够的监督。此外，我们定期从学习到的采样计划中估计采样分布，并对其进行扰动以在分布空间中进行搜索。这两种搜索的结合使我们能够学习到一个鲁棒的采样计划。我们将我们的自动采样方法应用于多种图像分类任务，展示了所提出方法的有效性。",
        "领域": "图像分类、深度学习优化、自动机器学习",
        "问题": "如何自动学习有效的深度学习模型训练数据采样计划",
        "动机": "由于数据采样计划作为超参数的高维度特性，手动设计和调整采样计划既困难又耗时，因此需要一种自动化的方法来寻找有效的采样计划。",
        "方法": "提出了一种结合多重利用步骤和探索步骤的自动采样方法，通过缩短利用周期和扰动采样分布来搜索有效的采样计划。",
        "关键词": [
            "自动采样",
            "数据采样计划",
            "深度学习优化",
            "图像分类",
            "超参数搜索"
        ],
        "涉及的技术概念": {
            "多重利用步骤": "旨在寻找最优局部采样计划的技术步骤，通过缩短利用周期来提供足够的监督。",
            "探索步骤": "寻找理想采样分布的技术步骤，通过扰动采样分布以在分布空间中进行搜索。",
            "采样分布扰动": "一种技术手段，用于在分布空间中进行搜索，以找到更优的采样计划。"
        },
        "success": true
    },
    {
        "order": 120,
        "title": "A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9581",
        "abstract": "Bi-level optimization model is able to capture a wide range of complex learning tasks with practical interest. Due to the witnessed efficiency in solving bi-level programs, gradient-based methods have gained popularity in the machine learning community. In this work, we propose a new gradient-based solution scheme, namely, the Bi-level Value-Function-based Interior-point Method (BVFIM). Following the main idea of the log-barrier interior-point scheme, we penalize the regularized value function of the lower level problem into the upper level objective. By further solving a sequence of differentiable unconstrained approximation problems, we consequently derive a sequential programming scheme. The numerical advantage of our scheme relies on the fact that, when gradient methods are applied to solve the approximation problem, we successfully avoid computing any expensive Hessian-vector or Jacobian-vector product. We prove the convergence without requiring any convexity assumption on either the upper level or the lower level objective. Experiments demonstrate the efficiency of the proposed BVFIM on non-convex bi-level problems.",
        "conference": "ICML",
        "中文标题": "基于价值函数的非凸双层优化内点法",
        "摘要翻译": "双层优化模型能够捕捉到广泛具有实际意义的复杂学习任务。由于在解决双层程序方面表现出的效率，基于梯度的方法在机器学习社区中获得了流行。在这项工作中，我们提出了一种新的基于梯度的解决方案，即基于双层价值函数的内点法（BVFIM）。遵循对数障碍内点法的主要思想，我们将下层问题的正则化价值函数惩罚到上层目标中。通过进一步解决一系列可微无约束近似问题，我们因此推导出一个序列编程方案。我们方案的数值优势在于，当应用梯度方法解决近似问题时，我们成功避免了计算任何昂贵的Hessian-向量或Jacobian-向量乘积。我们在不需要上层或下层目标的任何凸性假设的情况下证明了收敛性。实验证明了所提出的BVFIM在非凸双层问题上的效率。",
        "领域": "优化算法、机器学习、非凸优化",
        "问题": "解决非凸双层优化问题中的计算效率和收敛性问题",
        "动机": "为了在不需要凸性假设的情况下，提高解决非凸双层优化问题的效率和可行性",
        "方法": "提出了一种基于价值函数的内点法（BVFIM），通过惩罚下层问题的正则化价值函数到上层目标，并解决一系列可微无约束近似问题，避免了昂贵的Hessian-向量或Jacobian-向量乘积计算",
        "关键词": [
            "双层优化",
            "内点法",
            "非凸优化",
            "梯度方法",
            "价值函数"
        ],
        "涉及的技术概念": {
            "双层优化": "一种优化问题，其中一个问题（上层）的约束条件是另一个问题（下层）的解",
            "内点法": "一种解决约束优化问题的方法，通过引入障碍函数将约束问题转化为一系列无约束问题",
            "价值函数": "在优化问题中，用于表示下层问题解的质量的函数，本研究中用于上层目标的惩罚项"
        },
        "success": true
    },
    {
        "order": 121,
        "title": "Average-Reward Off-Policy Policy Evaluation with Function Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/8653",
        "abstract": "We consider off-policy policy evaluation with function approximation (FA) in average-reward MDPs, \nwhere the goal is to estimate both the reward rate and the differential value function. \nFor this problem, bootstrapping is necessary and, along with off-policy learning and FA, results in the deadly triad (Sutton & Barto, 2018).\nTo address the deadly triad, we propose\ntwo novel algorithms,\nreproducing the celebrated success of Gradient TD algorithms in the average-reward setting.\nIn terms of estimating the differential value function, the algorithms are the first convergent off-policy linear function approximation algorithms.\nIn terms of estimating the reward rate,\nthe algorithms are the first convergent off-policy linear function approximation algorithms that do not require estimating the density ratio.\nWe demonstrate empirically the advantage of the proposed algorithms, \nas well as their nonlinear variants,\nover a competitive density-ratio-based approach,\nin a simple domain as well as challenging robot simulation tasks.",
        "conference": "ICML",
        "中文标题": "基于函数逼近的平均奖励离策略策略评估",
        "摘要翻译": "我们考虑在平均奖励马尔可夫决策过程（MDPs）中使用函数逼近（FA）进行离策略策略评估，目标是同时估计奖励率和差分价值函数。对于这个问题，自助法是必要的，并且与离策略学习及FA一起，导致了所谓的‘致命三合会’（Sutton & Barto, 2018）。为了解决致命三合会，我们提出了两种新颖的算法，在平均奖励设置中重现了梯度TD算法的显著成功。在估计差分价值函数方面，这些算法是首个收敛的离策略线性函数逼近算法。在估计奖励率方面，这些算法是首个不需要估计密度比的收敛离策略线性函数逼近算法。我们通过实验证明了所提出算法及其非线性变体在一个简单领域以及具有挑战性的机器人模拟任务中，相对于基于密度比的竞争方法的优势。",
        "领域": "强化学习、函数逼近、机器人控制",
        "问题": "在平均奖励MDPs中，使用函数逼近进行离策略策略评估时，如何有效估计奖励率和差分价值函数。",
        "动机": "解决在平均奖励MDPs中使用函数逼近进行离策略策略评估时遇到的‘致命三合会’问题，即自助法、离策略学习和函数逼近共同作用导致的学习不稳定问题。",
        "方法": "提出了两种新颖的算法，这些算法在平均奖励设置中重现了梯度TD算法的成功，是首个收敛的离策略线性函数逼近算法，且不需要估计密度比。",
        "关键词": [
            "离策略策略评估",
            "函数逼近",
            "平均奖励MDPs",
            "梯度TD算法",
            "机器人控制"
        ],
        "涉及的技术概念": {
            "致命三合会": "指自助法、离策略学习和函数逼近共同作用导致的学习不稳定问题。",
            "梯度TD算法": "一种在强化学习中用于策略评估的算法，通过梯度下降来优化价值函数的估计。",
            "差分价值函数": "在平均奖励MDPs中，用于衡量状态相对于平均奖励的长期价值的函数。"
        },
        "success": true
    },
    {
        "order": 122,
        "title": "A Wasserstein Minimax Framework for Mixed Linear Regression",
        "html": "https://ICML.cc//virtual/2021/poster/8949",
        "abstract": "Multi-modal distributions are commonly used to model clustered data in statistical learning tasks. In this paper, we consider the Mixed Linear Regression (MLR) problem. We propose an optimal transport-based framework for MLR problems, Wasserstein Mixed Linear Regression (WMLR), which minimizes the Wasserstein distance between the learned and target mixture regression models. Through a model-based duality analysis, WMLR reduces the underlying MLR task to a nonconvex-concave minimax optimization problem, which can be provably solved to find a minimax stationary point by the Gradient Descent Ascent (GDA) algorithm. In the special case of mixtures of two linear regression models, we show that WMLR enjoys global convergence and generalization guarantees. We prove that WMLR’s sample complexity grows linearly with the dimension of data. Finally, we discuss the application of WMLR to the federated learning task where the training samples are collected by multiple agents in a network. Unlike the Expectation-Maximization algorithm, WMLR directly extends to the distributed, federated learning setting. We support our theoretical results through several numerical experiments, which highlight our framework’s ability to handle the federated learning setting with mixture models.",
        "conference": "ICML",
        "中文标题": "混合线性回归的Wasserstein极小极大框架",
        "摘要翻译": "多模态分布常用于统计学习任务中对聚类数据进行建模。本文中，我们考虑了混合线性回归（MLR）问题。我们提出了一个基于最优传输的MLR问题框架，Wasserstein混合线性回归（WMLR），该框架最小化了学习到的与目标的混合回归模型之间的Wasserstein距离。通过基于模型的对偶分析，WMLR将底层的MLR任务简化为一个非凸-凹的极小极大优化问题，该问题可以通过梯度下降上升（GDA）算法可证明地找到一个极小极大稳定点。在两个线性回归模型混合的特殊情况下，我们展示了WMLR享有全局收敛和泛化保证。我们证明了WMLR的样本复杂度随数据维度线性增长。最后，我们讨论了WMLR在联邦学习任务中的应用，其中训练样本由网络中的多个代理收集。与期望最大化算法不同，WMLR直接扩展到分布式、联邦学习设置。我们通过几个数值实验支持我们的理论结果，这些实验突出了我们框架处理具有混合模型的联邦学习设置的能力。",
        "领域": "混合线性回归, 最优传输, 联邦学习",
        "问题": "解决混合线性回归问题中的模型学习和优化问题",
        "动机": "提出一个能够有效处理混合线性回归问题，特别是在联邦学习环境中，的新框架",
        "方法": "提出基于最优传输的Wasserstein混合线性回归框架，通过非凸-凹的极小极大优化问题求解",
        "关键词": [
            "Wasserstein距离",
            "混合线性回归",
            "最优传输",
            "联邦学习",
            "梯度下降上升"
        ],
        "涉及的技术概念": {
            "Wasserstein距离": "用于衡量学习到的与目标的混合回归模型之间的差异",
            "非凸-凹极小极大优化问题": "将混合线性回归任务简化为可通过梯度下降上升算法求解的问题",
            "梯度下降上升（GDA）算法": "用于求解非凸-凹极小极大优化问题，找到极小极大稳定点"
        },
        "success": true
    },
    {
        "order": 123,
        "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9407",
        "abstract": "We consider the zeroth-order optimization problem in the huge-scale setting, where the dimension of the problem is so large that performing even basic vector operations on the decision variables is infeasible. In this paper, we propose a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity and has a much smaller per-iteration computational complexity. In addition, we discuss how the memory footprint of ZO-BCD can be reduced even further by the clever use of circulant measurement matrices. As an application of our new method, we propose the idea of crafting adversarial attacks on neural network based classifiers in a wavelet domain, which can result in problem dimensions of over one million. In particular, we show that crafting adversarial examples to audio classifiers in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9% with significantly less distortion. ",
        "conference": "ICML",
        "中文标题": "一种用于超大规模黑盒优化的零阶块坐标下降算法",
        "摘要翻译": "我们考虑了超大规模设置下的零阶优化问题，其中问题的维度如此之大，以至于对决策变量执行基本的向量操作都不可行。在本文中，我们提出了一种新颖的算法，称为ZO-BCD，该算法展现出有利的总体查询复杂度，并且每次迭代的计算复杂度要小得多。此外，我们还讨论了如何通过巧妙使用循环测量矩阵进一步减少ZO-BCD的内存占用。作为我们新方法的应用，我们提出了在基于神经网络的分类器的小波域中制作对抗性攻击的想法，这可能导致问题维度超过一百万。特别是，我们展示了在小波域中为音频分类器制作对抗性示例可以达到97.9%的最先进攻击成功率，且失真显著减少。",
        "领域": "优化算法、对抗性攻击、音频处理",
        "问题": "解决在超大规模维度下进行零阶优化的问题，以及在神经网络分类器中制作高效对抗性攻击的挑战。",
        "动机": "针对超大规模优化问题中计算和内存限制的挑战，以及提高对抗性攻击的效率和成功率。",
        "方法": "提出了一种名为ZO-BCD的新算法，该算法通过块坐标下降和循环测量矩阵减少计算复杂度和内存占用，应用于小波域中的对抗性攻击制作。",
        "关键词": [
            "零阶优化",
            "块坐标下降",
            "对抗性攻击",
            "小波域",
            "音频分类"
        ],
        "涉及的技术概念": {
            "零阶优化": "在无法直接计算梯度的情况下，通过函数评估来优化目标函数的技术。",
            "块坐标下降": "一种优化算法，通过轮流优化变量的子集来降低每次迭代的计算复杂度。",
            "循环测量矩阵": "用于减少算法内存占用的技术，通过循环结构有效存储和操作大规模数据。"
        },
        "success": true
    },
    {
        "order": 124,
        "title": "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10181",
        "abstract": "Back-door attack poses a severe threat to deep learning systems. It injects hidden malicious behaviors to a model such that any input stamped with a special pattern can trigger such behaviors. Detecting back-door is hence of pressing need. Many existing defense techniques use optimization to generate the smallest input pattern that forces the model to misclassify a set of benign inputs injected with the pattern to a target label. However, the complexity is quadratic to the number of class labels such that they can hardly handle models with many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we propose a K-Arm optimization method for backdoor detection. By iteratively and stochastically selecting the most promising labels for optimization with the guidance of an objective function, we substantially reduce the complexity, allowing to handle models with many classes. Moreover, by iteratively refining the selection of labels to optimize, it substantially mitigates the uncertainty in choosing the right labels, improving detection accuracy.  At the time of submission, the evaluation of our method on over 4000 models in the IARPA TrojAI competition from round 1 to the latest round 4 achieves top performance on the leaderboard. Our technique also supersedes five state-of-the-art techniques in terms of accuracy and the scanning time needed. The code of our work is available at https://github.com/PurduePAML/K-ARM_Backdoor_Optimization",
        "conference": "ICML",
        "中文标题": "通过K臂优化深度神经网络的后门扫描",
        "摘要翻译": "后门攻击对深度学习系统构成了严重威胁。它向模型中注入了隐藏的恶意行为，使得任何带有特殊模式的输入都能触发这些行为。因此，检测后门变得极为迫切。许多现有的防御技术使用优化来生成最小的输入模式，迫使模型将一组注入该模式的良性输入误分类为目标标签。然而，其复杂度与类别标签的数量成二次关系，使得它们难以处理具有许多类别的模型。受到强化学习中多臂老虎机的启发，我们提出了一种用于后门检测的K臂优化方法。通过迭代和随机选择最有希望的标签进行优化，并在目标函数的指导下，我们大幅降低了复杂度，使得能够处理具有许多类别的模型。此外，通过迭代优化选择的标签，它大大减轻了选择正确标签的不确定性，提高了检测的准确性。在提交时，我们对IARPA TrojAI竞赛从第1轮到最新第4轮的4000多个模型的评估，在排行榜上取得了顶级性能。我们的技术在准确性和所需的扫描时间上也超越了五种最先进的技术。我们的工作代码可在https://github.com/PurduePAML/K-ARM_Backdoor_Optimization获取。",
        "领域": "深度学习安全、后门攻击检测、模型安全评估",
        "问题": "如何在具有大量类别的模型中高效准确地检测后门攻击",
        "动机": "后门攻击对深度学习系统构成严重威胁，现有检测方法在处理多类别模型时效率低下，亟需一种更高效的检测技术",
        "方法": "提出了一种基于K臂优化的后门检测方法，通过迭代和随机选择最有希望的标签进行优化，降低复杂度并提高检测准确性",
        "关键词": [
            "后门检测",
            "K臂优化",
            "深度学习安全",
            "多类别模型",
            "IARPA TrojAI竞赛"
        ],
        "涉及的技术概念": {
            "K臂优化": "受到多臂老虎机启发，用于高效选择最有希望的标签进行优化，降低后门检测的复杂度",
            "后门攻击": "向模型中注入隐藏的恶意行为，特定输入模式可触发这些行为",
            "目标函数": "指导优化过程，帮助选择最有希望的标签，提高检测的准确性和效率"
        },
        "success": true
    },
    {
        "order": 125,
        "title": "Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9057",
        "abstract": "While Backpropagation (BP) has been applied to spiking neural networks (SNNs) achieving encouraging results, a key challenge involved is to backpropagate a differentiable continuous-valued loss over layers of spiking neurons exhibiting discontinuous all-or-none firing activities. Existing methods deal with this difficulty by introducing compromises that come with their own limitations, leading to potential performance degradation. We propose a novel BP-like method, called neighborhood aggregation (NA), which computes accurate error gradients guiding weight updates that may lead to discontinuous modifications of firing activities. NA achieves this goal by aggregating the error gradient over multiple spike trains in the neighborhood of the present spike train of each neuron. The employed aggregation is based on a generalized finite difference approximation with a proposed distance metric quantifying the similarity between a given pair of spike trains. Our experiments show that the proposed NA algorithm delivers state-of-the-art performance for SNN training on several datasets including CIFAR10.",
        "conference": "ICML",
        "中文标题": "反向传播邻域聚合用于精确训练脉冲神经网络",
        "摘要翻译": "虽然反向传播（BP）已被应用于脉冲神经网络（SNNs）并取得了令人鼓舞的结果，但其中的一个关键挑战是在表现出不连续的全或无放电活动的脉冲神经元层上反向传播可微分的连续值损失。现有方法通过引入带有自身限制的妥协来处理这一困难，这可能导致性能下降。我们提出了一种新颖的类似BP的方法，称为邻域聚合（NA），它计算准确的误差梯度，指导可能导致放电活动不连续修改的权重更新。NA通过在每个神经元当前脉冲序列的邻域内聚合多个脉冲序列的误差梯度来实现这一目标。所采用的聚合基于广义有限差分近似，并提出了一个量化给定脉冲序列对之间相似度的距离度量。我们的实验表明，所提出的NA算法在包括CIFAR10在内的多个数据集上为SNN训练提供了最先进的性能。",
        "领域": "脉冲神经网络、深度学习优化、神经形态计算",
        "问题": "在脉冲神经网络中反向传播连续值损失时，如何处理神经元放电活动的不连续性",
        "动机": "解决现有方法在处理脉冲神经网络训练时因神经元放电活动不连续性而导致的性能下降问题",
        "方法": "提出邻域聚合（NA）方法，通过聚合邻域内多个脉冲序列的误差梯度来计算准确的误差梯度，指导权重更新",
        "关键词": [
            "脉冲神经网络",
            "反向传播",
            "邻域聚合",
            "误差梯度",
            "神经形态计算"
        ],
        "涉及的技术概念": {
            "邻域聚合（NA）": "一种新颖的类似反向传播的方法，通过聚合邻域内多个脉冲序列的误差梯度来计算准确的误差梯度",
            "广义有限差分近似": "用于在邻域聚合中计算误差梯度的数学方法",
            "距离度量": "量化给定脉冲序列对之间相似度的度量，用于邻域聚合中的误差梯度计算"
        },
        "success": true
    },
    {
        "order": 126,
        "title": "BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining",
        "html": "https://ICML.cc//virtual/2021/poster/8523",
        "abstract": "In this paper, we propose BANG, a new pretraining model to Bridge the gap between Autoregressive (AR) and Non-autoregressive (NAR) Generation. AR and NAR generation can be uniformly regarded as to what extent previous tokens can be attended, and BANG bridges AR and NAR generation through designing a novel model structure for large-scale pre-training. A pretrained BANG model can simultaneously support AR, NAR, and semi-NAR generation to meet different requirements.  Experiments on question generation (SQuAD 1.1), summarization (XSum), and dialogue generation (PersonaChat) show that BANG improves NAR and semi-NAR performance significantly as well as attaining comparable performance with strong AR pretrained models. Compared with the semi-NAR strong baselines, BANG achieves absolute improvements of 14.01 and 5.24 in the overall scores of SQuAD 1.1 and XSum, respectively. In addition, BANG achieves absolute improvements of 10.73, 6.39, and 5.90 in the overall scores of SQuAD, XSUM, and PersonaChat compared with the NAR strong baselines, respectively. Our code will be made publicly available.",
        "conference": "ICML",
        "中文标题": "BANG：通过大规模预训练桥接自回归与非自回归生成",
        "摘要翻译": "本文提出了BANG，一种新的预训练模型，旨在桥接自回归（AR）与非自回归（NAR）生成之间的差距。AR和NAR生成可以统一视为对先前标记可以关注到何种程度的问题，BANG通过设计一种新颖的大规模预训练模型结构来桥接AR和NAR生成。预训练的BANG模型可以同时支持AR、NAR和半NAR生成，以满足不同的需求。在问题生成（SQuAD 1.1）、摘要（XSum）和对话生成（PersonaChat）上的实验表明，BANG显著提高了NAR和半NAR的性能，同时达到了与强大的AR预训练模型相媲美的性能。与半NAR的强基线相比，BANG在SQuAD 1.1和XSum的总分上分别实现了14.01和5.24的绝对提升。此外，与NAR的强基线相比，BANG在SQuAD、XSUM和PersonaChat的总分上分别实现了10.73、6.39和5.90的绝对提升。我们的代码将公开提供。",
        "领域": "自然语言处理与视觉结合",
        "问题": "桥接自回归与非自回归生成之间的差距",
        "动机": "为了统一和桥接自回归与非自回归生成方法，满足不同生成需求",
        "方法": "设计了一种新颖的大规模预训练模型结构，支持AR、NAR和半NAR生成",
        "关键词": [
            "自回归生成",
            "非自回归生成",
            "大规模预训练",
            "桥接模型",
            "自然语言生成"
        ],
        "涉及的技术概念": {
            "自回归生成（AR）": "一种序列生成方法，每个步骤生成一个标记，依赖于之前生成的标记",
            "非自回归生成（NAR）": "一种序列生成方法，可以并行生成所有标记，不依赖于之前生成的标记",
            "大规模预训练": "在大量数据上预先训练模型，以提高模型在各种任务上的性能和泛化能力"
        },
        "success": true
    },
    {
        "order": 127,
        "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
        "html": "https://ICML.cc//virtual/2021/poster/10299",
        "abstract": "Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection. ",
        "conference": "ICML",
        "中文标题": "Barlow Twins：通过冗余减少实现自监督学习",
        "摘要翻译": "自监督学习（SSL）正在迅速缩小与大型计算机视觉基准测试中监督方法的差距。SSL的一个成功方法是学习对输入样本的扭曲不变的嵌入。然而，这种方法存在的一个反复出现的问题是存在平凡的常数解。大多数当前方法通过仔细的实现细节来避免这种解。我们提出了一个目标函数，通过测量由相同网络对样本的扭曲版本输出的交叉相关矩阵，并使其尽可能接近单位矩阵，自然地避免了崩溃。这使得样本的扭曲版本的嵌入向量相似，同时最小化这些向量组件之间的冗余。该方法被称为Barlow Twins，归功于神经科学家H. Barlow的冗余减少原理应用于一对相同的网络。Barlow Twins不需要大批量，也不需要网络双胞胎之间的不对称性，如预测网络、梯度停止或权重更新的移动平均。有趣的是，它从非常高维的输出向量中受益。Barlow Twins在ImageNet上的半监督分类在低数据制度下优于以前的方法，并且在使用线性分类器头进行ImageNet分类以及分类和目标检测的转移任务方面与当前的最新技术相当。",
        "领域": "自监督学习、图像分类、目标检测",
        "问题": "解决自监督学习中存在的平凡常数解问题",
        "动机": "通过减少嵌入向量组件之间的冗余，避免自监督学习中的崩溃现象",
        "方法": "提出一个目标函数，通过使交叉相关矩阵接近单位矩阵来避免崩溃，同时最小化嵌入向量组件之间的冗余",
        "关键词": [
            "自监督学习",
            "冗余减少",
            "图像分类",
            "目标检测",
            "交叉相关矩阵"
        ],
        "涉及的技术概念": {
            "自监督学习": "一种无需人工标注数据的学习方法，通过设计预测任务从数据本身学习有用的表示",
            "冗余减少": "通过最小化嵌入向量组件之间的冗余，避免学习到平凡的常数解",
            "交叉相关矩阵": "用于衡量两个网络输出之间关系的矩阵，目标是使其接近单位矩阵以避免崩溃"
        },
        "success": true
    },
    {
        "order": 128,
        "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
        "html": "https://ICML.cc//virtual/2021/poster/9213",
        "abstract": "We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. \nSparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. \nHowever, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions.\nIn contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens.  \nThis optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released.",
        "conference": "ICML",
        "中文标题": "BASE层：简化大型稀疏模型的训练",
        "摘要翻译": "我们为大型语言模型引入了一种新的平衡专家分配（BASE）层，极大地简化了现有的高容量稀疏层。稀疏层通过将每个令牌路由到仅包含模型参数一小部分的专门专家模块，可以显著提高训练和推理的效率。然而，学习充分利用可用专家的平衡路由函数可能很困难；现有方法通常使用路由启发式或辅助专家平衡损失函数。相比之下，我们将令牌到专家的分配表述为一个线性分配问题，允许一个最优分配方案，其中每个专家接收相等数量的令牌。这种最优分配方案通过保证平衡的计算负载来提高效率，并且通过不需要任何新的超参数或辅助损失来简化训练。代码已公开发布。",
        "领域": "自然语言处理与视觉结合",
        "问题": "如何简化大型稀疏模型的训练过程，同时保持或提高效率",
        "动机": "现有的稀疏层在训练大型语言模型时存在路由函数难以学习平衡的问题，导致专家模块利用不充分",
        "方法": "将令牌到专家的分配问题表述为线性分配问题，实现每个专家接收相等数量令牌的最优分配",
        "关键词": [
            "稀疏模型",
            "专家分配",
            "线性分配问题"
        ],
        "涉及的技术概念": {
            "平衡专家分配（BASE）层": "一种新的层设计，用于简化高容量稀疏层的训练，通过最优令牌分配提高效率",
            "线性分配问题": "将令牌到专家的分配问题数学化，确保每个专家接收相等数量的令牌，优化计算负载",
            "稀疏层": "通过路由令牌到专门专家模块来提高模型训练和推理效率的技术"
        },
        "success": true
    },
    {
        "order": 129,
        "title": "BASGD: Buffered Asynchronous SGD for Byzantine Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8819",
        "abstract": "Distributed learning has become a hot research topic due to its wide application in cluster-based large-scale learning, federated learning, edge computing and so on. Most traditional distributed learning methods typically assume no failure or attack. However, many unexpected cases, such as communication failure and even malicious attack, may happen in real applications. Hence, Byzantine learning (BL), which refers to distributed learning with failure or attack, has recently attracted much attention. Most existing BL methods are synchronous, which are impractical in some applications due to heterogeneous or offline workers. In these cases, asynchronous BL (ABL) is usually preferred. In this paper, we propose a novel method, called buffered asynchronous stochastic gradient descent (BASGD), for ABL. To the best of our knowledge, BASGD is the first ABL method that can resist malicious attack without storing any instances on server. Compared with those methods which need to store instances on server, BASGD has a wider scope of application. BASGD is proved to be convergent, and be able to resist failure or attack. Empirical results show that BASGD significantly outperforms vanilla asynchronous stochastic gradient descent (ASGD) and other ABL baselines when there exists failure or attack on workers.",
        "conference": "ICML",
        "中文标题": "BASGD：用于拜占庭学习的缓冲异步随机梯度下降",
        "摘要翻译": "分布式学习因其在基于集群的大规模学习、联邦学习、边缘计算等领域的广泛应用而成为研究热点。大多数传统的分布式学习方法通常假设没有故障或攻击。然而，在实际应用中可能会发生许多意外情况，如通信故障甚至恶意攻击。因此，拜占庭学习（BL），即指在存在故障或攻击的情况下的分布式学习，最近引起了广泛关注。大多数现有的BL方法是同步的，由于异构或离线的工作者，这些方法在某些应用中不切实际。在这些情况下，通常更倾向于异步BL（ABL）。在本文中，我们提出了一种名为缓冲异步随机梯度下降（BASGD）的新方法，用于ABL。据我们所知，BASGD是第一个无需在服务器上存储任何实例即可抵抗恶意攻击的ABL方法。与那些需要在服务器上存储实例的方法相比，BASGD具有更广泛的应用范围。BASGD被证明是收敛的，并且能够抵抗故障或攻击。实证结果表明，当工作者存在故障或攻击时，BASGD显著优于普通异步随机梯度下降（ASGD）和其他ABL基线。",
        "领域": "联邦学习, 边缘计算, 分布式学习",
        "问题": "解决在分布式学习中存在的故障或恶意攻击问题",
        "动机": "研究动机是为了开发一种能够在存在故障或攻击的情况下有效工作的异步分布式学习方法",
        "方法": "提出了一种名为缓冲异步随机梯度下降（BASGD）的新方法，该方法无需在服务器上存储任何实例即可抵抗恶意攻击",
        "关键词": [
            "拜占庭学习",
            "异步随机梯度下降",
            "分布式学习",
            "恶意攻击抵抗",
            "缓冲机制"
        ],
        "涉及的技术概念": {
            "拜占庭学习": "指在分布式学习环境中，存在故障或恶意攻击情况下的学习问题",
            "缓冲异步随机梯度下降": "一种新型的异步随机梯度下降方法，通过缓冲机制提高对恶意攻击的抵抗能力",
            "分布式学习": "一种利用多个计算节点协同工作以解决大规模机器学习问题的方法"
        },
        "success": true
    },
    {
        "order": 130,
        "title": "BasisDeVAE: Interpretable Simultaneous Dimensionality Reduction and Feature-Level Clustering with Derivative-Based Variational Autoencoders",
        "html": "https://ICML.cc//virtual/2021/poster/8543",
        "abstract": "The Variational Autoencoder (VAE) performs effective nonlinear dimensionality reduction in a variety of problem settings. However, the black-box neural network decoder function typically employed limits the ability of the decoder function to be constrained and interpreted, making the use of VAEs problematic in settings where prior knowledge should be embedded within the decoder. We present DeVAE, a novel VAE-based model with a derivative-based forward mapping, allowing for greater control over decoder behaviour via specification of the decoder function in derivative space. Additionally, we show how DeVAE can be paired with a sparse clustering prior to create BasisDeVAE and perform interpretable simultaneous dimensionality reduction and feature-level clustering. We demonstrate the performance and scalability of the DeVAE and BasisDeVAE models on synthetic and real-world data and present how the derivative-based approach allows for expressive yet interpretable forward models which respect prior knowledge.",
        "conference": "ICML",
        "中文标题": "BasisDeVAE：基于导数的变分自编码器实现可解释的同步降维与特征级聚类",
        "摘要翻译": "变分自编码器（VAE）在各种问题设置中实现了有效的非线性降维。然而，通常采用的黑盒神经网络解码器函数限制了解码器函数的约束和解释能力，这使得在需要将先验知识嵌入解码器的设置中使用VAE变得有问题。我们提出了DeVAE，一种基于导数前向映射的新型VAE模型，通过在导数空间中指定解码器函数，允许对解码器行为进行更大程度的控制。此外，我们还展示了如何将DeVAE与稀疏聚类先验配对，创建BasisDeVAE，并执行可解释的同步降维和特征级聚类。我们在合成和真实世界数据上展示了DeVAE和BasisDeVAE模型的性能和可扩展性，并展示了基于导数的方法如何允许表达性强且可解释的前向模型，这些模型尊重先验知识。",
        "领域": "变分自编码器、特征聚类、非线性降维",
        "问题": "传统VAE模型的黑盒特性限制了其解码器的可解释性和先验知识的嵌入能力。",
        "动机": "开发一种新型VAE模型，通过导数空间中的解码器函数指定，提高模型的可解释性和对先验知识的尊重。",
        "方法": "提出DeVAE模型，采用基于导数的前向映射，结合稀疏聚类先验创建BasisDeVAE，实现同步降维和特征级聚类。",
        "关键词": [
            "变分自编码器",
            "导数空间",
            "特征聚类",
            "非线性降维",
            "可解释性"
        ],
        "涉及的技术概念": {
            "变分自编码器（VAE）": "一种生成模型，用于非线性降维和特征学习。",
            "导数空间": "在DeVAE中用于指定解码器函数，提高模型的可解释性和控制能力。",
            "稀疏聚类先验": "与DeVAE结合使用，实现特征级聚类，增强模型的可解释性和实用性。"
        },
        "success": true
    },
    {
        "order": 131,
        "title": "Batch Value-function Approximation with Only Realizability",
        "html": "https://ICML.cc//virtual/2021/poster/9079",
        "abstract": "We make progress in a long-standing problem of batch reinforcement learning (RL): learning Q* from an exploratory and polynomial-sized dataset, using a realizable and otherwise arbitrary function class. In fact, all existing algorithms demand function-approximation assumptions stronger than realizability, and the mounting negative evidence has led to a conjecture that sample-efficient learning is impossible in this setting (Chen & Jiang, 2019). Our algorithm, BVFT, breaks the hardness conjecture (albeit under a stronger notion of exploratory data) via a tournament procedure that reduces the learning problem to pairwise comparison, and solves the latter with the help of a state-action-space partition constructed from the compared functions. We also discuss how BVFT can be applied to model selection among other extensions and open problems.",
        "conference": "ICML",
        "中文标题": "仅需可实现性的批量值函数逼近",
        "摘要翻译": "我们在批量强化学习（RL）的一个长期存在的问题上取得了进展：从一个探索性的、多项式大小的数据集中学习Q*，使用一个可实现的、其他方面任意的函数类。事实上，所有现有的算法都要求比可实现性更强的函数逼近假设，而越来越多的负面证据导致了一个猜想，即在这种设置下样本高效学习是不可能的（Chen & Jiang, 2019）。我们的算法BVFT通过一个将学习问题减少到成对比较的比赛程序，并在比较函数的帮助下构建的状态-动作空间分区解决后者，从而打破了这一硬度猜想（尽管是在更强的探索性数据概念下）。我们还讨论了BVFT如何应用于模型选择以及其他扩展和开放问题。",
        "领域": "强化学习、批量学习、函数逼近",
        "问题": "在仅满足可实现性假设的条件下，从探索性和多项式大小的数据集中高效学习最优动作值函数Q*。",
        "动机": "解决批量强化学习中，现有算法需要比可实现性更强的函数逼近假设，且存在样本高效学习不可能的猜想的问题。",
        "方法": "通过一个比赛程序将学习问题转化为成对比较，并利用比较函数构建的状态-动作空间分区来解决成对比较问题。",
        "关键词": [
            "批量强化学习",
            "值函数逼近",
            "可实现性",
            "样本效率",
            "模型选择"
        ],
        "涉及的技术概念": {
            "可实现性": "指存在一个函数类中的函数能够准确表示或逼近最优动作值函数Q*。",
            "批量强化学习": "一种强化学习范式，算法只能访问一个固定的、预先收集的数据集，而不能与环境进行交互。",
            "状态-动作空间分区": "通过比较函数构建的分区，用于在成对比较中指导学习过程，帮助识别和利用数据集中的结构信息。"
        },
        "success": true
    },
    {
        "order": 132,
        "title": "Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information",
        "html": "https://ICML.cc//virtual/2021/poster/10675",
        "abstract": "In many real world problems, we want to infer some property of an expensive black-box function f, given a budget of T function evaluations. One example is budget constrained global optimization of f, for which Bayesian optimization is a popular method. Other properties of interest include local optima, level sets, integrals, or graph-structured information induced by f. Often, we can find an algorithm A to compute the desired property, but it may require far more than T queries to execute. Given such an A, and a prior distribution over f, we refer to the problem of inferring the output of A using T evaluations as Bayesian Algorithm Execution (BAX). To tackle this problem, we present a procedure, InfoBAX, that sequentially chooses queries that maximize mutual information with respect to the algorithm's output. Applying this to Dijkstra’s algorithm, for instance, we infer shortest paths in synthetic and real-world graphs with black-box edge costs. Using evolution strategies, we yield variants of Bayesian optimization that target local, rather than global, optima. On these problems, InfoBAX uses up to 500 times fewer queries to f than required by the original algorithm. Our method is closely connected to other Bayesian optimal experimental design procedures such as entropy search methods and optimal sensor placement using Gaussian processes.",
        "conference": "ICML",
        "中文标题": "贝叶斯算法执行：利用互信息估计黑盒函数的可计算属性",
        "摘要翻译": "在许多现实世界的问题中，我们希望在给定T次函数评估的预算下，推断出昂贵黑盒函数f的某些属性。一个例子是f的预算约束全局优化，贝叶斯优化是一种流行的方法。其他感兴趣的属性包括局部最优、水平集、积分或由f诱导的图结构信息。通常，我们可以找到一个算法A来计算所需的属性，但它可能需要远超过T次查询才能执行。给定这样的A和f上的先验分布，我们将使用T次评估推断A输出的问题称为贝叶斯算法执行（BAX）。为了解决这个问题，我们提出了一个过程InfoBAX，它顺序选择最大化与算法输出互信息的查询。例如，将此应用于Dijkstra算法，我们在合成和真实世界的图中推断出具有黑盒边成本的最短路径。使用进化策略，我们产生了针对局部而非全局最优的贝叶斯优化变体。在这些问题上，InfoBAX使用的查询次数比原始算法少500倍。我们的方法与其他贝叶斯最优实验设计程序密切相关，如熵搜索方法和使用高斯过程的最优传感器放置。",
        "领域": "贝叶斯优化, 全局优化, 图算法",
        "问题": "如何在有限的函数评估预算下，高效推断黑盒函数的特定属性",
        "动机": "解决在预算有限的情况下，如何有效利用贝叶斯方法推断黑盒函数属性的问题",
        "方法": "提出InfoBAX过程，通过最大化与算法输出互信息的查询顺序选择，来推断黑盒函数的属性",
        "关键词": [
            "贝叶斯算法执行",
            "互信息",
            "黑盒函数",
            "全局优化",
            "图算法"
        ],
        "涉及的技术概念": {
            "贝叶斯算法执行（BAX）": "使用有限的函数评估预算推断算法输出的方法",
            "互信息": "用于衡量查询选择与算法输出之间信息量的指标",
            "高斯过程": "用于建模黑盒函数和优化查询选择的概率模型"
        },
        "success": true
    },
    {
        "order": 133,
        "title": "Bayesian Attention Belief Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9005",
        "abstract": "Attention-based neural networks have achieved state-of-the-art results on a wide range of tasks. Most such models use deterministic attention while stochastic attention is less explored due to the optimization difficulties or complicated model design. This paper introduces Bayesian attention belief networks, which construct a decoder network by modeling unnormalized attention weights with a hierarchy of gamma distributions, and an encoder network by stacking Weibull distributions with a deterministic-upward-stochastic-downward structure to approximate the posterior. The resulting auto-encoding networks can be optimized in a differentiable way with a variational lower bound. It is simple to convert any models with deterministic attention, including pretrained ones, to the proposed Bayesian attention belief networks. On a variety of language understanding tasks, we show that our method outperforms deterministic attention and state-of-the-art stochastic attention in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our method on neural machine translation and visual question answering, showing great potential of incorporating our method into various attention-related tasks.",
        "conference": "ICML",
        "中文标题": "贝叶斯注意力信念网络",
        "摘要翻译": "基于注意力的神经网络已在广泛的任务中取得了最先进的结果。大多数此类模型使用确定性注意力，而由于优化困难或模型设计复杂，随机注意力较少被探索。本文介绍了贝叶斯注意力信念网络，它通过用伽马分布的层次结构建模未归一化的注意力权重来构建解码器网络，并通过堆叠具有确定性向上-随机向下结构的威布尔分布来构建编码器网络以近似后验。所得到的自动编码网络可以通过变分下界以可微分的方式进行优化。将任何具有确定性注意力的模型（包括预训练模型）转换为所提出的贝叶斯注意力信念网络非常简单。在各种语言理解任务上，我们展示了我们的方法在准确性、不确定性估计、跨领域泛化能力和对抗攻击鲁棒性方面优于确定性注意力和最先进的随机注意力。我们进一步展示了我们的方法在神经机器翻译和视觉问答中的普遍适用性，显示出将我们的方法纳入各种注意力相关任务的巨大潜力。",
        "领域": "自然语言处理与视觉结合, 神经机器翻译, 视觉问答",
        "问题": "解决确定性注意力模型在优化困难和模型设计复杂方面的限制，以及提升模型在准确性、不确定性估计、跨领域泛化和对抗攻击鲁棒性方面的表现。",
        "动机": "探索随机注意力模型的潜力，以克服确定性注意力模型的局限性，并在多种任务中实现更优的性能。",
        "方法": "通过构建基于伽马分布和威布尔分布的贝叶斯注意力信念网络，以可微分的方式优化模型，并能够简单地将现有确定性注意力模型转换为随机注意力模型。",
        "关键词": [
            "贝叶斯注意力信念网络",
            "随机注意力",
            "伽马分布",
            "威布尔分布",
            "变分下界"
        ],
        "涉及的技术概念": {
            "伽马分布": "用于建模未归一化的注意力权重，构建解码器网络。",
            "威布尔分布": "用于构建编码器网络，具有确定性向上-随机向下的结构以近似后验。",
            "变分下界": "用于以可微分的方式优化自动编码网络。"
        },
        "success": true
    },
    {
        "order": 134,
        "title": "Bayesian Deep Learning via Subnetwork Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9939",
        "abstract": "The Bayesian paradigm has the potential to solve core issues of deep neural networks such as poor calibration and data inefficiency. Alas, scaling Bayesian inference to large weight spaces often requires restrictive approximations. In this work, we show that it suffices to perform inference over a small subset of model weights in order to obtain accurate predictive posteriors. The other weights are kept as point estimates. This subnetwork inference framework enables us to use expressive, otherwise intractable, posterior approximations over such subsets. In particular, we implement subnetwork linearized Laplace as a simple, scalable Bayesian deep learning method: We first obtain a MAP estimate of all weights and then infer a full-covariance Gaussian posterior over a subnetwork using the linearized Laplace approximation. We propose a subnetwork selection strategy that aims to maximally preserve the model’s predictive uncertainty. Empirically, our approach compares favorably to ensembles and less expressive posterior approximations over full networks.",
        "conference": "ICML",
        "中文标题": "通过子网络推理进行贝叶斯深度学习",
        "摘要翻译": "贝叶斯范式有潜力解决深度神经网络的核心问题，如校准不佳和数据效率低下。然而，将贝叶斯推理扩展到大型权重空间通常需要限制性近似。在这项工作中，我们表明，仅需对模型权重的一小部分进行推理即可获得准确的预测后验。其他权重保持为点估计。这种子网络推理框架使我们能够使用表达性强、否则难以处理的后验近似。特别是，我们实现了子网络线性化拉普拉斯作为一种简单、可扩展的贝叶斯深度学习方法：我们首先获得所有权重的最大后验估计，然后使用线性化拉普拉斯近似在子网络上推断全协方差高斯后验。我们提出了一种子网络选择策略，旨在最大限度地保留模型的预测不确定性。实证上，我们的方法在比较中优于集成和在全网络上表达能力较差的后验近似。",
        "领域": "贝叶斯深度学习、模型不确定性估计、神经网络优化",
        "问题": "解决深度神经网络在校准和数据效率方面的核心问题",
        "动机": "通过子网络推理简化贝叶斯深度学习的复杂性，同时保持预测准确性",
        "方法": "采用子网络线性化拉普拉斯方法，先进行所有权重的最大后验估计，再在选定的子网络上进行全协方差高斯后验推断",
        "关键词": [
            "贝叶斯深度学习",
            "子网络推理",
            "线性化拉普拉斯",
            "预测不确定性",
            "模型校准"
        ],
        "涉及的技术概念": {
            "贝叶斯范式": "用于解决深度神经网络校准不佳和数据效率低下的问题",
            "子网络推理": "仅对模型权重的一小部分进行推理，以简化计算并保持预测准确性",
            "线性化拉普拉斯近似": "在子网络上推断全协方差高斯后验的方法，用于实现简单、可扩展的贝叶斯深度学习"
        },
        "success": true
    },
    {
        "order": 135,
        "title": "Bayesian Optimistic Optimisation with Exponentially Decaying Regret",
        "html": "https://ICML.cc//virtual/2021/poster/9763",
        "abstract": " Bayesian optimisation (BO) is a well known algorithm for finding the global optimum of expensive, black-box functions. The current practical BO algorithms have regret bounds ranging from $\\mathcal{O}(\\frac{logN}{\\sqrt{N}})$ to $\\mathcal O(e^{-\\sqrt{N}})$, where $N$ is the number of evaluations. This paper explores the possibility of improving the regret bound in the noise-free setting by intertwining concepts from BO and optimistic optimisation methods which are based on partitioning the search space. We propose the BOO algorithm, a first practical approach which can achieve an exponential regret bound with order $\\mathcal O(N^{-\\sqrt{N}})$ under the assumption that the objective function is sampled from a Gaussian process with a Mat\\'ern kernel with smoothness parameter $\\nu > 4 +\\frac{D}{2}$, where $D$ is the number of dimensions. We perform experiments on optimisation of various synthetic functions and machine learning hyperparameter tuning tasks and show that our algorithm outperforms baselines.",
        "conference": "ICML",
        "success": true,
        "中文标题": "贝叶斯乐观优化与指数衰减遗憾",
        "摘要翻译": "贝叶斯优化（BO）是一种众所周知的算法，用于寻找昂贵黑盒函数的全局最优解。当前实用的BO算法的遗憾界限范围从O(logN/√N)到O(e^(-√N))，其中N是评估次数。本文探讨了在无噪声设置下，通过将BO与基于搜索空间划分的乐观优化方法的概念相结合，改善遗憾界限的可能性。我们提出了BOO算法，这是第一种实用的方法，在假设目标函数是从具有平滑度参数ν > 4 + D/2的Matérn核的高斯过程中采样的情况下，可以实现阶数为O(N^(-√N))的指数遗憾界限，其中D是维度数。我们在各种合成函数的优化和机器学习超参数调整任务上进行了实验，并显示我们的算法优于基线。",
        "领域": "贝叶斯优化, 超参数优化, 高斯过程",
        "问题": "提高贝叶斯优化算法在无噪声设置下的遗憾界限",
        "动机": "探索通过结合贝叶斯优化和乐观优化方法的概念，改善现有算法的遗憾界限",
        "方法": "提出BOO算法，结合贝叶斯优化和乐观优化方法，实现指数遗憾界限",
        "关键词": [
            "贝叶斯优化",
            "乐观优化",
            "高斯过程",
            "超参数调整",
            "遗憾界限"
        ],
        "涉及的技术概念": {
            "贝叶斯优化": "用于寻找昂贵黑盒函数全局最优解的算法",
            "乐观优化方法": "基于搜索空间划分的优化方法，用于提高优化效率",
            "高斯过程": "用于建模目标函数的概率模型，假设目标函数是从高斯过程中采样的"
        }
    },
    {
        "order": 136,
        "title": "Bayesian Optimization over Hybrid Spaces",
        "html": "https://ICML.cc//virtual/2021/poster/9183",
        "abstract": "We consider the problem of optimizing hybrid structures (mixture of discrete and continuous input variables) via expensive black-box function evaluations. This problem arises in many real-world applications. For example, in materials design optimization via lab experiments, discrete and continuous variables correspond to the presence/absence of primitive elements and their relative concentrations respectively. The key challenge is to accurately model the complex interactions between discrete and continuous variables. In this paper, we propose a novel approach referred as Hybrid Bayesian Optimization (HyBO) by utilizing diffusion kernels, which are naturally defined over continuous and discrete variables. We develop a principled approach for constructing diffusion kernels over hybrid spaces by utilizing the additive kernel formulation, which allows additive interactions of all orders in a tractable manner. We theoretically analyze the modeling strength of additive hybrid kernels and prove that it has the universal approximation property. Our experiments on synthetic and six diverse real-world benchmarks show that HyBO significantly outperforms the state-of-the-art methods.",
        "conference": "ICML",
        "中文标题": "混合空间上的贝叶斯优化",
        "摘要翻译": "我们考虑通过昂贵的黑盒函数评估来优化混合结构（离散和连续输入变量的混合）的问题。这个问题在许多实际应用中都会出现。例如，在通过实验室实验进行材料设计优化时，离散和连续变量分别对应于原始元素的存在/缺失及其相对浓度。关键挑战是准确建模离散和连续变量之间的复杂相互作用。在本文中，我们提出了一种称为混合贝叶斯优化（HyBO）的新方法，通过利用扩散核，这些核自然定义在连续和离散变量上。我们开发了一种基于原理的方法，通过利用加性核公式在混合空间上构建扩散核，这允许以可处理的方式实现所有阶的加性相互作用。我们从理论上分析了加性混合核的建模强度，并证明其具有通用逼近性质。我们在合成数据和六个不同的真实世界基准上的实验表明，HyBO显著优于最先进的方法。",
        "领域": "贝叶斯优化, 材料设计优化, 黑盒函数优化",
        "问题": "优化混合结构（离散和连续输入变量的混合）的问题",
        "动机": "准确建模离散和连续变量之间的复杂相互作用，以优化混合结构",
        "方法": "提出混合贝叶斯优化（HyBO）方法，利用扩散核在混合空间上构建加性核，实现所有阶的加性相互作用",
        "关键词": [
            "贝叶斯优化",
            "混合空间",
            "扩散核",
            "加性核",
            "材料设计"
        ],
        "涉及的技术概念": {
            "扩散核": "用于在连续和离散变量上自然定义的核，支持混合空间的建模",
            "加性核": "允许以可处理的方式实现所有阶的加性相互作用，用于构建混合空间上的扩散核",
            "通用逼近性质": "证明加性混合核能够逼近任何复杂的函数关系，确保模型的广泛适用性"
        },
        "success": true
    },
    {
        "order": 137,
        "title": "Bayesian Quadrature on Riemannian Data Manifolds",
        "html": "https://ICML.cc//virtual/2021/poster/9655",
        "abstract": "Riemannian manifolds provide a principled way to model nonlinear geometric structure inherent in data. A Riemannian metric on said manifolds determines geometry-aware shortest paths and provides the means to define statistical models accordingly. However, these operations are typically computationally demanding.\nTo ease this computational burden, we advocate probabilistic numerical methods for Riemannian statistics. In particular, we focus on Bayesian quadrature (BQ) to numerically compute integrals over normal laws on Riemannian manifolds learned from data. In this task, each function evaluation relies on the solution of an expensive initial value problem. We show that by leveraging both prior knowledge and an active exploration scheme, BQ significantly reduces the number of required evaluations and thus outperforms Monte Carlo methods on a wide range of integration problems. As a concrete application, we highlight the merits of adopting Riemannian geometry with our proposed framework on a nonlinear dataset from molecular dynamics.",
        "conference": "ICML",
        "中文标题": "黎曼数据流形上的贝叶斯积分",
        "摘要翻译": "黎曼流形为数据中固有的非线性几何结构提供了原则性的建模方法。流形上的黎曼度量决定了几何感知的最短路径，并提供了定义相应统计模型的手段。然而，这些操作通常计算量巨大。为了减轻这一计算负担，我们提倡使用概率数值方法进行黎曼统计。特别是，我们专注于贝叶斯积分（BQ），以数值计算从数据中学习到的黎曼流形上正态定律的积分。在这一任务中，每次函数评估都依赖于一个昂贵的初值问题的解。我们表明，通过利用先验知识和主动探索方案，BQ显著减少了所需的评估次数，从而在一系列积分问题上优于蒙特卡洛方法。作为一个具体应用，我们通过分子动力学的非线性数据集，展示了采用黎曼几何与我们提出的框架的优点。",
        "领域": "几何深度学习、统计机器学习、计算几何",
        "问题": "如何在黎曼流形上高效计算积分，以支持统计模型的构建和分析",
        "动机": "黎曼流形上的操作计算量大，需要开发更高效的计算方法来支持统计模型的构建和分析",
        "方法": "采用贝叶斯积分（BQ）方法，结合先验知识和主动探索方案，减少计算积分所需的函数评估次数",
        "关键词": [
            "贝叶斯积分",
            "黎曼流形",
            "统计机器学习",
            "计算几何",
            "分子动力学"
        ],
        "涉及的技术概念": {
            "贝叶斯积分": "一种概率数值方法，用于高效计算黎曼流形上的积分，通过利用先验知识减少计算量",
            "黎曼流形": "提供了一种建模数据非线性几何结构的方法，支持几何感知的统计模型构建",
            "主动探索方案": "在贝叶斯积分中用于优化函数评估的策略，以提高计算效率"
        },
        "success": true
    },
    {
        "order": 138,
        "title": "Bayesian Structural Adaptation for Continual Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9931",
        "abstract": "Continual Learning is a learning paradigm where learning systems are trained on a sequence of tasks. The goal here is to perform well on the current task without suffering from a performance drop on the previous tasks. Two notable directions among the recent advances in continual learning with neural networks are (1) variational Bayes based regularization by learning priors from previous tasks, and, (2) learning the structure of deep networks to adapt to new tasks. So far, these two approaches have been largely orthogonal. We present a novel Bayesian framework based on continually learning the structure of deep neural networks, to unify these distinct yet complementary approaches. The proposed framework learns the deep structure for each task by learning which weights to be used, and supports inter-task transfer through the overlapping of different sparse subsets of weights learned by different tasks. An appealing aspect of our proposed continual learning framework is that it is applicable to both discriminative (supervised) and generative (unsupervised) settings. Experimental results on supervised and unsupervised benchmarks demonstrate that our approach performs comparably or better than recent advances in continual learning.",
        "conference": "ICML",
        "中文标题": "贝叶斯结构自适应持续学习",
        "摘要翻译": "持续学习是一种学习范式，其中学习系统在一系列任务上进行训练。这里的目标是在当前任务上表现良好，同时不降低对先前任务的性能。神经网络持续学习近期进展中的两个显著方向是：（1）通过从先前任务中学习先验知识进行基于变分贝叶斯的正则化，以及（2）学习深度网络的结构以适应新任务。到目前为止，这两种方法在很大程度上是正交的。我们提出了一个基于持续学习深度神经网络结构的新贝叶斯框架，以统一这些不同但互补的方法。所提出的框架通过学习哪些权重被使用来为每个任务学习深度结构，并通过不同任务学习的不同稀疏权重子集的重叠支持任务间转移。我们提出的持续学习框架的一个吸引人的方面是，它既适用于判别性（监督）也适用于生成性（无监督）设置。在监督和无监督基准测试上的实验结果表明，我们的方法在持续学习中的表现与近期进展相当或更好。",
        "领域": "持续学习",
        "问题": "如何在持续学习过程中既适应新任务又不遗忘旧任务",
        "动机": "统一基于变分贝叶斯的正则化和深度网络结构学习这两种互补的持续学习方法",
        "方法": "提出一个贝叶斯框架，通过学习深度神经网络的结构和权重的稀疏子集来实现任务间的知识转移",
        "关键词": [
            "持续学习",
            "贝叶斯框架",
            "结构学习",
            "权重稀疏",
            "任务间转移"
        ],
        "涉及的技术概念": {
            "变分贝叶斯正则化": "用于从先前任务中学习先验知识，以防止遗忘",
            "深度网络结构学习": "通过调整网络结构来适应新任务",
            "权重稀疏子集": "不同任务学习的不同权重子集，通过它们的重叠实现任务间知识转移"
        },
        "success": true
    },
    {
        "order": 139,
        "title": "Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement",
        "html": "https://ICML.cc//virtual/2021/poster/9179",
        "abstract": "In representation learning, there has been recent interest in developing algorithms to disentangle the ground-truth generative factors behind a dataset, and metrics to quantify how fully this occurs.  However, these algorithms and metrics often assume that both representations and ground-truth factors are flat, continuous, and factorized, whereas many real-world generative processes involve rich hierarchical structure, mixtures of discrete and continuous variables with dependence between them, and even varying intrinsic dimensionality.  In this work, we develop benchmarks, algorithms, and metrics for learning such hierarchical representations.",
        "conference": "ICML",
        "中文标题": "层次解缠的基准、算法与度量",
        "摘要翻译": "在表示学习领域，近期有研究致力于开发算法以解缠数据集背后的真实生成因素，并量化这一过程的完整性。然而，这些算法和度量通常假设表示和真实因素都是平坦、连续且因子化的，而许多现实世界的生成过程涉及丰富的层次结构、离散与连续变量的混合及其间的依赖关系，甚至变化的固有维度。在本工作中，我们开发了用于学习此类层次表示的基准、算法和度量。",
        "领域": "表示学习、生成模型、层次结构学习",
        "问题": "解决现有解缠算法和度量在处理具有层次结构、混合变量及依赖关系的真实生成因素时的局限性。",
        "动机": "现实世界中的生成过程往往具有复杂的层次结构和变量间的依赖关系，而现有方法未能充分考虑这些特性，限制了算法的适用性和度量的准确性。",
        "方法": "开发新的基准、算法和度量，专门针对学习具有层次结构的表示，包括处理离散与连续变量的混合及其依赖关系。",
        "关键词": [
            "层次解缠",
            "表示学习",
            "生成模型",
            "度量标准",
            "算法开发"
        ],
        "涉及的技术概念": {
            "层次解缠": "指在表示学习中识别和分离数据生成过程中的层次化因素，以更准确地反映现实世界数据的复杂结构。",
            "表示学习": "通过算法自动学习数据的表示方式，以便于后续的机器学习任务，如分类或预测。",
            "生成模型": "能够学习数据分布并生成新数据样本的模型，用于理解和模拟数据的生成过程。"
        },
        "success": true
    },
    {
        "order": 140,
        "title": "Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9145",
        "abstract": "Most of existing statistical theories on deep neural networks have sample complexities cursed by the data dimension and therefore cannot well explain the empirical success of deep learning on high-dimensional data. To bridge this gap, we propose to exploit the low-dimensional structures of the real world datasets and establish theoretical guarantees of convolutional residual networks (ConvResNet) in terms of function approximation and statistical recovery for binary classification problem. Specifically, given the data lying on a $d$-dimensional manifold isometrically embedded in $\\mathbb{R}^D$, we prove that if the network architecture is properly chosen, ConvResNets can (1)  approximate {\\it Besov functions} on manifolds with arbitrary accuracy, and (2) learn a classifier by minimizing the empirical logistic risk, which gives an {\\it excess risk} in the order of $n^{-\\frac{s}{2s+2(s\\vee d)}}$, where $s$ is a smoothness parameter. This implies that the sample complexity depends on the intrinsic dimension $d$, instead of the data dimension $D$. Our results demonstrate that ConvResNets are adaptive to low-dimensional structures of data sets.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于卷积残差网络的低维流形上的Besov函数逼近与二元分类",
        "摘要翻译": "现有的关于深度神经网络的大多数统计理论都存在样本复杂度受数据维度影响的问题，因此无法很好地解释深度学习在高维数据上的经验成功。为了弥合这一差距，我们建议利用现实世界数据集的低维结构，并建立卷积残差网络（ConvResNet）在函数逼近和二元分类问题的统计恢复方面的理论保证。具体来说，给定数据位于等距嵌入到$mathbb{R}^D$中的$d$维流形上，我们证明，如果正确选择网络架构，ConvResNet可以（1）以任意精度逼近流形上的{\\it Besov函数}，并且（2）通过最小化经验逻辑风险来学习分类器，从而获得$n^{-\\frac{s}{2s+2(s\\vee d)}}$阶的{\\it 超额风险}，其中$s$是平滑度参数。这意味着样本复杂度取决于内在维度$d$，而不是数据维度$D$。我们的结果表明，ConvResNet能够适应数据集的低维结构。",
        "领域": "流形学习, 深度学习理论, 卷积神经网络",
        "问题": "如何解释深度学习在高维数据上的成功，并克服现有理论中样本复杂度受数据维度影响的局限性？",
        "动机": "现有的深度神经网络统计理论未能充分解释其在高维数据上的有效性，本研究旨在通过利用现实世界数据集的低维结构，为卷积残差网络在函数逼近和二元分类问题上提供理论保障。",
        "方法": "该论文通过理论分析，证明了卷积残差网络（ConvResNet）能够在低维流形上逼近Besov函数，并通过最小化经验逻辑风险学习分类器，从而获得与数据内在维度相关的样本复杂度。",
        "关键词": [
            "卷积残差网络",
            "低维流形",
            "Besov函数",
            "二元分类",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "卷积残差网络 (ConvResNet)": "一种深度神经网络架构，用于函数逼近和分类任务，特别适用于处理具有低维结构的数据。",
            "Besov函数": "一种函数空间，用于描述函数的平滑度，该研究证明了ConvResNet能够逼近流形上的Besov函数。"
        }
    },
    {
        "order": 141,
        "title": "Best Arm Identification in Graphical Bilinear Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/10093",
        "abstract": "We introduce a new graphical bilinear bandit problem where a learner (or a \\emph{central entity}) allocates arms to the nodes of a graph and observes for each edge a noisy bilinear reward representing the interaction between the two end nodes. We study the best arm identification problem in which the learner wants to find the graph allocation maximizing the sum of the bilinear rewards. By efficiently exploiting the geometry of this bandit problem, we propose a \\emph{decentralized} allocation strategy based on random sampling with theoretical guarantees. In particular, we characterize the influence of the graph structure (e.g. star, complete or circle) on the convergence rate and propose empirical experiments that confirm this dependency.",
        "conference": "ICML",
        "中文标题": "图结构双线性老虎机中的最佳臂识别",
        "摘要翻译": "我们引入了一个新的图结构双线性老虎机问题，其中学习者（或中央实体）将臂分配给图的节点，并观察每条边的噪声双线性奖励，该奖励代表两个端节点之间的交互。我们研究了最佳臂识别问题，其中学习者希望找到最大化双线性奖励总和的图分配。通过有效利用这个老虎机问题的几何特性，我们提出了一种基于随机采样的分散分配策略，并提供了理论保证。特别是，我们描述了图结构（如星形、完全或圆形）对收敛速度的影响，并提出了证实这种依赖性的实证实验。",
        "领域": "强化学习、图神经网络、多臂老虎机",
        "问题": "在图结构双线性老虎机问题中识别最大化双线性奖励总和的最佳臂分配",
        "动机": "研究图结构如何影响双线性老虎机问题中最佳臂识别的效率和收敛速度",
        "方法": "提出了一种基于随机采样的分散分配策略，并分析了不同图结构对策略性能的影响",
        "关键词": [
            "图结构双线性老虎机",
            "最佳臂识别",
            "分散分配策略",
            "收敛速度",
            "图结构影响"
        ],
        "涉及的技术概念": {
            "图结构双线性老虎机": "一种新的老虎机问题，其中奖励是通过图节点间的双线性交互产生的",
            "最佳臂识别": "在老虎机问题中识别能够提供最高累积奖励的臂的策略",
            "分散分配策略": "一种不依赖于中央控制的分配方法，通过随机采样实现，适用于图结构问题"
        },
        "success": true
    },
    {
        "order": 142,
        "title": "Best Model Identification: A Rested Bandit Formulation",
        "html": "https://ICML.cc//virtual/2021/poster/9257",
        "abstract": "We introduce and analyze a best arm identification problem in the rested bandit setting, wherein arms are themselves learning algorithms whose expected losses decrease with the number of times the arm has been played. The shape of the expected loss functions is similar across arms, and is assumed to be available up to unknown parameters that have to be learned on the fly. We define a novel notion of regret for this problem, where we compare to the policy that always plays the arm having the smallest expected loss at the end of the game. We analyze an arm elimination algorithm whose regret vanishes as the time horizon increases. The actual rate of convergence depends in a detailed way on the postulated functional form of the expected losses. We complement our analysis with lower bounds, indicating strengths and limitations of the proposed solution.",
        "conference": "ICML",
        "中文标题": "最佳模型识别：一种静止老虎机问题的表述",
        "摘要翻译": "我们介绍并分析了一种在静止老虎机设置下的最佳臂识别问题，其中臂本身就是学习算法，其预期损失随着臂被使用的次数而减少。预期损失函数的形状在臂之间相似，并且假设在需要实时学习的未知参数之前是可用的。我们为这个问题定义了一个新的遗憾概念，其中我们与总是在游戏结束时选择具有最小预期损失的臂的策略进行比较。我们分析了一种臂消除算法，其遗憾随着时间范围的增加而消失。实际的收敛速率以详细的方式依赖于预期的损失函数的假设形式。我们用下限补充我们的分析，指出了所提出解决方案的优势和局限性。",
        "领域": "强化学习、算法选择、多臂老虎机问题",
        "问题": "在静止老虎机设置下识别最佳臂（即最佳学习算法）的问题",
        "动机": "研究动机是为了解决在预期损失函数随时间减少的情况下，如何有效地识别和选择最佳学习算法的问题。",
        "方法": "采用了一种臂消除算法，该算法通过比较和消除预期损失较高的臂来识别最佳臂，并分析了其遗憾收敛的性质。",
        "关键词": [
            "最佳臂识别",
            "静止老虎机",
            "臂消除算法",
            "预期损失",
            "遗憾分析"
        ],
        "涉及的技术概念": {
            "静止老虎机": "在本文中指那些臂（即学习算法）的预期损失随着被使用次数减少的设置。",
            "臂消除算法": "一种通过逐步淘汰预期损失较高的臂来识别最佳臂的算法。",
            "遗憾": "在本文中定义为与始终选择游戏结束时具有最小预期损失的臂的策略相比的损失差异。"
        },
        "success": true
    },
    {
        "order": 143,
        "title": "Better Training using Weight-Constrained Stochastic Dynamics",
        "html": "https://ICML.cc//virtual/2021/poster/10583",
        "abstract": "We employ constraints to control the parameter space of deep neural networks throughout training. The use of customised, appropriately designed constraints can reduce the vanishing/exploding gradients problem, improve smoothness of classification boundaries, control weight magnitudes and stabilize deep neural networks, and thus enhance the robustness of training algorithms and the generalization capabilities of neural networks. We provide a general approach to efficiently incorporate constraints into a stochastic gradient Langevin framework, allowing enhanced exploration of the loss landscape. We also present specific examples of constrained training methods motivated by orthogonality preservation for weight matrices and explicit weight normalizations. Discretization schemes are provided both for the overdamped formulation of Langevin dynamics and the underdamped form, in which momenta further improve sampling efficiency. These optimisation schemes can be used directly, without needing to adapt neural network architecture design choices or to modify the objective with regularization terms, and see performance improvements in classification tasks.",
        "conference": "ICML",
        "中文标题": "使用权重约束随机动力学进行更好的训练",
        "摘要翻译": "我们在训练过程中使用约束来控制深度神经网络的参数空间。通过使用定制化、适当设计的约束，可以减少梯度消失/爆炸问题，提高分类边界的平滑度，控制权重的大小并稳定深度神经网络，从而增强训练算法的鲁棒性和神经网络的泛化能力。我们提供了一种将约束高效融入随机梯度Langevin框架的通用方法，允许对损失景观进行更深入的探索。我们还提出了受权重矩阵正交性保持和显式权重归一化启发的约束训练方法的具体示例。为Langevin动力学的过阻尼形式和欠阻尼形式提供了离散化方案，其中动量进一步提高了采样效率。这些优化方案可以直接使用，无需调整神经网络架构设计选择或通过正则化项修改目标，并在分类任务中看到了性能提升。",
        "领域": "深度学习优化、神经网络训练、随机梯度下降",
        "问题": "解决深度神经网络训练中的梯度消失/爆炸问题，提高训练算法的鲁棒性和网络的泛化能力。",
        "动机": "通过引入约束控制神经网络的参数空间，以改善训练过程的稳定性和效率。",
        "方法": "在随机梯度Langevin框架中融入定制化约束，包括权重矩阵正交性保持和显式权重归一化，提供过阻尼和欠阻尼Langevin动力学的离散化方案。",
        "关键词": [
            "权重约束",
            "随机梯度Langevin",
            "正交性保持",
            "权重归一化",
            "Langevin动力学"
        ],
        "涉及的技术概念": {
            "权重约束": "用于控制神经网络参数空间的约束，旨在减少梯度消失/爆炸问题，提高训练稳定性。",
            "随机梯度Langevin": "一种结合随机梯度下降和Langevin动力学的优化框架，用于更有效地探索损失景观。",
            "正交性保持": "一种约束方法，旨在保持权重矩阵的正交性，以提高网络的训练效率和性能。"
        },
        "success": true
    },
    {
        "order": 144,
        "title": "Beyond $log^2(T)$ regret for decentralized bandits in matching markets",
        "html": "https://ICML.cc//virtual/2021/poster/8743",
        "abstract": "We design decentralized algorithms for regret minimization in the two sided matching market with one-sided bandit feedback that significantly improves upon the prior works (Liu et al.\\,2020a, Sankararaman et al.\\,2020, Liu et al.\\,2020b). First, for general markets, for any $\\varepsilon >  0$, we design an algorithm that achieves a $O(\\log^{1+\\varepsilon}(T))$ regret to the agent-optimal stable matching, with unknown time horizon $T$,  improving upon the $O(\\log^{2}(T))$ regret achieved in (Liu et al.\\,2020b).  Second, we provide the optimal $\\Theta(\\log(T))$  agent-optimal regret for markets satisfying {\\em uniqueness consistency} -- markets where leaving participants don't alter the original stable matching. Previously, $\\Theta(\\log(T))$ regret was achievable (Sankararaman et al.\\,2020, Liu et al.\\,2020b) in the much restricted {\\em serial dictatorship} setting, when all arms have the same preference over the agents. We propose a phase based algorithm, where in each phase, besides deleting the globally communicated dominated arms the agents locally delete arms with which they collide often. This \\emph{local deletion} is pivotal in breaking deadlocks arising from rank heterogeneity of agents across arms.  We further demonstrate superiority of our algorithm over existing works through simulations.",
        "conference": "ICML",
        "中文标题": "超越对数平方(T)遗憾：匹配市场中的去中心化赌博机问题",
        "摘要翻译": "我们设计了一种用于双边匹配市场中单边赌博机反馈遗憾最小化的去中心化算法，该算法显著改进了先前的工作（Liu等人，2020a；Sankararaman等人，2020；Liu等人，2020b）。首先，对于一般市场，对于任何ε > 0，我们设计了一种算法，实现了对代理最优稳定匹配的O(log^(1+ε)(T))遗憾，其中时间范围T未知，这改进了在（Liu等人，2020b）中实现的O(log^2(T))遗憾。其次，我们为满足‘唯一一致性’的市场提供了最优的Θ(log(T))代理最优遗憾——在这些市场中，离开的参与者不会改变原有的稳定匹配。先前，在更为受限的‘序列独裁’设置中，当所有臂对代理有相同的偏好时，Θ(log(T))遗憾是可实现的（Sankararaman等人，2020；Liu等人，2020b）。我们提出了一种基于阶段的算法，在每个阶段中，除了删除全局通信的支配臂外，代理还本地删除他们经常碰撞的臂。这种‘本地删除’在打破由臂间代理等级异质性引起的僵局中起到了关键作用。我们通过仿真进一步证明了我们的算法相对于现有工作的优越性。",
        "领域": "多臂赌博机问题、匹配市场算法、去中心化学习",
        "问题": "在双边匹配市场中，如何设计去中心化算法以减少单边赌博机反馈带来的遗憾。",
        "动机": "改进现有算法在匹配市场中处理单边赌博机反馈时的遗憾性能，特别是在一般市场和满足唯一一致性的市场中。",
        "方法": "提出了一种基于阶段的算法，包括全局通信支配臂的删除和本地频繁碰撞臂的删除，以减少遗憾并打破代理等级异质性引起的僵局。",
        "关键词": [
            "去中心化算法",
            "遗憾最小化",
            "匹配市场",
            "单边赌博机反馈",
            "唯一一致性"
        ],
        "涉及的技术概念": {
            "代理最优稳定匹配": "算法旨在实现的匹配状态，其中每个代理的匹配结果是最优的且稳定。",
            "本地删除": "代理根据本地信息删除频繁碰撞的臂，以减少遗憾并提高匹配效率。",
            "唯一一致性": "市场的一种特性，指离开的参与者不会改变原有的稳定匹配状态，使得算法能够实现更低的遗憾。"
        },
        "success": true
    },
    {
        "order": 145,
        "title": "Beyond the Pareto Efficient Frontier: Constraint Active Search for Multiobjective Experimental Design",
        "html": "https://ICML.cc//virtual/2021/poster/8409",
        "abstract": "Many problems in engineering design and simulation require balancing competing objectives under the presence of uncertainty. Sample-efficient multiobjective optimization methods focus on the objective function values in metric space and ignore the sampling behavior of the design configurations in parameter space. Consequently, they may provide little actionable insight on how to choose designs in the presence of metric uncertainty or limited precision when implementing a chosen design. We propose a new formulation that accounts for the importance of the parameter space and is thus more suitable for multiobjective design problems; instead of searching for the Pareto-efficient frontier, we solicit the desired minimum performance thresholds on all objectives to define regions of satisfaction. We introduce an active search algorithm called Expected Coverage Improvement (ECI) to efficiently discover the region of satisfaction and simultaneously sample diverse acceptable configurations. We demonstrate our algorithm on several design and simulation domains:  mechanical design, additive manufacturing, medical monitoring, and plasma physics.",
        "conference": "ICML",
        "中文标题": "超越帕累托效率前沿：多目标实验设计的约束主动搜索",
        "摘要翻译": "在工程设计和仿真中的许多问题需要在存在不确定性的情况下平衡相互竞争的目标。样本高效的多目标优化方法侧重于度量空间中的目标函数值，而忽略了参数空间中设计配置的采样行为。因此，在存在度量不确定性或实施选定设计时精度有限的情况下，它们可能提供很少关于如何选择设计的可操作见解。我们提出了一种新的表述，考虑到了参数空间的重要性，因此更适合多目标设计问题；我们不是寻找帕累托效率前沿，而是征求所有目标上的期望最小性能阈值来定义满足区域。我们引入了一种称为期望覆盖改进（ECI）的主动搜索算法，以高效地发现满足区域并同时采样多样化的可接受配置。我们在几个设计和仿真领域展示了我们的算法：机械设计、增材制造、医疗监测和等离子体物理。",
        "领域": "多目标优化、工程设计优化、不确定性处理",
        "问题": "在存在不确定性的情况下，如何高效地平衡多目标优化问题中的竞争目标，并提供可操作的设计选择见解。",
        "动机": "现有的多目标优化方法忽视了参数空间中的采样行为，导致在度量不确定性或实施精度有限的情况下提供的设计选择见解不足。",
        "方法": "提出了一种新的多目标设计问题表述，通过定义满足区域来替代寻找帕累托效率前沿，并引入了期望覆盖改进（ECI）算法来高效发现满足区域和采样多样化可接受配置。",
        "关键词": [
            "多目标优化",
            "参数空间",
            "期望覆盖改进",
            "工程设计",
            "不确定性处理"
        ],
        "涉及的技术概念": {
            "帕累托效率前沿": "在多目标优化中，表示在所有目标上都无法进一步改进的解的集合。",
            "期望覆盖改进（ECI）": "一种主动搜索算法，用于高效发现满足性能阈值的设计配置区域。",
            "满足区域": "通过设定所有目标上的最小性能阈值定义的，表示设计配置满足所有性能要求的区域。"
        },
        "success": true
    },
    {
        "order": 146,
        "title": "Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9757",
        "abstract": "Bandit and reinforcement learning (RL) problems can often be framed as optimization problems where the goal is to maximize average performance while having access only to stochastic estimates of the true gradient. Traditionally, stochastic optimization theory predicts that learning dynamics are governed by the curvature of the loss function and the noise of the gradient estimates. In this paper we demonstrate that the standard view is too limited for bandit and RL problems. To allow our analysis to be interpreted in light of multi-step MDPs, we focus on techniques derived from stochastic optimization principles~(e.g., natural policy gradient and EXP3) and we show that some standard assumptions from optimization theory are violated in these problems. We present theoretical results showing that, at least for bandit problems, curvature and noise are not sufficient to explain the learning dynamics and that seemingly innocuous choices like the baseline can determine whether an algorithm converges. These theoretical findings match our empirical evaluation, which we extend to multi-state MDPs.",
        "conference": "ICML",
        "中文标题": "超越方差减少：理解基线对策略优化的真实影响",
        "摘要翻译": "强盗和强化学习（RL）问题通常可以被视为优化问题，其目标是最大化平均性能，同时只能访问真实梯度的随机估计。传统上，随机优化理论预测学习动态由损失函数的曲率和梯度估计的噪声决定。在本文中，我们证明了标准观点对强盗和RL问题来说过于局限。为了让我们的分析能够在多步MDP的背景下被解释，我们专注于从随机优化原则（例如，自然策略梯度和EXP3）衍生的技术，并且我们展示了优化理论中的一些标准假设在这些问题中被违反。我们提出的理论结果表明，至少对于强盗问题，曲率和噪声不足以解释学习动态，而看似无害的选择，如基线，可以决定算法是否收敛。这些理论发现与我们的实证评估相符，我们将其扩展到多状态MDP。",
        "领域": "强化学习、策略优化、多步马尔可夫决策过程",
        "问题": "理解基线在策略优化中的作用及其对算法收敛性的影响",
        "动机": "揭示传统随机优化理论在解释强盗和强化学习问题中的局限性，特别是基线选择对算法性能的影响",
        "方法": "通过理论分析和实证评估，研究自然策略梯度和EXP3等技术在多步MDP中的应用，以及基线选择对算法收敛性的影响",
        "关键词": [
            "策略优化",
            "基线影响",
            "强盗问题",
            "强化学习",
            "多步MDP"
        ],
        "涉及的技术概念": {
            "自然策略梯度": "一种从随机优化原则衍生的策略优化方法，用于更新策略参数",
            "EXP3": "一种用于强盗问题的算法，通过指数加权策略选择动作",
            "多步马尔可夫决策过程": "扩展的MDP框架，用于分析多步决策问题中的策略优化"
        },
        "success": true
    },
    {
        "order": 147,
        "title": "Bias-Free Scalable Gaussian Processes via Randomized Truncations",
        "html": "https://ICML.cc//virtual/2021/poster/10577",
        "abstract": "Scalable Gaussian Process methods are computationally attractive, yet introduce modeling biases that require rigorous study. This paper analyzes two common techniques: early truncated conjugate gradients (CG) and random Fourier features (RFF). We ﬁnd that both methods introduce a systematic bias on the learned hyperparameters: CG tends to underﬁt while RFF tends to overﬁt. We address these issues using randomized truncation estimators that eliminate bias in exchange for increased variance. In the case of RFF, we show that the bias-to-variance conversion is indeed a trade-off: the additional variance proves detrimental to optimization. However, in the case of CG, our unbiased learning procedure meaningfully outperforms its biased counterpart with minimal additional computation. Our code is available at https://github.com/ cunningham-lab/RTGPS.",
        "conference": "ICML",
        "中文标题": "通过随机截断实现无偏可扩展高斯过程",
        "摘要翻译": "可扩展的高斯过程方法在计算上具有吸引力，但引入了需要严格研究的建模偏差。本文分析了两种常见技术：早期截断共轭梯度（CG）和随机傅里叶特征（RFF）。我们发现这两种方法在学习超参数时都引入了系统性偏差：CG倾向于欠拟合，而RFF倾向于过拟合。我们使用随机截断估计器来解决这些问题，这些估计器通过增加方差来消除偏差。在RFF的情况下，我们表明偏差到方差的转换确实是一种权衡：额外的方差对优化有害。然而，在CG的情况下，我们的无偏学习过程在几乎没有额外计算的情况下，显著优于其有偏对应物。我们的代码可在https://github.com/cunningham-lab/RTGPS获取。",
        "领域": "机器学习、高斯过程、优化算法",
        "问题": "解决可扩展高斯过程方法中引入的建模偏差问题",
        "动机": "研究旨在消除可扩展高斯过程方法中的系统性偏差，提高模型的学习效率和准确性",
        "方法": "采用随机截断估计器技术，通过增加方差来消除CG和RFF方法中的偏差",
        "关键词": [
            "高斯过程",
            "随机截断",
            "无偏估计",
            "共轭梯度",
            "随机傅里叶特征"
        ],
        "涉及的技术概念": {
            "随机截断估计器": "用于消除CG和RFF方法中的系统性偏差，通过增加方差来实现无偏估计",
            "共轭梯度（CG）": "一种优化算法，本文中早期截断CG导致欠拟合，通过随机截断估计器改进",
            "随机傅里叶特征（RFF）": "一种用于近似高斯过程的技术，本文中RFF导致过拟合，通过随机截断估计器调整偏差与方差的权衡"
        },
        "success": true
    },
    {
        "order": 148,
        "title": "Bias-Robust Bayesian Optimization via Dueling Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/8493",
        "abstract": "We consider Bayesian optimization in settings where observations can be adversarially biased, for example by an uncontrolled hidden confounder. Our first contribution is a reduction of the confounded setting to the dueling bandit model. Then we propose a novel approach for dueling bandits based on information-directed sampling (IDS). Thereby, we obtain the first efficient kernelized algorithm for dueling bandits that comes with cumulative regret guarantees. Our analysis further generalizes a previously proposed semi-parametric linear bandit model to non-linear reward functions, and uncovers interesting links to doubly-robust estimation.",
        "conference": "ICML",
        "中文标题": "通过决斗老虎机实现偏差鲁棒的贝叶斯优化",
        "摘要翻译": "我们考虑在观测可能被对抗性偏差（例如由不可控的隐藏混杂因素引起）的情况下进行贝叶斯优化。我们的第一个贡献是将混杂设置简化为决斗老虎机模型。然后，我们提出了一种基于信息导向采样（IDS）的决斗老虎机新方法。由此，我们获得了第一个具有累积遗憾保证的高效核化决斗老虎机算法。我们的分析进一步将先前提出的半参数线性老虎机模型推广到非线性奖励函数，并揭示了与双重鲁棒估计的有趣联系。",
        "领域": "贝叶斯优化",
        "问题": "在观测可能被对抗性偏差影响的情况下进行贝叶斯优化",
        "动机": "解决在存在不可控隐藏混杂因素的情况下，贝叶斯优化观测可能被对抗性偏差影响的问题",
        "方法": "将混杂设置简化为决斗老虎机模型，并提出基于信息导向采样（IDS）的决斗老虎机新方法",
        "关键词": [
            "贝叶斯优化",
            "决斗老虎机",
            "信息导向采样",
            "对抗性偏差",
            "双重鲁棒估计"
        ],
        "涉及的技术概念": {
            "决斗老虎机模型": "用于简化和处理混杂设置下的贝叶斯优化问题",
            "信息导向采样（IDS）": "提出的新方法，用于高效解决决斗老虎机问题",
            "双重鲁棒估计": "分析中揭示的与决斗老虎机模型的有趣联系"
        },
        "success": true
    },
    {
        "order": 149,
        "title": "Bias-Variance Reduced Local SGD for Less Heterogeneous Federated  Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9507",
        "abstract": "Recently, local SGD has got much attention and been extensively studied in the distributed learning community to overcome the communication bottleneck problem. However, the superiority of local SGD to minibatch SGD only holds in quite limited situations. In this paper, we study a new local algorithm called Bias-Variance Reduced Local SGD (BVR-L-SGD) for nonconvex distributed optimization. Algorithmically, our proposed bias and variance reduced local gradient estimator fully utilizes small second-order heterogeneity of local objectives and suggests randomly picking up one of the local models instead of taking the average of them when workers are synchronized. Theoretically, under small heterogeneity of local objectives, we show that BVR-L-SGD achieves better communication complexity than both the previous non-local and local methods under mild conditions, and particularly BVR-L-SGD is the first method that breaks the barrier of communication complexity $\\Theta(1/\\varepsilon)$ for general nonconvex smooth objectives when the heterogeneity is small and the local computation budget is large. Numerical results are given to verify the theoretical findings and give empirical evidence of the superiority of our method.",
        "conference": "ICML",
        "success": true,
        "中文标题": "减少偏差-方差的本地SGD用于降低异构性的联邦学习",
        "摘要翻译": "最近，本地SGD在分布式学习社区中受到了广泛关注，并被深入研究以克服通信瓶颈问题。然而，本地SGD相对于小批量SGD的优越性仅在非常有限的情况下成立。在本文中，我们研究了一种名为偏差-方差减少本地SGD（BVR-L-SGD）的新本地算法，用于非凸分布式优化。算法上，我们提出的偏差和方差减少的本地梯度估计器充分利用了本地目标的小二阶异构性，并建议在工作者同步时随机选取一个本地模型，而不是取它们的平均值。理论上，在本地目标的小异构性下，我们表明BVR-L-SGD在温和条件下比之前的非本地和本地方法实现了更好的通信复杂度，特别是BVR-L-SGD是第一个打破一般非凸平滑目标通信复杂度Θ(1/ε)障碍的方法，当异构性小且本地计算预算大时。数值结果验证了理论发现，并提供了我们方法优越性的实证证据。",
        "领域": "联邦学习, 分布式优化, 非凸优化",
        "问题": "解决在分布式学习中本地SGD方法在通信效率和优化性能上的局限性",
        "动机": "提高分布式学习中本地SGD方法的通信效率和优化性能，特别是在异构性较小的环境下",
        "方法": "提出了一种新的偏差-方差减少本地SGD算法（BVR-L-SGD），通过利用本地目标的二阶异构性和随机选取本地模型来优化通信复杂度",
        "关键词": [
            "联邦学习",
            "本地SGD",
            "偏差-方差减少",
            "分布式优化",
            "非凸优化"
        ],
        "涉及的技术概念": {
            "本地SGD": "一种在分布式学习中用于减少通信次数的随机梯度下降变体",
            "偏差-方差减少": "通过算法设计减少估计器的偏差和方差，以提高优化效率和准确性",
            "非凸优化": "处理目标函数为非凸时的优化问题，这在机器学习和深度学习中非常常见"
        }
    },
    {
        "order": 150,
        "title": "Bilevel Optimization: Convergence Analysis and Enhanced Design",
        "html": "https://ICML.cc//virtual/2021/poster/9473",
        "abstract": "Bilevel optimization has arisen as a powerful tool for many machine learning problems such as meta-learning, hyperparameter optimization, and reinforcement learning. In this paper, we investigate the nonconvex-strongly-convex bilevel optimization problem. For deterministic bilevel optimization, we provide a comprehensive convergence rate analysis for two popular algorithms respectively based on approximate implicit differentiation (AID) and iterative differentiation (ITD). For the AID-based method, we orderwisely improve the previous convergence rate analysis due to a more practical parameter selection as well as a warm start strategy, and for the ITD-based method we establish the first theoretical convergence rate. Our analysis also provides a quantitative comparison between ITD and AID based approaches. For stochastic bilevel optimization, we propose a novel algorithm named stocBiO, which features a sample-efficient hypergradient estimator using efficient Jacobian- and Hessian-vector product computations. We provide the convergence rate guarantee for stocBiO, and show that stocBiO outperforms the best known computational complexities orderwisely with respect to the condition number $\\kappa$ and the target accuracy $\\epsilon$. We further validate our theoretical results and demonstrate the efficiency of bilevel optimization  algorithms by the experiments on meta-learning and hyperparameter optimization. ",
        "conference": "ICML",
        "中文标题": "双层优化：收敛性分析与增强设计",
        "摘要翻译": "双层优化已成为解决元学习、超参数优化和强化学习等许多机器学习问题的强大工具。在本文中，我们研究了非凸-强凸双层优化问题。对于确定性双层优化，我们分别为基于近似隐式微分（AID）和迭代微分（ITD）的两种流行算法提供了全面的收敛速率分析。对于基于AID的方法，由于更实际的参数选择和预热启动策略，我们在顺序上改进了之前的收敛速率分析；对于基于ITD的方法，我们首次建立了理论收敛速率。我们的分析还提供了ITD和AID方法之间的定量比较。对于随机双层优化，我们提出了一种名为stocBiO的新算法，该算法通过高效的雅可比矩阵和海森矩阵向量积计算，实现了样本高效的超梯度估计器。我们为stocBiO提供了收敛速率保证，并显示stocBiO在条件数κ和目标精度ε方面顺序上优于已知的最佳计算复杂度。我们通过在元学习和超参数优化上的实验进一步验证了我们的理论结果，并展示了双层优化算法的效率。",
        "领域": "元学习, 超参数优化, 强化学习",
        "问题": "解决非凸-强凸双层优化问题的收敛性分析和算法设计",
        "动机": "双层优化在机器学习领域有广泛应用，但现有方法在收敛性分析和计算效率上存在不足，需要更深入的理论分析和更高效的算法设计。",
        "方法": "对确定性双层优化，改进基于AID和ITD的算法的收敛性分析；对随机双层优化，提出新算法stocBiO，利用高效的雅可比矩阵和海森矩阵向量积计算提高样本效率。",
        "关键词": [
            "双层优化",
            "收敛性分析",
            "超梯度估计",
            "元学习",
            "超参数优化"
        ],
        "涉及的技术概念": {
            "近似隐式微分（AID）": "用于确定性双层优化的一种方法，通过近似隐式微分来求解优化问题，本文改进了其收敛速率分析。",
            "迭代微分（ITD）": "另一种用于确定性双层优化的方法，本文首次为其建立了理论收敛速率。",
            "stocBiO算法": "针对随机双层优化提出的新算法，通过高效的雅可比矩阵和海森矩阵向量积计算，实现了样本高效的超梯度估计。"
        },
        "success": true
    },
    {
        "order": 151,
        "title": "Bilinear Classes: A Structural Framework for Provable Generalization in RL",
        "html": "https://ICML.cc//virtual/2021/poster/9125",
        "abstract": "This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The framework incorporates nearly all existing models in which a polynomial sample complexity is achievable, and, notably, also includes new models, such as the Linear Q*/V* model in which both the optimal Q-function and the optimal V-function are linear in some known feature space. Our main result provides an RL algorithm which has polynomial sample complexity for Bilinear Classes; notably, this sample complexity is stated in terms of a reduction to the generalization error of an underlying supervised learning sub-problem. These bounds nearly match the best known sample complexity bounds for existing models. Furthermore, this framework also extends to the infinite dimensional (RKHS) setting: for the the Linear Q*/V* model, linear MDPs, and linear mixture MDPs, we provide sample complexities that have no explicit dependence on the explicit feature dimension (which could be infinite), but instead depends only on information theoretic quantities.",
        "conference": "ICML",
        "中文标题": "双线性类：强化学习中可证明泛化的结构框架",
        "摘要翻译": "本工作引入了双线性类，一个新的结构框架，该框架通过函数逼近的使用，在强化学习的多种设置中允许泛化。该框架几乎包含了所有可以实现多项式样本复杂度的现有模型，并且值得注意的是，还包括了一些新模型，例如线性Q*/V*模型，其中最优Q函数和最优V函数在某些已知的特征空间中都是线性的。我们的主要结果提供了一个针对双线性类的强化学习算法，该算法具有多项式样本复杂度；值得注意的是，这个样本复杂度是通过归约到基础监督学习子问题的泛化误差来表述的。这些边界几乎与现有模型的最佳已知样本复杂度边界相匹配。此外，该框架还扩展到无限维（RKHS）设置：对于线性Q*/V*模型、线性MDP和线性混合MDP，我们提供的样本复杂度不明确依赖于显式特征维度（可能是无限的），而是仅依赖于信息理论量。",
        "领域": "强化学习、函数逼近、样本复杂度分析",
        "问题": "如何在强化学习中通过函数逼近实现有效的泛化，并量化样本复杂度。",
        "动机": "为了解决强化学习在广泛设置中的泛化问题，并提供一个统一的框架来理解和量化样本复杂度。",
        "方法": "引入双线性类结构框架，开发一个具有多项式样本复杂度的强化学习算法，并通过归约到监督学习子问题的泛化误差来表述样本复杂度。",
        "关键词": [
            "双线性类",
            "强化学习",
            "函数逼近",
            "样本复杂度",
            "线性Q*/V*模型"
        ],
        "涉及的技术概念": {
            "双线性类": "一种新的结构框架，用于在强化学习中通过函数逼近实现泛化。",
            "函数逼近": "在强化学习中用于近似复杂函数的技术，以实现更高效的泛化。",
            "样本复杂度": "衡量学习算法性能的指标，表示算法达到一定性能水平所需的样本数量。"
        },
        "success": true
    },
    {
        "order": 152,
        "title": "Binary Classification from Multiple Unlabeled Datasets via Surrogate Set Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9865",
        "abstract": "To cope with high annotation costs, training a classifier only from weakly supervised data has attracted a great deal of attention these days. Among various approaches, strengthening supervision from completely unsupervised classification is a promising direction, which typically employs class priors as the only supervision and trains a binary classifier from unlabeled (U) datasets. While existing risk-consistent methods are theoretically grounded with high flexibility, they can learn only from two U sets. In this paper, we propose a new approach for binary classification from $m$ U-sets for $m\\ge2$. Our key idea is to consider an auxiliary classification task called surrogate set classification (SSC), which is aimed at predicting from which U set each observed sample is drawn. SSC can be solved by a standard (multi-class) classification method, and we use the SSC solution to obtain the final binary classifier through a certain linear-fractional transformation. We built our method in a flexible and efficient end-to-end deep learning framework and prove it to be classifier-consistent. Through experiments, we demonstrate the superiority of our proposed method over state-of-the-art methods.",
        "conference": "ICML",
        "中文标题": "通过替代集分类从多个未标记数据集进行二分类",
        "摘要翻译": "为了应对高标注成本，仅从弱监督数据训练分类器近年来引起了广泛关注。在各种方法中，从完全无监督分类中加强监督是一个有前景的方向，通常采用类别先验作为唯一的监督，从未标记（U）数据集中训练一个二分类器。虽然现有的风险一致方法在理论上具有高度的灵活性，但它们只能从两个U集中学习。在本文中，我们提出了一种新的方法，用于从m≥2的m个U集中进行二分类。我们的关键思想是考虑一个称为替代集分类（SSC）的辅助分类任务，其目的是预测每个观察样本是从哪个U集中抽取的。SSC可以通过标准的（多类）分类方法解决，我们使用SSC解决方案通过某种线性分数变换获得最终的二分类器。我们在一个灵活高效的端到端深度学习框架中构建了我们的方法，并证明它是分类器一致的。通过实验，我们证明了我们提出的方法优于最先进的方法。",
        "领域": "弱监督学习、二分类问题、深度学习框架",
        "问题": "如何从多个未标记数据集中有效地进行二分类",
        "动机": "减少高标注成本，利用弱监督数据进行有效的二分类",
        "方法": "提出替代集分类（SSC）作为辅助任务，通过标准多类分类方法解决，并利用线性分数变换获得最终的二分类器",
        "关键词": [
            "替代集分类",
            "二分类",
            "弱监督学习",
            "深度学习",
            "未标记数据"
        ],
        "涉及的技术概念": {
            "替代集分类（SSC）": "用于预测样本来源的U集的辅助分类任务，通过解决此任务来辅助最终的二分类",
            "线性分数变换": "将SSC解决方案转换为最终二分类器的技术手段",
            "分类器一致性": "证明了所提方法在理论上的有效性，确保分类器的性能"
        },
        "success": true
    },
    {
        "order": 153,
        "title": "Black-box density function estimation using recursive partitioning",
        "html": "https://ICML.cc//virtual/2021/poster/9471",
        "abstract": "We present a novel approach to Bayesian inference and general Bayesian computation that is defined through a sequential decision loop. Our method defines a recursive partitioning of the sample space. It neither relies on gradients nor requires any problem-specific tuning, and is asymptotically exact for any density function with a bounded domain. The output is an approximation to the whole density function including the normalisation constant, via partitions organised in efficient data structures. Such approximations may be used for evidence estimation or fast posterior sampling, but also as building blocks to treat a larger class of estimation problems. The algorithm shows competitive performance to recent state-of-the-art methods on synthetic and real-world problems including parameter inference for gravitational-wave physics.",
        "conference": "ICML",
        "中文标题": "使用递归划分的黑盒密度函数估计",
        "摘要翻译": "我们提出了一种新颖的贝叶斯推理和通用贝叶斯计算方法，该方法通过一个顺序决策循环定义。我们的方法定义了样本空间的递归划分。它既不依赖于梯度，也不需要任何特定问题的调整，并且对于任何有界域内的密度函数都是渐近精确的。输出是通过组织在高效数据结构中的划分，对包括归一化常数在内的整个密度函数的近似。这种近似可以用于证据估计或快速后验采样，也可以作为构建块来处理更大类别的估计问题。该算法在合成和现实世界问题上，包括引力波物理的参数推理，显示出与最新最先进方法相竞争的性能。",
        "领域": "贝叶斯推理, 密度估计, 参数推理",
        "问题": "如何在不需要梯度信息或特定问题调整的情况下，进行有效的密度函数估计和后验采样。",
        "动机": "开发一种不依赖梯度、无需问题特定调整且适用于任何有界域密度函数的通用贝叶斯计算方法。",
        "方法": "通过定义样本空间的递归划分，构建整个密度函数的近似，包括归一化常数，利用高效数据结构组织这些划分。",
        "关键词": [
            "递归划分",
            "贝叶斯推理",
            "密度估计",
            "后验采样",
            "引力波物理"
        ],
        "涉及的技术概念": {
            "递归划分": "通过递归方式划分样本空间，以构建密度函数的近似。",
            "贝叶斯推理": "利用贝叶斯定理进行统计推断，更新参数的分布。",
            "密度估计": "估计未知概率密度函数的过程，用于描述数据的分布。"
        },
        "success": true
    },
    {
        "order": 154,
        "title": "Blind Pareto Fairness and Subgroup Robustness",
        "html": "https://ICML.cc//virtual/2021/poster/9557",
        "abstract": "Much of the work in the field of group fairness addresses disparities between  predeﬁned groups based on protected features such as gender, age, and race, which need to be available at train, and often also at test, time. These approaches are static and retrospective, since algorithms designed to protect groups identified  a priori cannot anticipate and protect the needs of different at-risk groups in the future. In this work we analyze the space of solutions for worst-case fairness beyond demographics, and propose Blind Pareto Fairness (BPF), a method that leverages no-regret dynamics to recover a fair minimax classiﬁer that reduces worst-case risk of any potential subgroup of sufﬁcient size, and guarantees that the remaining population receives the best possible level of service. BPF addresses fairness beyond demographics, that is, it  does not rely on predeﬁned notions of at-risk groups, neither at train nor at test time. Our experimental results show that the proposed framework improves worst-case risk in multiple standard datasets, while simultaneously providing better levels of service for the remaining population. The code is available at github.com/natalialmg/BlindParetoFairness",
        "conference": "ICML",
        "中文标题": "盲帕累托公平性与子群体鲁棒性",
        "摘要翻译": "在群体公平性领域的大部分工作都致力于解决基于受保护特征（如性别、年龄和种族）预定义群体之间的差异，这些特征在训练时，通常也在测试时需要可用。这些方法是静态和回顾性的，因为旨在保护先验识别群体的算法无法预见并保护未来不同风险群体的需求。在这项工作中，我们分析了超越人口统计的最坏情况公平性解决方案空间，并提出了盲帕累托公平性（BPF），一种利用无悔动态恢复公平极小极大分类器的方法，该方法减少任何足够大小潜在子群体的最坏情况风险，并保证剩余群体获得最佳可能服务水平。BPF解决了超越人口统计的公平性问题，即它不依赖于预定义的风险群体概念，无论是在训练还是测试时。我们的实验结果表明，所提出的框架在多个标准数据集中改善了最坏情况风险，同时为剩余群体提供了更好的服务水平。代码可在github.com/natalialmg/BlindParetoFairness获取。",
        "领域": "公平机器学习、算法公平性、子群体鲁棒性",
        "问题": "解决超越预定义群体的公平性问题，特别是在不依赖预定义风险群体概念的情况下，减少任何潜在子群体的最坏情况风险。",
        "动机": "现有的公平性方法依赖于预定义的受保护特征，无法动态适应未来可能出现的风险群体需求。",
        "方法": "提出盲帕累托公平性（BPF），利用无悔动态恢复公平极小极大分类器，以减少任何足够大小子群体的最坏情况风险，并优化剩余群体的服务水平。",
        "关键词": [
            "盲帕累托公平性",
            "子群体鲁棒性",
            "无悔动态",
            "极小极大分类器",
            "算法公平性"
        ],
        "涉及的技术概念": {
            "无悔动态": "用于恢复公平极小极大分类器的技术，确保算法在不断变化的风险群体中保持公平性。",
            "极小极大分类器": "一种旨在最小化最坏情况风险的分类器，用于保护任何潜在子群体免受不公平待遇。",
            "算法公平性": "研究如何设计算法以避免对特定群体产生偏见或歧视，特别是在不依赖预定义群体特征的情况下。"
        },
        "success": true
    },
    {
        "order": 155,
        "title": "Boosting for Online Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10377",
        "abstract": "We consider the decision-making framework of online convex optimization with a very large number of experts. This setting is ubiquitous in contextual and reinforcement learning problems, where the size of the policy class renders enumeration and search within the policy class infeasible. Instead, we consider generalizing the methodology of online boosting. We define a weak learning algorithm as a mechanism that guarantees multiplicatively approximate regret against a base class of experts. In this access model, we give an efficient boosting algorithm that guarantees near-optimal regret against the convex hull of the base class. We consider both full and partial (a.k.a. bandit) information feedback models. We also give an analogous efficient boosting algorithm for the i.i.d. statistical setting. Our results simultaneously generalize online boosting and gradient boosting guarantees to contextual learning model, online convex optimization and bandit linear optimization settings. ",
        "conference": "ICML",
        "中文标题": "在线凸优化的提升方法",
        "摘要翻译": "我们考虑了一个具有极大量专家的在线凸优化决策框架。这一设置在上下文和强化学习问题中无处不在，其中策略类的大小使得在策略类中进行枚举和搜索变得不可行。相反，我们考虑推广在线提升的方法论。我们将弱学习算法定义为一种机制，保证对基础专家类别的乘性近似遗憾。在这种访问模型中，我们提出了一种高效的提升算法，保证对基础类别的凸包有接近最优的遗憾。我们考虑了完全和部分（即强盗）信息反馈模型。我们还为i.i.d.统计设置提供了一个类似的高效提升算法。我们的结果同时将在线提升和梯度提升的保证推广到了上下文学习模型、在线凸优化和强盗线性优化设置。",
        "领域": "在线学习、强化学习、优化算法",
        "问题": "在具有大量专家的在线凸优化中，如何高效地进行决策以避免枚举和搜索策略类的不可行性。",
        "动机": "解决在上下文和强化学习问题中，由于策略类规模庞大导致的枚举和搜索不可行的问题。",
        "方法": "推广在线提升的方法论，定义弱学习算法，并提出高效的提升算法以保证对基础类别的凸包有接近最优的遗憾。",
        "关键词": [
            "在线凸优化",
            "提升方法",
            "弱学习算法",
            "强盗反馈",
            "i.i.d.统计设置"
        ],
        "涉及的技术概念": {
            "在线凸优化": "一种决策框架，用于在连续的时间步骤中做出决策，以最小化累积遗憾。",
            "弱学习算法": "一种算法，保证对基础专家类别的乘性近似遗憾，用于构建更强大的学习模型。",
            "强盗反馈": "一种信息反馈模型，其中决策者只能观察到其选择行动的结果，而不是所有可能行动的结果。"
        },
        "success": true
    },
    {
        "order": 156,
        "title": "Boosting the Throughput and Accelerator Utilization of Specialized CNN Inference Beyond Increasing Batch Size",
        "html": "https://ICML.cc//virtual/2021/poster/8415",
        "abstract": "Datacenter vision systems widely use small, specialized convolutional neural networks (CNNs) trained on specific tasks for high-throughput inference. These settings employ accelerators with massive computational capacity, but which specialized CNNs underutilize due to having low arithmetic intensity. This results in suboptimal application-level throughput and poor returns on accelerator investment. Increasing batch size is the only known way to increase both application-level throughput and accelerator utilization for inference, but yields diminishing returns; specialized CNNs poorly utilize accelerators even with large batch size. We propose FoldedCNNs, a new approach to CNN design that increases inference throughput and utilization beyond large batch size. FoldedCNNs rethink the structure of inputs and layers of specialized CNNs to boost arithmetic intensity: in FoldedCNNs, f images with C channels each are concatenated into a single input with fC channels and jointly classified by a wider CNN. Increased arithmetic intensity in FoldedCNNs increases the throughput and GPU utilization of specialized CNN inference by up to 2.5x and 2.8x, with accuracy close to the original CNN in most cases.",
        "conference": "ICML",
        "中文标题": "超越增加批量大小：提升专用CNN推理的吞吐量和加速器利用率",
        "摘要翻译": "数据中心视觉系统广泛使用小型、专用的卷积神经网络（CNNs）在特定任务上进行高吞吐量推理。这些设置采用了具有巨大计算能力的加速器，但由于专用CNNs的算术强度低，导致加速器利用率不足。这导致了应用级吞吐量的次优和加速器投资的低回报。增加批量大小是唯一已知的增加应用级吞吐量和加速器利用率的方法，但收益递减；即使在大批量大小下，专用CNNs也难以充分利用加速器。我们提出了FoldedCNNs，一种新的CNN设计方法，可以在大批量大小之外提高推理吞吐量和利用率。FoldedCNNs重新思考了专用CNNs的输入和层结构，以提高算术强度：在FoldedCNNs中，f个图像，每个有C个通道，被连接成一个具有fC通道的单一输入，并由一个更宽的CNN共同分类。FoldedCNNs中增加的算术强度将专用CNN推理的吞吐量和GPU利用率提高了高达2.5倍和2.8倍，在大多数情况下准确率接近原始CNN。",
        "领域": "卷积神经网络优化、高吞吐量推理、GPU加速",
        "问题": "专用卷积神经网络在推理过程中算术强度低，导致加速器利用率不足和应用级吞吐量低",
        "动机": "提高专用CNN推理的吞吐量和加速器利用率，超越仅通过增加批量大小所能达到的效果",
        "方法": "提出FoldedCNNs，通过重新设计CNN的输入和层结构，将多个图像的通道连接起来，由一个更宽的CNN共同分类，以提高算术强度",
        "关键词": [
            "FoldedCNNs",
            "算术强度",
            "高吞吐量推理",
            "GPU利用率",
            "CNN优化"
        ],
        "涉及的技术概念": {
            "算术强度": "衡量计算操作与内存访问的比率，FoldedCNNs通过提高算术强度来增加吞吐量和加速器利用率",
            "FoldedCNNs": "一种新型CNN设计方法，通过合并多个图像的通道来提高算术强度和推理效率",
            "GPU利用率": "指GPU在执行计算任务时的使用效率，FoldedCNNs通过优化设计显著提高了这一指标"
        },
        "success": true
    },
    {
        "order": 157,
        "title": "Bootstrapping Fitted Q-Evaluation for Off-Policy Inference",
        "html": "https://ICML.cc//virtual/2021/poster/8507",
        "abstract": "Bootstrapping provides a flexible and effective approach for assessing the quality of batch reinforcement learning, yet its theoretical properties are poorly understood. In this paper, we study the use of bootstrapping in off-policy evaluation (OPE), and in particular, we focus on the fitted Q-evaluation (FQE) that is known to be minimax-optimal in the tabular and linear-model cases. We propose a bootstrapping FQE method for inferring the distribution of the policy evaluation error and show that this method is asymptotically efficient and distributionally consistent for off-policy statistical inference. To overcome the computation limit of bootstrapping, we further adapt a subsampling procedure that improves the runtime by an order of magnitude. We numerically evaluate the bootrapping method in classical RL environments for confidence interval estimation, estimating the variance of off-policy evaluator, and estimating the correlation between multiple off-policy evaluators.\n",
        "conference": "ICML",
        "中文标题": "自举拟合Q评估用于离策略推断",
        "摘要翻译": "自举提供了一种灵活而有效的方法来评估批量强化学习的质量，然而其理论性质却鲜为人知。在本文中，我们研究了自举在离策略评估（OPE）中的应用，特别是我们关注于在表格和线性模型情况下已知为极小极大最优的拟合Q评估（FQE）。我们提出了一种自举FQE方法，用于推断策略评估误差的分布，并表明这种方法对于离策略统计推断是渐近有效和分布一致的。为了克服自举的计算限制，我们进一步采用了一种子采样过程，将运行时间提高了一个数量级。我们在经典RL环境中对自举方法进行了数值评估，用于置信区间估计、估计离策略评估器的方差以及估计多个离策略评估器之间的相关性。",
        "领域": "强化学习、离策略评估、统计推断",
        "问题": "如何在离策略评估中有效且高效地使用自举方法来推断策略评估误差的分布",
        "动机": "探索自举在离策略评估中的应用，特别是在拟合Q评估（FQE）中的理论性质和实际效果，以提供更有效的统计推断方法",
        "方法": "提出了一种自举FQE方法，结合子采样过程以提高计算效率，用于推断策略评估误差的分布",
        "关键词": [
            "自举",
            "离策略评估",
            "拟合Q评估",
            "统计推断",
            "子采样"
        ],
        "涉及的技术概念": {
            "自举": "一种统计方法，用于通过重采样来估计统计量的分布，本文中用于评估策略的质量",
            "拟合Q评估（FQE）": "一种离策略评估方法，通过拟合Q函数来评估策略的性能，本文中结合自举方法以提高评估的准确性和效率",
            "子采样": "一种减少计算量的技术，通过从原始数据中抽取子集来进行计算，本文中用于提高自举方法的运行效率"
        },
        "success": true
    },
    {
        "order": 158,
        "title": "BORE: Bayesian Optimization by Density-Ratio Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/10201",
        "abstract": "Bayesian optimization (BO) is among the most effective and widely-used blackbox optimization methods. BO proposes solutions according to an explore-exploit trade-off criterion encoded in an acquisition function, many of which are computed from the posterior predictive of a probabilistic surrogate model. Prevalent among these is the expected improvement (EI). The need to ensure analytical tractability of the predictive often poses limitations that can hinder the efficiency and applicability of BO. In this paper, we cast the computation of EI as a binary classification problem, building on the link between class-probability estimation and density-ratio estimation, and the lesser-known link between density-ratios and EI. By circumventing the tractability constraints, this reformulation provides numerous advantages, not least in terms of expressiveness, versatility, and scalability.",
        "conference": "ICML",
        "中文标题": "BORE：基于密度比估计的贝叶斯优化",
        "摘要翻译": "贝叶斯优化（BO）是最有效且广泛使用的黑盒优化方法之一。BO根据在获取函数中编码的探索-利用权衡准则提出解决方案，其中许多解决方案是从概率替代模型的后验预测中计算得出的。其中最为普遍的是预期改进（EI）。确保预测的解析可处理性的需求常常带来限制，这些限制可能阻碍BO的效率和适用性。在本文中，我们将EI的计算视为一个二元分类问题，建立在类概率估计与密度比估计之间的联系，以及密度比与EI之间较少为人知的联系之上。通过规避可处理性约束，这种重新表述提供了众多优势，尤其是在表达性、多功能性和可扩展性方面。",
        "领域": "贝叶斯优化",
        "问题": "解决贝叶斯优化中预期改进（EI）计算的解析可处理性限制问题",
        "动机": "提高贝叶斯优化的效率、表达性、多功能性和可扩展性",
        "方法": "将预期改进（EI）的计算视为二元分类问题，利用类概率估计与密度比估计之间的联系",
        "关键词": [
            "贝叶斯优化",
            "密度比估计",
            "预期改进",
            "二元分类",
            "探索-利用权衡"
        ],
        "涉及的技术概念": {
            "贝叶斯优化": "一种用于黑盒优化的方法，通过概率模型指导搜索过程",
            "密度比估计": "用于估计两个概率密度函数比率的技术，本文中用于重新表述EI的计算",
            "预期改进": "贝叶斯优化中常用的获取函数，用于平衡探索和利用"
        },
        "success": true
    },
    {
        "order": 159,
        "title": "Breaking the Deadly Triad with a Target Network",
        "html": "https://ICML.cc//virtual/2021/poster/8651",
        "abstract": "The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, \r\nfunction approximation,\r\nand bootstrapping simultaneously.\r\nIn this paper,\r\nwe investigate the target network as a tool for breaking the deadly triad,\r\nproviding theoretical support for the conventional wisdom that a target network stabilizes training.\r\nWe first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. \r\nWe then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points.\r\nThose algorithms \r\nare off-policy with linear function approximation and bootstrapping,\r\nspanning both policy evaluation and control, as well as\r\nboth discounted and average-reward settings.\r\nIn particular,\r\nwe provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level optimization.",
        "conference": "ICML",
        "中文标题": "利用目标网络打破致命三角",
        "摘要翻译": "致命三角指的是当强化学习算法同时采用离策略学习、函数逼近和自举时的不稳定性。在本文中，我们研究了目标网络作为打破致命三角的工具，为传统认为目标网络能稳定训练的观点提供了理论支持。我们首先提出并分析了一种新颖的目标网络更新规则，该规则通过两次投影增强了常用的Polyak平均风格更新。然后，我们在几种不同的算法中应用了目标网络和岭正则化，并展示了它们向正则化TD固定点的收敛。这些算法具有线性函数逼近和自举的离策略特性，涵盖了策略评估和控制，以及折扣和平均奖励设置。特别是，我们首次提供了在非限制性和变化行为策略下无需双层优化的收敛线性Q学习算法。",
        "领域": "强化学习、算法稳定性、Q学习",
        "问题": "解决强化学习算法在同时采用离策略学习、函数逼近和自举时的不稳定性问题。",
        "动机": "研究目标网络如何作为工具来打破致命三角，为传统认为目标网络能稳定训练的观点提供理论支持。",
        "方法": "提出并分析了一种新颖的目标网络更新规则，结合Polyak平均风格更新和两次投影；在多种算法中应用目标网络和岭正则化，展示其向正则化TD固定点的收敛。",
        "关键词": [
            "目标网络",
            "致命三角",
            "强化学习",
            "算法稳定性",
            "Q学习"
        ],
        "涉及的技术概念": {
            "目标网络": "用于稳定强化学习算法训练的网络，通过延迟更新策略网络参数来减少学习过程中的不稳定性。",
            "Polyak平均": "一种平滑参数更新的技术，通过缓慢更新目标网络的参数来稳定训练过程。",
            "岭正则化": "一种用于防止模型过拟合的正则化技术，通过在损失函数中添加一个惩罚项来限制模型复杂度。"
        },
        "success": true
    },
    {
        "order": 160,
        "title": "Breaking the Limits of Message Passing Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8577",
        "abstract": "Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties.\nSo far, the best 3-WL equivalent graph neural networks have a computational complexity in $\\mathcal{O}(n^3)$ with memory usage in $\\mathcal{O}(n^2)$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "突破消息传递图神经网络的理论限制",
        "摘要翻译": "由于消息传递（图）神经网络（MPNNs）在应用于稀疏图时具有相对于节点数量的线性复杂度，尽管其理论表达能力仅限于一阶Weisfeiler-Lehman测试（1-WL），它们仍被广泛实现并引起大量兴趣。在本文中，我们表明，如果图卷积支持在谱域中通过特征值的非线性自定义函数设计，并用任意大的感受野进行掩蔽，那么MPNN在理论上比1-WL测试更强大，在实验上与现有的3-WL模型一样强大，同时保持空间局部性。此外，通过设计自定义滤波函数，输出可以具有各种频率分量，这使得卷积过程能够学习给定输入图信号与其相关属性之间的不同关系。到目前为止，最好的3-WL等效图神经网络的计算复杂度为O(n^3)，内存使用为O(n^2)，考虑非局部更新机制，并且不提供输出配置的谱丰富性。所提出的方法克服了所有上述问题，并在许多下游任务中达到了最先进的结果。",
        "领域": "图神经网络、图信号处理、图表示学习",
        "问题": "如何突破消息传递图神经网络在理论表达能力上的限制，同时保持计算效率和空间局部性。",
        "动机": "现有的消息传递图神经网络虽然在计算上高效，但其理论表达能力有限，无法捕捉复杂的图结构信息。",
        "方法": "通过在谱域中设计非线性自定义函数作为图卷积支持，并用大感受野进行掩蔽，以增强模型的理论表达能力和实验性能。",
        "关键词": [
            "图神经网络",
            "消息传递机制",
            "谱域分析",
            "Weisfeiler-Lehman测试",
            "非线性滤波"
        ],
        "涉及的技术概念": {
            "消息传递机制": "在图神经网络中用于节点间信息交换的基本框架，本文通过改进其理论表达能力来突破现有限制。",
            "谱域分析": "将图信号转换到谱域进行处理，通过设计非线性自定义函数来增强模型的表达能力。",
            "Weisfeiler-Lehman测试": "用于评估图神经网络理论表达能力的一种测试方法，本文提出的方法在理论上超越了1-WL测试的限制。"
        }
    },
    {
        "order": 161,
        "title": "Break-It-Fix-It: Unsupervised Learning for Program Repair",
        "html": "https://ICML.cc//virtual/2021/poster/10473",
        "abstract": "We consider repair tasks: given a critic (e.g., compiler) that assesses the quality of an input, the goal is to train a fixer that converts a bad example (e.g., code with syntax errors) into a good one (e.g., code with no errors). \nExisting works create training data consisting of (bad, good) pairs by corrupting good examples using heuristics (e.g., dropping tokens).  However, fixers trained on this synthetically-generated data do not extrapolate well to the real distribution of bad inputs.\nTo bridge this gap, we propose a new training approach, Break-It-Fix-It (BIFI), which has two key ideas:\n(i) we use the critic to check a fixer's output on real bad inputs and add good (fixed) outputs to the training data,\nand (ii) we train a breaker to generate realistic bad code from good code. \nBased on these ideas, we iteratively update the breaker and the fixer while using them in conjunction to generate more paired data.\nWe evaluate BIFI on two code repair datasets: GitHub-Python, a new dataset we introduce where the goal is to repair Python code with AST parse errors; and DeepFix, where the goal is to repair C code with compiler errors. BIFI outperforms existing methods, obtaining 90.5% repair accuracy on GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%).\nNotably, BIFI does not require any labeled data; we hope it will be a strong starting point for unsupervised learning of various repair tasks.",
        "conference": "ICML",
        "中文标题": "破坏-修复-它：程序修复的无监督学习",
        "摘要翻译": "我们考虑修复任务：给定一个评估输入质量的评判者（例如编译器），目标是训练一个修复器，将坏的例子（例如有语法错误的代码）转换为好的例子（例如没有错误的代码）。现有的工作通过使用启发式方法（例如丢弃标记）破坏好的例子来创建由（坏，好）对组成的训练数据。然而，在这种合成生成的数据上训练的修复器不能很好地推广到坏输入的真实分布。为了弥合这一差距，我们提出了一种新的训练方法，破坏-修复-它（BIFI），它有两个关键思想：（i）我们使用评判者检查修复器在真实坏输入上的输出，并将好的（修复后的）输出添加到训练数据中，（ii）我们训练一个破坏器从好代码生成真实的坏代码。基于这些想法，我们迭代更新破坏器和修复器，同时结合使用它们生成更多配对数据。我们在两个代码修复数据集上评估BIFI：GitHub-Python，我们引入的一个新数据集，目标是修复带有AST解析错误的Python代码；以及DeepFix，目标是修复带有编译器错误的C代码。BIFI优于现有方法，在GitHub-Python上获得90.5%的修复准确率（+28.5%），在DeepFix上获得71.7%（+5.6%）。值得注意的是，BIFI不需要任何标记数据；我们希望它将成为各种修复任务无监督学习的一个强大起点。",
        "领域": "程序修复、无监督学习、代码生成",
        "问题": "如何在没有标记数据的情况下，有效地训练一个能够修复坏代码的模型",
        "动机": "现有的基于合成数据的修复方法不能很好地推广到真实坏代码的分布，需要一种更有效的方法来训练修复器",
        "方法": "提出破坏-修复-它（BIFI）方法，通过迭代训练破坏器和修复器，结合使用它们生成更多真实的（坏，好）代码对，以增强修复器的泛化能力",
        "关键词": [
            "程序修复",
            "无监督学习",
            "代码生成",
            "破坏器",
            "修复器"
        ],
        "涉及的技术概念": {
            "破坏器": "用于从好代码生成真实的坏代码，以提供更接近真实分布的坏代码样本",
            "修复器": "用于将坏代码转换为好代码，通过评判者的反馈不断优化",
            "无监督学习": "在不需要标记数据的情况下，通过破坏器和修复器的交互学习，实现代码修复"
        },
        "success": true
    },
    {
        "order": 162,
        "title": "Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation",
        "html": "https://ICML.cc//virtual/2021/poster/10407",
        "abstract": "Multi-task learning (MTL) aims to improve the generalization of several related tasks by learning them jointly. As a comparison, in addition to the joint training scheme, modern meta-learning allows unseen tasks with limited labels during the test phase, in the hope of fast adaptation over them. Despite the subtle difference between MTL and meta-learning in the problem formulation, both learning paradigms share the same insight that the shared structure between existing training tasks could lead to better generalization and adaptation. In this paper, we take one important step further to understand the close connection between these two learning paradigms, through both theoretical analysis and empirical investigation. Theoretically, we first demonstrate that MTL shares the same optimization formulation with a class of gradient-based meta-learning (GBML) algorithms. We then prove that for over-parameterized neural networks with sufficient depth, the learned predictive functions of MTL and GBML are close. In particular, this result implies that the predictions given by these two models are similar over the same unseen task. Empirically, we corroborate our theoretical findings by showing that, with proper implementation, MTL is competitive against state-of-the-art GBML algorithms on a set of few-shot image classification benchmarks. Since existing GBML algorithms often involve costly second-order bi-level optimization, our first-order MTL method is an order of magnitude faster on large-scale datasets such as mini-ImageNet. We believe this work could help bridge the gap between these two learning paradigms, and provide a computationally efficient alternative to GBML that also supports fast task adaptation.",
        "conference": "ICML",
        "中文标题": "桥接多任务学习与元学习：迈向高效训练与有效适应",
        "摘要翻译": "多任务学习（MTL）旨在通过联合学习多个相关任务来提高它们的泛化能力。相比之下，除了联合训练方案外，现代元学习允许在测试阶段处理标签有限的未见任务，以期在这些任务上快速适应。尽管MTL和元学习在问题表述上存在细微差别，但这两种学习范式共享相同的见解，即现有训练任务之间的共享结构可以带来更好的泛化和适应。在本文中，我们通过理论分析和实证研究，进一步理解了这两种学习范式之间的紧密联系。理论上，我们首先证明了MTL与一类基于梯度的元学习（GBML）算法共享相同的优化公式。然后我们证明，对于具有足够深度的过参数化神经网络，MTL和GBML学习到的预测函数是接近的。特别是，这一结果意味着这两种模型对同一未见任务的预测是相似的。实证上，我们通过展示在适当实现的情况下，MTL在一组少样本图像分类基准上与最先进的GBML算法竞争，来证实我们的理论发现。由于现有的GBML算法通常涉及昂贵的二阶双层优化，我们的一阶MTL方法在如mini-ImageNet这样的大规模数据集上快一个数量级。我们相信这项工作有助于桥接这两种学习范式之间的差距，并为GBML提供一个计算效率高且支持快速任务适应的替代方案。",
        "领域": "多任务学习、元学习、少样本学习",
        "问题": "理解多任务学习与元学习之间的紧密联系，并提供一种计算效率高的方法支持快速任务适应",
        "动机": "探索多任务学习和元学习之间的理论联系，并通过实证研究验证MTL在少样本学习任务上的有效性，以提供一种更高效的训练方法",
        "方法": "通过理论分析证明MTL与GBML在优化公式上的相似性，并通过实证研究比较MTL与GBML在少样本图像分类任务上的表现",
        "关键词": [
            "多任务学习",
            "元学习",
            "少样本学习",
            "梯度下降",
            "图像分类"
        ],
        "涉及的技术概念": {
            "多任务学习（MTL）": "通过联合学习多个相关任务来提高泛化能力的技术",
            "基于梯度的元学习（GBML）": "一类利用梯度信息进行快速适应的元学习算法",
            "过参数化神经网络": "指参数数量远大于训练样本数量的神经网络，能够更好地拟合训练数据"
        },
        "success": true
    },
    {
        "order": 163,
        "title": "Budgeted Heterogeneous Treatment Effect Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/10267",
        "abstract": "Heterogeneous treatment effect (HTE) estimation is receiving increasing interest due to its important applications in fields such as healthcare, economics, and education. Current HTE estimation methods generally assume the existence of abundant observational data, though the acquisition of such data can be costly. In some real scenarios, it is easy to access the pre-treatment covariates and treatment assignments, but expensive to obtain the factual outcomes. To make HTE estimation more practical, in this paper, we examine the problem of estimating HTEs with a budget constraint on observational data, aiming to obtain accurate HTE estimates with limited costs. By deriving an informative generalization bound and connecting to active learning, we propose an effective and efficient method which is validated both theoretically and empirically.",
        "conference": "ICML",
        "中文标题": "预算约束下的异质处理效应估计",
        "摘要翻译": "异质处理效应（HTE）估计因其在医疗保健、经济学和教育等领域的重要应用而受到越来越多的关注。当前的HTE估计方法通常假设存在大量的观察数据，尽管获取这些数据的成本可能很高。在一些实际场景中，获取治疗前的协变量和治疗分配很容易，但获取实际结果却很昂贵。为了使HTE估计更加实用，本文研究了在观察数据预算约束下的HTE估计问题，旨在以有限的成本获得准确的HTE估计。通过推导一个信息丰富的泛化边界并与主动学习相联系，我们提出了一种有效且高效的方法，该方法在理论和实证上都得到了验证。",
        "领域": "因果推断、医疗数据分析、经济学模型",
        "问题": "在观察数据获取成本高昂的情况下，如何准确估计异质处理效应",
        "动机": "解决在实际应用中因数据获取成本高而难以准确估计异质处理效应的问题",
        "方法": "通过推导泛化边界并与主动学习相结合，提出了一种在预算约束下有效且高效的HTE估计方法",
        "关键词": [
            "异质处理效应",
            "预算约束",
            "主动学习",
            "泛化边界",
            "观察数据"
        ],
        "涉及的技术概念": {
            "异质处理效应": "指不同个体或群体对同一处理或干预的不同反应，是因果推断中的核心概念",
            "泛化边界": "用于衡量模型在未见数据上的表现，本文中用于理论分析HTE估计的准确性",
            "主动学习": "一种机器学习策略，通过选择最有信息量的样本进行学习，以减少数据获取成本"
        },
        "success": true
    },
    {
        "order": 164,
        "title": "Byzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data",
        "html": "https://ICML.cc//virtual/2021/poster/8727",
        "abstract": "We study stochastic gradient descent (SGD) with local iterations in the presence of Byzantine clients, motivated by the federated learning. The clients, instead of communicating with the server in every iteration, maintain their local models, which they update by taking several SGD iterations based on their own datasets and then communicate the net update with the server, thereby achieving communication-efficiency. Furthermore, only a subset of clients communicates with the server at synchronization times. The Byzantine clients may collude and send arbitrary vectors to the server to disrupt the learning process. To combat the adversary, we employ an efficient high-dimensional robust mean estimation algorithm at the server to filter-out corrupt vectors; and to analyze the outlier-filtering procedure, we develop a novel matrix concentration result that may be of independent interest. We provide convergence analyses for both strongly-convex and non-convex smooth objectives in the heterogeneous data setting. We believe that ours is the first Byzantine-resilient local SGD algorithm and analysis with non-trivial guarantees. We corroborate our theoretical results with preliminary experiments for neural network training.",
        "conference": "ICML",
        "中文标题": "异构数据上具有本地迭代的拜占庭弹性高维随机梯度下降",
        "摘要翻译": "我们研究了在存在拜占庭客户端的情况下，具有本地迭代的随机梯度下降（SGD），这是受到联邦学习的启发。客户端不是每次迭代都与服务器通信，而是维护其本地模型，这些模型通过基于自己的数据集采取多次SGD迭代来更新，然后与服务器通信净更新，从而实现通信效率。此外，只有一部分客户端在同步时间与服务器通信。拜占庭客户端可能合谋并发送任意向量到服务器以干扰学习过程。为了对抗对手，我们在服务器端采用了一种高效的高维鲁棒均值估计算法来过滤掉损坏的向量；为了分析异常值过滤过程，我们开发了一种新颖的矩阵集中结果，这可能具有独立的意义。我们为异构数据设置中的强凸和非凸平滑目标提供了收敛分析。我们相信，我们的工作是第一个具有非平凡保证的拜占庭弹性本地SGD算法和分析。我们通过神经网络训练的初步实验验证了我们的理论结果。",
        "领域": "联邦学习、分布式机器学习、鲁棒优化",
        "问题": "在存在拜占庭客户端的情况下，如何在异构数据上高效且安全地进行分布式随机梯度下降",
        "动机": "研究如何在联邦学习环境中，面对拜占庭客户端的干扰，实现高效且安全的模型训练",
        "方法": "采用本地迭代的SGD方法减少通信开销，使用高维鲁棒均值估计算法过滤拜占庭客户端发送的损坏向量，开发新的矩阵集中结果分析过滤过程",
        "关键词": [
            "拜占庭弹性",
            "本地SGD",
            "高维鲁棒均值估计",
            "联邦学习",
            "异构数据"
        ],
        "涉及的技术概念": {
            "拜占庭弹性": "指算法能够抵抗拜占庭客户端（即可能发送错误信息的恶意客户端）的干扰，保证学习过程的正确性和安全性",
            "本地SGD": "客户端在本地进行多次SGD迭代，减少与服务器的通信次数，提高通信效率",
            "高维鲁棒均值估计": "一种在高维数据中有效识别并过滤异常值（如拜占庭客户端发送的损坏向量）的技术，确保模型训练的鲁棒性"
        },
        "success": true
    },
    {
        "order": 165,
        "title": "Calibrate Before Use: Improving Few-shot Performance of Language Models",
        "html": "https://ICML.cc//virtual/2021/poster/10185",
        "abstract": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as 'N/A'. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.",
        "conference": "ICML",
        "中文标题": "使用前校准：提升语言模型的少样本性能",
        "摘要翻译": "GPT-3在提供包含少量训练示例的自然语言提示时，能够执行多种任务。我们发现这种少样本学习可能不稳定：提示格式的选择、训练示例、甚至示例的顺序都可能导致准确率从接近随机到接近最先进水平的波动。我们证明这种不稳定性源于语言模型对预测某些答案的偏好，例如那些位于提示末尾或在预训练数据中常见的答案。为了缓解这一问题，我们首先通过询问模型在给定训练提示和无内容测试输入（如'N/A'）时的预测来估计模型对每个答案的偏好。然后，我们拟合校准参数，使得对于这种输入的预测在各个答案之间均匀分布。在多样化的任务集上，这种上下文校准程序显著提高了GPT-3和GPT-2的准确率（最高达30.0%绝对提升），同时使学习过程更加稳定。",
        "领域": "自然语言处理与视觉结合",
        "问题": "解决语言模型在少样本学习中的不稳定性和偏见问题",
        "动机": "提高语言模型在少样本学习任务中的准确率和稳定性",
        "方法": "通过估计模型对答案的偏见并拟合校准参数来优化预测",
        "关键词": [
            "少样本学习",
            "语言模型校准",
            "GPT-3优化"
        ],
        "涉及的技术概念": {
            "少样本学习": "在仅有少量训练示例的情况下进行学习的技术",
            "语言模型校准": "调整模型预测以减少偏见和提高准确率的过程",
            "上下文校准": "通过调整模型对特定输入的响应来优化整体性能的技术"
        },
        "success": true
    },
    {
        "order": 166,
        "title": "Can Subnetwork Structure Be the Key to Out-of-Distribution Generalization?",
        "html": "https://ICML.cc//virtual/2021/poster/9481",
        "abstract": "Can models with particular structure avoid being biased towards spurious correlation in out-of-distribution (OOD) generalization? Peters et al. (2016) provides a positive answer for linear cases. In this paper, we use a functional modular probing method to analyze deep model structures under OOD setting. We demonstrate that even in biased models (which focus on spurious correlation) there still exist unbiased functional subnetworks. Furthermore, we articulate and confirm the functional lottery ticket hypothesis: the full network contains a subnetwork with proper structure that can achieve better OOD performance. We then propose Modular Risk Minimization to solve the subnetwork selection problem. Our algorithm learns the functional structure from a given dataset, and can be combined with any other OOD regularization methods. Experiments on various OOD generalization tasks corroborate the effectiveness of our method.",
        "conference": "ICML",
        "中文标题": "子网络结构能否成为分布外泛化的关键？",
        "摘要翻译": "具有特定结构的模型能否避免在分布外（OOD）泛化中对虚假相关性的偏向？Peters等人（2016）为线性情况提供了肯定的答案。在本文中，我们使用功能性模块探测方法来分析OOD设置下的深度模型结构。我们证明，即使在偏向模型（专注于虚假相关性）中，仍然存在无偏向的功能子网络。此外，我们阐述并验证了功能性彩票假设：完整网络包含一个具有适当结构的子网络，可以实现更好的OOD性能。然后，我们提出了模块化风险最小化来解决子网络选择问题。我们的算法从给定数据集中学习功能结构，并且可以与任何其他OOD正则化方法结合使用。在各种OOD泛化任务上的实验证实了我们方法的有效性。",
        "领域": "深度学习理论、模型泛化、神经网络结构",
        "问题": "研究特定结构的模型是否能够避免在分布外泛化中对虚假相关性的偏向，以及如何识别和利用网络中的无偏向子网络以提高泛化性能。",
        "动机": "探索深度神经网络在分布外泛化中的结构特性，寻找能够避免虚假相关性偏向的子网络，以提高模型的泛化能力。",
        "方法": "使用功能性模块探测方法分析深度模型结构，提出功能性彩票假设，并开发模块化风险最小化算法来选择有效的子网络。",
        "关键词": [
            "分布外泛化",
            "子网络结构",
            "功能性彩票假设",
            "模块化风险最小化",
            "虚假相关性"
        ],
        "涉及的技术概念": {
            "功能性模块探测方法": "用于分析深度模型在OOD设置下的结构特性，识别无偏向的功能子网络。",
            "功能性彩票假设": "假设完整网络中存在一个子网络，其结构适当，能够在OOD泛化中表现更好。",
            "模块化风险最小化": "一种算法，用于从数据中学习功能结构并选择有效的子网络，可与任何OOD正则化方法结合使用。"
        },
        "success": true
    },
    {
        "order": 167,
        "title": "CARTL: Cooperative Adversarially-Robust Transfer Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9669",
        "abstract": "Transfer learning eases the burden of training a well-performed model from scratch, especially when training data is scarce and computation power is limited. In deep learning, a typical strategy for transfer learning is to freeze the early layers of a pre-trained model and fine-tune the rest of its layers on the target domain. Previous work focuses on the accuracy of the transferred model but neglects the transfer of adversarial robustness. In this work, we first show that transfer learning improves the accuracy on the target domain but degrades the inherited robustness of the target model. To address such a problem, we propose a novel cooperative adversarially-robust transfer learning (CARTL) by pre-training the model via feature distance minimization and fine-tuning the pre-trained model with non-expansive fine-tuning for target domain tasks. Empirical results show that CARTL improves the inherited robustness by about 28% at most compared with the baseline with the same degree of accuracy. Furthermore, we study the relationship between the batch normalization (BN) layers and the robustness in the context of transfer learning, and we reveal that freezing BN layers can further boost the robustness transfer.",
        "conference": "ICML",
        "中文标题": "CARTL：协作式对抗鲁棒迁移学习",
        "摘要翻译": "迁移学习减轻了从零开始训练一个表现良好模型的负担，尤其是在训练数据稀缺且计算能力有限的情况下。在深度学习中，迁移学习的典型策略是冻结预训练模型的早期层，并在目标域上微调其余层。以往的工作集中于迁移模型的准确性，但忽视了对抗鲁棒性的迁移。在这项工作中，我们首先展示了迁移学习提高了目标域的准确性，但降低了目标模型的继承鲁棒性。为了解决这一问题，我们提出了一种新颖的协作式对抗鲁棒迁移学习（CARTL），通过特征距离最小化预训练模型，并使用非扩张性微调对预训练模型进行目标域任务的微调。实证结果表明，与基线相比，CARTL在相同准确度的情况下，最多提高了约28%的继承鲁棒性。此外，我们研究了批量归一化（BN）层与迁移学习背景下鲁棒性之间的关系，并揭示了冻结BN层可以进一步促进鲁棒性的迁移。",
        "领域": "对抗性机器学习, 迁移学习, 深度学习优化",
        "问题": "迁移学习在提高目标域模型准确性的同时，降低了模型的对抗鲁棒性。",
        "动机": "解决迁移学习中对抗鲁棒性下降的问题，提升模型在目标域中的安全性和可靠性。",
        "方法": "通过特征距离最小化进行模型预训练，并采用非扩张性微调方法在目标域上微调模型，同时研究批量归一化层对鲁棒性迁移的影响。",
        "关键词": [
            "对抗鲁棒性",
            "迁移学习",
            "特征距离最小化",
            "非扩张性微调",
            "批量归一化"
        ],
        "涉及的技术概念": {
            "对抗鲁棒性": "模型抵抗对抗攻击的能力，确保模型在面对精心设计的输入扰动时仍能保持性能。",
            "特征距离最小化": "在预训练阶段使用的一种技术，旨在减少源域和目标域特征分布之间的差异，以促进知识的迁移。",
            "非扩张性微调": "一种微调策略，限制模型参数的变化范围，以防止在微调过程中过度适应目标域而丧失预训练阶段学到的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 168,
        "title": "Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/8485",
        "abstract": "The early phase of training a deep neural network has a dramatic effect on the local curvature of the loss function. For instance, using a small learning rate does not guarantee stable optimization because the optimization trajectory has a tendency to steer towards regions of the loss surface with increasing local curvature. We ask whether this tendency is connected to the widely observed phenomenon that the choice of the learning rate strongly influences generalization. We first show that stochastic gradient descent (SGD) implicitly penalizes the trace of the Fisher Information Matrix (FIM), a measure of the local curvature, from the start of training. We argue it is an implicit regularizer in SGD by showing that explicitly penalizing the trace of the FIM can significantly improve generalization. We highlight that poor final generalization coincides with the trace of the FIM attaining a large value early in training, to which we refer as catastrophic Fisher explosion. Finally, to gain insight into the regularization effect of penalizing the trace of the FIM, we show that it limits memorization by reducing the learning speed of examples with noisy labels more than that of the examples with clean labels. ",
        "conference": "ICML",
        "中文标题": "灾难性Fisher爆炸：早期阶段Fisher矩阵对泛化的影响",
        "摘要翻译": "深度神经网络训练的早期阶段对损失函数的局部曲率有着显著影响。例如，使用较小的学习率并不能保证优化的稳定性，因为优化轨迹倾向于转向损失表面局部曲率增加的区域。我们探讨这种倾向是否与广泛观察到的现象——学习率的选择强烈影响泛化性能——有关。我们首先展示了随机梯度下降（SGD）从训练开始就隐式地惩罚Fisher信息矩阵（FIM）的迹，这是局部曲率的一种度量。我们通过证明显式惩罚FIM的迹可以显著提高泛化性能，认为这是SGD中的一种隐式正则化器。我们强调，最终泛化性能差与FIM的迹在训练早期达到较大值（我们称之为灾难性Fisher爆炸）相吻合。最后，为了理解惩罚FIM迹的正则化效应，我们展示了它通过比干净标签示例更多地减少噪声标签示例的学习速度来限制记忆。",
        "领域": "深度学习优化、神经网络训练、泛化性能研究",
        "问题": "探讨学习率选择如何通过影响训练早期阶段的Fisher信息矩阵（FIM）迹来影响深度神经网络的泛化性能。",
        "动机": "研究旨在揭示学习率选择影响泛化性能的机制，特别是通过分析训练早期阶段FIM迹的变化。",
        "方法": "通过分析SGD隐式惩罚FIM迹的行为，并显式惩罚FIM迹以验证其作为正则化器的效果，研究FIM迹与泛化性能的关系。",
        "关键词": [
            "Fisher信息矩阵",
            "泛化性能",
            "随机梯度下降",
            "正则化",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "Fisher信息矩阵（FIM）": "用于度量损失函数的局部曲率，研究显示其迹的大小与泛化性能密切相关。",
            "随机梯度下降（SGD）": "在训练过程中隐式地惩罚FIM的迹，影响模型的优化轨迹和泛化性能。",
            "正则化": "通过显式惩罚FIM的迹，限制模型对噪声标签的记忆，从而提高泛化性能。"
        },
        "success": true
    },
    {
        "order": 169,
        "title": "CATE: Computation-aware Neural Architecture Encoding with Transformers",
        "html": "https://ICML.cc//virtual/2021/poster/9051",
        "abstract": "Recent works (White et al., 2020a; Yan et al., 2020) demonstrate the importance of architecture encodings in Neural Architecture Search (NAS). These encodings encode either structure or computation information of the neural architectures. Compared to structure-aware encodings, computation-aware encodings map architectures with similar accuracies to the same region, which improves the downstream architecture search performance (Zhang et al., 2019; White et al., 2020a). In this work, we introduce a Computation-Aware Transformer-based Encoding method called CATE. Different from existing computation-aware encodings based on fixed transformation (e.g. path encoding), CATE employs a pairwise pre-training scheme to learn computation-aware encodings using Transformers with cross-attention. Such learned encodings contain dense and contextualized computation information of neural architectures. We compare CATE with eleven encodings under three major encoding-dependent NAS subroutines in both small and large search spaces. Our experiments show that CATE is beneficial to the downstream search, especially in the large search space. Moreover, the outside search space experiment demonstrates its superior generalization ability beyond the search space on which it was trained. Our code is available at: https://github.com/MSU-MLSys-Lab/CATE.",
        "conference": "ICML",
        "中文标题": "CATE：基于Transformer的计算感知神经架构编码方法",
        "摘要翻译": "近期研究（White等人，2020a；Yan等人，2020）表明，在神经架构搜索（NAS）中架构编码的重要性。这些编码要么编码神经架构的结构信息，要么编码计算信息。与结构感知编码相比，计算感知编码将具有相似准确度的架构映射到同一区域，从而提高了下游架构搜索的性能（Zhang等人，2019；White等人，2020a）。在本工作中，我们介绍了一种名为CATE的计算感知Transformer编码方法。不同于基于固定变换（如路径编码）的现有计算感知编码，CATE采用了一种成对预训练方案，利用带有交叉注意力的Transformer来学习计算感知编码。这种学习到的编码包含了神经架构的密集和上下文化的计算信息。我们在小型和大型搜索空间中，将CATE与十一种编码在三种主要的依赖于编码的NAS子程序下进行了比较。我们的实验表明，CATE对下游搜索有益，尤其是在大型搜索空间中。此外，外部搜索空间实验展示了其在训练搜索空间之外的卓越泛化能力。我们的代码可在https://github.com/MSU-MLSys-Lab/CATE获取。",
        "领域": "神经架构搜索、深度学习优化、自动化机器学习",
        "问题": "如何有效地编码神经架构的计算信息以提升神经架构搜索的性能",
        "动机": "现有的计算感知编码方法基于固定变换，限制了编码的表达能力和灵活性，影响了神经架构搜索的效果",
        "方法": "提出了一种基于Transformer的计算感知编码方法CATE，通过成对预训练方案学习包含密集和上下文化计算信息的编码",
        "关键词": [
            "神经架构搜索",
            "计算感知编码",
            "Transformer",
            "预训练方案",
            "交叉注意力"
        ],
        "涉及的技术概念": {
            "计算感知编码": "编码神经架构的计算信息，而非仅结构信息，以提升神经架构搜索的性能",
            "Transformer": "用于学习计算感知编码的模型，能够捕捉神经架构的密集和上下文化信息",
            "交叉注意力": "在Transformer中使用，帮助模型更好地理解和编码神经架构间的计算关系"
        },
        "success": true
    },
    {
        "order": 170,
        "title": "Catformer: Designing Stable Transformers via Sensitivity Analysis",
        "html": "https://ICML.cc//virtual/2021/poster/9035",
        "abstract": "Transformer architectures are widely used, but training them is non-trivial, requiring custom learning rate schedules, scaling terms, residual connections, careful placement of submodules such as normalization, and so on. In this paper, we improve upon recent analysis of Transformers and formalize a notion of sensitivity to capture the difficulty of training. Sensitivity characterizes how the variance of activation and gradient norms change in expectation when parameters are randomly perturbed. We analyze the sensitivity of previous Transformer architectures and design a new architecture, the Catformer, which replaces residual connections or RNN-based gating mechanisms with concatenation. We prove that Catformers are less sensitive than other Transformer variants and demonstrate that this leads to more stable training. On DMLab30, a suite of high-dimension reinforcement tasks, Catformer outperforms other transformers, including Gated Transformer-XL---the state-of-the-art architecture designed to address stability---by 13%.",
        "conference": "ICML",
        "中文标题": "Catformer：通过敏感性分析设计稳定的Transformer",
        "摘要翻译": "Transformer架构被广泛使用，但训练它们并非易事，需要自定义学习率计划、缩放项、残差连接、子模块（如归一化）的精心放置等。在本文中，我们改进了最近对Transformer的分析，并形式化了一个敏感性概念来捕捉训练的难度。敏感性描述了当参数被随机扰动时，激活和梯度范数的方差在期望中如何变化。我们分析了先前Transformer架构的敏感性，并设计了一个新架构——Catformer，它用连接替换了残差连接或基于RNN的门控机制。我们证明了Catformer比其他Transformer变体更不敏感，并展示了这导致更稳定的训练。在DMLab30上，一套高维度强化任务，Catformer的表现优于其他Transformer，包括设计用于解决稳定性问题的最先进架构Gated Transformer-XL，性能提升了13%。",
        "领域": "自然语言处理与视觉结合, 强化学习, 深度学习模型优化",
        "问题": "如何设计一个训练更稳定的Transformer架构",
        "动机": "现有的Transformer架构在训练过程中存在稳定性问题，需要复杂的调整和优化策略。",
        "方法": "通过敏感性分析来理解和改进Transformer的训练稳定性，设计了一个新的架构Catformer，该架构通过使用连接而非残差连接或基于RNN的门控机制来减少敏感性。",
        "关键词": [
            "Transformer",
            "敏感性分析",
            "训练稳定性",
            "Catformer",
            "强化学习"
        ],
        "涉及的技术概念": {
            "敏感性分析": "用于量化模型参数随机扰动时激活和梯度范数方差的变化，以评估训练难度。",
            "Catformer": "一种新的Transformer架构，通过替换残差连接或基于RNN的门控机制为连接，以减少训练过程中的敏感性。",
            "训练稳定性": "指模型在训练过程中性能的波动程度，高稳定性意味着训练过程更加平滑，易于优化。"
        },
        "success": true
    },
    {
        "order": 171,
        "title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10025",
        "abstract": "Humans show an innate ability to learn the regularities of the world through interaction. By performing experiments in our environment, we are able to discern the causal factors of variation and infer how they affect the dynamics of our world. Analogously, here we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce a novel intrinsic reward, called causal curiosity, and show that it allows our agents to learn optimal sequences of actions, and to discover causal factors in the dynamics. The learned behavior allows the agent to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., to differentiate between heavy and light blocks, our agents learn to lift them), and are learnt in a self-supervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "因果好奇心：强化学习代理发现用于因果表示学习的自监督实验",
        "摘要翻译": "人类展现出通过互动学习世界规律的天生能力。通过在环境中进行实验，我们能够辨别变化的因果因素，并推断它们如何影响我们世界的动态。类似地，这里我们尝试赋予强化学习代理进行实验的能力，这些实验有助于对展开的轨迹进行分类，并随后以分层的方式推断环境的因果因素。我们引入了一种称为因果好奇心的新型内在奖励，并表明它允许我们的代理学习最优动作序列，并发现动态中的因果因素。学习到的行为使代理能够为每个环境中的真实因果因素推断出一个二进制量化表示。此外，我们发现这些实验行为在语义上是有效的（例如，为了区分重块和轻块，我们的代理学会了举起它们），并且以自监督的方式学习，数据量比传统的监督规划者少约2.5倍。我们展示了这些行为可以被重新利用和微调（例如，从举起到推动或其他下游任务）。最后，我们展示了因果因素表示的知识有助于更复杂任务的零样本学习。",
        "领域": "强化学习, 因果表示学习, 自监督学习",
        "问题": "如何使强化学习代理通过自监督实验发现环境中的因果因素",
        "动机": "赋予强化学习代理类似人类的实验能力，以发现和理解环境中的因果因素",
        "方法": "引入因果好奇心作为内在奖励，使代理能够学习最优动作序列并发现动态中的因果因素",
        "关键词": [
            "因果好奇心",
            "强化学习",
            "自监督学习",
            "因果表示学习",
            "零样本学习"
        ],
        "涉及的技术概念": {
            "因果好奇心": "一种新型内在奖励，用于激励代理进行有助于发现因果因素的实验",
            "自监督学习": "代理在没有外部监督的情况下，通过自身行为学习环境规律的方法",
            "零样本学习": "利用已学习的因果因素表示，无需额外训练即可适应新任务的能力"
        }
    },
    {
        "order": 172,
        "title": "Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners",
        "html": "https://ICML.cc//virtual/2021/poster/9607",
        "abstract": "Linear residualization is a common practice for confounding adjustment in machine learning applications. Recently, causality-aware predictive modeling has been proposed as an alternative causality-inspired approach for adjusting for confounders. In this paper, we compare the linear residualization approach against the causality-aware confounding adjustment in anticausal prediction tasks. Our comparisons include both the settings where the training and test sets come from the same distributions, as well as, when the training and test sets are shifted due to selection biases. In the absence of dataset shifts, we show that the causality-aware approach tends to (asymptotically) outperform the residualization adjustment in terms of predictive performance in linear learners. Importantly, our results still holds even when the true model generating the data is not linear. We illustrate our results in both regression and classification tasks. Furthermore, in the presence of dataset shifts in the joint distribution of the confounders and outcome variables, we show that the causality-aware approach is more stable than linear residualization.",
        "conference": "ICML",
        "中文标题": "因果感知的反事实混杂调整作为线性学习器中反因果预测任务的线性残差化替代方法",
        "摘要翻译": "线性残差化是机器学习应用中混杂调整的常见做法。最近，因果感知的预测建模被提出作为一种替代的、受因果启发的混杂调整方法。在本文中，我们比较了线性残差化方法与因果感知混杂调整在反因果预测任务中的表现。我们的比较包括训练集和测试集来自相同分布的情况，以及由于选择偏差导致训练集和测试集发生偏移的情况。在没有数据集偏移的情况下，我们表明，就线性学习器的预测性能而言，因果感知方法往往（渐近地）优于残差化调整。重要的是，即使生成数据的真实模型不是线性的，我们的结果仍然成立。我们在回归和分类任务中都展示了我们的结果。此外，在混杂因素和结果变量的联合分布存在数据集偏移的情况下，我们表明因果感知方法比线性残差化更稳定。",
        "领域": "因果机器学习、反因果预测、线性学习器",
        "问题": "比较线性残差化和因果感知混杂调整在反因果预测任务中的效果",
        "动机": "探索在存在或不存在数据集偏移的情况下，因果感知方法是否能够提供比传统线性残差化更优的预测性能和稳定性",
        "方法": "在回归和分类任务中比较线性残差化和因果感知混杂调整方法的效果，包括相同分布和数据集偏移的情况",
        "关键词": [
            "因果感知",
            "反因果预测",
            "线性残差化",
            "混杂调整",
            "数据集偏移"
        ],
        "涉及的技术概念": {
            "线性残差化": "一种传统的混杂调整方法，通过从预测变量中去除混杂因素的影响来调整模型",
            "因果感知混杂调整": "一种基于因果推理的混杂调整方法，旨在更准确地识别和调整混杂因素对预测的影响",
            "反因果预测": "预测任务中，预测变量被认为是结果变量的原因，这种情况下混杂调整尤为重要"
        },
        "success": true
    },
    {
        "order": 173,
        "title": "ChaCha for Online AutoML",
        "html": "https://ICML.cc//virtual/2021/poster/10137",
        "abstract": "We propose the ChaCha (Champion-Challengers) algorithm for making an online choice of hyperparameters in online learning settings. ChaCha handles the process of determining a champion and scheduling a set of `live' challengers over time based on sample complexity bounds. It is guaranteed to have sublinear regret after the optimal configuration is added into consideration by an application-dependent oracle based on the champions. Empirically, we show that ChaCha provides good performance across a wide array of datasets when optimizing over featurization and hyperparameter decisions.",
        "conference": "ICML",
        "中文标题": "ChaCha算法：在线自动机器学习中的超参数选择",
        "摘要翻译": "我们提出了ChaCha（冠军-挑战者）算法，用于在线学习环境中的超参数在线选择。ChaCha处理确定冠军和基于样本复杂度边界随时间调度一组‘活跃’挑战者的过程。它保证在通过依赖应用的oracle基于冠军将最优配置纳入考虑后，具有次线性遗憾。实证上，我们展示了ChaCha在优化特征化和超参数决策时，在广泛的数据集上提供了良好的性能。",
        "领域": "自动机器学习、在线学习、超参数优化",
        "问题": "在线学习环境中如何高效选择超参数",
        "动机": "为了解决在线学习环境中超参数选择的效率和效果问题",
        "方法": "提出ChaCha算法，通过冠军-挑战者机制和样本复杂度边界来动态选择和优化超参数",
        "关键词": [
            "自动机器学习",
            "在线学习",
            "超参数优化",
            "ChaCha算法",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "ChaCha算法": "一种用于在线学习环境中超参数选择的算法，通过冠军-挑战者机制动态优化",
            "样本复杂度边界": "用于评估和调度挑战者的理论依据，确保算法的效率和效果",
            "次线性遗憾": "算法性能的理论保证，表示在最优配置被考虑后，算法的遗憾增长速度低于线性"
        },
        "success": true
    },
    {
        "order": 174,
        "title": "Characterizing Fairness Over the Set of Good Models Under Selective Labels",
        "html": "https://ICML.cc//virtual/2021/poster/8471",
        "abstract": "Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the ``Rashomon Effect.'' These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or ``the set of good models.'' Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.",
        "conference": "ICML",
        "中文标题": "在选择性标签下良好模型集合中的公平性特征化",
        "摘要翻译": "算法风险评估被用于在多种高风险环境中为决策提供信息。通常，多个预测模型在整体性能上相似，但在个别案例的预测上差异显著，这一经验现象被称为“拉什莫尔效应”。这些模型在不同群体上可能具有不同的属性，因此具有不同的预测公平性属性。我们开发了一个框架，用于在提供相似整体性能的模型集合（即“良好模型集合”）上特征化预测公平性属性。我们的框架解决了在选择性标记数据设置中的实证相关挑战，其中选择决策和结果在给定观察到的数据特征下是无混杂的。我们的框架可用于：1）审计预测偏差；或2）用具有更好公平性属性的模型替换现有模型。我们在一个累犯预测任务和一个真实世界的信用评分任务上说明了这些用例。",
        "领域": "公平机器学习、风险评估、信用评分",
        "问题": "在多个性能相近的预测模型中，如何特征化和优化它们的公平性属性。",
        "动机": "解决在选择性标记数据环境下，模型预测公平性的评估和优化问题。",
        "方法": "开发一个框架，用于在良好模型集合上特征化预测公平性属性，并应用于审计预测偏差和优化模型公平性。",
        "关键词": [
            "公平机器学习",
            "拉什莫尔效应",
            "风险评估",
            "信用评分",
            "预测偏差"
        ],
        "涉及的技术概念": {
            "拉什莫尔效应": "描述多个预测模型在整体性能相似但在个别预测上差异显著的现象。",
            "选择性标记数据": "指在数据收集过程中，某些样本的标签由于选择偏差而缺失的情况。",
            "预测公平性": "评估和确保模型预测在不同群体间不产生歧视或偏差的属性。"
        },
        "success": true
    },
    {
        "order": 175,
        "title": "Characterizing Structural Regularities of Labeled Data in Overparameterized Models",
        "html": "https://ICML.cc//virtual/2021/poster/10133",
        "abstract": "Humans are accustomed to environments that contain both regularities and exceptions. For example, at most gas stations, one pays prior to pumping, but the occasional rural station does not accept payment in advance. Likewise, deep neural networks can generalize across instances that share common patterns or structures, yet have the capacity to memorize rare or irregular forms. We analyze how individual instances are treated by a model via a consistency score. The score characterizes the expected accuracy for a held-out instance given training sets of varying size sampled from the data distribution. We obtain empirical estimates of this score for individual instances in multiple data sets, and we show that the score identifies out-of-distribution and mislabeled examples at one end of the continuum and strongly regular examples at the other end. We identify computationally inexpensive proxies to the consistency score using statistics collected during training. We apply the score toward understanding the dynamics of representation learning and to filter outliers during training.",
        "conference": "ICML",
        "中文标题": "过参数化模型中标记数据的结构规律性特征化",
        "摘要翻译": "人类习惯于既包含规律性又包含例外的环境。例如，在大多数加油站，人们在加油前付款，但偶尔的乡村加油站不接受提前付款。同样，深度神经网络可以泛化共享共同模式或结构的实例，同时有能力记忆罕见或不规则的形式。我们通过一致性分数分析模型如何处理单个实例。该分数描述了给定从数据分布中采样的不同大小的训练集时，保留实例的预期准确性。我们为多个数据集中的单个实例获得了该分数的经验估计，并表明该分数在连续体的一端识别出分布外和错误标记的示例，在另一端识别出强规律性示例。我们使用训练期间收集的统计数据确定了计算成本低廉的一致性分数代理。我们将该分数应用于理解表示学习的动态以及在训练期间过滤异常值。",
        "领域": "深度学习理论、异常检测、表示学习",
        "问题": "如何在过参数化模型中识别和处理标记数据中的结构规律性及异常值",
        "动机": "探索深度神经网络在处理具有规律性和例外情况的数据时的行为，特别是在泛化和记忆能力方面的表现",
        "方法": "通过一致性分数分析模型对单个实例的处理方式，利用训练数据统计寻找计算效率高的代理方法",
        "关键词": [
            "一致性分数",
            "过参数化模型",
            "异常检测",
            "表示学习",
            "数据规律性"
        ],
        "涉及的技术概念": {
            "一致性分数": "用于描述模型对单个实例处理方式的一致性程度，帮助识别异常值和规律性强的实例",
            "过参数化模型": "指参数数量远大于训练样本数量的模型，本研究探讨其处理数据规律性和异常值的能力",
            "表示学习": "研究模型如何从数据中学习有效的表示，一致性分数被用来理解这一过程的动态"
        },
        "success": true
    },
    {
        "order": 176,
        "title": "Characterizing the Gap Between Actor-Critic and Policy Gradient",
        "html": "https://ICML.cc//virtual/2021/poster/9711",
        "abstract": "Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although it is understood that AC methods are closely related to policy gradient (PG), their precise connection has not been fully characterized previously. In this paper, we explain the gap between AC and PG methods by identifying the exact adjustment to the AC objective/gradient that recovers the true policy gradient of the cumulative reward objective (PG). Furthermore, by viewing the AC method as a two-player Stackelberg game between the actor and critic, we show that the Stackelberg policy gradient can be recovered as a special case of our more general analysis. Based on these results, we develop practical algorithms, Residual Actor-Critic and Stackelberg Actor-Critic, for estimating the correction between AC and PG and use these to modify the standard AC algorithm. Experiments on popular tabular and continuous environments show the proposed corrections can improve both the sample efficiency and final performance of existing AC methods.",
        "conference": "ICML",
        "中文标题": "刻画演员-评论家与策略梯度之间的差距",
        "摘要翻译": "演员-评论家（AC）方法在强化学习中无处不在。尽管人们理解AC方法与策略梯度（PG）密切相关，但它们之间的精确联系此前尚未完全刻画。在本文中，我们通过识别AC目标/梯度的确切调整来解释AC与PG方法之间的差距，这种调整恢复了累积奖励目标（PG）的真实策略梯度。此外，通过将AC方法视为演员和评论家之间的两玩家Stackelberg博弈，我们展示了Stackelberg策略梯度可以作为我们更一般分析的特殊情况被恢复。基于这些结果，我们开发了实用的算法——残差演员-评论家和Stackelberg演员-评论家，用于估计AC与PG之间的校正，并利用这些来修改标准的AC算法。在流行的表格和连续环境上的实验表明，所提出的校正可以提高现有AC方法的样本效率和最终性能。",
        "领域": "强化学习",
        "问题": "明确演员-评论家方法与策略梯度方法之间的精确联系及其差距",
        "动机": "为了更深入地理解演员-评论家方法与策略梯度方法之间的关系，并开发出能够提高现有方法性能的校正算法",
        "方法": "通过识别AC目标/梯度的调整来恢复真实策略梯度，并将AC方法视为Stackelberg博弈，开发残差演员-评论家和Stackelberg演员-评论家算法",
        "关键词": [
            "演员-评论家方法",
            "策略梯度",
            "Stackelberg博弈",
            "强化学习算法",
            "样本效率"
        ],
        "涉及的技术概念": {
            "演员-评论家方法": "一种结合了价值函数评估（评论家）和策略优化（演员）的强化学习方法",
            "策略梯度": "直接对策略参数进行梯度上升以优化累积奖励的方法",
            "Stackelberg博弈": "一种领导者-追随者博弈模型，用于描述演员和评论家之间的交互"
        },
        "success": true
    },
    {
        "order": 177,
        "title": "Chebyshev Polynomial Codes: Task Entanglement-based Coding for Distributed Matrix Multiplication",
        "html": "https://ICML.cc//virtual/2021/poster/9255",
        "abstract": "Distributed computing has been a prominent solution to efficiently process massive datasets in parallel. However, the existence of stragglers is one of the major concerns that slows down the overall speed of distributed computing. To deal with this problem, we consider a distributed matrix multiplication scenario where a master assigns multiple tasks to each worker to exploit stragglers' computing ability (which is typically wasted in conventional distributed computing). We propose Chebyshev polynomial codes, which can achieve order-wise improvement in encoding complexity at the master and communication load in distributed matrix multiplication using task entanglement. The key idea of task entanglement is to reduce the number of encoded matrices for multiple tasks assigned to each worker by intertwining encoded matrices. We experimentally demonstrate that, in cloud environments, Chebyshev polynomial codes can provide significant reduction in overall processing time in distributed computing for matrix multiplication, which is a key computational component in modern deep learning.",
        "conference": "ICML",
        "中文标题": "切比雪夫多项式编码：基于任务纠缠的分布式矩阵乘法编码",
        "摘要翻译": "分布式计算一直是高效并行处理海量数据的突出解决方案。然而，拖后腿者的存在是减缓分布式计算整体速度的主要问题之一。为了解决这个问题，我们考虑了一个分布式矩阵乘法场景，其中主节点为每个工作者分配多个任务，以利用拖后腿者的计算能力（这在传统分布式计算中通常被浪费）。我们提出了切比雪夫多项式编码，它可以通过任务纠缠在主节点的编码复杂度和分布式矩阵乘法中的通信负载上实现阶数级的改进。任务纠缠的关键思想是通过交织编码矩阵来减少分配给每个工作者的多个任务的编码矩阵数量。我们通过实验证明，在云环境中，切比雪夫多项式编码可以显著减少矩阵乘法在分布式计算中的整体处理时间，这是现代深度学习中的一个关键计算组件。",
        "领域": "分布式计算优化、矩阵运算加速、深度学习计算效率",
        "问题": "解决分布式计算中因拖后腿者导致的计算速度下降问题",
        "动机": "利用拖后腿者的计算能力，提高分布式矩阵乘法的效率",
        "方法": "提出切比雪夫多项式编码，通过任务纠缠减少编码矩阵数量，优化编码复杂度和通信负载",
        "关键词": [
            "切比雪夫多项式编码",
            "任务纠缠",
            "分布式矩阵乘法",
            "计算效率优化",
            "深度学习"
        ],
        "涉及的技术概念": {
            "切比雪夫多项式编码": "用于减少主节点编码复杂度和通信负载的编码方法",
            "任务纠缠": "通过交织编码矩阵减少分配给每个工作者的任务编码数量，提高计算效率",
            "分布式矩阵乘法": "现代深度学习中的关键计算组件，本文旨在优化其在分布式计算中的处理效率"
        },
        "success": true
    },
    {
        "order": 178,
        "title": "CIFS: Improving Adversarial Robustness of CNNs via Channel-wise Importance-based Feature Selection",
        "html": "https://ICML.cc//virtual/2021/poster/10647",
        "abstract": "We investigate the adversarial robustness of CNNs from the perspective of channel-wise activations.  By comparing normally trained and adversarially trained models, we observe that adversarial training (AT) robustifies CNNs by aligning the channel-wise activations of adversarial data with those of their natural counterparts. However, the channels that are \\textit{negatively-relevant} (NR) to predictions are still over-activated when processing adversarial data. Besides, we also observe that AT does not result in similar robustness for all classes. For the robust classes, channels with larger activation magnitudes are usually more \\textit{positively-relevant} (PR) to predictions, but this alignment does not hold for the non-robust classes. Given these observations, we hypothesize that suppressing NR channels and aligning PR ones with their relevances further enhances the robustness of CNNs under AT. To examine this hypothesis, we introduce a novel mechanism, \\textit{i.e.}, \\underline{C}hannel-wise \\underline{I}mportance-based \\underline{F}eature \\underline{S}election (CIFS). The CIFS manipulates channels' activations of certain layers by generating non-negative multipliers to these channels based on their relevances to predictions. Extensive experiments on benchmark datasets including CIFAR10 and SVHN clearly verify the hypothesis and CIFS's effectiveness of robustifying CNNs.",
        "conference": "ICML",
        "中文标题": "CIFS：通过基于通道重要性的特征选择提升CNN的对抗鲁棒性",
        "摘要翻译": "我们从通道激活的角度研究了CNN的对抗鲁棒性。通过比较正常训练和对抗训练的模型，我们观察到对抗训练（AT）通过将对抗数据的通道激活与其自然对应物的通道激活对齐来增强CNN的鲁棒性。然而，在处理对抗数据时，那些与预测负相关（NR）的通道仍然被过度激活。此外，我们还观察到AT并未对所有类别产生相似的鲁棒性。对于鲁棒类别，激活幅度较大的通道通常与预测正相关（PR）更强，但这种对齐关系在非鲁棒类别中并不成立。基于这些观察，我们假设抑制NR通道并将PR通道与其相关性对齐可以进一步增强AT下CNN的鲁棒性。为了验证这一假设，我们引入了一种新机制，即基于通道重要性的特征选择（CIFS）。CIFS通过根据通道与预测的相关性生成非负乘数来操纵某些层的通道激活。在包括CIFAR10和SVHN在内的基准数据集上的大量实验清楚地验证了这一假设以及CIFS在增强CNN鲁棒性方面的有效性。",
        "领域": "对抗性机器学习、图像分类、深度学习安全",
        "问题": "提升卷积神经网络（CNN）在对抗攻击下的鲁棒性",
        "动机": "对抗训练（AT）虽然能提升CNN的鲁棒性，但存在负相关通道过度激活和不同类别鲁棒性不一致的问题",
        "方法": "提出基于通道重要性的特征选择（CIFS）机制，通过调整通道激活来增强模型鲁棒性",
        "关键词": [
            "对抗鲁棒性",
            "通道激活",
            "特征选择",
            "对抗训练",
            "CNN"
        ],
        "涉及的技术概念": {
            "对抗训练（AT）": "一种通过引入对抗样本来训练模型以提升其对抗攻击鲁棒性的技术",
            "负相关（NR）通道": "在对抗数据中过度激活但与预测结果负相关的通道，可能降低模型鲁棒性",
            "基于通道重要性的特征选择（CIFS）": "一种新机制，通过调整通道激活来抑制NR通道并强化PR通道，以增强模型鲁棒性"
        },
        "success": true
    },
    {
        "order": 179,
        "title": "Class2Simi: A Noise Reduction Perspective on Learning with Noisy Labels",
        "html": "https://ICML.cc//virtual/2021/poster/9641",
        "abstract": "Learning with noisy labels has attracted a lot of attention in recent years, where the mainstream approaches are in \\emph{pointwise} manners. Meanwhile, \\emph{pairwise} manners have shown great potential in supervised metric learning and unsupervised contrastive learning.\nThus, a natural question is raised: does learning in a pairwise manner \\emph{mitigate} label noise?\nTo give an affirmative answer, in this paper, we propose a framework called \\emph{Class2Simi}: it transforms data points with noisy \\emph{class labels} to data pairs with noisy \\emph{similarity labels}, where a similarity label denotes whether a pair shares the class label or not.\nThrough this transformation, the \\emph{reduction of the noise rate} is theoretically guaranteed, and hence it is in principle easier to handle noisy similarity labels.\nAmazingly, DNNs that predict the \\emph{clean} class labels can be trained from noisy data pairs if they are first pretrained from noisy data points.\nClass2Simi is \\emph{computationally efficient} because not only this transformation is on-the-fly in mini-batches, but also it just changes loss computation on top of model prediction into a pairwise manner.\nIts effectiveness is verified by extensive experiments.",
        "conference": "ICML",
        "中文标题": "Class2Simi：从噪声减少的角度学习带有噪声标签的数据",
        "摘要翻译": "近年来，学习带有噪声标签的数据引起了广泛关注，其中主流方法采用点对点的方式。与此同时，在监督度量学习和无监督对比学习中，成对方式显示出了巨大的潜力。因此，一个自然的问题被提出：以成对方式学习是否能减轻标签噪声？为了给出肯定的答案，本文提出了一个名为Class2Simi的框架：它将带有噪声类别标签的数据点转换为带有噪声相似性标签的数据对，其中相似性标签表示一对数据是否共享类别标签。通过这种转换，理论上保证了噪声率的降低，因此在原理上更容易处理噪声相似性标签。令人惊讶的是，可以训练预测干净类别标签的深度神经网络，如果它们首先从带有噪声的数据点进行预训练。Class2Simi在计算上是高效的，因为不仅这种转换是在小批量中即时进行的，而且它只是将模型预测之上的损失计算改为成对方式。其有效性通过大量实验得到了验证。",
        "领域": "噪声标签学习",
        "问题": "如何减轻标签噪声对深度学习模型训练的影响",
        "动机": "探索成对学习方式在减轻标签噪声方面的潜力",
        "方法": "提出Class2Simi框架，将带有噪声类别标签的数据转换为带有噪声相似性标签的数据对，理论上降低噪声率，并通过预训练和成对损失计算提高模型训练效率和效果",
        "关键词": [
            "噪声标签学习",
            "成对学习",
            "Class2Simi框架",
            "噪声减少",
            "深度神经网络"
        ],
        "涉及的技术概念": {
            "噪声标签学习": "研究如何从带有错误或不准确标签的数据中学习有效的模型",
            "成对学习": "通过比较数据对之间的相似性来学习，而不是直接使用单个数据的标签",
            "Class2Simi框架": "将噪声类别标签转换为噪声相似性标签的框架，旨在降低噪声率并简化噪声处理"
        },
        "success": true
    },
    {
        "order": 180,
        "title": "Classification with Rejection Based on Cost-sensitive Classification",
        "html": "https://ICML.cc//virtual/2021/poster/10499",
        "abstract": "The goal of classification with rejection is to avoid risky misclassification in error-critical applications such as medical diagnosis and product inspection. In this paper, based on the relationship between classification with rejection and cost-sensitive classification, we propose a novel method of classification with rejection by learning an ensemble of cost-sensitive classifiers, which satisfies all the following properties:\n(i) it can avoid estimating class-posterior probabilities, resulting in improved classification accuracy. \n(ii) it allows a flexible choice of losses including non-convex ones, \n(iii) it does not require complicated modifications when using different losses, \n(iv) it is applicable to both binary and multiclass cases, and \n(v) it is theoretically justifiable for any classification-calibrated loss.\nExperimental results demonstrate the usefulness of our proposed approach in clean-labeled, noisy-labeled, and positive-unlabeled classification.",
        "conference": "ICML",
        "中文标题": "基于成本敏感分类的带拒绝分类",
        "摘要翻译": "带拒绝分类的目标是在诸如医疗诊断和产品检测等错误关键应用中避免风险性的错误分类。本文基于带拒绝分类与成本敏感分类之间的关系，提出了一种通过学习成本敏感分类器集成来实现带拒绝分类的新方法，该方法满足以下所有特性：(i) 可以避免估计类别后验概率，从而提高分类准确性。(ii) 允许灵活选择损失函数，包括非凸损失。(iii) 在使用不同损失函数时不需要复杂的修改。(iv) 适用于二分类和多分类情况。(v) 对于任何分类校准的损失函数，理论上都是合理的。实验结果证明了我们提出的方法在干净标签、噪声标签和正未标记分类中的有效性。",
        "领域": "机器学习、成本敏感学习、分类算法",
        "问题": "在错误关键应用中避免风险性的错误分类",
        "动机": "通过避免估计类别后验概率和允许灵活选择损失函数，提高分类准确性并简化方法的应用",
        "方法": "通过学习成本敏感分类器集成来实现带拒绝分类，支持多种损失函数和分类场景",
        "关键词": [
            "带拒绝分类",
            "成本敏感学习",
            "分类器集成",
            "非凸损失",
            "分类校准"
        ],
        "涉及的技术概念": {
            "带拒绝分类": "一种分类方法，允许模型在不确定时拒绝做出分类决策，以避免高风险错误",
            "成本敏感学习": "一种考虑不同分类错误成本的学习方法，旨在最小化总体分类成本",
            "分类器集成": "通过组合多个分类器的预测来提高分类性能和鲁棒性的技术"
        },
        "success": true
    },
    {
        "order": 181,
        "title": "Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed",
        "html": "https://ICML.cc//virtual/2021/poster/8535",
        "abstract": " A recent series of theoretical works showed that the dynamics of neural\n  networks with a certain initialisation are well-captured by kernel methods.\n  Concurrent empirical work demonstrated that kernel methods can come close to\n  the performance of neural networks on some image classification tasks.\n  These results raise the question of whether neural networks only learn\n  successfully if kernels also learn successfully, despite being the more\n  expressive function class.\n  Here, we show that two-layer neural networks with *only a few neurons* achieve\n  near-optimal performance on high-dimensional Gaussian mixture classification\n  while lazy training approaches such as random features and kernel methods do\n  not.\n  Our analysis is based on the derivation of a set of ordinary differential\n  equations that exactly track the dynamics of the network and thus allow to\n  extract the asymptotic performance of the network as a function of\n  regularisation or signal-to-noise ratio.\n  We also show how over-parametrising the neural network leads to faster convergence, but does not improve its final performance.",
        "conference": "ICML",
        "中文标题": "高维高斯混合分类：核方法失效与神经网络成功之处",
        "摘要翻译": "最近的一系列理论研究表明，具有特定初始化的神经网络的动态可以被核方法很好地捕捉。同时，实证工作表明，核方法在某些图像分类任务上可以接近神经网络的性能。这些结果引发了一个问题：神经网络是否仅在核方法也能成功学习的情况下才能成功学习，尽管神经网络是更具表达力的函数类别。在这里，我们展示了仅含少量神经元的双层神经网络在高维高斯混合分类上实现了接近最优的性能，而懒惰训练方法如随机特征和核方法则未能做到。我们的分析基于一组精确跟踪网络动态的常微分方程的推导，从而允许提取网络作为正则化或信噪比函数的渐近性能。我们还展示了如何通过过度参数化神经网络来加快收敛速度，但不会改善其最终性能。",
        "领域": "高维数据分类、核方法、神经网络理论",
        "问题": "探讨在高维高斯混合分类任务中，核方法与神经网络性能差异的根本原因",
        "动机": "研究神经网络在核方法失败的情况下仍能成功学习的机制，以及过度参数化对神经网络性能的影响",
        "方法": "通过推导精确跟踪神经网络动态的常微分方程，分析网络的渐近性能，并比较核方法与神经网络的性能差异",
        "关键词": [
            "高维高斯混合",
            "核方法",
            "神经网络",
            "常微分方程",
            "过度参数化"
        ],
        "涉及的技术概念": {
            "核方法": "一种基于核函数的机器学习方法，用于捕捉数据间的非线性关系",
            "神经网络动态": "研究神经网络在训练过程中的行为变化，包括权重更新和性能演变",
            "常微分方程": "用于精确描述和预测神经网络训练过程中的动态变化"
        },
        "success": true
    },
    {
        "order": 182,
        "title": "CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients",
        "html": "https://ICML.cc//virtual/2021/poster/8461",
        "abstract": "The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training method that encourages representations of instances to be similar to one another. We propose a family of contrastive learning methods, CLOCS, that encourages representations across space, time, \\textit{and} patients to be similar to one another. We show that CLOCS consistently outperforms the state-of-the-art methods, BYOL and SimCLR, when performing a linear evaluation of, and fine-tuning on, downstream tasks. We also show that CLOCS achieves strong generalization performance with only 25\\% of labelled training data. Furthermore, our training procedure naturally generates patient-specific representations that can be used to quantify patient-similarity. ",
        "conference": "ICML",
        "中文标题": "CLOCS：跨空间、时间和患者的心脏信号对比学习",
        "摘要翻译": "医疗行业生成了大量未标记的生理数据。这些数据可以通过对比学习来利用，这是一种自监督的预训练方法，鼓励实例的表示彼此相似。我们提出了一系列对比学习方法，CLOCS，它鼓励跨空间、时间和患者的表示彼此相似。我们展示了在执行下游任务的线性评估和微调时，CLOCS始终优于最先进的方法BYOL和SimCLR。我们还展示了CLOCS仅用25%的标记训练数据就能实现强大的泛化性能。此外，我们的训练过程自然地生成了可以用于量化患者相似性的患者特定表示。",
        "领域": "医疗图像分析、自监督学习、心脏信号处理",
        "问题": "如何有效利用大量未标记的生理数据进行心脏信号分析",
        "动机": "利用对比学习提高心脏信号分析的效率和准确性，尤其是在标记数据稀缺的情况下",
        "方法": "提出了一种名为CLOCS的对比学习方法，通过鼓励跨空间、时间和患者的信号表示相似性，来提高模型性能",
        "关键词": [
            "对比学习",
            "心脏信号",
            "自监督学习",
            "患者相似性",
            "医疗数据分析"
        ],
        "涉及的技术概念": {
            "对比学习": "一种自监督学习方法，通过比较不同实例的表示来学习有用的特征",
            "自监督预训练": "在没有人工标注的情况下，利用数据本身的结构进行模型预训练",
            "患者特定表示": "通过模型学习到的能够反映个体患者特征的信号表示，用于量化患者之间的相似性"
        },
        "success": true
    },
    {
        "order": 183,
        "title": "Clusterability as an Alternative to Anchor Points When Learning with Noisy Labels",
        "html": "https://ICML.cc//virtual/2021/poster/10625",
        "abstract": "The label noise transition matrix, characterizing the probabilities of a training instance being wrongly annotated, is crucial to designing popular solutions to learning with noisy labels. Existing works heavily rely on finding ``anchor points'' or their approximates, defined as instances belonging to a particular class almost surely. Nonetheless, finding anchor points remains a non-trivial task, and the estimation accuracy is also often throttled by the number of available anchor points. In this paper, we propose an alternative option to the above task. Our main contribution is the discovery of an efficient estimation procedure based on a clusterability condition. We prove that with clusterable representations of features, using up to third-order consensuses of noisy labels among neighbor representations is sufficient to estimate a unique transition matrix. Compared with methods using anchor points, our approach uses substantially more instances and benefits from a much better sample complexity. We demonstrate the estimation accuracy and advantages of our estimates using both synthetic noisy labels (on CIFAR-10/100) and real human-level noisy labels (on Clothing1M and our self-collected human-annotated CIFAR-10). Our code and human-level noisy CIFAR-10 labels are available at https://github.com/UCSC-REAL/HOC.",
        "conference": "ICML",
        "中文标题": "将可聚类性作为学习带噪声标签时的锚点替代方案",
        "摘要翻译": "标签噪声转移矩阵，描述了训练实例被错误标注的概率，对于设计带噪声标签学习的流行解决方案至关重要。现有工作严重依赖于寻找“锚点”或其近似，锚点被定义为几乎肯定属于特定类别的实例。然而，寻找锚点仍然是一项非平凡的任务，而且估计精度也常常受到可用锚点数量的限制。在本文中，我们提出了上述任务的替代方案。我们的主要贡献是基于可聚类性条件发现了一种高效的估计程序。我们证明，通过特征的可聚类表示，使用邻居表示中噪声标签的至多三阶共识足以估计一个唯一的转移矩阵。与使用锚点的方法相比，我们的方法使用了更多的实例，并从更好的样本复杂性中受益。我们通过在合成噪声标签（CIFAR-10/100上）和真实人类级别噪声标签（Clothing1M和我们自收集的人类标注CIFAR-10上）上展示估计精度和我们估计的优势。我们的代码和人类级别噪声CIFAR-10标签可在https://github.com/UCSC-REAL/HOC获取。",
        "领域": "噪声标签学习",
        "问题": "如何在带噪声标签的学习中高效估计标签噪声转移矩阵",
        "动机": "现有方法依赖于寻找锚点，但锚点的寻找困难且估计精度受限，因此需要一种不依赖锚点的替代方案",
        "方法": "提出基于特征可聚类性的估计程序，利用邻居表示中噪声标签的至多三阶共识来估计转移矩阵",
        "关键词": [
            "噪声标签学习",
            "标签噪声转移矩阵",
            "可聚类性",
            "样本复杂性",
            "估计精度"
        ],
        "涉及的技术概念": {
            "标签噪声转移矩阵": "描述训练实例被错误标注概率的矩阵，对设计带噪声标签学习的解决方案至关重要",
            "可聚类性": "特征的表示能够形成清晰的聚类，使得基于邻居表示的噪声标签共识可以用于估计转移矩阵",
            "样本复杂性": "指在统计学习中，为了达到一定的学习精度所需的样本数量，本文方法通过利用更多实例改善了样本复杂性"
        },
        "success": true
    },
    {
        "order": 184,
        "title": "Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10543",
        "abstract": "This work addresses the problem of optimizing communications between server and clients in federated learning (FL). Current sampling approaches in FL are either biased, or non optimal in terms of server-clients communications and training stability. To overcome this issue, we introduce clustered sampling for clients selection. We prove that clustered sampling leads to better clients representatitivity and to reduced variance of the clients stochastic aggregation weights in FL. Compatibly with our theory, we provide two different clustering approaches enabling clients aggregation based on 1) sample size, and 2) models similarity. Through a series of experiments in non-iid and unbalanced scenarios, we demonstrate that model aggregation through clustered sampling consistently leads to better training convergence and variability when compared to standard sampling approaches. Our approach does not require any additional operation on the clients side, and can be seamlessly integrated in standard FL implementations. Finally, clustered sampling is compatible with existing methods and technologies for privacy enhancement, and for communication reduction through model compression.",
        "conference": "ICML",
        "中文标题": "聚类采样：联邦学习中客户选择的低方差与提高代表性",
        "摘要翻译": "本工作解决了联邦学习（FL）中服务器与客户端之间通信优化的问题。当前FL中的采样方法要么存在偏差，要么在服务器-客户端通信和训练稳定性方面不是最优的。为了克服这一问题，我们引入了用于客户选择的聚类采样。我们证明，聚类采样能够带来更好的客户代表性，并减少FL中客户随机聚合权重的方差。与我们的理论兼容，我们提供了两种不同的聚类方法，使客户能够基于1）样本大小，和2）模型相似性进行聚合。通过一系列非独立同分布和不平衡场景下的实验，我们证明，与标准采样方法相比，通过聚类采样进行的模型聚合始终能够带来更好的训练收敛性和变异性。我们的方法不需要在客户端进行任何额外操作，并且可以无缝集成到标准的FL实现中。最后，聚类采样与现有的隐私增强方法和技术以及通过模型压缩减少通信的方法兼容。",
        "领域": "联邦学习、机器学习优化、分布式学习",
        "问题": "优化联邦学习中服务器与客户端之间的通信效率和训练稳定性",
        "动机": "解决当前联邦学习采样方法存在的偏差和通信效率不高的问题",
        "方法": "引入聚类采样方法，基于样本大小和模型相似性进行客户选择，以减少方差并提高代表性",
        "关键词": [
            "聚类采样",
            "联邦学习",
            "通信优化",
            "训练稳定性",
            "模型聚合"
        ],
        "涉及的技术概念": {
            "聚类采样": "一种用于联邦学习中客户选择的方法，旨在通过聚类减少方差并提高代表性",
            "联邦学习": "一种分布式机器学习方法，允许多个客户端在保护数据隐私的前提下共同训练模型",
            "模型聚合": "在联邦学习中，将来自不同客户端的模型更新聚合成一个全局模型的过程"
        },
        "success": true
    },
    {
        "order": 185,
        "title": "Coach-Player Multi-agent Reinforcement Learning for Dynamic Team Composition",
        "html": "https://ICML.cc//virtual/2021/poster/9723",
        "abstract": "In real-world multi-agent systems, agents with different capabilities may join or leave without altering the team's overarching goals. Coordinating teams with such dynamic composition is challenging: the optimal team strategy varies with the composition. We propose COPA, a coach-player framework to tackle this problem. We assume the coach has a global view of the environment and coordinates the players, who only have partial views, by distributing individual strategies. Specifically, we 1) adopt the attention mechanism for both the coach and the players; 2) propose a variational objective to regularize learning; and 3) design an adaptive communication method to let the coach decide when to communicate with the players. We validate our methods on a resource collection task, a rescue game, and the StarCraft micromanagement tasks. We demonstrate zero-shot generalization to new team compositions. Our method achieves comparable or better performance than the setting where all players have a full view of the environment. Moreover, we see that the performance remains high even when the coach communicates as little as 13% of the time using the adaptive communication strategy.",
        "conference": "ICML",
        "中文标题": "教练-玩家多智能体强化学习用于动态团队组成",
        "摘要翻译": "在现实世界的多智能体系统中，具有不同能力的智能体可能会加入或离开，而不改变团队的总体目标。协调这种动态组成的团队具有挑战性：最优的团队策略随组成而变化。我们提出了COPA，一个教练-玩家框架来解决这个问题。我们假设教练拥有环境的全局视角，并通过分配个体策略来协调只有部分视角的玩家。具体来说，我们1) 为教练和玩家采用注意力机制；2) 提出了一个变分目标来规范化学习；3) 设计了一种自适应通信方法，让教练决定何时与玩家通信。我们在资源收集任务、救援游戏和星际争霸微管理任务上验证了我们的方法。我们展示了对新团队组成的零样本泛化能力。我们的方法实现了与所有玩家拥有环境完整视角设置相当或更好的性能。此外，我们发现，即使教练使用自适应通信策略仅通信13%的时间，性能仍然保持高水平。",
        "领域": "多智能体系统、强化学习、动态团队协调",
        "问题": "解决动态团队组成下的最优策略协调问题",
        "动机": "研究如何在不改变团队总体目标的情况下，有效协调具有不同能力且可能动态变化的智能体团队",
        "方法": "采用教练-玩家框架，结合注意力机制、变分学习目标和自适应通信策略",
        "关键词": [
            "多智能体强化学习",
            "动态团队组成",
            "注意力机制",
            "自适应通信",
            "零样本泛化"
        ],
        "涉及的技术概念": {
            "注意力机制": "用于教练和玩家，以增强对关键信息的关注",
            "变分目标": "规范化学习过程，提高模型的泛化能力",
            "自适应通信策略": "允许教练根据当前情况决定通信频率，优化团队协调效率"
        },
        "success": true
    },
    {
        "order": 186,
        "title": "Coded-InvNet for Resilient Prediction Serving Systems",
        "html": "https://ICML.cc//virtual/2021/poster/8601",
        "abstract": "Inspired by a new coded computation algorithm for invertible functions, we propose Coded-InvNet a new approach to design resilient prediction serving systems that can gracefully handle stragglers or node failures. Coded-InvNet leverages recent findings in the deep learning literature such as invertible neural networks, Manifold Mixup, and domain translation algorithms, identifying interesting research directions that span across machine learning and systems. Our experimental results show that Coded-InvNet can outperform existing approaches, especially when the compute resource overhead is as low as 10%. For instance, without knowing which of the ten workers is going to fail, our algorithm can design a backup task so that it can correctly recover the missing prediction result with an accuracy of 85.9%, significantly outperforming the previous SOTA by 32.5%.",
        "conference": "ICML",
        "中文标题": "用于弹性预测服务系统的编码逆网络",
        "摘要翻译": "受一种新的可逆函数编码计算算法的启发，我们提出了Coded-InvNet，这是一种设计能够优雅处理滞后或节点故障的弹性预测服务系统的新方法。Coded-InvNet利用了深度学习文献中的最新发现，如可逆神经网络、流形混合和领域转换算法，识别了跨越机器学习和系统的有趣研究方向。我们的实验结果表明，Coded-InvNet能够超越现有方法，尤其是在计算资源开销低至10%的情况下。例如，在不知道十个工作者中哪一个会失败的情况下，我们的算法可以设计一个备份任务，从而能够以85.9%的准确率正确恢复缺失的预测结果，显著优于之前的SOTA 32.5%。",
        "领域": "可逆神经网络、弹性计算系统、预测服务优化",
        "问题": "设计一个能够优雅处理滞后或节点故障的弹性预测服务系统",
        "动机": "提高预测服务系统在面临计算节点故障或滞后时的恢复能力和准确性",
        "方法": "利用可逆神经网络、流形混合和领域转换算法设计新的编码逆网络方法",
        "关键词": [
            "可逆神经网络",
            "弹性计算",
            "预测服务",
            "节点故障恢复",
            "编码计算"
        ],
        "涉及的技术概念": {
            "可逆神经网络": "用于设计能够在节点故障时恢复预测结果的网络结构",
            "流形混合": "通过混合不同数据流形提升模型的泛化能力和恢复能力",
            "领域转换算法": "用于在不同数据领域间转换，增强系统的适应性和弹性"
        },
        "success": true
    },
    {
        "order": 187,
        "title": "Collaborative Bayesian Optimization with Fair Regret",
        "html": "https://ICML.cc//virtual/2021/poster/9459",
        "abstract": "Bayesian optimization (BO) is a popular tool for optimizing complex and costly-to-evaluate black-box objective functions. To further reduce the number of function evaluations, any party performing BO may be interested to collaborate with others to optimize the same objective function concurrently. To do this, existing BO algorithms have considered optimizing a batch of input queries in parallel and provided theoretical bounds on their cumulative regret reflecting inefficiency. However, when the objective function values are correlated with real-world rewards (e.g., money), parties may be hesitant to collaborate if they risk incurring larger cumulative regret (i.e., smaller real-world reward) than others. This paper shows that fairness and efficiency are both necessary for the collaborative BO setting. Inspired by social welfare concepts from economics, we propose a new notion of regret capturing these properties and a collaborative BO algorithm whose convergence rate can be theoretically guaranteed by bounding the new regret, both of which share an adjustable parameter for trading off between fairness vs. efficiency. We empirically demonstrate the benefits (e.g., increased fairness) of our algorithm using synthetic and real-world datasets.",
        "conference": "ICML",
        "中文标题": "公平遗憾的协作贝叶斯优化",
        "摘要翻译": "贝叶斯优化（BO）是一种流行的工具，用于优化复杂且评估成本高的黑盒目标函数。为了进一步减少函数评估的次数，任何进行BO的方可能希望与其他方合作，同时优化相同的目标函数。为此，现有的BO算法考虑了并行优化一批输入查询，并提供了反映效率低下的累积遗憾的理论界限。然而，当目标函数值与现实世界奖励（如金钱）相关时，各方可能会犹豫是否合作，如果他们冒着比其他人产生更大的累积遗憾（即更小的现实世界奖励）的风险。本文表明，公平和效率对于协作BO设置都是必要的。受经济学中社会福利概念的启发，我们提出了一种新的遗憾概念，捕捉这些属性，以及一种协作BO算法，其收敛速度可以通过限制新的遗憾来理论上保证，两者都有一个可调参数，用于在公平与效率之间进行权衡。我们使用合成和真实世界数据集实证证明了我们算法的好处（例如，增加的公平性）。",
        "领域": "贝叶斯优化、机器学习优化、公平性算法",
        "问题": "在协作贝叶斯优化中，如何平衡公平性和效率，以减少各方在合作过程中的累积遗憾。",
        "动机": "研究动机是为了解决在协作优化过程中，由于目标函数值与现实世界奖励相关，各方可能因担心获得不公平的奖励而犹豫合作的问题。",
        "方法": "提出了一种新的遗憾概念和协作BO算法，通过一个可调参数在公平与效率之间进行权衡，并通过理论保证算法的收敛速度。",
        "关键词": [
            "贝叶斯优化",
            "协作优化",
            "公平性",
            "遗憾最小化",
            "社会福利"
        ],
        "涉及的技术概念": {
            "贝叶斯优化": "用于优化复杂且评估成本高的黑盒目标函数的技术。",
            "累积遗憾": "反映在优化过程中效率低下的度量，本文中与公平性相关联。",
            "社会福利概念": "从经济学中借鉴的概念，用于在协作优化中平衡公平与效率。"
        },
        "success": true
    },
    {
        "order": 188,
        "title": "Combinatorial Blocking Bandits with Stochastic Delays",
        "html": "https://ICML.cc//virtual/2021/poster/10487",
        "abstract": "Recent work has considered natural variations of the {\\em multi-armed bandit} problem, where the reward distribution of each arm is a special function of the time passed since its last pulling. In this direction, a simple (yet widely applicable) model is that of {\\em blocking bandits}, where an arm becomes unavailable for a deterministic number of rounds after each play. In this work, we extend the above model in two directions: (i) We consider the general combinatorial setting where more than one arms can be played at each round, subject to feasibility constraints. (ii) We allow the blocking time of each arm to be stochastic. We first study the computational/unconditional hardness of the above setting and identify the necessary conditions for the problem to become tractable (even in an approximate sense). Based on these conditions, we provide a tight analysis of the approximation guarantee of a natural greedy heuristic that always plays the maximum expected reward feasible subset among the available (non-blocked) arms. When the arms' expected rewards are unknown, we adapt the above heuristic into a bandit algorithm, based on UCB, for which we provide sublinear (approximate) regret guarantees, matching the theoretical lower bounds in the limiting case of absence of delays.",
        "conference": "ICML",
        "success": true,
        "中文标题": "具有随机延迟的组合阻断老虎机",
        "摘要翻译": "最近的研究考虑了{\\\\em 多臂老虎机}问题的一些自然变体，其中每个臂的奖励分布是自上次拉动以来经过的时间的特殊函数。在这个方向上，一个简单（但适用范围广泛）的模型是{\\\\em 阻断老虎机}，其中一个臂在每次拉动后，会在确定数量的回合内变得不可用。在这项工作中，我们从两个方向扩展了上述模型：（i）我们考虑了一般的组合设置，其中可以在每一轮中玩多个臂，但要受到可行性约束。（ii）我们允许每个臂的阻断时间是随机的。我们首先研究了上述设置的计算/无条件硬度，并确定了问题变得易于处理（即使是在近似意义上）的必要条件。基于这些条件，我们对一种自然贪婪启发式的近似保证进行了严格的分析，该启发式总是选择可用（未被阻断）臂中预期奖励最大的可行子集。当臂的预期奖励未知时，我们将上述启发式方法调整为基于UCB的老虎机算法，我们为其提供了次线性（近似）后悔保证，在没有延迟的极限情况下，该保证与理论下界相匹配。",
        "领域": "强化学习、组合优化、在线学习",
        "问题": "如何在具有随机延迟和组合约束的阻断老虎机问题中，最大化累积奖励并最小化遗憾。",
        "动机": "现有的阻断老虎机模型通常假设确定性的阻断时间，且每次只能选择一个臂。为了更贴近现实应用，需要研究更一般的组合设置以及随机阻断时间的情况。",
        "方法": "提出了基于UCB的贪婪启发式算法，该算法在每个回合选择预期奖励最大的可用臂组合，并结合UCB策略来探索未知的臂奖励。通过理论分析，证明了该算法具有次线性的近似遗憾保证。",
        "关键词": [
            "阻断老虎机",
            "组合优化",
            "随机延迟",
            "UCB",
            "遗憾最小化"
        ],
        "涉及的技术概念": {
            "UCB（置信上限）": "UCB是一种探索-利用策略，用于平衡探索未知臂的奖励和利用已知高奖励臂。在算法中，UCB用于估计臂的预期奖励的上界，并选择具有最高上界的臂。",
            "贪婪算法": "贪婪算法是一种在每一步选择局部最优解的算法。在本文中，贪婪算法用于选择当前可用的、预期奖励最大的臂的组合。"
        }
    },
    {
        "order": 189,
        "title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9877",
        "abstract": "In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time.\nTo ensure reliable performance, the RL agents need to exhibit robustness to such worst-case situations. \nThe robust-RL framework addresses this challenge via a minimax optimization between an agent and an adversary.\nPrevious robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems.\nWe propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. \nRH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty and efficiently explores both the agent and the adversary decision spaces during policy learning. \nWe scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. \nExperimentally we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments.",
        "conference": "ICML",
        "中文标题": "结合悲观与乐观策略实现稳健且高效的基于模型的深度强化学习",
        "摘要翻译": "在现实世界的任务中，强化学习（RL）智能体经常会遇到训练时未见过的情况。为了确保可靠的性能，RL智能体需要对这些最坏情况表现出鲁棒性。鲁棒-RL框架通过智能体与对手之间的极小极大优化来应对这一挑战。以往的鲁棒RL算法要么样本效率低下，要么缺乏鲁棒性保证，要么无法扩展到大型问题。我们提出了鲁棒幻觉上置信度RL（RH-UCRL）算法，可证明解决这一问题，同时获得接近最优的样本复杂度保证。RH-UCRL是一种基于模型的强化学习（MBRL）算法，能有效区分认知不确定性和随机不确定性，并在策略学习过程中高效探索智能体和对手的决策空间。我们通过神经网络集成模型以及神经网络策略将RH-UCRL扩展到复杂任务。实验证明，RH-UCRL在各种对抗环境中优于其他鲁棒深度RL算法。",
        "领域": "深度强化学习、对抗性学习、模型基于学习",
        "问题": "如何在强化学习中实现既稳健又高效的策略学习，以应对训练时未见过的最坏情况",
        "动机": "解决现有鲁棒强化学习算法样本效率低下、缺乏鲁棒性保证或无法扩展到大型问题的问题",
        "方法": "提出RH-UCRL算法，通过区分认知不确定性和随机不确定性，高效探索智能体和对手的决策空间，并利用神经网络集成模型和策略扩展到复杂任务",
        "关键词": [
            "鲁棒强化学习",
            "模型基于学习",
            "对抗性学习",
            "神经网络集成",
            "样本效率"
        ],
        "涉及的技术概念": {
            "RH-UCRL算法": "一种基于模型的强化学习算法，旨在通过区分和探索不同类型的不确定性来提高鲁棒性和样本效率",
            "认知不确定性与随机不确定性": "在强化学习中区分由于知识不足（认知）和环境的固有随机性（随机）引起的不确定性，以优化策略学习",
            "神经网络集成模型": "用于扩展RH-UCRL到复杂任务的技术，通过集成多个神经网络模型来提高预测的准确性和鲁棒性"
        },
        "success": true
    },
    {
        "order": 190,
        "title": "CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints",
        "html": "https://ICML.cc//virtual/2021/poster/9555",
        "abstract": "Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many NP-hard problems can be expressed as integer programs, in which the constraints play the role of their 'combinatorial specification'. In this work, we aim to integrate integer programming solvers into neural network architectures as layers capable of learning both the cost terms and the constraints. The resulting end-to-end trainable architectures jointly extract features from raw data and solve a suitable (learned) combinatorial problem with state-of-the-art integer programming solvers. We demonstrate the potential of such layers with an extensive performance analysis on synthetic data and with a demonstration on a competitive computer vision keypoint matching benchmark.",
        "conference": "ICML",
        "中文标题": "CombOptNet：通过学习整数规划约束拟合正确的NP难问题",
        "摘要翻译": "将逻辑和算法推理与现代机器学习技术相结合是一个具有潜在变革性影响的基础性挑战。在算法方面，许多NP难问题可以表示为整数规划问题，其中约束扮演了它们的'组合规范'角色。在这项工作中，我们旨在将整数规划求解器集成到神经网络架构中，作为能够学习成本项和约束的层。由此产生的端到端可训练架构共同从原始数据中提取特征，并使用最先进的整数规划求解器解决一个合适的（学习到的）组合问题。我们通过在合成数据上的广泛性能分析和在竞争性计算机视觉关键点匹配基准上的演示，展示了这种层的潜力。",
        "领域": "组合优化、计算机视觉、深度学习",
        "问题": "如何将整数规划求解器集成到神经网络中，以学习解决NP难问题的组合规范",
        "动机": "结合逻辑和算法推理与机器学习，以解决NP难问题，推动算法和机器学习领域的交叉发展",
        "方法": "将整数规划求解器作为神经网络的一层，学习成本项和约束，实现端到端训练",
        "关键词": [
            "组合优化",
            "整数规划",
            "NP难问题",
            "神经网络",
            "计算机视觉"
        ],
        "涉及的技术概念": {
            "整数规划": "在论文中用于表示和解决NP难问题的数学优化方法",
            "神经网络层": "论文中用于学习整数规划的成本项和约束的架构组成部分",
            "端到端训练": "论文中采用的训练方法，允许从原始数据直接学习到问题解决的完整流程"
        },
        "success": true
    },
    {
        "order": 191,
        "title": "Communication-Efficient Distributed Optimization with Quantized Preconditioners",
        "html": "https://ICML.cc//virtual/2021/poster/8611",
        "abstract": "We investigate fast and communication-efficient algorithms for the classic problem of minimizing a sum of strongly convex and smooth functions  that are distributed among $n$ different nodes, which can communicate using a limited number of bits. Most previous communication-efficient approaches for this problem are limited to first-order optimization, and therefore have \\emph{linear} dependence on the condition number in their communication complexity. \nWe show that this dependence is not inherent: communication-efficient methods can in fact have sublinear dependence on the condition number. \nFor this, we design and analyze the first communication-efficient distributed variants of preconditioned gradient descent for Generalized Linear Models, and for Newton's method. \nOur results rely on a new technique for quantizing both the preconditioner and the descent direction at each step of the algorithms, while controlling their convergence rate. \nWe also validate our findings experimentally, showing faster convergence and reduced communication relative to previous methods.",
        "conference": "ICML",
        "中文标题": "量化预条件子的通信高效分布式优化",
        "摘要翻译": "我们研究了针对强凸且平滑函数之和最小化这一经典问题的快速且通信高效的算法，这些函数分布在$n$个不同节点上，节点间可通过有限位数进行通信。此前大多数针对此问题的通信高效方法仅限于一阶优化，因此在通信复杂度中对条件数有线性依赖。我们表明这种依赖并非固有：通信高效方法实际上可以对条件数有次线性依赖。为此，我们设计并分析了针对广义线性模型的预条件梯度下降和牛顿方法的第一个通信高效分布式变体。我们的结果依赖于一种新技术，该技术在算法的每一步同时量化预条件子和下降方向，同时控制其收敛速率。我们还通过实验验证了我们的发现，显示出相对于以前的方法更快的收敛速度和减少的通信量。",
        "领域": "分布式优化、通信效率、预条件技术",
        "问题": "在分布式环境中，如何减少节点间通信量同时保持优化算法的快速收敛性。",
        "动机": "解决现有分布式优化方法在通信效率上的限制，特别是对条件数的线性依赖问题。",
        "方法": "设计了量化预条件子和下降方向的新技术，应用于预条件梯度下降和牛顿方法的分布式变体。",
        "关键词": [
            "分布式优化",
            "通信效率",
            "量化预条件子",
            "梯度下降",
            "牛顿方法"
        ],
        "涉及的技术概念": {
            "量化预条件子": "在算法的每一步对预条件子进行量化，以减少通信量。",
            "下降方向量化": "在优化过程中对下降方向进行量化，以控制通信成本。",
            "条件数次线性依赖": "通过新技术实现通信复杂度对条件数的次线性依赖，优于传统的线性依赖。"
        },
        "success": true
    },
    {
        "order": 192,
        "title": "Communication-Efficient Distributed SVD via Local Power Iterations",
        "html": "https://ICML.cc//virtual/2021/poster/8659",
        "abstract": "We study distributed computing of the truncated singular value decomposition (SVD).\nWe develop an algorithm that we call \\texttt{LocalPower} for improving communication efficiency.\nSpecifically, we uniformly partition the dataset among $m$ nodes and alternate between multiple (precisely $p$) local power iterations and one global aggregation.\nIn the aggregation, we propose to weight each local eigenvector matrix with orthogonal Procrustes transformation (OPT).\nAs a practical surrogate of OPT, sign-fixing, which uses a diagonal matrix with $\\pm 1$ entries as weights, has better computation complexity and stability in experiments. \nWe theoretically show that under certain assumptions \\texttt{LocalPower} lowers the required number of communications by a factor of $p$ to reach a constant accuracy.\nWe also show that the strategy of periodically decaying $p$ helps obtain high-precision solutions.\nWe conduct experiments to demonstrate the effectiveness of \\texttt{LocalPower}. ",
        "conference": "ICML",
        "中文标题": "通过本地幂迭代实现通信高效的分布式SVD",
        "摘要翻译": "我们研究了截断奇异值分解（SVD）的分布式计算。我们开发了一种名为LocalPower的算法，以提高通信效率。具体来说，我们将数据集均匀地分布在m个节点上，并在多个（精确地说是p个）本地幂迭代和一次全局聚合之间交替进行。在聚合过程中，我们提出使用正交Procrustes变换（OPT）对每个本地特征向量矩阵进行加权。作为OPT的一个实用替代方案，符号固定（使用一个对角线矩阵，其条目为±1作为权重）在实验中表现出更好的计算复杂度和稳定性。我们从理论上证明，在某些假设下，LocalPower将达到恒定精度所需的通信次数减少了p倍。我们还表明，定期衰减p的策略有助于获得高精度解。我们进行了实验以证明LocalPower的有效性。",
        "领域": "分布式计算、奇异值分解、通信优化",
        "问题": "如何在分布式环境中高效计算截断奇异值分解（SVD）",
        "动机": "减少分布式计算中截断SVD的通信开销，提高计算效率",
        "方法": "开发LocalPower算法，通过本地幂迭代和全局聚合交替进行，使用正交Procrustes变换（OPT）或其替代方案符号固定对本地特征向量矩阵进行加权",
        "关键词": [
            "分布式计算",
            "奇异值分解",
            "通信效率",
            "本地幂迭代",
            "正交Procrustes变换"
        ],
        "涉及的技术概念": {
            "本地幂迭代": "在分布式节点上进行的多次幂迭代，用于本地计算特征向量",
            "正交Procrustes变换（OPT）": "用于在全局聚合中对本地特征向量矩阵进行加权，以保持正交性",
            "符号固定": "作为OPT的替代方案，使用对角线矩阵（条目为±1）进行加权，以提高计算复杂度和稳定性"
        },
        "success": true
    },
    {
        "order": 193,
        "title": "Commutative Lie Group VAE for Disentanglement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8905",
        "abstract": "We view disentanglement learning as discovering an underlying structure that equivariantly reflects the factorized variations shown in data. Traditionally, such a structure is fixed to be a vector space with data variations represented by translations along individual latent dimensions. We argue this simple structure is suboptimal since it requires the model to learn to discard the properties (e.g. different scales of changes, different levels of abstractness) of data variations, which is an extra work than equivariance learning. Instead, we propose to encode the data variations with groups, a structure not only can equivariantly represent variations, but can also be adaptively optimized to preserve the properties of data variations. Considering it is hard to conduct training on group structures, we focus on Lie groups and adopt a parameterization using Lie algebra. Based on the parameterization, some disentanglement learning constraints are naturally derived. A simple model named Commutative Lie Group VAE is introduced to realize the group-based disentanglement learning. Experiments show that our model can effectively learn disentangled representations without supervision, and can achieve state-of-the-art performance without extra constraints.",
        "conference": "ICML",
        "中文标题": "可交换李群VAE用于解缠学习",
        "摘要翻译": "我们将解缠学习视为发现一种基础结构，该结构能够等变地反映数据中显示的因子化变化。传统上，这种结构被固定为一个向量空间，数据变化通过沿各个潜在维度的平移来表示。我们认为这种简单结构是次优的，因为它要求模型学习丢弃数据变化的属性（例如变化的不同尺度、不同层次的抽象性），这比等变学习额外增加了工作。相反，我们提出用群来编码数据变化，这种结构不仅能够等变地表示变化，还能自适应地优化以保留数据变化的属性。考虑到在群结构上进行训练较为困难，我们专注于李群，并采用李代数的参数化方法。基于这种参数化，自然地推导出一些解缠学习的约束。引入了一个名为可交换李群VAE的简单模型来实现基于群的解缠学习。实验表明，我们的模型能够有效地在无监督的情况下学习解缠表示，并且无需额外约束即可达到最先进的性能。",
        "领域": "解缠表示学习",
        "问题": "如何在无监督的情况下有效地学习解缠表示",
        "动机": "传统解缠学习方法固定使用向量空间结构，需要额外工作来丢弃数据变化的属性，这限制了模型的效率和性能。",
        "方法": "提出使用群结构来编码数据变化，专注于李群并采用李代数的参数化方法，引入可交换李群VAE模型实现解缠学习。",
        "关键词": [
            "解缠学习",
            "李群",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "解缠学习": "学习数据中独立的、可解释的变化因子",
            "李群": "一种连续群，用于编码数据变化，保留变化的属性",
            "可交换李群VAE": "基于李群的变分自编码器模型，用于实现解缠学习"
        },
        "success": true
    },
    {
        "order": 194,
        "title": "Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/10589",
        "abstract": "We focus on prediction problems with structured outputs that are subject to output validity constraints, e.g. pseudocode-to-code translation where the code must compile. While labeled input-output pairs are expensive to obtain, 'unlabeled' outputs, i.e. outputs without corresponding inputs, are freely available (e.g. code on GitHub) and provide information about output validity. Pre-training captures this structure by training a denoiser to denoise corrupted versions of unlabeled outputs. We first show that standard fine-tuning after pre-training destroys some of this structure. We then propose composed fine-tuning, which trains a predictor composed with the pre-trained denoiser. Importantly, the denoiser is fixed to preserve output structure. Like standard fine-tuning, the predictor is also initialized with the pre-trained denoiser. We prove for two-layer ReLU networks that composed fine-tuning significantly reduces the complexity of the predictor, thus improving generalization. Empirically, we show that composed fine-tuning improves over standard fine-tuning on two pseudocode-to-code translation datasets (3% and 6% relative). The improvement is magnified on out-of-distribution (OOD) examples (4% and 25% relative), suggesting that reducing predictor complexity improves OOD extrapolation.",
        "conference": "ICML",
        "中文标题": "组合微调：冻结预训练去噪自编码器以提升泛化能力",
        "摘要翻译": "我们关注于具有结构化输出且受输出有效性约束的预测问题，例如伪代码到代码的翻译，其中代码必须能够编译。虽然标记的输入-输出对获取成本高昂，但‘未标记’的输出（即没有对应输入的输出）可以自由获取（例如GitHub上的代码），并提供关于输出有效性的信息。预训练通过训练去噪器来去噪未标记输出的损坏版本，从而捕捉这种结构。我们首先展示，预训练后的标准微调会破坏部分这种结构。然后，我们提出组合微调，它训练一个与预训练去噪器组合的预测器。重要的是，去噪器被固定以保持输出结构。与标准微调一样，预测器也使用预训练的去噪器进行初始化。我们证明，对于两层ReLU网络，组合微调显著降低了预测器的复杂性，从而提高了泛化能力。实证上，我们展示了在两个伪代码到代码翻译数据集上，组合微调相对于标准微调有所改进（相对提升3%和6%）。在分布外（OOD）样本上的改进更为显著（相对提升4%和25%），这表明降低预测器复杂性有助于提高OOD外推能力。",
        "领域": "自然语言处理与视觉结合, 伪代码到代码翻译, 深度学习模型优化",
        "问题": "解决在结构化输出预测问题中，如何利用未标记数据保持输出有效性约束的同时，提高模型的泛化能力。",
        "动机": "在标记数据稀缺的情况下，利用大量未标记数据中的结构信息，避免标准微调过程中破坏预训练模型捕捉到的输出结构，从而提高模型的泛化能力和在分布外样本上的表现。",
        "方法": "提出组合微调方法，固定预训练的去噪自编码器以保持输出结构，同时训练一个与之组合的预测器，利用预训练模型初始化预测器，减少预测器的复杂性。",
        "关键词": [
            "组合微调",
            "去噪自编码器",
            "伪代码到代码翻译",
            "泛化能力",
            "分布外样本"
        ],
        "涉及的技术概念": {
            "去噪自编码器": "用于预训练阶段，通过去噪未标记输出的损坏版本来捕捉输出结构。",
            "组合微调": "固定预训练的去噪自编码器，训练一个与之组合的预测器，以保持输出结构并提高泛化能力。",
            "ReLU网络": "论文中用于理论证明的两层神经网络结构，展示组合微调如何降低预测器复杂性。"
        },
        "success": true
    },
    {
        "order": 195,
        "title": "Composing Normalizing Flows for Inverse Problems",
        "html": "https://ICML.cc//virtual/2021/poster/9461",
        "abstract": "Given an inverse problem with a normalizing flow prior, we wish to estimate the distribution of the underlying signal conditioned on the observations.  We approach this problem as a task of conditional inference on the pre-trained unconditional flow model.  We first establish that this is computationally hard for a large class of flow models.  Motivated by this, we propose a framework for approximate inference that estimates the target conditional as a composition of two flow models.  This formulation leads to a stable variational inference training procedure that avoids adversarial training.  Our method is evaluated on a variety of inverse problems and is shown to produce high-quality samples with uncertainty quantification.  We further demonstrate that our approach can be amortized for zero-shot inference.",
        "conference": "ICML",
        "中文标题": "组合归一化流解决逆问题",
        "摘要翻译": "给定一个具有归一化流先验的逆问题，我们希望估计基于观测值的底层信号的分布。我们将这个问题视为对预训练的无条件流模型进行条件推断的任务。我们首先证明这对于一大类流模型在计算上是困难的。受此启发，我们提出了一个近似推断的框架，该框架将目标条件估计为两个流模型的组合。这一表述导致了一个稳定的变分推断训练过程，避免了对抗训练。我们的方法在多种逆问题上进行了评估，结果显示能够产生具有不确定性量化能力的高质量样本。我们进一步证明，我们的方法可以用于零样本推断的摊销。",
        "领域": "逆问题求解、归一化流、变分推断",
        "问题": "估计基于观测值的底层信号的分布",
        "动机": "解决在预训练的无条件流模型上进行条件推断的计算困难问题",
        "方法": "提出一个近似推断框架，将目标条件估计为两个流模型的组合，实现稳定的变分推断训练",
        "关键词": [
            "归一化流",
            "逆问题",
            "变分推断",
            "条件推断",
            "零样本推断"
        ],
        "涉及的技术概念": {
            "归一化流": "用于建模复杂概率分布的技术，通过一系列可逆变换将简单分布转换为复杂分布",
            "变分推断": "一种近似推断方法，通过优化一个变分分布来近似真实的后验分布",
            "条件推断": "在给定某些观测值的情况下，推断其他变量的分布"
        },
        "success": true
    },
    {
        "order": 196,
        "title": "Compositional Video Synthesis with Action Graphs",
        "html": "https://ICML.cc//virtual/2021/poster/10449",
        "abstract": "Videos of actions are complex signals containing rich compositional structure in space and time. Current video generation methods lack the ability to condition the generation on multiple coordinated and potentially simultaneous timed actions. To address this challenge, we propose to represent the actions in a graph structure called Action Graph and present the new 'Action Graph To Video' synthesis task. Our generative model for this task (AG2Vid) disentangles motion and appearance features, and by incorporating a scheduling mechanism for actions facilitates a timely and coordinated video generation. We train and evaluate AG2Vid on CATER and Something-Something V2 datasets, which results in videos that have better visual quality and semantic consistency compared to baselines. Finally, our model demonstrates zero-shot abilities by synthesizing novel compositions of the learned actions.",
        "conference": "ICML",
        "中文标题": "基于动作图的组合视频合成",
        "摘要翻译": "动作视频是包含丰富时空组合结构的复杂信号。当前的视频生成方法缺乏基于多个协调且可能同时定时动作的条件生成能力。为了解决这一挑战，我们提出用称为动作图的图结构来表示动作，并提出了新的'动作图到视频'合成任务。我们为此任务设计的生成模型（AG2Vid）解耦了运动和外观特征，并通过引入动作调度机制，促进了及时且协调的视频生成。我们在CATER和Something-Something V2数据集上训练和评估了AG2Vid，结果表明，与基线相比，生成的视频具有更好的视觉质量和语义一致性。最后，我们的模型通过合成学习动作的新组合展示了零样本能力。",
        "领域": "视频生成、动作识别、时空建模",
        "问题": "当前视频生成方法难以基于多个协调且可能同时定时动作进行条件生成。",
        "动机": "为了解决视频生成中多动作协调和定时生成的挑战，提高生成视频的视觉质量和语义一致性。",
        "方法": "提出动作图结构表示动作，设计AG2Vid生成模型解耦运动和外观特征，并引入动作调度机制。",
        "关键词": [
            "动作图",
            "视频合成",
            "零样本学习",
            "时空建模",
            "生成模型"
        ],
        "涉及的技术概念": {
            "动作图": "用于表示和协调多个动作的图结构，促进视频生成中的动作调度和组合。",
            "AG2Vid": "解耦运动和外观特征的生成模型，专门设计用于从动作图合成视频。",
            "零样本能力": "模型能够合成未见过的动作组合，展示了良好的泛化能力。"
        },
        "success": true
    },
    {
        "order": 197,
        "title": "Compressed Maximum Likelihood",
        "html": "https://ICML.cc//virtual/2021/poster/8705",
        "abstract": "Maximum likelihood (ML) is one of the most fundamental and general statistical estimation techniques. Inspired by recent advances in estimating distribution functionals, we propose $\\textit{compressed maximum likelihood}$ (CML) that applies ML to the compressed samples. We then show that CML is sample-efficient for several essential learning tasks over both discrete and continuous domains, including learning densities with structures, estimating probability multisets, and inferring symmetric distribution functionals.",
        "conference": "ICML",
        "中文标题": "压缩最大似然",
        "摘要翻译": "最大似然（ML）是最基础和通用的统计估计技术之一。受到估计分布泛函最新进展的启发，我们提出了压缩最大似然（CML），它将ML应用于压缩样本。随后，我们证明了CML在离散和连续领域的几个基本学习任务中具有样本效率，包括学习具有结构的密度、估计概率多重集以及推断对称分布泛函。",
        "领域": "统计学习理论、密度估计、分布泛函估计",
        "问题": "如何在压缩样本上高效应用最大似然估计技术",
        "动机": "探索在压缩样本上应用最大似然估计的可能性，以提高在多种学习任务中的样本效率",
        "方法": "提出压缩最大似然（CML）方法，将最大似然估计应用于压缩样本，并验证其在多个学习任务中的有效性",
        "关键词": [
            "压缩最大似然",
            "样本效率",
            "密度估计",
            "分布泛函",
            "统计学习"
        ],
        "涉及的技术概念": {
            "最大似然估计": "一种统计估计技术，用于在给定样本数据的情况下估计模型参数",
            "压缩样本": "经过压缩处理的样本数据，旨在减少数据量同时保留关键信息",
            "分布泛函": "描述概率分布特性的函数，CML方法用于有效估计这些泛函"
        },
        "success": true
    },
    {
        "order": 198,
        "title": "Concentric mixtures of Mallows models for top-$k$ rankings: sampling and identifiability",
        "html": "https://ICML.cc//virtual/2021/poster/8509",
        "abstract": "In this paper, we study mixtures of two Mallows models for top-$k$ rankings with equal location parameters but with different scale parameters (a mixture of concentric Mallows models). These models arise when we have a heterogeneous population of voters formed by two populations, one of which is a subpopulation of expert voters. We show the identifiability of both components and the learnability of their respective parameters. These results are based upon, first, bounding the sample complexity for the Borda algorithm with top-$k$ rankings. Second, we characterize the distances between rankings, showing that an off-the-shelf clustering algorithm separates the rankings by components with high probability -provided the scales are well-separated.As a by-product, we include an efficient sampling algorithm for Mallows top-$k$ rankings. Finally, since the rank aggregation will suffer from a large amount of noise introduced by the non-expert voters, we adapt the Borda algorithm to be able to recover the ground truth consensus ranking which is especially consistent with the expert rankings. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "同心Mallows模型对top-k排名的混合：采样与可识别性",
        "摘要翻译": "本文研究了两种Mallows模型对于top-k排名的混合，这些模型具有相同的位置参数但不同的尺度参数（同心Mallows模型的混合）。这些模型出现在我们有一个由两个群体组成的异质选民群体时，其中一个群体是专家选民子群体。我们展示了两个组成部分的可识别性以及它们各自参数的可学习性。这些结果基于，首先，为带有top-k排名的Borda算法界定样本复杂度。其次，我们描述了排名之间的距离特征，表明一个现成的聚类算法能够以高概率通过组成部分分离排名——前提是尺度被良好分离。作为副产品，我们包括了一个高效的Mallows top-k排名采样算法。最后，由于排名聚合将受到非专家选民引入的大量噪声的影响，我们调整了Borda算法，使其能够恢复特别与专家排名一致的地面真实共识排名。",
        "领域": "排名学习、统计模型、数据聚类",
        "问题": "研究在异质选民群体中，如何识别和学习由专家和非专家选民组成的同心Mallows模型的参数，以及如何有效地采样和聚合排名。",
        "动机": "为了解决在存在大量非专家选民噪声的情况下，如何准确识别专家意见并恢复共识排名的问题。",
        "方法": "通过界定Borda算法的样本复杂度，描述排名间距离特征，并使用聚类算法分离排名，同时开发高效的采样算法和调整Borda算法以恢复共识排名。",
        "关键词": [
            "Mallows模型",
            "top-k排名",
            "Borda算法",
            "样本复杂度",
            "共识排名"
        ],
        "涉及的技术概念": {
            "Mallows模型": "用于建模排名的概率模型，本文中用于描述专家和非专家选民的排名行为。",
            "Borda算法": "一种排名聚合方法，本文中用于处理top-k排名并界定其样本复杂度。",
            "样本复杂度": "指学习算法为了达到一定的性能水平所需的样本数量，本文中用于分析Borda算法在top-k排名中的应用。"
        }
    },
    {
        "order": 199,
        "title": "Conditional Distributional Treatment Effect with Kernel Conditional Mean Embeddings and U-Statistic Regression",
        "html": "https://ICML.cc//virtual/2021/poster/8681",
        "abstract": "We propose to analyse the conditional distributional treatment effect (CoDiTE), which, in contrast to the more common conditional average treatment effect (CATE), is designed to encode a treatment's distributional aspects beyond the mean. We first introduce a formal definition of the CoDiTE associated with a distance function between probability measures. Then we discuss the CoDiTE associated with the maximum mean discrepancy via kernel conditional mean embeddings, which, coupled with a hypothesis test, tells us whether there is any conditional distributional effect of the treatment. Finally, we investigate what kind of conditional distributional effect the treatment has, both in an exploratory manner via the conditional witness function, and in a quantitative manner via U-statistic regression, generalising the CATE to higher-order moments. Experiments on synthetic, semi-synthetic and real datasets demonstrate the merits of our approach.",
        "conference": "ICML",
        "中文标题": "基于核条件均值嵌入与U统计量回归的条件分布处理效应分析",
        "摘要翻译": "我们提出分析条件分布处理效应（CoDiTE），与更常见的条件平均处理效应（CATE）相比，CoDiTE旨在编码超出均值的处理分布特性。我们首先引入了与概率测度间距离函数相关的CoDiTE的正式定义。然后，我们讨论了通过核条件均值嵌入与最大均值差异相关的CoDiTE，结合假设检验，告诉我们处理是否存在任何条件分布效应。最后，我们以探索性方式通过条件见证函数和定量方式通过U统计量回归，研究了处理具有何种条件分布效应，将CATE推广到高阶矩。在合成、半合成和真实数据集上的实验证明了我们方法的优点。",
        "领域": "因果推断、机器学习统计方法、核方法",
        "问题": "如何分析和量化超出均值的处理分布特性，即条件分布处理效应（CoDiTE）。",
        "动机": "为了更全面地理解处理效应，不仅关注平均效应，还要探索和量化处理对结果变量分布的更广泛影响。",
        "方法": "通过核条件均值嵌入和最大均值差异定义CoDiTE，结合假设检验判断处理的条件分布效应，使用条件见证函数和U统计量回归探索和量化这种效应。",
        "关键词": [
            "条件分布处理效应",
            "核条件均值嵌入",
            "U统计量回归",
            "最大均值差异",
            "因果推断"
        ],
        "涉及的技术概念": {
            "核条件均值嵌入": "用于在再生核希尔伯特空间中表示条件分布，便于处理效应的分析和比较。",
            "U统计量回归": "一种推广CATE到高阶矩的方法，用于量化处理的条件分布效应。",
            "最大均值差异": "用于衡量两个概率分布之间的差异，是定义和检验CoDiTE的基础。"
        },
        "success": true
    },
    {
        "order": 200,
        "title": "Conditional Temporal Neural Processes with Covariance Loss",
        "html": "https://ICML.cc//virtual/2021/poster/10515",
        "abstract": "We introduce a novel loss function, Covariance Loss, which is conceptually equivalent to conditional neural processes and has a form of regularization so that is applicable to many kinds of neural networks. With the proposed loss, mappings from input variables to target variables are highly affected by dependencies of target variables as well as mean activation and mean dependencies of input and target variables. This nature enables the resulting neural networks to become more robust to noisy observations and recapture missing dependencies from prior information. In order to show the validity of the proposed loss, we conduct extensive sets of experiments on real-world datasets with state-of-the-art models and discuss the benefits and drawbacks of the proposed Covariance Loss.",
        "conference": "ICML",
        "中文标题": "条件时序神经过程与协方差损失",
        "摘要翻译": "我们引入了一种新颖的损失函数——协方差损失，它在概念上等同于条件神经过程，并具有正则化的形式，因此适用于多种神经网络。通过提出的损失函数，从输入变量到目标变量的映射高度依赖于目标变量的依赖性，以及输入和目标变量的平均激活和平均依赖性。这一特性使得最终的神经网络对噪声观测更加鲁棒，并能够从先验信息中重新捕捉缺失的依赖性。为了展示所提出损失函数的有效性，我们在真实世界的数据集上使用最先进的模型进行了广泛的实验，并讨论了所提出的协方差损失的优点和缺点。",
        "领域": "深度学习",
        "问题": "如何提高神经网络对噪声观测的鲁棒性并重新捕捉缺失的依赖性",
        "动机": "开发一种新的损失函数，以增强神经网络在处理噪声数据和缺失依赖性时的性能",
        "方法": "引入协方差损失作为一种新的损失函数，用于正则化神经网络，提高其对噪声的鲁棒性和依赖性捕捉能力",
        "关键词": [
            "协方差损失",
            "条件神经过程",
            "正则化",
            "噪声鲁棒性",
            "依赖性捕捉"
        ],
        "涉及的技术概念": {
            "协方差损失": "一种新颖的损失函数，用于正则化神经网络，提高其对噪声的鲁棒性和依赖性捕捉能力",
            "条件神经过程": "一种能够根据条件输入调整其预测的神经过程模型",
            "正则化": "在损失函数中添加额外项以防止过拟合，提高模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 201,
        "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech",
        "html": "https://ICML.cc//virtual/2021/poster/9245",
        "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.",
        "conference": "ICML",
        "中文标题": "条件变分自编码器与对抗学习相结合的端到端文本到语音转换",
        "摘要翻译": "最近提出的几种端到端文本到语音（TTS）模型支持单阶段训练和并行采样，但其样本质量无法与两阶段TTS系统相媲美。在这项工作中，我们提出了一种并行端到端TTS方法，生成的音频比当前的两阶段模型听起来更加自然。我们的方法采用了变分推断增强的正规化流和对抗训练过程，从而提高了生成模型的表达能力。我们还提出了一种随机持续时间预测器，用于从输入文本合成具有多样节奏的语音。通过对潜在变量的不确定性建模和随机持续时间预测器，我们的方法表达了自然的一对多关系，即一个文本输入可以以不同的音高和节奏以多种方式说出。在LJ Speech（一个单说话者数据集）上进行的主观人类评估（平均意见分数，或MOS）显示，我们的方法优于公开可用的最佳TTS系统，并达到了与真实录音相当的MOS。",
        "领域": "语音合成, 生成对抗网络, 变分自编码器",
        "问题": "提高端到端文本到语音转换系统的音频质量和自然度",
        "动机": "解决当前端到端TTS模型在音频质量和自然度上不及两阶段系统的问题",
        "方法": "采用变分推断增强的正规化流和对抗训练过程，结合随机持续时间预测器",
        "关键词": [
            "文本到语音",
            "变分自编码器",
            "对抗学习",
            "正规化流",
            "随机持续时间预测"
        ],
        "涉及的技术概念": {
            "条件变分自编码器": "用于建模文本到语音转换中的潜在变量分布，提高生成音频的自然度",
            "对抗学习": "通过对抗训练过程增强生成模型的表达能力，提升音频质量",
            "随机持续时间预测器": "用于从输入文本合成具有多样节奏的语音，增强语音的自然度和表现力"
        },
        "success": true
    },
    {
        "order": 202,
        "title": "Confidence-Budget Matching for Sequential Budgeted Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9103",
        "abstract": "A core element in decision-making under uncertainty is the feedback on the quality of the performed actions. However, in many applications, such feedback is restricted. For example, in recommendation systems, repeatedly asking the user to provide feedback on the quality of recommendations will annoy them. In this work, we formalize decision-making problems with querying budget, where there is a (possibly time-dependent) hard limit on the number of reward queries allowed. Specifically, we focus on multi-armed bandits, linear contextual bandits, and reinforcement learning problems. We start by analyzing the performance of `greedy' algorithms that query a reward whenever they can. We show that in fully stochastic settings, doing so performs surprisingly well, but in the presence of any adversity, this might lead to linear regret. To overcome this issue, we propose the Confidence-Budget Matching (CBM) principle that queries rewards when the confidence intervals are wider than the inverse square root of the available budget. We analyze the performance of CBM based algorithms in different settings and show that it performs well in the presence of adversity in the contexts, initial states, and budgets.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "序列预算学习中的置信度-预算匹配",
        "摘要翻译": "在不确定性下的决策过程中，一个核心要素是对所执行行动质量的反馈。然而，在许多应用中，这种反馈是受限的。例如，在推荐系统中，反复要求用户提供关于推荐质量的反馈会让他们感到厌烦。在这项工作中，我们形式化了带有查询预算的决策问题，其中对允许的奖励查询次数有一个（可能是时间依赖的）硬性限制。具体来说，我们关注于多臂老虎机、线性上下文老虎机和强化学习问题。我们首先分析了‘贪婪’算法的性能，这些算法在可能的情况下查询奖励。我们表明，在完全随机的设置中，这样做表现惊人地好，但在存在任何逆境的情况下，这可能导致线性遗憾。为了克服这个问题，我们提出了置信度-预算匹配（CBM）原则，当置信区间比可用预算的平方根的倒数更宽时，查询奖励。我们分析了基于CBM的算法在不同设置下的性能，并表明它在上下文、初始状态和预算存在逆境的情况下表现良好。",
        "领域": "推荐系统, 多臂老虎机问题, 强化学习",
        "问题": "在查询预算受限的情况下，如何有效地进行决策以最大化奖励反馈",
        "动机": "解决在反馈受限环境下，传统贪婪算法可能导致线性遗憾的问题",
        "方法": "提出置信度-预算匹配（CBM）原则，根据置信区间和可用预算的关系决定何时查询奖励",
        "关键词": [
            "置信度-预算匹配",
            "多臂老虎机",
            "线性上下文老虎机",
            "强化学习",
            "决策制定"
        ],
        "涉及的技术概念": {
            "置信度-预算匹配（CBM）": "一种根据置信区间和可用预算决定查询奖励时机的原则，旨在优化决策过程",
            "多臂老虎机": "一种决策制定框架，用于在不确定性下选择最优行动以最大化奖励",
            "线性上下文老虎机": "在多臂老虎机问题中引入上下文信息，以更精确地预测行动奖励"
        }
    },
    {
        "order": 203,
        "title": "Confidence Scores Make Instance-dependent Label-noise Learning Possible",
        "html": "https://ICML.cc//virtual/2021/poster/10245",
        "abstract": "In learning with noisy labels, for every instance, its label can randomly walk to other classes following a transition distribution which is named a noise model. Well-studied noise models are all instance-independent, namely, the transition depends only on the original label but not the instance itself, and thus they are less practical in the wild. Fortunately, methods based on instance-dependent noise have been studied, but most of them have to rely on strong assumptions on the noise models. To alleviate this issue, we introduce confidence-scored instance-dependent noise (CSIDN), where each instance-label pair is equipped with a confidence score. We find that with the help of confidence scores, the transition distribution of each instance can be approximately estimated. Similarly to the powerful forward correction for instance-independent noise, we propose a novel instance-level forward correction for CSIDN. We demonstrate the utility and effectiveness of our method through multiple experiments on datasets with synthetic label noise and real-world unknown noise.",
        "conference": "ICML",
        "中文标题": "置信度评分使实例依赖的标签噪声学习成为可能",
        "摘要翻译": "在带有噪声标签的学习中，对于每一个实例，其标签可以按照一个被称为噪声模型的转移分布随机跳转到其他类别。已被深入研究的噪声模型都是实例无关的，即转移仅依赖于原始标签而非实例本身，因此它们在现实中的实用性较低。幸运的是，基于实例依赖噪声的方法已被研究，但大多数方法不得不依赖于对噪声模型的强假设。为了缓解这一问题，我们引入了带有置信度评分的实例依赖噪声（CSIDN），其中每个实例-标签对都配备了一个置信度评分。我们发现，在置信度评分的帮助下，可以近似估计每个实例的转移分布。类似于实例无关噪声中强大的前向校正，我们为CSIDN提出了一种新颖的实例级前向校正。通过在带有合成标签噪声和真实世界未知噪声的数据集上进行多次实验，我们证明了我们方法的实用性和有效性。",
        "领域": "噪声标签学习、深度学习鲁棒性、实例依赖噪声建模",
        "问题": "解决实例依赖噪声标签学习中的噪声模型估计问题",
        "动机": "现有的噪声标签学习方法大多假设噪声模型是实例无关的，这在现实中限制了方法的实用性。",
        "方法": "引入带有置信度评分的实例依赖噪声（CSIDN），并提出实例级前向校正方法来估计和校正噪声。",
        "关键词": [
            "置信度评分",
            "实例依赖噪声",
            "前向校正",
            "噪声标签学习",
            "深度学习鲁棒性"
        ],
        "涉及的技术概念": {
            "置信度评分": "用于量化实例-标签对的可靠性，帮助估计实例依赖的噪声转移分布。",
            "实例依赖噪声": "噪声转移分布依赖于实例本身，而不仅仅是标签，更贴近现实场景。",
            "前向校正": "一种校正噪声标签的方法，通过估计噪声转移矩阵来调整模型预测，提高学习效果。"
        },
        "success": true
    },
    {
        "order": 204,
        "title": "Conformal prediction interval for dynamic time-series",
        "html": "https://ICML.cc//virtual/2021/poster/10469",
        "abstract": "We develop a method to construct distribution-free prediction intervals for dynamic time-series, called \\Verb|EnbPI| that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. \\Verb|EnbPI| is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, \\textit{approximately valid} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, \\Verb|EnbPI| avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, \\Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.",
        "conference": "ICML",
        "中文标题": "动态时间序列的共形预测区间",
        "摘要翻译": "我们开发了一种名为EnbPI的方法，用于为动态时间序列构建无分布预测区间，该方法可以围绕任何自举集成估计器构建顺序预测区间。EnbPI与共形预测（CP）框架密切相关，但不需要数据可交换性。理论上，这些区间对于广泛的回归函数类和具有强混合随机误差的时间序列实现了有限样本、近似有效的边际覆盖。在计算上，EnbPI避免了过拟合，既不需要数据分割，也不需要训练多个集成估计器；它有效地聚合了已经训练好的自举估计器。总的来说，EnbPI易于实现，能够扩展到顺序产生任意多的预测区间，并且适用于广泛的回归函数。我们进行了广泛的实际数据分析以证明其有效性。",
        "领域": "时间序列预测、机器学习应用、统计学习",
        "问题": "构建无分布预测区间以适应动态时间序列的预测需求",
        "动机": "为了解决动态时间序列预测中传统方法需要数据可交换性和计算效率低下的问题",
        "方法": "开发了一种名为EnbPI的方法，该方法围绕自举集成估计器构建顺序预测区间，无需数据可交换性，避免了过拟合和数据分割",
        "关键词": [
            "共形预测",
            "动态时间序列",
            "自举集成",
            "预测区间",
            "无分布预测"
        ],
        "涉及的技术概念": {
            "共形预测（CP）": "一种预测框架，用于构建具有统计保证的预测区间，EnbPI与之相关但不需要数据可交换性",
            "自举集成估计器": "通过自举样本训练的多个估计器的集合，EnbPI利用这些估计器构建预测区间",
            "强混合随机误差": "时间序列中的误差项具有强混合性质，EnbPI能够适应这种性质以构建有效的预测区间"
        },
        "success": true
    },
    {
        "order": 205,
        "title": "Conjugate Energy-Based Models",
        "html": "https://ICML.cc//virtual/2021/poster/10361",
        "abstract": "In this paper, we propose conjugate energy-based models (CEBMs), a new class of energy-based models that define a joint density over data and latent variables. The joint density of a CEBM decomposes into an intractable distribution over data and a tractable posterior over latent variables. CEBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping from data to latent variables. However, these models omit a generator network, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that conjugate EBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-domain detection on a variety of datasets. ",
        "conference": "ICML",
        "中文标题": "共轭能量基模型",
        "摘要翻译": "在本文中，我们提出了共轭能量基模型（CEBMs），这是一类新的能量基模型，定义了数据和潜在变量的联合密度。CEBMs的联合密度分解为数据上的难处理分布和潜在变量上的易处理后验。CEBMs与变分自编码器有相似的使用案例，因为它们学习从数据到潜在变量的无监督映射。然而，这些模型省略了生成器网络，这使得它们能够学习数据点之间更灵活的相似性概念。我们的实验表明，共轭EBMs在各种数据集上的图像建模、潜在空间的预测能力和域外检测方面取得了竞争性的结果。",
        "领域": "无监督学习, 生成模型, 图像建模",
        "问题": "如何在无监督学习框架下更灵活地定义数据点之间的相似性",
        "动机": "探索一种新型的能量基模型，以克服现有方法在定义数据相似性方面的限制",
        "方法": "提出共轭能量基模型（CEBMs），通过省略生成器网络来学习更灵活的数据相似性概念",
        "关键词": [
            "共轭能量基模型",
            "无监督学习",
            "图像建模",
            "潜在变量",
            "域外检测"
        ],
        "涉及的技术概念": {
            "共轭能量基模型": "一种新型的能量基模型，定义了数据和潜在变量的联合密度",
            "无监督映射": "从数据到潜在变量的学习过程，无需标注数据",
            "潜在空间预测能力": "模型在潜在空间中预测数据特性的能力"
        },
        "success": true
    },
    {
        "order": 206,
        "title": "Connecting Interpretability and Robustness in Decision Trees through Separation",
        "html": "https://ICML.cc//virtual/2021/poster/10107",
        "abstract": "Recent research has recognized interpretability and robustness as essential properties of trustworthy classification. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Specifically, we focus on interpretation using decision trees and robustness to l_{\\infty}-perturbation. Previous works defined the notion of r-separation as a sufficient condition for robustness. We prove upper and lower bounds on the tree size in case  the data is r-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the first algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments confirm that our algorithm yields classifiers that are both interpretable and robust and have high accuracy. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过分离连接决策树中的可解释性和鲁棒性",
        "摘要翻译": "最近的研究已经认识到可解释性和鲁棒性是值得信赖的分类的重要属性。有趣的是，在经验上观察到鲁棒性和可解释性之间存在联系，但其背后的理论推理仍然难以捉摸。在本文中，我们严格研究这种联系。具体来说，我们专注于使用决策树进行解释以及对l_{\\infty}-扰动的鲁棒性。先前的工作将r-分离的概念定义为鲁棒性的充分条件。我们证明了在数据被r-分离的情况下，树大小的上限和下限。然后，我们证明了当数据线性分离时，可以获得更严格的大小界限。我们提供了第一个在决策树的上下文中，在鲁棒性、可解释性和准确性方面都具有可证明保证的算法。实验证实，我们的算法产生的分类器既可解释又鲁棒，并且具有很高的准确性。",
        "领域": "对抗攻击与防御、决策树、可解释机器学习",
        "问题": "缺乏对决策树模型中可解释性与鲁棒性之间联系的理论理解，以及如何构建同时具备高可解释性和鲁棒性的决策树模型。",
        "动机": "以往研究观察到决策树的可解释性和鲁棒性之间存在经验联系，但缺乏理论支撑。本研究旨在从理论上深入探讨并证明这种联系，并设计出能够同时保证决策树可解释性、鲁棒性和准确性的算法。",
        "方法": "通过理论分析，研究数据分离性（r-分离和线性分离）与决策树大小之间的关系，推导出树大小的上下界。提出了一种新的算法，该算法在决策树的构建过程中能够保证鲁棒性、可解释性和准确性，并通过实验验证算法的有效性。",
        "关键词": [
            "决策树",
            "可解释性",
            "鲁棒性",
            "对抗扰动",
            "分离性"
        ],
        "涉及的技术概念": {
            "可解释性": "指模型决策过程的透明度和易于理解的程度，本论文中通过限制决策树的复杂度（树的大小）来保证可解释性。"
        }
    },
    {
        "order": 207,
        "title": "Connecting Optimal Ex-Ante Collusion in Teams to Extensive-Form Correlation: Faster Algorithms and Positive Complexity Results",
        "html": "https://ICML.cc//virtual/2021/poster/10461",
        "abstract": "We focus on the problem of finding an optimal strategy for a team of players that faces an opponent in an imperfect-information zero-sum extensive-form game. Team members are not allowed to communicate during play but can coordinate before the game.  In this setting, it is known that the best the team can do is sample a profile of potentially randomized strategies (one per player) from a joint (a.k.a. correlated) probability distribution at the beginning of the game. \nIn this paper, we first provide new modeling results about computing such an optimal distribution by drawing a connection to a different literature on extensive-form correlation. \nSecond, we provide an algorithm that allows one for capping the number of profiles employed in the solution. This begets an anytime algorithm by increasing the cap. We find that often a handful of well-chosen such profiles suffices to reach optimal utility for the team. This\nenables team members to reach coordination through a simple and understandable plan. \nFinally, inspired by this observation and leveraging theoretical concepts that we introduce, we develop an efficient column-generation algorithm for finding an optimal distribution for the team. We evaluate it on a suite of common benchmark games. It is three orders of magnitude faster than the prior state of the art on games that the latter can solve and it can also solve several games that were previously unsolvable.",
        "conference": "ICML",
        "中文标题": "将团队中的最优事前合谋与扩展形式关联联系起来：更快的算法与积极复杂性结果",
        "摘要翻译": "我们关注于在一个不完美信息的零和扩展形式游戏中，为一支面对对手的玩家团队寻找最优策略的问题。团队成员在游戏过程中不允许交流，但可以在游戏前进行协调。在这种设置下，已知团队能做的最好的事情是在游戏开始时从一个联合（也称为相关）概率分布中采样一个可能随机化的策略配置（每位玩家一个）。在本文中，我们首先通过将计算这种最优分布与扩展形式关联的不同文献联系起来，提供了新的建模结果。其次，我们提供了一种算法，允许限制解决方案中使用的配置数量。通过增加上限，这产生了一个随时可用的算法。我们发现，通常少数精心选择的配置就足以让团队达到最优效用。这使得团队成员能够通过一个简单易懂的计划达到协调。最后，受这一观察的启发，并利用我们引入的理论概念，我们开发了一种高效的列生成算法，用于为团队寻找最优分布。我们在一系列常见的基准游戏上对其进行了评估。它在后者可以解决的游戏上比现有技术快三个数量级，并且还可以解决几个以前无法解决的游戏。",
        "领域": "博弈论、算法设计、团队协作策略",
        "问题": "在不完美信息的零和扩展形式游戏中，为一支玩家团队寻找最优策略的问题",
        "动机": "研究如何在不允许团队成员在游戏过程中交流的情况下，通过事前协调找到最优策略",
        "方法": "通过将问题与扩展形式关联的文献联系起来提供新的建模结果，开发限制策略配置数量的算法，并引入高效的列生成算法",
        "关键词": [
            "扩展形式游戏",
            "团队协作",
            "列生成算法",
            "最优策略",
            "事前协调"
        ],
        "涉及的技术概念": {
            "扩展形式关联": "用于描述团队成员在游戏前通过联合概率分布协调策略的概念",
            "列生成算法": "用于高效寻找团队最优策略分布的算法",
            "事前协调": "团队成员在游戏开始前通过联合策略配置进行协调的过程"
        },
        "success": true
    },
    {
        "order": 208,
        "title": "Connecting Sphere Manifolds Hierarchically for Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/10545",
        "abstract": "This paper considers classification problems with hierarchically organized classes. We force the classifier (hyperplane) of each class to belong to a sphere manifold, whose center is the classifier of its super-class. Then, individual sphere manifolds are connected based on their hierarchical relations. Our technique replaces the last layer of a neural network by combining a spherical fully-connected layer with a hierarchical layer. This regularization is shown to improve the performance of widely used deep neural network architectures (ResNet and DenseNet) on publicly available datasets (CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).",
        "conference": "ICML",
        "中文标题": "层次化连接球面流形以实现正则化",
        "摘要翻译": "本文考虑了具有层次化组织类别的分类问题。我们强制每个类别（超平面）的分类器属于一个球面流形，其中心是其超类别的分类器。然后，根据它们的层次关系连接各个球面流形。我们的技术通过结合球形全连接层和层次化层来替换神经网络的最后一层。这种正则化方法被证明能够提高广泛使用的深度神经网络架构（ResNet和DenseNet）在公开可用数据集（CIFAR100、CUB200、斯坦福狗、斯坦福汽车和Tiny-ImageNet）上的性能。",
        "领域": "图像分类、深度学习正则化、层次化学习",
        "问题": "解决在具有层次化组织类别的分类问题中，如何通过正则化方法提高模型性能的问题。",
        "动机": "通过利用类别之间的层次关系，引入球面流形作为正则化手段，以提升分类器的性能和泛化能力。",
        "方法": "将神经网络的最后一层替换为结合球形全连接层和层次化层的结构，强制分类器遵循层次化组织的球面流形约束。",
        "关键词": [
            "层次化分类",
            "球面流形",
            "正则化",
            "深度神经网络",
            "图像分类"
        ],
        "涉及的技术概念": {
            "球面流形": "用于约束分类器（超平面）的空间，使其属于以超类别分类器为中心的球面，以利用类别间的层次关系。",
            "球形全连接层": "神经网络的一种层结构，用于实现分类器在球面流形上的约束，替代传统的全连接层。",
            "层次化层": "一种网络层结构，用于明确地建模和处理类别之间的层次关系，以提升分类性能。"
        },
        "success": true
    },
    {
        "order": 209,
        "title": "Consensus Control for Decentralized Deep Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10027",
        "abstract": "Decentralized training of deep learning models enables on-device learning over networks, as well as efficient scaling to large compute clusters. Experiments in earlier works reveal that, even in a data-center setup, decentralized training often suffers from the degradation in the quality of the model: the training and test performance of models trained in a decentralized fashion is in general worse than that of models trained in a centralized fashion, and this performance drop is impacted by parameters such as network size, communication topology and data partitioning.\nWe identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. We show in theory that when the training consensus distance is lower than a critical quantity, decentralized training converges as fast as the centralized counterpart. We empirically validate that the relation between generalization performance and consensus distance is consistent with this theoretical observation. Our empirical insights allow the principled design of better decentralized training schemes that mitigate the performance drop. To this end, we provide practical training guidelines and exemplify its effectiveness on the data-center setup as the important first step.",
        "conference": "ICML",
        "中文标题": "去中心化深度学习的共识控制",
        "摘要翻译": "深度学习的去中心化训练支持网络上的设备端学习，以及高效扩展到大型计算集群。早期工作的实验表明，即使在数据中心设置中，去中心化训练也常常遭受模型质量下降的问题：以去中心化方式训练的模型的训练和测试性能通常比以中心化方式训练的模型差，并且这种性能下降受到网络规模、通信拓扑和数据分区等参数的影响。我们识别出设备间变化的共识距离作为解释中心化与去中心化训练之间差距的关键参数。我们在理论上表明，当训练共识距离低于一个临界量时，去中心化训练的收敛速度与中心化训练相同。我们实证验证了泛化性能与共识距离之间的关系与这一理论观察一致。我们的实证见解允许设计更好的去中心化训练方案，以减轻性能下降。为此，我们提供了实用的训练指南，并以数据中心设置为重要第一步，举例说明了其有效性。",
        "领域": "分布式深度学习、模型训练优化、网络通信优化",
        "问题": "去中心化深度学习训练中模型性能下降的问题",
        "动机": "探索去中心化训练与中心化训练性能差距的原因，并提出改进方案",
        "方法": "通过理论分析和实证研究，识别共识距离对训练性能的影响，并设计改进的去中心化训练方案",
        "关键词": [
            "去中心化训练",
            "共识距离",
            "模型性能优化",
            "分布式学习",
            "通信拓扑"
        ],
        "涉及的技术概念": {
            "共识距离": "设备间在训练过程中的参数差异，影响去中心化训练的性能",
            "去中心化训练": "在多个设备上分散进行模型训练，不依赖中心服务器",
            "泛化性能": "模型在未见数据上的表现，与共识距离密切相关"
        },
        "success": true
    },
    {
        "order": 210,
        "title": "Conservative Objective Models for Effective Offline Model-Based Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9385",
        "abstract": "In this paper, we aim to solve data-driven model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function provided access to only a static dataset of inputs and their corresponding objective values. Such data-driven optimization procedures are the only practical methods in many real-world domains where active data collection is expensive (e.g., when optimizing over proteins) or dangerous (e.g., when optimizing over aircraft designs, actively evaluating malformed aircraft designs is unsafe). Typical methods for MBO that optimize the input against a learned model of the unknown score function are affected by erroneous overestimation in the learned model caused due to distributional shift, that drives the optimizer to low-scoring or invalid inputs. To overcome this, we propose conservative objective models (COMs), a method that learns a model of the objective function which lower bounds the actual value of the ground-truth objective on out-of-distribution inputs and uses it for optimization. In practice, COMs outperform a number existing methods on a wide range of MBO problems, including optimizing controller parameters, robot morphologies, and superconducting materials. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "有效离线模型优化的保守目标模型",
        "摘要翻译": "在本文中，我们旨在解决数据驱动的基于模型的优化（MBO）问题，其目标是在仅能访问输入及其对应目标值的静态数据集的情况下，找到一个设计输入，以最大化未知的目标函数。在许多实际领域中，这种数据驱动的优化程序是唯一可行的方法，其中主动数据收集成本高昂（例如，在优化蛋白质时）或危险（例如，在优化飞机设计时，主动评估畸形飞机设计是不安全的）。典型的MBO方法针对未知评分函数的学习模型优化输入，会受到由于分布偏移引起的模型错误高估的影响，这会驱使优化器趋向于低分或无效的输入。为了克服这一点，我们提出了保守目标模型（COMs），这是一种学习目标函数模型的方法，该模型在分布外输入上对真实目标值的实际值进行下限估计，并用于优化。实际上，COMs在广泛的MBO问题上优于许多现有方法，包括优化控制器参数、机器人形态和超导材料。",
        "领域": "基于模型的优化, 机器学习应用, 自动化设计",
        "问题": "解决在仅能访问静态数据集的情况下，如何有效优化设计输入以最大化未知目标函数的问题。",
        "动机": "在主动数据收集成本高昂或危险的领域中，提供一种有效的数据驱动优化方法。",
        "方法": "提出保守目标模型（COMs），通过学习一个在分布外输入上对真实目标值进行下限估计的目标函数模型，用于优化。",
        "关键词": [
            "保守目标模型",
            "基于模型的优化",
            "数据驱动优化",
            "分布偏移",
            "自动化设计"
        ],
        "涉及的技术概念": {
            "保守目标模型（COMs）": "一种学习目标函数模型的方法，该模型在分布外输入上对真实目标值的实际值进行下限估计，并用于优化。",
            "基于模型的优化（MBO）": "一种优化方法，通过模型来预测和优化设计输入，以最大化目标函数。",
            "分布偏移": "指模型在面对与训练数据分布不同的输入时，预测性能下降的现象。"
        }
    },
    {
        "order": 211,
        "title": "Consistent Nonparametric Methods for Network Assisted Covariate Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9083",
        "abstract": "Networks with node covariates are commonplace: for example, people in a social network have interests, or product preferences, etc. If we know the covariates for some nodes, can we infer them for the remaining nodes? In this paper we propose a new similarity measure between two nodes based on the patterns of their 2-hop neighborhoods. We show that a simple algorithm (CN-VEC)  like nearest neighbor regression with this metric  is consistent for a wide range of models when the degree grows faster than $n^{1/3}$ up-to logarithmic factors, where $n$ is the  number of nodes. For 'low-rank' latent variable models, the natural contender will be to estimate the latent variables using SVD and use them for non-parametric regression. While we show consistency of this method under less stringent sparsity conditions, our experimental results  suggest that the simple local CN-VEC method either outperforms the global SVD-RBF method, or has comparable performance for low rank models. We also present simulated and real data experiments to show the effectiveness of our algorithms compared to the state of the art.",
        "conference": "ICML",
        "中文标题": "网络辅助协变量估计的一致性非参数方法",
        "摘要翻译": "带有节点协变量的网络非常常见：例如，社交网络中的人有兴趣或产品偏好等。如果我们知道某些节点的协变量，能否推断出其余节点的协变量？在本文中，我们提出了一种基于两跳邻域模式的新节点相似性度量。我们展示了一个简单算法（CN-VEC），如使用此度量的最近邻回归，在度数增长快于$n^{1/3}$（对数因子以内）时，对于广泛的模型是一致的，其中$n$是节点数量。对于'低秩'潜在变量模型，自然的竞争者将是使用SVD估计潜在变量并将其用于非参数回归。虽然我们在较不严格的稀疏性条件下展示了这种方法的一致性，但我们的实验结果表明，简单的局部CN-VEC方法要么优于全局SVD-RBF方法，要么在低秩模型上具有可比性能。我们还展示了模拟和真实数据实验，以显示我们的算法与现有技术相比的有效性。",
        "领域": "社交网络分析、非参数统计、图数据挖掘",
        "问题": "如何在已知部分节点协变量的情况下，推断网络中其余节点的协变量",
        "动机": "解决网络中节点协变量推断的问题，特别是在部分节点协变量已知的情况下，如何有效推断未知节点的协变量",
        "方法": "提出了一种基于两跳邻域模式的节点相似性度量，并开发了一个简单算法CN-VEC进行协变量估计",
        "关键词": [
            "节点相似性度量",
            "非参数回归",
            "社交网络分析",
            "协变量估计",
            "图数据挖掘"
        ],
        "涉及的技术概念": {
            "两跳邻域模式": "用于定义节点之间的相似性度量，基于节点两跳范围内的邻域结构",
            "CN-VEC算法": "一种基于最近邻回归的简单算法，用于节点协变量的估计",
            "SVD-RBF方法": "一种通过奇异值分解(SVD)估计潜在变量，并用于非参数回归的全局方法"
        },
        "success": true
    },
    {
        "order": 212,
        "title": "Consistent regression when oblivious outliers overwhelm",
        "html": "https://ICML.cc//virtual/2021/poster/10579",
        "abstract": "We consider a robust linear regression model $y=X\\beta^* + \\eta$, where an adversary oblivious to the design $X\\in \\mathbb{R}^{n\\times d}$ may choose $\\eta$ to corrupt all but an $\\alpha$ fraction of the observations $y$ in an arbitrary way.\nPrior to our work, even for Gaussian $X$, no estimator for $\\beta^*$ was known to be consistent in this model except for quadratic sample size $n \\gtrsim (d/\\alpha)^2$ or for logarithmic inlier fraction $\\alpha\\ge 1/\\log n$.\nWe show that consistent estimation is possible with nearly linear sample size and inverse-polynomial inlier fraction.\nConcretely, we show that the Huber loss estimator is consistent for every sample size $n= \\omega(d/\\alpha^2)$ \nand achieves an error rate of $O(d/\\alpha^2n)^{1/2}$ (both bounds are optimal up to constant factors).\nOur results extend to designs far beyond the Gaussian case and only require the column span of $X$ to not contain approximately sparse vectors\n(similar to the kind of assumption commonly made about the kernel space for compressed sensing).\nWe provide two technically similar proofs.\nOne proof is phrased in terms of strong convexity, extending work of [Tsakonas et al. '14], and particularly short.\nThe other proof highlights a connection between the Huber loss estimator and high-dimensional median computations. \nIn the special case of Gaussian designs, this connection leads us to a strikingly simple algorithm based on computing coordinate-wise medians that achieves nearly optimal guarantees in linear time, and that can exploit sparsity of $\\beta^*$.\nThe model studied here also captures heavy-tailed noise distributions that may not even have a first moment.",
        "conference": "ICML",
        "中文标题": "当无感知异常值占主导时的一致性回归",
        "摘要翻译": "我们考虑一个鲁棒的线性回归模型y=Xβ* + η，其中对手在不知道设计X∈ℝ^(n×d)的情况下，可以选择η以任意方式腐蚀除α比例观测值y外的所有观测值。在我们工作之前，即使对于高斯X，除了二次样本量n≳(d/α)^2或对数内点比例α≥1/log n外，没有已知的β*估计器在这个模型中是一致的。我们表明，几乎线性的样本量和逆多项式内点比例下，一致性估计是可能的。具体来说，我们表明Huber损失估计器对于每个样本量n=ω(d/α^2)都是一致的，并且实现了O(d/α^2n)^(1/2)的错误率（这两个界限在常数因子内都是最优的）。我们的结果扩展到远超出高斯情况的设计，并且仅要求X的列空间不包含近似稀疏向量（类似于压缩感知中通常对核空间所做的假设）。我们提供了两个技术上相似的证明。一个证明用强凸性来表述，扩展了[Tsakonas等人'14]的工作，并且特别简短。另一个证明突出了Huber损失估计器与高维中位数计算之间的联系。在高斯设计的特殊情况下，这种联系引导我们到一个基于计算坐标中位数的极其简单的算法，该算法在线性时间内实现了几乎最优的保证，并且可以利用β*的稀疏性。这里研究的模型还捕获了可能甚至没有一阶矩的重尾噪声分布。",
        "领域": "鲁棒回归分析、高维统计、异常值检测",
        "问题": "在高维数据中，当异常值无感知且占主导地位时，如何实现一致性回归估计。",
        "动机": "研究动机在于解决在高维数据中，当异常值以任意方式大量存在时，传统回归方法无法提供一致性估计的问题。",
        "方法": "采用Huber损失估计器，证明了在几乎线性样本量和逆多项式内点比例下的一致性，并提出了基于坐标中位数计算的简单算法。",
        "关键词": [
            "鲁棒回归",
            "Huber损失",
            "高维统计",
            "异常值检测",
            "一致性估计"
        ],
        "涉及的技术概念": {
            "Huber损失估计器": "用于在存在异常值时提供鲁棒性回归估计的技术，通过结合平方损失和绝对损失来减少异常值的影响。",
            "强凸性": "在证明中用于展示Huber损失估计器在特定条件下的最优性能，确保估计的一致性。",
            "高维中位数计算": "在高斯设计情况下，通过计算坐标中位数来实现简单且高效的回归估计，特别适用于稀疏β*的情况。"
        },
        "success": true
    },
    {
        "order": 213,
        "title": "Context-Aware Online Collective Inference for Templated Graphical Models",
        "html": "https://ICML.cc//virtual/2021/poster/9525",
        "abstract": "In this work, we examine online collective inference, the problem of maintaining and performing inference over a sequence of evolving graphical models. We utilize templated graphical models (TGM), a general class of graphical models expressed via templates and instantiated with data. A key challenge is minimizing the cost of instantiating the updated model. To address this, we define a class of exact and approximate context-aware methods for updating an existing TGM. These methods avoid a full re-instantiation by using the context of the updates to only add relevant components to the graphical model. Further, we provide stability bounds for the general online inference problem and regret bounds for a proposed approximation. Finally, we implement our approach in probabilistic soft logic, and test it on several online collective inference tasks. Through these experiments we verify the bounds on regret and stability, and show that our approximate online approach consistently runs two to five times faster than the offline alternative while, surprisingly, maintaining the quality of the predictions.\n",
        "conference": "ICML",
        "中文标题": "面向模板化图模型的上下文感知在线集体推理",
        "摘要翻译": "在这项工作中，我们研究了在线集体推理问题，即维护并对一系列演变的图模型进行推理。我们利用模板化图模型（TGM），这是一类通过模板表达并用数据实例化的通用图模型。一个关键挑战是最小化更新模型实例化的成本。为了解决这个问题，我们定义了一类精确和近似的上下文感知方法，用于更新现有的TGM。这些方法通过利用更新的上下文，仅向图模型添加相关组件，从而避免了完全重新实例化。此外，我们为一般在线推理问题提供了稳定性界限，并为提出的近似方法提供了遗憾界限。最后，我们在概率软逻辑中实现了我们的方法，并在几个在线集体推理任务上进行了测试。通过这些实验，我们验证了遗憾和稳定性的界限，并展示了我们的近似在线方法在保持预测质量的同时，运行速度始终比离线替代方案快两到五倍。",
        "领域": "概率图模型、在线学习、集体推理",
        "问题": "如何在图模型演化的序列中高效地进行在线集体推理",
        "动机": "减少图模型更新时的实例化成本，提高在线推理效率",
        "方法": "定义并实现了一类上下文感知的精确和近似方法，用于更新模板化图模型，避免完全重新实例化",
        "关键词": [
            "模板化图模型",
            "在线集体推理",
            "上下文感知",
            "概率软逻辑",
            "稳定性界限"
        ],
        "涉及的技术概念": {
            "模板化图模型（TGM）": "通过模板表达并用数据实例化的通用图模型，用于表示和推理复杂的关系和数据",
            "上下文感知方法": "利用更新的上下文信息，仅对图模型的相关部分进行更新，以减少计算成本",
            "概率软逻辑": "一种用于表示和推理不确定性和模糊知识的逻辑框架，本文中用于实现和测试提出的方法"
        },
        "success": true
    },
    {
        "order": 214,
        "title": "Continual Learning in the Teacher-Student Setup: Impact of Task Similarity",
        "html": "https://ICML.cc//virtual/2021/poster/9371",
        "abstract": "Continual learning—the ability to learn many tasks in sequence—is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forgetting, where learning new tasks erases knowledge of the earlier tasks. While catastrophic forgetting labels the problem, the theoretical reasons for interference between tasks remain unclear. Here, we attempt to narrow this gap between theory and practice by studying continual learning in the teacher-student setup. We extend previous analytical work on two-layer networks in the teacher-student setup to multiple teachers. Using each teacher to represent a different task, we investigate how the relationship between teachers affects the amount of forgetting and transfer exhibited by the student when the task switches. In line with recent work, we find that when tasks depend on similar features, intermediate task similarity leads to greatest forgetting. However, feature similarity is only one way in which tasks may be related. The teacher-student approach allows us to disentangle task similarity at the level of \\emph{readouts}  (hidden-to-output weights) as well as \\emph{features} (input-to-hidden weights). We find a complex interplay between both types of similarity, initial transfer/forgetting rates, maximum transfer/forgetting, and the long-time (post-switch) amount of transfer/forgetting. Together, these results help illuminate the diverse factors contributing to catastrophic forgetting.",
        "conference": "ICML",
        "中文标题": "师生设置中的持续学习：任务相似性的影响",
        "摘要翻译": "持续学习——即按顺序学习多个任务的能力——对于人工学习系统至关重要。然而，深度网络的标准训练方法常常遭受灾难性遗忘的困扰，即学习新任务会抹去对先前任务的知识。虽然灾难性遗忘标记了问题，但任务间干扰的理论原因仍不清楚。在此，我们试图通过研究师生设置中的持续学习来缩小理论与实践之间的差距。我们将先前关于师生设置中两层网络的分析工作扩展到多个教师。通过用每个教师代表不同的任务，我们研究了教师之间的关系如何影响学生在任务切换时的遗忘和迁移量。与最近的工作一致，我们发现当任务依赖于相似的特征时，中等任务相似性会导致最大的遗忘。然而，特征相似性只是任务可能相关的其中一种方式。师生方法使我们能够在‘读出’（隐藏到输出的权重）和‘特征’（输入到隐藏的权重）层面上解构任务相似性。我们发现这两种相似性、初始迁移/遗忘率、最大迁移/遗忘以及长期（切换后）迁移/遗忘量之间存在复杂的相互作用。这些结果共同帮助阐明了导致灾难性遗忘的多种因素。",
        "领域": "持续学习、深度学习理论、神经网络",
        "问题": "研究在持续学习过程中，任务相似性如何影响灾难性遗忘和知识迁移。",
        "动机": "理解并减少深度网络在持续学习中的灾难性遗忘现象，探索任务相似性对知识迁移和遗忘的影响机制。",
        "方法": "通过师生设置，扩展分析两层网络到多个教师模型，研究任务相似性在特征和读出层面对遗忘和迁移的影响。",
        "关键词": [
            "持续学习",
            "灾难性遗忘",
            "师生设置",
            "任务相似性",
            "知识迁移"
        ],
        "涉及的技术概念": {
            "灾难性遗忘": "在持续学习过程中，学习新任务导致对旧任务知识的遗忘现象。",
            "师生设置": "一种理论框架，其中‘教师’网络生成数据，‘学生’网络学习这些数据，用于研究学习动态。",
            "任务相似性": "不同任务在特征或读出层面上的相似程度，影响持续学习中的知识迁移和遗忘。"
        },
        "success": true
    },
    {
        "order": 215,
        "title": "Continuous Coordination As a Realistic Scenario for Lifelong Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8975",
        "abstract": "Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents’ policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi — a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.",
        "conference": "ICML",
        "中文标题": "持续协调作为终身学习的现实场景",
        "摘要翻译": "当前的深度强化学习（RL）算法仍然高度特定于任务，缺乏泛化到新环境的能力。然而，终身学习（LLL）旨在通过有效地在任务之间转移和利用知识来顺序解决多个任务。尽管近年来对终身RL的兴趣激增，但由于缺乏现实的测试平台，使得对LLL算法的稳健评估变得困难。另一方面，多代理RL（MARL）可以被视为终身RL的自然场景，因为其固有的非平稳性，即代理的策略随时间变化。在这项工作中，我们引入了一个支持零样本和少样本设置的多代理终身学习测试平台。我们的设置基于Hanabi——一个部分可观察、完全合作的多代理游戏，已被证明对零样本协调具有挑战性。其庞大的策略空间使其成为终身RL任务的理想环境。我们评估了几种最近的MARL方法，并在有限的内存和计算机制下对最先进的LLL算法进行了基准测试，以揭示它们的优势和劣势。这种持续学习范式还为我们提供了一种超越集中训练的实际方法，集中训练是MARL中最常用的训练协议。我们实证表明，在我们的设置中训练的代理能够与未见过的代理良好协调，而无需先前工作所做的任何额外假设。代码和所有预训练模型可在https://github.com/chandar-lab/Lifelong-Hanabi获取。",
        "领域": "多代理强化学习、终身学习、零样本协调",
        "问题": "深度强化学习算法缺乏泛化能力和在多任务环境中的持续学习能力",
        "动机": "为终身强化学习提供一个现实的测试平台，以评估和提升算法在多任务环境中的表现",
        "方法": "基于Hanabi游戏构建多代理终身学习测试平台，支持零样本和少样本设置，评估和比较多种MARL和LLL算法",
        "关键词": [
            "多代理强化学习",
            "终身学习",
            "零样本协调",
            "Hanabi游戏",
            "持续学习"
        ],
        "涉及的技术概念": {
            "多代理强化学习（MARL）": "用于模拟多个代理在共享环境中通过交互学习策略的技术，适用于终身学习场景",
            "终身学习（LLL）": "旨在通过顺序学习多个任务并有效转移知识来提高模型泛化能力的学习范式",
            "零样本协调": "指代理能够在没有预先训练或额外假设的情况下，与未见过的代理有效协调的能力"
        },
        "success": true
    },
    {
        "order": 216,
        "title": "Continuous-time Model-based Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8565",
        "abstract": "Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty.  We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, and can solve classic control problems in a sample-efficient manner.",
        "conference": "ICML",
        "中文标题": "基于连续时间模型的强化学习",
        "摘要翻译": "基于模型的强化学习（MBRL）方法依赖于离散时间状态转移模型，而物理系统和绝大多数控制任务是在连续时间中运行的。为了避免对基础过程进行时间离散化近似，我们提出了一种基于新型演员-评论家方法的连续时间MBRL框架。我们的方法还利用贝叶斯神经常微分方程（ODE）推断未知状态演化微分，以考虑认知不确定性。我们在一个新的ODE-RL套件上实现并测试了我们的方法，该套件明确解决了连续时间控制系统。我们的实验表明，该模型对不规则和噪声数据具有鲁棒性，并且能够以样本高效的方式解决经典控制问题。",
        "领域": "强化学习、控制系统、贝叶斯方法",
        "问题": "解决基于模型的强化学习在连续时间控制任务中的应用问题",
        "动机": "物理系统和大多数控制任务在连续时间中运行，而现有的MBRL方法依赖于离散时间模型，这可能导致对基础过程的时间离散化近似不准确。",
        "方法": "提出了一种基于新型演员-评论家方法的连续时间MBRL框架，利用贝叶斯神经常微分方程推断未知状态演化微分。",
        "关键词": [
            "连续时间强化学习",
            "演员-评论家方法",
            "贝叶斯神经ODE",
            "控制系统",
            "样本效率"
        ],
        "涉及的技术概念": {
            "演员-评论家方法": "在连续时间MBRL框架中用于平衡探索和利用的策略优化方法。",
            "贝叶斯神经常微分方程": "用于推断未知状态演化微分，考虑模型认知不确定性的技术。",
            "连续时间控制系统": "论文中提出的ODE-RL套件明确解决的控制系统类型，强调在连续时间中的操作。"
        },
        "success": true
    },
    {
        "order": 217,
        "title": "Contrastive Learning Inverts the Data Generating Process",
        "html": "https://ICML.cc//virtual/2021/poster/9643",
        "abstract": "Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.",
        "conference": "ICML",
        "中文标题": "对比学习反转数据生成过程",
        "摘要翻译": "对比学习最近在自监督学习中取得了巨大成功。然而，到目前为止，很大程度上尚不清楚为什么学习到的表示能够如此有效地泛化到各种下游任务。我们在此证明，使用属于常用InfoNCE家族的目标训练的feedforward模型学会隐式地反转观察数据的底层生成模型。虽然证明对生成模型做出了一定的统计假设，但我们通过实证观察发现，即使这些假设被严重违反，我们的发现仍然成立。我们的理论突出了对比学习、生成建模和非线性独立成分分析之间的基本联系，从而进一步加深了我们对学习到的表示的理解，并为推导更有效的对比损失提供了理论基础。",
        "领域": "自监督学习、生成模型、非线性独立成分分析",
        "问题": "理解对比学习为何能有效泛化到各种下游任务",
        "动机": "探索对比学习成功背后的理论原因，以及其与生成模型和非线性独立成分分析的联系",
        "方法": "通过理论证明和实证研究，分析使用InfoNCE家族目标训练的feedforward模型如何隐式反转数据的生成模型",
        "关键词": [
            "对比学习",
            "自监督学习",
            "生成模型",
            "InfoNCE",
            "非线性独立成分分析"
        ],
        "涉及的技术概念": {
            "对比学习": "一种自监督学习方法，通过比较正负样本对来学习数据表示",
            "InfoNCE": "一种常用的对比损失函数，用于最大化正样本对的相似度同时最小化负样本对的相似度",
            "非线性独立成分分析": "一种统计方法，用于从观察到的数据中恢复独立的非高斯源信号"
        },
        "success": true
    },
    {
        "order": 218,
        "title": "Controlling Graph Dynamics with  Reinforcement Learning and Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9909",
        "abstract": "We consider the problem of controlling a partially-observed\ndynamic process on a graph by a limited number of interventions. This problem naturally arises in contexts such as scheduling virus tests to curb an epidemic; targeted marketing in order to promote a product; and manually inspecting posts to detect fake news spreading on social networks. \n\nWe formulate this setup as a sequential decision problem over a temporal graph process. In face of an exponential state space, combinatorial action space and partial observability, we design a novel tractable scheme to control dynamical processes on temporal graphs. We successfully apply our approach to two popular problems that fall into our framework: prioritizing which nodes should be tested in order to curb the spread of an epidemic, and influence maximization on a graph.",
        "conference": "ICML",
        "中文标题": "利用强化学习和图神经网络控制图动态",
        "摘要翻译": "我们考虑通过有限数量的干预措施来控制图上部分可观测的动态过程的问题。这一问题自然出现在诸如安排病毒测试以遏制流行病、针对性营销以推广产品以及手动检查帖子以检测社交网络上传播的假新闻等情境中。我们将这一设置建模为时间图过程上的顺序决策问题。面对指数级的状态空间、组合动作空间和部分可观测性，我们设计了一种新颖的可处理方案来控制时间图上的动态过程。我们成功地将我们的方法应用于落入我们框架的两个流行问题：优先考虑应测试哪些节点以遏制流行病的传播，以及图上的影响力最大化。",
        "领域": "图神经网络应用, 强化学习应用, 动态过程控制",
        "问题": "如何在部分可观测的图动态过程中通过有限干预有效控制过程",
        "动机": "解决在复杂图结构上动态过程控制的实际应用问题，如流行病控制和影响力最大化",
        "方法": "结合强化学习和图神经网络，设计了一种处理时间图上动态过程控制的新方法",
        "关键词": [
            "图神经网络",
            "强化学习",
            "动态过程控制",
            "流行病控制",
            "影响力最大化"
        ],
        "涉及的技术概念": {
            "图神经网络": "用于处理和预测图结构数据，捕捉节点间复杂关系",
            "强化学习": "用于在部分可观测环境下做出序列决策，优化干预策略",
            "动态过程控制": "指在时间变化的图结构上，通过干预影响动态过程的发展方向"
        },
        "success": true
    },
    {
        "order": 219,
        "title": "Convex Regularization in Monte-Carlo Tree Search",
        "html": "https://ICML.cc//virtual/2021/poster/10667",
        "abstract": "Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequential decision making. The recent AlphaGo and AlphaZero algorithms have shown how to successfully combine these two paradigms to solve large-scale sequential decision problems. These methodologies exploit a variant of the well-known UCT algorithm to trade off the exploitation of good actions and the exploration of unvisited states, but their empirical success comes at the cost of poor sample-efficiency and high computation time. In this paper, we overcome these limitations by introducing the use of convex regularization in Monte-Carlo Tree Search (MCTS) to drive exploration efficiently and to improve policy updates. First, we introduce a unifying theory on the use of generic convex regularizers in MCTS, deriving the first regret analysis of regularized MCTS and showing that it guarantees an exponential convergence rate. Second, we exploit our theoretical framework to introduce novel regularized backup operators for MCTS, based on the relative entropy of the policy update and, more importantly, on the Tsallis entropy of the policy, for which we prove superior theoretical guarantees. We empirically verify the consequence of our theoretical results on a toy problem. Finally, we show how our framework can easily be incorporated in AlphaGo and we empirically show the superiority of convex regularization, w.r.t. representative baselines, on well-known RL problems across several Atari games.",
        "conference": "ICML",
        "中文标题": "蒙特卡洛树搜索中的凸正则化",
        "摘要翻译": "蒙特卡洛规划和强化学习（RL）对于序列决策制定至关重要。最近的AlphaGo和AlphaZero算法展示了如何成功结合这两种范式来解决大规模序列决策问题。这些方法利用了著名的UCT算法的一个变体，以权衡对好动作的利用和对未访问状态的探索，但它们的经验成功是以样本效率低和计算时间长为代价的。在本文中，我们通过在蒙特卡洛树搜索（MCTS）中引入凸正则化来高效驱动探索并改进策略更新，从而克服了这些限制。首先，我们介绍了在MCTS中使用通用凸正则化器的统一理论，推导了正则化MCTS的首个遗憾分析，并展示了它保证了指数收敛速度。其次，我们利用我们的理论框架为MCTS引入了基于策略更新的相对熵和更重要的是基于策略的Tsallis熵的新型正则化备份操作符，为此我们证明了优越的理论保证。我们在一个玩具问题上实证验证了我们理论结果的后果。最后，我们展示了我们的框架如何轻松地融入AlphaGo，并在几个Atari游戏的知名RL问题上，实证展示了凸正则化相对于代表性基线的优越性。",
        "领域": "强化学习、蒙特卡洛方法、游戏AI",
        "问题": "提高蒙特卡洛树搜索的样本效率和减少计算时间",
        "动机": "解决现有蒙特卡洛规划和强化学习方法在样本效率和计算时间上的不足",
        "方法": "在蒙特卡洛树搜索中引入凸正则化，以高效驱动探索并改进策略更新",
        "关键词": [
            "凸正则化",
            "蒙特卡洛树搜索",
            "强化学习",
            "策略更新",
            "Tsallis熵"
        ],
        "涉及的技术概念": {
            "凸正则化": "在MCTS中引入以改善探索效率和策略更新",
            "Tsallis熵": "用于策略更新的新型正则化备份操作符，提供优越的理论保证",
            "UCT算法": "用于权衡动作利用和状态探索的变体算法"
        },
        "success": true
    },
    {
        "order": 220,
        "title": "ConvexVST: A Convex Optimization Approach to Variance-stabilizing Transformation",
        "html": "https://ICML.cc//virtual/2021/poster/10549",
        "abstract": "The variance-stabilizing transformation (VST) problem is to transform heteroscedastic data to homoscedastic data so that they are more tractable for subsequent analysis. However, most of the existing approaches focus on finding an analytical solution for a certain parametric distribution, which severely limits the applications, because simple distributions cannot faithfully describe the real data while more complicated distributions cannot be analytically solved. In this paper, we converted the VST problem into a convex optimization problem, which can always be efficiently solved, identified the specific structure of the convex problem, which further improved the efficiency of the proposed algorithm, and showed that any finite discrete distributions and the discretized version of any continuous distributions from real data can be variance-stabilized in an easy and nonparametric way. We demonstrated the new approach on bioimaging data and achieved superior performance compared to peer algorithms in terms of not only the variance homoscedasticity but also the impact on subsequent analysis such as denoising. Source codes are available at https://github.com/yu-lab-vt/ConvexVST.",
        "conference": "ICML",
        "中文标题": "ConvexVST：一种基于凸优化的方差稳定变换方法",
        "摘要翻译": "方差稳定变换（VST）问题的核心在于将异方差数据转换为同方差数据，以便于后续分析。然而，现有方法大多集中于为特定参数分布寻找解析解，这极大地限制了其应用范围，因为简单分布无法真实描述实际数据，而复杂分布又无法解析求解。本文中，我们将VST问题转化为一个凸优化问题，该问题总能高效求解；识别了凸问题的特定结构，进一步提高了所提算法的效率；并证明了任何有限离散分布及来自实际数据的任何连续分布的离散化版本，都能以一种简单且非参数化的方式进行方差稳定。我们在生物成像数据上验证了新方法，在方差同质性及对后续分析（如去噪）的影响方面，均优于同类算法。源代码可在https://github.com/yu-lab-vt/ConvexVST获取。",
        "领域": "生物成像分析、统计机器学习、信号处理",
        "问题": "解决异方差数据转换为同方差数据的通用性问题",
        "动机": "现有方法局限于特定参数分布的解析解，无法适应复杂或真实数据分布的需求",
        "方法": "将方差稳定变换问题转化为凸优化问题，利用其特定结构提高算法效率，支持非参数化处理",
        "关键词": [
            "方差稳定变换",
            "凸优化",
            "非参数方法",
            "生物成像",
            "异方差数据"
        ],
        "涉及的技术概念": {
            "方差稳定变换": "将异方差数据转换为同方差数据的技术，便于后续统计分析",
            "凸优化": "用于高效求解转换问题，确保全局最优解",
            "非参数方法": "不依赖于数据分布的具体形式，适用于各种复杂数据"
        },
        "success": true
    },
    {
        "order": 221,
        "title": "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases",
        "html": "https://ICML.cc//virtual/2021/poster/8525",
        "abstract": "Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ``soft' convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.",
        "conference": "ICML",
        "中文标题": "ConViT：通过软卷积归纳偏置改进视觉Transformer",
        "摘要翻译": "卷积架构在视觉任务中已被证明极为成功。其硬性归纳偏置使得样本高效学习成为可能，但代价可能是性能上限的降低。视觉Transformer（ViTs）依赖于更灵活的自注意力层，最近在图像分类任务中超越了CNNs。然而，它们需要在大型外部数据集上进行昂贵的预训练，或从预训练的卷积网络中进行知识蒸馏。在本文中，我们提出以下问题：是否有可能结合这两种架构的优势，同时避免它们各自的局限性？为此，我们引入了门控位置自注意力（GPSA），这是一种可以配备“软”卷积归纳偏置的位置自注意力形式。我们将GPSA层初始化为模仿卷积层的局部性，然后通过调整一个门控参数来给予每个注意力头逃离局部性的自由，该参数调节对位置信息与内容信息的注意力。由此产生的类似卷积的ViT架构ConViT在ImageNet上超越了DeiT，同时提供了大大改善的样本效率。我们进一步通过首先量化在普通自注意力层中如何鼓励局部性，然后分析在GPSA层中如何逃离局部性，来研究学习中的局部性角色。最后，我们提出了各种消融研究以更好地理解ConViT的成功。我们的代码和模型已在https://github.com/facebookresearch/convit公开。",
        "领域": "图像分类、视觉Transformer、卷积神经网络",
        "问题": "如何结合卷积神经网络和视觉Transformer的优势，同时避免它们各自的局限性",
        "动机": "探索结合卷积神经网络和视觉Transformer的优势，以提高图像分类任务的性能和样本效率",
        "方法": "引入门控位置自注意力（GPSA），一种可以配备软卷积归纳偏置的位置自注意力形式，初始化为模仿卷积层的局部性，然后通过调整门控参数给予逃离局部性的自由",
        "关键词": [
            "视觉Transformer",
            "卷积归纳偏置",
            "门控位置自注意力",
            "图像分类",
            "样本效率"
        ],
        "涉及的技术概念": {
            "门控位置自注意力（GPSA）": "一种可以配备软卷积归纳偏置的位置自注意力形式，用于结合卷积神经网络和视觉Transformer的优势",
            "软卷积归纳偏置": "在GPSA中引入的偏置，初始化为模仿卷积层的局部性，但允许模型通过调整门控参数逃离局部性",
            "样本效率": "指模型在较少样本情况下达到良好性能的能力，ConViT通过结合卷积和Transformer的优势提高了样本效率"
        },
        "success": true
    },
    {
        "order": 222,
        "title": "Cooperative Exploration for Multi-Agent Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8947",
        "abstract": "Exploration is critical for good results in deep reinforcement learning and has attracted much attention. However, existing multi-agent deep reinforcement learning algorithms still use mostly noise-based techniques. Very recently, exploration methods that consider cooperation among multiple agents have been developed. However, existing methods suffer from a common challenge: agents struggle to identify states that are worth exploring, and hardly coordinate  exploration efforts toward those states. To address this shortcoming, in this paper, we propose cooperative multi-agent exploration (CMAE): agents share a common goal while exploring. The goal is selected from multiple projected state spaces by a normalized entropy-based technique. Then, agents are trained to reach the goal in a coordinated manner. We demonstrate that CMAE consistently outperforms baselines on various tasks, including a sparse-reward version of multiple-particle environment (MPE) and the  Starcraft multi-agent challenge (SMAC).  ",
        "conference": "ICML",
        "中文标题": "多智能体深度强化学习的协同探索",
        "摘要翻译": "探索对于深度强化学习取得良好结果至关重要，并已引起广泛关注。然而，现有的多智能体深度强化学习算法仍主要使用基于噪声的技术。最近，考虑多智能体间合作的探索方法已被开发出来。然而，现有方法面临一个共同的挑战：智能体难以识别值得探索的状态，并且很难协调对这些状态的探索努力。为了解决这一不足，本文提出了协同多智能体探索（CMAE）：智能体在探索时共享一个共同目标。该目标通过基于归一化熵的技术从多个投影状态空间中选择。然后，智能体被训练以协调的方式达到目标。我们证明，CMAE在各种任务上始终优于基线，包括多粒子环境（MPE）的稀疏奖励版本和星际争霸多智能体挑战（SMAC）。",
        "领域": "多智能体系统、深度强化学习、协同学习",
        "问题": "多智能体在深度强化学习中难以有效识别和协调探索有价值的状态",
        "动机": "解决多智能体在探索过程中缺乏有效协调和状态识别能力的问题",
        "方法": "提出协同多智能体探索（CMAE），通过共享共同目标和基于归一化熵的技术选择目标，训练智能体协调达到目标",
        "关键词": [
            "协同探索",
            "多智能体",
            "深度强化学习",
            "归一化熵",
            "状态空间"
        ],
        "涉及的技术概念": {
            "协同多智能体探索（CMAE）": "一种多智能体探索方法，通过共享共同目标和协调行动来提高探索效率",
            "归一化熵": "用于从多个投影状态空间中选择探索目标的技术，帮助智能体识别有价值的状态",
            "多粒子环境（MPE）": "一个模拟环境，用于测试和评估多智能体算法的性能，特别是在稀疏奖励条件下"
        },
        "success": true
    },
    {
        "order": 223,
        "title": "Correcting Exposure Bias for Link Recommendation",
        "html": "https://ICML.cc//virtual/2021/poster/10327",
        "abstract": "Link prediction methods are frequently applied in recommender systems, e.g., to suggest citations for academic papers or friends in social networks. However, exposure bias can arise when users are systematically underexposed to certain relevant items. For example, in citation networks, authors might be more likely to encounter papers from their own field and thus cite them preferentially. This bias can propagate through naively trained link predictors, leading to both biased evaluation and high generalization error (as assessed by true relevance). Moreover, this bias can be exacerbated by feedback loops. We propose estimators that leverage known exposure probabilities to mitigate this bias and consequent feedback loops. Next, we provide a loss function for learning the exposure probabilities from data. Finally, experiments on semi-synthetic data based on real-world citation networks, show that our methods reliably identify (truly) relevant citations. Additionally, our methods lead to greater diversity in the recommended papers' fields of study. The code is available at github.com/shantanu95/exposure-bias-link-rec.",
        "conference": "ICML",
        "中文标题": "纠正链接推荐中的曝光偏差",
        "摘要翻译": "链接预测方法经常被应用于推荐系统中，例如，为学术论文推荐引用或在社交网络中推荐朋友。然而，当用户系统地未接触到某些相关项目时，可能会出现曝光偏差。例如，在引用网络中，作者可能更倾向于遇到并引用自己领域的论文。这种偏差可能会通过未经训练的链接预测器传播，导致评估偏差和高泛化误差（根据真实相关性评估）。此外，这种偏差可能会因反馈循环而加剧。我们提出了利用已知曝光概率来减轻这种偏差及随之而来的反馈循环的估计器。接着，我们提供了一个从数据中学习曝光概率的损失函数。最后，基于真实世界引用网络的半合成数据实验表明，我们的方法能够可靠地识别（真正）相关的引用。此外，我们的方法还提高了推荐论文研究领域的多样性。代码可在github.com/shantanu95/exposure-bias-link-rec获取。",
        "领域": "推荐系统、社交网络分析、学术引用网络",
        "问题": "解决链接推荐中因用户未系统接触到某些相关项目而产生的曝光偏差问题",
        "动机": "曝光偏差会导致评估偏差和高泛化误差，并可能因反馈循环而加剧，影响推荐系统的准确性和多样性",
        "方法": "提出利用已知曝光概率的估计器来减轻偏差和反馈循环，并提供从数据中学习曝光概率的损失函数",
        "关键词": [
            "曝光偏差",
            "链接推荐",
            "反馈循环",
            "损失函数",
            "多样性"
        ],
        "涉及的技术概念": {
            "曝光偏差": "指用户系统地未接触到某些相关项目，导致推荐系统评估和性能上的偏差",
            "反馈循环": "指推荐系统的输出反过来影响其输入，可能加剧现有的偏差",
            "损失函数": "用于从数据中学习曝光概率，优化推荐系统的性能"
        },
        "success": true
    },
    {
        "order": 224,
        "title": "Correlation Clustering in Constant Many Parallel Rounds",
        "html": "https://ICML.cc//virtual/2021/poster/10213",
        "abstract": "Correlation clustering is a central topic in unsupervised learning, with many applications in ML and data mining.  In correlation clustering, one receives as input a signed graph and the goal is to partition it to minimize the number of disagreements. In this work we propose a massively parallel computation (MPC) algorithm for this problem that is considerably faster than prior work. In particular, our algorithm uses machines with memory sublinear in the number of nodes in the graph and returns a constant approximation while running only for a constant number of rounds. To the best of our knowledge, our algorithm is the first that can provably approximate a clustering problem using only a constant number of MPC rounds in the sublinear memory regime. We complement our analysis with an experimental scalability evaluation of our techniques.\n",
        "conference": "ICML",
        "中文标题": "在恒定多并行轮次中的相关性聚类",
        "摘要翻译": "相关性聚类是无监督学习中的一个核心主题，在机器学习和数据挖掘中有许多应用。在相关性聚类中，输入是一个带符号的图，目标是对其进行分区以最小化不一致的数量。在这项工作中，我们为这个问题提出了一个大规模并行计算（MPC）算法，该算法比之前的工作快得多。特别是，我们的算法使用的机器内存对图中的节点数是次线性的，并且在仅运行恒定数量的轮次时返回一个常数近似值。据我们所知，我们的算法是第一个可以在次线性内存制度下仅使用恒定数量的MPC轮次来近似聚类问题的算法。我们通过对我们技术的实验可扩展性评估来补充我们的分析。",
        "领域": "无监督学习, 大规模并行计算, 图分区",
        "问题": "如何在次线性内存和恒定数量的并行轮次中高效近似解决相关性聚类问题",
        "动机": "提高相关性聚类在大规模数据集上的计算效率，减少内存使用和计算轮次",
        "方法": "提出了一种大规模并行计算（MPC）算法，该算法在次线性内存和恒定数量的轮次内提供常数近似解",
        "关键词": [
            "相关性聚类",
            "大规模并行计算",
            "次线性内存",
            "常数近似",
            "图分区"
        ],
        "涉及的技术概念": {
            "相关性聚类": "一种无监督学习方法，旨在通过最小化图中的不一致数量来分区带符号的图",
            "大规模并行计算（MPC）": "一种计算模型，允许多台机器并行处理数据，特别适用于大规模数据集",
            "次线性内存": "算法使用的内存量少于输入数据的大小，适用于处理大规模数据"
        },
        "success": true
    },
    {
        "order": 225,
        "title": "Counterfactual Credit Assignment in Model-Free Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9795",
        "abstract": "Credit assignment in reinforcement learning is the problem of measuring an action’s influence on future rewards. In particular, this requires separating skill from luck, i.e. disentangling the effect of an action on rewards from that of external factors and subsequent actions. To achieve this, we adapt the notion of counterfactuals from causality theory to a model-free RL setup. \nThe key idea is to condition value functions on future events, by learning to extract relevant information from a trajectory. We formulate a family of policy gradient algorithms that use these future-conditional value functions as baselines or critics, and show that they are provably low variance. To avoid the potential bias from conditioning on future information, we constrain the hindsight information to not contain information about the agent's actions. We demonstrate the efficacy and validity of our algorithm on a number of illustrative and challenging problems.",
        "conference": "ICML",
        "中文标题": "无模型强化学习中的反事实信用分配",
        "摘要翻译": "强化学习中的信用分配问题是衡量一个行动对未来奖励影响的问题。特别是，这需要将技能与运气分开，即区分行动对奖励的影响与外部因素和后续行动的影响。为了实现这一点，我们将因果理论中的反事实概念适应到无模型强化学习设置中。关键思想是通过学习从轨迹中提取相关信息，将价值函数条件化于未来事件。我们制定了一系列策略梯度算法，这些算法使用这些未来条件价值函数作为基线或评论家，并证明它们是可证明的低方差。为了避免因条件化于未来信息而产生的潜在偏差，我们限制了后见之明信息，使其不包含关于代理行动的信息。我们在多个具有挑战性的问题上展示了我们算法的有效性和有效性。",
        "领域": "强化学习、信用分配、策略梯度算法",
        "问题": "衡量行动对未来奖励的影响，区分行动效果与外部因素和后续行动的影响",
        "动机": "解决强化学习中的信用分配问题，通过反事实概念区分行动对奖励的直接和间接影响",
        "方法": "将因果理论中的反事实概念应用于无模型强化学习，学习未来条件价值函数作为基线或评论家，制定低方差的策略梯度算法",
        "关键词": [
            "反事实信用分配",
            "无模型强化学习",
            "未来条件价值函数",
            "策略梯度算法",
            "信用分配"
        ],
        "涉及的技术概念": {
            "反事实信用分配": "用于区分行动对奖励的直接和间接影响，基于因果理论中的反事实概念",
            "未来条件价值函数": "通过条件化于未来事件来评估行动价值，作为策略梯度算法的基线或评论家",
            "策略梯度算法": "一类基于梯度的强化学习算法，通过优化策略参数来最大化预期奖励"
        },
        "success": true
    },
    {
        "order": 226,
        "title": "CountSketches, Feature Hashing and the Median of Three",
        "html": "https://ICML.cc//virtual/2021/poster/9691",
        "abstract": "In this paper, we revisit the classic CountSketch method, which is a sparse, random projection that transforms a (high-dimensional) Euclidean vector $v$ to a vector of dimension $(2t-1) s$, where $t, s > 0$ are integer parameters. It is known that a CountSketch allows estimating coordinates of $v$ with variance bounded by $\\|v\\|_2^2/s$. For $t > 1$, the estimator takes the median of $2t-1$ independent estimates, and the probability that the estimate is off by more than $2 \\|v\\|_2/\\sqrt{s}$ is exponentially small in $t$. This suggests choosing $t$ to be logarithmic in a desired inverse failure probability. However, implementations of CountSketch often use a small, constant $t$. Previous work only predicts a constant factor improvement in this setting. Our main contribution is a new analysis of CountSketch, showing an improvement in variance to $O(\\min\\{\\|v\\|_1^2/s^2,\\|v\\|_2^2/s\\})$ when $t > 1$.\nThat is, the variance decreases proportionally to $s^{-2}$, asymptotically for large enough $s$. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "CountSketches、特征哈希与三者中位数",
        "摘要翻译": "在本文中，我们重新审视了经典的CountSketch方法，这是一种稀疏的随机投影，它将一个（高维）欧几里得向量v转换为维度为(2t-1)s的向量，其中t, s > 0是整数参数。已知CountSketch允许估计v的坐标，其方差有界于‖v‖₂²/s。对于t > 1的情况，估计器取2t-1个独立估计的中位数，且估计值偏离超过2‖v‖₂/√s的概率在t中呈指数级小。这表明选择t为所需逆失败概率的对数。然而，CountSketch的实现通常使用一个小的常数t。先前的工作仅预测了在这种设置下的常数因子改进。我们的主要贡献是对CountSketch的新分析，显示当t > 1时，方差改进为O(min{‖v‖₁²/s²,‖v‖₂²/s})。也就是说，对于足够大的s，方差与s⁻²成比例地减少。",
        "领域": "数据压缩、机器学习、算法优化",
        "问题": "如何改进CountSketch方法的方差性能",
        "动机": "探索在CountSketch方法中使用大于1的t参数时，方差性能的潜在改进",
        "方法": "通过新的分析技术，展示了在特定条件下CountSketch方法的方差可以显著降低",
        "关键词": [
            "CountSketch",
            "特征哈希",
            "方差分析",
            "随机投影",
            "算法优化"
        ],
        "涉及的技术概念": {
            "CountSketch": "一种稀疏的随机投影技术，用于高维数据的降维和特征提取",
            "特征哈希": "一种将高维特征映射到低维空间的技术，常用于减少计算和存储需求",
            "中位数估计": "通过取多个独立估计的中位数来提高估计的鲁棒性和准确性"
        }
    },
    {
        "order": 227,
        "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9201",
        "abstract": "Federated Learning (FL) as a distributed learning paradigm that aggregates information from diverse clients to train a shared global model, has demonstrated great success. However, malicious clients can perform poisoning attacks and model replacement to introduce backdoors into the trained global model. Although there have been intensive studies designing robust aggregation methods and empirical robust federated training protocols against backdoors, existing approaches lack robustness certification. This paper provides the first general framework, Certifiably Robust Federated Learning (CRFL), to train certifiably robust FL models against backdoors. Our method exploits clipping and smoothing on model parameters to control the global model smoothness, which yields a sample-wise robustness certification on backdoors with limited magnitude. Our certification also specifies the relation to federated learning parameters, such as poisoning ratio on instance level, number of attackers, and training iterations. Practically, we conduct comprehensive experiments across a range of federated datasets, and provide the ﬁrst benchmark for certiﬁed robustness against backdoor attacks in federated learning. Our code is publicaly available at https://github.com/AI-secure/CRFL.",
        "conference": "ICML",
        "中文标题": "CRFL：可认证鲁棒的联邦学习抵御后门攻击",
        "摘要翻译": "联邦学习（FL）作为一种分布式学习范式，通过聚合来自不同客户端的信息来训练共享的全局模型，已展现出巨大成功。然而，恶意客户端可能通过投毒攻击和模型替换，在训练的全局模型中引入后门。尽管已有大量研究设计了针对后门的鲁棒聚合方法和经验性鲁棒联邦训练协议，现有方法缺乏鲁棒性认证。本文提出了第一个通用框架——可认证鲁棒的联邦学习（CRFL），以训练可认证鲁棒的联邦学习模型抵御后门攻击。我们的方法利用模型参数的裁剪和平滑来控制全局模型的平滑性，从而对幅度有限的后门提供样本级的鲁棒性认证。我们的认证还明确了与联邦学习参数的关系，如实例级别的投毒比例、攻击者数量和训练迭代次数。实际上，我们在多个联邦数据集上进行了全面实验，并提供了联邦学习中针对后门攻击的认证鲁棒性的首个基准。我们的代码公开在https://github.com/AI-secure/CRFL。",
        "领域": "联邦学习安全、后门攻击防御、鲁棒性认证",
        "问题": "如何在联邦学习中提供可认证的鲁棒性以抵御后门攻击",
        "动机": "现有联邦学习方法缺乏对后门攻击的鲁棒性认证，需要一种能够提供认证鲁棒性的框架",
        "方法": "通过模型参数的裁剪和平滑控制全局模型的平滑性，实现对后门攻击的样本级鲁棒性认证",
        "关键词": [
            "联邦学习",
            "后门攻击",
            "鲁棒性认证",
            "模型平滑",
            "投毒防御"
        ],
        "涉及的技术概念": {
            "模型裁剪和平滑": "用于控制全局模型的平滑性，是实现鲁棒性认证的关键技术",
            "样本级鲁棒性认证": "提供对每个样本抵御后门攻击的认证，确保模型的安全性",
            "联邦学习参数关系": "明确认证与联邦学习参数（如投毒比例、攻击者数量等）的关系，为实际应用提供指导"
        },
        "success": true
    },
    {
        "order": 228,
        "title": "Cross-domain Imitation from Observations",
        "html": "https://ICML.cc//virtual/2021/poster/8955",
        "abstract": "Imitation learning seeks to circumvent the difficulty in designing proper reward functions for training agents by utilizing expert behavior. With environments modeled as Markov Decision Processes (MDP), most of the existing imitation algorithms are contingent on the availability of expert demonstrations in the same MDP as the one in which a new imitation policy is to be learned. In this paper, we study the problem of how to imitate tasks when discrepancies exist between the expert and agent MDP. These discrepancies across domains could include differing dynamics, viewpoint, or morphology; we present a novel framework to learn correspondences across such domains. Importantly, in contrast to prior works, we use unpaired and unaligned trajectories containing only states in the expert domain, to learn this correspondence. We utilize a cycle-consistency constraint on both the state space and a domain agnostic latent space to do this. In addition, we enforce consistency on the temporal position of states via a normalized position estimator function, to align the trajectories across the two domains. Once this correspondence is found, we can directly transfer the demonstrations on one domain to the other and use it for imitation. Experiments across a wide variety of challenging domains demonstrate the efficacy of our approach. ",
        "conference": "ICML",
        "中文标题": "跨领域观察模仿学习",
        "摘要翻译": "模仿学习旨在通过利用专家行为来规避为训练智能体设计适当奖励函数的困难。在环境被建模为马尔可夫决策过程（MDP）的情况下，大多数现有的模仿算法依赖于在与学习新模仿策略相同的MDP中获取专家示范。本文研究了当专家和智能体MDP之间存在差异时如何模仿任务的问题。这些跨领域的差异可能包括不同的动态、视角或形态；我们提出了一个新颖的框架来学习这些领域之间的对应关系。重要的是，与之前的工作相比，我们使用仅包含专家领域状态的未配对和未对齐的轨迹来学习这种对应关系。我们利用状态空间和领域无关的潜在空间上的循环一致性约束来实现这一点。此外，我们通过归一化位置估计器函数在状态的时间位置上强制执行一致性，以跨两个领域对齐轨迹。一旦找到这种对应关系，我们就可以直接将一个领域的示范转移到另一个领域，并用于模仿。在多种挑战性领域的实验中证明了我们方法的有效性。",
        "领域": "模仿学习、跨领域学习、强化学习",
        "问题": "解决在专家和智能体MDP之间存在差异时的模仿学习问题",
        "动机": "研究如何在不同动态、视角或形态的跨领域环境中进行有效的模仿学习",
        "方法": "利用未配对和未对齐的专家状态轨迹，通过循环一致性约束和归一化位置估计器函数学习跨领域对应关系，实现示范转移",
        "关键词": [
            "跨领域模仿学习",
            "循环一致性约束",
            "归一化位置估计器",
            "示范转移",
            "马尔可夫决策过程"
        ],
        "涉及的技术概念": {
            "马尔可夫决策过程（MDP）": "用于建模环境和决策过程的框架，是模仿学习的基础",
            "循环一致性约束": "用于确保跨领域学习中的对应关系一致性，提高模仿的准确性",
            "归一化位置估计器函数": "用于在时间维度上对齐不同领域的轨迹，确保示范的有效转移"
        },
        "success": true
    },
    {
        "order": 229,
        "title": "Cross-Gradient Aggregation for Decentralized Learning from Non-IID Data",
        "html": "https://ICML.cc//virtual/2021/poster/9767",
        "abstract": "Decentralized learning enables a group of collaborative agents to learn models using a distributed dataset without the need for a central parameter server. Recently, decentralized learning algorithms have demonstrated state-of-the-art results on benchmark data sets, comparable with centralized algorithms. However, the key assumption to achieve competitive performance is that the data is independently and identically distributed (IID) among the agents which, in real-life applications, is often not applicable. Inspired by ideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a novel decentralized learning algorithm where (i) each agent aggregates cross-gradient information, i.e., derivatives of its model with respect to its neighbors' datasets, and (ii) updates its model using a projected gradient based on quadratic programming (QP). We theoretically analyze the convergence characteristics of CGA and demonstrate its efficiency on non-IID data distributions sampled from the MNIST and CIFAR-10 datasets. Our empirical comparisons show superior learning performance of CGA over existing state-of-the-art decentralized learning algorithms, as well as maintaining the improved performance under information compression to reduce\npeer-to-peer communication overhead. The code is available here on GitHub.",
        "conference": "ICML",
        "中文标题": "非独立同分布数据下的去中心化学习中的交叉梯度聚合",
        "摘要翻译": "去中心化学习使得一组协作代理能够利用分布式数据集学习模型，而无需中央参数服务器。最近，去中心化学习算法在基准数据集上展示了与集中式算法相媲美的先进成果。然而，实现竞争性能的关键假设是数据在代理之间独立同分布（IID），这在现实应用中往往不适用。受持续学习思想的启发，我们提出了交叉梯度聚合（CGA），一种新颖的去中心化学习算法，其中（i）每个代理聚合交叉梯度信息，即其模型相对于邻居数据集的导数，以及（ii）使用基于二次规划（QP）的投影梯度更新其模型。我们从理论上分析了CGA的收敛特性，并在从MNIST和CIFAR-10数据集采样的非IID数据分布上证明了其效率。我们的实证比较显示，CGA在现有最先进的去中心化学习算法上具有优越的学习性能，同时在信息压缩以减少点对点通信开销的情况下保持改进的性能。代码可在GitHub上获取。",
        "领域": "分布式机器学习、非独立同分布数据学习、去中心化优化",
        "问题": "解决在非独立同分布（non-IID）数据下，去中心化学习算法的性能问题",
        "动机": "现实应用中数据往往不符合独立同分布假设，这限制了去中心化学习算法的应用和性能",
        "方法": "提出交叉梯度聚合（CGA）算法，通过聚合邻居数据集的梯度信息和使用基于二次规划的投影梯度更新模型",
        "关键词": [
            "去中心化学习",
            "非独立同分布数据",
            "交叉梯度聚合",
            "二次规划",
            "信息压缩"
        ],
        "涉及的技术概念": {
            "交叉梯度聚合": "算法中每个代理聚合其模型相对于邻居数据集的导数，以处理非IID数据",
            "二次规划": "用于模型更新的优化方法，确保在非IID数据下的有效学习",
            "信息压缩": "减少点对点通信开销的技术，同时保持算法性能"
        },
        "success": true
    },
    {
        "order": 230,
        "title": "Cross-model Back-translated Distillation for Unsupervised Machine Translation",
        "html": "https://ICML.cc//virtual/2021/poster/10009",
        "abstract": "Recent unsupervised machine translation (UMT) systems usually employ three main principles: initialization, language modeling and iterative back-translation, though they may apply them differently. Crucially, iterative back-translation and denoising auto-encoding for language modeling provide data diversity to train the UMT systems. However, the gains from these diversification processes has seemed to plateau. We introduce a novel component to the standard UMT framework called Cross-model Back-translated Distillation (CBD), that is aimed to induce another level of data diversification that existing principles lack. CBD is applicable to all previous UMT approaches. In our experiments, CBD achieves the state of the art in the WMT'14 English-French, WMT'16 English-German and English-Romanian bilingual unsupervised translation tasks, with 38.2, 30.1, and 36.3 BLEU respectively. It also yields 1.5--3.3 BLEU improvements in IWSLT English-French and English-German tasks. Through extensive experimental analyses, we show that CBD is effective because it embraces data diversity while other similar variants do not.",
        "conference": "ICML",
        "中文标题": "跨模型反向翻译蒸馏用于无监督机器翻译",
        "摘要翻译": "近期的无监督机器翻译（UMT）系统通常采用三个主要原则：初始化、语言建模和迭代反向翻译，尽管它们的应用方式可能有所不同。关键的是，迭代反向翻译和用于语言建模的去噪自编码为UMT系统提供了数据多样性。然而，这些多样化过程的收益似乎已经趋于平稳。我们在标准UMT框架中引入了一个称为跨模型反向翻译蒸馏（CBD）的新组件，旨在引入现有原则所缺乏的另一层次的数据多样化。CBD适用于所有先前的UMT方法。在我们的实验中，CBD在WMT'14英语-法语、WMT'16英语-德语和英语-罗马尼亚双语无监督翻译任务中达到了最先进的水平，分别获得了38.2、30.1和36.3的BLEU分数。在IWSLT英语-法语和英语-德语任务中，它也带来了1.5到3.3的BLEU分数提升。通过广泛的实验分析，我们表明CBD之所以有效，是因为它拥抱了数据多样性，而其他类似变体则没有。",
        "领域": "无监督机器翻译",
        "问题": "如何进一步提高无监督机器翻译系统的性能，尤其是在数据多样性利用方面",
        "动机": "现有的无监督机器翻译系统在数据多样性利用上的收益已经趋于平稳，需要新的方法来进一步提高性能",
        "方法": "引入跨模型反向翻译蒸馏（CBD）组件，以增加数据多样性",
        "关键词": [
            "无监督机器翻译",
            "跨模型反向翻译蒸馏",
            "数据多样性"
        ],
        "涉及的技术概念": {
            "跨模型反向翻译蒸馏": "一种新的数据多样化方法，旨在通过跨模型的反向翻译过程来增加训练数据的多样性",
            "无监督机器翻译": "不依赖平行语料库的机器翻译方法，通过初始化、语言建模和迭代反向翻译等原则实现",
            "BLEU分数": "用于评估机器翻译质量的指标，通过比较机器翻译输出和参考翻译之间的相似度来计算"
        },
        "success": true
    },
    {
        "order": 231,
        "title": "Crowdsourcing via Annotator Co-occurrence Imputation and Provable Symmetric Nonnegative Matrix Factorization",
        "html": "https://ICML.cc//virtual/2021/poster/10351",
        "abstract": "Unsupervised learning of the Dawid-Skene (D&S) model from noisy, incomplete and crowdsourced annotations has been a long-standing challenge, and is a critical step towards reliably labeling massive data. A recent work takes a coupled nonnegative matrix factorization (CNMF) perspective, and shows appealing features: It ensures the identifiability of the D\\&S model and enjoys low sample complexity, as only the estimates of the co-occurrences of annotator labels are involved. However, the identifiability holds only when certain somewhat restrictive conditions are met in the context of crowdsourcing. Optimizing the CNMF criterion is also costly---and convergence assurances are elusive. This work recasts the pairwise co-occurrence based D&S model learning problem as a symmetric NMF (SymNMF) problem---which offers enhanced identifiability relative to CNMF. In practice, the SymNMF model is often (largely) incomplete, due to the lack of co-labeled items by some annotators. Two lightweight algorithms are proposed for co-occurrence imputation. Then, a low-complexity shifted rectified linear unit (ReLU)-empowered SymNMF algorithm is proposed to identify the D&S model. Various performance characterizations (e.g., missing co-occurrence recoverability, stability, and convergence) and evaluations are also presented.\n\n",
        "conference": "ICML",
        "中文标题": "通过标注者共现插值与可证明对称非负矩阵分解实现的众包",
        "摘要翻译": "从嘈杂、不完整和众包的注释中无监督学习Dawid-Skene（D&S）模型一直是一个长期存在的挑战，并且是可靠标记海量数据的关键步骤。最近的一项工作采用了耦合非负矩阵分解（CNMF）的视角，并展示了吸引人的特点：它确保了D&S模型的可识别性，并享有低样本复杂性，因为仅涉及标注者标签共现的估计。然而，可识别性仅在众包背景下满足某些较为严格的条件时才成立。优化CNMF准则也是成本高昂的——且收敛保证难以捉摸。这项工作将基于成对共现的D&S模型学习问题重新表述为对称NMF（SymNMF）问题——相对于CNMF提供了增强的可识别性。在实践中，由于某些标注者缺乏共同标记的项目，SymNMF模型往往（在很大程度上）是不完整的。提出了两种轻量级算法用于共现插值。然后，提出了一种低复杂度的移位修正线性单元（ReLU）赋能的SymNMF算法来识别D&S模型。还提出了各种性能特征（例如，缺失共现的可恢复性、稳定性和收敛性）和评估。",
        "领域": "众包学习、非负矩阵分解、无监督学习",
        "问题": "从嘈杂、不完整和众包的注释中无监督学习Dawid-Skene模型",
        "动机": "为了解决在众包环境下可靠标记海量数据的挑战，并提高模型的可识别性和降低样本复杂性",
        "方法": "将问题重新表述为对称NMF问题，提出共现插值算法和低复杂度的ReLU-empowered SymNMF算法",
        "关键词": [
            "众包学习",
            "对称非负矩阵分解",
            "无监督学习",
            "共现插值",
            "Dawid-Skene模型"
        ],
        "涉及的技术概念": {
            "对称非负矩阵分解（SymNMF）": "用于增强Dawid-Skene模型的可识别性，相对于耦合非负矩阵分解（CNMF）",
            "共现插值": "用于处理由于缺乏共同标记项目而导致的不完整SymNMF模型",
            "移位修正线性单元（ReLU）": "在SymNMF算法中用于识别Dawid-Skene模型，提高算法的效率和性能"
        },
        "success": true
    },
    {
        "order": 232,
        "title": "CRPO: A New Approach for Safe Reinforcement Learning with Convergence Guarantee",
        "html": "https://ICML.cc//virtual/2021/poster/8995",
        "abstract": "In safe reinforcement learning (SRL) problems, an agent explores the environment to maximize an expected total reward and meanwhile avoids violation of certain constraints on a number of expected total costs. In general, such SRL problems have nonconvex objective functions subject to multiple nonconvex constraints, and hence are very challenging to solve, particularly to provide a globally optimal policy. Many popular SRL algorithms adopt a primal-dual structure which utilizes the updating of dual variables for satisfying the constraints. In contrast, we propose a primal approach, called constraint-rectified policy optimization (CRPO), which updates the policy alternatingly between objective improvement and constraint satisfaction. CRPO provides a primal-type algorithmic framework to solve SRL problems, where each policy update can take any variant of policy optimization step. To demonstrate the theoretical performance of CRPO, we adopt natural policy gradient (NPG) for each policy update step and show that CRPO achieves an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate to the global optimal policy in the constrained policy set and an $\\mathcal{O}(1/\\sqrt{T})$ error bound on constraint satisfaction. This is the first finite-time analysis of primal SRL algorithms with global optimality guarantee. Our empirical results demonstrate that CRPO can outperform the existing primal-dual baseline algorithms significantly.",
        "conference": "ICML",
        "success": true,
        "中文标题": "CRPO: 一种具有收敛保证的安全强化学习新方法",
        "摘要翻译": "在安全强化学习(SRL)问题中，智能体探索环境以最大化预期总奖励，同时避免违反对预期总成本数量的某些约束。一般来说，这种SRL问题具有非凸目标函数，受多个非凸约束的约束，因此非常具有挑战性，特别是要提供全局最优策略。许多流行的SRL算法采用原始-对偶结构，该结构利用对偶变量的更新来满足约束。相比之下，我们提出了一种原始方法，称为约束校正策略优化(CRPO)，它在目标改进和约束满足之间交替更新策略。CRPO提供了一种原始类型的算法框架来解决SRL问题，其中每个策略更新都可以采用策略优化步骤的任何变体。为了证明CRPO的理论性能，我们为每个策略更新步骤采用自然策略梯度(NPG)，并表明CRPO在约束策略集中实现了到全局最优策略的收敛速度，并且在约束满足方面实现了误差界限。这是对具有全局最优性保证的原始SRL算法的第一个有限时间分析。我们的经验结果表明，CRPO可以显著优于现有的原始-对偶基线算法。",
        "领域": "强化学习、安全强化学习、策略优化",
        "问题": "解决安全强化学习中非凸目标函数和非凸约束带来的策略优化难题，提供全局最优策略。",
        "动机": "现有的安全强化学习算法大多采用原始-对偶结构，计算复杂，且难以保证全局最优性。因此，研究者希望提出一种原始方法，能够更有效地解决安全强化学习问题，并提供全局最优性保证。",
        "方法": "提出了一种名为约束校正策略优化(CRPO)的原始方法，该方法在目标改进和约束满足之间交替更新策略。使用自然策略梯度(NPG)进行策略更新，并进行了有限时间分析，证明了其收敛性和误差界限。",
        "关键词": [
            "安全强化学习",
            "约束校正策略优化",
            "自然策略梯度",
            "全局最优性",
            "策略优化"
        ],
        "涉及的技术概念": {
            "安全强化学习": "在强化学习中，智能体的目标不仅是最大化奖励，还要避免违反某些约束条件，例如避免进入危险区域或超过预算。",
            "约束校正策略优化": "一种原始优化方法，通过交替更新策略以改进目标并满足约束条件，从而找到安全且高效的策略。"
        }
    },
    {
        "order": 233,
        "title": "Crystallization Learning with the Delaunay Triangulation",
        "html": "https://ICML.cc//virtual/2021/poster/8829",
        "abstract": "Based on the Delaunay triangulation, we propose the crystallization learning to estimate the conditional expectation function in the framework of nonparametric regression. By conducting the crystallization search for the Delaunay simplices closest to the target point in a hierarchical way, the crystallization learning estimates the conditional expectation of the response by fitting a local linear model to the data points of the constructed Delaunay simplices. Instead of conducting the Delaunay triangulation for the entire feature space which would encounter enormous computational difficulty, our approach focuses only on the neighborhood of the target point and thus greatly expedites the estimation for high-dimensional cases. Because the volumes of Delaunay simplices are adaptive to the density of feature data points, our method selects neighbor data points uniformly in all directions and thus is more robust to the local geometric structure of the data than existing nonparametric regression methods. We develop the asymptotic properties of the crystallization learning and conduct numerical experiments on both synthetic and real data to demonstrate the advantages of our method in estimation of the conditional expectation function and prediction of the response.",
        "conference": "ICML",
        "中文标题": "基于Delaunay三角剖分的结晶学习",
        "摘要翻译": "基于Delaunay三角剖分，我们提出了结晶学习来估计非参数回归框架中的条件期望函数。通过以分层方式进行对最接近目标点的Delaunay单纯形的结晶搜索，结晶学习通过将局部线性模型拟合到构建的Delaunay单纯形的数据点来估计响应的条件期望。我们的方法不是对整个特征空间进行Delaunay三角剖分，这样会遇到巨大的计算困难，而是仅关注目标点的邻域，从而大大加快了高维情况下的估计速度。由于Delaunay单纯形的体积适应于特征数据点的密度，我们的方法在所有方向上均匀选择邻近数据点，因此比现有的非参数回归方法对数据的局部几何结构更加稳健。我们发展了结晶学习的渐近性质，并在合成和真实数据上进行了数值实验，以证明我们的方法在条件期望函数的估计和响应的预测方面的优势。",
        "领域": "非参数回归、高维数据分析、机器学习",
        "问题": "在高维非参数回归中高效估计条件期望函数",
        "动机": "解决传统方法在高维数据中计算量大和对数据局部几何结构敏感的问题",
        "方法": "通过分层结晶搜索构建局部Delaunay单纯形，并拟合局部线性模型来估计条件期望",
        "关键词": [
            "结晶学习",
            "Delaunay三角剖分",
            "非参数回归",
            "高维数据",
            "局部线性模型"
        ],
        "涉及的技术概念": {
            "Delaunay三角剖分": "用于构建数据点的几何结构，以便于局部线性模型的拟合",
            "结晶学习": "一种通过分层搜索最接近目标点的Delaunay单纯形来估计条件期望函数的方法",
            "局部线性模型": "在构建的Delaunay单纯形上拟合的模型，用于估计目标点的条件期望"
        },
        "success": true
    },
    {
        "order": 234,
        "title": "Cumulants of Hawkes Processes are Robust to Observation Noise",
        "html": "https://ICML.cc//virtual/2021/poster/10601",
        "abstract": "Multivariate Hawkes processes (MHPs) are widely used in a variety of fields to model the occurrence of causally related discrete events in continuous time. Most state-of-the-art approaches address the problem of learning MHPs from perfect traces without noise. In practice, the process through which events are collected might introduce noise in the timestamps. In this work, we address the problem of learning the causal structure of MHPs when the observed timestamps of events are subject to random and unknown shifts, also known as random translations. We prove that the cumulants of MHPs are invariant to random translations, and therefore can be used to learn their underlying causal structure. Furthermore, we empirically characterize the effect of random translations on state-of-the-art learning methods. We show that maximum likelihood-based estimators are brittle, while cumulant-based estimators remain stable even in the presence of significant time shifts.",
        "conference": "ICML",
        "中文标题": "霍克斯过程的累积量对观测噪声具有鲁棒性",
        "摘要翻译": "多元霍克斯过程（MHP）在多个领域被广泛用于模拟连续时间中因果相关的离散事件的发生。大多数最先进的方法解决了从无噪声的完美轨迹中学习MHP的问题。实际上，事件收集的过程可能会在时间戳中引入噪声。在这项工作中，我们解决了当观察到的事件时间戳受到随机和未知偏移（也称为随机平移）时学习MHP的因果结构的问题。我们证明了MHP的累积量对随机平移是不变的，因此可以用来学习它们的基础因果结构。此外，我们通过实验描述了随机平移对最先进学习方法的影响。我们表明，基于最大似然的估计器是脆弱的，而基于累积量的估计器即使在存在显著时间偏移的情况下也保持稳定。",
        "领域": "时间序列分析, 因果推理, 机器学习",
        "问题": "学习在事件时间戳受到随机和未知偏移时的多元霍克斯过程的因果结构",
        "动机": "解决实际应用中事件收集过程中时间戳噪声对学习多元霍克斯过程因果结构的影响",
        "方法": "利用累积量的不变性来学习因果结构，并比较基于最大似然和基于累积量的估计器在时间偏移下的表现",
        "关键词": [
            "多元霍克斯过程",
            "累积量",
            "因果结构学习",
            "时间偏移",
            "鲁棒性"
        ],
        "涉及的技术概念": {
            "多元霍克斯过程": "用于模拟连续时间中因果相关的离散事件发生的模型",
            "累积量": "用于描述随机变量分布的高阶统计量，对随机平移具有不变性",
            "因果结构学习": "从观测数据中推断变量间因果关系的技术"
        },
        "success": true
    },
    {
        "order": 235,
        "title": "CURI: A Benchmark for Productive Concept Learning Under Uncertainty",
        "html": "https://ICML.cc//virtual/2021/poster/9597",
        "abstract": "Humans can learn and reason under substantial uncertainty in a space of infinitely many compositional, productive concepts. For example, if a scene with two blue spheres qualifies as “daxy,” one can reason that the underlying concept may require scenes to have “only blue spheres” or “only spheres” or “only two objects.” In contrast, standard benchmarks for compositional reasoning do not explicitly capture a notion of reasoning under uncertainty or evaluate compositional concept acquisition. We introduce a new benchmark, Compositional Reasoning Under Uncertainty (CURI) that instantiates a series of few-shot, meta-learning tasks in a productive concept space to evaluate different aspects of systematic generalization under uncertainty, including splits that test abstract understandings of disentangling, productive generalization, learning boolean operations, variable binding, etc. Importantly, we also contribute a model-independent “compositionality gap” to evaluate the difficulty of generalizing out-of-distribution along each of these axes, allowing objective comparison of the difficulty of each compositional split. Evaluations across a range of modeling choices and splits reveal substantial room for improvement on the proposed benchmark.",
        "conference": "ICML",
        "中文标题": "CURI：不确定性下高效概念学习的基准",
        "摘要翻译": "人类能够在无限多组合性、高效性概念的空间中，在相当大的不确定性下学习和推理。例如，如果一个包含两个蓝色球体的场景被认定为“daxy”，人们可以推理出潜在的概念可能要求场景“只有蓝色球体”或“只有球体”或“只有两个物体”。相比之下，标准组合推理基准并未明确捕捉到不确定性下的推理概念或评估组合概念的获取。我们引入了一个新的基准，即不确定性下的组合推理（CURI），它在高效概念空间中实例化了一系列少样本、元学习任务，以评估不确定性下系统泛化的不同方面，包括测试解缠结的抽象理解、高效泛化、学习布尔运算、变量绑定等的分割。重要的是，我们还贡献了一个模型无关的“组合性差距”，以评估沿着这些轴线的分布外泛化的难度，从而允许客观比较每个组合分割的难度。对一系列建模选择和分割的评估揭示了在提出的基准上有很大的改进空间。",
        "领域": "元学习",
        "问题": "评估和比较在不确定性下进行组合推理和概念学习的模型性能",
        "动机": "现有标准基准未能充分评估不确定性下的组合推理和概念学习能力，需要一个新的基准来填补这一空白",
        "方法": "引入CURI基准，包含少样本、元学习任务，评估系统泛化的多个方面，并提出模型无关的“组合性差距”来衡量泛化难度",
        "关键词": [
            "组合推理",
            "不确定性学习",
            "元学习",
            "系统泛化",
            "概念学习"
        ],
        "涉及的技术概念": {
            "组合性差距": "用于衡量模型在组合推理任务中分布外泛化的难度，允许对不同任务难度进行客观比较",
            "少样本学习": "在数据稀缺的情况下学习新概念或任务，模拟人类快速学习能力",
            "元学习": "学习如何学习，旨在提高模型在新任务上的适应速度和效率"
        },
        "success": true
    },
    {
        "order": 236,
        "title": "Cyclically Equivariant Neural Decoders for Cyclic Codes",
        "html": "https://ICML.cc//virtual/2021/poster/10677",
        "abstract": "Neural decoders were introduced as a generalization of the classic Belief Propagation (BP) decoding algorithms, where the Trellis graph in the BP algorithm is viewed as a neural network, and the weights in the Trellis graph are optimized by training the neural network. In this work, we propose a novel neural decoder for cyclic codes by exploiting their cyclically invariant property. More precisely, we impose a shift invariant structure on the weights of our neural decoder so that any cyclic shift of inputs results in the same cyclic shift of outputs. Extensive simulations with BCH codes and punctured Reed-Muller (RM) codes show that our new decoder consistently outperforms previous neural decoders when decoding cyclic codes. Finally, we propose a list decoding procedure that can significantly reduce the decoding error probability for BCH codes and punctured RM codes. For certain high-rate codes, the gap between our list decoder and the Maximum Likelihood decoder is less than $0.1$dB. Code available at github.com/cyclicallyneuraldecoder",
        "conference": "ICML",
        "中文标题": "循环等变神经解码器用于循环码",
        "摘要翻译": "神经解码器被引入作为经典置信传播（BP）解码算法的泛化，其中BP算法中的Trellis图被视为神经网络，并且通过训练神经网络来优化Trellis图中的权重。在这项工作中，我们通过利用循环码的循环不变性质，提出了一种新颖的神经解码器。更准确地说，我们在神经解码器的权重上施加了一个移位不变结构，使得输入的任何循环移位都会导致输出的相同循环移位。与BCH码和穿孔Reed-Muller（RM）码的广泛模拟表明，我们的新解码器在解码循环码时始终优于以前的神经解码器。最后，我们提出了一种列表解码程序，可以显著降低BCH码和穿孔RM码的解码错误概率。对于某些高码率码，我们的列表解码器与最大似然解码器之间的差距小于0.1dB。代码可在github.com/cyclicallyneuraldecoder获取。",
        "领域": "编码理论、神经网络应用、通信系统",
        "问题": "如何提高循环码的解码效率和准确性",
        "动机": "利用循环码的循环不变性质，开发一种新的神经解码器，以提高解码性能",
        "方法": "在神经解码器的权重上施加移位不变结构，利用循环码的特性优化解码过程，并通过列表解码进一步降低错误概率",
        "关键词": [
            "循环码",
            "神经解码器",
            "置信传播",
            "列表解码",
            "移位不变性"
        ],
        "涉及的技术概念": {
            "神经解码器": "将传统的置信传播解码算法泛化为神经网络形式，通过训练优化解码性能",
            "循环不变性": "利用循环码的特性，确保解码器对输入的循环移位具有相同的输出循环移位",
            "列表解码": "一种解码技术，通过生成多个候选解码结果来显著降低解码错误概率"
        },
        "success": true
    },
    {
        "order": 237,
        "title": "DAGs with No Curl: An Efficient DAG Structure Learning Approach",
        "html": "https://ICML.cc//virtual/2021/poster/8423",
        "abstract": "Recently directed acyclic graph (DAG) structure learning is formulated as a constrained continuous optimization problem with continuous acyclicity constraints and was solved iteratively through subproblem optimization. To further improve efficiency, we propose a novel learning framework to model and learn the weighted adjacency matrices in the DAG space directly. Specifically, we first show that the set of weighted adjacency matrices of DAGs are equivalent to the set of weighted gradients of graph potential functions, and one may perform structure learning by searching in this equivalent set of DAGs. To instantiate this idea,  we propose a new algorithm,  DAG-NoCurl, which  solves the optimization problem efficiently with a two-step procedure: $1)$ first we find an initial non-acyclic solution to the optimization problem, and $2)$ then we employ the Hodge decomposition of graphs and learn an acyclic graph  by projecting the non-acyclic graph to the gradient of a potential function.  Experimental studies on benchmark datasets demonstrate that our method provides comparable accuracy but better efficiency than baseline DAG structure learning methods on both linear and generalized structural equation models, often by more than one order of magnitude.",
        "conference": "ICML",
        "success": true,
        "中文标题": "无旋有向无环图：一种高效的有向无环图结构学习方法",
        "摘要翻译": "最近，有向无环图（DAG）结构学习被表述为一个带有连续无环性约束的约束连续优化问题，并通过子问题优化迭代求解。为了进一步提高效率，我们提出了一种新颖的学习框架，直接在DAG空间中建模和学习加权邻接矩阵。具体来说，我们首先展示了DAG的加权邻接矩阵集合等价于图势函数的加权梯度集合，并且可以通过在这个等价的DAG集合中进行搜索来执行结构学习。为了实例化这一想法，我们提出了一种新算法DAG-NoCurl，该算法通过两步程序高效地解决了优化问题：1）首先我们找到优化问题的初始非环解，2）然后我们利用图的Hodge分解，通过将非环图投影到势函数的梯度上来学习一个无环图。在基准数据集上的实验研究表明，我们的方法在线性和广义结构方程模型上提供了与基线DAG结构学习方法相当的准确性，但效率更高，通常超过一个数量级。",
        "领域": "图结构学习、优化算法、结构方程模型",
        "问题": "如何高效地学习有向无环图（DAG）的结构",
        "动机": "提高有向无环图结构学习的效率，减少计算成本",
        "方法": "提出了一种直接在DAG空间中建模和学习加权邻接矩阵的新框架，通过两步优化过程实现高效学习",
        "关键词": [
            "有向无环图",
            "结构学习",
            "优化算法",
            "Hodge分解",
            "结构方程模型"
        ],
        "涉及的技术概念": {
            "加权邻接矩阵": "用于表示图中节点间连接强度的矩阵，是DAG结构学习的核心对象",
            "Hodge分解": "一种图分解方法，用于将图分解为梯度部分和旋度部分，帮助实现无环性",
            "结构方程模型": "用于建模变量间因果关系的统计模型，DAG结构学习在此模型中有广泛应用"
        }
    },
    {
        "order": 238,
        "title": "DANCE: Enhancing saliency maps using decoys",
        "html": "https://ICML.cc//virtual/2021/poster/9701",
        "abstract": "Saliency methods can make deep neural network predictions more interpretable by identifying a set of critical features in an input sample, such as pixels that contribute most strongly to a prediction made by an image classifier. Unfortunately, recent evidence suggests that many saliency methods poorly perform, especially in situations where gradients are saturated, inputs contain adversarial perturbations, or predictions rely upon inter-feature dependence. To address these issues, we propose a framework, DANCE, which improves the robustness of saliency methods by following a two-step procedure. First, we introduce a perturbation mechanism that subtly varies the input sample without changing its intermediate representations. Using this approach, we can gather a corpus of perturbed ('decoy') data samples while ensuring that the perturbed and original input samples follow similar distributions. Second, we compute saliency maps for the decoy samples and propose a new method to aggregate saliency maps. With this design, we offset influence of gradient saturation. From a theoretical perspective, we show that the aggregated saliency map not only captures inter-feature dependence but, more importantly, is robust against previously described adversarial perturbation methods. Our empirical results suggest that, both qualitatively and quantitatively, DANCE outperforms existing methods in a variety of application domains.",
        "conference": "ICML",
        "中文标题": "DANCE：利用诱饵增强显著性图",
        "摘要翻译": "显著性方法可以通过识别输入样本中的一组关键特征（如对图像分类器预测贡献最大的像素）来使深度神经网络预测更加可解释。然而，最近的证据表明，许多显著性方法表现不佳，特别是在梯度饱和、输入包含对抗性扰动或预测依赖于特征间依赖性的情况下。为了解决这些问题，我们提出了一个框架DANCE，该框架通过遵循两步程序来提高显著性方法的鲁棒性。首先，我们引入了一种扰动机制，该机制在不改变其中间表示的情况下微妙地变化输入样本。使用这种方法，我们可以收集一组扰动（'诱饵'）数据样本，同时确保扰动和原始输入样本遵循相似的分布。其次，我们计算诱饵样本的显著性图，并提出了一种新的方法来聚合显著性图。通过这种设计，我们抵消了梯度饱和的影响。从理论角度来看，我们表明聚合的显著性图不仅捕捉了特征间的依赖性，而且更重要的是，对先前描述的对抗性扰动方法是鲁棒的。我们的实证结果表明，无论是定性还是定量，DANCE在各种应用领域中都优于现有方法。",
        "领域": "图像分类、对抗性机器学习、模型可解释性",
        "问题": "提高显著性方法在梯度饱和、对抗性扰动和特征间依赖性情况下的鲁棒性",
        "动机": "解决现有显著性方法在特定条件下表现不佳的问题，提高深度神经网络预测的可解释性",
        "方法": "提出DANCE框架，通过引入扰动机制生成诱饵样本并聚合其显著性图，以提高鲁棒性",
        "关键词": [
            "显著性图",
            "对抗性扰动",
            "梯度饱和",
            "模型可解释性",
            "深度神经网络"
        ],
        "涉及的技术概念": {
            "显著性图": "用于识别对模型预测贡献最大的输入特征，增强模型预测的可解释性",
            "对抗性扰动": "对输入样本进行微小修改以误导模型预测的技术，DANCE框架旨在提高对此类扰动的鲁棒性",
            "梯度饱和": "在训练深度神经网络时，梯度变得非常小，导致学习速度减慢或停止的现象，DANCE通过聚合显著性图来抵消其影响"
        },
        "success": true
    },
    {
        "order": 239,
        "title": "Dash: Semi-Supervised Learning with Dynamic Thresholding",
        "html": "https://ICML.cc//virtual/2021/poster/10121",
        "abstract": "While semi-supervised learning (SSL) has received tremendous attentions in many machine learning tasks due to its successful use of unlabeled data, existing SSL algorithms use either all unlabeled examples or the unlabeled examples with a fixed high-confidence prediction during the training progress. However, it is possible that too many correct/wrong pseudo labeled examples are eliminated/selected. In this work we develop a simple yet powerful framework, whose key idea is to select a subset of training examples from the unlabeled data when performing existing SSL methods so that only the unlabeled examples with pseudo labels related to the labeled data will be used to train models. The selection is performed at each updating iteration by only keeping the examples whose losses are smaller than a given threshold that is dynamically adjusted through the iteration. Our proposed approach, Dash, enjoys its adaptivity in terms of unlabeled data selection and its theoretical guarantee. Specifically, we theoretically establish the convergence rate of Dash from the view of non-convex optimization. Finally, we empirically demonstrate the effectiveness of the proposed method in comparison with state-of-the-art over benchmarks.",
        "conference": "ICML",
        "中文标题": "Dash：基于动态阈值的半监督学习",
        "摘要翻译": "尽管半监督学习（SSL）因其成功利用未标记数据而在许多机器学习任务中受到极大关注，现有的SSL算法在训练过程中要么使用所有未标记样本，要么使用具有固定高置信度预测的未标记样本。然而，这可能导致过多正确/错误的伪标记样本被排除/选择。在这项工作中，我们开发了一个简单而强大的框架，其关键思想是在执行现有SSL方法时从未标记数据中选择一个训练样本子集，这样只有与标记数据相关的伪标记未标记样本将被用于训练模型。选择是在每次更新迭代时进行的，仅保留损失小于通过迭代动态调整的给定阈值的样本。我们提出的方法Dash，在其未标记数据选择的适应性和理论保证方面具有优势。具体来说，我们从非凸优化的角度理论上建立了Dash的收敛速度。最后，我们通过实验证明了所提出方法与现有技术相比在基准测试中的有效性。",
        "领域": "半监督学习、深度学习优化、机器学习理论",
        "问题": "解决在半监督学习中如何更有效地选择未标记样本进行训练的问题",
        "动机": "提高半监督学习算法在利用未标记数据时的效率和准确性",
        "方法": "提出了一种动态调整阈值的方法来选择未标记样本，仅使用与标记数据相关的伪标记样本进行训练",
        "关键词": [
            "半监督学习",
            "动态阈值",
            "非凸优化",
            "伪标记",
            "收敛速度"
        ],
        "涉及的技术概念": {
            "动态阈值": "在每次迭代中动态调整的阈值，用于选择损失较小的未标记样本",
            "伪标记": "为未标记数据生成的标签，用于训练模型",
            "非凸优化": "用于理论上分析Dash方法的收敛速度的数学框架"
        },
        "success": true
    },
    {
        "order": 240,
        "title": "Data augmentation for deep learning based accelerated MRI reconstruction with limited data",
        "html": "https://ICML.cc//virtual/2021/poster/9591",
        "abstract": "Deep neural networks have emerged as very successful tools for image restoration and reconstruction tasks. These networks are often trained end-to-end to directly reconstruct an image from a noisy or corrupted measurement of that image. \nTo achieve state-of-the-art performance, training on large and diverse sets of images is considered critical. However, it is often difficult and/or expensive to collect large amounts of training images. Inspired by the success of Data Augmentation (DA) for classification problems, in this paper, we propose a pipeline for data augmentation for accelerated MRI reconstruction and study its effectiveness at reducing the required training data in a variety of settings. Our DA pipeline, MRAugment, is specifically designed to utilize the invariances present in medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. \nThrough extensive studies on multiple datasets we demonstrate that in the low-data regime DA prevents overfitting and can match or even surpass the state of the art while using significantly fewer training data, whereas in the high-data regime it has diminishing returns. Furthermore, our findings show that DA improves the robustness of the model against various shifts in the test distribution.",
        "conference": "ICML",
        "中文标题": "基于深度学习的有限数据加速MRI重建的数据增强方法",
        "摘要翻译": "深度神经网络已成为图像恢复和重建任务中非常成功的工具。这些网络通常端到端训练，直接从噪声或损坏的图像测量中重建图像。为了达到最先进的性能，对大量多样化图像集的训练被认为是至关重要的。然而，收集大量训练图像往往既困难又昂贵。受到数据增强（DA）在分类问题中成功的启发，本文提出了一种用于加速MRI重建的数据增强流程，并研究了其在多种设置下减少所需训练数据的有效性。我们的DA流程MRAugment专门设计用于利用医学成像测量中存在的不变性，因为忽视问题物理特性的朴素DA策略会失败。通过对多个数据集的广泛研究，我们证明在数据量较少的情况下，DA可以防止过拟合并能匹配甚至超越使用显著较少训练数据时的最先进技术，而在数据量较多的情况下，其回报递减。此外，我们的发现表明，DA提高了模型对测试分布中各种变化的鲁棒性。",
        "领域": "医学图像重建、深度学习应用、数据增强技术",
        "问题": "在有限训练数据下如何提高加速MRI重建的深度学习模型性能",
        "动机": "解决因训练数据收集困难和昂贵导致的深度学习模型在加速MRI重建任务中性能受限的问题",
        "方法": "提出了一种专门针对加速MRI重建的数据增强流程MRAugment，利用医学成像测量中的不变性进行有效的数据增强",
        "关键词": [
            "数据增强",
            "MRI重建",
            "深度学习",
            "医学图像处理",
            "过拟合防止"
        ],
        "涉及的技术概念": {
            "数据增强（DA）": "通过人工方式扩展训练数据集，以提高模型的泛化能力和防止过拟合",
            "端到端训练": "直接从输入到输出进行模型训练，无需手动设计中间处理步骤",
            "医学成像测量中的不变性": "在医学图像采集过程中存在的特定物理特性，数据增强策略需考虑这些特性以保持重建图像的真实性"
        },
        "success": true
    },
    {
        "order": 241,
        "title": "Data Augmentation for Meta-Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10493",
        "abstract": "Conventional image classifiers are trained by randomly sampling mini-batches of images. To achieve state-of-the-art performance, practitioners use sophisticated data augmentation schemes to expand the amount of training data available for sampling. In contrast, meta-learning algorithms sample support data, query data, and tasks on each training step. In this complex sampling scenario, data augmentation can be used not only to expand the number of images available per class, but also to generate entirely new classes/tasks. We systematically dissect the meta-learning pipeline and investigate the distinct ways in which data augmentation can be integrated at both the image and class levels. Our proposed meta-specific data augmentation significantly improves the performance of meta-learners on few-shot classification benchmarks.",
        "conference": "ICML",
        "中文标题": "元学习的数据增强",
        "摘要翻译": "传统的图像分类器通过随机采样小批量图像进行训练。为了达到最先进的性能，实践者使用复杂的数据增强方案来扩展可用于采样的训练数据量。相比之下，元学习算法在每一步训练中采样支持数据、查询数据和任务。在这种复杂的采样场景中，数据增强不仅可以用于扩展每个类别可用的图像数量，还可以生成全新的类别/任务。我们系统地剖析了元学习流程，并研究了在图像和类别级别上整合数据增强的不同方式。我们提出的元特定数据增强显著提高了元学习者在少样本分类基准上的性能。",
        "领域": "元学习、少样本学习、图像分类",
        "问题": "如何在元学习中有效地应用数据增强以提升少样本分类性能",
        "动机": "探索在元学习框架下，通过数据增强扩展训练数据量和生成新任务的可能性，以提高模型在少样本学习任务中的表现",
        "方法": "系统地分析元学习流程，提出在图像和类别级别上整合数据增强的方法，并通过实验验证其有效性",
        "关键词": [
            "元学习",
            "数据增强",
            "少样本分类",
            "图像分类",
            "任务生成"
        ],
        "涉及的技术概念": {
            "元学习": "一种学习如何学习的方法，旨在通过少量样本快速适应新任务",
            "数据增强": "通过变换原始数据生成新的训练样本，以增加数据的多样性和数量",
            "少样本分类": "在只有少量标注样本的情况下，训练模型进行有效的分类任务"
        },
        "success": true
    },
    {
        "order": 242,
        "title": "Data-driven Prediction of General Hamiltonian Dynamics via Learning Exactly-Symplectic Maps",
        "html": "https://ICML.cc//virtual/2021/poster/9377",
        "abstract": "We consider the learning and prediction of nonlinear time series generated by a latent symplectic map. A special case is (not necessarily separable) Hamiltonian systems, whose solution flows give such symplectic maps. For this special case, both generic approaches based on learning the vector field of the latent ODE and specialized approaches based on learning the Hamiltonian that generates the vector field exist. Our method, however, is different as it does not rely on the vector field nor assume its existence; instead, it directly learns the symplectic evolution map in discrete time. Moreover, we do so by representing the symplectic map via a generating function, which we approximate by a neural network (hence the name GFNN). This way, our approximation of the evolution map is always \\emph{exactly} symplectic. This additional geometric structure allows the local prediction error at each step to accumulate in a controlled fashion, and we will prove, under reasonable assumptions, that the global prediction error grows at most \\emph{linearly} with long prediction time, which significantly improves an otherwise exponential growth. In addition, as a map-based and thus purely data-driven method, GFNN avoids two additional sources of inaccuracies common in vector-field based approaches, namely the error in approximating the vector field by finite difference of the data, and the error in numerical integration of the vector field for making predictions. Numerical experiments further demonstrate our claims.",
        "conference": "ICML",
        "中文标题": "通过精确辛映射学习实现一般哈密顿动力学的数据驱动预测",
        "摘要翻译": "我们考虑学习和预测由潜在辛映射生成的非线性时间序列。一个特例是（不一定可分离的）哈密顿系统，其解流给出了这样的辛映射。对于这一特例，存在基于学习潜在ODE向量场的通用方法和基于学习生成向量场的哈密顿量的专门方法。然而，我们的方法不同，因为它不依赖于向量场，也不假设其存在；相反，它直接在离散时间学习辛演化映射。此外，我们通过生成函数表示辛映射，并通过神经网络（因此称为GFNN）来近似这个生成函数。这样，我们对演化映射的近似总是精确辛的。这种额外的几何结构允许每一步的局部预测误差以受控方式累积，并且我们将在合理的假设下证明，全局预测误差最多随预测时间线性增长，这显著改善了原本的指数增长。此外，作为一种基于映射的纯数据驱动方法，GFNN避免了基于向量场的方法中常见的两个额外的不准确性来源，即通过数据的有限差分近似向量场的误差，以及为进行预测而对向量场进行数值积分的误差。数值实验进一步验证了我们的主张。",
        "领域": "动力系统学习、哈密顿系统、时间序列预测",
        "问题": "如何准确预测由潜在辛映射生成的非线性时间序列，特别是哈密顿系统的动力学行为。",
        "动机": "现有的基于向量场的方法存在近似误差和数值积分误差，且全局预测误差可能随时间指数增长，需要一种更准确且误差增长可控的预测方法。",
        "方法": "通过生成函数表示辛映射，并利用神经网络近似这个生成函数，直接在离散时间学习辛演化映射，确保映射的精确辛性。",
        "关键词": [
            "辛映射",
            "哈密顿系统",
            "神经网络",
            "时间序列预测",
            "数据驱动"
        ],
        "涉及的技术概念": {
            "辛映射": "在论文中用于保持哈密顿系统几何结构的演化映射，确保预测的物理准确性。",
            "生成函数": "用于表示辛映射的数学工具，通过神经网络近似以实现数据驱动的学习。",
            "全局预测误差": "论文中证明了在合理假设下，该方法使得全局预测误差最多随时间线性增长，显著优于传统方法的指数增长。"
        },
        "success": true
    },
    {
        "order": 243,
        "title": "Data-efficient Hindsight Off-policy Option Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10751",
        "abstract": "We introduce Hindsight Off-policy Options (HO2), a data-efficient option learning algorithm. Given any trajectory, HO2 infers likely option choices and backpropagates through the dynamic programming inference procedure to robustly train all policy components off-policy and end-to-end. The approach outperforms existing option learning methods on common benchmarks. To better understand the option framework and disentangle benefits from both temporal and action abstraction, we evaluate ablations with flat policies and mixture policies with comparable optimization. The results highlight the importance of both types of abstraction as well as off-policy training and trust-region constraints, particularly in challenging, simulated 3D robot manipulation tasks from raw pixel inputs. Finally, we intuitively adapt the inference step to investigate the effect of increased temporal abstraction on training with pre-trained options and from scratch.",
        "conference": "ICML",
        "中文标题": "数据高效的后见之明离策略选项学习",
        "摘要翻译": "我们介绍了后见之明离策略选项（HO2），一种数据高效的选项学习算法。给定任何轨迹，HO2推断可能的选项选择，并通过动态规划推理过程反向传播，以稳健地端到端训练所有策略组件。该方法在常见基准上优于现有的选项学习方法。为了更好地理解选项框架并分离时间和动作抽象的好处，我们评估了与平面策略和具有可比优化的混合策略的消融。结果突出了两种抽象类型以及离策略训练和信任区域约束的重要性，特别是在从原始像素输入进行的具有挑战性的模拟3D机器人操作任务中。最后，我们直观地调整推理步骤，以研究增加时间抽象对使用预训练选项和从头开始训练的影响。",
        "领域": "强化学习、机器人操作、深度学习",
        "问题": "提高选项学习算法的数据效率和训练效果",
        "动机": "为了解决现有选项学习方法在数据效率和训练效果上的不足，特别是在复杂的3D机器人操作任务中",
        "方法": "提出了一种名为Hindsight Off-policy Options (HO2)的数据高效选项学习算法，通过动态规划推理过程反向传播来训练策略组件",
        "关键词": [
            "后见之明学习",
            "离策略训练",
            "选项学习",
            "动态规划",
            "机器人操作"
        ],
        "涉及的技术概念": {
            "后见之明学习": "在强化学习中，利用未来的信息来改进过去的决策",
            "离策略训练": "允许学习算法从不同于当前策略生成的数据中学习",
            "动态规划": "一种通过将问题分解为子问题来解决问题的数学方法，用于HO2算法中的推理过程"
        },
        "success": true
    },
    {
        "order": 244,
        "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10091",
        "abstract": "Federated Learning (FL) is a decentralized machine-learning paradigm, in which a global server iteratively averages the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly averaging their model parameters. This approach, however, depends on a  proxy dataset, making it impractical unless such a prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. Inspired by the prior art, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. \n\nEmpirical studies powered by theoretical implications show that our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.",
        "conference": "ICML",
        "中文标题": "异构联邦学习的无数据知识蒸馏",
        "摘要翻译": "联邦学习（FL）是一种去中心化的机器学习范式，其中全局服务器迭代地平均本地用户的模型参数，而无需访问其数据。用户异构性给FL带来了重大挑战，可能导致全局模型漂移，收敛速度慢。知识蒸馏最近被提出来解决这个问题，通过利用异构用户的聚合知识来精炼服务器模型，而不是直接平均其模型参数。然而，这种方法依赖于一个代理数据集，除非满足这一前提条件，否则不切实际。此外，集成知识并未充分利用来指导本地模型学习，这可能反过来影响聚合模型的质量。受现有技术的启发，我们提出了一种无数据知识蒸馏方法来解决异构FL问题，其中服务器学习一个轻量级生成器，以无数据的方式集成用户信息，然后将其广播给用户，使用学到的知识作为归纳偏置来规范本地训练。基于理论启示的实证研究表明，与最先进的技术相比，我们的方法在更少的通信轮次中实现了更好的泛化性能。",
        "领域": "联邦学习、知识蒸馏、模型优化",
        "问题": "解决联邦学习中用户异构性导致的模型漂移和收敛慢的问题",
        "动机": "通过无数据知识蒸馏方法，提高联邦学习的效率和模型质量，减少对代理数据集的依赖",
        "方法": "提出一种无数据知识蒸馏方法，服务器学习一个轻量级生成器集成用户信息，并广播给用户以规范本地训练",
        "关键词": [
            "联邦学习",
            "知识蒸馏",
            "异构性",
            "模型优化",
            "无数据学习"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "通过聚合异构用户的知识来精炼服务器模型，而不是直接平均模型参数",
            "轻量级生成器": "服务器学习的用于集成用户信息的模型，以无数据方式工作",
            "归纳偏置": "使用学到的知识来规范本地训练，提高模型质量"
        },
        "success": true
    },
    {
        "order": 245,
        "title": "Dataset Condensation with Differentiable Siamese Augmentation",
        "html": "https://ICML.cc//virtual/2021/poster/8609",
        "abstract": "In many machine learning problems, large-scale datasets have become the de-facto standard to train state-of-the-art deep networks at the price of heavy computation load. In this paper, we focus on condensing large training sets into significantly smaller synthetic sets which can be used to train deep neural networks from scratch with minimum drop in performance. Inspired from the recent training set synthesis methods, we propose Differentiable Siamese Augmentation that enables effective use of data augmentation to synthesize more informative synthetic images and thus achieves better performance when training networks with augmentations. Experiments on multiple image classification benchmarks demonstrate that the proposed method obtains substantial gains over the state-of-the-art, 7% improvements on CIFAR10 and CIFAR100 datasets. We show with only less than 1% data that our method achieves 99.6%, 94.9%, 88.5%, 71.5% relative performance on MNIST, FashionMNIST, SVHN, CIFAR10 respectively. We also explore the use of our method in continual learning and neural architecture search, and show promising results.",
        "conference": "ICML",
        "中文标题": "可微分连体增强的数据集压缩",
        "摘要翻译": "在许多机器学习问题中，大规模数据集已成为训练最先进深度网络的事实标准，但代价是沉重的计算负担。本文中，我们专注于将大型训练集压缩成显著更小的合成集，这些合成集可用于从头训练深度神经网络，且性能下降最小。受到最近训练集合成方法的启发，我们提出了可微分连体增强，该方法能够有效利用数据增强来合成信息量更大的合成图像，从而在使用增强训练网络时获得更好的性能。在多个图像分类基准上的实验表明，所提出的方法相对于现有技术取得了显著提升，在CIFAR10和CIFAR100数据集上提高了7%。我们展示了仅使用不到1%的数据，我们的方法在MNIST、FashionMNIST、SVHN、CIFAR10上分别达到了99.6%、94.9%、88.5%、71.5%的相对性能。我们还探索了我们的方法在持续学习和神经架构搜索中的应用，并展示了有希望的结果。",
        "领域": "数据集压缩、图像分类、神经架构搜索",
        "问题": "如何有效压缩大规模训练集以减少计算负担，同时保持深度神经网络的训练性能。",
        "动机": "减少训练大规模数据集所需的计算资源，同时保持或提高深度神经网络的训练效率和性能。",
        "方法": "提出可微分连体增强方法，通过有效利用数据增强技术合成信息量更大的合成图像，以提高训练效率和性能。",
        "关键词": [
            "数据集压缩",
            "可微分连体增强",
            "图像分类",
            "持续学习",
            "神经架构搜索"
        ],
        "涉及的技术概念": {
            "可微分连体增强": "一种数据增强技术，通过合成信息量更大的合成图像来提高训练效率和性能。",
            "数据集压缩": "将大型训练集压缩成更小的合成集，以减少计算负担同时保持训练性能。",
            "神经架构搜索": "自动化设计神经网络结构的过程，本研究中探索了数据集压缩方法在此领域的应用。"
        },
        "success": true
    },
    {
        "order": 246,
        "title": "Dataset Dynamics via Gradient Flows in Probability Space",
        "html": "https://ICML.cc//virtual/2021/poster/9503",
        "abstract": "Various machine learning tasks, from generative modeling to domain adaptation, revolve around the concept of dataset transformation and manipulation. While various methods exist for transforming unlabeled datasets, principled methods to do so for labeled (e.g., classification) datasets are missing. In this work, we propose a novel framework for dataset transformation, which we cast as optimization over data-generating joint probability distributions. We approach this class of problems through Wasserstein gradient flows in probability space, and derive practical and efficient particle-based methods for a flexible but well-behaved class of objective functions. Through various experiments, we show that this framework can be used to impose constraints on classification datasets, adapt them for transfer learning, or to re-purpose fixed or black-box models to classify —with high accuracy— previously unseen datasets. ",
        "conference": "ICML",
        "中文标题": "概率空间中的梯度流实现数据集动态变化",
        "摘要翻译": "从生成建模到领域适应，各种机器学习任务都围绕着数据集转换和操作的概念展开。虽然存在多种方法用于转换未标记的数据集，但对于标记（例如分类）数据集的有原则的方法却缺失。在这项工作中，我们提出了一个新颖的数据集转换框架，将其视为对数据生成联合概率分布的优化。我们通过概率空间中的Wasserstein梯度流来解决这类问题，并为一种灵活但表现良好的目标函数类推导出实用且高效的基于粒子的方法。通过各种实验，我们展示了这一框架可用于对分类数据集施加约束，为迁移学习调整它们，或重新利用固定或黑盒模型以高准确率分类先前未见过的数据集。",
        "领域": "生成模型、迁移学习、概率建模",
        "问题": "如何有原则地转换标记数据集以适应不同的机器学习任务",
        "动机": "现有方法缺乏对标记数据集进行有原则转换的手段，限制了其在生成建模、迁移学习等任务中的应用",
        "方法": "提出基于Wasserstein梯度流的概率空间优化框架，采用粒子方法实现高效的数据集转换",
        "关键词": [
            "数据集转换",
            "Wasserstein梯度流",
            "概率建模",
            "迁移学习",
            "生成模型"
        ],
        "涉及的技术概念": {
            "Wasserstein梯度流": "用于在概率空间中优化数据生成分布，实现数据集的灵活转换",
            "粒子方法": "一种高效实现概率分布优化的技术，特别适用于处理大规模数据集",
            "联合概率分布": "描述数据及其标签之间关系的数学模型，是数据集转换优化的基础"
        },
        "success": true
    },
    {
        "order": 247,
        "title": "Debiasing a First-order Heuristic for Approximate Bi-level Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9027",
        "abstract": "Approximate bi-level optimization (ABLO) consists of (outer-level) optimization problems, involving numerical (inner-level) optimization loops. While ABLO has many applications across deep learning, it suffers from time and memory complexity proportional to the length $r$ of its inner optimization loop. To address this complexity, an earlier first-order method (FOM) was proposed as a heuristic which omits second derivative terms, yielding significant speed gains and requiring only constant memory. Despite FOM's popularity, there is a lack of theoretical understanding of its convergence properties. We contribute by theoretically characterizing FOM's gradient bias under mild assumptions. We further demonstrate a rich family of examples where FOM-based SGD does not converge to a stationary point of the ABLO objective. We address this concern by proposing an unbiased FOM (UFOM) enjoying constant memory complexity as a function of $r$. We characterize the introduced time-variance tradeoff, demonstrate convergence bounds, and find an optimal UFOM for a given ABLO problem. Finally, we propose an efficient adaptive UFOM scheme.",
        "conference": "ICML",
        "中文标题": "消除一阶启发式方法在近似双层优化中的偏差",
        "摘要翻译": "近似双层优化（ABLO）包含（外层）优化问题，涉及数值（内层）优化循环。虽然ABLO在深度学习中有许多应用，但它遭受时间和内存复杂度与内层优化循环长度$r$成正比的问题。为了解决这一复杂度，早期提出了一种一阶方法（FOM）作为启发式方法，它省略了二阶导数项，从而显著提高了速度并仅需要恒定的内存。尽管FOM很受欢迎，但对其收敛性质缺乏理论理解。我们的贡献在于在温和假设下理论上描述了FOM的梯度偏差。我们进一步展示了一系列例子，其中基于FOM的SGD不会收敛到ABLO目标的稳定点。我们通过提出一种无偏FOM（UFOM）来解决这一问题，它作为$r$的函数享有恒定的内存复杂度。我们描述了引入的时间-方差权衡，展示了收敛界限，并为给定的ABLO问题找到了最优的UFOM。最后，我们提出了一种高效的适应性UFOM方案。",
        "领域": "优化算法、深度学习、自动微分",
        "问题": "解决近似双层优化中一阶启发式方法的梯度偏差问题",
        "动机": "提高近似双层优化的效率和准确性，特别是在深度学习中应用时的时间和内存效率",
        "方法": "提出无偏一阶方法（UFOM），分析其时间-方差权衡，并开发适应性UFOM方案",
        "关键词": [
            "近似双层优化",
            "一阶方法",
            "梯度偏差",
            "收敛性分析",
            "适应性优化"
        ],
        "涉及的技术概念": {
            "近似双层优化（ABLO）": "涉及外层和内层优化循环的优化问题，广泛应用于深度学习",
            "一阶方法（FOM）": "省略二阶导数项的优化方法，以提高计算效率和减少内存使用",
            "无偏一阶方法（UFOM）": "改进的FOM版本，旨在消除梯度偏差，同时保持恒定的内存复杂度"
        },
        "success": true
    },
    {
        "order": 248,
        "title": "Debiasing Model Updates for Improving Personalized Federated Training",
        "html": "https://ICML.cc//virtual/2021/poster/9599",
        "abstract": "We propose a novel method for federated learning that is customized specifically to the objective of a given edge device. In our proposed method, a server trains a global meta-model by collaborating with devices without actually sharing data. The trained global meta-model is then personalized locally by each device to meet its specific objective. Different from the conventional federated learning setting, training customized models for each device is hindered by both the inherent data biases of the various devices, as well as the requirements imposed by the federated architecture. We propose gradient correction methods leveraging prior works, and explicitly de-bias the meta-model in the distributed heterogeneous data setting to learn personalized device models. We present convergence guarantees of our method for strongly convex, convex and nonconvex meta objectives. We empirically evaluate the performance of our method on benchmark datasets and demonstrate significant communication savings.",
        "conference": "ICML",
        "中文标题": "消除模型更新偏差以改进个性化联邦训练",
        "摘要翻译": "我们提出了一种新颖的联邦学习方法，该方法专门针对给定边缘设备的目标进行定制。在我们提出的方法中，服务器通过与设备协作训练一个全局元模型，而无需实际共享数据。训练好的全局元模型随后由每个设备在本地进行个性化，以满足其特定目标。与传统的联邦学习设置不同，为每个设备训练定制模型受到各种设备固有数据偏差以及联邦架构施加的要求的阻碍。我们提出了利用先前工作的梯度校正方法，并在分布式异构数据设置中明确消除元模型的偏差，以学习个性化设备模型。我们为强凸、凸和非凸元目标提供了我们方法的收敛保证。我们在基准数据集上实证评估了我们方法的性能，并展示了显著的通信节省。",
        "领域": "联邦学习、个性化学习、边缘计算",
        "问题": "解决在联邦学习环境中为每个设备训练个性化模型时遇到的数据偏差和架构限制问题",
        "动机": "为了在保护数据隐私的同时，为每个边缘设备提供定制化的模型，克服数据分布不均和通信效率低的挑战",
        "方法": "提出梯度校正方法，在分布式异构数据环境中消除元模型的偏差，实现个性化设备模型的训练",
        "关键词": [
            "联邦学习",
            "个性化学习",
            "梯度校正",
            "边缘计算",
            "数据偏差"
        ],
        "涉及的技术概念": {
            "全局元模型": "服务器训练的模型，作为个性化设备模型的基础",
            "梯度校正": "用于消除模型更新中的偏差，提高个性化模型训练效果的技术",
            "分布式异构数据": "指在联邦学习中，各设备数据分布不均且类型多样的环境"
        },
        "success": true
    },
    {
        "order": 249,
        "title": "Decentralized Riemannian Gradient Descent on the Stiefel Manifold",
        "html": "https://ICML.cc//virtual/2021/poster/8405",
        "abstract": "We consider a distributed non-convex optimization where a network of agents aims at minimizing a global function over the Stiefel manifold. The global function is represented as a finite sum of smooth local functions, where each local function is associated with one agent and agents communicate with each other over an undirected connected graph. The problem is non-convex as local functions are possibly non-convex (but smooth) and the Steifel manifold is a non-convex set. We present a decentralized Riemannian stochastic gradient method (DRSGD) with the convergence rate of $\\mathcal{O}(1/\\sqrt{K})$ to a stationary point. To have exact convergence with constant stepsize, we also propose a decentralized Riemannian gradient tracking algorithm (DRGTA) with the convergence rate of $\\mathcal{O}(1/K)$ to a stationary point. We use multi-step consensus to preserve the iteration in the local (consensus) region. DRGTA is the first decentralized algorithm with exact convergence for distributed optimization on Stiefel manifold.",
        "conference": "ICML",
        "success": true,
        "中文标题": "Stiefel流形上的分散式黎曼梯度下降",
        "摘要翻译": "我们考虑一个分布式非凸优化问题，其中一组智能体网络旨在最小化Stiefel流形上的全局函数。全局函数表示为平滑局部函数的有限和，其中每个局部函数与一个智能体相关联，并且智能体通过无向连通图相互通信。由于局部函数可能是非凸的（但平滑）且Stiefel流形是非凸集，该问题是非凸的。我们提出了一种分散式黎曼随机梯度方法（DRSGD），其收敛速度为$\\mathcal{O}(1/\\sqrt{K})$到一个稳定点。为了在恒定步长下实现精确收敛，我们还提出了一种分散式黎曼梯度跟踪算法（DRGTA），其收敛速度为$\\mathcal{O}(1/K)$到一个稳定点。我们使用多步共识来保持迭代在局部（共识）区域内。DRGTA是第一个在Stiefel流形上实现精确收敛的分散式分布式优化算法。",
        "领域": "分布式优化, 黎曼优化, 非凸优化",
        "问题": "解决在Stiefel流形上的分布式非凸优化问题",
        "动机": "研究如何在非凸的Stiefel流形上实现分布式优化，特别是当全局函数表示为局部函数的和时，如何设计有效的算法以实现精确收敛",
        "方法": "提出了分散式黎曼随机梯度方法（DRSGD）和分散式黎曼梯度跟踪算法（DRGTA），后者通过多步共识保持迭代在局部区域内，实现精确收敛",
        "关键词": [
            "分布式优化",
            "Stiefel流形",
            "黎曼梯度下降",
            "非凸优化",
            "梯度跟踪"
        ],
        "涉及的技术概念": {
            "Stiefel流形": "论文中优化的对象，是一个非凸集，用于表示问题的约束条件",
            "黎曼梯度下降": "在流形上进行的梯度下降方法，用于解决非凸优化问题"
        }
    },
    {
        "order": 250,
        "title": "Decentralized Single-Timescale Actor-Critic on Zero-Sum Two-Player Stochastic Games",
        "html": "https://ICML.cc//virtual/2021/poster/9619",
        "abstract": "We study the global convergence and global optimality of the actor-critic algorithm applied for the zero-sum two-player stochastic games in a decentralized manner. We focus on the single-timescale setting where the critic is updated by applying the Bellman operator only once and the actor is updated by policy gradient with the information from the critic. Our algorithm is in a decentralized manner, as we assume that each player has no access to the actions of the other one, which, in a way, protects the privacy of both players. Moreover, we consider linear function approximations for both actor and critic, and we prove that the sequence of joint policy generated by our decentralized linear algorithm converges to the minimax equilibrium at a sublinear rate \\(\\cO(\\sqrt{K})\\), where \\(K\\) is the number of iterations. To the best of our knowledge, we establish the global optimality and convergence of decentralized actor-critic algorithm on zero-sum two-player stochastic games with linear function approximations for the first time.",
        "conference": "ICML",
        "success": true,
        "中文标题": "去中心化单时间尺度演员-评论家算法在零和双人随机博弈中的应用",
        "摘要翻译": "我们研究了以去中心化方式应用于零和双人随机博弈的演员-评论家算法的全局收敛性和全局最优性。我们专注于单时间尺度设置，其中评论家通过仅应用一次贝尔曼算子进行更新，演员则利用评论家提供的信息通过策略梯度进行更新。我们的算法以去中心化的方式运行，因为我们假设每个玩家无法访问另一个玩家的动作，这在某种程度上保护了双方玩家的隐私。此外，我们为演员和评论家都考虑了线性函数近似，并证明了我们的去中心化线性算法生成的联合策略序列以亚线性速率收敛到极小极大均衡，其中是迭代次数。据我们所知，我们首次建立了具有线性函数近似的零和双人随机博弈上去中心化演员-评论家算法的全局最优性和收敛性。",
        "领域": "强化学习, 博弈论, 多智能体系统",
        "问题": "研究在零和双人随机博弈中，去中心化单时间尺度演员-评论家算法的全局收敛性和最优性问题。",
        "动机": "探索在保护玩家隐私的前提下，通过去中心化的方式实现演员-评论家算法在零和双人随机博弈中的有效应用。",
        "方法": "采用单时间尺度设置，结合贝尔曼算子和策略梯度更新演员和评论家，使用线性函数近似，并证明算法的收敛性。",
        "关键词": [
            "去中心化",
            "演员-评论家算法",
            "零和博弈",
            "随机博弈",
            "线性函数近似"
        ],
        "涉及的技术概念": {
            "贝尔曼算子": "用于更新评论家，评估当前策略的价值。",
            "策略梯度": "用于更新演员，优化策略以最大化预期回报。",
            "线性函数近似": "用于简化演员和评论家的表示，使算法能够处理高维状态空间。"
        }
    },
    {
        "order": 251,
        "title": "Deciding What to Learn: A Rate-Distortion Approach",
        "html": "https://ICML.cc//virtual/2021/poster/10475",
        "abstract": "Agents that learn to select optimal actions represent a prominent focus of the sequential decision-making literature. In the face of a complex environment or constraints on time and resources, however, aiming to synthesize such an optimal policy can become infeasible. These scenarios give rise to an important trade-off between the information an agent must acquire to learn and the sub-optimality of the resulting policy. While an agent designer has a preference for how this trade-off is resolved, existing approaches further require that the designer translate these preferences into a fixed learning target for the agent. In this work, leveraging rate-distortion theory, we automate this process such that the designer need only express their preferences via a single hyperparameter and the agent is endowed with the ability to compute its own learning targets that best achieve the desired trade-off. We establish a general bound on expected discounted regret for an agent that decides what to learn in this manner along with computational experiments that illustrate the expressiveness of designer preferences and even show improvements over Thompson sampling in identifying an optimal policy.",
        "conference": "ICML",
        "中文标题": "决定学习内容：一种率失真方法",
        "摘要翻译": "学习选择最优动作的代理是序列决策文献中的一个重要焦点。然而，面对复杂的环境或时间和资源的限制，试图合成这样一个最优策略可能变得不可行。这些情景引发了一个重要的权衡：代理必须获取的信息量与结果策略的次优性之间的权衡。虽然代理设计者对于如何解决这一权衡有偏好，但现有的方法还要求设计者将这些偏好转化为代理的固定学习目标。在这项工作中，利用率失真理论，我们自动化了这一过程，使得设计者只需通过一个超参数表达他们的偏好，代理就被赋予了计算其自身学习目标的能力，以最好地实现所需的权衡。我们为以这种方式决定学习内容的代理建立了一个关于预期折扣遗憾的一般界限，并通过计算实验说明了设计者偏好的表达能力，甚至展示了在识别最优策略方面对汤普森采样的改进。",
        "领域": "强化学习、决策理论、机器学习",
        "问题": "在复杂环境或资源限制下，如何自动化地决定代理的学习目标以平衡信息获取和策略次优性之间的权衡。",
        "动机": "为了解决设计者需要手动将偏好转化为固定学习目标的限制，以及优化代理在复杂环境下的学习效率和策略效果。",
        "方法": "利用率失真理论自动化学习目标的确定过程，设计者通过单一超参数表达偏好，代理自动计算最佳学习目标。",
        "关键词": [
            "率失真理论",
            "自动化学习",
            "决策理论",
            "强化学习",
            "策略优化"
        ],
        "涉及的技术概念": {
            "率失真理论": "用于量化信息获取和策略次优性之间的权衡，自动化学习目标的确定过程。",
            "预期折扣遗憾": "衡量代理决策性能的指标，用于评估学习目标的效率。",
            "汤普森采样": "一种策略优化方法，本文方法在识别最优策略方面展示了对其的改进。"
        },
        "success": true
    },
    {
        "order": 252,
        "title": "Decision-Making Under Selective Labels: Optimal Finite-Domain Policies and Beyond",
        "html": "https://ICML.cc//virtual/2021/poster/10109",
        "abstract": "Selective labels are a common feature of high-stakes decision-making applications, referring to the lack of observed outcomes under one of the possible decisions. This paper studies the learning of decision policies in the face of selective labels, in an online setting that balances learning costs against future utility. In the homogeneous case in which individuals' features are disregarded, the optimal decision policy is shown to be a threshold policy. The threshold becomes more stringent as more labels are collected; the rate at which this occurs is characterized. In the case of features drawn from a finite domain, the optimal policy consists of multiple homogeneous policies in parallel. For the general infinite-domain case, the homogeneous policy is extended by using a probabilistic classifier and bootstrapping to provide its inputs. In experiments on synthetic and real data, the proposed policies achieve consistently superior utility with no parameter tuning in the finite-domain case and lower parameter sensitivity in the general case.",
        "conference": "ICML",
        "中文标题": "选择性标签下的决策制定：有限域最优策略及其超越",
        "摘要翻译": "选择性标签是高风险决策应用中的一个常见特征，指的是在可能的决策之一下缺乏观察到的结果。本文研究了在面对选择性标签时，在一个平衡学习成本与未来效用的在线设置中，决策策略的学习。在忽略个体特征的均质情况下，最优决策策略被证明是一种阈值策略。随着收集到更多标签，阈值变得更加严格；本文描述了这一过程的发生速率。对于从有限域中提取特征的情况，最优策略由多个并行均质策略组成。对于一般的无限域情况，通过使用概率分类器和自举法提供其输入，扩展了均质策略。在合成和真实数据的实验中，所提出的策略在有限域情况下无需参数调整即可持续实现更优的效用，在一般情况下具有更低的参数敏感性。",
        "领域": "决策系统、机器学习应用、在线学习",
        "问题": "在高风险决策应用中，如何在缺乏某些决策结果观察（选择性标签）的情况下学习最优决策策略。",
        "动机": "解决在选择性标签存在的情况下，如何有效地学习和优化决策策略，以平衡学习成本和未来效用。",
        "方法": "在均质情况下采用阈值策略，有限域情况下采用并行均质策略，无限域情况下通过概率分类器和自举法扩展均质策略。",
        "关键词": [
            "选择性标签",
            "决策策略",
            "阈值策略",
            "概率分类器",
            "自举法"
        ],
        "涉及的技术概念": {
            "选择性标签": "指在决策过程中某些决策下的结果未被观察到的现象，影响决策策略的学习和优化。",
            "阈值策略": "在均质情况下采用的一种决策策略，通过设定阈值来决定采取何种决策，随着更多数据的收集调整阈值。",
            "概率分类器": "在无限域情况下用于扩展均质策略的技术，通过概率模型预测决策结果，辅助策略优化。"
        },
        "success": true
    },
    {
        "order": 253,
        "title": "Decomposable Submodular Function Minimization via Maximum Flow",
        "html": "https://ICML.cc//virtual/2021/poster/9123",
        "abstract": "This paper bridges discrete and continuous optimization approaches for decomposable submodular function minimization, in both the standard and parametric settings. We provide improved running times for this problem by reducing it to a number of calls to a maximum flow oracle. When each function in the decomposition acts on O(1) elements of the ground set V and is polynomially bounded, our running time is up to polylogarithmic factors equal to that of solving maximum flow in a sparse graph with O(|V|) vertices and polynomial integral capacities. We achieve this by providing a simple iterative method which can optimize to high precision any convex function defined on the submodular base polytope, provided we can efficiently minimize it on the base polytope corresponding to the cut function of a certain graph that we construct. We solve this minimization problem by lifting the solutions of a parametric cut problem, which we obtain via a new efficient combinatorial reduction to maximum flow. This reduction is of independent interest and implies some previously unknown bounds for the parametric minimum s,t-cut problem in multiple settings.",
        "conference": "ICML",
        "中文标题": "通过最大流分解可分解子模函数最小化",
        "摘要翻译": "本文在标准和参数化设置下，为可分解子模函数的最小化搭建了离散与连续优化方法之间的桥梁。通过将该问题转化为对最大流预言机的多次调用，我们提供了改进的运行时间。当分解中的每个函数作用于基础集V的O(1)个元素且多项式有界时，我们的运行时间在对数多项式因子内等于在具有O(|V|)顶点和多项式整数容量的稀疏图中解决最大流问题的时间。我们通过提供一个简单的迭代方法来实现这一点，该方法可以高精度地优化定义在子模基多面体上的任何凸函数，前提是我们能够有效地在对应于我们构建的某个图的割函数的基多面体上最小化它。我们通过提升参数化割问题的解来解决这个最小化问题，这是通过一个新的高效组合归约到最大流来实现的。这个归约具有独立的意义，并暗示了在多设置下参数化最小s,t-割问题的一些先前未知的界限。",
        "领域": "组合优化, 图算法, 凸优化",
        "问题": "解决可分解子模函数最小化问题，提高计算效率",
        "动机": "为了在离散和连续优化方法之间建立桥梁，提高可分解子模函数最小化的计算效率",
        "方法": "通过将问题转化为对最大流预言机的多次调用，并使用迭代方法优化定义在子模基多面体上的凸函数",
        "关键词": [
            "子模函数最小化",
            "最大流",
            "参数化割问题",
            "组合优化",
            "凸优化"
        ],
        "涉及的技术概念": {
            "子模函数最小化": "论文中研究的主要问题，旨在找到使子模函数值最小的元素集合",
            "最大流预言机": "用于解决子模函数最小化问题的工具，通过多次调用最大流算法来优化",
            "参数化割问题": "论文中通过归约技术解决的一个关键问题，用于支持子模函数最小化的高效计算"
        },
        "success": true
    },
    {
        "order": 254,
        "title": "Decomposed Mutual Information Estimation for Contrastive Representation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9751",
        "abstract": "Recent contrastive representation learning methods rely on estimating mutual information (MI) between multiple views of an underlying context. E.g., we can derive multiple views of a given image by applying data augmentation, or we can split a sequence into views comprising the past and future of some step in the sequence. Contrastive lower bounds on MI are easy to optimize, but have a strong underestimation bias when estimating large amounts of MI. We propose decomposing the full MI estimation problem into a sum of smaller estimation problems by splitting one of the views into progressively more informed subviews and by applying the chain rule on MI between the decomposed views. This expression contains a sum of unconditional and conditional MI terms, each measuring modest chunks of the total MI, which facilitates approximation via contrastive bounds. To maximize the sum, we formulate a contrastive lower bound on the conditional MI which can be approximated efficiently. We refer to our general approach as Decomposed Estimation of Mutual Information (DEMI). We show that DEMI can capture a larger amount of MI than standard non-decomposed contrastive bounds in a synthetic setting, and learns better representations in a vision domain and for dialogue generation.",
        "conference": "ICML",
        "中文标题": "对比表示学习中分解互信息估计",
        "摘要翻译": "最近的对比表示学习方法依赖于估计基础上下文多个视图之间的互信息（MI）。例如，我们可以通过应用数据增强来导出给定图像的多个视图，或者我们可以将一个序列分割成包含序列中某个步骤的过去和未来的视图。对比MI的下界易于优化，但在估计大量MI时具有强烈的低估偏差。我们提出将完整的MI估计问题分解为一系列较小的估计问题，通过将一个视图分割成逐渐更多信息的子视图，并在分解的视图之间应用MI的链式法则。这个表达式包含了一系列无条件MI和有条件MI项的和，每一项都测量了总MI的适度部分，这有助于通过对比边界进行近似。为了最大化这个和，我们制定了一个关于有条件MI的对比下界，可以有效地近似。我们将我们的通用方法称为分解互信息估计（DEMI）。我们表明，在合成设置中，DEMI可以比标准的非分解对比边界捕获更多的MI，并在视觉领域和对话生成中学习到更好的表示。",
        "领域": "对比学习、表示学习、互信息估计",
        "问题": "解决对比表示学习中互信息估计时的低估偏差问题",
        "动机": "提高对比表示学习方法中互信息估计的准确性和效率，以学习更好的数据表示",
        "方法": "通过分解互信息估计问题为一系列较小的估计问题，并应用对比下界来近似有条件互信息",
        "关键词": [
            "对比学习",
            "互信息估计",
            "表示学习",
            "数据增强",
            "对话生成"
        ],
        "涉及的技术概念": {
            "互信息（MI）": "用于衡量两个变量之间的统计依赖性，是论文中估计视图间关系的基础",
            "对比下界": "用于近似互信息的技术，易于优化但在估计大量互信息时存在低估偏差",
            "分解互信息估计（DEMI）": "论文提出的方法，通过分解互信息估计问题来提高估计的准确性和效率"
        },
        "success": true
    },
    {
        "order": 255,
        "title": "Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices",
        "html": "https://ICML.cc//virtual/2021/poster/8991",
        "abstract": "The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration’s utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: https://ezliu.github.io/dream/\n",
        "conference": "ICML",
        "中文标题": "解耦探索与利用：无需牺牲的元强化学习",
        "摘要翻译": "元强化学习（meta-RL）的目标是通过利用相关任务上的先前经验，构建能够快速学习新任务的智能体。学习一个新任务通常需要既探索以收集任务相关信息，又利用这些信息来解决任务。原则上，通过简单地最大化任务性能，可以端到端地学习最优的探索和利用。然而，由于鸡与蛋的问题，这样的元强化学习方法难以摆脱局部最优：学习探索需要良好的利用来评估探索的效用，但学习利用需要通过探索收集的信息。为探索和利用分别优化目标可以避免这个问题，但先前的元强化学习探索目标会产生收集与任务无关信息的次优策略。我们通过构建一个自动识别任务相关信息的利用目标和一个仅恢复这些信息的探索目标，来缓解这两个问题。这避免了端到端训练中的局部最优，同时不牺牲最优探索。实证上，DREAM在复杂的元强化学习问题上，如稀疏奖励的3D视觉导航，显著优于现有方法。DREAM的视频：https://ezliu.github.io/dream/",
        "领域": "元强化学习、稀疏奖励学习、3D视觉导航",
        "问题": "解决元强化学习中探索与利用的耦合问题，避免局部最优和收集无关信息",
        "动机": "元强化学习中的探索与利用相互依赖，导致学习过程中的局部最优问题，需要一种方法来解耦这两者，同时不牺牲探索的效率",
        "方法": "构建自动识别任务相关信息的利用目标和仅恢复这些信息的探索目标，以解耦探索与利用",
        "关键词": [
            "元强化学习",
            "探索与利用解耦",
            "稀疏奖励",
            "3D视觉导航",
            "局部最优"
        ],
        "涉及的技术概念": {
            "元强化学习": "通过利用相关任务上的先前经验，快速学习新任务的强化学习方法",
            "探索与利用解耦": "将探索和利用过程分离，分别优化，以避免相互依赖导致的局部最优问题",
            "稀疏奖励": "在强化学习中，奖励信号稀少且难以获取的环境设置，增加了学习的难度"
        },
        "success": true
    },
    {
        "order": 256,
        "title": "Decoupling Representation Learning from Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10141",
        "abstract": "In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning.  To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss.  In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments.  Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others.  We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks.  Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation.  Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at \\url{https://github.com/astooke/rlpyt/tree/master/rlpyt/ul}.",
        "conference": "ICML",
        "中文标题": "将表征学习与强化学习解耦",
        "摘要翻译": "为了克服深度强化学习（RL）中基于奖励的特征学习的局限性，我们提出将表征学习与策略学习解耦。为此，我们引入了一种新的无监督学习（UL）任务，称为增强时间对比（ATC），该任务训练一个卷积编码器，在图像增强和使用对比损失的情况下，关联由短时间差分隔的观察对。在在线RL实验中，我们发现仅使用ATC训练编码器在大多数环境中匹配或优于端到端RL。此外，我们通过预训练编码器在专家演示上，并在RL代理中使用它们（权重冻结），对几种领先的UL算法进行了基准测试；我们发现使用ATC训练编码器的代理优于所有其他代理。我们还对来自多个环境的数据训练多任务编码器，并展示了对不同下游RL任务的泛化能力。最后，我们消融了ATC的组件，并引入了一种新的数据增强方法，以便在RL需要增强时从预训练编码器重放（压缩的）潜在图像。我们的实验涵盖了DeepMind Control、DeepMind Lab和Atari中视觉多样化的RL基准，我们的完整代码可在https://github.com/astooke/rlpyt/tree/master/rlpyt/ul获取。",
        "领域": "深度强化学习、无监督学习、表征学习",
        "问题": "解决深度强化学习中基于奖励的特征学习的局限性",
        "动机": "克服深度强化学习中奖励驱动特征学习的限制，提高学习效率和性能",
        "方法": "提出将表征学习与策略学习解耦，引入增强时间对比（ATC）无监督学习任务训练卷积编码器",
        "关键词": [
            "表征学习",
            "强化学习",
            "无监督学习",
            "增强时间对比",
            "卷积编码器"
        ],
        "涉及的技术概念": {
            "增强时间对比（ATC）": "一种新的无监督学习任务，用于训练卷积编码器关联时间上接近的观察对，以学习有效的表征",
            "卷积编码器": "用于从图像中提取特征的网络结构，在本研究中通过ATC任务进行训练",
            "对比损失": "用于在无监督学习中衡量观察对之间相似性的损失函数，促进编码器学习有意义的表征"
        },
        "success": true
    },
    {
        "order": 257,
        "title": "Decoupling Value and Policy for Generalization in Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9343",
        "abstract": "Standard deep reinforcement learning algorithms use a shared representation for the policy and value function, especially when training directly from images. However, we argue that more information is needed to accurately estimate the value function than to learn the optimal policy. Consequently, the use of a shared representation for the policy and value function can lead to overfitting. To alleviate this problem, we propose two approaches which are combined to create IDAAC: Invariant Decoupled Advantage Actor-Critic. First, IDAAC decouples the optimization of the policy and value function, using separate networks to model them. Second, it introduces an auxiliary loss which encourages the representation to be invariant to task-irrelevant properties of the environment. IDAAC shows good generalization to unseen environments, achieving a new state-of-the-art on the Procgen benchmark and outperforming popular methods on DeepMind Control tasks with distractors. Our implementation is available at https://github.com/rraileanu/idaac.",
        "conference": "ICML",
        "中文标题": "解耦价值与策略以实现强化学习的泛化",
        "摘要翻译": "标准的深度强化学习算法在直接从图像训练时，通常使用共享表示来处理策略和价值函数。然而，我们认为准确估计价值函数比学习最优策略需要更多的信息。因此，策略和价值函数共享表示可能导致过拟合。为了缓解这个问题，我们提出了两种方法，结合创建了IDAAC：不变解耦优势演员-评论家。首先，IDAAC解耦了策略和价值函数的优化，使用独立的网络来建模它们。其次，它引入了一个辅助损失，鼓励表示对环境中的任务无关属性保持不变。IDAAC在未见过的环境中显示出良好的泛化能力，在Procgen基准测试中达到了新的最先进水平，并在带有干扰物的DeepMind控制任务中优于流行方法。我们的实现可在https://github.com/rraileanu/idaac获取。",
        "领域": "强化学习、深度强化学习、策略优化",
        "问题": "解决策略和价值函数共享表示导致的过拟合问题",
        "动机": "提高强化学习模型在未见环境中的泛化能力",
        "方法": "提出IDAAC方法，通过解耦策略和价值函数的优化以及引入辅助损失来鼓励表示对任务无关属性的不变性",
        "关键词": [
            "强化学习",
            "泛化",
            "解耦",
            "IDAAC",
            "过拟合"
        ],
        "涉及的技术概念": {
            "解耦优化": "通过独立网络分别优化策略和价值函数，减少过拟合",
            "辅助损失": "用于鼓励模型表示对任务无关属性保持不变，提高泛化能力",
            "IDAAC": "结合解耦优化和辅助损失的方法，旨在提高强化学习模型的泛化性能"
        },
        "success": true
    },
    {
        "order": 258,
        "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design",
        "html": "https://ICML.cc//virtual/2021/poster/9523",
        "abstract": "We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.",
        "conference": "ICML",
        "中文标题": "深度自适应设计：分摊序列贝叶斯实验设计的成本",
        "摘要翻译": "我们介绍了深度自适应设计（DAD），这是一种用于分摊自适应贝叶斯实验设计成本的方法，使得实验能够实时进行。传统的序列贝叶斯最优实验设计方法在实验的每个阶段都需要大量的计算，这使得它们不适合大多数现实世界的应用，因为决策通常需要快速做出。DAD通过学习一个预先分摊的设计网络，然后在部署时使用这个网络快速运行（多个）自适应实验，来解决这一限制。这个网络代表了一个设计策略，它以来自前几步的数据作为输入，并通过一次前向传递输出下一个设计；这些设计决策可以在实时实验中以毫秒级的速度做出。为了训练这个网络，我们引入了适用于序列设置的对比信息界限作为目标，并提出了一种利用关键对称性的定制网络架构。我们证明，DAD成功地分摊了实验设计的过程，在多个问题上优于替代策略。",
        "领域": "贝叶斯优化、自适应实验设计、深度学习应用",
        "问题": "解决传统序列贝叶斯最优实验设计方法计算成本高、无法实时决策的问题",
        "动机": "为了使自适应贝叶斯实验设计能够适用于需要快速决策的现实世界应用",
        "方法": "通过学习一个分摊设计网络，利用对比信息界限和定制网络架构，实现快速自适应实验设计",
        "关键词": [
            "深度自适应设计",
            "贝叶斯实验设计",
            "实时决策",
            "分摊成本",
            "对比信息界限"
        ],
        "涉及的技术概念": {
            "分摊设计网络": "预先学习的网络，用于在部署时快速运行自适应实验，减少实时计算需求",
            "对比信息界限": "用于序列设置的训练目标，帮助网络学习有效的设计策略",
            "定制网络架构": "利用实验设计中的对称性，提高网络效率和性能的专门设计的架构"
        },
        "success": true
    },
    {
        "order": 259,
        "title": "Deep Coherent Exploration for Continuous Control",
        "html": "https://ICML.cc//virtual/2021/poster/9225",
        "abstract": "In policy search methods for reinforcement learning (RL), exploration is often performed by injecting noise either in action space at each step independently or in parameter space over each full trajectory. In prior work, it has been shown that with linear policies, a more balanced trade-off between these two exploration strategies is beneficial. However, that method did not scale to policies using deep neural networks. In this paper, we introduce deep coherent exploration, a general and scalable exploration framework for deep RL algorithms for continuous control, that generalizes step-based and trajectory-based exploration. This framework models the last layer parameters of the policy network as latent variables and uses a recursive inference step within the policy update to handle these latent variables in a scalable manner. We find that deep coherent exploration improves the speed and stability of learning of A2C, PPO, and SAC on several continuous control tasks.",
        "conference": "ICML",
        "中文标题": "深度连贯探索在连续控制中的应用",
        "摘要翻译": "在强化学习（RL）的策略搜索方法中，探索通常通过在动作空间中每一步独立注入噪声或在参数空间中针对整个轨迹注入噪声来实现。先前的研究表明，对于线性策略，这两种探索策略之间更平衡的权衡是有益的。然而，该方法未能扩展到使用深度神经网络的策略。本文介绍了深度连贯探索，这是一个为连续控制的深度RL算法设计的通用且可扩展的探索框架，它概括了基于步骤和基于轨迹的探索。该框架将策略网络的最后一层参数建模为潜在变量，并在策略更新中使用递归推理步骤以可扩展的方式处理这些潜在变量。我们发现，深度连贯探索提高了A2C、PPO和SAC在多个连续控制任务中学习的速度和稳定性。",
        "领域": "强化学习、连续控制、深度神经网络",
        "问题": "如何在深度强化学习中实现更有效的探索策略，以平衡基于步骤和基于轨迹的探索。",
        "动机": "解决现有探索策略在深度神经网络策略中不可扩展的问题，提高学习效率和稳定性。",
        "方法": "提出深度连贯探索框架，通过将策略网络的最后一层参数建模为潜在变量，并采用递归推理步骤处理这些变量，实现探索策略的通用和可扩展。",
        "关键词": [
            "深度连贯探索",
            "连续控制",
            "强化学习",
            "策略搜索",
            "深度神经网络"
        ],
        "涉及的技术概念": {
            "深度连贯探索": "一种为深度RL算法设计的探索框架，通过建模策略网络最后一层参数为潜在变量，实现探索策略的通用和可扩展。",
            "递归推理步骤": "在策略更新中用于处理潜在变量的方法，确保探索策略的可扩展性。",
            "潜在变量": "在深度连贯探索框架中，策略网络最后一层参数被建模为潜在变量，以平衡基于步骤和基于轨迹的探索。"
        },
        "success": true
    },
    {
        "order": 260,
        "title": "Deep Continuous Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9169",
        "abstract": "CNNs and computational models of biological vision share some fundamental principles, which opened new avenues of research. However, fruitful cross-field research is hampered by conventional CNN architectures being based on spatially and depthwise discrete representations, which cannot accommodate certain aspects of biological complexity such as continuously varying receptive field sizes and dynamics of neuronal responses. Here we propose deep continuous networks (DCNs), which combine spatially continuous filters, with the continuous depth framework of neural ODEs. This allows us to learn the spatial support of the filters during training, as well as model the continuous evolution of feature maps, linking DCNs closely to biological models. We show that DCNs are versatile and highly applicable to standard image classification and reconstruction problems, where they improve parameter and data efficiency, and allow for meta-parametrization. We illustrate the biological plausibility of the scale distributions learned by DCNs and explore their performance in a neuroscientifically inspired pattern completion task. Finally, we investigate an efficient implementation of DCNs by changing input contrast.",
        "conference": "ICML",
        "中文标题": "深度连续网络",
        "摘要翻译": "卷积神经网络（CNNs）与生物视觉的计算模型共享一些基本原则，这为研究开辟了新途径。然而，传统的CNN架构基于空间和深度离散的表示，这无法适应生物复杂性的某些方面，如连续变化的感受野大小和神经元反应的动态性，从而阻碍了跨领域研究的丰硕成果。在此，我们提出了深度连续网络（DCNs），它结合了空间连续滤波器和神经ODE的连续深度框架。这使我们能够在训练过程中学习滤波器的空间支持，以及模拟特征图的连续演化，将DCNs与生物模型紧密联系起来。我们展示了DCNs在标准图像分类和重建问题中的多功能性和高度适用性，在这些问题中，它们提高了参数和数据效率，并允许元参数化。我们说明了DCNs学习的尺度分布的生物学合理性，并探索了其在神经科学启发的模式完成任务中的表现。最后，我们通过改变输入对比度研究了DCNs的高效实现。",
        "领域": "图像分类, 图像重建, 神经科学模型",
        "问题": "传统CNN架构无法适应生物视觉中的连续性和动态性",
        "动机": "探索更接近生物视觉模型的深度学习架构，以提高模型在图像处理任务中的效率和适应性",
        "方法": "结合空间连续滤波器和神经ODE的连续深度框架，提出深度连续网络（DCNs）",
        "关键词": [
            "深度连续网络",
            "神经ODE",
            "生物视觉模型",
            "图像分类",
            "图像重建"
        ],
        "涉及的技术概念": {
            "空间连续滤波器": "在DCNs中用于适应连续变化的感受野大小，提高模型对图像细节的捕捉能力",
            "神经ODE": "提供连续深度框架，允许特征图的连续演化，增强模型的动态适应性",
            "元参数化": "在DCNs中实现，允许模型在训练过程中自动调整参数，提高数据效率"
        },
        "success": true
    },
    {
        "order": 261,
        "title": "Deep Generative Learning via Schrödinger Bridge",
        "html": "https://ICML.cc//virtual/2021/poster/8715",
        "abstract": "We propose to learn a generative model via entropy interpolation with a Schrödinger Bridge. The generative learning task can be formulated as interpolating between a reference distribution and a target distribution based on the Kullback-Leibler divergence. At the population level, this entropy interpolation is characterized via an SDE on [0,1] with a time-varying drift term. At the sample level, we derive our Schrödinger Bridge algorithm by plugging the drift term estimated by a deep score estimator and a deep density ratio estimator into the Euler-Maruyama method. Under some mild smoothness assumptions of the target distribution, we prove the consistency of both the score estimator and the density ratio estimator, and then establish the consistency of the proposed Schrödinger Bridge approach. Our theoretical results guarantee that the distribution learned by our approach converges to the target distribution. Experimental results on multimodal synthetic data and benchmark data support our theoretical findings and indicate that the generative model via Schrödinger Bridge is comparable with state-of-the-art GANs, suggesting a new formulation of generative learning. We demonstrate its usefulness in image interpolation and image inpainting.",
        "conference": "ICML",
        "中文标题": "通过薛定谔桥的深度生成学习",
        "摘要翻译": "我们提出通过熵插值与薛定谔桥学习生成模型。生成学习任务可以表述为基于Kullback-Leibler散度在参考分布和目标分布之间进行插值。在总体水平上，这种熵插值通过一个具有时变漂移项的[0,1]区间上的随机微分方程来表征。在样本水平上，我们通过将深度分数估计器和深度密度比估计器估计的漂移项插入到Euler-Maruyama方法中，推导出我们的薛定谔桥算法。在目标分布的一些温和平滑性假设下，我们证明了分数估计器和密度比估计器的一致性，然后建立了所提出的薛定谔桥方法的一致性。我们的理论结果保证了通过学习到的分布收敛于目标分布。在多模态合成数据和基准数据上的实验结果支持了我们的理论发现，并表明通过薛定谔桥的生成模型与最先进的GANs相当，提出了生成学习的新表述。我们展示了其在图像插值和图像修复中的实用性。",
        "领域": "生成模型、图像处理、深度学习",
        "问题": "如何通过熵插值与薛定谔桥学习生成模型，以在参考分布和目标分布之间进行有效的插值。",
        "动机": "探索一种新的生成学习表述，通过薛定谔桥方法在生成模型中实现与最先进GANs相当的性能，同时提供理论保证。",
        "方法": "提出了一种基于熵插值和薛定谔桥的生成学习方法，利用深度分数估计器和深度密度比估计器估计漂移项，并通过Euler-Maruyama方法实现样本水平的算法。",
        "关键词": [
            "薛定谔桥",
            "生成模型",
            "熵插值",
            "深度分数估计",
            "深度密度比估计"
        ],
        "涉及的技术概念": {
            "薛定谔桥": "用于在参考分布和目标分布之间进行插值的数学框架，是本方法的核心。",
            "熵插值": "基于Kullback-Leibler散度的插值方法，用于生成学习任务中。",
            "Euler-Maruyama方法": "用于数值解随机微分方程的方法，在本研究中用于实现样本水平的薛定谔桥算法。"
        },
        "success": true
    },
    {
        "order": 262,
        "title": "Deep kernel processes",
        "html": "https://ICML.cc//virtual/2021/poster/9135",
        "abstract": "We define deep kernel processes in which positive definite Gram matrices are progressively transformed by nonlinear kernel functions and by sampling from (inverse) Wishart distributions. Remarkably, we find that deep Gaussian processes (DGPs), Bayesian neural networks (BNNs), infinite BNNs, and infinite BNNs with bottlenecks can all be written as deep kernel processes. For DGPs the equivalence arises because the Gram matrix formed by the inner product of features is Wishart distributed, and as we show, standard isotropic kernels can be written entirely in terms of this Gram matrix  --- we do not need knowledge of the underlying features. We define a tractable deep kernel process, the deep inverse Wishart process, and give a doubly-stochastic inducing-point variational inference scheme that operates on the Gram matrices, not on the features, as in DGPs. We show that the deep inverse Wishart process gives superior performance to DGPs and infinite BNNs on fully-connected baselines.",
        "conference": "ICML",
        "中文标题": "深度核过程",
        "摘要翻译": "我们定义了深度核过程，其中正定的Gram矩阵通过非线性核函数和从（逆）Wishart分布中采样逐步转换。值得注意的是，我们发现深度高斯过程（DGPs）、贝叶斯神经网络（BNNs）、无限BNNs和带有瓶颈的无限BNNs都可以表示为深度核过程。对于DGPs，等价性出现是因为由特征内积形成的Gram矩阵是Wishart分布的，并且如我们所示，标准的各向同性核可以完全用这个Gram矩阵来表示——我们不需要了解底层特征。我们定义了一个可处理的深度核过程，即深度逆Wishart过程，并给出了一个双重随机诱导点变分推理方案，该方案在Gram矩阵上操作，而不是像在DGPs中那样在特征上操作。我们表明，深度逆Wishart过程在全连接基线上提供了优于DGPs和无限BNNs的性能。",
        "领域": "贝叶斯深度学习, 高斯过程, 核方法",
        "问题": "如何通过深度核过程改进现有的深度高斯过程和贝叶斯神经网络的性能",
        "动机": "探索深度核过程作为一种统一框架，能够表示和超越现有的深度高斯过程和贝叶斯神经网络模型",
        "方法": "定义深度核过程，特别是深度逆Wishart过程，并开发一种在Gram矩阵上操作的双重随机诱导点变分推理方案",
        "关键词": [
            "深度核过程",
            "深度逆Wishart过程",
            "贝叶斯神经网络",
            "高斯过程",
            "变分推理"
        ],
        "涉及的技术概念": {
            "深度核过程": "一种通过非线性核函数和Wishart分布采样逐步转换Gram矩阵的过程",
            "深度逆Wishart过程": "一种特定的深度核过程，用于提供优于传统深度高斯过程和贝叶斯神经网络的性能",
            "双重随机诱导点变分推理": "一种在Gram矩阵而非特征上进行操作的变分推理方案，旨在提高模型的效率和性能"
        },
        "success": true
    },
    {
        "order": 263,
        "title": "Deep Latent Graph Matching",
        "html": "https://ICML.cc//virtual/2021/poster/8835",
        "abstract": "Deep learning for graph matching (GM) has emerged as an important research topic due to its superior performance over traditional methods and insights it provides for solving other combinatorial problems on graph. While recent deep methods for GM extensively investigated effective node/edge feature learning or downstream GM solvers given such learned features, there is little existing work questioning if the fixed connectivity/topology typically constructed using heuristics (e.g., Delaunay or k-nearest) is indeed suitable for GM. From a learning perspective, we argue that the fixed topology may restrict the model capacity and thus potentially hinder the performance. To address this, we propose to learn the (distribution of) latent topology, which can better support the downstream GM task. We devise two latent graph generation procedures, one deterministic and one generative. Particularly, the generative procedure emphasizes the across-graph consistency and thus can be viewed as a matching-guided co-generative model. Our methods deliver superior performance over previous state-of-the-arts on public benchmarks, hence supporting our hypothesis.",
        "conference": "ICML",
        "中文标题": "深度潜在图匹配",
        "摘要翻译": "图匹配（GM）的深度学习因其优于传统方法的性能以及为解决图上其他组合问题提供的见解而成为一个重要的研究课题。尽管最近的深度学习方法广泛研究了有效的节点/边特征学习或基于这些学习特征的下游GM求解器，但很少有现有工作质疑通常使用启发式方法（如Delaunay或k近邻）构建的固定连接性/拓扑是否确实适合GM。从学习的角度来看，我们认为固定拓扑可能会限制模型容量，从而潜在地阻碍性能。为了解决这个问题，我们提出学习潜在拓扑的（分布），这可以更好地支持下游GM任务。我们设计了两种潜在图生成过程，一种是确定性的，一种是生成性的。特别是，生成性过程强调了跨图一致性，因此可以被视为匹配引导的共生成模型。我们的方法在公共基准测试中提供了优于先前最先进技术的性能，从而支持了我们的假设。",
        "领域": "图匹配、深度学习、组合优化",
        "问题": "解决图匹配中固定拓扑结构可能限制模型性能的问题",
        "动机": "探索和验证学习潜在拓扑结构是否能提升图匹配任务的性能",
        "方法": "提出两种潜在图生成过程：确定性生成和生成性过程，后者强调跨图一致性，作为匹配引导的共生成模型",
        "关键词": [
            "图匹配",
            "潜在拓扑",
            "深度学习",
            "组合优化",
            "共生成模型"
        ],
        "涉及的技术概念": {
            "潜在拓扑": "学习图的结构分布，以更灵活地支持图匹配任务",
            "匹配引导的共生成模型": "强调跨图一致性的生成过程，优化图匹配性能",
            "图匹配": "解决图中节点或边之间对应关系的问题，是组合优化和深度学习的重要应用"
        },
        "success": true
    },
    {
        "order": 264,
        "title": "Deep Learning for Functional Data Analysis with Adaptive Basis Layers",
        "html": "https://ICML.cc//virtual/2021/poster/9717",
        "abstract": "Despite their widespread success, the application of deep neural networks to functional data remains scarce today. The infinite dimensionality of functional data means standard learning algorithms can be applied only after appropriate dimension reduction, typically achieved via basis expansions. Currently, these bases are  chosen a priori without the information for the task at hand and thus may not be effective for the designated task. We instead propose to adaptively learn these bases in an end-to-end fashion. We introduce neural networks that employ a new Basis Layer whose hidden units are each basis functions  themselves implemented as a micro neural network. Our architecture learns to apply parsimonious dimension reduction to functional inputs that focuses only on information relevant to the target rather than irrelevant variation in the input function. Across numerous classification/regression tasks with functional data, our method empirically outperforms other types of neural networks, and we prove that our approach is statistically consistent with low generalization error.",
        "conference": "ICML",
        "中文标题": "深度学习在功能性数据分析中的应用：自适应基函数层",
        "摘要翻译": "尽管深度神经网络取得了广泛的成功，但目前其在功能性数据上的应用仍然较少。功能性数据的无限维特性意味着标准的学习算法只有在经过适当的降维处理后才能应用，这通常通过基展开来实现。目前，这些基函数是在没有考虑手头任务信息的情况下预先选择的，因此可能对指定任务无效。我们提出了一种端到端的方式自适应学习这些基函数。我们引入了采用新型基函数层的神经网络，其中每个隐藏单元本身就是一个作为微型神经网络实现的基函数。我们的架构学会了对功能性输入应用简约的降维处理，仅关注与目标相关的信息，而非输入函数中的无关变化。在多个功能性数据的分类/回归任务中，我们的方法在经验上优于其他类型的神经网络，并且我们证明了我们的方法在统计上具有低泛化误差的一致性。",
        "领域": "功能性数据分析、深度学习、自适应学习",
        "问题": "如何有效地对无限维的功能性数据进行降维处理，以提高深度神经网络在功能性数据上的应用效果。",
        "动机": "现有的功能性数据降维方法通常不考虑任务的具体信息，导致降维后的数据可能不适用于特定任务，影响了深度神经网络的应用效果。",
        "方法": "提出了一种新型的神经网络架构，通过自适应学习基函数层，实现对功能性数据的有效降维，仅保留与任务目标相关的信息。",
        "关键词": [
            "功能性数据分析",
            "自适应基函数层",
            "深度神经网络",
            "降维处理",
            "统计一致性"
        ],
        "涉及的技术概念": {
            "基函数层": "作为神经网络的一部分，每个隐藏单元都是一个基函数，用于对功能性数据进行降维处理。",
            "端到端学习": "整个模型从输入到输出都是可学习的，包括基函数的选择和调整，以适应特定任务的需求。",
            "统计一致性": "证明了所提方法在统计上具有低泛化误差，意味着在大量数据下能够收敛到真实模型。"
        },
        "success": true
    },
    {
        "order": 265,
        "title": "Deeply-Debiased Off-Policy Interval Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/10481",
        "abstract": "Off-policy evaluation learns a target policy's value with a historical dataset generated by a different behavior policy. In addition to a point estimate, many applications would benefit significantly from having a confidence interval (CI) that quantifies the uncertainty of the point estimate. In this paper, we propose a novel procedure to construct an efficient, robust, and flexible CI on a target policy's value. Our method is justified by theoretical results and numerical experiments. A Python implementation of the proposed procedure is available at https://github.com/\nRunzheStat/D2OPE.\n",
        "conference": "ICML",
        "中文标题": "深度去偏的非策略区间估计",
        "摘要翻译": "非策略评估利用由不同行为策略生成的历史数据集来学习目标策略的价值。除了点估计外，许多应用还会极大地受益于拥有一个量化点估计不确定性的置信区间（CI）。在本文中，我们提出了一种新颖的方法来构建一个高效、稳健且灵活的目标策略价值CI。我们的方法通过理论结果和数值实验得到了验证。所提出方法的Python实现可在https://github.com/RunzheStat/D2OPE获取。",
        "领域": "强化学习、统计学习、策略评估",
        "问题": "如何在非策略评估中构建一个高效、稳健且灵活的置信区间来量化目标策略价值点估计的不确定性。",
        "动机": "为了在非策略评估中提供更全面的不确定性量化，支持决策制定。",
        "方法": "提出了一种新颖的置信区间构建方法，结合理论分析和数值实验验证其有效性。",
        "关键词": [
            "非策略评估",
            "置信区间",
            "深度去偏",
            "强化学习",
            "统计学习"
        ],
        "涉及的技术概念": {
            "非策略评估": "利用历史数据评估目标策略的性能，而不需要直接执行该策略。",
            "置信区间": "用于量化估计值的不确定性范围，提供统计上的可靠性保证。",
            "深度去偏": "通过深度学习方法减少估计中的偏差，提高评估的准确性和可靠性。"
        },
        "success": true
    },
    {
        "order": 266,
        "title": "DeepReDuce:  ReLU Reduction for Fast Private Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9709",
        "abstract": "The recent rise of privacy concerns has led researchers to devise\nmethods for private neural inference---where  inferences are made directly on encrypted data, never seeing inputs. The primary challenge facing private inference is that computing on encrypted data levies an impractically-high latency penalty, stemming mostly from non-linear operators like ReLU. Enabling practical and private inference requires new optimization methods that minimize network ReLU counts while preserving accuracy.  This paper proposes DeepReDuce: a set of optimizations for the judicious removal of ReLUs to reduce private inference latency.\nThe key insight is that not all ReLUs contribute equally to accuracy. We leverage this insight to drop, or remove, ReLUs from classic networks to significantly \nreduce inference latency and maintain high accuracy. Given a network architecture,\nDeepReDuce outputs a Pareto frontier of networks that tradeoff the number of ReLUs and accuracy. Compared to the state-of-the-art for private inference\nDeepReDuce improves accuracy and reduces ReLU count by up to 3.5% (iso-ReLU count) and 3.5x (iso-accuracy), respectively.",
        "conference": "ICML",
        "中文标题": "DeepReDuce：用于快速隐私推理的ReLU削减",
        "摘要翻译": "近年来隐私问题的兴起促使研究人员开发出隐私神经推理方法——在这种方法中，推理直接在加密数据上进行，从不查看输入数据。隐私推理面临的主要挑战是，在加密数据上进行计算会带来不切实际的高延迟惩罚，这主要源于像ReLU这样的非线性操作符。实现实用且隐私的推理需要新的优化方法，这些方法能在保持准确性的同时最小化网络中的ReLU数量。本文提出了DeepReDuce：一组明智地移除ReLU以减少隐私推理延迟的优化方法。关键见解是并非所有的ReLU对准确性的贡献相同。我们利用这一见解从经典网络中删除或移除ReLU，以显著减少推理延迟并保持高准确性。给定一个网络架构，DeepReDuce输出一个网络帕累托前沿，这些网络在ReLU数量和准确性之间进行权衡。与隐私推理的最新技术相比，DeepReDuce在相同ReLU数量下提高了准确性高达3.5%，在相同准确性下减少了ReLU数量高达3.5倍。",
        "领域": "隐私保护计算、神经网络优化、加密数据推理",
        "问题": "如何在保持神经网络推理准确性的同时，减少因加密数据上的非线性操作（如ReLU）导致的高延迟问题。",
        "动机": "解决隐私神经推理中因加密数据计算导致的高延迟问题，特别是由ReLU等非线性操作引起的延迟。",
        "方法": "提出DeepReDuce优化方法，通过明智地移除对准确性贡献较小的ReLU，减少隐私推理的延迟，同时保持高准确性。",
        "关键词": [
            "隐私推理",
            "ReLU削减",
            "神经网络优化",
            "加密数据",
            "延迟减少"
        ],
        "涉及的技术概念": {
            "ReLU": "非线性激活函数，隐私推理中主要的高延迟来源。",
            "帕累托前沿": "在优化问题中表示在多个目标之间最优权衡的解集，此处用于展示ReLU数量与准确性之间的权衡。",
            "加密数据推理": "在不解密的情况下直接在加密数据上进行神经网络推理，保护数据隐私。"
        },
        "success": true
    },
    {
        "order": 267,
        "title": "Deep Reinforcement Learning amidst Continual Structured Non-Stationarity",
        "html": "https://ICML.cc//virtual/2021/poster/10467",
        "abstract": "As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment  from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.",
        "conference": "ICML",
        "中文标题": "持续结构化非平稳性中的深度强化学习",
        "摘要翻译": "作为人类，我们的目标和环境在我们的一生中会根据我们的经验、行动以及内外驱动力持续变化。相比之下，典型的强化学习问题设置考虑的是跨情节平稳的决策过程。我们能否开发出能够应对前者更为现实的问题设置中持续变化的强化学习算法？虽然原则上可以将策略梯度等策略上算法扩展到非平稳设置，但对于在学习时重放过去经验的更高效策略外算法来说，情况并非如此。在这项工作中，我们形式化了这个问题设置，并借鉴了在线学习和概率推断文献中的思想，推导出一种能够推理并应对这种终身非平稳性的策略外强化学习算法。我们的方法利用潜在变量模型从当前和过去的经验中学习环境的表示，并使用这种表示进行策略外强化学习。我们进一步引入了几个表现出终身非平稳性的模拟环境，并通过实证发现，我们的方法大大优于那些不考虑环境变化的策略。",
        "领域": "强化学习、非平稳环境适应、终身学习",
        "问题": "开发能够在持续变化的非平稳环境中有效学习的强化学习算法",
        "动机": "现实世界中的决策过程往往是非平稳的，而现有的强化学习算法大多假设环境是平稳的，这限制了它们在现实世界中的应用",
        "方法": "利用潜在变量模型学习环境的表示，并结合策略外强化学习算法来处理终身非平稳性",
        "关键词": [
            "深度强化学习",
            "非平稳环境",
            "终身学习",
            "策略外学习",
            "潜在变量模型"
        ],
        "涉及的技术概念": {
            "策略外强化学习": "一种通过重放过去经验来学习的强化学习方法，提高了学习效率",
            "潜在变量模型": "用于从当前和过去的经验中学习环境的表示，帮助模型理解和适应环境的变化",
            "终身非平稳性": "指环境随时间持续变化的特性，要求算法能够适应这种变化以保持性能"
        },
        "success": true
    },
    {
        "order": 268,
        "title": "DeepWalking Backwards: From Embeddings Back to Graphs",
        "html": "https://ICML.cc//virtual/2021/poster/9715",
        "abstract": "Low-dimensional node embeddings play a key role in analyzing graph datasets.\nHowever, little work studies exactly what information is encoded by popular embedding methods, and how this information correlates with performance in downstream learning tasks.\nWe tackle this question by studying whether embeddings can be inverted to (approximately) recover the graph used to generate them.\nFocusing on a variant of the popular DeepWalk method \\cite{PerozziAl-RfouSkiena:2014, QiuDongMa:2018}, we present algorithms for accurate embedding inversion -- i.e., from the low-dimensional embedding of a graph $G$, we can find a graph $\\tilde G$ with a very similar embedding. We perform numerous experiments on real-world networks, observing that significant information about $G$, such as specific edges and bulk properties like triangle density, is often lost in $\\tilde G$. However, community structure is often preserved or even enhanced. Our findings are a step towards a more rigorous understanding of exactly what information embeddings encode about the input graph, and why this information is useful for learning tasks.",
        "conference": "ICML",
        "中文标题": "深度游走逆向：从嵌入回到图",
        "摘要翻译": "低维节点嵌入在分析图数据集中扮演着关键角色。然而，很少有研究确切探讨流行嵌入方法编码了哪些信息，以及这些信息如何与下游学习任务的性能相关联。我们通过研究嵌入是否可以被逆向（近似）恢复生成它们的图来探讨这个问题。专注于流行DeepWalk方法的一个变体，我们提出了精确的嵌入逆向算法——即，从一个图G的低维嵌入，我们可以找到一个具有非常相似嵌入的图˜G。我们在真实世界的网络上进行了大量实验，观察到关于G的大量信息，如特定边和整体属性如三角形密度，经常在˜G中丢失。然而，社区结构经常被保留甚至增强。我们的发现是朝着更严格理解嵌入究竟编码了输入图的哪些信息，以及为什么这些信息对学习任务有用的一步。",
        "领域": "图嵌入、网络分析、社区检测",
        "问题": "研究图嵌入方法编码的具体信息及其与下游任务性能的关系",
        "动机": "理解嵌入方法编码的信息内容及其对学习任务的影响",
        "方法": "提出并实施嵌入逆向算法，通过实验分析嵌入信息保留情况",
        "关键词": [
            "图嵌入",
            "DeepWalk",
            "嵌入逆向",
            "社区结构",
            "网络分析"
        ],
        "涉及的技术概念": {
            "图嵌入": "将图中的节点映射到低维空间的技术，用于捕捉和保留图的结构信息",
            "DeepWalk": "一种流行的图嵌入方法，通过模拟随机游走来学习节点的低维表示",
            "社区结构": "图中节点群的划分，这些节点群内部连接紧密而与外部连接稀疏"
        },
        "success": true
    },
    {
        "order": 269,
        "title": "Defense against backdoor attacks via robust covariance estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9013",
        "abstract": "Modern machine learning increasingly requires training on a large collection of data from multiple sources, not all of which can be trusted. A particularly frightening scenario is when a small fraction of corrupted data changes the behavior of the trained model when triggered by an attacker-specified watermark. Such a compromised model will be deployed unnoticed as the model is accurate otherwise. There has been promising attempts to use the intermediate representations of such a model to separate corrupted examples from clean ones. However, these methods require a significant fraction of the data to be corrupted, in order to have strong enough signal for detection. We propose a novel defense algorithm using robust covariance estimation to amplify the spectral signature of corrupted data. This defense is able to  completely remove backdoors whenever the benchmark backdoor attacks are successful, even in regimes where previous methods have no hope for detecting poisoned examples.",
        "conference": "ICML",
        "中文标题": "通过鲁棒协方差估计防御后门攻击",
        "摘要翻译": "现代机器学习越来越需要在来自多个来源的大量数据上进行训练，而这些数据并非全部可信。一个特别令人担忧的情景是，当一小部分被污染的数据在攻击者指定的水印触发时，改变了训练模型的行为。这样的被攻陷模型将在未被察觉的情况下部署，因为模型在其他情况下是准确的。已有研究尝试使用此类模型的中间表示来区分被污染的样本和干净的样本。然而，这些方法需要数据中有较大比例被污染，以便有足够强的信号进行检测。我们提出了一种新颖的防御算法，使用鲁棒协方差估计来放大被污染数据的光谱特征。这种防御能够在基准后门攻击成功的情况下完全移除后门，即使在以往方法无法检测到被污染样本的情况下也是如此。",
        "领域": "深度学习安全、对抗性机器学习、模型鲁棒性",
        "问题": "防御机器学习模型中的后门攻击，特别是在数据来源不可信的情况下。",
        "动机": "解决现有方法在检测被污染数据时需要较高污染比例的限制，提高模型的安全性。",
        "方法": "提出了一种基于鲁棒协方差估计的新算法，用于放大被污染数据的光谱特征，从而有效识别和移除后门。",
        "关键词": [
            "后门攻击防御",
            "鲁棒协方差估计",
            "模型安全",
            "对抗性机器学习",
            "数据污染检测"
        ],
        "涉及的技术概念": {
            "鲁棒协方差估计": "用于放大被污染数据的光谱特征，提高检测的敏感性和准确性。",
            "后门攻击": "一种通过在训练数据中植入特定触发器来操控模型行为的攻击方式。",
            "光谱特征": "指数据在特定表示空间中的特征分布，用于区分正常和被污染的数据。"
        },
        "success": true
    },
    {
        "order": 270,
        "title": "Delving into Deep Imbalanced Regression",
        "html": "https://ICML.cc//virtual/2021/poster/9493",
        "abstract": "Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at: https://github.com/YyzHarry/imbalanced-regression.",
        "conference": "ICML",
        "中文标题": "深入探索深度不平衡回归",
        "摘要翻译": "现实世界的数据往往呈现出不平衡的分布，其中某些目标值的观测数量显著较少。现有的处理不平衡数据的技术主要集中在具有分类索引的目标上，即不同的类别。然而，许多任务涉及连续的目标，其中类别之间不存在硬边界。我们将深度不平衡回归（DIR）定义为从这种具有连续目标的不平衡数据中学习，处理某些目标值可能缺失的数据，并推广到整个目标范围。受到分类和连续标签空间内在差异的启发，我们提出了对标签和特征的分布平滑，这明确承认了附近目标的影响，并校准了标签和学习到的特征分布。我们从计算机视觉、自然语言处理和医疗领域的常见现实世界任务中整理并基准测试了大规模DIR数据集。大量实验验证了我们策略的优越性能。我们的工作填补了实际不平衡回归问题在基准和技术上的空白。代码和数据可在https://github.com/YyzHarry/imbalanced-regression获取。",
        "领域": "不平衡学习、回归分析、多模态学习",
        "问题": "解决连续目标值数据中的不平衡分布问题",
        "动机": "针对连续目标值数据中的不平衡分布问题，提出有效的深度学习方法",
        "方法": "提出分布平滑技术，对标签和特征进行平滑处理，以考虑附近目标的影响并校准分布",
        "关键词": [
            "深度不平衡回归",
            "分布平滑",
            "连续目标",
            "不平衡学习",
            "多模态学习"
        ],
        "涉及的技术概念": {
            "深度不平衡回归（DIR）": "从具有连续目标的不平衡数据中学习，处理缺失数据并推广到整个目标范围",
            "分布平滑": "对标签和特征进行平滑处理，考虑附近目标的影响，以校准分布",
            "连续目标": "任务中涉及的目标值是连续的，类别之间不存在硬边界"
        },
        "success": true
    },
    {
        "order": 271,
        "title": "Demonstration-Conditioned Reinforcement Learning for Few-Shot Imitation",
        "html": "https://ICML.cc//virtual/2021/poster/10007",
        "abstract": "In few-shot imitation, an agent is given a few demonstrations of a previously unseen task, and must then successfully perform that task. We propose a novel approach to learning few-shot-imitation agents that we call demonstration-conditioned reinforcement learning (DCRL). Given a training set consisting of demonstrations, reward functions and transition distributions for multiple tasks, the idea is to work with a policy that takes demonstrations as input, and to train this policy to maximize the average of the cumulative reward over the set of training tasks. Relative to previously proposed few-shot imitation methods that use behaviour cloning or infer reward functions from demonstrations, our method has the disadvantage that it requires reward functions at training time. However, DCRL also has several advantages, such as the ability to improve upon suboptimal demonstrations, to operate given state-only demonstrations, and to cope with a domain shift between the demonstrator and the agent. Moreover, we show that DCRL outperforms methods based on behaviour cloning by a large margin, on navigation tasks and on robotic manipulation tasks from the Meta-World benchmark.",
        "conference": "ICML",
        "success": true,
        "中文标题": "演示条件强化学习用于少样本模仿",
        "摘要翻译": "在少样本模仿中，智能体被提供一些之前未见过的任务的演示，然后必须成功执行该任务。我们提出了一种新的学习少样本模仿智能体的方法，称为演示条件强化学习（DCRL）。给定一个由多个任务的演示、奖励函数和转移分布组成的训练集，我们的想法是使用一个将演示作为输入的策略，并训练这个策略以最大化训练任务集上的累积奖励的平均值。相对于之前提出的使用行为克隆或从演示中推断奖励函数的少样本模仿方法，我们的方法的缺点是在训练时需要奖励函数。然而，DCRL也有几个优点，比如能够改进次优演示，能够在只有状态演示的情况下操作，以及能够应对演示者和智能体之间的领域转移。此外，我们展示了在导航任务和来自Meta-World基准的机器人操作任务上，DCRL大幅优于基于行为克隆的方法。",
        "领域": "强化学习, 机器人操作, 导航任务",
        "问题": "如何在只有少量演示的情况下，让智能体成功模仿并执行新任务",
        "动机": "解决少样本模仿任务中智能体如何有效学习和模仿的问题，尤其是在演示质量不一或存在领域转移的情况下",
        "方法": "提出演示条件强化学习（DCRL），通过将演示作为策略输入并最大化累积奖励的平均值来训练智能体",
        "关键词": [
            "少样本模仿",
            "演示条件强化学习",
            "机器人操作",
            "导航任务",
            "Meta-World基准"
        ],
        "涉及的技术概念": {
            "演示条件强化学习（DCRL）": "一种新的强化学习方法，通过将演示作为输入来训练策略，以解决少样本模仿问题",
            "行为克隆": "一种模仿学习方法，通过直接复制演示中的行为来训练智能体，DCRL在此方法基础上进行了改进",
            "领域转移": "指演示者和智能体在操作环境或条件上的差异，DCRL能够有效应对这种情况"
        }
    },
    {
        "order": 272,
        "title": "Demystifying Inductive Biases for (Beta-)VAE Based Architectures",
        "html": "https://ICML.cc//virtual/2021/poster/9281",
        "abstract": "The performance of Beta-Variational-Autoencoders and their variants on learning semantically meaningful, disentangled representations is unparalleled. On the other hand, there are theoretical arguments suggesting the impossibility of unsupervised  disentanglement. In this work, we shed light on the inductive bias responsible for the success of VAE-based architectures. We show that in classical datasets the structure of variance, induced by the generating factors, is conveniently aligned with the latent directions fostered by the VAE objective. This builds the pivotal bias on which the disentangling abilities of VAEs rely. By small, elaborate perturbations of existing datasets, we hide the convenient correlation structure that is easily exploited by a variety of architectures. To demonstrate this, we construct modified versions of standard datasets in which (i) the generative factors are perfectly preserved; (ii) each image undergoes a mild transformation causing a small change of variance; (iii) the leading VAE-based disentanglement architectures fail to produce disentangled representations whilst the performance of a non-variational method remains unchanged.",
        "conference": "ICML",
        "中文标题": "揭秘基于(Beta-)VAE架构的归纳偏置",
        "摘要翻译": "Beta-变分自编码器及其变体在学习语义上有意义的解耦表示方面的表现是无与伦比的。另一方面，有理论论点表明无监督解耦是不可能的。在这项工作中，我们揭示了导致基于VAE架构成功的归纳偏置。我们表明，在经典数据集中，由生成因素引起的方差结构与VAE目标促进的潜在方向方便地对齐。这构成了VAEs解耦能力所依赖的关键偏置。通过对现有数据集进行精细的小扰动，我们隐藏了容易被各种架构利用的方便的相关结构。为了证明这一点，我们构建了标准数据集的修改版本，其中（i）生成因素被完美保留；（ii）每张图像都经过轻微的变换，导致方差的微小变化；（iii）领先的基于VAE的解耦架构未能产生解耦表示，而非变分方法的性能保持不变。",
        "领域": "变分自编码器、表示学习、无监督学习",
        "问题": "揭示基于VAE架构成功学习解耦表示的归纳偏置",
        "动机": "理解并解释VAE架构在无监督学习解耦表示方面的成功背后的原因",
        "方法": "通过精细扰动现有数据集，隐藏方便的相关结构，构建修改版本的数据集来测试不同架构的性能",
        "关键词": [
            "Beta-VAE",
            "解耦表示",
            "归纳偏置",
            "无监督学习",
            "变分自编码器"
        ],
        "涉及的技术概念": {
            "Beta-VAE": "一种变分自编码器的变体，通过引入额外的超参数来平衡重构精度和解耦表示的学习",
            "解耦表示": "指在潜在空间中，数据的生成因素被分离成独立的维度，每个维度对应一个生成因素",
            "归纳偏置": "指学习算法在学习过程中对某些假设的偏好，这些假设有助于模型在未见数据上的泛化"
        },
        "success": true
    },
    {
        "order": 273,
        "title": "Dense for the Price of Sparse: Improved Performance of Sparsely Initialized Networks via a Subspace Offset",
        "html": "https://ICML.cc//virtual/2021/poster/9247",
        "abstract": "That neural networks may be pruned to high sparsities and retain high accuracy is well established. Recent research efforts focus on pruning immediately after initialization so as to allow the computational savings afforded by sparsity to extend to the training process. In this work, we introduce a new `DCT plus Sparse' layer architecture, which maintains information propagation and trainability even with as little as 0.01% trainable parameters remaining. We show that standard training of networks built with these layers, and pruned at initialization, achieves state-of-the-art accuracy for extreme sparsities on a variety of benchmark network architectures and datasets. Moreover, these results are achieved using only simple heuristics to determine the locations of the trainable parameters in the network, and thus without having to initially store or compute with the full, unpruned network, as is required by competing prune-at-initialization algorithms. Switching from standard sparse layers to DCT plus Sparse layers does not increase the storage footprint of a network and incurs only a small additional computational overhead.",
        "conference": "ICML",
        "中文标题": "稀疏的价格换取密集：通过子空间偏移提升稀疏初始化网络的性能",
        "摘要翻译": "神经网络可以被修剪到高稀疏度并保持高准确性，这一点已经得到广泛认可。最近的研究工作集中在初始化后立即进行修剪，以便使稀疏性带来的计算节省能够延伸到训练过程。在这项工作中，我们引入了一种新的‘DCT加稀疏’层架构，即使在仅有0.01%的可训练参数保留的情况下，也能保持信息传播和可训练性。我们展示了使用这些层构建的网络，在初始化时进行修剪，通过标准训练，在各种基准网络架构和数据集上实现了极端稀疏性下的最先进准确性。此外，这些结果仅通过简单的启发式方法确定网络中可训练参数的位置即可实现，因此无需像竞争性的初始化时修剪算法那样，最初存储或计算完整的未修剪网络。从标准稀疏层切换到DCT加稀疏层不会增加网络的存储占用，并且仅带来少量的额外计算开销。",
        "领域": "神经网络修剪、深度学习优化、计算机视觉",
        "问题": "如何在极端的稀疏性下保持神经网络的高准确性和可训练性",
        "动机": "探索在神经网络初始化阶段进行修剪，以节省训练过程中的计算资源，同时保持或提升模型的性能",
        "方法": "引入‘DCT加稀疏’层架构，结合简单的启发式方法确定可训练参数的位置，实现极端稀疏性下的高效训练",
        "关键词": [
            "神经网络修剪",
            "稀疏训练",
            "DCT加稀疏层",
            "初始化修剪",
            "计算优化"
        ],
        "涉及的技术概念": {
            "DCT加稀疏层": "一种新的层架构，用于在极端稀疏性下保持信息传播和网络的可训练性",
            "初始化修剪": "在神经网络初始化阶段进行参数修剪，以节省训练过程中的计算资源",
            "启发式方法": "用于确定网络中可训练参数位置的简单策略，避免存储或计算完整的未修剪网络"
        },
        "success": true
    },
    {
        "order": 274,
        "title": "Density Constrained Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8985",
        "abstract": "We study constrained reinforcement learning (CRL) from a novel perspective by setting constraints directly on state density functions, rather than the value functions considered by previous works. State density has a clear physical and mathematical interpretation, and is able to express a wide variety of constraints such as resource limits and safety requirements. Density constraints can also avoid the time-consuming process of designing and tuning cost functions required by value function-based constraints to encode system specifications. We leverage the duality between density functions and Q functions to develop an effective algorithm to solve the density constrained RL problem optimally and the constrains are guaranteed to be satisfied. We prove that the proposed algorithm converges to a near-optimal solution with a bounded error even when the policy update is imperfect. We use a set of comprehensive experiments to demonstrate the advantages of our approach over state-of-the-art CRL methods, with a wide range of density constrained tasks as well as standard CRL benchmarks such as Safety-Gym.",
        "conference": "ICML",
        "中文标题": "密度约束的强化学习",
        "摘要翻译": "我们从一个新颖的角度研究约束强化学习（CRL），即直接在状态密度函数上设置约束，而非以往工作中考虑的价值函数。状态密度具有清晰的物理和数学解释，能够表达多种约束，如资源限制和安全要求。密度约束还可以避免基于价值函数的约束在编码系统规范时所需的设计和调整成本函数的耗时过程。我们利用密度函数与Q函数之间的对偶性，开发了一种有效算法来最优地解决密度约束的RL问题，并保证约束得到满足。我们证明了即使策略更新不完美，所提出的算法也能收敛到一个有界误差的近优解。我们通过一系列综合实验，展示了我们的方法在广泛的密度约束任务以及标准CRL基准（如Safety-Gym）上相对于最先进的CRL方法的优势。",
        "领域": "强化学习、安全约束学习、资源优化",
        "问题": "如何在强化学习中直接对状态密度函数施加约束，以更直观和高效地表达资源限制和安全要求。",
        "动机": "传统的基于价值函数的约束方法需要设计和调整复杂的成本函数，过程耗时且不够直观。通过直接在状态密度函数上设置约束，可以更直接地表达各种系统规范，同时避免上述问题。",
        "方法": "利用密度函数与Q函数之间的对偶性，开发了一种算法来最优地解决密度约束的强化学习问题，并保证约束的满足。",
        "关键词": [
            "约束强化学习",
            "状态密度",
            "对偶性",
            "安全约束",
            "资源优化"
        ],
        "涉及的技术概念": {
            "状态密度函数": "用于直接表达强化学习中的约束条件，如资源限制和安全要求，具有清晰的物理和数学解释。",
            "对偶性": "在密度函数与Q函数之间建立联系，使得能够开发出有效算法来解决密度约束的强化学习问题。",
            "策略更新": "即使在策略更新不完美的情况下，所提出的算法也能保证收敛到一个有界误差的近优解。"
        },
        "success": true
    },
    {
        "order": 275,
        "title": "Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers",
        "html": "https://ICML.cc//virtual/2021/poster/8645",
        "abstract": "Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than 50,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.",
        "conference": "ICML",
        "中文标题": "穿越拥挤的山谷 - 深度学习优化器基准测试",
        "摘要翻译": "选择优化器被认为是深度学习中最为关键的设计决策之一，而这并非易事。日益增长的文献现在列出了数百种优化方法。在缺乏明确的理论指导和确凿的实证证据的情况下，决策往往基于轶事。在这项工作中，我们的目标是取代这些轶事，如果不是用一个确凿的排名，那么至少是用有证据支持的经验法则。为此，我们对十五种特别受欢迎的深度学习优化器进行了广泛、标准化的基准测试，同时简要概述了可能选择的广泛范围。分析了超过50,000次独立运行后，我们贡献了以下三点：(i) 优化器性能在不同任务间差异很大。(ii) 我们观察到，使用默认参数评估多个优化器的效果大致与调整单个固定优化器的超参数相当。(iii) 虽然我们无法辨别出一种在所有测试任务中明显占优的优化方法，但我们确定了一个显著减少的特定优化器和参数选择子集，这些选择在我们的实验中通常能带来有竞争力的结果：Adam仍然是一个强有力的竞争者，新方法未能显著且持续地超越它。我们的开源结果可作为具有挑战性和良好调整的基线，用于更有效地评估新型优化方法，而无需额外的计算努力。",
        "领域": "深度学习优化、机器学习基准测试、超参数优化",
        "问题": "深度学习优化器的选择和性能评估缺乏系统性和实证支持",
        "动机": "为深度学习社区提供基于实证的优化器选择指南，减少基于轶事的决策",
        "方法": "通过标准化基准测试分析15种流行优化器的性能，基于超过50,000次运行的数据",
        "关键词": [
            "优化器基准测试",
            "深度学习优化",
            "Adam优化器",
            "超参数调整",
            "性能评估"
        ],
        "涉及的技术概念": {
            "优化器基准测试": "系统地评估和比较不同优化器在多种任务上的性能",
            "超参数调整": "调整优化器的参数以优化模型训练过程",
            "Adam优化器": "一种结合了动量法和RMSprop的优化算法，广泛用于深度学习"
        },
        "success": true
    },
    {
        "order": 276,
        "title": "Detecting Rewards Deterioration in Episodic Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8883",
        "abstract": "In many RL applications, once training ends, it is vital to detect any deterioration in the agent performance as soon as possible. Furthermore, it often has to be done without modifying the policy and under minimal assumptions regarding the environment.\nIn this paper, we address this problem by focusing directly on the rewards and testing for degradation. We consider an episodic framework, where the rewards within each episode are not independent, nor identically-distributed, nor Markov. We present this problem as a multivariate mean-shift detection problem with possibly partial observations. We define the mean-shift in a way corresponding to deterioration of a temporal signal (such as the rewards), and derive a test for this problem with optimal statistical power.\nEmpirically, on deteriorated rewards in control problems (generated using various environment modifications), the test is demonstrated to be more powerful than standard tests - often by orders of magnitude.\nWe also suggest a novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to episodic (non-i.i.d) signal and allowing our test to run sequentially in an online manner.\nOur method does not rely on a learned model of the environment, is entirely external to the agent, and in fact can be applied to detect changes or drifts in any episodic signal.",
        "conference": "ICML",
        "中文标题": "检测周期性强化学习中的奖励退化",
        "摘要翻译": "在许多强化学习应用中，一旦训练结束，尽快检测到代理性能的任何退化至关重要。此外，这通常需要在不对策略进行修改且对环境做出最小假设的情况下完成。在本文中，我们通过直接关注奖励并测试退化来解决这个问题。我们考虑了一个周期性框架，其中每个周期内的奖励既不是独立的，也不是同分布的，也不是马尔可夫的。我们将这个问题呈现为一个可能部分观测的多变量均值漂移检测问题。我们以一种对应于时间信号（如奖励）退化的方式定义了均值漂移，并为此问题推导出了一个具有最优统计功效的测试。实证上，在控制问题中的退化奖励（通过各种环境修改生成）上，该测试被证明比标准测试更强大——通常高出几个数量级。我们还提出了一种新颖的自举机制用于误报率控制（BFAR），适用于周期性（非独立同分布）信号，并允许我们的测试以在线方式顺序运行。我们的方法不依赖于学习的环境模型，完全独立于代理，实际上可以应用于检测任何周期性信号的变化或漂移。",
        "领域": "强化学习、奖励机制分析、在线学习",
        "问题": "在强化学习训练结束后，如何在不修改策略和最小环境假设下，有效检测代理性能的退化。",
        "动机": "为了确保强化学习代理在部署后的性能稳定性，需要一种能够在不干扰代理操作的情况下，及时检测性能退化的方法。",
        "方法": "通过将问题建模为多变量均值漂移检测问题，提出了一种针对周期性非独立同分布奖励信号的退化检测测试，并引入了一种新的自举机制用于控制误报率。",
        "关键词": [
            "奖励退化检测",
            "周期性强化学习",
            "均值漂移检测",
            "误报率控制",
            "在线学习"
        ],
        "涉及的技术概念": {
            "多变量均值漂移检测": "用于检测奖励信号中的退化，通过统计方法识别信号均值的变化。",
            "自举机制（BFAR）": "一种新颖的误报率控制方法，适用于周期性非独立同分布信号，支持在线顺序检测。",
            "非独立同分布信号": "指周期性强化学习中的奖励信号，这些信号在时间上相关且分布可能随时间变化。"
        },
        "success": true
    },
    {
        "order": 277,
        "title": "Detection of Signal in the Spiked Rectangular Models",
        "html": "https://ICML.cc//virtual/2021/poster/10495",
        "abstract": "We consider the problem of detecting signals in the rank-one signal-plus-noise data matrix models that generalize the spiked Wishart matrices. We show that the principal component analysis can be improved by pre-transforming the matrix entries if the noise is non-Gaussian. As an intermediate step, we prove a sharp phase transition of the largest eigenvalues of spiked rectangular matrices, which extends the Baik--Ben Arous--P\\'ech\\'e (BBP) transition. We also propose a hypothesis test to detect the presence of signal with low computational complexity, based on the linear spectral statistics, which minimizes the sum of the Type-I and Type-II errors when the noise is Gaussian. ",
        "conference": "ICML",
        "中文标题": "尖峰矩形模型中信号的检测",
        "摘要翻译": "我们考虑在秩一信号加噪声数据矩阵模型中检测信号的问题，该模型推广了尖峰Wishart矩阵。我们证明，如果噪声是非高斯的，通过对矩阵条目进行预变换可以改进主成分分析。作为中间步骤，我们证明了尖峰矩形矩阵最大特征值的尖锐相变，这扩展了Baik--Ben Arous--Péché (BBP)相变。我们还提出了一种基于线性谱统计量的假设检验方法，用于检测信号的存在，该方法在噪声为高斯时最小化I型和II型错误的总和，具有较低的计算复杂度。",
        "领域": "信号处理、统计机器学习、高维统计",
        "问题": "在秩一信号加噪声数据矩阵模型中检测信号的问题",
        "动机": "改进主成分分析在非高斯噪声条件下的性能，并扩展尖峰矩形矩阵最大特征值的相变理论",
        "方法": "通过对矩阵条目进行预变换改进主成分分析，证明尖峰矩形矩阵最大特征值的尖锐相变，提出基于线性谱统计量的假设检验方法",
        "关键词": [
            "信号检测",
            "主成分分析",
            "尖峰矩阵",
            "线性谱统计量",
            "假设检验"
        ],
        "涉及的技术概念": {
            "尖峰Wishart矩阵": "用于描述信号加噪声数据矩阵模型，推广了传统的Wishart矩阵",
            "Baik--Ben Arous--Péché (BBP)相变": "描述了尖峰矩阵最大特征值的相变行为，本工作将其扩展到尖峰矩形矩阵",
            "线性谱统计量": "用于构建假设检验的统计量，能够有效检测信号的存在并最小化错误率"
        },
        "success": true
    },
    {
        "order": 278,
        "title": "DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9111",
        "abstract": "In fully cooperative multi-agent reinforcement learning (MARL) settings, the environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of the other agents. To address the above issues, we integrate distributional RL and value function factorization methods by proposing a Distributional Value Function Factorization (DFAC) framework to generalize expected value function factorization methods to their distributional variants. DFAC extends the individual utility functions from deterministic variables to random variables, and models the quantile function of the total return as a quantile mixture. To validate DFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game with stochastic rewards and perform experiments on all Super Hard tasks of StarCraft Multi-Agent Challenge, showing that DFAC is able to outperform expected value function factorization baselines.",
        "conference": "ICML",
        "中文标题": "DFAC框架：通过分位数混合分解价值函数以实现多智能体分布Q学习",
        "摘要翻译": "在完全合作的多智能体强化学习（MARL）环境中，由于每个智能体的部分可观察性以及其他智能体策略的不断变化，环境具有高度的随机性。为了解决上述问题，我们通过提出一个分布价值函数分解（DFAC）框架，将分布RL和价值函数分解方法结合起来，将期望价值函数分解方法推广到它们的分布变体。DFAC将个体效用函数从确定性变量扩展到随机变量，并将总回报的分位数函数建模为分位数混合。为了验证DFAC，我们展示了DFAC在具有随机奖励的简单两步矩阵游戏中分解的能力，并在《星际争霸》多智能体挑战的所有超级困难任务上进行了实验，结果表明DFAC能够超越期望价值函数分解基线。",
        "领域": "多智能体强化学习、分布强化学习、价值函数分解",
        "问题": "解决在高度随机的多智能体环境中，如何有效地分解和优化价值函数的问题。",
        "动机": "研究动机是为了在多智能体强化学习环境中，通过分布RL和价值函数分解方法的结合，提高模型对高度随机环境的适应性和性能。",
        "方法": "提出了一个分布价值函数分解（DFAC）框架，通过将个体效用函数扩展到随机变量，并将总回报的分位数函数建模为分位数混合，来推广期望价值函数分解方法。",
        "关键词": [
            "多智能体强化学习",
            "分布强化学习",
            "价值函数分解",
            "分位数混合",
            "随机环境"
        ],
        "涉及的技术概念": {
            "分布强化学习": "在论文中用于处理环境的高度随机性，通过考虑回报的分布而不仅仅是期望值来提高模型的适应性。",
            "价值函数分解": "论文中用于将复杂的多智能体问题分解为更简单的子问题，通过分解价值函数来优化每个智能体的策略。",
            "分位数混合": "论文中用于建模总回报的分位数函数，作为一种新的方法来组合个体智能体的分布信息，以优化整体性能。"
        },
        "success": true
    },
    {
        "order": 279,
        "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm via Langevin Monte Carlo within Gibbs",
        "html": "https://ICML.cc//virtual/2021/poster/9275",
        "abstract": "Performing reliable Bayesian inference on a big data scale is becoming a keystone in the modern era of machine learning. A workhorse class of methods to achieve this task are Markov chain Monte Carlo (MCMC) algorithms and their design to handle distributed datasets has been the subject of many works. However, existing methods are not completely either reliable or computationally efficient. In this paper, we propose to fill this gap in the case where the dataset is partitioned and stored on computing nodes within a cluster under a master/slaves architecture. We derive a user-friendly centralised distributed MCMC algorithm with provable scaling in high-dimensional settings. We illustrate the relevance of the proposed methodology on both synthetic and real data experiments.",
        "conference": "ICML",
        "中文标题": "DG-LMC：一种基于吉布斯采样中朗之万蒙特卡洛的即插即用可扩展同步分布式MCMC算法",
        "摘要翻译": "在大数据规模上执行可靠的贝叶斯推理正成为现代机器学习时代的基石。实现这一任务的主力方法类别是马尔可夫链蒙特卡洛（MCMC）算法，它们处理分布式数据集的设计已成为许多研究的主题。然而，现有方法在可靠性或计算效率上并不完全令人满意。本文提出，在数据集被分区并存储在集群中的计算节点上，采用主/从架构的情况下填补这一空白。我们推导出一个用户友好的集中式分布式MCMC算法，在高维设置中具有可证明的扩展性。我们通过合成和真实数据实验说明了所提出方法的相关性。",
        "领域": "贝叶斯推理、分布式计算、高维统计",
        "问题": "现有分布式MCMC算法在可靠性和计算效率上的不足",
        "动机": "填补在大数据规模上执行可靠贝叶斯推理时，现有分布式MCMC算法在可靠性和计算效率上的空白",
        "方法": "提出了一种基于吉布斯采样中朗之万蒙特卡洛的即插即用可扩展同步分布式MCMC算法",
        "关键词": [
            "分布式MCMC",
            "朗之万蒙特卡洛",
            "吉布斯采样",
            "贝叶斯推理",
            "高维统计"
        ],
        "涉及的技术概念": {
            "马尔可夫链蒙特卡洛（MCMC）": "用于在大数据规模上执行贝叶斯推理的主力方法类别",
            "朗之万蒙特卡洛": "在吉布斯采样中使用的一种技术，用于提高算法的效率和可靠性",
            "吉布斯采样": "一种MCMC方法，用于从复杂的多变量概率分布中采样"
        },
        "success": true
    },
    {
        "order": 280,
        "title": "Dichotomous Optimistic Search to Quantify Human Perception",
        "html": "https://ICML.cc//virtual/2021/poster/8599",
        "abstract": "In this paper we address a variant of  the continuous multi-armed bandits problem, called the threshold estimation problem, which is  at the heart of many psychometric experiments. Here, the objective is to estimate the sensitivity threshold for an unknown psychometric function Psi, which is assumed to be non decreasing and continuous. Our algorithm, Dichotomous Optimistic Search (DOS), efficiently solves this task by taking inspiration from hierarchical multi-armed bandits and Black-box optimization. Compared to previous approaches, DOS is model free and only makes minimal assumption on Psi smoothness, while having strong theoretical guarantees that compares favorably to recent methods from both Psychophysics  and Global Optimization. We also empirically evaluate DOS and show that it significantly outperforms these methods, both in experiments that mimics the conduct of a psychometric experiment, and in tests with large pulls budgets that illustrate the faster convergence rate.",
        "conference": "ICML",
        "中文标题": "二分乐观搜索量化人类感知",
        "摘要翻译": "本文我们探讨了连续多臂老虎机问题的一个变体，称为阈值估计问题，这是许多心理测量实验的核心。目标是估计一个未知心理测量函数Psi的敏感度阈值，该函数被假定为非递减且连续的。我们的算法，二分乐观搜索（DOS），通过从分层多臂老虎机和黑盒优化中汲取灵感，高效地解决了这一任务。与之前的方法相比，DOS是无模型的，仅对Psi的平滑性做出最小假设，同时具有强有力的理论保证，这些保证与心理物理学和全局优化中的最新方法相比具有优势。我们还对DOS进行了实证评估，结果表明，无论是在模拟心理测量实验进行的实验中，还是在展示更快收敛速度的大规模拉动预算测试中，DOS都显著优于这些方法。",
        "领域": "心理测量学、全局优化、多臂老虎机问题",
        "问题": "估计未知心理测量函数的敏感度阈值",
        "动机": "解决心理测量实验中阈值估计的效率与准确性问题",
        "方法": "采用二分乐观搜索（DOS）算法，结合分层多臂老虎机和黑盒优化的思想",
        "关键词": [
            "二分乐观搜索",
            "阈值估计",
            "心理测量函数",
            "多臂老虎机",
            "黑盒优化"
        ],
        "涉及的技术概念": {
            "二分乐观搜索（DOS）": "一种无模型算法，用于高效估计心理测量函数的敏感度阈值，仅需最小平滑性假设",
            "心理测量函数Psi": "非递减且连续的函数，代表人类感知的敏感度，算法旨在估计其阈值",
            "分层多臂老虎机": "DOS算法灵感来源之一，用于解决连续动作空间中的决策问题"
        },
        "success": true
    },
    {
        "order": 281,
        "title": "Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution",
        "html": "https://ICML.cc//virtual/2021/poster/8827",
        "abstract": "Model quantization is challenging due to many tedious hyper-parameters such as precision (bitwidth), dynamic range (minimum and maximum discrete values) and stepsize (interval between discrete values). \nUnlike prior arts that carefully tune these values, we present a fully differentiable approach to learn all of them, named Differentiable Dynamic Quantization (DDQ), which has several  benefits. \n(1) DDQ is able to quantize challenging lightweight architectures like MobileNets, where different layers prefer different quantization parameters.\n(2) DDQ is hardware-friendly and can be easily implemented using low-precision matrix-vector multiplication, making it  capable in many hardware such as ARM.\n(3) Extensive experiments show that DDQ outperforms prior arts on many networks and benchmarks, especially when models are already efficient and compact. e.g., DDQ is the first approach that achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.",
        "conference": "ICML",
        "中文标题": "可微分动态量化：混合精度与自适应分辨率",
        "摘要翻译": "模型量化由于涉及许多繁琐的超参数（如精度（位宽）、动态范围（最小和最大离散值）和步长（离散值之间的间隔）而具有挑战性。与之前需要仔细调整这些值的方法不同，我们提出了一种完全可微分的方法来学习所有这些参数，称为可微分动态量化（DDQ），它具有几个优点。（1）DDQ能够量化具有挑战性的轻量级架构，如MobileNets，其中不同层偏好不同的量化参数。（2）DDQ对硬件友好，可以轻松使用低精度矩阵向量乘法实现，使其能够在许多硬件如ARM上运行。（3）大量实验表明，DDQ在许多网络和基准测试上优于之前的方法，特别是当模型已经高效且紧凑时。例如，DDQ是首个在ImageNet上为MobileNetV2实现无损4位量化的方法。",
        "领域": "模型量化、轻量级神经网络、硬件加速",
        "问题": "解决模型量化过程中超参数调整繁琐且不灵活的问题",
        "动机": "为了简化模型量化过程并提高量化模型的性能，特别是在轻量级架构上",
        "方法": "提出了一种完全可微分的方法（DDQ），通过学习量化参数（精度、动态范围、步长）来实现自动化和优化的量化",
        "关键词": [
            "可微分量化",
            "混合精度",
            "自适应分辨率",
            "轻量级网络",
            "硬件友好"
        ],
        "涉及的技术概念": {
            "可微分动态量化（DDQ）": "一种通过学习量化参数来实现模型量化的方法，能够自动优化量化过程",
            "混合精度": "允许不同层或操作使用不同的位宽进行量化，以提高模型效率和性能",
            "自适应分辨率": "根据模型各层的需求动态调整量化的步长和范围，以优化量化效果"
        },
        "success": true
    },
    {
        "order": 282,
        "title": "Differentiable Particle Filtering via Entropy-Regularized Optimal Transport",
        "html": "https://ICML.cc//virtual/2021/poster/8557",
        "abstract": "Particle Filtering (PF) methods are an established class of procedures for performing inference in non-linear state-space models. Resampling is a key ingredient of PF necessary to obtain low variance likelihood and states estimates. However, traditional resampling methods result in PF-based loss functions being non-differentiable with respect to model and PF parameters. In a variational inference context, resampling also yields high variance gradient estimates of the PF-based evidence lower bound. By leveraging optimal transport ideas, we introduce a principled differentiable particle filter and provide convergence results. We demonstrate this novel method on a variety of applications.",
        "conference": "ICML",
        "中文标题": "通过熵正则化最优传输的可微分粒子滤波",
        "摘要翻译": "粒子滤波（PF）方法是一类成熟的非线性状态空间模型推理程序。重采样是PF中获取低方差似然和状态估计的关键步骤。然而，传统的重采样方法导致基于PF的损失函数对模型和PF参数不可微分。在变分推理的背景下，重采样还会导致基于PF的证据下界的梯度估计具有高方差。通过利用最优传输的思想，我们引入了一种原理性的可微分粒子滤波器，并提供了收敛结果。我们在多种应用上展示了这一新方法。",
        "领域": "状态空间模型推理, 变分推理, 最优传输",
        "问题": "解决传统粒子滤波方法中重采样步骤导致的损失函数不可微分和梯度估计高方差的问题。",
        "动机": "为了在变分推理框架下实现粒子滤波的可微分性，降低梯度估计的方差，提高推理效率。",
        "方法": "利用最优传输理论，提出了一种新的可微分粒子滤波方法，并提供了理论收敛性分析。",
        "关键词": [
            "可微分粒子滤波",
            "熵正则化",
            "最优传输",
            "变分推理",
            "状态空间模型"
        ],
        "涉及的技术概念": {
            "粒子滤波": "一种用于非线性状态空间模型推理的蒙特卡洛方法，通过粒子集近似表示后验分布。",
            "最优传输": "用于衡量两个概率分布之间差异的数学框架，本文中用于实现重采样的可微分性。",
            "熵正则化": "在最优传输问题中引入熵项以平滑优化问题，促进可微分性和数值稳定性。"
        },
        "success": true
    },
    {
        "order": 283,
        "title": "Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision",
        "html": "https://ICML.cc//virtual/2021/poster/8439",
        "abstract": "Sorting and ranking supervision is a method for training neural networks end-to-end based on ordering constraints. That is, the ground truth order of sets of samples is known, while their absolute values remain unsupervised. For that, we propose differentiable sorting networks by relaxing their pairwise conditional swap operations. To address the problems of vanishing gradients and extensive blurring that arise with larger numbers of layers, we propose mapping activations to regions with moderate gradients. We consider odd-even as well as bitonic sorting networks, which outperform existing relaxations of the sorting operation. We show that bitonic sorting networks can achieve stable training on large input sets of up to 1024 elements.",
        "conference": "ICML",
        "中文标题": "可微分排序网络：用于可扩展的排序与排名监督",
        "摘要翻译": "排序与排名监督是一种基于排序约束端到端训练神经网络的方法。即，样本集的真实顺序已知，而其绝对值仍处于无监督状态。为此，我们通过松弛其成对条件交换操作，提出了可微分排序网络。为了解决随着层数增加而出现的梯度消失和广泛模糊问题，我们提出将激活映射到具有中等梯度的区域。我们考虑了奇偶排序网络和双调排序网络，它们在排序操作的现有松弛方法中表现更优。我们展示双调排序网络能够在多达1024个元素的大输入集上实现稳定训练。",
        "领域": "深度学习优化、排序算法、神经网络训练",
        "问题": "如何在无监督绝对值的情况下，基于已知的样本顺序约束，端到端地训练神经网络进行排序和排名。",
        "动机": "为了解决在排序和排名任务中，传统方法无法直接利用排序约束进行端到端训练的问题，以及在大规模输入下训练稳定性差的问题。",
        "方法": "通过松弛排序网络中的成对条件交换操作，提出可微分排序网络，并采用激活映射到中等梯度区域的方法解决梯度消失和模糊问题，比较了奇偶排序网络和双调排序网络的性能。",
        "关键词": [
            "可微分排序网络",
            "排序与排名监督",
            "双调排序网络",
            "梯度消失",
            "端到端训练"
        ],
        "涉及的技术概念": {
            "可微分排序网络": "通过松弛排序网络中的条件交换操作，使其可微分，从而支持基于梯度的端到端训练。",
            "双调排序网络": "一种高效的排序网络结构，能够在大型输入集上实现稳定的训练和较高的排序性能。",
            "梯度消失": "在深层网络中，梯度在反向传播过程中逐渐变小，导致训练困难的问题，本文通过映射激活到中等梯度区域来解决。"
        },
        "success": true
    },
    {
        "order": 284,
        "title": "Differentiable Spatial Planning using Transformers",
        "html": "https://ICML.cc//virtual/2021/poster/9101",
        "abstract": "We consider the problem of spatial path planning. In contrast to the classical solutions which optimize a new plan from scratch and assume access to the full map with ground truth obstacle locations, we learn a planner from the data in a differentiable manner that allows us to leverage statistical regularities from past data. We propose Spatial Planning Transformers (SPT), which given an obstacle map learns to generate actions by planning over long-range spatial dependencies, unlike prior data-driven planners that propagate information locally via convolutional structure in an iterative manner. In the setting where the ground truth map is not known to the agent, we leverage pre-trained SPTs in an end-to-end framework that has the structure of mapper and planner built into it which allows seamless generalization to out-of-distribution maps and goals. SPTs outperform prior state-of-the-art differentiable planners across all the setups for both manipulation and navigation tasks, leading to an absolute improvement of 7-19\\%.\n",
        "conference": "ICML",
        "中文标题": "使用Transformer的可微分空间规划",
        "摘要翻译": "我们考虑空间路径规划的问题。与从零开始优化新计划并假设可以访问带有真实障碍物位置的完整地图的经典解决方案不同，我们以可微分的方式从数据中学习规划器，这使我们能够利用过去数据中的统计规律。我们提出了空间规划Transformer（SPT），给定一个障碍物地图，它通过学习在长距离空间依赖上进行规划来生成动作，这与之前通过卷积结构以迭代方式局部传播信息的数据驱动规划器不同。在智能体不知道真实地图的情况下，我们利用预训练的SPT在一个端到端的框架中，该框架内置了映射器和规划器的结构，允许无缝泛化到分布外的地图和目标。SPT在所有设置中都优于之前最先进的可微分规划器，无论是操作还是导航任务，都带来了7-19%的绝对改进。",
        "领域": "路径规划、机器人导航、深度学习与机器人结合",
        "问题": "如何在不知道完整地图的情况下，利用过去的数据进行有效的空间路径规划。",
        "动机": "传统的空间路径规划方法需要完整的障碍物地图信息，且每次规划都需从零开始，效率低下。本研究旨在通过利用历史数据的统计规律，开发一种可学习的规划器，以提高规划效率和适应性。",
        "方法": "提出了空间规划Transformer（SPT），通过长距离空间依赖学习生成动作，采用端到端框架结合预训练的SPT，实现对新地图和目标的泛化。",
        "关键词": [
            "空间规划",
            "Transformer",
            "可微分学习",
            "机器人导航",
            "端到端学习"
        ],
        "涉及的技术概念": {
            "可微分学习": "允许模型通过梯度下降直接从数据中学习规划策略，提高了学习效率和适应性。",
            "Transformer": "用于捕捉长距离空间依赖，使得规划器能够全局考虑障碍物和目标位置，优化路径规划。",
            "端到端框架": "整合了映射器和规划器，使得模型能够处理未知地图和目标，提高了泛化能力。"
        },
        "success": true
    },
    {
        "order": 285,
        "title": "Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message",
        "html": "https://ICML.cc//virtual/2021/poster/8511",
        "abstract": "The shuffle model of differential privacy has attracted attention in the literature due to it being a middle ground between the well-studied central and local models. In this work, we study the problem of summing (aggregating) real numbers or integers, a basic primitive in numerous machine learning tasks, in the shuffle model. We give a protocol achieving error arbitrarily close to that of the (Discrete) Laplace mechanism in central differential privacy, while each user only sends 1 + o(1) short messages in expectation.",
        "conference": "ICML",
        "中文标题": "差分隐私在混洗模型中的聚合：几乎中心化的准确性与近乎单条消息",
        "摘要翻译": "差分隐私的混洗模型因其作为中央模型和本地模型之间的中间地带而在文献中引起了关注。在这项工作中，我们研究了在混洗模型中求和（聚合）实数或整数的问题，这是众多机器学习任务中的一个基本原语。我们提出了一种协议，其误差可以任意接近中央差分隐私中的（离散）拉普拉斯机制，而每个用户平均只发送1 + o(1)条短消息。",
        "领域": "差分隐私、机器学习安全、数据聚合",
        "问题": "在混洗模型中实现差分隐私下的高效数据聚合",
        "动机": "研究旨在在保护用户隐私的同时，实现与中央差分隐私模型相近的数据聚合准确性，同时减少通信开销。",
        "方法": "提出了一种协议，通过优化消息传递机制，实现在混洗模型中的高效数据聚合，同时保证差分隐私。",
        "关键词": [
            "差分隐私",
            "混洗模型",
            "数据聚合",
            "机器学习",
            "通信效率"
        ],
        "涉及的技术概念": {
            "混洗模型": "一种差分隐私的实现框架，通过在用户数据发布前进行混洗来增强隐私保护。",
            "拉普拉斯机制": "在差分隐私中用于添加噪声以保护隐私的一种技术，本研究中用于比较协议的性能。",
            "通信效率": "研究重点之一，旨在减少用户间通信的消息数量和大小，以提高协议的实用性。"
        },
        "success": true
    },
    {
        "order": 286,
        "title": "Differentially Private Bayesian Inference for  Generalized Linear Models",
        "html": "https://ICML.cc//virtual/2021/poster/9971",
        "abstract": "Generalized linear models (GLMs) such as logistic regression are among the most widely used arms in data analyst’s repertoire and often used on sensitive datasets. A large body of prior works that investigate GLMs under differential privacy (DP) constraints provide only private point estimates of the regression coefficients, and are not able to quantify parameter uncertainty.\n\nIn this work, with logistic and Poisson regression as running examples, we introduce a generic noise-aware DP Bayesian inference method for a GLM at hand, given a noisy sum of summary statistics. Quantifying uncertainty allows us to determine which of the regression coefficients are statistically significantly different from zero. We provide a previously unknown tight privacy analysis and experimentally demonstrate that the posteriors obtained from our model, while adhering to strong privacy guarantees, are close to the non-private posteriors.",
        "conference": "ICML",
        "中文标题": "广义线性模型的差分隐私贝叶斯推断",
        "摘要翻译": "广义线性模型（GLMs），如逻辑回归，是数据分析师工具库中使用最广泛的工具之一，常被用于敏感数据集。大量关于在差分隐私（DP）约束下研究GLMs的先前工作仅提供了回归系数的私有点估计，无法量化参数的不确定性。在这项工作中，以逻辑回归和泊松回归为例，我们引入了一种通用的噪声感知DP贝叶斯推断方法，用于处理给定摘要统计量噪声和的GLM。量化不确定性使我们能够确定哪些回归系数在统计上显著不同于零。我们提供了一个先前未知的紧密隐私分析，并通过实验证明，从我们的模型获得的后验分布，在遵守强隐私保证的同时，接近于非私有的后验分布。",
        "领域": "隐私保护机器学习, 贝叶斯统计, 广义线性模型",
        "问题": "在差分隐私约束下，广义线性模型无法量化参数不确定性的问题",
        "动机": "解决在敏感数据上使用广义线性模型时，如何在保证差分隐私的同时量化参数不确定性的需求",
        "方法": "引入一种通用的噪声感知差分隐私贝叶斯推断方法，用于处理给定摘要统计量噪声和的广义线性模型",
        "关键词": [
            "差分隐私",
            "贝叶斯推断",
            "广义线性模型",
            "参数不确定性",
            "隐私保护"
        ],
        "涉及的技术概念": {
            "差分隐私": "一种隐私保护技术，确保数据处理的输出不会泄露个体信息",
            "贝叶斯推断": "利用贝叶斯定理更新参数的概率分布，量化参数不确定性",
            "广义线性模型": "扩展线性回归模型，允许响应变量通过链接函数与线性预测器相关"
        },
        "success": true
    },
    {
        "order": 287,
        "title": "Differentially-Private Clustering of Easy Instances",
        "html": "https://ICML.cc//virtual/2021/poster/8567",
        "abstract": "Clustering is a fundamental problem in data analysis. In differentially private clustering, the goal is to identify k cluster centers without disclosing information on individual data points. Despite significant research progress, the problem had so far resisted practical solutions.  In this work we aim at providing simple implementable differentrially private clustering algorithms when the the data is 'easy,' e.g., when there exists a significant separation between the clusters.\n\nFor the easy instances we consider, we have a simple implementation based on utilizing non-private clustering algorithms, and combining them privately. We are able to get improved sample complexity bounds in some cases of Gaussian mixtures and k-means. We complement our theoretical algorithms with experiments of simulated data. ",
        "conference": "ICML",
        "中文标题": "易实例的差分隐私聚类",
        "摘要翻译": "聚类是数据分析中的一个基本问题。在差分隐私聚类中，目标是在不泄露个别数据点信息的情况下识别k个聚类中心。尽管研究取得了显著进展，但该问题至今仍缺乏实用的解决方案。在这项工作中，我们的目标是在数据'容易'时，例如当聚类之间存在显著分离时，提供简单可实现的差分隐私聚类算法。对于我们考虑的易实例，我们有一个简单的实现，基于利用非隐私聚类算法，并将它们以隐私保护的方式结合。在某些高斯混合和k-means的情况下，我们能够获得改进的样本复杂度界限。我们用模拟数据的实验补充了我们的理论算法。",
        "领域": "差分隐私、聚类算法、数据保护",
        "问题": "在差分隐私条件下，如何有效且实用地实现数据聚类，特别是在数据易于分离的情况下。",
        "动机": "解决在差分隐私保护下聚类算法实用化的问题，特别是在数据易于处理的情况下，提供简单且有效的解决方案。",
        "方法": "利用非隐私聚类算法，通过差分隐私保护的方式结合这些算法，针对易于分离的数据实例实现聚类。",
        "关键词": [
            "差分隐私",
            "聚类算法",
            "数据分离",
            "高斯混合",
            "k-means"
        ],
        "涉及的技术概念": {
            "差分隐私": "在聚类过程中保护个体数据点不被泄露的隐私保护技术。",
            "聚类算法": "用于将数据点分组到不同簇中的算法，本文中特别关注在隐私保护条件下的实现。",
            "数据分离": "指数据中不同聚类之间存在明显的分离，使得聚类任务相对容易进行。"
        },
        "success": true
    },
    {
        "order": 288,
        "title": "Differentially Private Correlation Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/10369",
        "abstract": "Correlation clustering is a  widely used technique in unsupervised machine learning.\nMotivated by applications where individual privacy is a concern, we initiate the study of differentially private correlation clustering.\nWe propose an algorithm that achieves subquadratic additive error compared to the optimal cost. \nIn contrast, straightforward adaptations of  existing non-private algorithms all lead to a trivial quadratic error.\nFinally, we give a lower bound showing that any pure differentially private algorithm for correlation clustering requires additive error Ω(n).",
        "conference": "ICML",
        "中文标题": "差分隐私相关聚类",
        "摘要翻译": "相关聚类是无监督机器学习中广泛使用的技术。出于对个体隐私保护的考虑，我们首次研究了差分隐私相关聚类。我们提出了一种算法，与最优成本相比，实现了次二次的加性误差。相比之下，直接适应现有的非隐私算法都会导致平凡的二次误差。最后，我们给出了一个下界，表明任何纯粹的差分隐私相关聚类算法都需要Ω(n)的加性误差。",
        "领域": "隐私保护机器学习、无监督学习、聚类分析",
        "问题": "在保护个体隐私的前提下，实现高效的相关聚类",
        "动机": "针对需要保护个体隐私的应用场景，研究如何在相关聚类中应用差分隐私技术",
        "方法": "提出了一种新的差分隐私相关聚类算法，该算法在加性误差上实现了次二次的优化",
        "关键词": [
            "差分隐私",
            "相关聚类",
            "无监督学习",
            "隐私保护",
            "机器学习"
        ],
        "涉及的技术概念": {
            "差分隐私": "在聚类分析中保护个体数据隐私的技术，确保算法的输出不会泄露个体信息",
            "相关聚类": "一种无监督学习方法，旨在基于数据点之间的相似性进行聚类，无需预先指定聚类数量",
            "加性误差": "算法性能的一种度量，表示算法输出与最优解之间的差异"
        },
        "success": true
    },
    {
        "order": 289,
        "title": "Differentially Private Densest Subgraph Detection",
        "html": "https://ICML.cc//virtual/2021/poster/8641",
        "abstract": "Densest subgraph detection is a fundamental graph mining problem,  with a large number of applications. There has been a lot of work on efficient algorithms for finding the densest subgraph in massive networks. However, in many domains, the network is private, and returning a densest subgraph can reveal information about the network. Differential privacy is a powerful framework to handle such settings.\nWe study the densest subgraph problem in the edge privacy model, in which the edges of the graph are private. We present the first sequential and parallel differentially private algorithms for this problem. We show that our algorithms have an additive approximation guarantee.\nWe evaluate our algorithms on a large number of real-world networks, and observe a good privacy-accuracy tradeoff when the network has high density. \n",
        "conference": "ICML",
        "中文标题": "差分隐私下的最密集子图检测",
        "摘要翻译": "最密集子图检测是一个基础的图挖掘问题，拥有大量的应用场景。已有大量工作致力于在大规模网络中寻找最密集子图的高效算法。然而，在许多领域，网络是私有的，返回一个最密集子图可能会泄露网络的信息。差分隐私是一个强大的框架，用于处理此类情况。我们研究了边隐私模型中的最密集子图问题，其中图的边是私有的。我们提出了针对此问题的首个序列化和并行化的差分隐私算法。我们展示了我们的算法具有加性近似保证。我们在大量真实世界的网络上评估了我们的算法，并观察到当网络具有高密度时，隐私与准确性之间存在良好的权衡。",
        "领域": "图挖掘、隐私保护、并行计算",
        "问题": "在保护网络隐私的同时，高效地检测出最密集的子图",
        "动机": "解决在私有网络中检测最密集子图时可能泄露敏感信息的问题",
        "方法": "提出序列化和并行化的差分隐私算法，确保在边隐私模型下的最密集子图检测",
        "关键词": [
            "差分隐私",
            "最密集子图",
            "图挖掘",
            "隐私保护",
            "并行算法"
        ],
        "涉及的技术概念": {
            "差分隐私": "用于保护图中边的隐私，确保在分析过程中不泄露敏感信息",
            "最密集子图": "图中的一个子集，其边的密度高于图中其他任何子集",
            "并行算法": "用于提高在大规模网络上检测最密集子图的效率"
        },
        "success": true
    },
    {
        "order": 290,
        "title": "Differentially Private Quantiles",
        "html": "https://ICML.cc//virtual/2021/poster/8845",
        "abstract": "Quantiles are often used for summarizing and understanding data. If that data is sensitive, it may be necessary to compute quantiles in a way that is differentially private, providing theoretical guarantees that the result does not reveal private information. However, when multiple quantiles are needed, existing differentially private algorithms fare poorly: they either compute quantiles individually, splitting the privacy budget, or summarize the entire distribution, wasting effort. In either case the result is reduced accuracy. In this work we propose an instance of the exponential mechanism that simultaneously estimates exactly $m$ quantiles from $n$ data points while guaranteeing differential privacy. The utility function is carefully structured to allow for an efficient implementation that returns estimates of all $m$ quantiles in time $O(mn\\log(n) + m^2n)$. Experiments show that our method significantly outperforms the current state of the art on both real and synthetic data while remaining efficient enough to be practical.",
        "conference": "ICML",
        "中文标题": "差分隐私分位数",
        "摘要翻译": "分位数常用于总结和理解数据。如果这些数据是敏感的，可能需要以差分隐私的方式计算分位数，从而提供理论保证，确保结果不会泄露私人信息。然而，当需要多个分位数时，现有的差分隐私算法表现不佳：它们要么单独计算每个分位数，分散隐私预算，要么总结整个分布，浪费努力。无论哪种情况，结果都是准确性的降低。在这项工作中，我们提出了指数机制的一个实例，该实例在保证差分隐私的同时，从n个数据点中精确估计m个分位数。效用函数经过精心设计，允许高效实现，能够在O(mnlog(n) + m^2n)的时间内返回所有m个分位数的估计。实验表明，我们的方法在真实和合成数据上都显著优于当前的最新技术，同时保持足够的效率以实用。",
        "领域": "数据隐私保护、统计数据分析、差分隐私算法",
        "问题": "在需要计算多个分位数时，现有的差分隐私算法要么分散隐私预算导致准确性降低，要么浪费计算资源总结整个分布。",
        "动机": "提高在差分隐私条件下计算多个分位数的准确性和效率，以更好地服务于敏感数据的统计分析需求。",
        "方法": "提出了一种基于指数机制的算法，精心设计效用函数以高效同时估计多个分位数，保证差分隐私。",
        "关键词": [
            "差分隐私",
            "分位数估计",
            "指数机制",
            "数据隐私保护",
            "高效算法"
        ],
        "涉及的技术概念": {
            "差分隐私": "提供理论保证，确保计算结果不会泄露私人信息的技术。",
            "指数机制": "一种差分隐私机制，用于在保护隐私的同时从一组可能的输出中选择最优解。",
            "效用函数": "在指数机制中用于衡量输出质量的函数，本文中精心设计以支持高效的多分位数估计。"
        },
        "success": true
    },
    {
        "order": 291,
        "title": "Differentially Private Query Release Through Adaptive Projection",
        "html": "https://ICML.cc//virtual/2021/poster/9359",
        "abstract": "We propose, implement, and evaluate a new algo-rithm for releasing answers to very large numbersof statistical queries likek-way marginals, sub-ject to differential privacy. Our algorithm makesadaptive use of a continuous relaxation of thePro-jection Mechanism, which answers queries on theprivate dataset using simple perturbation, and thenattempts to find the synthetic dataset that mostclosely matches the noisy answers. We use a con-tinuous relaxation of the synthetic dataset domainwhich makes the projection loss differentiable,and allows us to use efficient ML optimizationtechniques and tooling. Rather than answering allqueries up front, we make judicious use of ourprivacy budget by iteratively finding queries forwhich our (relaxed) synthetic data has high error,and then repeating the projection.  Randomizedrounding allows us to obtain synthetic data in theoriginal schema. We perform experimental evalu-ations across a range of parameters and datasets,and find that our method outperforms existingalgorithms on large query classes.",
        "conference": "ICML",
        "中文标题": "通过自适应投影实现差分隐私查询发布",
        "摘要翻译": "我们提出、实现并评估了一种新算法，用于在差分隐私约束下发布对大量统计查询（如k-way边际）的答案。我们的算法自适应地使用了投影机制的连续松弛，该机制通过简单扰动在私有数据集上回答查询，然后尝试找到最接近噪声答案的合成数据集。我们使用了合成数据集域的连续松弛，这使得投影损失可微分，并允许我们使用高效的机器学习优化技术和工具。我们不是一次性回答所有查询，而是通过迭代地找到我们的（松弛）合成数据误差高的查询，然后重复投影，明智地使用我们的隐私预算。随机舍入使我们能够在原始模式中获得合成数据。我们在一系列参数和数据集上进行了实验评估，发现我们的方法在大查询类上优于现有算法。",
        "领域": "差分隐私、数据合成、统计查询处理",
        "问题": "如何在差分隐私约束下高效准确地发布大量统计查询的答案",
        "动机": "解决现有算法在处理大规模统计查询时效率和准确性的不足，同时满足差分隐私的要求",
        "方法": "采用自适应投影机制的连续松弛，结合机器学习优化技术和迭代查询选择策略，有效利用隐私预算",
        "关键词": [
            "差分隐私",
            "统计查询",
            "数据合成",
            "自适应投影",
            "机器学习优化"
        ],
        "涉及的技术概念": {
            "差分隐私": "保护数据集中个体隐私的严格数学框架，确保查询发布不会泄露个体信息",
            "投影机制": "一种在差分隐私下回答查询的方法，通过扰动真实答案并寻找最佳匹配的合成数据集",
            "连续松弛": "将离散的合成数据集域转换为连续空间，使得优化问题可微分，便于应用高效的机器学习优化技术"
        },
        "success": true
    },
    {
        "order": 292,
        "title": "Differentially Private Sliced Wasserstein Distance",
        "html": "https://ICML.cc//virtual/2021/poster/10729",
        "abstract": "Developing machine learning methods that are privacy preserving is today a central topic of research, with huge practical impacts. Among the numerous ways to address privacy-preserving learning, we here take the perspective of computing the divergences between distributions under the Differential Privacy (DP) framework --- being able to compute divergences between distributions is pivotal for many machine learning problems, such as  learning generative models or domain adaptation problems.  Instead of resorting to the popular gradient-based sanitization method for DP, we tackle the problem at its roots by focusing on the Sliced Wasserstein Distance and seamlessly making it differentially private. Our main contribution is as follows: we analyze the property of adding a Gaussian perturbation to the intrinsic randomized mechanism of the Sliced Wasserstein Distance, and we establish the sensitivity of the resulting differentially private mechanism. One of our important findings is that this DP mechanism transforms the Sliced Wasserstein distance into another distance, that we call the Smoothed Sliced Wasserstein Distance. This new differentially private distribution distance can be plugged into generative models and domain adaptation algorithms in a transparent way, and we empirically show that it yields highly competitive performance compared with gradient-based DP approaches from the literature,\nwith almost no loss in accuracy for the domain adaptation problems that we consider.",
        "conference": "ICML",
        "success": true,
        "中文标题": "差分隐私切片瓦瑟斯坦距离",
        "摘要翻译": "开发隐私保护的机器学习方法是当今研究的核心课题，具有巨大的实际影响。在众多解决隐私保护学习的方法中，我们这里采取的是在差分隐私（DP）框架下计算分布之间差异的视角——能够计算分布之间的差异对于许多机器学习问题至关重要，例如学习生成模型或领域适应问题。我们没有采用流行的基于梯度的DP净化方法，而是通过专注于切片瓦瑟斯坦距离并无缝地使其具有差分隐私性，从根本上解决了这个问题。我们的主要贡献如下：我们分析了向切片瓦瑟斯坦距离的内在随机机制添加高斯扰动的性质，并建立了由此产生的差分隐私机制的敏感性。我们的一个重要发现是，这种DP机制将切片瓦瑟斯坦距离转变为另一种距离，我们称之为平滑切片瓦瑟斯坦距离。这种新的差分隐私分布距离可以透明地插入到生成模型和领域适应算法中，我们实证表明，与文献中基于梯度的DP方法相比，它提供了极具竞争力的性能，对于我们考虑的领域适应问题几乎没有准确性损失。",
        "领域": "隐私保护机器学习, 生成模型, 领域适应",
        "问题": "如何在差分隐私框架下计算分布之间的差异，以支持隐私保护的机器学习应用",
        "动机": "开发能够在保护数据隐私的同时，有效计算分布差异的方法，以支持如生成模型和领域适应等机器学习任务",
        "方法": "通过分析向切片瓦瑟斯坦距离的内在随机机制添加高斯扰动的性质，建立差分隐私机制，并将其应用于生成模型和领域适应算法",
        "关键词": [
            "差分隐私",
            "切片瓦瑟斯坦距离",
            "生成模型",
            "领域适应",
            "高斯扰动"
        ],
        "涉及的技术概念": {
            "差分隐私": "一种隐私保护框架，确保数据处理的隐私性，通过向数据或计算结果添加噪声来实现",
            "切片瓦瑟斯坦距离": "一种计算分布之间距离的方法，通过将高维分布投影到一维空间来简化计算",
            "高斯扰动": "在差分隐私机制中，通过添加高斯噪声来保护数据隐私的技术"
        }
    },
    {
        "order": 293,
        "title": "Diffusion Earth Mover's Distance and Distribution Embeddings",
        "html": "https://ICML.cc//virtual/2021/poster/9747",
        "abstract": "We propose a new fast method of measuring distances between large numbers of related high dimensional datasets called the Diffusion Earth Mover's Distance (EMD). We model the datasets as distributions supported on common data graph that is derived from the affinity matrix computed on the combined data. In such cases where the graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion EMD is topologically equivalent to the standard EMD with a geodesic ground distance. Diffusion EMD can be computed in Õ(n) time and is more accurate than similarly fast algorithms such as tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as deep neural networks. Finally, we demonstrate an application of Diffusion EMD to single cell data collected from 210 COVID-19 patient samples at Yale New Haven Hospital. Here, Diffusion EMD can derive distances between patients on the manifold of cells at least two orders of magnitude faster than equally accurate methods. This distance matrix between patients can be embedded into a higher level patient manifold which uncovers structure and heterogeneity in patients. More generally, Diffusion EMD is applicable to all datasets that are massively collected in parallel in many medical and biological systems.",
        "conference": "ICML",
        "中文标题": "扩散地球移动距离与分布嵌入",
        "摘要翻译": "我们提出了一种新的快速测量大量相关高维数据集之间距离的方法，称为扩散地球移动距离（EMD）。我们将数据集建模为支持在共同数据图上的分布，该图是从组合数据的亲和矩阵计算得出的。在图是底层黎曼闭流形的离散化的情况下，我们证明了扩散EMD在拓扑上等同于具有测地线地面距离的标准EMD。扩散EMD可以在Õ(n)时间内计算，并且比类似快速的算法（如基于树的EMD）更准确。我们还展示了扩散EMD是完全可微的，使其适用于未来在梯度下降框架（如深度神经网络）中的使用。最后，我们展示了扩散EMD在从耶鲁纽黑文医院收集的210个COVID-19患者样本的单细胞数据中的应用。在这里，扩散EMD可以在细胞流形上以至少两个数量级的速度比同样准确的方法推导出患者之间的距离。这个患者之间的距离矩阵可以嵌入到一个更高层次的患者流形中，揭示患者的结构和异质性。更一般地说，扩散EMD适用于在多种医疗和生物系统中大规模并行收集的所有数据集。",
        "领域": "单细胞数据分析, 高维数据距离测量, 医学图像处理",
        "问题": "如何快速准确地测量大量相关高维数据集之间的距离",
        "动机": "为了更有效地分析和比较大规模并行收集的高维数据集，特别是在医疗和生物系统中",
        "方法": "提出扩散地球移动距离（EMD）方法，通过将数据集建模为共同数据图上的分布，并利用亲和矩阵计算距离",
        "关键词": [
            "扩散地球移动距离",
            "高维数据",
            "单细胞分析",
            "数据嵌入",
            "梯度下降"
        ],
        "涉及的技术概念": {
            "扩散地球移动距离（EMD）": "一种新的快速测量高维数据集之间距离的方法，比传统方法更快速且准确",
            "共同数据图": "从组合数据的亲和矩阵计算得出的图，用于支持数据集的分布建模",
            "梯度下降框架": "扩散EMD完全可微，适用于如深度神经网络等梯度下降框架中的使用"
        },
        "success": true
    },
    {
        "order": 294,
        "title": "Diffusion Source Identification on Networks with Statistical Confidence",
        "html": "https://ICML.cc//virtual/2021/poster/9357",
        "abstract": "Diffusion source identification on networks is a problem of fundamental importance in a broad class of applications, including controlling the spreading of rumors on social media, identifying a computer virus over cyber networks, or identifying the disease center during epidemiology. Though this problem has received significant recent attention, most known approaches are well-studied in only very restrictive settings and lack theoretical guarantees for more realistic networks. We introduce a statistical framework for the study of this problem and develop a confidence set inference approach inspired by hypothesis testing. Our method efficiently produces a small subset of nodes, which provably covers the source node with any pre-specified confidence level without restrictive assumptions on network structures. To our knowledge, this is the first diffusion source identification method with a practically useful theoretical guarantee on general networks. We demonstrate our approach via extensive synthetic experiments on well-known random network models, a large data set of real-world networks as well as a mobility network between cities concerning the COVID-19 spreading in January 2020.",
        "conference": "ICML",
        "中文标题": "网络扩散源识别的统计置信方法",
        "摘要翻译": "网络扩散源识别在一系列应用中具有基础重要性，包括控制社交媒体上的谣言传播、识别网络中的计算机病毒或在流行病学中识别疾病中心。尽管这一问题最近受到了显著关注，但大多数已知方法仅在非常受限的环境中得到充分研究，并且缺乏对更现实网络的理论保证。我们引入了一个统计框架来研究这一问题，并开发了一种受假设检验启发的置信集推断方法。我们的方法高效地产生了一个小的节点子集，该子集在不对网络结构施加限制性假设的情况下，以任何预先指定的置信水平覆盖源节点。据我们所知，这是在一般网络上具有实用理论保证的第一个扩散源识别方法。我们通过在著名的随机网络模型、大量现实世界网络数据集以及2020年1月关于COVID-19传播的城市间移动网络上的广泛合成实验来展示我们的方法。",
        "领域": "网络分析、流行病学建模、社交媒体监控",
        "问题": "如何在一般网络上高效且可靠地识别扩散源",
        "动机": "现有方法在现实网络中的应用缺乏理论保证，限制了其在实际中的可靠性和有效性",
        "方法": "引入统计框架，开发基于假设检验的置信集推断方法，以高效识别扩散源",
        "关键词": [
            "扩散源识别",
            "统计置信",
            "网络分析",
            "假设检验",
            "COVID-19传播"
        ],
        "涉及的技术概念": {
            "统计框架": "为扩散源识别问题提供理论基础和方法指导",
            "置信集推断": "基于假设检验的方法，用于生成覆盖源节点的小子集",
            "网络结构": "研究不依赖于特定网络结构的通用方法，提高方法的适用性和灵活性"
        },
        "success": true
    },
    {
        "order": 295,
        "title": "Dimensionality Reduction for the Sum-of-Distances Metric ",
        "html": "https://ICML.cc//virtual/2021/poster/9299",
        "abstract": "We give a dimensionality reduction procedure to approximate the sum of distances of a given set of $n$ points in $R^d$ to any ``shape'' that lies in a $k$-dimensional subspace. Here, by ``shape'' we mean any set of points in $R^d$. Our algorithm takes an input in the form of an $n \\times d$ matrix $A$, where each row of $A$ denotes a data point, and outputs a subspace $P$ of dimension $O(k^{3}/\\epsilon^6)$ such that the projections of each of the $n$ points onto the subspace $P$ and the distances of each of the points to the subspace $P$ are sufficient to obtain an $\\epsilon$-approximation to the sum of distances to any arbitrary shape that lies in a $k$-dimensional subspace of $R^d$. These include important problems such as $k$-median, $k$-subspace approximation, and $(j,l)$ subspace clustering with $j \\cdot l \\leq k$. Dimensionality reduction reduces the data storage requirement to $(n+d)k^{3}/\\epsilon^6$ from nnz$(A)$.   \nHere nnz$(A)$ could potentially be as large as $nd$. Our algorithm runs in time nnz$(A)/\\epsilon^2 + (n+d)$poly$(k/\\epsilon)$, up to logarithmic factors. For dense matrices, where nnz$(A) \\approx nd$, we give a faster algorithm, that runs in time $nd + (n+d)$poly$(k/\\epsilon)$ up to logarithmic factors. Our dimensionality reduction algorithm can also be used to obtain poly$(k/\\epsilon)$ size coresets for $k$-median and $(k,1)$-subspace approximation problems in polynomial time.",
        "conference": "ICML",
        "success": true,
        "中文标题": "针对距离和度量的降维处理",
        "摘要翻译": "我们提出了一种降维方法，用于近似计算给定n个点在R^d空间中到任何位于k维子空间中的‘形状’的距离之和。这里的‘形状’指的是R^d空间中的任何点集。我们的算法以n×d矩阵A的形式输入，其中A的每一行代表一个数据点，并输出一个维度为O(k^3/ε^6)的子空间P，使得将每个点投影到子空间P上以及每个点到子空间P的距离足以获得对任何位于R^d的k维子空间中的任意形状的距离之和的ε近似。这包括k-中位数、k-子空间近似和(j,l)子空间聚类（j·l≤k）等重要问题。降维将数据存储需求从nnz(A)减少到(n+d)k^3/ε^6。这里nnz(A)可能高达nd。我们的算法在nnz(A)/ε^2 + (n+d)poly(k/ε)时间内运行，直到对数因子。对于密集矩阵，其中nnz(A)≈nd，我们提供了一个更快的算法，在nd + (n+d)poly(k/ε)时间内运行，直到对数因子。我们的降维算法也可以用于在多项式时间内获得k-中位数和(k,1)-子空间近似问题的poly(k/ε)大小核心集。",
        "领域": "降维技术, 聚类分析, 近似算法",
        "问题": "如何高效地近似计算高维空间中点到任意形状的距离之和",
        "动机": "减少高维数据处理中的存储需求和计算复杂度",
        "方法": "提出了一种降维算法，通过投影到低维子空间来近似距离和，同时保证计算效率",
        "关键词": [
            "降维",
            "距离和度量",
            "近似算法",
            "子空间聚类",
            "核心集"
        ],
        "涉及的技术概念": {
            "降维": "通过减少数据的维度来降低存储和计算的复杂度",
            "子空间投影": "将数据点投影到低维子空间以近似原始空间中的距离计算",
            "核心集": "用于在降维后仍能保持原始数据特性的小子集，以支持高效算法设计"
        }
    },
    {
        "order": 296,
        "title": "Directed Graph Embeddings in Pseudo-Riemannian Manifolds",
        "html": "https://ICML.cc//virtual/2021/poster/10411",
        "abstract": "The inductive biases of graph representation learning algorithms are often encoded in the background geometry of their embedding space. In this paper, we show that general directed graphs can be effectively represented by an embedding model that combines three components: a pseudo-Riemannian metric structure, a non-trivial global topology, and a unique likelihood function that explicitly incorporates a preferred direction in embedding space. We demonstrate the representational capabilities of this method by applying it to the task of link prediction on a series of synthetic and real directed graphs from natural language applications and biology. In particular, we show that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce equal or better graph representations than curved Riemannian manifolds of higher dimensions.",
        "conference": "ICML",
        "中文标题": "伪黎曼流形中的有向图嵌入",
        "摘要翻译": "图表示学习算法的归纳偏差通常编码在其嵌入空间的背景几何中。在本文中，我们展示了一般有向图可以通过一个结合了三个组件的嵌入模型有效表示：一个伪黎曼度量结构、一个非平凡的全局拓扑结构，以及一个在嵌入空间中明确包含一个首选方向的独特似然函数。我们通过将这种方法应用于一系列来自自然语言应用和生物学的合成和真实有向图的链接预测任务，展示了该方法的表示能力。特别是，我们展示了低维圆柱形闵可夫斯基和反德西特时空可以产生与更高维度的弯曲黎曼流形相等或更好的图表示。",
        "领域": "图表示学习、链接预测、几何深度学习",
        "问题": "如何有效地在伪黎曼流形中嵌入有向图以进行链接预测",
        "动机": "探索和验证伪黎曼流形结合特定几何和拓扑结构对有向图表示的有效性",
        "方法": "结合伪黎曼度量结构、非平凡全局拓扑和独特似然函数的嵌入模型",
        "关键词": [
            "有向图嵌入",
            "伪黎曼流形",
            "链接预测",
            "几何深度学习",
            "图表示学习"
        ],
        "涉及的技术概念": {
            "伪黎曼度量结构": "用于定义嵌入空间中的距离和角度，支持有向图的几何表示",
            "非平凡全局拓扑": "引入复杂的空间结构，增强模型对图结构的表示能力",
            "独特似然函数": "明确考虑嵌入空间中的方向性，优化有向图的链接预测性能"
        },
        "success": true
    },
    {
        "order": 297,
        "title": "Directional Bias Amplification",
        "html": "https://ICML.cc//virtual/2021/poster/10553",
        "abstract": "Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $BiasAmp_{\\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https://github.com/princetonvisualai/directional-bias-amp.",
        "conference": "ICML",
        "中文标题": "方向性偏差放大",
        "摘要翻译": "减轻机器学习系统中的偏差需要深化我们对偏差传播路径的理解：从社会结构到大规模数据，再到训练模型，最后到对社会的影响。在这项工作中，我们关注问题的一个方面，即偏差放大：模型倾向于放大它们训练数据中存在的偏差。Zhao等人（2017）的开创性工作引入了一个用于测量偏差放大的指标；然而，正如我们所展示的，这个指标存在一些缺点，包括混淆了不同类型的偏差放大，并且未能考虑到受保护属性的不同基础率。我们引入并分析了一个新的、解耦的测量偏差放大的指标，$BiasAmp_{\rightarrow}$（方向性偏差放大）。我们全面分析并讨论了这一指标的技术假设和规范含义。我们提供了关于其测量的建议，警告不要预测敏感属性，鼓励使用置信区间，因为模型在不同运行中的公平性存在波动，并讨论了这个指标捕捉的局限性。在整篇论文中，我们努力提供一个对偏差放大技术测量的质疑性视角，以我们希望它包含的规范理念为指导。代码位于https://github.com/princetonvisualai/directional-bias-amp。",
        "领域": "机器学习公平性",
        "问题": "如何准确测量和减轻机器学习模型中的偏差放大问题",
        "动机": "现有偏差放大测量指标存在混淆不同类型偏差放大和忽略受保护属性基础率差异的问题，需要更精确的测量方法",
        "方法": "引入并分析了一个新的、解耦的测量偏差放大的指标$BiasAmp_{\rightarrow}$，讨论了其技术假设和规范含义，并提供了测量建议",
        "关键词": [
            "偏差放大",
            "机器学习公平性",
            "方向性偏差放大"
        ],
        "涉及的技术概念": {
            "偏差放大": "模型倾向于放大训练数据中存在的偏差",
            "方向性偏差放大": "一种新的、解耦的测量偏差放大的指标，旨在更准确地反映偏差放大的方向和程度",
            "受保护属性基础率": "在测量偏差放大时需要考虑的受保护属性在不同群体中的分布比例"
        },
        "success": true
    },
    {
        "order": 298,
        "title": "Directional Graph Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10053",
        "abstract": "The lack of anisotropic kernels in graph neural networks (GNNs) strongly limits their expressiveness, contributing to well-known issues such as over-smoothing. To overcome this limitation, we propose the first globally consistent anisotropic kernels for GNNs, allowing for graph convolutions that are defined according to topologicaly-derived directional flows.\nFirst, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. \nThen, we propose the use of the Laplacian eigenvectors as such vector field.\nWe show that the method generalizes CNNs on an $n$-dimensional grid and is provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman 1-WL test.\nWe evaluate our method on different standard benchmarks and see a relative error reduction of 8% on the CIFAR10 graph dataset and 11% to 32% on the molecular ZINC dataset, and a relative increase in precision of 1.6% on the MolPCBA dataset. \nAn important outcome of this work is that it enables graph networks to embed directions in an unsupervised way, thus allowing a better representation of the anisotropic features in different physical or biological problems. ",
        "conference": "ICML",
        "中文标题": "方向性图网络",
        "摘要翻译": "图神经网络（GNNs）中缺乏各向异性核极大地限制了它们的表达能力，导致了诸如过度平滑等众所周知的问题。为了克服这一限制，我们提出了第一个全局一致的GNNs各向异性核，允许根据拓扑导出的方向流定义图卷积。首先，通过在图中定义一个向量场，我们开发了一种应用方向导数和通过将节点特定消息投影到场中进行平滑的方法。然后，我们提出使用拉普拉斯特征向量作为这样的向量场。我们展示了该方法在n维网格上泛化了CNNs，并且在Weisfeiler-Lehman 1-WL测试方面比标准GNNs更具区分性。我们在不同的标准基准上评估了我们的方法，在CIFAR10图数据集上看到了8%的相对误差减少，在分子ZINC数据集上看到了11%到32%的相对误差减少，在MolPCBA数据集上精确度相对提高了1.6%。这项工作的重要成果是，它使图网络能够以无监督的方式嵌入方向，从而允许更好地表示不同物理或生物问题中的各向异性特征。",
        "领域": "图神经网络、分子图学习、图像识别",
        "问题": "图神经网络中缺乏各向异性核，限制了其表达能力和导致过度平滑问题",
        "动机": "克服图神经网络在各向异性特征表示上的限制，提升其在物理和生物问题中的应用能力",
        "方法": "提出全局一致的各向异性核，利用拉普拉斯特征向量作为向量场，开发方向导数和消息投影方法",
        "关键词": [
            "各向异性核",
            "图神经网络",
            "方向性图卷积",
            "拉普拉斯特征向量",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "各向异性核": "用于增强图神经网络在各方向上的特征提取能力，解决过度平滑问题",
            "拉普拉斯特征向量": "作为向量场的基础，用于定义图中的方向流和进行方向性卷积",
            "Weisfeiler-Lehman 1-WL测试": "用于评估图神经网络的区分能力，证明所提方法的优越性"
        },
        "success": true
    },
    {
        "order": 299,
        "title": "Disambiguation of Weak Supervision leading to Exponential Convergence rates",
        "html": "https://ICML.cc//virtual/2021/poster/10613",
        "abstract": "Machine learning approached through supervised learning requires expensive annotation of data. This motivates weakly supervised learning, where data are annotated with incomplete yet discriminative information. In this paper, we focus on partial labelling, an instance of weak supervision where, from a given input, we are given a set of potential targets. We review a disambiguation principle to recover full supervision from weak supervision, and propose an empirical disambiguation algorithm. We prove exponential convergence rates of our algorithm under classical learnability assumptions, and we illustrate the usefulness of our method on practical examples.",
        "conference": "ICML",
        "中文标题": "弱监督消歧实现指数级收敛速率",
        "摘要翻译": "通过监督学习进行的机器学习需要昂贵的数据标注。这促使了弱监督学习的发展，其中数据被标注为不完整但具有区分性的信息。在本文中，我们专注于部分标注，这是弱监督的一个实例，其中给定一个输入，我们提供一组潜在的目标。我们回顾了一种从弱监督中恢复完全监督的消歧原则，并提出了一种经验消歧算法。我们在经典可学习性假设下证明了我们算法的指数级收敛速率，并通过实际例子说明了我们方法的实用性。",
        "领域": "弱监督学习、机器学习理论、算法收敛性分析",
        "问题": "如何在弱监督学习环境下，通过消歧原则恢复完全监督，并实现算法的指数级收敛。",
        "动机": "减少机器学习中对昂贵数据标注的依赖，通过弱监督学习提高学习效率和实用性。",
        "方法": "提出一种经验消歧算法，从部分标注的弱监督数据中恢复完全监督，并在理论证明其指数级收敛速率。",
        "关键词": [
            "弱监督学习",
            "消歧原则",
            "指数收敛",
            "部分标注",
            "经验算法"
        ],
        "涉及的技术概念": {
            "弱监督学习": "一种机器学习范式，使用不完整或噪声较多的标注信息进行模型训练。",
            "消歧原则": "从弱监督数据中识别和恢复出准确的监督信号的方法。",
            "指数级收敛": "算法在迭代过程中误差以指数速度减少的性质，表明高效的学习能力。"
        },
        "success": true
    },
    {
        "order": 300,
        "title": "Discovering symbolic policies with deep reinforcement learning",
        "html": "https://ICML.cc//virtual/2021/poster/9985",
        "abstract": "Deep reinforcement learning (DRL) has proven successful for many difficult control problems by learning policies represented by neural networks. However, the complexity of neural network-based policies—involving thousands of composed non-linear operators—can render them problematic to understand, trust, and deploy. In contrast, simple policies comprising short symbolic expressions can facilitate human understanding, while also being transparent and exhibiting predictable behavior. To this end, we propose deep symbolic policy, a novel approach to directly search the space of symbolic policies. We use an autoregressive recurrent neural network to generate control policies represented by tractable mathematical expressions, employing a risk-seeking policy gradient to maximize performance of the generated policies. To scale to environments with multi-dimensional action spaces, we propose an 'anchoring' algorithm that distills pre-trained neural network-based policies into fully symbolic policies, one action dimension at a time. We also introduce two novel methods to improve exploration in DRL-based combinatorial optimization, building on ideas of entropy regularization and distribution initialization. Despite their dramatically reduced complexity, we demonstrate that discovered symbolic policies outperform seven state-of-the-art DRL algorithms in terms of average rank and average normalized episodic reward across eight benchmark environments.",
        "conference": "ICML",
        "中文标题": "通过深度强化学习发现符号化策略",
        "摘要翻译": "深度强化学习（DRL）通过学习由神经网络表示的策略，已被证明对许多困难的控制问题有效。然而，基于神经网络的策略的复杂性——涉及数千个组合的非线性操作符——可能使它们难以理解、信任和部署。相比之下，包含简短符号表达式的简单策略可以促进人类理解，同时保持透明并展示可预测的行为。为此，我们提出了深度符号策略，一种直接搜索符号策略空间的新方法。我们使用自回归循环神经网络生成由可处理的数学表达式表示的控制策略，采用风险寻求策略梯度以最大化生成策略的性能。为了适应具有多维动作空间的环境，我们提出了一种‘锚定’算法，将预训练的基于神经网络的策略提炼为完全符号化的策略，一次一个动作维度。我们还引入了两种基于DRL的组合优化的新方法，以改进探索，这些方法建立在熵正则化和分布初始化的思想基础上。尽管复杂性显著降低，但我们发现符号化策略在八个基准环境中的平均排名和平均归一化回合奖励方面优于七种最先进的DRL算法。",
        "领域": "强化学习、自动控制、符号回归",
        "问题": "解决基于神经网络的策略复杂、难以理解和信任的问题",
        "动机": "开发简单、透明且易于理解的符号化策略，以替代复杂的神经网络策略",
        "方法": "使用自回归循环神经网络生成符号策略，采用风险寻求策略梯度优化，提出锚定算法处理多维动作空间，引入熵正则化和分布初始化改进探索",
        "关键词": [
            "深度强化学习",
            "符号策略",
            "自回归循环神经网络",
            "风险寻求策略梯度",
            "锚定算法"
        ],
        "涉及的技术概念": {
            "自回归循环神经网络": "用于生成由数学表达式表示的符号策略，能够处理序列数据",
            "风险寻求策略梯度": "一种优化方法，旨在最大化生成策略的性能，特别关注高风险高回报的策略",
            "锚定算法": "一种将预训练的神经网络策略转化为符号化策略的技术，逐步处理多维动作空间"
        },
        "success": true
    },
    {
        "order": 301,
        "title": "Discrete-Valued Latent Preference Matrix Estimation with Graph Side Information",
        "html": "https://ICML.cc//virtual/2021/poster/10051",
        "abstract": "Incorporating graph side information into recommender systems has been widely used to better predict ratings, but relatively few works have focused on theoretical guarantees. Ahn et al. (2018) firstly characterized the optimal sample complexity in the presence of graph side information, but the results are limited due to strict, unrealistic assumptions made on the unknown latent preference matrix and the structure of user clusters. In this work, we propose a new model in which 1) the unknown latent preference matrix can have any discrete values, and 2) users can be clustered into multiple clusters, thereby relaxing the assumptions made in prior work. Under this new model, we fully characterize the optimal sample complexity and develop a computationally-efficient algorithm that matches the optimal sample complexity. Our algorithm is robust to model errors and outperforms the existing algorithms in terms of prediction performance on both synthetic and real data.",
        "conference": "ICML",
        "中文标题": "离散值潜在偏好矩阵估计与图侧信息",
        "摘要翻译": "将图侧信息融入推荐系统已被广泛用于更好地预测评分，但相对较少的工作关注于理论保证。Ahn等人（2018年）首次在存在图侧信息的情况下描述了最优样本复杂度，但由于对未知潜在偏好矩阵和用户集群结构的严格、不现实假设，结果有限。在这项工作中，我们提出了一个新模型，其中1）未知潜在偏好矩阵可以具有任何离散值，2）用户可以被聚类到多个集群中，从而放宽了先前工作中的假设。在这个新模型下，我们完全描述了最优样本复杂度，并开发了一个计算效率高的算法，该算法与最优样本复杂度相匹配。我们的算法对模型错误具有鲁棒性，并且在合成和真实数据上的预测性能优于现有算法。",
        "领域": "推荐系统、图神经网络、聚类分析",
        "问题": "如何在放宽对潜在偏好矩阵和用户集群结构的严格假设下，利用图侧信息提高推荐系统的预测性能和理论保证。",
        "动机": "现有研究在利用图侧信息提高推荐系统性能时，存在对潜在偏好矩阵和用户集群结构的严格假设，限制了理论的适用性和实际应用效果。",
        "方法": "提出一个新模型，允许潜在偏好矩阵具有任何离散值，用户可被聚类到多个集群中，并开发了一个计算效率高、与最优样本复杂度匹配的算法。",
        "关键词": [
            "推荐系统",
            "图侧信息",
            "潜在偏好矩阵",
            "样本复杂度",
            "聚类分析"
        ],
        "涉及的技术概念": {
            "离散值潜在偏好矩阵": "在推荐系统中，用于表示用户对项目偏好的矩阵，其元素为离散值，允许更灵活地建模用户偏好。",
            "图侧信息": "指在推荐系统中，除了用户-项目交互数据外，还可以利用的用户或项目之间的关系图信息，用于提高推荐性能。",
            "最优样本复杂度": "在统计学习中，达到一定预测精度所需的最小样本量，本研究在放宽假设的条件下，完全描述了这一复杂度。"
        },
        "success": true
    },
    {
        "order": 302,
        "title": "Discretization Drift in Two-Player Games",
        "html": "https://ICML.cc//virtual/2021/poster/10315",
        "abstract": "Gradient-based methods for two-player games produce rich dynamics that can solve challenging problems, yet can be difficult to stabilize and understand. Part of this complexity originates from the discrete update steps given by simultaneous or alternating gradient descent, which causes each player to drift away from the continuous gradient flow --  a phenomenon we call discretization drift. Using backward error analysis, we derive modified continuous dynamical systems that closely follow the discrete dynamics. These modified dynamics provide an insight into the notorious challenges associated with zero-sum games, including Generative Adversarial Networks. In particular, we identify distinct components of the discretization drift that can alter performance and in some cases destabilize the game. Finally, quantifying discretization drift allows us to identify regularizers that explicitly cancel harmful forms of drift or strengthen beneficial forms of drift, and thus improve performance of GAN training.",
        "conference": "ICML",
        "中文标题": "双人游戏中的离散化漂移",
        "摘要翻译": "基于梯度的双人游戏方法产生了可以解决挑战性问题的丰富动态，但这些动态可能难以稳定和理解。这种复杂性部分源于由同步或交替梯度下降给出的离散更新步骤，这导致每个玩家从连续梯度流中漂移——我们称这种现象为离散化漂移。使用后向误差分析，我们推导出紧密跟随离散动态的修改后的连续动态系统。这些修改后的动态为与零和游戏相关的著名挑战提供了见解，包括生成对抗网络。特别是，我们识别了离散化漂移的不同组成部分，这些部分可以改变性能并在某些情况下使游戏不稳定。最后，量化离散化漂移使我们能够识别那些明确取消有害漂移形式或加强有益漂移形式的正则化器，从而改善GAN训练的性能。",
        "领域": "生成对抗网络、优化算法、博弈论",
        "问题": "双人游戏中基于梯度的方法产生的动态难以稳定和理解，特别是离散更新步骤导致的离散化漂移问题。",
        "动机": "理解和解决双人游戏中离散更新步骤导致的离散化漂移问题，以提高生成对抗网络等零和游戏的训练性能和稳定性。",
        "方法": "使用后向误差分析推导修改后的连续动态系统，识别离散化漂移的不同组成部分，并通过正则化器优化训练过程。",
        "关键词": [
            "离散化漂移",
            "生成对抗网络",
            "后向误差分析",
            "正则化器",
            "零和游戏"
        ],
        "涉及的技术概念": {
            "离散化漂移": "由离散更新步骤导致玩家从连续梯度流中漂移的现象，影响游戏性能和稳定性。",
            "后向误差分析": "用于推导紧密跟随离散动态的修改后的连续动态系统的分析方法。",
            "正则化器": "用于明确取消有害漂移形式或加强有益漂移形式，从而改善训练性能的技术手段。"
        },
        "success": true
    },
    {
        "order": 303,
        "title": "Discriminative Complementary-Label Learning with Weighted Loss",
        "html": "https://ICML.cc//virtual/2021/poster/10739",
        "abstract": "Complementary-label learning (CLL) deals with the weak supervision scenario where each training instance is associated with one \\emph{complementary} label, which specifies the class label that the instance does \\emph{not} belong to. Given the training instance ${\\bm x}$, existing CLL approaches aim at modeling the \\emph{generative} relationship between the complementary label $\\bar y$, i.e. $P(\\bar y\\mid {\\bm x})$, and the ground-truth label $y$, i.e. $P(y\\mid {\\bm x})$. Nonetheless, as the ground-truth label is not directly accessible for complementarily labeled training instance, strong generative assumptions may not hold for real-world CLL tasks. In this paper, we derive a simple and theoretically-sound \\emph{discriminative} model towards $P(\\bar y\\mid {\\bm x})$, which naturally leads to a risk estimator with estimation error bound at $\\mathcal{O}(1/\\sqrt{n})$ convergence rate. Accordingly, a practical CLL approach is proposed by further introducing weighted loss to the empirical risk to maximize the predictive gap between potential ground-truth label and complementary label. Extensive experiments clearly validate the effectiveness of the proposed discriminative complementary-label learning approach.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于加权损失的判别式互补标签学习",
        "摘要翻译": "互补标签学习 (CLL) 处理的是一种弱监督场景，其中每个训练实例都与一个互补标签相关联，该标签指定了该实例不属于的类别标签。给定训练实例 x，现有的 CLL 方法旨在建模互补标签 y_bar（即 P(y_bar|x)）与真实标签 y（即 P(y|x)）之间的生成关系。然而，由于互补标记的训练实例无法直接访问真实标签，因此对于现实世界的 CLL 任务来说，强大的生成假设可能不成立。在本文中，我们推导出一个简单的、理论上合理的判别模型 P(y_bar|x)，它自然会导致风险估计器，其估计误差界限的收敛速度为 O(1/√n)。因此，通过进一步引入加权损失到经验风险中，以最大化潜在真实标签和互补标签之间的预测差距，提出了一种实用的 CLL 方法。大量的实验清楚地验证了所提出的判别式互补标签学习方法的有效性。",
        "领域": "弱监督学习、标签噪声学习、分类",
        "问题": "解决互补标签学习中，由于难以直接访问真实标签，导致基于生成模型的CLL方法效果不佳的问题。",
        "动机": "现有的互补标签学习方法依赖于对真实标签的生成假设，但在实际应用中，这些假设往往难以满足。因此，研究一种不依赖强生成假设的判别式互补标签学习方法具有重要意义。",
        "方法": "提出一种判别式互补标签学习模型，直接建模互补标签的条件概率分布，并结合加权损失来增大真实标签和互补标签之间的预测差距。通过理论分析证明了该方法的收敛性，并通过实验验证了其有效性。",
        "关键词": [
            "互补标签学习",
            "弱监督学习",
            "判别模型",
            "加权损失",
            "标签噪声"
        ],
        "涉及的技术概念": {
            "互补标签学习": "一种弱监督学习方法，其中每个样本都被标记为不属于的类别，而不是属于的类别。",
            "判别模型": "直接学习输入和输出之间的条件概率分布，而无需对数据的生成过程进行建模。本文采用判别模型来直接建模互补标签的条件概率。"
        }
    },
    {
        "order": 304,
        "title": "Disentangling Sampling and Labeling Bias for Learning in Large-output Spaces",
        "html": "https://ICML.cc//virtual/2021/poster/10435",
        "abstract": "Negative sampling schemes enable efficient training given a large number of classes, by offering a means to approximate a computationally expensive loss function that takes all labels into account. In this paper, we present a new connection between these schemes and loss modification techniques for countering label imbalance. We show that different negative sampling schemes implicitly trade-off performance on dominant versus rare labels. Further, we provide a unified means to explicitly tackle both sampling bias, arising from working with a subset of all labels, and labeling bias, which is inherent to the data due to label imbalance. \nWe empirically verify our findings on long-tail classification and retrieval benchmarks.",
        "conference": "ICML",
        "中文标题": "解构大规模输出空间学习中的采样与标注偏差",
        "摘要翻译": "负采样方案通过提供一种近似计算所有标签的昂贵损失函数的方法，使得在大量类别的情况下能够高效训练。在本文中，我们提出了这些方案与用于对抗标签不平衡的损失修改技术之间的新联系。我们展示了不同的负采样方案如何在主导标签与稀有标签之间隐式地权衡性能。此外，我们提供了一种统一的方法，以明确解决由处理所有标签的子集引起的采样偏差，以及由于标签不平衡而固有的标注偏差。我们在长尾分类和检索基准上实证验证了我们的发现。",
        "领域": "长尾分类、图像检索、不平衡学习",
        "问题": "解决在大规模输出空间中学习时遇到的采样偏差和标注偏差问题",
        "动机": "研究旨在揭示负采样方案与损失修改技术之间的联系，并提出一种统一方法来同时解决采样偏差和标注偏差，以提高模型在长尾分布数据上的性能。",
        "方法": "通过分析不同负采样方案对主导和稀有标签性能的影响，提出了一种统一框架来明确处理采样和标注偏差，并在长尾分类和检索任务上进行了实证验证。",
        "关键词": [
            "负采样",
            "长尾分类",
            "标签不平衡",
            "采样偏差",
            "标注偏差"
        ],
        "涉及的技术概念": {
            "负采样": "用于在大规模类别中高效训练的技术，通过近似计算所有标签的损失函数来减少计算成本。",
            "损失修改技术": "用于对抗标签不平衡的技术，通过调整损失函数来改善模型在稀有标签上的性能。",
            "采样偏差": "由于仅使用所有标签的子集进行训练而引入的偏差，影响模型性能。"
        },
        "success": true
    },
    {
        "order": 305,
        "title": "Disentangling syntax and semantics in the brain with deep networks",
        "html": "https://ICML.cc//virtual/2021/poster/9271",
        "abstract": "The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. However, the nature of these activations remains largely unknown and presumably conflate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2's activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of ~4.6 hours of narrated text. The results highlight two findings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.",
        "conference": "ICML",
        "中文标题": "用深度网络解开大脑中的句法和语义",
        "摘要翻译": "像GPT-2这样的语言变换器的激活已被证明能够线性映射到语音理解时的大脑活动上。然而，这些激活的性质在很大程度上仍然是未知的，并且可能混淆了不同的语言类别。在这里，我们提出了一个分类法，将语言模型的高维激活分解为四个组合类别：词汇、组合、句法和语义表示。然后，我们引入了一种统计方法，通过GPT-2的激活视角，分解了345名受试者在听约4.6小时的叙述性文本时用功能磁共振成像（fMRI）记录的大脑活动。结果突出了两个发现。首先，组合表示比词汇表示招募了更广泛的皮质网络，并包括双侧颞叶、顶叶和前额叶皮质。其次，与之前的说法相反，句法和语义并不与分离的模块相关联，而是似乎共享一个共同且分布式的神经基质。总的来说，这项研究引入了一个多功能框架，以在大脑活动中隔离语言构造的分布式表示。",
        "领域": "自然语言处理与视觉结合、脑机接口、认知神经科学",
        "问题": "如何区分和识别大脑在处理语言时的句法和语义活动",
        "动机": "理解语言变换器模型（如GPT-2）的激活如何映射到大脑活动，并区分不同的语言处理类别",
        "方法": "提出一种分类法分解语言模型的激活，并使用统计方法分析fMRI数据以识别语言处理的大脑活动模式",
        "关键词": [
            "语言模型",
            "大脑活动映射",
            "句法语义分离",
            "fMRI分析",
            "分布式表示"
        ],
        "涉及的技术概念": {
            "语言变换器": "如GPT-2，用于生成语言表示，本研究通过其激活映射大脑活动",
            "功能磁共振成像（fMRI）": "用于记录受试者在听叙述性文本时的大脑活动，以分析语言处理模式",
            "分布式神经基质": "指句法和语义处理在大脑中的共享和分布式的神经基础，挑战了之前关于它们分离的观点"
        },
        "success": true
    },
    {
        "order": 306,
        "title": "Dissecting Supervised Constrastive Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9677",
        "abstract": "Minimizing cross-entropy over the softmax scores of a linear map composed with a high-capacity encoder is arguably the most popular choice for training neural networks on supervised learning tasks. However, recent works show that one can directly optimize the encoder instead, to obtain equally (or even more) discriminative representations via a supervised variant of a contrastive objective. In this work, we address the question whether there are fundamental differences in the sought-for representation geometry in the output space of the encoder at minimal loss. Specifically, we prove, under mild assumptions, that both losses attain their minimum once the representations of each class collapse to the vertices of a regular simplex, inscribed in a hypersphere. We provide empirical evidence that this configuration is attained in practice and that reaching a close-to-optimal state typically indicates good generalization performance. Yet, the two losses show remarkably different optimization behavior. The number of iterations required to perfectly fit to data scales superlinearly with the amount of randomly flipped labels for the supervised contrastive loss. This is in contrast to the approximately linear scaling previously reported for networks trained with cross-entropy.",
        "conference": "ICML",
        "中文标题": "剖析监督对比学习",
        "摘要翻译": "在监督学习任务中，通过线性映射与高容量编码器组合后的softmax分数最小化交叉熵，可以说是训练神经网络最流行的选择。然而，最近的研究表明，可以直接优化编码器，通过监督对比目标的变体获得同样（或更）具区分性的表示。在这项工作中，我们探讨了在最小损失下，编码器输出空间中寻求的表示几何是否存在根本差异。具体来说，我们在温和的假设下证明，一旦每个类的表示崩溃到一个规则单纯形的顶点，内接于一个超球面，两种损失都达到其最小值。我们提供的经验证据表明，这种配置在实践中是可以达到的，并且达到接近最优的状态通常预示着良好的泛化性能。然而，这两种损失显示出显著不同的优化行为。完美拟合数据所需的迭代次数与监督对比损失的随机翻转标签数量呈超线性比例。这与之前报道的对于使用交叉熵训练的网络大约呈线性比例形成对比。",
        "领域": "深度学习、表示学习、监督学习",
        "问题": "探讨监督对比学习与交叉熵损失在编码器输出空间表示几何上的根本差异及其优化行为",
        "动机": "研究监督对比学习是否能提供比传统交叉熵损失更优的表示几何和优化效率",
        "方法": "理论证明和经验分析监督对比学习与交叉熵损失在表示几何和优化行为上的差异",
        "关键词": [
            "监督对比学习",
            "表示几何",
            "优化行为",
            "交叉熵损失",
            "编码器优化"
        ],
        "涉及的技术概念": {
            "监督对比学习": "一种通过直接优化编码器以获得区分性表示的学习方法",
            "表示几何": "指编码器输出空间中数据表示的空间分布和结构",
            "交叉熵损失": "传统用于训练神经网络的损失函数，通过最小化预测与真实标签之间的差异来优化模型"
        },
        "success": true
    },
    {
        "order": 307,
        "title": "Distributed Nystr\\'{o}m Kernel Learning with Communications",
        "html": "https://ICML.cc//virtual/2021/poster/8483",
        "abstract": "We study the statistical performance for distributed kernel ridge regression\nwith Nystr\\'{o}m (DKRR-NY) and with Nystr\\'{o}m and iterative solvers (DKRR-NY-PCG) and successfully derive the optimal learning rates, which can improve the ranges of the number of local processors $p$ to the optimal in existing state-of-art bounds. More precisely, our theoretical analysis show that DKRR-NY and DKRR-NY-PCG achieve the same learning rates as the exact KRR requiring essentially $\\mathcal{O}(|D|^{1.5})$ time and $\\mathcal{O}(|D|)$ memory with relaxing the restriction on $p$ in expectation, where $|D|$ is the number of data, which exhibits the average effectiveness of multiple trials. Furthermore, for showing the generalization performance in a single trial, we deduce the learning rates for DKRR-NY and DKRR-NY-PCG in probability. Finally, we propose a novel algorithm DKRR-NY-CM based on DKRR-NY, which employs a communication strategy to further improve the learning performance, whose effectiveness of communications is validated in theoretical and experimental analysis.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "分布式Nyström核学习与通信",
        "摘要翻译": "我们研究了使用Nyström方法的分布式核岭回归（DKRR-NY）以及结合Nyström方法和迭代求解器（DKRR-NY-PCG）的统计性能，并成功推导出了最优学习率，这可以改进现有最先进界限中本地处理器数量p的范围至最优。更准确地说，我们的理论分析表明，DKRR-NY和DKRR-NY-PCG实现了与精确KRR相同的学习率，后者本质上需要O(|D|^1.5)的时间和O(|D|)的内存，同时放宽了对p的限制，其中|D|是数据量，这展示了多次试验的平均有效性。此外，为了展示单次试验中的泛化性能，我们推导了DKRR-NY和DKRR-NY-PCG的概率学习率。最后，我们基于DKRR-NY提出了一种新算法DKRR-NY-CM，该算法采用通信策略进一步提高了学习性能，其通信有效性在理论和实验分析中得到了验证。",
        "领域": "核方法, 分布式学习, 机器学习优化",
        "问题": "改进分布式核岭回归的学习率和处理器数量范围",
        "动机": "提高分布式环境下核岭回归的学习效率和性能",
        "方法": "结合Nyström方法和迭代求解器，提出新的通信策略算法",
        "关键词": [
            "分布式核岭回归",
            "Nyström方法",
            "迭代求解器",
            "通信策略",
            "学习率优化"
        ],
        "涉及的技术概念": {
            "Nyström方法": "用于近似核矩阵，减少计算和内存需求",
            "分布式核岭回归": "在分布式环境下进行的核岭回归，旨在处理大规模数据集",
            "通信策略": "在分布式学习中用于优化处理器间通信，提高学习性能"
        }
    },
    {
        "order": 308,
        "title": "Distributed Second Order Methods with Fast Rates and Compressed Communication",
        "html": "https://ICML.cc//virtual/2021/poster/10191",
        "abstract": "We develop several new communication-efficient second-order methods for distributed optimization. Our first method, NEWTON-STAR, is a variant of Newton's method from which it inherits its fast local quadratic rate. However, unlike Newton's method, NEWTON-STAR enjoys the same per iteration communication cost as gradient descent. While this method is impractical as it relies on the use of certain unknown parameters characterizing the Hessian of the objective function at the optimum,  it serves as the starting point which enables us to design practical variants thereof with strong theoretical guarantees. In particular, we design a stochastic sparsification strategy for learning the unknown parameters in an iterative fashion in a communication efficient manner. Applying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN, for which we prove  local linear and superlinear rates independent of the condition number. When applicable, this method can have dramatically superior convergence behavior when compared to state-of-the-art methods. Finally, we develop a globalization strategy using cubic regularization which leads to our next method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear convergence rates, and a fast superlinear rate. Our results are supported with experimental results on real datasets, and show several orders of magnitude improvement on baseline and state-of-the-art methods in terms of communication complexity. ",
        "conference": "ICML",
        "中文标题": "分布式二阶方法：快速收敛率与压缩通信",
        "摘要翻译": "我们开发了几种新的通信高效的分布式优化二阶方法。我们的第一种方法，NEWTON-STAR，是牛顿法的一个变体，继承了其快速的局部二次收敛率。然而，与牛顿法不同，NEWTON-STAR每次迭代的通信成本与梯度下降相同。虽然这种方法不切实际，因为它依赖于在最优解处目标函数Hessian矩阵的某些未知参数的使用，但它作为起点，使我们能够设计出具有强大理论保证的实用变体。特别是，我们设计了一种随机稀疏化策略，以通信高效的方式迭代学习未知参数。将这一策略应用于NEWTON-STAR，我们得到了下一个方法，NEWTON-LEARN，我们证明了其局部线性和超线性收敛率与条件数无关。在适用的情况下，这种方法与最先进的方法相比，可以具有显著优越的收敛行为。最后，我们开发了一种使用三次正则化的全球化策略，这导致了我们的下一个方法，CUBIC-NEWTON-LEARN，我们证明了其全局次线性和线性收敛率，以及快速的超线性收敛率。我们的结果得到了在真实数据集上的实验结果的支持，并在通信复杂度方面显示出比基线和最先进方法几个数量级的改进。",
        "领域": "分布式优化、机器学习优化、通信效率优化",
        "问题": "开发通信高效的分布式优化二阶方法，以解决传统方法在通信成本和收敛速度上的限制。",
        "动机": "为了在分布式环境中实现更高效的优化过程，减少通信开销同时保持或提升收敛速度。",
        "方法": "提出了NEWTON-STAR、NEWTON-LEARN和CUBIC-NEWTON-LEARN三种方法，分别通过变体牛顿法、随机稀疏化策略和三次正则化策略，实现通信效率和收敛速度的优化。",
        "关键词": [
            "分布式优化",
            "二阶方法",
            "通信效率",
            "收敛率",
            "随机稀疏化"
        ],
        "涉及的技术概念": {
            "NEWTON-STAR": "牛顿法的变体，保持快速局部二次收敛率的同时，通信成本与梯度下降相同。",
            "随机稀疏化策略": "用于在通信高效的方式下迭代学习Hessian矩阵的未知参数，支持NEWTON-LEARN方法的实现。",
            "三次正则化": "用于全球化策略，支持CUBIC-NEWTON-LEARN方法，实现全局收敛和快速的超线性收敛率。"
        },
        "success": true
    },
    {
        "order": 309,
        "title": "Distributionally Robust Optimization with Markovian Data",
        "html": "https://ICML.cc//virtual/2021/poster/10259",
        "abstract": "We study a stochastic program where the probability distribution of the uncertain problem parameters is unknown and only indirectly observed via finitely many correlated samples generated by an unknown Markov chain with $d$ states. \nWe propose a data-driven distributionally robust optimization model to estimate the problem's objective function and optimal solution. By leveraging results from large deviations theory, we derive statistical guarantees on the quality of these estimators. The underlying worst-case expectation problem is nonconvex and involves $\\mathcal O(d^2)$ decision variables. Thus, it cannot be solved efficiently for large $d$. By exploiting the structure of this problem, we devise a customized Frank-Wolfe algorithm with convex direction-finding subproblems of size $\\mathcal O(d)$. We prove that this algorithm finds a stationary point efficiently under mild conditions. The efficiency of the method is predicated on a dimensionality reduction enabled by a dual reformulation. Numerical experiments indicate that our approach has better computational and statistical properties than the state-of-the-art methods.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于马尔可夫数据的分布鲁棒优化",
        "摘要翻译": "我们研究一个随机规划问题，其中不确定问题参数的概率分布是未知的，并且只能通过由具有 $d$ 个状态的未知马尔可夫链生成的有限多个相关样本间接观察到。我们提出了一个数据驱动的分布鲁棒优化模型来估计问题的目标函数和最优解。通过利用大偏差理论的结果，我们推导出这些估计器的质量的统计保证。潜在的最坏情况期望问题是非凸的，并且涉及 O(d^2) 个决策变量。因此，对于大的 $d$ 值，它无法有效地求解。通过利用此问题的结构，我们设计了一种定制的 Frank-Wolfe 算法，其凸方向查找子问题的大小为 O(d)。我们证明，在温和条件下，该算法可以有效地找到一个驻点。该方法的效率基于由对偶重构实现的降维。数值实验表明，我们的方法比最先进的方法具有更好的计算和统计特性。",
        "领域": "优化算法, 随机过程, 统计推断",
        "问题": "如何在高维马尔可夫链数据下，有效解决不确定概率分布的随机规划问题，并保证解的质量。",
        "动机": "现有的方法在高维数据下计算效率低，且统计性质较差，因此需要开发一种更高效、更具统计保证的分布鲁棒优化方法。",
        "方法": "提出了一种数据驱动的分布鲁棒优化模型，并利用大偏差理论推导统计保证。同时，设计了一种定制的 Frank-Wolfe 算法，通过对偶重构实现降维，提高了计算效率。",
        "关键词": [
            "分布鲁棒优化",
            "马尔可夫链",
            "Frank-Wolfe算法",
            "大偏差理论",
            "随机规划"
        ],
        "涉及的技术概念": {
            "分布鲁棒优化": "处理不确定概率分布下的优化问题，通过考虑最坏情况的分布来保证解的鲁棒性。",
            "Frank-Wolfe算法": "一种用于解决约束凸优化问题的迭代算法，通过线性化目标函数并求解线性规划子问题来逼近最优解。"
        }
    },
    {
        "order": 310,
        "title": "Distribution-Free Calibration Guarantees for Histogram Binning without Sample Splitting",
        "html": "https://ICML.cc//virtual/2021/poster/10205",
        "abstract": "We prove calibration guarantees for the popular histogram binning (also called uniform-mass binning) method of Zadrozny and Elkan (2001). Histogram binning has displayed strong practical performance, but theoretical guarantees have only been shown for sample split versions that avoid 'double dipping' the data. We demonstrate that the statistical cost of sample splitting is practically significant on a credit default dataset. We then prove calibration guarantees for the original method that double dips the data, using a certain Markov property of order statistics. Based on our results, we make practical recommendations for choosing the number of bins in histogram binning. In our illustrative simulations, we propose a new tool for assessing calibration---validity plots---which provide more information than an ECE estimate.",
        "conference": "ICML",
        "中文标题": "无需样本分割的直方图分箱的无分布校准保证",
        "摘要翻译": "我们证明了Zadrozny和Elkan（2001年）提出的流行直方图分箱（也称为均匀质量分箱）方法的校准保证。直方图分箱在实际应用中表现出色，但理论保证仅针对避免'双重浸入'数据的样本分割版本展示。我们在一个信用违约数据集上证明了样本分割的统计成本实际上非常显著。随后，我们利用顺序统计量的某种马尔可夫性质，为原始方法（即双重浸入数据的方法）证明了校准保证。基于我们的结果，我们提出了选择直方图分箱中分箱数量的实用建议。在我们的示例模拟中，我们提出了一种评估校准的新工具——有效性图，它比ECE估计提供更多信息。",
        "领域": "机器学习模型校准、信用风险评估、统计学习方法",
        "问题": "直方图分箱方法在校准保证方面的理论支持不足，尤其是在不进行样本分割的情况下。",
        "动机": "解决直方图分箱方法在不分割样本情况下的校准保证问题，以减少样本分割带来的统计成本。",
        "方法": "利用顺序统计量的马尔可夫性质，为原始直方图分箱方法提供校准保证，并提出选择分箱数量的实用建议。",
        "关键词": [
            "直方图分箱",
            "模型校准",
            "信用风险评估",
            "顺序统计量",
            "有效性图"
        ],
        "涉及的技术概念": {
            "直方图分箱": "一种将数据分成若干区间（或称为'箱'）的方法，用于模型校准和概率估计。",
            "顺序统计量的马尔可夫性质": "用于分析数据排序特性的统计性质，本文中用于证明不分割样本情况下的校准保证。",
            "有效性图": "一种新的工具，用于评估模型的校准程度，比传统的ECE估计提供更丰富的信息。"
        },
        "success": true
    },
    {
        "order": 311,
        "title": "Ditto: Fair and Robust Federated Learning Through Personalization",
        "html": "https://ICML.cc//virtual/2021/poster/10571",
        "abstract": "Fairness and robustness are two important concerns for federated learning systems. In this work, we identify that robustness to data and model poisoning attacks and fairness, measured as the uniformity of performance across devices, are competing constraints in statistically heterogeneous networks. To address these constraints, we propose employing a simple, general framework for personalized federated learning, Ditto, that can inherently provide fairness and robustness benefits, and develop a scalable solver for it. Theoretically, we analyze the ability of Ditto to achieve fairness and robustness simultaneously on a class of linear problems. Empirically, across a suite of federated datasets, we show that Ditto not only achieves competitive performance relative to recent personalization methods, but also enables more accurate, robust, and fair models relative to state-of-the-art fair or robust baselines.",
        "conference": "ICML",
        "中文标题": "Ditto：通过个性化实现公平且鲁棒的联邦学习",
        "摘要翻译": "公平性和鲁棒性是联邦学习系统的两个重要关注点。在这项工作中，我们发现，在统计异构网络中，对数据和模型投毒攻击的鲁棒性与公平性（以设备间性能的一致性衡量）是相互竞争的约束。为了解决这些约束，我们提出了一个简单、通用的个性化联邦学习框架Ditto，该框架能够固有地提供公平性和鲁棒性优势，并为其开发了一个可扩展的求解器。理论上，我们分析了Ditto在一类线性问题上同时实现公平性和鲁棒性的能力。实证上，通过一系列联邦数据集，我们展示了Ditto不仅相对于最近的个性化方法实现了竞争性的性能，而且相对于最先进的公平或鲁棒基线，能够实现更准确、更鲁棒和更公平的模型。",
        "领域": "联邦学习、个性化学习、模型鲁棒性",
        "问题": "解决联邦学习系统中公平性和鲁棒性之间的竞争约束问题",
        "动机": "在统计异构网络中，公平性和鲁棒性作为相互竞争的约束，需要一种能够同时满足这两者的解决方案",
        "方法": "提出了一个名为Ditto的个性化联邦学习框架，并开发了一个可扩展的求解器，理论上分析其能力，并通过实证验证其效果",
        "关键词": [
            "联邦学习",
            "个性化学习",
            "公平性",
            "鲁棒性",
            "模型投毒攻击"
        ],
        "涉及的技术概念": {
            "个性化联邦学习": "通过为每个客户端定制模型来满足其特定需求，同时保持全局模型的一致性",
            "公平性": "在联邦学习中，确保所有设备或客户端在模型性能上享有平等的待遇",
            "鲁棒性": "模型对数据和模型投毒攻击的抵抗能力，确保在恶意攻击下仍能保持性能"
        },
        "success": true
    },
    {
        "order": 312,
        "title": "Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration",
        "html": "https://ICML.cc//virtual/2021/poster/10269",
        "abstract": "In this paper, sample-aware policy entropy regularization is proposed to enhance the conventional policy entropy regularization for better exploration. Exploiting the sample distribution obtainable from the replay buffer, the proposed sample-aware entropy regularization maximizes the entropy of the weighted sum of the policy action distribution and the sample action distribution from the replay buffer for sample-efficient exploration. A practical algorithm named diversity actor-critic (DAC) is developed by applying policy iteration to the objective function with the proposed sample-aware entropy regularization. Numerical results show that DAC significantly outperforms existing recent algorithms for reinforcement learning. ",
        "conference": "ICML",
        "中文标题": "多样性行动者-评论家：样本感知的熵正则化以实现样本高效探索",
        "摘要翻译": "本文提出了样本感知的策略熵正则化方法，以增强传统的策略熵正则化，从而实现更好的探索。利用从回放缓冲区可获得的样本分布，所提出的样本感知熵正则化通过最大化策略动作分布与回放缓冲区中样本动作分布的加权和的熵，以实现样本高效的探索。通过将策略迭代应用于包含所提出的样本感知熵正则化的目标函数，开发了一种名为多样性行动者-评论家（DAC）的实用算法。数值结果表明，DAC在强化学习领域显著优于现有的最新算法。",
        "领域": "强化学习",
        "问题": "如何在强化学习中实现更高效的探索",
        "动机": "增强传统的策略熵正则化方法，以提升探索效率",
        "方法": "提出样本感知的策略熵正则化方法，并开发多样性行动者-评论家（DAC）算法",
        "关键词": [
            "强化学习",
            "熵正则化",
            "样本高效探索",
            "多样性行动者-评论家",
            "策略迭代"
        ],
        "涉及的技术概念": {
            "样本感知熵正则化": "通过利用样本分布来增强策略熵正则化，以提高探索效率",
            "多样性行动者-评论家（DAC）": "一种结合样本感知熵正则化的强化学习算法，旨在实现样本高效的探索",
            "策略迭代": "用于优化包含样本感知熵正则化的目标函数的方法"
        },
        "success": true
    },
    {
        "order": 313,
        "title": "Domain Generalization using Causal Matching",
        "html": "https://ICML.cc//virtual/2021/poster/8585",
        "abstract": "In the domain generalization literature, a common objective is to learn representations independent of the domain after conditioning on the class label. We show that this objective is not sufficient: there exist counter-examples where a model fails to generalize to unseen domains even after satisfying class-conditional domain invariance. We formalize this observation through a structural causal model and show the importance of modeling within-class variations for generalization. Specifically, classes contain objects that characterize specific causal features, and domains can be interpreted as interventions on these objects that change non-causal features. We highlight an alternative condition: inputs across domains should have the same representation if they are derived from the same object. Based on this objective, we propose matching-based algorithms when base objects are observed (e.g., through data augmentation) and approximate the objective when objects are not observed (MatchDG). Our simple matching-based algorithms are competitive to prior work on out-of-domain accuracy for rotated MNIST, Fashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers ground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from MatchDG have over 50% overlap with ground-truth matches.",
        "conference": "ICML",
        "中文标题": "使用因果匹配的领域泛化",
        "摘要翻译": "在领域泛化文献中，一个常见的目标是在类别标签条件下学习独立于领域的表示。我们表明这一目标并不充分：存在反例，即使满足类别条件领域不变性，模型也无法泛化到未见过的领域。我们通过结构因果模型形式化这一观察，并展示了建模类内变化对泛化的重要性。具体来说，类别包含表征特定因果特征的对象，而领域可以被解释为对这些对象的干预，改变非因果特征。我们强调了一个替代条件：如果输入来自同一对象，则跨领域的输入应具有相同的表示。基于这一目标，我们提出了当基础对象被观察到时（例如通过数据增强）的基于匹配的算法，并在对象未被观察到时（MatchDG）近似该目标。我们简单的基于匹配的算法在旋转MNIST、Fashion-MNIST、PACS和Chest-Xray数据集上的域外准确性方面与先前的工作相比具有竞争力。我们的方法MatchDG还能恢复真实对象匹配：在MNIST和Fashion-MNIST上，MatchDG的前10匹配与真实匹配有超过50%的重叠。",
        "领域": "领域泛化、图像分类、数据增强",
        "问题": "解决模型在满足类别条件领域不变性后仍无法泛化到未见领域的问题",
        "动机": "揭示类别条件领域不变性不足以保证泛化能力，提出通过建模类内变化来提高泛化性能",
        "方法": "提出基于因果匹配的算法，包括在基础对象被观察到时的匹配算法和未被观察到时的近似算法MatchDG",
        "关键词": [
            "领域泛化",
            "因果匹配",
            "类内变化",
            "数据增强",
            "结构因果模型"
        ],
        "涉及的技术概念": {
            "结构因果模型": "用于形式化领域泛化问题，揭示类别条件领域不变性的不足",
            "类内变化": "指类别内部对象的变化，建模这些变化对提高泛化性能至关重要",
            "因果匹配": "基于输入来自同一对象应具有相同表示的原则，提出的匹配算法以提高泛化能力"
        },
        "success": true
    },
    {
        "order": 314,
        "title": "Don’t Just Blame Over-parametrization for Over-confidence: Theoretical Analysis of Calibration in Binary Classification",
        "html": "https://ICML.cc//virtual/2021/poster/8711",
        "abstract": "Modern machine learning models with high accuracy are often miscalibrated---the predicted top probability does not reflect the actual accuracy, and tends to be \\emph{over-confident}. It is commonly believed that such over-confidence is mainly due to \\emph{over-parametrization}, in particular when the model is large enough to memorize the training data and maximize the confidence.\n\n  In this paper, we show theoretically that over-parametrization is not the only reason for over-confidence. We prove that \\emph{logistic regression is inherently over-confident}, in the realizable, under-parametrized setting where the data is generated from the logistic model, and the sample size is much larger than the number of parameters. Further, this over-confidence happens for general well-specified binary classification problems as long as the activation is symmetric and concave on the positive part. Perhaps surprisingly, we also show that over-confidence is not always the case---there exists another activation function (and a suitable loss function) under which the learned classifier is \\emph{under-confident} at some probability values. Overall, our theory provides a precise characterization of calibration in realizable binary classification, which we verify on simulations and real data experiments.",
        "conference": "ICML",
        "中文标题": "不要仅仅将过度参数化归咎于过度自信：二元分类中校准的理论分析",
        "摘要翻译": "现代高准确率的机器学习模型常常存在校准不当的问题——预测的最高概率并不反映实际准确率，且往往表现出过度自信。人们普遍认为，这种过度自信主要是由于过度参数化，特别是当模型足够大以记忆训练数据并最大化置信度时。在本文中，我们从理论上证明，过度参数化并非过度自信的唯一原因。我们证明了在可实现、参数不足的设置下，即数据由逻辑模型生成且样本量远大于参数数量的情况下，逻辑回归本质上是过度自信的。此外，只要激活函数在正部分是对称且凹的，这种过度自信就会发生在一般的良好指定的二元分类问题中。或许令人惊讶的是，我们还表明，过度自信并非总是如此——存在另一种激活函数（及适当的损失函数），在该函数下学习到的分类器在某些概率值下是自信不足的。总体而言，我们的理论为可实现二元分类中的校准提供了精确的特征描述，我们在模拟和真实数据实验中验证了这一点。",
        "领域": "机器学习理论、二元分类、模型校准",
        "问题": "高准确率的机器学习模型为何会出现校准不当，特别是过度自信的现象",
        "动机": "探讨过度参数化是否是导致模型过度自信的唯一原因，以及在不同条件下模型校准的表现",
        "方法": "通过理论分析，研究逻辑回归在特定条件下的校准特性，并探索不同激活函数对模型校准的影响",
        "关键词": [
            "模型校准",
            "过度自信",
            "逻辑回归",
            "激活函数",
            "二元分类"
        ],
        "涉及的技术概念": {
            "过度参数化": "模型参数数量过多，可能导致模型记忆训练数据而表现出过度自信",
            "逻辑回归": "一种用于二元分类的统计方法，本文中证明其在特定条件下本质上是过度自信的",
            "激活函数": "神经网络中引入非线性因素的函数，本文探讨了不同激活函数对模型校准特性的影响"
        },
        "success": true
    },
    {
        "order": 315,
        "title": "DORO: Distributional and Outlier Robust Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/8919",
        "abstract": "Many machine learning tasks involve subpopulation shift where the testing data distribution is a subpopulation of the training distribution. For such settings, a line of recent work has proposed the use of a variant of empirical risk minimization(ERM) known as distributionally robust optimization (DRO). In this work, we apply DRO to real, large-scale tasks with subpopulation shift, and observe that DRO performs relatively poorly, and moreover has severe instability. We identify one direct cause of this phenomenon: sensitivity of DRO to outliers in the datasets. To resolve this issue, we propose the framework of DORO, for Distributional and Outlier Robust Optimization. At the core of this approach is a refined risk function which prevents DRO from overfitting to potential outliers. We instantiate DORO for the Cressie-Read family of R\\'enyi divergence, and delve into two specific instances of this family: CVaR and $\\chi^2$-DRO. We theoretically prove the effectiveness of the proposed method, and empirically show that DORO improves the performance and stability of DRO with experiments on large modern datasets, thereby positively addressing the open question raised by Hashimoto et al., 2018. Codes are available at https://github.com/RuntianZ/doro.",
        "conference": "ICML",
        "中文标题": "DORO: 分布与异常值鲁棒优化",
        "摘要翻译": "许多机器学习任务涉及子群体偏移，其中测试数据分布是训练分布的一个子群体。针对这种情况，最近的一系列工作提出使用一种称为分布鲁棒优化（DRO）的经验风险最小化（ERM）变体。在这项工作中，我们将DRO应用于具有子群体偏移的真实、大规模任务，并观察到DRO表现相对较差，且存在严重的不稳定性。我们确定了这一现象的一个直接原因：DRO对数据集中的异常值敏感。为了解决这个问题，我们提出了DORO框架，即分布与异常值鲁棒优化。该方法的核心是一个改进的风险函数，可以防止DRO对潜在异常值的过拟合。我们为Cressie-Read家族的Rényi散度实例化了DORO，并深入研究了该家族的两个具体实例：CVaR和χ²-DRO。我们从理论上证明了所提出方法的有效性，并通过在现代大型数据集上的实验，实证显示DORO提高了DRO的性能和稳定性，从而正面回答了Hashimoto等人2018年提出的开放性问题。代码可在https://github.com/RuntianZ/doro获取。",
        "领域": "分布鲁棒优化、异常检测、机器学习鲁棒性",
        "问题": "解决分布鲁棒优化（DRO）在子群体偏移任务中对异常值敏感导致的性能不佳和不稳定性问题",
        "动机": "提高DRO在存在子群体偏移和异常值的大规模机器学习任务中的性能和稳定性",
        "方法": "提出DORO框架，通过改进的风险函数防止DRO对异常值的过拟合，并在Cressie-Read家族的Rényi散度中实例化DORO",
        "关键词": [
            "分布鲁棒优化",
            "异常值鲁棒性",
            "子群体偏移",
            "Cressie-Read家族",
            "Rényi散度"
        ],
        "涉及的技术概念": {
            "分布鲁棒优化（DRO）": "一种经验风险最小化的变体，旨在提高模型在数据分布变化下的鲁棒性",
            "Rényi散度": "用于衡量两个概率分布之间差异的指标，DORO框架中用于实例化改进的风险函数",
            "CVaR和χ²-DRO": "Cressie-Read家族中的两个具体实例，用于在DORO框架中实现分布与异常值的鲁棒优化"
        },
        "success": true
    },
    {
        "order": 316,
        "title": "Double-Win Quant: Aggressively Winning Robustness of Quantized Deep Neural Networks via Random Precision Training and Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9959",
        "abstract": "Quantization is promising in enabling powerful yet complex deep neural networks (DNNs) to be deployed into resource constrained platforms. However, quantized DNNs are vulnerable to adversarial attacks unless being equipped with sophisticated techniques, leading to a dilemma of struggling between DNNs' efficiency and robustness. In this work, we demonstrate a new perspective regarding quantization's role in DNNs' robustness, advocating that quantization can be leveraged to largely boost DNNs’ robustness, and propose a framework dubbed Double-Win Quant that can boost the robustness of quantized DNNs over their full precision counterparts by a large margin. Specifically, we for the first time identify that when an adversarially trained model is quantized to different precisions in a post-training manner, the associated adversarial attacks transfer poorly between different precisions. Leveraging this intriguing observation, we further develop Double-Win Quant integrating random precision inference and training to further reduce and utilize the poor adversarial transferability, enabling an aggressive ``win-win' in terms of DNNs' robustness and efficiency. Extensive experiments and ablation studies consistently validate Double-Win Quant's effectiveness and advantages over state-of-the-art (SOTA) adversarial training methods across various attacks/models/datasets. Our codes are available at: https://github.com/RICE-EIC/Double-Win-Quant.",
        "conference": "ICML",
        "中文标题": "双赢量化：通过随机精度训练与推断大幅提升量化深度神经网络的鲁棒性",
        "摘要翻译": "量化技术有望使强大但复杂的深度神经网络（DNNs）能够部署到资源受限的平台中。然而，除非配备复杂的技术，否则量化的DNNs容易受到对抗性攻击，导致在DNNs的效率和鲁棒性之间挣扎的困境。在这项工作中，我们展示了关于量化在DNNs鲁棒性中作用的新视角，主张可以利用量化大幅提升DNNs的鲁棒性，并提出了一个名为双赢量化的框架，该框架可以大幅提升量化DNNs相对于其全精度对应物的鲁棒性。具体来说，我们首次发现，当一个经过对抗训练的模型以后训练方式量化到不同的精度时，相关的对抗攻击在不同精度之间的转移性较差。利用这一有趣的观察，我们进一步开发了双赢量化，整合随机精度推断和训练，以进一步减少和利用这种较差的对抗转移性，从而在DNNs的鲁棒性和效率方面实现激进的“双赢”。广泛的实验和消融研究一致验证了双赢量化在各种攻击/模型/数据集上相对于最先进的对抗训练方法的有效性和优势。我们的代码可在以下网址获取：https://github.com/RICE-EIC/Double-Win-Quant。",
        "领域": "对抗性防御、模型量化、深度神经网络优化",
        "问题": "量化深度神经网络在保持高效率的同时如何提升对抗攻击的鲁棒性",
        "动机": "解决量化深度神经网络在效率和鲁棒性之间的权衡问题，探索量化技术提升模型鲁棒性的潜力",
        "方法": "提出双赢量化框架，通过随机精度训练和推断减少对抗攻击的转移性，从而提升模型的鲁棒性和效率",
        "关键词": [
            "对抗性防御",
            "模型量化",
            "随机精度训练",
            "深度神经网络",
            "鲁棒性提升"
        ],
        "涉及的技术概念": {
            "量化": "将深度神经网络的权重和激活从浮点数转换为低精度的定点数，以减少模型大小和加速推断",
            "对抗性训练": "通过在训练过程中引入对抗样本来增强模型对对抗攻击的鲁棒性",
            "随机精度推断": "在模型推断过程中随机选择不同的量化精度，以减少对抗攻击的转移性"
        },
        "success": true
    },
    {
        "order": 317,
        "title": "Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality",
        "html": "https://ICML.cc//virtual/2021/poster/8701",
        "abstract": "Designing off-policy reinforcement learning algorithms is typically a very challenging task, because a desirable iteration update often involves an expectation over an on-policy distribution. Prior off-policy actor-critic (AC) algorithms have introduced a new critic that uses the density ratio for adjusting the distribution mismatch in order to stabilize the convergence, but at the cost of potentially introducing high biases due to the estimation errors of both the density ratio and value function. In this paper, we develop a doubly robust off-policy AC (DR-Off-PAC) for discounted MDP, which can take advantage of learned nuisance functions to reduce estimation errors. Moreover, DR-Off-PAC adopts a single timescale structure, in which both actor and critics are updated simultaneously with constant stepsize, and is thus more sample efficient than prior algorithms that adopt either two timescale or nested-loop structure. We study the finite-time convergence rate and characterize the sample complexity for DR-Off-PAC to attain an $\\epsilon$-accurate optimal policy. We also show that the overall convergence of DR-Off-PAC is doubly robust to the approximation errors that depend only on the expressive power of approximation functions. To the best of our knowledge, our study establishes the first overall sample complexity analysis for single time-scale off-policy AC algorithm.",
        "conference": "ICML",
        "success": true,
        "中文标题": "双重稳健离策略行动者评论家方法：收敛性与最优性",
        "摘要翻译": "设计离策略强化学习算法通常是一项极具挑战性的任务，因为理想的迭代更新往往涉及到对策略分布的期望。先前的离策略行动者评论家（AC）算法引入了一种新的评论家，该评论家使用密度比来调整分布不匹配以稳定收敛，但代价是由于密度比和价值函数的估计误差可能引入高偏差。在本文中，我们为折扣MDP开发了一种双重稳健离策略AC（DR-Off-PAC），它可以利用学习的冗余函数来减少估计误差。此外，DR-Off-PAC采用单一时间尺度结构，其中行动者和评论家同时以恒定步长更新，因此比采用双时间尺度或嵌套循环结构的先前算法更样本高效。我们研究了有限时间收敛速率，并描述了DR-Off-PAC达到ε-准确最优策略的样本复杂度。我们还表明，DR-Off-PAC的整体收敛对于仅依赖于近似函数表达能力的近似误差是双重稳健的。据我们所知，我们的研究首次为单一时间尺度离策略AC算法建立了整体样本复杂度分析。",
        "领域": "强化学习, 机器学习优化, 算法收敛性分析",
        "问题": "解决离策略强化学习算法中由于分布不匹配和估计误差导致的高偏差问题",
        "动机": "提高离策略强化学习算法的样本效率和收敛稳定性，减少估计误差",
        "方法": "开发了一种双重稳健离策略AC算法（DR-Off-PAC），利用学习的冗余函数减少估计误差，采用单一时间尺度结构提高样本效率",
        "关键词": [
            "双重稳健",
            "离策略学习",
            "行动者评论家方法",
            "样本复杂度",
            "单一时间尺度"
        ],
        "涉及的技术概念": {
            "双重稳健": "在算法设计中同时考虑价值函数和密度比的估计误差，以提高算法的稳健性",
            "离策略学习": "算法能够利用非当前策略生成的数据进行学习，提高数据利用效率",
            "单一时间尺度结构": "行动者和评论家以相同的步长同时更新，简化算法结构并提高样本效率"
        }
    },
    {
        "order": 318,
        "title": "DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8445",
        "abstract": "Games are abstractions of the real world, where artificial agents learn to compete and cooperate with other agents. While significant achievements have been made in various perfect- and imperfect-information games, DouDizhu (a.k.a. Fighting the Landlord), a three-player card game, is still unsolved. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. Unfortunately, modern reinforcement learning algorithms mainly focus on simple and small action spaces, and not surprisingly, are shown not to make satisfactory progress in DouDizhu. In this work, we propose a conceptually simple yet effective DouDizhu AI system, namely DouZero, which enhances traditional Monte-Carlo methods with deep neural networks, action encoding, and parallel actors. Starting from scratch in a single server with four GPUs, DouZero outperformed all the existing DouDizhu AI programs in days of training and was ranked the first in the Botzone leaderboard among 344 AI agents. Through building DouZero, we show that classic Monte-Carlo methods can be made to deliver strong results in a hard domain with a complex action space. The code and an online demo are released at https://github.com/kwai/DouZero with the hope that this insight could motivate future work.",
        "conference": "ICML",
        "中文标题": "DouZero：通过自对弈深度强化学习掌握斗地主",
        "摘要翻译": "游戏是现实世界的抽象，人工智能代理在其中学习与其他代理竞争和合作。尽管在各种完美和不完美信息游戏中取得了显著成就，但斗地主（又称“打地主”），一种三人纸牌游戏，仍未得到解决。斗地主是一个极具挑战性的领域，涉及竞争、合作、不完美信息、大状态空间，尤其是庞大的可能动作集，其中合法动作每轮变化巨大。不幸的是，现代强化学习算法主要关注简单和小动作空间，不足为奇，它们在斗地主中未能取得令人满意的进展。在这项工作中，我们提出了一个概念上简单但有效的斗地主AI系统，即DouZero，它通过深度神经网络、动作编码和并行执行者增强了传统的蒙特卡洛方法。从零开始在单台配备四块GPU的服务器上，DouZero在数天的训练中超越了所有现有的斗地主AI程序，并在Botzone排行榜上344个AI代理中排名第一。通过构建DouZero，我们展示了经典的蒙特卡洛方法可以在具有复杂动作空间的困难领域中提供强大的结果。代码和在线演示发布于https://github.com/kwai/DouZero，希望这一见解能激励未来的工作。",
        "领域": "强化学习应用、游戏AI、多智能体系统",
        "问题": "解决在斗地主这一具有复杂动作空间和不完美信息的游戏中，现有强化学习算法表现不佳的问题。",
        "动机": "探索和证明在复杂动作空间和不完美信息的游戏中，通过增强传统蒙特卡洛方法，可以实现超越现有AI的表现。",
        "方法": "结合深度神经网络、动作编码和并行执行者，增强传统的蒙特卡洛方法，构建高效的斗地主AI系统。",
        "关键词": [
            "深度强化学习",
            "斗地主AI",
            "蒙特卡洛方法",
            "动作编码",
            "并行执行者"
        ],
        "涉及的技术概念": {
            "深度强化学习": "结合深度学习和强化学习的技术，用于处理复杂环境下的决策问题。",
            "蒙特卡洛方法": "一种通过随机采样来估计数值结果的统计方法，在DouZero中用于策略评估和改进。",
            "动作编码": "将复杂的动作空间编码为神经网络可以处理的格式，以解决动作空间大的问题。"
        },
        "success": true
    },
    {
        "order": 319,
        "title": "Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training",
        "html": "https://ICML.cc//virtual/2021/poster/10199",
        "abstract": "In this paper, we introduce a new perspective on training deep neural networks capable of state-of-the-art performance without the need for the expensive over-parameterization by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training. By starting from a random sparse network and continuously exploring sparse connectivities during training, we can perform an Over-Parameterization over the course of training, closing the gap in the expressibility between sparse training and dense training. We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training (DST) and discover that the benefits of DST come from its ability to consider across time all possible parameters when searching for the optimal sparse connectivity. As long as sufficient parameters have been reliably explored, DST can outperform the dense neural network by a large margin. We present a series of experiments to support our conjecture and achieve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More impressively, ITOP achieves dominant performance over the overparameterization-based sparse methods at extreme sparsities. When trained with ResNet-34 on CIFAR-100, ITOP can match the performance of the dense model at an extreme sparsity 98%. ",
        "conference": "ICML",
        "中文标题": "我们真的需要密集过参数化吗？稀疏训练中的实时过参数化",
        "摘要翻译": "在本文中，我们提出了一种新的视角，通过提出稀疏训练中的实时过参数化（ITOP）概念，无需昂贵的过参数化即可训练出能够达到最先进性能的深度神经网络。通过从一个随机稀疏网络开始，并在训练过程中不断探索稀疏连接性，我们可以在训练过程中进行过参数化，缩小稀疏训练与密集训练在表达能力上的差距。我们进一步使用ITOP来理解动态稀疏训练（DST）的潜在机制，并发现DST的优势来自于其在寻找最优稀疏连接性时能够考虑所有可能的参数。只要充分可靠地探索了足够的参数，DST就能大幅超越密集神经网络。我们进行了一系列实验来支持我们的猜想，并在ImageNet上使用ResNet-50实现了最先进的稀疏训练性能。更令人印象深刻的是，在极端稀疏性下，ITOP表现优于基于过参数化的稀疏方法。当在CIFAR-100上使用ResNet-34训练时，ITOP在98%的极端稀疏性下能够匹配密集模型的性能。",
        "领域": "稀疏训练",
        "问题": "如何在不需要密集过参数化的情况下，通过稀疏训练达到或超越密集神经网络的性能",
        "动机": "探索稀疏训练中实时过参数化的潜力，以减少计算资源消耗同时保持或提升模型性能",
        "方法": "提出实时过参数化（ITOP）概念，通过动态稀疏训练（DST）在训练过程中探索稀疏连接性",
        "关键词": [
            "稀疏训练",
            "实时过参数化",
            "动态稀疏训练",
            "ResNet",
            "ImageNet"
        ],
        "涉及的技术概念": {
            "实时过参数化（ITOP）": "在稀疏训练过程中动态调整网络参数，以实现与密集训练相当的表达能力",
            "动态稀疏训练（DST）": "一种训练方法，通过不断探索和调整网络的稀疏连接性来寻找最优参数配置",
            "极端稀疏性": "指网络在极高稀疏度（如98%）下仍能保持良好性能的能力"
        },
        "success": true
    },
    {
        "order": 320,
        "title": "DriftSurf: Stable-State / Reactive-State Learning under Concept Drift",
        "html": "https://ICML.cc//virtual/2021/poster/9929",
        "abstract": "When learning from streaming data, a change in the data distribution,\nalso known as concept drift, can render a previously-learned model\ninaccurate and require training a new model. We present an adaptive\nlearning algorithm that extends previous drift-detection-based methods\nby incorporating drift detection into a broader\nstable-state/reactive-state process. The advantage of our approach is\nthat we can use aggressive drift detection in the stable state to\nachieve a high detection rate, but mitigate the false positive rate of\nstandalone drift detection via a reactive state that reacts quickly to\ntrue drifts while eliminating most false positives. The algorithm is\ngeneric in its base learner and can be applied across a variety of\nsupervised learning problems. Our theoretical analysis shows that the\nrisk of the algorithm is (i) statistically better than standalone\ndrift detection and (ii) competitive to an algorithm with oracle\nknowledge of when (abrupt) drifts occur. Experiments on synthetic and\nreal datasets with concept drifts confirm our theoretical analysis.",
        "conference": "ICML",
        "中文标题": "DriftSurf：概念漂移下的稳态/反应态学习",
        "摘要翻译": "在从流数据中学习时，数据分布的变化，也称为概念漂移，可能会使先前学习的模型变得不准确，并需要训练一个新模型。我们提出了一种自适应学习算法，该算法通过将漂移检测纳入更广泛的稳态/反应态过程，扩展了之前基于漂移检测的方法。我们方法的优势在于，我们可以在稳态中使用积极的漂移检测以实现高检测率，但通过反应态快速响应真实漂移同时消除大多数误报，从而减轻独立漂移检测的误报率。该算法在其基础学习器上是通用的，可以应用于各种监督学习问题。我们的理论分析表明，该算法的风险（i）在统计上优于独立漂移检测，并且（ii）与具有漂移发生（突然）先知知识的算法相比具有竞争力。在具有概念漂移的合成和真实数据集上的实验证实了我们的理论分析。",
        "领域": "流数据学习、概念漂移检测、自适应学习算法",
        "问题": "解决在流数据学习过程中，概念漂移导致模型性能下降的问题",
        "动机": "提高在概念漂移情况下的模型适应性和检测效率，减少误报",
        "方法": "提出了一种结合稳态和反应态的自适应学习算法，通过积极的漂移检测和快速响应机制优化模型性能",
        "关键词": [
            "概念漂移",
            "自适应学习",
            "漂移检测",
            "稳态学习",
            "反应态学习"
        ],
        "涉及的技术概念": {
            "概念漂移": "数据分布随时间变化的现象，影响模型准确性",
            "稳态/反应态过程": "算法通过稳态进行高检测率的漂移检测，反应态快速响应真实漂移并减少误报",
            "自适应学习算法": "能够根据数据变化自动调整学习策略的算法，适用于流数据学习场景"
        },
        "success": true
    },
    {
        "order": 321,
        "title": "Dropout: Explicit Forms and Capacity Control",
        "html": "https://ICML.cc//virtual/2021/poster/10381",
        "abstract": "We investigate the capacity control provided by dropout in various machine learning problems. First, we study dropout for matrix completion, where it induces a distribution-dependent regularizer that equals the weighted trace-norm of the product of the factors. In deep learning, we show that the distribution-dependent regularizer due to dropout directly controls the Rademacher complexity of the underlying class of deep neural networks. These developments enable us to give concrete generalization error bounds for the dropout algorithm in both matrix completion as well as training deep neural networks.",
        "conference": "ICML",
        "中文标题": "Dropout: 显式形式与容量控制",
        "摘要翻译": "我们研究了dropout在各种机器学习问题中提供的容量控制。首先，我们研究了dropout在矩阵补全中的应用，其中它引入了一个依赖于分布的正则化器，该正则化器等于因子乘积的加权迹范数。在深度学习中，我们展示了由于dropout引入的依赖于分布的正则化器直接控制了深度神经网络底层类的Rademacher复杂性。这些进展使我们能够为dropout算法在矩阵补全以及训练深度神经网络中提供具体的泛化误差界限。",
        "领域": "深度学习正则化、矩阵补全、神经网络训练",
        "问题": "研究dropout在不同机器学习问题中的容量控制能力及其对泛化误差的影响",
        "动机": "探索dropout作为一种正则化方法，如何通过控制模型的容量来防止过拟合，并提高模型的泛化能力",
        "方法": "通过理论分析dropout在矩阵补全和深度神经网络中的应用，研究其引入的分布依赖正则化器对模型Rademacher复杂性和泛化误差的影响",
        "关键词": [
            "dropout",
            "容量控制",
            "矩阵补全",
            "深度神经网络",
            "泛化误差"
        ],
        "涉及的技术概念": {
            "dropout": "一种正则化技术，通过在训练过程中随机忽略一部分神经元来防止神经网络过拟合",
            "Rademacher复杂性": "衡量函数类复杂度的指标，用于分析学习算法的泛化能力",
            "加权迹范数": "在矩阵补全问题中，dropout引入的正则化器，用于控制模型的容量"
        },
        "success": true
    },
    {
        "order": 322,
        "title": "Dual Principal Component Pursuit for Robust Subspace Learning: Theory and Algorithms for a Holistic Approach",
        "html": "https://ICML.cc//virtual/2021/poster/10547",
        "abstract": "The Dual Principal Component Pursuit (DPCP) method has been proposed to robustly recover a subspace of high-relative dimension from corrupted data. Existing analyses and algorithms of DPCP, however, mainly focus on finding a normal to a single hyperplane that contains the inliers. Although these algorithms can be extended to a subspace of higher co-dimension through a recursive approach that sequentially finds a new basis element of the space orthogonal to the subspace,  this procedure is computationally expensive and lacks convergence guarantees. In this paper, we consider a DPCP approach for simultaneously computing the entire basis of the orthogonal complement subspace (we call this a holistic approach) by solving a non-convex non-smooth optimization problem over the Grassmannian. We provide geometric and statistical analyses for the global optimality and prove that it can tolerate as many outliers as the square of the number of inliers, under both noiseless and noisy settings. We then present a Riemannian regularity condition for the  problem, which is then used to prove that a Riemannian subgradient method converges linearly to a neighborhood of the orthogonal subspace with error proportional to the noise level.",
        "conference": "ICML",
        "中文标题": "双主成分追踪用于鲁棒子空间学习的理论与算法：一种整体方法",
        "摘要翻译": "双主成分追踪（DPCP）方法被提出用于从被污染的数据中鲁棒地恢复高相对维度的子空间。然而，现有的DPCP分析和算法主要集中于寻找包含内点的单个超平面的法线。尽管这些算法可以通过递归方法扩展到更高余维的子空间，即顺序寻找与子空间正交的空间的新基元素，但这一过程计算成本高昂且缺乏收敛保证。在本文中，我们考虑了一种DPCP方法，通过解决Grassmann流形上的非凸非光滑优化问题，同时计算正交补子空间的整个基（我们称之为整体方法）。我们为全局最优性提供了几何和统计分析，并证明在无噪声和有噪声设置下，它可以容忍的异常值数量与内点数量的平方相同。然后，我们提出了该问题的Riemannian正则性条件，用于证明Riemannian次梯度方法线性收敛到正交子空间的邻域，误差与噪声水平成比例。",
        "领域": "子空间学习、鲁棒主成分分析、高维数据处理",
        "问题": "如何从被污染的数据中鲁棒地恢复高相对维度的子空间",
        "动机": "现有的DPCP方法在扩展到更高余维的子空间时计算成本高且缺乏收敛保证，需要一种更高效且具有理论保证的方法",
        "方法": "通过解决Grassmann流形上的非凸非光滑优化问题，同时计算正交补子空间的整个基",
        "关键词": [
            "双主成分追踪",
            "鲁棒子空间学习",
            "Grassmann流形",
            "非凸优化",
            "Riemannian优化"
        ],
        "涉及的技术概念": {
            "双主成分追踪（DPCP）": "一种用于从被污染的数据中鲁棒地恢复子空间的方法",
            "Grassmann流形": "用于表示子空间的空间，本文中的优化问题在此流形上定义",
            "Riemannian次梯度方法": "用于解决Grassmann流形上的非光滑优化问题，保证线性收敛到解"
        },
        "success": true
    },
    {
        "order": 323,
        "title": "Dueling Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10441",
        "abstract": "We address the problem of convex optimization with preference (dueling) feedback. Like the traditional optimization objective, the goal is to find the optimal point with the least possible query complexity, however, without the luxury of even a zeroth order feedback. Instead, the learner can only observe a single noisy bit which is win-loss feedback for a pair of queried points based on their function values.\n%\nThe problem is certainly of great practical relevance as in many real-world scenarios, such as recommender systems or learning from customer preferences, where the system feedback is often restricted to just one binary-bit preference information. \n%\nWe consider the problem of online convex optimization (OCO) solely by actively querying $\\{0,1\\}$ noisy-comparison feedback of decision point pairs, with the objective of finding a near-optimal point (function minimizer) with the least possible number of queries. %a very general class of monotonic, non-decreasing transfer functions, and analyze the problem for any $d$-dimensional smooth convex function.\n%\nFor the non-stationary OCO setup, where the underlying convex function may change over time, we prove an impossibility result towards achieving the above objective.\nWe next focus only on the stationary OCO problem, and our main contribution lies in designing a normalized gradient descent based algorithm towards finding a $\\epsilon$-best optimal point. Towards this, our algorithm is shown to yield a convergence rate of $\\tilde O(\\nicefrac{d\\beta}{\\epsilon \\nu^2})$ ($\\nu$ being the noise parameter) when the underlying function is $\\beta$-smooth. Further we show an improved convergence rate of just $\\tilde O(\\nicefrac{d\\beta}{\\alpha \\nu^2} \\log \\frac{1}{\\epsilon})$ when the function is additionally also $\\alpha$-strongly convex.",
        "conference": "ICML",
        "中文标题": "对决凸优化",
        "摘要翻译": "我们解决了带有偏好（对决）反馈的凸优化问题。与传统优化目标类似，目标是以尽可能少的查询复杂度找到最优点，然而，甚至无法获得零阶反馈。相反，学习者只能观察到单个噪声位，这是基于查询点对函数值的胜负反馈。这个问题在许多实际场景中具有极大的相关性，例如推荐系统或从客户偏好中学习，其中系统反馈通常仅限于一个二进制位的偏好信息。我们考虑仅通过主动查询决策点对的{0,1}噪声比较反馈来进行在线凸优化（OCO）的问题，目标是以尽可能少的查询次数找到一个接近最优的点（函数最小化器）。对于非平稳的OCO设置，其中潜在的凸函数可能随时间变化，我们证明了实现上述目标的不可能性结果。接下来，我们仅关注平稳的OCO问题，我们的主要贡献在于设计了一种基于归一化梯度下降的算法，以找到一个ε-最佳最优点。为此，当潜在函数是β-平滑时，我们的算法显示出收敛速度为Õ(dβ/εν²)（ν为噪声参数）。此外，当函数额外也是α-强凸时，我们展示了仅Õ(dβ/αν² log 1/ε)的改进收敛速度。",
        "领域": "在线凸优化、推荐系统、偏好学习",
        "问题": "在仅能获得二进制偏好反馈的情况下，如何高效地进行凸优化",
        "动机": "解决在现实世界场景中，如推荐系统和客户偏好学习，系统反馈仅限于二进制偏好信息时的优化问题",
        "方法": "设计了一种基于归一化梯度下降的算法，以在仅能获得噪声比较反馈的情况下找到接近最优的点",
        "关键词": [
            "凸优化",
            "偏好反馈",
            "在线学习",
            "梯度下降",
            "噪声比较"
        ],
        "涉及的技术概念": {
            "在线凸优化（OCO）": "在仅能获得有限反馈的情况下进行凸优化的框架",
            "归一化梯度下降": "用于在噪声比较反馈下优化目标函数的算法",
            "强凸性": "描述函数凸性强度的属性，影响优化算法的收敛速度"
        },
        "success": true
    },
    {
        "order": 324,
        "title": "Dynamic Balancing for Model Selection in Bandits and RL",
        "html": "https://ICML.cc//virtual/2021/poster/9907",
        "abstract": "We propose a framework for model selection by combining base algorithms in stochastic bandits and reinforcement learning. We require a candidate regret bound for each base algorithm that may or may not hold. We select base algorithms to play in each round using a ``balancing condition'' on the candidate regret bounds. Our approach simultaneously recovers previous worst-case regret bounds, while also obtaining much smaller regret in natural scenarios when some base learners significantly exceed their candidate bounds. Our framework is relevant in many settings, including linear bandits and MDPs with nested function classes, linear bandits with unknown misspecification, and tuning confidence parameters of algorithms such as LinUCB. Moreover, unlike recent efforts in model selection for linear stochastic bandits, our approach can be extended to consider adversarial rather than stochastic contexts.",
        "conference": "ICML",
        "中文标题": "强盗算法与强化学习中模型选择的动态平衡",
        "摘要翻译": "我们提出了一个框架，通过结合随机强盗算法和强化学习中的基础算法来进行模型选择。我们要求每个基础算法有一个候选的遗憾界限，这个界限可能成立也可能不成立。我们使用候选遗憾界限上的‘平衡条件’来选择在每一轮中要使用的基础算法。我们的方法同时恢复了之前的最坏情况遗憾界限，同时在一些基础学习者显著超过其候选界限的自然场景中获得了更小的遗憾。我们的框架适用于许多设置，包括具有嵌套函数类的线性强盗算法和MDPs、具有未知错误指定的线性强盗算法，以及调整如LinUCB等算法的置信参数。此外，与最近在线性随机强盗算法的模型选择中的努力不同，我们的方法可以扩展到考虑对抗性而非随机性的上下文。",
        "领域": "强化学习、线性强盗算法、模型选择",
        "问题": "如何在强盗算法和强化学习中有效地进行模型选择，以在不同场景下实现最优的遗憾界限。",
        "动机": "为了解决在强盗算法和强化学习中模型选择的问题，特别是在基础算法的性能超出预期时如何减少遗憾。",
        "方法": "提出了一种基于候选遗憾界限的‘平衡条件’来选择基础算法的框架，适用于多种设置，并能适应对抗性上下文。",
        "关键词": [
            "模型选择",
            "强盗算法",
            "强化学习",
            "遗憾界限",
            "动态平衡"
        ],
        "涉及的技术概念": {
            "候选遗憾界限": "每个基础算法预设的性能界限，用于模型选择和平衡。",
            "平衡条件": "用于在每一轮选择基础算法的条件，旨在优化整体遗憾。",
            "对抗性上下文": "与随机性上下文相对，指算法需要在对抗性环境中进行模型选择和优化。"
        },
        "success": true
    },
    {
        "order": 325,
        "title": "Dynamic Game Theoretic Neural Optimizer",
        "html": "https://ICML.cc//virtual/2021/poster/10261",
        "abstract": "The connection between training deep neural networks (DNNs) and optimal control theory (OCT) has attracted considerable attention as a principled tool of algorithmic design. Despite few attempts being made, they have been limited to architectures where the layer propagation resembles a Markovian dynamical system. This casts doubts on their flexibility to modern networks that heavily rely on non-Markovian dependencies between layers (e.g. skip connections in residual networks). In this work, we propose a novel dynamic game perspective by viewing each layer as a player in a dynamic game characterized by the DNN itself. Through this lens, different classes of optimizers can be seen as matching different types of Nash equilibria, depending on the implicit information structure of each (p)layer. The resulting method, called Dynamic Game Theoretic Neural Optimizer (DGNOpt), not only generalizes OCT-inspired optimizers to richer network class; it also motivates a new training principle by solving a multi-player cooperative game. DGNOpt shows convergence improvements over existing methods on image classification datasets with residual and inception networks. Our work marries strengths from both OCT and game theory, paving ways to new algorithmic opportunities from robust optimal control and bandit-based optimization.",
        "conference": "ICML",
        "中文标题": "动态博弈论神经优化器",
        "摘要翻译": "深度神经网络（DNNs）训练与最优控制理论（OCT）之间的联系作为算法设计的原理工具已引起广泛关注。尽管已有少数尝试，但这些尝试仅限于层传播类似于马尔可夫动态系统的架构。这让人怀疑它们对严重依赖层间非马尔可夫依赖性的现代网络（如残差网络中的跳跃连接）的灵活性。在这项工作中，我们提出了一种新颖的动态博弈视角，将每一层视为由DNN本身表征的动态博弈中的玩家。通过这一视角，不同类型的优化器可以被视为匹配不同类型的纳什均衡，取决于每个（层）玩家的隐式信息结构。由此产生的方法，称为动态博弈论神经优化器（DGNOpt），不仅将受OCT启发的优化器推广到更丰富的网络类别；它还通过解决多玩家合作博弈激发了一种新的训练原则。DGNOpt在具有残差和初始网络的图像分类数据集上显示出对现有方法的收敛性改进。我们的工作结合了OCT和博弈论的优点，为来自鲁棒最优控制和基于强盗的优化的新算法机会铺平了道路。",
        "领域": "深度学习优化、神经网络训练、最优控制理论应用",
        "问题": "如何将最优控制理论和博弈论应用于深度神经网络的训练过程中，以提升训练效率和模型性能",
        "动机": "探索深度神经网络训练与最优控制理论之间的联系，并利用博弈论视角解决现代网络中非马尔可夫依赖性问题",
        "方法": "提出动态博弈论神经优化器（DGNOpt），将每一层视为动态博弈中的玩家，通过匹配不同类型的纳什均衡来优化网络训练",
        "关键词": [
            "动态博弈论",
            "神经优化器",
            "最优控制理论",
            "深度神经网络训练",
            "纳什均衡"
        ],
        "涉及的技术概念": {
            "动态博弈论": "用于将每一层神经网络视为博弈中的玩家，通过博弈论视角优化网络训练",
            "最优控制理论": "作为算法设计的原理工具，与深度神经网络训练相结合，提升训练效率",
            "纳什均衡": "用于匹配不同类型的优化器，取决于每个层玩家的隐式信息结构，优化网络训练过程"
        },
        "success": true
    },
    {
        "order": 326,
        "title": "Dynamic Planning and Learning under Recovering Rewards",
        "html": "https://ICML.cc//virtual/2021/poster/10681",
        "abstract": "Motivated by emerging applications such as live-streaming e-commerce, promotions and recommendations, we introduce a general class of multi-armed bandit problems that have the following two features: (i) the decision maker can pull and collect rewards from at most $K$ out of $N$ different arms in each time period; (ii) the expected reward of an arm immediately drops after it is pulled, and then non-parametrically recovers as the idle time increases. With the objective of maximizing expected cumulative rewards over $T$ time periods, we propose, construct and prove performance guarantees for a class of ``Purely Periodic Policies''. For the offline problem when all model parameters are known, our proposed policy obtains an approximation ratio that is at the order of $1-\\mathcal O(1/\\sqrt{K})$, which is asymptotically optimal when $K$ grows to infinity. For the online problem when the model parameters are unknown and need to be learned, we design an Upper Confidence Bound (UCB) based policy that approximately has $\\widetilde\\mathcal O(N\\sqrt{T})$ regret against the offline benchmark. Our framework and policy design may have the potential to be adapted into other offline planning and online learning applications with non-stationary and recovering rewards.",
        "conference": "ICML",
        "success": true,
        "中文标题": "恢复奖励下的动态规划与学习",
        "摘要翻译": "受直播电商、促销和推荐等新兴应用的启发，我们引入了一类具有以下两个特征的多臂老虎机问题：（i）决策者在每个时间段最多可以从N个不同的臂中拉动并收集K个臂的奖励；（ii）臂的期望奖励在被拉动后立即下降，然后随着空闲时间的增加非参数地恢复。以在T个时间段内最大化期望累积奖励为目标，我们提出、构建并证明了一类‘纯周期性策略’的性能保证。对于所有模型参数已知的离线问题，我们提出的策略获得的近似比在1−O(1/√K)的量级，当K增长到无穷大时，这是渐近最优的。对于模型参数未知且需要学习的在线问题，我们设计了一个基于上置信界（UCB）的策略，相对于离线基准，其遗憾大约为Õ(N√T)。我们的框架和策略设计有可能适应于其他具有非平稳和恢复奖励的离线规划和在线学习应用。",
        "领域": "强化学习, 多臂老虎机问题, 在线学习",
        "问题": "如何在奖励非平稳且随时间恢复的多臂老虎机问题中，设计有效的策略以最大化累积奖励。",
        "动机": "受直播电商、促销和推荐等新兴应用的启发，研究在奖励非平稳且随时间恢复的环境下，如何有效规划和学习的策略。",
        "方法": "提出了一类‘纯周期性策略’，对于离线问题提供近似最优解，对于在线问题设计了基于上置信界（UCB）的策略以减少遗憾。",
        "关键词": [
            "多臂老虎机",
            "非平稳奖励",
            "纯周期性策略",
            "上置信界",
            "在线学习"
        ],
        "涉及的技术概念": {
            "多臂老虎机问题": "论文研究的核心问题框架，涉及在多个选择中做出决策以最大化累积奖励。",
            "纯周期性策略": "论文提出的策略类别，用于在奖励恢复的环境下进行有效的规划和决策。",
            "上置信界（UCB）": "用于在线学习中平衡探索与利用的策略，以减少未知模型参数带来的遗憾。"
        }
    },
    {
        "order": 327,
        "title": "Efficient Deviation Types and Learning for Hindsight Rationality in Extensive-Form Games",
        "html": "https://ICML.cc//virtual/2021/poster/9549",
        "abstract": "Hindsight rationality is an approach to playing general-sum games that prescribes no-regret learning dynamics for individual agents with respect to a set of deviations, and further describes jointly rational behavior among multiple agents with mediated equilibria. To develop hindsight rational learning in sequential decision-making settings, we formalize behavioral deviations as a general class of deviations that respect the structure of extensive-form games. Integrating the idea of time selection into counterfactual regret minimization (CFR), we introduce the extensive-form regret minimization (EFR) algorithm that achieves hindsight rationality for any given set of behavioral deviations with computation that scales closely with the complexity of the set. We identify behavioral deviation subsets, the partial sequence deviation types, that subsume previously studied types and lead to efficient EFR instances in games with moderate lengths. In addition, we present a thorough empirical analysis of EFR instantiated with different deviation types in benchmark games, where we find that stronger types typically induce better performance.",
        "conference": "ICML",
        "中文标题": "扩展形式博弈中高效偏差类型与后见之明理性的学习",
        "摘要翻译": "后见之明理性是一种在一般和博弈中指导个体代理针对一组偏差采用无遗憾学习动态的方法，并进一步通过调解均衡描述了多代理之间的联合理性行为。为了在序列决策制定环境中发展后见之明理性学习，我们将行为偏差形式化为尊重扩展形式博弈结构的一类通用偏差。将时间选择的概念整合到反事实遗憾最小化（CFR）中，我们引入了扩展形式遗憾最小化（EFR）算法，该算法对任何给定的行为偏差集实现后见之明理性，其计算复杂度与偏差集的复杂度紧密相关。我们识别了行为偏差子集，即部分序列偏差类型，这些子集包含了先前研究的类型，并在长度适中的博弈中导致高效的EFR实例。此外，我们在基准博弈中对使用不同偏差类型实例化的EFR进行了彻底的实证分析，发现更强的类型通常能诱导更好的性能。",
        "领域": "博弈论与机器学习结合、多智能体系统、序列决策制定",
        "问题": "如何在扩展形式博弈中实现高效的后见之明理性学习，特别是在考虑不同类型的行为偏差时。",
        "动机": "研究旨在开发一种方法，使个体代理在序列决策制定环境中能够有效地学习和适应，以实现后见之明理性，同时考虑到博弈的结构和可能的偏差类型。",
        "方法": "通过将时间选择整合到反事实遗憾最小化（CFR）中，提出了扩展形式遗憾最小化（EFR）算法，该算法能够针对任何给定的行为偏差集实现后见之明理性，并识别了导致高效EFR实例的行为偏差子集。",
        "关键词": [
            "后见之明理性",
            "扩展形式博弈",
            "行为偏差",
            "EFR算法",
            "序列决策"
        ],
        "涉及的技术概念": {
            "后见之明理性": "一种在博弈中指导个体代理采用无遗憾学习动态的方法，旨在实现个体和联合理性行为。",
            "扩展形式遗憾最小化（EFR）算法": "一种新提出的算法，通过整合时间选择和反事实遗憾最小化，实现对任何给定行为偏差集的后见之明理性。",
            "行为偏差": "在扩展形式博弈中，尊重博弈结构的一类通用偏差，用于形式化代理的可能行为偏离。"
        },
        "success": true
    },
    {
        "order": 328,
        "title": "Efficient Differentiable Simulation of Articulated Bodies",
        "html": "https://ICML.cc//virtual/2021/poster/9049",
        "abstract": "We present a method for efficient differentiable simulation of articulated bodies. This enables integration of articulated body dynamics into deep learning frameworks, and gradient-based optimization of neural networks that operate on articulated bodies. We derive the gradients of the contact solver using spatial algebra and the adjoint method. Our approach is an order of magnitude faster than autodiff tools. By only saving the initial states throughout the simulation process, our method reduces memory requirements by two orders of magnitude. We demonstrate the utility of efficient differentiable dynamics for articulated bodies in a variety of applications. We show that reinforcement learning with articulated systems can be accelerated using gradients provided by our method. In applications to control and inverse problems, gradient-based optimization enabled by our work accelerates convergence by more than an order of magnitude.",
        "conference": "ICML",
        "中文标题": "高效可微分铰接体模拟",
        "摘要翻译": "我们提出了一种高效的可微分铰接体模拟方法。这使得铰接体动力学能够被整合到深度学习框架中，并支持基于梯度的神经网络优化，这些网络操作于铰接体上。我们利用空间代数和伴随方法推导了接触求解器的梯度。我们的方法比自动微分工具快一个数量级。通过在整个模拟过程中仅保存初始状态，我们的方法将内存需求降低了两个数量级。我们展示了高效可微分动力学在铰接体上的多种应用效用。我们证明，使用我们方法提供的梯度可以加速铰接系统的强化学习。在控制和逆问题的应用中，我们的工作所支持的基于梯度的优化使收敛速度加快了一个数量级以上。",
        "领域": "物理模拟与深度学习结合、机器人控制、强化学习",
        "问题": "如何高效地进行铰接体的可微分模拟，以支持深度学习框架中的梯度优化",
        "动机": "为了将铰接体动力学整合到深度学习框架中，并加速基于梯度的优化过程",
        "方法": "利用空间代数和伴随方法推导接触求解器的梯度，优化内存使用和计算效率",
        "关键词": [
            "可微分模拟",
            "铰接体",
            "深度学习",
            "梯度优化",
            "强化学习"
        ],
        "涉及的技术概念": {
            "空间代数": "用于推导接触求解器的梯度，是处理铰接体动力学的数学工具",
            "伴随方法": "用于高效计算梯度，支持基于梯度的优化",
            "可微分模拟": "允许模拟过程中的梯度计算，使得模拟结果可以直接用于神经网络的训练和优化"
        },
        "success": true
    },
    {
        "order": 329,
        "title": "Efficient Generative Modelling of Protein Structure Fragments using a Deep Markov Model",
        "html": "https://ICML.cc//virtual/2021/poster/8501",
        "abstract": "Fragment libraries are often used in protein structure prediction, simulation and design as a means to significantly reduce the vast conformational search space.\nCurrent state-of-the-art methods for fragment library generation do not properly account for aleatory and epistemic uncertainty, respectively due to the dynamic nature of proteins and experimental errors in protein structures. \nAdditionally, they typically rely on information that is not generally or readily available, such as homologous sequences, related protein structures and other complementary information.\nTo address these issues, we developed BIFROST, a novel take on the fragment library problem based on a Deep Markov Model architecture combined with directional statistics for angular degrees of freedom, implemented in the deep probabilistic programming language Pyro.\nBIFROST is a probabilistic, generative model of the protein backbone dihedral angles conditioned solely on the amino acid sequence.\nBIFROST generates fragment libraries with a quality on par with current state-of-the-art methods at a fraction of the run-time, while requiring considerably less information and allowing efficient evaluation of probabilities.",
        "conference": "ICML",
        "中文标题": "使用深度马尔可夫模型高效生成蛋白质结构片段的建模",
        "摘要翻译": "片段库常用于蛋白质结构预测、模拟和设计，作为一种显著减少庞大构象搜索空间的手段。当前最先进的片段库生成方法未能充分考虑由于蛋白质的动态性质和蛋白质结构中的实验误差所导致的偶然性和认知不确定性。此外，这些方法通常依赖于通常不易获得的信息，如同源序列、相关蛋白质结构和其他补充信息。为了解决这些问题，我们开发了BIFROST，这是一种基于深度马尔可夫模型架构结合方向统计用于角度自由度的新方法，使用深度概率编程语言Pyro实现。BIFROST是一个仅以氨基酸序列为条件的蛋白质骨架二面角的概率生成模型。BIFROST生成的片段库质量与当前最先进方法相当，但运行时间大幅减少，同时需要的信息量显著减少，并允许高效评估概率。",
        "领域": "蛋白质结构预测、生物信息学、深度学习",
        "问题": "当前片段库生成方法未能充分考虑蛋白质动态性质和实验误差带来的不确定性，且依赖不易获得的信息。",
        "动机": "开发一种更高效、信息需求更少的蛋白质片段库生成方法，以克服现有方法的局限性。",
        "方法": "基于深度马尔可夫模型架构结合方向统计用于角度自由度，使用深度概率编程语言Pyro实现。",
        "关键词": [
            "蛋白质结构预测",
            "深度马尔可夫模型",
            "片段库生成",
            "方向统计",
            "Pyro"
        ],
        "涉及的技术概念": {
            "深度马尔可夫模型": "用于建模蛋白质骨架二面角的概率分布，作为生成片段库的基础架构。",
            "方向统计": "用于处理蛋白质骨架中的角度自由度，确保生成的片段在物理上是合理的。",
            "Pyro": "深度概率编程语言，用于实现和训练深度马尔可夫模型，支持高效的模型训练和概率评估。"
        },
        "success": true
    },
    {
        "order": 330,
        "title": "Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-Object Representations",
        "html": "https://ICML.cc//virtual/2021/poster/9403",
        "abstract": "Unsupervised multi-object representation learning depends on inductive biases to guide the discovery of object-centric representations that generalize. However, we observe that methods for learning these representations are either impractical due to long training times and large memory consumption or forego key inductive biases. In this work, we introduce EfficientMORL, an efficient framework for the unsupervised learning of object-centric representations. We show that optimization challenges caused by requiring both symmetry and disentanglement can in fact be addressed by high-cost iterative amortized inference by designing the framework to minimize its dependence on it. We take a two-stage approach to inference: first, a hierarchical variational autoencoder extracts symmetric and disentangled representations through bottom-up inference, and second, a lightweight network refines the representations with top-down feedback. The number of refinement steps taken during training is reduced following a curriculum, so that at test time with zero steps the model achieves 99.1% of the refined decomposition performance. We demonstrate strong object decomposition and disentanglement on the standard multi-object benchmark while achieving nearly an order of magnitude faster training and test time inference over the previous state-of-the-art model.",
        "conference": "ICML",
        "中文标题": "高效迭代摊销推理学习对称与解缠的多对象表示",
        "摘要翻译": "无监督的多对象表示学习依赖于归纳偏置来指导发现能够泛化的对象中心表示。然而，我们观察到，学习这些表示的方法要么由于训练时间长和内存消耗大而不切实际，要么放弃了关键的归纳偏置。在这项工作中，我们介绍了EfficientMORL，一个用于无监督学习对象中心表示的高效框架。我们展示了，通过设计框架以最小化对高成本迭代摊销推理的依赖，可以解决由要求对称性和解缠性引起的优化挑战。我们采用两阶段推理方法：首先，一个分层变分自编码器通过自下而上的推理提取对称和解缠的表示；其次，一个轻量级网络通过自上而下的反馈精炼表示。训练期间采取的细化步骤数量按照课程减少，因此在测试时零步骤的情况下，模型达到了细化分解性能的99.1%。我们在标准的多对象基准上展示了强大的对象分解和解缠能力，同时实现了比之前最先进模型快近一个数量级的训练和测试时间推理。",
        "领域": "无监督学习、对象中心表示学习、变分自编码器",
        "问题": "解决无监督多对象表示学习中的训练时间长、内存消耗大以及关键归纳偏置缺失的问题",
        "动机": "开发一个高效框架，以无监督方式学习对象中心表示，同时保持对称性和解缠性，减少对高成本迭代摊销推理的依赖",
        "方法": "采用两阶段推理方法，结合分层变分自编码器和轻量级网络，通过课程学习减少细化步骤",
        "关键词": [
            "无监督学习",
            "对象中心表示",
            "变分自编码器",
            "对称性",
            "解缠性"
        ],
        "涉及的技术概念": {
            "分层变分自编码器": "用于通过自下而上的推理提取对称和解缠的表示",
            "轻量级网络": "通过自上而下的反馈精炼表示",
            "课程学习": "训练期间逐步减少细化步骤数量，以提高效率"
        },
        "success": true
    },
    {
        "order": 331,
        "title": "Efficient Lottery Ticket Finding: Less Data is More",
        "html": "https://ICML.cc//virtual/2021/poster/10505",
        "abstract": "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.",
        "conference": "ICML",
        "中文标题": "高效彩票券发现：数据越少效果越好",
        "摘要翻译": "彩票券假设（LTH）揭示了密集网络中存在获胜券（稀疏但关键的子网络），这些子网络可以从随机初始化开始独立训练，以达到与密集网络相同的准确度。然而，发现获胜券需要在训练-剪枝-再训练的过程中进行繁重的计算，尤其是在大规模数据集（如ImageNet）上，这限制了它们的实际效益。本文探索了一种新的视角，通过仅使用一个特别选择的数据子集（称为剪枝感知关键集，PrAC集），而不是使用完整的训练集，来更高效地发现彩票券。PrAC集的概念受到最近观察的启发，即深度网络有一些样本在训练过程中难以记忆，或在剪枝过程中容易被遗忘。因此，假设PrAC集能够捕捉到对密集模型最具挑战性和信息量的例子。我们观察到，通过在非常紧凑的PrAC集上训练和剪枝密集网络，可以找到高质量的获胜券，这可以大大节省彩票券发现过程的训练迭代次数。大量实验验证了我们的提议在多种数据集和网络架构上的有效性。具体来说，在CIFAR-10、CIFAR-100和Tiny ImageNet上，我们在其训练集大小的35.32%~78.19%范围内定位到了有效的PrAC集。在此基础上，我们可以为相应的密集网络获得同样具有竞争力的获胜券，同时分别节省高达82.85%~92.77%、63.54%~74.92%和76.14%~86.56%的训练迭代次数。关键的是，我们发现一个PrAC集可以在不同的网络架构中重复使用，这可以分摊寻找PrAC集的额外成本，为高效彩票券发现提供了一种实用的方案。",
        "领域": "神经网络剪枝",
        "问题": "如何在减少计算负担的同时高效发现神经网络中的获胜券（即关键的稀疏子网络）",
        "动机": "减少发现获胜券所需的计算资源，提高彩票券假设在实际应用中的可行性",
        "方法": "通过使用特别选择的数据子集（PrAC集）来训练和剪枝密集网络，以高效发现获胜券",
        "关键词": [
            "彩票券假设",
            "神经网络剪枝",
            "PrAC集",
            "高效训练",
            "稀疏子网络"
        ],
        "涉及的技术概念": {
            "彩票券假设": "揭示了密集网络中存在可以通过独立训练达到相同准确度的稀疏子网络",
            "PrAC集": "一个特别选择的数据子集，用于高效发现获胜券，包含对密集模型最具挑战性和信息量的例子",
            "神经网络剪枝": "通过移除网络中不重要的连接来减少模型大小和计算负担的技术"
        },
        "success": true
    },
    {
        "order": 332,
        "title": "Efficient Message Passing for 0–1 ILPs with Binary Decision Diagrams",
        "html": "https://ICML.cc//virtual/2021/poster/10673",
        "abstract": "We present a message passing method for 0–1 integer linear programs. Our algorithm is based on a decomposition of the original problem into subproblems that are represented as binary deci- sion diagrams. The resulting Lagrangean dual is solved iteratively by a series of efficient block coordinate ascent steps. Our method has linear iteration complexity in the size of the decomposi- tion and can be effectively parallelized. The char- acteristics of our approach are desirable towards solving ever larger problems arising in structured prediction. We present experimental results on combinatorial problems from MAP inference for Markov Random Fields, quadratic assignment, discrete tomography and cell tracking for develop- mental biology and show promising performance.",
        "conference": "ICML",
        "中文标题": "基于二元决策图的高效消息传递方法解决0-1整数线性规划问题",
        "摘要翻译": "我们提出了一种用于0-1整数线性规划问题的消息传递方法。我们的算法基于将原问题分解为表示为二元决策图的子问题。通过一系列高效的块坐标上升步骤迭代求解得到的拉格朗日对偶问题。我们的方法在分解规模上具有线性迭代复杂度，并且可以有效地并行化。我们方法的特性对于解决结构化预测中出现的越来越大的问题是非常理想的。我们在马尔可夫随机场的MAP推理、二次分配、离散断层扫描和发育生物学中的细胞跟踪等组合问题上展示了实验结果，并显示出良好的性能。",
        "领域": "组合优化, 结构化预测, 并行计算",
        "问题": "解决0-1整数线性规划问题的高效算法设计",
        "动机": "为了更高效地解决结构化预测中出现的越来越大的0-1整数线性规划问题",
        "方法": "基于二元决策图的分解和拉格朗日对偶的迭代求解方法",
        "关键词": [
            "消息传递",
            "二元决策图",
            "拉格朗日对偶",
            "并行计算",
            "结构化预测"
        ],
        "涉及的技术概念": {
            "二元决策图": "用于表示和分解0-1整数线性规划问题的子问题",
            "拉格朗日对偶": "通过迭代求解拉格朗日对偶问题来优化原问题",
            "块坐标上升": "一种高效的优化技术，用于迭代求解分解后的子问题"
        },
        "success": true
    },
    {
        "order": 333,
        "title": "EfficientNetV2: Smaller Models and Faster Training",
        "html": "https://ICML.cc//virtual/2021/poster/8849",
        "abstract": "This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose an improved method of progressive learning, which adaptively adjusts regularization (e.g. data augmentation) along with image size.  With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources.",
        "conference": "ICML",
        "中文标题": "EfficientNetV2：更小的模型与更快的训练",
        "摘要翻译": "本文介绍了EfficientNetV2，这是一个新的卷积网络家族，相比之前的模型，它们具有更快的训练速度和更好的参数效率。为了开发这些模型，我们结合了训练感知的神经架构搜索和缩放技术，共同优化训练速度和参数效率。这些模型是从一个丰富了新操作（如Fused-MBConv）的搜索空间中搜索出来的。我们的实验表明，EfficientNetV2模型的训练速度比最先进的模型快得多，同时模型大小最多可缩小6.8倍。通过逐步增加训练过程中的图像大小，可以进一步加快我们的训练速度，但这通常会导致准确率下降。为了补偿这种准确率下降，我们提出了一种改进的渐进学习方法，该方法随着图像大小的变化自适应地调整正则化（例如数据增强）。通过渐进学习，我们的EfficientNetV2在ImageNet和CIFAR/Cars/Flowers数据集上显著优于之前的模型。通过在相同的ImageNet21k上进行预训练，我们的EfficientNetV2在ImageNet ILSVRC2012上达到了87.3%的top-1准确率，比最近的ViT高出2.0%的准确率，同时使用相同的计算资源训练速度提高了5倍到11倍。",
        "领域": "卷积神经网络优化",
        "问题": "如何在保持或提高模型准确率的同时，减少模型大小和加快训练速度",
        "动机": "为了解决现有卷积神经网络在训练速度和参数效率上的不足，开发更高效的模型",
        "方法": "结合训练感知的神经架构搜索和缩放技术，以及改进的渐进学习方法，共同优化训练速度和参数效率",
        "关键词": [
            "EfficientNetV2",
            "神经架构搜索",
            "渐进学习",
            "参数效率",
            "训练速度"
        ],
        "涉及的技术概念": {
            "训练感知的神经架构搜索": "用于共同优化训练速度和参数效率的技术",
            "Fused-MBConv": "一种新的操作，用于丰富搜索空间，提高模型效率",
            "渐进学习": "一种改进的训练方法，通过自适应调整正则化来补偿因图像大小增加而导致的准确率下降"
        },
        "success": true
    },
    {
        "order": 334,
        "title": "Efficient Online Learning for Dynamic k-Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/8755",
        "abstract": "In this work, we study dynamic clustering problems from the perspective of online learning. We consider an online learning problem, called \\textit{Dynamic $k$-Clustering}, in which $k$ centers are maintained in a metric space over time (centers may change positions) such as a dynamically changing set of $r$ clients is served in the best possible way. The connection cost at round  $t$ is given by the \\textit{$p$-norm} of the vector formed by the distance of each client to its closest center at round $t$, for some $p\\geq 1$. We design a \\textit{$\\Theta\\left( \\min(k,r) \\right)$-regret} polynomial-time online learning algorithm, while we show that, under some well-established computational complexity conjectures, \\textit{constant-regret} cannot be achieved in polynomial-time. In addition to the efficient solution of Dynamic $k$-Clustering, our work contributes to the long line of research of combinatorial online learning.",
        "conference": "ICML",
        "中文标题": "动态k聚类的高效在线学习",
        "摘要翻译": "在这项工作中，我们从在线学习的角度研究了动态聚类问题。我们考虑了一个称为\textit{动态k聚类}的在线学习问题，其中在度量空间中随时间维护k个中心（中心可能改变位置），以便以最佳方式服务于动态变化的r个客户端集合。第t轮连接成本由第t轮每个客户端到其最近中心距离形成的向量的\textit{p范数}给出，对于某些p≥1。我们设计了一个\textit{Θ(min(k,r))-遗憾}多项式时间在线学习算法，同时我们表明，在一些公认的计算复杂性猜想下，\textit{常数遗憾}不能在多项式时间内实现。除了动态k聚类的有效解决方案外，我们的工作还对组合在线学习的长线研究做出了贡献。",
        "领域": "在线学习、动态聚类、组合优化",
        "问题": "如何在动态变化的环境中高效地进行k聚类，以最小化连接成本。",
        "动机": "研究动态聚类问题，旨在在动态变化的环境中，通过在线学习的方法，有效地维护聚类中心，以优化服务成本。",
        "方法": "设计了一个基于p范数的多项式时间在线学习算法，用于动态k聚类问题，并分析了其遗憾界限。",
        "关键词": [
            "动态聚类",
            "在线学习",
            "p范数",
            "遗憾最小化",
            "组合优化"
        ],
        "涉及的技术概念": {
            "动态k聚类": "在动态变化的环境中维护k个聚类中心，以优化服务成本。",
            "p范数": "用于计算连接成本的数学工具，衡量客户端到最近中心距离的向量范数。",
            "遗憾最小化": "在线学习算法的性能度量，旨在最小化算法性能与最佳固定策略之间的差距。"
        },
        "success": true
    },
    {
        "order": 335,
        "title": "Efficient Performance Bounds for Primal-Dual Reinforcement Learning from Demonstrations",
        "html": "https://ICML.cc//virtual/2021/poster/9901",
        "abstract": "We consider large-scale Markov decision processes with an unknown cost function and address the problem of learning a policy from a finite set of expert demonstrations.\n\t\t\tWe assume that the learner is not allowed to interact with the expert and has no access to reinforcement signal of any kind.\n\t\t\tExisting inverse reinforcement learning methods come with strong theoretical guarantees, but are computationally expensive, while state-of-the-art policy optimization algorithms achieve significant empirical success, but are hampered by limited theoretical understanding.\n\t\t\tTo bridge the gap between theory and practice, we introduce a novel bilinear saddle-point framework using Lagrangian duality.\n\t\t\tThe proposed primal-dual viewpoint allows us to develop a model-free provably efficient algorithm through the lens of stochastic convex optimization. The method enjoys the advantages of simplicity of implementation, low memory requirements, and computational and sample complexities independent of the number of states. We further present an equivalent no-regret online-learning interpretation.",
        "conference": "ICML",
        "中文标题": "基于原始对偶的演示强化学习高效性能界限",
        "摘要翻译": "我们考虑具有未知成本函数的大规模马尔可夫决策过程，并解决从有限专家演示中学习策略的问题。我们假设学习者不允许与专家互动，也无法获得任何形式的强化信号。现有的逆强化学习方法虽然具有强大的理论保证，但计算成本高昂；而最先进的策略优化算法虽然在实证上取得了显著成功，但受限于理论理解的不足。为了弥合理论与实践之间的差距，我们引入了一种新颖的使用拉格朗日对偶的双线性鞍点框架。提出的原始对偶视角使我们能够通过随机凸优化的视角开发出一种模型无关的、可证明高效的算法。该方法具有实现简单、内存需求低、计算和样本复杂度与状态数量无关的优点。我们进一步提出了一种等效的无悔在线学习解释。",
        "领域": "逆强化学习、策略优化、马尔可夫决策过程",
        "问题": "从有限的专家演示中学习策略，而不需要与专家互动或获得强化信号。",
        "动机": "现有逆强化学习方法计算成本高，而策略优化算法缺乏理论支持，需要一种既高效又有理论保证的方法。",
        "方法": "引入基于拉格朗日对偶的双线性鞍点框架，开发模型无关的高效算法。",
        "关键词": [
            "原始对偶学习",
            "逆强化学习",
            "策略优化",
            "马尔可夫决策过程",
            "随机凸优化"
        ],
        "涉及的技术概念": {
            "拉格朗日对偶": "用于构建双线性鞍点框架，以开发高效算法。",
            "随机凸优化": "作为开发算法的理论基础，确保算法的高效性和可证明性。",
            "无悔在线学习": "提供算法的另一种解释，强调其在在线学习环境下的性能保证。"
        },
        "success": true
    },
    {
        "order": 336,
        "title": "Efficient Statistical Tests: A Neural Tangent Kernel Approach",
        "html": "https://ICML.cc//virtual/2021/poster/9791",
        "abstract": "For machine learning models to make reliable predictions in deployment, one needs to ensure the previously unknown test samples need to be sufficiently similar to the training data.  The commonly used shift-invariant kernels do not have the compositionality and fail to capture invariances in high-dimensional data in computer vision.  We propose a shift-invariant convolutional neural tangent kernel (SCNTK) based outlier detector and two-sample tests with maximum mean discrepancy (MMD) that is O(n) in the number of samples due to using the random feature approximation.  On MNIST and CIFAR10 with various types of dataset shifts, we empirically show that statistical tests with such compositional kernels,  inherited from infinitely wide neural networks, achieve higher detection accuracy than existing non-parametric methods. Our method also provides a competitive alternative to adapted kernel methods that require a training phase.",
        "conference": "ICML",
        "中文标题": "高效统计测试：一种神经正切核方法",
        "摘要翻译": "为了使机器学习模型在部署时做出可靠的预测，需要确保先前未知的测试样本与训练数据足够相似。常用的平移不变核缺乏组合性，无法捕捉高维计算机视觉数据中的不变性。我们提出了一种基于平移不变卷积神经正切核（SCNTK）的异常检测器和最大均值差异（MMD）的两样本测试，由于使用了随机特征近似，其样本数量为O(n)。在MNIST和CIFAR10数据集上，针对各种类型的数据集偏移，我们实证表明，这种继承自无限宽神经网络的组合核的统计测试，比现有的非参数方法具有更高的检测准确率。我们的方法也为需要训练阶段的适应核方法提供了一个有竞争力的替代方案。",
        "领域": "异常检测、统计测试、计算机视觉",
        "问题": "解决机器学习模型在部署时对未知测试样本的可靠性预测问题，以及高维数据中不变性的捕捉问题。",
        "动机": "提高机器学习模型在未知测试样本上的预测可靠性，以及在高维数据中更有效地捕捉不变性。",
        "方法": "提出了一种基于平移不变卷积神经正切核（SCNTK）的异常检测器和最大均值差异（MMD）的两样本测试，利用随机特征近似实现样本数量为O(n)的高效计算。",
        "关键词": [
            "神经正切核",
            "异常检测",
            "最大均值差异",
            "随机特征近似",
            "数据集偏移"
        ],
        "涉及的技术概念": {
            "神经正切核": "用于构建高效统计测试的核方法，继承自无限宽神经网络，能够捕捉高维数据中的不变性。",
            "最大均值差异": "用于两样本测试的统计方法，衡量两个分布之间的差异。",
            "随机特征近似": "一种技术手段，用于降低计算复杂度，使得样本数量为O(n)的高效计算成为可能。"
        },
        "success": true
    },
    {
        "order": 337,
        "title": "Efficient Training of Robust Decision Trees Against Adversarial Examples",
        "html": "https://ICML.cc//virtual/2021/poster/10209",
        "abstract": "Current state-of-the-art algorithms for training robust decision trees have high runtime costs and require hours to run. We present GROOT, an efficient algorithm for training robust decision trees and random forests that runs in a matter of seconds to minutes. Where before the worst-case Gini impurity was computed iteratively, we find that we can solve this function analytically to improve time complexity from O(n) to O(1) in terms of n samples. Our results on both single trees and ensembles on 14 structured datasets as well as on MNIST and Fashion-MNIST demonstrate that GROOT runs several orders of magnitude faster than the state-of-the-art works and also shows better performance in terms of adversarial accuracy on structured data.",
        "conference": "ICML",
        "中文标题": "高效训练对抗样本鲁棒决策树",
        "摘要翻译": "当前训练鲁棒决策树的最先进算法运行时成本高，需要数小时才能完成。我们提出了GROOT，一种高效训练鲁棒决策树和随机森林的算法，运行时间仅需几秒到几分钟。之前最坏情况下的基尼不纯度是通过迭代计算的，而我们发现可以解析地求解这个函数，从而将时间复杂度从O(n)提高到O(1)，其中n为样本数。我们在14个结构化数据集以及MNIST和Fashion-MNIST上对单棵树和集成树的结果表明，GROOT的运行速度比最先进的工作快几个数量级，并且在结构化数据上的对抗精度也表现更好。",
        "领域": "对抗性机器学习、决策树算法、随机森林",
        "问题": "提高训练鲁棒决策树和随机森林的效率，减少运行时间",
        "动机": "现有的鲁棒决策树训练算法运行时间过长，限制了其在实际应用中的使用",
        "方法": "通过解析求解最坏情况下的基尼不纯度函数，显著提高训练效率",
        "关键词": [
            "鲁棒决策树",
            "随机森林",
            "对抗样本",
            "高效训练",
            "基尼不纯度"
        ],
        "涉及的技术概念": {
            "GROOT算法": "一种高效训练鲁棒决策树和随机森林的算法，显著减少训练时间",
            "基尼不纯度": "用于衡量决策树分裂质量的指标，GROOT通过解析求解优化了其计算过程",
            "对抗精度": "模型在面对对抗样本时的性能表现，GROOT在保持或提高此性能的同时大幅提升训练效率"
        },
        "success": true
    },
    {
        "order": 338,
        "title": "EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture",
        "html": "https://ICML.cc//virtual/2021/poster/8945",
        "abstract": "In this work, we address the Text-to-Speech (TTS) task by proposing a non-autoregressive architecture called EfficientTTS. Unlike the dominant non-autoregressive TTS models, which are trained with the need of external aligners, EfficientTTS optimizes all its parameters with a stable, end-to-end training procedure, allowing for synthesizing high quality speech in a fast and efficient manner. EfficientTTS is motivated by a new monotonic alignment modeling approach, which specifies monotonic constraints to the sequence alignment with almost no increase of computation. By combining EfficientTTS with different feed-forward network structures, we develop a family of TTS models, including both text-to-melspectrogram and text-to-waveform networks. We experimentally show that the proposed models significantly outperform counterpart models such as Tacotron 2 and Glow-TTS in terms of speech quality, training efficiency and synthesis speed, while still producing the speeches of strong robustness and great diversity. In addition, we demonstrate that proposed approach can be easily extended to autoregressive models such as Tacotron 2.",
        "conference": "ICML",
        "中文标题": "高效TTS：一种高效且高质量的文本到语音架构",
        "摘要翻译": "在这项工作中，我们通过提出一种名为EfficientTTS的非自回归架构来解决文本到语音（TTS）任务。与需要外部对齐器训练的主导非自回归TTS模型不同，EfficientTTS通过一个稳定的端到端训练程序优化所有参数，从而能够快速高效地合成高质量语音。EfficientTTS的动机来自于一种新的单调对齐建模方法，该方法为序列对齐指定了单调约束，几乎不增加计算量。通过将EfficientTTS与不同的前馈网络结构结合，我们开发了一系列TTS模型，包括文本到梅尔频谱图和文本到波形网络。实验表明，所提出的模型在语音质量、训练效率和合成速度方面显著优于Tacotron 2和Glow-TTS等对比模型，同时仍能产生具有强鲁棒性和高度多样性的语音。此外，我们还证明了所提出的方法可以轻松扩展到Tacotron 2等自回归模型。",
        "领域": "语音合成、文本到语音转换、深度学习模型优化",
        "问题": "解决传统非自回归TTS模型需要外部对齐器训练的问题，提高语音合成的效率和质量",
        "动机": "通过引入单调对齐建模方法，减少计算量，实现高效且高质量的语音合成",
        "方法": "提出EfficientTTS非自回归架构，结合端到端训练和单调对齐建模，优化语音合成的效率和质量",
        "关键词": [
            "非自回归TTS",
            "端到端训练",
            "单调对齐建模",
            "语音合成效率",
            "高质量语音"
        ],
        "涉及的技术概念": {
            "非自回归TTS": "一种不依赖于序列中先前元素的预测来生成当前元素的TTS模型，提高了合成速度",
            "端到端训练": "一种训练方法，直接从输入到输出进行优化，无需中间步骤或外部对齐器",
            "单调对齐建模": "一种确保输入序列与输出序列对齐单调性的方法，减少了计算复杂度"
        },
        "success": true
    },
    {
        "order": 339,
        "title": "Elastic Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9583",
        "abstract": "While many existing graph neural networks (GNNs) have been proven to perform $\\ell_2$-based graph smoothing that enforces smoothness globally, in this work we aim to further enhance the local smoothness adaptivity of GNNs via $\\ell_1$-based graph smoothing. As a result, we introduce a family of GNNs (Elastic GNNs) based on $\\ell_1$ and $\\ell_2$-based graph smoothing. In particular, we propose a novel and general message passing scheme into GNNs. This message passing algorithm is not only friendly to back-propagation training but also achieves the desired smoothing properties with a theoretical convergence guarantee. Experiments on semi-supervised learning tasks demonstrate that the proposed Elastic GNNs obtain better adaptivity on benchmark datasets and are significantly robust to graph adversarial attacks. The implementation of Elastic GNNs is available at \\url{https://github.com/lxiaorui/ElasticGNN}.",
        "conference": "ICML",
        "success": true,
        "中文标题": "弹性图神经网络",
        "摘要翻译": "虽然许多现有的图神经网络（GNN）已被证明可以执行基于ℓ₂的图平滑，从而在全球范围内强制执行平滑性，但在本工作中，我们的目标是通过基于ℓ₁的图平滑进一步增强GNN的局部平滑自适应性。 因此，我们引入了一系列基于ℓ₁和ℓ₂的图平滑的GNN（弹性GNN）。 特别是，我们提出了一种新颖且通用的消息传递方案到GNN中。 这种消息传递算法不仅对反向传播训练友好，而且以理论收敛保证实现了所需的平滑特性。 在半监督学习任务上的实验表明，所提出的弹性GNN在基准数据集上获得了更好的适应性，并且对图对抗攻击具有显着的鲁棒性。 弹性GNN的实现可在https://github.com/lxiaorui/ElasticGNN获得。",
        "领域": "图神经网络, 半监督学习, 图对抗攻击",
        "问题": "如何提高图神经网络的局部平滑自适应性，并增强其对抗图对抗攻击的鲁棒性。",
        "动机": "现有的图神经网络通常采用基于ℓ₂的图平滑，这种方法侧重于全局平滑，而忽略了局部平滑自适应性。为了解决这个问题，本研究旨在通过引入基于ℓ₁的图平滑来增强GNN的局部平滑自适应性。",
        "方法": "提出一种基于ℓ₁和ℓ₂图平滑的弹性图神经网络（Elastic GNNs），并设计了一种新颖通用的消息传递方案，该方案具有理论收敛保证，并能有效提升模型在半监督学习任务中的性能和对抗攻击的鲁棒性。",
        "关键词": [
            "图神经网络",
            "图平滑",
            "局部自适应性",
            "半监督学习",
            "图对抗攻击"
        ],
        "涉及的技术概念": {
            "图神经网络": "一种用于处理图结构数据的神经网络，通过节点之间的消息传递来学习节点和图的表示。",
            "图平滑": "一种通过平滑节点特征来提高图神经网络性能的技术，旨在使相邻节点的特征更加相似。"
        }
    },
    {
        "order": 340,
        "title": "EL-Attention: Memory Efficient Lossless Attention for Generation",
        "html": "https://ICML.cc//virtual/2021/poster/9703",
        "abstract": "Transformer model with multi-head attention requires caching intermediate results for efficient inference in generation tasks. However, cache brings new memory-related costs and prevents leveraging larger batch size for faster speed. We propose memory-efficient lossless attention (called EL-attention) to address this issue. It avoids heavy operations for building multi-head keys and values, cache for them is not needed. EL-attention constructs an ensemble of attention results by expanding query while keeping key and value shared. It produces the same result as multi-head attention with less GPU memory and faster inference speed. We conduct extensive experiments on Transformer, BART, and GPT-2 for summarization and question generation tasks. The results show EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss.",
        "conference": "ICML",
        "中文标题": "EL-注意力：用于生成任务的内存高效无损注意力机制",
        "摘要翻译": "采用多头注意力机制的Transformer模型在生成任务中需要缓存中间结果以实现高效推理。然而，缓存带来了新的内存相关成本，并阻碍了利用更大的批量大小以加快速度。我们提出了内存高效的无损注意力机制（称为EL-注意力）来解决这一问题。它避免了构建多头键和值的繁重操作，不需要为它们缓存。EL-注意力通过扩展查询同时保持键和值共享，构建了一个注意力结果的集合。它以更少的GPU内存和更快的推理速度产生与多头注意力相同的结果。我们在Transformer、BART和GPT-2上进行了广泛的实验，用于摘要和问题生成任务。结果表明，EL-注意力在不损失准确性的情况下，将现有模型的速度提高了1.6倍至5.3倍。",
        "领域": "自然语言处理与视觉结合, 文本生成, 模型优化",
        "问题": "解决Transformer模型在生成任务中因缓存中间结果而导致的内存效率低下和推理速度慢的问题",
        "动机": "提高生成任务的推理效率，减少内存消耗，同时保持模型的准确性",
        "方法": "提出了一种内存高效的无损注意力机制（EL-注意力），通过扩展查询并共享键和值，避免了多头注意力中的缓存需求",
        "关键词": [
            "EL-注意力",
            "内存高效",
            "无损注意力",
            "生成任务",
            "模型优化"
        ],
        "涉及的技术概念": {
            "多头注意力机制": "传统的注意力机制，通过多个注意力头并行处理信息，但需要缓存中间结果",
            "EL-注意力": "提出的一种新型注意力机制，通过扩展查询并共享键和值，减少内存消耗并提高推理速度",
            "GPU内存优化": "通过减少模型运行时的内存占用，提高模型的运行效率和批量处理能力"
        },
        "success": true
    },
    {
        "order": 341,
        "title": "Elementary superexpressive activations",
        "html": "https://ICML.cc//virtual/2021/poster/10131",
        "abstract": "We call a finite family of activation functions \\emph{superexpressive} if any multivariate continuous function can be approximated by a neural network that uses these activations and has a fixed architecture only depending on the number of input variables (i.e., to achieve any accuracy we only need to adjust the weights, without increasing the number of neurons). Previously, it was known that superexpressive activations exist, but their form was quite complex. We give examples of very simple superexpressive families: for example, we prove that the family \\{sin, arcsin\\} is superexpressive. We also show that most practical activations (not involving periodic functions) are not superexpressive.",
        "conference": "ICML",
        "中文标题": "基础超表达激活函数",
        "摘要翻译": "我们称一个有限的激活函数族为‘超表达’，如果任何多元连续函数都可以通过使用这些激活函数且具有固定架构（即仅依赖于输入变量的数量，为了达到任何精度，我们只需要调整权重，而不需要增加神经元数量）的神经网络来近似。之前已知超表达激活函数存在，但它们的形式相当复杂。我们给出了一些非常简单的超表达函数族的例子：例如，我们证明了{sin, arcsin}族是超表达的。我们还表明，大多数实际使用的激活函数（不涉及周期函数）不是超表达的。",
        "领域": "神经网络理论、函数逼近、激活函数设计",
        "问题": "探索和证明简单激活函数族是否能够实现超表达性，即固定架构下近似任意多元连续函数的能力。",
        "动机": "之前发现的超表达激活函数形式复杂，不利于实际应用，研究旨在发现形式简单且具有超表达性的激活函数族。",
        "方法": "通过理论证明和举例，展示特定简单激活函数族（如{sin, arcsin}）的超表达性，并分析常见非周期激活函数的局限性。",
        "关键词": [
            "超表达性",
            "激活函数",
            "神经网络",
            "函数逼近",
            "固定架构"
        ],
        "涉及的技术概念": {
            "超表达性": "指激活函数族在固定架构神经网络中能够近似任意多元连续函数的特性。",
            "激活函数": "神经网络中引入非线性因素的函数，影响网络的学习能力和表现。",
            "固定架构": "指神经网络的层数和每层神经元数量固定，不随学习过程调整。"
        },
        "success": true
    },
    {
        "order": 342,
        "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL",
        "html": "https://ICML.cc//virtual/2021/poster/9553",
        "abstract": "Off-policy reinforcement learning (RL) holds the promise of sample-efficient learning of decision-making policies by leveraging past experience. However, in the offline RL setting -- where a fixed collection of interactions are provided and no further interactions are allowed -- it has been shown that standard off-policy RL methods can significantly underperform. In this work, we closely investigate an important simplification of BCQ (Fujimoto et al., 2018) -- a prior approach for offline RL -- removing a heuristic design choice. Importantly, in contrast to their original theoretical considerations, we derive this simplified algorithm through the introduction of a novel backup operator, Expected-Max Q-Learning (EMaQ), which is more closely related to the resulting practical algorithm. Specifically, in addition to the distribution support, EMaQ explicitly considers the number of samples and the proposal distribution, allowing us to derive new sub-optimality bounds. In the offline RL setting -- the main focus of this work -- EMaQ matches and outperforms prior state-of-the-art in the D4RL benchmarks (Fu et al., 2020). In the online RL setting, we demonstrate that EMaQ is competitive with Soft Actor Critic (SAC). The key contributions of our empirical findings are demonstrating the importance of careful generative model design for estimating behavior policies, and an intuitive notion of complexity for offline RL problems. With its simple interpretation and fewer moving parts, such as no explicit function approximator representing the policy, EMaQ serves as a strong yet easy to implement baseline for future work.",
        "conference": "ICML",
        "中文标题": "EMaQ：简单而有效的离线和在线强化学习的期望最大Q学习算子",
        "摘要翻译": "离策略强化学习（RL）通过利用过去的经验，有望实现决策策略的样本高效学习。然而，在离线RL设置中——提供一个固定的交互集合且不允许进一步的交互——已经表明标准的离策略RL方法可能表现不佳。在这项工作中，我们仔细研究了BCQ（Fujimoto等人，2018）的一个重要简化——一种离线RL的先前方法——移除了一个启发式设计选择。重要的是，与他们最初的理论考虑相反，我们通过引入一种新的备份算子——期望最大Q学习（EMaQ）——推导出这种简化算法，这与最终的实际算法更为密切相关。具体来说，除了分布支持外，EMaQ还明确考虑了样本数量和提议分布，使我们能够推导出新的次优性界限。在离线RL设置中——这项工作的主要焦点——EMaQ在D4RL基准测试（Fu等人，2020）中匹配并超越了先前的最先进技术。在在线RL设置中，我们证明了EMaQ与软演员评论家（SAC）具有竞争力。我们实证发现的关键贡献是展示了仔细生成模型设计对于估计行为策略的重要性，以及离线RL问题复杂性的直观概念。凭借其简单的解释和较少的移动部分，例如没有明确表示策略的函数近似器，EMaQ为未来的工作提供了一个强大且易于实现的基线。",
        "领域": "强化学习、离线学习、在线学习",
        "问题": "解决离线强化学习中标准离策略方法表现不佳的问题",
        "动机": "通过简化BCQ方法并引入新的备份算子EMaQ，提高离线强化学习的效率和性能",
        "方法": "引入期望最大Q学习（EMaQ）算子，明确考虑样本数量和提议分布，简化BCQ方法",
        "关键词": [
            "离线强化学习",
            "期望最大Q学习",
            "BCQ简化",
            "D4RL基准",
            "软演员评论家"
        ],
        "涉及的技术概念": {
            "期望最大Q学习（EMaQ）": "一种新的备份算子，用于简化离线强化学习算法，提高性能",
            "BCQ": "一种先前的离线强化学习方法，EMaQ通过简化其设计提高了效率",
            "软演员评论家（SAC）": "一种在线强化学习方法，EMaQ在在线设置中与其竞争"
        },
        "success": true
    },
    {
        "order": 343,
        "title": "Emergent Social Learning via Multi-agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9105",
        "abstract": "Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过多智能体强化学习实现涌现式社会学习",
        "摘要翻译": "社会学习是人类和动物智能的关键组成部分。通过从环境中专家的行为中获取线索，社会学习者可以习得复杂的行为并迅速适应新环境。本文研究了在多智能体环境中，独立的强化学习（RL）智能体是否能够学会利用社会学习来提高其性能。我们发现，在大多数情况下，普通的无模型RL智能体不会使用社会学习。我们分析了这一缺陷的原因，并表明通过对训练环境施加约束和引入基于模型的辅助损失，我们能够获得广义的社会学习策略，这些策略使智能体能够：i）发现从单智能体训练中无法学到的复杂技能，以及ii）通过从新环境中存在的专家那里获取线索，在线适应新环境。相比之下，使用无模型RL或模仿学习训练的智能体泛化能力较差，在迁移任务中未能成功。通过混合多智能体和单独训练，我们可以获得利用社会学习获得技能的智能体，这些技能可以在单独时部署，甚至优于从一开始就单独训练的智能体。",
        "领域": "多智能体系统, 强化学习, 社会学习",
        "问题": "研究在多智能体环境中，独立的强化学习智能体是否能够学会利用社会学习来提高其性能。",
        "动机": "探索智能体是否能够通过社会学习获得复杂技能并快速适应新环境，以提高其性能和适应性。",
        "方法": "通过施加环境约束和引入基于模型的辅助损失，获得广义的社会学习策略，混合多智能体和单独训练。",
        "关键词": [
            "多智能体强化学习",
            "社会学习",
            "模型辅助损失",
            "技能迁移",
            "适应性学习"
        ],
        "涉及的技术概念": {
            "多智能体强化学习": "在多智能体环境中应用强化学习，研究智能体间的互动和学习策略。",
            "社会学习": "智能体通过观察和模仿环境中其他智能体（专家）的行为来学习新技能和适应新环境。",
            "模型辅助损失": "引入基于模型的辅助损失函数，以促进智能体学习更有效的策略和技能。"
        }
    },
    {
        "order": 344,
        "title": "Emphatic Algorithms for Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9923",
        "abstract": "Off-policy learning allows us to learn about possible policies of behavior from experience generated by a different behavior policy. Temporal difference (TD) learning algorithms can become unstable when combined with function approximation and off-policy sampling---this is known as the ``deadly triad''. Emphatic temporal difference (ETD(λ)) algorithm ensures convergence in the linear case by appropriately weighting the TD(λ) updates. In this paper, we extend the use of emphatic methods to deep reinforcement learning agents. We show that naively adapting ETD(λ) to popular deep reinforcement learning algorithms, which use forward view multi-step returns, results in poor performance. We then derive new emphatic algorithms for use in the context of such algorithms, and we demonstrate that they provide noticeable benefits in small problems designed to highlight the instability of TD methods. Finally, we observed improved performance when applying these algorithms at scale on classic Atari games from the Arcade Learning Environment.",
        "conference": "ICML",
        "中文标题": "深度强化学习中的强调算法",
        "摘要翻译": "离策略学习允许我们从由不同行为策略生成的经验中学习可能的行为策略。当与函数逼近和离策略采样结合时，时间差分（TD）学习算法可能会变得不稳定——这被称为“致命三元组”。强调时间差分（ETD(λ)）算法通过适当加权TD(λ)更新，在线性情况下确保了收敛。在本文中，我们将强调方法的使用扩展到深度强化学习代理。我们展示了将ETD(λ)简单地适应于使用前向视图多步回报的流行深度强化学习算法，会导致性能不佳。然后，我们为这类算法的上下文推导出新的强调算法，并证明它们在设计用于突出TD方法不稳定性的小问题上提供了显著的好处。最后，我们观察到在Arcade学习环境的经典Atari游戏上大规模应用这些算法时性能有所提升。",
        "领域": "深度强化学习",
        "问题": "解决深度强化学习算法在离策略学习和函数逼近结合时的不稳定性问题",
        "动机": "提高深度强化学习算法在复杂环境中的稳定性和性能",
        "方法": "扩展强调时间差分（ETD(λ)）算法到深度强化学习，并针对前向视图多步回报算法设计新的强调算法",
        "关键词": [
            "深度强化学习",
            "离策略学习",
            "时间差分学习",
            "强调算法",
            "Atari游戏"
        ],
        "涉及的技术概念": {
            "离策略学习": "从不同于目标策略的行为策略生成的经验中学习",
            "时间差分学习": "一种结合了蒙特卡罗方法和动态规划思想的强化学习方法",
            "强调算法": "通过适当加权更新来确保算法收敛的技术"
        },
        "success": true
    },
    {
        "order": 345,
        "title": "End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series",
        "html": "https://ICML.cc//virtual/2021/poster/10001",
        "abstract": "This paper presents a novel approach for hierarchical time series forecasting that produces coherent, probabilistic forecasts without requiring any explicit post-processing reconciliation. Unlike the state-of-the-art, the proposed method simultaneously learns from all time series in the hierarchy and incorporates the reconciliation step into a single trainable model. This is achieved by applying the reparameterization trick and casting reconciliation as an optimization problem with a closed-form solution. These model features make end-to-end learning of hierarchical forecasts possible, while accomplishing the challenging task of generating forecasts that are both probabilistic and coherent. Importantly, our approach also accommodates general aggregation constraints including grouped and temporal hierarchies. An extensive empirical evaluation on real-world hierarchical datasets demonstrates the advantages of the proposed approach over the state-of-the-art.",
        "conference": "ICML",
        "中文标题": "端到端学习层次时间序列的连贯概率预测",
        "摘要翻译": "本文提出了一种新颖的层次时间序列预测方法，该方法无需任何显式的后处理协调即可产生连贯的概率预测。与现有技术不同，所提出的方法同时从层次结构中的所有时间序列中学习，并将协调步骤整合到一个单一的可训练模型中。这是通过应用重参数化技巧并将协调作为一个具有闭式解的优化问题来实现的。这些模型特性使得层次预测的端到端学习成为可能，同时完成了生成既概率又连贯的预测这一挑战性任务。重要的是，我们的方法还适应包括分组和时间层次在内的通用聚合约束。对真实世界层次数据集的大量实证评估证明了所提出方法相对于现有技术的优势。",
        "领域": "时间序列预测、概率模型、层次数据分析",
        "问题": "如何在无需显式后处理协调的情况下，生成既概率又连贯的层次时间序列预测。",
        "动机": "解决现有层次时间序列预测方法需要显式后处理协调且难以同时保证预测的概率性和连贯性的问题。",
        "方法": "通过重参数化技巧和将协调问题转化为具有闭式解的优化问题，实现层次时间序列的端到端学习，生成连贯的概率预测。",
        "关键词": [
            "层次时间序列",
            "概率预测",
            "端到端学习",
            "重参数化",
            "优化问题"
        ],
        "涉及的技术概念": {
            "重参数化技巧": "用于在模型中引入随机性，同时保持梯度可计算，使得模型能够进行端到端训练。",
            "闭式解优化问题": "将协调步骤转化为一个可以直接求解的优化问题，避免了复杂的迭代过程。",
            "层次时间序列": "指具有层次结构的时间序列数据，如按地理或产品类别聚合的销售数据。"
        },
        "success": true
    },
    {
        "order": 346,
        "title": "E(n) Equivariant Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9279",
        "abstract": "This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.",
        "conference": "ICML",
        "中文标题": "E(n)等变图神经网络",
        "摘要翻译": "本文介绍了一种新的模型，用于学习对旋转、平移、反射和排列等变的图神经网络，称为E(n)-等变图神经网络（EGNNs）。与现有方法相比，我们的工作不需要在中间层进行计算成本高的高阶表示，同时仍然达到竞争或更好的性能。此外，现有方法仅限于在3维空间上的等变性，而我们的模型可以轻松扩展到更高维的空间。我们在动力系统建模、图自动编码器中的表示学习和分子性质预测方面证明了我们方法的有效性。",
        "领域": "图神经网络、分子性质预测、表示学习",
        "问题": "如何在图神经网络中实现高效的旋转、平移、反射和排列等变性，同时避免高计算成本。",
        "动机": "为了解决现有图神经网络在处理高维空间等变性时的计算效率低下和局限性问题。",
        "方法": "提出E(n)-等变图神经网络（EGNNs），通过避免在中间层使用高阶表示来降低计算成本，同时保持或超越现有方法的性能，并支持更高维空间的等变性。",
        "关键词": [
            "等变图神经网络",
            "分子性质预测",
            "表示学习",
            "动力系统建模",
            "高维空间"
        ],
        "涉及的技术概念": {
            "E(n)-等变性": "指模型对n维欧几里得空间中的旋转、平移、反射和排列等变换具有不变性或等变性，使得模型在处理这些变换时保持性能。",
            "图神经网络": "一种专门用于处理图结构数据的神经网络，能够捕捉节点之间的关系和图的全局结构。",
            "表示学习": "通过模型自动学习数据的有效表示，以便于后续任务如分类、预测等，这里特指在图结构数据上的表示学习。"
        },
        "success": true
    },
    {
        "order": 347,
        "title": "Enhancing Robustness of Neural Networks through Fourier Stabilization",
        "html": "https://ICML.cc//virtual/2021/poster/10041",
        "abstract": "Despite the considerable success of neural networks in security settings such as malware detection, such models have proved vulnerable to evasion attacks, in which attackers make slight changes to inputs (e.g., malware) to bypass detection. We propose a novel approach, Fourier stabilization, for designing evasion-robust neural networks with binary inputs. This approach, which is complementary to other forms of defense, replaces the weights of individual neurons with robust analogs derived using Fourier analytic tools. The choice of which neurons to stabilize in a neural network is then a combinatorial optimization problem, and we propose several methods for approximately solving it. We provide a formal bound on the per-neuron drop in accuracy due to Fourier stabilization, and experimentally demonstrate the effectiveness of the proposed approach in boosting robustness of neural networks in several detection settings. Moreover, we show that our approach effectively composes with adversarial training.",
        "conference": "ICML",
        "中文标题": "通过傅里叶稳定增强神经网络的鲁棒性",
        "摘要翻译": "尽管神经网络在恶意软件检测等安全设置中取得了相当大的成功，但这类模型已被证明容易受到规避攻击的影响，攻击者通过对输入（如恶意软件）进行轻微修改以绕过检测。我们提出了一种新颖的方法——傅里叶稳定，用于设计具有二进制输入的规避鲁棒神经网络。这种方法与其他防御形式互补，通过使用傅里叶分析工具导出的鲁棒类似物替换单个神经元的权重。在神经网络中选择哪些神经元进行稳定是一个组合优化问题，我们提出了几种近似解决方法。我们为傅里叶稳定导致的每个神经元准确度下降提供了正式界限，并通过实验证明了所提方法在提升多种检测设置中神经网络鲁棒性方面的有效性。此外，我们还展示了我们的方法能够有效地与对抗训练相结合。",
        "领域": "恶意软件检测、对抗性机器学习、神经网络安全",
        "问题": "提高神经网络对规避攻击的鲁棒性",
        "动机": "神经网络在安全关键应用中容易受到精心设计的输入修改攻击，导致检测失败",
        "方法": "提出傅里叶稳定方法，通过傅里叶分析工具替换神经元权重，增强模型对规避攻击的防御能力",
        "关键词": [
            "傅里叶稳定",
            "规避攻击",
            "神经网络鲁棒性",
            "对抗性训练",
            "恶意软件检测"
        ],
        "涉及的技术概念": {
            "傅里叶稳定": "使用傅里叶分析工具导出的鲁棒类似物替换神经元权重，以提高模型对规避攻击的防御能力",
            "规避攻击": "攻击者通过对输入进行轻微修改以绕过模型检测的攻击方式",
            "对抗性训练": "一种通过包含对抗样本的训练过程来提高模型鲁棒性的方法"
        },
        "success": true
    },
    {
        "order": 348,
        "title": "Ensemble Bootstrapping for Q-Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8459",
        "abstract": "Q-learning (QL), a common reinforcement learning algorithm, suffers from over-estimation bias due to the maximization term in the optimal Bellman operator. This bias may lead to sub-optimal behavior. Double-Q-learning tackles this issue by utilizing two estimators, yet results in an under-estimation bias. Similar to over-estimation in Q-learning, in certain scenarios, the under-estimation bias may degrade performance. In this work, we introduce a new bias-reduced algorithm called Ensemble Bootstrapped Q-Learning (EBQL), a natural extension of Double-Q-learning to ensembles. We analyze our method both theoretically and empirically. Theoretically, we prove that EBQL-like updates yield lower MSE when estimating the maximal mean of a set of independent random variables. Empirically, we show that there exist domains where both over and under-estimation result in sub-optimal performance. Finally, We demonstrate the superior performance of a deep RL variant of EBQL over other deep QL algorithms for a suite of ATARI games.",
        "conference": "ICML",
        "中文标题": "集成自举Q学习",
        "摘要翻译": "Q学习（QL）作为一种常见的强化学习算法，由于最优贝尔曼算子中的最大化项而遭受过估计偏差。这种偏差可能导致次优行为。双Q学习通过使用两个估计器来解决这个问题，但却导致了低估偏差。与Q学习中的过估计类似，在某些情况下，低估偏差可能会降低性能。在这项工作中，我们引入了一种名为集成自举Q学习（EBQL）的新偏差减少算法，这是双Q学习向集成学习的自然扩展。我们从理论和实证两方面分析了我们的方法。理论上，我们证明了类似EBQL的更新在估计一组独立随机变量的最大均值时会产生较低的均方误差。实证上，我们展示了存在一些领域，其中过估计和低估都会导致性能次优。最后，我们证明了EBQL的深度强化学习变体在一套ATARI游戏中优于其他深度QL算法的性能。",
        "领域": "强化学习、深度强化学习、游戏AI",
        "问题": "解决Q学习中的过估计和低估偏差问题",
        "动机": "Q学习中的过估计和低估偏差可能导致次优性能，需要一种新的方法来减少这些偏差",
        "方法": "引入集成自举Q学习（EBQL），作为双Q学习向集成学习的扩展，通过理论和实证分析验证其有效性",
        "关键词": [
            "集成学习",
            "Q学习",
            "偏差减少",
            "深度强化学习",
            "ATARI游戏"
        ],
        "涉及的技术概念": {
            "集成自举Q学习（EBQL）": "一种新的偏差减少算法，通过集成学习方法扩展双Q学习，以减少过估计和低估偏差",
            "双Q学习": "使用两个估计器来减少Q学习中的过估计偏差，但可能引入低估偏差",
            "贝尔曼算子": "在强化学习中用于更新Q值的算子，其中的最大化项可能导致过估计偏差"
        },
        "success": true
    },
    {
        "order": 349,
        "title": "Environment Inference for Invariant Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10603",
        "abstract": "Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into ``domains'' or ``environments''. Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds dataset. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.",
        "conference": "ICML",
        "中文标题": "环境推断用于不变学习",
        "摘要翻译": "学习能够优雅处理分布变化的模型是领域泛化、鲁棒优化和公平性研究的核心。一个有前景的提法是不变学习，它识别了学习哪些特征是领域特定与领域不变的关键问题。这一领域的一个重要假设是训练样本被划分为“域”或“环境”。我们的关注点是在更常见的设置中，这些划分并未提供。我们提出了EIIL，一个用于领域不变学习的通用框架，它结合了环境推断来直接推断对下游不变学习最有信息量的划分。我们展示了EIIL在不使用环境标签的情况下，在CMNIST基准测试中优于不变学习方法，并且在Waterbirds数据集上的最差组表现上显著优于ERM。最后，我们建立了EIIL与算法公平性之间的联系，这使得EIIL能够在公平预测问题中提高准确性和校准。",
        "领域": "领域泛化, 鲁棒优化, 算法公平性",
        "问题": "在训练样本未被划分为域或环境的情况下，如何进行有效的领域不变学习",
        "动机": "解决在没有明确环境划分的情况下，如何通过环境推断来促进领域不变学习，以提高模型在分布变化下的泛化能力和公平性",
        "方法": "提出了EIIL框架，通过环境推断直接推断对下游不变学习最有信息量的划分，不依赖环境标签",
        "关键词": [
            "领域不变学习",
            "环境推断",
            "算法公平性",
            "分布变化",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "领域不变学习": "识别并学习领域特定与领域不变的特征，以提高模型在分布变化下的泛化能力",
            "环境推断": "在缺乏明确环境划分的情况下，推断出对不变学习最有信息量的环境划分",
            "算法公平性": "确保模型预测在不同群体间保持公平，避免偏见，EIIL通过改进准确性和校准来增强公平性"
        },
        "success": true
    },
    {
        "order": 350,
        "title": "Equivariant Learning of Stochastic Fields: Gaussian Processes and Steerable Conditional Neural Processes",
        "html": "https://ICML.cc//virtual/2021/poster/9935",
        "abstract": "Motivated by objects such as electric fields or fluid streams, we study the problem of learning stochastic fields, i.e. stochastic processes whose samples are fields like those occurring in physics and engineering. Considering general\ntransformations such as rotations and reflections, we show that spatial invariance of stochastic fields requires an inference model to be equivariant. Leveraging recent advances from the equivariance literature, we study equivariance in two classes of models. Firstly, we fully characterise equivariant Gaussian processes. Secondly, we introduce Steerable Conditional Neural Processes (SteerCNPs), a new, fully equivariant member of the Neural Process family. In experiments with Gaussian process vector fields, images, and real-world weather data, we observe that SteerCNPs significantly improve the performance of previous models and equivariance leads to improvements in transfer learning tasks.",
        "conference": "ICML",
        "中文标题": "随机场的等变学习：高斯过程与可转向条件神经过程",
        "摘要翻译": "受电场或流体流等物体的启发，我们研究了学习随机场的问题，即样本为物理学和工程学中出现的那类场的随机过程。考虑到旋转和反射等一般变换，我们展示了随机场的空间不变性要求推理模型具有等变性。利用等变文献中的最新进展，我们研究了两类模型中的等变性。首先，我们全面描述了等变高斯过程。其次，我们引入了可转向条件神经过程（SteerCNPs），这是神经过程家族中一个全新的、完全等变的成员。在高斯过程向量场、图像和真实世界天气数据的实验中，我们观察到SteerCNPs显著提高了先前模型的性能，并且等变性在迁移学习任务中带来了改进。",
        "领域": "随机场学习、等变学习、迁移学习",
        "问题": "如何学习具有空间不变性的随机场",
        "动机": "研究随机场学习的问题，特别是那些在物理学和工程学中出现的场，如电场或流体流，以及如何通过等变性提高模型的性能。",
        "方法": "研究了两类模型中的等变性：全面描述了等变高斯过程，并引入了可转向条件神经过程（SteerCNPs），这是一个全新的、完全等变的神经过程家族成员。",
        "关键词": [
            "随机场学习",
            "等变学习",
            "高斯过程",
            "可转向条件神经过程",
            "迁移学习"
        ],
        "涉及的技术概念": {
            "等变性": "在模型中实现空间变换下的不变性，使得模型能够处理如旋转和反射等变换。",
            "高斯过程": "一种用于建模随机场的非参数方法，能够提供预测的不确定性估计。",
            "可转向条件神经过程": "一种新型的神经过程，通过结合等变性和条件神经过程，提高了模型在处理空间变换时的性能和灵活性。"
        },
        "success": true
    },
    {
        "order": 351,
        "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
        "html": "https://ICML.cc//virtual/2021/poster/8499",
        "abstract": "Message passing neural networks have become a method of choice for learning on graphs, in particular the prediction of chemical properties and the acceleration of molecular dynamics studies. While they readily scale to large training data sets, previous approaches have proven to be less data efficient than kernel methods. We identify limitations of invariant representations as a major reason and extend the message passing formulation to rotationally equivariant representations. On this basis, we propose the polarizable atom interaction neural network (PaiNN) and improve on common molecule benchmarks over previous networks, while reducing model size and inference time. We leverage the equivariant atomwise representations obtained by PaiNN for the prediction of tensorial properties. Finally, we apply this to the simulation of molecular spectra, achieving speedups of 4-5 orders of magnitude compared to the electronic structure reference.",
        "conference": "ICML",
        "中文标题": "等变消息传递用于张量性质和分子光谱的预测",
        "摘要翻译": "消息传递神经网络已成为图学习的一种首选方法，特别是在化学性质预测和分子动力学研究加速方面。虽然它们能够轻松扩展到大型训练数据集，但先前的方法已被证明在数据效率上不如核方法。我们将不变表示的局限性确定为一个主要原因，并将消息传递公式扩展到旋转等变表示。在此基础上，我们提出了极化原子交互神经网络（PaiNN），并在常见分子基准测试中改进了先前的网络，同时减少了模型大小和推理时间。我们利用PaiNN获得的等变原子表示来预测张量性质。最后，我们将其应用于分子光谱的模拟，与电子结构参考相比，实现了4-5个数量级的加速。",
        "领域": "分子动力学模拟、化学性质预测、分子光谱分析",
        "问题": "提高图神经网络在化学性质预测和分子动力学研究中的数据效率和性能",
        "动机": "解决不变表示在化学性质预测和分子动力学研究中的局限性，提高数据效率和模型性能",
        "方法": "扩展消息传递神经网络到旋转等变表示，提出极化原子交互神经网络（PaiNN）",
        "关键词": [
            "等变消息传递",
            "极化原子交互神经网络",
            "分子光谱模拟",
            "张量性质预测",
            "分子动力学"
        ],
        "涉及的技术概念": {
            "等变消息传递": "扩展了传统的消息传递神经网络，使其能够处理旋转等变表示，提高了模型对分子结构的理解能力",
            "极化原子交互神经网络（PaiNN）": "一种新型的神经网络架构，用于提高分子性质预测的准确性和效率，同时减少模型大小和推理时间",
            "分子光谱模拟": "利用改进的神经网络方法加速分子光谱的模拟过程，实现了比传统电子结构方法更高的计算效率"
        },
        "success": true
    },
    {
        "order": 352,
        "title": "Equivariant Networks for Pixelized Spheres",
        "html": "https://ICML.cc//virtual/2021/poster/10393",
        "abstract": "Pixelizations of Platonic solids such as the cube and icosahedron have been widely used to represent spherical data, from climate records to Cosmic Microwave Background maps. Platonic solids have well-known global symmetries. Once we pixelize each face of the solid, each face also possesses its own local symmetries in the form of Euclidean isometries. One way to combine these symmetries is through a hierarchy. However, this approach does not adequately model the interplay between the two levels of symmetry transformations. We show how to model this interplay using ideas from group theory, identify the equivariant linear maps, and introduce equivariant padding that respects these symmetries. Deep networks that use these maps as their building blocks generalize gauge equivariant CNNs on pixelized spheres. These deep networks achieve state-of-the-art results on semantic segmentation for climate data and omnidirectional image processing. Code is available at https://git.io/JGiZA.",
        "conference": "ICML",
        "中文标题": "等变网络用于像素化球体",
        "摘要翻译": "柏拉图立体如立方体和二十面体的像素化已被广泛用于表示球形数据，从气候记录到宇宙微波背景图。柏拉图立体具有众所周知的全局对称性。一旦我们对立体的每个面进行像素化，每个面也以其自身的欧几里得等距形式拥有局部对称性。结合这些对称性的一种方式是通过层次结构。然而，这种方法未能充分模拟两个层次对称变换之间的相互作用。我们展示了如何利用群论的思想来模拟这种相互作用，识别等变线性映射，并引入尊重这些对称性的等变填充。使用这些映射作为构建块的深度网络在像素化球体上推广了规范等变CNN。这些深度网络在气候数据的语义分割和全方位图像处理上取得了最先进的结果。代码可在https://git.io/JGiZA获取。",
        "领域": "语义分割, 全方位图像处理, 气候数据分析",
        "问题": "如何在像素化球体数据上有效结合全局和局部对称性以提升深度学习模型的性能",
        "动机": "探索在像素化球体表示中结合全局和局部对称性的新方法，以改进气候数据和全方位图像的深度学习应用",
        "方法": "利用群论思想模拟对称性相互作用，识别等变线性映射，并引入等变填充，构建深度网络",
        "关键词": [
            "等变网络",
            "像素化球体",
            "语义分割",
            "全方位图像处理",
            "气候数据"
        ],
        "涉及的技术概念": {
            "等变线性映射": "在像素化球体上识别出的线性映射，能够保持输入数据的对称性",
            "等变填充": "一种填充技术，确保在像素化球体上的操作尊重数据的对称性",
            "规范等变CNN": "一种卷积神经网络，其设计保证了在特定变换下的等变性，用于处理像素化球体数据"
        },
        "success": true
    },
    {
        "order": 353,
        "title": "Estimating $\\alpha$-Rank from A Few Entries with Low Rank Matrix Completion",
        "html": "https://ICML.cc//virtual/2021/poster/10661",
        "abstract": "Multi-agent evaluation aims at the assessment of an agent's strategy on the basis of interaction with others. Typically, existing methods such as $\\alpha$-rank and its approximation still require to exhaustively compare all pairs of joint strategies for an accurate ranking, which in practice is computationally expensive. In this paper, we aim to reduce the number of pairwise comparisons in  recovering a satisfying ranking for $n$ strategies in two-player meta-games, by exploring the fact that agents with similar skills may achieve similar payoffs against others. Two situations are considered: the first one is when we can obtain the true payoffs; the other one is when we can only access noisy payoff. Based on these formulations, we leverage low-rank matrix completion and design two novel algorithms for noise-free and noisy evaluations respectively. For both of these settings, we theorize that  $O(nr \\log n)$ ($n$ is the number of agents and $r$ is the rank of the payoff matrix) payoff entries are required to achieve sufficiently well strategy evaluation performance. Empirical results on evaluating the strategies in three synthetic games and twelve real world games demonstrate that strategy evaluation from a few entries can lead to comparable performance to algorithms with full knowledge of the payoff matrix.",
        "conference": "ICML",
        "中文标题": "通过低秩矩阵补全从少量条目估计α-Rank",
        "摘要翻译": "多智能体评估旨在通过与其他智能体的交互来评估某个智能体的策略。通常，现有方法如α-Rank及其近似仍然需要详尽比较所有联合策略对以获得准确的排名，这在实践中计算成本高昂。在本文中，我们旨在通过探索具有相似技能的智能体可能获得相似回报的事实，减少在双人元游戏中恢复n个策略满意排名所需的成对比较次数。考虑了两种情况：第一种是当我们能够获得真实回报时；另一种是当我们只能访问有噪声的回报时。基于这些表述，我们利用低秩矩阵补全，并分别为无噪声和有噪声评估设计了两种新颖算法。对于这两种设置，我们理论上认为需要O(nr log n)（n是智能体数量，r是回报矩阵的秩）个回报条目才能实现足够好的策略评估性能。在三个合成游戏和十二个真实世界游戏中评估策略的实证结果表明，从少量条目进行策略评估可以达到与完全了解回报矩阵的算法相当的性能。",
        "领域": "多智能体系统、博弈论、机器学习",
        "问题": "减少在多智能体策略评估中所需的成对比较次数，以降低计算成本。",
        "动机": "现有方法如α-Rank需要详尽比较所有联合策略对，计算成本高，因此需要一种更高效的方法。",
        "方法": "利用低秩矩阵补全技术，设计两种算法分别处理无噪声和有噪声的回报数据，以减少所需的成对比较次数。",
        "关键词": [
            "多智能体评估",
            "低秩矩阵补全",
            "策略排名",
            "α-Rank",
            "计算效率"
        ],
        "涉及的技术概念": {
            "低秩矩阵补全": "用于从少量观测数据中恢复完整的低秩矩阵，以减少所需的成对比较次数。",
            "α-Rank": "一种用于多智能体策略评估的排名方法，本文旨在减少其计算成本。",
            "回报矩阵": "表示智能体间交互结果的矩阵，其低秩性质被利用以减少评估所需的数据量。"
        },
        "success": true
    },
    {
        "order": 354,
        "title": "Estimating Identifiable Causal Effects on Markov Equivalence Class through Double Machine Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10727",
        "abstract": "General methods have been developed for estimating causal effects from observational data under causal assumptions encoded in the form of a causal graph. Most of this literature assumes that the underlying causal graph is completely specified. However, only observational data is available in most practical settings, which means that one can learn at most a Markov equivalence class (MEC) of the underlying causal graph. In this paper, we study the problem of causal estimation from a MEC represented by a partial ancestral graph (PAG), which is learnable from observational data. We develop a general estimator for any identifiable causal effects in a PAG. The result fills a gap for an end-to-end solution to causal inference from observational data to effects estimation.  Specifically,  we develop a complete identification algorithm that derives an influence function for any identifiable causal effects from PAGs. We then construct a double/debiased machine learning (DML) estimator that is robust to model misspecification and biases in nuisance function estimation, permitting the use of modern machine learning techniques.  Simulation results corroborate with the theory. ",
        "conference": "ICML",
        "中文标题": "通过双重机器学习估计马尔可夫等价类上的可识别因果效应",
        "摘要翻译": "已经开发了从观察数据中估计因果效应的一般方法，这些方法基于以因果图形式编码的因果假设。大多数文献假设基础因果图是完全指定的。然而，在大多数实际设置中，只有观察数据可用，这意味着最多可以学习到基础因果图的马尔可夫等价类（MEC）。在本文中，我们研究了从由部分祖先图（PAG）表示的MEC进行因果估计的问题，PAG可以从观察数据中学习。我们为PAG中的任何可识别因果效应开发了一个通用估计器。这一结果填补了从观察数据到效应估计的因果推理端到端解决方案的空白。具体来说，我们开发了一个完整的识别算法，该算法从PAGs中导出任何可识别因果效应的影响函数。然后，我们构建了一个双重/去偏机器学习（DML）估计器，该估计器对模型错误指定和干扰函数估计中的偏差具有鲁棒性，允许使用现代机器学习技术。模拟结果与理论相符。",
        "领域": "因果推理、机器学习、统计学习",
        "问题": "在仅能学习到马尔可夫等价类（MEC）的情况下，如何从观察数据中估计可识别的因果效应。",
        "动机": "填补从观察数据到效应估计的因果推理端到端解决方案的空白，特别是在基础因果图不完全指定的情况下。",
        "方法": "开发了一个完整的识别算法来从部分祖先图（PAG）中导出任何可识别因果效应的影响函数，并构建了一个双重/去偏机器学习（DML）估计器。",
        "关键词": [
            "因果推理",
            "马尔可夫等价类",
            "双重机器学习",
            "部分祖先图",
            "影响函数"
        ],
        "涉及的技术概念": {
            "马尔可夫等价类（MEC）": "表示可以从观察数据中学习到的因果图集合，用于在不完全指定因果图的情况下进行因果推理。",
            "部分祖先图（PAG）": "一种可以从观察数据中学习的图形表示，用于表示马尔可夫等价类中的因果结构。",
            "双重/去偏机器学习（DML）": "一种估计技术，通过分离因果效应估计和干扰函数估计来减少偏差，允许使用灵活的机器学习模型。"
        },
        "success": true
    },
    {
        "order": 355,
        "title": "Estimation and Quantization of Expected Persistence Diagrams",
        "html": "https://ICML.cc//virtual/2021/poster/9625",
        "abstract": "Persistence diagrams (PDs) are the most common descriptors used to encode the topology of structured data appearing in challenging learning tasks;~think e.g.~of graphs, time series or point clouds sampled close to a manifold.\nGiven random objects and the corresponding distribution of PDs, one may want to build a statistical summary---such as a mean---of these random PDs, which is however not a trivial task as the natural geometry of the space of PDs is not linear. \nIn this article, we study two such summaries, the Expected Persistence Diagram (EPD), and its quantization. The EPD is a measure supported on $\\mathbb{R}^2$, which may be approximated by its empirical counterpart. We prove that this estimator is optimal from a minimax standpoint on a large class of models with a parametric rate of convergence. The empirical EPD is simple and efficient to compute, but possibly has a very large support, hindering its use in practice. To overcome this issue, we propose an algorithm to compute a quantization of the empirical EPD, a measure with small support which is shown to approximate with near-optimal rates a quantization of the theoretical EPD.",
        "conference": "ICML",
        "中文标题": "期望持久图的估计与量化",
        "摘要翻译": "持久图（PDs）是用于编码在具有挑战性的学习任务中出现的结构化数据拓扑的最常见描述符；例如，考虑图、时间序列或接近流形采样的点云。给定随机对象和相应的PDs分布，人们可能希望构建这些随机PDs的统计摘要——例如均值——然而这并不是一项简单的任务，因为PDs空间的自然几何不是线性的。在本文中，我们研究了两种这样的摘要，期望持久图（EPD）及其量化。EPD是支持在R2上的一个测度，可以通过其经验对应物来近似。我们证明，这个估计器在具有参数收敛速率的一大类模型上从极小极大角度看是最优的。经验EPD计算简单且高效，但可能具有非常大的支持集，这阻碍了其在实际中的使用。为了克服这个问题，我们提出了一种算法来计算经验EPD的量化，这是一种具有小支持集的测度，被证明能以接近最优的速率近似理论EPD的量化。",
        "领域": "拓扑数据分析、机器学习、统计学习",
        "问题": "如何在非线性几何的持久图空间中构建有效的统计摘要",
        "动机": "解决在持久图空间中由于非线性几何特性导致的统计摘要构建难题",
        "方法": "研究期望持久图（EPD）及其量化，提出一种算法计算经验EPD的量化",
        "关键词": [
            "持久图",
            "期望持久图",
            "量化",
            "统计摘要",
            "非线性几何"
        ],
        "涉及的技术概念": {
            "持久图（PDs）": "用于编码结构化数据拓扑的描述符",
            "期望持久图（EPD）": "作为随机持久图的统计摘要，支持在R2上的测度",
            "量化": "减少经验EPD支持集大小的过程，以便于实际应用"
        },
        "success": true
    },
    {
        "order": 356,
        "title": "Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?",
        "html": "https://ICML.cc//virtual/2021/poster/10405",
        "abstract": "Dirichlet-based uncertainty (DBU) models are a recent and promising class of uncertainty-aware models. DBU models predict the parameters of a Dirichlet distribution to provide fast, high-quality uncertainty estimates alongside with class predictions. In this work, we present the first large-scale, in-depth study of the robustness of DBU models under adversarial attacks. Our results suggest that uncertainty estimates of DBU models are not robust w.r.t. three important tasks: (1)\nindicating correctly and wrongly classified samples; (2) detecting adversarial examples; and (3) distinguishing between in-distribution (ID) and out-of-distribution (OOD) data. Additionally, we explore the first approaches to make DBU mod-\nels more robust. While adversarial training has\na minor effect, our median smoothing based ap-\nproach significantly increases robustness of DBU\nmodels.",
        "conference": "ICML",
        "中文标题": "评估预测不确定性估计的鲁棒性：基于Dirichlet的模型可靠吗？",
        "摘要翻译": "基于Dirichlet的不确定性（DBU）模型是近期一类有前景的不确定性感知模型。DBU模型通过预测Dirichlet分布的参数，能够快速提供高质量的不确定性估计以及类别预测。在这项工作中，我们首次对DBU模型在对抗攻击下的鲁棒性进行了大规模、深入的研究。我们的结果表明，DBU模型的不确定性估计在三个重要任务上并不鲁棒：（1）正确和错误分类样本的指示；（2）对抗样本的检测；（3）区分分布内（ID）和分布外（OOD）数据。此外，我们探索了使DBU模型更加鲁棒的首批方法。虽然对抗训练的效果有限，但我们基于中值平滑的方法显著提高了DBU模型的鲁棒性。",
        "领域": "对抗性机器学习、不确定性估计、深度学习安全",
        "问题": "评估和提升基于Dirichlet的不确定性估计模型在对抗攻击下的鲁棒性。",
        "动机": "研究DBU模型在对抗攻击下的不确定性估计是否可靠，并探索提升其鲁棒性的方法。",
        "方法": "通过大规模实验评估DBU模型在对抗攻击下的表现，并采用对抗训练和中值平滑方法来提升模型的鲁棒性。",
        "关键词": [
            "对抗性攻击",
            "不确定性估计",
            "Dirichlet分布",
            "鲁棒性",
            "中值平滑"
        ],
        "涉及的技术概念": {
            "Dirichlet分布": "用于模型预测不确定性估计的概率分布，通过预测其参数来提供不确定性估计。",
            "对抗性攻击": "旨在欺骗机器学习模型的恶意输入，测试模型在对抗条件下的鲁棒性。",
            "中值平滑": "一种提高模型对抗攻击鲁棒性的技术，通过对输入数据进行中值滤波处理来减少对抗性扰动的影响。"
        },
        "success": true
    },
    {
        "order": 357,
        "title": "Evaluating the Implicit Midpoint Integrator for Riemannian Hamiltonian Monte Carlo",
        "html": "https://ICML.cc//virtual/2021/poster/10427",
        "abstract": "Riemannian manifold Hamiltonian Monte Carlo is traditionally carried out using the generalized leapfrog integrator. However, this integrator is not the only choice and other integrators yielding valid Markov chain transition operators may be considered. In this work, we examine the implicit midpoint integrator as an alternative to the generalized leapfrog integrator. We discuss advantages and disadvantages of the implicit midpoint integrator for Hamiltonian Monte Carlo, its theoretical properties, and an empirical assessment of the critical attributes of such an integrator for Hamiltonian Monte Carlo: energy conservation, volume preservation, and reversibility. Empirically, we find that while leapfrog iterations are faster, the implicit midpoint integrator has better energy conservation, leading to higher acceptance rates, as well as better conservation of volume and better reversibility, arguably yielding a more accurate sampling procedure.",
        "conference": "ICML",
        "中文标题": "评估黎曼哈密顿蒙特卡洛中的隐式中点积分器",
        "摘要翻译": "黎曼流形哈密顿蒙特卡洛传统上是使用广义跳跃积分器进行的。然而，这个积分器并不是唯一的选择，也可以考虑其他产生有效马尔可夫链转移算子的积分器。在这项工作中，我们研究了隐式中点积分器作为广义跳跃积分器的替代方案。我们讨论了隐式中点积分器在哈密顿蒙特卡洛中的优缺点、其理论特性，以及对此类积分器在哈密顿蒙特卡洛中关键属性的实证评估：能量守恒、体积保持和可逆性。实证上，我们发现虽然跳跃迭代更快，但隐式中点积分器具有更好的能量守恒性，导致更高的接受率，以及更好的体积保持和更好的可逆性，可以说产生了一个更准确的采样过程。",
        "领域": "概率图模型, 蒙特卡洛方法, 计算统计学",
        "问题": "探索和评估在黎曼流形哈密顿蒙特卡洛中使用隐式中点积分器作为传统广义跳跃积分器替代方案的可行性和效果。",
        "动机": "研究隐式中点积分器在哈密顿蒙特卡洛中的应用，以寻找比传统广义跳跃积分器更优的采样方法。",
        "方法": "通过理论分析和实证评估，比较隐式中点积分器和广义跳跃积分器在能量守恒、体积保持和可逆性等关键属性上的表现。",
        "关键词": [
            "黎曼流形",
            "哈密顿蒙特卡洛",
            "隐式中点积分器",
            "能量守恒",
            "可逆性"
        ],
        "涉及的技术概念": {
            "黎曼流形": "论文中用于描述高维概率分布的空间结构，为哈密顿蒙特卡洛提供几何基础。",
            "哈密顿蒙特卡洛": "一种基于哈密顿动力学的马尔可夫链蒙特卡洛方法，用于高效采样高维概率分布。",
            "隐式中点积分器": "论文中研究的数值积分方法，用于模拟哈密顿动力学，以在黎曼流形上进行采样。"
        },
        "success": true
    },
    {
        "order": 358,
        "title": "Event Outlier Detection in Continuous Time",
        "html": "https://ICML.cc//virtual/2021/poster/8871",
        "abstract": "Continuous-time event sequences represent discrete events occurring in continuous time. Such sequences arise frequently in real-life. Usually we expect the sequences to follow some regular pattern over time. However, sometimes these patterns may be interrupted by unexpected absence or occurrences of events. Identification of these unexpected cases can be very important as they may point to abnormal situations that need human attention. In this work, we study and develop methods for detecting outliers in continuous-time event sequences, including unexpected absence and unexpected occurrences of events. Since the patterns that event sequences tend to follow may change in different contexts, we develop outlier detection methods based on point processes that can take context information into account. Our methods are based on Bayesian decision theory and hypothesis testing with theoretical guarantees. To test the performance of the methods, we conduct experiments on both synthetic data and real-world clinical data and show the effectiveness of the proposed methods.",
        "conference": "ICML",
        "中文标题": "连续时间中的事件异常检测",
        "摘要翻译": "连续时间事件序列表示在连续时间内发生的离散事件。这种序列在现实生活中频繁出现。通常，我们期望这些序列随时间遵循某种规律模式。然而，有时这些模式可能会被事件的意外缺失或发生所打断。识别这些意外情况非常重要，因为它们可能指向需要人类关注的异常情况。在这项工作中，我们研究并开发了用于检测连续时间事件序列中异常的方法，包括事件的意外缺失和意外发生。由于事件序列倾向于遵循的模式可能会在不同的上下文中变化，我们开发了基于点过程的异常检测方法，这些方法可以考虑上下文信息。我们的方法基于贝叶斯决策理论和具有理论保证的假设检验。为了测试这些方法的性能，我们在合成数据和真实世界的临床数据上进行了实验，并展示了所提出方法的有效性。",
        "领域": "时间序列分析、异常检测、医疗数据分析",
        "问题": "检测连续时间事件序列中的异常，包括事件的意外缺失和意外发生。",
        "动机": "识别事件序列中的异常情况，这些异常可能指示需要关注的异常状态。",
        "方法": "基于点过程的异常检测方法，结合贝叶斯决策理论和假设检验，考虑上下文信息。",
        "关键词": [
            "连续时间事件序列",
            "异常检测",
            "点过程",
            "贝叶斯决策理论",
            "假设检验"
        ],
        "涉及的技术概念": {
            "点过程": "用于建模事件在连续时间中的发生，考虑上下文信息以检测异常。",
            "贝叶斯决策理论": "提供了一种基于概率的决策框架，用于识别和分类异常事件。",
            "假设检验": "用于统计验证事件序列中的异常，确保检测方法的理论可靠性。"
        },
        "success": true
    },
    {
        "order": 359,
        "title": "Evolving Attention with Residual Convolutions",
        "html": "https://ICML.cc//virtual/2021/poster/8881",
        "abstract": "Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",
        "conference": "ICML",
        "中文标题": "通过残差卷积演化的注意力机制",
        "摘要翻译": "Transformer是自然语言处理中普遍使用的模型，并在计算机视觉领域引起了广泛关注。注意力图对于Transformer模型编码输入令牌之间的依赖关系是不可或缺的。然而，它们在每一层中独立学习，有时无法捕捉精确的模式。在本文中，我们提出了一种基于演化注意力的新颖通用机制，以提高Transformer的性能。一方面，不同层的注意力图共享共同知识，因此前层的注意力图可以通过残差连接指导后层的注意力。另一方面，低级和高级注意力在抽象层次上有所不同，因此我们采用卷积层来模拟注意力图的演化过程。所提出的演化注意力机制在多个任务上，包括图像分类、自然语言理解和机器翻译，相比各种最先进的模型实现了显著的性能提升。",
        "领域": "自然语言处理与视觉结合, 图像分类, 机器翻译",
        "问题": "Transformer模型中注意力图独立学习，无法有效捕捉输入令牌间的依赖关系。",
        "动机": "提高Transformer模型通过共享层间知识和模拟注意力演化过程来捕捉更精确依赖关系的能力。",
        "方法": "提出一种基于演化注意力的机制，利用残差连接共享层间注意力知识，并通过卷积层模拟注意力图的演化过程。",
        "关键词": [
            "演化注意力",
            "残差连接",
            "卷积层",
            "Transformer模型",
            "性能提升"
        ],
        "涉及的技术概念": {
            "演化注意力": "一种机制，通过模拟注意力图的演化过程来提高Transformer模型捕捉依赖关系的能力。",
            "残差连接": "用于在不同层之间共享注意力知识，指导后续层的注意力学习。",
            "卷积层": "用于模拟注意力图从低级到高级的演化过程，以适应不同抽象层次的注意力需求。"
        },
        "success": true
    },
    {
        "order": 360,
        "title": "Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models",
        "html": "https://ICML.cc//virtual/2021/poster/10045",
        "abstract": "Recent work showed that there could be a large gap between the classical uniform convergence bound and the actual test error of zero-training-error predictors (interpolators) such as deep neural networks. \nTo better understand this gap, we study the uniform convergence in the nonlinear random feature model and perform a precise theoretical analysis on how uniform convergence depends on the sample size and the number of parameters.\nWe derive and prove analytical expressions for three quantities in this model: 1) classical uniform convergence over norm balls, 2) uniform convergence over interpolators in the norm ball (recently proposed by~\\citet{zhou2021uniform}), and 3) the risk of minimum norm interpolator. \nWe show that, in the setting where the classical uniform convergence bound is vacuous (diverges to $\\infty$), uniform convergence over the interpolators still gives a non-trivial bound of the test error of interpolating solutions. \nWe also showcase a different setting where classical uniform convergence bound is non-vacuous, but uniform convergence over interpolators can give an improved sample complexity guarantee.\nOur result provides a first exact comparison between the test errors and uniform convergence bounds for interpolators beyond simple linear models.",
        "conference": "ICML",
        "中文标题": "随机特征模型中泛化误差与一致收敛之间精确差距的研究",
        "摘要翻译": "最近的研究表明，在零训练误差预测器（如深度神经网络）中，经典的一致收敛界限与实际测试误差之间可能存在较大差距。为了更好地理解这一差距，我们研究了非线性随机特征模型中的一致收敛，并对一致收敛如何依赖于样本大小和参数数量进行了精确的理论分析。我们推导并证明了该模型中三个量的解析表达式：1) 经典的在范数球上的一致收敛，2) 在范数球内插值器上的一致收敛（最近由Zhou等人提出），以及3) 最小范数插值器的风险。我们表明，在经典一致收敛界限无意义（发散至无穷大）的情况下，插值器上的一致收敛仍然给出了插值解测试误差的非平凡界限。我们还展示了一个不同的设置，其中经典的一致收敛界限是有意义的，但插值器上的一致收敛可以提供改进的样本复杂度保证。我们的结果首次提供了超出简单线性模型的插值器测试误差与一致收敛界限之间的精确比较。",
        "领域": "深度学习理论、统计学习理论、神经网络泛化分析",
        "问题": "研究在随机特征模型中，泛化误差与一致收敛界限之间的精确差距",
        "动机": "理解深度神经网络等零训练误差预测器中经典一致收敛界限与实际测试误差之间差距的原因",
        "方法": "通过理论分析非线性随机特征模型，推导并比较经典一致收敛、插值器上的一致收敛及最小范数插值器风险的解析表达式",
        "关键词": [
            "随机特征模型",
            "一致收敛",
            "泛化误差",
            "插值器",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "随机特征模型": "用于研究非线性模型中的学习动态和泛化性能的理论框架",
            "一致收敛": "衡量学习算法在训练数据上的性能与在测试数据上性能之间差异的理论工具",
            "最小范数插值器": "在零训练误差条件下，选择具有最小范数的解作为预测器，用于研究插值现象对泛化性能的影响"
        },
        "success": true
    },
    {
        "order": 361,
        "title": "Exact Optimization of Conformal Predictors via Incremental and Decremental Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9681",
        "abstract": "Conformal Predictors (CP) are wrappers around ML models, providing error guarantees under weak assumptions on the data distribution. They are suitable for a wide range of problems, from classification and regression to anomaly detection. Unfortunately, their very high computational complexity limits their applicability to large datasets.\r\nIn this work, we show that it is possible to speed up a CP classifier considerably, by studying it in conjunction with the underlying ML method, and by exploiting incremental\\&decremental learning. For methods such as k-NN, KDE, and kernel LS-SVM, our approach reduces the running time by one order of magnitude, whilst producing exact solutions. With similar ideas, we also achieve a linear speed up for the harder case of bootstrapping.\r\nFinally, we extend these techniques to improve upon an optimization of k-NN CP for regression. We evaluate our findings empirically, and discuss when methods are suitable for CP optimization.",
        "conference": "ICML",
        "中文标题": "通过增量与减量学习精确优化共形预测器",
        "摘要翻译": "共形预测器（CP）是机器学习模型的包装器，在数据分布假设较弱的情况下提供错误保证。它们适用于从分类和回归到异常检测的广泛问题。不幸的是，它们极高的计算复杂性限制了它们在大数据集上的应用。在这项工作中，我们表明，通过与基础机器学习方法结合研究，并利用增量与减量学习，可以显著加快CP分类器的速度。对于k-NN、KDE和核LS-SVM等方法，我们的方法将运行时间减少了一个数量级，同时产生精确解。通过类似的想法，我们还实现了对更难的引导案例的线性加速。最后，我们扩展这些技术以改进k-NN CP回归的优化。我们实证评估了我们的发现，并讨论了何时方法适合CP优化。",
        "领域": "机器学习优化、共形预测、增量学习",
        "问题": "共形预测器在大数据集上的高计算复杂性限制了其应用",
        "动机": "提高共形预测器的计算效率，扩大其在大型数据集上的应用范围",
        "方法": "结合基础机器学习方法，利用增量与减量学习技术优化共形预测器",
        "关键词": [
            "共形预测器",
            "增量学习",
            "减量学习",
            "计算优化",
            "机器学习"
        ],
        "涉及的技术概念": {
            "共形预测器": "一种提供错误保证的机器学习模型包装器，适用于分类、回归和异常检测等多种问题",
            "增量与减量学习": "通过逐步增加或减少数据来优化模型训练过程，提高计算效率",
            "k-NN、KDE和核LS-SVM": "基础机器学习方法，共形预测器通过这些方法实现分类和回归任务"
        },
        "success": true
    },
    {
        "order": 362,
        "title": "Examining and Combating Spurious Features under Distribution Shift",
        "html": "https://ICML.cc//virtual/2021/poster/9131",
        "abstract": "A central goal of machine learning is to learn robust representations that capture the fundamental relationship between inputs and output labels. However, minimizing training errors over finite or biased datasets results in models latching on to spurious correlations between the training input/output pairs that are not fundamental to the problem at hand. In this paper, we define and analyze robust and spurious representations using the information-theoretic concept of minimal sufficient statistics. We prove that even when there is only bias of the input distribution (i.e. covariate shift), models can still pick up spurious features from their training data.\nGroup distributionally robust optimization (DRO) provides an effective tool to alleviate covariate shift by minimizing the worst-case training losses over a set of pre-defined groups. Inspired by our analysis, we demonstrate that group DRO can fail when groups do not directly account for various spurious correlations that occur in the data. To address this, we further propose to minimize the worst-case losses over a more flexible set of distributions that are defined on the joint distribution of groups and instances, instead of treating each group as a whole at optimization time. Through extensive experiments on one image and two language tasks, we show that our model is significantly more robust than comparable baselines under various partitions.",
        "conference": "ICML",
        "中文标题": "检验与对抗分布偏移下的虚假特征",
        "摘要翻译": "机器学习的一个核心目标是学习能够捕捉输入与输出标签之间基本关系的鲁棒表示。然而，在有限或有偏数据集上最小化训练误差会导致模型依赖于训练输入/输出对之间的虚假相关性，这些相关性并非手头问题的根本。在本文中，我们使用信息论中的最小充分统计量概念来定义和分析鲁棒与虚假表示。我们证明，即使仅存在输入分布的偏差（即协变量偏移），模型仍可能从训练数据中提取虚假特征。组分布鲁棒优化（DRO）通过最小化预定义组集合上的最坏情况训练损失，为缓解协变量偏移提供了有效工具。受我们分析的启发，我们展示了当组不直接考虑数据中出现的各种虚假相关性时，组DRO可能会失败。为了解决这个问题，我们进一步提出在更灵活的分布集合上最小化最坏情况损失，这些分布定义在组和实例的联合分布上，而不是在优化时将每个组作为一个整体。通过在图像和两个语言任务上的广泛实验，我们表明在各种分区下，我们的模型比可比基线显著更鲁棒。",
        "领域": "机器学习鲁棒性、分布偏移适应、虚假相关性检测",
        "问题": "机器学习模型在有限或有偏数据集上训练时，容易学习到输入与输出之间的虚假相关性，而非根本关系。",
        "动机": "研究旨在解决机器学习模型在分布偏移下依赖虚假特征的问题，以提高模型的鲁棒性和泛化能力。",
        "方法": "使用信息论的最小充分统计量分析鲁棒与虚假表示，提出在组和实例的联合分布上最小化最坏情况损失的方法，以替代传统的组DRO方法。",
        "关键词": [
            "虚假特征",
            "分布鲁棒优化",
            "协变量偏移",
            "最小充分统计量",
            "联合分布优化"
        ],
        "涉及的技术概念": {
            "最小充分统计量": "用于定义和分析鲁棒与虚假表示的信息论概念，帮助区分根本特征与虚假特征。",
            "组分布鲁棒优化（DRO）": "通过最小化预定义组集合上的最坏情况训练损失来缓解协变量偏移的方法。",
            "联合分布优化": "在组和实例的联合分布上最小化最坏情况损失，以提高模型对虚假相关性的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 363,
        "title": "Explainable Automated Graph Representation Learning with Hyperparameter Importance",
        "html": "https://ICML.cc//virtual/2021/poster/9679",
        "abstract": "Current graph representation (GR) algorithms require huge demand of human experts in hyperparameter tuning, which significantly limits their practical applications, leading to an urge for automated graph representation without human intervention. Although automated machine learning (AutoML) serves as a good candidate for automatic hyperparameter tuning, little literature has been reported on automated graph presentation learning and the only existing work employs a black-box strategy, lacking insights into explaining the relative importance of different hyperparameters. To address this issue, we study explainable automated graph representation with hyperparameter importance in this paper. We propose an explainable AutoML approach for graph representation (e-AutoGR) which utilizes explainable graph features during performance estimation and learns decorrelated importance weights for different hyperparameters in affecting the model performance through a non-linear decorrelated weighting regression. These learned importance weights can in turn help to provide more insights in hyperparameter search procedure. We theoretically prove the soundness of the decorrelated weighting algorithm. Extensive experiments on real-world datasets demonstrate the superiority of our proposed e-AutoGR model against state-of-the-art methods in terms of both model performance and hyperparameter importance explainability.",
        "conference": "ICML",
        "中文标题": "可解释的自动化图表示学习与超参数重要性",
        "摘要翻译": "当前的图表示（GR）算法在超参数调优方面需要大量依赖人类专家，这极大地限制了它们的实际应用，从而迫切需要无需人工干预的自动化图表示。尽管自动化机器学习（AutoML）作为自动超参数调优的良好候选者，但关于自动化图表示学习的文献报道极少，唯一现有的工作采用了黑盒策略，缺乏解释不同超参数相对重要性的洞察力。为了解决这一问题，我们在本文中研究了具有超参数重要性的可解释自动化图表示。我们提出了一种用于图表示的可解释AutoML方法（e-AutoGR），该方法在性能估计过程中利用可解释的图特征，并通过非线性去相关加权回归学习不同超参数在影响模型性能方面的去相关重要性权重。这些学习到的重要性权重反过来可以帮助在超参数搜索过程中提供更多洞察。我们从理论上证明了去相关加权算法的合理性。在真实世界数据集上的大量实验表明，我们提出的e-AutoGR模型在模型性能和超参数重要性可解释性方面均优于最先进的方法。",
        "领域": "图表示学习、自动化机器学习、超参数优化",
        "问题": "解决图表示学习中超参数调优依赖人工、缺乏超参数重要性解释的问题",
        "动机": "减少图表示学习中对人类专家的依赖，提高超参数调优的自动化程度和可解释性",
        "方法": "提出了一种可解释的AutoML方法（e-AutoGR），利用可解释的图特征和非线性去相关加权回归学习超参数的重要性权重",
        "关键词": [
            "图表示学习",
            "自动化机器学习",
            "超参数优化",
            "可解释性",
            "去相关加权回归"
        ],
        "涉及的技术概念": {
            "可解释的图特征": "在性能估计过程中使用的特征，有助于理解模型决策的依据",
            "非线性去相关加权回归": "用于学习不同超参数在影响模型性能方面的去相关重要性权重的技术",
            "超参数重要性解释": "通过学习的权重提供超参数在模型性能中作用的洞察"
        },
        "success": true
    },
    {
        "order": 364,
        "title": "Explaining Time Series Predictions with Dynamic Masks",
        "html": "https://ICML.cc//virtual/2021/poster/8815",
        "abstract": "How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant. ",
        "conference": "ICML",
        "中文标题": "用动态掩码解释时间序列预测",
        "摘要翻译": "我们如何解释机器学习模型的预测？当数据以多元时间序列的形式结构化时，这个问题引发了额外的困难，例如解释需要体现时间依赖性以及大量的输入。为了应对这些挑战，我们提出了动态掩码（Dynamask）。该方法通过将扰动掩码拟合到输入序列，为每个时间步的每个特征生成实例级的重要性分数。为了融入数据的时间依赖性，Dynamask研究了动态扰动操作的效果。为了解决大量输入的问题，我们提出了一种方案，使特征选择既简约（选择不超过必要的特征）又清晰（我们通过信息理论的平行概念详细说明了这一概念）。通过合成和真实世界的数据，我们证明了Dynamask的动态基础及其简约性，在随时间识别特征重要性方面提供了明显的改进。Dynamask的模块化使其成为提高广泛机器学习模型透明度的理想插件，适用于医学和金融等时间序列丰富的领域。",
        "领域": "时间序列分析、机器学习解释性、特征选择",
        "问题": "如何在多元时间序列数据中解释机器学习模型的预测，同时处理时间依赖性和大量输入的问题。",
        "动机": "提高机器学习模型在时间序列数据上的预测解释性，特别是在需要理解时间依赖性和处理大量输入特征的场景。",
        "方法": "提出动态掩码（Dynamask）方法，通过拟合扰动掩码到输入序列来生成特征和时间步的重要性分数，研究动态扰动操作的效果，并提出简约和清晰的特征选择方案。",
        "关键词": [
            "动态掩码",
            "时间序列分析",
            "机器学习解释性",
            "特征选择",
            "信息理论"
        ],
        "涉及的技术概念": {
            "动态掩码": "一种通过拟合扰动掩码到输入序列来生成特征和时间步重要性分数的方法，用于提高模型预测的解释性。",
            "时间依赖性": "数据随时间变化的特性，Dynamask通过研究动态扰动操作的效果来融入这一特性。",
            "特征选择": "Dynamask提出的一种简约和清晰的方案，用于从大量输入中选择不超过必要的特征，以提高解释的清晰度。"
        },
        "success": true
    },
    {
        "order": 365,
        "title": "Explanations for Monotonic Classifiers.",
        "html": "https://ICML.cc//virtual/2021/poster/9955",
        "abstract": "In many classification tasks there is a requirement of monotonicity. Concretely, if all else remains constant, increasing (resp.~decreasing) the value of one or more features must not decrease (resp.~increase) the value of the prediction. Despite comprehensive efforts on learning monotonic classifiers, dedicated approaches for explaining monotonic classifiers are scarce and classifier-specific. This paper describes novel algorithms for the computation of one formal explanation of a (black-box) monotonic classiﬁer. These novel algorithms are polynomial (indeed linear) in the run time complexity of the classifier. Furthermore, the paper presents a practically efficient model-agnostic algorithm for enumerating formal explanations. ",
        "conference": "ICML",
        "中文标题": "单调分类器的解释方法",
        "摘要翻译": "在许多分类任务中，存在单调性的要求。具体来说，如果其他条件保持不变，增加（或减少）一个或多个特征的值不得导致预测值的减少（或增加）。尽管在学习单调分类器方面已有全面的努力，但专门用于解释单调分类器的方法却很少且特定于分类器。本文描述了计算（黑盒）单调分类器的一种形式解释的新算法。这些新算法在分类器的运行时间复杂度上是多项式（实际上是线性）的。此外，本文还提出了一种实际高效的模型无关算法，用于枚举形式解释。",
        "领域": "机器学习解释性、单调性学习、形式化方法",
        "问题": "如何为单调分类器提供形式化的解释方法",
        "动机": "解决单调分类器解释方法稀缺且特定于分类器的问题",
        "方法": "提出了计算单调分类器形式解释的新算法和模型无关的枚举算法",
        "关键词": [
            "单调分类器",
            "形式解释",
            "模型无关算法",
            "多项式时间复杂度",
            "枚举算法"
        ],
        "涉及的技术概念": {
            "单调分类器": "在分类任务中，分类器的预测值随特征值的增加而单调不减或随特征值的减少而单调不增",
            "形式解释": "对分类器决策过程的数学或逻辑上的严格描述，用于提高模型的可解释性",
            "模型无关算法": "不依赖于特定模型结构的算法，能够广泛应用于不同类型的分类器"
        },
        "success": true
    },
    {
        "order": 366,
        "title": "Exploiting Shared Representations for Personalized Federated Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10309",
        "abstract": "Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.",
        "conference": "ICML",
        "中文标题": "利用共享表示进行个性化联邦学习",
        "摘要翻译": "深度神经网络已展现出从图像和文本等数据中提取通用特征表示的能力，这些表示对各种学习任务都非常有用。然而，在联邦学习环境中，表示学习的成果尚未得到充分实现。尽管联邦环境中的数据通常在各客户端之间是非独立同分布的，但集中式深度学习的成功表明，数据通常共享一个全局特征表示，而客户端或任务之间的统计异质性则集中在标签上。基于这一直觉，我们提出了一种新颖的联邦学习框架和算法，用于学习跨客户端的共享数据表示和每个客户端的独特本地头部。我们的算法利用客户端的分布式计算能力，在每次更新表示时，对低维本地参数执行多次本地更新。我们证明，在线性设置中，该方法能以接近最优的样本复杂度线性收敛到真实表示，表明它能有效地减少每个客户端的问题维度。此外，我们提供了大量实验结果，证明了我们的方法在异构环境中优于其他个性化联邦学习方法。",
        "领域": "联邦学习、个性化学习、表示学习",
        "问题": "在联邦学习环境中，如何有效利用共享特征表示来处理非独立同分布数据，同时保持个性化学习能力。",
        "动机": "解决联邦学习环境中数据非独立同分布导致的统计异质性问题，同时利用共享特征表示提高学习效率和效果。",
        "方法": "提出了一种联邦学习框架和算法，通过跨客户端学习共享数据表示和每个客户端的独特本地头部，利用分布式计算能力进行高效学习。",
        "关键词": [
            "联邦学习",
            "个性化学习",
            "表示学习",
            "非独立同分布",
            "统计异质性"
        ],
        "涉及的技术概念": {
            "共享数据表示": "在联邦学习环境中，跨客户端共享的全局特征表示，用于捕捉数据的通用特征。",
            "本地头部": "每个客户端独有的参数部分，用于处理客户端特定的统计异质性。",
            "分布式计算": "利用多个客户端的计算资源并行处理数据，提高学习效率和可扩展性。"
        },
        "success": true
    },
    {
        "order": 367,
        "title": "Exploiting structured data for learning contagious diseases under incomplete testing",
        "html": "https://ICML.cc//virtual/2021/poster/8807",
        "abstract": "One of the ways that machine learning algorithms can help control the spread of an infectious disease is by building models that predict who is likely to become infected making them good candidates for preemptive interventions. In this work we ask: can we build reliable infection prediction models when the observed data is collected under limited, and biased testing that prioritizes testing symptomatic individuals? Our analysis suggests that when the infection is highly transmissible, incomplete testing might be sufficient to achieve good out-of-sample prediction error. Guided by this insight, we develop an algorithm that predicts infections, and show that it outperforms baselines on simulated data. We apply our model to data from a large hospital to predict Clostridioides difficile infections; a communicable disease that is characterized by both symptomatically infected and asymptomatic (i.e., untested) carriers. Using a proxy instead of the unobserved untested-infected state, we show that our model outperforms benchmarks in predicting infections.",
        "conference": "ICML",
        "中文标题": "利用结构化数据在不完整检测条件下学习传染性疾病",
        "摘要翻译": "机器学习算法帮助控制传染病传播的一种方式是通过建立预测谁可能被感染的模型，使他们成为先发制人干预的良好候选者。在这项工作中，我们提出：当观察到的数据是在有限且有偏见的检测条件下收集的，优先检测有症状的个体时，我们能否建立可靠的感染预测模型？我们的分析表明，当感染具有高度传播性时，不完整的检测可能足以实现良好的样本外预测误差。基于这一见解，我们开发了一种预测感染的算法，并显示其在模拟数据上优于基线。我们将我们的模型应用于来自一家大型医院的数据，以预测艰难梭菌感染；这是一种以有症状感染者和无症状（即未检测）携带者为特征的传染病。使用代理而非未观察到的未检测感染状态，我们表明我们的模型在预测感染方面优于基准。",
        "领域": "传染病预测、医疗数据分析、机器学习应用",
        "问题": "在有限且有偏见的检测条件下，如何建立可靠的感染预测模型。",
        "动机": "探索在不完整和有偏见的检测数据下，机器学习算法是否能够有效预测传染病感染，以支持先发制人的干预措施。",
        "方法": "开发了一种预测感染的算法，该算法在模拟数据和真实医院数据上进行了测试，特别是在预测艰难梭菌感染方面，使用代理变量处理未检测的感染状态。",
        "关键词": [
            "传染病预测",
            "机器学习",
            "医疗数据分析",
            "艰难梭菌感染",
            "样本外预测"
        ],
        "涉及的技术概念": {
            "样本外预测误差": "用于评估模型在未见数据上的预测性能，确保模型的泛化能力。",
            "代理变量": "在无法直接观测到某些变量的情况下，使用可观测的变量作为替代，以间接估计或预测目标变量。",
            "先发制人干预": "基于预测模型的输出，对可能被感染的个体采取早期干预措施，以控制疾病的传播。"
        },
        "success": true
    },
    {
        "order": 368,
        "title": "Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10379",
        "abstract": "To rapidly learn a new task, it is often essential for agents to explore efficiently - especially when performance matters from the first timestep. One way to learn such behaviour is via meta-learning. Many existing methods however rely on dense rewards for meta-training, and can fail catastrophically if the rewards are sparse.  \tWithout a suitable reward signal, the need for exploration during meta-training is exacerbated. To address this, we propose HyperX, which uses novel reward bonuses for meta-training to explore in approximate hyper-state space (where hyper-states represent the environment state and the agent's task belief). We show empirically that HyperX meta-learns better task-exploration and adapts more successfully to new tasks than existing methods.",
        "conference": "ICML",
        "中文标题": "在近似超状态空间中探索元强化学习",
        "摘要翻译": "为了快速学习新任务，智能体通常需要高效探索——尤其是在性能从第一时间步就至关重要时。学习这种行为的一种方式是通过元学习。然而，许多现有方法依赖于密集奖励进行元训练，如果奖励稀疏，可能会灾难性地失败。没有合适的奖励信号，元训练期间对探索的需求会更加突出。为了解决这个问题，我们提出了HyperX，它使用新颖的奖励奖金在近似超状态空间（其中超状态表示环境状态和智能体的任务信念）中进行元训练探索。我们通过实验证明，与现有方法相比，HyperX能够更好地元学习任务探索，并更成功地适应新任务。",
        "领域": "元学习、强化学习、探索策略",
        "问题": "解决在稀疏奖励环境下元强化学习中的高效探索问题",
        "动机": "在稀疏奖励条件下，现有元强化学习方法可能无法有效探索，导致学习失败，因此需要开发新的探索策略以提高学习效率和适应性",
        "方法": "提出HyperX方法，通过在近似超状态空间中使用新颖的奖励奖金来促进元训练期间的探索",
        "关键词": [
            "元强化学习",
            "探索策略",
            "超状态空间",
            "稀疏奖励",
            "任务适应"
        ],
        "涉及的技术概念": {
            "超状态空间": "表示环境状态和智能体任务信念的组合空间，用于更全面地指导探索",
            "奖励奖金": "在元训练期间引入的额外奖励信号，以鼓励在稀疏奖励环境下的探索",
            "元学习": "通过学习如何学习，使智能体能够快速适应新任务的技术"
        },
        "success": true
    },
    {
        "order": 369,
        "title": "Explore Visual Concept Formation for Image Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9891",
        "abstract": "Human beings acquire the ability of image classification through visual concept learning, in which the process of concept formation involves intertwined searches of common properties and concept descriptions. However, in most image classification algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are fixed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this, we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of ConvNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves the ability of identifying unseen samples on cifar10, STL10, flower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. The code is available at \\url{https://github.com/elvintanhust/LSOVCF}.",
        "conference": "ICML",
        "中文标题": "探索视觉概念形成在图像分类中的应用",
        "摘要翻译": "人类通过视觉概念学习获得图像分类的能力，在这一过程中，概念的形成涉及对共同属性和概念描述的相互交织的探索。然而，在大多数使用深度卷积神经网络（ConvNet）的图像分类算法中，表示空间的构建前提是概念描述被固定为一热编码，这限制了属性的挖掘和识别未见样本的能力。受此启发，我们提出了一种基于ConvNet的视觉概念形成学习策略（LSOVCF），其中概念形成的两个相互交织的部分，即特征提取和概念描述，被一起学习。首先，LSOVCF采用ConvNet最后一层的样本响应来归纳假设为高斯分布的概念描述，这是训练过程的一部分。其次，设计了探索和经验损失进行优化，采用经验缓存池来加速收敛。实验表明，基于从经典VGG到SOTA Ghostnet的多种骨干网络，LSOVCF在cifar10、STL10、flower17和ImageNet上提高了识别未见样本的能力。代码可在https://github.com/elvintanhust/LSOVCF获取。",
        "领域": "图像分类、深度学习、视觉概念学习",
        "问题": "解决现有图像分类算法中概念描述固定为一热编码，限制属性挖掘和识别未见样本能力的问题",
        "动机": "受人类视觉概念学习过程的启发，探索更灵活的概念描述方式以提升图像分类的性能",
        "方法": "提出一种视觉概念形成学习策略（LSOVCF），将特征提取和概念描述作为相互交织的部分一起学习，采用高斯分布假设概念描述，并设计探索和经验损失进行优化",
        "关键词": [
            "视觉概念形成",
            "图像分类",
            "深度学习",
            "卷积神经网络",
            "未见样本识别"
        ],
        "涉及的技术概念": {
            "视觉概念形成": "论文中提出的学习策略，旨在通过模拟人类视觉概念学习过程来提升图像分类的性能",
            "高斯分布": "用于假设概念描述的统计分布，作为训练过程的一部分，帮助模型更灵活地学习概念",
            "探索和经验损失": "论文中设计的优化损失函数，通过经验缓存池加速模型收敛，提升学习效率"
        },
        "success": true
    },
    {
        "order": 370,
        "title": "Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL",
        "html": "https://ICML.cc//virtual/2021/poster/8421",
        "abstract": "Several practical applications of reinforcement learning involve an agent learning from past data without the possibility of further exploration. Often these applications require us to 1) identify a near optimal policy or to 2) estimate the value of a target policy. For both tasks we derive exponential information-theoretic lower bounds in discounted infinite horizon MDPs with a linear function representation for the action value function even if 1) realizability holds, 2) the batch algorithm observes the exact reward and transition functions, and 3) the batch algorithm is given the best a priori data distribution for the problem class. Our work introduces a new `oracle + batch algorithm' framework to prove lower bounds that hold for every  distribution. The work shows an exponential separation between batch and online reinforcement learning.",
        "conference": "ICML",
        "中文标题": "批量强化学习的指数级下界：批量RL可能比在线RL难指数级",
        "摘要翻译": "强化学习的几个实际应用涉及代理从过去的数据中学习，而没有进一步探索的可能性。这些应用通常需要我们1)识别一个接近最优的策略，或2)估计目标策略的价值。对于这两个任务，即使在1)可实现性成立，2)批量算法观察到确切的奖励和转移函数，以及3)批量算法获得了问题类别的最佳先验数据分布的情况下，我们在折扣无限水平MDP中为动作价值函数的线性函数表示推导出了指数级的信息论下界。我们的工作引入了一个新的‘预言机+批量算法’框架来证明对每个分布都成立的下界。这项工作展示了批量与在线强化学习之间的指数级分离。",
        "领域": "强化学习理论",
        "问题": "在批量强化学习中，即使满足理想条件，识别接近最优策略或估计目标策略价值的任务仍然面临指数级的信息论下界。",
        "动机": "揭示批量强化学习与在线强化学习在理论上的性能差异，特别是在信息获取效率方面的根本限制。",
        "方法": "引入‘预言机+批量算法’框架，通过信息论方法推导出在折扣无限水平MDP中动作价值函数线性表示下的指数级下界。",
        "关键词": [
            "批量强化学习",
            "信息论下界",
            "在线强化学习",
            "MDP",
            "线性函数表示"
        ],
        "涉及的技术概念": {
            "折扣无限水平MDP": "用于建模强化学习问题的数学框架，其中未来的奖励被折扣因子折现。",
            "线性函数表示": "动作价值函数的一种参数化表示方法，假设其可以表示为特征的线性组合。",
            "信息论下界": "在信息理论中，用于量化解决问题所需的最小信息量的界限，这里用于证明批量强化学习的效率限制。"
        },
        "success": true
    },
    {
        "order": 371,
        "title": "Exponentially Many Local Minima in Quantum Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10073",
        "abstract": "Quantum Neural Networks (QNNs), or the so-called variational quantum circuits,\nare important quantum applications both because of their similar promises as\nclassical neural networks and because of the feasibility of their implementation\non near-term intermediate-size noisy quantum machines (NISQ). However, the\ntraining task of QNNs is challenging and much less understood. We conduct a\nquantitative investigation on the landscape of loss functions of QNNs and\nidentify a class of simple yet extremely hard  QNN instances for training.\nSpecifically, we show for typical under-parameterized QNNs,\nthere exists a dataset that induces a loss function with the number of spurious\nlocal minima depending exponentially on the number of parameters.\nMoreover, we show the optimality of our construction by providing an almost\nmatching upper bound on such dependence.\nWhile local minima in classical neural networks are due to non-linear\nactivations, in quantum neural networks local minima appear as a result of the\nquantum interference phenomenon.\nFinally, we empirically confirm that our constructions can\nindeed be hard instances in practice with typical gradient-based optimizers, which\ndemonstrates the practical value of our findings. \n",
        "conference": "ICML",
        "中文标题": "量子神经网络中指数级多的局部极小值",
        "摘要翻译": "量子神经网络（QNNs），或称变分量子电路，因其与经典神经网络相似的承诺以及在近期中等规模噪声量子机器（NISQ）上实现的可行性，成为重要的量子应用。然而，QNNs的训练任务具有挑战性且理解较少。我们对QNNs损失函数的景观进行了定量研究，并识别出一类简单但极其难以训练的QNN实例。具体来说，我们展示了对于典型的欠参数化QNNs，存在一个数据集，其诱导的损失函数中虚假局部极小值的数量与参数数量呈指数依赖关系。此外，我们通过提供几乎匹配的上界来展示我们构造的最优性。虽然经典神经网络中的局部极小值是由于非线性激活函数引起的，但在量子神经网络中，局部极小值是由于量子干涉现象产生的。最后，我们通过实验证实，我们的构造在实践中确实可以成为典型的基于梯度的优化器的困难实例，这证明了我们发现的实用价值。",
        "领域": "量子机器学习、变分量子算法、量子优化",
        "问题": "量子神经网络训练过程中存在的指数级多的局部极小值问题",
        "动机": "探索量子神经网络训练中的挑战，特别是局部极小值问题，以提高训练效率和模型性能",
        "方法": "通过定量研究量子神经网络损失函数的景观，识别并构造出具有指数级多局部极小值的简单QNN实例，并通过实验验证其训练难度",
        "关键词": [
            "量子神经网络",
            "局部极小值",
            "变分量子电路",
            "量子干涉",
            "梯度优化"
        ],
        "涉及的技术概念": {
            "量子神经网络（QNNs）": "一种结合量子计算和神经网络原理的模型，用于处理量子数据或利用量子特性进行高效计算",
            "变分量子电路": "量子神经网络的一种实现方式，通过参数化的量子门序列来近似解决特定问题",
            "量子干涉": "量子力学中的现象，在量子神经网络中导致损失函数景观中出现局部极小值"
        },
        "success": true
    },
    {
        "order": 372,
        "title": "Exponential Reduction in Sample Complexity with Learning of Ising Model Dynamics",
        "html": "https://ICML.cc//virtual/2021/poster/8691",
        "abstract": "The usual setting for learning the structure and parameters of a graphical model assumes the availability of independent samples produced from the corresponding multivariate probability distribution. However, for many models the mixing time of the respective Markov chain can be very large and i.i.d. samples may not be obtained. We study the problem of reconstructing binary graphical models from correlated samples produced by a dynamical process, which is natural in many applications. We analyze the sample complexity of two estimators that are based on the interaction screening objective and the conditional likelihood loss. We observe that for samples coming from a dynamical process far from equilibrium, the sample complexity reduces exponentially compared to a dynamical process that mixes quickly.",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过伊辛模型动力学学习实现样本复杂度的指数级降低",
        "摘要翻译": "通常学习图形模型结构和参数的设置假设可以从相应的多元概率分布中获得独立样本。然而，对于许多模型来说，相应马尔可夫链的混合时间可能非常长，可能无法获得独立同分布样本。我们研究了从动态过程产生的相关样本中重建二元图形模型的问题，这在许多应用中是很自然的。我们分析了基于交互筛选目标和条件似然损失的两种估计器的样本复杂度。我们观察到，对于远离平衡的动态过程产生的样本，与快速混合的动态过程相比，样本复杂度呈指数级降低。",
        "领域": "统计机器学习、概率图模型、动态系统学习",
        "问题": "如何从动态过程产生的相关样本中有效学习二元图形模型的结构和参数",
        "动机": "解决在无法获得独立同分布样本的情况下，如何有效学习图形模型的问题，特别是在动态过程远离平衡时样本复杂度显著降低的现象",
        "方法": "基于交互筛选目标和条件似然损失的估计器分析，比较不同动态过程下的样本复杂度",
        "关键词": [
            "伊辛模型",
            "样本复杂度",
            "动态过程",
            "图形模型",
            "交互筛选"
        ],
        "涉及的技术概念": {
            "交互筛选目标": "用于评估和选择图形模型中变量间交互作用的目标函数，帮助从相关样本中学习模型结构",
            "条件似然损失": "基于条件概率的损失函数，用于优化图形模型参数，考虑样本间的相关性",
            "马尔可夫链混合时间": "描述马尔可夫链从初始分布收敛到平稳分布所需的时间，影响样本的独立性和模型学习效率"
        }
    },
    {
        "order": 373,
        "title": "Expressive 1-Lipschitz Neural Networks for Robust Multiple Graph Learning against Adversarial Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9793",
        "abstract": "Recent findings have shown multiple graph learning models, such as graph classification and graph matching, are highly vulnerable to adversarial attacks, i.e. small input perturbations in graph structures and node attributes can cause the model failures. Existing defense techniques often defend specific attacks on particular multiple graph learning tasks. This paper proposes an attack-agnostic graph-adaptive 1-Lipschitz neural network, ERNN, for improving the robustness of deep multiple graph learning while achieving remarkable expressive power. A K_l-Lipschitz Weibull activation function is designed to enforce the gradient norm as K_l at layer l. The nearest matrix orthogonalization and polar decomposition techniques are utilized to constraint the weight norm as 1/K_l and make the norm-constrained weight close to the original weight. The theoretical analysis is conducted to derive lower and upper bounds of feasible K_l under the 1-Lipschitz constraint. The combination of norm-constrained weight and activation function leads to the 1-Lipschitz neural network for expressive and robust multiple graph learning.",
        "conference": "ICML",
        "中文标题": "表达性1-Lipschitz神经网络用于对抗攻击下的鲁棒多图学习",
        "摘要翻译": "最近的研究发现，多图学习模型，如图分类和图匹配，对对抗攻击高度脆弱，即图结构和节点属性的小输入扰动可能导致模型失败。现有的防御技术通常针对特定多图学习任务的特定攻击进行防御。本文提出了一种攻击无关的图自适应1-Lipschitz神经网络ERNN，用于在实现显著表达能力的同时提高深度多图学习的鲁棒性。设计了K_l-Lipschitz Weibull激活函数，以在层l强制执行梯度范数为K_l。利用最近矩阵正交化和极分解技术将权重范数约束为1/K_l，并使范数约束的权重接近原始权重。进行了理论分析，以推导在1-Lipschitz约束下可行的K_l的下界和上界。范数约束权重和激活函数的结合导致了用于表达性和鲁棒多图学习的1-Lipschitz神经网络。",
        "领域": "图神经网络、对抗性防御、多图学习",
        "问题": "提高多图学习模型对对抗攻击的鲁棒性",
        "动机": "现有的多图学习模型对对抗攻击高度脆弱，需要一种通用的防御方法来提高模型的鲁棒性",
        "方法": "提出了一种攻击无关的图自适应1-Lipschitz神经网络ERNN，结合K_l-Lipschitz Weibull激活函数和范数约束权重技术",
        "关键词": [
            "1-Lipschitz神经网络",
            "对抗性防御",
            "多图学习",
            "Weibull激活函数",
            "范数约束"
        ],
        "涉及的技术概念": {
            "1-Lipschitz神经网络": "用于提高模型对对抗攻击的鲁棒性，通过限制模型的Lipschitz常数来确保输入的微小变化不会导致输出的剧烈变化",
            "Weibull激活函数": "设计用于在神经网络层中强制执行特定的梯度范数，以增强模型的表达能力和鲁棒性",
            "范数约束权重": "通过矩阵正交化和极分解技术约束权重范数，使模型在保持性能的同时提高对抗攻击的抵抗力"
        },
        "success": true
    },
    {
        "order": 374,
        "title": "Factor-analytic inverse regression for high-dimension, small-sample dimensionality reduction",
        "html": "https://ICML.cc//virtual/2021/poster/8695",
        "abstract": "Sufficient dimension reduction (SDR) methods are a family of supervised methods for dimensionality reduction that seek to reduce dimensionality while preserving information about a target variable of interest. However, existing SDR methods typically require more observations than the number of dimensions ($N > p$). To overcome this limitation, we propose Class-conditional Factor Analytic Dimensions (CFAD), a model-based dimensionality reduction method for high-dimensional, small-sample data. We show that CFAD substantially outperforms existing SDR methods in the small-sample regime, and can be extended to incorporate prior information such as smoothness in the projection axes. We demonstrate the effectiveness of CFAD with an application to functional magnetic resonance imaging (fMRI) measurements during visual object recognition and working memory tasks, where it outperforms existing SDR and a variety of other dimensionality-reduction methods.",
        "conference": "ICML",
        "中文标题": "高维小样本降维的因子分析逆回归",
        "摘要翻译": "充分降维（SDR）方法是一类监督降维方法，旨在降低维度的同时保留关于感兴趣目标变量的信息。然而，现有的SDR方法通常需要的观测数量多于维度数量（N > p）。为了克服这一限制，我们提出了类条件因子分析维度（CFAD），一种基于模型的高维小样本数据降维方法。我们证明，在小样本情况下，CFAD显著优于现有的SDR方法，并且可以扩展以纳入先验信息，如投影轴的平滑性。我们通过在视觉对象识别和工作记忆任务中的功能磁共振成像（fMRI）测量应用，展示了CFAD的有效性，其表现优于现有的SDR和其他多种降维方法。",
        "领域": "高维数据分析、小样本学习、功能磁共振成像分析",
        "问题": "解决在高维小样本数据中进行有效降维的问题",
        "动机": "克服现有充分降维方法在小样本情况下性能不足的限制",
        "方法": "提出了一种基于模型的降维方法CFAD，能够在小样本情况下有效工作，并可纳入先验信息",
        "关键词": [
            "充分降维",
            "高维小样本",
            "因子分析",
            "功能磁共振成像",
            "监督学习"
        ],
        "涉及的技术概念": {
            "充分降维（SDR）": "一类监督降维方法，旨在降低维度的同时保留关于目标变量的信息",
            "类条件因子分析维度（CFAD）": "一种基于模型的高维小样本数据降维方法，能够在小样本情况下有效工作",
            "功能磁共振成像（fMRI）": "用于测量大脑活动的技术，本文中用于验证CFAD方法的有效性"
        },
        "success": true
    },
    {
        "order": 375,
        "title": "Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees",
        "html": "https://ICML.cc//virtual/2021/poster/10551",
        "abstract": "We present an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes. Compared to prior work, our framework can be employed with a very general class of linear and linear-fractional fairness constraints, can handle multiple, non-binary protected attributes, and outputs a classifier that comes with provable guarantees on both accuracy and fairness. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise is large, in two real-world datasets.",
        "conference": "ICML",
        "中文标题": "具有可证明保证的噪声保护属性公平分类框架",
        "摘要翻译": "我们提出了一个优化框架，用于在保护属性存在噪声扰动的情况下学习公平分类器。与之前的工作相比，我们的框架可以与非常广泛的线性和线性分数公平约束类一起使用，可以处理多个非二进制保护属性，并且输出的分类器在准确性和公平性上都有可证明的保证。实证上，我们展示了即使在噪声很大的情况下，我们的框架也可以用于在两个真实世界的数据集中实现统计率或假阳性率的公平保证，同时准确性的损失最小。",
        "领域": "公平机器学习、分类算法、数据噪声处理",
        "问题": "在保护属性存在噪声扰动的情况下，如何学习一个既准确又公平的分类器。",
        "动机": "解决现有公平分类方法在处理噪声保护属性和多种非二进制保护属性时的局限性，提供一个具有可证明保证的框架。",
        "方法": "提出一个优化框架，支持广泛的公平约束，处理多种保护属性，并通过实证研究验证其有效性。",
        "关键词": [
            "公平分类",
            "噪声保护属性",
            "优化框架",
            "可证明保证",
            "统计率公平"
        ],
        "涉及的技术概念": {
            "线性公平约束": "框架中用于定义公平性的一类约束，支持广泛的公平性定义。",
            "线性分数公平约束": "扩展了公平约束的定义，允许更复杂的公平性度量。",
            "统计率公平": "一种公平性保证，确保不同群体的统计率相等，框架中用于评估分类器的公平性。"
        },
        "success": true
    },
    {
        "order": 376,
        "title": "Fairness and Bias in Online Selection",
        "html": "https://ICML.cc//virtual/2021/poster/10113",
        "abstract": "There is growing awareness and concern about fairness in machine learning and algorithm design. This is particularly true in online selection problems where decisions are often biased, for example, when assessing credit risks or hiring staff. We address the issues of fairness and bias in online selection by introducing multi-color versions of the classic secretary and prophet problem. Interestingly, existing algorithms for these problems are either very unfair or very inefficient, so we develop optimal fair algorithms for these new problems and provide tight bounds on their competitiveness. We validate our theoretical findings on real-world data.",
        "conference": "ICML",
        "中文标题": "在线选择中的公平性与偏见",
        "摘要翻译": "在机器学习和算法设计中，对公平性的认识和关注日益增加。这在在线选择问题中尤为明显，决策往往带有偏见，例如在评估信用风险或招聘员工时。我们通过引入经典秘书问题和先知问题的多色版本来解决在线选择中的公平性和偏见问题。有趣的是，现有算法对于这些问题要么非常不公平，要么效率极低，因此我们为这些新问题开发了最优公平算法，并提供了它们竞争力的严格界限。我们在真实世界数据上验证了我们的理论发现。",
        "领域": "算法公平性、在线决策、机器学习应用",
        "问题": "解决在线选择问题中的公平性和偏见问题",
        "动机": "现有在线选择算法在公平性和效率之间存在明显不足，需要开发更公平且高效的算法",
        "方法": "引入多色版本的秘书和先知问题，开发最优公平算法，并通过理论分析和真实数据验证",
        "关键词": [
            "公平性",
            "在线选择",
            "算法设计",
            "偏见",
            "多色问题"
        ],
        "涉及的技术概念": {
            "多色秘书问题": "经典秘书问题的扩展，用于研究在线选择中的公平性问题",
            "多色先知问题": "经典先知问题的扩展，用于在不确定性条件下研究公平决策",
            "竞争力界限": "用于评估算法性能的理论界限，确保算法在公平性和效率之间的最优平衡"
        },
        "success": true
    },
    {
        "order": 377,
        "title": "Fairness for Image Generation with Uncertain Sensitive Attributes",
        "html": "https://ICML.cc//virtual/2021/poster/8877",
        "abstract": "  This work tackles the issue of fairness in the context of generative\n  procedures, such as image super-resolution, which entail different\n  definitions from the standard classification setting. Moreover,\n  while traditional group fairness definitions are typically defined\n  with respect to specified protected groups -- camouflaging the\n  fact that these groupings are artificial and carry historical and\n  political motivations -- we emphasize that there are no  ground\n  truth identities. For instance, should South and East Asians be\n  viewed as a single group or separate groups?  Should we consider one\n  race as a whole or further split by gender?  Choosing which groups\n  are valid and who belongs in them is an impossible dilemma and being\n  ``fair'' with respect to Asians may require being ``unfair'' with\n  respect to South Asians.  This motivates the introduction of\n  definitions that allow algorithms to be \\emph{oblivious} to the\n  relevant groupings. \n\n  We define several intuitive notions of group fairness and study\n  their incompatibilities and trade-offs. We show that the natural\n  extension of demographic parity is strongly dependent on the\n  grouping, and \\emph{impossible} to achieve obliviously.  On the\n  other hand, the conceptually new definition we introduce,\n  Conditional Proportional Representation, can be achieved obliviously\n  through Posterior Sampling.  Our experiments validate our\n  theoretical results and achieve fair image reconstruction using\n  state-of-the-art generative models.",
        "conference": "ICML",
        "中文标题": "具有不确定敏感属性的图像生成公平性",
        "摘要翻译": "这项工作解决了在生成过程中（如图像超分辨率）的公平性问题，这些问题与标准分类设置有不同的定义。此外，虽然传统的群体公平定义通常是相对于指定的受保护群体定义的——掩盖了这些分组是人为的，并带有历史和政治动机的事实——但我们强调没有真实身份。例如，南亚人和东亚人应该被视为一个群体还是分开的群体？我们应该将整个种族视为一个整体，还是进一步按性别划分？选择哪些群体是有效的以及谁属于这些群体是一个不可能的困境，对亚洲人‘公平’可能需要对南亚人‘不公平’。这促使引入允许算法对相关分组‘视而不见’的定义。我们定义了几种直观的群体公平概念，并研究了它们的不兼容性和权衡。我们表明，人口统计平等的自然扩展强烈依赖于分组，并且‘视而不见’地实现是不可能的。另一方面，我们引入的概念上新的定义，条件比例代表，可以通过后验采样‘视而不见’地实现。我们的实验验证了我们的理论结果，并使用最先进的生成模型实现了公平的图像重建。",
        "领域": "图像生成公平性、生成对抗网络、计算机视觉伦理",
        "问题": "解决在图像生成过程中如何实现公平性，特别是在敏感属性不确定的情况下。",
        "动机": "传统公平性定义依赖于人为定义的受保护群体，这些分组带有历史和政治动机，且不存在真实身份，导致公平性实现的困境。",
        "方法": "引入新的公平性定义——条件比例代表，并通过后验采样实现，同时使用最先进的生成模型进行公平图像重建。",
        "关键词": [
            "图像生成公平性",
            "条件比例代表",
            "后验采样",
            "生成对抗网络",
            "敏感属性"
        ],
        "涉及的技术概念": {
            "条件比例代表": "一种新的公平性定义，允许算法在不明确指定分组的情况下实现公平。",
            "后验采样": "用于实现条件比例代表的技术手段，通过采样后验分布来生成公平的图像。",
            "生成对抗网络": "用于图像重建的先进生成模型，本研究中用于实现公平的图像生成。"
        },
        "success": true
    },
    {
        "order": 378,
        "title": "Fairness of Exposure in Stochastic Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9431",
        "abstract": "Contextual bandit algorithms have become widely used for recommendation in online systems (e.g. marketplaces, music streaming, news), where they now wield substantial influence on which items get shown to users. This raises questions of fairness to the items --- and to the sellers, artists, and writers that benefit from this exposure. We argue that the conventional bandit formulation can lead to an undesirable and unfair winner-takes-all allocation of exposure. To remedy this problem, we propose a new bandit objective that guarantees merit-based fairness of exposure to the items while optimizing utility to the users. We formulate fairness regret and reward regret in this setting and present algorithms for both stochastic multi-armed bandits and stochastic linear bandits. We prove that the algorithms achieve sublinear fairness regret and reward regret. Beyond the theoretical analysis, we also provide empirical evidence that these algorithms can allocate exposure to different arms effectively. ",
        "conference": "ICML",
        "中文标题": "随机多臂老虎机中的曝光公平性",
        "摘要翻译": "上下文老虎机算法已广泛用于在线系统（如市场、音乐流媒体、新闻）的推荐，这些算法现在对哪些项目展示给用户具有重大影响。这引发了关于项目公平性的问题——以及对这些曝光受益的卖家、艺术家和作家的公平性。我们认为，传统的老虎机公式可能导致一种不理想且不公平的赢家通吃的曝光分配。为了解决这个问题，我们提出了一种新的老虎机目标，该目标在优化用户效用的同时，保证项目基于价值的曝光公平性。我们在此设置中制定了公平性遗憾和奖励遗憾，并为随机多臂老虎机和随机线性老虎机提出了算法。我们证明了这些算法实现了次线性的公平性遗憾和奖励遗憾。除了理论分析外，我们还提供了经验证据，表明这些算法可以有效地将曝光分配给不同的臂。",
        "领域": "推荐系统, 公平性算法, 在线学习",
        "问题": "解决在线推荐系统中项目曝光分配的不公平问题",
        "动机": "传统老虎机算法可能导致赢家通吃的曝光分配，不利于项目和内容创作者的公平性",
        "方法": "提出新的老虎机目标，保证项目基于价值的曝光公平性，同时优化用户效用，并设计相应算法",
        "关键词": [
            "曝光公平性",
            "随机多臂老虎机",
            "推荐系统",
            "公平性算法",
            "在线学习"
        ],
        "涉及的技术概念": {
            "公平性遗憾": "衡量算法在分配曝光时偏离公平性标准的程度",
            "奖励遗憾": "衡量算法在优化用户效用方面的表现",
            "次线性遗憾": "算法在长期运行中遗憾增长速度低于线性，表明算法性能良好"
        },
        "success": true
    },
    {
        "order": 379,
        "title": "Fair Selective Classification Via Sufficiency",
        "html": "https://ICML.cc//virtual/2021/poster/9311",
        "abstract": "Selective classification is a powerful tool for decision-making in scenarios where mistakes are costly but abstentions are allowed. In general, by allowing a classifier to abstain, one can improve the performance of a model at the cost of reducing coverage and classifying fewer samples. However, recent work has shown, in some cases, that selective classification can magnify disparities between groups, and has illustrated this phenomenon on multiple real-world datasets. We prove that the sufficiency criterion can be used to mitigate these disparities by ensuring that selective classification increases performance on all groups, and introduce a method for mitigating the disparity in precision across the entire coverage scale based on this criterion. We then provide an upper bound on the conditional mutual information between the class label and sensitive attribute, conditioned on the learned features, which can be used as a regularizer to achieve fairer selective classification. The effectiveness of the method is demonstrated on the Adult, CelebA, Civil Comments, and CheXpert datasets.",
        "conference": "ICML",
        "中文标题": "通过充分性实现公平选择性分类",
        "摘要翻译": "选择性分类是在错误成本高昂但允许弃权的决策场景中的一种强大工具。通常，通过允许分类器弃权，可以以减少覆盖范围和分类更少样本为代价，提高模型的性能。然而，最近的研究表明，在某些情况下，选择性分类可能会放大群体之间的差异，并在多个真实世界的数据集上说明了这一现象。我们证明，充分性标准可以用来通过确保选择性分类提高所有群体的性能来缓解这些差异，并基于这一标准引入了一种方法来缓解整个覆盖范围内精度的差异。然后，我们提供了一个关于类别标签和敏感属性之间条件互信息的上界，条件是学习到的特征，这可以作为正则化器来实现更公平的选择性分类。该方法的有效性在Adult、CelebA、Civil Comments和CheXpert数据集上得到了验证。",
        "领域": "公平机器学习、选择性分类、模型性能优化",
        "问题": "选择性分类在提高模型性能的同时可能放大群体间的不公平差异",
        "动机": "解决选择性分类中群体间性能差异扩大的问题，确保所有群体都能从选择性分类中受益",
        "方法": "利用充分性标准来缓解选择性分类中的群体间差异，并通过条件互信息的上界作为正则化器实现更公平的选择性分类",
        "关键词": [
            "选择性分类",
            "公平性",
            "充分性标准",
            "条件互信息",
            "正则化"
        ],
        "涉及的技术概念": {
            "选择性分类": "允许分类器在不确定时弃权，以提高模型性能的技术",
            "充分性标准": "用于确保选择性分类对所有群体都有益的标准，缓解群体间差异",
            "条件互信息": "在给定学习到的特征条件下，类别标签和敏感属性之间的互信息，用于作为正则化器实现公平性"
        },
        "success": true
    },
    {
        "order": 380,
        "title": "Fast active learning for pure exploration in reinforcement learning",
        "html": "https://ICML.cc//virtual/2021/poster/8541",
        "abstract": "Realistic environments often provide agents with very limited feedback.\nWhen the environment is initially unknown, the feedback, in the beginning, can be completely absent,\nand the agents may first choose to devote all their effort on \\emph{exploring efficiently.}\nThe exploration remains a challenge while it has been addressed with many hand-tuned heuristics with different levels\nof generality on one side, and a few theoretically-backed exploration strategies on the other.\nMany of them are incarnated by \\emph{intrinsic motivation} and in particular \\emph{explorations bonuses}.\nA common choice is to use $1/\\sqrt{n}$ bonus,\nwhere $n$ is a number of times this particular  state-action pair was visited.\nWe show that, surprisingly, for a pure-exploration objective of \\emph{reward-free exploration}, bonuses that scale with $1/n$ bring faster learning rates, improving the known upper bounds with respect to the dependence on the horizon $H$.\nFurthermore, we show that with an improved analysis of the stopping time, we can improve by a factor $H$ the sample complexity\nin the \\emph{best-policy identification} setting, which is another pure-exploration objective, where the environment provides rewards but the agent is not penalized for its behavior during the\nexploration phase.",
        "conference": "ICML",
        "success": true,
        "中文标题": "强化学习中纯探索的快速主动学习",
        "摘要翻译": "现实环境通常为智能体提供非常有限的反馈。当环境最初未知时，初始阶段的反馈可能完全缺失，智能体可能首先选择将所有精力投入到高效探索上。探索仍然是一个挑战，一方面已经通过许多手工调整的启发式方法以不同层次的通用性得到解决，另一方面也有少数理论支持的探索策略。其中许多通过内在动机特别是探索奖励来体现。一个常见的选择是使用1/√n的奖励，其中n是特定状态-动作对被访问的次数。我们出人意料地表明，对于奖励无关探索的纯探索目标，与1/n成比例的奖励带来了更快的学习速率，改善了已知上界对地平线H的依赖。此外，我们表明，通过对停止时间的改进分析，我们可以在最佳策略识别设置中将样本复杂度提高一个因子H，这是另一个纯探索目标，其中环境提供奖励但智能体在探索阶段的行为不会受到惩罚。",
        "领域": "强化学习, 主动学习, 探索策略",
        "问题": "解决在强化学习中如何高效进行纯探索的问题",
        "动机": "研究动机是为了提高在未知环境中智能体的探索效率，特别是在反馈有限或完全缺失的情况下",
        "方法": "通过改进探索奖励的分析和停止时间的优化，提出了一种更高效的纯探索策略",
        "关键词": [
            "强化学习",
            "纯探索",
            "主动学习",
            "探索奖励",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "内在动机": "在论文中用于驱动智能体在没有外部奖励的情况下进行探索",
            "探索奖励": "论文中提出的与访问次数成反比的奖励机制，用于加速学习过程",
            "停止时间分析": "论文中用于优化样本复杂度的技术，特别适用于最佳策略识别场景"
        }
    },
    {
        "order": 381,
        "title": "Fast Algorithms for Stackelberg Prediction Game with Least Squares Loss",
        "html": "https://ICML.cc//virtual/2021/poster/8495",
        "abstract": "The Stackelberg prediction game (SPG) has been extensively used to model the interactions between the learner and data provider in the training process of various machine learning algorithms. Particularly, SPGs played prominent roles in cybersecurity applications, such as intrusion detection, banking fraud detection, spam filtering, and malware detection. Often formulated as NP-hard bi-level optimization problems, it is generally computationally intractable to find global solutions to SPGs. As an interesting progress in this area, a special class of SPGs with the least squares loss (SPG-LS) have recently been shown polynomially solvable by a bisection method. However, in each iteration of this method, a semidefinite program (SDP) needs to be solved. The resulted high computational costs prevent its applications for large-scale problems. In contrast, we propose a novel approach that reformulates a SPG-LS as a single SDP of a similar form and the same dimension as those solved in the bisection method. Our SDP reformulation is, evidenced by our numerical experiments, orders of magnitude faster than the existing bisection method. We further show that the obtained SDP can be reduced to a second order cone program (SOCP). This allows us to provide real-time response to large-scale SPG-LS problems. Numerical results on both synthetic and real world datasets indicate that the proposed SOCP method is up to 20,000+ times faster than the state of the art.",
        "conference": "ICML",
        "中文标题": "最小二乘损失斯塔克尔伯格预测博弈的快速算法",
        "摘要翻译": "斯塔克尔伯格预测博弈（SPG）已被广泛用于模拟各种机器学习算法训练过程中学习者和数据提供者之间的互动。特别是在网络安全应用中，如入侵检测、银行欺诈检测、垃圾邮件过滤和恶意软件检测，SPG发挥了重要作用。通常被表述为NP难的双层优化问题，找到SPG的全局解在计算上通常是不可行的。作为这一领域的一个有趣进展，最近通过二分法显示，具有最小二乘损失的特殊类别的SPG（SPG-LS）是可多项式求解的。然而，在这种方法的每次迭代中，都需要解决一个半定规划（SDP）。由此产生的高计算成本阻碍了其在大规模问题中的应用。相比之下，我们提出了一种新方法，将SPG-LS重新表述为一个单一SDP，其形式和维度与二分法中解决的SDP相似。我们的数值实验证明，我们的SDP重新表述比现有的二分法快几个数量级。我们进一步表明，所获得的SDP可以简化为二阶锥规划（SOCP）。这使我们能够对大规模SPG-LS问题提供实时响应。在合成和真实世界数据集上的数值结果表明，所提出的SOCP方法比现有技术快20,000倍以上。",
        "领域": "网络安全, 机器学习优化, 博弈论应用",
        "问题": "解决斯塔克尔伯格预测博弈（SPG）在大规模问题中计算成本高的问题",
        "动机": "为了克服现有方法在处理大规模SPG-LS问题时的高计算成本，提出更高效的算法",
        "方法": "将SPG-LS重新表述为单一SDP，并进一步简化为SOCP，以提高计算效率",
        "关键词": [
            "斯塔克尔伯格预测博弈",
            "最小二乘损失",
            "半定规划",
            "二阶锥规划",
            "大规模优化"
        ],
        "涉及的技术概念": {
            "斯塔克尔伯格预测博弈": "用于模拟学习者和数据提供者之间互动的博弈模型",
            "半定规划": "用于解决SPG-LS问题的优化方法，具有较高的计算效率",
            "二阶锥规划": "通过简化SDP得到的优化问题，能够实现实时响应大规模问题"
        },
        "success": true
    },
    {
        "order": 382,
        "title": "Faster Kernel Matrix Algebra via Density Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9737",
        "abstract": "We study fast algorithms for computing basic properties of an n x n positive semidefinite kernel matrix K corresponding to n points x_1,...,x_n in R^d. In particular, we consider the estimating the sum of kernel matrix entries, along with its top eigenvalue and eigenvector. These are some of the most basic problems defined over kernel matrices.\n\nWe show that the sum of matrix entries can be estimated up to a multiplicative factor of 1+\\epsilon in time sublinear in n and linear in d for many popular kernel functions, including the Gaussian, exponential, and rational quadratic kernels. For these kernels, we also show that the top eigenvalue (and a witnessing approximate eigenvector) can be approximated to a multiplicative factor of 1+\\epsilon in time sub-quadratic in n and linear in d. \n\nOur algorithms represent significant advances in the best known runtimes for these problems. They leverage the positive definiteness of the kernel matrix, along with a recent line of work on efficient kernel density estimation.",
        "conference": "ICML",
        "中文标题": "通过密度估计实现更快的核矩阵代数",
        "摘要翻译": "我们研究了快速算法，用于计算对应于R^d中n个点x_1,...,x_n的n x n正半定核矩阵K的基本属性。特别是，我们考虑了估计核矩阵元素的总和，以及其顶部特征值和特征向量。这些是定义在核矩阵上的一些最基本的问题。我们表明，对于许多流行的核函数，包括高斯核、指数核和有理二次核，可以在时间上亚线性于n且线性于d的情况下，将矩阵元素的总和估计到1+ε的乘法因子。对于这些核函数，我们还表明，可以在时间上亚二次于n且线性于d的情况下，将顶部特征值（以及一个见证的近似特征向量）近似到1+ε的乘法因子。我们的算法代表了这些问题已知最佳运行时间的显著进步。它们利用了核矩阵的正定性，以及最近一系列关于高效核密度估计的工作。",
        "领域": "核方法、机器学习算法优化、大规模数据处理",
        "问题": "如何高效计算大规模核矩阵的基本属性，如元素总和、顶部特征值和特征向量。",
        "动机": "解决在大规模数据集上计算核矩阵基本属性时的高计算复杂度问题。",
        "方法": "利用核矩阵的正定性和核密度估计的最新进展，开发出在时间和空间复杂度上更优的算法。",
        "关键词": [
            "核矩阵",
            "密度估计",
            "算法优化",
            "特征值计算",
            "大规模数据处理"
        ],
        "涉及的技术概念": {
            "核矩阵": "用于表示数据点间相似性的矩阵，是核方法的基础。",
            "密度估计": "估计数据点分布的技术，用于优化核矩阵的计算。",
            "正定性": "核矩阵的一个关键性质，确保算法的数学基础和稳定性。"
        },
        "success": true
    },
    {
        "order": 383,
        "title": "Fast margin maximization via dual acceleration",
        "html": "https://ICML.cc//virtual/2021/poster/9251",
        "abstract": "We present and analyze a momentum-based gradient method for training linear classifiers with an exponentially-tailed loss (e.g., the exponential or logistic loss), which maximizes the classification margin on separable data at a rate of O(1/t^2).  This contrasts with a rate of O(1/log(t)) for standard gradient descent, and O(1/t) for normalized gradient descent.  The momentum-based method is derived via the convex dual of the maximum-margin problem, and specifically by applying Nesterov acceleration to this dual, which manages to result in a simple and intuitive method in the primal.  This dual view can also be used to derive a stochastic variant, which performs adaptive non-uniform sampling via the dual variables.\n",
        "conference": "ICML",
        "中文标题": "通过双重加速实现快速边际最大化",
        "摘要翻译": "我们提出并分析了一种基于动量的梯度方法，用于训练具有指数尾部损失（例如，指数损失或逻辑损失）的线性分类器，该方法在可分离数据上以O(1/t^2)的速率最大化分类边际。这与标准梯度下降的O(1/log(t))速率和归一化梯度下降的O(1/t)速率形成对比。这种基于动量的方法是通过最大边际问题的凸对偶推导出来的，特别是通过对这个对偶应用Nesterov加速，从而在原始问题中产生了一个简单直观的方法。这种对偶视角也可以用来推导出一个随机变体，它通过对偶变量执行自适应非均匀采样。",
        "领域": "机器学习优化算法、线性分类器训练、边际最大化",
        "问题": "提高线性分类器在可分离数据上的边际最大化速率",
        "动机": "为了解决标准梯度下降和归一化梯度下降在边际最大化问题上速率较慢的问题，提出一种更快的基于动量的梯度方法",
        "方法": "通过最大边际问题的凸对偶，应用Nesterov加速技术，推导出一种基于动量的梯度方法，并在原始问题中实现简单直观的优化",
        "关键词": [
            "边际最大化",
            "Nesterov加速",
            "线性分类器",
            "指数尾部损失",
            "自适应采样"
        ],
        "涉及的技术概念": {
            "动量梯度方法": "一种利用历史梯度信息加速当前梯度下降过程的优化技术，用于提高训练速度",
            "Nesterov加速": "一种优化技术，通过预测下一步的梯度方向来调整当前的更新步骤，以加速收敛",
            "凸对偶": "在优化问题中，通过构造对偶问题来简化原始问题的求解过程，这里用于推导边际最大化的高效算法"
        },
        "success": true
    },
    {
        "order": 384,
        "title": "Fast Projection Onto Convex Smooth Constraints",
        "html": "https://ICML.cc//virtual/2021/poster/9373",
        "abstract": "The Euclidean projection onto a convex set is an important problem that arises in numerous constrained optimization tasks. Unfortunately, in many cases, computing projections is computationally demanding. In this work, we focus on projection problems where the constraints are smooth and the number of constraints is significantly smaller than the dimension. The runtime of existing approaches to solving such problems is either cubic in the dimension or polynomial in the inverse of the target accuracy. Conversely, we propose  a simple and efficient  primal-dual approach, with a runtime that scales only linearly with the dimension, and only logarithmically in the inverse of the target accuracy. We empirically demonstrate its performance, and compare it with standard baselines.",
        "conference": "ICML",
        "中文标题": "快速投影到凸光滑约束集",
        "摘要翻译": "欧几里得投影到凸集是一个在众多约束优化任务中出现的重要问题。不幸的是，在许多情况下，计算投影在计算上是昂贵的。在这项工作中，我们专注于约束条件光滑且约束数量远小于维度的投影问题。现有解决这类问题的方法的运行时间要么是维度的立方，要么是目标精度倒数的多项式。相反，我们提出了一种简单高效的原对偶方法，其运行时间仅随维度线性增长，并且仅与目标精度倒数的对数相关。我们通过实验展示了其性能，并与标准基线进行了比较。",
        "领域": "凸优化、机器学习优化、数值计算",
        "问题": "解决在高维空间中，计算欧几里得投影到凸集的计算成本高的问题。",
        "动机": "为了在约束优化任务中，提高计算投影的效率，特别是在约束条件光滑且数量远小于维度的情况下。",
        "方法": "提出了一种简单高效的原对偶方法，显著降低了计算复杂度。",
        "关键词": [
            "凸优化",
            "欧几里得投影",
            "原对偶方法",
            "计算效率",
            "高维数据"
        ],
        "涉及的技术概念": {
            "欧几里得投影": "在凸集上找到最近点的过程，用于约束优化任务。",
            "原对偶方法": "一种优化技术，通过同时考虑原始问题和对偶问题来提高计算效率。",
            "计算复杂度": "衡量算法在解决特定问题时所需计算资源的量度，本文中特别关注运行时间与维度和精度的关系。"
        },
        "success": true
    },
    {
        "order": 385,
        "title": "Fast Sketching of Polynomial Kernels of Polynomial Degree",
        "html": "https://ICML.cc//virtual/2021/poster/9109",
        "abstract": "Kernel methods are fundamental in machine learning, and faster algorithms for kernel approximation provide direct speedups for many core tasks in machine learning. The polynomial kernel is especially important as other kernels can often be approximated by the polynomial kernel via a Taylor series expansion. Recent techniques in oblivious sketching reduce the dependence in the running time on the degree $q$ of the polynomial kernel from exponential to polynomial, which is useful for the Gaussian kernel, for which $q$ can be chosen to be polylogarithmic. However, for more slowly growing kernels, such as the neural tangent and arc cosine kernels, $q$ needs to be polynomial, and previous work incurs a polynomial factor slowdown in the running time. We give a new oblivious sketch which greatly improves upon this running time, by removing the dependence on $q$ in the leading order term. Combined with a novel sampling scheme, we give the fastest algorithms for approximating a large family of slow-growing kernels.",
        "conference": "ICML",
        "中文标题": "多项式核的多项式次数的快速草图构建",
        "摘要翻译": "核方法在机器学习中占据基础地位，而核近似的快速算法直接为机器学习中的许多核心任务提供了加速。多项式核尤其重要，因为其他核通常可以通过泰勒级数展开由多项式核近似。最近的无意识草图技术将多项式核的运行时间对多项式次数q的依赖从指数级降低到了多项式级，这对于高斯核非常有用，因为q可以选择为多对数级。然而，对于增长较慢的核，如神经切线和弧余弦核，q需要是多项式级的，而之前的工作在运行时间上遭受了多项式级的减速。我们提出了一种新的无意识草图，通过去除主导项中对q的依赖，大大改善了运行时间。结合一种新颖的采样方案，我们为近似一大类增长缓慢的核提供了最快的算法。",
        "领域": "核方法近似、机器学习加速算法、多项式核应用",
        "问题": "如何高效近似增长缓慢的核函数，特别是多项式核，以减少计算时间",
        "动机": "为了克服现有技术在近似增长缓慢核函数时运行时间上的多项式级减速问题",
        "方法": "提出了一种新的无意识草图技术，结合新颖的采样方案，去除主导项中对多项式次数q的依赖",
        "关键词": [
            "核方法",
            "多项式核",
            "无意识草图",
            "机器学习加速",
            "核近似"
        ],
        "涉及的技术概念": {
            "无意识草图": "一种减少核近似计算中对多项式次数依赖的技术，用于加速核方法的计算",
            "多项式核": "一种重要的核函数，能够通过泰勒级数展开近似其他核函数",
            "采样方案": "论文中提出的新颖采样方法，用于进一步提高核近似的效率"
        },
        "success": true
    },
    {
        "order": 386,
        "title": "Fast Stochastic Bregman Gradient Methods: Sharp Analysis and Variance Reduction",
        "html": "https://ICML.cc//virtual/2021/poster/9483",
        "abstract": "We study the problem of minimizing a relatively-smooth convex function using stochastic Bregman gradient methods. We first prove the convergence of Bregman Stochastic  Gradient Descent (BSGD) to a region that depends on the noise (magnitude of the gradients) at the optimum. In particular, BSGD quickly converges to the exact minimizer when this noise is zero (interpolation setting, in which the data is fit perfectly). Otherwise, when the objective has a finite sum structure, we show that variance reduction can be used to counter the effect of noise. In particular, fast convergence to the exact minimizer can be obtained under additional regularity assumptions on the Bregman reference function. We illustrate the effectiveness of our approach on two key applications of relative smoothness: tomographic reconstruction with Poisson noise and statistical preconditioning for distributed optimization.",
        "conference": "ICML",
        "中文标题": "快速随机布雷格曼梯度方法：锐利分析与方差缩减",
        "摘要翻译": "我们研究了使用随机布雷格曼梯度方法最小化相对平滑凸函数的问题。我们首先证明了布雷格曼随机梯度下降（BSGD）收敛到一个依赖于最优解处噪声（梯度大小）的区域。特别是，当这种噪声为零时（插值设置，即数据完美拟合），BSGD能快速收敛到精确的最小化器。否则，当目标函数具有有限和结构时，我们展示了可以使用方差缩减来抵消噪声的影响。特别是在布雷格曼参考函数上附加了额外的正则性假设时，可以获得快速收敛到精确最小化器的结果。我们通过相对平滑性的两个关键应用：泊松噪声的断层扫描重建和分布式优化的统计预处理，说明了我们方法的有效性。",
        "领域": "优化算法、分布式优化、图像重建",
        "问题": "最小化相对平滑凸函数的问题，特别是在存在噪声的情况下如何快速收敛到最优解",
        "动机": "研究在数据完美拟合和存在噪声的不同情况下，如何通过布雷格曼梯度方法和方差缩减技术，实现快速收敛到最优解",
        "方法": "采用布雷格曼随机梯度下降（BSGD）方法，结合方差缩减技术，以及在特定条件下对布雷格曼参考函数的正则性假设",
        "关键词": [
            "布雷格曼梯度",
            "方差缩减",
            "相对平滑性",
            "断层扫描重建",
            "分布式优化"
        ],
        "涉及的技术概念": {
            "布雷格曼梯度": "用于最小化相对平滑凸函数的梯度方法，通过引入布雷格曼散度来定义梯度下降方向",
            "方差缩减": "一种减少随机梯度估计方差的技术，用于加速收敛，特别是在有限和结构的优化问题中",
            "相对平滑性": "描述函数相对于某个参考函数的平滑性质，是布雷格曼梯度方法应用的关键假设"
        },
        "success": true
    },
    {
        "order": 387,
        "title": "f-Domain Adversarial Learning: Theory and Algorithms",
        "html": "https://ICML.cc//virtual/2021/poster/9515",
        "abstract": "Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a)  as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines,  and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.",
        "conference": "ICML",
        "中文标题": "f域对抗学习：理论与算法",
        "摘要翻译": "无监督领域适应在许多机器学习应用中被使用，其中在训练期间，模型可以访问目标域中的未标记数据和相关的标记数据集。在本文中，我们引入了一个新颖且通用的领域对抗框架。具体来说，我们推导了一个新的领域适应泛化界限，该界限利用了一种基于f-散度变分特性的新分布差异度量。它作为特例恢复了Ben-David等人（2010a）的理论结果，并支持实践中使用的散度。基于这一界限，我们推导了一个新的算法框架，该框架在Ganin等人（2016）的原始对抗训练方法中引入了一个关键修正。我们表明，为了达到与（如果不是优于）最先进的领域对抗方法相媲美的性能，过去几年在这一框架中引入的许多正则器和临时目标不再需要。在真实世界的自然语言和计算机视觉数据集上进行的实验分析表明，我们的框架优于现有基线，并为以前在领域对抗学习中未考虑的f-散度获得了最佳结果。",
        "领域": "领域适应、对抗学习、f-散度优化",
        "问题": "如何在无监督领域适应中更有效地利用f-散度来减少源域和目标域之间的分布差异",
        "动机": "为了解决现有领域对抗学习方法在理论和实践上的局限性，特别是在利用f-散度进行分布差异度量方面的不足",
        "方法": "引入基于f-散度变分特性的新分布差异度量，并在此基础上提出新的算法框架，修正原始对抗训练方法",
        "关键词": [
            "无监督领域适应",
            "对抗学习",
            "f-散度",
            "泛化界限",
            "分布差异度量"
        ],
        "涉及的技术概念": {
            "f-散度": "用于度量源域和目标域之间分布差异的一种方法，基于变分特性提供更灵活的差异度量",
            "对抗训练": "通过引入对抗性过程来减少源域和目标域之间的分布差异，提高模型的泛化能力",
            "泛化界限": "理论分析工具，用于评估模型在未见数据上的性能，本文中基于f-散度推导了新的泛化界限"
        },
        "success": true
    },
    {
        "order": 388,
        "title": "Feature Clustering for Support Identification in Extreme Regions",
        "html": "https://ICML.cc//virtual/2021/poster/10013",
        "abstract": "Understanding the complex structure of multivariate extremes is a major challenge in various fields from portfolio monitoring and environmental risk management to insurance. In the framework of multivariate Extreme Value Theory, a common characterization of extremes' dependence structure is the angular measure. It is a suitable measure to work in extreme regions as it provides meaningful insights concerning the subregions where extremes tend to concentrate their mass. The present paper develops a novel optimization-based approach to assess the dependence structure of extremes. This support identification scheme rewrites as estimating clusters of features which best capture the support  of extremes. The dimension reduction technique we provide is applied to statistical learning tasks such as feature clustering and anomaly detection. Numerical experiments provide strong empirical evidence of the relevance of our approach.",
        "conference": "ICML",
        "中文标题": "极端区域中支持识别的特征聚类",
        "摘要翻译": "理解多元极值的复杂结构是投资组合监控、环境风险管理和保险等多个领域的主要挑战。在多元极值理论的框架下，极值依赖结构的一个常见表征是角度测度。这是一种适合在极端区域工作的测度，因为它提供了关于极值倾向于集中其质量的子区域的有意义见解。本文开发了一种基于优化的新方法来评估极值的依赖结构。这种支持识别方案重写为估计最能捕捉极值支持的特征聚类。我们提供的降维技术应用于特征聚类和异常检测等统计学习任务。数值实验提供了强有力的经验证据，证明了我们方法的相关性。",
        "领域": "多元极值理论、特征聚类、异常检测",
        "问题": "评估和理解多元极值的依赖结构",
        "动机": "为了在投资组合监控、环境风险管理和保险等领域中更好地理解和应用多元极值的复杂结构",
        "方法": "开发了一种基于优化的新方法，通过特征聚类来识别极值的支持",
        "关键词": [
            "多元极值理论",
            "特征聚类",
            "异常检测",
            "角度测度",
            "降维技术"
        ],
        "涉及的技术概念": {
            "角度测度": "用于表征极值依赖结构的测度，提供极值集中区域的见解",
            "特征聚类": "用于识别和估计极值支持的技术，通过聚类分析降维",
            "降维技术": "应用于统计学习任务，如特征聚类和异常检测，以提高分析效率和准确性"
        },
        "success": true
    },
    {
        "order": 389,
        "title": "Federated Composite Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10485",
        "abstract": "Federated Learning (FL) is a distributed learning paradigm that scales on-device learning collaboratively and privately. Standard FL algorithms such as FᴇᴅAᴠɢ are primarily geared towards smooth unconstrained settings. In this paper, we study the Federated Composite Optimization (FCO) problem, in which the loss function contains a non-smooth regularizer. Such problems arise naturally in FL applications that involve sparsity, low-rank, monotonicity, or more general constraints. We first show that straightforward extensions of primal algorithms such as FedAvg are not well-suited for FCO since they suffer from the 'curse of primal averaging,' resulting in poor convergence. As a solution, we propose a new primal-dual algorithm, Federated Dual Averaging (FedDualAvg), which by employing a novel server dual averaging procedure circumvents the curse of primal averaging. Our theoretical analysis and empirical experiments demonstrate that FedDualAvg outperforms the other baselines.",
        "conference": "ICML",
        "中文标题": "联邦复合优化",
        "摘要翻译": "联邦学习（FL）是一种分布式学习范式，它能够在设备端进行协作且私密的学习扩展。标准的FL算法，如FᴇᴅAᴠɢ，主要针对平滑无约束的环境。在本文中，我们研究了联邦复合优化（FCO）问题，其中损失函数包含一个非平滑的正则化器。这类问题在涉及稀疏性、低秩、单调性或更一般约束的FL应用中自然出现。我们首先表明，像FedAvg这样的原始算法的直接扩展并不适合FCO，因为它们受到'原始平均诅咒'的影响，导致收敛性差。作为解决方案，我们提出了一种新的原始-对偶算法，联邦对偶平均（FedDualAvg），通过采用一种新颖的服务器对偶平均程序，绕过了原始平均的诅咒。我们的理论分析和实证实验表明，FedDualAvg优于其他基线方法。",
        "领域": "联邦学习、优化算法、机器学习",
        "问题": "解决在联邦学习环境中，当损失函数包含非平滑正则化器时的优化问题。",
        "动机": "研究动机是为了克服现有联邦学习算法在处理非平滑正则化器时的局限性，特别是在涉及稀疏性、低秩、单调性或更一般约束的应用中。",
        "方法": "提出了一种新的原始-对偶算法，联邦对偶平均（FedDualAvg），通过服务器对偶平均程序来优化非平滑正则化问题。",
        "关键词": [
            "联邦学习",
            "复合优化",
            "非平滑正则化",
            "原始-对偶算法",
            "收敛性"
        ],
        "涉及的技术概念": {
            "联邦学习": "一种分布式学习范式，允许多个设备或服务器协作学习模型，同时保护数据隐私。",
            "非平滑正则化器": "在损失函数中引入的不可导项，用于促进模型参数的稀疏性、低秩或其他结构特性。",
            "原始-对偶算法": "一种优化算法，通过同时考虑原始问题和对偶问题来寻找最优解，适用于包含约束或非平滑项的优化问题。"
        },
        "success": true
    },
    {
        "order": 390,
        "title": "Federated Continual Learning with Weighted Inter-client Transfer",
        "html": "https://ICML.cc//virtual/2021/poster/9849",
        "abstract": "There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge.  To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our FedWeIT against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost.",
        "conference": "ICML",
        "中文标题": "联邦持续学习与加权客户端间迁移",
        "摘要翻译": "近年来，持续学习和联邦学习引起了广泛关注，这两者在现实世界场景中的深度神经网络中都非常重要。然而，关于每个客户端从私有本地数据流中学习一系列任务的情景，研究却很少。联邦持续学习的这一新问题为持续学习带来了新的挑战，例如如何利用来自其他客户端的知识，同时防止不相关知识的干扰。为了解决这些问题，我们提出了一种新颖的联邦持续学习框架——联邦加权客户端间迁移（FedWeIT），该框架将网络权重分解为全局联邦参数和稀疏的任务特定参数，每个客户端通过加权组合其他客户端的任务特定参数来选择性接收知识。FedWeIT最小化了不兼容任务之间的干扰，并在学习过程中允许跨客户端的积极知识迁移。我们在不同客户端间任务相似度的条件下，将FedWeIT与现有的联邦学习和持续学习方法进行了比较验证，我们的模型在显著降低通信成本的同时，性能大幅优于它们。",
        "领域": "联邦学习, 持续学习, 深度学习优化",
        "问题": "解决在联邦学习环境中，客户端从私有数据流中学习序列任务时，如何有效利用其他客户端的知识同时避免不相关知识干扰的问题。",
        "动机": "研究动机在于探索联邦持续学习这一新兴领域，解决现有方法在知识迁移和干扰避免方面的不足。",
        "方法": "提出联邦加权客户端间迁移（FedWeIT）框架，通过分解网络权重为全局联邦参数和稀疏任务特定参数，实现选择性知识迁移和最小化任务间干扰。",
        "关键词": [
            "联邦持续学习",
            "加权客户端间迁移",
            "知识迁移",
            "任务特定参数",
            "通信成本优化"
        ],
        "涉及的技术概念": {
            "联邦持续学习": "结合联邦学习和持续学习的技术，旨在在保护数据隐私的同时，实现模型在连续任务上的持续学习能力。",
            "加权客户端间迁移": "通过加权组合其他客户端的任务特定参数，实现选择性知识迁移，减少不相关知识的干扰。",
            "任务特定参数": "针对特定任务优化的网络参数，与全局联邦参数共同构成模型权重，用于实现任务间的知识迁移和干扰最小化。"
        },
        "success": true
    },
    {
        "order": 391,
        "title": "Federated  Deep AUC Maximization for Hetergeneous Data with a Constant Communication Complexity",
        "html": "https://ICML.cc//virtual/2021/poster/9645",
        "abstract": "Deep AUC (area under the ROC curve) Maximization (DAM) has attracted much attention recently due to its great potential for imbalanced data classification. However, the research on Federated Deep AUC Maximization (FDAM) is still limited. Compared with standard federated learning (FL) approaches that focus on decomposable minimization objectives, FDAM is more complicated due to its minimization objective is non-decomposable over individual examples. In this paper, we propose improved FDAM algorithms for heterogeneous data by solving the popular non-convex strongly-concave min-max formulation of DAM in a distributed fashion, which can also be applied to a class of non-convex strongly-concave min-max problems. A striking result of this paper is that the communication complexity of the proposed algorithm is a constant independent of the number of machines and also independent of the accuracy level, which improves an existing result by orders of magnitude. The experiments have demonstrated the effectiveness of our FDAM algorithm on benchmark datasets, and on medical chest X-ray images from different organizations. Our experiment shows that the performance of FDAM using data from multiple hospitals can improve the AUC score on testing data from a single hospital for detecting life-threatening diseases based on chest radiographs.",
        "conference": "ICML",
        "中文标题": "针对异构数据的联邦深度AUC最大化：恒定通信复杂度",
        "摘要翻译": "深度AUC（ROC曲线下面积）最大化（DAM）由于其在不平衡数据分类中的巨大潜力，最近引起了广泛关注。然而，关于联邦深度AUC最大化（FDAM）的研究仍然有限。与专注于可分解最小化目标的标准联邦学习（FL）方法相比，FDAM由于其最小化目标在个别示例上不可分解而更为复杂。在本文中，我们通过以分布式方式解决DAM的流行非凸强凹最小-最大公式，提出了针对异构数据的改进FDAM算法，该算法也可以应用于一类非凸强凹最小-最大问题。本文的一个显著结果是，所提出算法的通信复杂度是一个常数，不依赖于机器数量，也不依赖于精度水平，这比现有结果提高了数量级。实验证明了我们的FDAM算法在基准数据集上，以及来自不同组织的医学胸部X射线图像上的有效性。我们的实验表明，使用来自多家医院的数据的FDAM可以提高基于胸部X光片检测危及生命疾病的测试数据的AUC分数。",
        "领域": "联邦学习、医学图像分析、不平衡数据分类",
        "问题": "解决在联邦学习环境下，针对异构数据实现深度AUC最大化的通信效率问题",
        "动机": "研究动机是为了提高在分布式环境下处理不平衡数据分类时的通信效率和模型性能",
        "方法": "提出了一种改进的联邦深度AUC最大化算法，通过分布式解决非凸强凹最小-最大问题，实现了恒定通信复杂度",
        "关键词": [
            "联邦学习",
            "AUC最大化",
            "异构数据",
            "通信复杂度",
            "医学图像分析"
        ],
        "涉及的技术概念": {
            "联邦学习": "一种分布式机器学习方法，允许多个参与方共同训练模型而不共享数据",
            "AUC最大化": "通过优化ROC曲线下面积来提高模型在不平衡数据上的分类性能",
            "非凸强凹最小-最大问题": "一类优化问题，本文中用于描述和解决深度AUC最大化的数学框架"
        },
        "success": true
    },
    {
        "order": 392,
        "title": "Federated Learning of User Verification Models Without Sharing Embeddings",
        "html": "https://ICML.cc//virtual/2021/poster/8753",
        "abstract": "We consider the problem of training User Verification (UV) models in federated setup, where each user has access to the data of only one class and user embeddings cannot be shared with the server or other users. To address this problem, we propose Federated User Verification (FedUV), a framework in which users jointly learn a set of vectors and maximize the correlation of their instance embeddings with a secret linear combination of those vectors. We show that choosing the linear combinations from the codewords of an error-correcting code allows users to collaboratively train the model without revealing their embedding vectors. We present the experimental results for user verification with voice, face, and handwriting data and show that FedUV is on par with existing approaches, while not sharing the embeddings with other users or the server.",
        "conference": "ICML",
        "中文标题": "联邦学习中不共享嵌入的用户验证模型训练",
        "摘要翻译": "我们考虑了在联邦学习设置下训练用户验证（UV）模型的问题，其中每个用户只能访问一个类别的数据，并且用户嵌入不能与服务器或其他用户共享。为了解决这个问题，我们提出了联邦用户验证（FedUV）框架，在该框架中，用户共同学习一组向量，并最大化其实例嵌入与这些向量的秘密线性组合的相关性。我们表明，从纠错码的码字中选择线性组合可以使用户在不透露其嵌入向量的情况下协作训练模型。我们展示了使用语音、面部和手写数据进行用户验证的实验结果，并表明FedUV在不与其他用户或服务器共享嵌入的情况下，与现有方法表现相当。",
        "领域": "联邦学习、用户验证、隐私保护",
        "问题": "在联邦学习环境中训练用户验证模型时，如何在保护用户嵌入隐私的同时实现有效的模型训练。",
        "动机": "解决在联邦学习框架下，用户数据隐私保护与模型训练效率之间的矛盾。",
        "方法": "提出FedUV框架，通过共同学习一组向量并利用纠错码的码字作为线性组合，实现不共享用户嵌入的协作训练。",
        "关键词": [
            "联邦学习",
            "用户验证",
            "隐私保护",
            "纠错码",
            "嵌入向量"
        ],
        "涉及的技术概念": {
            "联邦学习": "一种机器学习方法，允许多个用户或设备协作训练模型，而无需共享原始数据。",
            "用户验证": "通过生物特征或行为特征验证用户身份的技术。",
            "纠错码": "用于检测和纠正数据传输或存储中错误的编码技术，在此用于保护用户嵌入的隐私。"
        },
        "success": true
    },
    {
        "order": 393,
        "title": "Federated Learning under Arbitrary Communication Patterns",
        "html": "https://ICML.cc//virtual/2021/poster/9203",
        "abstract": "Federated Learning is a distributed learning setting where the goal is to train a centralized model with training data distributed over a large number of heterogeneous clients, each with unreliable and relatively slow network connections. A common optimization approach used in federated learning is based on the idea of local SGD: each client runs some number of SGD steps locally and then the updated local models are averaged to form the updated global model on the coordinating server. In this paper, we investigate the performance of an asynchronous version of local SGD wherein the clients can communicate with the server at arbitrary time intervals. Our main result shows that for smooth strongly convex and smooth nonconvex functions we achieve convergence rates that match the synchronous version that requires all clients to communicate simultaneously. ",
        "conference": "ICML",
        "中文标题": "任意通信模式下的联邦学习",
        "摘要翻译": "联邦学习是一种分布式学习设置，其目标是利用分布在大量异构客户端上的训练数据训练一个集中式模型，每个客户端都有不可靠且相对较慢的网络连接。联邦学习中常用的一种优化方法是基于本地随机梯度下降（SGD）的思想：每个客户端在本地运行一定数量的SGD步骤，然后更新后的本地模型在协调服务器上进行平均以形成更新后的全局模型。在本文中，我们研究了一种异步版本的本地SGD的性能，其中客户端可以在任意时间间隔与服务器通信。我们的主要结果表明，对于平滑强凸和平滑非凸函数，我们实现了与需要所有客户端同时通信的同步版本相匹配的收敛速度。",
        "领域": "联邦学习、分布式优化、异步学习",
        "问题": "研究在任意通信模式下联邦学习的性能问题",
        "动机": "探索在客户端与服务器通信时间间隔不固定的情况下，联邦学习的收敛性能是否能够与同步通信模式下的性能相匹配",
        "方法": "采用异步版本的本地随机梯度下降（SGD）方法，允许客户端在任意时间间隔与服务器通信",
        "关键词": [
            "联邦学习",
            "异步学习",
            "本地SGD",
            "分布式优化",
            "收敛速度"
        ],
        "涉及的技术概念": {
            "联邦学习": "一种分布式机器学习方法，旨在保护数据隐私的同时利用分散的数据训练模型",
            "本地SGD": "在联邦学习中，客户端在本地执行随机梯度下降步骤以更新模型参数",
            "异步学习": "一种学习模式，允许客户端在不同时间与服务器通信，而不需要所有客户端同步更新"
        },
        "success": true
    },
    {
        "order": 394,
        "title": "Few-Shot Conformal Prediction with Auxiliary Tasks",
        "html": "https://ICML.cc//virtual/2021/poster/8739",
        "abstract": "We develop a novel approach to conformal prediction when the target task has limited data available for training. Conformal prediction identifies a small set of promising output candidates in place of a single prediction, with guarantees that the set contains the correct answer with high probability. When training data is limited, however, the predicted set can easily become unusably large. In this work, we obtain substantially tighter prediction sets while maintaining desirable marginal guarantees by casting conformal prediction as a meta-learning paradigm over exchangeable collections of auxiliary tasks. Our conformalization algorithm is simple, fast, and agnostic to the choice of underlying model, learning algorithm, or dataset. We demonstrate the effectiveness of this approach across a number of few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery. ",
        "conference": "ICML",
        "中文标题": "基于辅助任务的少样本共形预测",
        "摘要翻译": "我们开发了一种新颖的共形预测方法，适用于目标任务的训练数据有限的情况。共形预测通过识别一小组有希望的输出候选来代替单一预测，并保证该集合以高概率包含正确答案。然而，当训练数据有限时，预测集合很容易变得过大而无法使用。在这项工作中，我们通过将共形预测视为可交换辅助任务集合上的元学习范式，显著缩小了预测集合的大小，同时保持了理想的边际保证。我们的共形化算法简单、快速，并且与底层模型、学习算法或数据集的选择无关。我们在自然语言处理、计算机视觉和药物发现的计算化学中的多个少样本分类和回归任务上证明了这种方法的有效性。",
        "领域": "少样本学习, 共形预测, 元学习",
        "问题": "在训练数据有限的情况下，如何缩小共形预测的预测集合大小，同时保持高概率包含正确答案的保证。",
        "动机": "解决在数据稀缺环境下，传统共形预测方法产生的预测集合过大而无法实用的问题。",
        "方法": "将共形预测视为元学习范式，利用可交换的辅助任务集合来优化预测过程，从而在保持边际保证的同时缩小预测集合。",
        "关键词": [
            "少样本学习",
            "共形预测",
            "元学习",
            "预测集合优化",
            "数据稀缺"
        ],
        "涉及的技术概念": {
            "共形预测": "一种统计方法，通过产生包含正确答案的预测集合来提供预测的不确定性估计。",
            "元学习": "学习如何学习的方法，通过利用多个任务的经验来优化学习算法，提高在新任务上的表现。",
            "可交换性": "指数据或任务的顺序不影响其联合概率分布的性质，是共形预测理论中的一个关键假设。"
        },
        "success": true
    },
    {
        "order": 395,
        "title": "Few-shot Language Coordination by Modeling Theory of Mind",
        "html": "https://ICML.cc//virtual/2021/poster/9765",
        "abstract": "No man is an island. Humans develop the ability to communicate with a large community by coordinating with different interlocutors within short conversations. This ability is largely understudied by the research on building neural language communicative agents. We study the task of few-shot language coordination: agents quickly adapting to their conversational partners’ language abilities. Different from current communicative agents trained with self-play, we in- investigate this more general paradigm by requiring the lead agent to coordinate with a population of agents each of whom has different linguistic abilities. This leads to a general agent able to quickly adapt to communicating with unseen agents in the population. Unlike prior work, success here requires the ability to model the partner’s beliefs, a vital component of human communication.\nDrawing inspiration from the study of theory-of-mind (ToM; Premack & Woodruff (1978)), we study the effect of the speaker explicitly modeling the listener’s mental state. Learning by communicating with a population, the speakers, as shown in our experiments, acquire the ability to learn to predict the reactions of their partner upon various messages on-the-fly. The speaker’s predictions for the future actions help it generate the best instructions in order to maximize communicative goal with message costs. To examine our hypothesis that the instructions generated with ToM modeling yield better communication per- performance, we employ our agents in both a referential game and a language navigation task. Positive results from our experiments also hint at the importance of explicitly modeling language acquisition as a socio-pragmatic progress.",
        "conference": "ICML",
        "中文标题": "通过建模心智理论实现少样本语言协调",
        "摘要翻译": "没有人是一座孤岛。人类通过与不同对话者在简短对话中协调，发展出与大型社区沟通的能力。这一能力在构建神经语言交流代理的研究中很大程度上未被充分研究。我们研究了少样本语言协调的任务：代理快速适应其对话伙伴的语言能力。与当前通过自我对弈训练的交流代理不同，我们通过要求主导代理与一群具有不同语言能力的代理协调来研究这一更普遍的范式。这导致了一个能够快速适应与群体中未见代理交流的通用代理。与先前的工作不同，这里的成功需要能够建模伙伴的信念，这是人类交流的重要组成部分。受到心智理论（ToM；Premack & Woodruff（1978））研究的启发，我们研究了说话者明确建模听者心理状态的效果。通过与群体交流学习，如我们的实验所示，说话者获得了学习预测伙伴对各种消息即时反应的能力。说话者对未来行动的预测帮助其生成最佳指令，以最大化交流目标与消息成本。为了检验我们的假设，即通过ToM建模生成的指令能带来更好的交流性能，我们在指称游戏和语言导航任务中部署了我们的代理。实验的积极结果也暗示了将语言习得明确建模为社会语用过程的重要性。",
        "领域": "自然语言处理与视觉结合、多智能体系统、人机交互",
        "问题": "研究如何在少样本情况下，使语言交流代理快速适应不同对话伙伴的语言能力。",
        "动机": "探索和实现更接近人类交流能力的神经语言交流代理，特别是在快速适应不同对话伙伴方面。",
        "方法": "通过建模心智理论（ToM），使代理能够预测和适应不同对话伙伴的语言能力，进而在指称游戏和语言导航任务中验证其有效性。",
        "关键词": [
            "少样本学习",
            "语言协调",
            "心智理论",
            "多智能体系统",
            "社会语用学"
        ],
        "涉及的技术概念": {
            "心智理论（ToM）": "用于建模和预测对话伙伴的心理状态，以优化交流策略。",
            "少样本学习": "使代理能够在极少量的交互样本中快速适应新对话伙伴的语言能力。",
            "社会语用过程": "强调语言习得和交流的社会性和实用性，指导代理设计更加符合人类交流习惯。"
        },
        "success": true
    },
    {
        "order": 396,
        "title": "Few-Shot Neural Architecture Search",
        "html": "https://ICML.cc//virtual/2021/poster/8891",
        "abstract": "Efficient evaluation of a network architecture drawn from a large search space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS evaluates each architecture by training from scratch, which gives the true performance but is extremely time-consuming.  Recently, one-shot NAS substantially reduces the computation cost by training only one supernetwork, a.k.a. supernet, to approximate the performance of every architecture in the search space via weight-sharing. However, the performance estimation can be very inaccurate due to the co-adaption among operations. In this paper, we propose few-shot NAS that uses multiple supernetworks, called sub-supernet, each covering different regions of the search space to alleviate the undesired co-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of architecture evaluation with a small increase of evaluation cost. With only up to 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds models that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy at 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra data or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously published results by up to 20%. Extensive experiments show that few-shot NAS significantly improves various one-shot methods, including 4 gradient-based and 6 search-based methods on 3 different tasks in NasBench-201 and NasBench1-shot-1.",
        "conference": "ICML",
        "中文标题": "少样本神经架构搜索",
        "摘要翻译": "在大规模搜索空间中高效评估网络架构仍然是神经架构搜索（NAS）中的一个关键挑战。传统的NAS通过从头开始训练每个架构来评估其性能，虽然能得到真实性能但极其耗时。最近，一次性NAS通过仅训练一个超级网络（即超网）来近似搜索空间中每个架构的性能，大幅降低了计算成本。然而，由于操作间的共同适应，性能估计可能非常不准确。本文提出少样本NAS，使用多个超级网络（称为子超网），每个覆盖搜索空间的不同区域，以减轻不期望的共同适应。与一次性NAS相比，少样本NAS以较小的评估成本提高了架构评估的准确性。仅使用最多7个子超网，少样本NAS就建立了新的技术状态：在ImageNet上，它找到了在600 MB FLOPS下达到80.5% top-1准确率和在238 MFLOPS下达到77.5% top-1准确率的模型；在CIFAR10上，它达到了98.72%的top-1准确率，无需使用额外数据或迁移学习。在Auto-GAN中，少样本NAS比之前发布的结果高出20%。大量实验表明，少样本NAS显著改进了各种一次性方法，包括在NasBench-201和NasBench1-shot-1的3个不同任务上的4种基于梯度和6种基于搜索的方法。",
        "领域": "神经架构搜索, 深度学习优化, 自动化机器学习",
        "问题": "解决在大规模搜索空间中高效评估网络架构的挑战，以及一次性NAS中由于操作间共同适应导致的性能估计不准确问题。",
        "动机": "提高神经架构搜索中架构评估的准确性和效率，减少计算成本。",
        "方法": "提出少样本NAS方法，使用多个子超网覆盖搜索空间的不同区域，以减轻操作间的共同适应，提高评估准确性。",
        "关键词": [
            "少样本学习",
            "神经架构搜索",
            "超级网络",
            "性能评估",
            "自动化机器学习"
        ],
        "涉及的技术概念": {
            "神经架构搜索（NAS）": "自动化设计神经网络架构的技术，旨在减少人工设计架构的需求。",
            "超级网络（Supernet）": "一个包含多个子网络的大网络，通过权重共享来近似评估搜索空间中每个架构的性能。",
            "共同适应（Co-adaption）": "在超级网络中，不同操作间相互影响导致性能估计不准确的现象。"
        },
        "success": true
    },
    {
        "order": 397,
        "title": "FILTRA: Rethinking Steerable CNN by Filter Transform",
        "html": "https://ICML.cc//virtual/2021/poster/8859",
        "abstract": "Steerable CNN imposes the prior knowledge of transformation invariance or equivariance in the network architecture to enhance the the network robustness on geometry transformation of data and reduce overfitting. It has been an intuitive and widely used technique to construct a steerable filter by augmenting a filter with its transformed copies in the past decades, which is named as filter transform in this paper. Recently, the problem of steerable CNN has been studied from aspect of group representation theory, which reveals the function space structure of a steerable kernel function. However, it is not yet clear on how this theory is related to the filter transform technique. In this paper, we show that kernel constructed by filter transform can also be interpreted in the group representation theory. This interpretation help complete the puzzle of steerable CNN theory and provides a novel and simple approach to implement steerable convolution operators. Experiments are executed on multiple datasets to verify the feasibility of the proposed approach.",
        "conference": "ICML",
        "中文标题": "FILTRA：通过滤波器变换重新思考可操纵CNN",
        "摘要翻译": "可操纵CNN通过在网络架构中施加变换不变性或等变性的先验知识，以增强网络对数据几何变换的鲁棒性并减少过拟合。过去几十年中，通过将滤波器与其变换副本增强来构建可操纵滤波器是一种直观且广泛使用的技术，本文称之为滤波器变换。最近，可操纵CNN的问题从群表示理论的角度进行了研究，揭示了可操纵核函数的函数空间结构。然而，这一理论与滤波器变换技术之间的关系尚不明确。在本文中，我们展示了通过滤波器变换构建的核也可以在群表示理论中解释。这一解释有助于完成可操纵CNN理论的拼图，并提供了一种新颖且简单的方法来实现可操纵卷积算子。在多个数据集上进行了实验以验证所提出方法的可行性。",
        "领域": "图像识别、深度学习模型优化、几何深度学习",
        "问题": "如何理解和实现可操纵CNN中的滤波器变换技术，以及其在群表示理论中的解释。",
        "动机": "探索滤波器变换技术与群表示理论之间的联系，以更深入地理解可操纵CNN的工作原理，并简化其实现。",
        "方法": "通过将滤波器变换技术与群表示理论相结合，提供了一种新的理论解释和实现可操纵卷积算子的方法。",
        "关键词": [
            "可操纵CNN",
            "滤波器变换",
            "群表示理论",
            "卷积算子",
            "几何变换"
        ],
        "涉及的技术概念": {
            "可操纵CNN": "一种通过在设计时引入变换不变性或等变性来增强模型对几何变换鲁棒性的卷积神经网络。",
            "滤波器变换": "通过将滤波器与其变换副本增强来构建可操纵滤波器的技术。",
            "群表示理论": "用于描述和解释可操纵核函数函数空间结构的数学理论。"
        },
        "success": true
    },
    {
        "order": 398,
        "title": "Finding k in Latent $k-$ polytope",
        "html": "https://ICML.cc//virtual/2021/poster/8751",
        "abstract": "The recently introduced Latent $k-$ Polytope($\\LkP$)\nencompasses several stochastic  Mixed Membership models including Topic Models.\nThe problem of finding $k$, the number of extreme points of $\\LkP$, is a fundamental challenge and includes several important open problems such as determination of number of components in Ad-mixtures. This paper addresses this challenge by introducing\nInterpolative Convex Rank(\\INR)\n of a matrix defined as the minimum number of its columns whose convex hull is\nwithin Hausdorff distance $\\varepsilon$ of the convex hull of all columns. The first important contribution of this paper is to show that under \\emph{standard assumptions} $k$ equals the \\INR of a \\emph{subset smoothed data matrix} defined from Data generated from an $\\LkP$.\nThe second  important contribution of the paper is a polynomial time\nalgorithm for finding $k$ under standard assumptions.\nAn immediate  corollary  is the first polynomial time algorithm for finding the \\emph{inner dimension} in Non-negative matrix factorisation(NMF) with\nassumptions which are qualitatively different than existing ones such as \\emph{Separability}.\n%An immediate  corollary  is the first polynomial time algorithm for finding the \\emph{inner dimension} in Non-negative matrix factorisation(NMF) with assumptions considerably weaker than \\emph{Separability}.",
        "conference": "ICML",
        "中文标题": "在潜在k-多面体中寻找k",
        "摘要翻译": "最近引入的潜在k-多面体（LkP）涵盖了几种随机混合成员模型，包括主题模型。寻找k，即LkP的极值点数量，是一个基本挑战，并包括几个重要的开放问题，如确定混合物中的组件数量。本文通过引入矩阵的插值凸秩（INR）来应对这一挑战，该秩定义为矩阵列的最小数量，其凸包在所有列的凸包的Hausdorff距离ε内。本文的第一个重要贡献是在标准假设下，k等于从LkP生成的数据定义的子集平滑数据矩阵的INR。本文的第二个重要贡献是在标准假设下找到一个多项式时间算法来寻找k。一个直接的推论是在非负矩阵分解（NMF）中，在质量上与现有假设（如可分离性）不同的假设下，首次找到内维的多项式时间算法。",
        "领域": "主题模型, 非负矩阵分解, 混合成员模型",
        "问题": "确定潜在k-多面体中的极值点数量k",
        "动机": "解决在随机混合成员模型和主题模型中确定组件数量的基本挑战",
        "方法": "引入插值凸秩（INR）概念，并提出在标准假设下寻找k的多项式时间算法",
        "关键词": [
            "潜在k-多面体",
            "插值凸秩",
            "非负矩阵分解",
            "多项式时间算法",
            "混合成员模型"
        ],
        "涉及的技术概念": {
            "潜在k-多面体（LkP）": "涵盖几种随机混合成员模型的结构，用于建模数据",
            "插值凸秩（INR）": "定义为矩阵列的最小数量，其凸包在所有列的凸包的Hausdorff距离ε内，用于确定k",
            "非负矩阵分解（NMF）": "一种矩阵分解方法，用于在特定假设下找到数据的内维"
        },
        "success": true
    },
    {
        "order": 399,
        "title": "Finding Relevant Information via a Discrete Fourier Expansion",
        "html": "https://ICML.cc//virtual/2021/poster/10135",
        "abstract": "A  fundamental obstacle in learning information from data is the presence of nonlinear redundancies and dependencies in it. To address this, we propose a Fourier-based approach to extract relevant information in the supervised setting. We first develop a novel Fourier expansion for functions of correlated binary random variables. This expansion is a generalization of the standard Fourier analysis on the Boolean cube beyond product probability spaces. We further extend our Fourier analysis to stochastic mappings. As an important application of this analysis, we investigate learning with feature subset selection. We reformulate this problem in the Fourier domain and introduce a computationally efficient measure for selecting features. Bridging the Bayesian error rate with the Fourier coefficients, we demonstrate that the Fourier expansion provides a powerful tool to characterize nonlinear dependencies in the features-label relation. Via theoretical analysis, we show that our proposed measure finds provably asymptotically optimal feature subsets. Lastly, we present an algorithm based on our measure and verify our findings via numerical experiments on various datasets.\n",
        "conference": "ICML",
        "中文标题": "通过离散傅里叶展开寻找相关信息",
        "摘要翻译": "从数据中学习信息的一个基本障碍是其中存在的非线性冗余和依赖性。为了解决这个问题，我们提出了一种基于傅里叶的方法来在有监督的环境中提取相关信息。我们首先为相关二元随机变量的函数开发了一种新颖的傅里叶展开。这种展开是布尔立方体上标准傅里叶分析在非乘积概率空间之外的推广。我们进一步将傅里叶分析扩展到随机映射。作为这一分析的一个重要应用，我们研究了特征子集选择的学习问题。我们在傅里叶域重新表述了这个问题，并引入了一种计算高效的度量来选择特征。通过将贝叶斯错误率与傅里叶系数联系起来，我们证明了傅里叶展开为特征-标签关系中的非线性依赖性提供了一个强大的表征工具。通过理论分析，我们展示了我们提出的度量能够找到可证明的渐近最优特征子集。最后，我们提出了一种基于我们度量的算法，并通过在各种数据集上的数值实验验证了我们的发现。",
        "领域": "特征选择、非线性依赖分析、监督学习",
        "问题": "解决数据中非线性冗余和依赖性对信息提取的障碍",
        "动机": "开发一种能够有效提取数据中相关信息的方法，特别是在存在非线性依赖性的情况下",
        "方法": "提出了一种基于傅里叶的分析方法，包括开发新的傅里叶展开、扩展到随机映射、以及在特征子集选择中的应用",
        "关键词": [
            "傅里叶展开",
            "特征选择",
            "非线性依赖",
            "监督学习",
            "贝叶斯错误率"
        ],
        "涉及的技术概念": {
            "傅里叶展开": "用于分析相关二元随机变量函数的工具，推广了标准傅里叶分析",
            "随机映射": "将傅里叶分析扩展到随机过程，以处理更广泛的数据类型",
            "贝叶斯错误率": "与傅里叶系数结合，用于表征特征-标签关系中的非线性依赖性"
        },
        "success": true
    },
    {
        "order": 400,
        "title": "Finding the Stochastic Shortest Path with Low Regret: the Adversarial Cost and Unknown Transition Case",
        "html": "https://ICML.cc//virtual/2021/poster/10629",
        "abstract": "We make significant progress toward the stochastic shortest path problem with adversarial costs and unknown transition.\nSpecifically, we develop algorithms that achieve $O(\\sqrt{S^2ADT_\\star K})$ regret for the full-information setting and $O(\\sqrt{S^3A^2DT_\\star K})$ regret for the bandit feedback setting, where $D$ is the diameter, $T_\\star$ is the expected hitting time of the optimal policy, $S$ is the number of states, $A$ is the number of actions, and $K$ is the number of episodes.\nOur work strictly improves (Rosenberg and Mansour, 2020) in the full information setting,\nextends (Chen et al., 2020) from known transition to unknown transition, \nand is also the first to consider the most challenging combination: bandit feedback with adversarial costs and unknown transition.\nTo remedy the gap between our upper bounds and the current best lower bounds constructed via a stochastically oblivious adversary,\nwe also propose algorithms with near-optimal regret for this special case.",
        "conference": "ICML",
        "success": true,
        "中文标题": "寻找低遗憾的随机最短路径：对抗性成本和未知转移情况",
        "摘要翻译": "我们在对抗性成本和未知转移情况下，针对随机最短路径问题取得了显著进展。具体而言，我们开发了算法，在完全信息设置下实现了O(√(S²ADT∗K))的遗憾值，在bandit反馈设置下实现了O(√(S³A²DT∗K))的遗憾值，其中D是直径，T∗是最佳策略的预期命中时间，S是状态的数量，A是行动的数量，K是episode的数量。我们的工作在完全信息设置下严格改进了(Rosenberg and Mansour, 2020)的结果，将(Chen et al., 2020)从已知转移扩展到未知转移，并且也是第一个考虑最具挑战性的组合：具有对抗性成本和未知转移的bandit反馈。为了弥补我们的上限与通过随机遗忘的对手构建的当前最佳下限之间的差距，我们还为这种特殊情况提出了具有接近最优遗憾的算法。",
        "领域": "强化学习、路径规划、在线学习",
        "问题": "如何在对抗性成本和未知转移概率的情况下，找到具有低遗憾的随机最短路径。",
        "动机": "现有的随机最短路径算法在对抗性成本和未知转移情况下性能不佳，并且缺乏针对这种情况的有效算法。",
        "方法": "开发了新的算法，分别在完全信息和bandit反馈设置下，实现了更好的遗憾界限。同时，针对随机遗忘对手这种特殊情况，设计了接近最优的算法。",
        "关键词": [
            "随机最短路径",
            "对抗性成本",
            "未知转移",
            "遗憾最小化",
            "Bandit反馈"
        ],
        "涉及的技术概念": {
            "遗憾最小化": "衡量算法性能的指标，目标是使算法的累积损失与最优策略的损失之间的差距最小化。",
            "Bandit反馈": "一种强化学习设置，其中智能体只能观察到所采取行动的直接奖励，而无法获得关于环境的完整信息。"
        }
    },
    {
        "order": 401,
        "title": "Finite mixture models do not reliably learn the number of components",
        "html": "https://ICML.cc//virtual/2021/poster/10705",
        "abstract": "Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. A common suggestion is to use a finite mixture model (FMM) with a prior on the number of components.  Past work has shown the resulting FMM component-count posterior is consistent; that is, the posterior concentrates on the true, generating number of components.  But consistency requires the assumption that the component likelihoods are perfectly specified, which is unrealistic in practice.  In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM component-count posterior diverges: the posterior probability of any particular finite number of components converges to 0 in the limit of infinite data.  Contrary to intuition, posterior-density consistency is not sufficient to establish this result.  We develop novel sufficient conditions that are more realistic and easily checkable than those common in the asymptotics literature. We illustrate practical consequences of our theory on simulated and real data.",
        "conference": "ICML",
        "success": true,
        "中文标题": "有限混合模型无法可靠地学习组件数量",
        "摘要翻译": "科学家和工程师常常对了解数据集中存在的子群体（或组件）数量感兴趣。一个常见的建议是使用有限混合模型（FMM）并在组件数量上设置先验。过去的工作表明，由此产生的FMM组件数量后验是一致的；也就是说，后验集中在真实的、生成的组件数量上。但一致性要求组件似然被完美指定的假设，这在实际中是不现实的。在本文中，我们通过证明即使在最轻微的模型错误指定下，FMM组件数量后验也会发散，为数据分析的民间智慧增添了严谨性：在无限数据的极限下，任何特定有限数量组件的后验概率收敛于0。与直觉相反，后验密度一致性不足以建立这一结果。我们开发了新的充分条件，这些条件比渐近文献中常见的条件更现实且易于检查。我们在模拟和真实数据上展示了我们理论的实际后果。",
        "领域": "统计机器学习, 贝叶斯非参数, 模型选择",
        "问题": "有限混合模型在实际应用中无法准确学习数据中的子群体数量",
        "动机": "探讨有限混合模型在模型错误指定情况下对组件数量学习的可靠性",
        "方法": "通过理论证明和实证分析，研究有限混合模型在模型错误指定下的行为",
        "关键词": [
            "有限混合模型",
            "模型错误指定",
            "后验发散",
            "统计一致性",
            "贝叶斯非参数"
        ],
        "涉及的技术概念": {
            "有限混合模型": "用于建模数据中潜在子群体的统计模型",
            "后验一致性": "指后验分布随着数据量增加而集中在真实参数值的性质",
            "模型错误指定": "指模型假设与真实数据生成过程不符的情况"
        }
    },
    {
        "order": 402,
        "title": "Finite-Sample Analysis of Off-Policy Natural Actor-Critic Algorithm",
        "html": "https://ICML.cc//virtual/2021/poster/10627",
        "abstract": "In this paper, we provide finite-sample convergence guarantees for an off-policy variant of the natural actor-critic (NAC) algorithm based on Importance Sampling. In particular, we show that the algorithm converges to a global optimal policy with a sample complexity of $\\mathcal{O}(\\epsilon^{-3}\\log^2(1/\\epsilon))$ under an appropriate choice of stepsizes. In order to overcome the issue of large variance due to Importance Sampling, we propose the $Q$-trace algorithm for the critic, which is inspired by the V-trace algorithm (Espeholt et al., 2018). This enables us to explicitly control the bias and variance, and characterize the trade-off between them. As an advantage of off-policy sampling, a major feature of our result is that we do not need any additional assumptions, beyond the ergodicity of the Markov chain induced by the behavior policy. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "离策略自然Actor-Critic算法的有限样本分析",
        "摘要翻译": "本文为基于重要性采样的离策略自然Actor-Critic (NAC) 算法提供有限样本收敛保证。 特别是，我们证明了在适当的步长选择下，该算法以 $\\\\mathcal{O}(\\\\epsilon^{-3}\\\\log^2(1/\\\\epsilon))$ 的样本复杂度收敛到全局最优策略。 为了克服由于重要性采样引起的大方差问题，我们为评论家提出了 $Q$-trace 算法，该算法的灵感来自 V-trace 算法 (Espeholt et al., 2018)。 这使我们能够显式地控制偏差和方差，并描述它们之间的权衡。 作为离策略采样的优势，我们的结果的一个主要特点是，除了行为策略引起的马尔可夫链的遍历性之外，我们不需要任何额外的假设。",
        "领域": "强化学习, 策略优化, 采样方法",
        "问题": "如何在离策略强化学习中，保证自然Actor-Critic算法的有限样本收敛性，并解决重要性采样导致的大方差问题。",
        "动机": "现有的强化学习算法在离策略学习时，由于重要性采样可能导致高方差，从而影响算法的稳定性和收敛速度。该研究旨在通过改进算法设计，提供更强的收敛保证和更好的性能。",
        "方法": "提出一种基于重要性采样的离策略自然Actor-Critic算法，并结合Q-trace算法作为评论家，显式地控制偏差和方差，以提高算法的稳定性和收敛速度。",
        "关键词": [
            "自然Actor-Critic",
            "离策略学习",
            "重要性采样",
            "有限样本",
            "Q-trace"
        ],
        "涉及的技术概念": {
            "自然Actor-Critic (NAC)": "一种强化学习算法，结合了Actor-Critic方法和自然梯度，旨在更有效地更新策略。",
            "重要性采样": "一种用于估计不同概率分布下期望值的技术，在离策略学习中用于校正行为策略和目标策略之间的差异。"
        }
    },
    {
        "order": 403,
        "title": "First-Order Methods for Wasserstein Distributionally Robust MDP",
        "html": "https://ICML.cc//virtual/2021/poster/8837",
        "abstract": "Markov decision processes (MDPs) are known to be sensitive to parameter specification.  Distributionally robust MDPs alleviate this issue by allowing for \\textit{ambiguity sets} which give a set of possible distributions over parameter sets. The goal is to find an optimal policy with respect to the worst-case parameter distribution. We propose a framework for solving Distributionally robust MDPs via first-order methods, and instantiate it for several types of Wasserstein ambiguity sets. By developing efficient proximal updates, our algorithms achieve a convergence rate of $O\\left(NA^{2.5}S^{3.5}\\log(S)\\log(\\epsilon^{-1})\\epsilon^{-1.5} \\right)$ for the number of kernels $N$ in the support of the nominal distribution, states $S$, and actions $A$; this rate varies slightly based on the Wasserstein setup. Our dependence on $N,A$ and $S$ is significantly better than existing methods, which have a complexity of $O\\left(N^{3.5}A^{3.5}S^{4.5}\\log^{2}(\\epsilon^{-1}) \\right)$. Numerical experiments show that our algorithm is significantly more scalable than state-of-the-art approaches across several domains.",
        "conference": "ICML",
        "中文标题": "用于Wasserstein分布鲁棒MDP的一阶方法",
        "摘要翻译": "已知马尔可夫决策过程（MDPs）对参数规范敏感。分布鲁棒MDPs通过允许参数集上的可能分布集合（称为模糊集）来缓解这一问题。目标是找到相对于最坏情况参数分布的最优策略。我们提出了一个通过一阶方法解决分布鲁棒MDPs的框架，并针对几种类型的Wasserstein模糊集进行了实例化。通过开发高效的近端更新，我们的算法在名义分布支持的内核数N、状态S和动作A方面实现了O(NA^2.5S^3.5log(S)log(ε^-1)ε^-1.5)的收敛速度；这个速度根据Wasserstein设置略有不同。我们在N、A和S上的依赖性显著优于现有方法，现有方法的复杂度为O(N^3.5A^3.5S^4.5log^2(ε^-1))。数值实验表明，我们的算法在多个领域中比最先进的方法具有显著的可扩展性。",
        "领域": "强化学习、分布鲁棒优化、马尔可夫决策过程",
        "问题": "解决马尔可夫决策过程对参数规范敏感的问题，通过分布鲁棒优化方法寻找最优策略。",
        "动机": "提高马尔可夫决策过程在参数不确定性下的鲁棒性和可扩展性。",
        "方法": "提出一个通过一阶方法解决分布鲁棒MDPs的框架，并针对几种类型的Wasserstein模糊集进行实例化，开发高效的近端更新算法。",
        "关键词": [
            "分布鲁棒MDP",
            "Wasserstein模糊集",
            "一阶方法",
            "近端更新",
            "可扩展性"
        ],
        "涉及的技术概念": {
            "Wasserstein模糊集": "用于描述参数分布的不确定性集合，帮助模型在不确定性下保持鲁棒性。",
            "一阶方法": "优化算法，用于高效解决分布鲁棒MDPs问题，提高计算效率。",
            "近端更新": "算法中的关键步骤，用于在每次迭代中有效地更新策略，保证算法的收敛速度。"
        },
        "success": true
    },
    {
        "order": 404,
        "title": "Fixed-Parameter and Approximation Algorithms for PCA with Outliers",
        "html": "https://ICML.cc//virtual/2021/poster/9215",
        "abstract": "PCA with Outliers is the fundamental problem of identifying an underlying low-dimensional subspace in a data set  corrupted with outliers. A large body of work is devoted to the information-theoretic aspects of this problem. However, from the computational perspective, its complexity is still not well-understood. We study this problem from the perspective of parameterized complexity  by investigating how parameters like the dimension of the data, the subspace dimension,  the number of outliers and their structure, and approximation error, influence the computational complexity of the problem. Our algorithmic methods are based on techniques of randomized linear algebra and algebraic geometry. ",
        "conference": "ICML",
        "中文标题": "PCA异常值处理的固定参数与近似算法",
        "摘要翻译": "PCA（主成分分析）异常值处理是识别数据集中被异常值破坏的潜在低维子空间的基本问题。大量工作致力于这一问题的信息理论方面。然而，从计算的角度来看，其复杂性仍未得到充分理解。我们通过研究数据的维度、子空间维度、异常值的数量及其结构以及近似误差等参数如何影响问题的计算复杂性，从参数化复杂性的角度研究这一问题。我们的算法方法基于随机线性代数和代数几何的技术。",
        "领域": "异常检测、降维技术、参数化算法",
        "问题": "研究PCA在处理含有异常值的数据时，如何通过参数化方法提高计算效率。",
        "动机": "理解PCA异常值处理问题的计算复杂性，探索参数化方法在提高算法效率方面的潜力。",
        "方法": "采用随机线性代数和代数几何的技术，研究不同参数对PCA异常值处理问题计算复杂性的影响。",
        "关键词": [
            "PCA异常值处理",
            "参数化算法",
            "随机线性代数",
            "代数几何",
            "计算复杂性"
        ],
        "涉及的技术概念": {
            "参数化复杂性": "研究问题在不同参数下的计算复杂性，旨在为PCA异常值处理问题提供更高效的算法解决方案。",
            "随机线性代数": "利用随机化技术处理线性代数问题，用于提高PCA异常值处理算法的效率和可扩展性。",
            "代数几何": "应用代数几何的理论和方法，分析和解决PCA异常值处理中的数学问题。"
        },
        "success": true
    },
    {
        "order": 405,
        "title": "FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Analysis ",
        "html": "https://ICML.cc//virtual/2021/poster/10617",
        "abstract": "Federated Learning (FL) is an emerging learning scheme that allows different distributed clients to train deep neural networks together without data sharing. Neural networks have become popular due to their unprecedented success. To the best of our knowledge, the theoretical guarantees of FL concerning neural networks with explicit forms and multi-step updates are unexplored.  Nevertheless, training analysis of neural networks in FL is non-trivial for two reasons: first, the objective loss function we are optimizing is non-smooth and non-convex, and second, we are even not updating in the gradient direction. Existing convergence results for gradient descent-based methods heavily rely on the fact that the gradient direction is used for updating. The current paper presents a new class of convergence analysis for FL, Federated Neural Tangent Kernel (FL-NTK), which corresponds to overparamterized ReLU neural networks trained by gradient descent in FL and is inspired by the analysis in Neural Tangent Kernel (NTK). Theoretically, FL-NTK converges to a global-optimal solution at a linear rate with properly tuned learning parameters. Furthermore, with proper distributional assumptions, FL-NTK can also achieve good generalization. The proposed theoretical analysis scheme can be generalized to more complex neural networks.",
        "conference": "ICML",
        "中文标题": "FL-NTK：基于神经切线核的联邦学习分析框架",
        "摘要翻译": "联邦学习（FL）是一种新兴的学习方案，它允许不同的分布式客户端在不共享数据的情况下共同训练深度神经网络。神经网络因其前所未有的成功而变得流行。据我们所知，关于具有明确形式和多步更新的神经网络在联邦学习中的理论保证尚未被探索。然而，在联邦学习中对神经网络的训练分析并非易事，原因有二：首先，我们正在优化的目标损失函数是非光滑和非凸的；其次，我们甚至没有在梯度方向上进行更新。现有的基于梯度下降方法的收敛结果在很大程度上依赖于使用梯度方向进行更新的事实。本文提出了一种新的联邦学习收敛分析类别——联邦神经切线核（FL-NTK），它对应于在联邦学习中通过梯度下降训练的过参数化ReLU神经网络，并受到神经切线核（NTK）分析的启发。理论上，FL-NTK在适当调整学习参数的情况下以线性速率收敛到全局最优解。此外，在适当的分布假设下，FL-NTK也能实现良好的泛化。所提出的理论分析方案可以推广到更复杂的神经网络。",
        "领域": "联邦学习、深度学习理论、神经网络优化",
        "问题": "探索在联邦学习环境下，具有明确形式和多步更新的神经网络的理论保证",
        "动机": "解决联邦学习中神经网络训练分析的非光滑、非凸目标函数及非梯度方向更新的挑战",
        "方法": "提出联邦神经切线核（FL-NTK）框架，基于神经切线核（NTK）理论，分析过参数化ReLU神经网络在联邦学习中的收敛性和泛化能力",
        "关键词": [
            "联邦学习",
            "神经切线核",
            "过参数化神经网络",
            "收敛分析",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "联邦学习（FL）": "一种允许分布式客户端在不共享数据的情况下共同训练模型的学习方案",
            "神经切线核（NTK）": "用于分析过参数化神经网络训练动态的理论工具，本文中用于分析联邦学习环境下的神经网络收敛性",
            "过参数化ReLU神经网络": "指网络参数数量远大于训练样本数的ReLU激活神经网络，FL-NTK框架针对此类网络的训练进行分析"
        },
        "success": true
    },
    {
        "order": 406,
        "title": "Flow-based Attribution in Graphical Models: A Recursive Shapley Approach",
        "html": "https://ICML.cc//virtual/2021/poster/10049",
        "abstract": "We study the attribution problem in a graphical model, wherein the objective is to quantify how the effect of changes at the source nodes propagates through the graph. We develop a model-agnostic flow-based attribution method, called recursive Shapley value (RSV). RSV generalizes a number of existing node-based methods and uniquely satisfies a set of flow-based axioms. In addition to admitting a natural characterization for linear models and facilitating mediation analysis for non-linear models, RSV satisfies a mix of desirable properties discussed in the recent literature, including implementation invariance, sensitivity, monotonicity, and affine scale invariance.",
        "conference": "ICML",
        "中文标题": "图模型中的基于流的归因：一种递归Shapley方法",
        "摘要翻译": "我们研究了图模型中的归因问题，其目标是量化源节点变化的效果如何通过图传播。我们开发了一种与模型无关的基于流的归因方法，称为递归Shapley值（RSV）。RSV概括了许多现有的基于节点的方法，并独特地满足了一组基于流的公理。除了为线性模型提供自然特性描述和为非线模型促进中介分析外，RSV还满足最近文献中讨论的一系列理想属性，包括实现不变性、敏感性、单调性和仿射尺度不变性。",
        "领域": "图神经网络、因果推理、机器学习解释性",
        "问题": "量化图模型中源节点变化效果如何通过图传播的问题",
        "动机": "开发一种能够概括现有方法并满足基于流公理的归因方法，以更好地理解和解释图模型中的变化传播",
        "方法": "提出了一种称为递归Shapley值（RSV）的模型无关的基于流的归因方法，该方法概括了现有基于节点的方法并满足基于流的公理",
        "关键词": [
            "图模型",
            "归因分析",
            "递归Shapley值",
            "模型解释性",
            "流基方法"
        ],
        "涉及的技术概念": {
            "递归Shapley值（RSV）": "一种与模型无关的基于流的归因方法，用于量化图模型中源节点变化的效果传播",
            "基于流的公理": "一组RSV方法满足的公理，确保归因结果的合理性和一致性",
            "实现不变性": "RSV方法满足的属性之一，确保归因结果不依赖于模型的具体实现方式"
        },
        "success": true
    },
    {
        "order": 407,
        "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for  Protein Design",
        "html": "https://ICML.cc//virtual/2021/poster/9593",
        "abstract": " Designing novel protein sequences for a desired 3D topological fold is a fundamental yet  non-trivial task in protein engineering. Challenges exist due to the complex sequence--fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. \nTo overcome these challenges, we propose Fold2Seq, a novel transformer-based generative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence--structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared  to existing state-of-the-art methods that include  data-driven deep generative models and physics-based RosettaDesign. The unique advantages of fold-based Fold2Seq,  in comparison to a  structure-based deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",
        "conference": "ICML",
        "中文标题": "Fold2Seq：一种基于联合序列（1D）-折叠（3D）嵌入的生成模型用于蛋白质设计",
        "摘要翻译": "为期望的3D拓扑折叠设计新颖的蛋白质序列是蛋白质工程中一项基本但非平凡的任务。由于复杂的序列-折叠关系，以及难以捕捉折叠内序列（因此结构和功能）的多样性，存在挑战。为了克服这些挑战，我们提出了Fold2Seq，一种基于Transformer的新型生成框架，用于设计针对特定目标折叠的蛋白质序列。为了模拟复杂的序列-结构关系，Fold2Seq联合学习了一个使用Transformer的序列嵌入和一个来自3D体素中二级结构元素密度的折叠嵌入。在测试集上，对于单个折叠的单、高分辨率和完整结构输入，我们的实验表明，Fold2Seq在序列设计的速度、覆盖范围和可靠性方面，与现有的最先进方法（包括数据驱动的深度生成模型和基于物理的RosettaDesign）相比，具有改进或相当的性能。与基于结构的深度模型和RosettaDesign相比，Fold2Seq基于折叠的独特优势在三个源自低质量、不完整或模糊输入结构的现实世界挑战中变得更加明显。源代码和数据可在https://github.com/IBM/fold2seq获取。",
        "领域": "蛋白质设计、深度学习、生物信息学",
        "问题": "如何设计新颖的蛋白质序列以适应特定的3D拓扑折叠",
        "动机": "解决蛋白质工程中由于复杂的序列-折叠关系及序列多样性带来的设计挑战",
        "方法": "提出Fold2Seq，一个基于Transformer的生成框架，联合学习序列和折叠的嵌入，以设计针对特定目标折叠的蛋白质序列",
        "关键词": [
            "蛋白质设计",
            "Transformer",
            "序列嵌入",
            "折叠嵌入",
            "生成模型"
        ],
        "涉及的技术概念": {
            "Transformer": "用于学习蛋白质序列的嵌入，捕捉序列的复杂模式",
            "折叠嵌入": "从3D体素中二级结构元素的密度学习，用于表示蛋白质的3D结构",
            "生成模型": "用于设计新颖蛋白质序列的框架，能够根据特定的3D折叠生成相应的序列"
        },
        "success": true
    },
    {
        "order": 408,
        "title": "Follow-the-Regularized-Leader Routes to Chaos in Routing Games",
        "html": "https://ICML.cc//virtual/2021/poster/9621",
        "abstract": "We study the emergence of chaotic behavior of Follow-the-Regularized Leader (FoReL)  dynamics in games. We focus on the effects of increasing the population size or the scale of costs in  congestion games, and generalize recent results on unstable, chaotic behaviors in the Multiplicative Weights Update dynamics to a much larger class of FoReL dynamics. We establish that, even in simple linear non-atomic congestion games with two parallel links and \\emph{any} fixed learning rate, unless the game is fully symmetric, increasing the population size or the scale of costs causes learning dynamics to becomes unstable and eventually chaotic, in the sense of Li-Yorke and positive topological entropy. Furthermore, we prove the existence of  novel non-standard phenomena such as the coexistence of stable Nash equilibria and chaos in the same game. We also observe the simultaneous creation of a chaotic attractor as another chaotic attractor gets destroyed.  Lastly, although FoReL dynamics can be strange and non-equilibrating, we prove that the time average still converges to an \\emph{exact} equilibrium for any choice of learning rate and any scale of costs.",
        "conference": "ICML",
        "中文标题": "跟随正则化领导者路径在路由游戏中的混沌现象",
        "摘要翻译": "我们研究了跟随正则化领导者（FoReL）动态在游戏中混沌行为的出现。我们专注于增加人口规模或拥堵游戏成本规模的影响，并将最近关于乘法权重更新动态中不稳定、混沌行为的结果推广到更广泛的FoReL动态类别。我们证实，即使在具有两条平行链路和任何固定学习率的简单线性非原子拥堵游戏中，除非游戏完全对称，增加人口规模或成本规模会导致学习动态变得不稳定并最终混沌，这是从Li-Yorke和正拓扑熵的意义上说的。此外，我们证明了新颖的非标准现象的存在，如在同一游戏中稳定纳什均衡和混沌的共存。我们还观察到，随着另一个混沌吸引子被破坏，同时创建了一个混沌吸引子。最后，尽管FoReL动态可能是奇怪且不均衡的，我们证明了时间平均仍然会收敛到一个精确的均衡，无论学习率和成本规模如何选择。",
        "领域": "博弈论与机器学习交叉应用、动态系统稳定性分析、复杂网络行为研究",
        "问题": "研究跟随正则化领导者动态在拥堵游戏中导致混沌行为的条件和机制",
        "动机": "探索在非对称条件下，增加人口规模或成本规模如何导致学习动态的不稳定性和混沌行为，以及这种现象对游戏理论模型的影响",
        "方法": "通过理论分析和数学证明，扩展乘法权重更新动态中混沌行为的研究到更广泛的跟随正则化领导者动态类别，并分析其在不同条件下的行为",
        "关键词": [
            "跟随正则化领导者",
            "混沌行为",
            "拥堵游戏",
            "动态稳定性",
            "纳什均衡"
        ],
        "涉及的技术概念": {
            "跟随正则化领导者（FoReL）": "一种在博弈论中用于描述玩家如何根据历史信息调整策略的动态过程，本研究探讨其在特定条件下导致混沌行为的机制",
            "Li-Yorke混沌": "一种混沌的定义方式，用于描述动态系统中出现的不可预测和复杂行为，本研究通过此概念证明FoReL动态的混沌性质",
            "正拓扑熵": "衡量动态系统复杂性和不可预测性的指标，本研究利用此概念量化FoReL动态的混沌程度"
        },
        "success": true
    },
    {
        "order": 409,
        "title": "FOP: Factorizing Optimal Joint Policy of Maximum-Entropy Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9269",
        "abstract": "Value decomposition recently injects vigorous vitality into multi-agent actor-critic methods. However, existing decomposed actor-critic methods cannot guarantee the convergence of global optimum. In this paper, we present a novel multi-agent actor-critic method, FOP, which can factorize the optimal joint policy induced by maximum-entropy multi-agent reinforcement learning (MARL) into individual policies. Theoretically, we prove that factorized individual policies of FOP converge to the global optimum. Empirically, in the well-known matrix game and differential game, we verify that FOP can converge to the global optimum for both discrete and continuous action spaces. We also evaluate FOP on a set of StarCraft II micromanagement tasks, and demonstrate that FOP substantially outperforms state-of-the-art decomposed value-based and actor-critic methods.",
        "conference": "ICML",
        "中文标题": "FOP：最大熵多智能体强化学习中最优联合策略的因子分解",
        "摘要翻译": "价值分解最近为多智能体行动者-评论家方法注入了新的活力。然而，现有的分解行动者-评论家方法无法保证全局最优的收敛性。在本文中，我们提出了一种新颖的多智能体行动者-评论家方法FOP，它可以将由最大熵多智能体强化学习（MARL）诱导的最优联合策略分解为个体策略。理论上，我们证明了FOP分解的个体策略能够收敛到全局最优。实证上，在著名的矩阵游戏和微分游戏中，我们验证了FOP对于离散和连续动作空间都能收敛到全局最优。我们还在StarCraft II微管理任务集上评估了FOP，结果表明FOP显著优于最先进的分解价值基和行动者-评论家方法。",
        "领域": "多智能体强化学习、策略优化、价值分解",
        "问题": "现有的分解行动者-评论家方法无法保证全局最优的收敛性",
        "动机": "为了解决多智能体强化学习中全局最优策略收敛性的问题",
        "方法": "提出了一种新的多智能体行动者-评论家方法FOP，通过因子分解最优联合策略为个体策略，确保收敛到全局最优",
        "关键词": [
            "多智能体强化学习",
            "最大熵",
            "策略分解",
            "全局最优",
            "行动者-评论家方法"
        ],
        "涉及的技术概念": {
            "最大熵多智能体强化学习": "在强化学习中引入最大熵原理，以促进探索并找到更鲁棒的策略",
            "价值分解": "将全局价值函数分解为个体价值函数，以便于多智能体环境中的策略学习",
            "行动者-评论家方法": "结合策略梯度方法和价值函数方法的强化学习框架，用于策略优化"
        },
        "success": true
    },
    {
        "order": 410,
        "title": "From Local Structures to Size Generalization in Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10719",
        "abstract": "Graph neural networks (GNNs) can process graphs of different sizes, but their ability to generalize across sizes, specifically from small to large graphs, is still not well understood. \nIn this paper, we identify an important type of data where generalization from small to large graphs is challenging: graph distributions for which the local structure depends on the graph size. This effect occurs in multiple important graph learning domains, including social and biological networks. We first prove that when there is a difference between the local structures, GNNs are not guaranteed to generalize across sizes: there are 'bad' global minima that do well on small graphs but fail on large graphs. We then study the size-generalization problem empirically and demonstrate that when there is a discrepancy in local structure, GNNs tend to converge to non-generalizing solutions. Finally, we suggest two approaches for improving size generalization, motivated by our findings. Notably, we propose a novel Self-Supervised Learning (SSL) task aimed at learning meaningful representations of local structures that appear in large graphs. Our SSL task improves classification accuracy on several popular datasets.",
        "conference": "ICML",
        "中文标题": "从局部结构到图神经网络中的尺寸泛化",
        "摘要翻译": "图神经网络（GNNs）能够处理不同尺寸的图，但它们在小图到大图的尺寸泛化能力仍未被充分理解。本文中，我们识别了一种重要的数据类型，其中从小图到大图的泛化具有挑战性：局部结构依赖于图尺寸的图分布。这种现象在多个重要的图学习领域中出现，包括社交和生物网络。我们首先证明，当局部结构存在差异时，GNNs不能保证跨尺寸泛化：存在在小图上表现良好但在大图上失败的'坏'全局最小值。接着，我们实证研究了尺寸泛化问题，并证明当局部结构存在不一致时，GNNs倾向于收敛到不泛化的解。最后，基于我们的发现，我们提出了两种改进尺寸泛化的方法。值得注意的是，我们提出了一种新颖的自监督学习（SSL）任务，旨在学习大图中出现的局部结构的有意义表示。我们的SSL任务在几个流行数据集上提高了分类准确率。",
        "领域": "图神经网络、社交网络分析、生物信息学",
        "问题": "图神经网络在小图到大图的尺寸泛化能力不足",
        "动机": "理解和改进图神经网络在不同尺寸图上的泛化能力，特别是在局部结构依赖于图尺寸的情况下",
        "方法": "理论证明和实证研究尺寸泛化问题，并提出基于自监督学习的改进方法",
        "关键词": [
            "图神经网络",
            "尺寸泛化",
            "自监督学习",
            "局部结构",
            "分类准确率"
        ],
        "涉及的技术概念": {
            "图神经网络（GNNs）": "用于处理图结构数据的深度学习模型，能够捕捉图中节点和边的复杂关系",
            "尺寸泛化": "指模型在不同尺寸输入上的性能表现，特别是在小图训练后在大图上的应用能力",
            "自监督学习（SSL）": "一种无需人工标注的学习方法，通过设计预测任务从数据本身学习有用的表示"
        },
        "success": true
    },
    {
        "order": 411,
        "title": "From Local to Global Norm Emergence: Dissolving Self-reinforcing Substructures with Incremental Social Instruments",
        "html": "https://ICML.cc//virtual/2021/poster/10711",
        "abstract": "Norm emergence is a process where agents in a multi-agent system establish self-enforcing conformity through repeated interactions. When such interactions are confined to a social topology, several self-reinforcing substructures (SRS) may emerge within the population. This prevents a formation of a global norm. We propose incremental social instruments (ISI) to dissolve these SRSs by creating ties between agents. Establishing ties requires some effort and cost. Hence, it is worth to design methods that build a small number of ties yet dissolve the SRSs. By using the notion of information entropy, we propose an indicator called the BA-ratio that measures the current SRSs. We find that by building ties with minimal BA-ratio, our ISI is effective in facilitating the global norm emergence. We explain this through our experiments and theoretical results. Furthermore, we propose the small-degree principle in minimising the BA-ratio that helps us to design efficient ISI algorithms for finding the optimal ties. Experiments on both synthetic and real-world network topologies demonstrate that our adaptive ISI is efficient at dissolving SRS.",
        "conference": "ICML",
        "中文标题": "从局部到全局规范的涌现：通过增量社会工具消解自我强化的子结构",
        "摘要翻译": "规范涌现是多智能体系统中智能体通过重复互动建立自我强化一致性的过程。当这种互动局限于社会拓扑结构时，群体中可能会出现几个自我强化的子结构（SRS），这阻碍了全局规范的形成。我们提出增量社会工具（ISI）通过在智能体之间建立联系来消解这些SRS。建立联系需要一定的努力和成本。因此，设计一种建立少量联系却能消解SRS的方法是有价值的。通过使用信息熵的概念，我们提出了一个称为BA比率的指标，用于测量当前的SRS。我们发现，通过建立具有最小BA比率的联系，我们的ISI在促进全局规范涌现方面是有效的。我们通过实验和理论结果解释了这一点。此外，我们提出了最小化BA比率的小度原则，这有助于我们设计高效的ISI算法来寻找最优联系。在合成和现实世界网络拓扑上的实验表明，我们的自适应ISI在消解SRS方面是高效的。",
        "领域": "多智能体系统、社会网络分析、规范涌现",
        "问题": "在多智能体系统中，如何消解自我强化的子结构以促进全局规范的涌现。",
        "动机": "研究旨在解决多智能体系统中由于局部互动导致的自我强化子结构（SRS）阻碍全局规范形成的问题。",
        "方法": "提出增量社会工具（ISI）和BA比率指标，通过建立最小BA比率的联系来消解SRS，并设计高效的ISI算法。",
        "关键词": [
            "规范涌现",
            "自我强化子结构",
            "增量社会工具",
            "BA比率",
            "小度原则"
        ],
        "涉及的技术概念": {
            "增量社会工具（ISI）": "用于在多智能体系统中建立联系以消解自我强化子结构的技术。",
            "BA比率": "基于信息熵的指标，用于测量当前自我强化子结构的状态。",
            "小度原则": "在最小化BA比率过程中提出的原则，用于设计高效的ISI算法。"
        },
        "success": true
    },
    {
        "order": 412,
        "title": "From Poincaré Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/9903",
        "abstract": "In this paper we investigate the Follow the Regularized Leader dynamics in sequential imperfect information games (IIG). We generalize existing results of Poincaré recurrence from normal-form games to  zero-sum two-player imperfect information games and other sequential game settings. We then investigate how adapting the reward (by adding a regularization term) of the game can give strong convergence guarantees in monotone games. We continue by showing how this reward adaptation technique can be leveraged to build algorithms that converge exactly to the Nash equilibrium. Finally, we show how these insights can be directly used to build state-of-the-art model-free\nalgorithms for zero-sum two-player Imperfect Information Games (IIG).",
        "conference": "ICML",
        "中文标题": "从庞加莱回归到不完全信息博弈中的收敛：通过正则化寻找均衡",
        "摘要翻译": "本文研究了序列不完全信息博弈(IIG)中的跟随正则化领导者动态。我们将庞加莱回归的现有结果从标准形式博弈推广到零和双人不完全信息博弈及其他序列博弈设置。接着，我们探讨了如何通过调整游戏的奖励（通过添加正则化项）在单调游戏中提供强收敛保证。我们进一步展示了如何利用这种奖励调整技术构建精确收敛至纳什均衡的算法。最后，我们展示了如何直接利用这些见解为零和双人不完全信息博弈(IIG)构建最先进的无模型算法。",
        "领域": "博弈论与强化学习结合、不完全信息博弈、纳什均衡求解",
        "问题": "在不完全信息博弈中，如何通过调整奖励机制和利用正则化技术，实现算法的强收敛性并精确找到纳什均衡。",
        "动机": "研究动机在于扩展庞加莱回归理论的应用范围至不完全信息博弈，并通过正则化方法改进算法的收敛性能，以更有效地求解纳什均衡。",
        "方法": "采用跟随正则化领导者动态，通过添加正则化项调整游戏奖励，构建能够精确收敛至纳什均衡的算法，并应用于不完全信息博弈中。",
        "关键词": [
            "不完全信息博弈",
            "正则化",
            "纳什均衡",
            "庞加莱回归",
            "无模型算法"
        ],
        "涉及的技术概念": {
            "跟随正则化领导者动态": "一种在序列决策问题中，通过引入正则化项来调整策略更新，以优化长期表现的动态策略。",
            "庞加莱回归": "在动力系统中，系统状态会无限接近其初始状态的现象，本文将其理论应用于博弈论中，分析博弈动态的长期行为。",
            "纳什均衡": "在博弈论中，指所有参与者的策略组合，使得没有任何一方能通过单方面改变策略而获得更好的结果。"
        },
        "success": true
    },
    {
        "order": 413,
        "title": "Functional Space Analysis of Local GAN Convergence",
        "html": "https://ICML.cc//virtual/2021/poster/10305",
        "abstract": "Recent work demonstrated the benefits of studying continuous-time dynamics governing the GAN training. However, this dynamics is analyzed in the model parameter space, which results in finite-dimensional dynamical systems. We propose a novel perspective where we study the local dynamics of adversarial training in the general functional space and show how it can be represented as a system of partial differential equations. Thus, the convergence properties can be inferred from the eigenvalues of the resulting differential operator. We show that these eigenvalues can be efficiently estimated from the target dataset before training. Our perspective reveals several insights on the practical tricks commonly used to stabilize GANs, such as gradient penalty, data augmentation, and advanced integration schemes. As an immediate practical benefit, we demonstrate how one can a priori select an optimal data augmentation strategy for a particular generation task.",
        "conference": "ICML",
        "中文标题": "局部GAN收敛的功能空间分析",
        "摘要翻译": "最近的研究展示了研究控制GAN训练的连续时间动态的好处。然而，这种动态是在模型参数空间中分析的，这导致了有限维动态系统。我们提出了一种新颖的视角，在一般的功能空间中研究对抗训练的局部动态，并展示了如何将其表示为一个偏微分方程系统。因此，收敛性质可以从所得微分算子的特征值中推断出来。我们展示了这些特征值可以在训练前从目标数据集中高效估计。我们的视角揭示了关于通常用于稳定GAN的实用技巧的几个见解，如梯度惩罚、数据增强和高级积分方案。作为一个直接的实际好处，我们展示了如何为特定的生成任务先验地选择最优的数据增强策略。",
        "领域": "生成对抗网络、深度学习优化、图像生成",
        "问题": "分析GAN训练过程中的局部动态及其收敛性质",
        "动机": "为了更深入地理解GAN训练的动态过程，并基于此优化训练策略",
        "方法": "在功能空间中研究GAN的局部动态，将其建模为偏微分方程系统，并通过特征值分析收敛性质",
        "关键词": [
            "生成对抗网络",
            "功能空间分析",
            "偏微分方程",
            "数据增强",
            "训练稳定性"
        ],
        "涉及的技术概念": {
            "功能空间分析": "在一般的功能空间中研究GAN的局部动态，而非传统的参数空间",
            "偏微分方程系统": "将GAN的局部动态表示为偏微分方程系统，用于分析收敛性质",
            "特征值分析": "通过分析微分算子的特征值来推断GAN训练的收敛性质"
        },
        "success": true
    },
    {
        "order": 414,
        "title": "Function Contrastive Learning of Transferable Meta-Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10763",
        "abstract": "Meta-learning algorithms adapt quickly to new tasks that are drawn from the same task distribution as the training tasks. The mechanism leading to fast adaptation is the conditioning of a downstream predictive model on the inferred representation of the task's underlying data generative process, or \\emph{function}. This \\emph{meta-representation}, which is computed from a few observed examples of the underlying function, is learned jointly with the predictive model. In this work, we study the implications of this joint training on the transferability of the meta-representations. Our goal is to learn meta-representations that are robust to noise in the data and facilitate solving a wide range of downstream tasks that share the same underlying functions. To this end, we propose a decoupled encoder-decoder approach to supervised meta-learning, where the encoder is trained with a contrastive objective to find a good representation of the underlying function. In particular, our training scheme is driven by the self-supervision signal indicating whether two sets of examples\nstem from the same function. Our experiments on a number of synthetic and real-world datasets show that the representations we obtain outperform strong baselines in terms of downstream performance and noise robustness, even when these baselines are trained in an end-to-end manner.",
        "conference": "ICML",
        "success": true,
        "中文标题": "功能对比学习可转移元表示",
        "摘要翻译": "元学习算法能够快速适应与训练任务来自同一任务分布的新任务。导致快速适应的机制是基于对任务底层数据生成过程或功能的推断表示来调节下游预测模型。这种元表示是从底层功能的几个观察示例中计算出来的，与预测模型联合学习。在这项工作中，我们研究了这种联合训练对元表示可转移性的影响。我们的目标是学习对数据中的噪声具有鲁棒性，并有助于解决共享相同底层功能的广泛下游任务的元表示。为此，我们提出了一种解耦的编码器-解码器方法来进行监督元学习，其中编码器通过对比目标进行训练，以找到底层功能的良好表示。特别是，我们的训练方案是由自监督信号驱动的，该信号指示两组示例是否来自同一功能。我们在多个合成和真实世界数据集上的实验表明，我们获得的表示在下游性能和噪声鲁棒性方面优于强基线，即使这些基线是以端到端方式训练的。",
        "领域": "元学习, 对比学习, 表示学习",
        "问题": "如何提高元表示的可转移性和对噪声的鲁棒性",
        "动机": "学习能够适应广泛下游任务并对数据噪声具有鲁棒性的元表示",
        "方法": "提出了一种解耦的编码器-解码器方法，通过对比学习训练编码器以优化元表示",
        "关键词": [
            "元学习",
            "对比学习",
            "表示学习",
            "噪声鲁棒性",
            "可转移性"
        ],
        "涉及的技术概念": {
            "元表示": "从底层功能的几个观察示例中计算出来的表示，用于调节下游预测模型",
            "对比学习": "通过比较不同示例是否来自同一功能来训练编码器的方法",
            "编码器-解码器": "一种解耦的学习框架，其中编码器负责生成元表示，解码器负责下游任务"
        }
    },
    {
        "order": 415,
        "title": "Fundamental Tradeoffs in Distributionally Adversarial Training",
        "html": "https://ICML.cc//virtual/2021/poster/8907",
        "abstract": "Adversarial training is among the most effective techniques to improve robustness of models against adversarial perturbations. However, the full effect of this approach on models is not well understood. For example, while adversarial training can reduce the adversarial risk (prediction error against an adversary), it sometimes increase standard risk (generalization error when there is no adversary). In this paper, we focus on \\emph{distribution perturbing} adversary framework wherein the adversary can change the test distribution within a neighborhood of the training data distribution. The neighborhood is defined via Wasserstein distance between distributions and the radius of the neighborhood is a measure of adversary's manipulative power. We study the tradeoff between standard risk and adversarial risk and derive the Pareto-optimal tradeoff, achievable over specific classes of models, in the infinite data limit with features dimension kept fixed. We consider three learning settings: 1) Regression with the class of linear models; 2) Binary classification under the Gaussian mixtures data model, with the class of linear classifiers; 3) Regression with the class of random features model (which can be equivalently represented as two-layer neural network with random first-layer weights). We show that a tradeoff between standard and adversarial risk is manifested in all three settings. We further characterize the Pareto-optimal tradeoff curves and discuss how a variety of factors, such as features correlation, adversary's power or the width of two-layer neural network would affect this tradeoff. \n",
        "conference": "ICML",
        "success": true,
        "中文标题": "分布对抗训练中的基本权衡",
        "摘要翻译": "对抗训练是提高模型对抗扰动鲁棒性的最有效技术之一。然而，这种方法对模型的全面影响尚未被充分理解。例如，虽然对抗训练可以减少对抗风险（对对抗者的预测错误），但有时会增加标准风险（无对抗者时的泛化错误）。在本文中，我们专注于一个分布扰动对抗者框架，其中对抗者可以在训练数据分布的邻域内改变测试分布。邻域通过分布之间的Wasserstein距离定义，邻域的半径是对抗者操纵能力的度量。我们研究了标准风险和对抗风险之间的权衡，并在特征维度保持固定的无限数据限制下，推导出在特定模型类别上可实现的帕累托最优权衡。我们考虑了三种学习设置：1）使用线性模型类进行回归；2）在高斯混合数据模型下，使用线性分类器类进行二元分类；3）使用随机特征模型类进行回归（可以等效表示为具有随机第一层权重的两层神经网络）。我们展示了在这三种设置中都显现了标准风险和对抗风险之间的权衡。我们进一步描述了帕累托最优权衡曲线，并讨论了各种因素，如特征相关性、对抗者的能力或两层神经网络的宽度，如何影响这一权衡。",
        "领域": "对抗学习, 机器学习理论, 深度学习鲁棒性",
        "问题": "研究对抗训练中标准风险与对抗风险之间的权衡关系",
        "动机": "理解对抗训练对模型标准风险和对抗风险的影响，探索在不同学习设置下的最优权衡",
        "方法": "在分布扰动对抗者框架下，通过Wasserstein距离定义对抗者的能力，研究三种不同学习设置下的风险权衡，并推导帕累托最优权衡曲线",
        "关键词": [
            "对抗训练",
            "Wasserstein距离",
            "帕累托最优",
            "标准风险",
            "对抗风险"
        ],
        "涉及的技术概念": {
            "对抗训练": "一种提高模型对抗扰动鲁棒性的技术，通过模拟对抗者的攻击来训练模型",
            "Wasserstein距离": "用于衡量两个概率分布之间的距离，本文中用于定义对抗者可以改变的测试分布的范围",
            "帕累托最优": "在多个目标优化问题中，无法在不损害至少一个目标的情况下改进另一个目标的状态，本文中用于描述标准风险和对抗风险之间的最优权衡"
        }
    },
    {
        "order": 416,
        "title": "Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation",
        "html": "https://ICML.cc//virtual/2021/poster/10225",
        "abstract": "Recently, representation learning for text and speech has successfully improved many language related tasks. However, all existing methods suffer from two limitations: (a) they only learn from one input modality, while a unified representation for both speech and text is needed by tasks such as end-to-end speech translation, and as a result, (b) they can not exploit various large-scale text and speech data and their performance is limited by the scarcity of parallel speech translation data. To address these problems, we propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly learns a unified representation for both acoustic and text input from various types of corpora including parallel data for speech recognition and machine translation, and even pure speech and text data. Within this cross-modal representation learning framework, we further present an end-to-end model for Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three translation directions show that by fine-tuning from FAT-MLM, our proposed speech translation models substantially improve translation quality by up to +5.9 BLEU.",
        "conference": "ICML",
        "中文标题": "融合声学与文本编码的多模态双语预训练与语音翻译",
        "摘要翻译": "最近，文本和语音的表征学习已成功改进了许多与语言相关的任务。然而，所有现有方法都存在两个局限性：(a) 它们仅从单一输入模态学习，而诸如端到端语音翻译等任务需要语音和文本的统一表征；(b) 它们无法利用各种大规模的文本和语音数据，其性能受到平行语音翻译数据稀缺的限制。为了解决这些问题，我们提出了一种融合声学与文本的掩码语言模型（FAT-MLM），该模型从包括语音识别和机器翻译的平行数据，甚至纯语音和文本数据在内的各种类型语料库中，联合学习声学和文本输入的统一表征。在这个跨模态表征学习框架内，我们进一步提出了一个用于融合声学与文本语音翻译（FAT-ST）的端到端模型。在三个翻译方向上的实验表明，通过从FAT-MLM进行微调，我们提出的语音翻译模型显著提高了翻译质量，最高提升了+5.9 BLEU。",
        "领域": "语音翻译、多模态学习、自然语言处理与语音结合",
        "问题": "现有方法无法学习语音和文本的统一表征，且无法充分利用大规模文本和语音数据。",
        "动机": "解决语音和文本统一表征学习的问题，并利用大规模数据提升语音翻译性能。",
        "方法": "提出融合声学与文本的掩码语言模型（FAT-MLM）和端到端的融合声学与文本语音翻译模型（FAT-ST）。",
        "关键词": [
            "语音翻译",
            "多模态学习",
            "掩码语言模型",
            "端到端模型",
            "表征学习"
        ],
        "涉及的技术概念": {
            "融合声学与文本的掩码语言模型（FAT-MLM）": "联合学习声学和文本输入的统一表征，从各种类型语料库中学习。",
            "端到端模型（FAT-ST）": "在跨模态表征学习框架内提出的语音翻译模型，通过微调FAT-MLM提升翻译质量。",
            "表征学习": "通过学习输入数据的表征来改进与语言相关的任务，本文中用于统一语音和文本的表征。"
        },
        "success": true
    },
    {
        "order": 417,
        "title": "GANMEX: One-vs-One Attributions using GAN-based Model Explainability",
        "html": "https://ICML.cc//virtual/2021/poster/9015",
        "abstract": "Attribution methods have been shown as promising approaches for identifying key features that led to learned model predictions. While most existing attribution methods rely on a baseline input for performing feature perturbations, limited research has been conducted to address the baseline selection issues. Poor choices of baselines limit the ability of one-vs-one explanations for multi-class classifiers, which means the attribution methods were not able to explain why an input belongs to its original class but not the other specified target class. Achieving one-vs-one explanation is crucial when certain classes are more similar than others, e.g. two bird types among multiple animals, by focusing on key differentiating features rather than shared features across classes. In this paper, we present GANMEX, a novel approach applying Generative Adversarial Networks (GAN) by incorporating the to-be-explained classifier as part of the adversarial networks. Our approach effectively selects the baseline as the closest realistic sample belong to the target class, which allows attribution methods to provide true one-vs-one explanations. We showed that GANMEX baselines improved the saliency maps and led to stronger performance on multiple evaluation metrics over the existing baselines. Existing attribution results are known for being insensitive to model randomization, and we demonstrated that GANMEX baselines led to better outcome under the cascading randomization of the model. ",
        "conference": "ICML",
        "中文标题": "GANMEX：基于GAN模型可解释性的一对一归因",
        "摘要翻译": "归因方法已被证明是识别导致学习模型预测的关键特征的有前途的方法。虽然大多数现有的归因方法依赖于基线输入进行特征扰动，但针对基线选择问题的研究有限。基线的选择不当限制了一对一解释对于多类分类器的能力，这意味着归因方法无法解释为什么一个输入属于其原始类别而不是其他指定的目标类别。当某些类比其他类更相似时，例如多种动物中的两种鸟类，通过关注关键区分特征而非类别间共享特征，实现一对一解释至关重要。在本文中，我们提出了GANMEX，一种新颖的方法，通过将待解释的分类器作为对抗网络的一部分，应用生成对抗网络（GAN）。我们的方法有效地选择属于目标类的最接近的现实样本作为基线，这使得归因方法能够提供真正的一对一解释。我们展示了GANMEX基线改善了显著性图，并在多个评估指标上比现有基线带来了更强的性能。已知现有的归因结果对模型随机化不敏感，我们证明了在模型的级联随机化下，GANMEX基线带来了更好的结果。",
        "领域": "模型可解释性、生成对抗网络、多类分类",
        "问题": "解决多类分类器中一对一解释的基线选择问题",
        "动机": "提高归因方法在多类分类器中一对一解释的能力，特别是在相似类别之间区分关键特征",
        "方法": "应用生成对抗网络（GAN），将待解释的分类器作为对抗网络的一部分，选择最接近目标类的现实样本作为基线",
        "关键词": [
            "GANMEX",
            "模型可解释性",
            "一对一归因",
            "生成对抗网络",
            "多类分类"
        ],
        "涉及的技术概念": {
            "生成对抗网络（GAN）": "用于生成接近目标类的现实样本，作为归因方法的基线",
            "一对一归因": "专注于解释为什么输入属于其原始类别而不是其他指定的目标类别",
            "显著性图": "通过改善显著性图来展示归因方法的效果，提高模型解释的直观性"
        },
        "success": true
    },
    {
        "order": 418,
        "title": "Gaussian Process-Based Real-Time Learning for Safety Critical Applications",
        "html": "https://ICML.cc//virtual/2021/poster/10619",
        "abstract": "The safe operation of physical systems typically relies on high-quality models. Since a continuous stream of data is generated during run-time, such models are often obtained through the application of Gaussian process regression because it provides guarantees on the prediction error. Due to its high computational complexity, Gaussian process regression must be used offline on batches of data, which prevents applications, where a fast adaptation through online learning is necessary to ensure safety. In order to overcome this issue, we propose the LoG-GP.  It achieves a logarithmic update and prediction complexity in the number of training points through the aggregation of locally active Gaussian process models.  Under weak assumptions on the aggregation scheme, it inherits safety guarantees from exact Gaussian process regression.  These theoretical advantages are exemplarily exploited in the design of a safe and data-efficient, online-learning control policy. The efficiency and performance of the proposed real-time learning \napproach is demonstrated in a comparison to state-of-the-art methods.",
        "conference": "ICML",
        "中文标题": "基于高斯过程的实时学习在安全关键应用中的应用",
        "摘要翻译": "物理系统的安全运行通常依赖于高质量的模型。由于在运行时持续生成数据流，这类模型通常通过应用高斯过程回归获得，因为它提供了预测误差的保证。由于其高计算复杂度，高斯过程回归必须离线在数据批次上使用，这阻碍了那些需要通过在线学习快速适应以确保安全的应用。为了克服这个问题，我们提出了LoG-GP。它通过聚合局部活跃的高斯过程模型，实现了在训练点数上的对数更新和预测复杂度。在关于聚合方案的弱假设下，它继承了精确高斯过程回归的安全保证。这些理论优势在设计一个安全且数据高效的在线学习控制策略中得到了示例性利用。与最先进方法的比较中，展示了所提出的实时学习方法的效率和性能。",
        "领域": "机器学习安全应用、实时系统控制、高斯过程回归",
        "问题": "高斯过程回归在实时安全关键应用中因高计算复杂度而难以在线使用的问题",
        "动机": "为了解决高斯过程回归在需要快速在线学习以确保安全的应用中的使用限制",
        "方法": "提出LoG-GP方法，通过聚合局部活跃的高斯过程模型实现对数复杂度的更新和预测",
        "关键词": [
            "高斯过程回归",
            "实时学习",
            "安全关键应用",
            "在线学习",
            "控制策略"
        ],
        "涉及的技术概念": {
            "高斯过程回归": "用于提供预测误差保证的统计学习方法",
            "LoG-GP": "通过聚合局部活跃的高斯过程模型实现高效在线学习的方法",
            "在线学习控制策略": "利用LoG-GP方法设计的安全且数据高效的控制策略"
        },
        "success": true
    },
    {
        "order": 419,
        "title": "GBHT: Gradient Boosting Histogram Transform for Density Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/8481",
        "abstract": "In this paper, we propose a density estimation algorithm called \\textit{Gradient Boosting Histogram Transform} (GBHT), where we adopt the \\textit{Negative Log Likelihood} as the loss function to make the boosting procedure available for the unsupervised tasks. From a learning theory viewpoint, we first prove fast convergence rates for GBHT with the smoothness assumption that the underlying density function lies in the space $C^{0,\\alpha}$. Then when the target density function lies in spaces $C^{1,\\alpha}$, we present an upper bound for GBHT which is smaller than the lower bound of its corresponding base learner, in the sense of convergence rates. To the best of our knowledge, we make the first attempt to theoretically explain why boosting can enhance the performance of its base learners for density estimation problems. In experiments, we not only conduct performance comparisons with the widely used KDE, but also apply GBHT to anomaly detection to showcase a further application of GBHT.",
        "conference": "ICML",
        "success": true,
        "中文标题": "GBHT: 用于密度估计的梯度提升直方图变换",
        "摘要翻译": "在本文中，我们提出了一种称为梯度提升直方图变换（GBHT）的密度估计算法，其中我们采用负对数似然作为损失函数，使得提升过程可用于无监督任务。从学习理论的角度出发，我们首先证明了在平滑性假设下，即基础密度函数位于空间C^{0,α}中，GBHT具有快速收敛率。然后，当目标密度函数位于空间C^{1,α}中时，我们提出了GBHT的一个上界，该上界在收敛率的意义上小于其对应基础学习器的下界。据我们所知，我们首次尝试从理论上解释为什么提升可以增强其基础学习器在密度估计问题上的性能。在实验中，我们不仅与广泛使用的KDE进行了性能比较，还将GBHT应用于异常检测，以展示GBHT的进一步应用。",
        "领域": "密度估计, 异常检测, 无监督学习",
        "问题": "如何通过梯度提升方法提高密度估计的准确性和效率",
        "动机": "探索提升技术在无监督密度估计任务中的应用潜力，并理论验证其相对于基础学习器的优势",
        "方法": "采用负对数似然作为损失函数的梯度提升直方图变换方法，理论分析其在特定函数空间中的收敛性能，并通过实验验证其效果",
        "关键词": [
            "梯度提升",
            "直方图变换",
            "密度估计",
            "无监督学习",
            "异常检测"
        ],
        "涉及的技术概念": {
            "梯度提升": "一种集成学习方法，通过迭代地添加弱学习器来优化损失函数，用于提高模型的性能",
            "直方图变换": "一种将数据转换为直方图表示的方法，用于简化数据结构和加速计算",
            "负对数似然": "用作损失函数，衡量模型预测与真实数据分布之间的差异，用于无监督学习中的模型优化"
        }
    },
    {
        "order": 420,
        "title": "Generalised Lipschitz Regularisation Equals Distributional Robustness",
        "html": "https://ICML.cc//virtual/2021/poster/10611",
        "abstract": "The problem of adversarial examples has highlighted the need for a theory of regularisation that is general enough to apply to exotic function classes, such as universal approximators. In response, we have been able to significantly sharpen existing results regarding the relationship between distributional robustness and regularisation, when defined with a transportation cost uncertainty set. The theory allows us to characterise the conditions under which the distributional robustness equals a Lipschitz-regularised model, and to tightly quantify, for the first time, the slackness under very mild assumptions. As a theoretical application we show a new result explicating the connection between adversarial learning and distributional robustness. We then give new results for how to achieve Lipschitz regularisation of kernel classifiers, which are demonstrated experimentally.",
        "conference": "ICML",
        "中文标题": "广义Lipschitz正则化等同于分布鲁棒性",
        "摘要翻译": "对抗性示例的问题凸显了对正则化理论的需求，这一理论需要足够通用以适用于诸如通用逼近器等异质函数类。作为回应，我们能够显著锐化关于分布鲁棒性与正则化之间关系的现有结果，当使用运输成本不确定性集定义时。该理论使我们能够描述分布鲁棒性等同于Lipschitz正则化模型的条件，并首次在非常温和的假设下严格量化松弛度。作为一个理论应用，我们展示了一个新结果，阐明了对抗性学习与分布鲁棒性之间的联系。然后，我们给出了如何实现核分类器的Lipschitz正则化的新结果，并通过实验进行了验证。",
        "领域": "对抗性学习、核方法、正则化理论",
        "问题": "如何在异质函数类中实现有效的正则化，以及分布鲁棒性与正则化之间的精确关系",
        "动机": "对抗性示例的出现促使需要更通用的正则化理论，以适用于广泛的函数类，特别是通用逼近器",
        "方法": "通过运输成本不确定性集定义分布鲁棒性与正则化的关系，理论分析Lipschitz正则化与分布鲁棒性的等价条件，并实验验证核分类器的Lipschitz正则化方法",
        "关键词": [
            "对抗性学习",
            "分布鲁棒性",
            "Lipschitz正则化",
            "核分类器",
            "运输成本"
        ],
        "涉及的技术概念": {
            "Lipschitz正则化": "用于确保模型在输入微小变化时输出变化有限的正则化技术，提高模型对抗性",
            "分布鲁棒性": "模型在面对输入数据分布变化时保持性能的能力，与正则化密切相关",
            "运输成本不确定性集": "用于定义分布鲁棒性中的不确定性，通过运输成本度量不同分布之间的距离"
        },
        "success": true
    },
    {
        "order": 421,
        "title": "Generalizable Episodic Memory for Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9885",
        "abstract": "Episodic memory-based methods can rapidly latch onto past successful strategies by a non-parametric memory and improve sample efficiency of traditional reinforcement learning. However, little effort is put into the continuous domain, where a state is never visited twice, and previous episodic methods fail to efficiently aggregate experience across trajectories. To address this problem, we propose Generalizable Episodic Memory (GEM), which effectively organizes the state-action values of episodic memory in a generalizable manner and supports implicit planning on memorized trajectories. GEM utilizes a double estimator to reduce the overestimation bias induced by value propagation in the planning process. Empirical evaluation shows that our method significantly outperforms existing trajectory-based methods on various MuJoCo continuous control tasks. To further show the general applicability, we evaluate our method on Atari games with discrete action space, which also shows a significant improvement over baseline algorithms.",
        "conference": "ICML",
        "中文标题": "可泛化的情景记忆用于深度强化学习",
        "摘要翻译": "基于情景记忆的方法可以通过非参数记忆快速锁定过去的成功策略，并提高传统强化学习的样本效率。然而，在连续领域中，一个状态永远不会被访问两次，之前的情景记忆方法未能有效地跨轨迹聚合经验。为了解决这个问题，我们提出了可泛化的情景记忆（GEM），它以可泛化的方式有效组织情景记忆的状态-动作值，并支持对记忆轨迹的隐式规划。GEM利用双重估计器来减少规划过程中价值传播引起的高估偏差。实证评估表明，我们的方法在各种MuJoCo连续控制任务上显著优于现有的基于轨迹的方法。为了进一步展示其普遍适用性，我们在具有离散动作空间的Atari游戏上评估了我们的方法，结果显示相对于基线算法也有显著改进。",
        "领域": "深度强化学习、连续控制任务、离散动作空间游戏",
        "问题": "在连续领域中，情景记忆方法无法有效跨轨迹聚合经验的问题",
        "动机": "提高深度强化学习在连续领域和离散动作空间游戏中的样本效率和性能",
        "方法": "提出可泛化的情景记忆（GEM），通过双重估计器减少高估偏差，有效组织情景记忆的状态-动作值，支持隐式规划",
        "关键词": [
            "情景记忆",
            "深度强化学习",
            "连续控制",
            "双重估计器",
            "隐式规划"
        ],
        "涉及的技术概念": {
            "情景记忆": "用于快速锁定过去的成功策略，提高样本效率的非参数记忆",
            "双重估计器": "用于减少规划过程中价值传播引起的高估偏差的技术",
            "隐式规划": "支持对记忆轨迹的规划，无需显式地构建规划模型"
        },
        "success": true
    },
    {
        "order": 422,
        "title": "Generalization Bounds in the Presence of Outliers: a Median-of-Means Study",
        "html": "https://ICML.cc//virtual/2021/poster/8843",
        "abstract": "In contrast to the empirical mean, the Median-of-Means (MoM) is an estimator of the mean θ of a square integrable r.v. Z, around which accurate nonasymptotic confidence bounds can be built, even when Z does not exhibit a sub-Gaussian tail behavior. Thanks to the high confidence it achieves on heavy-tailed data, MoM has found various applications in machine learning, where it is used to design training procedures that are not sensitive to atypical observations. More recently, a new line of work is now trying to characterize and leverage MoM’s ability to deal with corrupted data. In this context, the present work proposes a general study of MoM’s concentration properties under the contamination regime, that provides a clear understanding on the impact of the outlier proportion and the number of blocks chosen. The analysis is extended to (multisample) U-statistics, i.e. averages over tuples of observations, that raise additional challenges due to the dependence induced. Finally, we show that the latter bounds can be used in a straightforward fashion to derive generalization guarantees for pairwise learning in a contaminated setting, and propose an algorithm to compute provably reliable decision functions.",
        "conference": "ICML",
        "中文标题": "异常值存在下的泛化界限：中位数均值研究",
        "摘要翻译": "与经验均值相比，中位数均值（MoM）是一种用于估计平方可积随机变量Z的均值θ的估计器，即使在Z不表现出亚高斯尾部行为时，也能围绕其构建精确的非渐近置信界限。由于MoM在重尾数据上实现的高置信度，它已在机器学习中找到多种应用，用于设计对非典型观察不敏感的训练程序。最近，一系列新工作正试图描述并利用MoM处理被污染数据的能力。在此背景下，本研究提出了在污染机制下MoM集中性质的一般研究，提供了关于异常值比例和所选块数影响的清晰理解。分析扩展到（多样本）U统计量，即对观察元组的平均，由于诱导的依赖性而带来额外的挑战。最后，我们展示了后者界限可以以直接的方式用于在污染环境中为成对学习推导泛化保证，并提出了一种算法来计算可证明可靠的决策函数。",
        "领域": "统计学习理论、鲁棒机器学习、异常检测",
        "问题": "研究在数据中存在异常值的情况下，如何利用中位数均值（MoM）估计器提供可靠的泛化界限。",
        "动机": "探索MoM在处理被污染数据时的能力，特别是在机器学习中设计对异常值不敏感的训练程序。",
        "方法": "在污染机制下对MoM的集中性质进行一般研究，分析扩展到多样本U统计量，并提出算法计算可靠的决策函数。",
        "关键词": [
            "中位数均值",
            "泛化界限",
            "鲁棒学习",
            "U统计量",
            "异常值"
        ],
        "涉及的技术概念": {
            "中位数均值（MoM）": "一种用于估计均值的鲁棒估计器，能够在数据存在异常值时提供可靠的置信界限。",
            "U统计量": "对观察元组的平均，用于扩展分析到多样本情况，尽管存在依赖性挑战。",
            "泛化保证": "在污染环境中为成对学习提供的理论保证，确保学习算法的可靠性。"
        },
        "success": true
    },
    {
        "order": 423,
        "title": "Generalization Error Bound for Hyperbolic Ordinal Embedding",
        "html": "https://ICML.cc//virtual/2021/poster/9477",
        "abstract": "Hyperbolic ordinal embedding (HOE) represents entities as points in hyperbolic space so that they agree as well as possible with given constraints in the form of entity $i$ is more similar to entity $j$ than to entity $k$. It has been experimentally shown that HOE can obtain representations of hierarchical data such as a knowledge base and a citation network effectively, owing to hyperbolic space's exponential growth property. However, its theoretical analysis has been limited to ideal noiseless settings, and its generalization error in compensation for hyperbolic space's exponential representation ability has not been guaranteed. The difficulty is that existing generalization error bound derivations for ordinal embedding based on the Gramian matrix are not applicable in HOE, since hyperbolic space is not inner-product space. In this paper, through our novel characterization of HOE with decomposed Lorentz Gramian matrices, we provide a generalization error bound of HOE for the first time, which is at most exponential with respect to the embedding space's radius. Our comparison between the bounds of HOE and Euclidean ordinal embedding shows that HOE's generalization error comes at a reasonable cost considering its exponential representation ability.",
        "conference": "ICML",
        "success": true,
        "中文标题": "双曲序嵌入的泛化误差界",
        "摘要翻译": "双曲序嵌入（HOE）将实体表示为双曲空间中的点，以便它们尽可能与给定的约束条件一致，这些约束条件的形式为实体$i$比实体$k$更类似于实体$j$。实验表明，由于双曲空间的指数增长特性，HOE可以有效地表示层次数据，如知识库和引用网络。然而，其理论分析仅限于理想的无噪声设置，并且其在补偿双曲空间指数表示能力方面的泛化误差尚未得到保证。困难在于，现有的基于Gramian矩阵的序嵌入泛化误差界推导在HOE中不适用，因为双曲空间不是内积空间。在本文中，通过我们对HOE与分解的Lorentz Gramian矩阵的新颖表征，我们首次提供了HOE的泛化误差界，该误差界最多相对于嵌入空间的半径呈指数增长。我们对HOE和欧几里得序嵌入的误差界进行比较，表明HOE的泛化误差在考虑其指数表示能力时是合理的代价。",
        "领域": "知识表示学习, 网络嵌入, 层次数据建模",
        "问题": "双曲序嵌入在无噪声理想条件下的理论分析有限，且其泛化误差在补偿双曲空间指数表示能力方面未得到保证。",
        "动机": "解决双曲序嵌入在理论分析上的不足，特别是在泛化误差界的推导上，以支持其在层次数据表示中的应用。",
        "方法": "通过引入分解的Lorentz Gramian矩阵对双曲序嵌入进行新颖表征，首次推导出其泛化误差界。",
        "关键词": [
            "双曲序嵌入",
            "泛化误差界",
            "Lorentz Gramian矩阵",
            "层次数据",
            "知识表示"
        ],
        "涉及的技术概念": {
            "双曲序嵌入": "将实体表示为双曲空间中的点，以匹配给定的相似性约束，适用于层次数据的表示。",
            "泛化误差界": "衡量模型在未见数据上表现的理论界限，本文首次为双曲序嵌入推导出这一界限。",
            "Lorentz Gramian矩阵": "用于表征双曲空间中的点间关系，支持双曲序嵌入泛化误差界的推导。"
        }
    },
    {
        "order": 424,
        "title": "Generalization Guarantees for Neural Architecture Search with Train-Validation Split",
        "html": "https://ICML.cc//virtual/2021/poster/10005",
        "abstract": "Neural Architecture Search (NAS) is a popular method for automatically designing optimized deep-learning architectures. NAS methods commonly use bilevel optimization where one optimizes the weights over the training data (lower-level problem) and hyperparameters - such as the architecture - over the validation data (upper-level problem). This paper explores the statistical aspects of such problems with train-validation splits. In practice, the lower-level problem is often overparameterized and can easily achieve zero loss. Thus, a-priori, it seems impossible to distinguish the right hyperparameters based on training loss alone which motivates a better understanding of  train-validation split. To this aim, we first show that refined properties of the validation loss such as risk and hyper-gradients are indicative of those of the true test loss and help prevent overfitting with a near-minimal validation sample size. Importantly, this is established for continuous search spaces which are relevant for differentiable search schemes. We then establish generalization bounds for NAS problems with an emphasis on an activation search problem and gradient-based methods. Finally, we show rigorous connections between NAS and low-rank matrix learning which leads to algorithmic insights where the solution of the upper problem can be accurately learned via spectral methods to achieve near-minimal risk.",
        "conference": "ICML",
        "中文标题": "神经架构搜索中训练-验证分割的泛化保证",
        "摘要翻译": "神经架构搜索（NAS）是一种自动设计优化深度学习架构的流行方法。NAS方法通常使用双层优化，其中在训练数据上优化权重（下层问题），在验证数据上优化超参数——如架构（上层问题）。本文探讨了这种带有训练-验证分割问题的统计方面。在实践中，下层问题往往是过参数化的，可以轻松达到零损失。因此，仅基于训练损失似乎无法先验地区分正确的超参数，这促使了对训练-验证分割的更好理解。为此，我们首先展示了验证损失的精细属性，如风险和超梯度，能够指示真实测试损失的属性，并有助于防止过拟合，且验证样本量接近最小。重要的是，这是针对与可微分搜索方案相关的连续搜索空间建立的。然后，我们为NAS问题建立了泛化边界，重点在于激活搜索问题和基于梯度的方法。最后，我们展示了NAS与低秩矩阵学习之间的严格联系，这导致了算法上的见解，即上层问题的解可以通过谱方法准确学习，以实现接近最小的风险。",
        "领域": "神经架构搜索、深度学习优化、自动机器学习",
        "问题": "探讨神经架构搜索中训练-验证分割的统计特性及其对泛化能力的影响",
        "动机": "理解训练-验证分割在神经架构搜索中的作用，以防止过拟合并提高模型的泛化能力",
        "方法": "通过分析验证损失的风险和超梯度，建立与测试损失的关联，提出针对连续搜索空间的泛化边界，并探索NAS与低秩矩阵学习的联系",
        "关键词": [
            "神经架构搜索",
            "泛化保证",
            "训练-验证分割",
            "双层优化",
            "低秩矩阵学习"
        ],
        "涉及的技术概念": {
            "双层优化": "在神经架构搜索中同时优化模型权重和超参数的方法",
            "泛化边界": "用于评估模型在未见数据上表现的统计保证",
            "谱方法": "用于解决上层优化问题的技术，通过学习低秩矩阵来实现接近最小的风险"
        },
        "success": true
    },
    {
        "order": 425,
        "title": "Generalized Doubly Reparameterized Gradient Estimators",
        "html": "https://ICML.cc//virtual/2021/poster/10709",
        "abstract": "Efficient low-variance gradient estimation enabled by the reparameterization trick (RT) has been essential to the success of variational autoencoders. Doubly-reparameterized gradients (DReGs) improve on the RT for multi-sample variational bounds by applying reparameterization a second time for an additional reduction in variance. Here, we develop two generalizations of the DReGs estimator and show that they can be used to train conditional and hierarchical VAEs on image modelling tasks more effectively. We first extend the estimator to hierarchical models with several stochastic layers by showing how to treat additional score function terms due to the hierarchical variational posterior. We then generalize DReGs to score functions of arbitrary distributions instead of just those of the sampling distribution, which makes the estimator applicable to the parameters of the prior in addition to those of the posterior.",
        "conference": "ICML",
        "中文标题": "广义双重重参数化梯度估计器",
        "摘要翻译": "通过重参数化技巧（RT）实现的高效低方差梯度估计对变分自编码器的成功至关重要。双重重参数化梯度（DReGs）通过在多样本变分边界上第二次应用重参数化来进一步减少方差，从而改进了RT。在此，我们开发了DReGs估计器的两种广义形式，并展示了它们可以更有效地用于训练条件性和层次性VAE在图像建模任务上。我们首先通过展示如何处理由于层次变分后验引起的额外得分函数项，将估计器扩展到具有多个随机层的层次模型。然后，我们将DReGs推广到任意分布的得分函数，而不仅仅是采样分布的得分函数，这使得估计器除了适用于后验参数外，还适用于先验参数。",
        "领域": "变分自编码器、图像建模、深度学习优化",
        "问题": "提高变分自编码器在图像建模任务中的训练效率和效果",
        "动机": "为了进一步减少梯度估计的方差，提高变分自编码器在复杂模型（如条件性和层次性VAE）中的训练效率和效果",
        "方法": "开发了双重重参数化梯度（DReGs）估计器的两种广义形式，分别针对层次模型和任意分布的得分函数",
        "关键词": [
            "双重重参数化梯度",
            "变分自编码器",
            "图像建模",
            "梯度估计",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "重参数化技巧（RT）": "一种通过引入外部噪声源来重写期望梯度的方法，使得梯度估计的方差降低",
            "双重重参数化梯度（DReGs）": "在RT基础上，对多样本变分边界第二次应用重参数化，进一步减少梯度估计的方差",
            "层次变分后验": "在层次模型中，变分后验分布本身也具有层次结构，增加了模型的表达能力但同时也带来了额外的计算挑战"
        },
        "success": true
    },
    {
        "order": 426,
        "title": "Generating images with sparse representations",
        "html": "https://ICML.cc//virtual/2021/poster/8791",
        "abstract": "The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. ",
        "conference": "ICML",
        "中文标题": "使用稀疏表示生成图像",
        "摘要翻译": "图像的高维性为基于似然的生成模型带来了架构和采样效率上的挑战。以往的方法如VQ-VAE使用深度自动编码器来获得紧凑的表示，这些表示作为基于似然模型的输入更为实用。我们提出了一种替代方法，受到JPEG等常见图像压缩方法的启发，将图像转换为量化的离散余弦变换（DCT）块，这些块被稀疏地表示为DCT通道、空间位置和DCT系数三元组的序列。我们提出了一种基于Transformer的自回归架构，该架构被训练来顺序预测此类序列中下一个元素的条件分布，并且能够有效地扩展到高分辨率图像。在一系列图像数据集上，我们证明了我们的方法可以生成高质量、多样化的图像，其样本度量分数与最先进的方法相竞争。此外，我们还展示了简单修改我们的方法可以产生有效的图像着色和超分辨率模型。",
        "领域": "图像生成、图像压缩、自回归模型",
        "问题": "解决高维图像在基于似然的生成模型中的架构和采样效率挑战",
        "动机": "探索一种更高效的图像表示方法，以提高生成模型的性能和效率",
        "方法": "采用基于离散余弦变换（DCT）的稀疏表示和Transformer自回归架构来生成图像",
        "关键词": [
            "稀疏表示",
            "图像生成",
            "Transformer",
            "离散余弦变换",
            "自回归模型"
        ],
        "涉及的技术概念": {
            "离散余弦变换（DCT）": "用于将图像转换为稀疏表示的量化块，减少数据的维度",
            "Transformer自回归架构": "用于顺序预测图像序列中下一个元素的条件分布，支持高分辨率图像生成",
            "稀疏表示": "通过减少表示中的非零元素数量来提高数据处理的效率和效果"
        },
        "success": true
    },
    {
        "order": 427,
        "title": "Generative Adversarial Networks for Markovian Temporal Dynamics: Stochastic Continuous Data Generation",
        "html": "https://ICML.cc//virtual/2021/poster/10659",
        "abstract": "In this paper, we present a novel generative adversarial network (GAN) that can describe Markovian temporal dynamics. To generate stochastic sequential data, we introduce a novel stochastic differential equation-based conditional generator and spatial-temporal constrained discriminator networks. To stabilize the learning dynamics of the min-max type of the GAN objective function, we propose well-posed constraint terms for both networks. We also propose a novel conditional Markov Wasserstein distance to induce a pathwise Wasserstein distance. The experimental results demonstrate that our method outperforms state-of-the-art methods using several different types of data. ",
        "conference": "ICML",
        "中文标题": "用于马尔可夫时间动态的生成对抗网络：随机连续数据生成",
        "摘要翻译": "在本文中，我们提出了一种新颖的生成对抗网络（GAN），能够描述马尔可夫时间动态。为了生成随机序列数据，我们引入了一种基于随机微分方程的新型条件生成器和时空约束判别器网络。为了稳定GAN目标函数的最小-最大类型的学习动态，我们为两个网络提出了适定的约束项。我们还提出了一种新的条件马尔可夫Wasserstein距离，以诱导路径Wasserstein距离。实验结果表明，我们的方法在使用几种不同类型的数据时优于最先进的方法。",
        "领域": "生成对抗网络、时间序列分析、随机过程建模",
        "问题": "如何生成具有马尔可夫时间动态的随机连续数据",
        "动机": "为了解决现有方法在生成随机序列数据时无法有效描述马尔可夫时间动态的问题",
        "方法": "提出了一种基于随机微分方程的条件生成器和时空约束判别器网络，以及适定的约束项和条件马尔可夫Wasserstein距离",
        "关键词": [
            "生成对抗网络",
            "马尔可夫动态",
            "随机微分方程",
            "Wasserstein距离",
            "时空约束"
        ],
        "涉及的技术概念": {
            "随机微分方程": "用于构建条件生成器，以生成具有时间动态的随机序列数据",
            "时空约束判别器网络": "用于评估生成数据的时空一致性，确保生成数据的时间动态符合马尔可夫性质",
            "条件马尔可夫Wasserstein距离": "用于衡量生成数据与真实数据在路径空间上的距离，优化生成过程"
        },
        "success": true
    },
    {
        "order": 428,
        "title": "Generative Adversarial Transformers",
        "html": "https://ICML.cc//virtual/2021/poster/8997",
        "abstract": "We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linear efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at https://github.com/dorarad/gansformer.",
        "conference": "ICML",
        "中文标题": "生成对抗变换器",
        "摘要翻译": "我们介绍了GANsformer，一种新颖且高效的变换器类型，并探索其在视觉生成建模任务中的应用。该网络采用了一种二分结构，能够在图像上实现长距离交互，同时保持线性计算效率，可以轻松扩展到高分辨率合成。它迭代地将信息从一组潜在变量传播到演变的视觉特征，反之亦然，以支持在相互启发下对每一方进行细化，并鼓励对象和场景的组合表示的出现。与经典的变换器架构相比，它利用了乘法集成，允许基于区域的灵活调制，因此可以被视为成功的StyleGAN网络的泛化。我们通过对一系列数据集的仔细评估，从模拟的多对象环境到丰富的现实世界室内外场景，展示了模型的强度和鲁棒性，显示其在图像质量和多样性方面达到了最先进的结果，同时享有快速学习和更好的数据效率。进一步的定性和定量实验为我们提供了对模型内部工作的洞察，揭示了改进的可解释性和更强的解缠性，并说明了我们方法的益处和有效性。模型的实现可在https://github.com/dorarad/gansformer找到。",
        "领域": "生成对抗网络、图像合成、视觉生成建模",
        "问题": "如何在保持计算效率的同时，实现高分辨率的视觉生成建模，并提升生成图像的质量和多样性。",
        "动机": "探索一种新型变换器结构，以解决现有视觉生成模型在长距离交互、计算效率和图像质量方面的限制。",
        "方法": "采用二分结构的变换器网络，通过迭代传播潜在变量与视觉特征之间的信息，实现高效的视觉生成建模。",
        "关键词": [
            "GANsformer",
            "视觉生成建模",
            "高分辨率合成",
            "乘法集成",
            "解缠性"
        ],
        "涉及的技术概念": {
            "GANsformer": "一种新型变换器，用于高效的视觉生成建模，支持长距离交互和线性计算效率。",
            "二分结构": "网络架构设计，用于实现潜在变量与视觉特征之间的有效信息交换。",
            "乘法集成": "允许基于区域的灵活调制，是StyleGAN网络的泛化，提升生成图像的质量和多样性。"
        },
        "success": true
    },
    {
        "order": 429,
        "title": "Generative Causal Explanations for Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8417",
        "abstract": "This paper presents {\\em Gem}, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, {\\em Gem} explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, {\\em Gem}, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of {\\em additive feature attribution methods}. Experimental results on synthetic and real-world datasets show that {\\em Gem} achieves a relative increase of the explanation accuracy by up to $30\\%$ and speeds up the explanation process by up to $110\\times$ as compared to its state-of-the-art alternatives. \n",
        "conference": "ICML",
        "success": true,
        "中文标题": "图神经网络的生成因果解释",
        "摘要翻译": "本文提出了Gem，一种模型无关的方法，用于为各种图学习任务中的任何图神经网络（GNNs）提供可解释的解释。具体来说，我们将为GNNs的决策提供解释的问题表述为一个因果学习任务。然后，我们训练一个因果解释模型，该模型配备了基于Granger因果关系的损失函数。与现有的GNNs解释器不同，Gem从因果角度解释图结构数据上的GNNs。由于它对GNNs的内部结构或图学习任务的先验知识没有要求，因此具有更好的泛化能力。此外，Gem一旦训练完成，就可以非常快速地用于解释目标GNN。我们的理论分析表明，几个最近的解释器属于一个统一的框架，即加性特征归因方法。在合成和真实世界数据集上的实验结果表明，与最先进的替代方案相比，Gem的解释准确性相对提高了高达30%，并且解释过程加速了高达110倍。",
        "领域": "图神经网络解释性、因果学习、图结构数据分析",
        "问题": "为图神经网络（GNNs）的决策提供可解释的解释",
        "动机": "提高GNNs解释的泛化能力和解释速度，同时不依赖于GNNs的内部结构或图学习任务的先验知识",
        "方法": "训练一个基于Granger因果关系的因果解释模型，从因果角度解释GNNs的决策",
        "关键词": [
            "图神经网络",
            "因果解释",
            "Granger因果关系",
            "解释性",
            "图学习"
        ],
        "涉及的技术概念": {
            "Granger因果关系": "用于构建因果解释模型的损失函数，帮助模型学习输入特征与输出之间的因果关系",
            "加性特征归因方法": "一个统一的解释框架，Gem的理论分析表明多个现有解释器属于此框架",
            "模型无关的解释方法": "Gem不依赖于特定GNNs的内部结构，能够为任何GNNs提供解释"
        }
    },
    {
        "order": 430,
        "title": "Generative Particle Variational Inference via Estimation of Functional Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/9249",
        "abstract": "Recently, particle-based variational inference (ParVI) methods have gained interest because they can avoid arbitrary parametric assumptions that are common in variational inference. However, many ParVI approaches do not allow arbitrary sampling from the posterior, and the few that do allow such sampling suffer from suboptimality. This work proposes a new method for learning to approximately sample from the posterior distribution. We construct a neural sampler that is trained with the functional gradient of the KL-divergence between the empirical sampling distribution and the target distribution, assuming the gradient resides within a reproducing kernel Hilbert space. Our generative ParVI (GPVI) approach maintains the asymptotic performance of ParVI methods while offering the flexibility of a generative sampler. Through carefully constructed experiments, we show that GPVI outperforms previous generative ParVI methods such as amortized SVGD, and is competitive with ParVI as well as gold-standard approaches like Hamiltonian Monte Carlo for fitting both exactly known and intractable target distributions. ",
        "conference": "ICML",
        "中文标题": "通过功能梯度估计的生成粒子变分推断",
        "摘要翻译": "最近，基于粒子的变分推断（ParVI）方法因其能够避免变分推断中常见的任意参数假设而受到关注。然而，许多ParVI方法不允许从后验中任意采样，而少数允许此类采样的方法又存在次优问题。本研究提出了一种新的方法，用于学习近似从后验分布中采样。我们构建了一个神经采样器，该采样器通过经验采样分布与目标分布之间KL散度的功能梯度进行训练，假设该梯度存在于再生核希尔伯特空间中。我们的生成ParVI（GPVI）方法在保持ParVI方法渐近性能的同时，提供了生成采样器的灵活性。通过精心设计的实验，我们展示了GPVI在拟合已知和难以处理的目标分布方面，优于先前的生成ParVI方法（如摊销SVGD），并且与ParVI以及金标准方法（如哈密顿蒙特卡洛）相竞争。",
        "领域": "变分推断、生成模型、贝叶斯统计",
        "问题": "解决基于粒子的变分推断方法中不允许任意采样或采样效率低下的问题",
        "动机": "开发一种既能保持变分推断方法渐近性能，又能提供灵活采样能力的新方法",
        "方法": "构建一个通过KL散度功能梯度训练的神经采样器，假设梯度存在于再生核希尔伯特空间中",
        "关键词": [
            "生成粒子变分推断",
            "功能梯度",
            "神经采样器",
            "KL散度",
            "再生核希尔伯特空间"
        ],
        "涉及的技术概念": {
            "粒子变分推断（ParVI）": "一种避免任意参数假设的变分推断方法",
            "KL散度": "用于衡量经验采样分布与目标分布之间的差异",
            "再生核希尔伯特空间": "提供了一种框架，用于定义和计算功能梯度"
        },
        "success": true
    },
    {
        "order": 431,
        "title": "Generative Video Transformer: Can Objects be the Words?",
        "html": "https://ICML.cc//virtual/2021/poster/9969",
        "abstract": "Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the ObjectCentric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also signiﬁcantly more memory-efﬁcient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.",
        "conference": "ICML",
        "中文标题": "生成式视频Transformer：物体能否作为词汇？",
        "摘要翻译": "Transformer在许多自然语言处理任务中取得了成功。然而，由于高计算复杂性和缺乏自然标记化，将Transformer应用于视频领域以进行长期视频生成和场景理解等任务仍然难以实现。在本文中，我们提出了以物体为中心的Video Transformer（OCVT），它利用以物体为中心的方法将场景分解为适合生成式视频Transformer使用的标记。通过将视频分解为物体，我们的完全无监督模型能够学习场景中多个交互物体的复杂时空动态，并生成视频的未来帧。我们的模型也比基于像素的模型显著更高效，因此能够在单个48GB GPU上训练长达70帧的视频。我们将我们的模型与之前基于RNN的方法以及其他可能的视频Transformer基线进行了比较。我们证明了OCVT在生成未来帧方面与基线相比表现良好。OCVT还为视频推理开发了有用的表示，在CATER任务上实现了最先进的性能。",
        "领域": "视频生成、场景理解、时空动态建模",
        "问题": "如何有效地将Transformer应用于视频领域，以解决长期视频生成和场景理解的高计算复杂性和缺乏自然标记化的问题。",
        "动机": "探索以物体为中心的方法来分解视频场景，以便于Transformer模型处理，从而学习复杂时空动态并生成未来视频帧。",
        "方法": "提出ObjectCentric Video Transformer（OCVT），采用以物体为中心的分解方法，将视频场景分解为适合Transformer处理的标记，实现无监督学习复杂时空动态和未来帧生成。",
        "关键词": [
            "视频生成",
            "Transformer",
            "物体中心",
            "时空动态",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "以物体为中心的分解": "将视频场景分解为物体标记，便于Transformer模型处理和学习。",
            "无监督学习": "模型无需标注数据即可学习视频中物体的复杂时空动态。",
            "时空动态建模": "模型能够理解和预测视频中物体随时间的空间变化和交互。"
        },
        "success": true
    },
    {
        "order": 432,
        "title": "GeomCA: Geometric Evaluation of Data Representations",
        "html": "https://ICML.cc//virtual/2021/poster/9663",
        "abstract": "Evaluating the quality of learned representations without relying on a downstream task remains one of the challenges in representation learning. In this work, we present Geometric Component Analysis (GeomCA) algorithm that evaluates representation spaces based on their geometric and topological properties. GeomCA can be applied to representations of any dimension, independently of the model that generated them. We demonstrate its applicability by analyzing representations obtained from a variety of scenarios, such as contrastive learning models, generative models and supervised learning models. ",
        "conference": "ICML",
        "中文标题": "GeomCA：数据表示的几何评估",
        "摘要翻译": "在不依赖下游任务的情况下评估学习表示的质量仍然是表示学习中的挑战之一。在这项工作中，我们提出了几何成分分析（GeomCA）算法，该算法基于表示空间的几何和拓扑属性来评估表示空间。GeomCA可以应用于任何维度的表示，独立于生成它们的模型。我们通过分析从对比学习模型、生成模型和监督学习模型等多种场景中获得的表示，展示了其适用性。",
        "领域": "表示学习、几何数据分析、拓扑数据分析",
        "问题": "如何在不依赖下游任务的情况下评估学习表示的质量",
        "动机": "解决表示学习中评估学习表示质量的挑战，提供一种独立于模型和维度的评估方法",
        "方法": "提出几何成分分析（GeomCA）算法，基于表示空间的几何和拓扑属性进行评估",
        "关键词": [
            "表示学习",
            "几何分析",
            "拓扑分析",
            "对比学习",
            "生成模型"
        ],
        "涉及的技术概念": {
            "几何成分分析（GeomCA）": "用于评估表示空间质量的算法，基于几何和拓扑属性",
            "表示学习": "学习数据的有用表示，以便于后续任务",
            "拓扑属性": "描述数据表示空间的结构特性，如连通性和维度"
        },
        "success": true
    },
    {
        "order": 433,
        "title": "Geometric convergence of elliptical slice sampling",
        "html": "https://ICML.cc//virtual/2021/poster/10663",
        "abstract": "For Bayesian learning, given likelihood function and Gaussian prior, the elliptical slice sampler, introduced by Murray, Adams and MacKay 2010, provides a tool for the construction of a Markov chain for approximate sampling of the underlying posterior distribution. Besides of its wide applicability and simplicity its main feature is that no tuning is necessary.  Under weak regularity assumptions on the posterior density we show that the corresponding Markov chain is geometrically ergodic and therefore yield qualitative convergence guarantees. We illustrate our result for Gaussian posteriors as they appear in Gaussian process regression in a  fully Gaussian scenario, which for example is exhibited in Gaussian process regression, as well as in a setting of a multi-modal distribution. Remarkably, our numerical experiments indicate a dimension-independent performance of  elliptical slice sampling even in situations where our ergodicity result does not apply.",
        "conference": "ICML",
        "中文标题": "椭圆切片采样的几何收敛",
        "摘要翻译": "对于贝叶斯学习，给定似然函数和高斯先验，Murray、Adams和MacKay于2010年引入的椭圆切片采样器，为构建马尔可夫链以近似采样底层后验分布提供了一个工具。除了其广泛的适用性和简单性之外，其主要特点是不需要进行调参。在后验密度满足弱正则性假设的条件下，我们证明了相应的马尔可夫链是几何遍历的，因此提供了定性收敛保证。我们在高斯过程回归中完全高斯场景下展示了我们的结果，例如在高斯过程回归中展示的，以及在多模态分布设置中的结果。值得注意的是，我们的数值实验表明，椭圆切片采样在即使我们的遍历性结果不适用的情况下，也表现出与维度无关的性能。",
        "领域": "贝叶斯学习, 高斯过程回归, 马尔可夫链蒙特卡洛方法",
        "问题": "如何在贝叶斯学习中高效且无需调参地近似采样后验分布",
        "动机": "为了解决在贝叶斯学习中近似采样后验分布时调参复杂和效率低下的问题",
        "方法": "使用椭圆切片采样器构建马尔可夫链，证明其在弱正则性假设下的几何遍历性",
        "关键词": [
            "椭圆切片采样",
            "几何遍历性",
            "高斯过程回归",
            "贝叶斯学习",
            "马尔可夫链"
        ],
        "涉及的技术概念": {
            "椭圆切片采样": "一种无需调参的马尔可夫链蒙特卡洛方法，用于在贝叶斯学习中近似采样后验分布",
            "几何遍历性": "马尔可夫链的一种收敛性质，保证了采样过程的效率和稳定性",
            "高斯过程回归": "一种基于高斯过程的非参数回归方法，用于建模和预测连续值输出"
        },
        "success": true
    },
    {
        "order": 434,
        "title": "Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances",
        "html": "https://ICML.cc//virtual/2021/poster/10331",
        "abstract": "We study how permutation symmetries in overparameterized multi-layer neural\nnetworks generate `symmetry-induced' critical points.\nAssuming a network with $ L $ layers of minimal widths $ r_1^*, \\ldots, r_{L-1}^* $ reaches a zero-loss minimum at $ r_1^*! \\cdots r_{L-1}^*! $ isolated points that are permutations of one another, \nwe show that adding one extra neuron to each layer is sufficient to connect all these previously discrete minima into a single manifold.\nFor a two-layer overparameterized network of width $ r^*+ h =: m $ we explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $ affine subspaces of dimension at least $ h $ that are connected to one another.\nFor a network of width $m$, we identify the number $G(r,m)$ of affine subspaces containing only symmetry-induced critical points that are related to the critical points of a smaller network of width $r<r^*$.\nVia a combinatorial analysis, we derive closed-form formulas for $ T $ and $ G $ and show that the number of symmetry-induced critical subspaces dominates the number of affine subspaces forming the global minima manifold in the mildly overparameterized regime (small $ h $) and vice versa in the vastly overparameterized regime ($h \\gg r^*$).\nOur results provide new insights into the minimization of the non-convex loss function of overparameterized neural networks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "过参数化神经网络中损失景观的几何：对称性和不变性",
        "摘要翻译": "我们研究了过参数化多层神经网络中的置换对称性如何产生“对称性诱导”的临界点。假设一个具有 L 层且最小宽度为 r_1^*, ..., r_{L-1}^* 的网络在 r_1^*! ... r_{L-1}^*! 个孤立点达到零损失最小值，这些孤立点是彼此的排列，我们表明，向每一层添加一个额外的神经元足以将所有这些先前离散的最小值连接成一个单一的流形。对于宽度为 r^*+ h =: m 的两层过参数化网络，我们明确地描述了全局最小值的流形：它由 T(r^*, m) 个维度至少为 h 的仿射子空间组成，这些子空间彼此连接。对于宽度为 m 的网络，我们确定了 G(r,m) 个仅包含对称性诱导临界点的仿射子空间的数量，这些临界点与宽度较小 r<r^* 的网络的临界点相关。通过组合分析，我们推导出 T 和 G 的闭式公式，并表明在轻微过参数化状态（小 h）下，对称性诱导的临界子空间的数量主导了形成全局最小值流形的仿射子空间的数量，而在严重过参数化状态（h >> r^*）下则相反。我们的结果为过参数化神经网络的非凸损失函数的最小化提供了新的见解。",
        "领域": "神经网络优化, 损失景观分析, 深度学习理论",
        "问题": "研究过参数化神经网络中损失函数的几何结构，特别是对称性如何影响临界点的分布和全局最小值的流形。",
        "动机": "理解过参数化神经网络的损失景观可以为优化算法的设计和模型的泛化能力提供理论指导。",
        "方法": "通过数学推导和组合分析，研究了网络中的置换对称性如何导致特定类型的临界点（对称性诱导的临界点）的产生，并分析了这些临界点与全局最小值流形之间的关系。推导出了关于仿射子空间数量的闭式公式，以量化对称性在损失景观中的影响。",
        "关键词": [
            "过参数化神经网络",
            "损失景观",
            "对称性",
            "临界点",
            "全局最小值"
        ],
        "涉及的技术概念": {
            "过参数化": "神经网络的参数数量远大于训练数据量，这种情况下模型通常能够很好地拟合训练数据，但也容易出现过拟合现象。",
            "损失景观": "损失函数在参数空间中的图形表示，包含了局部最小值、全局最小值和鞍点等关键特征，影响着优化算法的性能。"
        }
    },
    {
        "order": 435,
        "title": "Global Convergence of Policy Gradient for Linear-Quadratic Mean-Field Control/Game in Continuous Time",
        "html": "https://ICML.cc//virtual/2021/poster/8961",
        "abstract": "Recent years have witnessed the success of multi-agent reinforcement learning, which has motivated new research directions for mean-field control (MFC) and mean-field game (MFG), as the multi-agent system can be well approximated by a mean-field problem when the number of agents grows to be very large. In this paper, we study the policy gradient (PG) method for the linear-quadratic mean-field control and game, where we assume each agent has identical linear state transitions and quadratic cost functions. While most recent works on policy gradient for MFC and MFG are based on discrete-time models, we focus on a continuous-time model where some of our analyzing techniques could be valuable to the interested readers. For both the MFC and the MFG, we provide PG update and show that it converges to the optimal solution at a linear rate, which is verified by a synthetic simulation. For the MFG, we also provide sufficient conditions for the existence and uniqueness of the Nash equilibrium.",
        "conference": "ICML",
        "中文标题": "连续时间线性二次平均场控制/博弈中策略梯度的全局收敛性",
        "摘要翻译": "近年来，多智能体强化学习的成功为平均场控制（MFC）和平均场博弈（MFG）开辟了新的研究方向，因为当智能体数量变得非常大时，多智能体系统可以被平均场问题很好地近似。在本文中，我们研究了线性二次平均场控制和博弈的策略梯度（PG）方法，其中我们假设每个智能体具有相同的线性状态转移和二次成本函数。虽然最近关于MFC和MFG的策略梯度的工作大多基于离散时间模型，但我们关注的是一个连续时间模型，其中我们的一些分析技术可能对感兴趣的读者有价值。对于MFC和MFG，我们提供了PG更新，并表明它以线性速率收敛到最优解，这一点通过合成模拟得到了验证。对于MFG，我们还提供了纳什均衡存在和唯一性的充分条件。",
        "领域": "多智能体强化学习, 平均场控制, 平均场博弈",
        "问题": "研究在连续时间线性二次平均场控制和博弈中策略梯度方法的全局收敛性问题",
        "动机": "探索多智能体系统在智能体数量极大时，通过平均场问题近似的方法，以及策略梯度方法在此类问题中的有效性和收敛性",
        "方法": "采用策略梯度方法，研究其在连续时间线性二次平均场控制和博弈模型中的应用，并通过理论分析和合成模拟验证其收敛性",
        "关键词": [
            "策略梯度",
            "平均场控制",
            "平均场博弈",
            "线性二次模型",
            "连续时间"
        ],
        "涉及的技术概念": {
            "策略梯度": "用于优化智能体策略的方法，通过梯度上升来最小化成本函数",
            "平均场控制": "在多智能体系统中，通过平均场近似来简化问题，研究集中控制策略",
            "纳什均衡": "在博弈论中，指所有参与者策略的组合，使得没有任何一方能通过单方面改变策略而获得更好的结果"
        },
        "success": true
    },
    {
        "order": 436,
        "title": "Global inducing point variational posteriors for Bayesian neural networks and deep Gaussian processes",
        "html": "https://ICML.cc//virtual/2021/poster/9139",
        "abstract": "We consider the optimal approximate posterior over the top-layer weights in a Bayesian neural network for regression, and show that it exhibits strong dependencies on the lower-layer weights. We adapt this result to develop a correlated approximate posterior over the weights at all layers in a Bayesian neural network. We extend this approach to deep Gaussian processes, unifying inference in the two model classes. Our approximate posterior uses learned 'global'' inducing points, which are defined only at the input layer and propagated through the network to obtain inducing inputs at subsequent layers. By contrast, standard, 'local'', inducing point methods from the deep Gaussian process literature optimise a separate set of inducing inputs at every layer, and thus do not model correlations across layers. Our method gives state-of-the-art performance for a variational Bayesian method, without data augmentation or tempering, on CIFAR-10 of 86.7%, which is comparable to SGMCMC without tempering but with data augmentation (88% in Wenzel et al. 2020).",
        "conference": "ICML",
        "success": true,
        "中文标题": "贝叶斯神经网络和深度高斯过程的全局诱导点变分后验",
        "摘要翻译": "我们考虑了回归任务中贝叶斯神经网络顶层权重的最优近似后验，并展示了它与下层权重之间的强依赖性。我们利用这一结果开发了一个在贝叶斯神经网络所有层权重上的相关近似后验。我们将这一方法扩展到深度高斯过程，统一了这两类模型的推断。我们的近似后验使用了学习到的‘全局’诱导点，这些点仅在输入层定义，并通过网络传播以获得后续层的诱导输入。相比之下，深度高斯过程文献中的标准‘局部’诱导点方法在每一层优化一组独立的诱导输入，因此不建模跨层的相关性。我们的方法在不使用数据增强或温度调节的情况下，在CIFAR-10上达到了86.7%的最新性能，这与不使用温度调节但使用数据增强的SGMCMC相当（Wenzel等人2020年的88%）。",
        "领域": "贝叶斯深度学习, 深度高斯过程, 变分推断",
        "问题": "解决贝叶斯神经网络和深度高斯过程中权重后验分布的近似问题，特别是跨层权重的相关性建模。",
        "动机": "为了更准确地建模贝叶斯神经网络和深度高斯过程中各层权重之间的依赖关系，提高模型的推断性能。",
        "方法": "开发了一种使用全局诱导点的变分后验方法，通过在输入层定义诱导点并传播至后续层，以建模跨层权重的相关性。",
        "关键词": [
            "贝叶斯神经网络",
            "深度高斯过程",
            "变分推断",
            "全局诱导点",
            "相关性建模"
        ],
        "涉及的技术概念": {
            "全局诱导点": "在输入层定义并通过网络传播的诱导点，用于建模跨层权重的相关性。",
            "变分后验": "用于近似真实后验分布的方法，优化以最小化与真实后验的KL散度。",
            "深度高斯过程": "一种多层结构的非参数模型，每层都是一个高斯过程，用于复杂数据的建模。"
        }
    },
    {
        "order": 437,
        "title": "Globally-Robust Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9707",
        "abstract": "The threat of adversarial examples has motivated work on training certifiably robust neural networks to facilitate efficient verification of local robustness at inference time. We formalize a notion of global robustness, which captures the operational properties of on-line local robustness certification while yielding a natural learning objective for robust training. We show that widely-used architectures can be easily adapted to this objective by incorporating efficient global Lipschitz bounds into the network, yielding certifiably-robust models by construction that achieve state-of-the-art verifiable accuracy. Notably, this approach requires significantly less time and memory than recent certifiable training methods, and leads to negligible costs when certifying points on-line; for example, our evaluation shows that it is possible to train a large robust Tiny-Imagenet model in a matter of hours. Our models effectively leverage inexpensive global Lipschitz bounds for real-time certification, despite prior suggestions that tighter local bounds are needed for good performance; we posit this is possible because our models are specifically trained to achieve tighter global bounds. Namely, we prove that the maximum achievable verifiable accuracy for a given dataset is not improved by using a local bound.",
        "conference": "ICML",
        "中文标题": "全局鲁棒神经网络",
        "摘要翻译": "对抗样本的威胁促使了在训练可证明鲁棒的神经网络方面的工作，以便在推理时高效验证局部鲁棒性。我们形式化了一个全局鲁棒性的概念，它捕捉了在线局部鲁棒性认证的操作特性，同时为鲁棒训练提供了一个自然的学习目标。我们展示了广泛使用的架构可以通过将高效的全局Lipschitz界限纳入网络来轻松适应这一目标，从而通过构建实现最先进的可验证准确性的可证明鲁棒模型。值得注意的是，这种方法比最近的可证明训练方法需要显著更少的时间和内存，并且在在线认证点时导致可忽略的成本；例如，我们的评估显示，可以在几小时内训练一个大型鲁棒的Tiny-Imagenet模型。我们的模型有效地利用了廉价的全局Lipschitz界限进行实时认证，尽管之前有建议认为需要更严格的局部界限以获得良好性能；我们认为这是可能的，因为我们的模型专门训练以实现更严格的全局界限。即，我们证明了对于给定数据集，使用局部界限不会提高最大可达到的可验证准确性。",
        "领域": "对抗样本防御、神经网络鲁棒性、深度学习安全",
        "问题": "如何在神经网络中实现高效的全局鲁棒性认证，以防御对抗样本的攻击。",
        "动机": "对抗样本的存在威胁了神经网络的安全性，需要开发能够提供高效在线局部鲁棒性认证的方法。",
        "方法": "通过将高效的全局Lipschitz界限纳入神经网络架构，训练出可证明鲁棒的模型，实现实时认证。",
        "关键词": [
            "全局鲁棒性",
            "Lipschitz界限",
            "对抗样本防御",
            "可验证准确性",
            "神经网络安全"
        ],
        "涉及的技术概念": {
            "全局鲁棒性": "论文中形式化的概念，用于捕捉在线局部鲁棒性认证的操作特性，并作为鲁棒训练的学习目标。",
            "Lipschitz界限": "用于衡量神经网络对输入变化的敏感度，全局Lipschitz界限被纳入网络以实现高效的鲁棒性认证。",
            "可验证准确性": "指在对抗样本存在的情况下，模型能够保持其预测准确性的能力，论文中通过全局Lipschitz界限优化这一指标。"
        },
        "success": true
    },
    {
        "order": 438,
        "title": "Global Optimality Beyond Two Layers: Training Deep ReLU Networks via Convex Programs",
        "html": "https://ICML.cc//virtual/2021/poster/10037",
        "abstract": "Understanding the fundamental mechanism behind the success of deep neural networks is one of the key challenges in the modern machine learning literature. Despite numerous attempts, a solid theoretical analysis is yet to be developed. In this paper, we develop a novel unified framework to reveal a hidden regularization mechanism through the lens of convex optimization. We first show that the training of multiple three-layer ReLU sub-networks with weight decay regularization can be equivalently cast as a convex optimization problem in a higher dimensional space, where sparsity is enforced via a group $\\ell_1$-norm regularization. Consequently, ReLU networks can be interpreted as high dimensional feature selection methods. More importantly, we then prove that the equivalent convex problem can be globally optimized by a standard convex optimization solver with a polynomial-time complexity with respect to the number of samples and data dimension when the width of the network is fixed. Finally, we numerically validate our theoretical results via experiments involving both synthetic and real datasets.",
        "conference": "ICML",
        "success": true,
        "中文标题": "超越两层的全局最优性：通过凸规划训练深度ReLU网络",
        "摘要翻译": "理解深度神经网络成功背后的基本机制是现代机器学习文献中的关键挑战之一。尽管已经进行了大量的尝试，但尚未开发出可靠的理论分析。在本文中，我们开发了一个新的统一框架，通过凸优化的视角揭示一个隐藏的正则化机制。我们首先证明，具有权重衰减正则化的多个三层ReLU子网络的训练可以等效地转化为更高维空间中的凸优化问题，其中通过群组l1范数正则化来强制执行稀疏性。因此，ReLU网络可以被解释为高维特征选择方法。更重要的是，我们随后证明，当网络宽度固定时，等效的凸问题可以通过标准的凸优化求解器以多项式时间复杂度（相对于样本数量和数据维度）进行全局优化。最后，我们通过涉及合成数据集和真实数据集的实验，在数值上验证了我们的理论结果。",
        "领域": "神经网络优化, 凸优化, 理论分析",
        "问题": "缺乏对深度ReLU网络训练机制的理论理解和全局优化方法。",
        "动机": "揭示深度神经网络成功背后的基本机制，并提供可全局优化的深度网络训练方法。",
        "方法": "将深度ReLU网络的训练转化为高维空间中的凸优化问题，并利用群组l1范数正则化实现稀疏性，最后通过标准凸优化求解器进行全局优化。",
        "关键词": [
            "ReLU网络",
            "凸优化",
            "全局最优性",
            "正则化",
            "特征选择"
        ],
        "涉及的技术概念": {
            "ReLU激活函数": "一种非线性激活函数，在神经网络中引入非线性，提高模型表达能力。",
            "凸优化": "一种优化方法，寻找凸函数的最优解，保证找到全局最优解。"
        }
    },
    {
        "order": 439,
        "title": "Global Prosody Style Transfer Without Text Transcriptions",
        "html": "https://ICML.cc//virtual/2021/poster/9835",
        "abstract": "Prosody plays an important role in characterizing the style of a speaker or an emotion, but most non-parallel voice or emotion style transfer algorithms do not convert any prosody information. Two major components of prosody are pitch and rhythm.  Disentangling the prosody information, particularly the rhythm component, from the speech is challenging because it involves breaking the synchrony between the input speech and the disentangled speech representation.  As a result, most existing prosody style transfer algorithms would need to rely on some form of text transcriptions to identify the content information, which confines their application to high-resource languages only. Recently, SpeechSplit has made sizeable progress towards unsupervised prosody style transfer, but it is unable to extract high-level global prosody style in an unsupervised manner.  In this paper, we propose AutoPST, which can disentangle global prosody style from speech without relying on any text transcriptions.  AutoPST is an Autoencoder-based Prosody Style Transfer framework with a thorough rhythm removal module guided by the self-expressive representation learning. Experiments on different style transfer tasks show that AutoPST can effectively convert prosody that correctly reflects the styles of the target domains.",
        "conference": "ICML",
        "中文标题": "无需文本转录的全局韵律风格转换",
        "摘要翻译": "韵律在表征说话者或情感的风格中扮演着重要角色，但大多数非平行的声音或情感风格转换算法并未转换任何韵律信息。韵律的两个主要组成部分是音高和节奏。从语音中分离韵律信息，尤其是节奏成分，具有挑战性，因为它涉及打破输入语音与分离后的语音表示之间的同步性。因此，大多数现有的韵律风格转换算法需要依赖某种形式的文本转录来识别内容信息，这限制了它们仅适用于资源丰富的语言。最近，SpeechSplit在无监督韵律风格转换方面取得了显著进展，但它无法以无监督的方式提取高级全局韵律风格。在本文中，我们提出了AutoPST，它可以在不依赖任何文本转录的情况下从语音中分离全局韵律风格。AutoPST是一个基于自动编码器的韵律风格转换框架，具有由自表达表示学习指导的彻底节奏去除模块。在不同风格转换任务上的实验表明，AutoPST可以有效地转换韵律，正确反映目标领域的风格。",
        "领域": "语音合成、情感计算、无监督学习",
        "问题": "解决在无文本转录情况下进行全局韵律风格转换的挑战",
        "动机": "开发一种不依赖文本转录的韵律风格转换方法，以扩展其应用到资源较少的语言",
        "方法": "提出基于自动编码器的框架AutoPST，利用自表达表示学习指导的节奏去除模块，实现无监督的全局韵律风格转换",
        "关键词": [
            "韵律风格转换",
            "无监督学习",
            "自动编码器",
            "节奏去除",
            "自表达表示学习"
        ],
        "涉及的技术概念": {
            "自动编码器": "用于构建韵律风格转换框架的基础结构，旨在编码和解码语音信号",
            "自表达表示学习": "指导节奏去除模块的学习过程，帮助模型在不依赖文本的情况下理解语音内容",
            "全局韵律风格": "指代语音中高级别的韵律特征，如情感或说话者风格，AutoPST旨在无监督地提取和转换这些特征"
        },
        "success": true
    },
    {
        "order": 440,
        "title": "GLSearch: Maximum Common Subgraph Detection via Learning to Search",
        "html": "https://ICML.cc//virtual/2021/poster/10447",
        "abstract": "Detecting the Maximum Common Subgraph (MCS) between two input graphs is fundamental for applications in drug synthesis, malware detection, cloud computing, etc. However, MCS computation is NP-hard, and state-of-the-art MCS solvers rely on heuristic search algorithms which in practice cannot find good solution for large graph pairs given a limited computation budget. We propose GLSearch, a Graph Neural Network (GNN) based learning to search model. Our model is built upon the branch and bound algorithm, which selects one pair of nodes from the two input graphs to expand at a time. We propose a novel GNN-based Deep Q-Network (DQN) to select the node pair, making the search process much faster. Experiments on synthetic and real-world graph pairs demonstrate that our model learns a search strategy that is able to detect significantly larger common subgraphs than existing MCS solvers given the same computation budget. GLSearch can be potentially extended to solve many other combinatorial problems with constraints on graphs.",
        "conference": "ICML",
        "中文标题": "GLSearch：通过学习搜索实现最大公共子图检测",
        "摘要翻译": "检测两个输入图之间的最大公共子图（MCS）对于药物合成、恶意软件检测、云计算等应用至关重要。然而，MCS计算是NP难的，现有的MCS求解器依赖于启发式搜索算法，这些算法在实践中无法在有限的计算预算下为大型图对找到良好的解决方案。我们提出了GLSearch，一个基于图神经网络（GNN）的学习搜索模型。我们的模型建立在分支定界算法之上，该算法每次从两个输入图中选择一对节点进行扩展。我们提出了一种新颖的基于GNN的深度Q网络（DQN）来选择节点对，使得搜索过程大大加快。在合成和真实世界的图对上的实验表明，我们的模型学习到了一种搜索策略，能够在相同的计算预算下检测到比现有MCS求解器显著更大的公共子图。GLSearch有可能扩展到解决许多其他具有图约束的组合问题。",
        "领域": "图神经网络应用、组合优化、图算法",
        "问题": "如何在有限的计算预算下高效检测两个图之间的最大公共子图",
        "动机": "现有的MCS求解器在处理大型图对时效率低下，无法在有限的计算预算内找到良好的解决方案",
        "方法": "基于图神经网络和深度Q网络的学习搜索模型，结合分支定界算法优化节点对选择过程",
        "关键词": [
            "最大公共子图",
            "图神经网络",
            "深度Q网络",
            "分支定界算法",
            "组合优化"
        ],
        "涉及的技术概念": {
            "图神经网络（GNN）": "用于学习图的表示，帮助模型理解图的结构特征",
            "深度Q网络（DQN）": "用于选择最优的节点对进行扩展，加速搜索过程",
            "分支定界算法": "作为搜索框架，系统地探索可能的解空间，确保找到最大公共子图"
        },
        "success": true
    },
    {
        "order": 441,
        "title": "GMAC: A Distributional Perspective on Actor-Critic Framework",
        "html": "https://ICML.cc//virtual/2021/poster/8971",
        "abstract": "In this paper, we devise a distributional framework on actor-critic as a solution to distributional instability, action type restriction, and conflation between samples and statistics.\nWe propose a new method that minimizes the Cramér distance with the multi-step Bellman target distribution generated from a novel Sample-Replacement algorithm denoted SR(\\lambda), which learns the correct value distribution under multiple Bellman operations.\nParameterizing a value distribution with Gaussian Mixture Model further improves the efficiency and the performance of the method, which we name GMAC.\nWe empirically show that GMAC captures the correct representation of value distributions and improves the performance of a conventional actor-critic method with low computational cost, in both discrete and continuous action spaces using Arcade Learning Environment (ALE) and PyBullet environment.",
        "conference": "ICML",
        "中文标题": "GMAC：基于分布视角的行动者-评论家框架",
        "摘要翻译": "在本文中，我们设计了一个基于行动者-评论家的分布框架，以解决分布不稳定性、动作类型限制以及样本与统计量之间的混淆问题。我们提出了一种新方法，该方法通过最小化与由新颖的样本替换算法SR(λ)生成的多步贝尔曼目标分布之间的Cramér距离，从而在多个贝尔曼操作下学习正确的价值分布。使用高斯混合模型参数化价值分布进一步提高了该方法的效率和性能，我们将其命名为GMAC。我们通过实验证明，GMAC能够捕捉价值分布的正确表示，并在离散和连续动作空间中，使用Arcade学习环境(ALE)和PyBullet环境，以较低的计算成本提高了传统行动者-评论家方法的性能。",
        "领域": "强化学习、深度强化学习、策略优化",
        "问题": "解决强化学习中行动者-评论家框架的分布不稳定性、动作类型限制及样本与统计量混淆的问题",
        "动机": "为了提高强化学习模型在离散和连续动作空间中的性能，同时降低计算成本",
        "方法": "提出了一种基于分布视角的新方法GMAC，通过最小化Cramér距离和使用高斯混合模型参数化价值分布，结合样本替换算法SR(λ)来优化行动者-评论家框架",
        "关键词": [
            "行动者-评论家框架",
            "分布强化学习",
            "高斯混合模型",
            "Cramér距离",
            "样本替换算法"
        ],
        "涉及的技术概念": {
            "Cramér距离": "用于衡量两个概率分布之间的差异，本文中用于最小化与目标分布之间的距离",
            "高斯混合模型": "用于参数化价值分布，提高方法的效率和性能",
            "样本替换算法SR(λ)": "一种新颖的算法，用于生成多步贝尔曼目标分布，帮助学习正确的价值分布"
        },
        "success": true
    },
    {
        "order": 442,
        "title": "GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings",
        "html": "https://ICML.cc//virtual/2021/poster/10757",
        "abstract": "We present GNNAutoScale (GAS), a framework for scaling arbitrary message-passing GNNs to large graphs. GAS prunes entire sub-trees of the computation graph by utilizing historical embeddings from prior training iterations, leading to constant GPU memory consumption in respect to input node size without dropping any data. While existing solutions weaken the expressive power of message passing due to sub-sampling of edges or non-trainable propagations, our approach is provably able to maintain the expressive power of the original GNN. We achieve this by providing approximation error bounds of historical embeddings and show how to tighten them in practice. Empirically, we show that the practical realization of our framework, PyGAS, an easy-to-use extension for PyTorch Geometric, is both fast and memory-efficient, learns expressive node representations, closely resembles the performance of their non-scaling counterparts, and reaches state-of-the-art performance on large-scale graphs.",
        "conference": "ICML",
        "中文标题": "GNNAutoScale：通过历史嵌入实现可扩展且富有表达力的图神经网络",
        "摘要翻译": "我们提出了GNNAutoScale（GAS），一个用于将任意消息传递图神经网络（GNNs）扩展到大型图上的框架。GAS通过利用先前训练迭代中的历史嵌入，修剪计算图的整个子树，从而在不丢弃任何数据的情况下，实现相对于输入节点大小的恒定GPU内存消耗。虽然现有解决方案由于边的子采样或不可训练的传播而削弱了消息传递的表达能力，但我们的方法被证明能够保持原始GNN的表达能力。我们通过提供历史嵌入的近似误差界限并展示如何在实践中收紧这些界限来实现这一点。实证上，我们展示了我们框架的实际实现——PyGAS，一个易于使用的PyTorch Geometric扩展，既快速又内存高效，学习富有表达力的节点表示，与非扩展对应物性能相近，并在大规模图上达到了最先进的性能。",
        "领域": "图神经网络、大规模图处理、深度学习框架优化",
        "问题": "如何在保持图神经网络表达能力的同时，实现其在大型图上的可扩展性。",
        "动机": "解决现有图神经网络在大规模图上扩展时面临的内存消耗大和表达能力下降的问题。",
        "方法": "利用历史嵌入技术修剪计算图，保持原始GNN的表达能力，同时实现恒定的GPU内存消耗。",
        "关键词": [
            "图神经网络",
            "历史嵌入",
            "可扩展性",
            "PyTorch Geometric",
            "大规模图处理"
        ],
        "涉及的技术概念": {
            "历史嵌入": "利用先前训练迭代中的节点嵌入来近似当前迭代的嵌入，以减少计算和内存消耗。",
            "消息传递": "图神经网络中节点间信息交换的基本机制，GAS保持其原始表达能力。",
            "近似误差界限": "为历史嵌入提供的理论保证，确保其在实际应用中的有效性和可靠性。"
        },
        "success": true
    },
    {
        "order": 443,
        "title": "Goal-Conditioned Reinforcement Learning with Imagined Subgoals",
        "html": "https://ICML.cc//virtual/2021/poster/10147",
        "abstract": "Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don’t require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning.  Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy.  We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.",
        "conference": "ICML",
        "中文标题": "基于想象子目标的目标条件强化学习",
        "摘要翻译": "目标条件强化学习赋予代理多种技能，但在解决需要更长时间跨度推理的任务时往往遇到困难。在本工作中，我们提出将想象的子目标纳入策略学习，以促进复杂任务的学习。想象的子目标由一个独立的高层策略预测，该策略与策略及其评论家同时训练。这个高层策略利用价值函数作为可达性度量，预测到目标中途的中间状态。我们并不要求策略明确达到这些子目标。相反，我们使用它们来定义一个先验策略，并将这个先验纳入一个KL约束的策略迭代方案中，以加速和规范化学习。想象的子目标在策略学习期间使用，但在测试时不使用，那时我们只应用学习到的策略。我们在复杂的机器人导航和操作任务上评估了我们的方法，并显示它大幅优于现有方法。",
        "领域": "机器人导航、机器人操作、强化学习",
        "问题": "解决目标条件强化学习在需要长时间跨度推理的复杂任务中的困难",
        "动机": "通过引入想象的子目标来促进复杂任务的学习，提高策略学习的效率和效果",
        "方法": "提出一种方法，通过高层策略预测想象的子目标，并将这些子目标用于定义先验策略，进而通过KL约束的策略迭代方案加速和规范化学习",
        "关键词": [
            "目标条件强化学习",
            "想象子目标",
            "KL约束策略迭代",
            "机器人导航",
            "机器人操作"
        ],
        "涉及的技术概念": {
            "想象子目标": "由高层策略预测的中间状态，用于促进复杂任务的学习",
            "KL约束策略迭代": "一种策略优化方法，通过KL散度约束来规范化策略更新，加速学习过程",
            "价值函数": "用作可达性度量，帮助高层策略预测有效的子目标"
        },
        "success": true
    },
    {
        "order": 444,
        "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9129",
        "abstract": "Gaussian processes (GPs) are non-parametric, flexible, models that work well in many tasks. Combining GPs with deep learning methods via deep kernel learning (DKL) is especially compelling due to the strong representational power induced by the network. However, inference in GPs, whether with or without DKL, can be computationally challenging on large datasets. Here, we propose GP-Tree, a novel method for multi-class classification with Gaussian processes and DKL. We develop a tree-based hierarchical model in which each internal node of the tree fits a GP to the data using the Pólya-Gamma augmentation scheme. As a result, our method scales well with both the number of classes and data size. We demonstrate the effectiveness of our method against other Gaussian process training baselines, and we show how our general GP approach achieves improved accuracy on standard incremental few-shot learning benchmarks.",
        "conference": "ICML",
        "中文标题": "GP-Tree: 一种用于少样本增量学习的高斯过程分类器",
        "摘要翻译": "高斯过程（GPs）是一种非参数、灵活的模型，在许多任务中表现良好。通过深度核学习（DKL）将GPs与深度学习方法相结合，由于网络诱导的强大表示能力，显得尤为引人注目。然而，无论是否使用DKL，GPs在大数据集上的推断都可能面临计算上的挑战。在此，我们提出了GP-Tree，一种用于高斯过程和DKL的多类分类新方法。我们开发了一种基于树的层次模型，其中树的每个内部节点使用Pólya-Gamma增强方案对数据拟合一个GP。因此，我们的方法在类别数量和数据大小方面都具有良好的扩展性。我们展示了我们的方法相对于其他高斯过程训练基线的有效性，并展示了我们的一般GP方法如何在标准的增量少样本学习基准上实现更高的准确性。",
        "领域": "少样本学习, 增量学习, 高斯过程分类",
        "问题": "解决高斯过程在大数据集和多类分类任务中的计算挑战",
        "动机": "结合高斯过程和深度核学习以提高模型的表示能力和分类性能",
        "方法": "提出了一种基于树的层次模型，每个内部节点使用Pólya-Gamma增强方案拟合高斯过程",
        "关键词": [
            "高斯过程",
            "深度核学习",
            "少样本学习",
            "增量学习",
            "多类分类"
        ],
        "涉及的技术概念": {
            "高斯过程": "一种非参数的贝叶斯方法，用于建模数据的分布和不确定性",
            "深度核学习": "结合深度学习和核方法的技术，用于增强模型的表示能力",
            "Pólya-Gamma增强方案": "一种用于高斯过程分类的增强技术，有助于模型的训练和推断"
        },
        "success": true
    },
    {
        "order": 445,
        "title": "Gradient Disaggregation: Breaking Privacy in Federated Learning by Reconstructing the User Participant Matrix",
        "html": "https://ICML.cc//virtual/2021/poster/8851",
        "abstract": "We show that aggregated model updates in federated learning may be insecure. An untrusted central server may disaggregate user updates from sums of updates across participants given repeated  observations, enabling the server to recover privileged information about individual users' private training data via traditional gradient inference attacks. Our method revolves around reconstructing participant information (e.g: which rounds of training users participated in) from aggregated model updates by leveraging summary information from device analytics commonly used to monitor, debug, and manage federated learning systems. Our attack is parallelizable and we successfully disaggregate user updates on settings with up to thousands of participants. We quantitatively and qualitatively demonstrate significant improvements in the capability of various inference attacks on the disaggregated updates. Our attack enables the attribution of learned properties to individual users, violating anonymity, and shows that a determined central server may undermine the secure aggregation protocol to break individual users' data privacy in federated learning.",
        "conference": "ICML",
        "中文标题": "梯度分解：通过重构用户参与矩阵打破联邦学习中的隐私保护",
        "摘要翻译": "我们展示了联邦学习中聚合的模型更新可能是不安全的。一个不受信任的中央服务器可能通过重复观察从参与者的更新总和中分解出用户更新，使得服务器能够通过传统的梯度推断攻击恢复关于个别用户私有训练数据的特权信息。我们的方法围绕从聚合模型更新中重构参与者信息（例如：用户参与了哪几轮训练）展开，利用通常用于监控、调试和管理联邦学习系统的设备分析摘要信息。我们的攻击是可并行化的，并且我们成功地在拥有多达数千名参与者的设置中分解了用户更新。我们定量和定性地展示了在各种推断攻击能力上的显著提升。我们的攻击使得学习到的属性可以归因于个别用户，违反了匿名性，并表明一个决心的中央服务器可能破坏安全聚合协议，从而打破个别用户在联邦学习中的数据隐私。",
        "领域": "联邦学习安全、隐私保护、梯度推断攻击",
        "问题": "解决联邦学习中聚合模型更新可能被中央服务器利用来重构用户私有数据的问题",
        "动机": "揭示联邦学习中即使使用安全聚合协议，用户的隐私数据仍可能通过梯度分解攻击被泄露的风险",
        "方法": "通过从聚合模型更新中重构用户参与信息，利用设备分析摘要信息进行梯度分解攻击",
        "关键词": [
            "联邦学习",
            "梯度分解",
            "隐私攻击",
            "安全聚合",
            "数据隐私"
        ],
        "涉及的技术概念": {
            "梯度分解": "通过分析聚合的模型更新，分离出个别用户的贡献，从而可能泄露私有数据",
            "安全聚合协议": "联邦学习中用于保护用户数据隐私的技术，通过聚合用户更新防止直接访问个别用户数据",
            "梯度推断攻击": "利用模型梯度信息推断出训练数据中的敏感信息的攻击方法"
        },
        "success": true
    },
    {
        "order": 446,
        "title": "GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training",
        "html": "https://ICML.cc//virtual/2021/poster/9861",
        "abstract": "The great success of modern machine learning models on large datasets is contingent on extensive computational resources with high financial and environmental costs. One way to address this is by extracting subsets that generalize on par with the full data. In this work, we propose a general framework, GRAD-MATCH, which finds subsets that closely match the gradient of the \\emph{training or validation} set. We find such subsets effectively using an orthogonal matching pursuit algorithm. We show rigorous theoretical and convergence guarantees of the proposed algorithm and, through our extensive experiments on real-world datasets, show the effectiveness of our proposed framework. We show that GRAD-MATCH significantly and consistently outperforms several recent data-selection algorithms and achieves the best accuracy-efficiency trade-off. GRAD-MATCH is available as a part of the CORDS toolkit: \\url{https://github.com/decile-team/cords}.",
        "conference": "ICML",
        "中文标题": "GRAD-MATCH：基于梯度匹配的数据子集选择以实现高效的深度模型训练",
        "摘要翻译": "现代机器学习模型在大数据集上的巨大成功依赖于大量的计算资源，这些资源伴随着高昂的财务和环境成本。解决这一问题的一种方法是提取能够与完整数据泛化能力相当的数据子集。在这项工作中，我们提出了一个通用框架GRAD-MATCH，它能够找到与训练集或验证集的梯度紧密匹配的子集。我们使用正交匹配追踪算法有效地找到这样的子集。我们展示了所提出算法的严格理论和收敛保证，并通过在真实世界数据集上的广泛实验，展示了我们提出的框架的有效性。我们证明，GRAD-MATCH显著且持续地优于几种最近的数据选择算法，并实现了最佳的准确性与效率的权衡。GRAD-MATCH作为CORDS工具包的一部分提供：https://github.com/decile-team/cords。",
        "领域": "深度学习优化、数据子集选择、模型训练效率",
        "问题": "如何在大规模数据集上高效训练深度学习模型，同时减少计算资源的消耗。",
        "动机": "减少深度学习模型训练过程中的计算资源消耗，同时保持模型的泛化能力。",
        "方法": "提出GRAD-MATCH框架，通过梯度匹配选择数据子集，使用正交匹配追踪算法实现高效子集选择。",
        "关键词": [
            "梯度匹配",
            "数据子集选择",
            "正交匹配追踪",
            "深度学习优化",
            "训练效率"
        ],
        "涉及的技术概念": {
            "梯度匹配": "用于选择与完整数据集梯度相似的数据子集，以保持模型训练的有效性。",
            "正交匹配追踪算法": "用于高效地找到与目标梯度最匹配的数据子集。",
            "数据子集选择": "通过选择代表性的数据子集来减少训练数据量，提高训练效率。"
        },
        "success": true
    },
    {
        "order": 447,
        "title": "Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech",
        "html": "https://ICML.cc//virtual/2021/poster/8569",
        "abstract": "Recently, denoising diffusion probabilistic models and generative score matching have shown high potential in modelling complex data distributions while stochastic calculus has provided a unified point of view on these techniques allowing for flexible inference schemes. In this paper we introduce Grad-TTS, a novel text-to-speech model with score-based decoder producing mel-spectrograms by gradually transforming noise predicted by encoder and aligned with text input by means of Monotonic Alignment Search. The framework of stochastic differential equations helps us to generalize conventional diffusion probabilistic models to the case of reconstructing data from noise with different parameters and allows to make this reconstruction flexible by explicitly controlling trade-off between sound quality and inference speed. Subjective human evaluation shows that Grad-TTS is competitive with state-of-the-art text-to-speech approaches in terms of Mean Opinion Score.",
        "conference": "ICML",
        "中文标题": "Grad-TTS：一种用于文本到语音的扩散概率模型",
        "摘要翻译": "最近，去噪扩散概率模型和生成式分数匹配在建模复杂数据分布方面显示出巨大潜力，而随机微分为这些技术提供了统一的视角，允许灵活的推理方案。在本文中，我们介绍了Grad-TTS，这是一种新颖的文本到语音模型，其基于分数的解码器通过逐渐转换由编码器预测并与文本输入通过单调对齐搜索对齐的噪声来生成梅尔频谱图。随机微分方程的框架帮助我们推广了传统的扩散概率模型，以从具有不同参数的噪声中重建数据，并通过明确控制音质和推理速度之间的权衡使这种重建变得灵活。主观人类评估表明，Grad-TTS在平均意见得分方面与最先进的文本到语音方法具有竞争力。",
        "领域": "语音合成, 生成模型, 音频处理",
        "问题": "如何高效且高质量地实现文本到语音的转换",
        "动机": "探索扩散概率模型和生成式分数匹配在文本到语音转换中的应用潜力，以提高合成语音的质量和灵活性",
        "方法": "采用基于分数的解码器和单调对齐搜索技术，结合随机微分方程框架，灵活控制音质与推理速度的权衡",
        "关键词": [
            "文本到语音",
            "扩散概率模型",
            "梅尔频谱图",
            "单调对齐搜索",
            "随机微分方程"
        ],
        "涉及的技术概念": {
            "扩散概率模型": "用于逐步从噪声中重建数据，提高语音合成的质量",
            "单调对齐搜索": "确保生成的梅尔频谱图与输入文本对齐，提升语音的自然度和可懂度",
            "随机微分方程": "提供理论框架，使模型能够灵活控制合成过程中的音质和速度权衡"
        },
        "success": true
    },
    {
        "order": 448,
        "title": "GRAND: Graph Neural Diffusion",
        "html": "https://ICML.cc//virtual/2021/poster/8889",
        "abstract": "We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.",
        "conference": "ICML",
        "中文标题": "GRAND：图神经扩散",
        "摘要翻译": "我们提出了图神经扩散（GRAND），它将图上的深度学习视为一个连续的扩散过程，并将图神经网络（GNNs）视为基础偏微分方程（PDE）的离散化。在我们的模型中，层结构和拓扑对应于时间和空间算子的离散化选择。我们的方法允许有原则地开发一类新的GNNs，能够解决图学习模型的常见问题，如深度、过度平滑和瓶颈。我们模型成功的关键在于对数据扰动的稳定性，这为隐式和显式离散化方案都得到了解决。我们开发了GRAND的线性和非线性版本，在许多标准图基准测试中取得了竞争性的结果。",
        "领域": "图神经网络、深度学习、偏微分方程",
        "问题": "解决图学习模型中的深度、过度平滑和瓶颈问题",
        "动机": "通过将图神经网络视为偏微分方程的离散化，开发一类新的GNNs，以解决图学习中的常见问题",
        "方法": "将图上的深度学习视为连续的扩散过程，开发线性和非线性版本的GRAND模型",
        "关键词": [
            "图神经扩散",
            "图神经网络",
            "偏微分方程",
            "深度学习",
            "稳定性"
        ],
        "涉及的技术概念": {
            "图神经扩散": "将图上的深度学习视为连续的扩散过程，开发新的GNNs",
            "偏微分方程": "作为图神经网络离散化的基础，用于模型开发",
            "稳定性": "模型对数据扰动的稳定性，是成功的关键因素"
        },
        "success": true
    },
    {
        "order": 449,
        "title": "Graph Contrastive Learning Automated",
        "html": "https://ICML.cc//virtual/2021/poster/9823",
        "abstract": "Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous 'best practices' observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.",
        "conference": "ICML",
        "中文标题": "图对比学习自动化",
        "摘要翻译": "近年来，基于图结构数据的自监督学习在从未标记的图中学习通用、可迁移和鲁棒的表示方面引起了广泛关注。其中，图对比学习（GraphCL）以其出色的表示学习性能脱颖而出。然而，与图像数据上的对比学习不同，GraphCL的有效性依赖于特定的数据增强方法，这些方法由于图数据的多样性，必须通过经验法则或试错法为每个数据集手动选择，这大大限制了GraphCL的更广泛应用。为了填补这一关键空白，本文提出了一个统一的双层优化框架，用于在特定图数据上执行GraphCL时自动、自适应和动态地选择数据增强方法。这一通用框架被称为联合增强优化（JOAO），被实例化为最小-最大优化。JOAO选择的增强方法总体上与之前通过手工调整观察到的'最佳实践'一致，但现在实现了自动化，更加灵活和多功能。此外，我们提出了一种新的增强感知投影头机制，该机制将通过不同的投影头路由输出特征，对应于每个训练步骤选择的不同增强方法。大量实验表明，JOAO在多个不同规模和类型的图数据集上的表现与包括GraphCL在内的最先进竞争对手相当或有时更好，而无需对增强选择进行任何繁琐的数据集特定调整。我们在https://github.com/Shen-Lab/GraphCL_Automated上发布了代码。",
        "领域": "图神经网络、自监督学习、图表示学习",
        "问题": "如何自动选择适合特定图数据的数据增强方法以提高图对比学习的性能和适用性",
        "动机": "解决图对比学习依赖手动选择数据增强方法的问题，提高其自动化程度和适用范围",
        "方法": "提出了一个统一的双层优化框架（JOAO），自动、自适应和动态地选择数据增强方法，并引入增强感知投影头机制",
        "关键词": [
            "图对比学习",
            "数据增强",
            "自监督学习",
            "双层优化",
            "图表示学习"
        ],
        "涉及的技术概念": {
            "图对比学习（GraphCL）": "一种基于图结构数据的自监督学习方法，通过对比学习从未标记的图中学习通用、可迁移和鲁棒的表示",
            "联合增强优化（JOAO）": "一个双层优化框架，用于自动、自适应和动态地选择数据增强方法，提高图对比学习的性能和适用性",
            "增强感知投影头机制": "一种新的机制，通过不同的投影头路由输出特征，对应于每个训练步骤选择的不同增强方法，以提高模型的灵活性和多功能性"
        },
        "success": true
    },
    {
        "order": 450,
        "title": "Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/8741",
        "abstract": "Recently there has been increased interest in semi-supervised classification in the presence of graphical information. A new class of learning models has emerged that relies, at its most basic level, on classifying the data after first applying a graph convolution. To understand the merits of this approach, we study the classification of a mixture of Gaussians, where the data corresponds to the node attributes of a stochastic block model. We show that graph convolution extends the regime in which the data is linearly separable by a factor of roughly $1/\\sqrt{D}$, where $D$ is the expected degree of a node, as compared to the mixture model data on its own. Furthermore, we find that the linear classifier obtained by minimizing the cross-entropy loss after the graph convolution  generalizes to out-of-distribution data where the unseen data can have different intra- and inter-class edge probabilities from the training data.",
        "conference": "ICML",
        "success": true,
        "中文标题": "用于半监督分类的图卷积：改进的线性可分性和分布外泛化",
        "摘要翻译": "最近，在存在图形信息的情况下，人们对半监督分类的兴趣日益浓厚。出现了一类新的学习模型，它最基本的层面上依赖于在首先应用图卷积后对数据进行分类。为了理解这种方法的优点，我们研究了高斯混合模型的分类，其中数据对应于随机块模型的节点属性。我们表明，与单独的混合模型数据相比，图卷积将数据线性可分的范围扩展了大约$1/\\sqrt{D}$倍，其中$D$是节点的预期度数。此外，我们发现通过最小化图卷积后的交叉熵损失获得的线性分类器可以推广到分布外数据，其中未见数据可能具有与训练数据不同的类内和类间边缘概率。",
        "领域": "图神经网络、半监督学习、图表示学习",
        "问题": "如何利用图卷积改进半监督分类的性能，特别是在线性可分性和分布外泛化方面？",
        "动机": "现有的半监督分类方法在处理具有图结构的数据时，如何有效利用图信息来提升模型的泛化能力，尤其是在面对分布外数据时仍是一个挑战。研究旨在通过图卷积来扩展线性可分性并提升分布外泛化能力。",
        "方法": "通过分析高斯混合模型在随机块模型上的分类，研究图卷积对线性可分性的影响。通过最小化交叉熵损失训练线性分类器，并评估其在分布外数据上的泛化性能。",
        "关键词": [
            "图卷积",
            "半监督分类",
            "线性可分性",
            "分布外泛化",
            "随机块模型"
        ],
        "涉及的技术概念": {
            "图卷积": "一种在图结构数据上进行卷积操作的技术，用于提取节点和边的特征表示，从而实现对图数据的学习和分析。",
            "半监督学习": "一种机器学习方法，它结合了有标签数据和无标签数据来训练模型，从而在标签数据稀缺的情况下提高模型的性能。"
        }
    },
    {
        "order": 451,
        "title": "Graph Cuts Always Find a Global Optimum for Potts Models (With a Catch)",
        "html": "https://ICML.cc//virtual/2021/poster/10087",
        "abstract": "We prove that the alpha-expansion algorithm for MAP inference always returns a globally optimal assignment for Markov Random Fields with Potts pairwise potentials, with a catch: the returned assignment is only guaranteed to be optimal for an instance within a small perturbation of the original problem instance.  In other words, all local minima with respect to expansion moves are global minima to slightly perturbed versions of the problem.  On 'real-world' instances, MAP assignments of small perturbations of the problem should be very similar to the MAP assignment(s) of the original problem instance.  We design an algorithm that can certify whether this is the case in practice.  On several MAP inference problem instances from computer vision, this algorithm certifies that MAP solutions to all of these perturbations are very close to solutions of the original instance. These results taken together give a cohesive explanation for the good performance of 'graph cuts' algorithms in practice. Every local expansion minimum is a global minimum in a small perturbation of the problem, and all of these global minima are close to the original solution.",
        "conference": "ICML",
        "success": true,
        "中文标题": "图割算法总能找到Potts模型的全局最优解（但有一个条件）",
        "摘要翻译": "我们证明了用于最大后验概率（MAP）推断的alpha-expansion算法总能返回具有Potts成对势能的马尔可夫随机场的全局最优分配，但有一个条件：返回的分配仅保证在原始问题实例的小扰动范围内是最优的。换句话说，所有关于扩展移动的局部最小值都是问题略微扰动版本的全局最小值。在‘现实世界’的实例中，问题小扰动的MAP分配应该与原始问题实例的MAP分配非常相似。我们设计了一个算法，可以在实践中验证是否如此。在来自计算机视觉的几个MAP推断问题实例上，该算法验证了所有这些扰动的MAP解都非常接近原始实例的解。这些结果共同为‘图割’算法在实践中的良好性能提供了一个连贯的解释。每个局部扩展最小值都是问题小扰动中的全局最小值，并且所有这些全局最小值都接近原始解。",
        "领域": "图像分割, 马尔可夫随机场, 计算机视觉中的优化问题",
        "问题": "证明alpha-expansion算法在Potts成对势能的马尔可夫随机场中总能找到全局最优解的条件及其在实际中的应用验证。",
        "动机": "解释图割算法在实际应用中表现良好的原因，并提供一种验证方法。",
        "方法": "设计算法验证在小扰动下MAP解与原始问题解的接近程度，并在计算机视觉的MAP推断问题实例上进行测试。",
        "关键词": [
            "图割算法",
            "Potts模型",
            "马尔可夫随机场",
            "alpha-expansion算法",
            "MAP推断"
        ],
        "涉及的技术概念": {
            "alpha-expansion算法": "用于马尔可夫随机场中最大后验概率推断的算法，通过扩展移动寻找能量函数的局部最小值。",
            "Potts成对势能": "一种用于马尔可夫随机场的成对势能函数，常用于图像分割等领域，鼓励相邻节点具有相同的标签。",
            "马尔可夫随机场": "一种概率图模型，用于建模具有马尔可夫性质的随机变量集合，广泛应用于计算机视觉中的图像分割和立体匹配等问题。"
        }
    },
    {
        "order": 452,
        "title": "GraphDF: A Discrete Flow Model for Molecular Graph Generation",
        "html": "https://ICML.cc//virtual/2021/poster/10443",
        "abstract": "We consider the problem of molecular graph generation using deep models. While graphs are discrete, most existing methods use continuous latent variables, resulting in inaccurate modeling of discrete graph structures. In this work, we propose GraphDF, a novel discrete latent variable model for molecular graph generation based on normalizing flow methods. GraphDF uses invertible modulo shift transforms to map discrete latent variables to graph nodes and edges. We show that the use of discrete latent variables reduces computational costs and eliminates the negative effect of dequantization. Comprehensive experimental results show that GraphDF outperforms prior methods on random generation, property optimization, and constrained optimization tasks.",
        "conference": "ICML",
        "中文标题": "GraphDF：一种用于分子图生成的离散流模型",
        "摘要翻译": "我们考虑使用深度模型进行分子图生成的问题。虽然图是离散的，但大多数现有方法使用连续的潜在变量，导致对离散图结构的建模不准确。在这项工作中，我们提出了GraphDF，一种基于归一化流方法的新型离散潜在变量模型，用于分子图生成。GraphDF使用可逆模移位变换将离散潜在变量映射到图节点和边。我们表明，使用离散潜在变量降低了计算成本，并消除了去量化的负面影响。全面的实验结果表明，GraphDF在随机生成、属性优化和约束优化任务上优于先前的方法。",
        "领域": "分子图生成",
        "问题": "现有方法使用连续潜在变量导致对离散图结构的建模不准确",
        "动机": "提高分子图生成的准确性和效率",
        "方法": "提出基于归一化流方法的离散潜在变量模型GraphDF，使用可逆模移位变换",
        "关键词": [
            "分子图生成",
            "离散潜在变量",
            "归一化流",
            "可逆模移位变换",
            "计算效率"
        ],
        "涉及的技术概念": {
            "离散潜在变量": "用于准确建模离散图结构，降低计算成本",
            "归一化流方法": "用于构建从潜在空间到数据空间的可逆变换",
            "可逆模移位变换": "用于将离散潜在变量映射到图节点和边，消除去量化的负面影响"
        },
        "success": true
    },
    {
        "order": 453,
        "title": "Graph Mixture Density Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9631",
        "abstract": "We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions.",
        "conference": "ICML",
        "中文标题": "图混合密度网络",
        "摘要翻译": "我们介绍了图混合密度网络，这是一种新的机器学习模型家族，能够拟合基于任意拓扑图的多模态输出分布。通过结合混合模型和图表示学习的理念，我们解决了一类更广泛的依赖于结构化数据的挑战性条件密度估计问题。在这方面，我们在一个新的基准应用上评估了我们的方法，该应用利用随机图进行随机流行病模拟。我们展示了在考虑多模态和结构时，流行病结果可能性的显著提高。实证分析还补充了两个真实世界的回归任务，展示了我们的方法在建模输出预测不确定性方面的有效性。图混合密度网络在研究表现出非平凡条件输出分布的结构依赖现象中开辟了吸引人的研究机会。",
        "领域": "图神经网络、条件密度估计、流行病模拟",
        "问题": "解决基于任意拓扑图的多模态输出分布的条件密度估计问题",
        "动机": "为了更有效地处理结构化数据中的多模态输出分布问题，特别是在流行病模拟等应用中",
        "方法": "结合混合模型和图表示学习的理念，开发图混合密度网络模型",
        "关键词": [
            "图混合密度网络",
            "条件密度估计",
            "多模态输出分布",
            "图表示学习",
            "流行病模拟"
        ],
        "涉及的技术概念": {
            "图混合密度网络": "结合混合模型和图表示学习的新模型，用于拟合基于图的多模态输出分布",
            "条件密度估计": "在给定输入（如图结构）的条件下，估计输出的概率分布",
            "多模态输出分布": "输出分布具有多个峰值或模式，反映了数据中的多种可能结果"
        },
        "success": true
    },
    {
        "order": 454,
        "title": "Graph Neural Networks Inspired by Classical Iterative Algorithms",
        "html": "https://ICML.cc//virtual/2021/poster/8797",
        "abstract": "Despite the recent success of graph neural networks (GNN), common architectures often exhibit significant limitations, including sensitivity to oversmoothing, long-range dependencies, and spurious edges, e.g., as can occur as a result of graph heterophily or adversarial attacks.  To at least partially address these issues within a simple transparent framework, we consider a new family of GNN layers designed to mimic and integrate the update rules of two classical iterative algorithms, namely, proximal gradient descent and iterative reweighted least squares (IRLS).  The former defines an extensible base GNN architecture that is immune to oversmoothing while nonetheless capturing long-range dependencies by allowing arbitrary propagation steps.  In contrast, the latter produces a novel attention mechanism that is explicitly anchored to an underlying end-to-end energy function, contributing stability with respect to edge uncertainty.  When combined we obtain an extremely simple yet robust model that we evaluate across disparate scenarios including standardized benchmarks, adversarially-perturbated graphs, graphs with heterophily, and graphs involving long-range dependencies.  In doing so, we compare against SOTA GNN approaches that have been explicitly designed for the respective task, achieving competitive or superior node classification accuracy.  Our code is available at  https://github.com/FFTYYY/TWIRLS.  And for an extended version of this work, please see https://arxiv.org/abs/2103.06064.",
        "conference": "ICML",
        "中文标题": "受经典迭代算法启发的图神经网络",
        "摘要翻译": "尽管图神经网络（GNN）近年来取得了成功，但常见的架构往往表现出显著的局限性，包括对过度平滑、长距离依赖和虚假边的敏感性，例如，这些可能由于图的异质性或对抗性攻击而产生。为了在一个简单透明的框架内至少部分解决这些问题，我们考虑了一种新的GNN层家族，旨在模仿并整合两种经典迭代算法的更新规则，即近端梯度下降和迭代重加权最小二乘法（IRLS）。前者定义了一个可扩展的基础GNN架构，该架构对过度平滑免疫，同时通过允许任意的传播步骤来捕捉长距离依赖。相比之下，后者产生了一种新颖的注意力机制，该机制明确锚定于一个端到端的能量函数，有助于在边不确定性方面提供稳定性。当两者结合时，我们获得了一个极其简单却鲁棒的模型，我们在包括标准化基准、对抗性扰动的图、具有异质性的图以及涉及长距离依赖的图在内的不同场景中对其进行了评估。在此过程中，我们与为各自任务明确设计的最先进的GNN方法进行了比较，实现了竞争性或更优的节点分类准确率。我们的代码可在https://github.com/FFTYYY/TWIRLS获取。关于这项工作的扩展版本，请参见https://arxiv.org/abs/2103.06064。",
        "领域": "图神经网络、对抗性攻击防御、异质性图处理",
        "问题": "解决图神经网络对过度平滑、长距离依赖和虚假边的敏感性",
        "动机": "通过模仿经典迭代算法的更新规则，提升图神经网络在处理异质性图和对抗性攻击时的鲁棒性和性能",
        "方法": "结合近端梯度下降和迭代重加权最小二乘法（IRLS）的更新规则，设计新的GNN层，以增强模型的稳定性和捕捉长距离依赖的能力",
        "关键词": [
            "图神经网络",
            "迭代算法",
            "对抗性防御",
            "异质性图",
            "长距离依赖"
        ],
        "涉及的技术概念": {
            "近端梯度下降": "用于定义对过度平滑免疫的基础GNN架构，允许任意传播步骤以捕捉长距离依赖",
            "迭代重加权最小二乘法（IRLS）": "产生一种新颖的注意力机制，明确锚定于端到端能量函数，增强模型在边不确定性方面的稳定性",
            "端到端能量函数": "为注意力机制提供理论基础，确保模型在处理不确定性和异质性时的稳定性和鲁棒性"
        },
        "success": true
    },
    {
        "order": 455,
        "title": "GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training",
        "html": "https://ICML.cc//virtual/2021/poster/10655",
        "abstract": "Normalization is known to help the optimization of deep neural networks. Curiously, different architectures require specialized normalization methods. In this paper, we study what normalization is effective for Graph Neural Networks (GNNs). First, we adapt and evaluate the existing methods from other domains to GNNs. Faster convergence is achieved with InstanceNorm compared to BatchNorm and LayerNorm. We provide an explanation by showing that InstanceNorm serves as a preconditioner for GNNs, but such preconditioning effect is weaker with BatchNorm due to the heavy batch noise in graph datasets. Second, we show that the shift operation in InstanceNorm results in an expressiveness degradation of GNNs for highly regular graphs. We address this issue by proposing GraphNorm with a learnable shift. Empirically, GNNs with GraphNorm converge faster compared to GNNs using other normalization. GraphNorm also improves the generalization of GNNs, achieving better performance on graph classification benchmarks.",
        "conference": "ICML",
        "中文标题": "GraphNorm：一种加速图神经网络训练的原则性方法",
        "摘要翻译": "已知归一化有助于深度神经网络的优化。有趣的是，不同的架构需要专门的归一化方法。在本文中，我们研究了什么归一化方法对图神经网络（GNNs）有效。首先，我们调整并评估了从其他领域到GNNs的现有方法。与BatchNorm和LayerNorm相比，InstanceNorm实现了更快的收敛。我们通过展示InstanceNorm作为GNNs的预处理器提供了一个解释，但由于图数据集中的大量批次噪声，BatchNorm的这种预处理效果较弱。其次，我们展示了InstanceNorm中的移位操作导致了对高度规则图的GNNs表达能力下降。我们通过提出具有可学习移位的GraphNorm来解决这个问题。经验上，使用GraphNorm的GNNs比使用其他归一化的GNNs收敛得更快。GraphNorm还提高了GNNs的泛化能力，在图分类基准上实现了更好的性能。",
        "领域": "图神经网络、深度学习优化、图分类",
        "问题": "如何有效地归一化图神经网络以加速训练并提高性能",
        "动机": "研究图神经网络中不同归一化方法的效果，解决现有方法在处理图数据时的不足",
        "方法": "提出GraphNorm，一种具有可学习移位的归一化方法，以解决InstanceNorm在处理高度规则图时的表达能力下降问题",
        "关键词": [
            "图神经网络",
            "归一化方法",
            "训练加速",
            "图分类",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "InstanceNorm": "作为图神经网络的预处理器，有助于加速收敛",
            "BatchNorm": "由于图数据集中的批次噪声，预处理效果较弱",
            "GraphNorm": "提出的具有可学习移位的归一化方法，旨在解决InstanceNorm在处理高度规则图时的表达能力下降问题"
        },
        "success": true
    },
    {
        "order": 456,
        "title": "Grey-box Extraction of Natural Language Models",
        "html": "https://ICML.cc//virtual/2021/poster/8783",
        "abstract": "Model extraction attacks attempt to replicate a target machine learning model by querying its inference API. State-of-the-art attacks are learning-based and construct replicas by supervised training on the target model's predictions, but an emerging class of attacks exploit algebraic properties to obtain high-fidelity replicas using orders of magnitude fewer queries. So far, these algebraic attacks have been limited to neural networks with few hidden layers and ReLU activations. In this paper we present algebraic and hybrid algebraic/learning-based attacks on large-scale natural language models. We consider a grey-box setting, targeting models with a pre-trained (public) encoder followed by a single (private) classification layer. Our key findings are that (i) with a frozen encoder, high-fidelity extraction is possible with a small number of in-distribution queries, making extraction attacks indistinguishable from legitimate use; (ii) when the encoder is fine-tuned, a hybrid learning-based/algebraic attack improves over the learning-based state-of-the-art without requiring additional queries.\n",
        "conference": "ICML",
        "中文标题": "自然语言模型的灰盒提取",
        "摘要翻译": "模型提取攻击试图通过查询其推理API来复制目标机器学习模型。最先进的攻击是基于学习的，通过在目标模型的预测上进行监督训练来构建复制品，但一类新兴的攻击利用代数特性，使用数量级更少的查询获得高保真复制品。到目前为止，这些代数攻击仅限于具有少量隐藏层和ReLU激活的神经网络。在本文中，我们提出了针对大规模自然语言模型的代数和混合代数/基于学习的攻击。我们考虑了一个灰盒设置，目标是具有预训练（公共）编码器和单个（私有）分类层的模型。我们的关键发现是（i）在编码器固定的情况下，使用少量分布内查询即可实现高保真提取，使提取攻击与合法使用难以区分；（ii）当编码器进行微调时，混合基于学习/代数的攻击在不需额外查询的情况下，优于基于学习的最新技术。",
        "领域": "自然语言处理与视觉结合, 模型安全, 对抗性机器学习",
        "问题": "如何高效且隐蔽地从大规模自然语言模型中提取信息",
        "动机": "探索在灰盒设置下，通过代数和基于学习的混合方法，实现对大规模自然语言模型的高效提取，同时保持攻击的隐蔽性。",
        "方法": "采用代数和混合代数/基于学习的方法，针对具有预训练编码器和单个分类层的模型，进行高保真提取。",
        "关键词": [
            "模型提取攻击",
            "自然语言模型",
            "灰盒设置",
            "代数攻击",
            "高保真复制"
        ],
        "涉及的技术概念": {
            "灰盒设置": "攻击者对目标模型有部分了解，如预训练的编码器结构，但不知道分类层的具体参数。",
            "代数攻击": "利用模型的代数特性，通过少量查询高效提取模型信息的技术。",
            "高保真复制": "指提取的模型在功能上与目标模型高度相似，能够产生几乎相同的预测结果。"
        },
        "success": true
    },
    {
        "order": 457,
        "title": "Grid-Functioned Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9297",
        "abstract": "We introduce a new neural network architecture that we call 'grid-functioned' neural networks. It utilises a grid structure of network parameterisations that can be specialised for different subdomains of the problem, while maintaining smooth, continuous behaviour. The grid gives the user flexibility to prevent gross features from overshadowing important minor ones. We present a full characterisation of its computational and spatial complexity, and demonstrate its potential, compared to a traditional architecture, over a set of synthetic regression problems. We further illustrate the benefits through a real-world 3D skeletal animation case study, where it offers the same visual quality as a state-of-the-art model, but with lower computational complexity and better control accuracy.",
        "conference": "ICML",
        "中文标题": "网格功能神经网络",
        "摘要翻译": "我们介绍了一种新的神经网络架构，称之为'网格功能'神经网络。它利用网络参数化的网格结构，可以针对问题的不同子域进行专门化，同时保持平滑、连续的行为。网格为用户提供了灵活性，以防止主要特征掩盖重要的次要特征。我们全面描述了其计算和空间复杂性，并通过一组合成回归问题，与传统架构相比，展示了其潜力。我们通过一个真实世界的3D骨骼动画案例研究进一步说明了其优势，该架构提供了与最先进模型相同的视觉质量，但具有更低的计算复杂度和更好的控制精度。",
        "领域": "神经网络架构设计, 3D动画技术, 回归分析",
        "问题": "如何设计一种灵活的神经网络架构，既能针对问题的不同子域进行专门化，又能保持平滑连续的行为，同时避免主要特征掩盖次要特征。",
        "动机": "为了解决传统神经网络在处理具有不同子域特性问题时缺乏灵活性和效率的问题，以及提高模型在保持高质量输出的同时降低计算复杂度。",
        "方法": "提出了一种名为'网格功能'的新型神经网络架构，利用网格结构对网络参数进行专门化，同时确保行为的平滑性和连续性，并通过合成回归问题和3D骨骼动画案例验证其有效性。",
        "关键词": [
            "网格功能神经网络",
            "网络参数化",
            "3D骨骼动画",
            "回归分析",
            "计算复杂度"
        ],
        "涉及的技术概念": {
            "网格结构": "用于网络参数化的网格结构，允许针对问题的不同子域进行专门化，同时保持平滑连续的行为。",
            "计算和空间复杂性": "全面描述了网格功能神经网络的计算和空间复杂性，展示了其在保持性能的同时降低复杂度的潜力。",
            "3D骨骼动画": "作为真实世界的应用案例，展示了网格功能神经网络在提供高质量视觉输出的同时，具有更低的计算复杂度和更好的控制精度。"
        },
        "success": true
    },
    {
        "order": 458,
        "title": "Grounding Language to Entities and Dynamics for Generalization in Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9965",
        "abstract": "We investigate the use of natural language to drive the generalization of control policies and introduce the new multi-task environment Messenger with free-form text manuals describing the environment dynamics. Unlike previous work, Messenger does not assume prior knowledge connecting text and state observations — the control policy must simultaneously ground the game manual to entity symbols and dynamics in the environment. We develop a new model, EMMA (Entity Mapper with Multi-modal Attention) which uses an entity-conditioned attention module that allows for selective focus over relevant descriptions in the manual for each entity in the environment. EMMA is end-to-end differentiable and learns a latent grounding of entities and dynamics from text to observations using only environment rewards. EMMA achieves successful zero-shot generalization to unseen games with new dynamics, obtaining a 40% higher win rate compared to multiple baselines. However, win rate on the hardest stage of Messenger remains low (10%), demonstrating the need for additional work in this direction.",
        "conference": "ICML",
        "中文标题": "通过语言与实体及动态的关联实现强化学习的泛化",
        "摘要翻译": "我们研究了利用自然语言驱动控制策略的泛化，并引入了新的多任务环境Messenger，该环境包含描述环境动态的自由格式文本手册。与之前的工作不同，Messenger不假设文本和状态观察之间存在先验知识——控制策略必须同时将游戏手册中的内容与环境中的实体符号和动态关联起来。我们开发了一个新模型EMMA（带有多模态注意力的实体映射器），它使用了一个以实体为条件的注意力模块，允许对环境中每个实体相关的手册描述进行选择性聚焦。EMMA是端到端可微分的，并且仅使用环境奖励从文本到观察学习实体和动态的潜在关联。EMMA成功实现了对新动态未见游戏的零样本泛化，与多个基线相比，获胜率提高了40%。然而，在Messenger最难的阶段，获胜率仍然较低（10%），表明在这一方向上还需要进一步的工作。",
        "领域": "自然语言处理与视觉结合、强化学习、多模态学习",
        "问题": "如何利用自然语言描述来驱动控制策略在未见过的游戏动态中的泛化。",
        "动机": "探索自然语言描述与控制策略之间的关联，以实现强化学习模型在新环境中的零样本泛化。",
        "方法": "开发了EMMA模型，该模型通过实体条件注意力模块选择性地聚焦于手册中与环境中每个实体相关的描述，实现文本到观察的实体和动态潜在关联学习。",
        "关键词": [
            "自然语言处理",
            "强化学习",
            "多模态学习",
            "零样本泛化",
            "实体映射"
        ],
        "涉及的技术概念": {
            "实体条件注意力模块": "允许模型选择性地聚焦于手册中与环境中每个实体相关的描述，以提高模型对文本信息的理解和应用能力。",
            "端到端可微分": "EMMA模型能够直接从输入到输出进行训练，无需手动设计中间表示，提高了模型的灵活性和效率。",
            "零样本泛化": "模型能够在没有直接训练数据的情况下，通过理解自然语言描述来适应新的游戏动态，展示了强大的泛化能力。"
        },
        "success": true
    },
    {
        "order": 459,
        "title": "Group Fisher Pruning for Practical Network Compression",
        "html": "https://ICML.cc//virtual/2021/poster/9875",
        "abstract": "Network compression has been widely studied since it is able to reduce the memory and computation cost during inference. However, previous methods seldom deal with complicated structures like residual connections, group/depth-wise convolution and feature pyramid network, where channels of multiple layers are coupled and need to be pruned simultaneously. In this paper, we present a general channel pruning approach that can be applied to various complicated structures. Particularly, we propose a layer grouping algorithm to find coupled channels automatically. Then we derive a unified metric based on Fisher information to evaluate the importance of a single channel and coupled channels. Moreover, we find that inference speedup on GPUs is more correlated with the reduction of memory rather than FLOPs, and thus we employ the memory reduction of each channel to normalize the importance. Our method can be used to prune any structures including those with coupled channels. We conduct extensive experiments on various backbones, including the classic ResNet and ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image classification and object detection which is under-explored. Experimental results validate that our method can effectively prune sophisticated networks, boosting inference speed without sacrificing accuracy.",
        "conference": "ICML",
        "中文标题": "群组费舍尔剪枝用于实用网络压缩",
        "摘要翻译": "网络压缩已被广泛研究，因为它能够减少推理过程中的内存和计算成本。然而，先前的方法很少处理复杂的结构，如残差连接、群组/深度卷积和特征金字塔网络，其中多个层的通道是耦合的，需要同时进行剪枝。在本文中，我们提出了一种通用的通道剪枝方法，可以应用于各种复杂结构。特别是，我们提出了一种层分组算法来自动找到耦合的通道。然后，我们基于费舍尔信息导出了一个统一的度量标准，以评估单个通道和耦合通道的重要性。此外，我们发现GPU上的推理加速与内存的减少更相关，而不是FLOPs，因此我们利用每个通道的内存减少来归一化重要性。我们的方法可以用于剪枝任何结构，包括那些具有耦合通道的结构。我们在各种骨干网络上进行了广泛的实验，包括经典的ResNet和ResNeXt、移动友好的MobileNetV2以及基于NAS的RegNet，涵盖了图像分类和目标检测，后者研究较少。实验结果验证了我们的方法可以有效地剪枝复杂网络，在不牺牲准确性的情况下提升推理速度。",
        "领域": "网络压缩、深度学习优化、计算机视觉",
        "问题": "如何在保持模型性能的同时，有效地剪枝复杂网络结构中的耦合通道，以提升推理速度。",
        "动机": "现有的网络压缩方法在处理具有耦合通道的复杂结构时效果不佳，需要一种能够自动识别并剪枝这些耦合通道的通用方法。",
        "方法": "提出了一种基于费舍尔信息的通用通道剪枝方法，包括层分组算法自动识别耦合通道，以及利用内存减少归一化通道重要性的策略。",
        "关键词": [
            "网络剪枝",
            "费舍尔信息",
            "耦合通道",
            "推理加速",
            "内存优化"
        ],
        "涉及的技术概念": {
            "费舍尔信息": "用于评估通道重要性的统一度量标准，帮助识别对模型性能影响最大的通道。",
            "层分组算法": "自动识别网络中耦合的通道，使得这些通道可以被同时剪枝，保持网络结构的完整性。",
            "内存减少归一化": "通过考虑每个通道对内存使用的影响来调整其重要性评分，优化推理速度。"
        },
        "success": true
    },
    {
        "order": 460,
        "title": "Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings",
        "html": "https://ICML.cc//virtual/2021/poster/8927",
        "abstract": "Sparse regression has recently been applied to enable transfer learning from very limited data. We study an extension of this approach to unsupervised learning---in particular, learning word embeddings from unstructured text corpora using low-rank matrix factorization. Intuitively, when transferring word embeddings to a new domain, we expect that the embeddings change for only a small number of words---e.g., the ones with novel meanings in that domain. We propose a novel group-sparse penalty that exploits this sparsity to perform transfer learning when there is very little text data available in the target domain---e.g., a single article of text. We prove generalization bounds for our algorithm. Furthermore, we empirically evaluate its effectiveness, both in terms of prediction accuracy in downstream tasks as well as in terms of interpretability of the results.",
        "conference": "ICML",
        "中文标题": "用于词嵌入迁移学习的群稀疏矩阵分解",
        "摘要翻译": "稀疏回归最近被应用于从非常有限的数据中进行迁移学习。我们研究了将这种方法扩展到无监督学习——特别是使用低秩矩阵分解从非结构化文本语料库中学习词嵌入。直观地说，当将词嵌入迁移到新领域时，我们预计只有少量词的嵌入会发生变化——例如，那些在该领域有新含义的词。我们提出了一种新颖的群稀疏惩罚，利用这种稀疏性在目标领域可用文本数据非常少的情况下（例如，一篇文章的文本）进行迁移学习。我们为我们的算法证明了泛化界限。此外，我们实证评估了其有效性，无论是在下游任务的预测准确性方面，还是在结果的可解释性方面。",
        "领域": "自然语言处理与视觉结合",
        "问题": "如何在目标领域可用文本数据非常少的情况下进行有效的词嵌入迁移学习",
        "动机": "探索在无监督学习环境下，特别是从非结构化文本语料库中学习词嵌入时，如何利用稀疏性进行有效的迁移学习",
        "方法": "提出了一种新颖的群稀疏惩罚方法，用于在目标领域文本数据极少的情况下进行词嵌入的迁移学习",
        "关键词": [
            "群稀疏矩阵分解",
            "迁移学习",
            "词嵌入",
            "无监督学习",
            "稀疏回归"
        ],
        "涉及的技术概念": {
            "群稀疏矩阵分解": "一种利用群稀疏性进行矩阵分解的技术，用于在迁移学习中优化词嵌入的表示",
            "迁移学习": "将从一个领域学到的知识应用到另一个相关领域的技术，本文中用于词嵌入的跨领域应用",
            "词嵌入": "将词汇表中的单词映射到实数向量的技术，用于捕捉单词之间的语义关系"
        },
        "success": true
    },
    {
        "order": 461,
        "title": "Guarantees for Tuning the Step Size using a Learning-to-Learn Approach",
        "html": "https://ICML.cc//virtual/2021/poster/9003",
        "abstract": "Choosing the right parameters for optimization algorithms is often the key to their success in practice. Solving this problem using a learning-to-learn approach---using meta-gradient descent on a meta-objective based on the trajectory that the optimizer generates---was recently shown to be effective. However, the meta-optimization problem is difficult. In particular, the meta-gradient can often explode/vanish, and the learned optimizer may not have good generalization performance if the meta-objective is not chosen carefully. In this paper we give meta-optimization guarantees for the learning-to-learn approach on a simple problem of tuning the step size for quadratic loss. Our results show that the na\\'ive objective suffers from meta-gradient explosion/vanishing problem. Although there is a way to design the meta-objective so that the meta-gradient remains polynomially bounded, computing the meta-gradient directly using backpropagation leads to numerical issues. We also characterize when it is necessary to compute the meta-objective on a separate validation set to ensure the generalization performance of the learned optimizer. Finally, we verify our results empirically and show that a similar phenomenon appears even for more complicated learned optimizers parametrized by neural networks.",
        "conference": "ICML",
        "中文标题": "使用学习到学习方法调整步长的保证",
        "摘要翻译": "为优化算法选择正确的参数通常是其在实践中成功的关键。最近研究表明，采用学习到学习的方法——即基于优化器生成的轨迹对元目标使用元梯度下降——可以有效解决这一问题。然而，元优化问题颇具挑战性。特别是，元梯度经常会出现爆炸或消失的问题，且如果元目标选择不当，学习到的优化器可能无法展现出良好的泛化性能。本文针对二次损失函数调整步长这一简单问题，给出了学习到学习方法的元优化保证。我们的结果表明，朴素目标会遭受元梯度爆炸或消失的问题。尽管存在设计元目标以使元梯度保持多项式有界的方法，但直接使用反向传播计算元梯度会导致数值问题。我们还描述了何时需要在单独的验证集上计算元目标，以确保学习到的优化器的泛化性能。最后，我们通过实验验证了我们的结果，并表明即使对于由神经网络参数化的更复杂的学习优化器，也会出现类似的现象。",
        "领域": "优化算法、元学习、深度学习优化",
        "问题": "如何有效选择和调整优化算法的参数，特别是步长，以提高算法的性能和泛化能力。",
        "动机": "解决元优化过程中元梯度爆炸或消失的问题，以及确保学习到的优化器具有良好的泛化性能。",
        "方法": "采用学习到学习的方法，通过元梯度下降调整步长，并对元优化问题提供理论保证。",
        "关键词": [
            "元学习",
            "优化算法",
            "步长调整",
            "元梯度",
            "泛化性能"
        ],
        "涉及的技术概念": {
            "元学习": "通过元梯度下降方法学习如何调整优化算法的参数，特别是步长。",
            "元梯度": "在元学习过程中用于更新优化器参数的梯度，其稳定性对学习效果至关重要。",
            "泛化性能": "学习到的优化器在未见数据上的表现能力，需要通过精心设计的元目标和验证策略来保证。"
        },
        "success": true
    },
    {
        "order": 462,
        "title": "Guided Exploration with Proximal Policy Optimization using a Single Demonstration",
        "html": "https://ICML.cc//virtual/2021/poster/10255",
        "abstract": "Solving sparse reward tasks through exploration is one of the major challenges in deep reinforcement learning, especially in three-dimensional, partially-observable environments. Critically, the algorithm proposed in this article is capable of using a single human demonstration to solve hard-exploration problems. We train an agent on a combination of demonstrations and own experience to solve  problems with variable initial conditions and we integrate it with proximal policy optimization (PPO). The agent is also able to increase its performance and to tackle harder problems by replaying its own past trajectories prioritizing them based on the obtained reward and the maximum value of the trajectory.\nWe finally compare variations of this algorithm to different imitation learning algorithms on a set of hard-exploration tasks in the Animal-AI Olympics environment.\nTo the best of our knowledge, learning a task in a three-dimensional environment with comparable difficulty has never been considered before using only one human demonstration.",
        "conference": "ICML",
        "中文标题": "使用单一示范引导探索的近端策略优化",
        "摘要翻译": "通过探索解决稀疏奖励任务是深度强化学习中的主要挑战之一，尤其是在三维、部分可观测的环境中。重要的是，本文提出的算法能够利用单一的人类示范来解决难以探索的问题。我们训练一个智能体结合示范和自身经验来解决具有可变初始条件的问题，并将其与近端策略优化（PPO）集成。该智能体还能够通过重放其过去的轨迹，并根据获得的奖励和轨迹的最大值来优先处理它们，从而提高其性能并解决更难的问题。最后，我们在Animal-AI奥运会环境中的一组难以探索的任务上，比较了该算法的变体与不同的模仿学习算法。据我们所知，仅使用一次人类示范来学习在难度相当的三维环境中的任务，此前从未被考虑过。",
        "领域": "深度强化学习、模仿学习、三维环境导航",
        "问题": "解决在三维、部分可观测环境中稀疏奖励任务的探索难题",
        "动机": "利用单一人类示范来解决难以探索的问题，提高智能体在复杂环境中的学习效率和性能",
        "方法": "结合示范和自身经验训练智能体，集成近端策略优化（PPO），并通过重放和优先处理过去轨迹来提高性能",
        "关键词": [
            "近端策略优化",
            "模仿学习",
            "稀疏奖励",
            "三维环境",
            "单一示范"
        ],
        "涉及的技术概念": {
            "近端策略优化（PPO）": "一种强化学习算法，用于优化策略以在复杂环境中实现高效学习",
            "模仿学习": "通过模仿专家示范来学习任务，减少探索的难度",
            "稀疏奖励": "在任务中奖励信号稀少，增加了学习的难度"
        },
        "success": true
    },
    {
        "order": 463,
        "title": "HardCoRe-NAS: Hard Constrained diffeRentiable Neural Architecture Search",
        "html": "https://ICML.cc//virtual/2021/poster/8443",
        "abstract": "Realistic use of neural networks often requires adhering to multiple constraints on latency, energy and memory among others.\nA popular approach to find fitting networks is through constrained Neural Architecture Search (NAS), however, previous methods enforce the constraint only softly. \nTherefore,  the resulting networks do not exactly adhere to the resource constraint and their accuracy is harmed.\nIn this work we resolve this by introducing Hard Constrained diffeRentiable NAS (HardCoRe-NAS), that is based on an accurate formulation of the expected resource requirement and a scalable search method that satisfies the hard constraint throughout the search.\nOur experiments show that HardCoRe-NAS generates state-of-the-art architectures, surpassing other NAS methods, while strictly satisfying the hard resource constraints without any tuning required.",
        "conference": "ICML",
        "中文标题": "HardCoRe-NAS: 硬约束可微分神经架构搜索",
        "摘要翻译": "神经网络的实际应用往往需要满足延迟、能耗和内存等多方面的约束条件。寻找合适网络的一种流行方法是通过约束神经架构搜索（NAS），然而，以往的方法仅以软方式执行约束。因此，生成的网络并不完全遵守资源约束，其准确性也受到影响。在这项工作中，我们通过引入硬约束可微分NAS（HardCoRe-NAS）来解决这一问题，该方法基于对预期资源需求的准确表述和一种可扩展的搜索方法，在整个搜索过程中满足硬约束。我们的实验表明，HardCoRe-NAS生成的架构达到了最先进的水平，超越了其他NAS方法，同时严格满足硬资源约束，无需任何调整。",
        "领域": "神经架构搜索、深度学习优化、资源约束优化",
        "问题": "如何在神经架构搜索中严格满足资源约束条件",
        "动机": "解决现有神经架构搜索方法仅能软性满足资源约束，导致生成的网络不完全遵守约束且准确性受损的问题",
        "方法": "引入硬约束可微分NAS（HardCoRe-NAS），基于准确预期资源需求表述和可扩展搜索方法，确保搜索过程中严格满足硬约束",
        "关键词": [
            "神经架构搜索",
            "硬约束",
            "可微分搜索",
            "资源优化",
            "深度学习"
        ],
        "涉及的技术概念": {
            "硬约束可微分NAS（HardCoRe-NAS）": "一种新型神经架构搜索方法，通过准确建模资源需求和可扩展搜索策略，确保生成的网络严格满足资源约束",
            "资源约束": "在神经架构搜索过程中对网络延迟、能耗和内存等方面的限制条件",
            "可微分搜索方法": "一种允许通过梯度下降优化网络架构的搜索技术，提高了搜索效率和效果"
        },
        "success": true
    },
    {
        "order": 464,
        "title": "HAWQ-V3: Dyadic Neural Network Quantization",
        "html": "https://ICML.cc//virtual/2021/poster/10099",
        "abstract": "Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQ-V3, a novel mixed-precision integer-only quantization framework. The contributions of HAWQ-V3 are the following: (i) An integer-only inference where the entire computational graph is performed only with integer multiplication, addition, and bit shifting, without any floating point operations or even integer division; (ii) A novel hardware-aware mixed-precision quantization method where the bit-precision is calculated by solving an integer linear programming problem that balances the trade-off between model perturbation and other constraints, e.g., memory footprint and latency; (iii) Direct hardware deployment and open source contribution for 4-bit uniform/mixed-precision quantization in TVM, achieving an average speed up of 1.45x for uniform 4-bit, as compared to uniform 8-bit for ResNet50 on T4 GPUs; and (iv) extensive evaluation of the proposed methods on ResNet18/50 and InceptionV3, for various model compression levels with/without mixed precision. For ResNet50, our INT8 quantization achieves an accuracy of 77.58%, which is 2.68% higher than prior integer-only work, and our mixed-precision INT4/8 quantization can reduce INT8 latency by 23% and still achieve 76.73% accuracy. Our framework and the TVM implementation have been open sourced (HAWQ, 2020).",
        "conference": "ICML",
        "中文标题": "HAWQ-V3：动态神经网络量化",
        "摘要翻译": "当前的低精度量化算法常常隐藏着从浮点到量化整数值来回转换的成本。这一隐藏成本限制了通过量化神经网络实现的延迟改进。为了解决这个问题，我们提出了HAWQ-V3，一种新颖的混合精度仅整数量化框架。HAWQ-V3的贡献如下：（i）一种仅整数的推理，其中整个计算图仅通过整数乘法、加法和位移操作完成，无需任何浮点操作甚至整数除法；（ii）一种新颖的硬件感知混合精度量化方法，其中位精度通过解决一个整数线性规划问题来计算，该问题平衡了模型扰动与其他约束（如内存占用和延迟）之间的权衡；（iii）在TVM中直接硬件部署和开源贡献，用于4位均匀/混合精度量化，与T4 GPU上的ResNet50的均匀8位相比，均匀4位平均加速1.45倍；（iv）对ResNet18/50和InceptionV3在各种模型压缩级别（有/无混合精度）上提出的方法进行了广泛评估。对于ResNet50，我们的INT8量化实现了77.58%的准确率，比之前的仅整数工作高2.68%，而我们的混合精度INT4/8量化可以减少INT8延迟23%，同时仍达到76.73%的准确率。我们的框架和TVM实现已经开源（HAWQ，2020）。",
        "领域": "神经网络量化、模型压缩、硬件加速",
        "问题": "解决低精度量化算法中隐藏的浮点到整数转换成本问题，以提高神经网络量化的延迟改进。",
        "动机": "当前量化方法在实现延迟改进方面存在限制，主要由于隐藏的浮点到整数的转换成本。",
        "方法": "提出HAWQ-V3，一种混合精度仅整数量化框架，包括仅整数推理、硬件感知混合精度量化方法、直接硬件部署和开源实现。",
        "关键词": [
            "神经网络量化",
            "混合精度",
            "硬件加速",
            "模型压缩",
            "整数线性规划"
        ],
        "涉及的技术概念": {
            "仅整数推理": "整个计算图仅通过整数乘法、加法和位移操作完成，无需浮点操作或整数除法，以减少延迟。",
            "硬件感知混合精度量化": "通过解决整数线性规划问题来计算位精度，平衡模型扰动与内存占用和延迟等约束。",
            "TVM实现": "在TVM中实现4位均匀/混合精度量化的直接硬件部署，以提高执行效率。"
        },
        "success": true
    },
    {
        "order": 465,
        "title": "HEMET: A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture",
        "html": "https://ICML.cc//virtual/2021/poster/10389",
        "abstract": "Recently Homomorphic Encryption (HE) is used to implement Privacy-Preserving Neural Networks (PPNNs) that perform inferences directly on encrypted data without decryption. Prior PPNNs adopt mobile network architectures such as SqueezeNet for smaller computing overhead, but we find na\\'ively using mobile network architectures for a PPNN does not necessarily achieve shorter inference latency. Despite having less parameters, a mobile network architecture typically introduces more layers and increases the HE multiplicative depth of a PPNN, thereby prolonging its inference latency. In this paper, we propose a \\textbf{HE}-friendly privacy-preserving \\textbf{M}obile neural n\\textbf{ET}work architecture, \\textbf{HEMET}. Experimental results show that, compared to state-of-the-art (SOTA) PPNNs, HEMET reduces the inference latency by $59.3\\%\\sim 61.2\\%$, and improves the inference accuracy by $0.4 \\% \\sim 0.5\\%$.",
        "conference": "ICML",
        "中文标题": "HEMET：一种同态加密友好的隐私保护移动神经网络架构",
        "摘要翻译": "最近，同态加密（HE）被用于实现隐私保护神经网络（PPNNs），这些网络可以直接在加密数据上进行推理而无需解密。先前的PPNNs采用了如SqueezeNet这样的移动网络架构以减少计算开销，但我们发现，简单地使用移动网络架构构建PPNN并不一定能实现更短的推理延迟。尽管参数较少，移动网络架构通常会引入更多的层数，增加PPNN的HE乘法深度，从而延长其推理延迟。在本文中，我们提出了一种HE友好的隐私保护移动神经网络架构HEMET。实验结果表明，与最先进的（SOTA）PPNNs相比，HEMET将推理延迟降低了59.3%至61.2%，并将推理准确率提高了0.4%至0.5%。",
        "领域": "隐私保护计算、同态加密应用、神经网络优化",
        "问题": "如何在保证隐私的前提下，减少同态加密神经网络（PPNNs）的推理延迟并提高准确率",
        "动机": "现有的PPNNs采用移动网络架构虽减少了计算开销，但由于增加了层数和HE乘法深度，反而延长了推理延迟，因此需要一种更高效的架构来解决这一问题",
        "方法": "提出了一种名为HEMET的同态加密友好的隐私保护移动神经网络架构，旨在减少推理延迟同时提高准确率",
        "关键词": [
            "同态加密",
            "隐私保护神经网络",
            "移动网络架构",
            "推理延迟优化",
            "HEMET"
        ],
        "涉及的技术概念": {
            "同态加密": "一种允许在加密数据上直接进行计算而无需先解密的加密技术，用于保护数据隐私",
            "隐私保护神经网络": "能够在加密数据上进行推理的神经网络，保护用户数据不被泄露",
            "HE乘法深度": "同态加密操作中乘法操作的次数，影响加密计算的效率和延迟"
        },
        "success": true
    },
    {
        "order": 466,
        "title": "Heterogeneity for the Win: One-Shot Federated Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/9973",
        "abstract": "In this work, we explore the unique challenges---and opportunities---of unsupervised federated learning (FL). We develop and analyze a one-shot federated clustering scheme, kfed, based on the widely-used Lloyd's method for $k$-means clustering. In contrast to many supervised problems, we show that the issue of statistical heterogeneity in federated networks can in fact benefit our analysis. We analyse kfed under a center separation assumption and compare it to the best known requirements of its centralized counterpart. Our analysis shows that in heterogeneous regimes where the number of clusters per device $(k')$ is smaller than the total number of clusters over the network $k$, $(k'\\le \\sqrt{k})$, we can use heterogeneity to our advantage---significantly weakening the cluster separation requirements for kfed. From a practical viewpoint, kfed also has many desirable properties: it requires only round of communication, can run asynchronously, and can handle partial participation or node/network failures. We motivate our analysis with experiments on common FL benchmarks, and highlight the practical utility of one-shot clustering through use-cases in personalized FL and device sampling.",
        "conference": "ICML",
        "success": true,
        "中文标题": "异质性致胜：一次性联邦聚类",
        "摘要翻译": "在这项工作中，我们探索了无监督联邦学习（FL）的独特挑战和机遇。我们基于广泛使用的Lloyd's方法为k-means聚类开发并分析了一种一次性联邦聚类方案kfed。与许多监督问题相比，我们展示了联邦网络中的统计异质性问题实际上可以有益于我们的分析。我们在中心分离假设下分析了kfed，并将其与其集中式对应物的最佳已知要求进行了比较。我们的分析表明，在异质性制度下，每个设备的聚类数（k'）小于网络中总聚类数k（k'≤√k），我们可以利用异质性——显著减弱kfed的聚类分离要求。从实际角度来看，kfed还具有许多理想的特性：它只需要一轮通信，可以异步运行，并且可以处理部分参与或节点/网络故障。我们通过在常见的FL基准上的实验来激发我们的分析，并通过个性化FL和设备抽样的用例突出了一次性聚类的实际效用。",
        "领域": "联邦学习, 无监督学习, 聚类分析",
        "问题": "解决在联邦学习环境中进行无监督聚类时的统计异质性问题",
        "动机": "探索如何利用联邦网络中的统计异质性来优化聚类分析，减少通信和计算资源的需求",
        "方法": "开发了一种基于Lloyd's方法的一次性联邦聚类方案kfed，分析其在异质性条件下的性能",
        "关键词": [
            "联邦学习",
            "无监督聚类",
            "一次性学习",
            "统计异质性",
            "k-means"
        ],
        "涉及的技术概念": {
            "联邦学习": "一种分布式机器学习方法，允许多个设备或服务器协作训练模型，同时保持数据本地化",
            "无监督聚类": "在没有标签的情况下，将数据分组到具有相似特征的类别中",
            "统计异质性": "指不同设备或数据源上的数据分布不同，这在联邦学习中是一个常见挑战，但在此研究中被转化为优势"
        }
    },
    {
        "order": 467,
        "title": "Heterogeneous Risk Minimization",
        "html": "https://ICML.cc//virtual/2021/poster/9881",
        "abstract": "Machine learning algorithms with empirical risk minimization usually suffer from poor generalization performance due to the greedy exploitation of correlations among the training data, which are not stable under distributional shifts. Recently, some invariant learning methods for out-of-distribution (OOD) generalization have been proposed by leveraging multiple training environments to find invariant relationships. However, modern datasets are frequently assembled by merging data from multiple sources without explicit source labels. The resultant unobserved heterogeneity renders many invariant learning methods inapplicable. In this paper, we propose Heterogeneous Risk Minimization (HRM) framework to achieve joint learning of latent heterogeneity among the data and invariant relationship, which leads to stable prediction despite distributional shifts. We theoretically characterize the roles of the environment labels in invariant learning and justify our newly proposed HRM framework. Extensive experimental results validate the effectiveness of our HRM framework.",
        "conference": "ICML",
        "中文标题": "异构风险最小化",
        "摘要翻译": "采用经验风险最小化的机器学习算法通常由于对训练数据间相关性的贪婪利用而遭受泛化性能不佳的问题，这些相关性在分布变化下并不稳定。最近，一些针对分布外（OOD）泛化的不变性学习方法被提出，通过利用多个训练环境来寻找不变关系。然而，现代数据集经常是通过合并来自多个来源的数据而组装的，没有明确的来源标签。由此产生的未观察到的异质性使得许多不变性学习方法不适用。在本文中，我们提出了异构风险最小化（HRM）框架，以实现对数据中潜在异质性和不变关系的联合学习，从而在分布变化下实现稳定预测。我们从理论上描述了环境标签在不变性学习中的作用，并证明了我们新提出的HRM框架的合理性。大量的实验结果验证了我们HRM框架的有效性。",
        "领域": "分布外泛化、不变性学习、机器学习",
        "问题": "解决在数据来源不明或未标记的情况下，机器学习模型因分布变化导致的泛化性能下降问题。",
        "动机": "现代数据集常合并自多个未标记来源，导致传统不变性学习方法无法应用，需要一种新方法来处理数据中的潜在异质性并学习不变关系。",
        "方法": "提出异构风险最小化（HRM）框架，联合学习数据中的潜在异质性和不变关系，以实现稳定预测。",
        "关键词": [
            "异构风险最小化",
            "分布外泛化",
            "不变性学习",
            "机器学习",
            "数据异质性"
        ],
        "涉及的技术概念": {
            "异构风险最小化（HRM）": "提出的新框架，旨在通过联合学习数据中的潜在异质性和不变关系，提高模型在分布变化下的泛化能力。",
            "分布外（OOD）泛化": "指模型在面对与训练数据分布不同的测试数据时的泛化能力，是本文研究的核心问题。",
            "不变性学习": "一种学习方法，旨在寻找在不同环境下保持不变的预测关系，以提高模型的泛化性能。"
        },
        "success": true
    },
    {
        "order": 468,
        "title": "'Hey, that's not an ODE': Faster ODE Adjoints via Seminorms",
        "html": "https://ICML.cc//virtual/2021/poster/10271",
        "abstract": "Neural differential equations may be trained by backpropagating gradients via the adjoint method, which is another differential equation typically solved using an adaptive-step-size numerical differential equation solver. A proposed step is accepted if its error, \\emph{relative to some norm}, is sufficiently small; else it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate that the particular structure of the adjoint equations makes the usual choices of norm (such as $L^2$) unnecessarily stringent. By replacing it with a more appropriate (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made faster. This requires only minor code modifications. \nExperiments on a wide range of tasks---including time series, generative modeling, and physical control---demonstrate a median improvement of 40\\% fewer function evaluations. On some problems we see as much as 62\\% fewer function evaluations, so that the overall training time is roughly halved.",
        "conference": "ICML",
        "中文标题": "嘿，那不是常微分方程：通过半范数加速ODE伴随计算",
        "摘要翻译": "神经微分方程可以通过伴随方法反向传播梯度进行训练，这通常需要使用自适应步长的数值微分方程求解器来解决另一个微分方程。如果一个步骤的误差相对于某个范数足够小，则该步骤被接受；否则，该步骤被拒绝，步长缩小，并重复该过程。在这里，我们证明了伴随方程的特定结构使得通常选择的范数（如L2范数）过于严格。通过用更合适的（半）范数替换它，可以减少不必要的步骤拒绝，从而加快反向传播速度。这只需要对代码进行较小的修改。在包括时间序列、生成建模和物理控制在内的广泛任务上的实验表明，函数评估次数中位数减少了40%。在某些问题上，我们看到函数评估次数减少了多达62%，因此整体训练时间大约减半。",
        "领域": "微分方程求解、深度学习优化、自适应步长控制",
        "问题": "解决伴随方法中因使用不适当的范数导致的过多步骤拒绝和计算效率低下的问题",
        "动机": "提高神经微分方程训练过程中伴随方法的计算效率，减少不必要的计算开销",
        "方法": "通过引入更适合伴随方程结构的半范数来替代传统的L2范数，减少步骤拒绝，从而加速反向传播过程",
        "关键词": [
            "神经微分方程",
            "伴随方法",
            "半范数",
            "自适应步长",
            "反向传播优化"
        ],
        "涉及的技术概念": {
            "伴随方法": "用于神经微分方程训练中梯度反向传播的技术，通过解决另一个微分方程来计算梯度",
            "半范数": "在本文中用于替代传统范数，以减少伴随方程求解过程中的步骤拒绝，提高计算效率",
            "自适应步长控制": "根据误差估计动态调整数值求解器的步长，以平衡计算精度和效率"
        },
        "success": true
    },
    {
        "order": 469,
        "title": "Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time",
        "html": "https://ICML.cc//virtual/2021/poster/9207",
        "abstract": "We study the widely-used hierarchical agglomerative clustering (HAC)\nalgorithm on edge-weighted graphs. We define an algorithmic framework\nfor hierarchical agglomerative graph clustering that provides the\nfirst efficient $\\tilde{O}(m)$ time exact algorithms for classic\nlinkage measures, such as complete- and WPGMA-linkage, as well as\nother measures. Furthermore, for average-linkage, arguably the most\npopular variant of HAC, we provide an algorithm that runs in\n$\\tilde{O}(n\\sqrt{m})$ time. For this variant, this is the first\nexact algorithm that runs in subquadratic time, as long as\n$m=n^{2-\\epsilon}$ for some constant $\\epsilon > 0$. We complement\nthis result with a simple $\\epsilon$-close approximation algorithm for\naverage-linkage in our framework that runs in $\\tilde{O}(m)$ time.\nAs an application of our algorithms, we consider clustering points in\na metric space by first using $k$-NN to generate a graph from the\npoint set, and then running our algorithms on the resulting weighted\ngraph. We validate the performance of our algorithms on publicly\navailable datasets, and show that our approach can speed up clustering\nof point datasets by a factor of 20.7--76.5x.",
        "conference": "ICML",
        "success": true,
        "中文标题": "近线性时间内的层次凝聚图聚类",
        "摘要翻译": "我们研究了在边加权图上广泛使用的层次凝聚聚类（HAC）算法。我们定义了一个层次凝聚图聚类的算法框架，该框架为经典的链接度量（如完全链接和WPGMA链接）以及其他度量提供了第一个高效的$\\tilde{O}(m)$时间精确算法。此外，对于平均链接——可以说是HAC最流行的变体，我们提供了一个在$\\tilde{O}(n\\sqrt{m})$时间内运行的算法。对于这种变体，这是第一个在次二次时间内运行的精确算法，只要$m=n^{2-\\epsilon}$对于某个常数$\\epsilon > 0$。我们通过一个简单的$\\epsilon$-接近近似算法来补充这一结果，该算法在我们的框架中运行时间为$\\tilde{O}(m)$。作为我们算法的一个应用，我们考虑通过首先使用$k$-NN从点集生成图，然后在得到的加权图上运行我们的算法来聚类度量空间中的点。我们在公开可用的数据集上验证了我们算法的性能，并表明我们的方法可以将点数据集的聚类速度提高20.7--76.5倍。",
        "领域": "图聚类, 层次聚类, 近似算法",
        "问题": "如何在近线性时间内高效执行层次凝聚图聚类，特别是对于流行的平均链接变体。",
        "动机": "解决现有层次凝聚聚类算法在处理大规模图数据时效率低下的问题，特别是在精确算法的时间复杂度上实现突破。",
        "方法": "提出了一个算法框架，为多种链接度量提供高效的精确算法，并为平均链接变体设计了首个次二次时间精确算法，同时提供了一个快速的近似算法。",
        "关键词": [
            "层次凝聚聚类",
            "图聚类",
            "近似算法",
            "时间复杂度",
            "k-NN"
        ],
        "涉及的技术概念": {
            "层次凝聚聚类（HAC）": "一种自底向上的聚类方法，通过逐步合并最相似的簇来构建聚类层次结构。"
        }
    },
    {
        "order": 470,
        "title": "Hierarchical Clustering of Data Streams: Scalable Algorithms and Approximation Guarantees",
        "html": "https://ICML.cc//virtual/2021/poster/10115",
        "abstract": "We investigate the problem of hierarchically clustering data streams containing metric data in R^d. We introduce a desirable invariance property for such algorithms, describe a general family of hyperplane-based methods enjoying this property, and analyze two scalable instances of this general family against recently popularized similarity/dissimilarity-based metrics for hierarchical clustering. We prove a number of new results related to the approximation ratios of these algorithms, improving in various ways over the literature on this subject. Finally, since our algorithms are principled but also very practical, we carry out an experimental comparison on both synthetic and real-world datasets showing competitive results against known baselines.",
        "conference": "ICML",
        "中文标题": "数据流的层次聚类：可扩展算法与近似保证",
        "摘要翻译": "我们研究了包含R^d中度量数据的数据流的层次聚类问题。我们为此类算法引入了一个理想的不变性属性，描述了一个基于超平面的通用方法家族，该家族享有此属性，并分析了这一通用家族的两个可扩展实例，针对最近流行的基于相似性/不相似性的层次聚类度量。我们证明了与这些算法的近似比率相关的若干新结果，在多个方面改进了该主题的现有文献。最后，由于我们的算法既有原则性又非常实用，我们在合成和真实世界的数据集上进行了实验比较，显示出与已知基线相比具有竞争力的结果。",
        "领域": "数据挖掘、机器学习、大数据分析",
        "问题": "如何在数据流中有效地进行层次聚类，同时保证算法的可扩展性和近似质量",
        "动机": "解决数据流层次聚类中的算法可扩展性和近似保证问题，以应对大数据时代的需求",
        "方法": "引入基于超平面的通用方法家族，分析其两个可扩展实例，并通过实验验证其性能",
        "关键词": [
            "层次聚类",
            "数据流",
            "可扩展算法",
            "近似保证",
            "超平面方法"
        ],
        "涉及的技术概念": {
            "层次聚类": "一种将数据集分成不同层次的聚类方法，用于发现数据中的层次结构",
            "数据流": "连续不断产生的数据序列，需要实时或近实时处理",
            "超平面方法": "利用超平面将数据空间分割，以实现聚类的技术"
        },
        "success": true
    },
    {
        "order": 471,
        "title": "Hierarchical VAEs Know What They Don’t Know",
        "html": "https://ICML.cc//virtual/2021/poster/9665",
        "abstract": "Deep generative models have been demonstrated as state-of-the-art density estimators. Yet, recent work has found that they often assign a higher likelihood to data from outside the training distribution. This seemingly paradoxical behavior has caused concerns over the quality of the attained density estimates. In the context of hierarchical variational autoencoders, we provide evidence to explain this behavior by out-of-distribution data having in-distribution low-level features. We argue that this is both expected and desirable behavior. With this insight in hand, we develop a fast, scalable and fully unsupervised likelihood-ratio score for OOD detection that requires data to be in-distribution across all feature-levels. We benchmark the method on a vast set of data and model combinations and achieve state-of-the-art results on out-of-distribution detection.",
        "conference": "ICML",
        "中文标题": "分层变分自编码器知道它们不知道什么",
        "摘要翻译": "深度生成模型已被证明是最先进的密度估计器。然而，最近的工作发现，它们往往会给训练分布之外的数据分配更高的似然。这种看似矛盾的行为引发了人们对所获得的密度估计质量的担忧。在分层变分自编码器的背景下，我们通过分布外数据具有分布内低级特征的证据来解释这种行为。我们认为这是既预期又理想的行为。有了这一见解，我们开发了一种快速、可扩展且完全无监督的似然比评分方法，用于分布外检测，要求数据在所有特征级别上都是分布内的。我们在大量数据和模型组合上对该方法进行了基准测试，并在分布外检测方面取得了最先进的结果。",
        "领域": "变分自编码器、异常检测、深度学习",
        "问题": "解释并解决深度生成模型在分布外数据上分配更高似然的矛盾行为",
        "动机": "理解并利用深度生成模型在分布外数据上的行为，以提高密度估计的质量和分布外检测的准确性",
        "方法": "开发了一种快速、可扩展且完全无监督的似然比评分方法，用于分布外检测，要求数据在所有特征级别上都是分布内的",
        "关键词": [
            "分层变分自编码器",
            "分布外检测",
            "似然比评分",
            "无监督学习",
            "密度估计"
        ],
        "涉及的技术概念": {
            "分层变分自编码器": "用于生成模型和密度估计的深度学习架构，通过分层结构捕捉数据的多层次特征",
            "分布外检测": "识别不符合训练数据分布的数据样本，是异常检测的一种形式",
            "似然比评分": "一种统计方法，用于比较两个统计模型的似然函数值，以评估数据样本的分布一致性"
        },
        "success": true
    },
    {
        "order": 472,
        "title": "High Confidence Generalization for Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9323",
        "abstract": "We present several classes of reinforcement learning algorithms that safely generalize to Markov decision processes (MDPs) not seen during training. Specifically, we study the setting in which some set of MDPs is accessible for training. The goal is to generalize safely to MDPs that are sampled from the same distribution, but which  may not be in the set accessible for training. For various definitions of safety, our algorithms give probabilistic guarantees that agents can safely generalize to MDPs that are sampled from the same distribution but are not necessarily in the training set. These algorithms are a type of Seldonian algorithm (Thomas et al., 2019), which is a class of machine learning algorithms that return models with probabilistic safety guarantees for user-specified definitions of safety.",
        "conference": "ICML",
        "中文标题": "强化学习的高置信度泛化",
        "摘要翻译": "我们提出了几类强化学习算法，这些算法能够安全地泛化到训练期间未见过的马尔可夫决策过程（MDPs）。具体来说，我们研究了这样一种设置：在训练过程中可以访问一组MDPs。目标是安全地泛化到从同一分布中采样但可能不在训练可访问集中的MDPs。对于安全的各种定义，我们的算法提供了概率保证，即智能体可以安全地泛化到从同一分布中采样但不一定在训练集中的MDPs。这些算法是一种Seldonian算法（Thomas等人，2019），这是一类机器学习算法，它们返回具有用户定义安全性的概率安全保证的模型。",
        "领域": "强化学习、马尔可夫决策过程、机器学习安全",
        "问题": "如何使强化学习算法能够安全地泛化到训练期间未见过的马尔可夫决策过程。",
        "动机": "研究动机是为了开发能够在未知环境中安全运行的强化学习算法，确保智能体在泛化到新环境时的安全性。",
        "方法": "采用Seldonian算法框架，提供概率安全保证，使强化学习算法能够安全地泛化到未见过的MDPs。",
        "关键词": [
            "强化学习",
            "马尔可夫决策过程",
            "安全泛化",
            "Seldonian算法",
            "概率安全保证"
        ],
        "涉及的技术概念": {
            "马尔可夫决策过程（MDPs）": "用于建模强化学习中的决策问题，其中智能体在环境中采取行动以达成目标。",
            "Seldonian算法": "一类机器学习算法，旨在提供模型的安全性和可靠性保证，特别是在泛化到新数据时。",
            "概率安全保证": "算法提供的保证，表明在特定概率下，智能体的行为将满足预定义的安全标准。"
        },
        "success": true
    },
    {
        "order": 473,
        "title": "High-dimensional Experimental Design and Kernel Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9741",
        "abstract": "In recent years methods from optimal linear experimental design have been leveraged to obtain state of the art results for linear bandits. \nA design returned from an objective such as G-optimal design is actually a probability distribution over a pool of potential measurement vectors. \nConsequently, one nuisance of the approach is the task of converting this continuous probability distribution into a discrete assignment of N measurements. \nWhile sophisticated rounding techniques have been proposed, in d dimensions they require N to be at least d, d log(log(d)), or d^2 based on the sub-optimality of the solution.\nIn this paper we are interested in settings where N may be much less than d, such as in experimental design in an RKHS where d may be effectively infinite.  \nIn this work, we propose a rounding procedure that frees N of any dependence on the dimension d, while achieving nearly the same performance guarantees of existing rounding procedures.\nWe evaluate the procedure against a baseline that projects the problem to a lower dimensional space and performs rounding there, which requires N to just be at least a notion of the effective dimension. We also leverage our new approach in a new algorithm for kernelized bandits to obtain state of the art results for regret minimization and pure exploration. \nAn advantage of our approach over existing UCB-like approaches is that our kernel bandit algorithms are provably robust to model misspecification. ",
        "conference": "ICML",
        "中文标题": "高维实验设计与核赌博机",
        "摘要翻译": "近年来，最优线性实验设计的方法被用来获得线性赌博机的最先进结果。从诸如G-最优设计等目标返回的设计实际上是潜在测量向量池上的概率分布。因此，这种方法的一个麻烦是将这个连续概率分布转换为N个测量的离散分配任务。虽然已经提出了复杂的舍入技术，但在d维空间中，它们要求N至少为d、d log(log(d))或d^2，基于解的次优性。在本文中，我们感兴趣的是N可能远小于d的设置，例如在RKHS中的实验设计，其中d可能实际上是无限的。在这项工作中，我们提出了一种舍入过程，使N摆脱对维度d的任何依赖，同时几乎达到现有舍入过程的相同性能保证。我们评估了该过程与一个基线的对比，该基线将问题投影到低维空间并在那里进行舍入，这要求N至少是一个有效维度的概念。我们还在一个新的核化赌博机算法中利用我们的新方法，以获得遗憾最小化和纯探索的最先进结果。与现有的类似UCB的方法相比，我们的方法的一个优势是我们的核赌博机算法被证明对模型错误指定具有鲁棒性。",
        "领域": "强化学习、核方法、高维统计",
        "问题": "在高维或无限维空间中，如何有效地将连续概率分布转换为离散测量分配，同时减少对维度d的依赖。",
        "动机": "解决在高维或无限维空间中进行实验设计时，现有方法对样本数量N的高维依赖问题，以及提高核赌博机算法的鲁棒性和性能。",
        "方法": "提出了一种新的舍入过程，减少了对维度d的依赖，并在核赌博机算法中应用这一方法，以提高性能。",
        "关键词": [
            "高维实验设计",
            "核赌博机",
            "舍入过程",
            "模型错误指定",
            "有效维度"
        ],
        "涉及的技术概念": {
            "G-最优设计": "一种实验设计方法，旨在最小化预测方差的最大值，用于生成测量向量的概率分布。",
            "舍入过程": "将连续概率分布转换为离散测量分配的技术，本文提出的方法减少了对维度d的依赖。",
            "核赌博机": "利用核方法处理高维或无限维空间中的赌博机问题，本文提出的算法提高了鲁棒性和性能。"
        },
        "success": true
    },
    {
        "order": 474,
        "title": "High-Dimensional Gaussian Process Inference with Derivatives",
        "html": "https://ICML.cc//virtual/2021/poster/9649",
        "abstract": "Although it is widely known that Gaussian processes can be conditioned on observations of the gradient, this functionality is of limited use due to the prohibitive computational cost of $\\mathcal{O}(N^3 D^3)$ in data points $N$ and dimension $D$.\nThe dilemma of gradient observations is that a single one of them comes at the same cost as $D$ independent function evaluations, so the latter are often preferred.\nCareful scrutiny reveals, however, that derivative observations give rise to highly structured kernel Gram matrices for very general classes of kernels (inter alia, stationary kernels).\nWe show that in the \\emph{low-data} regime $N<D$, the Gram matrix can be decomposed in a manner that reduces the cost of inference to $\\mathcal{O}(N^2D + (N^2)^3)$ (i.e.,~linear in the number of dimensions) and, in special cases, to $\\mathcal{O}(N^2D + N^3)$.\nThis reduction in complexity opens up new use-cases for inference with gradients especially in the high-dimensional regime, where the information-to-cost ratio of gradient observations significantly increases.\nWe demonstrate this potential in a variety of tasks relevant for machine learning, such as optimization and Hamiltonian Monte Carlo with predictive gradients.",
        "conference": "ICML",
        "success": true,
        "中文标题": "高维高斯过程导数推断",
        "摘要翻译": "尽管众所周知高斯过程可以基于梯度观测进行条件化，但由于在数据点N和维度D上的计算成本高达O(N^3 D^3)，这一功能的应用受到限制。梯度观测的困境在于，单个梯度观测的成本等同于D次独立函数评估的成本，因此后者往往更受青睐。然而，仔细研究发现，对于非常广泛的核类（尤其是平稳核），导数观测会导致高度结构化的核Gram矩阵。我们展示在低数据区域N<D时，Gram矩阵可以以一种方式分解，将推断成本降低到O(N^2D + (N^2)^3)（即，在维度数量上线性），在特殊情况下，甚至可以降低到O(N^2D + N^3)。这种复杂度的降低为梯度推断开辟了新的应用场景，特别是在高维区域，梯度观测的信息成本比显著提高。我们在机器学习相关的多种任务中展示了这一潜力，例如使用预测梯度的优化和哈密尔顿蒙特卡洛。",
        "领域": "高斯过程, 机器学习优化, 概率建模",
        "问题": "解决高维高斯过程在梯度观测下计算成本过高的问题",
        "动机": "探索在低数据区域下，通过结构化核Gram矩阵分解降低计算成本，扩展梯度观测在高维高斯过程推断中的应用",
        "方法": "在低数据区域N<D下，通过特定方式分解Gram矩阵，显著降低计算复杂度",
        "关键词": [
            "高斯过程",
            "梯度观测",
            "计算复杂度",
            "核Gram矩阵",
            "高维推断"
        ],
        "涉及的技术概念": {
            "高斯过程": "一种非参数化的概率模型，用于建模函数分布，广泛应用于机器学习中的回归和分类问题",
            "核Gram矩阵": "在高斯过程中，用于描述数据点之间相似性的矩阵，其结构对计算效率有重要影响",
            "梯度观测": "指在模型训练或推断过程中，利用目标函数的梯度信息来优化模型性能或提高推断效率"
        }
    },
    {
        "order": 475,
        "title": "High-Performance Large-Scale Image Recognition Without Normalization",
        "html": "https://ICML.cc//virtual/2021/poster/9957",
        "abstract": "Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations.\nIn this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%.\nIn addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when fine-tuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%.",
        "conference": "ICML",
        "中文标题": "无需归一化的高性能大规模图像识别",
        "摘要翻译": "批量归一化是大多数图像分类模型的关键组成部分，但由于其对批次大小和样本间交互的依赖，它具有许多不良特性。尽管最近的工作已经成功地在没有归一化层的情况下训练了深度残差网络，但这些模型无法与最佳批量归一化网络的测试准确度相匹配，并且在大学习率或强数据增强时往往不稳定。在这项工作中，我们开发了一种自适应梯度裁剪技术，克服了这些不稳定性，并设计了一类显著改进的无归一化残差网络。我们较小的模型在ImageNet上的测试准确度与EfficientNet-B7相匹配，同时训练速度提高了8.7倍，而我们最大的模型达到了86.5%的最新顶级准确度。此外，当在300万标记图像的大规模预训练后对ImageNet进行微调时，无归一化模型的性能显著优于其批量归一化的对应模型，我们的最佳模型获得了89.2%的准确度。",
        "领域": "图像分类、深度学习优化、大规模图像识别",
        "问题": "解决批量归一化在图像分类模型中因依赖批次大小和样本间交互而导致的不良特性和不稳定性问题",
        "动机": "开发一种无需归一化层即可训练高性能深度残差网络的方法，以克服批量归一化的限制",
        "方法": "开发自适应梯度裁剪技术并设计无归一化残差网络，以提高训练速度和模型准确度",
        "关键词": [
            "无归一化残差网络",
            "自适应梯度裁剪",
            "大规模图像识别",
            "图像分类",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "自适应梯度裁剪": "用于克服无归一化模型在大学习率或强数据增强时的不稳定性",
            "无归一化残差网络": "一类不依赖归一化层的深度残差网络，旨在提高训练效率和模型性能",
            "大规模预训练": "在大量标记图像上预训练模型，以提高在特定任务（如ImageNet分类）上的微调性能"
        },
        "success": true
    },
    {
        "order": 476,
        "title": "Homomorphic Sensing: Sparsity and Noise",
        "html": "https://ICML.cc//virtual/2021/poster/8505",
        "abstract": "\\emph{Unlabeled sensing} is a recent problem encompassing many data science and engineering applications and typically formulated as solving linear equations whose right-hand side vector has undergone an unknown permutation. It was generalized to the \\emph{homomorphic sensing} problem by replacing the unknown permutation with an unknown linear map from a given finite set of linear maps. In this paper we present tighter and simpler conditions for the homomorphic sensing problem to admit a unique solution. We show that this solution is locally stable under noise, while under a sparsity assumption it remains unique under less demanding conditions. Sparsity in the context of unlabeled sensing leads to the problem of \\textit{unlabeled compressed sensing}, and a consequence of our general theory is the existence under mild conditions of a unique sparsest solution. On the algorithmic level, we solve unlabeled compressed sensing by an iterative algorithm validated by synthetic data experiments. Finally, under the unifying homomorphic sensing framework we connect unlabeled sensing to other important practical problems.",
        "conference": "ICML",
        "中文标题": "同态感知：稀疏性与噪声",
        "摘要翻译": "无标签感知是近年来涵盖数据科学与工程应用的广泛问题，通常被表述为求解右端向量经过未知排列的线性方程组。通过将未知排列替换为来自给定有限线性映射集的未知线性映射，该问题被推广为同态感知问题。本文中，我们提出了更严格且更简单的条件，以确保同态感知问题具有唯一解。我们证明了该解在噪声下局部稳定，而在稀疏性假设下，该解在要求较低的条件下仍保持唯一。无标签感知中的稀疏性导致了无标签压缩感知问题，而我们一般理论的一个结果是在温和条件下存在唯一的最稀疏解。在算法层面，我们通过迭代算法解决了无标签压缩感知问题，并通过合成数据实验验证了该算法。最后，在同态感知的统一框架下，我们将无标签感知与其他重要的实际问题联系起来。",
        "领域": "信号处理、压缩感知、线性代数应用",
        "问题": "解决同态感知问题中唯一解的存在条件及其在噪声和稀疏性假设下的稳定性问题。",
        "动机": "为了在数据科学和工程应用中更有效地处理无标签感知问题，特别是在存在噪声和稀疏性条件下寻找唯一解的需求。",
        "方法": "提出了更严格且更简单的条件来确保同态感知问题的唯一解，并通过迭代算法解决无标签压缩感知问题。",
        "关键词": [
            "同态感知",
            "无标签感知",
            "压缩感知",
            "稀疏性",
            "噪声稳定性"
        ],
        "涉及的技术概念": {
            "同态感知": "将无标签感知问题推广为通过未知线性映射求解的问题，扩展了问题的应用范围。",
            "无标签压缩感知": "在无标签感知问题中引入稀疏性假设，导致的新问题，旨在寻找最稀疏的解。",
            "迭代算法": "用于解决无标签压缩感知问题的算法，通过迭代方法寻找解，并通过实验验证其有效性。"
        },
        "success": true
    },
    {
        "order": 477,
        "title": "HoroPCA: Hyperbolic Dimensionality Reduction via Horospherical Projections",
        "html": "https://ICML.cc//virtual/2021/poster/10723",
        "abstract": "This paper studies Principal Component Analysis (PCA) for data lying in hyperbolic spaces. Given directions, PCA relies on: (1) a parameterization of subspaces spanned by these directions, (2) a method of projection onto subspaces that preserves information in these directions, and (3) an objective to optimize, namely the variance explained by projections. We generalize each of these concepts to the hyperbolic space and propose HoroPCA, a method for hyperbolic dimensionality reduction. By focusing on the core problem of extracting principal directions, HoroPCA theoretically better preserves information in the original data such as distances, compared to previous generalizations of PCA. Empirically, we validate that HoroPCA outperforms existing dimensionality reduction methods, significantly reducing error in distance preservation. As a data whitening method, it improves downstream classification by up to 3.9% compared to methods that don’t use whitening. Finally, we show that HoroPCA can be used to visualize hyperbolic data in two dimensions.",
        "conference": "ICML",
        "中文标题": "HoroPCA：通过球面投影实现的双曲降维",
        "摘要翻译": "本文研究了针对双曲空间中数据的主成分分析（PCA）。给定方向，PCA依赖于：（1）由这些方向张成的子空间的参数化，（2）一种投影到子空间的方法，以保留这些方向上的信息，以及（3）一个优化目标，即投影解释的方差。我们将这些概念推广到双曲空间，并提出了HoroPCA，一种用于双曲降维的方法。通过专注于提取主方向的核心问题，与之前PCA的推广相比，HoroPCA在理论上更好地保留了原始数据中的信息，如距离。实证上，我们验证了HoroPCA优于现有的降维方法，显著减少了距离保持中的误差。作为一种数据白化方法，与不使用白化的方法相比，它将下游分类的准确率提高了高达3.9%。最后，我们展示了HoroPCA可以用于在二维空间中可视化双曲数据。",
        "领域": "双曲空间学习、降维技术、数据可视化",
        "问题": "如何在双曲空间中有效地进行数据降维，同时保留原始数据的关键信息如距离。",
        "动机": "现有的PCA方法在双曲空间中的应用有限，无法有效保留数据的关键信息，如距离，因此需要一种新的降维方法。",
        "方法": "提出HoroPCA方法，通过推广PCA的三个核心概念到双曲空间，实现更有效的数据降维和信息保留。",
        "关键词": [
            "双曲降维",
            "主成分分析",
            "数据白化",
            "距离保持",
            "数据可视化"
        ],
        "涉及的技术概念": {
            "双曲空间": "一种非欧几里得空间，用于建模具有特定几何特性的数据。",
            "主成分分析（PCA）": "一种统计方法，通过线性变换将数据转换到新的坐标系，使得任何投影数据的第一大方差在第一个坐标上，第二大方差在第二个坐标上，依此类推。",
            "数据白化": "一种预处理技术，旨在减少数据中的冗余，使得特征之间不相关，并且具有单位方差。"
        },
        "success": true
    },
    {
        "order": 478,
        "title": "Householder Sketch for Accurate and Accelerated Least-Mean-Squares Solvers",
        "html": "https://ICML.cc//virtual/2021/poster/9399",
        "abstract": "Least-Mean-Squares (\\textsc{LMS}) solvers comprise a class of fundamental optimization problems such as linear regression, and regularized regressions such as Ridge, LASSO, and Elastic-Net. Data summarization techniques for big data generate summaries called coresets and sketches to speed up model learning under streaming and distributed settings. For example, \\citep{nips2019} design a fast and accurate Caratheodory set on input data to boost the performance of existing \\textsc{LMS} solvers. In retrospect, we explore classical Householder transformation as a candidate for sketching and accurately solving LMS problems. We find it to be a simpler, memory-efficient, and faster alternative that always existed to the above strong baseline. We also present a scalable algorithm based on the construction of distributed Householder sketches to solve \\textsc{LMS} problem across multiple worker nodes. We perform thorough empirical analysis with large synthetic and real datasets to evaluate the performance of Householder sketch and compare with \\citep{nips2019}. Our results show Householder sketch speeds up existing \\textsc{LMS} solvers in the scikit-learn library up to $100$x-$400$x. Also, it is $10$x-$100$x faster than the above baseline with similar numerical stability. The distributed algorithm demonstrates linear scalability with a near-negligible communication overhead.",
        "conference": "ICML",
        "success": true,
        "中文标题": "用于精确和加速最小均方求解器的豪斯霍尔德草图",
        "摘要翻译": "最小均方 (LMS) 求解器包含一类基本的优化问题，例如线性回归，以及正则化回归，如岭回归、LASSO 和 Elastic-Net。大数据的数据摘要技术生成称为核心集和草图的摘要，以加速流式和分布式设置下的模型学习。例如，[nips2019] 在输入数据上设计了一个快速且准确的 Caratheodory 集，以提高现有 LMS 求解器的性能。回顾过去，我们探索了经典的豪斯霍尔德变换，作为草图绘制和准确解决 LMS 问题的候选方案。我们发现它是一种更简单、内存效率更高且更快的替代方案，并且一直存在于上述强大的基线之上。我们还提出了一种基于构建分布式豪斯霍尔德草图的可扩展算法，以解决跨多个工作节点的 LMS 问题。我们使用大型合成和真实数据集进行了彻底的经验分析，以评估豪斯霍尔德草图的性能并与 [nips2019] 进行比较。我们的结果表明，豪斯霍尔德草图将 scikit-learn 库中现有的 LMS 求解器加速了高达 100 倍 - 400 倍。此外，它比上述基线快 10 倍 - 100 倍，并且具有相似的数值稳定性。分布式算法展示了线性可扩展性，且通信开销几乎可以忽略不计。",
        "领域": "线性回归, 大数据分析, 分布式计算",
        "问题": "如何在大数据环境下加速最小均方(LMS)求解器的计算，同时保持计算精度。",
        "动机": "现有的LMS求解器在大数据环境下计算效率低下，需要更快速、更节省内存的替代方案。现有Caratheodory集方法性能提升有限，因此探索更优的草图技术来提升LMS求解器的效率。",
        "方法": "采用豪斯霍尔德变换作为草图绘制方法，并设计了一种基于分布式豪斯霍尔德草图的可扩展算法，以在多个工作节点上解决LMS问题。通过实验分析验证了该方法在加速LMS求解器方面的有效性，并与现有方法进行了比较。",
        "关键词": [
            "豪斯霍尔德变换",
            "最小均方",
            "数据草图",
            "分布式算法",
            "线性回归"
        ],
        "涉及的技术概念": {
            "豪斯霍尔德变换": "一种正交变换，用于将向量变换为具有特定形式的向量，这里用于数据降维和草图绘制，降低计算复杂度。",
            "最小均方(LMS)": "一种迭代自适应滤波算法，用于估计未知参数，这里用于线性回归等优化问题。"
        }
    },
    {
        "order": 479,
        "title": "How and Why to Use Experimental Data to Evaluate Methods for Observational Causal Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9159",
        "abstract": "Methods that infer causal dependence from observational data are central to many areas of science, including medicine, economics, and the social sciences.  A variety of theoretical properties of these methods have been proven, but empirical evaluation remains a challenge, largely due to the lack of observational data sets for which treatment effect is known.  We describe and analyze observational sampling from randomized controlled trials (OSRCT), a method for evaluating causal inference methods using data from randomized controlled trials (RCTs). This method can be used to create constructed observational data sets with corresponding unbiased estimates of treatment effect, substantially increasing the number of data sets available for evaluating causal inference methods.  We show that, in expectation, OSRCT creates data sets that are equivalent to those produced by randomly sampling from empirical data sets in which all potential outcomes are available.  We then perform a large-scale evaluation of seven causal inference methods over 37 data sets, drawn from RCTs, as well as simulators, real-world computational systems, and observational data sets augmented with a synthetic response variable.  We find notable performance differences when comparing across data from different sources, demonstrating the importance of using data from a variety of sources when evaluating any causal inference method.",
        "conference": "ICML",
        "中文标题": "如何及为何使用实验数据评估观察性因果推断方法",
        "摘要翻译": "从观察性数据推断因果依赖的方法对许多科学领域至关重要，包括医学、经济学和社会科学。这些方法的多种理论性质已被证明，但实证评估仍然是一个挑战，主要原因是缺乏已知治疗效果的观察性数据集。我们描述并分析了从随机对照试验中进行观察性抽样（OSRCT），这是一种使用随机对照试验（RCTs）数据评估因果推断方法的方法。该方法可用于创建具有相应无偏治疗效果估计的构造观察性数据集，显著增加可用于评估因果推断方法的数据集数量。我们表明，在期望中，OSRCT创建的数据集等同于从所有潜在结果可用的经验数据集中随机抽样产生的数据集。然后，我们对37个数据集中的七种因果推断方法进行了大规模评估，这些数据集来自RCTs，以及模拟器、真实世界计算系统和增加了合成响应变量的观察性数据集。我们发现，当比较来自不同来源的数据时，存在显著的性能差异，这证明了在评估任何因果推断方法时使用来自多种来源的数据的重要性。",
        "领域": "因果推断、医学统计、社会科学数据分析",
        "问题": "如何有效评估观察性因果推断方法的性能",
        "动机": "由于缺乏已知治疗效果的观察性数据集，评估因果推断方法的实证性能面临挑战",
        "方法": "提出并分析从随机对照试验中进行观察性抽样（OSRCT）的方法，创建具有无偏治疗效果估计的构造观察性数据集，并进行大规模方法评估",
        "关键词": [
            "因果推断",
            "随机对照试验",
            "观察性数据",
            "治疗效果评估",
            "实证评估"
        ],
        "涉及的技术概念": {
            "观察性抽样（OSRCT）": "从随机对照试验中创建观察性数据集的方法，用于评估因果推断方法",
            "无偏治疗效果估计": "在构造的观察性数据集中提供的治疗效果估计，用于准确评估因果推断方法",
            "潜在结果": "在因果推断中，指个体在接受不同治疗条件下的可能结果，用于理论分析和实证评估"
        },
        "success": true
    },
    {
        "order": 480,
        "title": "How could Neural Networks understand Programs?",
        "html": "https://ICML.cc//virtual/2021/poster/9087",
        "abstract": "Semantic understanding of programs is a fundamental problem for programming language processing (PLP). Recent works that learn representations of code based on pre-training techniques in NLP have pushed the frontiers in this direction. However, the semantics of PL and NL have essential differences. These being ignored, we believe it is difficult to build a model to better understand programs, by either directly applying off-the-shelf NLP pre-training techniques to the source code, or adding features to the model by the heuristic. In fact, the semantics of a program can be rigorously defined by formal semantics in PL theory. For example, the operational semantics, describes the meaning of a valid program as updating the environment (i.e., the memory address-value function) through fundamental operations, such as memory I/O and conditional branching. Inspired by this, we propose a novel program semantics learning paradigm, that the model should learn from information composed of (1) the representations which align well with the fundamental operations in operational semantics, and (2) the information of environment transition, which is indispensable for program understanding. To validate our proposal, we present a hierarchical Transformer-based pre-training model called OSCAR to better facilitate the understanding of programs. OSCAR learns from intermediate representation (IR) and an encoded representation derived from static analysis, which are used for representing the fundamental operations and approximating the environment transitions respectively. OSCAR empirically shows the outstanding capability of program semantics understanding on many practical software engineering tasks. Code and models are released at: \\url{https://github.com/pdlan/OSCAR}.",
        "conference": "ICML",
        "中文标题": "神经网络如何理解程序？",
        "摘要翻译": "程序语义理解是编程语言处理（PLP）中的一个基本问题。最近，基于自然语言处理（NLP）中预训练技术学习代码表示的工作推动了这一方向的前沿。然而，编程语言（PL）和自然语言（NL）的语义存在本质差异。如果忽略这些差异，我们认为无论是直接将现成的NLP预训练技术应用于源代码，还是通过启发式方法向模型添加特征，都难以构建一个更好地理解程序的模型。事实上，程序的语义可以通过PL理论中的形式语义严格定义。例如，操作语义描述了有效程序的含义，即通过基本操作（如内存I/O和条件分支）更新环境（即内存地址-值函数）。受此启发，我们提出了一种新颖的程序语义学习范式，即模型应从由（1）与操作语义中的基本操作良好对齐的表示和（2）环境转换信息（这对程序理解不可或缺）组成的信息中学习。为了验证我们的提议，我们提出了一个基于分层Transformer的预训练模型OSCAR，以更好地促进程序的理解。OSCAR从中级表示（IR）和源自静态分析的编码表示中学习，这些表示分别用于表示基本操作和近似环境转换。OSCAR在许多实际软件工程任务中展示了出色的程序语义理解能力。代码和模型发布于：https://github.com/pdlan/OSCAR。",
        "领域": "程序语义理解、编程语言处理、软件工程",
        "问题": "如何使神经网络更好地理解程序的语义",
        "动机": "现有的NLP预训练技术直接应用于源代码或通过启发式方法增强模型难以充分理解程序语义，因为编程语言与自然语言在语义上存在本质差异。",
        "方法": "提出了一种基于操作语义和环境转换信息的程序语义学习范式，并开发了一个分层Transformer预训练模型OSCAR，通过学习中级表示和静态分析编码表示来理解程序。",
        "关键词": [
            "程序语义理解",
            "操作语义",
            "分层Transformer",
            "预训练模型",
            "静态分析"
        ],
        "涉及的技术概念": {
            "操作语义": "描述了程序通过基本操作（如内存I/O和条件分支）更新环境（内存地址-值函数）的语义，是程序语义理解的基础。",
            "分层Transformer": "一种基于Transformer架构的预训练模型，用于从程序的中级表示和静态分析编码表示中学习，以理解程序语义。",
            "静态分析": "在不执行程序的情况下分析代码的技术，用于生成编码表示，近似环境转换信息，辅助程序理解。"
        },
        "success": true
    },
    {
        "order": 481,
        "title": "How Do Adam and Training Strategies Help BNNs Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/8613",
        "abstract": "The best performing Binary Neural Networks (BNNs) are usually attained using Adam optimization and its multi-step training variants. However, to the best of our knowledge, few studies explore the fundamental reasons why Adam is superior to other optimizers like SGD for BNN optimization or provide analytical explanations that support specific training strategies. To address this, in this paper we first investigate the trajectories of gradients and weights in BNNs during the training process. We show the regularization effect of second-order momentum in Adam is crucial to revitalize the weights that are dead due to the activation saturation in BNNs. We find that Adam, through its adaptive learning rate strategy, is better equipped to handle the rugged loss surface of BNNs and reaches a better optimum with higher generalization ability. Furthermore, we inspect the intriguing role of the real-valued weights in binary networks, and reveal the effect of weight decay on the stability and sluggishness of BNN optimization. Through extensive experiments and analysis, we derive a simple training scheme, building on existing Adam-based optimization, which achieves 70.5% top-1 accuracy on the ImageNet dataset using the same architecture as the state-of-the-art ReActNet while achieving 1.1% higher accuracy. Code and models are available at https://github.com/liuzechun/AdamBNN.",
        "conference": "ICML",
        "中文标题": "Adam与训练策略如何助力二值神经网络的优化",
        "摘要翻译": "性能最佳的二值神经网络（BNNs）通常是通过使用Adam优化器及其多步训练变体来实现的。然而，据我们所知，很少有研究探讨为什么Adam在BNN优化中优于其他优化器如SGD的根本原因，或提供支持特定训练策略的分析性解释。为了解决这个问题，本文首先研究了BNN训练过程中梯度和权重的轨迹。我们展示了Adam中二阶动量的正则化效应对于激活饱和导致的死亡权重的复苏至关重要。我们发现，Adam通过其自适应学习率策略，更能够处理BNN的崎岖损失面，并达到具有更高泛化能力的更好最优解。此外，我们研究了二值网络中实值权重的有趣作用，并揭示了权重衰减对BNN优化稳定性和迟缓性的影响。通过广泛的实验和分析，我们基于现有的Adam优化，提出了一个简单的训练方案，在ImageNet数据集上使用与最先进的ReActNet相同的架构，实现了70.5%的top-1准确率，同时准确率提高了1.1%。代码和模型可在https://github.com/liuzechun/AdamBNN获取。",
        "领域": "二值神经网络优化",
        "问题": "探讨Adam优化器在二值神经网络优化中优于其他优化器的原因，并提出改进的训练策略",
        "动机": "理解Adam优化器在二值神经网络中的优势，并基于此提出更有效的训练方法",
        "方法": "分析训练过程中梯度和权重的轨迹，研究Adam优化器的二阶动量正则化效应和自适应学习率策略，以及权重衰减的影响",
        "关键词": [
            "二值神经网络",
            "Adam优化器",
            "训练策略",
            "权重复苏",
            "ImageNet"
        ],
        "涉及的技术概念": {
            "二阶动量": "Adam优化器中的二阶动量用于正则化，帮助复苏因激活饱和而死亡的权重",
            "自适应学习率": "Adam优化器通过自适应调整学习率，更好地处理BNN的崎岖损失面",
            "权重衰减": "在二值网络中，权重衰减影响优化的稳定性和训练过程的迟缓性"
        },
        "success": true
    },
    {
        "order": 482,
        "title": "How Does Loss Function Affect Generalization Performance of Deep Learning? Application to Human Age Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9895",
        "abstract": "Good generalization performance across a wide variety of domains caused by many external and internal factors is the fundamental goal of any machine learning algorithm. \nThis paper theoretically proves that the choice of loss function matters for improving the generalization performance of deep learning-based systems. \nBy deriving the generalization error bound for deep neural models trained by stochastic gradient descent, we pinpoint the characteristics of the loss function that is linked to the generalization error and can therefore be used for guiding the loss function selection process. \nIn summary, our main statement in this paper is: choose a stable loss function, generalize better.\nFocusing on human age estimation from the face which is a challenging topic in computer vision, we then propose a novel loss function for this learning problem.\nWe theoretically prove that the proposed loss function achieves stronger stability, and consequently a tighter generalization error bound, compared to the other common loss functions for this problem.\nWe have supported our findings theoretically, and demonstrated the merits of the guidance process experimentally, achieving significant improvements. ",
        "conference": "ICML",
        "中文标题": "损失函数如何影响深度学习的泛化性能？应用于人类年龄估计",
        "摘要翻译": "良好的泛化性能，由许多外部和内部因素引起，跨越各种领域，是任何机器学习算法的基本目标。本文从理论上证明了损失函数的选择对于提高基于深度学习的系统的泛化性能至关重要。通过推导由随机梯度下降训练的深度神经模型的泛化误差界限，我们指出了与泛化误差相关联的损失函数的特性，因此可以用于指导损失函数的选择过程。总之，我们在本文中的主要陈述是：选择一个稳定的损失函数，泛化得更好。聚焦于从面部进行人类年龄估计这一计算机视觉中的挑战性课题，我们随后为这一学习问题提出了一种新的损失函数。我们从理论上证明了所提出的损失函数相比其他常见损失函数实现了更强的稳定性，从而获得了更紧的泛化误差界限。我们不仅在理论上支持了我们的发现，而且通过实验展示了指导过程的优点，实现了显著的改进。",
        "领域": "深度学习优化、计算机视觉、人脸分析",
        "问题": "损失函数的选择如何影响深度学习模型的泛化性能，特别是在人类年龄估计任务中。",
        "动机": "探索损失函数特性与深度学习模型泛化性能之间的关系，以提高模型在人类年龄估计等任务中的表现。",
        "方法": "通过理论推导泛化误差界限，提出一种新的损失函数，并通过实验验证其在人类年龄估计任务中的有效性。",
        "关键词": [
            "泛化性能",
            "损失函数",
            "人类年龄估计",
            "深度学习优化",
            "稳定性"
        ],
        "涉及的技术概念": {
            "泛化误差界限": "用于衡量模型在未见数据上的表现，本文通过推导这一界限来指导损失函数的选择。",
            "随机梯度下降": "一种常用的优化算法，用于训练深度神经网络，本文中用于训练模型并分析损失函数的影响。",
            "稳定性": "损失函数的一个特性，本文证明稳定的损失函数能够带来更紧的泛化误差界限，从而提高模型的泛化性能。"
        },
        "success": true
    },
    {
        "order": 483,
        "title": "How Framelets Enhance Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8465",
        "abstract": "This paper presents a new approach for assembling graph neural networks based on framelet transforms. The latter provides a multi-scale representation for graph-structured data. We decompose an input graph into low-pass and high-pass frequencies coefficients for network training, which then defines a framelet-based graph convolution. The framelet decomposition naturally induces a graph pooling strategy by aggregating the graph feature into low-pass and high-pass spectra, which considers both the feature values and geometry of the graph data and conserves the total information. The graph neural networks with the proposed framelet convolution and pooling achieve state-of-the-art performance in many node and graph prediction tasks. Moreover, we propose shrinkage as a new activation for the framelet convolution, which thresholds high-frequency information at different scales. Compared to ReLU, shrinkage activation improves model performance on denoising and signal compression: noises in both node and structure can be significantly reduced by accurately cutting off the high-pass coefficients from framelet decomposition, and the signal can be compressed to less than half its original size with well-preserved prediction performance.",
        "conference": "ICML",
        "中文标题": "框架小波如何增强图神经网络",
        "摘要翻译": "本文提出了一种基于框架小波变换构建图神经网络的新方法。框架小波变换为图结构数据提供了多尺度表示。我们将输入图分解为低通和高通频率系数用于网络训练，从而定义了基于框架小波的图卷积。框架小波分解通过将图特征聚合到低通和高通频谱中，自然地诱导出一种图池化策略，该策略既考虑了图数据的特征值又考虑了其几何形状，并保留了总信息。采用所提出的框架小波卷积和池化的图神经网络在许多节点和图预测任务中实现了最先进的性能。此外，我们提出了收缩作为框架小波卷积的新激活函数，它在不同尺度上对高频信息进行阈值处理。与ReLU相比，收缩激活在去噪和信号压缩方面提高了模型性能：通过准确切断框架小波分解中的高通系数，可以显著减少节点和结构中的噪声，并且信号可以压缩到其原始大小的一半以下，同时保持良好的预测性能。",
        "领域": "图神经网络、信号处理、图表示学习",
        "问题": "如何利用框架小波变换增强图神经网络的表示能力和性能",
        "动机": "探索框架小波变换在图神经网络中的应用，以提升模型对图结构数据的多尺度表示能力和处理效率",
        "方法": "通过框架小波变换将图数据分解为多尺度频率系数，定义基于框架小波的图卷积和池化策略，并引入收缩作为新的激活函数以优化高频信息处理",
        "关键词": [
            "框架小波变换",
            "图神经网络",
            "多尺度表示",
            "图卷积",
            "信号压缩"
        ],
        "涉及的技术概念": {
            "框架小波变换": "用于图结构数据的多尺度分解，提供低通和高通频率系数，增强图神经网络的表示能力",
            "图卷积": "基于框架小波变换定义的卷积操作，用于处理图结构数据",
            "收缩激活": "一种新的激活函数，通过阈值处理高频信息，优化图神经网络在去噪和信号压缩任务中的性能"
        },
        "success": true
    },
    {
        "order": 484,
        "title": "How Important is the Train-Validation Split in Meta-Learning?",
        "html": "https://ICML.cc//virtual/2021/poster/9855",
        "abstract": "Meta-learning aims to perform fast adaptation on a new task through learning a “prior” from multiple existing tasks. A common practice in meta-learning is to perform a train-validation split (\\emph{train-val method}) where the prior adapts to the task on one split of the data, and the resulting predictor is evaluated on another split. Despite its prevalence, the importance of the train-validation split is not well understood either in theory or in practice, particularly in comparison to the more direct \\emph{train-train method}, which uses all the per-task data for both training and evaluation.\n  \n  We provide a detailed theoretical study on whether and when the train-validation split is helpful in the linear centroid meta-learning problem. In the agnostic case, we show that the expected loss of the train-val method is minimized at the optimal prior for meta testing, and this is not the case for the train-train method in general without structural assumptions on the data. In contrast, in the realizable case where the data are generated from linear models, we show that both the train-val and train-train losses are minimized at the optimal prior in expectation. Further, perhaps surprisingly, our main result shows that the train-train method achieves a \\emph{strictly better} excess loss in this realizable case, even when the regularization parameter and split ratio are optimally tuned for both methods. Our results highlight that sample splitting may not always be preferable, especially when the data is realizable by the model. We validate our theories by experimentally showing that the train-train method can indeed outperform the train-val method, on both simulations and real meta-learning tasks.",
        "conference": "ICML",
        "中文标题": "元学习中训练-验证分割的重要性有多大？",
        "摘要翻译": "元学习旨在通过从多个现有任务中学习一个“先验”，来快速适应新任务。元学习中的一个常见做法是进行训练-验证分割（训练-验证方法），其中先验在数据的一个分割上适应任务，而得到的预测器在另一个分割上进行评估。尽管这种方法普遍存在，但训练-验证分割的重要性在理论上或实践中都没有得到很好的理解，特别是与更直接的训练-训练方法相比，后者使用所有每任务数据进行训练和评估。我们提供了一个详细的理论研究，探讨在什么情况下以及何时训练-验证分割在线性质心元学习问题中是有帮助的。在不可知的情况下，我们展示了训练-验证方法的预期损失在元测试的最优先验处最小化，而对于训练-训练方法，在没有数据结构性假设的情况下，这通常不成立。相反，在数据由线性模型生成的可实现情况下，我们展示了训练-验证和训练-训练损失在预期中的最优先验处都最小化。此外，也许令人惊讶的是，我们的主要结果表明，在这种可实现情况下，即使两种方法的正则化参数和分割比例都经过最优调整，训练-训练方法也能实现严格更好的超额损失。我们的结果强调了样本分割并不总是更优的，特别是当数据可由模型实现时。我们通过实验验证了我们的理论，表明训练-训练方法确实可以在模拟和真实元学习任务上优于训练-验证方法。",
        "领域": "元学习",
        "问题": "探讨在元学习中训练-验证分割的重要性及其与训练-训练方法的比较",
        "动机": "理解训练-验证分割在元学习中的作用，特别是在不同数据假设下的效果差异",
        "方法": "通过理论分析和实验验证，比较训练-验证和训练-训练方法在不同数据假设下的表现",
        "关键词": [
            "元学习",
            "训练-验证分割",
            "训练-训练方法",
            "线性质心",
            "超额损失"
        ],
        "涉及的技术概念": {
            "训练-验证方法": "在元学习中，先验在数据的一个分割上适应任务，预测器在另一个分割上评估",
            "训练-训练方法": "使用所有每任务数据进行训练和评估的元学习方法",
            "线性质心元学习": "一种元学习方法，假设数据由线性模型生成，用于研究训练-验证和训练-训练方法的效果"
        },
        "success": true
    },
    {
        "order": 485,
        "title": "How rotational invariance of common kernels prevents generalization in high dimensions",
        "html": "https://ICML.cc//virtual/2021/poster/8801",
        "abstract": "Kernel ridge regression is well-known to achieve minimax optimal rates in low-dimensional settings. However, its behavior in high dimensions is much less understood.  Recent work establishes consistency for high-dimensional kernel regression for a number of specific assumptions on the data distribution. In this paper, we show that in high dimensions,  the rotational invariance property of commonly studied kernels (such as RBF, inner product kernels and fully-connected NTK of any depth) leads to inconsistent estimation unless the ground truth is a low-degree polynomial. Our lower bound on the generalization error holds  for a wide range of distributions and kernels with different eigenvalue decays.  This lower bound suggests that consistency results for kernel ridge regression in high dimensions generally require a more refined analysis that depends on the structure of the kernel beyond its eigenvalue decay. ",
        "conference": "ICML",
        "中文标题": "常见核的旋转不变性如何在高维中阻碍泛化",
        "摘要翻译": "众所周知，核岭回归在低维设置下可以达到极小极大最优速率。然而，其在高维中的行为却鲜为人知。最近的工作为高维核回归建立了一致性，基于对数据分布的一系列特定假设。在本文中，我们表明，在高维中，常见研究核（如RBF、内积核和任何深度的全连接NTK）的旋转不变性属性会导致不一致的估计，除非真实情况是低次多项式。我们对泛化误差的下界适用于广泛的分布和具有不同特征值衰减的核。这一下界表明，高维核岭回归的一致性结果通常需要一个更精细的分析，该分析依赖于核的结构，而不仅仅是其特征值衰减。",
        "领域": "核方法理论、高维统计学习、机器学习理论",
        "问题": "探讨高维环境下常见核的旋转不变性如何影响核岭回归的泛化能力",
        "动机": "理解高维核回归的行为，特别是旋转不变性对泛化误差的影响，以填补现有理论在解释高维核回归性能方面的空白",
        "方法": "通过理论分析，展示高维中常见核的旋转不变性导致的不一致估计问题，并提出泛化误差的下界",
        "关键词": [
            "核岭回归",
            "高维统计",
            "旋转不变性",
            "泛化误差",
            "核方法"
        ],
        "涉及的技术概念": {
            "核岭回归": "一种基于核方法的回归技术，通过最小化带有正则项的损失函数来进行学习",
            "旋转不变性": "核函数在输入数据旋转后保持不变的性质，影响高维空间中的学习性能",
            "泛化误差": "模型在未见数据上的预期误差，用于衡量模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 486,
        "title": "How to Learn when Data Reacts to Your Model: Performative Gradient Descent",
        "html": "https://ICML.cc//virtual/2021/poster/9355",
        "abstract": "Performative distribution shift captures the setting where the choice of which ML model is deployed  changes the data distribution. For example, a bank which uses the number of open credit lines to determine a customer's risk of default on a loan may induce customers to open more credit lines in order to improve their chances of being approved. Because of the interactions between the model and data distribution, finding the optimal model parameters is challenging. Works in this area have focused on finding stable points, which can be far from optimal. Here we introduce \\emph{performative gradient descent} (PerfGD), an algorithm for computing performatively optimal points. Under regularity assumptions on the performative loss, PerfGD is the first algorithm which provably converges to an optimal point. PerfGD explicitly captures how changes in the model affects the data distribution and is simple to use. We support our findings with theory and experiments.",
        "conference": "ICML",
        "中文标题": "当数据对模型做出反应时如何学习：执行性梯度下降",
        "摘要翻译": "执行性分布偏移捕捉了部署哪个ML模型的选择会改变数据分布的场景。例如，一家银行使用开放信用额度的数量来确定客户贷款违约的风险，可能会诱导客户开设更多的信用额度以提高其贷款获批的机会。由于模型和数据分布之间的相互作用，找到最优的模型参数具有挑战性。这一领域的工作主要集中在寻找稳定点，但这些点可能远离最优。在这里，我们介绍了执行性梯度下降（PerfGD），一种用于计算执行性最优点的算法。在执行性损失的规律性假设下，PerfGD是第一个被证明能够收敛到最优点的算法。PerfGD明确捕捉了模型变化如何影响数据分布，并且使用简单。我们通过理论和实验支持我们的发现。",
        "领域": "机器学习、金融风险评估、算法优化",
        "问题": "在模型部署影响数据分布的情况下，如何找到最优的模型参数。",
        "动机": "解决由于模型选择影响数据分布而导致的模型参数优化难题。",
        "方法": "引入执行性梯度下降（PerfGD）算法，明确考虑模型变化对数据分布的影响，并在执行性损失的规律性假设下，确保算法收敛到最优点。",
        "关键词": [
            "执行性分布偏移",
            "梯度下降",
            "模型优化",
            "数据分布",
            "机器学习"
        ],
        "涉及的技术概念": {
            "执行性分布偏移": "描述模型部署选择如何影响数据分布的现象。",
            "执行性梯度下降（PerfGD）": "一种算法，用于在模型影响数据分布的情况下找到最优模型参数。",
            "执行性损失": "在考虑模型对数据分布影响的情况下定义的损失函数，用于算法优化。"
        },
        "success": true
    },
    {
        "order": 487,
        "title": "HyperHyperNetwork for the Design of Antenna Arrays",
        "html": "https://ICML.cc//virtual/2021/poster/9671",
        "abstract": "We present deep learning methods for the design of arrays and single instances of small antennas. Each design instance is conditioned on a target radiation pattern and is required to conform to specific spatial dimensions and to include, as part of its metallic structure, a set of predetermined locations. The solution, in the case of a single antenna, is based on a composite neural network that combines a simulation network, a hypernetwork, and a refinement network. In the design of the antenna array, we add an additional design level and employ a hypernetwork within a hypernetwork. The learning objective is based on measuring the similarity of the obtained radiation pattern to the desired one. Our experiments demonstrate that our approach is able to design novel antennas and antenna arrays that are compliant with the design requirements, considerably better than the baseline methods. We compare the solutions obtained by our method to existing designs and demonstrate a high level of overlap. When designing the antenna array of a cellular phone, the obtained solution displays improved properties over the existing one.",
        "conference": "ICML",
        "中文标题": "超超网络在天线阵列设计中的应用",
        "摘要翻译": "我们提出了用于设计阵列和小型天线单体的深度学习方法。每个设计实例都以目标辐射模式为条件，并需要符合特定的空间尺寸，同时在其金属结构中包含一组预定位置。在单个天线的解决方案中，基于一个复合神经网络，该网络结合了模拟网络、超网络和细化网络。在天线阵列的设计中，我们增加了一个额外的设计层级，并在超网络中使用了超网络。学习目标基于测量获得的辐射模式与期望模式之间的相似性。我们的实验表明，我们的方法能够设计出符合设计要求的新型天线和天线阵列，性能显著优于基线方法。我们将通过我们的方法获得的解决方案与现有设计进行比较，并展示了高度的重叠性。在设计手机天线阵列时，获得的解决方案显示出优于现有设计的性能。",
        "领域": "天线设计优化、深度学习应用、电磁模拟",
        "问题": "如何利用深度学习方法设计符合特定辐射模式和空间尺寸要求的天线及天线阵列",
        "动机": "探索深度学习在天线设计中的应用，以提高设计效率和性能，满足特定的辐射模式和空间尺寸要求",
        "方法": "采用复合神经网络结构，结合模拟网络、超网络和细化网络，以及在天线阵列设计中引入超网络中的超网络，通过测量辐射模式相似性来优化设计",
        "关键词": [
            "天线设计",
            "深度学习",
            "超网络",
            "辐射模式",
            "电磁模拟"
        ],
        "涉及的技术概念": {
            "复合神经网络": "结合模拟网络、超网络和细化网络，用于单个天线的设计优化",
            "超网络中的超网络": "在天线阵列设计中引入的额外设计层级，用于处理更复杂的设计需求",
            "辐射模式相似性": "作为学习目标，用于评估设计的天线或天线阵列性能是否符合预期"
        },
        "success": true
    },
    {
        "order": 488,
        "title": "Hyperparameter Selection for Imitation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10565",
        "abstract": "We address the issue of tuning hyperparameters (HPs) for imitation learning algorithms in the context of continuous-control, when the underlying reward function of the demonstrating expert cannot be observed at any time. The vast literature in imitation learning mostly considers this reward function to be available for HP selection, but this is not a realistic setting. Indeed, would this reward function be available, it could then directly be used for policy training and imitation would not be necessary. To tackle this mostly ignored problem, we propose\na number of possible proxies to the external reward. We evaluate them in an extensive empirical study (more than 10'000 agents across 9 environments) and make practical recommendations for selecting HPs. Our results show that while imitation learning algorithms are sensitive to HP choices, it is often possible to select good enough HPs through a proxy to the reward function.",
        "conference": "ICML",
        "中文标题": "模仿学习中的超参数选择",
        "摘要翻译": "我们解决了在连续控制背景下，模仿学习算法超参数（HPs）调优的问题，当展示专家的潜在奖励函数在任何时候都无法观察到时。模仿学习的大量文献大多认为这个奖励函数可用于HP选择，但这并不是一个现实的设定。确实，如果这个奖励函数可用，它可以直接用于策略训练，模仿就不必要了。为了解决这个大多被忽视的问题，我们提出了几种可能的替代外部奖励的方案。我们在一个广泛的实证研究（超过10,000个代理，跨越9个环境）中评估了它们，并为选择HPs提供了实用建议。我们的结果表明，虽然模仿学习算法对HP选择敏感，但通过奖励函数的替代方案往往可以选择足够好的HPs。",
        "领域": "模仿学习、连续控制、强化学习",
        "问题": "在无法观察到专家奖励函数的情况下，如何为模仿学习算法选择超参数",
        "动机": "模仿学习领域大多假设专家的奖励函数可用于超参数选择，这一假设在实际应用中不成立，因此需要研究在无法直接观察奖励函数的情况下如何有效选择超参数",
        "方法": "提出并评估了几种替代外部奖励的方案，通过广泛的实证研究验证这些方案的有效性，并给出选择超参数的实用建议",
        "关键词": [
            "模仿学习",
            "超参数选择",
            "连续控制",
            "奖励函数替代",
            "实证研究"
        ],
        "涉及的技术概念": {
            "模仿学习": "一种通过观察专家的行为来学习策略的方法，本研究中用于在无法直接观察奖励函数的情况下学习",
            "超参数选择": "在机器学习模型中，超参数的选择对模型性能有重要影响，本研究探讨在模仿学习中如何有效选择超参数",
            "奖励函数替代": "在无法直接观察专家奖励函数的情况下，提出的几种替代方案，用于指导超参数的选择和策略的训练"
        },
        "success": true
    },
    {
        "order": 489,
        "title": "I-BERT: Integer-only BERT Quantization",
        "html": "https://ICML.cc//virtual/2021/poster/9811",
        "abstract": "Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4- 4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced.",
        "conference": "ICML",
        "中文标题": "I-BERT：纯整数BERT量化",
        "摘要翻译": "基于Transformer的模型，如BERT和RoBERTa，已在许多自然语言处理任务中取得了最先进的结果。然而，它们的内存占用、推理延迟和功耗在边缘甚至数据中心进行高效推理时都是不可接受的。虽然量化可以是一个可行的解决方案，但之前关于量化基于Transformer的模型的工作在推理过程中使用浮点运算，这无法有效利用仅支持整数运算的逻辑单元，如最近的Turing Tensor Cores或传统的仅支持整数的ARM处理器。在这项工作中，我们提出了I-BERT，一种新颖的基于Transformer模型的量化方案，该方案使用纯整数运算进行整个推理。基于对非线性操作（如GELU、Softmax和层归一化）的轻量级纯整数近似方法，I-BERT执行端到端的纯整数BERT推理，无需任何浮点计算。我们在GLUE下游任务上使用RoBERTa-Base/Large评估了我们的方法。我们表明，对于这两种情况，I-BERT与全精度基线相比达到了相似（甚至略高）的准确度。此外，我们对I-BERT的初步实现显示，在T4 GPU系统上，INT8推理与FP32推理相比，速度提高了2.4至4.0倍。该框架已在PyTorch中开发并开源。",
        "领域": "自然语言处理模型优化、模型量化、边缘计算",
        "问题": "解决基于Transformer的模型在推理过程中因使用浮点运算而无法高效利用整数运算单元的问题",
        "动机": "为了在边缘计算和数据中心实现更高效的推理，减少模型的内存占用、推理延迟和功耗",
        "方法": "提出了一种纯整数运算的量化方案I-BERT，通过轻量级整数近似方法处理非线性操作，实现端到端的纯整数推理",
        "关键词": [
            "模型量化",
            "纯整数推理",
            "Transformer优化",
            "边缘计算",
            "自然语言处理"
        ],
        "涉及的技术概念": {
            "纯整数近似方法": "用于处理GELU、Softmax和层归一化等非线性操作的轻量级方法，使得整个推理过程可以在纯整数运算下完成",
            "模型量化": "通过减少模型参数的位数来减小模型大小和加速推理过程的技术",
            "端到端推理": "指从输入到输出完全在模型内部完成，无需外部干预的推理过程"
        },
        "success": true
    },
    {
        "order": 490,
        "title": "iDARTS: Differentiable Architecture Search with Stochastic Implicit Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/8823",
        "abstract": "Differentiable ARchiTecture Search(DARTS) has recently become the mainstream in the neural architecture search (NAS) due to its efficiency and simplicity.  With a gradient-based bi-level optimization, DARTS alternately optimizes the inner model weights and the outer architecture parameter in a weight-sharing supernet. A key challenge to the scalability and quality of the learned architectures is the need for differentiating through the inner-loop optimisation. While much has been discussed about several potentially fatal factors in DARTS, the architecture gradient, a.k.a. hypergradient, has received less attention. In this paper, we tackle the hypergradient computation in DARTS based on the implicit function theorem, making it only depends on the obtained solution to the inner-loop optimization and agnostic to the optimization path. To further reduce the computational requirements, we formulate a stochastic hypergradient approximation for differentiable NAS, and theoretically show that the architecture optimization with the proposed method is expected to converge to a stationary point. Comprehensive experiments on two NAS benchmark search spaces and the common NAS search space verify the effectiveness of our proposed method. It leads to architectures outperforming, with large margins, those learned by the baseline methods.",
        "conference": "ICML",
        "中文标题": "iDARTS: 基于随机隐式梯度的可微分架构搜索",
        "摘要翻译": "可微分架构搜索(DARTS)由于其效率和简洁性，最近已成为神经架构搜索(NAS)的主流。通过基于梯度的双层优化，DARTS在权重共享的超网络中交替优化内部模型权重和外部架构参数。扩展性和学习架构质量的一个关键挑战是需要通过内部循环优化进行微分。尽管关于DARTS中几个潜在致命因素的讨论很多，但架构梯度（也称为超梯度）受到的关注较少。在本文中，我们基于隐函数定理解决了DARTS中的超梯度计算问题，使其仅依赖于内部循环优化获得的解，而与优化路径无关。为了进一步减少计算需求，我们为可微分NAS制定了一个随机超梯度近似，并从理论上证明了使用所提出方法的架构优化有望收敛到一个稳定点。在两个NAS基准搜索空间和常见的NAS搜索空间上的全面实验验证了我们提出方法的有效性。它导致架构性能大幅超越基线方法学习的架构。",
        "领域": "神经架构搜索、深度学习优化、自动化机器学习",
        "问题": "解决DARTS中架构梯度（超梯度）计算的问题，提高神经架构搜索的效率和性能。",
        "动机": "由于DARTS中超梯度计算对内部循环优化的依赖性，影响了架构搜索的扩展性和质量，本研究旨在通过隐函数定理和随机超梯度近似来解决这一问题。",
        "方法": "基于隐函数定理改进DARTS中的超梯度计算，使其不依赖于优化路径，并引入随机超梯度近似以减少计算需求。",
        "关键词": [
            "可微分架构搜索",
            "隐式梯度",
            "神经架构搜索",
            "双层优化",
            "超梯度近似"
        ],
        "涉及的技术概念": {
            "可微分架构搜索(DARTS)": "一种高效的神经架构搜索方法，通过梯度优化在权重共享的超网络中搜索最优架构。",
            "隐函数定理": "用于解决DARTS中超梯度计算问题，使梯度计算仅依赖于优化结果，而不依赖于优化路径。",
            "随机超梯度近似": "为减少计算需求而提出的方法，理论上保证架构优化能收敛到稳定点。"
        },
        "success": true
    },
    {
        "order": 491,
        "title": "Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection",
        "html": "https://ICML.cc//virtual/2021/poster/9533",
        "abstract": "Training on datasets with long-tailed distributions has been challenging for major recognition tasks such as classification and detection. To deal with this challenge, image resampling is typically introduced as a simple but effective approach. However, we observe that long-tailed detection differs from classification since multiple classes may be present in one image. As a result, image resampling alone is not enough to yield a sufficiently balanced distribution at the object-level. We address object-level resampling by introducing an object-centric sampling strategy based on a dynamic, episodic memory bank. Our proposed strategy has two benefits: 1) convenient object-level resampling without significant extra computation, and 2) implicit feature-level augmentation from model updates. We show that image-level and object-level resamplings are both important, and thus unify them with a joint resampling strategy. Our method achieves state-of-the-art performance on the rare categories of LVIS, with 1.89% and 3.13% relative improvements over Forest R-CNN on detection and instance segmentation.",
        "conference": "ICML",
        "中文标题": "图像级还是对象级？长尾检测中两种重采样策略的故事",
        "摘要翻译": "在长尾分布的数据集上进行训练对于分类和检测等主要识别任务一直具有挑战性。为了应对这一挑战，图像重采样通常被引入作为一种简单但有效的方法。然而，我们观察到长尾检测与分类不同，因为一个图像中可能存在多个类别。因此，仅靠图像重采样不足以在对象级别产生足够平衡的分布。我们通过引入一种基于动态、情景记忆库的对象中心采样策略来解决对象级重采样问题。我们提出的策略有两个好处：1）无需显著增加额外计算即可方便地进行对象级重采样，2）通过模型更新实现隐式的特征级增强。我们展示了图像级和对象级重采样都很重要，因此将它们与联合重采样策略统一起来。我们的方法在LVIS的稀有类别上实现了最先进的性能，检测和实例分割相对于Forest R-CNN分别提高了1.89%和3.13%。",
        "领域": "目标检测, 实例分割, 长尾学习",
        "问题": "解决长尾分布数据集中对象级和图像级重采样不平衡的问题",
        "动机": "长尾检测与分类不同，一个图像中可能存在多个类别，仅靠图像重采样不足以在对象级别产生足够平衡的分布",
        "方法": "引入一种基于动态、情景记忆库的对象中心采样策略，结合图像级和对象级重采样",
        "关键词": [
            "长尾检测",
            "对象级重采样",
            "图像级重采样",
            "动态记忆库",
            "联合重采样策略"
        ],
        "涉及的技术概念": {
            "动态记忆库": "用于存储和更新对象信息，支持对象级重采样",
            "对象中心采样策略": "基于动态记忆库的策略，专注于对象级别的重采样",
            "联合重采样策略": "统一图像级和对象级重采样的方法，以提高长尾数据集的检测性能"
        },
        "success": true
    },
    {
        "order": 492,
        "title": "Imitation by Predicting Observations",
        "html": "https://ICML.cc//virtual/2021/poster/10285",
        "abstract": "Imitation learning enables agents to reuse and adapt the hard-won expertise of others, offering a solution to several key challenges in learning behavior. Although it is easy to observe behavior in the real-world, the underlying actions may not be accessible. We present a new method for imitation solely from observations that achieves comparable performance to experts on challenging continuous control tasks while also exhibiting robustness in the presence of observations unrelated to the task. Our method, which we call FORM (for 'Future Observation Reward Model') is derived from an inverse RL objective and imitates using a model of expert behavior learned by generative modelling of the expert's observations, without needing ground truth actions. We show that FORM performs comparably to a strong baseline IRL method (GAIL) on the DeepMind Control Suite benchmark, while outperforming GAIL in the presence of task-irrelevant features. ",
        "conference": "ICML",
        "中文标题": "通过预测观察进行模仿",
        "摘要翻译": "模仿学习使智能体能够重用和适应他人来之不易的专业知识，为解决学习行为中的几个关键挑战提供了方案。虽然在现实世界中观察行为很容易，但潜在的动作可能无法获取。我们提出了一种仅从观察中进行模仿的新方法，该方法在具有挑战性的连续控制任务上达到了与专家相当的性能，同时在存在与任务无关的观察时也表现出鲁棒性。我们的方法，我们称之为FORM（即'未来观察奖励模型'），是从逆向强化学习目标中推导出来的，并通过生成建模专家观察的行为模型进行模仿，而不需要真实动作。我们表明，在DeepMind控制套件基准测试中，FORM的表现与强大的基线IRL方法（GAIL）相当，同时在存在任务无关特征的情况下优于GAIL。",
        "领域": "模仿学习",
        "问题": "如何在无法获取专家动作的情况下，仅通过观察专家的行为进行有效的模仿学习。",
        "动机": "解决在现实世界中观察行为容易但获取潜在动作困难的情况下，如何有效模仿专家行为的问题。",
        "方法": "提出了一种名为FORM的新方法，该方法基于逆向强化学习目标，通过生成建模专家的观察来学习专家行为模型，无需真实动作数据。",
        "关键词": [
            "模仿学习",
            "逆向强化学习",
            "生成建模",
            "连续控制任务",
            "观察学习"
        ],
        "涉及的技术概念": {
            "模仿学习": "一种通过观察和模仿专家行为来学习的技术，无需直接编程或明确的奖励信号。",
            "逆向强化学习": "从观察到的行为中推断出潜在的奖励函数，然后使用这个奖励函数来指导学习过程。",
            "生成建模": "通过模型生成与专家观察相似的数据，用于模仿专家的行为模式。"
        },
        "success": true
    },
    {
        "order": 493,
        "title": "Implicit Bias of Linear RNNs",
        "html": "https://ICML.cc//virtual/2021/poster/10445",
        "abstract": "Contemporary wisdom based on empirical studies suggests that standard recurrent neural networks (RNNs) do not perform well on tasks requiring long-term memory. However, RNNs' poor ability to capture long-term dependencies has not been fully understood. This paper provides a rigorous explanation of this property in the special case of linear RNNs.  Although this work is limited to linear RNNs, even these systems have traditionally been difficult to analyze due to their non-linear parameterization. Using recently-developed kernel regime analysis, our main result shows that as the number of hidden units goes to infinity, linear RNNs learned from random initializations are functionally equivalent to a certain weighted 1D-convolutional network.  Importantly, the weightings in the equivalent model cause an implicit bias to elements with smaller time lags in the convolution, and hence shorter memory.  The degree of this bias depends on the variance of the transition matrix at initialization and is related to the classic exploding and vanishing gradients problem. The theory is validated with both synthetic and real data experiments.",
        "conference": "ICML",
        "中文标题": "线性循环神经网络的隐式偏差",
        "摘要翻译": "基于实证研究的当代智慧表明，标准的循环神经网络（RNNs）在需要长期记忆的任务上表现不佳。然而，RNNs捕捉长期依赖性的能力差尚未得到充分理解。本文在特殊情况下，即线性RNNs，对这一特性提供了严格的解释。尽管这项工作仅限于线性RNNs，但由于其非线性参数化，即使是这些系统传统上也难以分析。利用最近开发的内核机制分析，我们的主要结果表明，随着隐藏单元数量的增加，从随机初始化学习的线性RNNs在功能上等同于某种加权的一维卷积网络。重要的是，等效模型中的加权导致了对卷积中具有较小时间滞后的元素（因此记忆较短）的隐式偏差。这种偏差的程度取决于初始化时转移矩阵的方差，并与经典的梯度爆炸和消失问题相关。该理论通过合成和真实数据实验得到了验证。",
        "领域": "循环神经网络、长期依赖性分析、梯度问题研究",
        "问题": "解释线性循环神经网络在捕捉长期依赖性方面的性能不佳问题",
        "动机": "理解并解释线性RNNs在长期记忆任务中表现不佳的根本原因",
        "方法": "使用内核机制分析，研究线性RNNs在隐藏单元数量趋于无穷大时的行为，并将其与加权一维卷积网络进行功能等价性比较",
        "关键词": [
            "线性循环神经网络",
            "隐式偏差",
            "长期依赖性",
            "梯度爆炸和消失",
            "内核机制分析"
        ],
        "涉及的技术概念": {
            "线性循环神经网络": "本文研究的特定类型的RNN，其参数化方式使得分析变得复杂",
            "隐式偏差": "指线性RNNs在学习过程中对较短记忆的偏好，这是由其初始化参数的特性引起的",
            "内核机制分析": "用于分析线性RNNs在无限隐藏单元情况下的行为，揭示了其与加权一维卷积网络的功能等价性"
        },
        "success": true
    },
    {
        "order": 494,
        "title": "Implicit-PDF: Non-Parametric Representation of Probability Distributions on the Rotation Manifold",
        "html": "https://ICML.cc//virtual/2021/poster/10375",
        "abstract": "In the deep learning era, the vast majority of methods to predict pose from a single image are trained to classify or regress to a single given ground truth pose per image. Such methods have two main shortcomings, i) they cannot represent uncertainty about the predictions, and ii) they cannot handle symmetric objects, where multiple (potentially infinite) poses may be correct. Only recently these shortcomings have been addressed, but current approaches as limited in that they cannot express the full rich space of distributions on the rotation manifold.  To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability density, given the input image and a candidate pose. At inference time, grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the density at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to demonstrate its expressive power we introduce a new dataset containing symmetric and nearly-symmetric objects. Our method also shows advantages on the popular object pose estimation benchmarks ModelNet10-SO(3) and T-LESS. Code, data, and visualizations may be found at implicit-pdf.github.io.",
        "conference": "ICML",
        "中文标题": "隐式概率密度函数：旋转流形上概率分布的非参数表示",
        "摘要翻译": "在深度学习时代，绝大多数从单张图像预测姿态的方法被训练为对每张图像的单一给定真实姿态进行分类或回归。这类方法有两个主要缺点：i) 它们无法表示预测的不确定性；ii) 它们无法处理对称物体，其中可能有多个（甚至无限多个）姿态是正确的。直到最近，这些缺点才得到解决，但当前的方法在表达旋转流形上分布的完整丰富空间方面仍有限制。为此，我们引入了一种方法来估计SO(3)上的任意非参数分布。我们的关键思想是用一个神经网络隐式地表示分布，该网络在给定输入图像和候选姿态的情况下估计概率密度。在推理时，可以使用网格采样或梯度上升来找到最可能的姿态，但也可以在任何姿态评估密度，从而实现对对称性和不确定性的推理。这是在流形上表示分布的最通用方式，为了展示其表达能力，我们引入了一个包含对称和近对称物体的新数据集。我们的方法在流行的物体姿态估计基准ModelNet10-SO(3)和T-LESS上也显示出了优势。代码、数据和可视化可在implicit-pdf.github.io找到。",
        "领域": "姿态估计、对称物体处理、不确定性量化",
        "问题": "解决从单张图像预测姿态时无法表示预测不确定性和处理对称物体的问题",
        "动机": "为了克服现有方法在表示旋转流形上分布的完整性和处理对称物体方面的限制",
        "方法": "引入一种隐式表示概率分布的方法，使用神经网络估计给定输入图像和候选姿态的概率密度，支持网格采样或梯度上升进行推理",
        "关键词": [
            "隐式概率密度函数",
            "旋转流形",
            "对称物体处理",
            "姿态估计",
            "不确定性量化"
        ],
        "涉及的技术概念": {
            "隐式概率密度函数": "用于表示旋转流形上的非参数分布，通过神经网络估计概率密度",
            "旋转流形(SO(3))": "表示三维旋转的空间，论文中用于描述物体姿态的分布",
            "网格采样和梯度上升": "在推理时用于找到最可能姿态的技术，同时支持在任何姿态评估密度"
        },
        "success": true
    },
    {
        "order": 495,
        "title": "Implicit rate-constrained optimization of non-decomposable objectives",
        "html": "https://ICML.cc//virtual/2021/poster/9587",
        "abstract": "We consider a popular family of constrained optimization problems arising in machine learning that involve optimizing a non-decomposable evaluation metric with a certain thresholded form, while constraining another metric of interest. Examples of such problems include optimizing false negative rate at a fixed false positive rate, optimizing precision at a fixed recall, optimizing the area under the precision-recall or ROC curves, etc. Our key idea is to formulate a rate-constrained optimization that expresses the threshold parameter as a function of the model parameters via the Implicit Function theorem. We show how the resulting optimization problem can be solved using standard gradient based methods. Experiments on benchmark datasets demonstrate the effectiveness of our proposed method over existing state-of-the-art approaches for these problems.",
        "conference": "ICML",
        "中文标题": "隐式速率约束优化不可分解目标",
        "摘要翻译": "我们考虑了一类在机器学习中流行的约束优化问题，这些问题涉及优化具有某种阈值形式的不可分解评估指标，同时约束另一个感兴趣的指标。这类问题的例子包括在固定的假阳性率下优化假阴性率、在固定的召回率下优化精确率、优化精确率-召回率或ROC曲线下的面积等。我们的关键思想是通过隐函数定理将阈值参数表示为模型参数的函数，从而构建一个速率约束优化问题。我们展示了如何使用标准的基于梯度的方法来解决由此产生的优化问题。在基准数据集上的实验证明了我们提出的方法在这些问题上优于现有的最先进方法。",
        "领域": "机器学习优化、性能指标优化、梯度下降优化",
        "问题": "优化具有阈值形式的不可分解评估指标，同时约束另一个感兴趣的指标",
        "动机": "解决在机器学习中优化不可分解评估指标时遇到的约束优化问题",
        "方法": "通过隐函数定理将阈值参数表示为模型参数的函数，构建速率约束优化问题，并使用标准的基于梯度的方法进行求解",
        "关键词": [
            "隐式优化",
            "速率约束",
            "不可分解指标",
            "梯度下降",
            "性能指标优化"
        ],
        "涉及的技术概念": {
            "隐函数定理": "用于将阈值参数表示为模型参数的函数，从而构建速率约束优化问题",
            "梯度下降方法": "用于解决构建的速率约束优化问题",
            "不可分解评估指标": "指那些不能简单地分解为单个样本贡献的评估指标，如精确率-召回率曲线下的面积"
        },
        "success": true
    },
    {
        "order": 496,
        "title": "Implicit Regularization in Tensor Factorization",
        "html": "https://ICML.cc//virtual/2021/poster/8767",
        "abstract": "Recent efforts to unravel the mystery of implicit regularization in deep learning have led to a theoretical focus on matrix factorization --- matrix completion via linear neural network. As a step further towards practical deep learning, we provide the first theoretical analysis of implicit regularization in tensor factorization --- tensor completion via certain type of non-linear neural network. We circumvent the notorious difficulty of tensor problems by adopting a dynamical systems perspective, and characterizing the evolution induced by gradient descent. The characterization suggests a form of greedy low tensor rank search, which we rigorously prove under certain conditions, and empirically demonstrate under others. Motivated by tensor rank capturing the implicit regularization of a non-linear neural network, we empirically explore it as a measure of complexity, and find that it captures the essence of datasets on which neural networks generalize. This leads us to believe that tensor rank may pave way to explaining both implicit regularization in deep learning, and the properties of real-world data translating this implicit regularization to generalization.",
        "conference": "ICML",
        "中文标题": "张量分解中的隐式正则化",
        "摘要翻译": "近期，为了揭开深度学习中隐式正则化的神秘面纱，理论研究集中在矩阵分解——通过线性神经网络完成的矩阵补全。作为向实用深度学习迈进的一步，我们首次对张量分解中的隐式正则化进行了理论分析——通过某种类型的非线性神经网络完成的张量补全。我们通过采用动力系统的视角，并描述由梯度下降引起的演化，绕过了张量问题的著名困难。这一描述暗示了一种贪婪的低张量秩搜索形式，我们在某些条件下严格证明了这一点，并在其他条件下进行了实证展示。受到张量秩捕捉非线性神经网络隐式正则化的启发，我们实证探索了它作为一种复杂性度量，并发现它捕捉了神经网络泛化的数据集的本质。这使我们相信，张量秩可能为解释深度学习中的隐式正则化，以及将这种隐式正则化转化为泛化的现实世界数据的特性铺平道路。",
        "领域": "深度学习理论、张量分解、神经网络泛化",
        "问题": "理解深度学习中隐式正则化的机制及其在张量分解中的应用",
        "动机": "探索张量分解中隐式正则化的理论基础，以揭示深度学习模型泛化能力的来源",
        "方法": "采用动力系统视角分析梯度下降在张量分解中的演化过程，提出并验证贪婪低张量秩搜索的理论",
        "关键词": [
            "隐式正则化",
            "张量分解",
            "梯度下降",
            "神经网络泛化",
            "张量秩"
        ],
        "涉及的技术概念": {
            "隐式正则化": "在训练过程中，优化算法无意中引入的偏好或约束，影响模型的泛化能力",
            "张量分解": "将高阶张量分解为低秩结构的过程，用于数据补全和特征提取",
            "梯度下降": "一种优化算法，通过迭代调整参数以最小化损失函数，用于训练神经网络"
        },
        "success": true
    },
    {
        "order": 497,
        "title": "Improved Algorithms for Agnostic Pool-based Active Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9979",
        "abstract": "We consider active learning for binary classification in the agnostic pool-based setting. The vast majority of works in active learning in the agnostic setting are inspired by the CAL algorithm where each query is uniformly sampled from the disagreement region of the current version space. The sample complexity of such algorithms is described by a quantity known as the disagreement coefficient which captures both the geometry of the hypothesis space as well as the underlying probability space. To date, the disagreement coefficient has been justified by minimax lower bounds only, leaving the door open for superior instance dependent sample complexities. In this work we propose an algorithm that, in contrast to uniform sampling over the disagreement region, solves an experimental design problem to determine a distribution over examples from which to request labels. We show that the new approach achieves sample complexity bounds that are never worse than the best disagreement coefficient-based bounds, but in specific cases can be dramatically smaller. From a practical perspective, the proposed algorithm requires no hyperparameters to tune (e.g., to control the aggressiveness of sampling), and is computationally efficient by means of assuming access to an empirical risk minimization oracle (without any constraints). Empirically, we demonstrate that our algorithm is superior to state of the art agnostic active learning algorithms on image classification datasets.",
        "conference": "ICML",
        "中文标题": "改进的不可知池基主动分类算法",
        "摘要翻译": "我们考虑了在不可知池基设置下的二元分类主动学习。绝大多数在不可知设置下的主动学习工作都受到CAL算法的启发，其中每个查询都是从当前版本空间的不一致区域均匀采样的。这类算法的样本复杂度由一个称为不一致系数的量来描述，该系数捕捉了假设空间的几何形状以及基础概率空间。迄今为止，不一致系数仅通过极小极大下界来证明，这为依赖于实例的优越样本复杂度敞开了大门。在这项工作中，我们提出了一种算法，与在不一致区域上均匀采样不同，该算法通过解决一个实验设计问题来确定请求标签的样本分布。我们展示了新方法实现的样本复杂度界限从不比基于不一致系数的最佳界限差，但在特定情况下可以显著更小。从实际角度来看，所提出的算法不需要调整超参数（例如，控制采样的激进程度），并且通过假设可以访问经验风险最小化预言机（没有任何约束）来实现计算效率。实证上，我们证明了我们的算法在图像分类数据集上优于最先进的不可知主动学习算法。",
        "领域": "主动学习",
        "问题": "在不可知池基设置下，如何更有效地进行二元分类主动学习",
        "动机": "现有的主动学习算法主要依赖于不一致系数，这限制了样本复杂度的优化空间，尤其是在特定实例下可能无法达到最优性能。",
        "方法": "提出了一种新的算法，通过解决实验设计问题来确定请求标签的样本分布，而不是简单地从不一致区域均匀采样。",
        "关键词": [
            "主动学习",
            "二元分类",
            "不可知池基设置",
            "样本复杂度",
            "实验设计"
        ],
        "涉及的技术概念": {
            "不一致系数": "用于描述主动学习算法样本复杂度的量，捕捉了假设空间的几何形状和基础概率空间。",
            "实验设计问题": "算法中用于确定请求标签样本分布的方法，旨在优化样本复杂度。",
            "经验风险最小化预言机": "算法中假设可以访问的计算工具，用于实现计算效率，无需任何约束。"
        },
        "success": true
    },
    {
        "order": 498,
        "title": "Improved Confidence Bounds for the Linear Logistic Model and Applications to Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/8411",
        "abstract": "We propose improved fixed-design confidence bounds for the linear logistic model. Our bounds significantly improve upon the state-of-the-art bound by Li et al. (2017) via recent developments of the self-concordant analysis of the logistic loss (Faury et al., 2020). Specifically, our confidence bound avoids a direct dependence on $1/\\kappa$, where $\\kappa$ is the minimal variance over all arms' reward distributions. In general, $1/\\kappa$ scales exponentially with the norm of the unknown linear parameter $\\theta^*$. Instead of relying on this worst case quantity, our confidence bound for the reward of any given arm depends directly on the variance of that arm's reward distribution. We present two applications of our novel bounds to pure exploration and regret minimization logistic bandits improving upon state-of-the-art performance guarantees. For pure exploration we also provide a lower bound highlighting a dependence on $1/\\kappa$ for a family of instances.",
        "conference": "ICML",
        "success": true,
        "中文标题": "线性Logistic模型的改进置信界限及其在Bandit算法中的应用",
        "摘要翻译": "我们为线性Logistic模型提出了改进的固定设计置信界限。通过Logistic损失的自和谐分析的最新发展（Faury et al., 2020），我们的界限显著优于Li et al.（2017）的最先进的界限。具体来说，我们的置信界限避免了对1/κ的直接依赖，其中κ是所有臂的奖励分布的最小方差。通常，1/κ随未知线性参数θ^*的范数呈指数级增长。我们的奖励置信界限没有依赖于这个最坏情况的量，而是直接依赖于该臂的奖励分布的方差。我们提出了我们的新颖界限在纯探索和后悔最小化Logistic Bandit算法中的两种应用，从而改进了最先进的性能保证。对于纯探索，我们还提供了一个下界，强调了对于一类实例对1/κ的依赖。",
        "领域": "强化学习, Bandit算法, 统计推断",
        "问题": "如何在logistic bandit问题中，更精确地估计奖励的置信区间，从而提高探索效率和降低后悔值。",
        "动机": "现有的线性logistic模型置信界限依赖于最小方差的倒数，该值可能随未知参数的范数呈指数增长，导致置信区间过大。因此，希望避免对这个最坏情况量的依赖，直接利用每个臂的奖励分布的方差来构建更精确的置信界限。",
        "方法": "通过logistic损失的自和谐分析的最新发展，推导出改进的固定设计置信界限。该界限避免了对最小方差倒数的直接依赖，转而直接依赖于每个臂的奖励分布方差。然后，将该界限应用于纯探索和后悔最小化logistic bandits问题。",
        "关键词": [
            "置信界限",
            "线性Logistic模型",
            "Bandit算法",
            "纯探索",
            "后悔最小化"
        ],
        "涉及的技术概念": {
            "置信界限": "用于估计未知参数的取值范围，并在一定概率水平下保证真实参数位于该范围内的概念。在bandit算法中，用于估计每个臂的奖励期望，从而指导探索和利用。",
            "Logistic模型": "一种广义线性模型，用于处理二元分类或概率预测问题。在本文中，用于建模bandit算法中每个臂的奖励分布概率。"
        }
    },
    {
        "order": 499,
        "title": "Improved Contrastive Divergence Training of Energy-Based Models",
        "html": "https://ICML.cc//virtual/2021/poster/10055",
        "abstract": "Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.",
        "conference": "ICML",
        "中文标题": "改进的对比散度能量基模型训练方法",
        "摘要翻译": "对比散度是训练能量基模型的一种流行方法，但众所周知，它在训练稳定性方面存在困难。我们提出了一种改进对比散度训练的适应方法，通过仔细检查一个难以计算且经常为了方便而被忽略的梯度项。我们表明，这个梯度项在数值上是显著的，在实践中对于避免训练不稳定性非常重要，同时它是可以估计的。我们进一步强调了如何使用数据增强和多尺度处理来提高模型的鲁棒性和生成质量。最后，我们通过实证评估模型架构的稳定性，并在图像生成、OOD检测和组合生成等一系列基准测试和用例中展示了改进的性能。",
        "领域": "图像生成, 异常检测, 组合生成",
        "问题": "解决对比散度训练能量基模型时的稳定性问题",
        "动机": "提高能量基模型训练的稳定性和生成质量",
        "方法": "通过分析并包含一个常被忽略的梯度项，结合数据增强和多尺度处理技术",
        "关键词": [
            "对比散度",
            "能量基模型",
            "训练稳定性",
            "数据增强",
            "多尺度处理"
        ],
        "涉及的技术概念": {
            "对比散度": "用于训练能量基模型的方法，通过比较数据分布和模型分布的差异来更新模型参数",
            "能量基模型": "一种通过能量函数来定义数据概率分布的模型，常用于生成任务",
            "多尺度处理": "在多个尺度上处理数据，以提高模型对不同尺度特征的捕捉能力和生成质量"
        },
        "success": true
    },
    {
        "order": 500,
        "title": "Improved Corruption Robust Algorithms for Episodic Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8709",
        "abstract": "We study episodic reinforcement learning under unknown adversarial corruptions in both the rewards and the transition probabilities of the underlying system. We propose new algorithms which, compared to the existing results in \\cite{lykouris2020corruption}, achieve strictly better regret bounds in terms of total corruptions for the tabular setting. To be specific, firstly, our regret bounds depend on more precise numerical values of total rewards corruptions and transition corruptions, instead of only on the total number of corrupted episodes. Secondly, our regret bounds are the first of their kind in the reinforcement learning setting to have the number of corruptions show up additively with respect to $\\min\\{ \\sqrt{T},\\text{PolicyGapComplexity} \\}$ rather than multiplicatively. Our results follow from a general algorithmic framework that combines corruption-robust policy elimination meta-algorithms, and plug-in reward-free exploration sub-algorithms. Replacing the meta-algorithm or sub-algorithm may extend the framework to address other corrupted settings with potentially more structure.",
        "conference": "ICML",
        "success": true,
        "中文标题": "改进的周期性强化学习中的腐败鲁棒算法",
        "摘要翻译": "我们研究了在未知对抗性腐败影响下，周期性强化学习中的奖励和系统转移概率问题。与现有研究相比，我们提出的新算法在表格设置下，针对总腐败量实现了更严格的遗憾界限。具体来说，首先，我们的遗憾界限依赖于总奖励腐败和转移腐败的更精确数值，而不仅仅是腐败周期的总数。其次，我们的遗憾界限在强化学习设置中是首次以腐败数量相对于min{√T,策略差距复杂度}的加法形式出现，而非乘法形式。我们的结果来源于一个结合了腐败鲁棒策略消除元算法和插件式无奖励探索子算法的通用算法框架。替换元算法或子算法可能扩展该框架，以解决其他具有潜在更多结构的腐败设置。",
        "领域": "强化学习, 对抗性学习, 算法鲁棒性",
        "问题": "在周期性强化学习中，如何提高算法对未知对抗性腐败（包括奖励和转移概率的腐败）的鲁棒性。",
        "动机": "现有的研究在处理强化学习中的腐败问题时，遗憾界限不够精确，且腐败数量的影响以乘法形式出现，限制了算法的鲁棒性和效率。",
        "方法": "提出了一种结合腐败鲁棒策略消除元算法和插件式无奖励探索子算法的通用框架，以实现更精确的遗憾界限和加法形式的腐败影响。",
        "关键词": [
            "强化学习",
            "对抗性腐败",
            "算法鲁棒性",
            "策略消除",
            "无奖励探索"
        ],
        "涉及的技术概念": {
            "腐败鲁棒策略消除元算法": "用于在存在腐败的情况下，有效地筛选和优化策略，以提高算法的鲁棒性。",
            "插件式无奖励探索子算法": "在不需要外部奖励信号的情况下，探索环境以收集信息，支持策略的优化。",
            "遗憾界限": "衡量算法性能的一种指标，表示算法表现与最优策略之间的差距，本研究通过精确计算腐败影响改进了这一指标。"
        }
    },
    {
        "order": 501,
        "title": "Improved Denoising Diffusion Probabilistic Models",
        "html": "https://ICML.cc//virtual/2021/poster/9531",
        "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.",
        "conference": "ICML",
        "中文标题": "改进的去噪扩散概率模型",
        "摘要翻译": "去噪扩散概率模型（DDPM）是最近被证明能够产生优秀样本的一类生成模型。我们展示了通过一些简单的修改，DDPMs也能在保持高样本质量的同时实现有竞争力的对数似然。此外，我们发现学习反向扩散过程的方差允许以数量级更少的前向传递进行采样，而样本质量的差异可以忽略不计，这对于这些模型的实际部署非常重要。我们还使用精确度和召回率来比较DDPMs和GANs覆盖目标分布的能力。最后，我们展示了这些模型的样本质量和似然随着模型容量和训练计算量的增加而平滑扩展，使其易于扩展。我们在https://github.com/openai/improved-diffusion发布了我们的代码和预训练模型。",
        "领域": "生成模型、图像生成、概率模型",
        "问题": "提高去噪扩散概率模型的样本质量和计算效率",
        "动机": "探索如何通过简单修改提升DDPMs的性能，使其在实际应用中更加高效和实用",
        "方法": "通过修改DDPMs结构、学习反向扩散过程的方差以及使用精确度和召回率评估模型性能",
        "关键词": [
            "去噪扩散概率模型",
            "生成模型",
            "样本质量",
            "计算效率",
            "模型扩展"
        ],
        "涉及的技术概念": {
            "去噪扩散概率模型（DDPM）": "一类生成模型，通过逐步去噪过程生成数据样本",
            "反向扩散过程": "DDPM中从噪声数据逐步恢复原始数据的过程，学习其方差可以提高采样效率",
            "精确度和召回率": "用于评估生成模型覆盖目标分布能力的指标"
        },
        "success": true
    },
    {
        "order": 502,
        "title": "Improved, Deterministic Smoothing for L_1 Certified Robustness",
        "html": "https://ICML.cc//virtual/2021/poster/9445",
        "abstract": "Randomized smoothing is a general technique for computing sample-dependent robustness guarantees against adversarial attacks for deep classifiers. Prior works on randomized smoothing against L_1 adversarial attacks use additive smoothing noise and provide probabilistic robustness guarantees. In this work, we propose a non-additive and deterministic smoothing method, Deterministic Smoothing with Splitting Noise (DSSN). To develop DSSN, we first develop SSN, a randomized method which involves generating each noisy smoothing sample by first randomly splitting the input space and then returning a representation of the center of the subdivision occupied by the input sample. In contrast to uniform additive smoothing, the SSN certification does not require the random noise components used to be independent. Thus, smoothing can be done effectively in just one dimension and can therefore be efficiently derandomized for quantized data (e.g., images). To the best of our knowledge, this is the first work to provide deterministic 'randomized smoothing' for a norm-based adversarial threat model while allowing for an arbitrary classifier (i.e., a deep model) to be used as a base classifier and without requiring an exponential number of smoothing samples. On CIFAR-10 and ImageNet datasets, we provide substantially larger L_1 robustness certificates compared to prior works, establishing a new state-of-the-art. The determinism of our method also leads to significantly faster certificate computation. Code is available at: https://github.com/alevine0/smoothingSplittingNoise.",
        "conference": "ICML",
        "中文标题": "改进的确定性平滑方法用于L_1认证鲁棒性",
        "摘要翻译": "随机平滑是一种通用技术，用于为深度分类器计算样本依赖的对抗攻击鲁棒性保证。先前关于针对L_1对抗攻击的随机平滑研究使用加性平滑噪声并提供概率鲁棒性保证。在本工作中，我们提出了一种非加性且确定性的平滑方法，即分裂噪声确定性平滑（DSSN）。为了开发DSSN，我们首先开发了SSN，这是一种随机方法，涉及通过首先随机分割输入空间，然后返回输入样本占据的分割中心的表示来生成每个噪声平滑样本。与均匀加性平滑相比，SSN认证不需要使用的随机噪声组件是独立的。因此，平滑可以仅在一个维度上有效进行，并且因此可以有效地对量化数据（例如，图像）进行确定性处理。据我们所知，这是第一个为基于范数的对抗威胁模型提供确定性'随机平滑'的工作，同时允许使用任意分类器（即，深度模型）作为基础分类器，并且不需要指数数量的平滑样本。在CIFAR-10和ImageNet数据集上，我们提供了比先前工作大得多的L_1鲁棒性证书，建立了新的最先进水平。我们方法的确定性也导致了证书计算的显著加快。代码可在以下网址获取：https://github.com/alevine0/smoothingSplittingNoise。",
        "领域": "对抗性防御、深度学习安全、图像分类",
        "问题": "提高深度分类器对抗L_1范数攻击的鲁棒性认证",
        "动机": "现有随机平滑方法使用加性噪声并提供概率保证，本研究旨在开发一种非加性、确定性的平滑方法，以提高认证的鲁棒性和计算效率",
        "方法": "提出分裂噪声确定性平滑（DSSN），通过随机分割输入空间并返回分割中心的表示来生成平滑样本，实现确定性处理",
        "关键词": [
            "确定性平滑",
            "L_1鲁棒性",
            "对抗性防御",
            "分裂噪声",
            "深度分类器"
        ],
        "涉及的技术概念": {
            "分裂噪声确定性平滑（DSSN）": "一种非加性、确定性的平滑方法，通过分裂输入空间并返回分割中心的表示来提高对抗攻击的鲁棒性认证",
            "随机分割输入空间": "在SSN方法中，首先随机分割输入空间，为生成平滑样本提供基础",
            "L_1鲁棒性证书": "用于量化分类器对L_1范数对抗攻击的抵抗能力，本研究通过DSSN方法显著提高了证书的大小和计算效率"
        },
        "success": true
    },
    {
        "order": 503,
        "title": "Improved OOD Generalization via Adversarial Training and Pretraing",
        "html": "https://ICML.cc//virtual/2021/poster/10511",
        "abstract": "Recently, learning a model that generalizes well on out-of-distribution (OOD) data has attracted great attention in the machine learning community. In this paper, after defining OOD generalization by Wasserstein distance, we theoretically justify that a model robust to input perturbation also generalizes well on OOD data. Inspired by previous findings that adversarial training helps improve robustness, we show that models trained by adversarial training have converged excess risk on OOD data. Besides, in the paradigm of pre-training then fine-tuning, we theoretically justify that the input perturbation robust model in the pre-training stage provides an initialization that generalizes well on downstream OOD data. Finally, various experiments conducted on image classification and natural language understanding tasks verify our theoretical findings.",
        "conference": "ICML",
        "中文标题": "通过对抗训练和预训练改进OOD泛化能力",
        "摘要翻译": "最近，在机器学习社区中，学习一个在分布外（OOD）数据上泛化能力强的模型引起了极大的关注。在本文中，通过使用Wasserstein距离定义OOD泛化后，我们从理论上证明了对输入扰动鲁棒的模型在OOD数据上也有良好的泛化能力。受到之前发现对抗训练有助于提高鲁棒性的启发，我们展示了通过对抗训练训练的模型在OOD数据上具有收敛的过剩风险。此外，在预训练后微调的范式中，我们从理论上证明了预训练阶段对输入扰动鲁棒的模型提供了一个在下游OOD数据上泛化能力良好的初始化。最后，在图像分类和自然语言理解任务上进行的各种实验验证了我们的理论发现。",
        "领域": "对抗学习、迁移学习、自然语言处理与视觉结合",
        "问题": "提高模型在分布外数据上的泛化能力",
        "动机": "探索对抗训练和预训练如何帮助模型在未见过的数据上表现更好",
        "方法": "使用对抗训练提高模型鲁棒性，并通过预训练和微调策略优化模型在OOD数据上的泛化能力",
        "关键词": [
            "对抗训练",
            "预训练",
            "OOD泛化",
            "Wasserstein距离",
            "模型鲁棒性"
        ],
        "涉及的技术概念": {
            "对抗训练": "通过引入对抗性样本训练模型，提高模型对输入扰动的鲁棒性",
            "Wasserstein距离": "用于衡量两个概率分布之间的距离，本文中用于定义OOD泛化",
            "预训练和微调": "先在大量数据上进行预训练，然后在特定任务上进行微调，以提高模型性能"
        },
        "success": true
    },
    {
        "order": 504,
        "title": "Improved Regret Bound and Experience Replay in Regularized Policy Iteration",
        "html": "https://ICML.cc//virtual/2021/poster/9177",
        "abstract": "In this work, we study algorithms for learning in infinite-horizon undiscounted Markov decision processes (MDPs) with function approximation. \nWe first show that the regret analysis of the Politex algorithm (a version of regularized policy iteration) can be sharpened from $O(T^{3/4})$ to $O(\\sqrt{T})$ under nearly identical assumptions, and instantiate the bound with linear function approximation.  Our result provides the first high-probability $O(\\sqrt{T})$ regret bound for a computationally efficient algorithm in this setting. The exact implementation of Politex with neural network function approximation is inefficient in terms of memory and computation. Since our analysis suggests that we need to approximate the average of the action-value functions of past policies well, we propose a simple efficient implementation where we train a single Q-function on a replay buffer with past data. We show that this often leads to superior performance over other implementation choices, especially in terms of wall-clock time. Our work also provides a novel theoretical justification for using experience replay within policy iteration algorithms.",
        "conference": "ICML",
        "success": true,
        "中文标题": "正则化策略迭代中改进的遗憾界限和经验回放",
        "摘要翻译": "在这项工作中，我们研究了使用函数逼近在无限视界无折扣马尔可夫决策过程（MDP）中学习的算法。我们首先证明，在几乎相同的假设下，Politex算法（正则化策略迭代的一个版本）的遗憾分析可以从$O(T^{3/4})$ 改进到 $O(\\sqrt{T})$，并使用线性函数逼近来实例化这个界限。我们的结果为在这种设置下计算高效算法提供了第一个高概率$O(\\sqrt{T})$遗憾界限。使用神经网络函数逼近的Politex的精确实现，在内存和计算方面是低效的。由于我们的分析表明，我们需要很好地近似过去策略的动作值函数的平均值，因此我们提出了一种简单的有效实现，即在具有过去数据的重放缓冲区上训练单个Q函数。我们表明，与其他实现选择相比，这通常会导致更好的性能，尤其是在挂钟时间方面。我们的工作还为在策略迭代算法中使用经验回放提供了一种新的理论依据。",
        "领域": "强化学习、函数逼近、策略优化",
        "问题": "如何提高正则化策略迭代算法在无限视界无折扣马尔可夫决策过程中的学习效率，并降低其计算和内存需求。",
        "动机": "现有的正则化策略迭代算法 Politex 在计算和内存方面效率低下，尤其是在使用神经网络进行函数逼近时。同时，理论上需要更好的分析和实践上更优的实现方法。",
        "方法": "1. 改进 Politex 算法的遗憾界限分析，将其从 $O(T^{3/4})$ 改进到 $O(\\sqrt{T})$。\\n2. 提出一种基于经验回放的 Politex 算法的简单高效实现，即在重放缓冲区上训练单个 Q 函数。",
        "关键词": [
            "正则化策略迭代",
            "经验回放",
            "马尔可夫决策过程",
            "函数逼近",
            "Q函数"
        ],
        "涉及的技术概念": {
            "正则化策略迭代": "一种强化学习算法，通过正则化来约束策略的更新，以提高算法的稳定性和泛化能力。",
            "经验回放": "一种强化学习技术，将过去的经验（状态、动作、奖励、下一个状态）存储在缓冲区中，并从中随机采样进行训练，以打破数据之间的相关性，提高学习效率。"
        }
    },
    {
        "order": 505,
        "title": "Improved Regret Bounds of Bilinear Bandits using Action Space Analysis",
        "html": "https://ICML.cc//virtual/2021/poster/10289",
        "abstract": "We consider the bilinear bandit problem where the learner chooses a pair of arms, each from two different action spaces of dimension $d_1$ and $d_2$, respectively. The learner then receives a reward whose expectation is a bilinear function of the two chosen arms with an unknown matrix parameter $\\Theta^*\\in\\mathbb{R}^{d_1 \\times d_2}$ with rank $r$. Despite abundant applications such as drug discovery, the optimal regret rate is unknown for this problem, though it was conjectured to be $\\tilde O(\\sqrt{d_1d_2(d_1+d_2)r T})$ by Jun et al. (2019) where $\\tilde O$ ignores polylogarithmic factors in $T$. In this paper, we make progress towards closing the gap between the upper and lower bound on the optimal regret. First, we reject the conjecture above by proposing algorithms that achieve the regret $\\tilde O(\\sqrt{d_1 d_2 (d_1+d_2) T})$ using the fact that the action space dimension $O(d_1+d_2)$ is significantly lower than the matrix parameter dimension $O(d_1 d_2)$. Second, we additionally devise an algorithm with better empirical performance than previous algorithms. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于动作空间分析的双线性Bandit算法的改进遗憾界",
        "摘要翻译": "我们考虑双线性bandit问题，其中学习者从两个不同的动作空间中选择一对臂，这两个动作空间的维度分别为$d_1$和$d_2$。然后，学习者获得一个奖励，其期望是两个所选臂的双线性函数，具有一个未知的矩阵参数 $\\\\Theta^*\\\\in\\\\mathbb{R}^{d_1 \\\\times d_2}$，其秩为$r$。尽管在药物发现等领域有大量的应用，但对于这个问题，最优遗憾率是未知的，尽管 Jun 等人 (2019) 猜测其为 $\\\\tilde O(\\\\sqrt{d_1d_2(d_1+d_2)r T})$，其中 $\\\\tilde O$ 忽略了 $T$ 中的多对数因子。在本文中，我们朝着缩小最优遗憾的上下界之间的差距迈出了步伐。首先，我们通过提出实现遗憾 $\\\\tilde O(\\\\sqrt{d_1 d_2 (d_1+d_2) T})$ 的算法来拒绝上述猜想，该算法利用了动作空间维度 $O(d_1+d_2)$ 远低于矩阵参数维度 $O(d_1 d_2)$ 的事实。其次，我们还设计了一种算法，其经验性能优于以前的算法。",
        "领域": "强化学习、在线学习、bandit算法",
        "问题": "如何改进双线性bandit算法的遗憾界，使其更接近理论最优值，并提高实际应用中的性能。",
        "动机": "现有的双线性bandit算法的遗憾界可能不是最优的，并且在实际应用中可能存在性能瓶颈。希望通过更有效地利用动作空间的结构信息，降低遗憾上界，并提升经验性能。",
        "方法": "通过分析动作空间维度远小于矩阵参数维度的特点，提出了新的算法，该算法能够实现更优的遗憾界。同时，还设计了一种经验性能更好的算法，并在实验中验证了其有效性。",
        "关键词": [
            "双线性bandit",
            "遗憾界",
            "动作空间分析",
            "在线学习",
            "药物发现"
        ],
        "涉及的技术概念": {
            "遗憾界": "衡量在线学习算法性能的指标，表示算法的累计损失与最优策略之间的差距。",
            "双线性bandit": "一种特殊的bandit问题，其中奖励函数是两个臂选择的双线性函数。"
        }
    },
    {
        "order": 506,
        "title": "Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies.",
        "html": "https://ICML.cc//virtual/2021/poster/9693",
        "abstract": "In this work, we focus on the ability of graph neural networks (GNNs) to learn long-range patterns in graphs with edge features. Learning patterns that involve longer paths in the graph, requires using deeper GNNs. However, GNNs suffer from a drop in performance with increasing network depth. To improve the performance of deeper GNNs, previous works have investigated normalization techniques and various types of skip connections. While they are designed to improve depth-wise backpropagation between the representations of the same node in successive layers, they do not improve breadth-wise backpropagation between representations of neighbouring nodes. To analyse the consequences, we design synthetic datasets serving as a testbed for the ability of GNNs to learn long-range patterns. Our analysis shows that several commonly used GNN variants with only depth-wise skip connections indeed have problems learning long-range patterns. They are clearly outperformed by an attention-based GNN architecture that we propose for improving both depth- and breadth-wise backpropagation. We also verify that the presented architecture is competitive on real-world data.",
        "conference": "ICML",
        "中文标题": "改进图神经网络中的广度反向传播有助于学习长距离依赖关系",
        "摘要翻译": "在这项工作中，我们关注图神经网络（GNNs）在具有边特征的图中学习长距离模式的能力。学习涉及图中较长路径的模式，需要使用更深的GNNs。然而，随着网络深度的增加，GNNs的性能会下降。为了提高更深GNNs的性能，先前的工作已经研究了归一化技术和各种类型的跳跃连接。虽然它们旨在改进同一节点在连续层表示之间的深度反向传播，但它们并未改进相邻节点表示之间的广度反向传播。为了分析后果，我们设计了合成数据集，作为GNNs学习长距离模式能力的测试平台。我们的分析表明，几种仅具有深度跳跃连接的常用GNN变体确实在学习长距离模式方面存在问题。它们明显被我们提出的基于注意力的GNN架构所超越，该架构旨在改进深度和广度反向传播。我们还验证了所提出的架构在真实世界数据上的竞争力。",
        "领域": "图神经网络、深度学习、图表示学习",
        "问题": "解决图神经网络在学习长距离模式时性能下降的问题",
        "动机": "提高图神经网络在处理长距离依赖关系时的性能",
        "方法": "提出了一种基于注意力的图神经网络架构，改进深度和广度反向传播",
        "关键词": [
            "图神经网络",
            "长距离依赖",
            "注意力机制",
            "反向传播",
            "图表示学习"
        ],
        "涉及的技术概念": {
            "广度反向传播": "改进相邻节点表示之间的信息传递，帮助学习长距离依赖关系",
            "注意力机制": "用于提升模型对图中重要节点和边的关注，优化信息传递路径",
            "跳跃连接": "通过跳过某些层直接连接，帮助缓解深度网络中的梯度消失问题"
        },
        "success": true
    },
    {
        "order": 507,
        "title": "Improving Generalization in Meta-learning via Task Augmentation",
        "html": "https://ICML.cc//virtual/2021/poster/9091",
        "abstract": "Meta-learning has proven to be a powerful paradigm for transferring the knowledge from previous tasks to facilitate the learning of a novel task. Current dominant algorithms train a well-generalized model initialization which is adapted to each task via the support set. The crux lies in optimizing the generalization capability of the initialization, which is measured by the performance of the adapted model on the query set of each task. Unfortunately, this generalization measure, evidenced by empirical results, pushes the initialization to overfit the meta-training tasks, which significantly impairs the generalization and adaptation to novel tasks. To address this issue, we actively augment a meta-training task with “more data” when evaluating the generalization. Concretely, we propose two task augmentation methods, including MetaMix and Channel Shuffle. MetaMix linearly combines features and labels of samples from both the support and query sets. For each class of samples, Channel Shuffle randomly replaces a subset of their channels with the corresponding ones from a different class. Theoretical studies show how task augmentation improves the generalization of meta-learning. Moreover, both MetaMix and Channel Shuffle outperform state-of-the-art results by a large margin across many datasets and are compatible with existing meta-learning algorithms.",
        "conference": "ICML",
        "中文标题": "通过任务增强改进元学习的泛化能力",
        "摘要翻译": "元学习已被证明是一种强大的范式，能够将先前任务的知识迁移以促进新任务的学习。当前主导的算法训练一个泛化良好的模型初始化，该初始化通过支持集适应每个任务。关键在于优化初始化的泛化能力，这是通过适应模型在每个任务的查询集上的表现来衡量的。不幸的是，实证结果表明，这种泛化测量方法使初始化过度拟合元训练任务，这严重损害了对新任务的泛化和适应。为了解决这个问题，我们在评估泛化时主动用“更多数据”增强元训练任务。具体来说，我们提出了两种任务增强方法，包括MetaMix和Channel Shuffle。MetaMix线性结合了来自支持集和查询集的样本的特征和标签。对于每一类样本，Channel Shuffle随机用来自不同类别的相应通道替换它们的一部分通道。理论研究表明任务增强如何提高元学习的泛化能力。此外，MetaMix和Channel Shuffle在许多数据集上都大幅超越了最先进的结果，并且与现有的元学习算法兼容。",
        "领域": "元学习",
        "问题": "解决元学习初始化模型在元训练任务上过拟合的问题",
        "动机": "提高元学习模型对新任务的泛化和适应能力",
        "方法": "提出了两种任务增强方法：MetaMix和Channel Shuffle",
        "关键词": [
            "元学习",
            "任务增强",
            "MetaMix",
            "Channel Shuffle",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "MetaMix": "线性结合支持集和查询集的样本特征和标签的任务增强方法",
            "Channel Shuffle": "随机替换样本通道以增强任务多样性的方法",
            "泛化能力": "模型在新任务上的适应和表现能力"
        },
        "success": true
    },
    {
        "order": 508,
        "title": "Improving Gradient Regularization using Complex-Valued Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10759",
        "abstract": "Gradient regularization is a neural network defense technique that requires no prior knowledge of an adversarial attack and that brings only limited increase in training computational complexity. A form of complex-valued neural network (CVNN) is proposed to improve the performance of gradient regularization on classification tasks of real-valued input in adversarial settings. The activation derivatives of each layer of the CVNN are dependent on the combination of inputs to the layer, and locally stable representations can be learned for inputs the network is trained on. Furthermore, the properties of the CVNN parameter derivatives resist decrease of performance on the standard objective that is caused by competition with the gradient regularization objective. Experimental results show that the performance of gradient regularized CVNN surpasses that of real-valued neural networks with comparable storage and computational complexity. Moreover, gradient regularized complex-valued networks exhibit robust performance approaching that of real-valued networks trained with multi-step adversarial training.",
        "conference": "ICML",
        "中文标题": "利用复数神经网络改进梯度正则化",
        "摘要翻译": "梯度正则化是一种神经网络防御技术，它不需要对抗攻击的先验知识，并且仅带来有限的训练计算复杂度增加。提出了一种复数神经网络（CVNN）的形式，以提高在对抗环境下对实值输入分类任务的梯度正则化性能。CVNN每一层的激活导数依赖于该层输入的组合，并且可以为网络训练过的输入学习到局部稳定的表示。此外，CVNN参数导数的特性抵抗了由梯度正则化目标与标准目标竞争引起的性能下降。实验结果表明，梯度正则化的CVNN性能超过了具有可比存储和计算复杂度的实值神经网络。此外，梯度正则化的复数网络展现出接近通过多步对抗训练训练的实值网络的鲁棒性能。",
        "领域": "对抗性机器学习、复数神经网络、梯度正则化",
        "问题": "提高在对抗环境下实值输入分类任务的梯度正则化性能",
        "动机": "探索复数神经网络在提高梯度正则化性能方面的潜力，特别是在对抗性环境中",
        "方法": "提出一种复数神经网络（CVNN）形式，利用其激活导数和参数导数的特性来改进梯度正则化",
        "关键词": [
            "复数神经网络",
            "梯度正则化",
            "对抗性机器学习",
            "分类任务",
            "鲁棒性能"
        ],
        "涉及的技术概念": {
            "复数神经网络（CVNN）": "一种处理复数数据的神经网络，能够通过其独特的激活和参数导数特性提高模型在对抗性环境中的性能",
            "梯度正则化": "一种防御技术，通过正则化梯度来提高模型对对抗攻击的鲁棒性，而不显著增加计算复杂度",
            "对抗性训练": "通过在多步过程中引入对抗样本来训练模型，以提高其在对抗性环境中的鲁棒性"
        },
        "success": true
    },
    {
        "order": 509,
        "title": "Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding",
        "html": "https://ICML.cc//virtual/2021/poster/10517",
        "abstract": "Latent variable models have been successfully applied in lossless compression with the bits-back coding algorithm. However, bits-back suffers from an increase in the bitrate equal to the KL divergence between the approximate posterior and the true posterior. In this paper, we show how to remove this gap asymptotically by deriving bits-back coding algorithms from tighter variational bounds. The key idea is to exploit extended space representations of Monte Carlo estimators of the marginal likelihood. Naively applied, our schemes would require more initial bits than the standard bits-back coder, but we show how to drastically reduce this additional cost with couplings in the latent space. When parallel architectures can be exploited, our coders can achieve better rates than bits-back with little additional cost. We demonstrate improved lossless compression rates in a variety of settings, especially in out-of-distribution or sequential data compression.",
        "conference": "ICML",
        "中文标题": "通过蒙特卡洛比特回退编码提升无损压缩率",
        "摘要翻译": "潜在变量模型已成功应用于无损压缩中的比特回退编码算法。然而，比特回退会因近似后验与真实后验之间的KL散度而增加比特率。在本文中，我们展示了如何通过从更紧密的变分边界推导比特回退编码算法来渐近消除这一差距。关键思想是利用蒙特卡洛边际似然估计器的扩展空间表示。如果直接应用，我们的方案将需要比标准比特回退编码器更多的初始比特，但我们展示了如何通过潜在空间中的耦合大幅减少这一额外成本。当可以利用并行架构时，我们的编码器可以以很少的额外成本实现比比特回退更好的速率。我们在多种设置中展示了改进的无损压缩率，特别是在分布外或顺序数据压缩中。",
        "领域": "无损数据压缩、变分推断、蒙特卡洛方法",
        "问题": "如何减少无损压缩中比特回退编码算法因近似后验与真实后验之间的KL散度而增加的比特率",
        "动机": "消除比特回退编码算法中因近似后验与真实后验之间的KL散度而导致的比特率增加，提升无损压缩效率",
        "方法": "通过从更紧密的变分边界推导比特回退编码算法，并利用蒙特卡洛边际似然估计器的扩展空间表示，减少额外比特成本",
        "关键词": [
            "无损压缩",
            "比特回退编码",
            "变分推断",
            "蒙特卡洛方法",
            "KL散度"
        ],
        "涉及的技术概念": {
            "比特回退编码": "一种用于无损压缩的算法，通过潜在变量模型实现数据压缩",
            "KL散度": "衡量近似后验分布与真实后验分布之间差异的指标，影响压缩效率",
            "蒙特卡洛边际似然估计器": "用于估计边际似然的蒙特卡洛方法，本文中通过其扩展空间表示优化编码效率"
        },
        "success": true
    },
    {
        "order": 510,
        "title": "Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity",
        "html": "https://ICML.cc//virtual/2021/poster/10323",
        "abstract": "Rationalizing which parts of a molecule drive the predictions of a molecular graph convolutional neural network (GCNN) can be difficult. To help, we propose two simple regularization techniques to apply during the training of GCNNs: Batch Representation Orthonormalization (BRO) and Gini regularization. BRO, inspired by molecular orbital theory, encourages graph convolution operations to generate orthonormal node embeddings. Gini regularization is applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions. We show that Gini and BRO regularization can improve the accuracy of state-of-the-art GCNN attribution methods on artificial benchmark datasets. In a real-world setting, we demonstrate that medicinal chemists significantly prefer explanations extracted from regularized models. While we only study these regularizers in the context of GCNNs, both can be applied to other types of neural networks.",
        "conference": "ICML",
        "中文标题": "通过正交化和诱导稀疏性提升分子图神经网络的可解释性",
        "摘要翻译": "合理化分子中哪些部分驱动分子图卷积神经网络（GCNN）的预测可能很困难。为了帮助解决这一问题，我们提出了两种简单的正则化技术应用于GCNN的训练过程中：批量表示正交化（BRO）和基尼正则化。BRO受到分子轨道理论的启发，鼓励图卷积操作生成正交的节点嵌入。基尼正则化应用于输出层的权重，限制了模型可用于预测的维度数量。我们展示了基尼和BRO正则化可以提高最先进的GCNN归因方法在人工基准数据集上的准确性。在真实世界场景中，我们证明了药物化学家显著偏好从正则化模型中提取的解释。虽然我们仅在GCNN的背景下研究这些正则化器，但两者都可以应用于其他类型的神经网络。",
        "领域": "图神经网络、药物发现、化学信息学",
        "问题": "提高分子图神经网络预测的可解释性",
        "动机": "解决分子图卷积神经网络预测驱动因素难以合理化的问题",
        "方法": "提出批量表示正交化（BRO）和基尼正则化两种正则化技术，应用于GCNN的训练过程中",
        "关键词": [
            "分子图神经网络",
            "可解释性",
            "正交化",
            "稀疏性",
            "正则化"
        ],
        "涉及的技术概念": {
            "批量表示正交化（BRO）": "受到分子轨道理论启发，鼓励图卷积操作生成正交的节点嵌入，以提高模型的可解释性",
            "基尼正则化": "应用于输出层的权重，限制模型可用于预测的维度数量，诱导稀疏性",
            "图卷积神经网络（GCNN）": "用于处理图结构数据的神经网络，特别适用于分子结构分析"
        },
        "success": true
    },
    {
        "order": 511,
        "title": "Improving Predictors via Combination Across Diverse Task Categories",
        "html": "https://ICML.cc//virtual/2021/poster/10419",
        "abstract": "Predictor combination is the problem of improving a task predictor using predictors of other tasks when the forms of individual predictors are unknown. Previous work approached this problem by nonparametrically assessing predictor relationships based on their joint evaluations on a shared sample. This limits their application to cases where all predictors are defined on the same task category, e.g. all predictors estimate attributes of shoes. We present a new predictor combination algorithm that overcomes this limitation. Our algorithm aligns the heterogeneous domains of different predictors in a shared latent space to facilitate comparisons of predictors independently of the domains on which they are originally defined. We facilitate this by a new data alignment scheme that matches data distributions across task categories. Based on visual attribute ranking experiments on datasets that span diverse task categories (e.g. shoes and animals), we demonstrate that our approach often significantly improves the performances of the initial predictors.",
        "conference": "ICML",
        "中文标题": "通过跨多样任务类别的组合改进预测器",
        "摘要翻译": "预测器组合的问题在于，当单个预测器的形式未知时，如何利用其他任务的预测器来改进一个任务的预测器。先前的工作通过基于共享样本上的联合评估非参数地评估预测器之间的关系来解决这个问题。这限制了它们的应用场景，即所有预测器必须在相同的任务类别上定义，例如所有预测器都估计鞋子的属性。我们提出了一种新的预测器组合算法，克服了这一限制。我们的算法将不同预测器的异构领域对齐到一个共享的潜在空间中，以便于比较预测器，而不受它们最初定义领域的影响。我们通过一种新的数据对齐方案来实现这一点，该方案匹配跨任务类别的数据分布。基于涵盖多样任务类别（例如鞋子和动物）的数据集上的视觉属性排序实验，我们证明了我们的方法通常能显著提高初始预测器的性能。",
        "领域": "多任务学习、跨领域学习、视觉属性预测",
        "问题": "如何在不同任务类别的预测器之间进行有效组合以提高预测性能",
        "动机": "克服现有预测器组合方法局限于相同任务类别的限制，实现跨多样任务类别的预测器组合",
        "方法": "提出一种新的预测器组合算法，通过将不同预测器的异构领域对齐到共享潜在空间，并采用新的数据对齐方案匹配跨任务类别的数据分布",
        "关键词": [
            "预测器组合",
            "跨领域学习",
            "数据对齐",
            "潜在空间",
            "多任务学习"
        ],
        "涉及的技术概念": {
            "预测器组合": "利用其他任务的预测器来改进一个任务的预测器，当单个预测器的形式未知时",
            "异构领域对齐": "将不同预测器的异构领域对齐到一个共享的潜在空间中，以便于比较预测器",
            "数据对齐方案": "匹配跨任务类别的数据分布，以支持预测器组合算法的实施"
        },
        "success": true
    },
    {
        "order": 512,
        "title": "Improving Ultrametrics Embeddings Through Coresets",
        "html": "https://ICML.cc//virtual/2021/poster/8775",
        "abstract": "To tackle the curse of dimensionality in data analysis and unsupervised learning, it is critical to be able to efficiently compute ``simple'' faithful representations of the data that helps extract information, improves understanding and visualization of the structure.  When the dataset consists of $d$-dimensional vectors, simple representations of the data may consist in trees or ultrametrics, and the goal is to best preserve the distances (i.e.: dissimilarity values) between data elements. To circumvent the quadratic running times of the most popular methods for fitting ultrametrics, such as average, single, or complete linkage,~\\citet{CKL20} recently presented a new algorithm that for any $c \\ge 1$, outputs in time $n^{1+O(1/c^2)}$ an ultrametric $\\Delta$ such that for any two points $u, v$, $\\Delta(u, v)$ is within a multiplicative factor of $5c$ to the distance between $u$ and $v$ in the ``best'' ultrametric representation.  We improve the above result and show how to improve the above guarantee from $5c$ to $\\sqrt{2}c + \\varepsilon$ while achieving the same asymptotic running time. To complement the improved theoretical bound, we additionally show that the performances of our algorithm are significantly better for various real-world datasets.",
        "conference": "ICML",
        "中文标题": "通过核心集改进超度量嵌入",
        "摘要翻译": "为了应对数据分析和无监督学习中的维度灾难，能够高效计算数据的‘简单’忠实表示至关重要，这有助于提取信息、改善对结构的理解和可视化。当数据集由d维向量组成时，数据的简单表示可能包括树或超度量，目标是最好地保留数据元素之间的距离（即：不相似性值）。为了避免拟合超度量最流行方法（如平均、单或完全链接）的二次运行时间，CKL20最近提出了一种新算法，对于任何c≥1，该算法在时间n^(1+O(1/c^2))内输出一个超度量Δ，使得对于任意两点u、v，Δ(u, v)与‘最佳’超度量表示中u和v之间的距离在乘法因子5c内。我们改进了上述结果，并展示了如何将上述保证从5c改进到√2c + ε，同时实现相同的渐近运行时间。为了补充改进的理论界限，我们还展示了我们的算法在各种实际数据集上的性能显著更好。",
        "领域": "无监督学习、数据降维、聚类分析",
        "问题": "解决在高维数据分析和无监督学习中，如何高效计算数据的简单忠实表示以保留数据元素之间的距离的问题。",
        "动机": "改进现有算法，减少在拟合超度量时的计算复杂度，同时提高距离保留的准确性。",
        "方法": "提出了一种改进的算法，能够在保持相同渐近运行时间的同时，将距离保留的乘法因子从5c降低到√2c + ε，并在实际数据集上验证了算法的性能。",
        "关键词": [
            "超度量嵌入",
            "核心集",
            "数据降维",
            "无监督学习",
            "聚类分析"
        ],
        "涉及的技术概念": {
            "超度量嵌入": "用于在数据分析和无监督学习中表示数据的一种方法，能够保留数据元素之间的距离。",
            "核心集": "一种数据子集，能够在保持原数据集某些性质的同时，减少计算复杂度。",
            "渐近运行时间": "算法在最坏情况下的运行时间增长率，用于评估算法的效率。"
        },
        "success": true
    },
    {
        "order": 513,
        "title": "Incentivized Bandit Learning with Self-Reinforcing User Preferences",
        "html": "https://ICML.cc//virtual/2021/poster/8731",
        "abstract": "In this paper, we investigate a new multi-armed bandit (MAB) online learning model that considers real-world phenomena in many recommender systems: (i) the learning agent cannot pull the arms by itself and thus has to offer rewards to users to incentivize arm-pulling indirectly; and (ii) if users with specific arm preferences are well rewarded, they induce a 'self-reinforcing' effect in the sense that they will attract more users of similar arm preferences. Besides addressing the tradeoff of exploration and exploitation, another key feature of this new MAB model is to balance reward and incentivizing payment. The goal of the agent is to maximize the total reward over a fixed time horizon $T$ with a low total payment. Our contributions in this paper are two-fold: (i) We propose a new MAB model with random arm selection that considers the relationship of users' self-reinforcing preferences and incentives; and (ii) We leverage the properties of a multi-color Polya urn with nonlinear feedback model to propose two MAB policies termed 'At-Least-$n$ Explore-Then-Commit' and 'UCB-List'. We prove that both policies achieve $O(log T)$ expected regret with $O(log T)$ expected payment over a time horizon $T$. We conduct numerical simulations to demonstrate and verify the performances of these two policies and study their robustness under various settings.",
        "conference": "ICML",
        "中文标题": "激励式老虎机学习与自我强化用户偏好",
        "摘要翻译": "在本文中，我们研究了一种新的多臂老虎机（MAB）在线学习模型，该模型考虑了许多推荐系统中的现实现象：（i）学习代理无法自行拉动臂，因此必须向用户提供奖励以间接激励臂拉动；（ii）如果具有特定臂偏好的用户得到良好奖励，它们会产生一种‘自我强化’效应，即会吸引更多具有相似臂偏好的用户。除了解决探索与利用的权衡外，这一新MAB模型的另一个关键特征是平衡奖励和激励支付。代理的目标是在固定时间范围$T$内以低总支付最大化总奖励。我们在本文中的贡献有两方面：（i）我们提出了一种新的MAB模型，该模型考虑了用户自我强化偏好与激励之间的关系，并采用随机臂选择；（ii）我们利用具有非线性反馈模型的多色Polya urn的性质，提出了两种MAB策略，分别称为‘至少$n$次探索后提交’和‘UCB列表’。我们证明了这两种策略在时间范围$T$内实现了$O(log T)$的预期遗憾和$O(log T)$的预期支付。我们进行了数值模拟以展示和验证这两种策略的性能，并在各种设置下研究了它们的鲁棒性。",
        "领域": "推荐系统、在线学习、多臂老虎机问题",
        "问题": "如何在推荐系统中平衡探索与利用以及奖励与激励支付，以最大化总奖励同时最小化总支付。",
        "动机": "研究推荐系统中用户偏好自我强化现象对激励机制设计的影响，以及如何有效利用这一现象优化推荐策略。",
        "方法": "提出了一种新的MAB模型，结合随机臂选择和用户自我强化偏好，并基于多色Polya urn的非线性反馈模型设计了两种策略：‘At-Least-$n$ Explore-Then-Commit’和‘UCB-List’。",
        "关键词": [
            "多臂老虎机",
            "自我强化偏好",
            "激励机制",
            "在线学习",
            "推荐系统"
        ],
        "涉及的技术概念": {
            "多臂老虎机（MAB）": "一种在线学习框架，用于在不确定环境下通过尝试不同的选择（臂）来最大化累积奖励。",
            "自我强化偏好": "用户偏好的一种现象，即用户的选择会吸引更多具有相似偏好的用户，形成正反馈循环。",
            "多色Polya urn模型": "一种概率模型，用于描述具有非线性反馈的随机过程，本文中用于建模用户偏好的动态变化。"
        },
        "success": true
    },
    {
        "order": 514,
        "title": "Incentivizing Compliance with Algorithmic Instruments",
        "html": "https://ICML.cc//virtual/2021/poster/10359",
        "abstract": "Randomized experiments can be susceptible to selection bias due to potential non-compliance by the participants. While much of the existing work has studied compliance as a static behavior, we propose a game-theoretic model to study compliance as dynamic behavior that may change over time. In rounds, a social planner interacts with a sequence of heterogeneous agents who arrive with their unobserved private type that determines both their prior preferences across the actions (e.g., control and treatment) and their baseline rewards without taking any treatment. The planner provides each agent with a randomized recommendation that may alter their beliefs and their action selection. We develop a novel recommendation mechanism that views the planner's recommendation as a form of instrumental variable (IV) that only affects an agents' action selection, but not the observed rewards. We construct such IVs by carefully mapping the history --the interactions between the planner and the previous agents-- to a random recommendation. Even though the initial agents may be completely non-compliant, our mechanism can incentivize compliance over time, thereby enabling the estimation of the treatment effect of each treatment, and minimizing the cumulative regret of the planner whose goal is to identify the optimal treatment.",
        "conference": "ICML",
        "中文标题": "激励算法工具的合规性",
        "摘要翻译": "随机实验可能因参与者的潜在不遵守行为而受到选择偏差的影响。虽然现有的大部分研究将遵守行为视为静态行为，但我们提出了一种博弈论模型，将遵守行为研究为可能随时间变化的动态行为。在多轮实验中，社会规划者与一系列异质代理互动，这些代理带着他们未观察到的私人类型到达，这些类型决定了他们在行动（如控制和治疗）之间的先验偏好以及不接受任何治疗的基线奖励。规划者为每个代理提供一个随机推荐，这可能会改变他们的信念和行动选择。我们开发了一种新颖的推荐机制，将规划者的推荐视为一种工具变量（IV），它只影响代理的行动选择，而不影响观察到的奖励。我们通过仔细将历史——规划者与先前代理之间的互动——映射到随机推荐来构建这样的工具变量。尽管初始代理可能完全不遵守，但我们的机制可以随着时间的推移激励遵守行为，从而能够估计每种治疗的治疗效果，并最小化规划者的累积遗憾，其目标是确定最佳治疗。",
        "领域": "博弈论与机制设计、算法激励、随机实验分析",
        "问题": "解决随机实验中因参与者不遵守行为导致的选择偏差问题",
        "动机": "研究如何通过动态博弈论模型激励参与者遵守行为，以提高随机实验的准确性和效率",
        "方法": "提出了一种基于工具变量的推荐机制，通过历史互动映射到随机推荐，激励参与者随时间变化的遵守行为",
        "关键词": [
            "博弈论模型",
            "工具变量",
            "动态遵守行为",
            "随机实验",
            "累积遗憾"
        ],
        "涉及的技术概念": {
            "工具变量（IV）": "在论文中用作一种推荐机制，仅影响代理的行动选择而不直接影响观察到的奖励，用于解决选择偏差问题",
            "动态遵守行为": "研究参与者随时间变化的遵守行为，区别于传统的静态遵守行为研究",
            "累积遗憾": "规划者在识别最佳治疗过程中的累积损失，论文旨在通过激励遵守行为最小化这一遗憾"
        },
        "success": true
    },
    {
        "order": 515,
        "title": "In-Database Regression in Input Sparsity Time",
        "html": "https://ICML.cc//virtual/2021/poster/8647",
        "abstract": "Sketching is a powerful dimensionality reduction technique for accelerating algorithms for data analysis. A crucial step in sketching methods is to compute a subspace embedding (SE) for a large matrix $A \\in \\mathbb{R}^{N \\times d}$. SE's are the primary tool for obtaining extremely efficient solutions for many linear-algebraic tasks, such as least squares regression and low rank approximation.  Computing an SE often requires an explicit representation of $A$ and running time proportional to the size of $A$. However, if $A= T_1 \\Join T_2 \\Join \\dots \\Join T_m$ is the  result of a database join query on several smaller tables  $T_i \\in \\mathbb{R}^{n_i \\times d_i}$, then this running time can be prohibitive, as $A$ itself can have as many as $O(n_1 n_2 \\cdots n_m)$ rows. In this work, we design subspace embeddings for database joins which can be computed significantly faster than computing the join.  For the case of a two table join $A = T_1 \\Join T_2$ we give input-sparsity algorithms for computing subspace embeddings, with running time bounded by the number of non-zero entries in $T_1,T_2$. This results in input-sparsity time algorithms for high accuracy regression, significantly improving upon the running time of prior FAQ-based methods for regression. We extend our results to arbitrary joins for the ridge regression problem, also considerably improving the running time of prior methods. Empirically, we apply our method to real datasets and show that it is significantly faster than existing algorithms. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "数据库内回归的输入稀疏时间处理",
        "摘要翻译": "素描是一种强大的降维技术，用于加速数据分析算法。素描方法中的一个关键步骤是为大型矩阵A ∈ ℝ^(N×d)计算子空间嵌入（SE）。SE是获得许多线性代数任务（如最小二乘回归和低秩近似）极高效解决方案的主要工具。计算SE通常需要A的显式表示和与A大小成比例的运行时间。然而，如果A = T₁⋈T₂⋈…⋈T_m是几个较小表T_i ∈ ℝ^(n_i×d_i)上数据库连接查询的结果，那么这种运行时间可能会非常长，因为A本身可以有O(n₁n₂⋯n_m)行。在这项工作中，我们设计了用于数据库连接的子空间嵌入，其计算速度比计算连接快得多。对于两表连接A = T₁⋈T₂的情况，我们给出了计算子空间嵌入的输入稀疏算法，其运行时间受限于T₁,T₂中的非零条目数量。这为高精度回归提供了输入稀疏时间算法，显著改善了之前基于FAQ的回归方法的运行时间。我们将结果扩展到岭回归问题的任意连接，也显著改善了之前方法的运行时间。实证上，我们将我们的方法应用于真实数据集，并显示它比现有算法快得多。",
        "领域": "数据库内分析, 线性代数加速, 高维数据处理",
        "问题": "如何高效处理大型数据库连接查询结果的子空间嵌入计算，以加速线性代数任务如回归分析。",
        "动机": "针对数据库连接查询结果的高维矩阵，传统子空间嵌入计算方法因运行时间与矩阵大小成比例而效率低下，需要更快的算法。",
        "方法": "设计了针对数据库连接查询结果的输入稀疏子空间嵌入算法，显著减少计算时间，特别是在两表连接情况下，运行时间仅与非零条目数量相关。",
        "关键词": [
            "子空间嵌入",
            "输入稀疏算法",
            "数据库连接",
            "回归分析",
            "岭回归"
        ],
        "涉及的技术概念": {
            "子空间嵌入": "用于降维和加速线性代数任务计算的技术，能够在大矩阵上高效执行。",
            "输入稀疏算法": "针对稀疏输入优化的算法，运行时间与输入中的非零元素数量相关，提高了处理效率。",
            "数据库连接": "数据库中表之间的连接操作，可能导致结果矩阵行数激增，影响后续处理效率。"
        }
    },
    {
        "order": 516,
        "title": "Inference for Network Regression Models with Community Structure",
        "html": "https://ICML.cc//virtual/2021/poster/9945",
        "abstract": "Network regression models, where the outcome comprises the valued edge in a network and the predictors are actor or dyad-level covariates, are used extensively in the social and biological sciences. Valid inference relies on accurately modeling the residual dependencies among the relations.  Frequently homogeneity assumptions are placed on the errors which are commonly incorrect and ignore critical natural clustering of the actors.  In this work, we present a novel regression modeling framework that models the errors as resulting from a community-based dependence structure and exploits the subsequent exchangeability properties of the error distribution to obtain parsimonious standard errors for regression parameters.",
        "conference": "ICML",
        "success": true,
        "中文标题": "具有社区结构的网络回归模型推断",
        "摘要翻译": "网络回归模型，其中结果包括网络中的有价值边，预测因子是参与者或二元组级别的协变量，在社会科学和生物科学中被广泛使用。有效的推断依赖于准确建模关系之间的残差依赖性。经常对误差施加同质性假设，这些假设通常是不正确的，并且忽略了参与者的关键自然聚类。在这项工作中，我们提出了一个新颖的回归建模框架，该框架将误差建模为由基于社区的依赖结构产生，并利用误差分布的后续可交换性属性来获得回归参数的简约标准误差。",
        "领域": "社会网络分析, 生物网络分析, 统计建模",
        "问题": "解决网络回归模型中残差依赖性的准确建模问题，以及传统同质性假设忽略参与者自然聚类的不足。",
        "动机": "为了提供更准确的网络回归模型推断方法，考虑到参与者的自然聚类和残差依赖性。",
        "方法": "提出一个新颖的回归建模框架，该框架通过基于社区的依赖结构建模误差，并利用误差分布的可交换性属性来简化回归参数的标准误差计算。",
        "关键词": [
            "网络回归模型",
            "社区结构",
            "残差依赖性",
            "统计推断",
            "可交换性"
        ],
        "涉及的技术概念": {
            "社区结构": "在模型中用于描述参与者自然聚类的结构，影响误差的依赖性。",
            "残差依赖性": "指网络关系中未被预测变量解释的部分之间的统计依赖性，是模型推断的关键。",
            "可交换性": "一种统计属性，允许简化误差分布的处理，从而获得更简约的标准误差估计。"
        }
    },
    {
        "order": 517,
        "title": "Inferring Latent Dynamics Underlying Neural Population Activity via Neural Differential Equations",
        "html": "https://ICML.cc//virtual/2021/poster/9219",
        "abstract": "An important problem in systems neuroscience is to identify the latent dynamics underlying neural population activity. Here we address this problem by introducing a low-dimensional nonlinear model for latent neural population dynamics using neural ordinary differential equations (neural ODEs), with noisy sensory inputs and Poisson spike train outputs. We refer to this as the Poisson Latent Neural Differential Equations (PLNDE) model. We apply the PLNDE framework to a variety of synthetic datasets, and show that it accurately infers the phase portraits and fixed points of nonlinear systems augmented to produce spike train data, including the FitzHugh-Nagumo oscillator, a 3-dimensional nonlinear spiral, and a nonlinear sensory decision-making model with attractor dynamics. Our model significantly outperforms existing methods at inferring single-trial neural firing rates and the corresponding latent trajectories that generated them, especially in the regime where the spike counts and number of trials are low. We then apply our model to multi-region neural population recordings from medial frontal cortex of rats performing an auditory decision-making task. Our model provides a general, interpretable framework for investigating the neural mechanisms of decision-making and other cognitive computations through the lens of dynamical systems.",
        "conference": "ICML",
        "中文标题": "通过神经微分方程推断神经群体活动背后的潜在动态",
        "摘要翻译": "在系统神经科学中，一个重要的问题是识别神经群体活动背后的潜在动态。我们通过引入一个低维非线性模型来解决这个问题，该模型使用神经常微分方程（神经ODE）来描述潜在的神经群体动态，包括噪声感官输入和泊松尖峰序列输出。我们将其称为泊松潜在神经微分方程（PLNDE）模型。我们将PLNDE框架应用于多种合成数据集，并展示它能够准确推断出非线性系统的相位图和固定点，这些系统被增强以产生尖峰序列数据，包括FitzHugh-Nagumo振荡器、一个三维非线性螺旋和一个具有吸引子动态的非线性感官决策模型。我们的模型在推断单次试验神经放电率和生成它们的相应潜在轨迹方面显著优于现有方法，尤其是在尖峰计数和试验次数较低的情况下。然后，我们将我们的模型应用于大鼠执行听觉决策任务时内侧前额叶皮层的多区域神经群体记录。我们的模型提供了一个通用、可解释的框架，用于通过动态系统的视角研究决策和其他认知计算的神经机制。",
        "领域": "神经动力学建模, 系统神经科学, 认知计算",
        "问题": "识别神经群体活动背后的潜在动态",
        "动机": "为了更准确地理解和模拟神经群体活动背后的动态过程，尤其是在尖峰计数和试验次数较低的情况下",
        "方法": "引入泊松潜在神经微分方程（PLNDE）模型，使用神经常微分方程描述潜在的神经群体动态",
        "关键词": [
            "神经微分方程",
            "潜在动态",
            "泊松尖峰序列",
            "认知计算",
            "动态系统"
        ],
        "涉及的技术概念": {
            "神经常微分方程（神经ODE）": "用于描述潜在的神经群体动态，包括噪声感官输入和泊松尖峰序列输出",
            "泊松潜在神经微分方程（PLNDE）模型": "一个低维非线性模型，用于推断神经群体活动背后的潜在动态",
            "动态系统": "提供了一个框架，用于研究决策和其他认知计算的神经机制"
        },
        "success": true
    },
    {
        "order": 518,
        "title": "Inferring serial correlation with dynamic backgrounds",
        "html": "https://ICML.cc//virtual/2021/poster/8639",
        "abstract": "Sequential data with serial correlation and an unknown, unstructured, and dynamic background is ubiquitous in neuroscience, psychology, and econometrics. Inferring serial correlation for such data is a fundamental challenge in statistics. We propose a Total Variation (TV) constrained least square estimator coupled with hypothesis tests to infer the serial correlation in the presence of unknown and unstructured dynamic background. The TV constraint on the dynamic background encourages a piecewise constant structure, which can approximate a wide range of dynamic backgrounds. The tuning parameter is selected via the Ljung-Box test to control the bias-variance trade-off. We establish a non-asymptotic upper bound for the estimation error through variational inequalities. We also derive a lower error bound via Fano's method and show the proposed method is near-optimal. Numerical simulation and a real study in psychology demonstrate the excellent performance of our proposed method compared with the state-of-the-art.",
        "conference": "ICML",
        "中文标题": "推断具有动态背景的序列相关性",
        "摘要翻译": "在神经科学、心理学和计量经济学中，具有序列相关性及未知、非结构化、动态背景的序列数据无处不在。推断此类数据的序列相关性是统计学中的一个基本挑战。我们提出了一种结合假设检验的总变差（TV）约束最小二乘估计器，以在存在未知和非结构化动态背景的情况下推断序列相关性。对动态背景的TV约束鼓励了分段常数结构，这种结构可以近似广泛的动态背景。通过Ljung-Box测试选择调节参数以控制偏差-方差权衡。我们通过变分不等式建立了估计误差的非渐近上界。我们还通过Fano方法推导了下界误差，并展示了所提出方法接近最优。数值模拟和心理学中的实际研究表明，与现有技术相比，我们提出的方法具有优异的性能。",
        "领域": "时间序列分析、统计建模、心理测量学",
        "问题": "在存在未知和非结构化动态背景的情况下，如何准确推断序列数据的序列相关性。",
        "动机": "解决在神经科学、心理学和计量经济学等领域中，处理具有复杂动态背景的序列数据时，序列相关性推断的挑战。",
        "方法": "提出了一种结合总变差（TV）约束的最小二乘估计器和假设检验的方法，用于在动态背景下推断序列相关性，并通过Ljung-Box测试选择调节参数以优化性能。",
        "关键词": [
            "序列相关性",
            "总变差约束",
            "动态背景",
            "Ljung-Box测试",
            "非渐近分析"
        ],
        "涉及的技术概念": {
            "总变差（TV）约束": "用于鼓励动态背景具有分段常数结构，从而近似广泛的动态背景。",
            "Ljung-Box测试": "用于选择调节参数，控制估计过程中的偏差-方差权衡。",
            "非渐近分析": "通过变分不等式和Fano方法，建立了估计误差的上下界，证明了方法的近优性。"
        },
        "success": true
    },
    {
        "order": 519,
        "title": "Infinite-Dimensional Optimization for Zero-Sum Games via Variational Transport",
        "html": "https://ICML.cc//virtual/2021/poster/9863",
        "abstract": "Game optimization has been extensively studied when decision variables lie in a finite-dimensional space, of which solutions correspond to pure strategies at the Nash equilibrium (NE), and the gradient descent-ascent (GDA) method works widely in practice. In this paper, we consider infinite-dimensional zero-sum games by a min-max distributional optimization problem over a space of probability measures defined on a continuous variable set, which is inspired by finding a mixed NE for finite-dimensional zero-sum games. We then aim to answer the following question: \\textit{Will GDA-type algorithms still be provably efficient when extended to infinite-dimensional zero-sum games?} To answer this question, we propose a particle-based variational transport algorithm based on GDA in the functional spaces. Specifically,  the algorithm performs multi-step functional gradient descent-ascent in the Wasserstein space via pushing two sets of particles in the variable space. By characterizing the gradient estimation error from variational form maximization and the convergence behavior of each player with different objective landscapes, we prove rigorously that the generalized GDA algorithm converges to the NE or the value of the game efficiently for a class of games under the Polyak-\\L ojasiewicz (PL) condition. To conclude, we provide complete statistical and convergence guarantees for solving an infinite-dimensional zero-sum game via a provably efficient particle-based method. Additionally, our work provides the first thorough statistical analysis for the particle-based algorithm to learn an objective functional with a variational form using  universal approximators (\\textit{i.e.}, neural networks (NNs)), \nwhich is of independent interest.",
        "conference": "ICML",
        "中文标题": "通过变分传输的无限维零和博弈优化",
        "摘要翻译": "当决策变量位于有限维空间时，博弈优化已被广泛研究，其解对应于纳什均衡（NE）下的纯策略，而梯度下降-上升（GDA）方法在实践中广泛应用。在本文中，我们通过在一个连续变量集上定义的概率测度空间上的最小-最大分布优化问题，考虑了无限维零和博弈，这是受到寻找有限维零和博弈的混合NE的启发。然后，我们旨在回答以下问题：当扩展到无限维零和博弈时，GDA类型算法是否仍然具有可证明的效率？为了回答这个问题，我们提出了一种基于GDA在函数空间中的粒子变分传输算法。具体来说，该算法通过在变量空间中推动两组粒子，在Wasserstein空间中执行多步函数梯度下降-上升。通过描述变分形式最大化中的梯度估计误差和每个玩家在不同目标景观下的收敛行为，我们严格证明了广义GDA算法在Polyak-Łojasiewicz（PL）条件下对一类博弈有效地收敛到NE或博弈值。总之，我们为通过一种可证明有效的基于粒子的方法解决无限维零和博弈提供了完整的统计和收敛保证。此外，我们的工作首次为基于粒子的算法提供了全面的统计分析，以学习具有变分形式的目标函数，使用通用逼近器（即神经网络（NNs）），这具有独立的意义。",
        "领域": "博弈论优化、机器学习优化、概率测度空间优化",
        "问题": "解决无限维零和博弈中的优化问题，并验证GDA类型算法在此类问题中的效率。",
        "动机": "受到有限维零和博弈中寻找混合纳什均衡的启发，探索无限维空间中的博弈优化问题及其解决方法。",
        "方法": "提出了一种基于GDA的粒子变分传输算法，通过在Wasserstein空间中执行多步函数梯度下降-上升，推动两组粒子在变量空间中移动。",
        "关键词": [
            "无限维零和博弈",
            "变分传输",
            "梯度下降-上升",
            "Wasserstein空间",
            "Polyak-Łojasiewicz条件"
        ],
        "涉及的技术概念": {
            "变分传输": "用于在无限维空间中优化概率测度，通过变分方法调整粒子分布以达到优化目标。",
            "Wasserstein空间": "提供了一个度量概率测度之间距离的框架，用于分析和优化粒子分布。",
            "Polyak-Łojasiewicz条件": "用于证明算法收敛性的关键条件，确保在优化过程中能够有效地接近最优解。"
        },
        "success": true
    },
    {
        "order": 520,
        "title": "Information Obfuscation of Graph Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8413",
        "abstract": "While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.  \n",
        "conference": "ICML",
        "中文标题": "图神经网络的信息混淆",
        "摘要翻译": "尽管图神经网络（GNNs）的出现极大地改善了许多应用中的节点和图表示学习，但邻域聚合方案暴露了额外的脆弱性，使得攻击者能够提取关于敏感属性的节点级信息。在本文中，我们研究了在学习图结构数据时通过信息混淆保护敏感属性的问题。我们提出了一个框架，通过对抗训练与总变差和Wasserstein距离，在本地过滤掉预定的敏感属性。我们的方法创建了一个强大的防御机制来对抗推理攻击，同时只在任务性能上遭受很小的损失。理论上，我们分析了我们的框架对抗最坏情况攻击者的有效性，并描述了在最大化预测准确性和最小化信息泄露之间的固有权衡。在来自推荐系统、知识图谱和量子化学的多个数据集上的实验表明，所提出的方法在各种图结构和任务中提供了强大的防御，同时为下游任务生成了具有竞争力的GNN编码器。",
        "领域": "图神经网络安全、隐私保护机器学习、对抗性防御",
        "问题": "保护图结构数据中的敏感属性不被推理攻击提取",
        "动机": "解决图神经网络在处理图结构数据时，邻域聚合方案可能暴露敏感属性的问题",
        "方法": "通过对抗训练结合总变差和Wasserstein距离，在本地过滤敏感属性",
        "关键词": [
            "图神经网络",
            "信息混淆",
            "对抗训练",
            "隐私保护",
            "推理攻击防御"
        ],
        "涉及的技术概念": {
            "对抗训练": "用于通过模拟攻击者的行为来训练模型，以提高模型对攻击的鲁棒性",
            "总变差": "用于衡量信息混淆过程中的变化量，帮助控制信息泄露",
            "Wasserstein距离": "用于评估两个概率分布之间的差异，优化信息混淆的效果"
        },
        "success": true
    },
    {
        "order": 521,
        "title": "Instabilities of Offline RL with Pre-Trained Neural Representation",
        "html": "https://ICML.cc//virtual/2021/poster/9197",
        "abstract": "In offline reinforcement learning (RL), we seek to utilize offline data to evaluate (or learn) policies in scenarios where the data are collected from a distribution that substantially differs from that of the target policy to be evaluated. Recent theoretical advances have shown that such sample-efficient offline RL is indeed possible provided certain strong representational conditions hold, else there are lower bounds exhibiting exponential error amplification (in the problem horizon) unless the data collection distribution has only a mild distribution shift relative to the target policy. This work studies these issues from an empirical perspective to gauge how stable offline RL methods are. In particular, our methodology explores these ideas when using features from pre-trained neural networks, in the hope that these representations are powerful enough to permit sample efficient offline RL. Through extensive experiments on a range of tasks, we see that substantial error amplification does occur even when using such pre-trained representations (trained on the same task itself); we find offline RL is stable only under extremely mild distribution shift. The implications of these results, both from a theoretical and an empirical perspective, are that successful offline RL (where we seek to go beyond the low distribution shift regime) requires substantially stronger conditions beyond those which suffice for successful supervised learning. ",
        "conference": "ICML",
        "中文标题": "预训练神经表征下离线强化学习的不稳定性",
        "摘要翻译": "在离线强化学习（RL）中，我们试图利用离线数据来评估（或学习）策略，这些数据收集自与待评估目标策略分布显著不同的场景。最近的理论进展表明，只要某些强有力的表征条件成立，这种样本高效的离线RL确实是可能的，否则除非数据收集分布相对于目标策略只有轻微的分布偏移，否则存在展示指数级误差放大（在问题范围内）的下界。这项工作从实证角度研究了这些问题，以评估离线RL方法的稳定性。特别是，我们的方法在探索这些想法时使用了预训练神经网络的特征，希望这些表征足够强大以允许样本高效的离线RL。通过对一系列任务的广泛实验，我们发现即使使用这样的预训练表征（在同一任务上训练），显著的误差放大确实会发生；我们发现离线RL仅在极其轻微的分布偏移下是稳定的。这些结果的含义，无论是从理论还是实证角度来看，都表明成功的离线RL（我们寻求超越低分布偏移制度）需要比那些足以成功监督学习更强的条件。",
        "领域": "强化学习、表征学习、策略评估",
        "问题": "离线强化学习在数据分布与目标策略分布显著不同时的稳定性和样本效率问题",
        "动机": "探索预训练神经表征是否能提供足够的样本效率以支持离线强化学习，尤其是在数据分布与目标策略分布存在显著差异时",
        "方法": "通过使用预训练神经网络的特征进行广泛的实验，评估离线强化学习方法的稳定性",
        "关键词": [
            "离线强化学习",
            "预训练表征",
            "分布偏移",
            "样本效率",
            "误差放大"
        ],
        "涉及的技术概念": {
            "离线强化学习": "在不需要与环境交互的情况下，利用已有数据学习或评估策略的强化学习方法",
            "预训练神经表征": "通过预训练神经网络获得的特征表示，旨在提高学习任务的样本效率",
            "分布偏移": "数据收集分布与目标策略分布之间的差异，影响学习算法的泛化能力和稳定性"
        },
        "success": true
    },
    {
        "order": 522,
        "title": "Instance-Optimal Compressed Sensing via Posterior Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/8875",
        "abstract": "  We characterize the measurement complexity of compressed sensing of\n  signals drawn from a known prior distribution, even when the support\n  of the prior is the entire space (rather than, say, sparse vectors).\n  We show for Gaussian measurements and \\emph{any} prior distribution\n  on the signal, that the posterior sampling estimator achieves\n  near-optimal recovery guarantees.  Moreover, this result is robust\n  to model mismatch, as long as the distribution estimate (e.g., from\n  an invertible generative model) is close to the true distribution in\n  Wasserstein distance. We implement the posterior sampling\n  estimator for deep generative priors using Langevin dynamics, and\n  empirically find that it produces accurate estimates with more\n  diversity than MAP.",
        "conference": "ICML",
        "中文标题": "通过后验采样实现实例最优的压缩感知",
        "摘要翻译": "我们描述了从已知先验分布中提取信号进行压缩感知的测量复杂度，即使先验的支持是整个空间（而不是稀疏向量等）。我们证明了对于高斯测量和信号的任何先验分布，后验采样估计器都能实现接近最优的恢复保证。此外，只要分布估计（例如，来自可逆生成模型）在Wasserstein距离上接近真实分布，这个结果对模型失配是鲁棒的。我们使用Langevin动力学实现了深度生成先验的后验采样估计器，并经验性地发现它比MAP产生更准确的估计和更多的多样性。",
        "领域": "压缩感知、生成模型、信号恢复",
        "问题": "在压缩感知中，如何从已知先验分布中高效恢复信号，即使信号不稀疏。",
        "动机": "探索在信号不稀疏的情况下，利用后验采样方法实现接近最优的信号恢复，并验证其对模型失配的鲁棒性。",
        "方法": "采用后验采样估计器结合Langevin动力学，对深度生成先验进行实现，以恢复信号。",
        "关键词": [
            "压缩感知",
            "后验采样",
            "Langevin动力学",
            "生成模型",
            "Wasserstein距离"
        ],
        "涉及的技术概念": {
            "后验采样估计器": "用于从已知先验分布中恢复信号的估计器，能够实现接近最优的恢复保证。",
            "Langevin动力学": "一种用于实现深度生成先验后验采样的方法，有助于提高估计的准确性和多样性。",
            "Wasserstein距离": "用于衡量分布估计与真实分布之间差异的度量，确保模型对失配的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 523,
        "title": "Instance Specific Approximations for Submodular Maximization",
        "html": "https://ICML.cc//virtual/2021/poster/9073",
        "abstract": "The predominant measure for the performance of an algorithm is its worst-case approximation guarantee. While worst-case approximations give desirable robustness guarantees, they can differ significantly from the performance of an algorithm in practice.  For the problem of monotone submodular maximization under a cardinality constraint, the greedy algorithm is known to obtain a 1-1/e  approximation guarantee, which is optimal for a polynomial-time algorithm.  However, very little is known about the approximation achieved by greedy and other submodular maximization algorithms on real instances.\n\nWe develop an algorithm that gives an instance-specific approximation for any solution of an instance of monotone submodular maximization under a cardinality constraint. This algorithm uses a novel dual approach to submodular maximization. In particular, it relies on the construction of a lower bound to the dual objective that can also be exactly minimized. We use this algorithm to show that on a wide variety of real-world datasets and objectives, greedy and other algorithms find solutions that approximate the optimal solution significantly better than the 1-1/e ~ 0.63 worst-case approximation guarantee, often exceeding 0.9.",
        "conference": "ICML",
        "中文标题": "针对子模最大化的实例特定近似方法",
        "摘要翻译": "算法性能的主要衡量标准是其最坏情况下的近似保证。虽然最坏情况下的近似提供了理想的鲁棒性保证，但它们可能与算法在实际中的表现有显著差异。对于在基数约束下的单调子模最大化问题，已知贪心算法可以获得1-1/e的近似保证，这对于多项式时间算法是最优的。然而，关于贪心算法和其他子模最大化算法在实际实例上达到的近似程度知之甚少。我们开发了一种算法，该算法为基数约束下的单调子模最大化实例的任何解提供了实例特定的近似。该算法使用了一种新颖的子模最大化对偶方法。特别是，它依赖于可以精确最小化的对偶目标下界的构建。我们使用该算法表明，在多种真实世界的数据集和目标上，贪心算法和其他算法找到的解比1-1/e ~ 0.63的最坏情况近似保证显著更好，通常超过0.9。",
        "领域": "组合优化, 算法设计, 机器学习",
        "问题": "解决在基数约束下的单调子模最大化问题中，算法在实际实例上的表现与最坏情况下的近似保证之间的差距问题。",
        "动机": "研究动机是为了更好地理解贪心算法和其他子模最大化算法在实际应用中的表现，以及开发能够提供实例特定近似保证的算法。",
        "方法": "采用了一种新颖的对偶方法，通过构建可以精确最小化的对偶目标下界，为基数约束下的单调子模最大化实例的任何解提供实例特定的近似。",
        "关键词": [
            "子模最大化",
            "实例特定近似",
            "贪心算法",
            "基数约束",
            "对偶方法"
        ],
        "涉及的技术概念": {
            "子模最大化": "在基数约束下寻找单调子模函数的最大值的问题，广泛应用于机器学习、数据挖掘等领域。",
            "实例特定近似": "针对特定实例提供比最坏情况近似保证更好的近似解的方法。",
            "对偶方法": "通过构建和最小化对偶目标下界来求解原问题的新颖方法。"
        },
        "success": true
    },
    {
        "order": 524,
        "title": "Integer Programming for Causal Structure Learning in the Presence of Latent Variables",
        "html": "https://ICML.cc//virtual/2021/poster/10075",
        "abstract": "The problem of finding an ancestral acyclic directed mixed graph (ADMG) that represents the causal relationships between a set of variables is an important area of research on causal inference. Most existing score-based structure learning methods focus on learning directed acyclic graph (DAG) models without latent variables. A number of  score-based methods have recently been proposed for the ADMG learning, yet they are heuristic in nature and do not guarantee an optimal solution. We propose a novel exact score-based method that solves an integer programming (IP) formulation and returns a score-maximizing ancestral ADMG for a set of continuous variables that follow a multivariate Gaussian distribution. We generalize the state-of-the-art IP model for DAG learning problems and derive new classes of valid inequalities to formulate an IP model for ADMG learning. Empirically, our model can be solved efficiently for medium-sized problems and achieves better accuracy than state-of-the-art score-based methods as well as benchmark constraint-based methods.",
        "conference": "ICML",
        "中文标题": "存在潜在变量情况下因果结构学习的整数规划方法",
        "摘要翻译": "寻找一个代表一组变量间因果关系的祖先无环有向混合图（ADMG）的问题是因果推理研究中的一个重要领域。大多数现有的基于分数的结构学习方法专注于学习没有潜在变量的有向无环图（DAG）模型。最近，已经提出了许多基于分数的方法用于ADMG学习，但它们本质上是启发式的，并不保证最优解。我们提出了一种新颖的精确基于分数的方法，该方法解决了一个整数规划（IP）公式，并为一组遵循多元高斯分布的连续变量返回一个分数最大化的祖先ADMG。我们泛化了最先进的DAG学习问题的IP模型，并推导出新的有效不等式类别来为ADMG学习制定IP模型。经验上，我们的模型可以有效地解决中等规模的问题，并且比最先进的基于分数的方法以及基准基于约束的方法实现了更好的准确性。",
        "领域": "因果推理、结构学习、多元统计分析",
        "问题": "在存在潜在变量的情况下，如何有效地学习代表变量间因果关系的祖先无环有向混合图（ADMG）",
        "动机": "现有的基于分数的结构学习方法大多不考虑潜在变量，且现有的ADMG学习方法多为启发式，无法保证最优解，因此需要一种能够保证解的最优性并考虑潜在变量的方法。",
        "方法": "提出了一种基于整数规划（IP）的精确方法，通过泛化最先进的DAG学习IP模型并引入新的有效不等式，来学习分数最大化的ADMG。",
        "关键词": [
            "因果结构学习",
            "整数规划",
            "潜在变量",
            "多元高斯分布",
            "有效不等式"
        ],
        "涉及的技术概念": {
            "祖先无环有向混合图（ADMG）": "用于表示变量间因果关系的图模型，允许存在潜在变量和双向边。",
            "整数规划（IP）": "一种数学优化方法，用于在约束条件下寻找整数解的最优解，本文中用于精确学习ADMG。",
            "多元高斯分布": "描述多个连续变量联合分布的概率模型，本文中假设变量遵循此分布以进行因果结构学习。"
        },
        "success": true
    },
    {
        "order": 525,
        "title": "Integrated Defense for Resilient Graph Matching",
        "html": "https://ICML.cc//virtual/2021/poster/9569",
        "abstract": "A recent study has shown that graph matching models are vulnerable to adversarial manipulation of their input which is intended to cause a mismatching. Nevertheless, there is still a lack of a comprehensive solution for further enhancing the robustness of graph matching against adversarial attacks. In this paper, we identify and study two types of unique topology attacks in graph matching: inter-graph dispersion and intra-graph assembly attacks. We propose an integrated defense model, IDRGM, for resilient graph matching with two novel defense techniques to defend against the above two attacks simultaneously. A detection technique of inscribed simplexes in the hyperspheres consisting of multiple matched nodes is proposed to tackle inter-graph dispersion attacks, in which the distances among the matched nodes in multiple graphs are maximized to form regular simplexes. A node separation method based on phase-type distribution and maximum likelihood estimation is developed to estimate the distribution of perturbed graphs and separate the nodes within the same graphs over a wide space, for defending intra-graph assembly attacks, such that the interference from the similar neighbors of the perturbed nodes is significantly reduced. We evaluate the robustness of our IDRGM model on real datasets against state-of-the-art algorithms.",
        "conference": "ICML",
        "中文标题": "集成防御实现弹性图匹配",
        "摘要翻译": "最近的研究表明，图匹配模型容易受到旨在导致不匹配的输入对抗性操纵的影响。然而，目前仍缺乏一个全面的解决方案来进一步增强图匹配对抗攻击的鲁棒性。在本文中，我们识别并研究了图匹配中两种独特的拓扑攻击：图间分散和图内组装攻击。我们提出了一个集成防御模型IDRGM，用于弹性图匹配，该模型采用两种新颖的防御技术同时防御上述两种攻击。提出了一种在由多个匹配节点组成的高维球体中内接单纯形的检测技术，以应对图间分散攻击，其中多个图中匹配节点之间的距离被最大化以形成规则单纯形。开发了一种基于相位型分布和最大似然估计的节点分离方法，用于估计扰动图的分布并在广泛的空间内分离同一图中的节点，以防御图内组装攻击，从而显著减少来自扰动节点相似邻居的干扰。我们在真实数据集上评估了我们的IDRGM模型对抗最先进算法的鲁棒性。",
        "领域": "图匹配、对抗性攻击防御、图神经网络",
        "问题": "增强图匹配模型对抗对抗性攻击的鲁棒性",
        "动机": "当前图匹配模型容易受到对抗性攻击的影响，缺乏有效的防御机制",
        "方法": "提出集成防御模型IDRGM，包含针对图间分散攻击的内接单纯形检测技术和针对图内组装攻击的节点分离方法",
        "关键词": [
            "图匹配",
            "对抗性攻击",
            "鲁棒性",
            "集成防御",
            "拓扑攻击"
        ],
        "涉及的技术概念": {
            "内接单纯形检测技术": "用于检测和防御图间分散攻击，通过最大化匹配节点之间的距离形成规则单纯形",
            "相位型分布和最大似然估计": "用于估计扰动图的分布并分离节点，减少来自相似邻居的干扰",
            "集成防御模型IDRGM": "结合两种防御技术，同时应对图间分散和图内组装攻击，提高图匹配的鲁棒性"
        },
        "success": true
    },
    {
        "order": 526,
        "title": "Interaction-Grounded Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8937",
        "abstract": "Consider a prosthetic arm, learning to adapt to its user's control signals. We propose \\emph{Interaction-Grounded Learning} for this novel setting, in which a learner's goal is to interact with the environment with no grounding or explicit reward to optimize its policies. Such a problem evades common RL solutions which require an explicit reward. The learning agent observes a multidimensional \\emph{context vector}, takes an \\emph{action}, and then observes a multidimensional \\emph{feedback vector}. This multidimensional feedback vector has \\emph{no} explicit reward information. In order to succeed, the algorithm must learn how to evaluate the feedback vector to discover a latent reward signal, with which it can ground its policies without supervision. We show that in an Interaction-Grounded Learning setting, with certain natural assumptions, a learner can discover the latent reward and ground its policy for successful interaction. We provide theoretical guarantees and a proof-of-concept empirical evaluation to demonstrate the effectiveness of our proposed approach.",
        "conference": "ICML",
        "中文标题": "交互式基础学习",
        "摘要翻译": "考虑一个假肢手臂，学习适应用户的控制信号。我们为这一新颖场景提出了交互式基础学习，其中学习者的目标是与环境互动，没有基础或明确的奖励来优化其策略。这样的问题规避了需要明确奖励的常见强化学习解决方案。学习代理观察一个多维上下文向量，采取一个动作，然后观察一个多维反馈向量。这个多维反馈向量没有明确的奖励信息。为了成功，算法必须学习如何评估反馈向量以发现潜在的奖励信号，通过它可以在无监督的情况下基础其策略。我们展示了在交互式基础学习设置中，在一定的自然假设下，学习者可以发现潜在的奖励并基础其策略以实现成功的交互。我们提供了理论保证和概念验证的实证评估，以证明我们提出的方法的有效性。",
        "领域": "强化学习、人机交互、自适应控制",
        "问题": "在缺乏明确奖励信号的环境中，如何通过学习交互反馈来优化策略",
        "动机": "解决在无明确奖励指导的环境下，如何通过交互反馈学习有效策略的问题",
        "方法": "提出交互式基础学习方法，通过分析多维反馈向量来发现潜在奖励信号，并基于此优化策略",
        "关键词": [
            "交互式基础学习",
            "强化学习",
            "无监督学习",
            "反馈向量",
            "潜在奖励"
        ],
        "涉及的技术概念": {
            "交互式基础学习": "一种在无明确奖励环境下，通过交互反馈学习策略的方法",
            "多维反馈向量": "代理在与环境交互后接收到的多维反馈信息，用于潜在奖励信号的发现",
            "潜在奖励信号": "在无明确奖励的情况下，通过分析反馈向量发现的隐含奖励信息，用于策略优化"
        },
        "success": true
    },
    {
        "order": 527,
        "title": "Interactive Learning from Activity Description",
        "html": "https://ICML.cc//virtual/2021/poster/10497",
        "abstract": "We present a novel interactive learning protocol that enables training request-fulfilling agents by verbally describing their activities. Unlike imitation learning (IL), our protocol allows the teaching agent to provide feedback in a language that is most appropriate for them. Compared with reward in reinforcement learning (RL), the description feedback is richer and allows for improved sample complexity. We develop a probabilistic framework and an algorithm that practically implements our protocol. Empirical results in two challenging request-fulfilling problems demonstrate the strengths of our approach: compared with RL baselines, it is more sample-efficient; compared with IL baselines, it achieves competitive success rates without requiring the teaching agent to be able to demonstrate the desired behavior using the learning agent’s actions. Apart from empirical evaluation, we also provide theoretical guarantees for our algorithm under certain assumptions about the teacher and the environment.",
        "conference": "ICML",
        "中文标题": "基于活动描述的交互式学习",
        "摘要翻译": "我们提出了一种新颖的交互式学习协议，该协议通过口头描述活动来训练满足请求的代理。与模仿学习（IL）不同，我们的协议允许教学代理以最适合他们的语言提供反馈。与强化学习（RL）中的奖励相比，描述反馈更为丰富，并能提高样本复杂度。我们开发了一个概率框架和一个实际实现我们协议的算法。在两个具有挑战性的满足请求问题中的实证结果表明了我们方法的优势：与RL基线相比，它更样本高效；与IL基线相比，它在不需要教学代理能够使用学习代理的动作展示期望行为的情况下，达到了竞争性的成功率。除了实证评估外，我们还为我们的算法在关于教师和环境的某些假设下提供了理论保证。",
        "领域": "自然语言处理与视觉结合, 交互式学习, 代理训练",
        "问题": "如何通过口头描述活动来训练满足请求的代理，而不是依赖于模仿学习或强化学习的传统方法。",
        "动机": "开发一种更自然、更高效的交互式学习协议，使教学代理能够以语言形式提供反馈，从而提高学习效率和成功率。",
        "方法": "开发了一个概率框架和算法，实现了通过口头描述活动来训练代理的交互式学习协议。",
        "关键词": [
            "交互式学习",
            "活动描述",
            "代理训练",
            "样本效率",
            "成功率"
        ],
        "涉及的技术概念": {
            "交互式学习协议": "一种新颖的学习方法，允许通过口头描述活动来训练代理，而不是传统的模仿或强化学习方法。",
            "概率框架": "用于实现交互式学习协议的数学框架，支持通过描述反馈进行学习。",
            "样本复杂度": "衡量学习算法效率的指标，本文中的方法通过丰富的描述反馈提高了样本复杂度。"
        },
        "success": true
    },
    {
        "order": 528,
        "title": "Intermediate Layer Optimization for Inverse Problems using Deep Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/9231",
        "abstract": "We propose Intermediate Layer Optimization (ILO), a novel optimization algorithm for solving inverse problems with deep generative models. Instead of optimizing only over the initial latent code, we progressively change the input layer obtaining successively more expressive generators. To explore the higher dimensional spaces, our method searches for latent codes that lie within a small l1 ball around the manifold induced by the previous layer. Our theoretical analysis shows that by keeping the radius of the ball relatively small, we can improve the established error bound for compressed sensing with deep generative models. We empirically show that our approach outperforms state-of-the-art methods introduced in StyleGAN2 and PULSE for a wide range of inverse problems including inpainting, denoising, super-resolution and compressed sensing.",
        "conference": "ICML",
        "中文标题": "使用深度生成模型解决逆问题的中间层优化方法",
        "摘要翻译": "我们提出了中间层优化（ILO），一种新颖的优化算法，用于利用深度生成模型解决逆问题。与仅优化初始潜在代码不同，我们逐步改变输入层，从而获得表达能力更强的生成器。为了探索更高维的空间，我们的方法寻找位于由前一层诱导的流形周围小l1球内的潜在代码。我们的理论分析表明，通过保持球的半径相对较小，我们可以改进深度生成模型压缩感知的既定误差界限。我们通过实验证明，我们的方法在包括修复、去噪、超分辨率和压缩感知在内的广泛逆问题上，优于StyleGAN2和PULSE中引入的最先进方法。",
        "领域": "图像修复, 图像超分辨率, 压缩感知",
        "问题": "解决使用深度生成模型进行逆问题求解时的优化效率和效果问题",
        "动机": "提高深度生成模型在解决逆问题时的表达能力和优化效率",
        "方法": "提出中间层优化（ILO）算法，通过逐步改变输入层并探索高维空间中的潜在代码，优化深度生成模型的性能",
        "关键词": [
            "中间层优化",
            "深度生成模型",
            "逆问题求解",
            "图像修复",
            "压缩感知"
        ],
        "涉及的技术概念": {
            "中间层优化（ILO）": "一种逐步改变输入层以优化深度生成模型性能的算法",
            "l1球": "用于在高维空间中限制潜在代码搜索范围的约束条件，有助于保持优化过程的稳定性",
            "深度生成模型": "用于生成数据的模型，本文中用于解决包括图像修复、去噪、超分辨率和压缩感知在内的逆问题"
        },
        "success": true
    },
    {
        "order": 529,
        "title": "Interpretable Stability Bounds for Spectral Graph Filters",
        "html": "https://ICML.cc//virtual/2021/poster/10145",
        "abstract": "Graph-structured data arise in a variety of real-world context ranging from sensor and transportation to biological and social networks. As a ubiquitous tool to process graph-structured data, spectral graph filters have been used to solve common tasks such as denoising and anomaly detection, as well as design deep learning architectures such as graph neural networks. Despite being an important tool, there is a lack of theoretical understanding of the stability properties of spectral graph filters, which are important for designing robust machine learning models. In this paper, we study filter stability and provide a novel and interpretable upper bound on the change of filter output, where the bound is expressed in terms of the endpoint degrees of the deleted and newly added edges, as well as the spatial proximity of those edges. This upper bound allows us to reason, in terms of structural properties of the graph, when a spectral graph filter will be stable. We further perform extensive experiments to verify intuition that can be gained from the bound.",
        "conference": "ICML",
        "中文标题": "可解释的谱图滤波器稳定性界限",
        "摘要翻译": "图结构数据出现在从传感器和交通到生物和社会网络等多种现实世界场景中。作为一种处理图结构数据的普遍工具，谱图滤波器已被用于解决去噪和异常检测等常见任务，以及设计如图神经网络等深度学习架构。尽管谱图滤波器是一个重要工具，但对其稳定性特性的理论理解仍显不足，这对于设计鲁棒的机器学习模型至关重要。在本文中，我们研究了滤波器的稳定性，并提供了一个新颖且可解释的滤波器输出变化上界，该界限以删除和新添加边的端点度数以及这些边的空间接近度表示。这一上界使我们能够根据图的结构特性，推理出谱图滤波器何时将是稳定的。我们进一步进行了大量实验，以验证从界限中获得的直觉。",
        "领域": "图神经网络、异常检测、图信号处理",
        "问题": "缺乏对谱图滤波器稳定性特性的理论理解",
        "动机": "为了设计鲁棒的机器学习模型，需要深入理解谱图滤波器的稳定性特性",
        "方法": "研究滤波器稳定性，提供基于图结构特性的滤波器输出变化上界，并通过实验验证",
        "关键词": [
            "谱图滤波器",
            "稳定性界限",
            "图神经网络",
            "异常检测",
            "图信号处理"
        ],
        "涉及的技术概念": {
            "谱图滤波器": "用于处理图结构数据的工具，能够执行去噪和异常检测等任务",
            "稳定性界限": "描述了滤波器输出变化的上界，基于图的结构特性",
            "图神经网络": "一种深度学习架构，利用图结构数据进行学习和预测"
        },
        "success": true
    },
    {
        "order": 530,
        "title": "Interpretable Stein Goodness-of-fit Tests on Riemannian Manifold",
        "html": "https://ICML.cc//virtual/2021/poster/10699",
        "abstract": "In many applications, we encounter data on Riemannian manifolds such as torus and rotation groups.\nStandard statistical procedures for multivariate data are not applicable to such data.\nIn this study, we develop goodness-of-fit testing and interpretable model criticism methods for general distributions on Riemannian manifolds, including those with an intractable normalization constant.\nThe proposed methods are based on extensions of kernel Stein discrepancy, which are derived from Stein operators on Riemannian manifolds.\nWe discuss the connections between the proposed tests with existing ones and provide a theoretical analysis of their asymptotic Bahadur efficiency.\nSimulation results and real data applications show the validity and usefulness of the proposed methods.",
        "conference": "ICML",
        "中文标题": "黎曼流形上的可解释性Stein拟合优度检验",
        "摘要翻译": "在许多应用中，我们会遇到如环面和旋转群等黎曼流形上的数据。针对这类数据，传统的多元统计方法并不适用。本研究开发了适用于黎曼流形上一般分布的拟合优度检验和可解释模型批评方法，包括那些具有难以处理的归一化常数的分布。所提出的方法基于对核Stein差异的扩展，这些扩展源自黎曼流形上的Stein算子。我们讨论了所提出的检验与现有方法之间的联系，并对其渐近Bahadur效率进行了理论分析。模拟结果和实际数据应用证明了所提出方法的有效性和实用性。",
        "领域": "统计学习、流形学习、非参数统计",
        "问题": "开发适用于黎曼流形上一般分布的拟合优度检验和可解释模型批评方法",
        "动机": "传统多元统计方法不适用于黎曼流形上的数据，需要开发新的统计方法来处理这类数据",
        "方法": "基于对核Stein差异的扩展，源自黎曼流形上的Stein算子",
        "关键词": [
            "黎曼流形",
            "拟合优度检验",
            "核Stein差异",
            "模型批评",
            "渐近Bahadur效率"
        ],
        "涉及的技术概念": {
            "核Stein差异": "用于衡量概率分布之间差异的指标，基于Stein方法在黎曼流形上的扩展",
            "Stein算子": "在黎曼流形上定义的算子，用于构建核Stein差异",
            "渐近Bahadur效率": "用于评估统计检验性能的理论框架，衡量检验在样本量增加时的效率"
        },
        "success": true
    },
    {
        "order": 531,
        "title": "Interpreting and Disentangling Feature Components of Various Complexity from DNNs",
        "html": "https://ICML.cc//virtual/2021/poster/10221",
        "abstract": "This paper aims to define, visualize, and analyze the feature complexity that is learned by a DNN. We propose a generic definition for the feature complexity. Given the feature of a certain layer in the DNN, our method decomposes and visualizes feature components of different complexity orders from the feature. The feature decomposition enables us to evaluate the reliability, the effectiveness, and the significance of over-fitting of these feature components. Furthermore, such analysis helps to improve the performance of DNNs. As a generic method, the feature complexity also provides new insights into existing deep-learning techniques, such as network compression and knowledge distillation.",
        "conference": "ICML",
        "中文标题": "解释与解构深度神经网络中不同复杂度的特征组件",
        "摘要翻译": "本文旨在定义、可视化并分析深度神经网络(DNN)学习到的特征复杂度。我们提出了一个通用的特征复杂度定义。给定DNN某一层的特征，我们的方法从该特征中分解并可视化不同复杂度阶数的特征组件。这种特征分解使我们能够评估这些特征组件的可靠性、有效性以及过拟合的重要性。此外，此类分析有助于提升DNN的性能。作为一种通用方法，特征复杂度还为现有的深度学习技术，如网络压缩和知识蒸馏，提供了新的见解。",
        "领域": "深度学习解释性、网络压缩、知识蒸馏",
        "问题": "如何定义、可视化并分析深度神经网络中学习到的特征复杂度，以及如何利用这些分析提升网络性能。",
        "动机": "为了深入理解深度神经网络中特征的学习过程，评估不同复杂度特征组件的性能影响，并为现有深度学习技术提供新的视角。",
        "方法": "提出了一种通用的特征复杂度定义方法，通过分解和可视化DNN中的特征组件，评估其性能和过拟合情况，进而优化网络性能。",
        "关键词": [
            "特征复杂度",
            "特征分解",
            "网络性能优化",
            "深度学习解释性",
            "知识蒸馏"
        ],
        "涉及的技术概念": {
            "特征复杂度": "用于描述深度神经网络中特征组件的复杂度阶数，帮助理解和分析网络学习过程。",
            "特征分解": "将DNN中的特征分解为不同复杂度的组件，便于评估各组件对网络性能的影响。",
            "知识蒸馏": "一种深度学习技术，通过特征复杂度的分析，可以为此技术提供新的见解和优化方向。"
        },
        "success": true
    },
    {
        "order": 532,
        "title": "Inverse Constrained Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9667",
        "abstract": "In real world settings, numerous constraints are present which are hard to specify mathematically. However, for the real world deployment of reinforcement learning (RL), it is critical that RL agents are aware of these constraints, so that they can act safely. In this work, we consider the problem of learning constraints from demonstrations of a constraint-abiding agent's behavior. We experimentally validate our approach and show that our framework can successfully learn the most likely constraints that the agent respects. We further show that these learned constraints are \\textit{transferable} to new agents that may have different morphologies and/or reward functions. Previous works in this regard have either mainly been restricted to tabular (discrete) settings, specific types of constraints or assume the environment's transition dynamics. In contrast, our framework is able to learn arbitrary \\textit{Markovian} constraints in high-dimensions in a completely model-free setting. The code is available at: \\url{https://github.com/shehryar-malik/icrl}.\n",
        "conference": "ICML",
        "中文标题": "逆向约束强化学习",
        "摘要翻译": "在现实世界的环境中，存在许多难以用数学方式明确指定的约束。然而，为了实现强化学习（RL）在现实世界中的部署，至关重要的是RL代理能够意识到这些约束，以便它们能够安全地行动。在这项工作中，我们考虑了从遵守约束的代理行为演示中学习约束的问题。我们通过实验验证了我们的方法，并展示了我们的框架能够成功学习代理遵守的最可能的约束。我们进一步展示了这些学习到的约束可以转移到可能具有不同形态和/或奖励函数的新代理中。之前在这方面的研究要么主要局限于表格（离散）设置、特定类型的约束，要么假设环境的转移动态。相比之下，我们的框架能够在完全无模型的高维环境中学习任意的马尔可夫约束。代码可在以下网址获取：https://github.com/shehryar-malik/icrl。",
        "领域": "强化学习安全约束、行为克隆、约束转移学习",
        "问题": "如何从遵守约束的代理行为演示中学习约束，以便于强化学习代理能够在现实世界中安全行动。",
        "动机": "现实世界中的许多约束难以数学化指定，但强化学习代理需要意识到这些约束以确保安全行动。",
        "方法": "提出一个框架，从遵守约束的代理行为演示中学习最可能的约束，并展示这些约束可以转移到具有不同形态和/或奖励函数的新代理中。",
        "关键词": [
            "逆向约束学习",
            "强化学习安全",
            "约束转移"
        ],
        "涉及的技术概念": {
            "马尔可夫约束": "在无模型的高维环境中学习任意的约束，这些约束满足马尔可夫性质。",
            "行为克隆": "通过观察和模仿遵守约束的代理行为来学习约束。",
            "约束转移": "将学习到的约束应用于具有不同形态和/或奖励函数的新代理中。"
        },
        "success": true
    },
    {
        "order": 533,
        "title": "Inverse Decision Modeling: Learning Interpretable Representations of Behavior",
        "html": "https://ICML.cc//virtual/2021/poster/9785",
        "abstract": "Decision analysis deals with modeling and enhancing decision processes. A principal challenge in improving behavior is in obtaining a transparent *description* of existing behavior in the first place. In this paper, we develop an expressive, unifying perspective on *inverse decision modeling*: a framework for learning parameterized representations of sequential decision behavior. First, we formalize the *forward* problem (as a normative standard), subsuming common classes of control behavior. Second, we use this to formalize the *inverse* problem (as a descriptive model), generalizing existing work on imitation/reward learning---while opening up a much broader class of research problems in behavior representation. Finally, we instantiate this approach with an example (*inverse bounded rational control*), illustrating how this structure enables learning (interpretable) representations of (bounded) rationality---while naturally capturing intuitive notions of suboptimal actions, biased beliefs, and imperfect knowledge of environments.",
        "conference": "ICML",
        "中文标题": "逆向决策建模：学习行为的可解释表示",
        "摘要翻译": "决策分析涉及对决策过程的建模和优化。改进行为的首要挑战在于首先获得对现有行为的透明*描述*。在本文中，我们提出了一个关于*逆向决策建模*的表达性、统一视角：一个用于学习序列决策行为的参数化表示的框架。首先，我们将*正向*问题形式化（作为规范性标准），涵盖了控制行为的常见类别。其次，我们利用这一点来形式化*逆向*问题（作为描述性模型），概括了现有关于模仿/奖励学习的工作——同时开辟了行为表示研究中更广泛的问题类别。最后，我们通过一个例子（*逆向有限理性控制*）实例化了这一方法，说明了这种结构如何能够学习（可解释的）（有限）理性表示——同时自然地捕捉到次优行动、偏见信念和环境知识不完善的直观概念。",
        "领域": "决策分析、行为建模、机器学习",
        "问题": "如何获得对现有决策行为的透明描述，并学习其参数化表示。",
        "动机": "为了改进决策行为，首先需要理解和描述现有行为，逆向决策建模提供了一种框架来实现这一目标。",
        "方法": "通过形式化正向和逆向决策问题，开发一个统一的框架来学习决策行为的参数化表示，并通过实例验证其有效性。",
        "关键词": [
            "逆向决策建模",
            "行为表示",
            "有限理性控制",
            "模仿学习",
            "奖励学习"
        ],
        "涉及的技术概念": {
            "逆向决策建模": "一个框架，用于从观察到的行为中学习决策过程的参数化表示。",
            "有限理性控制": "在决策过程中考虑决策者的认知限制和信息不完全性。",
            "模仿/奖励学习": "通过模仿专家行为或通过奖励信号学习，来优化决策策略的方法。"
        },
        "success": true
    },
    {
        "order": 534,
        "title": "Isometric Gaussian Process Latent Variable Model for Dissimilarity Data",
        "html": "https://ICML.cc//virtual/2021/poster/9023",
        "abstract": "We present a probabilistic model where the latent variable respects both the distances and the topology of the modeled data. The model leverages the Riemannian geometry of the generated manifold to endow the latent space with a well-defined stochastic distance measure, which is modeled locally as Nakagami distributions. These stochastic distances are sought to be as similar as possible to observed distances along a neighborhood graph through a censoring process. The model is inferred by variational inference based on observations of pairwise distances. We demonstrate how the new model can encode invariances in the learned manifolds.",
        "conference": "ICML",
        "中文标题": "用于不相似数据的等距高斯过程潜在变量模型",
        "摘要翻译": "我们提出了一种概率模型，其中潜在变量既尊重建模数据的距离又尊重其拓扑结构。该模型利用生成流形的黎曼几何，赋予潜在空间一个定义良好的随机距离度量，该度量在局部被建模为Nakagami分布。通过一个审查过程，这些随机距离被寻求尽可能类似于沿邻域图观察到的距离。该模型基于对成对距离的观察，通过变分推断进行推断。我们展示了新模型如何在学习的流形中编码不变性。",
        "领域": "流形学习, 概率建模, 变分推断",
        "问题": "如何在潜在变量模型中同时尊重数据的距离和拓扑结构",
        "动机": "为了在潜在空间中更准确地建模数据的几何和拓扑特性，从而更好地理解和利用数据的结构",
        "方法": "利用黎曼几何和Nakagami分布定义潜在空间的随机距离度量，通过变分推断进行模型推断",
        "关键词": [
            "高斯过程",
            "潜在变量模型",
            "流形学习",
            "Nakagami分布",
            "变分推断"
        ],
        "涉及的技术概念": {
            "黎曼几何": "用于定义潜在空间的几何结构，使模型能够尊重数据的距离和拓扑",
            "Nakagami分布": "用于局部建模潜在空间的随机距离度量，提供灵活的分布形式以适应不同的数据特性",
            "变分推断": "用于模型的推断过程，通过优化变分下界来近似后验分布"
        },
        "success": true
    },
    {
        "order": 535,
        "title": "Is Pessimism Provably Efficient for Offline RL?",
        "html": "https://ICML.cc//virtual/2021/poster/10409",
        "abstract": "We study offline reinforcement learning (RL), which aims to learn an optimal policy based on a dataset collected a priori. Due to the lack of further interactions with the environment,  offline RL suffers from the insufficient coverage of the dataset, which eludes most existing theoretical analysis. In this paper, we propose a pessimistic variant of the value iteration algorithm (PEVI), which incorporates an uncertainty quantifier as the penalty function. Such a penalty function simply flips the sign of the bonus function for promoting exploration in online RL, which makes it easily implementable and compatible with general function approximators. \n\nWithout assuming the sufficient coverage of the dataset, we establish a data-dependent upper bound on the suboptimality of PEVI for general Markov decision processes (MDPs). When specialized to linear MDPs, it matches the information-theoretic lower bound up to multiplicative factors of the dimension and horizon. In other words, pessimism is not only provably efficient but also minimax optimal. In particular, given the dataset, the learned policy serves as the ``best effort'' among all policies, as no other policies can do better. Our theoretical analysis identifies the critical role of pessimism in eliminating a notion of spurious correlation, which emerges from the ``irrelevant'' trajectories that are less covered by the dataset and not informative for the optimal policy. ",
        "conference": "ICML",
        "中文标题": "悲观主义在离线强化学习中是否可证明高效？",
        "摘要翻译": "我们研究离线强化学习（RL），其目标是根据先验收集的数据集学习最优策略。由于缺乏与环境的进一步交互，离线RL受到数据集覆盖不足的困扰，这逃避了大多数现有的理论分析。在本文中，我们提出了价值迭代算法（PEVI）的一个悲观变体，它引入了一个不确定性量化器作为惩罚函数。这样的惩罚函数简单地翻转了在线RL中用于促进探索的奖励函数的符号，这使得它易于实现且与一般函数逼近器兼容。在不假设数据集充分覆盖的情况下，我们为一般马尔可夫决策过程（MDPs）建立了PEVI次优性的数据依赖性上界。当专门针对线性MDPs时，它与信息论下界在维度和视野的乘法因子内匹配。换句话说，悲观主义不仅可证明高效，而且是极小极大最优的。特别是，给定数据集，学习到的策略在所有策略中作为“最佳努力”，因为没有其他策略可以做得更好。我们的理论分析确定了悲观主义在消除一种虚假相关性概念中的关键作用，这种虚假相关性来自于数据集中较少覆盖且对最优策略无信息的“无关”轨迹。",
        "领域": "离线强化学习",
        "问题": "离线强化学习在数据集覆盖不足的情况下如何学习最优策略",
        "动机": "解决离线强化学习由于缺乏环境交互导致的数据集覆盖不足问题",
        "方法": "提出了一种悲观的价值迭代算法（PEVI），通过引入不确定性量化器作为惩罚函数来优化策略",
        "关键词": [
            "离线强化学习",
            "价值迭代算法",
            "悲观主义",
            "马尔可夫决策过程",
            "数据依赖性上界"
        ],
        "涉及的技术概念": {
            "悲观变体的价值迭代算法（PEVI）": "在离线强化学习中，通过引入不确定性量化器作为惩罚函数来优化策略的算法",
            "不确定性量化器": "用于作为惩罚函数，帮助算法在不充分覆盖的数据集中识别和利用有效信息",
            "虚假相关性": "指那些由数据集中较少覆盖且对学习最优策略无帮助的轨迹引起的误导性关联，PEVI通过悲观主义策略来消除这种影响"
        },
        "success": true
    },
    {
        "order": 536,
        "title": "Is Space-Time Attention All You Need for Video Understanding?",
        "html": "https://ICML.cc//virtual/2021/poster/8941",
        "abstract": "We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named ``TimeSformer,'' adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that ``divided attention,'' where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at:  https://github.com/facebookresearch/TimeSformer.",
        "conference": "ICML",
        "中文标题": "时空注意力是否足以理解视频？",
        "摘要翻译": "我们提出了一种完全基于时空自注意力的无卷积视频分类方法。我们的方法名为‘TimeSformer’，通过直接从帧级补丁序列中学习时空特征，将标准Transformer架构适配于视频。我们的实验研究比较了不同的自注意力方案，并表明‘分治注意力’——在每个块内分别应用时间注意力和空间注意力——在所考虑的设计选择中带来了最佳的视频分类准确率。尽管设计全新，TimeSformer在多个动作识别基准测试中取得了最先进的结果，包括在Kinetics-400和Kinetics-600上报告的最佳准确率。最后，与3D卷积网络相比，我们的模型训练速度更快，可以实现显著更高的测试效率（准确率略有下降），并且还可以应用于更长的视频片段（超过一分钟）。代码和模型可在https://github.com/facebookresearch/TimeSformer获取。",
        "领域": "视频理解、动作识别、时空特征学习",
        "问题": "如何有效地利用自注意力机制进行视频分类和理解",
        "动机": "探索自注意力机制在视频理解中的应用潜力，以替代传统的卷积方法",
        "方法": "采用基于Transformer架构的TimeSformer模型，通过分治注意力机制学习视频的时空特征",
        "关键词": [
            "TimeSformer",
            "自注意力",
            "视频分类",
            "时空特征",
            "动作识别"
        ],
        "涉及的技术概念": {
            "自注意力": "用于直接从视频帧序列中学习特征，替代传统的卷积操作",
            "分治注意力": "在每个Transformer块内分别应用时间和空间注意力，以提高视频分类的准确率",
            "Transformer架构": "通过适配标准Transformer架构来处理视频数据，实现时空特征的有效学习"
        },
        "success": true
    },
    {
        "order": 537,
        "title": "Joining datasets via data augmentation in the label space for neural networks",
        "html": "https://ICML.cc//virtual/2021/poster/10343",
        "abstract": "Most, if not all, modern deep learning systems restrict themselves to a single dataset for neural network training and inference. In this article, we are interested in systematic ways to join datasets that are made of similar purposes. Unlike previous published works that ubiquitously conduct the dataset joining in the uninterpretable latent vectorial space, the core to our method is an augmentation procedure in the label space. The primary challenge to address the label space for dataset joining is the discrepancy between labels: non-overlapping label annotation sets, different labeling granularity or hierarchy and etc. Notably we propose a new technique leveraging artificially created knowledge graph, recurrent neural networks and policy gradient that successfully achieve the dataset joining in the label space. Empirical results on both image and text classification justify the validity of our approach.",
        "conference": "ICML",
        "中文标题": "通过标签空间数据增强实现神经网络数据集联合",
        "摘要翻译": "大多数现代深度学习系统，如果不是全部的话，都限制自己在单一数据集上进行神经网络的训练和推理。在本文中，我们感兴趣的是系统性地联合具有相似目的的数据集的方法。与之前普遍在不可解释的潜在向量空间进行数据集联合的已发表工作不同，我们方法的核心是在标签空间进行的一种增强程序。解决标签空间数据集联合的主要挑战是标签之间的差异：非重叠的标签注释集、不同的标签粒度或层次等。值得注意的是，我们提出了一种新技术，利用人工创建的知识图谱、循环神经网络和策略梯度，成功实现了标签空间的数据集联合。图像和文本分类的实证结果证明了我们方法的有效性。",
        "领域": "自然语言处理与视觉结合, 图像分类, 文本分类",
        "问题": "如何系统性地联合具有相似目的但标签不一致的数据集",
        "动机": "解决现代深度学习系统在单一数据集上训练和推理的限制，探索在标签空间联合数据集的方法",
        "方法": "提出一种在标签空间进行数据增强的新技术，利用人工创建的知识图谱、循环神经网络和策略梯度来实现数据集联合",
        "关键词": [
            "标签空间",
            "数据增强",
            "知识图谱",
            "循环神经网络",
            "策略梯度"
        ],
        "涉及的技术概念": {
            "标签空间": "论文中用于描述和操作数据集标签的抽象空间，是实现数据集联合的核心",
            "知识图谱": "人工创建的结构化知识表示，用于解决标签之间的差异，如非重叠标签集和不同标签粒度",
            "策略梯度": "一种强化学习方法，用于优化在标签空间中进行数据集联合的策略"
        },
        "success": true
    },
    {
        "order": 538,
        "title": "Joint Online Learning and Decision-making via Dual Mirror Descent",
        "html": "https://ICML.cc//virtual/2021/poster/9831",
        "abstract": "We consider an online revenue maximization problem over a finite time horizon subject to lower and upper bounds on cost. At each period, an agent receives a context vector sampled i.i.d. from an unknown distribution and needs to make a decision adaptively. The revenue and cost functions depend on the context vector as well as some fixed but possibly unknown parameter vector to be learned. We propose a novel offline benchmark and a new algorithm that mixes an online dual mirror descent scheme with a generic parameter learning process.  When the parameter vector is known, we demonstrate an $O(\\sqrt{T})$ regret result as well an $O(\\sqrt{T})$ bound on the possible constraint violations. When the parameter is not known and must be learned, we demonstrate that the regret and constraint violations are the sums of the previous $O(\\sqrt{T})$ terms plus terms that directly depend on the convergence of the learning process.",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过双重镜像下降实现联合在线学习与决策制定",
        "摘要翻译": "我们考虑在一个有限时间范围内，在成本上下限约束下的在线收益最大化问题。在每个时期，代理从一个未知分布中独立同分布地接收一个上下文向量，并需要自适应地做出决策。收益和成本函数依赖于上下文向量以及一些固定但可能未知的需要学习的参数向量。我们提出了一个新的离线基准和一个新算法，该算法将在线双重镜像下降方案与一个通用的参数学习过程相结合。当参数向量已知时，我们展示了一个O(√T)的遗憾结果以及一个O(√T)的可能约束违反的界限。当参数未知且必须学习时，我们展示了遗憾和约束违反是之前的O(√T)项加上直接依赖于学习过程收敛的项的总和。",
        "领域": "在线学习, 收益最大化, 自适应决策",
        "问题": "在有限时间范围内，在成本上下限约束下实现在线收益最大化",
        "动机": "研究如何在未知参数和自适应决策的情况下，通过在线学习最大化收益",
        "方法": "结合在线双重镜像下降方案与通用参数学习过程的新算法",
        "关键词": [
            "在线学习",
            "收益最大化",
            "双重镜像下降",
            "自适应决策",
            "参数学习"
        ],
        "涉及的技术概念": {
            "双重镜像下降": "用于在线优化问题，通过镜像下降方法在每次迭代中更新决策变量",
            "在线学习": "在数据逐步到达时进行学习，适用于动态环境下的决策问题",
            "参数学习": "通过学习未知参数向量来优化收益和成本函数，提高决策的准确性"
        }
    },
    {
        "order": 539,
        "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/8473",
        "abstract": "Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference.  A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings.  We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup.  Moreover, we find that existing methods may not generalize to realistic settings.  While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them.  In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.",
        "conference": "ICML",
        "中文标题": "数据投毒究竟有多毒？后门与数据投毒攻击的统一基准测试",
        "摘要翻译": "数据投毒和后门攻击通过操纵训练数据，导致模型在推理阶段失败。最近一项对行业从业者的调查发现，从模型窃取到对抗攻击的各种威胁中，数据投毒是最受关注的问题。然而，考虑到这些方法，即使是目标相同的方法，也未在一致或现实的设置中进行测试，因此尚不清楚投毒方法究竟有多危险，以及哪些方法更有效。我们观察到，数据投毒和后门攻击对测试设置的变化极为敏感。此外，我们发现现有方法可能无法推广到现实设置中。虽然这些现有工作作为数据投毒的有价值原型，但我们进行了严格的测试，以确定我们应该对它们感到恐惧的程度。为了促进未来工作中的公平比较，我们为数据投毒和后门攻击开发了标准化的基准测试。",
        "领域": "对抗性攻击、模型安全、深度学习安全",
        "问题": "评估数据投毒和后门攻击的实际危险性和有效性，以及现有方法在现实环境中的适用性。",
        "动机": "由于数据投毒和后门攻击对模型安全构成严重威胁，且现有研究缺乏一致的测试标准，因此需要建立一个统一的基准来评估这些攻击方法的实际影响。",
        "方法": "通过观察数据投毒和后门攻击对测试设置变化的敏感性，评估现有方法的泛化能力，并开发标准化的基准测试以促进公平比较。",
        "关键词": [
            "数据投毒",
            "后门攻击",
            "模型安全",
            "对抗性攻击",
            "基准测试"
        ],
        "涉及的技术概念": {
            "数据投毒": "通过向训练数据中注入恶意样本，影响模型的学习过程，导致模型在推理时表现异常。",
            "后门攻击": "一种特定的数据投毒攻击，通过在训练数据中植入特定的触发器，使得模型在遇到含有该触发器的输入时产生攻击者预设的错误输出。",
            "基准测试": "为评估和比较不同数据投毒和后门攻击方法的有效性和危险性而开发的一套标准化测试环境和评价指标。"
        },
        "success": true
    },
    {
        "order": 540,
        "title": "Just Train Twice: Improving Group Robustness without Training Group Information",
        "html": "https://ICML.cc//virtual/2021/poster/9147",
        "abstract": "Standard training via empirical risk minimization (ERM) can produce models that achieve low error on average but high error on minority groups, especially in the presence of spurious correlations between the input and label. Prior approaches to this problem, like group distributionally robust optimization (group DRO), generally require group annotations for every training point. On the other hand, approaches that do not use group annotations generally do not improve minority performance. For example, we find that joint DRO, which dynamically upweights examples with high training loss, tends to optimize for examples that are irrelevant to the specific groups we seek to do well on. In this paper, we propose a simple two-stage approach, JTT, that achieves comparable performance to group DRO while only requiring group annotations on a significantly smaller validation set. JTT first attempts to identify informative training examples, which are often minority examples, by training an initial ERM classifier and selecting the examples with high training loss.  Then, it trains a final classifier by upsampling the selected examples. Crucially, unlike joint DRO, JTT does not iteratively upsample examples that have high loss under the final classifier. On four image classification and natural language processing tasks with spurious correlations, we show that JTT closes 85% of the gap in accuracy on the worst group between ERM and group DRO.",
        "conference": "ICML",
        "中文标题": "只需训练两次：无需训练组信息即可提升群体鲁棒性",
        "摘要翻译": "通过经验风险最小化（ERM）进行的标准训练可以产生在平均上误差较低但在少数群体上误差较高的模型，尤其是在输入和标签之间存在虚假相关性的情况下。针对这一问题的先前方法，如群体分布鲁棒优化（group DRO），通常需要每个训练点的组注释。另一方面，不使用组注释的方法通常不会改善少数群体的性能。例如，我们发现联合DRO，它动态地增加训练损失高的例子的权重，倾向于优化那些与我们希望表现良好的特定群体无关的例子。在本文中，我们提出了一种简单的两阶段方法JTT，它在仅需要一个显著较小的验证集上的组注释的情况下，实现了与group DRO相当的性能。JTT首先尝试通过训练一个初始的ERM分类器并选择训练损失高的例子来识别信息丰富的训练例子，这些例子通常是少数群体的例子。然后，它通过上采样选定的例子来训练一个最终分类器。关键的是，与联合DRO不同，JTT不会迭代地上采样在最终分类器下具有高损失的例子。在四个具有虚假相关性的图像分类和自然语言处理任务上，我们展示了JTT在ERM和group DRO之间最差群体准确度差距上缩小了85%。",
        "领域": "群体鲁棒性优化、图像分类、自然语言处理",
        "问题": "解决在存在虚假相关性的情况下，标准训练方法在少数群体上表现不佳的问题",
        "动机": "提高模型在少数群体上的性能，而不需要大量的组注释",
        "方法": "提出两阶段方法JTT，首先通过ERM识别信息丰富的训练例子（通常是少数群体例子），然后通过上采样这些例子训练最终分类器",
        "关键词": [
            "群体鲁棒性",
            "虚假相关性",
            "两阶段训练",
            "上采样",
            "经验风险最小化"
        ],
        "涉及的技术概念": {
            "经验风险最小化（ERM）": "用于初始训练分类器，识别信息丰富的训练例子",
            "群体分布鲁棒优化（group DRO）": "作为比较基准，需要大量组注释的方法",
            "上采样": "在第二阶段用于增加少数群体例子的权重，以改善模型在这些群体上的性能"
        },
        "success": true
    },
    {
        "order": 541,
        "title": "KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation",
        "html": "https://ICML.cc//virtual/2021/poster/9025",
        "abstract": "Conventional unsupervised multi-source domain adaptation (UMDA) methods assume all source domains can be accessed directly. However, this assumption neglects the privacy-preserving policy, where all the data and computations must be kept decentralized. There exist three challenges in this scenario: (1) Minimizing the domain distance requires the pairwise calculation of the data from the source and target domains, while the data on the source domain is not available. (2) The communication cost and privacy security limit the application of existing UMDA methods, such as the domain adversarial training. (3) Since users cannot govern the data quality, the irrelevant or malicious source domains are more likely to appear, which causes negative transfer. To address the above problems, we propose a privacy-preserving UMDA paradigm named Knowledge Distillation based Decentralized Domain Adaptation (KD3A), which performs domain adaptation through the knowledge distillation on models from different source domains. The extensive experiments show that KD3A significantly outperforms state-of-the-art UMDA approaches. Moreover, the KD3A is robust to the negative transfer and brings a 100x reduction of communication cost compared with other decentralized UMDA methods.",
        "conference": "ICML",
        "中文标题": "KD3A：基于知识蒸馏的无监督多源去中心化领域自适应",
        "摘要翻译": "传统的无监督多源领域自适应（UMDA）方法假设所有源领域都可以直接访问。然而，这一假设忽略了隐私保护政策，其中所有数据和计算必须保持去中心化。在这一场景中存在三个挑战：（1）最小化领域距离需要成对计算源领域和目标领域的数据，而源领域的数据不可用。（2）通信成本和隐私安全限制了现有UMDA方法的应用，例如领域对抗训练。（3）由于用户无法控制数据质量，不相关或恶意的源领域更有可能出现，从而导致负迁移。为了解决上述问题，我们提出了一种名为基于知识蒸馏的去中心化领域自适应（KD3A）的隐私保护UMDA范式，该范式通过在不同源领域的模型上进行知识蒸馏来执行领域自适应。大量实验表明，KD3A显著优于最先进的UMDA方法。此外，KD3A对负迁移具有鲁棒性，并与其他去中心化UMDA方法相比，通信成本降低了100倍。",
        "领域": "领域自适应、隐私保护学习、知识蒸馏",
        "问题": "解决在隐私保护政策下，无监督多源领域自适应中的三个主要挑战：源领域数据不可用、通信成本和隐私安全限制、以及负迁移问题。",
        "动机": "研究动机是为了在遵守隐私保护政策的同时，实现有效的无监督多源领域自适应，克服现有方法在去中心化环境下的限制。",
        "方法": "提出了一种基于知识蒸馏的去中心化领域自适应方法（KD3A），通过在源领域模型上进行知识蒸馏来实现领域自适应。",
        "关键词": [
            "无监督多源领域自适应",
            "知识蒸馏",
            "隐私保护",
            "去中心化学习",
            "负迁移"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "用于在不同源领域的模型间传递知识，实现领域自适应，同时保护数据隐私。",
            "无监督多源领域自适应": "在无监督学习环境下，利用多个源领域的数据来适应目标领域，提高模型性能。",
            "负迁移": "指在迁移学习中，由于源领域和目标领域之间的不匹配，导致模型性能下降的现象。KD3A通过知识蒸馏减少负迁移的影响。"
        },
        "success": true
    },
    {
        "order": 542,
        "title": "Kernel-Based Reinforcement Learning: A Finite-Time Analysis",
        "html": "https://ICML.cc//virtual/2021/poster/8573",
        "abstract": "We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning problems whose state-action space is endowed with a metric. We introduce Kernel-UCBVI, a model-based optimistic algorithm that leverages the smoothness of the MDP and a non-parametric kernel estimator of the rewards and transitions to efficiently balance exploration and exploitation. For problems with $K$ episodes and horizon $H$, we provide a regret bound of $\\widetilde{O}\\left( H^3 K^{\\frac{2d}{2d+1}}\\right)$, where $d$ is the covering dimension of the joint state-action space. This is the first regret bound for kernel-based RL using smoothing kernels, which requires very weak assumptions on the MDP and applies to a wide range of tasks. We empirically validate our approach in continuous MDPs with sparse rewards.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于核的强化学习：有限时间分析",
        "摘要翻译": "我们考虑了状态-动作空间具有度量的有限时间范围强化学习问题中的探索-利用困境。我们介绍了Kernel-UCBVI，这是一种基于模型的乐观算法，它利用MDP的平滑性以及奖励和转移的非参数核估计器来有效平衡探索和利用。对于具有K个情节和时间范围H的问题，我们提供了一个遗憾边界为O~(H^3 K^(2d/(2d+1)))，其中d是联合状态-动作空间的覆盖维度。这是首次使用平滑核的基于核的强化学习的遗憾边界，它对MDP的要求非常弱，适用于广泛的任务。我们在稀疏奖励的连续MDP中实证验证了我们的方法。",
        "领域": "强化学习, 机器学习, 非参数估计",
        "问题": "解决在有限时间范围内，状态-动作空间具有度量的强化学习中的探索-利用困境。",
        "动机": "为了在强化学习中更有效地平衡探索和利用，特别是在状态-动作空间具有度量的情况下。",
        "方法": "引入Kernel-UCBVI算法，利用MDP的平滑性和非参数核估计器来平衡探索和利用。",
        "关键词": [
            "强化学习",
            "核方法",
            "探索-利用困境",
            "非参数估计",
            "遗憾边界"
        ],
        "涉及的技术概念": {
            "Kernel-UCBVI": "一种基于模型的乐观算法，利用MDP的平滑性和非参数核估计器来有效平衡探索和利用。",
            "平滑核": "用于基于核的强化学习，能够在非常弱的假设下工作，适用于广泛的任务。",
            "遗憾边界": "衡量算法性能的指标，本文中提出的遗憾边界为O~(H^3 K^(2d/(2d+1))，其中d是联合状态-动作空间的覆盖维度。"
        }
    },
    {
        "order": 543,
        "title": "Kernel Continual Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10451",
        "abstract": "This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-specific classifiers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classifiers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.",
        "conference": "ICML",
        "中文标题": "核持续学习",
        "摘要翻译": "本文介绍了核持续学习，这是一种简单但有效的持续学习变体，它利用核方法的非参数性质来解决灾难性遗忘问题。我们部署了一个情景记忆单元，该单元存储每个任务的一部分样本，以基于核岭回归学习任务特定的分类器。这不需要记忆回放，并系统地避免了分类器中的任务干扰。我们进一步引入了变分随机特征来为每个任务学习一个数据驱动的核。为此，我们将核持续学习表述为一个变分推断问题，其中随机傅里叶基被作为潜在变量纳入。随机傅里叶基上的变分后验分布是从每个任务的核心集中推断出来的。通过这种方式，我们能够为每个任务生成更具信息量的核，更重要的是，可以减少核心集的大小以实现更紧凑的记忆，从而实现基于情景记忆的更高效的持续学习。在四个基准上的广泛评估证明了核在持续学习中的有效性和前景。",
        "领域": "持续学习",
        "问题": "解决持续学习中的灾难性遗忘问题",
        "动机": "利用核方法的非参数性质来避免任务干扰和灾难性遗忘，提高持续学习的效率和效果",
        "方法": "采用核岭回归学习任务特定分类器，引入变分随机特征学习数据驱动的核，将核持续学习表述为变分推断问题",
        "关键词": [
            "核持续学习",
            "灾难性遗忘",
            "核岭回归",
            "变分随机特征",
            "情景记忆"
        ],
        "涉及的技术概念": {
            "核岭回归": "用于学习任务特定的分类器，避免任务干扰",
            "变分随机特征": "用于为每个任务学习数据驱动的核，提高核的信息量",
            "随机傅里叶基": "作为潜在变量纳入变分推断问题，用于推断变分后验分布"
        },
        "success": true
    },
    {
        "order": 544,
        "title": "Kernel Stein Discrepancy Descent",
        "html": "https://ICML.cc//virtual/2021/poster/9273",
        "abstract": "Among dissimilarities between probability distributions, the Kernel Stein Discrepancy (KSD) has received much interest recently. We investigate the properties of its Wasserstein gradient flow to approximate a target probability distribution $\\pi$ on $\\mathbb{R}^d$, known up to a normalization constant. This leads to a straightforwardly implementable, deterministic score-based method to sample from $\\pi$, named KSD Descent, which uses a set of particles to approximate $\\pi$. Remarkably, owing to a tractable loss function, KSD Descent can leverage robust parameter-free optimization schemes such as L-BFGS; this contrasts with other popular particle-based schemes such as the Stein Variational Gradient Descent algorithm. We study the convergence properties of KSD Descent and demonstrate its practical relevance. However, we also highlight failure cases by showing that the algorithm can get stuck in spurious local minima.",
        "conference": "ICML",
        "success": true,
        "中文标题": "核斯坦差异下降",
        "摘要翻译": "在概率分布之间的差异度量中，核斯坦差异（KSD）最近受到了广泛关注。我们研究了其Wasserstein梯度流的性质，以近似一个在ℝᵈ上已知但归一化常数未知的目标概率分布π。这导致了一种直接可实现的、确定性的基于分数的方法，名为KSD下降，它使用一组粒子来近似π。值得注意的是，由于损失函数易于处理，KSD下降可以利用稳健的无参数优化方案，如L-BFGS；这与其它流行的基于粒子的方案（如斯坦变分梯度下降算法）形成对比。我们研究了KSD下降的收敛性质，并展示了其实际相关性。然而，我们也通过展示算法可能会陷入虚假的局部最小值来突出其失败案例。",
        "领域": "概率分布近似, 优化算法, 机器学习",
        "问题": "如何有效地近似一个归一化常数未知的目标概率分布π",
        "动机": "研究KSD的Wasserstein梯度流性质，开发一种新的基于粒子的采样方法，以克服现有方法在优化和收敛方面的限制",
        "方法": "提出KSD下降方法，利用一组粒子和易于处理的损失函数，结合无参数优化方案如L-BFGS，来近似目标概率分布π",
        "关键词": [
            "核斯坦差异",
            "Wasserstein梯度流",
            "L-BFGS优化",
            "概率分布近似",
            "粒子方法"
        ],
        "涉及的技术概念": {
            "核斯坦差异（KSD）": "用于度量概率分布之间的差异，是KSD下降方法的核心理论基础",
            "Wasserstein梯度流": "用于研究KSD的性质，指导KSD下降方法的开发",
            "L-BFGS优化": "一种无参数的优化方案，KSD下降利用其稳健性来优化损失函数"
        }
    },
    {
        "order": 545,
        "title": "Keyframe-Focused Visual Imitation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10701",
        "abstract": "Imitation learning trains control policies by mimicking pre-recorded expert demonstrations. In partially observable settings, imitation policies must rely on observation histories, but many seemingly paradoxical results show better performance for policies that only access the most recent observation. Recent solutions ranging from causal graph learning to deep information bottlenecks have shown promising results, but failed to scale to realistic settings such as visual imitation. We propose a solution that outperforms these prior approaches by upweighting demonstration keyframes corresponding to expert action changepoints. This simple approach easily scales to complex visual imitation settings. Our experimental results demonstrate consistent performance improvements over all baselines on image-based Gym MuJoCo continuous control tasks. Finally, on the CARLA photorealistic vision-based urban driving simulator, we resolve a long-standing issue in behavioral cloning for driving by demonstrating effective imitation from observation histories. Supplementary materials and code at: \\url{https://tinyurl.com/imitation-keyframes}.",
        "conference": "ICML",
        "中文标题": "关键帧聚焦的视觉模仿学习",
        "摘要翻译": "模仿学习通过模仿预先录制的专家演示来训练控制策略。在部分可观察的环境中，模仿策略必须依赖于观察历史，但许多看似矛盾的结果显示，仅访问最近观察的策略表现更好。从因果图学习到深度信息瓶颈的最近解决方案已显示出有希望的结果，但未能扩展到现实设置，如视觉模仿。我们提出了一种解决方案，通过增加对应于专家行动变化点的演示关键帧的权重，优于这些先前的方法。这种简单的方法易于扩展到复杂的视觉模仿设置。我们的实验结果表明，在基于图像的Gym MuJoCo连续控制任务上，相对于所有基线，性能一致提升。最后，在CARLA基于照片级真实感视觉的城市驾驶模拟器上，我们通过展示从观察历史中有效模仿，解决了驾驶行为克隆中长期存在的问题。补充材料和代码见：https://tinyurl.com/imitation-keyframes。",
        "领域": "视觉模仿学习, 连续控制任务, 行为克隆",
        "问题": "在部分可观察的环境中，如何有效利用观察历史进行视觉模仿学习",
        "动机": "解决现有方法在视觉模仿学习中未能有效利用观察历史的问题，特别是在复杂现实设置下的性能提升",
        "方法": "通过增加对应于专家行动变化点的演示关键帧的权重，优化模仿学习策略",
        "关键词": [
            "关键帧",
            "视觉模仿学习",
            "连续控制",
            "行为克隆",
            "观察历史"
        ],
        "涉及的技术概念": {
            "关键帧": "对应于专家行动变化点的演示帧，用于提升模仿学习策略的性能",
            "视觉模仿学习": "通过视觉输入模仿专家行为的机器学习方法",
            "行为克隆": "一种模仿学习技术，通过直接复制专家的行为来训练策略"
        },
        "success": true
    },
    {
        "order": 546,
        "title": "KNAS: Green Neural Architecture Search",
        "html": "https://ICML.cc//virtual/2021/poster/9229",
        "abstract": "Many existing neural architecture search (NAS) solutions rely on downstream training for architecture evaluation, which takes enormous computations. Considering that these computations bring a large carbon footprint, this paper aims to explore a green (namely environmental-friendly) NAS solution that evaluates architectures without training. Intuitively, gradients, induced by the architecture itself, directly decide the convergence and generalization results. It motivates us to propose the gradient kernel hypothesis: Gradients can be used as a coarse-grained proxy of downstream training to evaluate random-initialized networks. To support the hypothesis, we conduct a theoretical analysis and find a practical gradient kernel that has good correlations with training loss and validation performance. According to this hypothesis, we propose a new kernel based architecture search approach KNAS.  Experiments show that KNAS achieves competitive results with orders of magnitude faster than ``train-then-test'' paradigms on image classification tasks. Furthermore, the extremely low search cost enables its wide applications. The searched network also outperforms strong baseline RoBERTA-large on two text classification tasks.",
        "conference": "ICML",
        "中文标题": "KNAS：绿色神经架构搜索",
        "摘要翻译": "许多现有的神经架构搜索（NAS）解决方案依赖于下游训练进行架构评估，这需要巨大的计算量。考虑到这些计算带来了大量的碳足迹，本文旨在探索一种绿色（即环保）的NAS解决方案，无需训练即可评估架构。直观上，由架构本身引起的梯度直接决定了收敛和泛化结果。这激励我们提出梯度核假设：梯度可以作为下游训练的粗粒度代理来评估随机初始化的网络。为了支持这一假设，我们进行了理论分析，并找到一个与训练损失和验证性能有良好相关性的实用梯度核。根据这一假设，我们提出了一种新的基于核的架构搜索方法KNAS。实验表明，KNAS在图像分类任务上取得了与“先训练后测试”范式相竞争的结果，且速度快了几个数量级。此外，极低的搜索成本使其具有广泛的应用前景。搜索到的网络在两个文本分类任务上也优于强大的基线RoBERTA-large。",
        "领域": "神经架构搜索、图像分类、文本分类",
        "问题": "减少神经架构搜索过程中的计算量和碳足迹",
        "动机": "探索一种无需训练即可评估架构的环保NAS解决方案，以减少计算量和碳足迹",
        "方法": "提出梯度核假设，通过理论分析找到与训练损失和验证性能相关的梯度核，并基于此开发KNAS方法",
        "关键词": [
            "绿色计算",
            "神经架构搜索",
            "梯度核",
            "图像分类",
            "文本分类"
        ],
        "涉及的技术概念": {
            "梯度核假设": "假设梯度可以作为下游训练的粗粒度代理来评估随机初始化的网络",
            "KNAS": "基于梯度核假设提出的新架构搜索方法，无需训练即可评估架构",
            "RoBERTA-large": "作为基线模型，用于比较KNAS在文本分类任务上的性能"
        },
        "success": true
    },
    {
        "order": 547,
        "title": "Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9789",
        "abstract": "Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via first-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, $\\mathcal{L}_p$ bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于知识增强的机器学习流程对抗多样化的对抗性攻击",
        "摘要翻译": "尽管深度神经网络（DNNs）取得了巨大成功，但最近的研究表明，它们容易受到对抗性样本的攻击。对抗性样本旨在通过添加微小的对抗性扰动来误导DNN。针对此类攻击已经提出了几种防御方法，但其中许多方法已被自适应地攻击。在这项工作中，我们旨在通过利用领域知识从不同的角度增强ML的鲁棒性：我们提出了一个知识增强的机器学习流程（KEMLP），通过一阶逻辑规则将领域知识（即不同预测之间的逻辑关系）整合到概率图模型中。特别地，我们通过基于其与执行目标任务的主DNN模型的逻辑关系，集成一组不同的弱辅助模型来开发KEMLP。从理论上讲，我们提供了收敛结果，并证明在温和的条件下，KEMLP的预测比主DNN模型的预测更鲁棒。在实验上，我们以道路标志识别为例，并利用道路标志与其形状和内容之间的关系作为领域知识。我们表明，与对抗训练和其他基线相比，在白盒和黑盒设置下，KEMLP在物理攻击、$\\mathcal{L}_p$有界攻击、不可预见的攻击和自然损坏方面实现了更高的鲁棒性，同时仍保持较高的清洁精度。",
        "领域": "对抗性攻击防御、鲁棒性机器学习、概率图模型",
        "问题": "深度神经网络容易受到对抗性攻击，现有的防御方法容易被自适应攻击绕过，导致模型鲁棒性不足。",
        "动机": "通过利用领域知识，从不同的角度增强机器学习模型的鲁棒性，以应对多样化的对抗性攻击，提高模型的可靠性和安全性。",
        "方法": "提出了一种知识增强的机器学习流程（KEMLP），将领域知识（即不同预测之间的逻辑关系）整合到概率图模型中，通过集成一组基于逻辑关系与主DNN模型的弱辅助模型来提升鲁棒性。",
        "关键词": [
            "对抗性攻击",
            "鲁棒性",
            "领域知识",
            "概率图模型",
            "知识增强"
        ],
        "涉及的技术概念": {
            "对抗性攻击": "通过对输入数据添加微小扰动，使深度学习模型产生错误预测的攻击方式。论文旨在提升模型对抗此类攻击的防御能力。",
            "概率图模型": "一种用于表示变量之间概率依赖关系的图形化模型。在KEMLP中，用于整合领域知识并进行推理，提升模型的鲁棒性。"
        }
    },
    {
        "order": 548,
        "title": "KO codes: inventing nonlinear encoding and decoding for reliable wireless communication via deep-learning",
        "html": "https://ICML.cc//virtual/2021/poster/8623",
        "abstract": "Landmark codes underpin reliable physical layer communication, e.g.,  Reed-Muller, BCH, Convolution, Turbo, LDPC, and Polar codes: each is a linear code and represents a mathematical breakthrough. The impact on humanity is huge: each of these codes has been used in global wireless communication standards (satellite, WiFi, cellular). Reliability of communication over the classical additive white Gaussian noise (AWGN) channel enables benchmarking and ranking of the different codes. In this paper, we construct KO codes, a computationally efficient family of deep-learning driven (encoder, decoder) pairs that outperform the state-of-the-art reliability performance on the standardized AWGN channel. KO codes beat state-of-the-art Reed-Muller and Polar codes, under the low-complexity successive cancellation decoding, in the challenging short-to-medium block length regime on the AWGN channel.  We show that the gains of KO codes are primarily due to the nonlinear mapping of information bits directly to transmit symbols (bypassing modulation) and yet possess an efficient, high-performance decoder.  The key technical innovation that renders this possible is design of a novel family of neural architectures inspired by the computation tree of the {\\bf K}ronecker {\\bf O}peration (KO) central to Reed-Muller and Polar codes. These architectures pave way for the discovery of a much richer class of hitherto unexplored nonlinear algebraic structures.",
        "conference": "ICML",
        "中文标题": "KO编码：通过深度学习发明非线性编解码以实现可靠的无线通信",
        "摘要翻译": "里程碑式的编码支撑了可靠的物理层通信，例如Reed-Muller、BCH、卷积、Turbo、LDPC和Polar码：每一种都是线性码，代表了一次数学突破。对人类的影响是巨大的：这些编码中的每一种都被用于全球无线通信标准（卫星、WiFi、蜂窝）。在经典加性白高斯噪声（AWGN）信道上的通信可靠性使得不同编码的基准测试和排名成为可能。在本文中，我们构建了KO编码，这是一种计算效率高的深度学习驱动的（编码器，解码器）对，其在标准化的AWGN信道上超越了最先进的可靠性性能。在AWGN信道的挑战性短到中等块长度体制下，KO编码在低复杂度的连续取消解码下，击败了最先进的Reed-Muller和Polar码。我们展示了KO编码的增益主要是由于信息比特直接到传输符号的非线性映射（绕过调制），并且拥有一个高效、高性能的解码器。使这成为可能的关键技术创新是设计了一类新颖的神经架构，这些架构受到了Reed-Muller和Polar码核心的Kronecker操作（KO）计算树的启发。这些架构为发现一类迄今为止未被探索的更丰富的非线性代数结构铺平了道路。",
        "领域": "深度学习驱动的编码技术、无线通信、物理层通信",
        "问题": "如何在AWGN信道上实现超越现有线性编码的可靠性和性能",
        "动机": "探索和开发非线性编码方法，以提升无线通信的可靠性和效率，特别是在短到中等块长度体制下",
        "方法": "设计了一种基于深度学习的非线性编码和解码方法，通过直接映射信息比特到传输符号，绕过传统调制步骤，并采用受Kronecker操作启发的神经架构",
        "关键词": [
            "KO编码",
            "非线性编码",
            "深度学习",
            "AWGN信道",
            "无线通信"
        ],
        "涉及的技术概念": {
            "非线性编码": "通过深度学习直接映射信息比特到传输符号，绕过传统调制步骤，提高通信效率",
            "Kronecker操作": "一种数学操作，用于构建编码和解码的计算树，是Reed-Muller和Polar码的核心",
            "AWGN信道": "加性白高斯噪声信道，是评估和比较不同编码性能的标准信道模型"
        },
        "success": true
    },
    {
        "order": 549,
        "title": "K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets",
        "html": "https://ICML.cc//virtual/2021/poster/10731",
        "abstract": "In one-shot weight sharing for NAS, the weights of each operation (at each layer) are supposed to be identical for all architectures (paths) in the supernet. However, this rules out the possibility of adjusting operation weights to cater for different paths, which limits the reliability of the evaluation results. In this paper, instead of counting on a single supernet, we introduce $K$-shot supernets and take their weights for each operation as a dictionary. The operation weight for each path is represented as a convex combination of items in a dictionary with a simplex code. \nThis enables a matrix approximation of the stand-alone weight matrix with a higher rank ($K>1$). A \\textit{simplex-net} is introduced to produce architecture-customized code for each path. As a result, all paths can adaptively learn how to share weights in the $K$-shot supernets and acquire corresponding weights for better evaluation.\n$K$-shot supernets and simplex-net can be iteratively trained, and we further extend the search to the channel dimension. Extensive experiments on benchmark datasets validate that K-shot NAS significantly improves the evaluation accuracy of paths and thus brings in impressive performance improvements.",
        "conference": "ICML",
        "中文标题": "K-shot NAS：具有K-shot超网的可学习权重共享的神经架构搜索",
        "摘要翻译": "在神经架构搜索（NAS）的一次性权重共享中，每个操作（在每一层）的权重对于超网中的所有架构（路径）应该是相同的。然而，这排除了调整操作权重以适应不同路径的可能性，从而限制了评估结果的可靠性。在本文中，我们不是依赖单个超网，而是引入了K-shot超网，并将每个操作的权重视为一个字典。每个路径的操作权重表示为字典中项目的凸组合，带有单纯形编码。这使得独立权重矩阵的矩阵近似具有更高的秩（K>1）。引入了一个单纯形网络来为每个路径生成架构定制的编码。因此，所有路径都可以自适应地学习如何在K-shot超网中共享权重，并获得相应的权重以进行更好的评估。K-shot超网和单纯形网络可以迭代训练，我们进一步将搜索扩展到通道维度。在基准数据集上的大量实验验证了K-shot NAS显著提高了路径的评估准确性，从而带来了显著的性能改进。",
        "领域": "神经架构搜索、深度学习优化、自动化机器学习",
        "问题": "解决一次性权重共享在神经架构搜索中无法调整操作权重以适应不同路径的问题，提高评估结果的可靠性。",
        "动机": "通过引入K-shot超网和单纯形编码，使所有路径能够自适应地学习如何共享权重，从而提高神经架构搜索的评估准确性和性能。",
        "方法": "引入K-shot超网和单纯形网络，通过字典和凸组合表示操作权重，实现更高秩的权重矩阵近似，并扩展搜索到通道维度。",
        "关键词": [
            "K-shot超网",
            "单纯形编码",
            "神经架构搜索",
            "权重共享",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "K-shot超网": "在神经架构搜索中引入多个超网，以提高权重共享的灵活性和评估准确性。",
            "单纯形编码": "用于为每个路径生成架构定制的编码，使得操作权重可以表示为字典中项目的凸组合。",
            "神经架构搜索": "一种自动化机器学习方法，用于自动设计神经网络架构，以提高模型性能。"
        },
        "success": true
    },
    {
        "order": 550,
        "title": "Label Distribution Learning Machine",
        "html": "https://ICML.cc//virtual/2021/poster/10653",
        "abstract": "Although Label Distribution Learning (LDL) has witnessed extensive classification applications, it faces the challenge of objective mismatch -- the objective of LDL mismatches that of classification, which has seldom been noticed in existing studies. \nOur goal is to solve the objective mismatch and improve the classification performance of LDL. Specifically, we extend the margin theory to LDL and propose a new LDL method called \\textbf{L}abel \\textbf{D}istribution \\textbf{L}earning \\textbf{M}achine (LDLM). First, we define the label distribution margin and propose the \\textbf{S}upport \\textbf{V}ector \\textbf{R}egression \\textbf{M}achine (SVRM) to learn the optimal label. Second, we propose the adaptive margin loss to learn label description degrees. In theoretical analysis, we develop a generalization theory for the SVRM and analyze the generalization of LDLM. Experimental results validate the better classification performance of LDLM.",
        "conference": "ICML",
        "中文标题": "标签分布学习机",
        "摘要翻译": "尽管标签分布学习（LDL）在分类应用中得到了广泛的应用，但它面临着目标不匹配的挑战——LDL的目标与分类的目标不匹配，这在现有研究中很少被注意到。我们的目标是解决目标不匹配问题并提高LDL的分类性能。具体来说，我们将边际理论扩展到LDL，并提出了一种新的LDL方法，称为标签分布学习机（LDLM）。首先，我们定义了标签分布边际，并提出了支持向量回归机（SVRM）来学习最优标签。其次，我们提出了自适应边际损失来学习标签描述度。在理论分析中，我们为SVRM开发了一个泛化理论，并分析了LDLM的泛化能力。实验结果验证了LDLM更好的分类性能。",
        "领域": "机器学习",
        "问题": "解决标签分布学习（LDL）中的目标不匹配问题，并提高其分类性能",
        "动机": "现有标签分布学习方法的目标与分类任务的目标不匹配，影响了分类性能",
        "方法": "扩展边际理论到LDL，提出标签分布学习机（LDLM），包括定义标签分布边际、提出支持向量回归机（SVRM）学习最优标签，以及自适应边际损失学习标签描述度",
        "关键词": [
            "标签分布学习",
            "边际理论",
            "支持向量回归机",
            "自适应边际损失",
            "分类性能"
        ],
        "涉及的技术概念": {
            "标签分布学习（LDL）": "一种学习方法，旨在处理每个实例与一组标签的关联程度，而不仅仅是单一标签",
            "边际理论": "在机器学习中用于提高模型泛化能力的理论，通过最大化分类边界来减少分类错误",
            "支持向量回归机（SVRM）": "一种基于支持向量机的回归方法，用于学习最优标签分布"
        },
        "success": true
    },
    {
        "order": 551,
        "title": "Label Inference Attacks from Log-loss Scores",
        "html": "https://ICML.cc//virtual/2021/poster/9893",
        "abstract": "Log-loss (also known as cross-entropy loss) metric is  ubiquitously used across machine learning applications to assess the performance of classification algorithms. In this paper, we investigate the problem of inferring the labels of a dataset from single (or multiple) log-loss score(s), without any other access to the dataset. Surprisingly, we show that for any finite number of label classes, it is possible to accurately infer the labels of the dataset from the reported log-loss score of a single carefully constructed prediction vector if we allow arbitrary precision arithmetic. Additionally, we present label inference algorithms (attacks) that succeed even under addition of noise to the log-loss scores and under limited precision arithmetic. All our algorithms rely on ideas from number theory and combinatorics and require no model training. We run experimental simulations on some real datasets to demonstrate the ease of running these attacks in practice.",
        "conference": "ICML",
        "中文标题": "从对数损失分数推断标签的攻击",
        "摘要翻译": "对数损失（也称为交叉熵损失）指标在机器学习应用中无处不在，用于评估分类算法的性能。在本文中，我们研究了从单个（或多个）对数损失分数推断数据集标签的问题，而无需对数据集进行任何其他访问。令人惊讶的是，我们表明，对于任何有限数量的标签类别，如果我们允许任意精度的算术运算，就可以从单个精心构建的预测向量的报告对数损失分数中准确推断出数据集的标签。此外，我们提出了即使在添加噪声到对数损失分数和有限精度算术下也能成功的标签推断算法（攻击）。我们所有的算法都依赖于数论和组合学的思想，并且不需要模型训练。我们在一些真实数据集上进行了实验模拟，以展示这些攻击在实践中的易用性。",
        "领域": "隐私保护机器学习, 对抗性攻击, 数据安全",
        "问题": "如何从对数损失分数中推断出数据集的标签，而无需直接访问数据集本身。",
        "动机": "研究对数损失分数可能泄露数据集标签的潜在风险，以增强机器学习模型的数据隐私保护。",
        "方法": "利用数论和组合学的思想，设计无需模型训练的标签推断算法，即使在添加噪声和有限精度算术的条件下也能有效工作。",
        "关键词": [
            "对数损失",
            "标签推断",
            "隐私攻击",
            "数论",
            "组合学"
        ],
        "涉及的技术概念": {
            "对数损失": "用于评估分类算法性能的指标，本文中作为推断标签的基础。",
            "标签推断攻击": "从模型输出或其他信息中推断出训练数据标签的技术，本文中针对对数损失分数设计。",
            "数论与组合学": "数学分支，本文中用于设计无需模型训练的标签推断算法。"
        },
        "success": true
    },
    {
        "order": 552,
        "title": "Label-Only Membership Inference Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9415",
        "abstract": "Membership inference is one of the simplest privacy threats faced by machine learning models that are trained on private sensitive data. In this attack, an adversary infers whether a particular point was used to train the model, or not, by observing the model's predictions. \nWhereas current attack methods all require access to the model's predicted confidence score, we introduce a label-only attack that instead evaluates the robustness of the model's predicted (hard) labels under perturbations of the input, to infer membership.\nOur label-only attack is not only as-effective as attacks requiring access to confidence scores, it also demonstrates that a class of defenses against membership inference, which we call ``confidence masking'' because they obfuscate the confidence scores to thwart attacks, are insufficient to prevent the leakage of private information.\nOur experiments show that training with differential privacy or strong L2 regularization are the only current defenses that meaningfully decrease leakage of private information, even for points that are outliers of the training distribution.",
        "conference": "ICML",
        "中文标题": "仅标签成员推断攻击",
        "摘要翻译": "成员推断是机器学习模型在训练于私有敏感数据时面临的最简单隐私威胁之一。在此攻击中，攻击者通过观察模型的预测来推断特定点是否被用于训练模型。而当前的攻击方法都需要访问模型预测的置信度分数，我们引入了一种仅标签攻击，该攻击通过评估模型在输入扰动下预测（硬）标签的鲁棒性来推断成员资格。我们的仅标签攻击不仅与需要访问置信度分数的攻击同样有效，它还表明了一类针对成员推断的防御措施，我们称之为‘置信度掩蔽’，因为它们混淆置信度分数以挫败攻击，不足以防止私人信息的泄露。我们的实验表明，使用差分隐私或强L2正则化进行训练是目前唯一能显著减少私人信息泄露的防御措施，即使对于训练分布的异常点也是如此。",
        "领域": "隐私保护机器学习、对抗性攻击、模型安全",
        "问题": "如何在仅能访问模型预测标签的情况下，有效进行成员推断攻击。",
        "动机": "现有成员推断攻击需要访问模型的置信度分数，限制了攻击的适用性；研究旨在开发一种不依赖置信度分数的攻击方法，并评估现有防御措施的有效性。",
        "方法": "通过评估模型在输入扰动下预测标签的鲁棒性来推断成员资格，不依赖置信度分数。",
        "关键词": [
            "成员推断攻击",
            "隐私保护",
            "对抗性攻击",
            "差分隐私",
            "L2正则化"
        ],
        "涉及的技术概念": {
            "仅标签攻击": "一种不依赖模型置信度分数，通过评估模型在输入扰动下预测标签的鲁棒性来推断成员资格的攻击方法。",
            "置信度掩蔽": "一类防御措施，通过混淆模型的置信度分数来防止成员推断攻击，但本研究表明其对仅标签攻击无效。",
            "差分隐私": "一种隐私保护技术，通过在数据或模型训练过程中添加噪声来保护个体数据不被泄露，本研究中证明其能有效减少信息泄露。"
        },
        "success": true
    },
    {
        "order": 553,
        "title": "LAMDA: Label Matching Deep Domain Adaptation",
        "html": "https://ICML.cc//virtual/2021/poster/10721",
        "abstract": "Deep domain adaptation (DDA) approaches have recently been shown to perform better than their shallow rivals with better modeling capacity on complex domains (e.g., image, structural data, and sequential data). The underlying idea is to learn domain invariant representations on a latent space that can bridge the gap between source and target domains. Several theoretical studies have established insightful understanding and the benefit of learning domain invariant features; however, they are usually limited to the case where there is no label shift, hence hindering its applicability. In this paper, we propose and study a new challenging setting that allows us to use a Wasserstein distance (WS) to not only quantify the data shift but also to define the label shift directly. We further develop a theory to demonstrate that minimizing the WS of the data shift leads to closing the gap between the source and target data distributions on the latent space (e.g., an intermediate layer of a deep net), while still being able to quantify the label shift with respect to this latent space. Interestingly, our theory can consequently explain certain drawbacks of learning domain invariant features on the latent space. Finally, grounded on the results and guidance of our developed theory, we propose the Label Matching Deep Domain Adaptation (LAMDA) approach that outperforms baselines on real-world datasets for DA problems.",
        "conference": "ICML",
        "中文标题": "LAMDA：标签匹配深度域适应",
        "摘要翻译": "深度域适应（DDA）方法最近显示出比其浅层对手更好的性能，具有在复杂领域（如图像、结构数据和序列数据）上更好的建模能力。其基本思想是在潜在空间上学习域不变表示，以弥合源域和目标域之间的差距。几项理论研究已经建立了对学习域不变特征的深刻理解和益处；然而，这些研究通常局限于没有标签偏移的情况，从而限制了其适用性。在本文中，我们提出并研究了一个新的具有挑战性的设置，允许我们使用Wasserstein距离（WS）不仅量化数据偏移，还直接定义标签偏移。我们进一步开发了一个理论，证明最小化数据偏移的WS可以缩小潜在空间（如深度网络的中间层）上源和目标数据分布之间的差距，同时仍然能够量化相对于该潜在空间的标签偏移。有趣的是，我们的理论因此可以解释在潜在空间上学习域不变特征的某些缺点。最后，基于我们开发的理论的结果和指导，我们提出了标签匹配深度域适应（LAMDA）方法，在真实世界数据集上的DA问题中优于基线。",
        "领域": "域适应、深度学习、迁移学习",
        "问题": "解决在存在标签偏移的情况下，深度域适应方法的适用性问题",
        "动机": "研究在标签偏移存在的情况下，如何有效进行深度域适应，以扩大其应用范围",
        "方法": "提出使用Wasserstein距离量化数据偏移和定义标签偏移，开发理论支持，并基于此提出LAMDA方法",
        "关键词": [
            "深度域适应",
            "Wasserstein距离",
            "标签偏移",
            "域不变特征",
            "LAMDA"
        ],
        "涉及的技术概念": {
            "Wasserstein距离": "用于量化数据偏移和直接定义标签偏移，是理论和方法的核心",
            "域不变特征": "在潜在空间上学习，旨在弥合源域和目标域之间的差距",
            "标签偏移": "研究中的一个新挑战，指源域和目标域之间标签分布的不同"
        },
        "success": true
    },
    {
        "order": 554,
        "title": "Large-Margin Contrastive Learning with Distance Polarization Regularizer",
        "html": "https://ICML.cc//virtual/2021/poster/8679",
        "abstract": "\\emph{Contrastive learning}~(CL) pretrains models in a pairwise manner, where given a data point, other data points are all regarded as dissimilar, including some that are \\emph{semantically} similar. The issue has been addressed by properly weighting similar and dissimilar pairs as in \\emph{positive-unlabeled learning}, so that the objective of CL is \\emph{unbiased} and CL is \\emph{consistent}. However, in this paper, we argue that this great solution is still not enough: its weighted objective \\emph{hides} the issue where the semantically similar pairs are still pushed away; as CL is pretraining, this phenomenon is not our desideratum and might affect downstream tasks. To this end, we propose \\emph{large-margin contrastive learning}~(LMCL) with \\emph{distance polarization regularizer}, motivated by the distribution characteristic of pairwise distances in \\emph{metric learning}. In LMCL, we can distinguish between \\emph{intra-cluster} and \\emph{inter-cluster} pairs, and then only push away inter-cluster pairs, which \\emph{solves} the above issue explicitly. Theoretically, we prove a tighter error bound for LMCL; empirically, the superiority of LMCL is demonstrated across multiple domains, \\emph{i.e.}, image classification, sentence representation, and reinforcement learning.",
        "conference": "ICML",
        "中文标题": "大间隔对比学习与距离极化正则化器",
        "摘要翻译": "对比学习（CL）以成对的方式预训练模型，其中给定一个数据点，其他所有数据点都被视为不相似，包括一些在语义上相似的点。这一问题已通过适当加权相似和不相似对（如在正未标记学习中）得到解决，使得CL的目标是无偏的且CL是一致的。然而，本文认为这一优秀解决方案仍不足够：其加权目标隐藏了语义相似对仍被推远的问题；由于CL是预训练，这一现象并非我们所期望，并可能影响下游任务。为此，我们提出了大间隔对比学习（LMCL）与距离极化正则化器，其动机来源于度量学习中成对距离的分布特性。在LMCL中，我们可以区分簇内对和簇间对，然后仅推远簇间对，从而明确解决了上述问题。理论上，我们证明了LMCL具有更紧的错误界；实证上，LMCL的优越性在多个领域得到展示，即图像分类、句子表示和强化学习。",
        "领域": "对比学习",
        "问题": "解决对比学习中语义相似对被错误推远的问题",
        "动机": "改进对比学习，避免语义相似对在预训练过程中被错误推远，以提高下游任务的性能",
        "方法": "提出大间隔对比学习（LMCL）与距离极化正则化器，通过区分簇内对和簇间对，仅推远簇间对",
        "关键词": [
            "大间隔对比学习",
            "距离极化正则化器",
            "度量学习",
            "图像分类",
            "句子表示"
        ],
        "涉及的技术概念": {
            "对比学习": "一种预训练模型的方法，通过比较数据点之间的相似性来学习表示",
            "距离极化正则化器": "用于在对比学习中明确区分和优化簇内对和簇间对的距离，避免语义相似对被错误推远",
            "度量学习": "学习一个距离度量，使得相似的数据点在度量空间中更接近，不相似的数据点更远"
        },
        "success": true
    },
    {
        "order": 555,
        "title": "Large-Scale Meta-Learning with Continual Trajectory Shifting",
        "html": "https://ICML.cc//virtual/2021/poster/8939",
        "abstract": "Meta-learning of shared initialization parameters has shown to be highly effective in solving few-shot learning tasks. However, extending the framework to many-shot scenarios, which may further enhance its practicality, has been relatively overlooked due to the technical difficulties of meta-learning over long chains of inner-gradient steps. In this paper, we first show that allowing the meta-learners to take a larger number of inner gradient steps better captures the structure of heterogeneous and large-scale task distributions, thus results in obtaining better initialization points. Further, in order to increase the frequency of meta-updates even with the excessively long inner-optimization trajectories, we propose to estimate the required shift of the task-specific parameters with respect to the change of the initialization parameters. By doing so, we can arbitrarily increase the frequency of meta-updates and thus greatly improve the meta-level convergence as well as the quality of the learned initializations. We validate our method on a heterogeneous set of large-scale tasks, and show that the algorithm largely outperforms the previous first-order meta-learning methods in terms of both generalization performance and convergence, as well as multi-task learning and fine-tuning baselines.\n",
        "conference": "ICML",
        "中文标题": "大规模元学习与连续轨迹偏移",
        "摘要翻译": "共享初始化参数的元学习已被证明在解决少样本学习任务中非常有效。然而，由于在长链内梯度步骤上进行元学习的技术困难，将该框架扩展到多样本场景以进一步增强其实用性的研究相对较少。在本文中，我们首先展示了允许元学习者采取更多内梯度步骤能更好地捕捉异构和大规模任务分布的结构，从而获得更好的初始化点。此外，为了即使在过长的内优化轨迹上也能增加元更新的频率，我们提出了估计任务特定参数相对于初始化参数变化的所需偏移。通过这样做，我们可以任意增加元更新的频率，从而大大改善元级收敛以及学习到的初始化质量。我们在异构的大规模任务集上验证了我们的方法，并表明该算法在泛化性能、收敛性以及多任务学习和微调基线方面大大优于之前的一阶元学习方法。",
        "领域": "元学习",
        "问题": "如何在大规模多样本场景中有效应用元学习",
        "动机": "解决在长链内梯度步骤上进行元学习的技术困难，扩展元学习框架到多样本场景以增强其实用性",
        "方法": "通过允许元学习者采取更多内梯度步骤和估计任务特定参数相对于初始化参数变化的所需偏移，增加元更新的频率",
        "关键词": [
            "元学习",
            "大规模学习",
            "连续轨迹偏移",
            "内梯度步骤",
            "初始化参数"
        ],
        "涉及的技术概念": {
            "元学习": "学习如何学习，旨在通过少量样本快速适应新任务",
            "内梯度步骤": "在元学习框架中，任务特定模型通过内梯度步骤进行优化的过程",
            "初始化参数": "元学习框架中共享的初始参数，用于快速适应新任务"
        },
        "success": true
    },
    {
        "order": 556,
        "title": "Large-Scale Multi-Agent  Deep FBSDEs",
        "html": "https://ICML.cc//virtual/2021/poster/9007",
        "abstract": "In this paper we present a scalable deep learning framework for finding Markovian Nash Equilibria in multi-agent stochastic games using fictitious play. The motivation is inspired by theoretical analysis of Forward Backward Stochastic Differential Equations and their implementation in a deep learning setting, which is the source of our algorithm's sample efficiency improvement. By taking advantage of the permutation-invariant property of agents in symmetric games, the scalability and performance is further enhanced significantly. We showcase superior performance of our framework over the state-of-the-art deep fictitious play algorithm on an inter-bank lending/borrowing problem in terms of multiple metrics. More importantly, our approach scales up to 3000 agents in simulation, a scale which, to the best of our knowledge, represents a new state-of-the-art. We also demonstrate the applicability of our framework in robotics on a belief space autonomous racing problem.",
        "conference": "ICML",
        "中文标题": "大规模多智能体深度FBSDEs",
        "摘要翻译": "本文提出了一种可扩展的深度学习框架，用于通过虚构博弈在多智能体随机博弈中寻找马尔可夫纳什均衡。这一动机来源于对前向后向随机微分方程的理论分析及其在深度学习环境中的实现，这是我们算法样本效率提升的源泉。通过利用对称博弈中智能体的置换不变性，进一步显著提高了可扩展性和性能。我们在银行间借贷问题上展示了我们的框架在多项指标上优于最先进的深度虚构博弈算法的卓越性能。更重要的是，我们的方法在模拟中可扩展至3000个智能体，据我们所知，这一规模代表了新的最先进水平。我们还在机器人领域的信念空间自主赛车问题上展示了我们框架的适用性。",
        "领域": "多智能体系统、随机博弈、深度学习",
        "问题": "在多智能体随机博弈中寻找马尔可夫纳什均衡",
        "动机": "通过前向后向随机微分方程的理论分析及其在深度学习环境中的实现，提升算法样本效率",
        "方法": "利用对称博弈中智能体的置换不变性，开发可扩展的深度学习框架",
        "关键词": [
            "多智能体系统",
            "随机博弈",
            "深度学习",
            "虚构博弈",
            "置换不变性"
        ],
        "涉及的技术概念": {
            "前向后向随机微分方程": "用于理论分析并提升算法样本效率的数学工具",
            "虚构博弈": "用于在多智能体随机博弈中寻找纳什均衡的方法",
            "置换不变性": "在对称博弈中利用智能体的这一性质以提高可扩展性和性能"
        },
        "success": true
    },
    {
        "order": 557,
        "title": "Large Scale Private Learning via Low-rank Reparametrization",
        "html": "https://ICML.cc//virtual/2021/poster/8901",
        "abstract": "We propose a reparametrization scheme to address the challenges of applying differentially private SGD  on large neural networks, which are 1) the huge memory cost of storing individual gradients, 2)  the added noise suffering notorious dimensional dependence.  Specifically, we reparametrize each weight matrix with two \\emph{gradient-carrier} matrices of small dimension and a \\emph{residual weight} matrix. We argue that such reparametrization keeps the forward/backward process unchanged while enabling us to compute the projected gradient without computing the gradient itself. To learn with differential privacy, we design \\emph{reparametrized gradient perturbation (RGP)} that perturbs the gradients on gradient-carrier matrices and reconstructs an update for the original weight from the noisy gradients. Importantly, we use historical updates to find the gradient-carrier matrices, whose optimality is rigorously justified under linear regression and empirically verified with deep learning tasks. RGP significantly reduces the memory cost and improves the utility. For example, we are the first able to apply differential privacy on the BERT model and achieve an average accuracy of $83.9\\%$ on four downstream tasks with $\\epsilon=8$, which is within $5\\%$ loss compared to the non-private baseline but enjoys much lower privacy leakage risk.",
        "conference": "ICML",
        "中文标题": "通过低秩重参数化实现大规模隐私学习",
        "摘要翻译": "我们提出了一种重参数化方案，以解决在大型神经网络上应用差分隐私随机梯度下降（SGD）时面临的挑战，这些挑战包括：1）存储个体梯度的巨大内存成本，2）添加的噪声遭受臭名昭著的维度依赖。具体来说，我们用两个小维度的梯度载体矩阵和一个残差权重矩阵对每个权重矩阵进行重参数化。我们认为，这种重参数化保持了前向/后向过程不变，同时使我们能够在不计算梯度本身的情况下计算投影梯度。为了在差分隐私下学习，我们设计了重参数化梯度扰动（RGP），该扰动对梯度载体矩阵上的梯度进行扰动，并从噪声梯度中重建对原始权重的更新。重要的是，我们使用历史更新来找到梯度载体矩阵，其最优性在线性回归下得到了严格的证明，并在深度学习任务中得到了实证验证。RGP显著降低了内存成本并提高了效用。例如，我们首次能够在BERT模型上应用差分隐私，并在四个下游任务上实现了83.9%的平均准确率，隐私预算ε=8，与非隐私基线相比损失在5%以内，但享有更低的隐私泄露风险。",
        "领域": "差分隐私学习",
        "问题": "解决在大型神经网络上应用差分隐私随机梯度下降时的内存成本和噪声维度依赖问题",
        "动机": "为了在保护数据隐私的同时，降低计算资源消耗并提高模型性能",
        "方法": "通过低秩重参数化技术减少梯度存储需求，设计重参数化梯度扰动（RGP）来保护隐私",
        "关键词": [
            "差分隐私",
            "低秩重参数化",
            "梯度扰动",
            "BERT模型",
            "隐私保护"
        ],
        "涉及的技术概念": {
            "差分隐私随机梯度下降（SGD）": "在训练过程中添加噪声以保护数据隐私的优化方法",
            "低秩重参数化": "通过减少参数矩阵的维度来降低内存需求和计算复杂度",
            "重参数化梯度扰动（RGP）": "一种在保护隐私的同时优化模型性能的技术，通过扰动梯度载体矩阵上的梯度来实现"
        },
        "success": true
    },
    {
        "order": 558,
        "title": "LARNet: Lie Algebra Residual Network for Face Recognition",
        "html": "https://ICML.cc//virtual/2021/poster/9233",
        "abstract": "Face recognition is an important yet challenging problem in computer vision. A major challenge in practical face recognition applications lies in significant variations between profile and frontal faces. Traditional techniques address this challenge either by synthesizing frontal faces or by pose invariant learning. In this paper, we propose a novel method with Lie algebra theory to explore how face rotation in the 3D space affects the deep feature generation process of convolutional neural networks (CNNs). We prove that face rotation in the image space is equivalent to an additive residual component in the feature space of CNNs, which is determined solely by the rotation. Based on this theoretical finding, we further design a Lie Algebraic Residual Network (LARNet) for tackling pose robust face recognition. Our LARNet consists of a residual subnet for decoding rotation information from input face images, and a gating subnet to learn rotation magnitude for controlling the strength of the residual component contributing to the feature learning process. Comprehensive experimental evaluations on both frontal-profile face datasets and general face recognition datasets convincingly demonstrate that our method consistently outperforms the state-of-the-art ones.",
        "conference": "ICML",
        "中文标题": "LARNet：用于人脸识别的李代数残差网络",
        "摘要翻译": "人脸识别是计算机视觉中一个重要但具有挑战性的问题。实际人脸识别应用中的一个主要挑战在于侧面和正面人脸之间的显著变化。传统技术通过合成正面人脸或通过姿态不变学习来解决这一挑战。在本文中，我们提出了一种利用李代数理论的新方法，以探索3D空间中的面部旋转如何影响卷积神经网络（CNNs）的深度特征生成过程。我们证明了图像空间中的面部旋转等同于CNNs特征空间中的一个加性残差分量，该分量仅由旋转决定。基于这一理论发现，我们进一步设计了一个李代数残差网络（LARNet）来处理姿态鲁棒的人脸识别。我们的LARNet包括一个用于从输入人脸图像解码旋转信息的残差子网，和一个用于学习旋转幅度以控制残差分量对特征学习过程贡献强度的门控子网。在正面-侧面人脸数据集和一般人脸识别数据集上的全面实验评估令人信服地证明，我们的方法始终优于最先进的方法。",
        "领域": "人脸识别、姿态估计、深度学习",
        "问题": "解决侧面和正面人脸之间的显著变化对人脸识别性能的影响",
        "动机": "探索面部旋转如何影响卷积神经网络的特征生成过程，并提出一种新的方法来提高姿态鲁棒的人脸识别性能",
        "方法": "利用李代数理论分析面部旋转对CNN特征的影响，设计了一个包含残差子网和门控子网的LARNet，用于解码旋转信息并控制残差分量对特征学习的贡献",
        "关键词": [
            "李代数",
            "残差网络",
            "人脸识别",
            "姿态鲁棒",
            "卷积神经网络"
        ],
        "涉及的技术概念": {
            "李代数理论": "用于分析面部旋转如何影响CNN特征生成过程的理论基础",
            "残差子网": "用于从输入人脸图像解码旋转信息的网络部分",
            "门控子网": "学习旋转幅度以控制残差分量对特征学习过程贡献强度的网络部分"
        },
        "success": true
    },
    {
        "order": 559,
        "title": "Latent Programmer: Discrete Latent Codes for Program Synthesis",
        "html": "https://ICML.cc//virtual/2021/poster/9563",
        "abstract": "A key problem in program synthesis is searching over the large space of possible programs.  Human programmers might decide  the high-level structure of the desired program before thinking about the details; motivated by this intuition, we consider two-level search for program synthesis, in which the synthesizer first generates a plan, a sequence of symbols that describes the desired program at a high level,  before generating the program. We propose to learn representations of programs that can act as plans to organize such a two-level search. Discrete latent codes are appealing for this purpose, and can be learned by applying recent work on discrete autoencoders. Based on these insights, we introduce the Latent Programmer (LP), a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the LP on two domains, demonstrating that it yields an improvement in accuracy, especially on longer programs for which search is most difficult.",
        "conference": "ICML",
        "中文标题": "潜在程序员：用于程序合成的离散潜在代码",
        "摘要翻译": "程序合成中的一个关键问题是在可能程序的大空间中进行搜索。人类程序员可能会在考虑细节之前决定所需程序的高级结构；受这一直觉的启发，我们考虑了两级搜索用于程序合成，其中合成器首先生成一个计划，即一系列符号，描述所需程序的高级结构，然后再生成程序。我们提出学习可以作为计划来组织这种两级搜索的程序表示。离散潜在代码对于这一目的具有吸引力，并且可以通过应用离散自动编码器的最新工作来学习。基于这些见解，我们介绍了潜在程序员（LP），这是一种程序合成方法，首先从输入/输出示例预测离散潜在代码，然后在目标语言中生成程序。我们在两个领域评估了LP，证明它在准确性上有所提高，尤其是在搜索最困难的长程序上。",
        "领域": "程序合成、自动编码器、机器学习",
        "问题": "在程序合成中高效搜索可能程序的大空间",
        "动机": "受人类程序员先规划高级结构再考虑细节的启发，提高程序合成的效率和准确性",
        "方法": "采用两级搜索策略，首先生成描述程序高级结构的计划，再生成具体程序，利用离散潜在代码作为计划组织搜索",
        "关键词": [
            "程序合成",
            "离散潜在代码",
            "两级搜索",
            "自动编码器",
            "机器学习"
        ],
        "涉及的技术概念": {
            "离散潜在代码": "用于表示程序高级结构的离散符号序列，作为两级搜索中的计划",
            "两级搜索": "一种程序合成策略，首先生成高级计划，再生成具体程序，以提高搜索效率和准确性",
            "自动编码器": "用于学习离散潜在代码的技术，通过编码和解码过程捕捉程序的高级结构特征"
        },
        "success": true
    },
    {
        "order": 560,
        "title": "Latent Space Energy-Based Model of Symbol-Vector Coupling for Text Generation and Classification",
        "html": "https://ICML.cc//virtual/2021/poster/8911",
        "abstract": "We propose a latent space energy-based prior model for text generation and classification. The model stands on a generator network that generates the text sequence based on a continuous latent vector. The energy term of the prior model couples a continuous  latent vector and a symbolic one-hot vector, so that discrete category can be inferred from the observed example based on the continuous latent vector. Such a latent space coupling naturally enables incorporation of information bottleneck regularization to encourage the continuous latent vector to extract information from the observed example that is informative of the underlying category.  In our learning method, the symbol-vector coupling, the generator network and the inference network are learned jointly. Our model can be learned in an unsupervised setting where no category labels are provided. It can also be learned in semi-supervised setting where category labels are provided for a subset of training examples. Our experiments demonstrate that the proposed model learns well-structured and meaningful latent space, which (1) guides the generator to generate text with high quality, diversity, and interpretability, and (2) effectively classifies text.",
        "conference": "ICML",
        "中文标题": "符号-向量耦合的潜在空间能量模型用于文本生成与分类",
        "摘要翻译": "我们提出了一种用于文本生成和分类的潜在空间能量先验模型。该模型基于一个生成器网络，该网络根据连续的潜在向量生成文本序列。先验模型的能量项耦合了一个连续的潜在向量和一个符号化的独热向量，从而可以从观察到的样本中基于连续的潜在向量推断出离散的类别。这种潜在空间的耦合自然地允许融入信息瓶颈正则化，以鼓励连续的潜在向量从观察到的样本中提取对潜在类别信息丰富的信息。在我们的学习方法中，符号-向量耦合、生成器网络和推理网络是联合学习的。我们的模型可以在没有提供类别标签的无监督设置下学习。它也可以在为训练样本的一个子集提供类别标签的半监督设置下学习。我们的实验表明，所提出的模型学习到了结构良好且有意义的潜在空间，这（1）指导生成器生成高质量、多样化和可解释的文本，以及（2）有效地分类文本。",
        "领域": "自然语言处理与视觉结合, 文本生成, 文本分类",
        "问题": "如何在文本生成和分类任务中有效地耦合符号向量和连续潜在向量，以提升生成文本的质量和分类的准确性。",
        "动机": "探索一种能够同时处理文本生成和分类的模型，通过潜在空间的能量模型和符号-向量耦合，提高模型在无监督和半监督学习下的性能。",
        "方法": "提出了一种潜在空间能量先验模型，通过耦合连续的潜在向量和符号化的独热向量，结合信息瓶颈正则化，联合学习生成器网络和推理网络。",
        "关键词": [
            "潜在空间能量模型",
            "符号-向量耦合",
            "信息瓶颈正则化",
            "文本生成",
            "文本分类"
        ],
        "涉及的技术概念": {
            "潜在空间能量模型": "用于文本生成和分类的先验模型，通过能量项耦合连续潜在向量和符号向量。",
            "符号-向量耦合": "将连续的潜在向量和符号化的独热向量耦合，以从观察到的样本中推断离散类别。",
            "信息瓶颈正则化": "鼓励模型从观察到的样本中提取对潜在类别信息丰富的信息，以提高模型的泛化能力。"
        },
        "success": true
    },
    {
        "order": 561,
        "title": "Learn2Hop: Learned Optimization on Rough Landscapes",
        "html": "https://ICML.cc//virtual/2021/poster/9417",
        "abstract": "Optimization of non-convex loss surfaces containing many local minima remains a critical problem in a variety of domains, including operations research, informatics, and material design. Yet, current techniques either require extremely high iteration counts or a large number of random restarts for good performance. In this work, we propose adapting recent developments in meta-learning to these many-minima problems by learning the optimization algorithm for various loss landscapes. We focus on problems from atomic structural optimization---finding low energy configurations of many-atom systems---including widely studied models such as bimetallic clusters and disordered silicon. We find that our optimizer learns a hopping behavior which enables efficient exploration and improves the rate of low energy minima discovery. Finally, our learned optimizers show promising generalization with efficiency gains on never before seen tasks (e.g. new elements or compositions). Code is available at https://learn2hop.page.link/github.",
        "conference": "ICML",
        "中文标题": "Learn2Hop：粗糙地形上的学习优化",
        "摘要翻译": "在包含许多局部最小值的非凸损失表面的优化仍然是多个领域中的一个关键问题，包括运筹学、信息学和材料设计。然而，当前的技术要么需要极高的迭代次数，要么需要大量的随机重启以获得良好的性能。在这项工作中，我们提出通过为各种损失地形学习优化算法，将元学习的最新发展适应于这些多最小值问题。我们专注于原子结构优化问题——寻找多原子系统的低能量配置——包括广泛研究的模型，如双金属团簇和无序硅。我们发现我们的优化器学习了一种跳跃行为，这使得高效探索成为可能，并提高了低能量最小值的发现率。最后，我们学习的优化器在未见过的任务（例如新元素或组合）上显示出有希望的泛化能力，并提高了效率。代码可在https://learn2hop.page.link/github获取。",
        "领域": "原子结构优化、元学习、非凸优化",
        "问题": "优化包含许多局部最小值的非凸损失表面",
        "动机": "解决当前技术在优化非凸损失表面时需极高迭代次数或大量随机重启的问题",
        "方法": "通过元学习为各种损失地形学习优化算法，专注于原子结构优化问题",
        "关键词": [
            "非凸优化",
            "元学习",
            "原子结构优化",
            "损失表面",
            "跳跃行为"
        ],
        "涉及的技术概念": {
            "元学习": "通过学习优化算法来适应多最小值问题",
            "非凸优化": "处理包含许多局部最小值的损失表面",
            "跳跃行为": "优化器学习的行为，使得高效探索成为可能，提高低能量最小值的发现率"
        },
        "success": true
    },
    {
        "order": 562,
        "title": "Learner-Private Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9455",
        "abstract": "Convex optimization with feedback is a framework where a learner relies on iterative queries and feedback to arrive at the minimizer of a convex function. The paradigm has gained significant popularity recently thanks to its scalability in large-scale optimization and machine learning. The repeated interactions, however, expose the learner to privacy risks from eavesdropping adversaries that observe the submitted queries. In this paper, we study how to optimally obfuscate the learner’s queries in convex optimization with first-order feedback, so that their learned optimal value is provably difficult to estimate for the eavesdropping adversary. We consider two formulations of learner privacy: a Bayesian formulation in which the convex function is drawn randomly, and a minimax formulation in which the function is fixed and the adversary’s probability of error is measured with respect to a minimax criterion.\n\nWe show that, if the learner wants to ensure the probability of the adversary estimating accurately be kept below 1/L, then the overhead in query complexity is additive in L in the minimax formulation, but multiplicative in L in the Bayesian formulation. Compared to existing learner-private sequential learning models with binary feedback, our results apply to the significantly richer family of general convex functions with full-gradient feedback. Our proofs are largely enabled by tools from the theory of Dirichlet processes, as well as more sophisticated lines of analysis aimed at measuring the amount of information leakage under a full-gradient oracle.",
        "conference": "ICML",
        "中文标题": "学习者隐私保护的凸优化",
        "摘要翻译": "带反馈的凸优化是一种框架，其中学习者依赖于迭代查询和反馈来达到凸函数的最小值。由于其在大规模优化和机器学习中的可扩展性，这一范式最近获得了极大的流行。然而，重复的互动使学习者面临来自观察提交查询的窃听对手的隐私风险。在本文中，我们研究了如何在一阶反馈的凸优化中最佳地混淆学习者的查询，从而使他们学习到的最优值对于窃听对手来说难以估计。我们考虑了两种学习者隐私的表述：一种是贝叶斯表述，其中凸函数是随机抽取的；另一种是极小极大表述，其中函数是固定的，对手的错误概率是相对于极小极大准则来衡量的。我们表明，如果学习者希望确保对手准确估计的概率保持在1/L以下，那么在极小极大表述中查询复杂度的开销是加性的L，而在贝叶斯表述中是乘性的L。与现有的带有二进制反馈的学习者隐私顺序学习模型相比，我们的结果适用于具有全梯度反馈的更广泛的凸函数家族。我们的证明很大程度上得益于Dirichlet过程理论的工具，以及旨在测量全梯度oracle下信息泄漏量的更复杂的分析路线。",
        "领域": "隐私保护机器学习, 凸优化, 梯度下降",
        "问题": "如何在带反馈的凸优化过程中保护学习者的隐私，防止窃听对手通过观察查询来估计最优值。",
        "动机": "在大规模优化和机器学习中，带反馈的凸优化虽然提高了效率，但重复的互动增加了隐私泄露的风险，需要开发新的方法来保护学习者的隐私。",
        "方法": "研究在一阶反馈的凸优化中混淆学习者查询的方法，通过贝叶斯和极小极大两种隐私表述，分析不同表述下保护隐私所需的查询复杂度开销。",
        "关键词": [
            "隐私保护",
            "凸优化",
            "梯度反馈",
            "Dirichlet过程",
            "信息泄漏"
        ],
        "涉及的技术概念": {
            "凸优化": "一种数学优化技术，用于找到凸函数的最小值，广泛应用于机器学习和数据分析中。",
            "贝叶斯表述": "在隐私保护中，假设凸函数是随机抽取的，用于分析隐私保护的效果。",
            "极小极大表述": "在隐私保护中，固定凸函数并基于极小极大准则衡量对手的错误概率，用于分析隐私保护的效果。"
        },
        "success": true
    },
    {
        "order": 563,
        "title": "Learning and Planning in Average-Reward Markov Decision Processes",
        "html": "https://ICML.cc//virtual/2021/poster/10105",
        "abstract": "We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use. ",
        "conference": "ICML",
        "中文标题": "平均奖励马尔可夫决策过程中的学习与规划",
        "摘要翻译": "我们介绍了针对平均奖励马尔可夫决策过程（MDPs）的学习和规划算法，包括：1）首个无需参考状态的、证明收敛的通用离策略模型无关控制算法；2）首个证明收敛的离策略模型无关预测算法；3）首个收敛到实际价值函数而非价值函数加偏移的离策略学习算法。我们的所有算法在更新平均奖励估计时，均基于时间差分误差而非传统误差。我们的证明技术是对Abounadi、Bertsekas和Borkar（2001年）工作的轻微推广。在访问控制排队任务的实验中，我们展示了依赖参考状态的方法可能遇到的困难，并论证了我们的新算法显著更易于使用。",
        "领域": "强化学习、马尔可夫决策过程、算法设计与分析",
        "问题": "解决在平均奖励马尔可夫决策过程中，如何设计和实现高效、易于使用且证明收敛的学习和规划算法。",
        "动机": "现有的平均奖励MDP算法往往依赖于参考状态或收敛到价值函数加偏移，这限制了算法的通用性和易用性。本研究旨在克服这些限制，提供更高效、更通用的解决方案。",
        "方法": "基于时间差分误差而非传统误差来更新平均奖励估计，设计了首个无需参考状态的、证明收敛的离策略模型无关控制算法和预测算法，以及首个收敛到实际价值函数的离策略学习算法。",
        "关键词": [
            "平均奖励MDPs",
            "离策略学习",
            "时间差分误差",
            "算法收敛",
            "参考状态"
        ],
        "涉及的技术概念": {
            "时间差分误差": "用于更新平均奖励估计的关键技术，替代传统误差计算方法，提高算法的效率和收敛性。",
            "离策略学习": "允许算法在学习过程中使用与当前策略不同的行为策略生成的数据，增加了算法的灵活性和适用范围。",
            "马尔可夫决策过程": "提供了一种数学框架，用于建模在不确定性环境下进行决策的问题，是强化学习领域的核心概念之一。"
        },
        "success": true
    },
    {
        "order": 564,
        "title": "Learning and Planning in Complex Action Spaces",
        "html": "https://ICML.cc//virtual/2021/poster/10413",
        "abstract": "Many important real-world problems have action spaces that are high-dimensional, continuous or both, making full enumeration of all possible actions infeasible. Instead, only small subsets of actions can be sampled for the purpose of policy evaluation and improvement. In this paper, we propose a general framework to reason in a principled way about policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can in principle be applied to any reinforcement learning algorithm based upon policy iteration. Concretely, we propose Sampled MuZero, an extension of the MuZero algorithm that is able to learn in domains with arbitrarily complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and on two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.",
        "conference": "ICML",
        "中文标题": "复杂动作空间中的学习与规划",
        "摘要翻译": "许多重要的现实世界问题具有高维、连续或两者兼有的动作空间，这使得对所有可能动作的完全枚举变得不可行。因此，只能采样动作的小子集用于策略评估和改进。在本文中，我们提出了一个通用框架，以原则性的方式对这类采样动作子集上的策略评估和改进进行推理。这种基于采样的策略迭代框架原则上可以应用于任何基于策略迭代的强化学习算法。具体来说，我们提出了Sampled MuZero，这是MuZero算法的一个扩展，能够通过规划采样动作在任意复杂动作空间的领域中学习。我们在经典棋盘游戏围棋以及两个连续控制基准领域：DeepMind Control Suite和Real-World RL Suite上展示了这种方法。",
        "领域": "强化学习、连续控制、游戏AI",
        "问题": "在高维或连续动作空间中，如何有效地进行策略评估和改进",
        "动机": "解决在高维或连续动作空间中，由于动作空间过大而无法枚举所有可能动作，导致策略评估和改进困难的问题",
        "方法": "提出了一个基于采样的策略迭代框架，并扩展了MuZero算法，使其能够通过规划采样动作在复杂动作空间中学习",
        "关键词": [
            "强化学习",
            "策略迭代",
            "复杂动作空间",
            "MuZero",
            "连续控制"
        ],
        "涉及的技术概念": {
            "策略迭代": "一种强化学习方法，通过交替进行策略评估和策略改进来优化策略",
            "MuZero算法": "一种结合了模型基础的规划和学习的强化学习算法，能够在未知环境中通过内部模型进行规划",
            "采样动作子集": "在动作空间中采样的小子集，用于在无法枚举所有动作的情况下进行策略评估和改进"
        },
        "success": true
    },
    {
        "order": 565,
        "title": "Learning a Universal Template for Few-shot Dataset Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/10585",
        "abstract": "Few-shot dataset generalization is a challenging variant of the well-studied few-shot classification problem where a diverse training set of several datasets is given, for the purpose of training an adaptable model that can then learn classes from \\emph{new datasets} using only a few examples. To this end, we propose to utilize the diverse training set to construct a \\emph{universal template}: a partial model that can define a wide array of dataset-specialized models, by plugging in appropriate components.\nFor each new few-shot classification problem, our approach therefore only requires inferring a small number of parameters to insert into the universal template. We design a separate network that produces an initialization of those parameters for each given task, and we then fine-tune its proposed initialization via a few steps of gradient descent. Our approach is more parameter-efficient, scalable and adaptable compared to previous methods, and achieves the state-of-the-art on the challenging Meta-Dataset benchmark.",
        "conference": "ICML",
        "中文标题": "学习一个用于少样本数据集泛化的通用模板",
        "摘要翻译": "少样本数据集泛化是经过深入研究的少样本分类问题的一个具有挑战性的变体，其中给出了多个数据集的多样化训练集，目的是训练一个适应性强的模型，然后该模型可以仅用几个示例学习来自新数据集的类别。为此，我们提出利用多样化的训练集构建一个通用模板：一个可以通过插入适当组件定义广泛数据集专用模型的部分模型。因此，对于每个新的少样本分类问题，我们的方法只需要推断少量参数插入到通用模板中。我们设计了一个单独的网络，为每个给定任务生成这些参数的初始化，然后通过几步梯度下降微调其提出的初始化。与之前的方法相比，我们的方法在参数效率、可扩展性和适应性方面更优，并在具有挑战性的Meta-Dataset基准测试中达到了最先进的水平。",
        "领域": "少样本学习, 元学习, 数据集泛化",
        "问题": "解决在多样化训练集上训练模型，使其能够快速适应新数据集的少样本分类问题",
        "动机": "为了开发一个能够仅用少量示例就能适应新数据集的模型，提高模型的泛化能力和适应性",
        "方法": "构建一个通用模板，通过推断少量参数并插入模板中，设计网络生成参数初始化并通过梯度下降微调",
        "关键词": [
            "少样本学习",
            "数据集泛化",
            "通用模板",
            "参数效率",
            "Meta-Dataset"
        ],
        "涉及的技术概念": {
            "通用模板": "一个部分模型，能够通过插入适当组件定义广泛的数据集专用模型",
            "参数初始化": "为每个任务生成参数的初始值，作为模型训练的起点",
            "梯度下降微调": "通过几步梯度下降优化模型参数，以提高模型性能"
        },
        "success": true
    },
    {
        "order": 566,
        "title": "Learning Binary Decision Trees by Argmin Differentiation",
        "html": "https://ICML.cc//virtual/2021/poster/8771",
        "abstract": "We address the problem of learning binary decision trees that partition data for some downstream task. We propose to learn discrete parameters (i.e., for tree traversals and node pruning) and continuous parameters (i.e., for tree split functions and prediction functions) simultaneously using argmin differentiation. We do so by sparsely relaxing a mixed-integer program for the discrete parameters, to allow gradients to pass through the program to continuous parameters. We derive customized algorithms to efficiently compute the forward and backward passes. This means that our tree learning procedure can be used as an (implicit) layer in arbitrary deep networks, and can be optimized with arbitrary loss functions. We demonstrate that our approach produces binary trees that are competitive with existing single tree and ensemble approaches, in both supervised and unsupervised settings. Further, apart from greedy approaches (which do not have competitive accuracies), our method is faster to train than all other tree-learning baselines we compare with.",
        "conference": "ICML",
        "中文标题": "通过Argmin微分学习二叉决策树",
        "摘要翻译": "我们解决了学习二叉决策树以对数据进行分区以用于某些下游任务的问题。我们提出同时学习离散参数（即用于树遍历和节点剪枝）和连续参数（即用于树分裂函数和预测函数），通过使用argmin微分。我们通过稀疏放松离散参数的混合整数程序来实现这一点，以允许梯度通过程序传递到连续参数。我们推导出定制算法以高效计算前向和后向传递。这意味着我们的树学习过程可以用作任意深度网络中的（隐式）层，并且可以用任意损失函数进行优化。我们证明，我们的方法在监督和无监督设置下产生的二叉决策树与现有的单树和集成方法相比具有竞争力。此外，除了贪婪方法（其不具有竞争性准确度）外，我们的方法比我们比较的所有其他树学习基线训练得更快。",
        "领域": "机器学习、决策树算法、深度学习",
        "问题": "如何同时学习二叉决策树的离散和连续参数以优化下游任务性能",
        "动机": "提高二叉决策树在监督和无监督学习任务中的性能和训练效率",
        "方法": "通过argmin微分和稀疏放松混合整数程序来同时优化离散和连续参数，并开发高效的前向和后向传递算法",
        "关键词": [
            "二叉决策树",
            "argmin微分",
            "混合整数程序",
            "深度学习",
            "监督学习"
        ],
        "涉及的技术概念": {
            "argmin微分": "用于同时优化决策树的离散和连续参数的技术，允许梯度通过优化问题传递",
            "混合整数程序": "用于表示决策树学习中的离散选择问题，通过稀疏放松使其可微分",
            "前向和后向传递算法": "定制的高效算法，用于在决策树学习过程中计算梯度和更新参数"
        },
        "success": true
    },
    {
        "order": 567,
        "title": "Learning Bounds for Open-Set Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9075",
        "abstract": "Traditional supervised learning aims to train a classifier in the closed-set world, where training and test samples share the same label space. In this paper, we target a more challenging and re\u0002alistic setting: open-set learning (OSL), where\nthere exist test samples from the classes that are unseen during training. Although researchers have designed many methods from the algorith\u0002mic perspectives, there are few methods that pro\u0002vide generalization guarantees on their ability to\nachieve consistent performance on different train\u0002ing samples drawn from the same distribution. Motivated by the transfer learning and probably approximate correct (PAC) theory, we make a bold attempt to study OSL by proving its general\u0002ization error−given training samples with size n, the estimation error will get close to order Op(1/√n). This is the first study to provide a generalization bound for OSL, which we do by theoretically investigating the risk of the tar\u0002get classifier on unknown classes. According to our theory, a novel algorithm, called auxiliary open-set risk (AOSR) is proposed to address the OSL problem. Experiments verify the efficacy of AOSR. The code is available at github.com/AnjinLiu/Openset_Learning_AOSR.",
        "conference": "ICML",
        "中文标题": "开放集学习的学习界限",
        "摘要翻译": "传统的监督学习旨在封闭集世界训练一个分类器，其中训练和测试样本共享相同的标签空间。在本文中，我们针对一个更具挑战性和现实性的设置：开放集学习（OSL），其中存在来自训练期间未见类别的测试样本。尽管研究人员从算法角度设计了许多方法，但很少有方法能够提供关于它们在不同训练样本上实现一致性能能力的泛化保证。受迁移学习和可能近似正确（PAC）理论的启发，我们大胆尝试通过证明其泛化误差来研究OSL——给定大小为n的训练样本，估计误差将接近Op(1/√n)的阶数。这是第一个为OSL提供泛化界限的研究，我们通过理论上研究目标分类器在未知类别上的风险来实现这一点。根据我们的理论，提出了一种称为辅助开放集风险（AOSR）的新算法来解决OSL问题。实验验证了AOSR的有效性。代码可在github.com/AnjinLiu/Openset_Learning_AOSR获取。",
        "领域": "开放集学习、迁移学习、泛化理论",
        "问题": "如何在开放集学习环境中提供泛化保证，确保分类器在未见类别上的性能一致性",
        "动机": "解决开放集学习中缺乏泛化理论支持的问题，为分类器在未知类别上的性能提供理论保证",
        "方法": "通过结合迁移学习和PAC理论，提出并证明开放集学习的泛化误差界限，并开发辅助开放集风险（AOSR）算法",
        "关键词": [
            "开放集学习",
            "泛化界限",
            "迁移学习",
            "PAC理论",
            "AOSR算法"
        ],
        "涉及的技术概念": {
            "开放集学习（OSL）": "一种学习设置，其中测试样本可能来自训练期间未见过的类别",
            "泛化误差": "衡量学习算法在未见数据上性能的指标，本文首次为OSL提供了泛化误差的理论界限",
            "辅助开放集风险（AOSR）": "本文提出的新算法，旨在通过理论指导解决开放集学习问题"
        },
        "success": true
    },
    {
        "order": 568,
        "title": "Learning by Turning: Neural Architecture Aware Optimisation",
        "html": "https://ICML.cc//virtual/2021/poster/8853",
        "abstract": "Descent methods for deep networks are notoriously capricious: they require careful tuning of step size, momentum and weight decay, and which method will work best on a new benchmark is a priori unclear. To address this problem, this paper conducts a combined study of neural architecture and optimisation, leading to a new optimiser called Nero: the neuronal rotator. Nero trains reliably without momentum or weight decay, works in situations where Adam and SGD fail, and requires little to no learning rate tuning. Also, Nero's memory footprint is ~ square root that of Adam or LAMB. Nero combines two ideas: (1) projected gradient descent over the space of balanced networks; (2) neuron-specific updates, where the step size sets the angle through which each neuron's hyperplane turns. The paper concludes by discussing how this geometric connection between architecture and optimisation may impact theories of generalisation in deep learning.",
        "conference": "ICML",
        "中文标题": "通过学习转向：神经架构感知优化",
        "摘要翻译": "深度网络的下降方法众所周知地难以捉摸：它们需要仔细调整步长、动量和权重衰减，并且在新的基准测试中哪种方法效果最好事先并不清楚。为了解决这个问题，本文对神经架构和优化进行了联合研究，从而产生了一种名为Nero的新优化器：神经元旋转器。Nero无需动量或权重衰减即可可靠地训练，在Adam和SGD失败的情况下也能工作，并且几乎不需要学习率调整。此外，Nero的内存占用大约是Adam或LAMB的平方根。Nero结合了两个想法：（1）在平衡网络空间上的投影梯度下降；（2）神经元特定的更新，其中步长设置每个神经元超平面旋转的角度。本文最后讨论了架构和优化之间的这种几何联系如何可能影响深度学习中泛化理论。",
        "领域": "深度学习优化、神经网络训练、自适应优化算法",
        "问题": "解决深度网络优化方法需要大量手动调参且性能不稳定的问题",
        "动机": "开发一种无需复杂调参、内存占用低且在各种情况下都能稳定工作的优化器",
        "方法": "结合平衡网络空间上的投影梯度下降和神经元特定的旋转角度更新",
        "关键词": [
            "神经架构感知优化",
            "投影梯度下降",
            "神经元旋转器",
            "自适应优化",
            "深度学习"
        ],
        "涉及的技术概念": {
            "投影梯度下降": "在平衡网络空间上进行的梯度下降，确保网络权重保持平衡",
            "神经元旋转器": "通过设置神经元超平面旋转角度来更新神经元，减少对动量和权重衰减的依赖",
            "平衡网络空间": "一种网络权重的约束空间，有助于优化过程的稳定性和效率"
        },
        "success": true
    },
    {
        "order": 569,
        "title": "Learning Curves for Analysis of Deep Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8899",
        "abstract": "Learning curves model a classifier's test error as a function of the number of training samples. Prior works show that learning curves can be used to select model parameters and extrapolate performance. We investigate how to use learning curves to evaluate design choices, such as pretraining, architecture, and data augmentation. We propose a method to robustly estimate learning curves, abstract their parameters into error and data-reliance, and evaluate the effectiveness of different parameterizations. Our experiments exemplify use of learning curves for analysis and yield several interesting observations.",
        "conference": "ICML",
        "中文标题": "深度网络分析的学习曲线",
        "摘要翻译": "学习曲线将分类器的测试误差建模为训练样本数量的函数。先前的研究表明，学习曲线可用于选择模型参数和外推性能。我们研究了如何利用学习曲线来评估设计选择，如预训练、架构和数据增强。我们提出了一种方法来稳健地估计学习曲线，将其参数抽象为误差和数据依赖性，并评估不同参数化的有效性。我们的实验展示了学习曲线在分析中的应用，并得出了几个有趣的观察结果。",
        "领域": "深度学习理论、模型评估、神经网络优化",
        "问题": "如何利用学习曲线来评估和优化深度学习模型的设计选择",
        "动机": "探索学习曲线在深度学习模型设计和优化中的应用，以提高模型性能和理解模型行为",
        "方法": "提出了一种稳健估计学习曲线的方法，并将学习曲线的参数抽象为误差和数据依赖性，以评估不同设计选择的效果",
        "关键词": [
            "学习曲线",
            "深度学习",
            "模型评估",
            "数据增强",
            "神经网络优化"
        ],
        "涉及的技术概念": {
            "学习曲线": "用于描述分类器测试误差随训练样本数量变化的曲线，帮助理解模型的学习动态和性能",
            "数据依赖性": "指模型性能对训练数据量的依赖程度，反映了模型从数据中学习的能力",
            "参数化": "指将学习曲线的特性通过参数进行抽象和表示，以便于比较和评估不同模型或设计选择的效果"
        },
        "success": true
    },
    {
        "order": 570,
        "title": "Learning Deep Neural Networks under Agnostic Corrupted Supervision",
        "html": "https://ICML.cc//virtual/2021/poster/10557",
        "abstract": "Training deep neural network models in the presence of corrupted supervision is challenging as the corrupted data points may significantly impact generalization performance. To alleviate this problem, we present an efficient robust algorithm that achieves strong guarantees without any assumption on the type of corruption and provides a unified framework for both classification and regression problems. Unlike many existing approaches that quantify the quality of the data points (e.g., based on their individual loss values), and filter them accordingly, the proposed algorithm focuses on controlling the collective impact of data points on the average gradient. \nEven when a corrupted data point failed to be excluded by our algorithm, the data point will have a very limited impact on the overall loss, as compared with state-of-the-art filtering methods based on loss values. Extensive experiments on multiple benchmark datasets have demonstrated the robustness of our algorithm under different types of corruption. Our code is available at \\url{https://github.com/illidanlab/PRL}.",
        "conference": "ICML",
        "中文标题": "在不可知损坏监督下学习深度神经网络",
        "摘要翻译": "在存在损坏监督的情况下训练深度神经网络模型具有挑战性，因为损坏的数据点可能会显著影响泛化性能。为了缓解这个问题，我们提出了一种高效的鲁棒算法，该算法在不假设损坏类型的情况下实现了强有力的保证，并为分类和回归问题提供了一个统一的框架。与许多现有方法不同，这些方法量化数据点的质量（例如，基于它们的个体损失值）并相应地过滤它们，所提出的算法专注于控制数据点对平均梯度的集体影响。即使一个损坏的数据点未能被我们的算法排除，与基于损失值的最先进过滤方法相比，该数据点对整体损失的影响也非常有限。在多个基准数据集上的大量实验证明了我们的算法在不同类型损坏下的鲁棒性。我们的代码可在https://github.com/illidanlab/PRL获取。",
        "领域": "深度学习鲁棒性、监督学习、异常检测",
        "问题": "在监督学习过程中，如何处理损坏的监督数据以提高模型的泛化性能。",
        "动机": "研究动机是为了解决在深度神经网络训练过程中，损坏的监督数据对模型性能的负面影响，提出一种不依赖于损坏类型假设的鲁棒算法。",
        "方法": "提出了一种新的鲁棒算法，通过控制数据点对平均梯度的集体影响，而不是单独过滤基于损失值的数据点，来处理损坏的监督数据。",
        "关键词": [
            "深度神经网络",
            "鲁棒学习",
            "损坏监督",
            "梯度控制",
            "泛化性能"
        ],
        "涉及的技术概念": {
            "鲁棒算法": "提出的算法能够在存在损坏监督的情况下，保证模型的泛化性能，不依赖于损坏类型的假设。",
            "平均梯度控制": "算法通过控制数据点对平均梯度的集体影响，减少损坏数据点对模型训练的影响。",
            "损坏监督": "指在监督学习过程中，标签或监督信号可能被损坏或错误的情况，影响模型的训练和性能。"
        },
        "success": true
    },
    {
        "order": 571,
        "title": "Learning de-identified representations of prosody from raw audio",
        "html": "https://ICML.cc//virtual/2021/poster/8805",
        "abstract": "We propose a method for learning de-identified prosody representations from raw audio using a contrastive self-supervised signal. Whereas prior work has relied on conditioning models with bottlenecks, we introduce a set of inductive biases that exploit the natural structure of prosody to minimize timbral information and decouple prosody from speaker representations. Despite aggressive downsampling of the input and having no access to linguistic information, our model performs comparably to state-of-the-art speech representations on DAMMP, a new benchmark we introduce for spoken language understanding. We use minimum description length probing to show that our representations have selectively learned the subcomponents of non-timbral prosody, and that the product quantizer naturally disentangles them without using bottlenecks. We derive an information-theoretic definition of speech de-identifiability and use it to demonstrate that our prosody representations are less identifiable than the other speech representations.",
        "conference": "ICML",
        "中文标题": "从原始音频中学习去身份化的韵律表示",
        "摘要翻译": "我们提出了一种方法，通过对比自监督信号从原始音频中学习去身份化的韵律表示。与之前依赖于带有瓶颈的条件模型的工作不同，我们引入了一组归纳偏差，这些偏差利用韵律的自然结构来最小化音色信息，并将韵律与说话者表示解耦。尽管对输入进行了激进的降采样且无法访问语言信息，我们的模型在DAMMP（我们为口语理解引入的新基准）上的表现与最先进的语音表示相当。我们使用最小描述长度探测来展示我们的表示已经选择性地学习了非音色韵律的子组件，并且产品量化器自然地解耦了它们，而无需使用瓶颈。我们推导了语音去身份化的信息论定义，并用它来证明我们的韵律表示比其他语音表示更不易识别。",
        "领域": "语音处理、自监督学习、语音去身份化",
        "问题": "如何从原始音频中学习去身份化的韵律表示，同时最小化音色信息并解耦韵律与说话者表示。",
        "动机": "解决现有方法依赖于带有瓶颈的条件模型，无法有效分离韵律和说话者信息的问题。",
        "方法": "采用对比自监督学习方法和一组归纳偏差，利用韵律的自然结构进行学习，并通过产品量化器自然解耦韵律子组件。",
        "关键词": [
            "去身份化",
            "韵律表示",
            "自监督学习",
            "产品量化器",
            "信息论"
        ],
        "涉及的技术概念": {
            "对比自监督信号": "用于从原始音频中学习韵律表示的自监督学习方法，通过对比不同样本间的特征来学习。",
            "归纳偏差": "一组假设或偏好，用于引导模型学习韵律的自然结构，同时最小化音色信息。",
            "产品量化器": "一种量化技术，用于自然解耦韵律的子组件，无需使用瓶颈结构。"
        },
        "success": true
    },
    {
        "order": 572,
        "title": "Learning disentangled representations via product manifold projection",
        "html": "https://ICML.cc//virtual/2021/poster/9635",
        "abstract": "We propose a novel approach to disentangle the generative factors of variation underlying a given set of observations.\nOur method builds upon the idea that the (unknown) low-dimensional manifold underlying the data space can be explicitly modeled as a product of submanifolds. \nThis definition of disentanglement gives rise to a novel weakly-supervised algorithm for recovering the unknown explanatory factors behind the data. At training time, our algorithm only requires pairs of non i.i.d. data samples whose elements share at least one, possibly multidimensional,  generative factor of variation.\nWe require no knowledge on the nature of these transformations, and do not make any limiting assumption on the properties of each subspace.\nOur approach is easy to implement, and can be successfully applied to different kinds of data (from images to 3D surfaces) undergoing arbitrary transformations.\nIn addition to standard synthetic benchmarks, we showcase our method in challenging real-world applications, where we compare favorably with the state of the art.",
        "conference": "ICML",
        "中文标题": "通过乘积流形投影学习解耦表示",
        "摘要翻译": "我们提出了一种新颖的方法来解耦给定观测数据背后的生成变化因素。我们的方法基于这样一个理念：数据空间背后的（未知）低维流形可以明确地建模为子流形的乘积。这种解耦的定义催生了一种新的弱监督算法，用于恢复数据背后的未知解释因素。在训练时，我们的算法仅需要非独立同分布的数据样本对，这些样本对的元素共享至少一个可能是多维的生成变化因素。我们不需要了解这些变换的性质，也不对每个子空间的属性做出任何限制性假设。我们的方法易于实现，并且可以成功地应用于经历任意变换的不同类型数据（从图像到3D表面）。除了标准的合成基准测试外，我们还在具有挑战性的现实世界应用中展示了我们的方法，在这些应用中，我们与现有技术相比表现更优。",
        "领域": "生成模型、表示学习、3D视觉",
        "问题": "解耦观测数据背后的生成变化因素",
        "动机": "为了更有效地理解和控制数据生成过程中的变化因素，提出一种能够明确建模数据低维流形结构的方法。",
        "方法": "提出了一种基于乘积流形建模的弱监督算法，通过非独立同分布的数据样本对来恢复数据背后的生成变化因素。",
        "关键词": [
            "解耦表示",
            "乘积流形",
            "弱监督学习",
            "生成模型",
            "3D视觉"
        ],
        "涉及的技术概念": {
            "乘积流形": "将数据空间背后的低维流形明确建模为子流形的乘积，用于解耦生成变化因素。",
            "弱监督学习": "在训练时仅需非独立同分布的数据样本对，无需详细了解变换性质，降低了监督需求。",
            "生成变化因素": "指影响数据生成过程的变化因素，解耦这些因素有助于更深入地理解数据。"
        },
        "success": true
    },
    {
        "order": 573,
        "title": "Learning Diverse-Structured Networks for Adversarial Robustness",
        "html": "https://ICML.cc//virtual/2021/poster/10509",
        "abstract": "In adversarial training (AT), the main focus has been the objective and optimizer while the model has been less studied, so that the models being used are still those classic ones in standard training (ST). Classic network architectures (NAs) are generally worse than searched NA in ST, which should be the same in AT.\nIn this paper, we argue that NA and AT cannot be handled independently, since given a dataset, the optimal NA in ST would be no longer optimal in AT. That being said, AT is time-consuming itself; if we directly search NAs in AT over large search spaces, the computation will be practically infeasible. Thus, we propose diverse-structured network (DS-Net), to significantly reduce the size of the search space: instead of low-level operations, we only consider predefined atomic blocks, where an atomic block is a time-tested building block like the residual block. There are only a few atomic blocks and thus we can weight all atomic blocks rather than find the best one in a searched block of DS-Net, which is an essential tradeoff between exploring diverse structures and exploiting the best structures. Empirical results demonstrate the advantages of DS-Net, i.e., weighting the atomic blocks.",
        "conference": "ICML",
        "中文标题": "学习多样化结构网络以增强对抗鲁棒性",
        "摘要翻译": "在对抗训练（AT）中，主要关注点一直是目标和优化器，而对模型的研究较少，因此使用的模型仍然是标准训练（ST）中的那些经典模型。经典网络架构（NAs）在ST中通常不如搜索到的NA好，这在AT中应该也是如此。在本文中，我们认为NA和AT不能独立处理，因为给定一个数据集，ST中的最优NA在AT中将不再是最优的。也就是说，AT本身是耗时的；如果我们直接在AT中在大的搜索空间上搜索NAs，计算将实际上不可行。因此，我们提出了多样化结构网络（DS-Net），以显著减少搜索空间的大小：我们不考虑低级操作，而是只考虑预定义的原子块，其中原子块是经过时间检验的构建块，如残差块。原子块只有少数几个，因此我们可以对所有原子块进行加权，而不是在DS-Net的搜索块中找到最好的一个，这是在探索多样化结构和利用最佳结构之间的一个重要权衡。实证结果证明了DS-Net的优势，即加权原子块。",
        "领域": "对抗训练、网络架构搜索、深度学习安全",
        "问题": "如何在对抗训练中优化网络架构以提高模型的对抗鲁棒性",
        "动机": "经典网络架构在标准训练中表现良好，但在对抗训练中可能不再是最优的，直接在大搜索空间中进行网络架构搜索计算成本过高",
        "方法": "提出多样化结构网络（DS-Net），通过预定义的原子块减少搜索空间大小，并对所有原子块进行加权而非寻找单一最优块",
        "关键词": [
            "对抗训练",
            "网络架构搜索",
            "多样化结构网络",
            "对抗鲁棒性",
            "原子块加权"
        ],
        "涉及的技术概念": {
            "对抗训练（AT）": "一种通过引入对抗样本来增强模型鲁棒性的训练方法",
            "网络架构搜索（NAS）": "自动寻找最优神经网络结构的技术",
            "多样化结构网络（DS-Net）": "一种通过预定义原子块和加权策略来减少搜索空间并提高对抗鲁棒性的网络架构"
        },
        "success": true
    },
    {
        "order": 574,
        "title": "Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9253",
        "abstract": "We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. We provide a theoretical analysis of the convergence of policy gradient for this problem. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account these two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods, both in terms of efficiency and equity.",
        "conference": "ICML",
        "中文标题": "学习去中心化合作多智能体强化学习中的公平策略",
        "摘要翻译": "我们考虑了在（深度）合作多智能体强化学习（MARL）中学习公平策略的问题。我们以一种原则性的方式将其形式化为优化一个明确编码公平两个重要方面：效率和公平的福利函数的问题。我们为这个问题提供了策略梯度收敛的理论分析。作为一种解决方法，我们提出了一种新颖的神经网络架构，该架构由两个子网络组成，专门设计用于考虑公平的这两个方面。在实验中，我们证明了这两个子网络对于公平优化的重要性。我们的整体方法是通用的，因为它可以适应任何（子）可微分的福利函数。因此，它与文献中提出的各种公平概念兼容（例如，字典序最大最小，广义基尼社会福利函数，比例公平）。我们的方法是通用的，可以在各种MARL设置中实现：集中训练和分散执行，或完全分散。最后，我们在各种领域中实验验证了我们的方法，并表明它在效率和公平方面都可以比之前的方法表现更好。",
        "领域": "多智能体强化学习、公平性学习、合作学习",
        "问题": "如何在多智能体强化学习中学习既高效又公平的策略",
        "动机": "解决多智能体强化学习中效率和公平性之间的平衡问题",
        "方法": "提出了一种由两个子网络组成的新型神经网络架构，分别针对效率和公平性进行优化",
        "关键词": [
            "多智能体强化学习",
            "公平性",
            "神经网络架构",
            "福利函数",
            "策略梯度"
        ],
        "涉及的技术概念": {
            "福利函数": "用于编码公平性的两个重要方面：效率和公平，作为优化的目标",
            "策略梯度": "用于理论分析策略优化的收敛性，并作为训练方法的一部分",
            "神经网络架构": "由两个子网络组成，专门设计用于分别考虑效率和公平性，以实现公平优化"
        },
        "success": true
    },
    {
        "order": 575,
        "title": "Learning from Biased Data: A Semi-Parametric Approach",
        "html": "https://ICML.cc//virtual/2021/poster/10043",
        "abstract": "We consider risk minimization problems where the (source) distribution $P_S$ of the training observations $Z_1, \\ldots, Z_n$ differs from the (target) distribution $P_T$ involved in the risk that one seeks to minimize. Under the natural assumption that $P_S$ dominates $P_T$, \\textit{i.e.} $P_T< \\! \\!<P_S$, we develop a semi-parametric framework in the situation where we \\textit{do not} observe any sample from $P_T$, but rather have access to some auxiliary information at the target population scale. More precisely, assuming that the Radon-Nikodym derivative $dP_T/dP_S(z)$ belongs to a parametric class $\\{g(z,\\alpha),\\ \\alpha\\in \\mathcal{A}\\}$ and that some (generalized) moments of $P_T$ are available to the learner, we propose a two-step learning procedure to perform the risk minimization task. We first select $\\hat{\\alpha}$ so as to match the moment constraints as closely as possible and then reweight each (biased) training observation $Z_i$ by $g(Z_i,\\hat{\\alpha})$ in the final Empirical Risk Minimization (ERM) algorithm. We establish a $O_{\\mathbb{P}}(1/\\sqrt{n})$ generalization bound proving that, remarkably, the solution to the weighted ERM problem thus constructed achieves a learning rate of the same order as that attained in absence of any sampling bias. Beyond these theoretical guarantees, numerical results providing strong empirical evidence of the relevance of the approach promoted in this article are displayed.",
        "conference": "ICML",
        "success": true,
        "中文标题": "从有偏数据中学习：一种半参数方法",
        "摘要翻译": "我们考虑风险最小化问题，其中训练观测值$Z_1, \\\\ldots, Z_n$的（源）分布$P_S$与需要最小化的风险涉及的（目标）分布$P_T$不同。在$P_S$主导$P_T$，即$P_T< \\\\! \\\\!<P_S$的自然假设下，我们开发了一个半参数框架，该框架适用于我们未从$P_T$中观察到任何样本，但在目标群体尺度上可以获得一些辅助信息的情况。更准确地说，假设Radon-Nikodym导数$dP_T/dP_S(z)$属于一个参数类$\\\\{g(z,\\\\alpha),\\\\ \\\\alpha\\\\in \\\\mathcal{A}\\\\}$，并且学习者可以获得$P_T$的一些（广义）矩，我们提出了一个两步学习程序来执行风险最小化任务。我们首先选择$\\\\hat{\\\\alpha}$以尽可能接近地匹配矩约束，然后在最终的实证风险最小化（ERM）算法中，通过$g(Z_i,\\\\hat{\\\\alpha})$对每个（有偏的）训练观测值$Z_i$进行重新加权。我们建立了一个$O_{\\\\mathbb{P}}(1/\\\\sqrt{n})$的泛化界限，证明这样构建的加权ERM问题的解决方案实现了与在没有任何采样偏差的情况下相同的学习率。除了这些理论保证外，数值结果提供了强有力的经验证据，证明了本文推广的方法的相关性。",
        "领域": "统计学习理论, 机器学习, 数据偏差校正",
        "问题": "解决在训练数据分布与目标分布不同的情况下，如何进行有效的风险最小化。",
        "动机": "研究动机在于开发一种方法，能够在无法直接从目标分布中获取样本的情况下，利用辅助信息进行有效的学习，以克服数据偏差带来的挑战。",
        "方法": "采用半参数框架，结合Radon-Nikodym导数的参数化估计和加权实证风险最小化（ERM）算法，以匹配目标分布的矩约束并重新加权训练数据。",
        "关键词": [
            "半参数方法",
            "风险最小化",
            "数据偏差",
            "Radon-Nikodym导数",
            "加权ERM"
        ],
        "涉及的技术概念": {
            "半参数框架": "结合了参数化和非参数化方法的框架，用于处理数据分布偏差问题。",
            "Radon-Nikodym导数": "用于衡量目标分布相对于源分布的密度比，是实现数据重新加权的关键。"
        }
    },
    {
        "order": 576,
        "title": "Learning from History for Byzantine Robust Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9987",
        "abstract": "Byzantine robustness has received significant attention recently given its importance for distributed and federated learning. In spite of this, we identify severe flaws in existing algorithms even when the data across the participants is identically distributed. First, we show realistic examples where current state of the art robust aggregation rules fail to converge even in the absence of any Byzantine attackers.\nSecondly, we prove that even if the aggregation rules may succeed in limiting the influence of the attackers in a single round, the attackers can couple their attacks across time eventually leading to divergence. To address these issues, we present two surprisingly simple strategies: a new robust iterative clipping procedure, and incorporating worker momentum to overcome time-coupled attacks.\nThis is the first provably robust method for the standard stochastic optimization setting.",
        "conference": "ICML",
        "中文标题": "从历史中学习拜占庭鲁棒优化",
        "摘要翻译": "拜占庭鲁棒性因其在分布式和联邦学习中的重要性最近受到了广泛关注。尽管如此，我们发现在现有算法中存在严重缺陷，即使参与者的数据是相同分布的。首先，我们展示了现实的例子，其中当前最先进的鲁棒聚合规则即使在没有任何拜占庭攻击者的情况下也无法收敛。其次，我们证明即使聚合规则可能成功限制攻击者在单轮中的影响，攻击者也可以跨时间耦合他们的攻击，最终导致发散。为了解决这些问题，我们提出了两种出奇简单的策略：一种新的鲁棒迭代剪裁程序，以及通过引入工作动量来克服时间耦合攻击。这是在标准随机优化设置中第一个可证明鲁棒的方法。",
        "领域": "分布式学习, 联邦学习, 鲁棒优化",
        "问题": "解决现有拜占庭鲁棒算法在数据同分布情况下仍存在的收敛问题和时间耦合攻击导致的发散问题",
        "动机": "提高分布式和联邦学习系统在面对拜占庭攻击时的鲁棒性和稳定性",
        "方法": "提出新的鲁棒迭代剪裁程序和引入工作动量来克服时间耦合攻击",
        "关键词": [
            "拜占庭鲁棒性",
            "分布式学习",
            "联邦学习",
            "鲁棒优化",
            "时间耦合攻击"
        ],
        "涉及的技术概念": {
            "拜占庭鲁棒性": "指系统在面对部分节点可能提供错误或恶意信息时仍能保持正确运行的能力",
            "鲁棒聚合规则": "用于在分布式系统中聚合来自不同节点的信息，以减少或消除恶意节点的影响",
            "工作动量": "一种技术，用于在优化过程中考虑历史梯度信息，以平滑更新步骤并提高对时间耦合攻击的抵抗能力"
        },
        "success": true
    },
    {
        "order": 577,
        "title": "Learning from Nested Data with Ornstein Auto-Encoders",
        "html": "https://ICML.cc//virtual/2021/poster/9609",
        "abstract": "Many of real-world data, e.g., the VGGFace2 dataset, which is a collection of multiple portraits of individuals, come with nested structures due to grouped observation. The Ornstein auto-encoder (OAE) is an emerging framework for representation learning from nested data, based on an optimal transport distance between random processes. An attractive feature of OAE is its ability to generate new variations nested within an observational unit, whether or not the unit is known to the model. A previously proposed algorithm for OAE, termed the random-intercept OAE (RIOAE), showed an impressive performance in learning nested representations, yet lacks theoretical justification. In this work, we show that RIOAE minimizes a loose upper bound of the employed optimal transport distance. After identifying several issues with RIOAE, we present the product-space OAE (PSOAE) that minimizes a tighter upper bound of the distance and achieves orthogonality in the representation space. PSOAE alleviates the instability of RIOAE and provides more flexible representation of nested data. We demonstrate the high performance of PSOAE in the three key tasks of generative models: exemplar generation, style transfer, and new concept generation.",
        "conference": "ICML",
        "中文标题": "从嵌套数据中学习：Ornstein自动编码器",
        "摘要翻译": "许多现实世界的数据，例如VGGFace2数据集，即多个个体肖像的集合，由于分组观察而具有嵌套结构。Ornstein自动编码器（OAE）是一个新兴的框架，用于从嵌套数据中学习表示，基于随机过程之间的最优传输距离。OAE的一个吸引人的特点是它能够生成嵌套在观察单元内的新变体，无论模型是否知道该单元。之前提出的OAE算法，称为随机截距OAE（RIOAE），在学习嵌套表示方面表现出色，但缺乏理论依据。在这项工作中，我们展示了RIOAE最小化了所采用的最优传输距离的一个宽松上界。在识别出RIOAE的几个问题后，我们提出了产品空间OAE（PSOAE），它最小化了距离的更紧上界，并在表示空间中实现了正交性。PSOAE缓解了RIOAE的不稳定性，并提供了更灵活的嵌套数据表示。我们在生成模型的三个关键任务中展示了PSOAE的高性能：范例生成、风格转换和新概念生成。",
        "领域": "表示学习、生成模型、风格转换",
        "问题": "如何有效地从具有嵌套结构的数据中学习表示，并生成新的数据变体",
        "动机": "解决现有Ornstein自动编码器（OAE）算法在理论依据和表示稳定性方面的不足",
        "方法": "提出产品空间OAE（PSOAE），通过最小化更紧的最优传输距离上界和实现表示空间的正交性，来改进嵌套数据的表示学习",
        "关键词": [
            "Ornstein自动编码器",
            "最优传输距离",
            "嵌套数据表示",
            "生成模型",
            "风格转换"
        ],
        "涉及的技术概念": {
            "Ornstein自动编码器（OAE）": "用于从嵌套数据中学习表示的框架，基于随机过程之间的最优传输距离",
            "最优传输距离": "衡量两个概率分布之间差异的度量，用于优化表示学习过程",
            "表示空间的正交性": "在表示空间中实现特征之间的独立性，以提高表示的灵活性和稳定性"
        },
        "success": true
    },
    {
        "order": 578,
        "title": "Learning from Noisy Labels with No Change to the Training Process",
        "html": "https://ICML.cc//virtual/2021/poster/9933",
        "abstract": "There has been much interest in recent years in developing learning algorithms that can learn accurate classifiers from data with noisy labels. A widely-studied noise model is that of \\emph{class-conditional noise} (CCN), wherein a label $y$ is flipped to a label $\\tilde{y}$ with some associated noise probability that depends on both $y$ and $\\tilde{y}$. In the multiclass setting, all previously proposed algorithms under the CCN model involve changing the training process, by introducing a `noise-correction' to the surrogate loss to be minimized over the noisy training examples. In this paper, we show that this is really unnecessary: one can simply perform class probability estimation (CPE) on the noisy examples, e.g.\\ using a standard (multiclass) logistic regression algorithm, and then apply noise-correction only in the final prediction step. This means that the training algorithm itself does not need any change, and one can simply use standard off-the-shelf implementations with no modification to the code for training. Our approach can handle general multiclass loss matrices, including the usual 0-1 loss but also other losses such as those used for ordinal regression problems. We also provide a quantitative regret transfer bound, which bounds the target regret on the true distribution in terms of the CPE regret on the noisy distribution; in doing so, we extend the notion of strong properness introduced for binary losses by Agarwal (2014) to the multiclass case. Our bound suggests that the sample complexity of learning under CCN increases as the noise matrix approaches singularity. We also provide fixes and potential improvements for noise estimation methods that involve computing anchor points. Our experiments confirm our theoretical findings.\n",
        "conference": "ICML",
        "中文标题": "无需改变训练过程的噪声标签学习",
        "摘要翻译": "近年来，开发能够从带有噪声标签的数据中学习准确分类器的算法引起了广泛关注。一个被广泛研究的噪声模型是类条件噪声（CCN），其中标签$y$以某种依赖于$y$和$\tilde{y}$的噪声概率被翻转到标签$\tilde{y}$。在多类设置中，所有先前在CCN模型下提出的算法都涉及改变训练过程，通过对噪声训练示例引入‘噪声校正’到要最小化的替代损失中。在本文中，我们表明这实际上是不必要的：可以简单地对噪声示例进行类概率估计（CPE），例如使用标准的（多类）逻辑回归算法，然后仅在最终预测步骤中应用噪声校正。这意味着训练算法本身不需要任何改变，可以简单地使用标准的现成实现，无需修改训练代码。我们的方法可以处理一般的多类损失矩阵，包括通常的0-1损失，但也包括用于序数回归问题的其他损失。我们还提供了一个定量的遗憾转移界限，该界限根据噪声分布上的CPE遗憾来界定真实分布上的目标遗憾；在这样做的过程中，我们将Agarwal（2014）为二元损失引入的强适当性概念扩展到多类情况。我们的界限表明，随着噪声矩阵接近奇异性，CCN下的学习样本复杂性增加。我们还为涉及计算锚点的噪声估计方法提供了修复和潜在改进。我们的实验证实了我们的理论发现。",
        "领域": "噪声标签学习、多类分类、序数回归",
        "问题": "如何在无需改变训练过程的情况下，从带有噪声标签的数据中学习准确的分类器。",
        "动机": "现有的噪声标签学习方法需要修改训练过程，这增加了实现的复杂性。本研究旨在探索一种无需改变训练过程即可处理噪声标签的方法。",
        "方法": "通过类概率估计（CPE）对噪声示例进行处理，并在最终预测步骤中应用噪声校正，从而无需修改训练算法本身。",
        "关键词": [
            "噪声标签学习",
            "类条件噪声",
            "类概率估计",
            "多类分类",
            "序数回归"
        ],
        "涉及的技术概念": {
            "类条件噪声（CCN）": "描述标签翻转概率依赖于原始标签和目标标签的噪声模型。",
            "类概率估计（CPE）": "在噪声数据上直接进行概率估计，无需修改训练过程。",
            "遗憾转移界限": "量化了在噪声分布上的CPE遗憾与真实分布上的目标遗憾之间的关系，扩展了强适当性概念到多类情况。"
        },
        "success": true
    },
    {
        "order": 579,
        "title": "Learning from Similarity-Confidence Data",
        "html": "https://ICML.cc//virtual/2021/poster/9869",
        "abstract": "Weakly supervised learning has drawn considerable attention recently to reduce the expensive time and labor consumption of labeling massive data. In this paper, we investigate a novel weakly supervised learning problem of learning from similarity-confidence (Sconf) data, where only unlabeled data pairs equipped with confidence that illustrates their degree of similarity (two examples are similar if they belong to the same class) are needed for training a discriminative binary classifier. We propose an unbiased estimator of the classification risk that can be calculated from only Sconf data and show that the estimation error bound achieves the optimal convergence rate. To alleviate potential overfitting when flexible models are used, we further employ a risk correction scheme on the proposed risk estimator. Experimental results demonstrate the effectiveness of the proposed methods.",
        "conference": "ICML",
        "中文标题": "从相似性置信数据中学习",
        "摘要翻译": "近年来，弱监督学习引起了广泛关注，以减少标注大量数据所需的高昂时间和人力成本。在本文中，我们研究了一种新颖的弱监督学习问题——从相似性置信（Sconf）数据中学习，其中仅需要配备有说明它们相似程度置信度的未标记数据对（如果两个示例属于同一类别，则它们是相似的）来训练一个判别性二元分类器。我们提出了一个可以从仅Sconf数据中计算出的分类风险的无偏估计器，并展示了估计误差界达到了最优收敛率。为了减轻使用灵活模型时可能出现的过拟合问题，我们进一步对提出的风险估计器采用了风险校正方案。实验结果证明了所提出方法的有效性。",
        "领域": "弱监督学习",
        "问题": "如何在仅使用未标记数据对及其相似性置信度的情况下训练一个有效的判别性二元分类器",
        "动机": "减少标注大量数据所需的时间和人力成本，通过弱监督学习方法利用相似性置信数据进行有效学习",
        "方法": "提出一个可以从相似性置信数据中计算出的分类风险的无偏估计器，并采用风险校正方案以减轻过拟合",
        "关键词": [
            "弱监督学习",
            "相似性置信数据",
            "二元分类器",
            "风险估计",
            "风险校正"
        ],
        "涉及的技术概念": {
            "相似性置信数据": "用于描述数据对之间相似程度的置信度，是训练判别性二元分类器的基础",
            "无偏估计器": "用于从相似性置信数据中计算分类风险，确保估计的准确性",
            "风险校正方案": "应用于风险估计器，以减轻使用灵活模型时可能出现的过拟合问题"
        },
        "success": true
    },
    {
        "order": 580,
        "title": "Learning Generalized Intersection Over Union for Dense Pixelwise Prediction",
        "html": "https://ICML.cc//virtual/2021/poster/9661",
        "abstract": " Intersection over union (IoU) score, also named Jaccard Index, is one of the most fundamental evaluation methods in machine learning. The original IoU computation cannot provide non-zero gradients and thus cannot be directly optimized by nowadays deep learning methods. Several recent works generalized IoU for bounding box regression, but they are not straightforward to adapt for pixelwise prediction. In particular, the original IoU fails to provide effective gradients for the non-overlapping and location-deviation cases, which results in performance plateau. In this paper, we propose PixIoU, a generalized IoU for pixelwise prediction that is sensitive to the distance for non-overlapping cases and the locations in prediction. We provide proofs that PixIoU holds many nice properties as the original IoU. To optimize the PixIoU, we also propose a loss function that is proved to be submodular, hence we can apply the Lov\\'asz functions, the efficient surrogates for submodular functions for learning this loss. Experimental results show consistent performance improvements by learning PixIoU over the original IoU for several different pixelwise prediction tasks on Pascal VOC, VOT-2020 and Cityscapes.",
        "conference": "ICML",
        "中文标题": "学习广义交并比用于密集像素级预测",
        "摘要翻译": "交并比（IoU）分数，也称为Jaccard指数，是机器学习中最基本的评估方法之一。原始的IoU计算无法提供非零梯度，因此无法直接被当前的深度学习方法优化。最近的一些工作将IoU推广用于边界框回归，但它们并不直接适用于像素级预测。特别是，原始的IoU无法为非重叠和位置偏差情况提供有效的梯度，这导致性能停滞。在本文中，我们提出了PixIoU，一种用于像素级预测的广义IoU，它对非重叠情况的距离和预测中的位置敏感。我们提供了证明，表明PixIoU保持了原始IoU的许多优良特性。为了优化PixIoU，我们还提出了一种损失函数，该函数被证明是子模的，因此我们可以应用Lovász函数，即子模函数的高效替代品，来学习这种损失。实验结果表明，通过在Pascal VOC、VOT-2020和Cityscapes上的几种不同像素级预测任务中学习PixIoU，相比原始IoU，性能有了一致的提升。",
        "领域": "图像分割, 目标检测, 语义分割",
        "问题": "解决原始IoU在像素级预测中无法提供有效梯度的问题",
        "动机": "为了提升像素级预测任务中模型对非重叠和位置偏差情况的敏感性和性能",
        "方法": "提出PixIoU，一种广义IoU，并设计了一种子模损失函数进行优化",
        "关键词": [
            "交并比",
            "像素级预测",
            "子模函数",
            "Lovász函数",
            "性能优化"
        ],
        "涉及的技术概念": {
            "PixIoU": "一种用于像素级预测的广义IoU，对非重叠情况的距离和预测中的位置敏感",
            "子模损失函数": "用于优化PixIoU的损失函数，被证明是子模的",
            "Lovász函数": "子模函数的高效替代品，用于学习子模损失函数"
        },
        "success": true
    },
    {
        "order": 581,
        "title": "Learning Gradient Fields for Molecular Conformation Generation",
        "html": "https://ICML.cc//virtual/2021/poster/8519",
        "abstract": "We study a fundamental problem in computational chemistry known as molecular conformation generation, trying to predict stable 3D structures from 2D molecular graphs. \nExisting machine learning approaches usually first predict distances between atoms and then generate a 3D structure satisfying the distances, where noise in predicted distances may induce extra errors during 3D coordinate generation. \nInspired by the traditional force field methods for molecular dynamics simulation, in this paper, we propose a novel approach called ConfGF by directly estimating the gradient fields of the log density of atomic coordinates. \nThe estimated gradient fields allow directly generating stable conformations via Langevin dynamics. \nHowever, the problem is very challenging as the gradient fields are roto-translation equivariant.\nWe notice that estimating the gradient fields of atomic coordinates can be translated to estimating the gradient fields of interatomic distances, and hence develop a novel algorithm based on recent score-based generative models to effectively estimate these gradients. \nExperimental results across multiple tasks show that ConfGF outperforms previous state-of-the-art baselines by a significant margin.",
        "conference": "ICML",
        "中文标题": "学习分子构象生成的梯度场",
        "摘要翻译": "我们研究了计算化学中的一个基本问题，即分子构象生成，试图从二维分子图中预测稳定的三维结构。现有的机器学习方法通常首先预测原子之间的距离，然后生成满足这些距离的三维结构，其中预测距离中的噪声可能在三维坐标生成过程中引入额外的误差。受传统分子动力学模拟的力场方法启发，本文提出了一种名为ConfGF的新方法，通过直接估计原子坐标对数密度的梯度场。估计的梯度场允许通过朗之万动力学直接生成稳定的构象。然而，这个问题非常具有挑战性，因为梯度场具有旋转平移等变性。我们注意到，估计原子坐标的梯度场可以转化为估计原子间距离的梯度场，因此基于最近的基于分数的生成模型开发了一种新算法，以有效估计这些梯度。跨多个任务的实验结果表明，ConfGF显著优于之前的最先进基线。",
        "领域": "分子构象生成、计算化学、分子动力学模拟",
        "问题": "从二维分子图预测稳定的三维分子结构，并减少预测过程中因噪声引入的误差。",
        "动机": "受传统分子动力学模拟的力场方法启发，旨在直接生成稳定的分子构象，避免现有方法中因预测距离噪声导致的误差累积。",
        "方法": "提出ConfGF方法，直接估计原子坐标对数密度的梯度场，通过朗之万动力学生成稳定构象，并开发基于分数生成模型的算法有效估计梯度。",
        "关键词": [
            "分子构象生成",
            "梯度场估计",
            "朗之万动力学",
            "分数生成模型",
            "计算化学"
        ],
        "涉及的技术概念": {
            "梯度场估计": "直接估计原子坐标对数密度的梯度场，用于生成稳定的分子构象。",
            "朗之万动力学": "利用估计的梯度场通过朗之万动力学直接生成稳定的分子构象。",
            "分数生成模型": "基于分数的生成模型用于有效估计原子间距离的梯度场，解决梯度场旋转平移等变性的挑战。"
        },
        "success": true
    },
    {
        "order": 582,
        "title": "Learning in Nonzero-Sum Stochastic Games with Potentials",
        "html": "https://ICML.cc//virtual/2021/poster/10095",
        "abstract": "Multi-agent reinforcement learning (MARL) has become effective in tackling discrete cooperative game scenarios. However, MARL has yet to penetrate settings beyond those modelled  by team and zero-sum games, confining it to a small subset of multi-agent systems.  In this paper, we introduce a new generation of MARL learners that can handle \\textit{nonzero-sum} payoff structures and continuous settings. In particular, we study the MARL problem in a class of games known as stochastic potential games (SPGs) with continuous state-action spaces. Unlike cooperative games, in which all agents share a common reward, SPGs are capable of modelling real-world scenarios where agents seek to fulfil their individual goals. \nWe prove theoretically our learning method, $\\ourmethod$, enables independent agents to learn Nash equilibrium strategies in \\textit{polynomial time}. We demonstrate our framework tackles previously unsolvable tasks such as \\textit{Coordination Navigation} and \\textit{large selfish routing games} and that it outperforms the state of the art MARL baselines such as MADDPG and COMIX in such scenarios. ",
        "conference": "ICML",
        "中文标题": "具有势能的非零和随机博弈中的学习",
        "摘要翻译": "多智能体强化学习（MARL）在解决离散合作游戏场景中已变得有效。然而，MARL尚未渗透到由团队和零和游戏建模之外的设置，将其限制在多智能体系统的一小部分中。在本文中，我们介绍了一代新的MARL学习者，能够处理非零和支付结构和连续设置。特别是，我们研究了在具有连续状态-动作空间的随机势游戏（SPGs）一类游戏中的MARL问题。与合作游戏不同，在合作游戏中所有智能体共享一个共同的奖励，SPGs能够建模现实世界场景，其中智能体寻求实现其个人目标。我们从理论上证明了我们的学习方法，ourmethod，使独立智能体能够在多项式时间内学习纳什均衡策略。我们展示了我们的框架解决了以前无法解决的任务，如协调导航和大型自私路由游戏，并且在这些场景中它优于最先进的MARL基线，如MADDPG和COMIX。",
        "领域": "多智能体强化学习、博弈论、连续状态-动作空间优化",
        "问题": "解决MARL在非零和支付结构和连续设置中的应用限制",
        "动机": "扩展MARL的应用范围，使其能够处理更广泛的多智能体系统，特别是那些需要智能体实现个人目标的场景",
        "方法": "研究在随机势游戏（SPGs）中的MARL问题，提出一种使独立智能体在多项式时间内学习纳什均衡策略的方法",
        "关键词": [
            "多智能体强化学习",
            "非零和博弈",
            "随机势游戏",
            "纳什均衡",
            "连续状态-动作空间"
        ],
        "涉及的技术概念": {
            "多智能体强化学习（MARL）": "用于解决多智能体系统中的学习问题，特别是在非零和博弈和连续设置中",
            "随机势游戏（SPGs）": "一类能够建模智能体寻求实现个人目标的游戏，扩展了MARL的应用范围",
            "纳什均衡": "在非合作游戏中，所有玩家策略的组合，使得没有任何玩家可以通过单方面改变策略而获得更好的结果"
        },
        "success": true
    },
    {
        "order": 583,
        "title": "Learning Interaction Kernels for Agent Systems on Riemannian Manifolds",
        "html": "https://ICML.cc//virtual/2021/poster/10167",
        "abstract": "Interacting agent and particle systems are extensively used to model complex phenomena in science and engineering.  We consider the problem of learning interaction kernels in these dynamical systems constrained to evolve on Riemannian manifolds from given trajectory data. The models we consider are based on interaction kernels depending on pairwise Riemannian distances between agents, with agents interacting locally along the direction of the shortest geodesic connecting them.  We show that our estimators converge at a rate that is independent of the dimension of the state space, and derive bounds on the trajectory estimation error, on the manifold, between the observed and estimated dynamics.  We demonstrate the performance of our estimator on two classical first order interacting systems: Opinion Dynamics and a Predator-Swarm system, with each system constrained on two prototypical manifolds, the $2$-dimensional sphere and the Poincar\\'e disk model of hyperbolic space.",
        "conference": "ICML",
        "中文标题": "学习黎曼流形上代理系统的交互核",
        "摘要翻译": "交互代理和粒子系统被广泛用于模拟科学和工程中的复杂现象。我们考虑从给定的轨迹数据中学习这些动态系统中受限于黎曼流形上演化的交互核的问题。我们考虑的模型基于依赖于代理之间成对黎曼距离的交互核，代理沿着连接它们的最短测地线方向局部交互。我们展示了我们的估计器以独立于状态空间维度的速率收敛，并推导了在流形上观察到的动态与估计动态之间的轨迹估计误差界限。我们在两个经典的一阶交互系统上展示了我们估计器的性能：意见动态和捕食者-群体系统，每个系统都受限于两个原型流形，即二维球面和双曲空间的庞加莱圆盘模型。",
        "领域": "多智能体系统, 黎曼几何在机器学习中的应用, 动态系统建模",
        "问题": "学习受限于黎曼流形上演化的动态系统中的交互核",
        "动机": "为了更准确地模拟和理解受限于复杂几何结构的动态系统中的交互行为",
        "方法": "基于成对黎曼距离的交互核模型，通过局部沿最短测地线方向的交互来学习动态系统的行为",
        "关键词": [
            "黎曼流形",
            "交互核学习",
            "多智能体系统",
            "动态系统建模",
            "轨迹估计"
        ],
        "涉及的技术概念": {
            "黎曼流形": "用于描述代理系统演化的几何空间，提供了距离和角度的度量",
            "交互核": "定义了代理之间相互作用的强度和方向，基于它们之间的黎曼距离",
            "最短测地线": "在黎曼流形上连接两点最短路径的方向，代理沿此方向进行局部交互"
        },
        "success": true
    },
    {
        "order": 584,
        "title": "Learning Intra-Batch Connections for Deep Metric Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10237",
        "abstract": "The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones. Particularly, deep metric learning utilizes neural networks to learn such a mapping. Most approaches rely on losses that only take the relations between pairs or triplets of samples into account, which either belong to the same class or two different classes. However, these methods do not explore the embedding space in its entirety. To this end, we propose an approach based on message  passing networks that takes all the relations in a mini-batch into account. We refine embedding vectors by exchanging messages among all samples in a given batch allowing the training process to be aware of its overall structure. Since not all samples are equally important to predict a decision boundary, we use an attention mechanism during message passing to allow samples to weigh the importance of each neighbor accordingly. We achieve state-of-the-art results on clustering and image retrieval on the CUB-200-2011, Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/intra_batch_connections.",
        "conference": "ICML",
        "中文标题": "学习批次内连接以进行深度度量学习",
        "摘要翻译": "度量学习的目标是学习一个函数，将样本映射到一个低维空间，在这个空间中，相似样本之间的距离比不相似样本之间的距离更近。特别是，深度度量学习利用神经网络来学习这样的映射。大多数方法依赖于仅考虑样本对或三元组之间关系的损失函数，这些样本对或三元组要么属于同一类别，要么属于两个不同的类别。然而，这些方法没有全面探索嵌入空间。为此，我们提出了一种基于消息传递网络的方法，该方法考虑了小批次中的所有关系。我们通过在给定批次中的所有样本之间交换消息来细化嵌入向量，使训练过程能够意识到其整体结构。由于并非所有样本对预测决策边界同等重要，我们在消息传递过程中使用注意力机制，允许样本相应地权衡每个邻居的重要性。我们在CUB-200-2011、Cars196、Stanford Online Products和In-Shop Clothes数据集上的聚类和图像检索任务中取得了最先进的结果。为了促进进一步的研究，我们在https://github.com/dvl-tum/intra_batch_connections上提供了代码和模型。",
        "领域": "深度度量学习, 图像检索, 聚类分析",
        "问题": "如何在小批次训练中全面探索嵌入空间并优化样本间的关系",
        "动机": "现有的深度度量学习方法主要关注样本对或三元组之间的关系，未能充分利用小批次中所有样本的全局信息，限制了模型性能的提升。",
        "方法": "提出了一种基于消息传递网络的方法，通过在小批次内所有样本间交换消息并利用注意力机制权衡邻居的重要性，以优化嵌入向量的学习。",
        "关键词": [
            "深度度量学习",
            "消息传递网络",
            "注意力机制",
            "图像检索",
            "聚类分析"
        ],
        "涉及的技术概念": {
            "消息传递网络": "用于在小批次内所有样本间交换信息，以细化嵌入向量并考虑整体结构。",
            "注意力机制": "在消息传递过程中用于权衡不同邻居样本的重要性，优化决策边界的预测。",
            "嵌入空间": "通过深度度量学习将样本映射到的低维空间，旨在使相似样本比不相似样本更接近。"
        },
        "success": true
    },
    {
        "order": 585,
        "title": "Learning Neural Network Subspaces",
        "html": "https://ICML.cc//virtual/2021/poster/9769",
        "abstract": "Recent observations have advanced our understanding of the neural network optimization landscape, revealing the existence of (1) paths of high accuracy containing diverse solutions and (2) wider minima offering improved performance. Previous methods observing diverse paths require multiple training runs. In contrast we aim to leverage both property (1) and (2) with a single method and in a single training run. With a similar computational cost as training one model, we learn lines, curves, and simplexes of high-accuracy neural networks. These neural network subspaces contain diverse solutions that can be ensembled, approaching the ensemble performance of independently trained networks without the training cost. Moreover, using the subspace midpoint boosts accuracy, calibration, and robustness to label noise, outperforming Stochastic Weight Averaging.",
        "conference": "ICML",
        "中文标题": "学习神经网络子空间",
        "摘要翻译": "最近的观察增进了我们对神经网络优化景观的理解，揭示了（1）包含多样解的高精度路径的存在以及（2）提供改进性能的更宽最小值。先前观察多样路径的方法需要多次训练运行。相比之下，我们的目标是在单次训练运行中利用属性（1）和（2）。以与训练一个模型相似的计算成本，我们学习高精度神经网络的线、曲线和单纯形。这些神经网络子空间包含可以集成的多样解，接近独立训练网络的集成性能，而无需训练成本。此外，使用子空间中点提高了准确性、校准和对标签噪声的鲁棒性，优于随机权重平均。",
        "领域": "神经网络优化、模型集成、深度学习训练策略",
        "问题": "如何在单次训练运行中发现并利用神经网络优化景观中的高精度路径和宽最小值，以提高模型性能。",
        "动机": "减少发现高精度神经网络解所需的训练成本，同时提高模型的准确性、校准和鲁棒性。",
        "方法": "通过单次训练运行学习神经网络的高精度子空间（包括线、曲线和单纯形），并利用这些子空间中的多样解进行集成。",
        "关键词": [
            "神经网络子空间",
            "模型集成",
            "训练优化",
            "高精度路径",
            "宽最小值"
        ],
        "涉及的技术概念": {
            "神经网络子空间": "论文中提出的概念，指的是通过特定方法在单次训练中发现的高精度神经网络解的集合，可以包含线、曲线或单纯形等多种形式。",
            "模型集成": "利用神经网络子空间中的多样解进行集成，以提高模型性能，接近独立训练网络的集成效果。",
            "宽最小值": "指优化景观中较平坦的区域，这些区域对应的解通常具有更好的泛化能力和鲁棒性。论文中利用这些宽最小值来提高模型的性能。"
        },
        "success": true
    },
    {
        "order": 586,
        "title": "Learning Node Representations Using Stationary Flow Prediction on Large Payment and Cash Transaction Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9941",
        "abstract": "Banks are required to analyse large transaction datasets as a part of the fight against financial crime. \nToday, this analysis is either performed manually by domain experts or using expensive feature engineering.\nGradient flow analysis allows for basic representation learning as node potentials can be inferred directly from network transaction data.\nHowever, the gradient model has a fundamental limitation: it cannot represent all types of of network flows. \nFurthermore, standard methods for learning the gradient flow are not appropriate for flow signals that span multiple orders of magnitude and contain outliers, i.e. transaction data.\nIn this work, the gradient model is extended to a gated version and we prove that it, unlike the gradient model, is a universal approximator for flows on graphs.\nTo tackle the mentioned challenges of transaction data, we propose a multi-scale and outlier robust loss function based on the Student-t log-likelihood.\nEthereum transaction data is used for evaluation and the gradient models outperform MLP models using hand-engineered and node2vec features in terms of relative error.\nThese results extend to 60 synthetic datasets, with experiments also showing that the gated gradient model learns qualitative information about the underlying synthetic generative flow distributions.",
        "conference": "ICML",
        "中文标题": "基于大规模支付和现金交易网络的稳态流预测学习节点表示",
        "摘要翻译": "作为打击金融犯罪的一部分，银行需要分析大量的交易数据集。目前，这种分析要么由领域专家手动完成，要么使用昂贵的特征工程方法。梯度流分析允许进行基本的表示学习，因为节点潜力可以直接从网络交易数据中推断出来。然而，梯度模型有一个根本性的限制：它不能表示所有类型的网络流。此外，学习梯度流的标准方法不适合跨越多个数量级并包含异常值（即交易数据）的流信号。在这项工作中，梯度模型被扩展为一个门控版本，我们证明它不同于梯度模型，是图上流的通用逼近器。为了解决交易数据的上述挑战，我们提出了一种基于Student-t对数似然的多尺度和异常值鲁棒的损失函数。以太坊交易数据用于评估，梯度模型在使用手工设计和node2vec特征的MLP模型方面在相对误差上表现更优。这些结果扩展到60个合成数据集，实验还表明门控梯度模型学习了关于底层合成生成流分布的定性信息。",
        "领域": "金融网络分析、异常检测、图表示学习",
        "问题": "如何在大规模支付和现金交易网络中有效学习节点表示，以支持金融犯罪分析",
        "动机": "现有的梯度流分析方法在表示所有类型的网络流和处理交易数据中的异常值方面存在限制",
        "方法": "扩展梯度模型至门控版本，并提出基于Student-t对数似然的多尺度和异常值鲁棒的损失函数",
        "关键词": [
            "门控梯度模型",
            "多尺度损失函数",
            "异常值鲁棒性",
            "图表示学习",
            "金融交易网络"
        ],
        "涉及的技术概念": {
            "门控梯度模型": "扩展的梯度模型版本，能够作为图上流的通用逼近器",
            "多尺度损失函数": "基于Student-t对数似然的损失函数，用于处理交易数据中的多尺度特性和异常值",
            "图表示学习": "从网络交易数据中学习节点表示的技术，用于金融犯罪分析"
        },
        "success": true
    },
    {
        "order": 587,
        "title": "Learning Noise Transition Matrix from Only Noisy Labels via Total Variation Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/10283",
        "abstract": "Many weakly supervised classification methods employ a noise transition matrix to capture the class-conditional label corruption. To estimate the transition matrix from noisy data, existing methods often need to estimate the noisy class-posterior, which could be unreliable due to the overconfidence of neural networks. In this work, we propose a theoretically grounded method that can estimate the noise transition matrix and learn a classifier simultaneously, without relying on the error-prone noisy class-posterior estimation. Concretely, inspired by the characteristics of the stochastic label corruption process, we propose total variation regularization, which encourages the predicted probabilities to be more distinguishable from each other. Under mild assumptions, the proposed method yields a consistent estimator of the transition matrix. We show the effectiveness of the proposed method through experiments on benchmark and real-world datasets.",
        "conference": "ICML",
        "中文标题": "通过全变分正则化仅从噪声标签中学习噪声转移矩阵",
        "摘要翻译": "许多弱监督分类方法采用噪声转移矩阵来捕捉类条件标签的损坏。为了从噪声数据中估计转移矩阵，现有方法通常需要估计噪声类后验，由于神经网络的过度自信，这可能不可靠。在这项工作中，我们提出了一种理论基础的方法，可以同时估计噪声转移矩阵和学习分类器，而不依赖于容易出错的噪声类后验估计。具体来说，受随机标签损坏过程特性的启发，我们提出了全变分正则化，鼓励预测概率之间更加可区分。在温和的假设下，所提出的方法产生了转移矩阵的一致估计器。我们通过在基准和真实世界数据集上的实验展示了所提出方法的有效性。",
        "领域": "弱监督学习、噪声标签处理、分类算法",
        "问题": "在仅使用噪声标签的情况下，如何可靠地估计噪声转移矩阵并同时学习分类器",
        "动机": "解决现有方法因依赖不可靠的噪声类后验估计而导致的噪声转移矩阵估计不准确的问题",
        "方法": "提出全变分正则化方法，通过鼓励预测概率之间的可区分性，同时估计噪声转移矩阵和学习分类器",
        "关键词": [
            "噪声转移矩阵",
            "全变分正则化",
            "弱监督学习",
            "噪声标签",
            "分类算法"
        ],
        "涉及的技术概念": {
            "噪声转移矩阵": "用于捕捉类条件标签损坏的矩阵，表示干净标签被错误标记为其他标签的概率",
            "全变分正则化": "一种正则化技术，用于鼓励模型预测的概率分布更加可区分，从而提高噪声转移矩阵的估计准确性",
            "弱监督学习": "在标签不完全准确或存在噪声的情况下进行学习的方法，本文专注于处理噪声标签的问题"
        },
        "success": true
    },
    {
        "order": 588,
        "title": "Learning Online Algorithms with Distributional Advice",
        "html": "https://ICML.cc//virtual/2021/poster/10065",
        "abstract": "We study the problem of designing online algorithms given advice about the input. \nWhile prior work had focused on deterministic advice, we only assume distributional access to the instances of interest, and the goal is to learn a competitive algorithm given access to i.i.d. samples. We aim to be competitive against an adversary with prior knowledge of the distribution, while also performing well against worst-case inputs.\n\nWe focus on the classical online problems of ski-rental and prophet-inequalities,\nand provide sample complexity bounds for the underlying learning tasks. \nFirst, we point out that for general distributions it is information-theoretically impossible to beat the worst-case competitive-ratio with any finite sample size.  \nAs our main contribution, we establish strong positive results for well-behaved distributions. Specifically, for the broad class of log-concave distributions, \nwe show that $\\mathrm{poly}(1/\\epsilon)$ samples suffice to obtain $(1+\\epsilon)$-competitive ratio. Finally, we show that this sample upper bound is close to best possible, even for very simple classes of distributions.",
        "conference": "ICML",
        "success": true,
        "中文标题": "学习带有分布建议的在线算法",
        "摘要翻译": "我们研究了在给定输入建议的情况下设计在线算法的问题。虽然之前的工作主要集中在确定性建议上，但我们仅假设对感兴趣的实例有分布性访问，目标是在给定独立同分布样本访问的情况下学习一个具有竞争力的算法。我们的目标是既对了解分布的先验知识的对手具有竞争力，又能在最坏情况输入下表现良好。我们专注于滑雪租赁和先知不等式这两个经典的在线问题，并为底层学习任务提供了样本复杂度界限。首先，我们指出，对于一般分布，任何有限的样本量都无法在信息理论上击败最坏情况的竞争比率。作为我们的主要贡献，我们为行为良好的分布建立了强有力的积极结果。具体来说，对于广泛的对数凹分布类，我们表明$\\mathrm{poly}(1/\\epsilon)$样本足以获得$(1+\\epsilon)$-竞争比率。最后，我们表明，即使对于非常简单的分布类，这个样本上限也接近于最佳可能。",
        "领域": "在线算法, 机器学习理论, 优化算法",
        "问题": "如何在仅假设对输入实例有分布性访问的情况下，设计出具有竞争力的在线算法。",
        "动机": "研究旨在探索在给定分布性建议的情况下，如何学习一个既对了解分布的先验知识的对手具有竞争力，又能在最坏情况输入下表现良好的在线算法。",
        "方法": "通过研究滑雪租赁和先知不等式这两个经典的在线问题，为底层学习任务提供样本复杂度界限，并对行为良好的分布建立积极结果。",
        "关键词": [
            "在线算法",
            "分布性建议",
            "样本复杂度",
            "对数凹分布",
            "竞争比率"
        ],
        "涉及的技术概念": {
            "在线算法": "在数据逐步到达时实时做出决策的算法，无需预先知道全部输入。",
            "分布性建议": "算法设计时假设对输入实例的分布有一定的了解或访问权限。"
        }
    },
    {
        "order": 589,
        "title": "Learning Optimal Auctions with Correlated Valuations from Samples",
        "html": "https://ICML.cc//virtual/2021/poster/8435",
        "abstract": "In single-item auction design, it is well known due to Cremer and McLean that when bidders’ valuations are drawn from a correlated prior distribution, the auctioneer can extract full social surplus as revenue. However, in most real-world applications, the prior is usually unknown and can only be learned from historical data. In this work, we investigate the robustness of the optimal auction with correlated valuations via sample complexity analysis. We prove upper and lower bounds on the number of samples from the unknown prior required to learn a (1-epsilon)-approximately optimal auction. Our results reinforce the common belief that optimal correlated auctions are sensitive to the distribution parameters and hard to learn unless the prior distribution is well-behaved.",
        "conference": "ICML",
        "中文标题": "从样本中学习具有相关估值的最优拍卖",
        "摘要翻译": "在单物品拍卖设计中，由于Cremer和McLean的研究，众所周知当投标人的估值是从一个相关的先验分布中抽取时，拍卖者可以提取全部社会剩余作为收入。然而，在大多数现实世界的应用中，先验通常是未知的，只能从历史数据中学习。在这项工作中，我们通过样本复杂性分析研究了具有相关估值的最优拍卖的鲁棒性。我们证明了从未知先验中学习一个(1-ε)近似最优拍卖所需的样本数量的上下界。我们的结果强化了普遍的观点，即最优相关拍卖对分布参数敏感，并且除非先验分布表现良好，否则难以学习。",
        "领域": "拍卖理论、机器学习应用、经济学与计算交叉领域",
        "问题": "如何在先验分布未知的情况下，从历史数据中学习并设计出近似最优的相关估值拍卖机制。",
        "动机": "现实世界中的拍卖设计往往面临先验分布未知的挑战，研究旨在探索从有限样本中学习最优拍卖机制的可行性及其样本复杂性。",
        "方法": "通过样本复杂性分析，研究学习近似最优相关估值拍卖所需的样本数量，并给出上下界证明。",
        "关键词": [
            "最优拍卖",
            "相关估值",
            "样本复杂性",
            "机器学习",
            "经济学"
        ],
        "涉及的技术概念": {
            "相关估值": "投标人的估值之间存在统计相关性，影响拍卖设计和收入最大化策略。",
            "样本复杂性": "研究从样本中学习最优拍卖机制所需的样本数量，衡量学习效率和可行性。",
            "近似最优拍卖": "在无法精确达到最优的情况下，设计接近最优性能的拍卖机制，平衡收益与计算可行性。"
        },
        "success": true
    },
    {
        "order": 590,
        "title": "Learning Queueing Policies for Organ Transplantation Allocation using Interpretable Counterfactual Survival Analysis",
        "html": "https://ICML.cc//virtual/2021/poster/9513",
        "abstract": "Organ transplantation is often the last resort for treating end-stage illnesses, but managing transplant wait-lists is challenging because of organ scarcity and the complexity of assessing donor-recipient compatibility. In this paper, we develop a data-driven model for (real-time) organ allocation using observational data for transplant outcomes. Our model integrates a queuing-theoretic framework with unsupervised learning to cluster the organs into ``organ types'', and then construct priority queues (associated with each organ type) wherein incoming patients are assigned. To reason about organ allocations, the model uses synthetic controls to infer a patient's survival outcomes under counterfactual allocations to the different organ types– the model is trained end-to-end to optimise the trade-off between patient waiting time and expected survival time. The usage of synthetic controls enable patient-level interpretations of allocation decisions that can be presented and understood by clinicians. We test our model on multiple data sets, and show that it outperforms other organ-allocation policies in terms of added life-years, and death count. Furthermore, we introduce a novel organ-allocation simulator to accurately test new policies.",
        "conference": "ICML",
        "中文标题": "使用可解释的反事实生存分析学习器官移植分配的排队策略",
        "摘要翻译": "器官移植通常是治疗终末期疾病的最后手段，但由于器官稀缺性和评估供体-受体兼容性的复杂性，管理移植等待列表具有挑战性。在本文中，我们开发了一个数据驱动的模型，用于（实时）器官分配，利用移植结果的观察数据。我们的模型将排队理论框架与无监督学习相结合，将器官聚类为‘器官类型’，然后构建优先队列（与每种器官类型相关联），将新来的患者分配到这些队列中。为了推理器官分配，模型使用合成控制来推断患者在不同器官类型反事实分配下的生存结果——模型端到端训练，以优化患者等待时间和预期生存时间之间的权衡。合成控制的使用使得分配决策能够进行患者层面的解释，这些解释可以被临床医生呈现和理解。我们在多个数据集上测试了我们的模型，并显示其在增加生命年和减少死亡数量方面优于其他器官分配策略。此外，我们引入了一种新颖的器官分配模拟器，以准确测试新策略。",
        "领域": "医疗决策支持系统、生存分析、器官移植管理",
        "问题": "如何在器官稀缺和供体-受体兼容性评估复杂的情况下，优化器官移植的分配策略。",
        "动机": "解决器官移植分配中的效率和公平性问题，通过数据驱动的方法优化患者等待时间和生存时间的平衡。",
        "方法": "结合排队理论框架和无监督学习，使用合成控制进行反事实生存分析，端到端训练模型以优化分配策略。",
        "关键词": [
            "器官移植分配",
            "反事实生存分析",
            "排队策略",
            "合成控制",
            "数据驱动模型"
        ],
        "涉及的技术概念": {
            "合成控制": "用于推断患者在不同器官类型反事实分配下的生存结果，支持患者层面的解释。",
            "无监督学习": "用于将器官聚类为不同的‘器官类型’，以便构建优先队列。",
            "排队理论框架": "用于管理器官分配过程中的优先队列，优化患者等待时间和生存时间的平衡。"
        },
        "success": true
    },
    {
        "order": 591,
        "title": "Learning Randomly Perturbed Structured Predictors for Direct Loss Minimization",
        "html": "https://ICML.cc//virtual/2021/poster/9319",
        "abstract": "Direct loss minimization is a popular approach for learning predictors over structured label spaces. This approach is computationally appealing as it replaces integration with optimization and allows to propagate gradients in a deep net using loss-perturbed prediction. Recently, this technique was extended to generative models,  by introducing a randomized predictor that samples a structure from a randomly perturbed score function. In this work, we interpolate between these techniques by learning the variance of randomized structured predictors as well as their mean, in order to balance between the learned score function and the randomized noise.  We demonstrate empirically the effectiveness of learning this balance in structured discrete spaces.",
        "conference": "ICML",
        "中文标题": "学习随机扰动的结构化预测器以实现直接损失最小化",
        "摘要翻译": "直接损失最小化是一种在结构化标签空间上学习预测器的流行方法。这种方法在计算上具有吸引力，因为它用优化代替了积分，并允许使用损失扰动的预测在深度网络中传播梯度。最近，这项技术通过引入一个随机预测器被扩展到生成模型，该预测器从随机扰动的评分函数中采样一个结构。在这项工作中，我们通过学习随机化结构化预测器的方差及其均值来在这些技术之间进行插值，以便在学习的评分函数和随机噪声之间找到平衡。我们在结构化离散空间中实证展示了学习这种平衡的有效性。",
        "领域": "结构化预测、生成模型、深度学习优化",
        "问题": "如何在结构化预测中平衡学习到的评分函数和随机噪声，以实现更有效的直接损失最小化。",
        "动机": "探索在结构化预测中通过学习随机化预测器的方差和均值，来优化直接损失最小化方法的性能。",
        "方法": "通过学习随机化结构化预测器的方差和均值，在学习的评分函数和随机噪声之间找到平衡，进而优化直接损失最小化。",
        "关键词": [
            "直接损失最小化",
            "结构化预测",
            "随机扰动",
            "生成模型",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "直接损失最小化": "一种在结构化标签空间上学习预测器的方法，通过优化代替积分，计算效率高。",
            "随机扰动预测": "通过引入随机噪声来扩展预测技术，用于生成模型中的结构采样。",
            "结构化离散空间": "指具有明确结构和离散性质的标签空间，本文在此空间中验证方法的有效性。"
        },
        "success": true
    },
    {
        "order": 592,
        "title": "Learning Representations by Humans, for Humans",
        "html": "https://ICML.cc//virtual/2021/poster/9071",
        "abstract": "When machine predictors can achieve higher performance than the human decision-makers they support,  improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human  performance.  This “Mind Composed with Machine” framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",
        "conference": "ICML",
        "中文标题": "为人类学习，由人类学习的表示",
        "摘要翻译": "当机器预测器能够达到比它们支持的人类决策者更高的性能时，提高人类决策者的性能常常与提高机器准确性混为一谈。在这里，我们提出了一个直接支持人类决策的框架，其中机器的角色是重新构建问题，而不是通过预测来规定行动。受到表示学习在提高机器预测器性能方面成功的启发，我们的框架学习了面向人类的表示，这些表示针对人类性能进行了优化。这个“机器与心智结合”的框架直接将人类决策模型纳入表示学习范式，并通过一种新颖的人类在环训练程序进行训练。我们通过实验证明了该框架在各种任务和表示形式中的成功应用。",
        "领域": "人机交互、表示学习、决策支持系统",
        "问题": "如何通过机器辅助提高人类决策者的性能，而非单纯追求机器预测的准确性",
        "动机": "探索机器如何通过重新构建问题而非直接预测来更有效地支持人类决策",
        "方法": "提出了一种结合人类决策模型的表示学习框架，采用人类在环训练方法优化人类性能",
        "关键词": [
            "表示学习",
            "人机交互",
            "决策支持",
            "人类在环训练",
            "心智与机器结合"
        ],
        "涉及的技术概念": {
            "表示学习": "通过学习数据的表示来改善机器预测性能的技术，在本框架中用于优化人类决策",
            "人类在环训练": "一种训练方法，将人类决策者直接纳入训练过程，以优化面向人类的表示",
            "心智与机器结合": "框架的核心概念，强调机器通过重新构建问题来支持而非替代人类决策"
        },
        "success": true
    },
    {
        "order": 593,
        "title": "Learning Routines for Effective Off-Policy Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9365",
        "abstract": "The performance of reinforcement learning depends upon designing an appropriate action space, where the effect of each action is measurable, yet, granular enough to permit flexible behavior. So far, this process involved non-trivial user choices in terms of the available actions and their execution frequency. We propose a novel framework for reinforcement learning that effectively lifts such constraints. Within our framework, agents learn effective behavior over a routine space: a new, higher-level action space, where each routine represents a set of 'equivalent' sequences of granular actions with arbitrary length. Our routine space is learned end-to-end to facilitate the accomplishment of underlying off-policy reinforcement learning objectives. We apply our framework to two state-of-the-art off-policy algorithms and show that the resulting agents obtain relevant performance improvements while requiring fewer interactions with the environment per episode, improving computational efficiency.",
        "conference": "ICML",
        "中文标题": "学习有效离策略强化学习的例程",
        "摘要翻译": "强化学习的性能取决于设计一个合适的动作空间，其中每个动作的效果是可测量的，同时又足够精细以允许灵活的行为。到目前为止，这一过程涉及到非平凡的用户选择，包括可用动作及其执行频率。我们提出了一个新颖的强化学习框架，有效地解除了这些限制。在我们的框架内，智能体在一个例程空间上学习有效行为：这是一个新的、更高层次的动作空间，其中每个例程代表一组具有任意长度的‘等效’精细动作序列。我们的例程空间是通过端到端学习来促进底层离策略强化学习目标的实现。我们将我们的框架应用于两种最先进的离策略算法，并表明由此产生的智能体在每集需要更少的环境交互的同时获得了相关的性能改进，提高了计算效率。",
        "领域": "强化学习算法优化、智能体行为学习、计算效率提升",
        "问题": "如何设计一个既灵活又高效的强化学习动作空间，以减少用户干预并提高学习效率",
        "动机": "减少强化学习中动作空间设计的用户干预需求，同时提高学习效率和计算效率",
        "方法": "提出一个基于例程空间的强化学习框架，通过端到端学习自动生成高效的动作序列",
        "关键词": [
            "强化学习",
            "例程空间",
            "离策略学习",
            "计算效率",
            "智能体行为"
        ],
        "涉及的技术概念": {
            "例程空间": "一个更高层次的动作空间，其中每个例程代表一组具有任意长度的等效精细动作序列，用于提高学习效率和灵活性",
            "离策略强化学习": "一种强化学习方法，允许智能体从不同于当前策略生成的数据中学习，以提高样本效率和灵活性",
            "端到端学习": "一种学习方法，直接从输入到输出进行学习，无需手动设计中间表示或特征，用于自动优化例程空间以适应强化学习目标"
        },
        "success": true
    },
    {
        "order": 594,
        "title": "Learning Self-Modulating Attention in Continuous Time Space with Applications to Sequential Recommendation",
        "html": "https://ICML.cc//virtual/2021/poster/8989",
        "abstract": "User interests are usually dynamic in the real world, which poses both theoretical and practical challenges for learning accurate preferences from rich behavior data. Among existing user behavior modeling solutions, attention networks are widely adopted for its effectiveness and relative simplicity. Despite being extensively studied, existing attentions still suffer from two limitations: i) conventional attentions mainly take into account the spatial correlation between user behaviors, regardless the distance between those behaviors in the continuous time space; and ii) these attentions mostly provide a dense and undistinguished distribution over all past behaviors then attentively encode them into the output latent representations. This is however not suitable in practical scenarios where a user's future actions are relevant to a small subset of her/his historical behaviors. In this paper, we propose a novel attention network, named \\textit{self-modulating attention}, that models the complex and non-linearly evolving dynamic user preferences. We empirically demonstrate the effectiveness of our method on top-N sequential recommendation tasks, and the results on three large-scale real-world datasets show that our model can achieve state-of-the-art performance.",
        "conference": "ICML",
        "中文标题": "学习连续时间空间中的自调制注意力及其在序列推荐中的应用",
        "摘要翻译": "在现实世界中，用户的兴趣通常是动态变化的，这为从丰富的行为数据中学习准确的偏好带来了理论和实践上的挑战。在现有的用户行为建模解决方案中，注意力网络因其有效性和相对简单性而被广泛采用。尽管已被广泛研究，现有的注意力机制仍存在两个局限性：i) 传统的注意力主要考虑用户行为之间的空间相关性，而忽略了这些行为在连续时间空间中的距离；ii) 这些注意力大多在所有过去行为上提供一个密集且无区别的分布，然后有注意力地将它们编码到输出的潜在表示中。然而，这在实际场景中并不适用，因为用户的未来行动仅与其历史行为的一小部分相关。在本文中，我们提出了一种新颖的注意力网络，称为自调制注意力，它模拟了复杂且非线性演化的动态用户偏好。我们在top-N序列推荐任务上实证证明了我们方法的有效性，三个大规模真实世界数据集上的结果表明，我们的模型可以实现最先进的性能。",
        "领域": "序列推荐、动态用户建模、注意力机制",
        "问题": "解决现有注意力机制在建模用户动态兴趣时忽略时间距离和提供无区别行为分布的问题",
        "动机": "为了更准确地捕捉用户兴趣的动态变化，提高序列推荐的性能",
        "方法": "提出自调制注意力网络，考虑行为间的时间距离，并区分历史行为的重要性",
        "关键词": [
            "自调制注意力",
            "序列推荐",
            "动态用户兴趣",
            "时间空间建模",
            "非线性演化"
        ],
        "涉及的技术概念": {
            "自调制注意力": "一种新颖的注意力机制，能够根据行为在连续时间空间中的距离和重要性动态调整注意力分布",
            "连续时间空间": "在模型中考虑用户行为发生的时间距离，以更准确地模拟用户兴趣的演化",
            "非线性演化": "描述用户兴趣随时间变化的复杂模式，自调制注意力能够捕捉这种非线性关系"
        },
        "success": true
    },
    {
        "order": 595,
        "title": "Learning Stochastic Behaviour from Aggregate Data",
        "html": "https://ICML.cc//virtual/2021/poster/8959",
        "abstract": "Learning nonlinear dynamics from aggregate data is a challenging problem because the full trajectory of each individual is not available, namely, the individual observed at one time may not be observed at the next time point, or the identity of individual is unavailable. This is in sharp contrast to learning dynamics with full trajectory data, on which the majority of existing methods are based. We propose a novel method using the weak form of Fokker Planck Equation (FPE) --- a partial differential equation --- to describe the density evolution of data in a sampled form, which is then combined with Wasserstein generative adversarial network (WGAN) in the training process. In such a sample-based framework we are able to learn the nonlinear dynamics from aggregate data without explicitly solving the partial differential equation (PDE) FPE. We demonstrate our approach in the context of a series of synthetic and real-world data sets.",
        "conference": "ICML",
        "中文标题": "从聚合数据中学习随机行为",
        "摘要翻译": "从聚合数据中学习非线性动态是一个具有挑战性的问题，因为每个个体的完整轨迹不可用，即在一个时间点观察到的个体可能在下一个时间点未被观察，或者个体的身份不可用。这与基于完整轨迹数据学习动态的现有大多数方法形成鲜明对比。我们提出了一种新方法，使用福克-普朗克方程（FPE）的弱形式——一个偏微分方程——以采样形式描述数据的密度演化，然后在训练过程中与Wasserstein生成对抗网络（WGAN）结合。在这样的基于样本的框架中，我们能够从聚合数据中学习非线性动态，而无需显式求解偏微分方程（PDE）FPE。我们在一系列合成和真实世界数据集的背景下展示了我们的方法。",
        "领域": "非线性动态学习、生成对抗网络、偏微分方程",
        "问题": "如何从聚合数据中学习非线性动态，尤其是在缺乏个体完整轨迹信息的情况下。",
        "动机": "解决在缺乏个体完整轨迹信息的情况下，从聚合数据中学习非线性动态的挑战。",
        "方法": "结合福克-普朗克方程的弱形式和Wasserstein生成对抗网络，以采样形式描述数据的密度演化，从而学习非线性动态。",
        "关键词": [
            "非线性动态学习",
            "聚合数据",
            "福克-普朗克方程",
            "Wasserstein生成对抗网络",
            "偏微分方程"
        ],
        "涉及的技术概念": {
            "福克-普朗克方程（FPE）": "用于描述数据的密度演化的偏微分方程，本文中采用其弱形式以避免显式求解。",
            "Wasserstein生成对抗网络（WGAN）": "在训练过程中用于与FPE弱形式结合，以学习非线性动态的生成对抗网络。",
            "偏微分方程（PDE）": "本文中指的是福克-普朗克方程，用于描述数据的密度演化，但通过弱形式和WGAN的结合避免了其显式求解。"
        },
        "success": true
    },
    {
        "order": 596,
        "title": "Learning Task Informed Abstractions",
        "html": "https://ICML.cc//virtual/2021/poster/9551",
        "abstract": "Current model-based reinforcement learning methods struggle when operating from complex visual scenes due to their inability to prioritize task-relevant features. To mitigate this problem, we propose learning Task Informed Abstractions (TIA) that explicitly separates reward-correlated visual features from distractors. For learning TIA, we introduce the formalism of Task Informed MDP (TiMDP) that is realized by training two models that learn visual features via cooperative reconstruction, but one model is adversarially dissociated from the reward signal. Empirical evaluation shows that TIA leads to significant performance gains over state-of-the-art methods on many visual control tasks where natural and unconstrained visual distractions pose a formidable challenge. Project page: https://xiangfu.co/tia",
        "conference": "ICML",
        "中文标题": "学习任务知情的抽象表示",
        "摘要翻译": "当前基于模型的强化学习方法在处理复杂视觉场景时表现不佳，因为它们无法优先考虑与任务相关的特征。为了缓解这一问题，我们提出了学习任务知情的抽象表示（TIA），它明确地将与奖励相关的视觉特征从干扰物中分离出来。为了学习TIA，我们引入了任务知情马尔可夫决策过程（TiMDP）的形式化方法，该方法通过训练两个模型来实现，这两个模型通过协作重建学习视觉特征，但其中一个模型与奖励信号对抗性地解耦。实证评估表明，在许多视觉控制任务中，TIA相比最先进的方法带来了显著的性能提升，尤其是在自然和无约束的视觉干扰构成巨大挑战的情况下。项目页面：https://xiangfu.co/tia",
        "领域": "强化学习、视觉控制、特征提取",
        "问题": "在复杂视觉场景中，基于模型的强化学习方法难以优先处理与任务相关的特征。",
        "动机": "提高强化学习方法在复杂视觉场景中的性能，通过分离与任务相关的视觉特征和干扰物。",
        "方法": "提出任务知情的抽象表示（TIA）和任务知情马尔可夫决策过程（TiMDP），通过协作重建和对抗性解耦学习视觉特征。",
        "关键词": [
            "任务知情抽象表示",
            "视觉控制",
            "强化学习",
            "特征分离",
            "对抗性学习"
        ],
        "涉及的技术概念": {
            "任务知情的抽象表示（TIA）": "一种明确分离与奖励相关视觉特征和干扰物的方法，用于提高强化学习在复杂视觉场景中的性能。",
            "任务知情马尔可夫决策过程（TiMDP）": "一种形式化方法，通过训练两个模型协作重建视觉特征，其中一个模型与奖励信号对抗性解耦。",
            "对抗性学习": "在TIA中用于解耦一个模型与奖励信号的技术，以增强模型对任务相关特征的识别能力。"
        },
        "success": true
    },
    {
        "order": 597,
        "title": "Learning to Generate Noise for Multi-Attack Robustness",
        "html": "https://ICML.cc//virtual/2021/poster/9867",
        "abstract": "Adversarial learning has emerged as one of the successful techniques to circumvent the susceptibility of existing methods against adversarial perturbations. However, the majority of existing defense methods are tailored to defend against a single category of adversarial perturbation (e.g. $\\ell_\\infty$-attack). In safety-critical applications, this makes these methods extraneous as the attacker can adopt diverse adversaries to deceive the system. Moreover, training on multiple perturbations simultaneously significantly increases the computational overhead during training. To address these challenges, we propose a novel meta-learning framework that explicitly learns to generate noise to improve the model's robustness against multiple types of attacks. Its key component is \\emph{Meta Noise Generator (MNG)} that outputs optimal noise to stochastically perturb a given sample, such that it helps lower the error on diverse adversarial perturbations. By utilizing samples generated by MNG, we train a model by enforcing the label consistency across multiple perturbations. We validate the robustness of models trained by our scheme on various datasets and against a wide variety of perturbations, demonstrating that it significantly outperforms the baselines across multiple perturbations with a marginal computational cost. ",
        "conference": "ICML",
        "中文标题": "学习生成噪声以实现多攻击鲁棒性",
        "摘要翻译": "对抗学习已成为规避现有方法对对抗性扰动敏感性的成功技术之一。然而，大多数现有的防御方法都是针对单一类别的对抗性扰动（例如ℓ∞攻击）进行防御的。在安全关键的应用中，这使得这些方法变得多余，因为攻击者可以采用多种对抗手段来欺骗系统。此外，同时针对多种扰动进行训练会显著增加训练过程中的计算开销。为了应对这些挑战，我们提出了一种新颖的元学习框架，该框架明确学习生成噪声，以提高模型对多种类型攻击的鲁棒性。其关键组件是元噪声生成器（MNG），它输出最优噪声以随机扰动给定样本，从而有助于降低对多种对抗性扰动的错误率。通过利用MNG生成的样本，我们通过强制多个扰动间的标签一致性来训练模型。我们在各种数据集上验证了通过我们的方案训练的模型的鲁棒性，并针对多种扰动进行了测试，结果表明它在多种扰动下显著优于基线方法，且计算成本仅略有增加。",
        "领域": "对抗性防御、元学习、多攻击鲁棒性",
        "问题": "现有防御方法通常只能防御单一类型的对抗性扰动，且同时防御多种扰动会显著增加计算开销。",
        "动机": "提高模型对多种对抗性攻击的鲁棒性，同时控制计算成本。",
        "方法": "提出一种元学习框架，通过元噪声生成器（MNG）生成最优噪声，用于随机扰动样本，以降低模型对多种对抗性扰动的错误率，并通过强制标签一致性来训练模型。",
        "关键词": [
            "对抗性防御",
            "元学习",
            "多攻击鲁棒性",
            "元噪声生成器",
            "标签一致性"
        ],
        "涉及的技术概念": {
            "元学习框架": "用于学习生成噪声以提高模型对多种对抗性攻击的鲁棒性。",
            "元噪声生成器（MNG）": "框架中的关键组件，负责生成最优噪声以随机扰动样本，帮助模型降低对多种对抗性扰动的错误率。",
            "标签一致性": "在训练过程中强制多个扰动间的标签一致性，以提高模型的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 598,
        "title": "Learning to Price Against a Moving Target",
        "html": "https://ICML.cc//virtual/2021/poster/9127",
        "abstract": "In the Learning to Price setting, a seller posts prices over time with the goal of maximizing revenue while learning the buyer's valuation. This problem is very well understood when values are stationary (fixed or iid). Here we study the problem where the buyer's value is a moving target, i.e., they change over time either by a stochastic process or adversarially with bounded variation. In either case, we provide matching upper and lower bounds on the optimal revenue loss. Since the target is moving, any information learned soon becomes out-dated, which forces the algorithms to keep switching between exploring and exploiting phases.",
        "conference": "ICML",
        "中文标题": "学习对移动目标定价",
        "摘要翻译": "在‘学习定价’设定中，卖家随时间发布价格，目的是在了解买家估值的同时最大化收入。当价值是静止的（固定或独立同分布）时，这个问题已经被很好地理解。这里我们研究的问题是买家的价值是一个移动目标，即它们随时间变化，无论是通过随机过程还是有界变化的对抗方式。在这两种情况下，我们都提供了关于最优收入损失的匹配上下界。由于目标是移动的，任何学到的信息很快就会过时，这迫使算法不断在探索和利用阶段之间切换。",
        "领域": "动态定价、在线学习、收益管理",
        "问题": "研究在买家价值随时间变化的情况下，如何通过定价策略最大化卖家的收入。",
        "动机": "传统的定价策略假设买家价值是静止的，而现实中买家价值可能随时间变化，这促使研究如何在这种动态环境下进行有效的定价。",
        "方法": "研究通过随机过程和对抗方式变化的买家价值，提供最优收入损失的上下界，并设计算法在探索和利用阶段之间切换以适应价值变化。",
        "关键词": [
            "动态定价",
            "在线学习",
            "收益管理",
            "移动目标",
            "探索与利用"
        ],
        "涉及的技术概念": {
            "移动目标": "指买家价值随时间变化，使得定价策略需要不断调整以适应这种变化。",
            "探索与利用": "算法需要在探索新信息和利用已有信息之间平衡，以应对买家价值的变化。",
            "最优收入损失": "研究在买家价值变化的情况下，定价策略与理想情况下的收入差距的上下界。"
        },
        "success": true
    },
    {
        "order": 599,
        "title": "Learning to Rehearse in Long Sequence Memorization",
        "html": "https://ICML.cc//virtual/2021/poster/9499",
        "abstract": "Existing reasoning tasks often have an important assumption that the input contents can be always accessed while reasoning, requiring unlimited storage resources and suffering from severe time delay on long sequences. To achieve efficient reasoning on long sequences with limited storage resources, memory augmented neural networks introduce a human-like write-read memory to compress and memorize the long input sequence in one pass, trying to answer subsequent queries only based on the memory. But they have two serious drawbacks: 1) they continually update the memory from current information and inevitably forget the early contents; 2) they do not distinguish what information is important and treat all contents equally. In this paper, we propose the Rehearsal Memory (RM) to enhance long-sequence memorization by self-supervised rehearsal with a history sampler. To alleviate the gradual forgetting of early information, we design self-supervised rehearsal training with recollection and familiarity tasks. Further, we design a history sampler to select informative fragments for rehearsal training, making the memory focus on the crucial information. We evaluate the performance of our rehearsal memory by the synthetic bAbI task and several downstream tasks, including text/video question answering and recommendation on long sequences.",
        "conference": "ICML",
        "中文标题": "学习在长序列记忆中进行排练",
        "摘要翻译": "现有的推理任务通常有一个重要假设，即在推理过程中可以始终访问输入内容，这需要无限的存储资源，并且在长序列上会遭受严重的时间延迟。为了实现有限存储资源下对长序列的高效推理，记忆增强神经网络引入了类似人类的读写记忆，以一次性压缩和记忆长输入序列，试图仅基于记忆回答后续查询。但它们有两个严重缺点：1）它们不断从当前信息更新记忆，不可避免地忘记早期内容；2）它们不区分哪些信息重要，平等对待所有内容。在本文中，我们提出了排练记忆（RM），通过带有历史采样器的自监督排练来增强长序列记忆。为了缓解早期信息的逐渐遗忘，我们设计了带有回忆和熟悉任务的自监督排练训练。此外，我们设计了一个历史采样器来选择信息丰富的片段进行排练训练，使记忆专注于关键信息。我们通过合成bAbI任务和几个下游任务（包括长序列上的文本/视频问答和推荐）来评估我们排练记忆的性能。",
        "领域": "自然语言处理与视觉结合, 长序列处理, 记忆增强神经网络",
        "问题": "如何在有限存储资源下高效处理长序列推理任务，同时避免早期信息的遗忘和不重要信息的平等处理。",
        "动机": "解决现有记忆增强神经网络在长序列处理中存在的两个主要问题：持续更新导致早期信息遗忘和未能区分信息重要性。",
        "方法": "提出排练记忆（RM），通过自监督排练和历史采样器选择重要信息片段进行训练，以增强长序列记忆。",
        "关键词": [
            "排练记忆",
            "长序列处理",
            "自监督学习",
            "历史采样器",
            "记忆增强"
        ],
        "涉及的技术概念": {
            "排练记忆（RM）": "通过自监督排练和历史采样器增强长序列记忆的技术，旨在解决记忆增强神经网络在长序列处理中的问题。",
            "自监督排练": "设计回忆和熟悉任务的自监督训练方法，用于缓解早期信息的逐渐遗忘。",
            "历史采样器": "选择信息丰富的片段进行排练训练的机制，使记忆能够专注于关键信息。"
        },
        "success": true
    },
    {
        "order": 600,
        "title": "Learning to Weight Imperfect Demonstrations",
        "html": "https://ICML.cc//virtual/2021/poster/8833",
        "abstract": "This paper investigates how to weight imperfect expert demonstrations for generative adversarial imitation learning (GAIL). The agent is expected to perform behaviors demonstrated by experts. But in many applications, experts could also make mistakes and their demonstrations would mislead or slow the learning process of the agent. Recently, existing methods for imitation learning from imperfect demonstrations mostly focus on using the preference or confidence scores to distinguish imperfect demonstrations. However, these auxiliary information needs to be collected with the help of an oracle, which is usually hard and expensive to afford in practice. In contrast, this paper proposes a method of learning to weight imperfect demonstrations in GAIL without imposing extensive prior information. We provide a rigorous mathematical analysis, presenting that the weights of demonstrations can be exactly determined by combining the discriminator and agent policy in GAIL. Theoretical analysis suggests that with the estimated weights the agent can learn a better policy beyond those plain expert demonstrations. Experiments in the Mujoco and Atari environments demonstrate that the proposed algorithm outperforms baseline methods in handling imperfect expert demonstrations.",
        "conference": "ICML",
        "中文标题": "学习加权不完美示范",
        "摘要翻译": "本文研究了如何为生成对抗模仿学习（GAIL）加权不完美的专家示范。代理期望执行专家示范的行为。但在许多应用中，专家也可能犯错，他们的示范可能会误导或减慢代理的学习过程。最近，现有的从不完美示范中进行模仿学习的方法主要集中在使用偏好或置信度分数来区分不完美的示范。然而，这些辅助信息需要在预言机的帮助下收集，这在实际中通常难以且昂贵。相比之下，本文提出了一种在GAIL中学习加权不完美示范的方法，而不需要强加大量的先验信息。我们提供了严格的数学分析，表明示范的权重可以通过结合GAIL中的判别器和代理策略来精确确定。理论分析表明，通过估计的权重，代理可以学习到一个超越那些普通专家示范的更好策略。在Mujoco和Atari环境中的实验表明，所提出的算法在处理不完美的专家示范方面优于基线方法。",
        "领域": "模仿学习、生成对抗网络、强化学习",
        "问题": "如何在不完美专家示范的情况下，有效地进行模仿学习。",
        "动机": "专家示范中可能存在错误，这些错误会误导或减慢学习过程，因此需要一种方法来加权这些不完美的示范，以提高学习效率和效果。",
        "方法": "提出了一种在生成对抗模仿学习（GAIL）中学习加权不完美示范的方法，该方法不需要额外的先验信息，通过结合判别器和代理策略来精确确定示范的权重。",
        "关键词": [
            "模仿学习",
            "生成对抗网络",
            "强化学习",
            "专家示范",
            "权重学习"
        ],
        "涉及的技术概念": {
            "生成对抗模仿学习（GAIL）": "一种结合生成对抗网络和模仿学习的方法，用于从专家示范中学习策略。",
            "判别器": "在GAIL中用于区分代理行为和专家示范的组件，帮助代理学习更接近专家的行为。",
            "代理策略": "代理在环境中采取行动的规则或策略，通过学习和优化来模仿专家的行为。"
        },
        "success": true
    },
    {
        "order": 601,
        "title": "Learning Transferable Visual Models From Natural Language Supervision",
        "html": "https://ICML.cc//virtual/2021/poster/9193",
        "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any\nother visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training\nexamples it was trained on.",
        "conference": "ICML",
        "中文标题": "从自然语言监督中学习可迁移的视觉模型",
        "摘要翻译": "最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限的监督形式限制了它们的通用性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从关于图像的原始文本中学习是一种有前途的替代方法，它利用了更广泛的监督来源。我们证明，预测哪个标题与哪个图像相匹配的简单预训练任务是一种高效且可扩展的方法，可以在从互联网收集的4亿（图像，文本）对的数据集上从头开始学习最先进的图像表示。预训练后，自然语言被用来引用学习到的视觉概念（或描述新的概念），使模型能够零样本迁移到下游任务。我们通过在30多个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，涵盖诸如OCR、视频中的动作识别、地理定位和许多类型的细粒度对象分类等任务。该模型在大多数任务上都有非平凡的迁移，并且通常与完全监督的基线竞争，而无需任何特定于数据集的训练。例如，我们在ImageNet上零样本匹配原始ResNet-50的准确性，而无需使用它训练中的128万个训练示例中的任何一个。",
        "领域": "自然语言处理与视觉结合、图像分类、零样本学习",
        "问题": "如何利用自然语言监督来学习可迁移的视觉模型，以克服传统计算机视觉系统在通用性和可用性上的限制。",
        "动机": "传统的计算机视觉系统依赖于固定的对象类别预测，这限制了其通用性和灵活性。直接从自然语言中学习视觉概念可以提供一个更广泛、更灵活的监督来源。",
        "方法": "采用预测图像与标题匹配的预训练任务，在大规模（图像，文本）对数据集上学习图像表示，然后利用自然语言实现零样本迁移到多种下游任务。",
        "关键词": [
            "自然语言监督",
            "可迁移学习",
            "零样本迁移",
            "图像表示学习",
            "预训练"
        ],
        "涉及的技术概念": {
            "自然语言监督": "利用自然语言作为监督信号来学习视觉模型，提供比传统固定类别标签更广泛的监督来源。",
            "零样本迁移": "模型能够在没有特定任务训练数据的情况下，通过自然语言描述直接适应新任务。",
            "图像表示学习": "通过预训练任务学习图像的通用表示，这些表示可以迁移到多种视觉任务中。"
        },
        "success": true
    },
    {
        "order": 602,
        "title": "Learning While Playing in Mean-Field Games: Convergence and Optimality",
        "html": "https://ICML.cc//virtual/2021/poster/9389",
        "abstract": "We study reinforcement learning in mean-field games. To achieve the Nash equilibrium, which consists of a policy and a mean-field state, existing algorithms require obtaining the optimal policy while fixing any mean-field state. In practice, however, the policy and the mean-field state evolve simultaneously, as each agent is learning while playing. To bridge such a gap, we propose a fictitious play algorithm, which alternatively updates the policy (learning) and the mean-field state (playing) by one step of policy optimization and gradient descent, respectively. Despite the nonstationarity induced by such an alternating scheme, we prove that the proposed algorithm converges to the Nash equilibrium with an explicit convergence rate. To the best of our knowledge, it is the first provably efficient algorithm that achieves learning while playing via alternating updates.\n",
        "conference": "ICML",
        "中文标题": "均值场博弈中的学习与游戏：收敛性与最优性",
        "摘要翻译": "我们研究了均值场博弈中的强化学习。为了实现由策略和均值场状态组成的纳什均衡，现有算法需要在固定任何均值场状态的情况下获得最优策略。然而，在实践中，策略和均值场状态是同时演变的，因为每个智能体在游戏的同时也在学习。为了弥补这一差距，我们提出了一种虚构游戏算法，该算法通过分别进行一步策略优化和梯度下降来交替更新策略（学习）和均值场状态（游戏）。尽管这种交替方案引入了非平稳性，但我们证明了所提出的算法能够以明确的收敛速度收敛到纳什均衡。据我们所知，这是第一个通过交替更新实现学习与游戏的可证明高效算法。",
        "领域": "强化学习、博弈论、多智能体系统",
        "问题": "在均值场博弈中实现策略和均值场状态的同步更新以达到纳什均衡",
        "动机": "解决现有算法在固定均值场状态下优化策略而无法反映实际中策略和均值场状态同时演变的局限性",
        "方法": "提出一种虚构游戏算法，通过交替进行策略优化和梯度下降来更新策略和均值场状态",
        "关键词": [
            "均值场博弈",
            "强化学习",
            "纳什均衡",
            "虚构游戏",
            "交替更新"
        ],
        "涉及的技术概念": {
            "均值场博弈": "用于描述大量相似智能体交互的博弈模型，其中每个智能体的行为对整体影响微小",
            "纳什均衡": "在博弈论中，指所有玩家的策略组合，使得没有任何玩家可以通过单方面改变策略而获得更好的结果",
            "虚构游戏算法": "一种通过交替更新策略和均值场状态来逼近纳什均衡的算法，适用于非平稳环境"
        },
        "success": true
    },
    {
        "order": 603,
        "title": "Learn-to-Share: A Hardware-friendly Transfer Learning Framework Exploiting Computation and Parameter Sharing",
        "html": "https://ICML.cc//virtual/2021/poster/9845",
        "abstract": "Task-specific fine-tuning on pre-trained transformers has achieved performance breakthroughs in multiple NLP tasks. Yet, as both computation and parameter size grows linearly with the number of sub-tasks, it is increasingly difficult to adopt such methods to the real world due to unrealistic memory and computation overhead on computing devices. Previous works on fine-tuning focus on reducing the growing parameter size to save storage cost by parameter sharing. However, compared to storage, the constraint of computation is a more critical issue with the fine-tuning models in modern computing environments. In this work, we propose LeTS, a framework that leverages both computation and parameter sharing across multiple tasks. Compared to traditional fine-tuning, LeTS proposes a novel neural architecture that contains a fixed pre-trained transformer model, plus learnable additive components for sub-tasks. The learnable components reuse the intermediate activations in the fixed pre-trained model, decoupling computation dependency. Differentiable neural architecture search is used to determine a task-specific computation sharing scheme, and a novel early stage pruning is applied to additive components for sparsity to achieve parameter sharing. Extensive experiments show that with 1.4% of extra parameters per task, LeTS reduces the computation by 49.5% on GLUE benchmarks with only 0.2% accuracy loss compared to full fine-tuning.",
        "conference": "ICML",
        "中文标题": "学会共享：一种利用计算与参数共享的硬件友好型迁移学习框架",
        "摘要翻译": "在预训练变换器上进行任务特定的微调已在多个自然语言处理任务中实现了性能突破。然而，由于计算和参数大小随着子任务数量的增加而线性增长，由于计算设备上不切实际的内存和计算开销，将这些方法应用于现实世界变得越来越困难。先前关于微调的工作主要集中在通过参数共享来减少增长的参数大小以节省存储成本。然而，与存储相比，在现代计算环境中，微调模型的计算约束是一个更为关键的问题。在这项工作中，我们提出了LeTS，一个跨多个任务利用计算和参数共享的框架。与传统的微调相比，LeTS提出了一种新颖的神经架构，包含一个固定的预训练变换器模型，以及用于子任务的可学习加法组件。可学习组件重用固定预训练模型中的中间激活，解耦计算依赖。使用可微分神经架构搜索来确定任务特定的计算共享方案，并对加法组件应用新颖的早期阶段修剪以实现稀疏性，从而实现参数共享。大量实验表明，与完全微调相比，LeTS在GLUE基准测试中以每个任务额外1.4%的参数，减少了49.5%的计算，仅损失0.2%的准确率。",
        "领域": "自然语言处理与视觉结合, 迁移学习, 模型优化",
        "问题": "如何在减少计算和参数大小的同时，保持或提高预训练变换器在多个NLP任务中的性能。",
        "动机": "解决预训练变换器在多个子任务上微调时计算和参数线性增长导致的内存和计算开销问题。",
        "方法": "提出LeTS框架，通过固定预训练模型和可学习加法组件实现计算和参数共享，使用可微分神经架构搜索和早期阶段修剪优化共享方案和稀疏性。",
        "关键词": [
            "迁移学习",
            "参数共享",
            "计算共享",
            "神经架构搜索",
            "模型稀疏化"
        ],
        "涉及的技术概念": {
            "可微分神经架构搜索": "用于自动确定任务特定的计算共享方案，优化模型结构。",
            "早期阶段修剪": "应用于加法组件以实现稀疏性，促进参数共享，减少模型大小。",
            "中间激活重用": "通过重用固定预训练模型中的中间激活，减少计算依赖，降低计算开销。"
        },
        "success": true
    },
    {
        "order": 604,
        "title": "LEGO: Latent Execution-Guided Reasoning for Multi-Hop Question Answering on Knowledge Graphs",
        "html": "https://ICML.cc//virtual/2021/poster/8925",
        "abstract": "Answering complex natural language questions on knowledge graphs (KGQA) is a challenging task. It requires reasoning with the input natural language questions as well as a massive, incomplete heterogeneous KG. Prior methods obtain an abstract structured query graph/tree from the input question and traverse the KG for answers following the query tree. However, they inherently cannot deal with missing links in the KG. Here we present LEGO, a Latent Execution-Guided reasOning framework to handle this challenge in KGQA. LEGO works in an iterative way, which alternates between (1) a Query Synthesizer, which synthesizes a reasoning action and grows the query tree step-by-step, and (2) a Latent Space Executor that executes the reasoning action in the latent embedding space to combat against the missing information in KG. To learn the synthesizer without step-wise supervision, we design a generic latent execution guided bottom-up search procedure to find good execution traces efficiently in the vast query space. Experimental results on several KGQA benchmarks demonstrate the effectiveness of our framework compared with previous state of the art.",
        "conference": "ICML",
        "中文标题": "LEGO：基于潜在执行引导的知识图谱多跳问答推理框架",
        "摘要翻译": "在知识图谱（KG）上回答复杂的自然语言问题（KGQA）是一项具有挑战性的任务。它需要对输入的自然语言问题以及一个庞大且不完整的异构知识图谱进行推理。先前的方法从输入问题中获取一个抽象的结构化查询图/树，并按照查询树遍历知识图谱以寻找答案。然而，这些方法本质上无法处理知识图谱中的缺失链接。本文提出了LEGO，一个潜在执行引导的推理框架，以应对KGQA中的这一挑战。LEGO以迭代方式工作，交替进行（1）查询合成器，它合成一个推理动作并逐步增长查询树，和（2）潜在空间执行器，它在潜在嵌入空间中执行推理动作，以对抗知识图谱中的缺失信息。为了在没有逐步监督的情况下学习合成器，我们设计了一个通用的潜在执行引导的自底向上搜索程序，以在广阔的查询空间中高效地找到良好的执行轨迹。在多个KGQA基准测试上的实验结果表明，与之前的最先进技术相比，我们的框架具有更高的有效性。",
        "领域": "知识图谱问答、多跳推理、自然语言处理与知识图谱结合",
        "问题": "处理知识图谱中缺失链接对多跳问答任务的影响",
        "动机": "解决现有方法在知识图谱问答中无法有效处理缺失链接的问题",
        "方法": "提出LEGO框架，通过查询合成器和潜在空间执行器的迭代工作，结合潜在执行引导的自底向上搜索程序，有效处理知识图谱中的缺失信息",
        "关键词": [
            "知识图谱问答",
            "多跳推理",
            "潜在执行引导",
            "查询合成",
            "潜在空间执行"
        ],
        "涉及的技术概念": {
            "潜在执行引导推理": "在潜在嵌入空间中执行推理动作，以对抗知识图谱中的缺失信息",
            "查询合成器": "合成推理动作并逐步增长查询树，支持多跳问答的逐步推理",
            "潜在空间执行器": "在潜在嵌入空间中执行查询合成器生成的推理动作，有效处理知识图谱中的缺失链接"
        },
        "success": true
    },
    {
        "order": 605,
        "title": "Lenient Regret and Good-Action Identification in Gaussian Process Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/10637",
        "abstract": "In this paper, we study the problem of Gaussian process (GP) bandits under relaxed optimization criteria stating that any function value above a certain threshold is ``good enough''.  On the theoretical side, we study various {\\em lenient regret} notions in which all near-optimal actions incur zero penalty, and provide upper bounds on the lenient regret for GP-UCB and an elimination algorithm, circumventing the usual $O(\\sqrt{T})$ term (with time horizon $T$) resulting from zooming extremely close towards the function maximum.  In addition, we complement these upper bounds with algorithm-independent lower bounds.  On the practical side, we consider the problem of finding a single ``good action'' according to a known pre-specified threshold, and introduce several good-action identification algorithms that exploit knowledge of the threshold.  We experimentally find that such algorithms can typically find a good action faster than standard optimization-based approaches.",
        "conference": "ICML",
        "success": true,
        "中文标题": "高斯过程赌博机中的宽松后悔与优质动作识别",
        "摘要翻译": "本文研究了在高斯过程（GP）赌博机中，采用宽松优化标准的问题，该标准指出任何高于特定阈值的函数值都“足够好”。在理论方面，我们研究了各种宽松后悔概念，其中所有接近最优的动作都不会受到惩罚，并为GP-UCB和一种消除算法提供了宽松后悔的上界，避免了通常由于极度接近函数最大值而产生的O(√T)项（时间范围为T）。此外，我们还用与算法无关的下界来补充这些上界。在实际应用方面，我们考虑了根据已知的预设阈值寻找单一“优质动作”的问题，并介绍了几种利用阈值知识的优质动作识别算法。我们通过实验发现，这类算法通常比基于标准优化的方法更快地找到优质动作。",
        "领域": "强化学习, 高斯过程, 赌博机算法",
        "问题": "在高斯过程赌博机中，如何通过宽松优化标准高效识别优质动作。",
        "动机": "研究动机是为了在高斯过程赌博机中，通过宽松优化标准，减少对接近最优动作的惩罚，从而更高效地识别优质动作。",
        "方法": "研究采用了理论分析和实验验证相结合的方法，包括对GP-UCB和消除算法的宽松后悔上界分析，以及引入几种利用阈值知识的优质动作识别算法。",
        "关键词": [
            "高斯过程",
            "赌博机算法",
            "宽松后悔",
            "优质动作识别",
            "阈值优化"
        ],
        "涉及的技术概念": {
            "宽松后悔": "在高斯过程赌博机中，所有接近最优的动作都不会受到惩罚的后悔概念。",
            "GP-UCB": "一种基于高斯过程的上界置信算法，用于优化赌博机问题。",
            "消除算法": "一种通过逐步消除次优动作来优化赌博机问题的算法。"
        }
    },
    {
        "order": 606,
        "title": "Let's Agree to Degree: Comparing Graph Convolutional Networks in the Message-Passing Framework",
        "html": "https://ICML.cc//virtual/2021/poster/10439",
        "abstract": "In this paper we cast neural networks defined on graphs as message-passing neural networks (MPNNs) to study the distinguishing power of different classes of such models. We are interested in when certain architectures are able to tell vertices apart based on the feature labels given as input with the graph. We consider two variants of MPNNS: anonymous MPNNs whose message functions depend only on the labels of vertices involved; and degree-aware MPNNs whose message functions can additionally use information regarding the degree of vertices. The former class covers popular graph neural network (GNN) formalisms for which the distinguished power is known. The latter covers graph convolutional networks (GCNs), introduced by Kipf and Welling, for which the distinguishing power was unknown. We obtain lower and upper bounds on the distinguishing power of (anonymous and degree-aware) MPNNs in terms of the distinguishing power of the Weisfeiler-Lehman (WL) algorithm. Our main results imply that (i) the distinguishing power of GCNs is bounded by the WL algorithm, but they may be one step ahead; (ii) the WL algorithm cannot be simulated by ``plain vanilla'' GCNs but the addition of a trade-off parameter between features of the vertex and those of its neighbours (as proposed by Kipf and Welling) resolves this problem.",
        "conference": "ICML",
        "中文标题": "让我们达成程度一致：在消息传递框架中比较图卷积网络",
        "摘要翻译": "在本文中，我们将定义在图上的神经网络视为消息传递神经网络（MPNNs），以研究不同类别此类模型的区分能力。我们关注的是某些架构何时能够基于图中给出的特征标签来区分顶点。我们考虑了MPNNs的两种变体：匿名MPNNs，其消息函数仅依赖于所涉及顶点的标签；以及度感知MPNNs，其消息函数可以额外使用关于顶点度的信息。前者涵盖了流行的图神经网络（GNN）形式主义，其区分能力已知。后者涵盖了由Kipf和Welling引入的图卷积网络（GCNs），其区分能力此前未知。我们获得了（匿名和度感知）MPNNs在Weisfeiler-Lehman（WL）算法的区分能力方面的下限和上限。我们的主要结果表明：（i）GCNs的区分能力受限于WL算法，但它们可能领先一步；（ii）WL算法不能被‘普通’GCNs模拟，但添加顶点特征与其邻居特征之间的权衡参数（如Kipf和Welling所提出的）解决了这个问题。",
        "领域": "图神经网络、图卷积网络、图表示学习",
        "问题": "比较不同类别的图神经网络在消息传递框架下的区分能力",
        "动机": "研究不同图神经网络架构在基于顶点特征标签区分顶点方面的能力",
        "方法": "将图神经网络视为消息传递神经网络，分析匿名和度感知MPNNs的区分能力，并与Weisfeiler-Lehman算法进行比较",
        "关键词": [
            "消息传递神经网络",
            "图卷积网络",
            "Weisfeiler-Lehman算法",
            "区分能力",
            "顶点特征"
        ],
        "涉及的技术概念": {
            "消息传递神经网络（MPNNs）": "一种将图上的神经网络视为消息传递过程的框架，用于研究图神经网络的区分能力",
            "Weisfeiler-Lehman（WL）算法": "一种用于图同构测试的经典算法，本文中用作比较图神经网络区分能力的基准",
            "图卷积网络（GCNs）": "由Kipf和Welling提出的一种图神经网络，本文研究了其在区分能力方面的表现"
        },
        "success": true
    },
    {
        "order": 607,
        "title": "Leveraged Weighted Loss for Partial Label Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9687",
        "abstract": "As an important branch of weakly supervised learning, partial label learning deals with data where each instance is assigned with a set of candidate labels, whereas only one of them is true. Despite many methodology studies on learning from partial labels, there still lacks theoretical understandings of their risk consistent properties under relatively weak assumptions, especially on the link between theoretical results and the empirical choice of parameters. In this paper, we propose a family of loss functions named \\textit{Leveraged Weighted} (LW) loss, which for the first time introduces the leverage parameter $\\beta$ to consider the trade-off between losses on partial labels and non-partial ones. From the theoretical side, we derive a generalized result of risk consistency for the LW loss in learning from partial labels, based on which we provide guidance to the choice of the leverage parameter $\\beta$. In experiments, we verify the theoretical guidance, and show the high effectiveness of our proposed LW loss on both benchmark and real datasets compared with other state-of-the-art partial label learning algorithms.",
        "conference": "ICML",
        "中文标题": "杠杆加权损失用于部分标签学习",
        "摘要翻译": "作为弱监督学习的一个重要分支，部分标签学习处理的是每个实例被赋予一组候选标签，而其中只有一个标签是真实的数据。尽管有许多关于从部分标签中学习的方法研究，但在相对较弱的假设下，尤其是理论结果与参数经验选择之间的联系方面，仍然缺乏对其风险一致性属性的理论理解。在本文中，我们提出了一系列名为杠杆加权（LW）损失的损失函数，首次引入了杠杆参数β来考虑部分标签和非部分标签损失之间的权衡。从理论方面，我们基于LW损失在从部分标签中学习中的风险一致性，推导出了一个广义结果，并据此为杠杆参数β的选择提供了指导。在实验中，我们验证了理论指导，并展示了我们提出的LW损失在基准和真实数据集上与其他最先进的部分标签学习算法相比的高效性。",
        "领域": "弱监督学习、部分标签学习、损失函数设计",
        "问题": "在弱监督学习环境下，如何设计一种能够有效处理部分标签数据的损失函数，并理解其风险一致性属性。",
        "动机": "为了解决在部分标签学习中缺乏对损失函数风险一致性属性的理论理解，尤其是在理论结果与参数选择之间的联系方面的问题。",
        "方法": "提出了一种名为杠杆加权（LW）损失的新损失函数家族，引入了杠杆参数β来权衡部分标签和非部分标签的损失，并从理论上分析了其风险一致性。",
        "关键词": [
            "部分标签学习",
            "杠杆加权损失",
            "风险一致性",
            "弱监督学习",
            "损失函数设计"
        ],
        "涉及的技术概念": {
            "杠杆加权（LW）损失": "一种新的损失函数，通过引入杠杆参数β来权衡部分标签和非部分标签的损失，旨在提高模型在部分标签数据上的学习效率。",
            "风险一致性": "指损失函数在理论上的性质，确保在部分标签学习中的模型能够收敛到真实风险最小化的解。",
            "杠杆参数β": "LW损失中的一个关键参数，用于控制部分标签和非部分标签损失之间的权衡，其选择基于理论分析提供的指导。"
        },
        "success": true
    },
    {
        "order": 608,
        "title": "Leveraging Good Representations in Linear Contextual Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9267",
        "abstract": "The linear contextual bandit literature is mostly focused on the design of efficient learning algorithms for a given representation.\nHowever, a contextual bandit problem may admit multiple linear representations, each one with different characteristics that directly impact the regret of the learning algorithm. In particular, recent works showed that there exist ``good'' representations for which constant problem-dependent regret can be achieved.\nIn this paper, we first provide a systematic analysis of the different definitions of ``good'' representations proposed in the literature. We then propose a novel selection algorithm able to adapt to the best representation in a set of $M$ candidates. We show that the regret is indeed never worse than the regret obtained by running \\textsc{LinUCB} on best representation (up to a $\\ln M$ factor). As a result, our algorithm achieves constant regret if a ``good'' representation is available in the set. Furthermore, we show the algorithm may still achieve constant regret by implicitly constructing a ``good'' representation, even when none of the initial representations is ``good''. Finally, we validate our theoretical findings in a number of standard contextual bandit problems.",
        "conference": "ICML",
        "success": true,
        "中文标题": "在线性上下文赌博机中利用良好表示",
        "摘要翻译": "线性上下文赌博机文献主要关注于为给定表示设计高效学习算法。然而，一个上下文赌博机问题可能允许多种线性表示，每种表示具有直接影响学习算法遗憾的不同特性。特别是，最近的工作表明存在可以实现与问题相关的常数遗憾的“良好”表示。在本文中，我们首先对文献中提出的“良好”表示的不同定义进行了系统分析。然后，我们提出了一种新颖的选择算法，能够适应一组M个候选中的最佳表示。我们表明，遗憾确实不会比在最佳表示上运行LinUCB获得的遗憾更差（最多相差一个ln M因子）。因此，如果集合中存在“良好”表示，我们的算法可以实现常数遗憾。此外，我们展示了即使初始表示中没有“良好”表示，算法仍可能通过隐式构建“良好”表示来实现常数遗憾。最后，我们在一些标准的上下文赌博机问题中验证了我们的理论发现。",
        "领域": "强化学习, 在线学习, 表示学习",
        "问题": "如何在多种线性表示中选择或构建能够实现常数遗憾的“良好”表示。",
        "动机": "研究动机是为了解决线性上下文赌博机问题中表示选择对学习算法性能的影响，探索实现常数遗憾的可能性。",
        "方法": "提出了一种新颖的选择算法，能够适应一组候选中的最佳表示，并在必要时隐式构建“良好”表示。",
        "关键词": [
            "线性上下文赌博机",
            "表示学习",
            "遗憾最小化",
            "在线学习",
            "强化学习"
        ],
        "涉及的技术概念": {
            "线性上下文赌博机": "一种在每次决策时考虑上下文信息的强化学习框架，用于在不确定环境中做出序列决策。",
            "表示学习": "学习数据表示的方法，旨在捕捉数据中的有用信息以改善学习算法的性能。",
            "遗憾最小化": "在在线学习中，通过最小化与最佳可能决策相比的累积损失来优化算法性能的策略。"
        }
    },
    {
        "order": 609,
        "title": "Leveraging Language to Learn Program Abstractions and Search Heuristics",
        "html": "https://ICML.cc//virtual/2021/poster/10371",
        "abstract": "Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, andgeneralizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains – string editing, image composition, and abstract reasoning about scenes – even when no natural language hints are available at test time.",
        "conference": "ICML",
        "中文标题": "利用语言学习程序抽象和搜索启发式",
        "摘要翻译": "归纳程序合成，或从期望行为的例子中推断程序，为构建可解释、健壮和可泛化的机器学习系统提供了一个通用范式。有效的程序合成依赖于两个关键要素：一个强大的函数库用于构建程序，以及一个高效的搜索策略用于找到解决给定任务的程序。我们介绍了LAPS（用于抽象和程序搜索的语言），一种利用自然语言注释来指导联合学习库和神经引导的合成搜索模型的技术。当集成到最先进的库学习系统（DreamCoder）中时，LAPS在三个领域——字符串编辑、图像组合和场景的抽象推理——上产生了更高质量的库，并提高了搜索效率和泛化能力，即使在测试时没有可用的自然语言提示。",
        "领域": "程序合成、机器学习系统、自然语言处理与程序结合",
        "问题": "如何有效地从示例中合成程序，并提高搜索效率和泛化能力。",
        "动机": "通过利用自然语言注释来联合学习程序库和搜索模型，以提高程序合成的质量和效率。",
        "方法": "引入LAPS技术，利用自然语言注释指导库和神经引导搜索模型的联合学习，并将其集成到DreamCoder系统中。",
        "关键词": [
            "程序合成",
            "自然语言处理",
            "神经引导搜索",
            "库学习",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "归纳程序合成": "从期望行为的例子中推断程序，用于构建可解释、健壮和可泛化的机器学习系统。",
            "神经引导搜索模型": "利用神经网络引导程序搜索过程，以提高搜索效率和准确性。",
            "自然语言注释": "使用自然语言提供的额外信息来指导程序库的学习和搜索策略的优化。"
        },
        "success": true
    },
    {
        "order": 610,
        "title": "Leveraging Non-uniformity in First-order Non-convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9457",
        "abstract": "Classical global convergence results for first-order methods rely on uniform smoothness and the \\L{}ojasiewicz inequality. Motivated by properties of objective functions that arise in machine learning, we propose a non-uniform refinement of these notions, leading to \\emph{Non-uniform Smoothness} (NS) and \\emph{Non-uniform \\L{}ojasiewicz inequality} (N\\L{}). The new definitions inspire new geometry-aware first-order methods that are able to converge to global optimality faster than the classical $\\Omega(1/t^2)$ lower bounds. To illustrate the power of these geometry-aware methods and their corresponding non-uniform analysis, we consider two important problems in machine learning: policy gradient optimization in reinforcement learning (PG), and generalized linear model training in supervised learning (GLM). For PG, we find that normalizing the gradient ascent method can accelerate convergence to $O(e^{- c \\cdot t})$ (where $c > 0$) while incurring less overhead than existing algorithms. For GLM, we show that geometry-aware normalized gradient descent can also achieve a linear convergence rate, which significantly improves the best known results. We additionally show that the proposed geometry-aware gradient descent methods escape landscape plateaus faster than standard gradient descent. Experimental results are used to illustrate and complement the theoretical findings.",
        "conference": "ICML",
        "中文标题": "利用一阶非凸优化中的非均匀性",
        "摘要翻译": "一阶方法的经典全局收敛结果依赖于均匀光滑性和Łojasiewicz不等式。受机器学习中目标函数性质的启发，我们提出了这些概念的非均匀细化，引出了非均匀光滑性（NS）和非均匀Łojasiewicz不等式（NŁ）。这些新定义激发了新的几何感知一阶方法，这些方法能够比经典的Ω(1/t²)下界更快地收敛到全局最优。为了说明这些几何感知方法及其对应的非均匀分析的力量，我们考虑了机器学习中的两个重要问题：强化学习中的策略梯度优化（PG）和监督学习中的广义线性模型训练（GLM）。对于PG，我们发现归一化梯度上升方法可以加速收敛到O(e^{-c·t})（其中c > 0），同时产生的开销比现有算法少。对于GLM，我们表明几何感知归一化梯度下降也可以实现线性收敛速率，这显著改善了最著名的结果。我们还表明，提出的几何感知梯度下降方法比标准梯度下降更快地逃离景观平台。实验结果用于说明和补充理论发现。",
        "领域": "优化算法, 强化学习, 监督学习",
        "问题": "如何在一阶非凸优化中利用非均匀性加速收敛速度",
        "动机": "受机器学习中目标函数性质的启发，探索非均匀光滑性和非均匀Łojasiewicz不等式，以开发更快的优化算法",
        "方法": "提出非均匀光滑性和非均匀Łojasiewicz不等式的概念，并基于这些概念开发几何感知的一阶优化方法",
        "关键词": [
            "非均匀光滑性",
            "非均匀Łojasiewicz不等式",
            "几何感知优化",
            "策略梯度优化",
            "广义线性模型"
        ],
        "涉及的技术概念": {
            "非均匀光滑性（NS）": "用于描述目标函数在不同区域具有不同光滑性质的概念，允许优化算法在不同区域采用不同的步长策略",
            "非均匀Łojasiewicz不等式（NŁ）": "扩展了经典的Łojasiewicz不等式，允许在非均匀条件下分析优化算法的收敛性",
            "几何感知优化方法": "利用目标函数的几何性质（如非均匀光滑性和非均匀Łojasiewicz不等式）设计的优化算法，旨在加速收敛速度"
        },
        "success": true
    },
    {
        "order": 611,
        "title": "Leveraging Public Data for Practical Private Query Release",
        "html": "https://ICML.cc//virtual/2021/poster/9383",
        "abstract": "In many statistical problems, incorporating priors can significantly improve performance. However, the use of prior knowledge in differentially private query release has remained underexplored, despite such priors commonly being available in the form of public datasets, such as previous US Census releases. With the goal of releasing statistics about a private dataset, we present PMW^Pub, which---unlike existing baselines---leverages public data drawn from a related distribution as prior information. We provide a theoretical analysis and an empirical evaluation on the American Community Survey (ACS) and ADULT datasets, which shows that our method outperforms state-of-the-art methods. Furthermore, PMW^Pub scales well to high-dimensional data domains, where running many existing methods would be computationally infeasible.",
        "conference": "ICML",
        "中文标题": "利用公共数据实现实用的隐私查询发布",
        "摘要翻译": "在许多统计问题中，融入先验知识可以显著提升性能。然而，在差分隐私查询发布中利用先验知识的研究仍然不足，尽管这类先验知识常以公共数据集的形式存在，如之前的美国人口普查发布。为了发布关于私有数据集的统计数据，我们提出了PMW^Pub，与现有基线不同，它利用来自相关分布的公共数据作为先验信息。我们在美国社区调查（ACS）和ADULT数据集上提供了理论分析和实证评估，结果表明我们的方法优于最先进的方法。此外，PMW^Pub能够很好地适应高维数据领域，而在这些领域中运行许多现有方法在计算上是不可行的。",
        "领域": "差分隐私、数据挖掘、统计学习",
        "问题": "如何在差分隐私查询发布中有效利用公共数据作为先验知识以提高性能",
        "动机": "探索在差分隐私查询发布中利用公共数据作为先验知识的潜力，以提升统计发布的性能和效率",
        "方法": "提出PMW^Pub方法，利用来自相关分布的公共数据作为先验信息，进行理论分析和实证评估",
        "关键词": [
            "差分隐私",
            "公共数据",
            "查询发布",
            "先验知识",
            "高维数据"
        ],
        "涉及的技术概念": {
            "差分隐私": "一种隐私保护技术，确保在发布统计数据时保护个体隐私",
            "公共数据": "作为先验知识的数据集，用于提升差分隐私查询发布的性能",
            "高维数据处理": "指PMW^Pub方法能够有效处理高维数据领域，这在许多现有方法中是计算上不可行的"
        },
        "success": true
    },
    {
        "order": 612,
        "title": "Leveraging Sparse Linear Layers for Debuggable Deep Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10529",
        "abstract": "We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantitatively and via human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks.",
        "conference": "ICML",
        "中文标题": "利用稀疏线性层构建可调试的深度网络",
        "摘要翻译": "我们展示了如何通过学习深度特征表示上的稀疏线性模型拟合，可以构建出更易于调试的神经网络。这些网络在保持高准确性的同时，也更适合人类解释，这一点我们通过定量分析和人类实验进行了验证。我们进一步说明了由此产生的稀疏解释如何帮助识别虚假相关性、解释错误分类，并在视觉和语言任务中诊断模型偏差。",
        "领域": "深度学习可解释性、计算机视觉、自然语言处理",
        "问题": "如何构建既保持高准确性又易于人类理解和调试的深度神经网络",
        "动机": "提高深度神经网络的可解释性和调试便利性，使其不仅性能优越，而且便于人类理解和诊断",
        "方法": "通过学习深度特征表示上的稀疏线性模型拟合，构建可调试的神经网络",
        "关键词": [
            "稀疏线性模型",
            "深度特征表示",
            "可调试性",
            "模型解释性",
            "虚假相关性"
        ],
        "涉及的技术概念": {
            "稀疏线性模型": "用于在深度特征表示上进行拟合，以实现模型的高可解释性和调试便利性",
            "深度特征表示": "通过深度学习模型提取的特征，作为稀疏线性模型的输入",
            "可调试性": "指网络能够通过稀疏解释等方法，便于人类理解和诊断模型行为的能力"
        },
        "success": true
    },
    {
        "order": 613,
        "title": "LieTransformer: Equivariant Self-Attention for Lie Groups",
        "html": "https://ICML.cc//virtual/2021/poster/8407",
        "abstract": "Group equivariant neural networks are used as building blocks of group invariant neural networks, which have been shown to improve generalisation performance and data efficiency through principled parameter sharing. Such works have mostly focused on group equivariant convolutions, building on the result that group equivariant linear maps are necessarily convolutions. In this work, we extend the scope of the literature to self-attention, that is emerging as a prominent building block of deep learning models. We propose the LieTransformer, an architecture composed of LieSelfAttention layers that are equivariant to arbitrary Lie groups and their discrete subgroups. We demonstrate the generality of our approach by showing experimental results that are competitive to baseline methods on a wide range of tasks: shape counting on point clouds, molecular property regression and modelling particle trajectories under Hamiltonian dynamics.",
        "conference": "ICML",
        "中文标题": "LieTransformer：李群等变自注意力机制",
        "摘要翻译": "群等变神经网络被用作群不变神经网络的构建模块，已通过原则性的参数共享显示出提高泛化性能和数据效率的能力。此类工作主要集中在群等变卷积上，基于群等变线性映射必然是卷积的结果。在这项工作中，我们将文献的范围扩展到自注意力，后者正成为深度学习模型的一个突出构建模块。我们提出了LieTransformer，一种由LieSelfAttention层组成的架构，这些层对任意李群及其离散子群具有等变性。我们通过展示在广泛任务上与基线方法竞争性的实验结果来证明我们方法的通用性：点云上的形状计数、分子属性回归和哈密顿动力学下的粒子轨迹建模。",
        "领域": "深度学习与几何结合、分子属性预测、粒子轨迹建模",
        "问题": "扩展群等变神经网络的应用范围至自注意力机制，实现对任意李群及其离散子群的等变性。",
        "动机": "探索自注意力机制在群等变神经网络中的应用，以提升模型在多种任务上的泛化能力和数据效率。",
        "方法": "提出LieTransformer架构，包含对任意李群及其离散子群等变的LieSelfAttention层。",
        "关键词": [
            "李群等变",
            "自注意力机制",
            "LieTransformer",
            "分子属性预测",
            "粒子轨迹建模"
        ],
        "涉及的技术概念": {
            "群等变神经网络": "通过参数共享实现输入变换下的输出等变性，提升模型的泛化能力和数据效率。",
            "自注意力机制": "用于捕捉输入数据内部的依赖关系，作为深度学习模型的构建模块。",
            "李群及其离散子群": "提供了一种数学框架，用于描述和分析对称性，是群等变神经网络的理论基础。"
        },
        "success": true
    },
    {
        "order": 614,
        "title": "Light RUMs",
        "html": "https://ICML.cc//virtual/2021/poster/8503",
        "abstract": "A Random Utility Model (RUM) is a distribution on permutations over a universe of items.  For each subset of the universe, a RUM induces a natural distribution of the winner in the subset: choose a permutation according to the RUM distribution and pick the maximum item in the subset according to the chosen permutation.  RUMs are widely used in the theory of discrete choice.  \n\nIn this paper we consider the question of the (lossy) compressibility of RUMs on a universe of size $n$, i.e., the minimum number of bits required to approximate the winning probabilities of each slate.  Our main result is that RUMs can be approximated using $\\tilde{O}(n^2)$ bits, an exponential improvement over the standard representation; furthermore, we show that this bound is optimal.   \n\nEn route, we sharpen the classical existential result of McFadden and Train (2000) by showing that the minimum size of a mixture of multinomial logits required to can approximate a general RUM is $\\tilde{\\Theta}(n)$.",
        "conference": "ICML",
        "success": true,
        "中文标题": "轻量级随机效用模型",
        "摘要翻译": "随机效用模型（RUM）是在一组项目上的排列分布。对于该组中的每一个子集，RUM会自然地导出一个关于子集中胜者的分布：根据RUM分布选择一个排列，然后根据所选的排列在子集中挑选最大项目。RUM在离散选择理论中被广泛应用。在本文中，我们考虑了大小为n的组上RUM的（有损）可压缩性问题，即近似每个候选列表的获胜概率所需的最小位数。我们的主要结果表明，RUM可以使用约O(n^2)位来近似，这是对标准表示法的指数级改进；此外，我们证明这个界限是最优的。在此过程中，我们通过展示近似一般RUM所需的多项式逻辑混合的最小大小为约Θ(n)，从而强化了McFadden和Train（2000）的经典存在性结果。",
        "领域": "离散选择理论, 数据压缩, 算法优化",
        "问题": "研究随机效用模型（RUM）在大小为n的组上的有损压缩问题，即如何用最少的位数近似每个候选列表的获胜概率。",
        "动机": "探索RUM在离散选择理论中的应用潜力，特别是在数据压缩和算法优化方面的可能性，以提高计算效率和存储效率。",
        "方法": "通过理论分析和算法设计，证明了RUM可以用约O(n^2)位来近似，并验证了这一界限的最优性。同时，通过改进McFadden和Train的结果，展示了近似一般RUM所需的多项式逻辑混合的最小大小。",
        "关键词": [
            "随机效用模型",
            "数据压缩",
            "离散选择理论",
            "算法优化",
            "多项式逻辑混合"
        ],
        "涉及的技术概念": {
            "随机效用模型（RUM）": "在离散选择理论中用于描述和预测个体在多个选项之间做出选择的概率分布模型。",
            "有损压缩": "在本文中指用较少的位数近似表示RUM的获胜概率，牺牲一定的精确性以换取存储和计算效率的提升。",
            "多项式逻辑混合": "一种用于近似或表示复杂选择概率模型的统计方法，本文中用于展示近似一般RUM所需的最小混合大小。"
        }
    },
    {
        "order": 615,
        "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning",
        "html": "https://ICML.cc//virtual/2021/poster/9435",
        "abstract": "While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce's view that deduction, induction, and abduction are the primitives of reasoning, we design three synthetic tasks that are intended to require the model to have these three abilities.\nWe specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called 'LIME' (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task. The code for generating LIME tasks is available at https://github.com/tonywu95/LIME.",
        "conference": "ICML",
        "中文标题": "LIME：学习数学推理基本元素的归纳偏差",
        "摘要翻译": "虽然在神经架构设计中引入归纳偏差已被广泛研究，但我们假设变压器网络足够灵活，能够从合适的通用任务中学习归纳偏差。在此，我们通过以数据集的形式编码归纳偏差，替代了架构工程。受皮尔斯观点启发，即演绎、归纳和溯因是推理的基本元素，我们设计了三个合成任务，旨在要求模型具备这三种能力。我们特别将这些任务设计为合成且不包含数学知识，以确保只能从这些任务中学习到基本的推理偏差。这定义了一种名为'LIME'（学习数学推理的归纳偏差）的新预训练方法。使用LIME训练的模型在四个非常不同的大型数学推理基准测试中显著优于普通变压器模型。与传统的预训练方法主导计算成本不同，LIME仅需典型下游任务计算成本的一小部分。生成LIME任务的代码可在https://github.com/tonywu95/LIME获取。",
        "领域": "自然语言处理与视觉结合、数学推理、预训练模型",
        "问题": "如何在神经网络中有效学习和应用归纳偏差以提升数学推理能力",
        "动机": "探索变压器网络是否能够从通用任务中学习归纳偏差，从而减少对架构工程的依赖，提升数学推理能力",
        "方法": "设计三个合成任务（演绎、归纳、溯因）来编码归纳偏差，开发名为LIME的预训练方法，通过小计算成本提升模型在数学推理任务上的表现",
        "关键词": [
            "归纳偏差",
            "数学推理",
            "预训练模型",
            "变压器网络",
            "合成任务"
        ],
        "涉及的技术概念": {
            "归纳偏差": "在模型设计中引入的假设或偏好，用于指导学习过程，使其更倾向于某些类型的解决方案",
            "变压器网络": "一种基于自注意力机制的深度学习模型架构，适用于处理序列数据",
            "预训练方法": "在大规模数据集上预先训练模型，以学习通用特征或知识，然后在下游任务上进行微调"
        },
        "success": true
    },
    {
        "order": 616,
        "title": "Linear Transformers Are Secretly Fast Weight Programmers",
        "html": "https://ICML.cc//virtual/2021/poster/10587",
        "abstract": "We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a slow neural net learns by gradient descent to program the fast weights of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.",
        "conference": "ICML",
        "中文标题": "线性变换器实为快速权重编程器",
        "摘要翻译": "我们展示了线性化自注意力机制与90年代初的快速权重控制器之间的形式等价性，其中慢速神经网络通过梯度下降学习，通过一系列基本编程指令（即自创激活模式的外积，现今称为键和值）来编程另一个网络的快速权重。这类快速权重编程器（FWPs）学习如何操作有限内存的内容并与之动态交互。我们推断出近期线性化softmax注意力变体的内存容量限制，并用类似delta规则的编程指令替代纯加性外积，使得FWP能更容易学习如何修正当前从键到值的映射。FWP还学习计算动态变化的学习率。我们还提出了一种新的核函数来线性化注意力，平衡了简单性和有效性。我们在合成检索问题以及标准机器翻译和语言建模任务上进行了实验，证明了我们方法的优势。",
        "领域": "自然语言处理与视觉结合, 机器翻译, 语言建模",
        "问题": "线性化自注意力机制的内存容量限制及其在动态交互中的应用效率问题",
        "动机": "探索线性化自注意力机制与早期快速权重控制器的等价性，提升模型在动态内存操作和交互中的能力",
        "方法": "通过引入类似delta规则的编程指令和新的核函数，优化快速权重编程器的学习效率和内存操作能力",
        "关键词": [
            "线性变换器",
            "快速权重编程器",
            "自注意力机制",
            "动态学习率",
            "核函数"
        ],
        "涉及的技术概念": {
            "线性化自注意力机制": "通过线性化处理减少计算复杂度，同时保持模型对输入序列的关注能力",
            "快速权重编程器": "一种能够动态调整其快速权重以优化任务性能的神经网络架构",
            "delta规则": "一种用于调整权重以最小化误差的学习规则，此处用于优化键到值的映射修正过程"
        },
        "success": true
    },
    {
        "order": 617,
        "title": "Link Prediction with Persistent Homology: An Interactive View",
        "html": "https://ICML.cc//virtual/2021/poster/9487",
        "abstract": "Link prediction is an important learning task for graph-structured data. In this paper, we propose a novel topological approach to characterize interactions between two nodes. Our topological feature, based on the extended persistent homology, encodes rich structural information regarding the multi-hop paths connecting nodes. Based on this feature, we propose a graph neural network method that outperforms state-of-the-arts on different benchmarks. As another contribution, we propose a novel algorithm to more efficiently compute the extended persistence diagrams for graphs. This algorithm can be generally applied to accelerate many other topological methods for graph learning tasks.",
        "conference": "ICML",
        "中文标题": "基于持久同调的链接预测：一种交互式视角",
        "摘要翻译": "链接预测是图结构数据的一项重要学习任务。本文中，我们提出了一种新颖的拓扑方法来刻画两个节点间的交互作用。我们的拓扑特征基于扩展的持久同调，编码了关于连接节点的多跳路径的丰富结构信息。基于这一特征，我们提出了一种图神经网络方法，该方法在不同基准测试中优于现有技术。作为另一项贡献，我们提出了一种新算法，用于更高效地计算图的扩展持久图。这一算法可普遍应用于加速许多其他用于图学习任务的拓扑方法。",
        "领域": "图神经网络、拓扑数据分析、链接预测",
        "问题": "如何更有效地预测图结构数据中的链接",
        "动机": "探索图结构中节点间交互的拓扑特征，以提高链接预测的准确性和效率",
        "方法": "提出基于扩展持久同调的拓扑特征和图神经网络方法，以及一种高效计算扩展持久图的新算法",
        "关键词": [
            "持久同调",
            "图神经网络",
            "链接预测",
            "拓扑数据分析",
            "扩展持久图"
        ],
        "涉及的技术概念": {
            "持久同调": "用于捕捉和量化图数据中的拓扑特征，特别是多跳路径的结构信息",
            "图神经网络": "利用图结构数据和拓扑特征进行链接预测的深度学习方法",
            "扩展持久图": "一种高效的算法，用于计算和优化图学习任务中的持久同调特征"
        },
        "success": true
    },
    {
        "order": 618,
        "title": "Lipschitz normalization for self-attention layers with application to graph neural networks",
        "html": "https://ICML.cc//virtual/2021/poster/8537",
        "abstract": "Attention based neural networks are state of the art in a large range of applications. However, their performance tends to degrade when the number of layers increases. In this work, we show that enforcing Lipschitz continuity by normalizing the attention scores can significantly improve the performance of deep attention models. First, we show that, for deep graph attention networks (GAT), gradient explosion appears during training, leading to poor performance of gradient-based training algorithms. To address this issue, we derive a theoretical analysis of the Lipschitz continuity of attention modules and introduce LipschitzNorm, a simple and parameter-free normalization for self-attention mechanisms that enforces the model to be Lipschitz continuous.\nWe then apply LipschitzNorm to GAT and Graph Transformers and show that their performance is substantially improved in the deep setting (10 to 30 layers). More specifically, we show that a deep GAT model with LipschitzNorm achieves state of the art results for node label prediction tasks that exhibit long-range dependencies, while showing consistent improvements over their unnormalized counterparts in benchmark node classification tasks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "自注意力层的Lipschitz归一化及其在图神经网络中的应用",
        "摘要翻译": "基于注意力的神经网络在大量应用中处于领先地位。然而，当层数增加时，它们的性能往往会下降。在这项工作中，我们展示了通过归一化注意力分数来强制Lipschitz连续性可以显著提高深度注意力模型的性能。首先，我们展示了对于深度图注意力网络（GAT），在训练过程中会出现梯度爆炸，导致基于梯度的训练算法性能不佳。为了解决这个问题，我们对注意力模块的Lipschitz连续性进行了理论分析，并引入了LipschitzNorm，这是一种简单且无需参数的自注意力机制归一化方法，强制模型保持Lipschitz连续。然后，我们将LipschitzNorm应用于GAT和图变换器，并展示了在深度设置（10到30层）中它们的性能得到了显著提升。更具体地说，我们展示了带有LipschitzNorm的深度GAT模型在表现出长距离依赖的节点标签预测任务中达到了最先进的结果，同时在基准节点分类任务中相对于未归一化的模型表现出一致的改进。",
        "领域": "图神经网络, 注意力机制, 深度学习优化",
        "问题": "深度注意力模型在层数增加时性能下降的问题",
        "动机": "提高深度注意力模型的性能，特别是在图神经网络中的应用",
        "方法": "通过理论分析引入LipschitzNorm归一化方法，强制自注意力机制保持Lipschitz连续性",
        "关键词": [
            "Lipschitz连续性",
            "自注意力机制",
            "图注意力网络",
            "深度神经网络",
            "节点分类"
        ],
        "涉及的技术概念": {
            "Lipschitz连续性": "在论文中用于确保模型训练过程中的稳定性，防止梯度爆炸",
            "自注意力机制": "论文中用于捕捉输入数据内部依赖关系的技术，通过归一化提高其在深度网络中的表现",
            "图注意力网络": "论文中应用LipschitzNorm的主要模型之一，用于处理图结构数据"
        }
    },
    {
        "order": 619,
        "title": "Local Algorithms for Finding Densely Connected Clusters",
        "html": "https://ICML.cc//virtual/2021/poster/10555",
        "abstract": "Local graph clustering is an important algorithmic technique for analysing massive graphs, and has been widely applied in many research fields of data science. While the objective of most (local) graph clustering algorithms is to find a vertex set of low conductance, there has been a sequence of recent studies that highlight the importance of the inter-connection between clusters when analysing real-world datasets. Following this line of research, in this work we study local algorithms for finding a pair of vertex sets defined with respect to their inter-connection and their relationship with  the rest of the graph. The key to our analysis is a new reduction technique that relates the structure of multiple sets to a single vertex set in the reduced graph. Among many potential applications, we show that our algorithms successfully recover densely connected clusters in the Interstate Disputes Dataset and the US Migration Dataset.\n ",
        "conference": "ICML",
        "中文标题": "寻找密集连接集群的局部算法",
        "摘要翻译": "局部图聚类是分析大规模图的重要算法技术，已被广泛应用于数据科学的许多研究领域。虽然大多数（局部）图聚类算法的目标是找到一个低导纳的顶点集，但最近的一系列研究强调了在分析现实世界数据集时集群间连接的重要性。沿着这一研究方向，在本工作中，我们研究了寻找一对顶点集的局部算法，这对顶点集是根据它们之间的互连以及它们与图中其余部分的关系来定义的。我们分析的关键是一种新的归约技术，它将多集合的结构与归约图中的单个顶点集联系起来。在许多潜在应用中，我们展示了我们的算法成功地在州际争端数据集和美国迁移数据集中恢复了密集连接的集群。",
        "领域": "图聚类算法、数据科学、网络分析",
        "问题": "如何在大型图中有效地找到密集连接的集群",
        "动机": "研究现实世界数据集中集群间连接的重要性，并提出新的算法来解决这一问题",
        "方法": "开发了一种新的归约技术，将多集合的结构与归约图中的单个顶点集联系起来，用于寻找密集连接的集群",
        "关键词": [
            "局部图聚类",
            "密集连接集群",
            "归约技术",
            "州际争端数据集",
            "美国迁移数据集"
        ],
        "涉及的技术概念": {
            "局部图聚类": "一种用于在大规模图中识别密集连接子图的算法技术",
            "归约技术": "将多集合的结构与单个顶点集联系起来的技术，用于简化图的分析",
            "密集连接集群": "指图中顶点之间连接非常紧密的子图，是图聚类算法寻找的目标"
        },
        "success": true
    },
    {
        "order": 620,
        "title": "Local Correlation Clustering with Asymmetric Classification Errors",
        "html": "https://ICML.cc//virtual/2021/poster/9161",
        "abstract": "In the Correlation Clustering problem, we are given a complete weighted graph $G$ with its edges labeled as ``similar' and ``dissimilar' by a noisy binary classifier. For a clustering $\\mathcal{C}$ of graph $G$, a similar edge is in disagreement with $\\mathcal{C}$, if its endpoints belong to distinct clusters; and a dissimilar edge is in disagreement with $\\mathcal{C}$ if its endpoints belong to the same cluster. The disagreements vector, $\\disagree$, is a vector indexed by the vertices of $G$ such that the $v$-th coordinate $\\disagree_v$ equals the weight of all disagreeing edges incident on $v$. The goal is to produce a clustering that minimizes the $\\ell_p$ norm of the disagreements vector for $p\\geq 1$. We study the $\\ell_p$ objective in Correlation Clustering under the following assumption: Every similar edge has weight in $[\\alpha\\mathbf{w},\\mathbf{w}]$ and every dissimilar edge has weight at least $\\alpha\\mathbf{w}$ (where $\\alpha \\leq 1$ and $\\mathbf{w}>0$ is a scaling parameter). We give an $O\\left((\\nicefrac{1}{\\alpha})^{\\nicefrac{1}{2}-\\nicefrac{1}{2p}}\\cdot \\log\\nicefrac{1}{\\alpha}\\right)$ approximation algorithm for this problem. Furthermore, we show an almost matching convex programming integrality gap.",
        "conference": "ICML",
        "中文标题": "局部相关聚类与不对称分类错误",
        "摘要翻译": "在相关聚类问题中，我们给定一个由噪声二元分类器标记边为‘相似’和‘不相似’的完全加权图G。对于图G的一个聚类C，如果一条相似边的端点属于不同的簇，则该边与C存在分歧；如果一条不相似边的端点属于同一个簇，则该边与C存在分歧。分歧向量disagree是一个由图G的顶点索引的向量，其中第v个坐标disagree_v等于所有与v相关的不一致边的权重之和。目标是为p≥1生成一个最小化分歧向量ℓp范数的聚类。我们在以下假设下研究了相关聚类中的ℓp目标：每条相似边的权重在[αw,w]之间，每条不相似边的权重至少为αw（其中α≤1，w>0是一个缩放参数）。我们给出了一个O((1/α)^(1/2−1/2p)⋅log(1/α))的近似算法来解决这个问题。此外，我们还展示了一个几乎匹配的凸规划完整性间隙。",
        "领域": "图聚类算法、近似算法、机器学习",
        "问题": "在存在噪声分类错误的情况下，如何有效地对图进行聚类以最小化分歧向量的ℓp范数。",
        "动机": "研究动机是为了解决在噪声数据中，如何更准确地进行图聚类，特别是在存在不对称分类错误的情况下，如何优化聚类结果。",
        "方法": "采用了一种基于凸规划的近似算法，该算法考虑了相似和不相似边的权重范围，旨在最小化分歧向量的ℓp范数。",
        "关键词": [
            "相关聚类",
            "近似算法",
            "ℓp范数",
            "噪声分类",
            "图聚类"
        ],
        "涉及的技术概念": {
            "相关聚类": "一种图聚类方法，旨在根据边的相似或不相似标记将图中的顶点分组。",
            "ℓp范数": "用于衡量分歧向量大小的数学工具，p≥1，不同的p值可以反映不同的优化目标。",
            "凸规划": "用于求解优化问题的数学方法，本文中用于开发近似算法并分析其性能。"
        },
        "success": true
    },
    {
        "order": 621,
        "title": "Locally Adaptive Label Smoothing Improves Predictive Churn",
        "html": "https://ICML.cc//virtual/2021/poster/8779",
        "abstract": "Training modern neural networks is an inherently noisy process that can lead to high \\emph{prediction churn}-- disagreements between re-trainings of the same model due to factors such as randomization in the parameter initialization and mini-batches-- even when the trained models all attain similar accuracies. Such prediction churn can be very undesirable in practice. In this paper, we present several baselines for reducing churn and show that training on soft labels obtained by adaptively smoothing each example's label based on the example's neighboring labels often outperforms the baselines on churn while improving accuracy on a variety of benchmark classification tasks and model architectures.",
        "conference": "ICML",
        "中文标题": "局部自适应标签平滑改善预测变动",
        "摘要翻译": "训练现代神经网络本质上是一个充满噪声的过程，这可能导致高预测变动——由于参数初始化和小批量中的随机化等因素，同一模型的多次重新训练之间会出现预测不一致——即使所有训练后的模型都达到了相似的准确度。这种预测变动在实践中可能非常不受欢迎。在本文中，我们提出了几种减少变动的基础方法，并表明基于每个示例的邻近标签自适应平滑其标签获得的软标签训练，通常在变动上优于基础方法，同时在多种基准分类任务和模型架构上提高了准确度。",
        "领域": "深度学习优化、分类任务、模型训练稳定性",
        "问题": "减少神经网络训练过程中的预测变动",
        "动机": "预测变动可能导致模型在实际应用中的不一致性，影响模型的可靠性和用户体验",
        "方法": "通过自适应平滑每个示例的标签来生成软标签，用于训练以减少预测变动",
        "关键词": [
            "预测变动",
            "标签平滑",
            "模型训练稳定性",
            "自适应学习",
            "分类任务"
        ],
        "涉及的技术概念": {
            "预测变动": "指同一模型在不同训练过程中由于随机因素导致的预测结果不一致",
            "标签平滑": "一种正则化技术，通过调整标签的硬分配来减少模型对训练数据的过拟合",
            "自适应学习": "根据数据或模型的状态动态调整学习策略，以提高模型性能或稳定性"
        },
        "success": true
    },
    {
        "order": 622,
        "title": "Locally Persistent Exploration in Continuous Control Tasks with Sparse Rewards",
        "html": "https://ICML.cc//virtual/2021/poster/9613",
        "abstract": "A major challenge in reinforcement learning is the design of exploration strategies, especially for environments with sparse reward structures and continuous state and action spaces. Intuitively, if the reinforcement signal is very scarce, the agent should rely on some form of short-term memory in order to cover its environment efficiently. We propose a new exploration method, based on two intuitions: (1) the choice of the next exploratory action should depend not only on the (Markovian) state of the environment, but also on the agent's trajectory so far, and (2) the agent should utilize a measure of spread in the state space to avoid getting stuck in a small region. Our method leverages concepts often used in statistical physics to provide explanations for the behavior of simplified (polymer) chains in order to generate persistent (locally self-avoiding) trajectories in state space. We discuss the theoretical properties of locally self-avoiding walks and their ability to provide a kind of short-term memory through a decaying temporal correlation within the trajectory. We provide empirical evaluations of our approach in a simulated 2D navigation task, as well as higher-dimensional MuJoCo continuous control locomotion tasks with sparse rewards.",
        "conference": "ICML",
        "中文标题": "稀疏奖励连续控制任务中的局部持久探索",
        "摘要翻译": "强化学习中的一个主要挑战是探索策略的设计，尤其是在奖励结构稀疏且状态和动作空间连续的环境中。直观上，如果强化信号非常稀少，智能体应依赖某种形式的短期记忆以有效覆盖其环境。我们提出了一种新的探索方法，基于两个直觉：（1）选择下一个探索性动作不仅应取决于环境的（马尔可夫）状态，还应取决于智能体迄今为止的轨迹；（2）智能体应利用状态空间中的一种扩散度量，以避免陷入小区域。我们的方法利用了统计物理学中常用的概念，为简化（聚合物）链的行为提供解释，以在状态空间中生成持久（局部自回避）的轨迹。我们讨论了局部自回避行走的理论特性及其通过轨迹内衰减的时间相关性提供一种短期记忆的能力。我们在模拟的2D导航任务以及具有稀疏奖励的高维MuJoCo连续控制运动任务中提供了我们方法的实证评估。",
        "领域": "强化学习探索策略、连续控制任务、稀疏奖励环境",
        "问题": "在稀疏奖励和连续状态动作空间的环境中设计有效的探索策略",
        "动机": "解决在稀疏奖励环境下智能体探索效率低下的问题，通过引入短期记忆和状态空间扩散度量来优化探索过程",
        "方法": "提出一种基于局部自回避行走理论的探索方法，结合智能体轨迹和状态空间扩散度量，生成持久且高效的探索轨迹",
        "关键词": [
            "强化学习",
            "探索策略",
            "稀疏奖励",
            "连续控制",
            "局部自回避行走"
        ],
        "涉及的技术概念": {
            "局部自回避行走": "一种在状态空间中生成持久探索轨迹的方法，通过避免重复访问同一区域来提高探索效率",
            "短期记忆": "智能体利用其历史轨迹信息来指导未来的探索行动，以在稀疏奖励环境中更有效地覆盖状态空间",
            "状态空间扩散度量": "用于量化智能体在状态空间中的探索范围，帮助避免陷入局部区域，确保探索的广泛性和多样性"
        },
        "success": true
    },
    {
        "order": 623,
        "title": "Locally Private k-Means in One Round",
        "html": "https://ICML.cc//virtual/2021/poster/8515",
        "abstract": "We provide an approximation algorithm for k-means clustering in the \\emph{one-round} (aka \\emph{non-interactive}) local model of differential privacy (DP). Our algorithm achieves an approximation ratio arbitrarily close to the best \\emph{non private} approximation algorithm, improving upon previously known algorithms that only guarantee large (constant) approximation ratios. Furthermore, ours is the first constant-factor approximation algorithm for k-means that requires only \\emph{one} round of communication in the local DP model, positively resolving an open question of Stemmer (SODA 2020). Our algorithmic framework is quite flexible; we demonstrate this by showing that it also yields a similar near-optimal approximation algorithm in the (one-round) shuffle DP model.",
        "conference": "ICML",
        "success": true,
        "中文标题": "本地隐私保护的一轮k均值聚类",
        "摘要翻译": "我们为差分隐私（DP）的本地模型中的一轮（也称为非交互式）k均值聚类提供了一种近似算法。我们的算法实现了与非隐私近似算法最佳近似比任意接近的近似比，改进了之前仅保证较大（常数）近似比的已知算法。此外，我们的算法是第一个在本地DP模型中仅需一轮通信即可实现k均值常数因子近似的算法，正面解决了Stemmer（SODA 2020）提出的开放性问题。我们的算法框架非常灵活；通过展示它也能在（一轮）洗牌DP模型中产生类似的近乎最优的近似算法，我们证明了这一点。",
        "领域": "差分隐私, 聚类算法, 机器学习",
        "问题": "在差分隐私的本地模型中，实现一轮通信的k均值聚类近似算法，以接近非隐私近似算法的性能。",
        "动机": "解决在保护数据隐私的同时，实现高效且准确的k均值聚类的问题，特别是在仅允许一轮通信的限制下。",
        "方法": "开发了一种灵活的算法框架，能够在本地差分隐私模型中实现接近最优的k均值聚类近似，并且该框架也适用于洗牌差分隐私模型。",
        "关键词": [
            "差分隐私",
            "k均值聚类",
            "一轮通信",
            "近似算法",
            "本地模型"
        ],
        "涉及的技术概念": {
            "差分隐私": "在算法中用于保护数据隐私的技术，确保算法的输出不会泄露个别数据点的敏感信息。",
            "k均值聚类": "一种将数据点划分为k个簇的聚类方法，目标是使每个簇内的数据点尽可能相似，而不同簇的数据点尽可能不同。",
            "一轮通信": "指算法在执行过程中仅需一次数据交换，这对于保护隐私和减少通信开销尤为重要。"
        }
    },
    {
        "order": 624,
        "title": "Logarithmic Regret for Reinforcement Learning with Linear Function Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/10521",
        "abstract": "Reinforcement learning (RL) with linear function approximation has received increasing attention recently.\nHowever, existing work has focused on obtaining $\\sqrt{T}$-type regret bound, where $T$ is the number\nof interactions with the MDP. \nIn this paper, we show that logarithmic regret is attainable under two recently proposed linear MDP assumptions provided that there exists a positive sub-optimality gap for the optimal action-value function. More specifically, under the linear MDP assumption (Jin et al., 2020), the LSVI-UCB algorithm can achieve $\\tilde{O}(d^{3}H^5/\\text{gap}_{\\text{min}}\\cdot \\log(T))$regret; and under the linear mixture MDP assumption (Ayoub et al., 2020), the UCRL-VTR algorithm can achieve $\\tilde{O}(d^{2}H^5/\\text{gap}_{\\text{min}}\\cdot \\log^3(T))$ regret, where $d$ is the dimension of feature mapping, $H$ is the length of episode, $\\text{gap}_{\\text{min}}$ is the minimal sub-optimality gap, and $\\tilde O$ hides all logarithmic terms except $\\log(T)$. To the best of our knowledge, these are the first logarithmic regret bounds for RL with linear function approximation. We also establish gap-dependent lower bounds for the two linear MDP models.",
        "conference": "ICML",
        "中文标题": "线性函数逼近强化学习的对数遗憾",
        "摘要翻译": "近年来，使用线性函数逼近的强化学习（RL）受到了越来越多的关注。然而，现有的工作主要集中在获得√T类型的遗憾界限，其中T是与马尔可夫决策过程（MDP）交互的次数。在本文中，我们表明，在最近提出的两种线性MDP假设下，只要最优动作值函数存在正的次优性差距，就可以实现对数的遗憾。具体来说，在线性MDP假设（Jin等人，2020）下，LSVI-UCB算法可以实现Õ(d³H⁵/gap_min·log(T))的遗憾；而在线性混合MDP假设（Ayoub等人，2020）下，UCRL-VTR算法可以实现Õ(d²H⁵/gap_min·log³(T))的遗憾，其中d是特征映射的维度，H是剧集的长度，gap_min是最小的次优性差距，Õ隐藏了除log(T)之外的所有对数项。据我们所知，这些是线性函数逼近强化学习的第一个对数遗憾界限。我们还为这两种线性MDP模型建立了依赖于差距的下界。",
        "领域": "强化学习、函数逼近、马尔可夫决策过程",
        "问题": "解决线性函数逼近强化学习中的遗憾界限问题，特别是如何实现对数的遗憾界限。",
        "动机": "现有的线性函数逼近强化学习方法主要集中在√T类型的遗憾界限，而本研究旨在探索在特定条件下实现对数的遗憾界限的可能性。",
        "方法": "在两种线性MDP假设下，分别使用LSVI-UCB和UCRL-VTR算法，通过理论分析证明可以实现对数的遗憾界限。",
        "关键词": [
            "线性函数逼近",
            "对数遗憾",
            "强化学习",
            "马尔可夫决策过程",
            "次优性差距"
        ],
        "涉及的技术概念": {
            "线性MDP假设": "假设马尔可夫决策过程的转移概率和奖励函数可以表示为特征映射的线性函数，简化了模型的结构。",
            "次优性差距": "最优动作值函数与其他动作值函数之间的最小差距，用于衡量策略的次优程度。",
            "LSVI-UCB算法": "一种结合了线性函数逼近和上置信界限（UCB）的强化学习算法，用于在线性MDP假设下实现有效的探索和利用。"
        },
        "success": true
    },
    {
        "order": 625,
        "title": "LogME: Practical Assessment of Pre-trained Models for Transfer Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10231",
        "abstract": "  This paper studies task adaptive pre-trained model selection, an underexplored problem of assessing pre-trained models for the target task and select best ones from the model zoo \\emph{without fine-tuning}. A few pilot works addressed the problem in transferring supervised pre-trained models to classification tasks, but they cannot handle emerging unsupervised pre-trained models or regression tasks. In pursuit of a practical assessment method, we propose to estimate the maximum value of label evidence given features extracted by pre-trained models. Unlike the maximum likelihood, the maximum evidence is \\emph{immune to over-fitting}, while its expensive computation can be dramatically reduced by our carefully designed algorithm. The Logarithm of Maximum Evidence (LogME) can be used to assess pre-trained models for transfer learning: a pre-trained model with a high LogME value is likely to have good transfer performance. LogME is \\emph{fast, accurate, and general}, characterizing itself as the first practical method for assessing pre-trained models. Compared with brute-force fine-tuning, LogME brings at most $3000\\times$ speedup in wall-clock time and requires only $1\\%$ memory footprint. It outperforms prior methods by a large margin in their setting and is applicable to new settings. It is general enough for diverse pre-trained models (supervised pre-trained and unsupervised pre-trained), downstream tasks (classification and regression), and modalities (vision and language). Code is available at this repository: \\href{https://github.com/thuml/LogME}{https://github.com/thuml/LogME}.",
        "conference": "ICML",
        "中文标题": "LogME：迁移学习中预训练模型的实用评估方法",
        "摘要翻译": "本文研究了任务自适应的预训练模型选择问题，这是一个尚未充分探索的领域，旨在评估预训练模型对目标任务的适用性，并从模型库中选择最佳模型，而无需进行微调。一些初步工作尝试解决将监督预训练模型迁移到分类任务中的问题，但它们无法处理新兴的无监督预训练模型或回归任务。为了追求一种实用的评估方法，我们提出估计给定预训练模型提取特征的最大标签证据值。与最大似然不同，最大证据对过拟合具有免疫力，而我们精心设计的算法可以显著减少其昂贵的计算成本。最大证据的对数（LogME）可用于评估迁移学习中的预训练模型：具有高LogME值的预训练模型很可能具有良好的迁移性能。LogME快速、准确且通用，是第一个用于评估预训练模型的实用方法。与暴力微调相比，LogME在挂钟时间上最多带来3000倍的加速，并且仅需1%的内存占用。在其设置中，它大幅优于先前的方法，并且适用于新设置。它足够通用，适用于多样化的预训练模型（监督预训练和无监督预训练）、下游任务（分类和回归）以及模态（视觉和语言）。代码可在以下仓库获取：https://github.com/thuml/LogME。",
        "领域": "迁移学习、模型选择、预训练模型评估",
        "问题": "如何在不进行微调的情况下，从模型库中选择最适合目标任务的预训练模型。",
        "动机": "解决现有方法无法处理无监督预训练模型和回归任务的问题，提供一种快速、准确且通用的预训练模型评估方法。",
        "方法": "提出估计预训练模型提取特征的最大标签证据值的方法，通过精心设计的算法减少计算成本，使用LogME评估预训练模型的迁移性能。",
        "关键词": [
            "迁移学习",
            "预训练模型",
            "模型选择",
            "LogME",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "最大证据": "用于评估预训练模型对目标任务的适用性，避免过拟合问题。",
            "LogME": "最大证据的对数，作为评估预训练模型迁移性能的指标，快速、准确且通用。",
            "无监督预训练模型": "新兴的预训练模型类型，本文提出的方法能够处理此类模型。"
        },
        "success": true
    },
    {
        "order": 626,
        "title": "Lossless Compression of Efficient Private Local Randomizers",
        "html": "https://ICML.cc//virtual/2021/poster/9967",
        "abstract": "Locally Differentially Private (LDP) Reports are commonly used for collection of statistics and machine learning in the federated setting. In many cases the best known LDP algorithms require sending prohibitively large messages from the client device to the server (such as when constructing histograms over a large domain or learning a high-dimensional model). Here we demonstrate a general approach that, under standard cryptographic assumptions, compresses every efficient LDP algorithm with negligible loss in privacy and utility guarantees. The practical implication of our result is that in typical applications every message can be compressed to the size of the server's pseudo-random generator seed.\nFrom this general approach we derive  low-communication algorithms for the problems of frequency estimation and high-dimensional mean estimation. Our algorithms are simpler and more accurate than existing low-communication LDP algorithms for these well-studied problems.",
        "conference": "ICML",
        "中文标题": "高效私有本地随机化器的无损压缩",
        "摘要翻译": "本地差分隐私（LDP）报告通常用于联邦设置中的统计收集和机器学习。在许多情况下，已知的最佳LDP算法需要从客户端设备向服务器发送过大到难以处理的消息（例如在大型域上构建直方图或学习高维模型时）。在此，我们展示了一种通用方法，在标准加密假设下，对每个高效的LDP算法进行压缩，隐私和效用保证的损失可以忽略不计。我们结果的实践意义在于，在典型应用中，每条消息都可以压缩到服务器伪随机生成器种子的大小。从这一通用方法出发，我们针对频率估计和高维均值估计问题推导出了低通信算法。对于这些经过充分研究的问题，我们的算法比现有的低通信LDP算法更简单、更准确。",
        "领域": "联邦学习, 差分隐私, 数据压缩",
        "问题": "解决在联邦学习环境中使用本地差分隐私（LDP）算法时，客户端与服务器之间通信量过大的问题。",
        "动机": "为了在保证数据隐私和算法效用的前提下，减少联邦学习中客户端与服务器之间的通信负担。",
        "方法": "提出了一种在标准加密假设下，对高效LDP算法进行无损压缩的通用方法，并基于此方法开发了针对频率估计和高维均值问题的低通信算法。",
        "关键词": [
            "本地差分隐私",
            "联邦学习",
            "数据压缩",
            "频率估计",
            "高维均值估计"
        ],
        "涉及的技术概念": {
            "本地差分隐私（LDP）": "用于在数据收集和处理过程中保护个体隐私的技术，确保算法在分析数据时不会泄露个人信息。",
            "联邦学习": "一种分布式机器学习方法，允许多个设备或服务器协作训练模型，而无需共享原始数据。",
            "伪随机生成器种子": "用于生成伪随机数的初始值，本研究中用于压缩消息大小，减少通信量。"
        },
        "success": true
    },
    {
        "order": 627,
        "title": "Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling",
        "html": "https://ICML.cc//virtual/2021/poster/10573",
        "abstract": "With a better understanding of the loss surfaces for multilayer networks, we can build more robust and accurate training procedures.\nRecently it was discovered that independently trained SGD solutions can be connected along one-dimensional paths of near-constant training loss.\nIn this paper, we in fact demonstrate the existence of mode-connecting simplicial complexes that form multi-dimensional manifolds of low loss, connecting many independently trained models. Building on this discovery, we show how to efficiently construct simplicial complexes for fast ensembling, outperforming independently trained deep ensembles in accuracy, calibration, and robustness to dataset shift. Notably, our approach is easy to apply and only requires a few training epochs to discover a low-loss simplex.",
        "conference": "ICML",
        "中文标题": "用于模式连接体积和快速集成的损失表面单纯形",
        "摘要翻译": "随着对多层网络损失表面更深入的理解，我们可以构建更稳健和准确的训练程序。最近发现，独立训练的SGD解决方案可以沿着训练损失几乎恒定的一维路径连接。在本文中，我们实际上证明了模式连接单纯复形的存在，这些复形形成了连接许多独立训练模型的低损失多维流形。基于这一发现，我们展示了如何高效地构建单纯复形以实现快速集成，在准确性、校准和对数据集偏移的鲁棒性方面优于独立训练的深度集成。值得注意的是，我们的方法易于应用，仅需几个训练周期即可发现低损失单纯形。",
        "领域": "深度学习优化、模型集成、神经网络训练",
        "问题": "如何高效地连接独立训练的模型以形成低损失的多维流形，并实现快速集成",
        "动机": "探索和理解多层网络损失表面的结构，以构建更稳健和准确的训练程序",
        "方法": "通过构建模式连接的单纯复形，形成低损失的多维流形，实现模型的快速集成",
        "关键词": [
            "损失表面",
            "单纯复形",
            "快速集成",
            "模式连接",
            "SGD解决方案"
        ],
        "涉及的技术概念": {
            "损失表面": "描述神经网络训练过程中损失函数随参数变化的几何形状，本文中用于理解模型的训练动态",
            "单纯复形": "本文中用于构建连接多个独立训练模型的多维低损失流形的数学结构",
            "快速集成": "通过构建单纯复形实现的高效模型集成方法，旨在提高模型的准确性、校准和鲁棒性"
        },
        "success": true
    },
    {
        "order": 628,
        "title": "Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?",
        "html": "https://ICML.cc//virtual/2021/poster/9423",
        "abstract": "In deep model compression, the recent finding 'Lottery Ticket Hypothesis' (LTH) pointed out that there could exist a  winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance than the original dense network. However, it is not easy to observe such winning property in many scenarios, where for example, a relatively large learning rate is used even if it benefits training the original dense model. In this work, we investigate the underlying condition and rationale behind the winning property, and find that the underlying reason is largely attributed to the correlation between initialized weights and final-trained weights when the learning rate is not sufficiently large. Thus, the existence of winning property is correlated with an insufficient DNN pretraining, and is unlikely to occur for a well-trained DNN. To overcome this limitation, we propose the 'pruning & fine-tuning' method that consistently outperforms lottery ticket sparse training under the same pruning algorithm and the same total training epochs. Extensive experiments over multiple deep models (VGG, ResNet, MobileNet-v2) on different datasets have been conducted to justify our proposals.",
        "conference": "ICML",
        "中文标题": "彩票票保持权重相关性：这是否可取？",
        "摘要翻译": "在深度模型压缩中，最近的发现'彩票票假设'（LTH）指出，可能存在一个获胜票（即一个适当修剪的子网络与原始权重初始化一起），可以达到与原始密集网络相媲美的性能。然而，在许多场景中观察到这种获胜属性并不容易，例如，即使较大的学习率有利于训练原始密集模型。在这项工作中，我们调查了获胜属性背后的潜在条件和原理，并发现当学习率不够大时，主要原因很大程度上归因于初始化权重与最终训练权重之间的相关性。因此，获胜属性的存在与DNN预训练不足相关，并且对于一个训练良好的DNN来说不太可能发生。为了克服这一限制，我们提出了'修剪与微调'方法，在相同的修剪算法和相同的总训练周期下，该方法始终优于彩票票稀疏训练。我们已经在不同数据集上的多个深度模型（VGG、ResNet、MobileNet-v2）上进行了大量实验，以证明我们的提议。",
        "领域": "模型压缩、深度学习优化、神经网络修剪",
        "问题": "探讨在深度模型压缩中，彩票票假设（LTH）的获胜属性存在的条件及其局限性。",
        "动机": "研究彩票票假设中获胜属性出现的条件，以及如何克服其在训练良好DNN中不易出现的问题。",
        "方法": "提出了一种'修剪与微调'方法，以在相同的修剪算法和训练周期下，超越彩票票稀疏训练的性能。",
        "关键词": [
            "彩票票假设",
            "模型压缩",
            "神经网络修剪",
            "微调",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "彩票票假设": "指在深度模型压缩中，存在一个适当修剪的子网络与原始权重初始化一起，可以达到与原始密集网络相媲美的性能。",
            "修剪与微调": "一种模型压缩方法，先对网络进行修剪，然后对修剪后的网络进行微调，以提高性能。",
            "权重相关性": "指初始化权重与最终训练权重之间的相关性，是影响彩票票假设中获胜属性出现的关键因素。"
        },
        "success": true
    },
    {
        "order": 629,
        "title": "Lower-Bounded Proper Losses for Weakly Supervised Classification",
        "html": "https://ICML.cc//virtual/2021/poster/8643",
        "abstract": "This paper discusses the problem of weakly supervised classification, in which instances are given weak labels that are produced by some label-corruption process. The goal is to derive conditions under which loss functions for weak-label learning are proper and lower-bounded---two essential requirements for the losses used in class-probability estimation. To this end, we derive a representation theorem for proper losses in supervised learning, which dualizes the Savage representation. We use this theorem to characterize proper weak-label losses and find a condition for them to be lower-bounded. From these theoretical findings, we derive a novel regularization scheme called generalized logit squeezing, which makes any proper weak-label loss bounded from below, without losing properness. Furthermore, we experimentally demonstrate the effectiveness of our proposed approach, as compared to improper or unbounded losses. The results highlight the importance of properness and lower-boundedness.",
        "conference": "ICML",
        "中文标题": "弱监督分类中的下界适当损失",
        "摘要翻译": "本文讨论了弱监督分类问题，其中实例被赋予由某种标签污染过程产生的弱标签。目标是推导出弱标签学习中损失函数适当且下界的条件——这是用于类概率估计的损失函数的两个基本要求。为此，我们推导了监督学习中适当损失的表示定理，该定理对Savage表示进行了对偶化。我们使用这个定理来表征适当的弱标签损失，并找到一个使它们下界的条件。基于这些理论发现，我们提出了一种称为广义对数挤压的新型正则化方案，该方案使得任何适当的弱标签损失在不失去适当性的情况下从下方有界。此外，我们通过实验证明了我们提出的方法与不适当或无界损失相比的有效性。结果突出了适当性和下界性的重要性。",
        "领域": "弱监督学习",
        "问题": "弱监督分类中损失函数的适当性和下界性条件",
        "动机": "为了确保弱监督学习中的损失函数既适当又有下界，从而更有效地用于类概率估计",
        "方法": "推导监督学习中适当损失的表示定理，提出广义对数挤压正则化方案",
        "关键词": [
            "弱监督分类",
            "适当损失",
            "下界性",
            "广义对数挤压",
            "类概率估计"
        ],
        "涉及的技术概念": {
            "适当损失": "在弱监督学习中，确保损失函数能够正确反映模型预测与真实标签之间差异的技术",
            "下界性": "损失函数的一个重要性质，确保在优化过程中损失值不会无限减小，避免模型训练不稳定",
            "广义对数挤压": "一种新型的正则化方案，旨在使弱标签损失函数在不失去适当性的情况下从下方有界"
        },
        "success": true
    },
    {
        "order": 630,
        "title": "Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries",
        "html": "https://ICML.cc//virtual/2021/poster/9501",
        "abstract": "Understanding the fundamental limits of robust supervised learning has emerged as a problem of immense interest, from both practical and theoretical standpoints. In particular, it is critical to determine classifier-agnostic bounds on the training loss to establish when learning is possible. In this paper, we determine optimal lower bounds on the cross-entropy loss in the presence of test-time adversaries, along with the corresponding optimal classification outputs. Our formulation of the bound as a solution to an optimization problem is general enough to encompass any loss function depending on soft classifier outputs. We also propose and provide a proof of correctness for a bespoke algorithm to compute this lower bound efficiently, allowing us to determine lower bounds for multiple practical datasets of interest. We use our lower bounds as a diagnostic tool to determine the effectiveness of current robust training methods and find a gap from optimality at larger budgets. Finally, we investigate the possibility of using of optimal classification outputs as soft labels to empirically improve robust training. ",
        "conference": "ICML",
        "中文标题": "测试时对抗存在下交叉熵损失的下界",
        "摘要翻译": "理解鲁棒监督学习的基本限制已成为一个从实践和理论角度都极具兴趣的问题。特别是，确定训练损失上与分类器无关的界限以确定学习何时可能至关重要。在本文中，我们确定了在测试时对抗存在下交叉熵损失的最优下界，以及相应的最优分类输出。我们将界限表述为一个优化问题的解，这一表述足够通用，可以涵盖任何依赖于软分类器输出的损失函数。我们还提出并提供了一个定制算法的正确性证明，以高效计算这一下界，使我们能够确定多个感兴趣的实际数据集的下界。我们使用我们的下界作为诊断工具，以确定当前鲁棒训练方法的有效性，并在较大预算下发现与最优性的差距。最后，我们研究了使用最优分类输出作为软标签以经验性地改进鲁棒训练的可能性。",
        "领域": "对抗性机器学习, 鲁棒学习, 监督学习",
        "问题": "在测试时对抗存在下，确定交叉熵损失的最优下界及相应的最优分类输出。",
        "动机": "从理论和实践角度理解鲁棒监督学习的基本限制，特别是在对抗性环境下学习可能性的界限。",
        "方法": "提出一个通用优化问题框架来确定损失函数的下界，开发一个定制算法高效计算这些下界，并利用这些下界评估和改进当前的鲁棒训练方法。",
        "关键词": [
            "交叉熵损失",
            "对抗性机器学习",
            "鲁棒学习",
            "监督学习",
            "优化问题"
        ],
        "涉及的技术概念": {
            "交叉熵损失": "用于衡量分类模型预测概率分布与真实分布之间差异的损失函数，本文中在对抗性环境下研究其下界。",
            "对抗性机器学习": "研究在存在对抗性攻击时机器学习模型的鲁棒性，本文聚焦于测试时的对抗性攻击。",
            "优化问题": "本文通过将下界问题表述为一个优化问题，提供了一个通用框架来研究不同损失函数在对抗性环境下的表现。"
        },
        "success": true
    },
    {
        "order": 631,
        "title": "Low-Precision Reinforcement Learning: Running Soft Actor-Critic in Half Precision",
        "html": "https://ICML.cc//virtual/2021/poster/10605",
        "abstract": "Low-precision training has become a popular approach to reduce compute requirements, memory footprint, and energy consumption in supervised learning. In contrast, this promising approach has not yet enjoyed similarly widespread adoption within the reinforcement learning (RL) community, partly because RL agents can be notoriously hard to train even in full precision. In this paper we consider continuous control with the state-of-the-art SAC agent and demonstrate that a na\\'ive adaptation of low-precision methods from supervised learning fails. We propose a set of six modifications, all straightforward to implement, that leaves the underlying agent and its hyperparameters unchanged but improves the numerical stability dramatically. The resulting modified SAC agent has lower memory and compute requirements while matching full-precision rewards, demonstrating that low-precision training can substantially accelerate state-of-the-art RL without parameter tuning.",
        "conference": "ICML",
        "中文标题": "低精度强化学习：在半精度下运行软演员-评论家算法",
        "摘要翻译": "低精度训练已成为减少监督学习中计算需求、内存占用和能量消耗的流行方法。相比之下，这一有前景的方法在强化学习（RL）社区中尚未得到类似的广泛采用，部分原因是即使在完全精度下，RL代理也可能非常难以训练。在本文中，我们考虑了使用最先进的SAC代理进行连续控制，并证明了简单地从监督学习中适应低精度方法会失败。我们提出了六项修改，所有这些都易于实现，不改变底层代理及其超参数，但显著提高了数值稳定性。修改后的SAC代理具有更低的内存和计算需求，同时匹配全精度的奖励，表明低精度训练可以在不调整参数的情况下显著加速最先进的RL。",
        "领域": "强化学习、连续控制、算法优化",
        "问题": "如何在低精度条件下有效训练强化学习代理，特别是在连续控制任务中。",
        "动机": "探索低精度训练在强化学习中的应用潜力，以减少计算资源消耗同时保持或提升性能。",
        "方法": "提出了六项易于实现的修改，以提高低精度训练下的数值稳定性，而不改变原有代理和超参数。",
        "关键词": [
            "低精度训练",
            "强化学习",
            "软演员-评论家算法",
            "连续控制",
            "数值稳定性"
        ],
        "涉及的技术概念": {
            "低精度训练": "通过减少数值表示的位数来降低计算和存储需求的技术。",
            "软演员-评论家算法（SAC）": "一种先进的强化学习算法，用于连续控制任务，以其稳定性和高效性著称。",
            "数值稳定性": "在数值计算中，算法对舍入误差和其他数值误差的抵抗能力，对于低精度训练尤为重要。"
        },
        "success": true
    },
    {
        "order": 632,
        "title": "Low-Rank Sinkhorn Factorization",
        "html": "https://ICML.cc//virtual/2021/poster/8545",
        "abstract": "Several recent applications of optimal transport (OT) theory to machine learning have relied on regularization, notably entropy and the Sinkhorn algorithm. Because matrix-vector products are pervasive in the Sinkhorn algorithm, several works have proposed to \\textit{approximate} kernel matrices appearing in its iterations using low-rank factors. Another route lies instead in imposing low-nonnegative rank constraints on the feasible set of couplings considered in OT problems, with no approximations on cost nor kernel matrices. This route was first explored by~\\citet{forrow2018statistical}, who proposed an algorithm tailored for the squared Euclidean ground cost, using a proxy objective that can be solved through the machinery of regularized 2-Wasserstein barycenters. Building on this, we introduce in this work a generic approach that aims at solving, in full generality, the OT problem under low-nonnegative rank constraints with arbitrary costs. Our algorithm relies on an explicit factorization of low-rank couplings as a product of \\textit{sub-coupling} factors linked by a common marginal; similar to an NMF approach, we alternatively updates these factors. We prove the non-asymptotic stationary convergence of this algorithm and illustrate its efficiency on benchmark experiments. ",
        "conference": "ICML",
        "中文标题": "低秩Sinkhorn分解",
        "摘要翻译": "最近，最优传输（OT）理论在机器学习中的几个应用依赖于正则化，特别是熵和Sinkhorn算法。由于矩阵向量乘积在Sinkhorn算法中无处不在，几项工作提出了在其迭代中使用低秩因子来近似核矩阵。另一种方法是在OT问题考虑的可行耦合集上施加低非负秩约束，而不对成本或核矩阵进行近似。这一方法最初由Forrow等人（2018）探索，他们提出了一种针对平方欧几里得地面成本的算法，使用可以通过正则化2-Wasserstein重心机制解决的代理目标。在此基础上，我们在这项工作中引入了一种通用方法，旨在全面解决具有任意成本的OT问题下的低非负秩约束。我们的算法依赖于将低秩耦合显式分解为通过共同边际链接的子耦合因子的乘积；类似于NMF方法，我们交替更新这些因子。我们证明了该算法的非渐近平稳收敛性，并在基准实验中展示了其效率。",
        "领域": "最优传输理论应用、机器学习正则化方法、低秩矩阵分解",
        "问题": "解决在最优传输问题中施加低非负秩约束的通用方法",
        "动机": "探索在不近似成本或核矩阵的情况下，通过低非负秩约束解决OT问题的方法",
        "方法": "提出了一种基于显式低秩耦合分解的通用算法，类似于NMF方法，交替更新子耦合因子",
        "关键词": [
            "最优传输",
            "低秩分解",
            "Sinkhorn算法",
            "非负矩阵分解",
            "Wasserstein重心"
        ],
        "涉及的技术概念": {
            "最优传输理论": "用于描述和解决分布之间传输成本最小化问题的数学框架",
            "Sinkhorn算法": "一种通过熵正则化解决最优传输问题的迭代算法",
            "低秩矩阵分解": "将矩阵分解为低秩因子的乘积，以减少计算复杂度和存储需求"
        },
        "success": true
    },
    {
        "order": 633,
        "title": "LTL2Action: Generalizing LTL Instructions for Multi-Task RL",
        "html": "https://ICML.cc//virtual/2021/poster/9191",
        "abstract": "We address the problem of teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language – linear temporal logic (LTL) – and can specify a diversity of complex, temporally extended behaviours, including conditionals and alternative realizations. Our proposed learning approach exploits the compositional syntax and the semantics of LTL, enabling our RL agent to learn task-conditioned policies that generalize to new instructions, not observed during training. To reduce the overhead of learning LTL semantics, we introduce an environment-agnostic LTL pretraining scheme which improves sample-efficiency in downstream environments. Experiments on discrete and continuous domains target combinatorial task sets of up to $\\sim10^{39}$ unique tasks and demonstrate the strength of our approach in learning to solve (unseen) tasks, given LTL instructions.",
        "conference": "ICML",
        "中文标题": "LTL2Action：泛化LTL指令用于多任务强化学习",
        "摘要翻译": "我们解决了在多任务环境中教导深度强化学习（RL）代理遵循指令的问题。指令以一种众所周知的正式语言——线性时序逻辑（LTL）——表达，可以指定多样化的、时间上扩展的复杂行为，包括条件语句和替代实现。我们提出的学习方法利用了LTL的组合语法和语义，使我们的RL代理能够学习到可以泛化到新指令的任务条件策略，这些指令在训练期间未被观察到。为了减少学习LTL语义的开销，我们引入了一种与环境无关的LTL预训练方案，提高了下游环境中的样本效率。在离散和连续领域的实验中，针对多达约10^39个独特任务的组合任务集，展示了我们的方法在学习解决（未见过的）任务方面的优势，给定LTL指令。",
        "领域": "强化学习、多任务学习、形式化方法",
        "问题": "教导深度强化学习代理在多任务环境中遵循以线性时序逻辑（LTL）表达的复杂指令",
        "动机": "开发一种能够理解和执行复杂、时间上扩展的LTL指令的强化学习代理，以实现在多任务环境中的高效学习和泛化",
        "方法": "利用LTL的组合语法和语义学习任务条件策略，引入环境无关的LTL预训练方案以提高样本效率",
        "关键词": [
            "线性时序逻辑",
            "多任务强化学习",
            "任务泛化",
            "预训练",
            "样本效率"
        ],
        "涉及的技术概念": {
            "线性时序逻辑（LTL）": "用于表达复杂、时间上扩展的指令，使强化学习代理能够理解和执行多样化的任务",
            "任务条件策略": "基于LTL指令学习的策略，能够泛化到训练期间未见的新指令",
            "环境无关的LTL预训练": "一种预训练方案，旨在减少学习LTL语义的开销，提高下游任务中的样本效率"
        },
        "success": true
    },
    {
        "order": 634,
        "title": "Machine Unlearning for Random Forests",
        "html": "https://ICML.cc//virtual/2021/poster/10523",
        "abstract": "Responding to user data deletion requests, removing noisy examples, or deleting corrupted training data are just a few reasons for wanting to delete instances from a machine learning (ML) model. However, efficiently removing this data from an ML model is generally difficult. In this paper, we introduce data removal-enabled (DaRE) forests, a variant of random forests that enables the removal of training data with minimal retraining. Model updates for each DaRE tree in the forest are exact, meaning that removing instances from a DaRE model yields exactly the same model as retraining from scratch on updated data.\n\nDaRE trees use randomness and caching to make data deletion efficient. The upper levels of DaRE trees use random nodes, which choose split attributes and thresholds uniformly at random. These nodes rarely require updates because they only minimally depend on the data. At the lower levels, splits are chosen to greedily optimize a split criterion such as Gini index or mutual information. DaRE trees cache statistics at each node and training data at each leaf, so that only the necessary subtrees are updated as data is removed. For numerical attributes, greedy nodes optimize over a random subset of thresholds, so that they can maintain statistics while approximating the optimal threshold. By adjusting the number of thresholds considered for greedy nodes, and the number of random nodes, DaRE trees can trade off between more accurate predictions and more efficient updates.\n\nIn experiments on 13 real-world datasets and one synthetic dataset, we find DaRE forests delete data orders of magnitude faster than retraining from scratch while sacrificing little to no predictive power.",
        "conference": "ICML",
        "中文标题": "随机森林的机器遗忘",
        "摘要翻译": "响应用户数据删除请求、移除噪声样本或删除损坏的训练数据，仅是希望从机器学习（ML）模型中删除实例的几个原因。然而，从ML模型中高效地移除这些数据通常是困难的。在本文中，我们介绍了数据移除启用（DaRE）森林，这是随机森林的一个变体，能够在最小化重新训练的情况下移除训练数据。森林中每棵DaRE树的模型更新是精确的，意味着从DaRE模型中移除实例会得到与在更新数据上从头开始重新训练完全相同的模型。DaRE树利用随机性和缓存来使数据删除变得高效。DaRE树的上层使用随机节点，这些节点均匀随机地选择分割属性和阈值。这些节点很少需要更新，因为它们仅最小程度地依赖于数据。在较低层，分割被选择以贪婪地优化分割标准，如基尼指数或互信息。DaRE树在每个节点缓存统计信息，并在每个叶子缓存训练数据，以便在数据被移除时仅更新必要的子树。对于数值属性，贪婪节点在阈值的随机子集上进行优化，以便在近似最优阈值的同时维护统计信息。通过调整贪婪节点考虑的阈值数量和随机节点的数量，DaRE树可以在更准确的预测和更高效的更新之间进行权衡。在13个真实世界数据集和一个合成数据集的实验中，我们发现DaRE森林删除数据的速度比从头开始重新训练快几个数量级，同时几乎不牺牲预测能力。",
        "领域": "机器学习模型优化、数据隐私保护、随机森林算法",
        "问题": "如何在随机森林模型中高效地删除特定训练数据而不需要完全重新训练",
        "动机": "为了响应用户数据删除请求、移除噪声或损坏的数据，需要一种方法能够在不显著影响模型性能的情况下，从机器学习模型中高效移除数据",
        "方法": "提出数据移除启用（DaRE）森林，通过随机性和缓存机制优化数据删除过程，上层使用随机节点减少数据依赖性，下层通过贪婪优化和缓存统计信息实现高效更新",
        "关键词": [
            "机器遗忘",
            "随机森林",
            "数据删除",
            "模型更新",
            "缓存机制"
        ],
        "涉及的技术概念": {
            "DaRE森林": "随机森林的变体，设计用于高效删除训练数据，通过精确的模型更新实现",
            "随机节点": "DaRE树的上层节点，随机选择分割属性和阈值，减少对数据的依赖，降低更新频率",
            "贪婪优化": "DaRE树下层节点的分割策略，通过优化如基尼指数或互信息等标准来选择分割，同时利用缓存统计信息实现高效更新"
        },
        "success": true
    },
    {
        "order": 635,
        "title": "Making Paper Reviewing Robust to Bid Manipulation Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/10609",
        "abstract": "Most computer science conferences rely on paper bidding to assign reviewers to papers. Although paper bidding enables high-quality assignments in days of unprecedented submission numbers, it also opens the door for dishonest reviewers to adversarially influence paper reviewing assignments. Anecdotal evidence suggests that some reviewers bid on papers by 'friends' or colluding authors, even though these papers are outside their area of expertise, and recommend them for acceptance without considering the merit of the work. In this paper, we study the efficacy of such bid manipulation attacks and find that, indeed, they can jeopardize the integrity of the review process. We develop a novel approach for paper bidding and assignment that is much more robust against such attacks. We show empirically that our approach provides robustness even when dishonest reviewers collude, have full knowledge of the assignment system's internal workings, and have access to the system's inputs. In addition to being more robust, the quality of our paper review assignments is comparable to that of current, non-robust assignment approaches.",
        "conference": "ICML",
        "中文标题": "使论文评审对投标操纵攻击具有鲁棒性",
        "摘要翻译": "大多数计算机科学会议依赖论文投标来分配评审员给论文。虽然论文投标在提交数量空前的时代能够实现高质量的分配，但它也为不诚实的评审员提供了机会，以对抗性地影响论文评审分配。轶事证据表明，一些评审员会投标'朋友'或共谋作者的论文，即使这些论文超出了他们的专业领域，并且不考虑工作的价值就推荐接受。在本文中，我们研究了这种投标操纵攻击的效果，并发现它们确实可能危及评审过程的完整性。我们开发了一种新颖的论文投标和分配方法，对这种攻击具有更强的鲁棒性。我们通过实证表明，即使不诚实的评审员共谋，完全了解分配系统的内部工作原理，并且能够访问系统的输入，我们的方法也能提供鲁棒性。除了更加鲁棒外，我们的论文评审分配质量与当前非鲁棒的分配方法相当。",
        "领域": "学术评审系统安全、计算机科学会议管理、论文评审自动化",
        "问题": "如何防止不诚实的评审员通过投标操纵影响论文评审的公正性",
        "动机": "保护学术评审过程的公正性和完整性，防止不诚实行为对评审结果的影响",
        "方法": "开发了一种新的论文投标和分配方法，该方法对投标操纵攻击具有更强的鲁棒性，并通过实证研究验证了其有效性",
        "关键词": [
            "论文评审",
            "投标操纵",
            "鲁棒性",
            "学术诚信",
            "评审分配"
        ],
        "涉及的技术概念": {
            "投标操纵攻击": "不诚实的评审员通过策略性地投标来影响论文评审分配的行为",
            "鲁棒性": "系统在面对攻击或异常输入时保持其功能和性能的能力",
            "评审分配系统": "用于将评审员分配给论文的自动化或半自动化系统，旨在优化评审质量和效率"
        },
        "success": true
    },
    {
        "order": 636,
        "title": "Making transport more robust and interpretable by moving data through a small number of anchor points",
        "html": "https://ICML.cc//virtual/2021/poster/10355",
        "abstract": "Optimal transport (OT) is a widely used technique for distribution alignment, with applications throughout the machine learning, graphics, and vision communities. Without any additional structural assumptions on transport, however, OT can be fragile to outliers or noise, especially in high dimensions.\nHere, we introduce Latent Optimal Transport (LOT), a new approach for OT that simultaneously learns low-dimensional structure in data while leveraging this structure to solve the alignment task. The idea behind our approach is to learn two sets of ``anchors'' that constrain the flow of transport between a source and target distribution.\nIn both theoretical and empirical studies, we show that LOT regularizes the rank of transport and makes it more robust to outliers and the sampling density. We show that by allowing the source and target to have different anchors, and using LOT to align the latent spaces between anchors, the resulting transport plan has better structural interpretability and highlights connections between both the individual data points and the local geometry of the datasets.",
        "conference": "ICML",
        "中文标题": "通过将数据通过少量锚点移动使传输更加稳健和可解释",
        "摘要翻译": "最优传输（OT）是一种广泛用于分布对齐的技术，在机器学习、图形学和视觉社区中有广泛应用。然而，如果在传输过程中没有任何额外的结构假设，OT在高维情况下对异常值或噪声特别敏感。本文介绍了潜在最优传输（LOT），这是一种新的OT方法，它同时学习数据的低维结构，并利用这种结构来解决对齐任务。我们的方法背后的思想是学习两组“锚点”，以约束源分布和目标分布之间的传输流。在理论和实证研究中，我们表明LOT规范了传输的秩，并使其对异常值和采样密度更加稳健。我们展示了通过允许源和目标有不同的锚点，并使用LOT来对齐锚点之间的潜在空间，得到的传输计划具有更好的结构可解释性，并突出了单个数据点之间以及数据集局部几何之间的联系。",
        "领域": "最优传输应用、机器学习、计算机视觉",
        "问题": "最优传输在高维数据中对异常值和噪声敏感，缺乏结构可解释性",
        "动机": "提高最优传输方法在异常值和噪声存在下的稳健性，同时增强其结构可解释性",
        "方法": "引入潜在最优传输（LOT），通过学习两组锚点约束传输流，同时学习数据的低维结构",
        "关键词": [
            "潜在最优传输",
            "分布对齐",
            "稳健性",
            "可解释性",
            "锚点"
        ],
        "涉及的技术概念": {
            "潜在最优传输（LOT）": "一种新的最优传输方法，通过学习数据的低维结构和锚点来提高传输的稳健性和可解释性",
            "锚点": "用于约束源分布和目标分布之间传输流的点，有助于提高传输的结构可解释性",
            "分布对齐": "最优传输的核心应用之一，旨在通过传输计划将源分布与目标分布对齐"
        },
        "success": true
    },
    {
        "order": 637,
        "title": "Mandoline: Model Evaluation under Distribution Shift",
        "html": "https://ICML.cc//virtual/2021/poster/9313",
        "abstract": "Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple 'slicing functions' – noisy, potentially correlated binary functions intended to capture possible axes of distribution shift – to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines.",
        "conference": "ICML",
        "中文标题": "Mandoline：分布偏移下的模型评估",
        "摘要翻译": "机器学习模型经常在不同于其训练和验证的环境中被部署，这给希望预测部署模型在目标分布上表现如何的实践者带来了挑战。如果可以获得来自目标分布的无标签样本，以及来自可能不同的源分布的有标签样本，可以应用重要性加权等标准方法来估计目标分布上的性能。然而，当源分布和目标分布的支持集不重叠或维度较高时，重要性加权会遇到困难。受到流行病学和民意调查等领域的启发，我们开发了Mandoline，一个新的评估框架，以缓解这些问题。我们的关键见解是，实践者可能对分布偏移的方式有先验知识，我们可以利用这些知识更好地指导重要性加权过程。具体来说，用户编写简单的‘切片函数’——旨在捕捉可能的分布偏移轴的噪声、可能相关的二元函数——来计算重新加权的性能估计。我们进一步描述了切片的密度比估计框架，并展示了其估计误差如何随切片质量和数据集大小而变化。在NLP和视觉任务上的实证验证表明，与标准基线相比，Mandoline可以更准确地估计目标分布上的性能，准确度提高了3倍。",
        "领域": "模型评估、分布偏移处理、机器学习应用",
        "问题": "在分布偏移的情况下准确评估机器学习模型的性能",
        "动机": "解决在源分布和目标分布支持集不重叠或高维时，传统重要性加权方法难以准确评估模型性能的问题",
        "方法": "开发Mandoline评估框架，利用用户提供的切片函数指导重要性加权过程，并通过密度比估计框架优化性能估计",
        "关键词": [
            "分布偏移",
            "模型评估",
            "重要性加权",
            "密度比估计",
            "切片函数"
        ],
        "涉及的技术概念": {
            "重要性加权": "用于在分布偏移情况下估计模型性能的标准方法，但在支持集不重叠或高维时效果有限",
            "切片函数": "用户定义的二元函数，用于捕捉可能的分布偏移轴，指导重要性加权过程",
            "密度比估计": "用于估计切片函数的密度比，优化性能估计的准确性"
        },
        "success": true
    },
    {
        "order": 638,
        "title": "Marginal Contribution Feature Importance - an Axiomatic Approach for Explaining Data",
        "html": "https://ICML.cc//virtual/2021/poster/10695",
        "abstract": "In recent years, methods were proposed for assigning feature importance scores to measure the contribution of individual features. While in some cases the goal is to understand a specific model, in many cases the goal is to understand the contribution of certain properties (features) to a real-world phenomenon. Thus, a distinction has been made between feature importance scores that explain a model and scores that explain the data. When explaining the data, machine learning models are used as proxies in settings where conducting many real-world experiments is expensive or prohibited. While existing feature importance scores show great success in explaining models, we demonstrate their limitations when explaining the data, especially in the presence of correlations between features. Therefore, we develop a set of axioms to capture properties expected from a feature importance score when explaining data and prove that there exists only one score that satisfies all of them, the Marginal Contribution Feature Importance (MCI). We analyze the theoretical properties of this score function and demonstrate its merits empirically.",
        "conference": "ICML",
        "中文标题": "边际贡献特征重要性——一种解释数据的公理化方法",
        "摘要翻译": "近年来，提出了多种方法来分配特征重要性分数，以衡量单个特征的贡献。在某些情况下，目标是理解特定模型，而在许多情况下，目标是理解某些属性（特征）对现实世界现象的贡献。因此，已经区分了解释模型的特征重要性分数和解释数据的分数。在解释数据时，机器学习模型被用作代理，在这种情况下，进行许多现实世界的实验是昂贵或禁止的。虽然现有的特征重要性分数在解释模型方面显示出巨大的成功，但我们展示了它们在解释数据时的局限性，特别是在特征之间存在相关性的情况下。因此，我们开发了一组公理来捕捉解释数据时特征重要性分数预期的属性，并证明只有一种分数满足所有这些属性，即边际贡献特征重要性（MCI）。我们分析了这一分数函数的理论性质，并通过实证展示了其优点。",
        "领域": "特征重要性分析、机器学习解释性、数据理解",
        "问题": "现有特征重要性分数在解释数据时的局限性，特别是在特征间存在相关性的情况下。",
        "动机": "开发一种能够准确解释数据中特征贡献的方法，尤其是在特征间存在相关性时。",
        "方法": "提出一组公理来定义解释数据时特征重要性分数应满足的属性，并证明边际贡献特征重要性（MCI）是唯一满足所有这些属性的分数。",
        "关键词": [
            "特征重要性",
            "数据解释",
            "公理化方法",
            "边际贡献",
            "机器学习"
        ],
        "涉及的技术概念": {
            "边际贡献特征重要性（MCI）": "一种满足特定公理的特征重要性分数，用于准确解释数据中特征的贡献。",
            "公理化方法": "通过定义一组公理来形式化特征重要性分数应满足的属性，确保方法的理论基础。",
            "特征相关性": "特征之间的统计关系，影响特征重要性分数的准确性和解释性。"
        },
        "success": true
    },
    {
        "order": 639,
        "title": "Marginalized Stochastic Natural Gradients for Black-Box Variational Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9853",
        "abstract": "Black-box variational inference algorithms use stochastic sampling to analyze diverse statistical models, like those expressed in probabilistic programming languages, without model-specific derivations. While the popular score-function estimator computes unbiased gradient estimates, its variance is often unacceptably large, especially in models with discrete latent variables. We propose a stochastic natural gradient estimator that is as broadly applicable and unbiased, but improves efficiency by exploiting the curvature of the variational bound, and provably reduces variance by marginalizing discrete latent variables. Our marginalized stochastic natural gradients have intriguing connections to classic coordinate ascent variational inference, but allow parallel updates of variational parameters, and provide superior convergence guarantees relative to naive Monte Carlo approximations. We integrate our method with the probabilistic programming language Pyro and evaluate real-world models of documents, images, networks, and crowd-sourcing. Compared to score-function estimators, we require far fewer Monte Carlo samples and consistently convergence orders of magnitude faster.",
        "conference": "ICML",
        "中文标题": "边缘化随机自然梯度用于黑盒变分推断",
        "摘要翻译": "黑盒变分推断算法使用随机采样来分析多样化的统计模型，如那些用概率编程语言表达的模型，而无需模型特定的推导。虽然流行的得分函数估计器计算无偏梯度估计，但其方差通常大得不可接受，尤其是在具有离散潜在变量的模型中。我们提出了一种随机自然梯度估计器，它同样广泛适用且无偏，但通过利用变分界的曲率提高了效率，并通过边缘化离散潜在变量可证明地减少了方差。我们的边缘化随机自然梯度与经典的坐标上升变分推断有着有趣的连接，但允许变分参数的并行更新，并提供了相对于朴素蒙特卡洛近似更优的收敛保证。我们将我们的方法与概率编程语言Pyro集成，并评估了文档、图像、网络和众包的真实世界模型。与得分函数估计器相比，我们需要的蒙特卡洛样本数量少得多，并且收敛速度一致快几个数量级。",
        "领域": "概率编程与变分推断、自然梯度优化、离散潜在变量模型",
        "问题": "解决在黑盒变分推断中，得分函数估计器方差过大，尤其是在处理离散潜在变量模型时效率低下的问题。",
        "动机": "提高黑盒变分推断在处理离散潜在变量模型时的效率和收敛速度，减少所需的蒙特卡洛样本数量。",
        "方法": "提出了一种边缘化随机自然梯度估计器，通过利用变分界的曲率和边缘化离散潜在变量来减少方差，提高效率。",
        "关键词": [
            "黑盒变分推断",
            "随机自然梯度",
            "离散潜在变量",
            "概率编程",
            "方差减少"
        ],
        "涉及的技术概念": {
            "随机自然梯度估计器": "一种广泛适用且无偏的梯度估计方法，通过利用变分界的曲率提高效率。",
            "边缘化离散潜在变量": "通过边缘化处理离散潜在变量，可证明地减少梯度估计的方差。",
            "变分界的曲率": "在变分推断中，利用目标函数的曲率信息来优化梯度估计，提高算法的收敛速度和稳定性。"
        },
        "success": true
    },
    {
        "order": 640,
        "title": "MARINA: Faster Non-Convex Distributed Learning with Compression",
        "html": "https://ICML.cc//virtual/2021/poster/9657",
        "abstract": "We develop and analyze MARINA: a new communication efficient method for non-convex distributed learning over heterogeneous datasets. MARINA employs a novel communication compression strategy based on the compression of gradient differences that is reminiscent of but different from the strategy employed in the DIANA method of Mishchenko et al. (2019). Unlike virtually all competing distributed first-order methods, including DIANA, ours is based on a carefully designed biased gradient estimator, which is the key to its superior theoretical and practical performance. The communication complexity bounds we prove for MARINA are evidently better than those of all previous first-order methods. Further, we develop and analyze two variants of MARINA: VR-MARINA and PP-MARINA. The first method is designed for the case when the local loss functions owned by clients are either of a finite sum or of an expectation form, and the second method allows for a partial participation of clients – a feature important in federated learning. All our methods are superior to previous state-of-the-art methods in terms of oracle/communication complexity. Finally, we provide a convergence analysis of all methods for problems satisfying the Polyak-Łojasiewicz condition.",
        "conference": "ICML",
        "success": true,
        "中文标题": "MARINA：更快的非凸分布式学习与压缩",
        "摘要翻译": "我们开发并分析了MARINA：一种针对异构数据集上非凸分布式学习的新型通信高效方法。MARINA采用了一种基于梯度差异压缩的新型通信压缩策略，这种策略让人联想到但不完全相同于Mishchenko等人（2019年）在DIANA方法中采用的策略。与几乎所有竞争性的分布式一阶方法（包括DIANA）不同，我们的方法基于精心设计的偏置梯度估计器，这是其优越理论和实际性能的关键。我们为MARINA证明的通信复杂性界限明显优于所有先前的一阶方法。此外，我们开发并分析了MARINA的两种变体：VR-MARINA和PP-MARINA。第一种方法设计用于当客户拥有的局部损失函数是有限和或期望形式的情况，第二种方法允许客户部分参与——这是在联邦学习中重要的一个特性。我们所有的方法在预言机/通信复杂性方面都优于先前的最先进方法。最后，我们为满足Polyak-Łojasiewicz条件的问题提供了所有方法的收敛分析。",
        "领域": "分布式学习, 非凸优化, 联邦学习",
        "问题": "解决异构数据集上非凸分布式学习中的通信效率问题",
        "动机": "开发一种通信效率更高的分布式学习方法，以在理论和实践中实现更优的性能",
        "方法": "基于梯度差异压缩的新型通信压缩策略和精心设计的偏置梯度估计器",
        "关键词": [
            "分布式学习",
            "非凸优化",
            "通信压缩",
            "梯度估计",
            "联邦学习"
        ],
        "涉及的技术概念": {
            "梯度差异压缩": "MARINA采用的新型通信压缩策略，通过压缩梯度差异来减少通信量",
            "偏置梯度估计器": "MARINA方法的核心，通过精心设计的偏置梯度估计器实现更优的理论和实际性能",
            "Polyak-Łojasiewicz条件": "用于分析MARINA及其变体收敛性的数学条件，确保在满足该条件的问题上的收敛性"
        }
    },
    {
        "order": 641,
        "title": "Markpainting: Adversarial Machine Learning meets Inpainting",
        "html": "https://ICML.cc//virtual/2021/poster/9367",
        "abstract": "Inpainting is a learned interpolation technique that is based on generative modeling and used to populate masked or missing pieces in an image; it has wide applications in picture editing and retouching. Recently, inpainting started being used for watermark removal, raising concerns. In this paper we study how to manipulate it using our markpainting technique. First, we show how an image owner with access to an inpainting model can augment their image in such a way that any attempt to edit it using that model will add arbitrary visible information. We find that we can target multiple different models simultaneously with our technique. This can be designed to reconstitute a watermark if the editor had been trying to remove it. \nSecond, we show that our markpainting technique is transferable to models that have different architectures or were trained on different datasets, so watermarks created using it are difficult for adversaries to remove.\nMarkpainting is novel and can be used as a manipulation alarm that becomes visible in the event of inpainting. Source code is available at: https://github.com/iliaishacked/markpainting.",
        "conference": "ICML",
        "中文标题": "标记绘画：对抗性机器学习与图像修复的相遇",
        "摘要翻译": "图像修复是一种基于生成建模的学习插值技术，用于填充图像中被遮挡或缺失的部分；它在图片编辑和修饰中有广泛的应用。最近，图像修复开始被用于水印去除，引发了担忧。在本文中，我们研究了如何使用我们的标记绘画技术来操控这一过程。首先，我们展示了拥有图像修复模型访问权限的图像所有者如何增强他们的图像，使得任何尝试使用该模型编辑图像的行为都会添加任意的可见信息。我们发现，我们的技术可以同时针对多个不同的模型。这可以被设计成如果编辑器试图去除水印，就会重新构成水印。其次，我们展示了我们的标记绘画技术可以迁移到具有不同架构或在不同数据集上训练的模型，因此使用它创建的水印对手难以去除。标记绘画是新颖的，可以用作在图像修复事件中变得可见的操作警报。源代码可在：https://github.com/iliaishacked/markpainting 获取。",
        "领域": "图像修复、对抗性机器学习、数字水印",
        "问题": "如何防止图像修复技术被滥用于水印去除",
        "动机": "研究图像修复技术被用于水印去除的问题，并提出一种防止滥用的方法",
        "方法": "开发了一种名为标记绘画的技术，通过在图像中嵌入特定的标记，使得任何尝试使用图像修复模型编辑图像的行为都会导致这些标记变得可见",
        "关键词": [
            "图像修复",
            "对抗性机器学习",
            "数字水印",
            "模型迁移",
            "操作警报"
        ],
        "涉及的技术概念": {
            "标记绘画": "一种通过在图像中嵌入特定标记来防止图像修复技术滥用的技术",
            "对抗性机器学习": "利用机器学习模型的弱点来操控其行为的技术",
            "模型迁移": "将在一个模型上学到的知识应用到另一个不同架构或训练数据的模型上的能力"
        },
        "success": true
    },
    {
        "order": 642,
        "title": "Massively Parallel and Asynchronous Tsetlin Machine Architecture Supporting Almost Constant-Time Scaling",
        "html": "https://ICML.cc//virtual/2021/poster/10157",
        "abstract": "Using logical clauses to represent patterns, Tsetlin Machine (TM) have recently obtained competitive performance in terms of accuracy, memory footprint, energy, and learning speed on several benchmarks. Each TM clause votes for or against a particular class, with classification resolved using a majority vote. While the evaluation of clauses is fast, being based on binary operators, the voting makes it necessary to synchronize the clause evaluation, impeding parallelization. In this paper, we propose a novel scheme for desynchronizing the evaluation of clauses, eliminating the voting bottleneck. In brief, every clause runs in its own thread for massive native parallelism. For each training example, we keep track of the class votes obtained from the clauses in local voting tallies. The local voting tallies allow us to detach the processing of each clause from the rest of the clauses, supporting decentralized learning. This means that the TM most of the time will operate on outdated voting tallies. We evaluated the proposed parallelization across diverse learning tasks and it turns out that our decentralized TM learning algorithm copes well with working on outdated data, resulting in no significant loss in learning accuracy. Furthermore, we show that the approach provides up to 50 times faster learning. Finally, learning time is almost constant for reasonable clause amounts (employing from 20 to 7,000 clauses on a Tesla V100 GPU). For sufficiently large clause numbers, computation time increases approximately proportionally. Our parallel and asynchronous architecture thus allows processing of more massive datasets and operating with more clauses for higher accuracy.",
        "conference": "ICML",
        "中文标题": "支持几乎恒定时间扩展的大规模并行异步Tsetlin机器架构",
        "摘要翻译": "使用逻辑子句来表示模式，Tsetlin Machine (TM)最近在多个基准测试中在准确性、内存占用、能量和学习速度方面获得了竞争性表现。每个TM子句对特定类别投赞成或反对票，分类通过多数票决定。虽然基于二进制运算符的子句评估速度快，但投票使得必须同步子句评估，阻碍了并行化。在本文中，我们提出了一种新颖的方案，用于异步评估子句，消除了投票瓶颈。简而言之，每个子句在自己的线程中运行，以实现大规模原生并行。对于每个训练示例，我们在本地投票计数中跟踪从子句获得的类别投票。本地投票计数使我们能够将每个子句的处理与其他子句分离，支持去中心化学习。这意味着TM大部分时间将操作于过时的投票计数上。我们在多样化的学习任务上评估了提出的并行化方案，结果表明我们的去中心化TM学习算法能够很好地处理过时数据，学习准确性没有显著损失。此外，我们展示了该方法提供了高达50倍的学习加速。最后，对于合理的子句数量（在Tesla V100 GPU上使用20到7,000个子句），学习时间几乎是恒定的。对于足够大的子句数量，计算时间大约成比例增加。因此，我们的并行和异步架构允许处理更大规模的数据集，并使用更多子句以获得更高的准确性。",
        "领域": "机器学习并行计算、逻辑学习系统、去中心化学习算法",
        "问题": "解决Tsetlin Machine在并行化处理中的投票同步瓶颈问题",
        "动机": "提高Tsetlin Machine的学习速度和可扩展性，以处理更大规模的数据集",
        "方法": "提出一种异步评估子句的方案，每个子句在独立线程中运行，使用本地投票计数实现去中心化学习",
        "关键词": [
            "Tsetlin Machine",
            "并行计算",
            "去中心化学习",
            "异步处理",
            "逻辑子句"
        ],
        "涉及的技术概念": {
            "Tsetlin Machine": "一种使用逻辑子句进行模式表示和分类的机器学习模型",
            "本地投票计数": "用于跟踪每个训练示例的类别投票，支持子句的异步处理和去中心化学习",
            "去中心化学习算法": "允许每个子句独立处理，减少同步需求，提高并行效率和扩展性"
        },
        "success": true
    },
    {
        "order": 643,
        "title": "Matrix Completion with Model-free Weighting",
        "html": "https://ICML.cc//virtual/2021/poster/8707",
        "abstract": "In this paper, we propose a novel method for matrix completion under general non-uniform missing structures. By controlling an upper bound of a novel balancing error, we construct weights that can actively adjust for the non-uniformity in the empirical risk without explicitly modeling the observation probabilities, and can be computed efficiently via convex optimization. The recovered matrix based on the proposed weighted empirical risk enjoys appealing theoretical guarantees. In particular, the proposed method achieves stronger guarantee than existing work in terms of the scaling with respect to the observation probabilities, under asymptotically heterogeneous missing settings (where entry-wise observation probabilities can be of different orders). These settings can be regarded as a better theoretical model of missing patterns with highly varying probabilities. We also provide a new minimax lower bound under a class of heterogeneous settings. Numerical experiments are also provided to demonstrate the effectiveness of the proposed method.",
        "conference": "ICML",
        "中文标题": "无模型加权的矩阵补全",
        "摘要翻译": "本文提出了一种在一般非均匀缺失结构下进行矩阵补全的新方法。通过控制一种新型平衡误差的上界，我们构建了能够主动调整经验风险中非均匀性的权重，而无需显式建模观测概率，并且可以通过凸优化高效计算。基于所提出的加权经验风险恢复的矩阵具有吸引人的理论保证。特别是，在渐近异质缺失设置下（其中条目观测概率可以有不同的阶数），所提出的方法在观测概率的缩放方面实现了比现有工作更强的保证。这些设置可以被视为具有高度变化概率的缺失模式的一个更好的理论模型。我们还提供了一类异质设置下的新极小极大下界。数值实验也证明了所提出方法的有效性。",
        "领域": "矩阵补全、非均匀缺失数据处理、凸优化",
        "问题": "解决在非均匀缺失结构下矩阵补全的问题，特别是在观测概率高度变化的情况下。",
        "动机": "为了在观测概率高度变化的缺失模式下，提供一种无需显式建模观测概率的矩阵补全方法，并实现更强的理论保证。",
        "方法": "通过控制新型平衡误差的上界构建权重，调整经验风险中的非均匀性，利用凸优化高效计算权重，恢复矩阵。",
        "关键词": [
            "矩阵补全",
            "非均匀缺失",
            "凸优化",
            "经验风险",
            "极小极大下界"
        ],
        "涉及的技术概念": {
            "平衡误差": "用于衡量和调整经验风险中非均匀性的新型误差度量。",
            "凸优化": "用于高效计算权重，解决矩阵补全问题的优化方法。",
            "极小极大下界": "在异质缺失设置下，提供理论保证的新下界，用于评估方法的性能极限。"
        },
        "success": true
    },
    {
        "order": 644,
        "title": "Matrix Sketching for Secure Collaborative Machine Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10417",
        "abstract": "Collaborative learning allows participants to jointly train a model without data sharing. To update the model parameters, the central server broadcasts model parameters to the clients, and the clients send updating directions such as gradients to the server. While data do not leave a client device, the communicated gradients and parameters will leak a client's privacy. Attacks that infer clients' privacy from gradients and parameters have been developed by prior work. Simple defenses such as dropout and differential privacy either fail to defend the attacks or seriously hurt test accuracy.\nWe propose a practical defense which we call Double-Blind Collaborative Learning (DBCL). The high-level idea is to apply random matrix sketching to the parameters (aka weights) and re-generate random sketching after each iteration. DBCL prevents clients from conducting gradient-based privacy inferences which are the most effective attacks. DBCL works because from the attacker's perspective, sketching is effectively random noise that outweighs the signal. Notably, DBCL does not much increase computation and communication costs and does not hurt test accuracy at all. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "矩阵草图技术在安全协作机器学习中的应用",
        "摘要翻译": "协作学习允许参与者在不共享数据的情况下联合训练模型。为了更新模型参数，中央服务器将模型参数广播给客户端，客户端则向服务器发送更新方向，如梯度。虽然数据不会离开客户端设备，但通信的梯度和参数会泄露客户端的隐私。先前的工作已经开发出从梯度和参数推断客户端隐私的攻击。简单的防御措施，如dropout和差分隐私，要么无法防御这些攻击，要么严重损害测试准确性。我们提出了一种实用的防御方法，称为双盲协作学习（DBCL）。其核心思想是对参数（即权重）应用随机矩阵草图技术，并在每次迭代后重新生成随机草图。DBCL防止了客户端进行基于梯度的隐私推断，这些是最有效的攻击。DBCL之所以有效，是因为从攻击者的角度来看，草图实际上是压倒信号的随机噪声。值得注意的是，DBCL不会显著增加计算和通信成本，也不会损害测试准确性。",
        "领域": "安全机器学习, 隐私保护, 协作学习",
        "问题": "如何在协作学习过程中保护客户端的隐私不被梯度或参数泄露",
        "动机": "现有的简单防御措施无法有效保护隐私或会严重影响模型性能，需要一种既能保护隐私又不损害模型准确性的方法",
        "方法": "应用随机矩阵草图技术并在每次迭代后重新生成随机草图，以防止基于梯度的隐私推断攻击",
        "关键词": [
            "双盲协作学习",
            "矩阵草图",
            "隐私保护",
            "协作学习",
            "梯度攻击防御"
        ],
        "涉及的技术概念": {
            "随机矩阵草图": "用于对模型参数进行随机变换，增加攻击者推断原始数据的难度",
            "梯度攻击防御": "通过技术手段防止攻击者利用梯度信息推断出敏感数据",
            "双盲协作学习": "一种结合随机矩阵草图技术的协作学习方法，旨在保护参与者的数据隐私"
        }
    },
    {
        "order": 645,
        "title": "Maximum Mean Discrepancy Test is Aware of Adversarial Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/10683",
        "abstract": "The maximum mean discrepancy (MMD) test could in principle detect any distributional discrepancy between two datasets. However, it has been shown that the MMD test is unaware of adversarial attacks--the MMD test failed to detect the discrepancy between natural data and adversarial data. Given this phenomenon, we raise a question: are natural and adversarial data really from different distributions? The answer is affirmative--the previous use of the MMD test on the purpose missed three key factors, and accordingly, we propose three components. Firstly, the Gaussian kernel has limited representation power, and we replace it with an effective deep kernel. Secondly, the test power of the MMD test was neglected, and we maximize it following asymptotic statistics. Finally, adversarial data may be non-independent, and we overcome this issue with the help of wild bootstrap. By taking care of the three factors, we verify that the MMD test is aware of adversarial attacks, which lights up a novel road for adversarial data detection based on two-sample tests.",
        "conference": "ICML",
        "中文标题": "最大均值差异测试能够感知对抗性攻击",
        "摘要翻译": "最大均值差异（MMD）测试原则上可以检测两个数据集之间的任何分布差异。然而，已经证明MMD测试无法感知对抗性攻击——MMD测试未能检测到自然数据与对抗性数据之间的差异。鉴于这一现象，我们提出了一个问题：自然数据和对抗性数据是否真的来自不同的分布？答案是肯定的——之前使用MMD测试的目的忽略了三个关键因素，因此，我们提出了三个组成部分。首先，高斯核的表示能力有限，我们用一个有效的深度核来替代它。其次，MMD测试的测试能力被忽视，我们根据渐近统计量最大化它。最后，对抗性数据可能是非独立的，我们借助wild bootstrap克服了这个问题。通过考虑这三个因素，我们验证了MMD测试能够感知对抗性攻击，这为基于双样本测试的对抗性数据检测开辟了一条新路。",
        "领域": "对抗性攻击检测、深度学习安全、统计测试",
        "问题": "MMD测试无法有效检测自然数据与对抗性数据之间的分布差异",
        "动机": "探究MMD测试在对抗性攻击检测中的局限性，并提出改进方法以提高其检测能力",
        "方法": "使用深度核替代高斯核以提高表示能力，根据渐近统计量最大化测试能力，利用wild bootstrap处理非独立的对抗性数据",
        "关键词": [
            "最大均值差异",
            "对抗性攻击",
            "深度核",
            "wild bootstrap",
            "双样本测试"
        ],
        "涉及的技术概念": {
            "最大均值差异（MMD）": "用于衡量两个数据集之间分布差异的统计测试方法",
            "深度核": "替代传统高斯核，提供更强的数据表示能力，用于提高MMD测试的检测能力",
            "wild bootstrap": "一种统计方法，用于处理非独立样本，确保MMD测试在对抗性数据上的有效性"
        },
        "success": true
    },
    {
        "order": 646,
        "title": "MC-LSTM: Mass-Conserving LSTM",
        "html": "https://ICML.cc//virtual/2021/poster/10425",
        "abstract": "The success of Convolutional Neural Networks (CNNs) in computer vision is mainly driven by their strong inductive bias, which is strong enough to allow CNNs to solve vision-related tasks with random weights, meaning without learning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias towards storing information over time. However, many real-world systems are governed by conservation laws, which lead to the redistribution of particular quantities — e.g.in physical and economical systems.  Our novel Mass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending the inductive bias of LSTM to model the redistribution of those stored quantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at learning arithmetic operations, such as addition tasks,which have a strong conservation law, as the sum is constant over time. Further, MC-LSTM is applied  to  traffic  forecasting,  modeling  a  pendulum, and a large benchmark dataset in hydrology, where it sets a new state-of-the-art for predicting peak flows. In the hydrology example, we show that MC-LSTM states correlate with real world processes and are therefore interpretable.",
        "conference": "ICML",
        "中文标题": "MC-LSTM: 质量守恒的长短期记忆网络",
        "摘要翻译": "卷积神经网络（CNNs）在计算机视觉中的成功主要归功于其强大的归纳偏置，这种偏置强大到足以让CNNs无需学习即能解决与视觉相关的任务。同样，长短期记忆网络（LSTM）在随时间存储信息方面也具有强大的归纳偏置。然而，许多现实世界的系统受守恒定律的支配，这导致了特定量的重新分配——例如在物理和经济系统中。我们新颖的质量守恒LSTM（MC-LSTM）通过扩展LSTM的归纳偏置来模拟这些存储量的重新分配，从而遵守这些守恒定律。MC-LSTM在学习算术操作（如加法任务）方面为神经算术单元设定了新的最先进水平，这些操作具有强烈的守恒定律，因为总和随时间保持不变。此外，MC-LSTM被应用于交通预测、摆建模和一个大型水文基准数据集，在那里它为预测峰值流量设定了新的最先进水平。在水文例子中，我们展示了MC-LSTM状态与现实世界过程相关，因此是可解释的。",
        "领域": "时间序列预测, 物理系统建模, 水文预测",
        "问题": "如何在遵守守恒定律的前提下，改进长短期记忆网络（LSTM）以更好地模拟现实世界系统中的量重新分配问题。",
        "动机": "现实世界中的许多系统受守恒定律支配，现有的LSTM虽然能有效存储时间序列信息，但缺乏对这些定律的考虑，限制了其在模拟这些系统时的准确性和可解释性。",
        "方法": "通过扩展LSTM的归纳偏置，提出质量守恒LSTM（MC-LSTM），使其能够模拟受守恒定律支配的量的重新分配。",
        "关键词": [
            "质量守恒",
            "LSTM",
            "时间序列预测",
            "物理系统建模",
            "水文预测"
        ],
        "涉及的技术概念": {
            "归纳偏置": "指模型在学习之前对数据分布或任务特性的假设，MC-LSTM通过扩展LSTM的归纳偏置来适应守恒定律。",
            "守恒定律": "指在封闭系统中某些物理量（如质量、能量）的总量保持不变的定律，MC-LSTM设计用于遵守这些定律。",
            "神经算术单元": "指能够执行算术运算的神经网络组件，MC-LSTM在学习算术操作方面表现出色，特别是在加法任务中。"
        },
        "success": true
    },
    {
        "order": 647,
        "title": "Measuring Robustness in Deep Learning Based Compressive Sensing",
        "html": "https://ICML.cc//virtual/2021/poster/9951",
        "abstract": "Deep neural networks give state-of-the-art accuracy for reconstructing images from few and noisy measurements, a problem arising for example in accelerated magnetic resonance imaging (MRI). However, recent works have raised concerns that deep-learning-based image reconstruction methods are sensitive to perturbations and are less robust than traditional methods: Neural networks (i) may be sensitive to small, yet adversarially-selected perturbations, (ii) may perform poorly under distribution shifts, and (iii) may fail to recover small but important features in an image. In order to understand the sensitivity to such perturbations, in this work, we measure the robustness of different approaches for image reconstruction including trained and un-trained neural networks as well as traditional sparsity-based methods. We find, contrary to prior works, that both trained and un-trained methods are vulnerable to adversarial perturbations. Moreover, both trained and un-trained methods tuned for a particular dataset suffer very similarly from distribution shifts. Finally, we demonstrate that an image reconstruction method that achieves higher reconstruction quality, also performs better in terms of accurately recovering fine details. Our results indicate that the state-of-the-art deep-learning-based image reconstruction methods provide improved performance than traditional methods without compromising robustness.",
        "conference": "ICML",
        "中文标题": "测量基于深度学习的压缩感知的鲁棒性",
        "摘要翻译": "深度神经网络在从少量且有噪声的测量中重建图像方面提供了最先进的准确性，这一问题例如在加速磁共振成像（MRI）中出现。然而，最近的工作提出了担忧，即基于深度学习的图像重建方法对扰动敏感，且不如传统方法鲁棒：神经网络（i）可能对小的、但对抗性选择的扰动敏感，（ii）在分布变化下可能表现不佳，（iii）可能无法恢复图像中虽小但重要的特征。为了理解对此类扰动的敏感性，在这项工作中，我们测量了包括训练和未训练的神经网络以及传统的基于稀疏性的方法在内的不同图像重建方法的鲁棒性。我们发现，与先前的工作相反，训练和未训练的方法都对对抗性扰动脆弱。此外，针对特定数据集调整的训练和未训练方法在分布变化下表现非常相似。最后，我们证明，实现更高重建质量的图像重建方法在准确恢复精细细节方面也表现更好。我们的结果表明，最先进的基于深度学习的图像重建方法在不损害鲁棒性的情况下提供了比传统方法更好的性能。",
        "领域": "图像重建、压缩感知、深度学习鲁棒性",
        "问题": "评估和比较基于深度学习的图像重建方法与传统方法在面对对抗性扰动和分布变化时的鲁棒性。",
        "动机": "解决关于深度学习在图像重建中鲁棒性不足的担忧，验证其在各种扰动下的表现是否真的逊于传统方法。",
        "方法": "通过实验测量和比较训练和未训练的神经网络以及传统稀疏性方法在对抗性扰动和分布变化下的表现，评估重建质量和细节恢复能力。",
        "关键词": [
            "深度学习",
            "图像重建",
            "鲁棒性测量",
            "对抗性扰动",
            "分布变化"
        ],
        "涉及的技术概念": {
            "对抗性扰动": "指那些经过精心设计的小扰动，能够显著影响深度学习模型的输出，用于测试模型的鲁棒性。",
            "分布变化": "指模型训练数据与实际应用数据之间的统计特性差异，用于评估模型在未见数据上的泛化能力。",
            "稀疏性方法": "传统图像重建技术之一，基于信号在某个域中的稀疏性假设，用于与深度学习方法进行鲁棒性比较。"
        },
        "success": true
    },
    {
        "order": 648,
        "title": "Mediated Uncoupled Learning: Learning Functions without Direct Input-output Correspondences",
        "html": "https://ICML.cc//virtual/2021/poster/9509",
        "abstract": "Ordinary supervised learning is useful when we have paired training data of input $X$ and output $Y$. However, such paired data can be difficult to collect in practice. In this paper, we consider the task of predicting $Y$ from $X$ when we have no paired data of them, but we have two separate, independent datasets of $X$ and $Y$ each observed with some mediating variable $U$, that is, we have two datasets $S_X = \\{(X_i, U_i)\\}$ and $S_Y = \\{(U'_j, Y'_j)\\}$. A naive approach is to predict $U$ from $X$ using $S_X$ and then $Y$ from $U$ using $S_Y$, but we show that this is not statistically consistent. Moreover, predicting $U$ can be more difficult than predicting $Y$ in practice, e.g., when $U$ has higher dimensionality. To circumvent the difficulty, we propose a new method that avoids predicting $U$ but directly learns $Y = f(X)$ by training $f(X)$ with $S_{X}$ to predict $h(U)$ which is trained with $S_{Y}$ to approximate $Y$. We prove statistical consistency and error bounds of our method and experimentally confirm its practical usefulness.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "中介解耦学习：在没有直接输入-输出对应关系的情况下学习函数",
        "摘要翻译": "普通的监督学习在有输入 X 和输出 Y 的配对训练数据时非常有用。然而，这种配对数据在实践中可能难以收集。在本文中，我们考虑在没有 X 和 Y 的配对数据的情况下，从 X 预测 Y 的任务，但我们有两个独立的 X 和 Y 数据集，每个数据集都用一些中介变量 U 观察到，也就是说，我们有两个数据集 Sx = {(Xi, Ui)} 和 Sy = {(U'j, Y'j)}。一种朴素的方法是使用 Sx 从 X 预测 U，然后使用 Sy 从 U 预测 Y，但我们证明这在统计上是不一致的。此外，在实践中，预测 U 可能比预测 Y 更困难，例如，当 U 具有更高的维度时。为了避免这种困难，我们提出了一种新方法，该方法避免预测 U，而是通过使用 Sx 训练 f(X) 来预测 h(U)，然后使用 Sy 训练 h(U) 来逼近 Y，从而直接学习 Y = f(X)。我们证明了我们方法的统计一致性和误差界限，并通过实验证实了它的实际效用。",
        "领域": "领域自适应、因果推断、半监督学习",
        "问题": "在缺乏配对的输入输出数据的情况下，如何学习输入到输出的映射函数，即解决非配对数据的函数学习问题。",
        "动机": "在许多实际应用中，配对的输入输出数据难以获取，而独立的输入和输出数据相对容易获取。传统的监督学习方法无法直接应用于这种情况，因此需要研究新的方法来利用这些非配对数据进行函数学习。",
        "方法": "提出了一种新的中介解耦学习方法，该方法避免直接预测中介变量 U，而是通过学习两个函数 f(X) 和 h(U) 来间接建立输入 X 和输出 Y 之间的关系。其中，f(X) 用于预测 h(U)，而 h(U) 用于近似 Y。通过这种方式，可以直接学习 Y = f(X)，而无需依赖配对数据。",
        "关键词": [
            "非配对学习",
            "解耦学习",
            "中介变量",
            "函数学习",
            "统计一致性"
        ],
        "涉及的技术概念": {
            "中介变量": "作为输入和输出之间的桥梁，通过与输入和输出的关联，间接建立输入和输出之间的映射关系。在本论文中，避免直接预测中介变量，而是通过学习输入和中介变量之间的关系，以及中介变量和输出之间的关系，来实现输入到输出的映射。",
            "统计一致性": "指的是在样本量趋于无穷大时，学习算法的输出会收敛到真实函数。本论文证明了所提出的中介解耦学习方法具有统计一致性，保证了在数据量足够大的情况下，算法能够学习到准确的映射函数。"
        }
    },
    {
        "order": 649,
        "title": "Megaverse: Simulating Embodied Agents at One Million Experiences per Second",
        "html": "https://ICML.cc//virtual/2021/poster/10247",
        "abstract": "We present Megaverse, a new 3D simulation platform for reinforcement learning and embodied AI research. The efficient design of our engine enables physics-based simulation with high-dimensional egocentric observations at more than 1,000,000 actions per second on a single 8-GPU node. Megaverse is up to 70x faster than DeepMind Lab in fully-shaded 3D scenes with interactive objects. We achieve this high simulation performance by leveraging batched simulation, thereby taking full advantage of the massive parallelism of modern GPUs. We use Megaverse to build a new benchmark that consists of several single-agent and multi-agent tasks covering a variety of cognitive challenges. We evaluate model-free RL on this benchmark to provide baselines and facilitate future research.",
        "conference": "ICML",
        "中文标题": "Megaverse：以每秒百万次体验模拟具身智能体",
        "摘要翻译": "我们介绍了Megaverse，一个为强化学习和具身人工智能研究设计的新型3D模拟平台。我们引擎的高效设计使得在单个8-GPU节点上能够实现基于物理的模拟，并以每秒超过1,000,000次动作的速度进行高维自我中心观察。在完全阴影的3D场景中，Megaverse比DeepMind Lab快达70倍，且包含交互式对象。我们通过利用批量模拟来实现这一高模拟性能，从而充分利用现代GPU的大规模并行性。我们使用Megaverse构建了一个新的基准测试，该测试包含几个单智能体和多智能体任务，涵盖了一系列认知挑战。我们在此基准测试上评估了无模型强化学习，以提供基线并促进未来的研究。",
        "领域": "强化学习、具身人工智能、3D模拟",
        "问题": "如何高效模拟具身智能体在复杂3D环境中的行为",
        "动机": "为了加速强化学习和具身AI的研究，需要一种能够高效模拟复杂3D环境中智能体行为的平台",
        "方法": "利用批量模拟和现代GPU的大规模并行性，设计高效的3D模拟引擎",
        "关键词": [
            "Megaverse",
            "强化学习",
            "具身AI",
            "3D模拟",
            "GPU并行计算"
        ],
        "涉及的技术概念": {
            "批量模拟": "通过同时处理多个模拟实例来提高模拟效率，充分利用GPU的并行计算能力",
            "高维自我中心观察": "模拟中智能体从自身视角获取的高维环境信息，用于决策和学习",
            "无模型强化学习": "不依赖于环境模型，直接从经验中学习的强化学习方法，用于在Megaverse平台上评估智能体性能"
        },
        "success": true
    },
    {
        "order": 650,
        "title": "Memory Efficient Online Meta Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9787",
        "abstract": "We propose a novel algorithm for online meta learning where task instances are sequentially revealed with limited supervision and a learner is expected to meta learn them in each round, so as to allow the learner to customize a task-specific model rapidly with little task-level supervision. A fundamental concern arising in online meta-learning is the scalability of memory as more tasks are viewed over time. Heretofore, prior works have allowed for perfect recall leading to linear increase in memory with time. Different from prior works, in our method, prior task instances are allowed to be deleted. We propose to leverage prior task instances by means of a fixed-size state-vector, which is updated sequentially. Our theoretical analysis demonstrates that our proposed memory efficient online learning (MOML) method suffers sub-linear regret with convex loss functions and sub-linear local regret for nonconvex losses. On benchmark datasets we show that our method can outperform prior works even though they allow for perfect recall. ",
        "conference": "ICML",
        "中文标题": "内存高效的在线元学习",
        "摘要翻译": "我们提出了一种新颖的在线元学习算法，其中任务实例在有限的监督下顺序揭示，学习者在每一轮中预期对这些任务进行元学习，以便学习者能够用很少的任务级监督快速定制特定于任务的模型。在线元学习中出现的一个基本问题是随着时间推移查看更多任务时内存的可扩展性。迄今为止，先前的工作允许完美回忆，导致内存随时间线性增加。与先前的工作不同，在我们的方法中，允许删除先前的任务实例。我们提出通过固定大小的状态向量来利用先前的任务实例，该状态向量顺序更新。我们的理论分析表明，我们提出的内存高效在线学习（MOML）方法在凸损失函数下遭受次线性遗憾，在非凸损失下遭受次线性局部遗憾。在基准数据集上，我们展示了我们的方法即使在不允许完美回忆的情况下也能优于先前的工作。",
        "领域": "元学习、在线学习、深度学习优化",
        "问题": "解决在线元学习过程中内存使用随时间线性增长的问题",
        "动机": "为了在有限的监督下快速适应新任务，同时控制内存消耗，避免随着任务数量的增加而线性增长的内存需求",
        "方法": "提出一种内存高效的在线元学习方法（MOML），通过固定大小的状态向量顺序更新来利用先前的任务实例，允许删除旧任务实例以减少内存使用",
        "关键词": [
            "在线元学习",
            "内存效率",
            "次线性遗憾",
            "固定大小状态向量",
            "任务适应"
        ],
        "涉及的技术概念": {
            "在线元学习": "在任务实例顺序揭示的情况下进行元学习，旨在快速适应新任务",
            "内存效率": "通过允许删除旧任务实例和使用固定大小的状态向量来控制内存使用",
            "次线性遗憾": "理论分析表明，该方法在凸和非凸损失函数下的性能遗憾随时间增长的速度低于线性"
        },
        "success": true
    },
    {
        "order": 651,
        "title": "Memory-Efficient Pipeline-Parallel DNN Training",
        "html": "https://ICML.cc//virtual/2021/poster/10457",
        "abstract": "Many state-of-the-art ML results have been obtained by scaling up the number of parameters in existing models. However, parameters and activations for such large models often do not fit in the memory of a single accelerator device; this means that it is necessary to distribute training of large models over multiple accelerators.  In this work, we propose PipeDream-2BW, a system that supports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream-2BW automatically partitions the model over the available hardware resources, while respecting hardware constraints such as memory capacities of accelerators and interconnect topologies. PipeDream-2BW can accelerate the training of large GPT and BERT language models by up to 20x with similar final model accuracy.",
        "conference": "ICML",
        "中文标题": "内存高效的管道并行深度神经网络训练",
        "摘要翻译": "许多最先进的机器学习成果是通过扩大现有模型的参数数量获得的。然而，这些大型模型的参数和激活通常无法适应单个加速器设备的内存；这意味着有必要在多个加速器上分配大型模型的训练。在这项工作中，我们提出了PipeDream-2BW，一个支持内存高效管道并行的系统。PipeDream-2BW采用了一种新颖的管道化和权重梯度合并策略，结合权重的双缓冲，以确保高吞吐量、低内存占用和类似于数据并行的权重更新语义。此外，PipeDream-2BW自动在可用的硬件资源上对模型进行分区，同时尊重硬件约束，如加速器的内存容量和互连拓扑。PipeDream-2BW可以将大型GPT和BERT语言模型的训练加速高达20倍，同时保持相似的最终模型准确性。",
        "领域": "深度学习并行计算、大规模模型训练、自然语言处理",
        "问题": "解决大型模型训练时参数和激活无法适应单个加速器设备内存的问题",
        "动机": "为了支持内存高效的管道并行，加速大型模型的训练",
        "方法": "采用管道化和权重梯度合并策略，结合权重的双缓冲，自动分区模型以适应硬件资源",
        "关键词": [
            "管道并行",
            "内存高效",
            "权重梯度合并",
            "双缓冲",
            "模型分区"
        ],
        "涉及的技术概念": {
            "管道并行": "一种并行计算策略，通过将模型的不同层分配到不同的加速器上，实现模型训练的高效并行",
            "权重梯度合并": "一种优化技术，通过合并权重梯度减少内存占用和通信开销",
            "双缓冲": "一种技术，通过同时维护两组权重，一组用于前向传播，一组用于后向传播，以提高训练效率"
        },
        "success": true
    },
    {
        "order": 652,
        "title": "Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9089",
        "abstract": "Active learning is widely used to reduce labeling effort and training time by repeatedly querying only the most beneficial samples from unlabeled data. In real-world problems where data cannot be stored indefinitely due to limited storage or privacy issues, the query selection and the model update should be performed as soon as a new data sample is observed. Various online active learning methods have been studied to deal with these challenges; however, there are difficulties in selecting representative query samples and updating the model efficiently without forgetting. In this study, we propose Message Passing Adaptive Resonance Theory (MPART) that learns the distribution and topology of input data online. Through message passing on the topological graph, MPART actively queries informative and representative samples, and continuously improves the classification performance using both labeled and unlabeled data. We evaluate our model in stream-based selective sampling scenarios with comparable query selection strategies, showing that MPART significantly outperforms competitive models.",
        "conference": "ICML",
        "中文标题": "消息传递自适应共振理论用于在线主动半监督学习",
        "摘要翻译": "主动学习通过仅从无标签数据中重复查询最有益的样本，被广泛用于减少标注努力和训练时间。在现实世界的问题中，由于存储空间有限或隐私问题，数据不能被无限期存储，查询选择和模型更新应在观察到新数据样本时立即进行。已经研究了各种在线主动学习方法来应对这些挑战；然而，在选择代表性查询样本和高效更新模型而不遗忘方面存在困难。在本研究中，我们提出了消息传递自适应共振理论（MPART），它在线学习输入数据的分布和拓扑结构。通过在拓扑图上进行消息传递，MPART主动查询信息丰富且具有代表性的样本，并利用有标签和无标签数据持续提高分类性能。我们在流式选择性采样场景中评估了我们的模型，与可比的查询选择策略相比，MPART显著优于竞争模型。",
        "领域": "在线学习、半监督学习、主动学习",
        "问题": "如何在有限存储和隐私约束下，高效选择代表性样本并更新模型而不遗忘",
        "动机": "解决在线主动学习中选择代表性查询样本和高效更新模型的挑战",
        "方法": "提出消息传递自适应共振理论（MPART），在线学习输入数据的分布和拓扑结构，通过消息传递主动查询信息丰富且具有代表性的样本，并利用有标签和无标签数据持续提高分类性能",
        "关键词": [
            "消息传递",
            "自适应共振理论",
            "在线主动学习",
            "半监督学习",
            "流式学习"
        ],
        "涉及的技术概念": {
            "消息传递": "在拓扑图上进行信息交换，用于选择信息丰富且具有代表性的样本",
            "自适应共振理论": "一种在线学习算法，用于学习输入数据的分布和拓扑结构",
            "在线主动学习": "一种学习策略，旨在通过选择最有信息的样本进行标注来减少标注努力，同时适应新数据"
        },
        "success": true
    },
    {
        "order": 653,
        "title": "Meta-Cal: Well-controlled Post-hoc Calibration by Ranking",
        "html": "https://ICML.cc//virtual/2021/poster/9773",
        "abstract": "In many applications, it is desirable that a classifier not only makes accurate predictions, but also outputs calibrated posterior probabilities. However, many existing classifiers, especially deep neural network classifiers, tend to be uncalibrated. Post-hoc calibration is a technique to recalibrate a model by learning a calibration map. Existing approaches mostly focus on constructing calibration maps with low calibration errors, however, this quality is inadequate for a calibrator being useful. In this paper, we introduce two constraints that are worth consideration in designing a calibration map for post-hoc calibration. Then we present Meta-Cal, which is built from a base calibrator and a ranking model. Under some mild assumptions, two high-probability bounds are given with respect to these constraints. Empirical results on CIFAR-10, CIFAR-100 and ImageNet and a range of popular network architectures show our proposed method significantly outperforms the current state of the art for post-hoc multi-class classification calibration.",
        "conference": "ICML",
        "中文标题": "Meta-Cal：基于排名的良好控制事后校准",
        "摘要翻译": "在许多应用中，不仅希望分类器能做出准确的预测，还希望其能输出校准后的后验概率。然而，许多现有的分类器，尤其是深度神经网络分类器，往往未经校准。事后校准是一种通过学习校准映射来重新校准模型的技术。现有方法主要关注于构建具有低校准误差的校准映射，然而，这一质量对于校准器来说是不够的。在本文中，我们介绍了在设计事后校准的校准映射时值得考虑的两个约束条件。然后，我们提出了Meta-Cal，它由一个基础校准器和一个排名模型构建而成。在一些温和的假设下，给出了与这些约束条件相关的两个高概率界限。在CIFAR-10、CIFAR-100和ImageNet以及一系列流行的网络架构上的实证结果表明，我们提出的方法在多类分类事后校准方面显著优于当前的最新技术。",
        "领域": "深度学习模型校准、多类分类、后验概率校准",
        "问题": "解决深度神经网络分类器输出未经校准的后验概率问题",
        "动机": "提高分类器输出的后验概率的校准质量，使其不仅准确而且可靠",
        "方法": "通过引入两个设计约束条件，构建基于基础校准器和排名模型的Meta-Cal方法",
        "关键词": [
            "模型校准",
            "后验概率",
            "多类分类",
            "深度神经网络",
            "Meta-Cal"
        ],
        "涉及的技术概念": {
            "事后校准": "一种通过学习校准映射来重新校准模型的技术，旨在提高模型输出的后验概率的校准质量",
            "校准映射": "用于调整模型输出概率的映射函数，目的是使预测概率与实际概率更加一致",
            "高概率界限": "在统计学中，指在给定概率下，某个统计量的上限或下限，用于评估校准映射的性能"
        },
        "success": true
    },
    {
        "order": 654,
        "title": "MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration",
        "html": "https://ICML.cc//virtual/2021/poster/9825",
        "abstract": "Meta reinforcement learning (meta-RL) extracts knowledge from previous tasks and achieves fast adaptation to new tasks. Despite recent progress, efficient exploration in meta-RL remains a key challenge in sparse-reward tasks, as it requires quickly finding informative task-relevant experiences in both meta-training and adaptation. To address this challenge, we explicitly model an exploration policy learning problem for meta-RL, which is separated from exploitation policy learning, and introduce a novel empowerment-driven exploration objective, which aims to maximize information gain for task identification. We derive a corresponding intrinsic reward and develop a new off-policy meta-RL framework, which efficiently learns separate context-aware exploration and exploitation policies by sharing the knowledge of task inference. Experimental evaluation shows that our meta-RL method significantly outperforms state-of-the-art baselines on various sparse-reward MuJoCo locomotion tasks and more complex sparse-reward Meta-World tasks.",
        "conference": "ICML",
        "中文标题": "MetaCURE：基于赋权驱动探索的元强化学习",
        "摘要翻译": "元强化学习（meta-RL）从先前任务中提取知识，并实现对新任务的快速适应。尽管最近有所进展，但在稀疏奖励任务中，有效的探索仍然是元强化学习的一个关键挑战，因为它需要在元训练和适应过程中快速找到信息丰富的任务相关经验。为了解决这一挑战，我们明确地为元强化学习建模了一个探索策略学习问题，该问题与利用策略学习分离，并引入了一种新颖的赋权驱动探索目标，旨在最大化任务识别的信息增益。我们推导了相应的内在奖励，并开发了一个新的离策略元强化学习框架，该框架通过共享任务推断的知识，有效地学习分离的上下文感知探索和利用策略。实验评估显示，我们的元强化学习方法在各种稀疏奖励的MuJoCo运动任务和更复杂的稀疏奖励Meta-World任务上显著优于最先进的基线方法。",
        "领域": "元强化学习、稀疏奖励任务、机器人运动控制",
        "问题": "解决在稀疏奖励任务中元强化学习的有效探索问题",
        "动机": "为了提高元强化学习在稀疏奖励任务中的探索效率，快速找到任务相关信息",
        "方法": "通过分离探索和利用策略学习，引入赋权驱动探索目标，并开发新的离策略元强化学习框架",
        "关键词": [
            "元强化学习",
            "稀疏奖励",
            "赋权驱动探索",
            "离策略学习",
            "任务推断"
        ],
        "涉及的技术概念": {
            "元强化学习": "从先前任务中提取知识，快速适应新任务的技术",
            "赋权驱动探索": "一种旨在最大化任务识别信息增益的探索策略",
            "离策略学习": "一种允许学习策略与行为策略不同的强化学习框架"
        },
        "success": true
    },
    {
        "order": 655,
        "title": "Meta-Learning Bidirectional Update Rules",
        "html": "https://ICML.cc//virtual/2021/poster/8529",
        "abstract": "In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional 'genome'. We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.",
        "conference": "ICML",
        "中文标题": "元学习双向更新规则",
        "摘要翻译": "本文介绍了一种新型的广义神经网络，其中神经元和突触维持多种状态。我们展示了神经网络中基于梯度的经典反向传播可以被视为一种两状态网络的特例，其中一种状态用于激活，另一种用于梯度，更新规则源自链式法则。在我们的广义框架中，网络既没有明确的概念，也从未接收梯度。突触和神经元使用由共享的低维‘基因组’参数化的双向Hebb式更新规则进行更新。我们展示了这样的基因组可以从零开始元学习，使用传统的优化技术或进化策略，如CMA-ES。所得到的更新规则在未见过的任务上具有泛化能力，并且在几个标准的计算机视觉和合成任务上比基于梯度下降的优化器训练得更快。",
        "领域": "元学习、神经网络优化、计算机视觉",
        "问题": "如何在没有显式梯度的情况下，通过元学习自动发现有效的神经网络更新规则",
        "动机": "探索超越传统梯度下降的神经网络训练方法，以提高训练效率和泛化能力",
        "方法": "提出了一种广义神经网络框架，使用双向Hebb式更新规则和共享低维‘基因组’进行元学习",
        "关键词": [
            "元学习",
            "双向更新规则",
            "Hebb式学习",
            "神经网络优化",
            "计算机视觉"
        ],
        "涉及的技术概念": {
            "广义神经网络": "一种神经元和突触维持多种状态的网络，扩展了传统神经网络的概念",
            "双向Hebb式更新规则": "一种不依赖于显式梯度的学习规则，通过元学习参数化",
            "元学习基因组": "一个共享的低维参数集，用于生成网络的更新规则，可以通过优化或进化策略学习"
        },
        "success": true
    },
    {
        "order": 656,
        "title": "Meta Learning for Support Recovery in High-dimensional Precision Matrix Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9285",
        "abstract": "In this paper, we study meta learning for support (i.e., the set of non-zero entries) recovery in high-dimensional precision matrix estimation where we reduce the sufficient sample complexity in a novel task with the information learned from other auxiliary tasks. In our setup, each task has a different random true precision matrix, each with a possibly different support. We assume that the union of the supports of all the true precision matrices (i.e., the true support union) is small in size. We propose to pool all the samples from different tasks, and \\emph{improperly} estimate a single precision matrix by minimizing the $\\ell_1$-regularized log-determinant Bregman divergence. We show that with high probability, the support of the \\emph{improperly} estimated single precision matrix is equal to the true support union, provided a sufficient number of samples per task $n \\in O((\\log N)/K)$, for $N$-dimensional vectors and $K$ tasks. That is, one requires less samples per task when more tasks are available. We prove a matching information-theoretic lower bound for the necessary number of samples, which is $n \\in \\Omega((\\log N)/K)$, and thus, our algorithm is minimax optimal. Then for the novel task, we prove that the minimization of the $\\ell_1$-regularized log-determinant Bregman divergence with the additional constraint that the support is a subset of the estimated support union could reduce the sufficient sample complexity of successful support recovery to $O(\\log(|S_{\\text{off}}|))$ where $|S_{\\text{off}}|$ is the number of off-diagonal elements in the support union and is much less than $N$ for sparse matrices. We also prove a matching information-theoretic lower bound of $\\Omega(\\log(|S_{\\text{off}}|))$ for the necessary number of samples.",
        "conference": "ICML",
        "success": true,
        "中文标题": "高维精度矩阵估计中支持恢复的元学习",
        "摘要翻译": "本文研究了高维精度矩阵估计中支持（即非零条目集合）恢复的元学习，其中我们通过从其他辅助任务中学习到的信息，在新任务中减少了足够的样本复杂度。在我们的设置中，每个任务都有一个不同的随机真实精度矩阵，每个矩阵可能有不同的支持。我们假设所有真实精度矩阵的支持的并集（即真实支持并集）的大小较小。我们建议汇集来自不同任务的所有样本，并通过最小化ℓ1正则化的对数行列式Bregman散度来不恰当地估计单个精度矩阵。我们证明，在足够数量的样本下，即每个任务的样本数n∈O((logN)/K)，对于N维向量和K个任务，不恰当估计的单个精度矩阵的支持等于真实支持并集的概率很高。也就是说，当有更多任务可用时，每个任务需要的样本更少。我们证明了必要样本数的匹配信息理论下界，即n∈Ω((logN)/K)，因此我们的算法是极小极大最优的。然后对于新任务，我们证明了在支持是估计支持并集的子集的附加约束下，最小化ℓ1正则化的对数行列式Bregman散度可以将成功支持恢复的足够样本复杂度降低到O(log(|Soff|))，其中|Soff|是支持并集中非对角线元素的数量，对于稀疏矩阵来说远小于N。我们还证明了必要样本数的匹配信息理论下界Ω(log(|Soff|))。",
        "领域": "高维统计学习, 稀疏矩阵估计, 元学习",
        "问题": "在高维精度矩阵估计中，如何减少新任务中支持恢复所需的样本复杂度",
        "动机": "通过从其他辅助任务中学习到的信息，减少新任务中支持恢复所需的样本复杂度",
        "方法": "汇集来自不同任务的样本，通过最小化ℓ1正则化的对数行列式Bregman散度来不恰当地估计单个精度矩阵，并证明其支持等于真实支持并集",
        "关键词": [
            "元学习",
            "高维精度矩阵",
            "支持恢复",
            "稀疏矩阵",
            "Bregman散度"
        ],
        "涉及的技术概念": {
            "元学习": "通过从其他辅助任务中学习到的信息，减少新任务中支持恢复所需的样本复杂度",
            "ℓ1正则化": "用于在估计精度矩阵时引入稀疏性，帮助识别非零条目",
            "Bregman散度": "用于度量两个概率分布之间的差异，本文中用于精度矩阵的估计和优化"
        }
    },
    {
        "order": 657,
        "title": "Meta-learning Hyperparameter Performance Prediction with Neural Processes",
        "html": "https://ICML.cc//virtual/2021/poster/9265",
        "abstract": "The surrogate that predicts the performance of hyperparameters has been a key component for sequential model-based hyperparameter optimization. In practical applications, a trial of a hyper-parameter configuration may be so costly that a surrogate is expected to return an optimal configuration with as few trials as possible. Observing that human experts draw on their expertise in a machine learning model by trying configurations that once performed well on other datasets, we are inspired to build a trial-efficient surrogate by transferring the meta-knowledge learned from historical trials on other datasets.  We propose an end-to-end surrogate named as Transfer NeuralProcesses (TNP) that learns a comprehensive set of meta-knowledge, including the parameters of historical surrogates, historical trials, and initial configurations for other datasets. Experiments on extensive OpenML datasets and three computer vision datasets demonstrate that the proposed algorithm achieves state-of-the-art performance in at least one order of magnitude less trials.",
        "conference": "ICML",
        "中文标题": "基于神经过程的元学习超参数性能预测",
        "摘要翻译": "预测超参数性能的替代模型一直是基于序列模型的超参数优化的关键组成部分。在实际应用中，一次超参数配置的尝试可能成本极高，因此期望替代模型能以尽可能少的尝试次数返回最优配置。观察到人类专家通过尝试在其他数据集上表现良好的配置来利用他们在机器学习模型中的专业知识，我们受到启发，通过转移从其他数据集的历史尝试中学到的元知识，构建一个尝试效率高的替代模型。我们提出了一种名为转移神经过程（TNP）的端到端替代模型，它学习了一套全面的元知识，包括历史替代模型的参数、历史尝试以及其他数据集的初始配置。在广泛的OpenML数据集和三个计算机视觉数据集上的实验表明，所提出的算法在至少少一个数量级的尝试次数下实现了最先进的性能。",
        "领域": "元学习、超参数优化、计算机视觉",
        "问题": "如何在尽可能少的尝试次数下预测超参数性能并找到最优配置",
        "动机": "受到人类专家利用历史经验优化超参数的启发，旨在通过元知识转移提高替代模型的效率",
        "方法": "提出了一种名为转移神经过程（TNP）的端到端替代模型，通过学习历史数据和初始配置来预测超参数性能",
        "关键词": [
            "元学习",
            "超参数优化",
            "神经过程",
            "计算机视觉",
            "替代模型"
        ],
        "涉及的技术概念": {
            "神经过程": "一种用于建模不确定性和进行预测的概率模型，本文中用于构建替代模型",
            "元知识转移": "将从其他数据集学到的知识应用到新数据集上，以提高模型的效率和性能",
            "端到端学习": "模型直接从输入数据学习到输出预测，无需手动设计中间步骤，本文中用于构建替代模型"
        },
        "success": true
    },
    {
        "order": 658,
        "title": "Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation",
        "html": "https://ICML.cc//virtual/2021/poster/9659",
        "abstract": "With rapid progress in neural text-to-speech (TTS) models, personalized speech generation is now in high demand for many applications. For practical applicability, a TTS model should generate high-quality speech with only a few audio samples from the given speaker, that are also short in length. However, existing methods either require to fine-tune the model or achieve low adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a new TTS model which not only synthesizes high-quality speech but also effectively adapts to new speakers. Specifically, we propose Style-Adaptive Layer Normalization (SALN) which aligns gain and bias of the text input according to the style extracted from a reference speech audio. With SALN, our model effectively synthesizes speech in the style of the target speaker even from a single speech audio. Furthermore, to enhance StyleSpeech's adaptation to speech from new speakers, we extend it to Meta-StyleSpeech by introducing two discriminators trained with style prototypes, and performing episodic training. The experimental results show that our models generate high-quality speech which accurately follows the speaker's voice with single short-duration (1-3 sec) speech audio, significantly outperforming baselines.",
        "conference": "ICML",
        "中文标题": "元风格语音：多说话者自适应文本到语音生成",
        "摘要翻译": "随着神经文本到语音（TTS）模型的快速发展，个性化语音生成现在在许多应用中需求很高。为了实际应用性，TTS模型应该仅用给定说话者的少量音频样本（这些样本也很短）就能生成高质量的语音。然而，现有方法要么需要对模型进行微调，要么在不进行微调的情况下实现较低的适应质量。在这项工作中，我们提出了StyleSpeech，一个新的TTS模型，它不仅合成高质量的语音，还能有效地适应新的说话者。具体来说，我们提出了风格自适应层归一化（SALN），它根据从参考语音音频中提取的风格对齐文本输入的增益和偏置。通过SALN，我们的模型即使从单个语音音频也能有效地合成目标说话者风格的语音。此外，为了增强StyleSpeech对新说话者语音的适应能力，我们通过引入两个使用风格原型训练的鉴别器，并进行情景训练，将其扩展为Meta-StyleSpeech。实验结果表明，我们的模型生成的语音质量高，能够准确地跟随说话者的声音，仅使用单个短时长（1-3秒）的语音音频，显著优于基线。",
        "领域": "语音合成、个性化语音生成、自适应学习",
        "问题": "如何在仅使用少量短时长音频样本的情况下，实现高质量且能快速适应新说话者的文本到语音生成。",
        "动机": "解决现有TTS模型在个性化语音生成中需要大量数据或微调的问题，实现更高效、高质量的语音合成。",
        "方法": "提出StyleSpeech模型，采用风格自适应层归一化（SALN）技术，以及通过引入风格原型训练的鉴别器和情景训练扩展为Meta-StyleSpeech。",
        "关键词": [
            "文本到语音",
            "风格自适应",
            "多说话者",
            "个性化语音生成",
            "元学习"
        ],
        "涉及的技术概念": {
            "风格自适应层归一化（SALN）": "用于根据参考语音音频提取的风格信息调整文本输入的增益和偏置，实现语音风格的自适应。",
            "风格原型": "用于训练鉴别器的参考风格样本，帮助模型更好地理解和适应新说话者的语音风格。",
            "情景训练": "一种训练策略，通过模拟不同的适应场景，提高模型对新说话者语音的适应能力。"
        },
        "success": true
    },
    {
        "order": 659,
        "title": "Meta-Thompson Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/9729",
        "abstract": "Efficient exploration in bandits is a fundamental online learning problem. We propose a variant of Thompson sampling that learns to explore better as it interacts with bandit instances drawn from an unknown prior. The algorithm meta-learns the prior and thus we call it MetaTS. We propose several efficient implementations of MetaTS and analyze it in Gaussian bandits. Our analysis shows the benefit of meta-learning and is of a broader interest, because we derive a novel prior-dependent Bayes regret bound for Thompson sampling. Our theory is complemented by empirical evaluation, which shows that MetaTS quickly adapts to the unknown prior.",
        "conference": "ICML",
        "中文标题": "元汤普森采样",
        "摘要翻译": "在赌博机问题中高效探索是一个基本的在线学习问题。我们提出了一种汤普森采样的变体，该变体在与来自未知先验的赌博机实例交互时学习更好地探索。该算法元学习先验，因此我们称之为MetaTS。我们提出了MetaTS的几种高效实现，并在高斯赌博机中对其进行了分析。我们的分析显示了元学习的好处，并且具有更广泛的兴趣，因为我们为汤普森采样推导出了一个新颖的依赖于先验的贝叶斯遗憾界。我们的理论得到了实证评估的补充，实证评估显示MetaTS能够快速适应未知的先验。",
        "领域": "在线学习、赌博机问题、贝叶斯方法",
        "问题": "如何在未知先验的赌博机问题中实现高效探索",
        "动机": "提高汤普森采样在未知先验赌博机问题中的探索效率",
        "方法": "提出了一种元学习先验的汤普森采样变体MetaTS，并提供了几种高效实现",
        "关键词": [
            "元学习",
            "汤普森采样",
            "赌博机问题",
            "贝叶斯遗憾界",
            "在线学习"
        ],
        "涉及的技术概念": {
            "元学习": "在MetaTS中用于学习未知先验，以优化探索策略",
            "汤普森采样": "一种基于贝叶斯方法的探索-利用平衡策略，MetaTS是其变体",
            "贝叶斯遗憾界": "用于分析MetaTS性能的理论工具，展示了元学习的好处"
        },
        "success": true
    },
    {
        "order": 660,
        "title": "Mind the Box: $l_1$-APGD for Sparse Adversarial Attacks on Image Classifiers",
        "html": "https://ICML.cc//virtual/2021/poster/8787",
        "abstract": "We show that when taking into account also the image domain $[0,1]^d$, established $l_1$-projected gradient descent (PGD) attacks are suboptimal as they do not consider that the effective threat model is the intersection of the $l_1$-ball and $[0,1]^d$. We study the expected sparsity of the steepest descent step for this effective threat model and show that the exact projection onto this set is computationally feasible and yields better performance. Moreover, we propose an adaptive form of PGD which is highly effective even with a small budget of iterations. Our resulting $l_1$-APGD is a strong white-box attack showing that prior works overestimated their $l_1$-robustness. Using $l_1$-APGD for adversarial training we get a robust classifier with SOTA $l_1$-robustness. Finally, we combine $l_1$-APGD and an adaptation of the Square Attack to $l_1$ into $l_1$-AutoAttack, an ensemble of attacks which reliably assesses adversarial robustness for the threat model of $l_1$-ball intersected with $[0,1]^d$.",
        "conference": "ICML",
        "中文标题": "注意边界：用于图像分类器稀疏对抗攻击的l1-APGD",
        "摘要翻译": "我们指出，当考虑到图像域[0,1]^d时，已建立的l1-投影梯度下降（PGD）攻击并非最优，因为它们没有考虑到有效威胁模型是l1-球与[0,1]^d的交集。我们研究了对于这种有效威胁模型的最陡下降步骤的预期稀疏性，并表明对这一集合的精确投影在计算上是可行的，且能带来更好的性能。此外，我们提出了一种自适应形式的PGD，即使在迭代次数较少的情况下也非常有效。我们得到的l1-APGD是一种强大的白盒攻击，表明之前的工作高估了它们的l1-鲁棒性。使用l1-APGD进行对抗训练，我们得到了具有最先进l1-鲁棒性的鲁棒分类器。最后，我们将l1-APGD和Square Attack的l1适应版本结合到l1-AutoAttack中，这是一个攻击集合，能够可靠地评估l1-球与[0,1]^d交集的威胁模型的对抗鲁棒性。",
        "领域": "对抗样本攻击",
        "问题": "现有的l1-投影梯度下降攻击在考虑图像域[0,1]^d时不是最优的，因为它们没有考虑到有效威胁模型是l1-球与[0,1]^d的交集。",
        "动机": "研究如何更有效地在考虑图像域限制的情况下进行对抗攻击，以提高攻击效率和效果。",
        "方法": "提出了一种自适应形式的PGD攻击（l1-APGD），并研究了最陡下降步骤的预期稀疏性，以及如何精确投影到有效威胁模型集合上。",
        "关键词": [
            "对抗攻击",
            "l1-鲁棒性",
            "自适应PGD",
            "对抗训练",
            "AutoAttack"
        ],
        "涉及的技术概念": {
            "l1-投影梯度下降（PGD）": "一种用于生成对抗样本的迭代方法，通过限制扰动在l1范数球内来攻击分类器。",
            "自适应PGD（APGD）": "PGD的一种改进版本，能够自适应调整步长，提高攻击效率。",
            "对抗训练": "通过在训练过程中引入对抗样本，提高模型对对抗攻击的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 661,
        "title": "Mixed Cross Entropy Loss for Neural Machine Translation",
        "html": "https://ICML.cc//virtual/2021/poster/9227",
        "abstract": "In neural machine translation, Cross Entropy loss (CE) is the standard loss function in two training methods of auto-regressive models, i.e., teacher forcing and scheduled sampling. In this paper, we propose mixed Cross Entropy loss (mixed CE) as a substitute for CE in both training approaches. In teacher forcing, the model trained with CE regards the translation problem as a one-to-one mapping process, while in mixed CE this process can be relaxed to one-to-many. In scheduled sampling, we show that mixed CE has the potential to encourage the training and testing behaviours to be similar to each other, more effectively mitigating the exposure bias problem. We demonstrate the superiority of mixed CE over CE on several machine translation datasets, WMT'16 Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE consistently outperforms CE on a multi-reference set as well as a challenging paraphrased reference set. We also found the model trained with mixed CE is able to provide a better probability distribution defined over the translation output space. Our code is available at https://github.com/haorannlp/mix.",
        "conference": "ICML",
        "中文标题": "混合交叉熵损失在神经机器翻译中的应用",
        "摘要翻译": "在神经机器翻译中，交叉熵损失（CE）是自回归模型两种训练方法（即教师强制和计划采样）中的标准损失函数。本文提出混合交叉熵损失（mixed CE）作为这两种训练方法中CE的替代。在教师强制中，使用CE训练的模型将翻译问题视为一对一映射过程，而在mixed CE中，这一过程可以放宽为一对多。在计划采样中，我们展示了mixed CE有潜力使训练和测试行为更加相似，更有效地缓解暴露偏差问题。我们在多个机器翻译数据集（WMT'16 Ro-En、WMT'16 Ru-En和WMT'14 En-De）上，在教师强制和计划采样设置下，证明了mixed CE优于CE。此外，在WMT'14 En-De上，我们还发现mixed CE在多参考集和具有挑战性的改写参考集上持续优于CE。我们还发现使用mixed CE训练的模型能够提供更好的翻译输出空间上的概率分布。我们的代码可在https://github.com/haorannlp/mix获取。",
        "领域": "神经机器翻译、损失函数优化、自回归模型训练",
        "问题": "解决传统交叉熵损失在神经机器翻译中无法有效处理一对多映射和暴露偏差的问题",
        "动机": "提高神经机器翻译模型的翻译质量和概率分布准确性，通过改进损失函数来优化训练过程",
        "方法": "提出混合交叉熵损失（mixed CE）作为传统交叉熵损失的替代，用于教师强制和计划采样训练方法，以放宽映射过程和缓解暴露偏差",
        "关键词": [
            "混合交叉熵损失",
            "神经机器翻译",
            "教师强制",
            "计划采样",
            "暴露偏差"
        ],
        "涉及的技术概念": {
            "混合交叉熵损失": "作为传统交叉熵损失的改进版本，允许模型处理一对多的映射过程，提高翻译的多样性和质量",
            "教师强制": "一种训练自回归模型的方法，通过使用真实的前一步输出来预测下一步，mixed CE在此方法中放宽了一对一映射的限制",
            "计划采样": "另一种训练方法，通过逐步引入模型自身的预测作为输入，mixed CE在此方法中帮助减少训练和测试之间的行为差异，缓解暴露偏差问题"
        },
        "success": true
    },
    {
        "order": 662,
        "title": "Mixed Nash Equilibria in the Adversarial Examples Game",
        "html": "https://ICML.cc//virtual/2021/poster/10029",
        "abstract": "This paper tackles the problem of adversarial examples from a game theoretic point of view. We study the open question of the existence of mixed Nash equilibria in the zero-sum game formed by the attacker and the classifier. While previous works usually allow only one player to use randomized strategies, we show the necessity of considering randomization for both the classifier and the attacker. We demonstrate that this game has no duality gap, meaning that it always admits approximate Nash equilibria. We also provide the first optimization algorithms to learn a mixture of classifiers that approximately realizes the value of this game, \\emph{i.e.} procedures to build an optimally robust randomized classifier. ",
        "conference": "ICML",
        "中文标题": "对抗样本游戏中的混合纳什均衡",
        "摘要翻译": "本文从博弈论的角度探讨了对抗样本的问题。我们研究了由攻击者和分类器形成的零和游戏中混合纳什均衡存在性的开放性问题。虽然以往的工作通常只允许一个玩家使用随机化策略，但我们展示了考虑分类器和攻击者双方随机化的必要性。我们证明了该游戏没有对偶间隙，意味着它总是允许近似纳什均衡的存在。我们还提供了首个优化算法，用于学习一类分类器的混合，以近似实现该游戏的价值，即构建最优鲁棒随机化分类器的程序。",
        "领域": "对抗样本防御、博弈论在机器学习中的应用、深度学习安全",
        "问题": "探讨在对抗样本的零和游戏中混合纳什均衡的存在性及其计算方法。",
        "动机": "以往研究通常只考虑一方玩家使用随机化策略，忽视了双方随机化的必要性，本研究旨在填补这一空白。",
        "方法": "通过博弈论分析，证明了游戏无对偶间隙，并开发了优化算法以学习实现游戏价值的分类器混合。",
        "关键词": [
            "对抗样本",
            "混合纳什均衡",
            "博弈论",
            "随机化分类器",
            "优化算法"
        ],
        "涉及的技术概念": {
            "混合纳什均衡": "在对抗样本的零和游戏中，分类器和攻击者双方采用随机化策略时的均衡状态。",
            "对偶间隙": "在优化问题中，原始问题和对偶问题最优解之间的差异，本研究中证明该游戏无对偶间隙。",
            "随机化分类器": "通过随机化策略构建的分类器，旨在提高对抗样本攻击下的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 663,
        "title": "Model-based Reinforcement Learning for Continuous Control with Posterior Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/9011",
        "abstract": "Balancing exploration and exploitation is crucial in reinforcement learning (RL). In this paper, we study model-based posterior sampling for reinforcement learning (PSRL) in continuous state-action spaces theoretically and empirically. First, we show the first regret bound of PSRL in continuous spaces which is polynomial in the episode length to the best of our knowledge. With the assumption that reward and transition functions can be modeled by Bayesian linear regression, we develop a regret bound of $\\tilde{O}(H^{3/2}d\\sqrt{T})$, where $H$ is the episode length, $d$ is the dimension of the state-action space, and $T$ indicates the total time steps. This result matches the best-known regret bound of non-PSRL methods in linear MDPs. Our bound can be extended to nonlinear cases as well with feature embedding: using linear kernels on the feature representation $\\phi$, the regret bound becomes $\\tilde{O}(H^{3/2}d_{\\phi}\\sqrt{T})$, where $d_\\phi$ is the dimension of the representation space. Moreover, we present MPC-PSRL, a model-based posterior sampling algorithm with model predictive control for action selection. To capture the uncertainty in models, we use Bayesian linear regression on the penultimate layer (the feature representation layer $\\phi$) of neural networks. Empirical results show that our algorithm achieves the state-of-the-art sample efficiency in benchmark continuous control tasks compared to prior model-based algorithms, and matches the asymptotic performance of model-free algorithms. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于模型的后验采样强化学习用于连续控制",
        "摘要翻译": "在强化学习（RL）中，平衡探索和利用至关重要。在本文中，我们从理论和经验两个方面研究了基于模型的后验采样强化学习（PSRL）在连续状态-动作空间中的应用。首先，我们展示了PSRL在连续空间中的第一个遗憾界限，据我们所知，它是关于episode长度的多项式。假设奖励和转移函数可以用贝叶斯线性回归建模，我们得出了一个遗憾界限O(H^(3/2)d√T)，其中H是episode长度，d是状态-动作空间的维度，T表示总时间步数。该结果与线性MDPs中非PSRL方法的最佳已知遗憾界限相匹配。我们的界限也可以扩展到具有特征嵌入的非线性情况：在使用特征表示φ上的线性核时，遗憾界限变为O(H^(3/2)d_φ√T)，其中d_φ是表示空间的维度。此外，我们提出了MPC-PSRL，一种基于模型的后验采样算法，结合模型预测控制进行动作选择。为了捕捉模型中的不确定性，我们使用贝叶斯线性回归在神经网络的倒数第二层（特征表示层φ）上进行。实验结果表明，与之前的基于模型的算法相比，我们的算法在基准连续控制任务中实现了最先进的样本效率，并且与无模型算法的渐近性能相匹配。",
        "领域": "强化学习, 连续控制, 模型预测控制",
        "问题": "如何在连续状态-动作空间中有效地进行基于模型的后验采样强化学习，以平衡探索和利用，并提高样本效率。",
        "动机": "现有的强化学习方法在连续控制任务中通常需要大量的样本才能达到较好的性能，并且难以有效地平衡探索和利用。因此，需要研究一种更有效的基于模型的后验采样方法，以提高样本效率，并获得更好的性能。",
        "方法": "提出了MPC-PSRL算法，该算法结合了模型预测控制和后验采样，使用贝叶斯线性回归对神经网络的特征表示层进行建模，以捕捉模型中的不确定性，从而实现更有效的探索和利用。",
        "关键词": [
            "后验采样强化学习",
            "连续控制",
            "模型预测控制",
            "贝叶斯线性回归",
            "样本效率"
        ],
        "涉及的技术概念": {
            "后验采样": "通过从模型参数的后验分布中采样，来估计模型的不确定性，从而指导探索，平衡探索和利用。",
            "模型预测控制": "使用模型预测未来的状态和奖励，并选择能够最大化累积奖励的动作序列，从而进行控制。"
        }
    },
    {
        "order": 664,
        "title": "Model-Based Reinforcement Learning via Latent-Space Collocation",
        "html": "https://ICML.cc//virtual/2021/poster/10165",
        "abstract": "The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad and general capabilities. However, realistic tasks require performing temporally extended reasoning, and cannot be solved with only myopic, short-sighted planning. Recent work in model-based reinforcement learning (RL) has shown impressive results on tasks that require only short-horizon reasoning. In this work, we study how the long-horizon planning abilities can be improved with an algorithm that optimizes over sequences of states, rather than actions, which allows better credit assignment. To achieve this, we draw on the idea of collocation and adapt it to the image-based setting by leveraging probabilistic latent variable models, resulting in an algorithm that optimizes trajectories over latent variables. Our latent collocation method (LatCo) provides a general and effective visual planning approach, and significantly outperforms prior model-based approaches on challenging visual control tasks with sparse rewards and long-term goals. See the videos on the supplementary website \\url{https://sites.google.com/view/latco-mbrl/.}",
        "conference": "ICML",
        "中文标题": "基于模型的强化学习：潜在空间配置法",
        "摘要翻译": "仅利用原始高维观测（如图像）进行未来规划的能力，可以为自主智能体提供广泛而通用的能力。然而，现实任务需要进行时间上的扩展推理，仅靠短视的规划无法解决。最近在基于模型的强化学习（RL）方面的研究在仅需短视推理的任务上取得了令人印象深刻的成果。在这项工作中，我们研究了如何通过一种优化状态序列而非动作序列的算法来提升长期规划能力，从而实现更好的信用分配。为此，我们借鉴了配置法的思想，并通过利用概率潜在变量模型将其适应于基于图像的设置，从而产生了一种优化潜在变量轨迹的算法。我们的潜在配置方法（LatCo）提供了一种通用且有效的视觉规划方法，并在具有稀疏奖励和长期目标的挑战性视觉控制任务上显著优于先前的基于模型的方法。更多视频请参见补充网站。",
        "领域": "强化学习、视觉控制、自主智能体",
        "问题": "如何在仅使用原始高维观测（如图像）的情况下，提升自主智能体在需要长期规划的任务中的表现。",
        "动机": "现实世界中的许多任务需要智能体进行长期规划，而现有的基于模型的强化学习方法在短视任务上表现良好，但在需要长期规划的任务上仍有提升空间。",
        "方法": "提出了一种优化状态序列而非动作序列的算法，利用概率潜在变量模型将配置法思想应用于基于图像的设置，从而优化潜在变量轨迹。",
        "关键词": [
            "强化学习",
            "潜在变量模型",
            "视觉规划",
            "长期规划",
            "自主智能体"
        ],
        "涉及的技术概念": {
            "潜在变量模型": "用于在潜在空间中表示和优化状态序列，以更好地进行长期规划。",
            "配置法": "一种优化技术，被适应于基于图像的设置，用于优化潜在变量轨迹。",
            "信用分配": "在强化学习中，指如何将长期奖励分配给导致该奖励的具体动作或状态序列，本方法通过优化状态序列改善了信用分配。"
        },
        "success": true
    },
    {
        "order": 665,
        "title": "Model Distillation for Revenue Optimization: Interpretable Personalized Pricing",
        "html": "https://ICML.cc//virtual/2021/poster/8575",
        "abstract": "Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be verified, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies that are not interpretable, resulting in slow adoption in practice. We present a novel, customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets.",
        "conference": "ICML",
        "中文标题": "收益优化的模型蒸馏：可解释的个性化定价",
        "摘要翻译": "数据驱动的定价策略正变得越来越普遍，其中根据预测客户对产品估值的特征为客户提供个性化价格。这种定价策略最好是简单且可解释的，以便可以验证、检查公平性并易于实施。然而，将机器学习融入定价框架的努力往往导致复杂且不可解释的定价策略，从而在实践中采用缓慢。我们提出了一种新颖的、定制化的、基于树的处方算法，该算法从复杂的黑盒机器学习算法中蒸馏知识，将具有相似估值的客户分段，并以最大化收益同时保持可解释性的方式规定价格。我们量化了由此产生的策略的遗憾，并在合成和真实世界数据集的应用中证明了其有效性。",
        "领域": "个性化定价、机器学习应用、收益管理",
        "问题": "如何在保持定价策略简单和可解释的同时，利用机器学习优化收益",
        "动机": "解决机器学习在定价策略中应用导致的复杂性和不可解释性问题，促进其在实践中的采用",
        "方法": "提出了一种定制化的基于树的处方算法，从复杂的机器学习模型中蒸馏知识，实现客户分段和价格规定，以优化收益并保持可解释性",
        "关键词": [
            "模型蒸馏",
            "个性化定价",
            "收益优化",
            "可解释性",
            "机器学习"
        ],
        "涉及的技术概念": {
            "模型蒸馏": "从复杂的机器学习模型中提取知识，用于创建更简单、可解释的模型",
            "基于树的算法": "用于客户分段和价格规定的算法，保持策略的简单性和可解释性",
            "收益优化": "通过个性化定价策略最大化收益，同时确保策略的可解释性和公平性"
        },
        "success": true
    },
    {
        "order": 666,
        "title": "Model-Free and Model-Based Policy Evaluation when Causality is Uncertain",
        "html": "https://ICML.cc//virtual/2021/poster/8969",
        "abstract": "When decision-makers can directly intervene, policy evaluation algorithms give valid causal estimates. In off-policy evaluation (OPE), there may exist unobserved variables that both impact the dynamics and are used by the unknown behavior policy. These ``confounders'' will introduce spurious correlations and naive estimates for a new policy will be biased. We develop worst-case bounds to assess sensitivity to these unobserved confounders in finite horizons when confounders are drawn iid each period. We demonstrate that a model-based approach with robust MDPs gives sharper lower bounds by exploiting domain knowledge about the dynamics. Finally, we show that when unobserved confounders are persistent over time, OPE is far more difficult and existing techniques produce extremely conservative bounds.",
        "conference": "ICML",
        "中文标题": "因果关系不确定时的无模型与基于模型的策略评估",
        "摘要翻译": "当决策者可以直接干预时，策略评估算法能提供有效的因果估计。在离策略评估（OPE）中，可能存在既影响动态又被未知行为策略使用的未观测变量。这些‘混杂因素’会引入虚假相关性，导致对新策略的朴素估计产生偏差。我们开发了最坏情况下的界限，以评估在混杂因素每期独立同分布时对这些未观测混杂因素的敏感性。我们证明，通过利用关于动态的领域知识，采用基于模型的鲁棒MDP方法可以得到更尖锐的下界。最后，我们表明，当未观测混杂因素随时间持续存在时，OPE变得更加困难，现有技术产生的界限极其保守。",
        "领域": "强化学习、因果推理、鲁棒优化",
        "问题": "解决在存在未观测混杂因素的情况下，策略评估的偏差问题",
        "动机": "评估和减少未观测混杂因素对策略评估准确性的影响",
        "方法": "开发最坏情况下的界限评估敏感性，利用基于模型的鲁棒MDP方法优化下界",
        "关键词": [
            "离策略评估",
            "混杂因素",
            "鲁棒MDP",
            "因果推理",
            "策略评估"
        ],
        "涉及的技术概念": {
            "离策略评估（OPE）": "评估在不同于数据收集策略的策略下的预期回报",
            "混杂因素": "未观测变量，既影响系统动态又被行为策略使用，导致估计偏差",
            "鲁棒MDP": "一种考虑不确定性的马尔可夫决策过程框架，用于在存在未观测混杂因素时优化策略评估"
        },
        "success": true
    },
    {
        "order": 667,
        "title": "Model-Free Reinforcement Learning: from Clipped Pseudo-Regret to Sample Complexity",
        "html": "https://ICML.cc//virtual/2021/poster/8593",
        "abstract": "In this paper we consider the problem of learning an $\\epsilon$-optimal policy for a discounted Markov Decision Process (MDP). Given an MDP with $S$ states, $A$ actions, the discount factor $\\gamma \\in (0,1)$, and an approximation threshold $\\epsilon > 0$, we provide a model-free  algorithm to learn an $\\epsilon$-optimal policy with sample complexity   $\\tilde{O}(\\frac{SA\\ln(1/p)}{\\epsilon^2(1-\\gamma)^{5.5}})$ \\footnote{In this work, the notation $\\tilde{O}(\\cdot)$ hides poly-logarithmic factors of $S,A,1/(1-\\gamma)$, and $1/\\epsilon$.} and success probability $(1-p)$. For small enough $\\epsilon$, we show an improved algorithm with sample complexity   $\\tilde{O}(\\frac{SA\\ln(1/p)}{\\epsilon^2(1-\\gamma)^{3}})$. While the first bound improves upon all known model-free algorithms and model-based ones with tight dependence on $S$, our second algorithm beats all known sample complexity bounds and matches the information theoretic lower bound up to logarithmic factors.",
        "conference": "ICML",
        "中文标题": "无模型强化学习：从裁剪伪遗憾到样本复杂度",
        "摘要翻译": "在本文中，我们考虑了学习折扣马尔可夫决策过程（MDP）的ε-最优策略的问题。给定一个具有S状态、A动作、折扣因子γ∈(0,1)和近似阈值ε>0的MDP，我们提供了一种无模型算法，以样本复杂度O~(SA ln(1/p)/ε²(1-γ)^5.5和成功概率(1-p)学习ε-最优策略。对于足够小的ε，我们展示了一种改进的算法，其样本复杂度为O~(SA ln(1/p)/ε²(1-γ)^3。虽然第一个界限改进了所有已知的无模型算法和依赖于S的紧密模型算法，但我们的第二个算法超越了所有已知的样本复杂度界限，并在对数因子内匹配了信息理论下界。",
        "领域": "强化学习、马尔可夫决策过程、样本复杂度分析",
        "问题": "学习折扣马尔可夫决策过程的ε-最优策略",
        "动机": "提高在无模型设置下学习最优策略的样本效率",
        "方法": "提出了一种无模型算法，通过改进样本复杂度来学习ε-最优策略",
        "关键词": [
            "无模型强化学习",
            "样本复杂度",
            "马尔可夫决策过程",
            "ε-最优策略",
            "裁剪伪遗憾"
        ],
        "涉及的技术概念": {
            "无模型强化学习": "不依赖于环境模型，直接从经验中学习策略的强化学习方法",
            "样本复杂度": "算法达到预定性能所需的最小样本数量",
            "马尔可夫决策过程": "用于建模决策问题的数学框架，其中结果部分随机且部分受决策者控制"
        },
        "success": true
    },
    {
        "order": 668,
        "title": "Model Fusion for Personalized Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9063",
        "abstract": "Production systems operating on a growing domain of analytic services often require generating warm-start solution models for emerging tasks with limited data. One potential approach to address this warm-start challenge is to adopt meta learning to generate a base model that can be adapted to solve unseen tasks with minimal fine-tuning. This however requires the training processes of previous solution models of existing tasks to be synchronized. This is not possible if these models were pre-trained separately on private data owned by different entities and cannot be synchronously re-trained. To accommodate for such scenarios, we develop a new personalized learning framework that synthesizes customized models for unseen tasks via fusion of independently pre-trained models of related tasks. We establish performance guarantee for the proposed framework and demonstrate its effectiveness on both synthetic and real datasets.",
        "conference": "ICML",
        "中文标题": "个性化学习的模型融合",
        "摘要翻译": "运行在不断增长的分析服务领域上的生产系统，常常需要为数据有限的新兴任务生成热启动解决方案模型。解决这一热启动挑战的一个潜在方法是采用元学习来生成一个基础模型，该模型可以通过最小的微调来适应解决未见过的任务。然而，这要求现有任务的先前解决方案模型的训练过程是同步的。如果这些模型是在不同实体拥有的私有数据上分别预训练的，并且无法同步重新训练，那么这是不可能的。为了适应这种情况，我们开发了一个新的个性化学习框架，该框架通过融合相关任务的独立预训练模型来为未见过的任务合成定制模型。我们为提出的框架建立了性能保证，并在合成和真实数据集上证明了其有效性。",
        "领域": "元学习",
        "问题": "如何在数据有限的情况下为新兴任务生成有效的热启动解决方案模型",
        "动机": "解决在私有数据上分别预训练的模型无法同步重新训练的问题，为新兴任务提供有效的热启动解决方案",
        "方法": "开发一个新的个性化学习框架，通过融合相关任务的独立预训练模型来合成定制模型",
        "关键词": [
            "模型融合",
            "个性化学习",
            "热启动",
            "元学习",
            "预训练模型"
        ],
        "涉及的技术概念": {
            "元学习": "用于生成一个基础模型，该模型可以通过最小的微调来适应解决未见过的任务",
            "模型融合": "通过融合相关任务的独立预训练模型来为未见过的任务合成定制模型",
            "热启动": "在数据有限的情况下为新兴任务生成有效的解决方案模型"
        },
        "success": true
    },
    {
        "order": 669,
        "title": "Modeling Hierarchical Structures with Continuous Recursive Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8993",
        "abstract": "Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference (Bowman et al., 2015b) and ListOps (Nangia & Bowman, 2018). We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.",
        "conference": "ICML",
        "success": true,
        "中文标题": "使用连续递归神经网络建模层次结构",
        "摘要翻译": "递归神经网络（RvNNs）根据其潜在的层次句法结构组合序列，在多项自然语言处理任务中表现优于没有结构偏见的类似模型。然而，传统的RvNNs无法自行在纯文本序列中诱导潜在结构。已有几种扩展被提出来克服这一限制。尽管如此，这些扩展往往依赖于替代梯度或强化学习，代价是更高的偏差或方差。在这项工作中，我们提出了连续递归神经网络（CRvNN）作为一种反向传播友好的替代方案，以解决上述限制。这是通过将连续松弛纳入诱导结构来实现的。我们证明，CRvNN在具有挑战性的合成任务中，如逻辑推理（Bowman等人，2015b）和ListOps（Nangia & Bowman，2018），实现了强大的性能。我们还表明，CRvNN在真实世界的任务中，如情感分析和自然语言推理，表现与先前的潜在结构模型相当或更好。",
        "领域": "自然语言处理, 递归神经网络, 结构预测",
        "问题": "传统递归神经网络无法自行在纯文本序列中诱导潜在结构",
        "动机": "克服传统RvNNs在诱导潜在结构方面的限制，同时避免使用高偏差或方差的替代梯度或强化学习方法",
        "方法": "提出连续递归神经网络（CRvNN），通过连续松弛技术改进结构诱导过程，使其更适合反向传播",
        "关键词": [
            "连续递归神经网络",
            "结构诱导",
            "自然语言处理",
            "逻辑推理",
            "情感分析"
        ],
        "涉及的技术概念": {
            "连续递归神经网络（CRvNN）": "一种改进的递归神经网络，通过连续松弛技术使结构诱导过程更适合反向传播",
            "连续松弛": "一种技术，用于在离散结构上引入连续近似，以便于梯度下降优化",
            "结构诱导": "指模型从无结构的输入中自动发现和利用潜在的层次结构信息的能力"
        }
    },
    {
        "order": 670,
        "title": "Modelling Behavioural Diversity for Learning in Open-Ended Games",
        "html": "https://ICML.cc//virtual/2021/poster/10599",
        "abstract": "\nPromoting behavioural diversity is critical for solving games with non-transitive dynamics where strategic cycles exist, and there is no consistent winner (e.g., Rock-Paper-Scissors). Yet, there is a lack of rigorous treatment for defining diversity and  constructing diversity-aware  learning dynamics. In this work, we offer a  geometric interpretation of  behavioural diversity in games and introduce a novel diversity metric based on \\emph{determinantal point processes} (DPP). By incorporating the diversity metric into  best-response dynamics, we develop \\emph{diverse fictitious play} and \\emph{diverse policy-space response oracle} for solving normal-form games and open-ended games. We prove  the uniqueness of the diverse best response and the convergence of our algorithms on two-player games. Importantly, we show that maximising the DPP-based diversity metric   guarantees to enlarge the \\emph{gamescape} -- convex polytopes spanned by agents' mixtures of strategies. To validate our diversity-aware solvers, we test on tens of games that show  strong non-transitivity. Results suggest that  our methods  achieve at least the same, and in most games, lower exploitability than PSRO   solvers by finding effective and diverse strategies. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "为开放式游戏学习建模行为多样性",
        "摘要翻译": "在解决具有非传递性动态且存在策略循环（如石头剪刀布）的游戏中，促进行为多样性至关重要。然而，目前缺乏对定义多样性及构建多样性感知学习动态的严格处理。在本工作中，我们提供了游戏中行为多样性的几何解释，并引入了一种基于行列式点过程（DPP）的新颖多样性度量。通过将多样性度量融入最佳响应动态，我们开发了用于解决正规形式游戏和开放式游戏的多样性虚构游戏和多样性策略空间响应预言。我们证明了多样性最佳响应的唯一性以及我们的算法在两人游戏上的收敛性。重要的是，我们表明，最大化基于DPP的多样性度量保证扩大游戏景观——由代理的策略混合所跨越的凸多面体。为了验证我们的多样性感知求解器，我们在数十个表现出强非传递性的游戏上进行了测试。结果表明，我们的方法通过找到有效且多样的策略，至少达到了与PSRO求解器相同的，在大多数游戏中更低的可利用性。",
        "领域": "博弈论、机器学习、多智能体系统",
        "问题": "如何在具有非传递性动态的游戏中定义和促进行为多样性",
        "动机": "解决现有方法在定义多样性和构建多样性感知学习动态方面的不足",
        "方法": "引入基于行列式点过程的多样性度量，并开发多样性虚构游戏和多样性策略空间响应预言",
        "关键词": [
            "行为多样性",
            "行列式点过程",
            "多样性虚构游戏",
            "策略空间响应预言",
            "非传递性动态"
        ],
        "涉及的技术概念": {
            "行列式点过程（DPP）": "用于度量行为多样性的新颖方法，基于行列式点过程的数学原理",
            "多样性虚构游戏": "一种融入多样性度量的最佳响应动态，用于解决正规形式游戏",
            "多样性策略空间响应预言": "一种融入多样性度量的策略空间响应方法，用于解决开放式游戏"
        }
    },
    {
        "order": 671,
        "title": "Model Performance Scaling with Multiple Data Sources",
        "html": "https://ICML.cc//virtual/2021/poster/9409",
        "abstract": "  Real-world machine learning systems are often trained using a mix of data sources with varying cost and quality. Understanding how the size and composition of a training dataset affect model performance is critical for advancing our understanding of generalization, as well as designing more effective data collection policies. We show\n  that there is a simple scaling law that predicts the loss incurred by a model even under varying dataset composition. Our work expands recent observations of scaling laws for log-linear generalization error in the i.i.d setting and uses this to cast model performance prediction as a learning problem. Using the theory of optimal experimental design, we derive a simple rational function approximation to generalization error that can be fitted using a few model training runs. Our approach can achieve highly accurate ($r^2\\approx .9$) predictions of model performance under substantial extrapolation in two different standard supervised learning tasks and is accurate ($r^2 \\approx .83$) on more challenging machine translation and question answering tasks where many baselines achieve worse-than-random performance.",
        "conference": "ICML",
        "中文标题": "多数据源下的模型性能扩展",
        "摘要翻译": "现实世界中的机器学习系统通常使用成本和质量各异的数据源混合进行训练。理解训练数据集的大小和组成如何影响模型性能，对于推进我们对泛化的理解以及设计更有效的数据收集策略至关重要。我们展示了一个简单的扩展定律，即使在数据集组成变化的情况下，也能预测模型所遭受的损失。我们的工作扩展了最近在独立同分布设置下对对数线性泛化误差扩展定律的观察，并利用这一点将模型性能预测作为一个学习问题来提出。利用最优实验设计理论，我们推导出了一个简单的有理函数近似来逼近泛化误差，该近似可以通过几次模型训练运行来拟合。我们的方法能够在两种不同的标准监督学习任务中，在大量外推的情况下，实现高度准确（r^2≈0.9）的模型性能预测，并且在更具挑战性的机器翻译和问答任务上也是准确的（r^2≈0.83），而许多基线方法在这些任务上的表现比随机还要差。",
        "领域": "机器学习泛化理论、数据收集策略优化、模型性能预测",
        "问题": "如何预测和优化在不同数据源组成和大小下的模型性能",
        "动机": "为了更有效地设计和优化机器学习系统的数据收集策略，以及深入理解模型泛化性能",
        "方法": "通过扩展对数线性泛化误差的扩展定律，并利用最优实验设计理论，推导出一个简单的有理函数近似来预测模型性能",
        "关键词": [
            "模型性能预测",
            "数据收集策略",
            "泛化误差",
            "最优实验设计",
            "机器学习系统"
        ],
        "涉及的技术概念": {
            "扩展定律": "用于预测在不同数据集组成和大小下模型性能的简单定律",
            "最优实验设计": "用于推导泛化误差近似的方法，以优化数据收集和模型训练",
            "有理函数近似": "用于逼近泛化误差的数学工具，通过少量模型训练运行即可拟合"
        },
        "success": true
    },
    {
        "order": 672,
        "title": "Model-Targeted Poisoning Attacks with Provable Convergence",
        "html": "https://ICML.cc//virtual/2021/poster/10349",
        "abstract": "In a poisoning attack, an adversary who controls a small fraction of the training data attempts to select that data, so a model is induced that misbehaves in a particular way. We consider poisoning attacks against convex machine learning models and propose an efficient poisoning attack designed to induce a model specified by the adversary. Unlike previous model-targeted poisoning attacks, our attack comes with provable convergence to any attainable target model. We also provide a lower bound on the minimum number of poisoning points needed to achieve a given target model. Our method uses online convex optimization and finds poisoning points incrementally. This provides more flexibility than previous attacks which require an a priori assumption about the number of poisoning points. Our attack is the first model-targeted poisoning attack that provides provable convergence for convex models. In our experiments, it either exceeds or matches state-of-the-art attacks in terms of attack success rate and distance to the target model. ",
        "conference": "ICML",
        "中文标题": "具有可证明收敛性的模型目标毒化攻击",
        "摘要翻译": "在毒化攻击中，控制一小部分训练数据的对手试图选择那些数据，以诱导模型以特定方式行为异常。我们考虑针对凸机器学习模型的毒化攻击，并提出了一种高效的毒化攻击方法，旨在诱导对手指定的模型。与之前的模型目标毒化攻击不同，我们的攻击方法能够可证明地收敛到任何可达成的目标模型。我们还提供了实现给定目标模型所需的最小毒化点数目的下限。我们的方法使用在线凸优化，并增量地找到毒化点。这比之前需要预先假设毒化点数目的攻击方法提供了更多的灵活性。我们的攻击是第一个为凸模型提供可证明收敛性的模型目标毒化攻击。在我们的实验中，无论是在攻击成功率还是与目标模型的距离方面，它都超过或匹配了最先进的攻击方法。",
        "领域": "对抗性机器学习、模型安全、凸优化",
        "问题": "如何在凸机器学习模型中设计和实施高效的毒化攻击，以诱导模型达到对手指定的行为。",
        "动机": "研究动机是为了开发一种能够可证明收敛到任何可达成的目标模型的毒化攻击方法，克服现有攻击方法在灵活性和收敛性方面的限制。",
        "方法": "采用在线凸优化技术，增量地寻找毒化点，无需预先假设毒化点的数目，从而提供更高的灵活性和可证明的收敛性。",
        "关键词": [
            "毒化攻击",
            "凸优化",
            "模型安全",
            "对抗性机器学习",
            "在线学习"
        ],
        "涉及的技术概念": {
            "毒化攻击": "一种对抗性攻击方法，通过操纵训练数据来诱导机器学习模型产生特定的错误行为。",
            "在线凸优化": "一种优化技术，用于在连续到达的数据点上逐步更新模型，本研究中用于高效地找到毒化点。",
            "模型收敛性": "指攻击方法能够确保模型参数最终收敛到对手指定的目标模型，本研究首次为凸模型提供了这种保证。"
        },
        "success": true
    },
    {
        "order": 673,
        "title": "Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment",
        "html": "https://ICML.cc//virtual/2021/poster/8697",
        "abstract": "Many transfer problems require re-using previously optimal decisions for solving new tasks, which suggests the need for learning algorithms that can modify the mechanisms for choosing certain actions independently of those for choosing others. However, there is currently no formalism nor theory for how to achieve this kind of modular credit assignment. To answer this question, we define modular credit assignment as a constraint on minimizing the algorithmic mutual information among feedback signals for different decisions. We introduce what we call the modularity criterion for testing whether a learning algorithm satisfies this constraint by performing causal analysis on the algorithm itself. We generalize the recently proposed societal decision-making framework as a more granular formalism than the Markov decision process to prove that for decision sequences that do not contain cycles, certain single-step temporal difference action-value methods meet this criterion while all policy-gradient methods do not. Empirical evidence suggests that such action-value methods are more sample efficient than policy-gradient methods on transfer problems that require only sparse changes to a sequence of previously optimal decisions.",
        "conference": "ICML",
        "中文标题": "强化学习中的模块化：通过信用分配的算法独立性实现",
        "摘要翻译": "许多迁移问题需要重用先前最优的决策来解决新任务，这表明需要学习算法能够独立于选择其他动作的机制来修改选择某些动作的机制。然而，目前尚无形式化方法或理论来实现这种模块化的信用分配。为了回答这个问题，我们将模块化信用分配定义为最小化不同决策反馈信号间算法互信息的约束。我们引入了所谓的模块性准则，通过对算法本身进行因果分析来测试学习算法是否满足这一约束。我们将最近提出的社会决策制定框架推广为一个比马尔可夫决策过程更细粒度的形式化方法，以证明对于不包含循环的决策序列，某些单步时间差分动作值方法满足这一准则，而所有策略梯度方法则不满足。实证证据表明，在仅需要对先前最优决策序列进行稀疏更改的迁移问题上，这类动作值方法比策略梯度方法具有更高的样本效率。",
        "领域": "强化学习、迁移学习、决策理论",
        "问题": "如何在强化学习中实现模块化的信用分配，以独立修改选择某些动作的机制而不影响其他动作的选择机制。",
        "动机": "解决迁移问题中需要重用和修改先前最优决策的需求，当前缺乏实现模块化信用分配的形式化方法和理论。",
        "方法": "定义模块化信用分配为最小化不同决策反馈信号间算法互信息的约束，引入模块性准则进行测试，并推广社会决策制定框架以证明特定动作值方法满足该准则。",
        "关键词": [
            "模块化信用分配",
            "算法互信息",
            "时间差分学习",
            "策略梯度方法",
            "迁移学习"
        ],
        "涉及的技术概念": {
            "模块化信用分配": "定义为最小化不同决策反馈信号间算法互信息的约束，旨在独立修改选择某些动作的机制。",
            "算法互信息": "用于衡量不同决策反馈信号间的依赖性，模块化信用分配通过最小化这一指标实现决策机制的独立性。",
            "社会决策制定框架": "比马尔可夫决策过程更细粒度的形式化方法，用于分析和证明特定学习算法满足模块化信用分配准则。"
        },
        "success": true
    },
    {
        "order": 674,
        "title": "Momentum Residual Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9463",
        "abstract": "The training of deep residual neural networks (ResNets) with backpropagation has a memory cost that increases linearly with respect to the depth of the network. A simple way to circumvent this issue is to use reversible architectures. In this paper, we propose to change the forward rule of a ResNet by adding a momentum term. The resulting networks, momentum residual neural networks (MomentumNets), are invertible. Unlike previous invertible architectures, they can be used as a drop-in replacement for any existing ResNet block. We show that MomentumNets can be interpreted in the infinitesimal step size regime as second-order ordinary differential equations (ODEs) and exactly characterize how adding momentum progressively increases the representation capabilities of MomentumNets: they can learn any linear mapping up to a multiplicative factor, while ResNets cannot. In a learning to optimize setting, where convergence to a fixed point is required, we show theoretically and empirically that our method succeeds while existing invertible architectures fail. We show on CIFAR and ImageNet that MomentumNets have the same accuracy as ResNets, while having a much smaller memory footprint, and show that pre-trained MomentumNets are promising for fine-tuning models.",
        "conference": "ICML",
        "中文标题": "动量残差神经网络",
        "摘要翻译": "深度残差神经网络（ResNets）通过反向传播训练时，其内存成本随着网络深度的增加而线性增长。规避这一问题的一个简单方法是使用可逆架构。本文中，我们提出通过添加动量项来改变ResNet的前向规则。由此产生的网络，即动量残差神经网络（MomentumNets），是可逆的。与之前的可逆架构不同，它们可以作为任何现有ResNet块的即插即用替代品。我们表明，在无限小步长范围内，MomentumNets可以被解释为二阶常微分方程（ODEs），并准确描述了添加动量如何逐步增强MomentumNets的表征能力：它们可以学习任何线性映射直至一个乘法因子，而ResNets则不能。在一个需要收敛到固定点的优化学习设置中，我们从理论上和实证上展示了我们的方法成功而现有的可逆架构失败。我们在CIFAR和ImageNet上展示了MomentumNets具有与ResNets相同的准确性，同时具有更小的内存占用，并展示了预训练的MomentumNets对于微调模型是有前景的。",
        "领域": "深度学习优化, 神经网络架构设计, 计算机视觉",
        "问题": "深度残差神经网络在训练过程中内存成本随深度线性增长的问题",
        "动机": "通过引入动量项，构建可逆的残差神经网络架构，以减少内存占用并保持或提升模型性能",
        "方法": "在残差神经网络的前向规则中添加动量项，构建可逆的MomentumNets架构",
        "关键词": [
            "动量残差网络",
            "可逆架构",
            "内存优化",
            "二阶ODE",
            "深度学习"
        ],
        "涉及的技术概念": {
            "动量项": "在残差网络的前向传播中添加的动量项，用于构建可逆网络架构",
            "可逆架构": "允许网络在前向和后向传播中无需存储中间激活值的架构，显著减少内存使用",
            "二阶常微分方程": "在无限小步长范围内，MomentumNets可以被解释为二阶ODE，这为理解其表征能力提供了理论基础"
        },
        "success": true
    },
    {
        "order": 675,
        "title": "Monotonic Robust Policy Optimization with Model Discrepancy",
        "html": "https://ICML.cc//virtual/2021/poster/10253",
        "abstract": " State-of-the-art deep reinforcement learning (DRL) algorithms tend to overfit due to the model discrepancy between source and target environments. Though applying domain randomization during training can improve the average performance by randomly generating a sufficient diversity of environments in simulator, the worst-case environment is still neglected without any performance guarantee. Since the average and worst-case performance are both important for generalization in RL, in this paper, we propose a policy optimization approach for concurrently improving the policy's performance in the average and worst-case environment. We theoretically derive a lower bound for the worst-case performance of a given policy by relating it to the expected performance. Guided by this lower bound, we formulate an optimization problem to jointly optimize the policy and sampling distribution, and prove that by iteratively solving it the worst-case performance is monotonically improved. We then develop a practical algorithm, named monotonic robust policy optimization (MRPO). Experimental evaluations in several robot control tasks demonstrate that MRPO can generally improve both the average and worst-case performance in the source environments for training, and facilitate in all cases the learned policy with a better generalization capability in some unseen testing environments.",
        "conference": "ICML",
        "中文标题": "单调鲁棒策略优化与模型差异",
        "摘要翻译": "最先进的深度强化学习（DRL）算法由于源环境和目标环境之间的模型差异而容易过拟合。虽然在训练过程中应用领域随机化可以通过在模拟器中随机生成足够多样化的环境来提高平均性能，但最坏情况下的环境仍然被忽视，没有任何性能保证。由于平均和最坏情况下的性能对于强化学习的泛化都很重要，本文提出了一种策略优化方法，用于同时提高策略在平均和最坏情况下的环境中的性能。我们通过将最坏情况下的性能与预期性能相关联，理论上推导出给定策略的最坏情况下性能的下界。在这个下界的指导下，我们制定了一个优化问题来联合优化策略和采样分布，并证明通过迭代解决这个问题，最坏情况下的性能单调提高。然后，我们开发了一个名为单调鲁棒策略优化（MRPO）的实用算法。在几个机器人控制任务中的实验评估表明，MRPO通常可以提高训练源环境中的平均和最坏情况下的性能，并在所有情况下促进学习到的策略在某些未见过的测试环境中具有更好的泛化能力。",
        "领域": "深度强化学习、机器人控制、策略优化",
        "问题": "解决深度强化学习算法在源环境和目标环境之间模型差异导致的过拟合问题，以及忽视最坏情况下环境性能的问题。",
        "动机": "提高强化学习策略在平均和最坏情况下的环境中的性能，以增强其泛化能力。",
        "方法": "提出单调鲁棒策略优化（MRPO）算法，通过理论推导最坏情况下性能的下界，并联合优化策略和采样分布来单调提高最坏情况下的性能。",
        "关键词": [
            "深度强化学习",
            "策略优化",
            "模型差异",
            "泛化能力",
            "机器人控制"
        ],
        "涉及的技术概念": {
            "模型差异": "指源环境和目标环境之间的差异，是导致深度强化学习算法过拟合的主要原因。",
            "单调鲁棒策略优化（MRPO）": "一种旨在同时提高策略在平均和最坏情况下环境中性能的算法，通过优化策略和采样分布来实现。",
            "泛化能力": "指学习到的策略在未见过的测试环境中表现良好的能力，是评估强化学习算法性能的重要指标。"
        },
        "success": true
    },
    {
        "order": 676,
        "title": "Monte Carlo Variational Auto-Encoders",
        "html": "https://ICML.cc//virtual/2021/poster/8527",
        "abstract": "Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. \nHowever, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels.\nIn this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.",
        "conference": "ICML",
        "中文标题": "蒙特卡洛变分自编码器",
        "摘要翻译": "变分自编码器（VAE）是一种流行的深度潜在变量模型，通过最大化证据下界（ELBO）进行训练。为了获得更紧的ELBO，从而得到更好的变分近似，有人提出使用重要性采样来获得证据的低方差估计。然而，众所周知，重要性采样在高维度下表现不佳。虽然文献中多次建议使用更复杂的算法，如退火重要性采样（AIS）及其序列重要性采样（SIS）扩展，但这些先进技术为VAE带来的潜在好处从未实现：AIS估计不易微分，而SIS需要指定精心选择的后向马尔可夫核。在本文中，我们解决了这两个问题，并在各种应用中展示了由此产生的蒙特卡洛VAE的性能。",
        "领域": "生成模型, 变分自编码器, 蒙特卡洛方法",
        "问题": "解决变分自编码器在高维空间中重要性采样效果不佳的问题",
        "动机": "探索更高效的变分近似方法，以提升变分自编码器的性能",
        "方法": "采用退火重要性采样（AIS）和序列重要性采样（SIS）扩展技术，解决微分和马尔可夫核选择问题",
        "关键词": [
            "变分自编码器",
            "蒙特卡洛方法",
            "重要性采样",
            "退火重要性采样",
            "序列重要性采样"
        ],
        "涉及的技术概念": {
            "变分自编码器（VAE）": "一种深度潜在变量模型，通过最大化证据下界（ELBO）进行训练",
            "重要性采样": "用于估计证据的低方差方法，但在高维度下表现不佳",
            "退火重要性采样（AIS）": "一种更复杂的采样技术，可以提升变分近似的质量，但不易微分"
        },
        "success": true
    },
    {
        "order": 677,
        "title": "Moreau-Yosida $f$-divergences",
        "html": "https://ICML.cc//virtual/2021/poster/10293",
        "abstract": "Variational representations of $f$-divergences are central to many machine learning algorithms, with Lipschitz constrained variants recently gaining attention. Inspired by this, we define the Moreau-Yosida approximation of $f$-divergences with respect to the Wasserstein-$1$ metric. The corresponding variational formulas provide a generalization of a number of recent results, novel special cases of interest and a relaxation of the hard Lipschitz constraint. Additionally, we prove that the so-called tight variational representation of $f$-divergences can be to be taken over the quotient space of Lipschitz functions, and give a characterization of functions achieving the supremum in the variational representation. On the practical side, we propose an algorithm to calculate the tight convex conjugate of $f$-divergences compatible with automatic differentiation frameworks. As an application of our results, we propose the Moreau-Yosida $f$-GAN, providing an implementation of the variational formulas for the Kullback-Leibler, reverse Kullback-Leibler, $\\chi^2$, reverse $\\chi^2$, squared Hellinger, Jensen-Shannon, Jeffreys, triangular discrimination and total variation divergences as GANs trained on CIFAR-10, leading to competitive results and a simple solution to the problem of uniqueness of the optimal critic.",
        "conference": "ICML",
        "中文标题": "Moreau-Yosida f-散度",
        "摘要翻译": "f-散度的变分表示是许多机器学习算法的核心，最近受到关注的是Lipschitz约束的变种。受此启发，我们定义了关于Wasserstein-1度量的f-散度的Moreau-Yosida近似。相应的变分公式提供了一系列最新结果的推广、感兴趣的新特例以及硬Lipschitz约束的松弛。此外，我们证明了所谓的f-散度的紧变分表示可以在Lipschitz函数的商空间上取得，并给出了在变分表示中达到上确界的函数的特征。在实际应用方面，我们提出了一种算法来计算与自动微分框架兼容的f-散度的紧凸共轭。作为我们结果的应用，我们提出了Moreau-Yosida f-GAN，为Kullback-Leibler、反向Kullback-Leibler、χ²、反向χ²、平方Hellinger、Jensen-Shannon、Jeffreys、三角判别和总变分散度作为在CIFAR-10上训练的GAN提供了变分公式的实现，取得了竞争性的结果，并为最优评论家的唯一性问题提供了一个简单的解决方案。",
        "领域": "生成对抗网络",
        "问题": "如何有效地计算f-散度的紧凸共轭，并应用于生成对抗网络中以解决最优评论家的唯一性问题",
        "动机": "受到Lipschitz约束变种f-散度变分表示在机器学习算法中的重要性启发，研究Moreau-Yosida近似以推广现有结果并解决实际问题",
        "方法": "定义关于Wasserstein-1度量的f-散度的Moreau-Yosida近似，提出计算紧凸共轭的算法，并实现为Moreau-Yosida f-GAN",
        "关键词": [
            "Moreau-Yosida近似",
            "f-散度",
            "生成对抗网络",
            "Wasserstein度量",
            "自动微分"
        ],
        "涉及的技术概念": {
            "Moreau-Yosida近似": "用于近似f-散度，提供了一种推广现有结果和松弛约束的方法",
            "f-散度": "衡量两个概率分布之间差异的函数，是变分表示和算法设计的核心",
            "Wasserstein-1度量": "用于定义散度的近似，确保度量的性质和算法的可行性"
        },
        "success": true
    },
    {
        "order": 678,
        "title": "More Powerful and General Selective Inference for Stepwise Feature Selection using Homotopy Method",
        "html": "https://ICML.cc//virtual/2021/poster/8547",
        "abstract": "Conditional selective inference (SI) has been actively studied as a new statistical inference framework for data-driven hypotheses. The basic idea of conditional SI is to make inferences conditional on the selection event characterized by a set of linear and/or quadratic inequalities. Conditional SI has been mainly studied in the context of feature selection such as stepwise feature selection (SFS). The main limitation of the existing conditional SI methods is the loss of power due to over-conditioning, which is required for computational tractability. In this study, we develop a more powerful and general conditional SI method for SFS using the homotopy method which enables us to overcome this limitation. The homotopy-based SI is especially effective for more complicated feature selection algorithms. As an example, we develop a conditional SI method for forward-backward SFS with AIC-based stopping criteria and show that it is not adversely affected by the increased complexity of the algorithm. We conduct several experiments to demonstrate the effectiveness and efficiency of the proposed method.",
        "conference": "ICML",
        "中文标题": "使用同伦方法进行逐步特征选择的更强大和通用的选择性推断",
        "摘要翻译": "条件选择性推断（SI）作为一种新的统计推断框架，针对数据驱动的假设进行了积极研究。条件SI的基本思想是在由一组线性和/或二次不等式表征的选择事件条件下进行推断。条件SI主要在特征选择的背景下进行研究，如逐步特征选择（SFS）。现有条件SI方法的主要局限是由于过度条件化而导致的功效损失，这是为了计算的可处理性所必需的。在本研究中，我们开发了一种更强大和通用的条件SI方法，用于SFS，使用同伦方法使我们能够克服这一局限。基于同伦的SI对于更复杂的特征选择算法尤其有效。作为一个例子，我们开发了一种条件SI方法，用于基于AIC停止准则的前后向SFS，并表明它不会因算法复杂度的增加而受到不利影响。我们进行了几项实验，以证明所提出方法的有效性和效率。",
        "领域": "统计学习、特征选择、选择性推断",
        "问题": "解决现有条件选择性推断方法因过度条件化导致的功效损失问题",
        "动机": "开发一种更强大和通用的条件选择性推断方法，以克服现有方法在逐步特征选择中的局限",
        "方法": "使用同伦方法开发了一种新的条件选择性推断方法，特别适用于复杂的特征选择算法",
        "关键词": [
            "选择性推断",
            "同伦方法",
            "逐步特征选择",
            "AIC停止准则",
            "统计学习"
        ],
        "涉及的技术概念": {
            "条件选择性推断（SI）": "一种统计推断框架，针对数据驱动的假设，在特定选择事件条件下进行推断",
            "同伦方法": "用于开发新的条件选择性推断方法，克服现有方法的局限，特别适用于复杂算法",
            "逐步特征选择（SFS）": "一种特征选择方法，通过逐步添加或移除特征来优化模型，本研究中的方法针对其进行了优化"
        },
        "success": true
    },
    {
        "order": 679,
        "title": "MorphVAE: Generating Neural Morphologies from 3D-Walks using a Variational Autoencoder with Spherical Latent Space",
        "html": "https://ICML.cc//virtual/2021/poster/10059",
        "abstract": "For the past century, the anatomy of a neuron has been considered one of its defining features: The shape of a neuron's dendrites and axon fundamentally determines what other neurons it can connect to. These neurites have been described using mathematical tools e.g. in the context of cell type classification, but generative models of these structures have only rarely been proposed and are often computationally inefficient. Here we propose MorphVAE, a sequence-to-sequence variational autoencoder with spherical latent space as a generative model for neural morphologies. The model operates on walks within the tree structure of a neuron and can incorporate expert annotations on a subset of the data using semi-supervised learning. We develop our model on artificially generated toy data and evaluate its performance on dendrites of excitatory cells and axons of inhibitory cells of mouse motor cortex (M1) and dendrites of retinal ganglion cells. We show that the learned latent feature space allows for better cell type discrimination than other commonly used features. By sampling new walks from the latent space we can easily construct new morphologies with a specified degree of similarity to their reference neuron, providing an efficient generative model for neural morphologies. ",
        "conference": "ICML",
        "中文标题": "MorphVAE：利用具有球形潜在空间的变分自编码器从3D行走生成神经形态",
        "摘要翻译": "过去一个世纪以来，神经元的解剖结构一直被视为其定义性特征之一：神经元树突和轴突的形状从根本上决定了它可以与哪些其他神经元连接。这些神经突已被使用数学工具描述，例如在细胞类型分类的背景下，但这些结构的生成模型很少被提出，并且往往计算效率低下。在此，我们提出了MorphVAE，一种具有球形潜在空间的序列到序列变分自编码器，作为神经形态的生成模型。该模型操作于神经元树结构内的行走，并可以通过半监督学习在数据子集上纳入专家注释。我们在人工生成的玩具数据上开发我们的模型，并在小鼠运动皮层（M1）的兴奋性细胞树突和抑制性细胞轴突以及视网膜神经节细胞的树突上评估其性能。我们展示了学习到的潜在特征空间比其他常用特征能更好地进行细胞类型区分。通过从潜在空间采样新的行走，我们可以轻松构建具有与参考神经元特定程度相似性的新形态，为神经形态提供了一个高效的生成模型。",
        "领域": "神经形态学建模、变分自编码器、半监督学习",
        "问题": "如何高效生成具有特定形态特征的神经元模型",
        "动机": "解决现有神经元形态生成模型计算效率低下且难以整合专家知识的问题",
        "方法": "开发了一种基于球形潜在空间的序列到序列变分自编码器，结合半监督学习整合专家注释",
        "关键词": [
            "神经形态生成",
            "变分自编码器",
            "球形潜在空间",
            "半监督学习",
            "细胞类型分类"
        ],
        "涉及的技术概念": {
            "变分自编码器": "用于生成神经形态的深度学习模型，通过编码和解码过程学习数据的潜在表示",
            "球形潜在空间": "一种特殊的潜在空间结构，有助于模型学习更有效的特征表示",
            "半监督学习": "允许模型在训练过程中利用少量标注数据和大量未标注数据，提高模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 680,
        "title": "MOTS: Minimax Optimal Thompson Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/9037",
        "abstract": "Thompson sampling is one of the most widely used algorithms in many online decision problems due to its simplicity for implementation and superior empirical performance over other state-of-the-art methods. Despite its popularity and empirical success, it has remained an open problem whether Thompson sampling can achieve the minimax optimal regret O(\\sqrt{TK}) for K-armed bandit problems, where T is the total time horizon. In this paper we fill this long open gap by proposing a new Thompson sampling algorithm called MOTS that adaptively truncates the sampling result of the chosen arm at each time step. We prove that this simple variant of Thompson sampling achieves the minimax optimal regret bound O(\\sqrt{TK}) for finite time horizon T and also the asymptotic optimal regret bound when $T$ grows to infinity as well. This is the first time that the minimax optimality of multi-armed bandit problems has been attained by Thompson sampling type of algorithms.",
        "conference": "ICML",
        "中文标题": "MOTS：极小极大最优汤普森采样",
        "摘要翻译": "汤普森采样因其实现简单且在其他最先进方法之上具有优越的实证性能，在许多在线决策问题中成为最广泛使用的算法之一。尽管它广受欢迎且实证成功，但汤普森采样是否能够为K臂老虎机问题实现极小极大最优遗憾O(√TK)，其中T是总时间范围，一直是一个未解决的问题。在本文中，我们通过提出一种名为MOTS的新汤普森采样算法填补了这一长期存在的空白，该算法在每个时间步骤自适应地截断所选臂的采样结果。我们证明，这种汤普森采样的简单变体对于有限时间范围T实现了极小极大最优遗憾界O(√TK)，并且当T增长到无穷大时也实现了渐近最优遗憾界。这是首次通过汤普森采样类型的算法实现了多臂老虎机问题的极小极大最优性。",
        "领域": "在线学习、多臂老虎机问题、决策算法",
        "问题": "解决汤普森采样在多臂老虎机问题中是否能够实现极小极大最优遗憾的问题。",
        "动机": "尽管汤普森采样在实践中表现出色，但其在理论上的极小极大最优性尚未得到证明，本研究旨在填补这一理论空白。",
        "方法": "提出了一种名为MOTS的新汤普森采样算法，该算法通过自适应地截断所选臂的采样结果来实现极小极大最优遗憾。",
        "关键词": [
            "汤普森采样",
            "极小极大最优",
            "多臂老虎机",
            "在线决策",
            "遗憾界"
        ],
        "涉及的技术概念": {
            "汤普森采样": "一种基于贝叶斯理论的随机算法，用于在线决策问题中平衡探索与利用。",
            "极小极大最优遗憾": "在最坏情况下能够达到的最小最大遗憾，是评估算法性能的重要指标。",
            "多臂老虎机问题": "一种经典的决策理论问题，用于研究在不确定环境下如何做出最优选择以最大化累积奖励。"
        },
        "success": true
    },
    {
        "order": 681,
        "title": "MSA Transformer",
        "html": "https://ICML.cc//virtual/2021/poster/9905",
        "abstract": "Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.",
        "conference": "ICML",
        "中文标题": "MSA Transformer",
        "摘要翻译": "通过数百万种不同序列进行训练的无监督蛋白质语言模型学习了蛋白质的结构和功能。迄今为止研究的蛋白质语言模型已被训练为从单个序列进行推理。计算生物学中长期以来的方法是通过为每个家族独立拟合模型，从进化相关序列家族中进行推理。在这项工作中，我们结合了这两种范式。我们引入了一种蛋白质语言模型，该模型以多序列比对的形式输入一组序列。该模型在输入序列之间交替进行行和列的注意力处理，并通过在许多蛋白质家族中采用掩码语言建模目标的变体进行训练。该模型的性能远超当前最先进的无监督结构学习方法，且参数效率远高于先前最先进的蛋白质语言模型。",
        "领域": "蛋白质结构预测、生物信息学、深度学习在生物学中的应用",
        "问题": "如何从多序列比对中更有效地学习蛋白质的结构和功能",
        "动机": "结合单个序列推理和家族序列推理的优势，提高蛋白质结构预测的准确性和效率",
        "方法": "引入一种新型蛋白质语言模型，采用多序列比对作为输入，通过交替行和列注意力处理及掩码语言建模目标进行训练",
        "关键词": [
            "蛋白质语言模型",
            "多序列比对",
            "无监督学习",
            "结构预测",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "多序列比对": "作为模型的输入，提供进化相关的蛋白质序列信息，帮助模型学习更广泛的结构和功能特征",
            "注意力机制": "模型通过交替行和列的注意力处理，有效地捕捉序列间的复杂关系",
            "掩码语言建模": "一种训练目标，通过预测被掩盖的序列部分，使模型学习到更深层次的序列特征"
        },
        "success": true
    },
    {
        "order": 682,
        "title": "Muesli: Combining Improvements in Policy Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10769",
        "abstract": "We propose a novel policy update that combines regularized policy optimization with model learning as an auxiliary loss. The update (henceforth Muesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli does so without using deep search: it acts directly with a policy network and has computation speed comparable to model-free baselines. The Atari results are complemented by extensive ablations, and by additional results on continuous control and 9x9 Go.",
        "conference": "ICML",
        "中文标题": "Muesli：结合策略优化的改进方法",
        "摘要翻译": "我们提出了一种新颖的策略更新方法，该方法将正则化策略优化与作为辅助损失的模型学习相结合。这一更新（此后称为Muesli）在Atari上达到了与MuZero相媲美的顶尖性能。值得注意的是，Muesli在不使用深度搜索的情况下实现了这一点：它直接通过策略网络行动，并且计算速度与无模型基线相当。Atari的结果通过广泛的消融实验以及在连续控制和9x9围棋上的额外结果得到了补充。",
        "领域": "强化学习、策略优化、游戏AI",
        "问题": "如何在不需要深度搜索的情况下，通过结合正则化策略优化和模型学习，实现与顶尖算法相媲美的性能。",
        "动机": "探索一种更高效、计算成本更低的策略优化方法，以在不牺牲性能的前提下减少对深度搜索的依赖。",
        "方法": "提出了一种结合正则化策略优化和模型学习作为辅助损失的策略更新方法Muesli，该方法直接通过策略网络行动，避免了深度搜索的使用。",
        "关键词": [
            "策略优化",
            "模型学习",
            "强化学习",
            "Atari",
            "MuZero"
        ],
        "涉及的技术概念": {
            "正则化策略优化": "用于防止过拟合，提高策略的泛化能力。",
            "模型学习": "作为辅助损失，帮助策略网络更好地理解环境动态。",
            "策略网络": "直接用于决策，避免了深度搜索的高计算成本。"
        },
        "success": true
    },
    {
        "order": 683,
        "title": "Multi-Agent Training beyond Zero-Sum with Correlated Equilibrium Meta-Solvers",
        "html": "https://ICML.cc//virtual/2021/poster/10003",
        "abstract": "Two-player, constant-sum games are well studied in the literature, but there has been limited progress outside of this setting. We propose Joint Policy-Space Response Oracles (JPSRO), an algorithm for training agents in n-player, general-sum extensive form games, which provably converges to an equilibrium. We further suggest correlated equilibria (CE) as promising meta-solvers, and propose a novel solution concept Maximum Gini Correlated Equilibrium (MGCE), a principled and computationally efficient family of solutions for solving the correlated equilibrium selection problem. We conduct several experiments using CE meta-solvers for JPSRO and demonstrate convergence on n-player, general-sum games.",
        "conference": "ICML",
        "中文标题": "超越零和博弈的多智能体训练与相关均衡元求解器",
        "摘要翻译": "双人常和博弈在文献中已有深入研究，但在此设定之外的进展有限。我们提出了联合策略空间响应预言（JPSRO），一种用于训练n人一般和扩展形式博弈中智能体的算法，该算法可证明收敛至均衡。我们进一步提出相关均衡（CE）作为有前景的元求解器，并提出了一种新的解决方案概念——最大基尼相关均衡（MGCE），这是一个原则性强且计算效率高的解决方案家族，用于解决相关均衡选择问题。我们使用CE元求解器对JPSRO进行了多项实验，并在n人一般和博弈中展示了收敛性。",
        "领域": "多智能体系统、博弈论、机器学习",
        "问题": "在n人一般和扩展形式博弈中训练智能体并确保其收敛至均衡的问题",
        "动机": "研究动机是解决双人常和博弈之外的多智能体训练问题，特别是在n人一般和博弈中实现智能体的有效训练和均衡收敛。",
        "方法": "提出了联合策略空间响应预言（JPSRO）算法，并引入相关均衡（CE）作为元求解器，进一步提出了最大基尼相关均衡（MGCE）作为新的解决方案概念。",
        "关键词": [
            "多智能体训练",
            "相关均衡",
            "博弈论",
            "JPSRO",
            "MGCE"
        ],
        "涉及的技术概念": {
            "联合策略空间响应预言（JPSRO）": "一种用于训练n人一般和扩展形式博弈中智能体的算法，确保收敛至均衡。",
            "相关均衡（CE）": "作为元求解器，用于解决多智能体博弈中的均衡选择问题。",
            "最大基尼相关均衡（MGCE）": "一种新的解决方案概念，用于解决相关均衡选择问题，具有原则性强和计算效率高的特点。"
        },
        "success": true
    },
    {
        "order": 684,
        "title": "Multi-Dimensional Classification via Sparse Label Encoding",
        "html": "https://ICML.cc//virtual/2021/poster/10561",
        "abstract": "In multi-dimensional classification (MDC), there are multiple class variables in the output space with each of them corresponding to one heterogeneous class space. Due to the heterogeneity of class spaces, it is quite challenging to consider the dependencies among class variables when learning from MDC examples. In this paper, we propose a novel MDC approach named SLEM which learns the predictive model in an encoded label space instead of the original heterogeneous one. Specifically, SLEM works in an encoding-training-decoding framework. In the encoding phase, each class vector is mapped into a real-valued one via three cascaded operations including pairwise grouping, one-hot conversion and sparse linear encoding. In the training phase, a multi-output regression model is learned within the encoded label space. In the decoding phase, the predicted class vector is obtained by adapting orthogonal matching pursuit over outputs of the learned multi-output regression model. Experimental results clearly validate the superiority of SLEM against state-of-the-art MDC approaches.",
        "conference": "ICML",
        "中文标题": "通过稀疏标签编码的多维分类",
        "摘要翻译": "在多维分类（MDC）中，输出空间中有多个类别变量，每个变量对应一个异构的类别空间。由于类别空间的异构性，在学习MDC示例时考虑类别变量之间的依赖关系相当具有挑战性。本文提出了一种名为SLEM的新型MDC方法，该方法在编码的标签空间而非原始的异构空间中学习预测模型。具体而言，SLEM工作在编码-训练-解码的框架中。在编码阶段，每个类别向量通过包括成对分组、独热转换和稀疏线性编码在内的三个级联操作映射为一个实值向量。在训练阶段，在编码的标签空间内学习一个多输出回归模型。在解码阶段，通过在学习到的多输出回归模型的输出上适应正交匹配追踪来获得预测的类别向量。实验结果清楚地验证了SLEM相对于最先进的MDC方法的优越性。",
        "领域": "多维分类、稀疏编码、多输出回归",
        "问题": "解决在多维分类中由于类别空间的异构性导致的学习类别变量间依赖关系的挑战。",
        "动机": "为了更有效地在多维分类中考虑类别变量之间的依赖关系，提高分类性能。",
        "方法": "提出SLEM方法，通过在编码的标签空间学习预测模型，采用编码-训练-解码框架，包括稀疏线性编码和多输出回归模型。",
        "关键词": [
            "多维分类",
            "稀疏标签编码",
            "多输出回归",
            "正交匹配追踪",
            "类别空间异构性"
        ],
        "涉及的技术概念": {
            "稀疏线性编码": "用于将类别向量映射为实值向量，减少维度并保留重要信息。",
            "多输出回归模型": "在编码的标签空间内学习，用于预测多个类别变量的输出。",
            "正交匹配追踪": "在解码阶段使用，用于从多输出回归模型的输出中恢复预测的类别向量。"
        },
        "success": true
    },
    {
        "order": 685,
        "title": "Multidimensional Scaling: Approximation and Complexity",
        "html": "https://ICML.cc//virtual/2021/poster/9993",
        "abstract": " Metric Multidimensional scaling (MDS) is a classical method for generating\n    meaningful (non-linear) low-dimensional embeddings of high-dimensional data.\n    MDS has a long history in the statistics, machine learning, and graph drawing\n    communities. In particular, the Kamada-Kawai force-directed graph drawing method is equivalent to MDS and is one of the most popular ways in practice to embed graphs into low dimensions. Despite its ubiquity, our theoretical understanding of MDS remains limited as its objective function is highly non-convex. In this paper, we prove that minimizing the Kamada-Kawai objective is NP-hard and give a provable approximation algorithm for optimizing it, which in particular is a PTAS on low-diameter graphs. We supplement this result with experiments suggesting possible connections between our greedy approximation algorithm and gradient-based methods.\n",
        "conference": "ICML",
        "中文标题": "多维尺度分析：近似与复杂性",
        "摘要翻译": "度量多维尺度分析（MDS）是一种经典方法，用于生成高维数据的有意义（非线性）低维嵌入。MDS在统计学、机器学习和图绘制社区有着悠久的历史。特别是，Kamada-Kawai力导向图绘制方法等同于MDS，并且是实践中将图嵌入低维的最流行方法之一。尽管MDS无处不在，但我们对MDS的理论理解仍然有限，因为它的目标函数高度非凸。在本文中，我们证明了最小化Kamada-Kawai目标是NP难的，并给出了优化它的可证明近似算法，特别是在低直径图上是一个PTAS。我们通过实验补充了这一结果，表明我们的贪婪近似算法与基于梯度的方法之间可能存在联系。",
        "领域": "图嵌入技术、非凸优化、近似算法",
        "问题": "解决度量多维尺度分析（MDS）中目标函数高度非凸导致的优化困难问题",
        "动机": "提高对MDS的理论理解，并开发有效的优化方法",
        "方法": "证明了最小化Kamada-Kawai目标是NP难的，并提出了一种可证明的近似算法，特别是在低直径图上是一个PTAS",
        "关键词": [
            "多维尺度分析",
            "非凸优化",
            "近似算法",
            "图嵌入",
            "Kamada-Kawai方法"
        ],
        "涉及的技术概念": {
            "度量多维尺度分析（MDS）": "一种用于生成高维数据低维嵌入的经典方法，特别适用于图数据的可视化",
            "Kamada-Kawai方法": "一种力导向图绘制方法，等同于MDS，用于将图嵌入低维空间",
            "PTAS（多项式时间近似方案）": "一种近似算法，可以在多项式时间内找到问题的近似解，特别适用于低直径图"
        },
        "success": true
    },
    {
        "order": 686,
        "title": "Multi-group Agnostic PAC Learnability",
        "html": "https://ICML.cc//virtual/2021/poster/10535",
        "abstract": "An agnostic PAC learning algorithm finds a predictor that is competitive with the best predictor in a benchmark hypothesis class, where competitiveness is measured with respect to a given loss function. However, its predictions might be quite sub-optimal for structured subgroups of individuals, such as protected demographic groups. Motivated by such fairness concerns, we study ``multi-group agnostic PAC learnability'': fixing a measure of loss, a benchmark class $\\H$ and a (potentially) rich collection of subgroups $\\G$, the objective is \n to learn a single predictor such that the loss experienced by every group $g \\in \\G$ is not much larger than the best possible loss for this group within $\\H$.   Under natural conditions, we provide a characterization of the loss functions for which such a predictor is guaranteed to exist. For any such loss function we construct a learning algorithm whose sample complexity is logarithmic in the size of the collection $\\G$. Our results unify and extend previous positive and negative results from the multi-group fairness literature, which applied for specific loss functions.",
        "conference": "ICML",
        "success": true,
        "中文标题": "多组不可知PAC可学习性",
        "摘要翻译": "一个不可知PAC学习算法寻找一个预测器，该预测器在基准假设类中与最佳预测器竞争，其中竞争力是根据给定的损失函数来衡量的。然而，对于结构化子群，如受保护的人口群体，其预测可能相当不理想。出于这种公平性考虑，我们研究了“多组不可知PAC可学习性”：固定一个损失度量、一个基准类H和一个（可能）丰富的子群集合G，目标是学习一个单一的预测器，使得每个群组g∈G所经历的损失不比H中该群组的最佳可能损失大多少。在自然条件下，我们提供了对于这种预测器保证存在的损失函数的特征描述。对于任何这样的损失函数，我们构建了一个学习算法，其样本复杂度在集合G的大小上是对数的。我们的结果统一并扩展了多组公平性文献中先前适用于特定损失函数的正面和负面结果。",
        "领域": "机器学习公平性、算法学习理论、统计学习理论",
        "问题": "研究在多组公平性约束下，如何学习一个单一的预测器，使得所有子群的损失都不显著高于各自在基准类中的最佳可能损失。",
        "动机": "解决在机器学习模型中存在的对特定子群（如受保护的人口群体）预测不公的问题，确保模型对所有子群都具有良好的预测性能。",
        "方法": "通过理论分析确定保证存在满足多组公平性约束的预测器的损失函数条件，并构建样本复杂度与子群集合大小对数相关的学习算法。",
        "关键词": [
            "多组公平性",
            "不可知PAC学习",
            "损失函数",
            "样本复杂度",
            "学习算法"
        ],
        "涉及的技术概念": {
            "不可知PAC学习": "一种学习框架，旨在找到与基准假设类中最佳预测器竞争的预测器，而不需要对数据生成过程做出假设。",
            "多组公平性": "确保机器学习模型对所有定义的子群（如不同的人口群体）都具有公平的预测性能。",
            "损失函数": "用于衡量预测器性能的函数，本研究关注在特定损失函数下多组公平性约束的可满足性。"
        }
    },
    {
        "order": 687,
        "title": "Multi-layered Network Exploration via Random Walks: From Offline Optimization to Online Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8749",
        "abstract": "Multi-layered network exploration (MuLaNE) problem is an important problem abstracted from many applications. In MuLaNE, there are multiple network layers where each node has an importance weight and each layer is explored by a random walk. The MuLaNE task is to allocate total random walk budget $B$ into each network layer so that the total weights of the unique nodes visited by random walks are maximized. We systematically study this problem from offline optimization to online learning. For the offline optimization setting where the network structure and node weights are known, we provide greedy based constant-ratio approximation algorithms for overlapping networks, and greedy or dynamic-programming based optimal solutions for non-overlapping networks. For the online learning setting, neither the network structure nor the node weights are known initially. We adapt the combinatorial multi-armed bandit framework and design algorithms to learn random walk related parameters and node weights while optimizing the budget allocation in multiple rounds, and prove that they achieve logarithmic regret bounds. Finally, we conduct experiments on a real-world social network dataset to validate our theoretical results.  ",
        "conference": "ICML",
        "success": true,
        "中文标题": "通过随机游走进行多层网络探索：从离线优化到在线学习",
        "摘要翻译": "多层网络探索（MuLaNE）问题是从许多应用中抽象出来的一个重要问题。在MuLaNE中，存在多个网络层，其中每个节点都有一个重要性权重，每个层通过随机游走进行探索。MuLaNE任务是将总的随机游走预算B分配到每个网络层，以最大化随机游走访问的唯一节点的总权重。我们从离线优化到在线学习系统地研究了这个问题。对于网络结构和节点权重已知的离线优化设置，我们为重叠网络提供了基于贪心的常数比率近似算法，为非重叠网络提供了基于贪心或动态规划的最优解。对于在线学习设置，最初既不知道网络结构也不知道节点权重。我们适应了组合多臂老虎机框架，并设计了算法来学习随机游走相关参数和节点权重，同时在多轮中优化预算分配，并证明它们实现了对数遗憾界限。最后，我们在一个真实世界的社交网络数据集上进行了实验，以验证我们的理论结果。",
        "领域": "网络分析, 随机游走算法, 在线学习",
        "问题": "如何在多层网络中分配随机游走预算以最大化访问节点的总权重",
        "动机": "解决多层网络探索中预算分配的问题，以优化访问节点的总权重",
        "方法": "对于离线优化，使用贪心算法和动态规划；对于在线学习，采用组合多臂老虎机框架",
        "关键词": [
            "多层网络探索",
            "随机游走",
            "预算分配",
            "在线学习",
            "组合多臂老虎机"
        ],
        "涉及的技术概念": {
            "贪心算法": "在离线优化设置中用于为重叠网络提供常数比率近似解",
            "动态规划": "在离线优化设置中用于为非重叠网络提供最优解",
            "组合多臂老虎机": "在在线学习设置中用于学习参数和优化预算分配，实现对数遗憾界限"
        }
    },
    {
        "order": 688,
        "title": "Multiplicative Noise and Heavy Tails in Stochastic Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9187",
        "abstract": "Although stochastic optimization is central to modern machine learning, the precise mechanisms underlying its success, and in particular, the precise role of the stochasticity, still remain unclear. Modeling stochastic optimization algorithms as discrete random recurrence relations, we show that multiplicative noise, as it commonly arises due to variance in local rates of convergence, results in heavy-tailed stationary behaviour in the parameters. Theoretical results are obtained characterizing this for a large class of (non-linear and even non-convex) models and optimizers (including momentum, Adam, and stochastic Newton), demonstrating that this phenomenon holds generally. We describe dependence on key factors, including step size, batch size, and data variability, all of which exhibit similar qualitative behavior to recent empirical results on state-of-the-art neural network models. Furthermore, we empirically illustrate how multiplicative noise and heavy-tailed structure improve capacity for basin hopping and exploration of non-convex loss surfaces, over commonly-considered stochastic dynamics with only additive noise and light-tailed structure.",
        "conference": "ICML",
        "中文标题": "随机优化中的乘性噪声与重尾现象",
        "摘要翻译": "尽管随机优化在现代机器学习中占据核心地位，但其成功的精确机制，特别是随机性的确切作用，仍然不明确。通过将随机优化算法建模为离散随机递推关系，我们展示了乘性噪声（通常由于局部收敛速度的方差而产生）会导致参数中出现重尾稳态行为。我们获得了针对一大类（非线性和甚至非凸）模型和优化器（包括动量法、Adam和随机牛顿法）的理论结果，证明这一现象普遍存在。我们描述了关键因素的依赖性，包括步长、批量大小和数据变异性，所有这些因素都表现出与最新神经网络模型上的实证结果相似的定性行为。此外，我们通过实证说明了乘性噪声和重尾结构如何提高盆地跳跃能力和非凸损失表面的探索能力，相比于通常考虑的仅具有加性噪声和轻尾结构的随机动态。",
        "领域": "随机优化、深度学习优化、非凸优化",
        "问题": "揭示随机优化算法中乘性噪声导致参数重尾稳态行为的机制及其对优化过程的影响",
        "动机": "理解随机优化算法成功背后的精确机制，特别是随机性（乘性噪声）在优化过程中的作用",
        "方法": "将随机优化算法建模为离散随机递推关系，理论分析乘性噪声对参数稳态行为的影响，并通过实证研究验证理论结果",
        "关键词": [
            "乘性噪声",
            "重尾稳态",
            "随机优化",
            "非凸优化",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "乘性噪声": "在随机优化过程中，由于局部收敛速度的方差产生的噪声，导致参数出现重尾稳态行为",
            "重尾稳态": "参数分布具有重尾特性的稳态行为，影响优化算法的探索能力和收敛特性",
            "离散随机递推关系": "用于建模随机优化算法的数学框架，帮助理解算法动态和稳态行为"
        },
        "success": true
    },
    {
        "order": 689,
        "title": "Multiplying Matrices Without Multiplying",
        "html": "https://ICML.cc//virtual/2021/poster/10633",
        "abstract": "Multiplying matrices is among the most fundamental and most computationally demanding operations in machine learning and scientific computing. Consequently, the task of efficiently approximating matrix products has received significant attention.\n\nWe introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs 10x faster than alternatives at a given level of error, as well as 100x faster than exact matrix multiplication. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds.\n\nThese results suggest that a mixture of hashing, averaging, and byte shuffling—the core operations of our method—could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.",
        "conference": "ICML",
        "中文标题": "无需乘法的矩阵乘法",
        "摘要翻译": "矩阵乘法是机器学习和科学计算中最基本且计算需求最大的操作之一。因此，高效近似矩阵乘积的任务受到了广泛关注。我们为这一任务引入了一种基于学习的算法，其性能远超现有方法。使用来自不同领域的数百个矩阵进行的实验表明，在给定的误差水平下，它通常比替代方法快10倍，比精确矩阵乘法快100倍。在一个矩阵已知的常见情况下，我们的方法还具有一个有趣的特性，即它不需要任何乘加操作。这些结果表明，混合哈希、平均和字节洗牌——我们方法的核心操作——可能是一个比最近大量研究和硬件投资关注的稀疏化、因子化和/或标量量化矩阵乘积更有前途的机器学习构建块。",
        "领域": "机器学习优化、科学计算、算法加速",
        "问题": "如何高效近似矩阵乘积以减少计算需求",
        "动机": "矩阵乘法在机器学习和科学计算中计算需求大，需要更高效的近似方法",
        "方法": "基于学习的算法，结合哈希、平均和字节洗牌操作，无需乘加操作",
        "关键词": [
            "矩阵乘法近似",
            "学习算法",
            "计算加速",
            "哈希技术",
            "字节操作"
        ],
        "涉及的技术概念": {
            "基于学习的算法": "用于高效近似矩阵乘积，性能远超现有方法",
            "哈希技术": "作为方法的核心操作之一，用于加速矩阵处理",
            "字节洗牌": "作为方法的核心操作之一，用于优化矩阵数据的处理和存储"
        },
        "success": true
    },
    {
        "order": 690,
        "title": "Multi-Receiver Online Bayesian Persuasion",
        "html": "https://ICML.cc//virtual/2021/poster/9107",
        "abstract": "Bayesian persuasion studies how an informed sender should partially disclose information to influence the behavior of a self-interested receiver. Classical models make the stringent assumption that the sender knows the receiver’s utility. This can be relaxed by considering an online learning framework in which the sender repeatedly faces a receiver of an unknown, adversarially selected type. We study, for the first time, an online Bayesian persuasion setting with multiple receivers. We focus on the case with no externalities and binary actions, as customary in offline models. Our goal is to design no-regret algorithms for the sender with polynomial per-iteration running time. First, we prove a negative result: for any 0 < α ≤ 1, there is no polynomial-time no-α-regret algorithm when the sender’s utility function is supermodular or anonymous. Then, we focus on the setting of submodular sender’s utility functions and we show that, in this case, it is possible to design a polynomial-time no-(1-1/e)-regret algorithm. To do so, we introduce a general online gradient descent framework to handle online learning problems with a finite number of possible loss functions. This requires the existence of an approximate projection oracle. We show that, in our setting, there exists one such projection oracle which can be implemented in polynomial time.",
        "conference": "ICML",
        "中文标题": "多接收者在线贝叶斯说服",
        "摘要翻译": "贝叶斯说服研究了一个知情的发送者应如何部分披露信息以影响自利接收者的行为。经典模型做出了严格的假设，即发送者知道接收者的效用。通过考虑一个在线学习框架，可以放宽这一假设，在该框架中，发送者反复面对一个未知的、对抗性选择的类型的接收者。我们首次研究了具有多个接收者的在线贝叶斯说服设置。我们专注于没有外部性和二元动作的情况，这在离线模型中很常见。我们的目标是为发送者设计具有每次迭代多项式运行时间的无悔算法。首先，我们证明了一个负面结果：对于任何0 < α ≤ 1，当发送者的效用函数是超模或匿名时，不存在多项式时间的无α-悔算法。然后，我们专注于子模发送者效用函数的设置，并表明在这种情况下，可以设计一个多项式时间的无(1-1/e)-悔算法。为此，我们引入了一个通用的在线梯度下降框架来处理具有有限可能损失函数的在线学习问题。这需要一个近似投影预言机的存在。我们展示了，在我们的设置中，存在一个可以在多项式时间内实现的投影预言机。",
        "领域": "在线学习、博弈论、信息设计",
        "问题": "研究在多接收者环境下，发送者如何通过部分披露信息来影响接收者的行为，特别是在发送者不知道接收者效用函数的情况下。",
        "动机": "放宽经典贝叶斯说服模型中发送者需知道接收者效用函数的严格假设，探索在更现实的在线学习框架下的解决方案。",
        "方法": "引入在线梯度下降框架和近似投影预言机，设计多项式时间的无悔算法来处理具有有限可能损失函数的在线学习问题。",
        "关键词": [
            "在线贝叶斯说服",
            "无悔算法",
            "子模效用函数",
            "在线梯度下降",
            "近似投影预言机"
        ],
        "涉及的技术概念": {
            "在线贝叶斯说服": "研究在在线环境下，发送者如何通过部分披露信息来影响接收者的行为。",
            "无悔算法": "设计算法使得发送者在长期交互中的表现不逊于任何固定策略。",
            "子模效用函数": "一种特定的效用函数类型，在此研究中用于设计有效的在线学习算法。"
        },
        "success": true
    },
    {
        "order": 691,
        "title": "Multiscale Invertible Generative Networks for High-Dimensional Bayesian Inference",
        "html": "https://ICML.cc//virtual/2021/poster/8825",
        "abstract": "We propose a Multiscale Invertible Generative Network (MsIGN) and associated training algorithm that leverages multiscale structure to solve high-dimensional Bayesian inference. To address the curse of dimensionality, MsIGN exploits the low-dimensional nature of the posterior, and generates samples from coarse to fine scale (low to high dimension) by iteratively upsampling and refining samples. MsIGN is trained in a multi-stage manner to minimize the Jeffreys divergence, which avoids mode dropping in high-dimensional cases. On two high-dimensional Bayesian inverse problems, we show superior performance of MsIGN over previous approaches in posterior approximation and multiple mode capture. On the natural image synthesis task, MsIGN achieves superior performance in bits-per-dimension over baseline models and yields great interpret-ability of its neurons in intermediate layers.",
        "conference": "ICML",
        "中文标题": "多尺度可逆生成网络在高维贝叶斯推断中的应用",
        "摘要翻译": "我们提出了一种多尺度可逆生成网络（MsIGN）及其相关训练算法，该算法利用多尺度结构来解决高维贝叶斯推断问题。为了应对维度灾难，MsIGN利用后验分布的低维特性，通过从粗到细的尺度（从低维到高维）迭代上采样和细化样本来生成样本。MsIGN采用多阶段训练方式，以最小化Jeffreys散度，从而避免在高维情况下的模式丢失。在两个高维贝叶斯逆问题上，MsIGN在后验近似和多模式捕获方面显示出优于先前方法的性能。在自然图像合成任务中，MsIGN在每维度比特数上优于基线模型，并在中间层神经元中展现出极好的可解释性。",
        "领域": "生成模型、贝叶斯推断、图像合成",
        "问题": "解决高维贝叶斯推断中的维度灾难问题，提高后验近似和多模式捕获的性能",
        "动机": "高维贝叶斯推断面临维度灾难的挑战，需要一种能够有效处理高维数据并保持模式多样性的方法",
        "方法": "提出多尺度可逆生成网络（MsIGN），利用多尺度结构和多阶段训练，最小化Jeffreys散度，从粗到细生成样本",
        "关键词": [
            "多尺度可逆生成网络",
            "高维贝叶斯推断",
            "Jeffreys散度",
            "自然图像合成",
            "模式捕获"
        ],
        "涉及的技术概念": {
            "多尺度可逆生成网络（MsIGN）": "一种利用多尺度结构生成高维样本的网络，通过从粗到细的尺度迭代上采样和细化样本",
            "Jeffreys散度": "用于衡量两个概率分布之间的差异，MsIGN通过最小化Jeffreys散度来避免高维情况下的模式丢失",
            "模式捕获": "指模型能够识别和生成数据中的多种模式，MsIGN在后验近似和多模式捕获方面表现出色"
        },
        "success": true
    },
    {
        "order": 692,
        "title": "Multi-Task Reinforcement Learning with Context-based Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10101",
        "abstract": "https://drive.google.com/file/d/1lRV72XaKoxZjgQrLXBJhsM82x54_1Vc4/view?usp=sharing",
        "conference": "ICML",
        "中文标题": "基于上下文表示的多任务强化学习",
        "摘要翻译": "由于无法直接访问提供的摘要链接内容，无法完成摘要的翻译和结构化信息提取。请提供具体的摘要文本内容以便进行后续处理。",
        "领域": "多任务学习、强化学习、上下文表示学习",
        "问题": "如何有效地在多个任务之间共享知识以提高学习效率和性能",
        "动机": "探索在多任务强化学习中利用上下文表示来提高任务间的知识共享和学习效率",
        "方法": "采用基于上下文表示的方法来共享和转移多个任务之间的知识",
        "关键词": [
            "多任务学习",
            "强化学习",
            "上下文表示",
            "知识共享",
            "学习效率"
        ],
        "涉及的技术概念": {
            "多任务学习": "在同一模型中同时学习多个相关任务，以提高学习效率和性能",
            "强化学习": "通过与环境交互学习最优策略的机器学习方法",
            "上下文表示": "用于捕获和表示任务间共享知识的技术，有助于知识转移和共享"
        },
        "success": true
    },
    {
        "order": 693,
        "title": "MURAL: Meta-Learning Uncertainty-Aware Rewards for Outcome-Driven Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8943",
        "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. A common technique to make learning easier is providing demonstrations from a human supervisor, but such demonstrations can be expensive and time-consuming to acquire. In this work, we study a more tractable class of reinforcement learning problems defined simply by examples of successful outcome states, which can be much easier to provide while still making the exploration problem more tractable. In this problem setting, the reward function can be obtained automatically by training a classifier to categorize states as successful or not. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult using standard techniques for training deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood (NML) distribution, leveraging tools from meta-learning to make this distribution tractable. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions, while also providing more effective guidance towards the goal. We demonstrate that our algorithm solves a number of challenging navigation and robotic manipulation tasks which prove difficult or impossible for prior methods.",
        "conference": "ICML",
        "中文标题": "MURAL：面向结果驱动强化学习的元学习不确定性感知奖励",
        "摘要翻译": "在强化学习中，探索通常是一个具有挑战性的问题。为了使学习变得更容易，一种常见的技术是提供来自人类监督者的演示，但这样的演示可能既昂贵又耗时。在这项工作中，我们研究了一类更易处理的强化学习问题，这些问题仅通过成功结果状态的例子来定义，这些例子可以更容易提供，同时仍然使探索问题更易处理。在这个问题设置中，可以通过训练一个分类器将状态分类为成功与否来自动获得奖励函数。然而，正如我们将展示的，这要求分类器做出不确定性感知的预测，这在标准深度网络训练技术中非常困难。为了解决这个问题，我们提出了一种新颖的机制，基于一种摊销技术来计算归一化最大似然（NML）分布，利用元学习的工具使这个分布变得可处理。我们展示了所得算法与基于计数的探索方法和学习奖励函数的先前算法有许多有趣的联系，同时也提供了更有效的目标指导。我们证明了我们的算法解决了许多对于先前方法来说困难或不可能的导航和机器人操作任务。",
        "领域": "强化学习、机器人操作、导航",
        "问题": "如何在仅通过成功结果状态的例子定义的强化学习问题中，有效地进行探索并自动获得奖励函数。",
        "动机": "解决在强化学习中探索困难的问题，特别是在仅通过成功结果状态的例子定义的问题中，减少对人类监督者演示的依赖。",
        "方法": "提出了一种基于摊销技术计算归一化最大似然（NML）分布的新机制，利用元学习工具使分布可处理，从而获得校准的不确定性感知奖励函数。",
        "关键词": [
            "元学习",
            "不确定性感知",
            "强化学习",
            "机器人操作",
            "导航"
        ],
        "涉及的技术概念": {
            "归一化最大似然（NML）分布": "用于计算不确定性感知奖励的基础分布，通过元学习技术使其在强化学习中变得可处理。",
            "元学习": "用于使NML分布变得可处理的技术，通过学习如何学习来提高算法的效率和效果。",
            "不确定性感知预测": "分类器在预测状态是否成功时考虑预测的不确定性，这对于自动获得有效的奖励函数至关重要。"
        },
        "success": true
    },
    {
        "order": 694,
        "title": "Narrow Margins: Classification, Margins and Fat Tails",
        "html": "https://ICML.cc//virtual/2021/poster/8803",
        "abstract": "It is well-known that, for separable data, the regularised two-class logistic regression or support vector machine re-normalised estimate converges to the maximal margin classifier as the regularisation hyper-parameter $\\lambda$ goes to 0.\nThe fact that different loss functions may lead to the same solution is of theoretical and practical relevance as margin maximisation allows more straightforward considerations in terms of generalisation and geometric interpretation. \nWe investigate the case where this convergence property is not guaranteed to hold and show that it can be fully characterised by the distribution of error terms in the latent variable interpretation of linear classifiers.\nIn particular, if errors follow a regularly varying distribution, then the regularised and re-normalised estimate does not converge to the maximal margin classifier. This shows that classification with fat tails has a qualitatively different behaviour, which should be taken into account when considering real-life data.",
        "conference": "ICML",
        "中文标题": "狭窄的边际：分类、边际与厚尾",
        "摘要翻译": "众所周知，对于可分离数据，当正则化超参数λ趋近于0时，经过正则化的两类逻辑回归或支持向量机的重新归一化估计会收敛于最大边际分类器。不同的损失函数可能导致相同的解，这一事实在理论和实践上都具有重要意义，因为边际最大化允许更直接地考虑泛化和几何解释。我们研究了这种收敛性质不一定成立的情况，并表明它可以通过线性分类器的潜在变量解释中的误差项分布来完全表征。特别是，如果误差遵循一种规则变化的分布，那么正则化和重新归一化的估计不会收敛于最大边际分类器。这表明，厚尾分类具有质的不同行为，在考虑现实生活中的数据时应予以考虑。",
        "领域": "统计学习理论、支持向量机、逻辑回归",
        "问题": "研究在误差项遵循规则变化分布的情况下，正则化和重新归一化的估计不收敛于最大边际分类器的现象。",
        "动机": "探讨不同损失函数导致相同解的理论和实践意义，特别是在边际最大化对泛化和几何解释的影响方面。",
        "方法": "通过分析线性分类器的潜在变量解释中的误差项分布，研究正则化和重新归一化估计的收敛性质。",
        "关键词": [
            "最大边际分类器",
            "正则化",
            "厚尾分布",
            "逻辑回归",
            "支持向量机"
        ],
        "涉及的技术概念": {
            "最大边际分类器": "在可分离数据中，通过最大化边际来提高分类器的泛化能力。",
            "正则化": "通过引入正则化项来防止模型过拟合，调整模型的复杂度。",
            "厚尾分布": "指那些尾部比正态分布更厚的概率分布，意味着极端值出现的概率更高。"
        },
        "success": true
    },
    {
        "order": 695,
        "title": "Navigation Turing Test (NTT): Learning to Evaluate Human-Like Navigation",
        "html": "https://ICML.cc//virtual/2021/poster/10031",
        "abstract": "A key challenge on the path to developing agents that learn complex human-like behavior is the need to quickly and accurately quantify human-likeness. While human assessments of such behavior can be highly accurate, speed and scalability are limited. We address these limitations through a novel automated Navigation Turing Test (ANTT) that learns to predict human judgments of human-likeness. We demonstrate the effectiveness of our automated NTT on a navigation task in a complex 3D environment. We investigate six classification models to shed light on the types of architectures best suited to this task, and validate them against data collected through a human NTT. Our best models achieve high accuracy when distinguishing true human and agent behavior. At the same time, we show that predicting finer-grained human assessment of agents’ progress towards human-like behavior remains unsolved. Our work takes an important step towards agents that more effectively learn complex human-like behavior.",
        "conference": "ICML",
        "中文标题": "导航图灵测试(NTT)：学习评估类人导航",
        "摘要翻译": "在开发能够学习复杂类人行为的智能体道路上，一个关键挑战是需要快速且准确地量化类人程度。虽然人类对此类行为的评估可以非常准确，但其速度和可扩展性有限。我们通过一种新颖的自动化导航图灵测试(ANTT)来解决这些限制，该测试学习预测人类对类人程度的判断。我们在一个复杂3D环境中的导航任务上展示了我们自动化NTT的有效性。我们研究了六种分类模型，以揭示最适合此任务的架构类型，并通过人类NTT收集的数据对它们进行了验证。我们的最佳模型在区分真实人类行为和智能体行为时达到了高准确率。同时，我们表明，预测人类对智能体向类人行为进展的更细粒度评估仍然是一个未解决的问题。我们的工作朝着更有效学习复杂类人行为的智能体迈出了重要一步。",
        "领域": "智能体学习、类人行为评估、3D导航",
        "问题": "如何快速且准确地量化智能体行为的类人程度",
        "动机": "解决人类评估类人行为时速度和可扩展性有限的问题",
        "方法": "开发自动化导航图灵测试(ANTT)来预测人类对类人程度的判断，并在复杂3D环境中验证其有效性",
        "关键词": [
            "导航图灵测试",
            "类人行为评估",
            "3D导航",
            "智能体学习",
            "自动化评估"
        ],
        "涉及的技术概念": {
            "导航图灵测试(NTT)": "用于评估智能体导航行为类人程度的测试方法",
            "自动化导航图灵测试(ANTT)": "通过学习预测人类判断来自动评估智能体行为的类人程度",
            "分类模型": "用于区分人类和智能体行为的模型，帮助理解最适合评估类人行为的架构类型"
        },
        "success": true
    },
    {
        "order": 696,
        "title": "Near-Optimal Algorithms for Explainable k-Medians and k-Means",
        "html": "https://ICML.cc//virtual/2021/poster/9189",
        "abstract": "We consider the problem of explainable $k$-medians and $k$-means introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem, our goal is to find a \\emph{threshold decision tree} that partitions data into $k$ clusters and minimizes the $k$-medians or $k$-means objective. The obtained clustering is easy to interpret because every decision node of a threshold tree splits data based on a single feature into two groups. We propose a new algorithm for this problem which is $\\tilde O(\\log k)$ competitive with $k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means. This is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by Dasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{\\nicefrac{3}{2}} k)$  competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is near-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$ for $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for $k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians with $\\ell_2$ norm.",
        "conference": "ICML",
        "中文标题": "可解释k-中位数和k-均值问题的近最优算法",
        "摘要翻译": "我们考虑了Dasgupta、Frost、Moshkovitz和Rashtchian（ICML 2020）引入的可解释k-中位数和k-均值问题。在这个问题中，我们的目标是找到一个阈值决策树，将数据分成k个簇，并最小化k-中位数或k-均值的目标函数。由于阈值树的每个决策节点基于单一特征将数据分成两组，因此获得的聚类易于解释。我们针对这个问题提出了一种新算法，该算法在使用ℓ1范数的k-中位数问题上具有O(log k)的竞争比，在k-均值问题上具有O(k)的竞争比。这比Dasgupta等人（2020年）之前提出的O(k)和O(k²)的保证有所改进。我们还提供了一种新算法，对于使用ℓ2范数的k-中位数问题，具有O(log^(3/2) k)的竞争比。我们的第一个算法是接近最优的：Dasgupta等人（2020年）展示了k-中位数问题的Ω(log k)下界；在这项工作中，我们证明了k-均值问题的Ω(k)下界。我们还为使用ℓ2范数的k-中位数问题提供了Ω(log k)的下界。",
        "领域": "聚类分析、机器学习优化、解释性人工智能",
        "问题": "如何在保持聚类结果可解释性的同时，优化k-中位数和k-均值问题的性能",
        "动机": "提高聚类算法的解释性，同时优化其性能，以满足实际应用中对算法透明度和效率的双重需求",
        "方法": "提出了一种基于阈值决策树的新算法，优化了k-中位数和k-均值问题的竞争比，并提供了相应的下界证明",
        "关键词": [
            "可解释聚类",
            "k-中位数",
            "k-均值",
            "阈值决策树",
            "竞争比"
        ],
        "涉及的技术概念": {
            "阈值决策树": "用于将数据分成k个簇的决策树，每个决策节点基于单一特征进行分割，提高聚类的可解释性",
            "竞争比": "衡量算法性能与最优解之间差距的指标，用于评估算法的效率",
            "ℓ1和ℓ2范数": "用于k-中位数问题中的距离度量，ℓ1范数对应于曼哈顿距离，ℓ2范数对应于欧几里得距离"
        },
        "success": true
    },
    {
        "order": 697,
        "title": "Near-Optimal Confidence Sequences for Bounded Random Variables",
        "html": "https://ICML.cc//virtual/2021/poster/10463",
        "abstract": "Many inference problems, such as sequential decision problems like A/B testing, adaptive sampling schemes like bandit selection, are often online in nature. The fundamental problem for online inference is to provide a sequence of confidence intervals that are valid uniformly over the growing-into-infinity sample sizes. To address this question, we provide a near-optimal confidence sequence for bounded random variables by utilizing Bentkus' concentration results. We show that it improves on the existing approaches that use the Cram{\\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett inequalities. The resulting confidence sequence is confirmed to be favorable in synthetic coverage problems, adaptive stopping algorithms, and multi-armed bandit problems.",
        "conference": "ICML",
        "中文标题": "有界随机变量的近最优置信序列",
        "摘要翻译": "许多推理问题，如A/B测试等顺序决策问题、强盗选择等自适应抽样方案，本质上是在线的。在线推理的基本问题是提供一系列在样本量增长至无穷大时仍保持有效的置信区间。针对这一问题，我们利用Bentkus的集中结果，为有界随机变量提供了一个近最优的置信序列。我们证明，它改进了现有使用Cramér-Chernoff技术（如Hoeffding、Bernstein和Bennett不等式）的方法。在合成覆盖问题、自适应停止算法和多臂强盗问题中，所得到的置信序列被证实是有利的。",
        "领域": "统计学习理论、在线学习、多臂强盗问题",
        "问题": "为有界随机变量提供在样本量增长至无穷大时仍保持有效的置信区间序列",
        "动机": "改进现有基于Cramér-Chernoff技术的置信区间方法，以更有效地处理在线推理问题",
        "方法": "利用Bentkus的集中结果构建近最优置信序列，并与现有方法进行比较",
        "关键词": [
            "置信序列",
            "有界随机变量",
            "在线推理",
            "Bentkus集中",
            "多臂强盗问题"
        ],
        "涉及的技术概念": {
            "置信序列": "一种在样本量不断增长时仍保持统计有效性的置信区间序列，用于在线推理问题",
            "Bentkus集中": "一种概率不等式，用于控制有界随机变量的偏离程度，比传统的Cramér-Chernoff技术更紧",
            "多臂强盗问题": "一种顺序决策问题，需要在多个选项（臂）之间进行选择以最大化累积奖励，常用于自适应采样和在线学习场景"
        },
        "success": true
    },
    {
        "order": 698,
        "title": "Near-Optimal Entrywise Anomaly Detection for Low-Rank Matrices with Sub-Exponential Noise",
        "html": "https://ICML.cc//virtual/2021/poster/8813",
        "abstract": "We study the problem of identifying anomalies in a low-rank matrix observed with sub-exponential noise, motivated by applications in retail and inventory management. State of the art approaches to anomaly detection in low-rank matrices apparently fall short, since they require that non-anomalous entries be observed with vanishingly small noise (which is not the case in our problem, and indeed in many applications). So motivated, we propose a conceptually simple entrywise approach to anomaly detection in low-rank matrices. Our approach accommodates a general class of probabilistic anomaly models. We extend recent work on entrywise error guarantees for matrix completion, establishing such guarantees for sub-exponential matrices, where in addition to missing entries, a fraction of entries are corrupted by (an also unknown) anomaly model. Viewing the anomaly detection as a classification task, to the best of our knowledge, we are the first to achieve the min-max optimal detection rate (up to log factors). Using data from a massive consumer goods retailer, we show that our approach provides significant improvements over incumbent approaches to anomaly detection.",
        "conference": "ICML",
        "success": true,
        "中文标题": "低秩矩阵中次指数噪声的近最优逐项异常检测",
        "摘要翻译": "我们研究了在次指数噪声下观察到的低秩矩阵中识别异常的问题，这一问题受到零售和库存管理应用的启发。目前最先进的低秩矩阵异常检测方法显然存在不足，因为它们要求非异常条目在观察时的噪声极小（这在我们的问题中并非如此，实际上在许多应用中也非如此）。因此，我们提出了一种概念上简单的逐项方法来检测低秩矩阵中的异常。我们的方法适应了一类广泛的概率异常模型。我们扩展了最近关于矩阵补全逐项误差保证的工作，为次指数矩阵建立了这样的保证，其中除了缺失条目外，还有一部分条目被（同样未知的）异常模型所破坏。将异常检测视为分类任务，据我们所知，我们是第一个实现最小最大最优检测率（达到对数因子）的。通过使用来自一家大型消费品零售商的数据，我们展示了我们的方法在异常检测方面比现有方法有显著改进。",
        "领域": "异常检测, 低秩矩阵恢复, 零售数据分析",
        "问题": "在次指数噪声环境下，如何有效识别低秩矩阵中的异常条目。",
        "动机": "现有低秩矩阵异常检测方法要求噪声极小，不适用于实际应用中常见的次指数噪声环境。",
        "方法": "提出了一种逐项异常检测方法，适用于广泛的概率异常模型，并扩展了矩阵补全的逐项误差保证至次指数矩阵。",
        "关键词": [
            "异常检测",
            "低秩矩阵",
            "次指数噪声",
            "矩阵补全",
            "零售数据分析"
        ],
        "涉及的技术概念": {
            "次指数噪声": "描述数据中噪声的统计特性，比高斯噪声更广泛，适用于实际应用中的噪声模型。",
            "低秩矩阵恢复": "从部分观测或噪声数据中恢复低秩矩阵的技术，用于异常检测前的数据预处理。",
            "逐项异常检测": "直接对矩阵中的每个条目进行异常检测的方法，提高了检测的灵活性和准确性。"
        }
    },
    {
        "order": 699,
        "title": "Near-Optimal Linear Regression under Distribution Shift",
        "html": "https://ICML.cc//virtual/2021/poster/8821",
        "abstract": "Transfer learning is essential when sufficient data comes from the source domain, with scarce labeled data from the target domain. We develop estimators that achieve minimax linear risk for linear regression problems under distribution shift. Our algorithms cover different transfer learning settings including covariate shift and model shift. We also consider when data are generated from either linear or general nonlinear models. We show that linear minimax estimators are within an absolute constant of the minimax risk even among nonlinear estimators for various source/target distributions.",
        "conference": "ICML",
        "中文标题": "分布偏移下的近最优线性回归",
        "摘要翻译": "当源域有充足数据而目标域标记数据稀缺时，迁移学习变得至关重要。我们开发了在分布偏移下实现线性回归问题极小极大线性风险的估计器。我们的算法涵盖了包括协变量偏移和模型偏移在内的不同迁移学习设置。我们还考虑了数据是从线性或一般非线性模型生成的情况。我们证明，对于各种源/目标分布，线性极小极大估计器即使是在非线性估计器中，其风险也在极小极大风险的绝对常数范围内。",
        "领域": "迁移学习、线性回归、分布偏移",
        "问题": "在分布偏移条件下，如何实现线性回归问题的极小极大线性风险",
        "动机": "解决在源域数据充足而目标域标记数据稀缺的情况下，如何有效进行迁移学习的问题",
        "方法": "开发了适用于不同迁移学习设置的线性回归估计器，包括协变量偏移和模型偏移，并考虑了线性和非线性数据生成模型",
        "关键词": [
            "迁移学习",
            "线性回归",
            "分布偏移",
            "极小极大风险",
            "估计器"
        ],
        "涉及的技术概念": {
            "极小极大线性风险": "在分布偏移下，线性回归问题中估计器能够达到的最小最大风险",
            "协变量偏移": "源域和目标域的输入分布不同，但条件分布相同的情况",
            "模型偏移": "源域和目标域的条件分布不同的情况"
        },
        "success": true
    },
    {
        "order": 700,
        "title": "Near-Optimal Model-Free Reinforcement Learning in Non-Stationary Episodic MDPs",
        "html": "https://ICML.cc//virtual/2021/poster/8427",
        "abstract": "We consider model-free reinforcement learning (RL) in non-stationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds  (RestartQ-UCB), the first model-free algorithm for non-stationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of $\\widetilde{O}(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H T^{\\frac{2}{3}})$, where $S$ and $A$ are the numbers of states and actions, respectively, $\\Delta>0$ is the variation budget, $H$ is the number of time steps per episode, and $T$ is the total number of time steps. We further show that our algorithm is \\emph{nearly optimal} by establishing an information-theoretical lower bound of $\\Omega(S^{\\frac{1}{3}} A^{\\frac{1}{3}} \\Delta^{\\frac{1}{3}} H^{\\frac{2}{3}} T^{\\frac{2}{3}})$, the first lower bound in non-stationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We further demonstrate the power of our results in the context of multi-agent RL, where non-stationarity is a key challenge. ",
        "conference": "ICML",
        "中文标题": "非稳态片段MDP中近乎最优的无模型强化学习",
        "摘要翻译": "我们考虑了非稳态马尔可夫决策过程（MDP）中的无模型强化学习（RL）。只要奖励函数和状态转移函数的累积变化不超过一定的变化预算，它们就可以随时间任意变化。我们提出了带有上置信界的重启Q学习（RestartQ-UCB），这是第一个用于非稳态RL的无模型算法，并显示其在动态后悔方面优于现有解决方案。具体来说，带有Freedman型奖励项的RestartQ-UCB实现了动态后悔界为O~(S^(1/3) A^(1/3) Δ^(1/3) H T^(2/3))，其中S和A分别是状态和动作的数量，Δ>0是变化预算，H是每片段的时间步数，T是总时间步数。我们进一步通过建立一个信息理论下界Ω(S^(1/3) A^(1/3) Δ^(1/3) H^(2/3) T^(2/3))，首次在非稳态RL中建立了下界，表明我们的算法近乎最优。数值实验验证了RestartQ-UCB在累积奖励和计算效率方面的优势。我们进一步在多智能体RL的背景下展示了我们结果的力量，其中非稳态性是一个关键挑战。",
        "领域": "强化学习、非稳态马尔可夫决策过程、多智能体系统",
        "问题": "解决非稳态马尔可夫决策过程中的无模型强化学习问题，特别是在奖励函数和状态转移函数随时间变化的情况下。",
        "动机": "为了在奖励函数和状态转移函数随时间变化的非稳态环境中，开发一种能够实现近乎最优性能的无模型强化学习算法。",
        "方法": "提出了RestartQ-UCB算法，这是一种结合了重启策略和上置信界（UCB）的无模型强化学习方法，用于处理非稳态环境中的RL问题。",
        "关键词": [
            "非稳态强化学习",
            "无模型算法",
            "动态后悔",
            "重启Q学习",
            "上置信界"
        ],
        "涉及的技术概念": {
            "非稳态马尔可夫决策过程": "在RL中，奖励函数和状态转移函数可以随时间变化的环境模型。",
            "动态后悔": "用于衡量算法在非稳态环境中性能的指标，与最优策略相比的累积奖励差异。",
            "上置信界（UCB）": "一种用于平衡探索和利用的策略，通过为每个动作的估计奖励添加一个置信区间来实现。"
        },
        "success": true
    },
    {
        "order": 701,
        "title": "Near-Optimal Representation Learning for Linear Bandits and Linear RL",
        "html": "https://ICML.cc//virtual/2021/poster/8469",
        "abstract": "This paper studies representation learning for multi-task linear bandits and multi-task episodic RL with linear value function approximation. We first consider the setting where we play $M$ linear bandits with dimension $d$ concurrently, and these bandits share a common $k$-dimensional linear representation so that $k\\ll d$ and $k \\ll M$. We propose a sample-efficient algorithm, MTLR-OFUL, which leverages the shared representation to achieve $\\tilde{O}(M\\sqrt{dkT} + d\\sqrt{kMT} )$ regret, with $T$ being the number of total steps. Our regret significantly improves upon the baseline $\\tilde{O}(Md\\sqrt{T})$ achieved by solving each task independently. We further develop a lower bound that shows our regret is near-optimal when $d > M$. Furthermore, we extend the algorithm and analysis to multi-task episodic RL with linear value function approximation under low inherent Bellman error (Zanette et al., 2020a). To the best of our knowledge, this is the first theoretical result that characterize the benefits of multi-task representation learning for exploration in RL with function approximation.",
        "conference": "ICML",
        "success": true,
        "中文标题": "线性赌博机和线性强化学习中近乎最优的表示学习",
        "摘要翻译": "本文研究了多任务线性赌博机和具有线性价值函数近似的多任务阶段性强化学习中的表示学习。我们首先考虑同时玩$M$个维度为$d$的线性赌博机的设置，这些赌博机共享一个共同的$k$维线性表示，使得$k\\\\ll d$且$k \\\\ll M$。我们提出了一种样本高效的算法MTLR-OFUL，该算法利用共享表示实现了$\\\\tilde{O}(M\\\\sqrt{dkT} + d\\\\sqrt{kMT} )$的遗憾，其中$T$是总步数。我们的遗憾显著优于通过独立解决每个任务实现的基线$\\\\tilde{O}(Md\\\\sqrt{T})$。我们进一步开发了一个下界，表明当$d > M$时，我们的遗憾是近乎最优的。此外，我们将算法和分析扩展到低固有Bellman误差下的具有线性价值函数近似的多任务阶段性强化学习（Zanette等人，2020a）。据我们所知，这是第一个理论结果，描述了多任务表示学习在具有函数近似的强化学习探索中的好处。",
        "领域": "强化学习, 表示学习, 多任务学习",
        "问题": "如何在多任务线性赌博机和多任务阶段性强化学习中有效地学习共享表示以提高样本效率和减少遗憾。",
        "动机": "探索多任务表示学习在强化学习中的应用，特别是在具有线性价值函数近似的环境中，以提高学习效率和性能。",
        "方法": "提出了一种名为MTLR-OFUL的样本高效算法，该算法利用共享的线性表示来优化多任务学习过程，显著减少了遗憾，并扩展到多任务阶段性强化学习中。",
        "关键词": [
            "多任务学习",
            "表示学习",
            "线性赌博机",
            "强化学习",
            "样本效率"
        ],
        "涉及的技术概念": {
            "线性表示": "在多任务学习中共享的低维表示，用于提高学习效率和减少计算复杂度。",
            "MTLR-OFUL算法": "一种样本高效的算法，专门设计用于利用共享表示优化多任务线性赌博机和强化学习任务。"
        }
    },
    {
        "order": 702,
        "title": " Near Optimal Reward-Free Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8597",
        "abstract": "We study the reward-free reinforcement learning framework, which is particularly suitable for batch reinforcement learning and scenarios where one needs policies for multiple reward functions.\nThis framework has two phases: in the exploration phase, the agent collects trajectories by interacting with the environment without using any reward signal; in the planning phase, the agent needs to return a near-optimal policy for arbitrary reward functions.\n%This framework is suitable for batch RL setting and the setting where there are multiple reward functions of interes\nWe give a new efficient algorithm, \\textbf{S}taged \\textbf{S}ampling + \\textbf{T}runcated \\textbf{P}lanning (\\algoname), which interacts with the environment at most $O\\left( \\frac{S^2A}{\\epsilon^2}\\poly\\log\\left(\\frac{SAH}{\\epsilon}\\right) \\right)$ episodes in the exploration phase, and guarantees to output a near-optimal policy for arbitrary reward functions in the planning phase, where $S$ is the size of state space, $A$ is the size of action space, $H$ is the planning horizon, and $\\epsilon$ is the target accuracy relative to the total reward.\nNotably, our sample complexity scales only \\emph{logarithmically} with $H$, in contrast to all existing results which scale \\emph{polynomially} with $H$.\nFurthermore, this bound matches the minimax lower bound $\\Omega\\left(\\frac{S^2A}{\\epsilon^2}\\right)$ up to logarithmic factors.\n Our results rely on three new techniques : 1) A new sufficient condition for the dataset to plan for an $\\epsilon$-suboptimal policy\n%  for any totally bounded reward function\n  ; 2) A new way to plan efficiently under the proposed condition using soft-truncated planning; 3) Constructing extended MDP to maximize the truncated accumulative rewards efficiently.",
        "conference": "ICML",
        "中文标题": "接近最优的无奖励强化学习",
        "摘要翻译": "我们研究了无奖励强化学习框架，这一框架特别适用于批量强化学习以及需要为多个奖励函数制定策略的场景。该框架分为两个阶段：在探索阶段，智能体通过与环境交互收集轨迹，而不使用任何奖励信号；在规划阶段，智能体需要为任意奖励函数返回接近最优的策略。我们提出了一种新的高效算法——阶段采样+截断规划（SSTP），在探索阶段最多与环境交互O(S²A/ε²·poly log(SAH/ε))次，并在规划阶段保证为任意奖励函数输出接近最优的策略，其中S是状态空间的大小，A是动作空间的大小，H是规划视野，ε是相对于总奖励的目标精度。值得注意的是，我们的样本复杂度仅与H呈对数关系，而现有所有结果与H呈多项式关系。此外，这一界限与极小极大下界Ω(S²A/ε²)在对数因子内匹配。我们的结果依赖于三项新技术：1) 为ε次优策略规划数据集的新充分条件；2) 在提出的条件下使用软截断规划进行高效规划的新方法；3) 构建扩展MDP以高效最大化截断累积奖励。",
        "领域": "强化学习、批量学习、多奖励策略优化",
        "问题": "如何在无奖励信号的情况下，通过探索和规划两阶段，为任意奖励函数制定接近最优的策略。",
        "动机": "解决批量强化学习和需要为多个奖励函数制定策略的场景中的效率问题。",
        "方法": "提出阶段采样+截断规划（SSTP）算法，通过探索阶段收集数据，规划阶段输出策略，样本复杂度与规划视野呈对数关系。",
        "关键词": [
            "无奖励强化学习",
            "批量强化学习",
            "多奖励策略",
            "阶段采样",
            "截断规划"
        ],
        "涉及的技术概念": {
            "阶段采样": "在探索阶段高效收集数据的方法，减少与环境交互的次数。",
            "截断规划": "在规划阶段使用软截断技术，高效制定接近最优的策略。",
            "扩展MDP": "构建扩展马尔可夫决策过程，以最大化截断累积奖励。"
        },
        "success": true
    },
    {
        "order": 703,
        "title": "Necessary and sufficient conditions for causal feature selection in time series with latent common causes",
        "html": "https://ICML.cc//virtual/2021/poster/8441",
        "abstract": "We study the identification of direct and indirect causes on time series with latent variables, and provide a constrained-based causal feature selection method, which we prove that is both sound and complete under some graph constraints. Our theory and estimation algorithm require only two conditional independence tests for each observed candidate time series to determine whether or not it is a cause of an observed target time series. Furthermore, our selection of the conditioning set is such that it improves signal to noise ratio. We apply our method on real data, and on a wide range of simulated experiments, which yield very low false positive and relatively low false negative rates.\n",
        "conference": "ICML",
        "中文标题": "时间序列中潜在共同原因下因果特征选择的必要与充分条件",
        "摘要翻译": "我们研究了在含有潜在变量的时间序列上直接和间接原因的识别问题，并提出了一种基于约束的因果特征选择方法。我们证明，在某些图约束下，该方法既是可靠的也是完备的。我们的理论和估计算法仅需对每个观察到的候选时间序列进行两次条件独立性测试，以确定其是否为观察到的目标时间序列的原因。此外，我们的条件集选择方式能够提高信噪比。我们将我们的方法应用于真实数据以及广泛的模拟实验中，结果显示该方法具有非常低的假阳性率和相对较低的假阴性率。",
        "领域": "因果推断、时间序列分析、特征选择",
        "问题": "在含有潜在变量的时间序列中识别直接和间接原因，并进行因果特征选择。",
        "动机": "开发一种可靠且完备的方法，用于在存在潜在共同原因的情况下，从时间序列数据中选择因果特征。",
        "方法": "提出了一种基于约束的因果特征选择方法，该方法通过条件独立性测试来确定因果关系，并通过优化条件集的选择来提高信噪比。",
        "关键词": [
            "因果推断",
            "时间序列分析",
            "特征选择",
            "潜在变量",
            "条件独立性测试"
        ],
        "涉及的技术概念": {
            "基于约束的因果特征选择": "一种通过图约束和条件独立性测试来确定时间序列间因果关系的方法。",
            "条件独立性测试": "用于确定两个变量在给定其他变量条件下是否独立，是识别因果关系的关键步骤。",
            "信噪比优化": "通过精心选择条件集来提高因果特征选择的准确性，减少噪声干扰。"
        },
        "success": true
    },
    {
        "order": 704,
        "title": "Neighborhood Contrastive Learning Applied to Online Patient Monitoring",
        "html": "https://ICML.cc//virtual/2021/poster/9349",
        "abstract": "Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series.",
        "conference": "ICML",
        "中文标题": "应用于在线患者监测的邻域对比学习",
        "摘要翻译": "重症监护病房（ICU）越来越多地寻求机器学习方法，以提供对重症患者的在线监测。在机器学习中，在线监测通常被表述为一个监督学习问题。最近，对比学习方法在竞争性监督基准测试中显示出有希望的改进。这些方法依赖于为图像数据开发的、易于理解的数据增强技术，这些技术不适用于在线监测。在这项工作中，我们通过将时间序列数据增强技术与一种新颖的对比学习目标相结合来克服这一限制，我们称之为邻域对比学习（NCL）。我们的目标明确地将来自每个患者的连续时间段分组，同时保持特定状态的信息。我们的实验表明，与现有将对比方法应用于医疗时间序列的工作相比，有显著的改进。",
        "领域": "医疗时间序列分析、对比学习、在线监测",
        "问题": "如何有效地将对比学习方法应用于医疗时间序列数据的在线监测",
        "动机": "现有的对比学习方法依赖于为图像设计的数据增强技术，这些技术不适用于医疗时间序列数据，限制了在线监测的效果",
        "方法": "提出了一种名为邻域对比学习（NCL）的新方法，通过结合时间序列数据增强技术和新的对比学习目标，明确分组患者的连续时间段并保持状态特定信息",
        "关键词": [
            "医疗时间序列",
            "对比学习",
            "在线监测",
            "数据增强",
            "邻域对比学习"
        ],
        "涉及的技术概念": {
            "邻域对比学习（NCL）": "一种新颖的对比学习目标，旨在通过分组患者的连续时间段并保持状态特定信息，改进医疗时间序列的在线监测",
            "时间序列数据增强": "用于增加训练数据的多样性，提高模型泛化能力的技术，特别适用于医疗时间序列数据",
            "监督学习": "一种机器学习方法，通过使用标记数据训练模型，用于在线监测等任务"
        },
        "success": true
    },
    {
        "order": 705,
        "title": "NeRF-VAE: A Geometry Aware 3D Scene Generative Model",
        "html": "https://ICML.cc//virtual/2021/poster/9067",
        "abstract": "We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via Neural Radiance Fields (NeRF) and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene---without the need to re-train---using amortized inference. NeRF-VAE's explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments of synthetic scenes using very few input images.  We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE's decoder, which improves model performance.",
        "conference": "ICML",
        "中文标题": "NeRF-VAE：一种几何感知的3D场景生成模型",
        "摘要翻译": "我们提出了NeRF-VAE，一种通过神经辐射场（NeRF）和可微分体积渲染结合几何结构的3D场景生成模型。与NeRF相比，我们的模型考虑了场景间的共享结构，并能够通过摊销推理推断出新场景的结构，而无需重新训练。NeRF-VAE的显式3D渲染过程进一步与之前缺乏几何结构的基于卷积的渲染生成模型形成对比。我们的模型是一个变分自编码器（VAE），通过学习辐射场的分布，并将其条件化于潜在场景表示上。我们展示了一旦训练完成，NeRF-VAE能够从合成场景的先前未见过的3D环境中，使用非常少的输入图像推断并渲染几何一致的场景。我们进一步证明了NeRF-VAE在分布外相机上泛化良好，而卷积模型则不行。最后，我们介绍并研究了NeRF-VAE解码器的基于注意力的条件机制，这提高了模型的性能。",
        "领域": "3D场景生成、神经辐射场、变分自编码器",
        "问题": "如何生成几何结构一致的3D场景，并能够从少量输入图像中推断新场景的结构",
        "动机": "解决现有3D场景生成模型缺乏几何结构感知能力，以及需要大量数据和重新训练的问题",
        "方法": "结合神经辐射场（NeRF）和变分自编码器（VAE），通过可微分体积渲染和摊销推理学习辐射场的分布，并引入基于注意力的条件机制",
        "关键词": [
            "3D场景生成",
            "神经辐射场",
            "变分自编码器",
            "几何结构",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "神经辐射场（NeRF）": "用于建模3D场景的几何和外观，通过可微分体积渲染实现高质量的3D重建",
            "变分自编码器（VAE）": "学习辐射场的分布，并通过潜在场景表示进行条件化，实现场景的生成和推断",
            "注意力机制": "在解码器中引入，用于提高模型对场景重要部分的关注，从而提升生成质量"
        },
        "success": true
    },
    {
        "order": 706,
        "title": "Network Inference and Influence Maximization from Samples",
        "html": "https://ICML.cc//virtual/2021/poster/9453",
        "abstract": "Influence maximization is the task of selecting a small number of seed nodes in a social network to maximize the spread of the influence from these seeds, and it has been widely investigated in the past two decades. In the canonical setting, the whole social network as well as its diffusion parameters is given as input. In this paper, we consider the more realistic sampling setting where the network is unknown and we only have a set of passively observed cascades that record the set of activated nodes at each diffusion step. We study the task of influence maximization from these cascade samples (IMS), and present constant approximation algorithms for this task under mild conditions on the seed set distribution. To achieve the optimization goal, we also provide a novel solution to the network inference problem, that is, learning diffusion parameters and the network structure from the cascade data. Comparing with prior solutions, our network inference algorithm requires weaker assumptions and does not rely on maximum-likelihood estimation and convex programming. Our IMS algorithms enhance the learning-and-then-optimization approach by allowing a constant approximation ratio even when the diffusion parameters are hard to learn, and we do not need any assumption related to the network structure or diffusion parameters.",
        "conference": "ICML",
        "中文标题": "从样本中进行网络推断与影响力最大化",
        "摘要翻译": "影响力最大化的任务是在社交网络中选择一小部分种子节点，以最大化从这些种子节点出发的影响力传播，这一任务在过去二十年中被广泛研究。在标准设定中，整个社交网络及其扩散参数作为输入给出。在本文中，我们考虑了更为现实的采样设定，其中网络未知，仅有一组被动观察到的级联数据，记录了每个扩散步骤中被激活的节点集合。我们研究了从这些级联样本中进行影响力最大化（IMS）的任务，并在种子节点分布温和的条件下，为此任务提出了常数近似算法。为了实现优化目标，我们还为网络推断问题提供了一个新颖的解决方案，即从级联数据中学习扩散参数和网络结构。与先前的解决方案相比，我们的网络推断算法需要更弱的假设，并且不依赖于最大似然估计和凸规划。我们的IMS算法通过学习然后优化的方法增强了学习过程，即使在扩散参数难以学习的情况下也允许常数近似比，并且我们不需要任何与网络结构或扩散参数相关的假设。",
        "领域": "社交网络分析、影响力最大化、网络推断",
        "问题": "在未知社交网络结构的情况下，如何通过观察到的级联数据实现影响力最大化。",
        "动机": "研究在更现实的采样设定下，即网络结构未知，仅通过观察到的级联数据，如何有效地选择种子节点以最大化影响力传播。",
        "方法": "提出了一种新颖的网络推断算法，用于从级联数据中学习扩散参数和网络结构，并在此基础上设计了常数近似算法进行影响力最大化。",
        "关键词": [
            "影响力最大化",
            "网络推断",
            "级联数据",
            "社交网络分析",
            "近似算法"
        ],
        "涉及的技术概念": {
            "影响力最大化": "在社交网络中选择种子节点以最大化影响力传播的任务。",
            "网络推断": "从观察到的数据中学习网络结构和扩散参数的过程。",
            "级联数据": "记录每个扩散步骤中被激活节点集合的数据，用于分析影响力传播过程。"
        },
        "success": true
    },
    {
        "order": 707,
        "title": "Neural Architecture Search without Training",
        "html": "https://ICML.cc//virtual/2021/poster/9263",
        "abstract": "The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NAS algorithms tend to be slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be alleviated if we could partially predict a network's trained accuracy from its initial state. In this work, we examine the overlap of activations between datapoints in untrained networks and motivate how this can give a measure which is usefully indicative of a network’s trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201, NATS-Bench, and Network Design Spaces. Our approach can be readily combined with more expensive search methods; we examine a simple adaptation of regularised evolutionary search. Code for reproducing our experiments is available at https://github.com/BayesWatch/nas-without-training.",
        "conference": "ICML",
        "中文标题": "无需训练的神经架构搜索",
        "摘要翻译": "手工设计深度神经网络所需的时间和精力是巨大的。这促使了神经架构搜索（NAS）技术的发展，以自动化这一设计过程。然而，NAS算法往往速度慢且成本高；它们需要训练大量的候选网络来指导搜索过程。如果我们能够从网络的初始状态部分预测其训练后的准确性，这一问题可以得到缓解。在这项工作中，我们研究了未训练网络中数据点之间激活的重叠，并探讨了如何利用这一点来提供一个对网络训练性能有指示意义的度量。我们将这一度量融入到一个简单的算法中，使我们能够在单个GPU上几秒钟内无需任何训练即可搜索到强大的网络，并在NAS-Bench-101、NAS-Bench-201、NATS-Bench和网络设计空间上验证了其有效性。我们的方法可以很容易地与更昂贵的搜索方法结合；我们研究了一种简单的正则化进化搜索的适应性。重现我们实验的代码可在https://github.com/BayesWatch/nas-without-training获取。",
        "领域": "神经架构搜索、自动化机器学习、深度学习优化",
        "问题": "如何减少神经架构搜索过程中的计算资源和时间消耗",
        "动机": "为了克服传统神经架构搜索方法需要大量计算资源和时间训练候选网络的问题，探索从网络初始状态预测其性能的可能性",
        "方法": "通过分析未训练网络中数据点激活的重叠来预测网络性能，并开发了一种无需训练即可快速搜索有效网络架构的算法",
        "关键词": [
            "神经架构搜索",
            "无需训练",
            "性能预测",
            "自动化设计",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "神经架构搜索（NAS）": "自动化设计深度神经网络架构的技术，旨在减少人工设计的负担",
            "激活重叠": "用于衡量未训练网络中不同数据点激活模式相似性的指标，作为预测网络性能的依据",
            "正则化进化搜索": "一种结合进化算法和正则化技术的搜索方法，用于优化神经架构搜索过程"
        },
        "success": true
    },
    {
        "order": 708,
        "title": "Neural Feature Matching in Implicit 3D Representations",
        "html": "https://ICML.cc//virtual/2021/poster/9921",
        "abstract": "Recently, neural implicit functions have achieved impressive results for encoding 3D shapes. Conditioning on low-dimensional latent codes generalises a single implicit function to learn shared representation space for a variety of shapes, with the advantage of smooth interpolation. While the benefits from the global latent space do not correspond to explicit points at local level, we propose to track the continuous point trajectory by matching implicit features with the latent code interpolating between shapes, from which we corroborate the hierarchical functionality of the deep implicit functions, where early layers map the latent code to fitting the coarse shape structure, and deeper layers further refine the shape details. Furthermore, the structured representation space of implicit functions enables to apply feature matching for shape deformation, with the benefits to handle topology and semantics inconsistency, such as from an armchair to a chair with no arms, without explicit flow functions or manual annotations.",
        "conference": "ICML",
        "中文标题": "隐式3D表示中的神经特征匹配",
        "摘要翻译": "最近，神经隐式函数在编码3D形状方面取得了令人印象深刻的成果。通过基于低维潜在代码的条件化，单个隐式函数能够泛化学习多种形状的共享表示空间，具有平滑插值的优势。虽然全局潜在空间的好处并不对应于局部层面的显式点，但我们提出通过匹配隐式特征与形状间插值的潜在代码来跟踪连续点轨迹，从而证实了深度隐式函数的分层功能，其中早期层将潜在代码映射到拟合粗形状结构，而更深层进一步细化形状细节。此外，隐式函数的结构化表示空间使得能够应用特征匹配进行形状变形，具有处理拓扑和语义不一致性的优势，例如从扶手椅到无扶手椅的变形，无需显式流函数或手动注释。",
        "领域": "3D形状重建, 隐式函数学习, 形状变形",
        "问题": "如何在隐式3D表示中实现有效的特征匹配以支持形状变形和插值",
        "动机": "探索深度隐式函数的分层功能及其在形状变形中的应用，解决拓扑和语义不一致性问题",
        "方法": "通过匹配隐式特征与插值潜在代码跟踪点轨迹，利用深度隐式函数的分层结构进行形状的粗到细重建和变形",
        "关键词": [
            "神经隐式函数",
            "特征匹配",
            "形状变形",
            "3D重建",
            "潜在空间插值"
        ],
        "涉及的技术概念": {
            "神经隐式函数": "用于编码3D形状的连续表示，能够通过潜在代码泛化学习多种形状",
            "潜在空间插值": "在潜在代码间进行平滑过渡，实现形状的平滑变形和插值",
            "分层功能": "深度隐式函数的分层结构，早期层处理粗形状结构，深层细化细节，支持从粗到细的形状重建和变形"
        },
        "success": true
    },
    {
        "order": 709,
        "title": "Neural Pharmacodynamic State Space Modeling",
        "html": "https://ICML.cc//virtual/2021/poster/9577",
        "abstract": "Modeling the time-series of high-dimensional, longitudinal data is important for predicting patient disease progression. However, existing neural network based approaches that learn representations of patient state, while very flexible, are susceptible to overfitting. We propose a deep generative model that makes use of a novel attention-based neural architecture inspired by the physics of how treatments affect disease state. The result is a scalable and accurate model of high-dimensional patient biomarkers as they vary over time. Our proposed model yields significant improvements in generalization and, on real-world clinical data, provides interpretable insights into the dynamics of cancer progression.",
        "conference": "ICML",
        "中文标题": "神经药效动力学状态空间建模",
        "摘要翻译": "对高维纵向数据的时间序列进行建模对于预测患者疾病进展非常重要。然而，现有的基于神经网络的学习患者状态表示的方法虽然非常灵活，但容易过拟合。我们提出了一种深度生成模型，该模型利用了一种新颖的基于注意力的神经架构，其灵感来源于治疗如何影响疾病状态的物理学原理。结果是一个可扩展且准确的高维患者生物标志物随时间变化的模型。我们提出的模型在泛化能力上取得了显著改进，并在真实世界的临床数据上，为癌症进展的动态提供了可解释的见解。",
        "领域": "医疗影像分析、时间序列预测、深度学习在医疗中的应用",
        "问题": "如何准确建模高维纵向数据的时间序列以预测患者疾病进展，同时避免过拟合。",
        "动机": "为了解决现有神经网络方法在建模患者状态时容易过拟合的问题，并提高模型的泛化能力和解释性。",
        "方法": "提出了一种基于注意力的深度生成模型，该模型受到治疗影响疾病状态的物理学原理启发，用于建模高维患者生物标志物的时间变化。",
        "关键词": [
            "深度生成模型",
            "注意力机制",
            "时间序列预测",
            "医疗数据分析",
            "癌症进展"
        ],
        "涉及的技术概念": {
            "深度生成模型": "用于建模高维患者生物标志物的时间变化，提高预测的准确性和泛化能力。",
            "注意力机制": "受到治疗影响疾病状态的物理学原理启发，用于增强模型对重要时间点的关注。",
            "时间序列预测": "核心任务，旨在准确预测患者疾病进展，为临床决策提供支持。"
        },
        "success": true
    },
    {
        "order": 710,
        "title": "Neural-Pull: Learning Signed Distance Function from Point clouds by Learning to Pull Space onto Surface",
        "html": "https://ICML.cc//virtual/2021/poster/9085",
        "abstract": "Reconstructing continuous surfaces from 3D point clouds is a fundamental operation in 3D geometry processing. Several recent state-of-the-art methods address this problem using neural networks to learn signed distance functions (SDFs). In this paper, we introduce Neural-Pull, a new approach that is simple and leads to high quality SDFs. Specifically, we train a neural network to pull query 3D locations to their closest points on the surface using the predicted signed distance values and the gradient at the query locations, both of which are computed by the network itself. The pulling operation moves each query location with a stride given by the distance predicted by the network. Based on the sign of the distance, this may move the query location along or against the direction of the gradient of the SDF. This is a differentiable operation that allows us to update the signed distance value and the gradient simultaneously during training. Our outperforming results under widely used benchmarks demonstrate that we can learn SDFs more accurately and flexibly for surface reconstruction and single image reconstruction than the state-of-the-art methods. Our code and data are available at https://github.com/mabaorui/NeuralPull.",
        "conference": "ICML",
        "中文标题": "神经拉取：通过学习将空间拉取到表面来从点云中学习有符号距离函数",
        "摘要翻译": "从3D点云重建连续表面是3D几何处理中的基本操作。最近几种最先进的方法通过使用神经网络学习有符号距离函数（SDFs）来解决这一问题。在本文中，我们介绍了Neural-Pull，这是一种简单且能产生高质量SDFs的新方法。具体来说，我们训练一个神经网络，利用预测的有符号距离值和查询位置的梯度（这两者均由网络本身计算）将查询的3D位置拉取到表面上最近的点。拉取操作根据网络预测的距离，以一定的步长移动每个查询位置。根据距离的符号，这可能使查询位置沿着或逆着SDF的梯度方向移动。这是一个可微分的操作，允许我们在训练过程中同时更新有符号距离值和梯度。我们在广泛使用的基准测试中的优异结果表明，与最先进的方法相比，我们能够更准确和灵活地学习SDFs，用于表面重建和单图像重建。我们的代码和数据可在https://github.com/mabaorui/NeuralPull获取。",
        "领域": "3D几何处理, 表面重建, 点云处理",
        "问题": "如何从3D点云中更准确和灵活地重建连续表面",
        "动机": "现有的通过神经网络学习有符号距离函数的方法在表面重建方面仍有提升空间，需要一种更简单且能产生高质量结果的新方法",
        "方法": "提出Neural-Pull方法，通过训练神经网络利用预测的有符号距离值和梯度将查询的3D位置拉取到表面上最近的点，实现表面重建",
        "关键词": [
            "有符号距离函数",
            "表面重建",
            "点云处理",
            "神经网络",
            "3D几何处理"
        ],
        "涉及的技术概念": {
            "有符号距离函数": "用于表示点到表面的距离及其相对于表面的位置（内部或外部）的函数，是表面重建中的关键技术",
            "梯度": "在有符号距离函数中，梯度表示距离变化最快的方向，用于指导查询位置的移动方向",
            "可微分操作": "允许在训练过程中通过梯度下降法优化网络参数，是实现端到端学习的关键"
        },
        "success": true
    },
    {
        "order": 711,
        "title": "Neural Rough Differential Equations for Long Time Series",
        "html": "https://ICML.cc//virtual/2021/poster/10039",
        "abstract": "Neural controlled differential equations (CDEs) are the continuous-time analogue of recurrent neural networks, as Neural ODEs are to residual networks, and offer a memory-efficient continuous-time way to model functions of potentially irregular time series. Existing methods for computing the forward pass of a Neural CDE involve embedding the incoming time series into path space, often via interpolation, and using evaluations of this path to drive the hidden state. Here, we use rough path theory to extend this formulation. Instead of directly embedding into path space, we instead represent the input signal over small time intervals through its \\textit{log-signature}, which are statistics describing how the signal drives a CDE. This is the approach for solving \\textit{rough differential equations} (RDEs), and correspondingly we describe our main contribution as the introduction of Neural RDEs. This extension has a purpose: by generalising the Neural CDE approach to a broader class of driving signals, we demonstrate particular advantages for tackling long time series. In this regime, we demonstrate efficacy on problems of length up to 17k observations and observe significant training speed-ups, improvements in model performance, and reduced memory requirements compared to existing approaches.",
        "conference": "ICML",
        "中文标题": "神经粗糙微分方程在长时间序列中的应用",
        "摘要翻译": "神经控制微分方程（CDEs）是循环神经网络在连续时间上的类比，正如神经ODE对应于残差网络一样，它们提供了一种内存效率高的连续时间方法来建模潜在不规则时间序列的函数。现有的计算神经CDE前向传递的方法通常通过插值将输入时间序列嵌入到路径空间，并使用该路径的评估来驱动隐藏状态。在这里，我们利用粗糙路径理论来扩展这一表述。我们不是直接将输入信号嵌入到路径空间，而是通过其'对数签名'在小时间间隔内表示输入信号，这些统计量描述了信号如何驱动CDE。这是解决'粗糙微分方程'（RDEs）的方法，相应地，我们将我们的主要贡献描述为神经RDEs的引入。这一扩展有其目的：通过将神经CDE方法推广到更广泛的驱动信号类别，我们展示了在解决长时间序列问题时的特定优势。在这一领域，我们展示了在长达17k观测值的问题上的有效性，并观察到与现有方法相比，训练速度显著加快，模型性能提升，内存需求减少。",
        "领域": "时间序列分析、深度学习、微分方程建模",
        "问题": "如何高效且内存友好地建模长时间序列数据",
        "动机": "为了解决长时间序列建模中的内存和计算效率问题，以及提高模型性能",
        "方法": "利用粗糙路径理论，通过输入信号的'对数签名'在小时间间隔内表示信号，引入神经粗糙微分方程（RDEs）来扩展神经控制微分方程（CDEs）的方法",
        "关键词": [
            "神经粗糙微分方程",
            "长时间序列",
            "粗糙路径理论",
            "对数签名",
            "内存效率"
        ],
        "涉及的技术概念": {
            "神经控制微分方程（CDEs）": "用于建模不规则时间序列的连续时间方法，类比于循环神经网络",
            "粗糙路径理论": "用于扩展神经CDE方法，允许处理更广泛的驱动信号类别",
            "对数签名": "描述信号如何驱动微分方程的统计量，用于在小时间间隔内表示输入信号"
        },
        "success": true
    },
    {
        "order": 712,
        "title": "Neural SDEs as Infinite-Dimensional GANs",
        "html": "https://ICML.cc//virtual/2021/poster/10277",
        "abstract": "Stochastic differential equations (SDEs) are a staple of mathematical modelling of temporal dynamics. However, a fundamental limitation has been that such models have typically been relatively inflexible, which recent work introducing Neural SDEs has sought to solve. Here, we show that the current classical approach to fitting SDEs may be approached as a special case of (Wasserstein) GANs, and in doing so the neural and classical regimes may be brought together. The input noise is Brownian motion, the output samples are time-evolving paths produced by a numerical solver, and by parameterising a discriminator as a Neural Controlled Differential Equation (CDE), we obtain Neural SDEs as (in modern machine learning parlance) continuous-time generative time series models. Unlike previous work on this problem, this is a direct extension of the classical approach without reference to either prespecified statistics or density functions. Arbitrary drift and diffusions are admissible, so as the Wasserstein loss has a unique global minima, in the infinite data limit \\textit{any} SDE may be learnt.",
        "conference": "ICML",
        "中文标题": "神经随机微分方程作为无限维生成对抗网络",
        "摘要翻译": "随机微分方程（SDEs）是时间动态数学建模的主要工具。然而，一个基本的限制是这类模型通常相对不够灵活，最近引入神经随机微分方程的工作试图解决这一问题。在这里，我们展示了当前拟合SDEs的经典方法可以被视为（Wasserstein）生成对抗网络（GANs）的一个特例，通过这样做，神经和经典体系可以被结合起来。输入噪声是布朗运动，输出样本是由数值求解器产生的时间演化路径，并且通过将判别器参数化为神经控制微分方程（CDE），我们得到了神经SDEs作为（在现代机器学习术语中）连续时间生成时间序列模型。与之前关于这个问题的研究不同，这是对经典方法的直接扩展，无需参考预设的统计量或密度函数。允许任意的漂移和扩散，因此当Wasserstein损失具有唯一的全局最小值时，在无限数据限制下，可以学习任何SDE。",
        "领域": "时间序列生成、生成对抗网络、随机微分方程",
        "问题": "解决随机微分方程模型在时间动态建模中的灵活性不足问题",
        "动机": "将神经随机微分方程与生成对抗网络结合，以提高模型在时间序列生成中的灵活性和表达能力",
        "方法": "将经典随机微分方程拟合方法视为Wasserstein生成对抗网络的特例，通过神经控制微分方程参数化判别器，实现神经随机微分方程作为连续时间生成时间序列模型",
        "关键词": [
            "神经随机微分方程",
            "生成对抗网络",
            "时间序列生成",
            "Wasserstein损失",
            "神经控制微分方程"
        ],
        "涉及的技术概念": {
            "神经随机微分方程": "在论文中用于提高时间动态建模的灵活性和表达能力",
            "生成对抗网络": "用于将神经和经典随机微分方程拟合方法结合起来，提高模型的生成能力",
            "Wasserstein损失": "用于优化模型训练，确保在无限数据限制下可以学习任何随机微分方程"
        },
        "success": true
    },
    {
        "order": 713,
        "title": "Neural Symbolic Regression that scales",
        "html": "https://ICML.cc//virtual/2021/poster/9713",
        "abstract": "Symbolic equations are at the core of scientific discovery.\r\nThe task of discovering the underlying equation from a set of input-output pairs is called symbolic regression. Traditionally, symbolic regression methods use hand-designed strategies that do not improve with experience.\r\nIn this paper, we introduce the first symbolic regression method that leverages large scale pre-training. We procedurally generate an unbounded set of equations, and simultaneously pre-train a Transformer to predict the symbolic equation from a corresponding set of input-output-pairs. At test time, we query the model on a new set of points and use its output to guide the search for the equation.\r\nWe show empirically that this approach can re-discover a set of well-known physical equations, and that it improves over time with more data and compute.",
        "conference": "ICML",
        "中文标题": "可扩展的神经符号回归",
        "摘要翻译": "符号方程是科学发现的核心。从一组输入-输出对中发现底层方程的任务被称为符号回归。传统上，符号回归方法使用手工设计的策略，这些策略不会随着经验的积累而改进。在本文中，我们介绍了第一种利用大规模预训练的符号回归方法。我们程序化生成了一个无界的方程集，并同时预训练一个Transformer模型，以从相应的输入-输出对集中预测符号方程。在测试时，我们在新的点上查询模型，并使用其输出来指导方程的搜索。我们通过实验证明，这种方法可以重新发现一组著名的物理方程，并且随着时间的推移，随着数据和计算资源的增加，其性能会不断提高。",
        "领域": "符号回归、深度学习与科学计算结合、Transformer模型应用",
        "问题": "如何从输入-输出对中自动发现符号方程，而无需依赖手工设计的策略",
        "动机": "传统符号回归方法缺乏学习能力，无法从经验中改进，限制了其在科学发现中的应用潜力",
        "方法": "利用程序化生成的无限方程集预训练Transformer模型，通过模型预测指导符号方程的搜索",
        "关键词": [
            "符号回归",
            "Transformer模型",
            "预训练",
            "科学计算",
            "方程发现"
        ],
        "涉及的技术概念": {
            "符号回归": "一种从输入-输出数据中自动发现数学表达式的技术，无需预先指定模型形式",
            "Transformer模型": "一种基于自注意力机制的深度学习模型，用于处理序列数据，在本研究中用于预测符号方程",
            "预训练": "在大规模数据集上预先训练模型，以提高其在特定任务上的性能和泛化能力"
        },
        "success": true
    },
    {
        "order": 714,
        "title": "Neural Tangent Generalization Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/10215",
        "abstract": "The remarkable performance achieved by Deep Neural Networks (DNNs) in many applications is followed by the rising concern about data privacy and security. Since DNNs usually require large datasets to train, many practitioners scrape data from external sources such as the Internet. However, an external data owner may not be willing to let this happen, causing legal or ethical issues. In this paper, we study the generalization attacks against DNNs, where an attacker aims to slightly modify training data in order to spoil the training process such that a trained network lacks generalizability. These attacks can be performed by data owners and protect data from unexpected use. However, there is currently no efficient generalization attack against DNNs due to the complexity of a bilevel optimization involved. We propose the Neural Tangent Generalization Attack (NTGA) that, to the best of our knowledge, is the first work enabling clean-label, black-box generalization attack against DNNs. We conduct extensive experiments, and the empirical results demonstrate the effectiveness of NTGA. Our code and perturbed datasets are available at: https://github.com/lionelmessi6410/ntga.",
        "conference": "ICML",
        "中文标题": "神经切线泛化攻击",
        "摘要翻译": "深度神经网络（DNNs）在许多应用中取得的显著性能伴随着对数据隐私和安全日益增长的关注。由于DNNs通常需要大量数据集进行训练，许多从业者从互联网等外部来源抓取数据。然而，外部数据所有者可能不愿意让这种情况发生，从而引发法律或伦理问题。在本文中，我们研究了针对DNNs的泛化攻击，攻击者旨在轻微修改训练数据以破坏训练过程，使得训练后的网络缺乏泛化能力。这些攻击可以由数据所有者执行，以保护数据不被意外使用。然而，由于涉及双层优化的复杂性，目前还没有针对DNNs的高效泛化攻击。我们提出了神经切线泛化攻击（NTGA），据我们所知，这是首个实现干净标签、黑盒泛化攻击的工作。我们进行了广泛的实验，实证结果证明了NTGA的有效性。我们的代码和扰动数据集可在https://github.com/lionelmessi6410/ntga获取。",
        "领域": "深度学习安全、对抗性攻击、数据隐私保护",
        "问题": "如何有效实施针对深度神经网络的泛化攻击，以保护数据不被未经授权的使用",
        "动机": "解决由于深度神经网络训练过程中数据抓取引发的隐私和安全问题，提供一种保护数据不被意外使用的方法",
        "方法": "提出神经切线泛化攻击（NTGA），通过轻微修改训练数据破坏训练过程，实现干净标签、黑盒条件下的泛化攻击",
        "关键词": [
            "神经切线泛化攻击",
            "深度神经网络安全",
            "数据隐私保护",
            "对抗性攻击",
            "双层优化"
        ],
        "涉及的技术概念": {
            "神经切线泛化攻击（NTGA）": "一种针对深度神经网络的泛化攻击方法，通过修改训练数据破坏模型的泛化能力",
            "双层优化": "在攻击过程中需要解决的复杂优化问题，涉及攻击目标和模型训练两个层面",
            "干净标签攻击": "一种攻击方式，攻击者在保持数据标签不变的情况下修改数据内容，以影响模型训练"
        },
        "success": true
    },
    {
        "order": 715,
        "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images",
        "html": "https://ICML.cc//virtual/2021/poster/10329",
        "abstract": "Data transformations (e.g. rotations, reflections, and cropping) play an important role in self-supervised learning. Typically, images are transformed into different views, and neural networks trained on tasks involving these views produce useful feature representations for downstream tasks, including anomaly detection. However, for anomaly detection beyond image data, it is often unclear which transformations to use. Here we present a simple end-to-end procedure for anomaly detection with learnable transformations. The key idea is to embed the transformed data into a semantic space such that the transformed data still resemble their untransformed form, while different transformations are easily distinguishable. Extensive experiments on time series show that our proposed method outperforms existing approaches in the one-vs.-rest setting and is competitive in the more challenging n-vs.-rest anomaly-detection  task.   On  medical  and  cyber-security tabular data, our method learns domain-specific transformations and detects anomalies more accurately than previous work.",
        "conference": "ICML",
        "中文标题": "神经变换学习：超越图像的深度异常检测",
        "摘要翻译": "数据变换（如旋转、反射和裁剪）在自监督学习中扮演着重要角色。通常，图像被转换成不同的视图，而神经网络在这些视图上训练的任务中产生的特征表示对于下游任务，包括异常检测，是有用的。然而，对于超越图像数据的异常检测，往往不清楚应该使用哪些变换。在这里，我们提出了一种简单的端到端过程，用于具有可学习变换的异常检测。关键思想是将变换后的数据嵌入到一个语义空间中，使得变换后的数据仍然类似于其未变换的形式，而不同的变换则易于区分。在时间序列上的大量实验表明，我们提出的方法在一对多设置中优于现有方法，并且在更具挑战性的多对多异常检测任务中具有竞争力。在医疗和网络安全表格数据上，我们的方法学习了领域特定的变换，并且比之前的工作更准确地检测异常。",
        "领域": "异常检测、自监督学习、时间序列分析",
        "问题": "解决在非图像数据上进行异常检测时，如何选择有效的数据变换的问题。",
        "动机": "研究动机是为了提高在非图像数据（如时间序列和表格数据）上进行异常检测的准确性和效率，特别是在缺乏明确变换指导的情况下。",
        "方法": "提出了一种端到端的异常检测方法，通过可学习的数据变换将数据嵌入到语义空间，保持变换后数据与原始数据的相似性，同时使不同变换易于区分。",
        "关键词": [
            "神经变换学习",
            "异常检测",
            "自监督学习",
            "时间序列",
            "表格数据"
        ],
        "涉及的技术概念": {
            "数据变换": "在自监督学习中用于生成不同数据视图的技术，如旋转、反射和裁剪，以增强模型的泛化能力。",
            "语义空间嵌入": "将变换后的数据映射到一个语义空间，使得数据在变换后仍保持其原始语义特征的技术。",
            "端到端学习": "一种直接从输入到输出进行学习的模型训练方法，无需手动设计中间步骤或特征提取。"
        },
        "success": true
    },
    {
        "order": 716,
        "title": "Neuro-algorithmic Policies Enable Fast Combinatorial Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/9465",
        "abstract": "Although model-based and model-free approaches to learning the control of systems have achieved impressive results on standard benchmarks, generalization to task variations is still lacking. Recent results suggest that generalization for standard architectures improves only after obtaining exhaustive amounts of data. We give evidence that generalization capabilities are in many cases bottlenecked by the inability to generalize on the combinatorial aspects of the problem.\nWe show that, for a certain subclass of the MDP framework, this can be alleviated by a neuro-algorithmic policy architecture that embeds a time-dependent shortest path solver in a deep neural network. Trained end-to-end via blackbox-differentiation, this method leads to considerable improvement in generalization capabilities in the low-data regime.\n",
        "conference": "ICML",
        "中文标题": "神经算法策略实现快速组合泛化",
        "摘要翻译": "尽管基于模型和无模型的学习系统控制方法在标准基准测试中取得了令人印象深刻的成果，但在任务变体上的泛化能力仍然不足。最近的研究结果表明，标准架构的泛化能力只有在获取大量数据后才能得到提升。我们提供的证据表明，在许多情况下，泛化能力的瓶颈在于无法对问题的组合方面进行泛化。我们证明，对于MDP框架的某个子类，这一问题可以通过一种神经算法策略架构得到缓解，该架构在深度神经网络中嵌入了一个时间依赖的最短路径求解器。通过黑盒微分进行端到端训练，这种方法在低数据量情况下显著提高了泛化能力。",
        "领域": "强化学习、组合优化、深度学习",
        "问题": "提高在任务变体上的泛化能力，特别是在低数据量情况下",
        "动机": "解决标准架构在组合问题泛化上的不足，提升在低数据量情况下的泛化能力",
        "方法": "采用神经算法策略架构，嵌入时间依赖的最短路径求解器于深度神经网络中，并通过黑盒微分进行端到端训练",
        "关键词": [
            "神经算法策略",
            "组合泛化",
            "最短路径求解器",
            "黑盒微分",
            "低数据量"
        ],
        "涉及的技术概念": {
            "神经算法策略": "结合神经网络和算法求解器的策略架构，用于提升模型在组合问题上的泛化能力",
            "时间依赖的最短路径求解器": "嵌入在神经网络中的算法组件，用于解决特定时间依赖的最短路径问题",
            "黑盒微分": "一种训练方法，允许对包含不可微分组件的模型进行端到端的梯度下降优化"
        },
        "success": true
    },
    {
        "order": 717,
        "title": "Newton Method over Networks is Fast up to the Statistical Precision",
        "html": "https://ICML.cc//virtual/2021/poster/10747",
        "abstract": "We propose a distributed cubic regularization of the Newton method for solving (constrained) empirical risk minimization problems over a network of agents, modeled as undirected graph. The algorithm employs an inexact, preconditioned Newton step at each agent's side: the gradient of the centralized loss is iteratively estimated via a gradient-tracking consensus mechanism and the Hessian is   subsampled over the local data sets. No Hessian matrices are exchanged over the network. We derive global complexity bounds for convex and strongly convex   losses. Our analysis reveals an interesting interplay between sample and iteration/communication complexity: statistically accurate solutions are achievable in roughly the same number of iterations of the centralized cubic Newton, with a communication cost per iteration of the order of $\\widetilde{\\mathcal{O}}\\big(1/\\sqrt{1-\\rho}\\big)$, where $\\rho$ characterizes the connectivity of the network. This represents a significant improvement with respect to existing, statistically oblivious, distributed Newton-based methods over networks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "网络上的牛顿方法快速达到统计精度",
        "摘要翻译": "我们提出了一种分布式立方正则化的牛顿方法，用于解决网络代理（建模为无向图）上的（约束）经验风险最小化问题。该算法在每个代理端采用了一个不精确的、预处理的牛顿步骤：通过梯度追踪共识机制迭代估计集中损失的梯度，并在局部数据集上对Hessian矩阵进行子采样。网络中不交换Hessian矩阵。我们为凸和强凸损失推导了全局复杂度界限。我们的分析揭示了样本和迭代/通信复杂度之间有趣的相互作用：统计上准确的解决方案可以在大致相同次数的集中立方牛顿迭代中实现，每次迭代的通信成本约为$\\widetilde{\\mathcal{O}}\\big(1/\\sqrt{1-\\rho}\\big)$，其中$\\rho$表征了网络的连通性。这相对于现有的、统计上无知的、基于牛顿的分布式网络方法，代表了一个显著的改进。",
        "领域": "分布式优化, 机器学习优化算法, 网络化系统",
        "问题": "解决网络代理上的（约束）经验风险最小化问题",
        "动机": "提高分布式网络中牛顿方法的效率和统计精度",
        "方法": "采用分布式立方正则化的牛顿方法，结合梯度追踪共识机制和局部Hessian子采样",
        "关键词": [
            "分布式优化",
            "立方正则化",
            "牛顿方法",
            "梯度追踪",
            "Hessian子采样"
        ],
        "涉及的技术概念": {
            "分布式立方正则化": "用于在分布式网络中解决优化问题，通过正则化提高算法的稳定性和效率",
            "梯度追踪共识机制": "用于在网络中迭代估计集中损失的梯度，确保信息的一致性和准确性"
        }
    },
    {
        "order": 718,
        "title": "Noise and Fluctuation of Finite Learning Rate Stochastic Gradient Descent",
        "html": "https://ICML.cc//virtual/2021/poster/10649",
        "abstract": "In the vanishing learning rate regime, stochastic gradient descent (SGD) is now relatively well understood. In this work, we propose to study the basic properties of SGD and its variants in the non-vanishing learning rate regime. The focus is on deriving exactly solvable results and discussing their implications. The main contributions of this work are to derive the stationary distribution for discrete-time SGD in a quadratic loss function with and without momentum; in particular, one implication of our result is that the fluctuation caused by discrete-time dynamics takes a distorted shape and is dramatically larger than a continuous-time theory could predict. Examples of applications of the proposed theory considered in this work include the approximation error of variants of SGD, the effect of minibatch noise, the optimal Bayesian inference, the escape rate from a sharp minimum, and the stationary covariance of a few second-order methods including damped Newton's method, natural gradient descent, and Adam.",
        "conference": "ICML",
        "中文标题": "有限学习率随机梯度下降的噪声与波动",
        "摘要翻译": "在消失学习率的情况下，随机梯度下降（SGD）现在已经相对被理解。在这项工作中，我们提出研究在非消失学习率情况下SGD及其变体的基本性质。重点是推导出可精确求解的结果并讨论它们的含义。这项工作的主要贡献是在有动量和无动量的二次损失函数中推导出离散时间SGD的稳态分布；特别是，我们的结果的一个含义是，由离散时间动力学引起的波动呈现扭曲的形状，并且比连续时间理论预测的要大得多。本工作中考虑的应用示例包括SGD变体的近似误差、小批量噪声的影响、最优贝叶斯推断、从尖锐最小值逃逸的速率，以及包括阻尼牛顿法、自然梯度下降和Adam在内的几种二阶方法的稳态协方差。",
        "领域": "优化算法、深度学习理论、机器学习",
        "问题": "研究在非消失学习率情况下随机梯度下降及其变体的基本性质和稳态分布。",
        "动机": "理解在非消失学习率情况下随机梯度下降及其变体的行为，特别是在离散时间动力学下的波动和稳态分布。",
        "方法": "在二次损失函数中推导离散时间SGD的稳态分布，并分析其与连续时间理论的差异。",
        "关键词": [
            "随机梯度下降",
            "学习率",
            "稳态分布",
            "离散时间动力学",
            "二阶方法"
        ],
        "涉及的技术概念": {
            "随机梯度下降（SGD）": "一种优化算法，用于最小化损失函数，特别是在大规模数据集上。",
            "稳态分布": "在长时间运行后，系统状态的概率分布不再随时间变化。",
            "离散时间动力学": "描述系统在离散时间步骤中如何演变的数学模型。"
        },
        "success": true
    },
    {
        "order": 719,
        "title": "Non-Autoregressive Electron Redistribution Modeling for Reaction Prediction",
        "html": "https://ICML.cc//virtual/2021/poster/9427",
        "abstract": "Reliably predicting the products of chemical reactions presents a fundamental challenge in synthetic chemistry. Existing machine learning approaches typically produce a reaction product by sequentially forming its subparts or intermediate molecules. Such autoregressive methods, however, not only require a pre-defined order for the incremental construction but preclude the use of parallel decoding for efficient computation. To address these issues, we devise a non-autoregressive learning paradigm that predicts reaction in one shot. Leveraging the fact that chemical reactions can be described as a redistribution of electrons in molecules, we formulate a reaction as an arbitrary electron flow and predict it with a novel multi-pointer decoding network. Experiments on the USPTO-MIT dataset show that our approach has established a new state-of-the-art top-1 accuracy and achieves at least 27 times inference speedup over the state-of-the-art methods. Also, our predictions are easier for chemists to interpret owing to predicting the electron flows.",
        "conference": "ICML",
        "中文标题": "非自回归电子重分布建模用于反应预测",
        "摘要翻译": "可靠地预测化学反应产物是合成化学中的一个基本挑战。现有的机器学习方法通常通过顺序形成产物的子部分或中间分子来生成反应产物。然而，这种自回归方法不仅需要预先定义增量构建的顺序，而且排除了使用并行解码进行高效计算的可能性。为了解决这些问题，我们设计了一种非自回归学习范式，一次性预测反应。利用化学反应可以描述为分子中电子的重新分布这一事实，我们将反应表述为任意电子流，并使用一种新颖的多指针解码网络进行预测。在USPTO-MIT数据集上的实验表明，我们的方法建立了新的最先进top-1准确率，并且比最先进的方法实现了至少27倍的推理加速。此外，由于预测了电子流，我们的预测更容易被化学家解释。",
        "领域": "化学信息学、分子建模、反应预测",
        "问题": "如何高效且准确地预测化学反应的产物",
        "动机": "解决现有自回归方法在反应预测中需要预定义构建顺序和无法并行计算的问题",
        "方法": "采用非自回归学习范式，通过多指针解码网络预测电子流来一次性预测反应",
        "关键词": [
            "非自回归学习",
            "电子重分布",
            "反应预测",
            "多指针解码",
            "化学信息学"
        ],
        "涉及的技术概念": {
            "非自回归学习": "一种不依赖于序列生成顺序的学习方法，允许并行解码，提高计算效率",
            "电子重分布": "描述化学反应中电子在分子间重新分配的过程，是反应预测的核心",
            "多指针解码网络": "一种新颖的网络架构，用于预测电子流，支持一次性反应预测"
        },
        "success": true
    },
    {
        "order": 720,
        "title": "Nondeterminism and Instability in Neural Network Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9001",
        "abstract": "Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability. ",
        "conference": "ICML",
        "中文标题": "神经网络优化中的非确定性与不稳定性",
        "摘要翻译": "神经网络优化中的非确定性导致性能上的不确定性，使得微小的改进难以与运行间的变异性区分开来。虽然通过训练多个模型副本可以减少不确定性，但这样做既耗时又昂贵，且损害了可重复性。在这项工作中，我们建立了一个实验协议，用于理解优化非确定性对模型多样性的影响，使我们能够隔离各种非确定性来源的影响。令人惊讶的是，我们发现所有非确定性来源对模型多样性的测量都有相似的影响。为了解释这一有趣的现象，我们将模型训练的不稳定性（作为一个端到端的过程）确定为关键决定因素。我们表明，即使是初始参数的一比特变化也会导致模型收敛到完全不同的值。最后，我们提出了两种方法来减少不稳定性对运行间变异性的影响。",
        "领域": "深度学习优化、神经网络训练稳定性、机器学习可重复性",
        "问题": "神经网络优化过程中的非确定性和不稳定性导致性能评估的不确定性和可重复性问题。",
        "动机": "为了解决神经网络优化中由于非确定性导致的性能评估不确定性和可重复性问题，研究旨在理解和减少这些影响。",
        "方法": "建立实验协议来研究优化非确定性对模型多样性的影响，识别训练不稳定性为关键因素，并提出减少不稳定性影响的方法。",
        "关键词": [
            "非确定性",
            "训练不稳定性",
            "模型多样性",
            "优化协议",
            "可重复性"
        ],
        "涉及的技术概念": {
            "非确定性": "指神经网络优化过程中由于随机性因素导致的性能不确定性。",
            "训练不稳定性": "描述神经网络训练过程中对初始条件或小变化敏感，导致训练结果大不相同的现象。",
            "模型多样性": "指由于训练过程中的非确定性因素，相同架构的模型在不同训练运行中可能表现出不同的行为和性能。"
        },
        "success": true
    },
    {
        "order": 721,
        "title": "Non-Exponentially Weighted Aggregation: Regret Bounds for Unbounded Loss Functions",
        "html": "https://ICML.cc//virtual/2021/poster/8841",
        "abstract": "We tackle the problem of online optimization with a general, possibly unbounded, loss function. It is well known that when the loss is bounded, the exponentially weighted aggregation strategy (EWA) leads to a regret in $\\sqrt{T}$ after $T$ steps. In this paper, we study a generalized aggregation strategy, where the weights no longer depend exponentially on the losses. Our strategy is based on Follow The Regularized Leader (FTRL): we minimize the expected losses plus a regularizer, that is here a $\\phi$-divergence. When the regularizer is the Kullback-Leibler divergence, we obtain EWA as a special case. Using alternative divergences enables unbounded losses, at the cost of a worst regret bound in some cases.",
        "conference": "ICML",
        "中文标题": "非指数加权聚合：无界损失函数的遗憾界",
        "摘要翻译": "我们解决了在线优化问题，其中损失函数是通用的，可能是无界的。众所周知，当损失有界时，指数加权聚合策略（EWA）在T步后会导致√T的遗憾。在本文中，我们研究了一种广义的聚合策略，其中权重不再依赖于损失的指数。我们的策略基于跟随正则化领导者（FTRL）：我们最小化预期损失加上一个正则化项，这里是一个φ-散度。当正则化项是Kullback-Leibler散度时，我们得到EWA作为一个特例。使用替代散度可以在某些情况下实现无界损失，但代价是更差的遗憾界。",
        "领域": "在线学习、优化算法、机器学习理论",
        "问题": "解决在线优化中无界损失函数的遗憾界问题",
        "动机": "研究在损失函数可能无界的情况下，如何通过广义聚合策略优化在线学习算法的性能",
        "方法": "采用基于跟随正则化领导者（FTRL）的方法，通过引入φ-散度作为正则化项，扩展了传统的指数加权聚合策略",
        "关键词": [
            "在线优化",
            "无界损失函数",
            "广义聚合策略",
            "FTRL",
            "φ-散度"
        ],
        "涉及的技术概念": {
            "指数加权聚合策略（EWA）": "一种传统的在线学习策略，通过指数方式加权历史损失来更新模型参数",
            "跟随正则化领导者（FTRL）": "一种在线优化框架，通过最小化预期损失加上正则化项来选择下一步的策略",
            "φ-散度": "用于FTRL框架中的正则化项，允许策略处理无界损失函数，Kullback-Leibler散度是其特例"
        },
        "success": true
    },
    {
        "order": 722,
        "title": "Nonmyopic Multifidelity Acitve Search",
        "html": "https://ICML.cc//virtual/2021/poster/10057",
        "abstract": "Active search is a learning paradigm where we seek to identify as many members of a rare, valuable class as possible given a labeling budget. Previous work on active search has assumed access to a faithful (and expensive) oracle reporting experimental results. However, some settings offer access to cheaper surrogates such as computational simulation that may aid in the search. We propose a model of multifidelity active search, as well as a novel, computationally efficient policy for this setting that is motivated by state-of-the-art classical policies. Our policy is nonmyopic and budget aware, allowing for a dynamic tradeoff between exploration and exploitation. We evaluate the performance of our solution on real-world datasets and demonstrate significantly better performance than natural benchmarks.",
        "conference": "ICML",
        "中文标题": "非近视多保真主动搜索",
        "摘要翻译": "主动搜索是一种学习范式，其目标是在给定的标注预算内尽可能多地识别稀有且有价值类别的成员。以往关于主动搜索的研究假设可以访问一个忠实（且昂贵）的报告实验结果的预言机。然而，在某些情况下，可以利用诸如计算模拟等更便宜的替代品来辅助搜索。我们提出了一个多保真主动搜索的模型，以及一种新颖的、计算效率高的策略，该策略受到最先进的经典策略的启发。我们的策略是非近视且预算感知的，允许在探索和利用之间进行动态权衡。我们在真实世界的数据集上评估了我们解决方案的性能，并展示了比自然基准显著更好的性能。",
        "领域": "机器学习、主动学习、计算模拟",
        "问题": "如何在有限的标注预算内，利用多保真数据源有效地识别稀有且有价值类别的成员。",
        "动机": "解决传统主动搜索方法依赖昂贵预言机的问题，探索利用更便宜的数据源（如计算模拟）来提高搜索效率和效果。",
        "方法": "提出了一种多保真主动搜索模型和一种新颖的计算效率高的策略，该策略结合了非近视和预算感知的特点，实现了探索与利用的动态平衡。",
        "关键词": [
            "多保真学习",
            "主动搜索",
            "非近视策略",
            "预算感知",
            "探索与利用平衡"
        ],
        "涉及的技术概念": {
            "多保真主动搜索": "一种结合不同保真度数据源的主动搜索方法，旨在提高搜索效率和降低成本。",
            "非近视策略": "一种考虑未来可能性的决策策略，不同于近视策略仅考虑即时回报。",
            "预算感知": "在决策过程中考虑可用资源的限制，以实现资源的最优分配。"
        },
        "success": true
    },
    {
        "order": 723,
        "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density Ratio Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/8605",
        "abstract": "Density ratio estimation (DRE) is at the core of various machine learning tasks such as anomaly detection and domain adaptation. In the DRE literature, existing studies have extensively studied methods based on Bregman divergence (BD) minimization. However, when we apply the BD minimization with highly flexible models, such as deep neural networks, it tends to suffer from what we call train-loss hacking, which is a source of over-fitting caused by a typical characteristic of empirical BD estimators. In this paper, to mitigate train-loss hacking, we propose non-negative correction for empirical BD estimators. Theoretically, we confirm the soundness of the proposed method through a generalization error bound. In our experiments, the proposed methods show favorable performances in inlier-based outlier detection.\n",
        "conference": "ICML",
        "中文标题": "非负Bregman散度最小化用于深度直接密度比估计",
        "摘要翻译": "密度比估计（DRE）是异常检测和领域适应等多种机器学习任务的核心。在DRE文献中，现有研究已广泛探讨了基于Bregman散度（BD）最小化的方法。然而，当我们应用BD最小化与高度灵活的模型（如深度神经网络）时，它往往会遭受我们称之为训练损失黑客的问题，这是由经验BD估计器的典型特性引起的过拟合源。在本文中，为了减轻训练损失黑客，我们提出了对经验BD估计器的非负校正。理论上，我们通过泛化误差界确认了所提出方法的合理性。在我们的实验中，所提出的方法在基于内点的异常检测中表现出良好的性能。",
        "领域": "异常检测, 领域适应, 密度比估计",
        "问题": "解决在使用高度灵活模型（如深度神经网络）进行Bregman散度最小化时出现的训练损失黑客问题，即过拟合问题。",
        "动机": "减轻由经验Bregman散度估计器的典型特性引起的过拟合问题，提高密度比估计的准确性和泛化能力。",
        "方法": "提出对经验Bregman散度估计器的非负校正方法，并通过理论分析和实验验证其有效性。",
        "关键词": [
            "非负校正",
            "Bregman散度",
            "密度比估计",
            "异常检测",
            "领域适应"
        ],
        "涉及的技术概念": {
            "Bregman散度": "用于度量两个概率分布之间的差异，是密度比估计中的核心概念。",
            "训练损失黑客": "指在使用高度灵活模型进行Bregman散度最小化时出现的过拟合现象。",
            "非负校正": "本文提出的方法，用于减轻训练损失黑客问题，提高估计的稳定性和准确性。"
        },
        "success": true
    },
    {
        "order": 724,
        "title": "Nonparametric Decomposition of Sparse Tensors",
        "html": "https://ICML.cc//virtual/2021/poster/10017",
        "abstract": "Tensor decomposition is a powerful framework for multiway data analysis. Despite the success of existing approaches, they ignore the sparse nature of the tensor data in many real-world applications, explicitly or implicitly assuming dense tensors. To address this model misspecification and to exploit the sparse tensor structures, we propose Nonparametric dEcomposition of Sparse Tensors (\\ours), which can capture both the sparse structure properties and complex relationships between the tensor nodes to enhance the embedding estimation. Specifically, we first use completely random measures to construct tensor-valued random processes. We prove that the entry growth is much slower than that of the corresponding tensor size, which implies sparsity. Given finite observations (\\ie projections), we then propose two nonparametric decomposition models that couple Dirichlet processes and Gaussian processes to jointly sample the sparse entry indices and the entry values (the latter as a nonlinear mapping of the embeddings), so as to encode both the structure properties and nonlinear relationships of the tensor nodes into the embeddings.  Finally, we use the stick-breaking construction and random Fourier features to develop a scalable, stochastic variational learning algorithm. We show the advantage of our approach in sparse tensor generation, and entry index and value prediction in several real-world applications. ",
        "conference": "ICML",
        "中文标题": "稀疏张量的非参数分解",
        "摘要翻译": "张量分解是多维数据分析的强大框架。尽管现有方法取得了成功，但在许多实际应用中，它们忽略了张量数据的稀疏性，显式或隐式地假设了密集张量。为了解决这一模型错误设定并利用稀疏张量结构，我们提出了稀疏张量的非参数分解（NEST），该方法能够捕捉稀疏结构特性以及张量节点之间的复杂关系，以增强嵌入估计。具体来说，我们首先使用完全随机测度来构建张量值随机过程。我们证明了条目增长远慢于相应张量大小的增长，这意味着稀疏性。给定有限的观测（即投影），我们随后提出了两种非参数分解模型，这些模型耦合了Dirichlet过程和高斯过程，以联合采样稀疏条目索引和条目值（后者作为嵌入的非线性映射），从而将张量节点的结构特性和非线性关系编码到嵌入中。最后，我们使用stick-breaking构造和随机傅里叶特征来开发一个可扩展的随机变分学习算法。我们在几个实际应用中展示了我们的方法在稀疏张量生成、条目索引和值预测方面的优势。",
        "领域": "张量分解、稀疏数据处理、非线性嵌入",
        "问题": "现有张量分解方法忽略了数据的稀疏性，无法有效利用稀疏张量结构进行嵌入估计。",
        "动机": "为了解决现有方法在稀疏张量分解中的不足，并有效捕捉稀疏结构和复杂关系，以提高嵌入估计的准确性。",
        "方法": "提出了一种非参数分解方法NEST，通过完全随机测度构建张量值随机过程，耦合Dirichlet过程和高斯过程进行稀疏条目索引和值的联合采样，并开发了可扩展的随机变分学习算法。",
        "关键词": [
            "稀疏张量",
            "非参数分解",
            "Dirichlet过程",
            "高斯过程",
            "随机变分学习"
        ],
        "涉及的技术概念": {
            "完全随机测度": "用于构建张量值随机过程，证明条目增长的稀疏性。",
            "Dirichlet过程和高斯过程耦合": "用于联合采样稀疏条目索引和条目值，编码结构特性和非线性关系。",
            "stick-breaking构造和随机傅里叶特征": "用于开发可扩展的随机变分学习算法，提高计算效率。"
        },
        "success": true
    },
    {
        "order": 725,
        "title": "Nonparametric Hamiltonian Monte Carlo",
        "html": "https://ICML.cc//virtual/2021/poster/8811",
        "abstract": "Probabilistic programming uses programs to express generative models whose posterior probability is then computed by built-in inference engines. A challenging goal is to develop general purpose inference algorithms that work out-of-the-box for arbitrary programs in a universal probabilistic programming language (PPL). The densities defined by such programs, which may use stochastic branching and recursion, are (in general) nonparametric, in the sense that they correspond to models on an infinite-dimensional parameter space. However standard inference algorithms, such as the Hamiltonian Monte Carlo (HMC) algorithm, target distributions with a fixed number of parameters. This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC) algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a new class of measurable functions called “tree representable”, which serve as a language-independent representation of the density functions of probabilistic programs in a universal PPL. We provide a correctness proof of NP-HMC, and empirically demonstrate significant performance improvements over existing approaches on several nonparametric examples.",
        "conference": "ICML",
        "中文标题": "非参数哈密尔顿蒙特卡洛",
        "摘要翻译": "概率编程使用程序来表达生成模型，其后验概率随后由内置的推理引擎计算。一个具有挑战性的目标是开发通用推理算法，这些算法能够即插即用，适用于通用概率编程语言（PPL）中的任意程序。这类程序定义的密度，可能使用随机分支和递归，在某种意义上来说是非参数的，因为它们对应于无限维参数空间上的模型。然而，标准的推理算法，如哈密尔顿蒙特卡洛（HMC）算法，针对的是具有固定数量参数的分布。本文介绍了非参数哈密尔顿蒙特卡洛（NP-HMC）算法，该算法将HMC推广到非参数模型。NP-HMC的输入是一类新的可测量函数，称为“树可表示的”，它们作为通用PPL中概率程序密度函数的语言独立表示。我们提供了NP-HMC的正确性证明，并在几个非参数示例上实证展示了相对于现有方法的显著性能改进。",
        "领域": "概率编程、贝叶斯推理、蒙特卡洛方法",
        "问题": "如何在无限维参数空间上对非参数模型进行有效的概率推理",
        "动机": "开发一种通用推理算法，能够处理通用概率编程语言中任意程序的非参数模型",
        "方法": "提出非参数哈密尔顿蒙特卡洛（NP-HMC）算法，通过树可表示的函数作为输入，推广HMC算法至非参数模型",
        "关键词": [
            "非参数模型",
            "哈密尔顿蒙特卡洛",
            "概率编程",
            "树可表示函数",
            "贝叶斯推理"
        ],
        "涉及的技术概念": {
            "非参数模型": "指模型参数空间为无限维的模型，适用于更广泛的概率编程应用",
            "哈密尔顿蒙特卡洛": "一种高效的马尔可夫链蒙特卡洛方法，用于从复杂概率分布中采样",
            "树可表示函数": "一类新的可测量函数，作为通用概率编程语言中概率程序密度函数的语言独立表示"
        },
        "success": true
    },
    {
        "order": 726,
        "title": "No-regret Algorithms for Capturing Events in Poisson Point Processes",
        "html": "https://ICML.cc//virtual/2021/poster/9633",
        "abstract": "\tInhomogeneous Poisson point processes are widely used  models of event occurrences. We address \\emph{adaptive sensing of Poisson Point processes}, namely, maximizing the number of captured events subject to sensing costs. We encode prior assumptions on the rate function by modeling it as a member of a known \\emph{reproducing kernel Hilbert space} (RKHS). By partitioning the domain into separate small regions, and using heteroscedastic linear regression, we propose a tractable estimator of Poisson process rates for two feedback models: \\emph{count-record}, where exact locations of events are observed, and \\emph{histogram} feedback, where only counts of events are observed. We derive provably accurate anytime confidence estimates for our estimators for sequentially acquired Poisson count data. Using these, we formulate algorithms based on optimism that provably incur sublinear count-regret. We demonstrate the practicality of the method on problems from crime modeling, revenue maximization as well as environmental monitoring. ",
        "conference": "ICML",
        "中文标题": "无遗憾算法在泊松点过程事件捕获中的应用",
        "摘要翻译": "非均匀泊松点过程是事件发生广泛使用的模型。我们解决了泊松点过程的自适应感知问题，即在感知成本约束下最大化捕获的事件数量。我们通过将速率函数建模为已知的再生核希尔伯特空间（RKHS）的成员，来编码关于速率函数的先验假设。通过将域划分为独立的小区域，并使用异方差线性回归，我们提出了两种反馈模型下泊松过程速率的可处理估计器：计数记录反馈，其中观察到事件的确切位置；以及直方图反馈，其中仅观察到事件的计数。我们为顺序获取的泊松计数数据推导出了可证明准确的任意时间置信度估计。利用这些估计，我们制定了基于乐观主义的算法，这些算法可证明产生次线性计数遗憾。我们在犯罪建模、收入最大化以及环境监测等问题上展示了该方法的实用性。",
        "领域": "事件预测建模, 自适应感知, 环境监测",
        "问题": "在感知成本约束下最大化捕获泊松点过程事件的数量",
        "动机": "开发能够有效捕获事件并最小化感知成本的算法，以应对实际应用中的挑战",
        "方法": "通过将速率函数建模为RKHS成员，使用异方差线性回归和区域划分技术，开发了两种反馈模型下的速率估计器，并基于乐观主义原则设计了算法",
        "关键词": [
            "泊松点过程",
            "自适应感知",
            "再生核希尔伯特空间",
            "异方差线性回归",
            "次线性遗憾"
        ],
        "涉及的技术概念": {
            "再生核希尔伯特空间（RKHS）": "用于编码速率函数的先验假设，提供了一种灵活的速率函数建模方式",
            "异方差线性回归": "用于估计泊松过程的速率，特别是在处理不同区域内的异方差性时",
            "次线性遗憾": "算法性能的度量标准，确保算法在长期运行中的性能接近最优"
        },
        "success": true
    },
    {
        "order": 727,
        "title": "Not All Memories are Created Equal: Learning to Forget by Expiring",
        "html": "https://ICML.cc//virtual/2021/poster/10741",
        "abstract": "Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.",
        "conference": "ICML",
        "中文标题": "并非所有记忆都同等重要：通过过期学习遗忘",
        "摘要翻译": "注意力机制在需要长期记忆的序列建模任务中已显示出有希望的结果。最近的工作研究了减少保存和存储记忆计算成本的机制。然而，并非过去的所有内容都同等重要。我们提出了Expire-Span方法，该方法学会保留最重要的信息并让不相关的信息过期。这种记忆的遗忘使得Transformers能够高效地扩展到处理数万个先前时间步的注意力，因为并非所有先前时间步的状态都被保留。我们证明，Expire-Span可以帮助模型识别和保留关键信息，并在专门设计来挑战这一功能的强化学习任务中表现出色。接下来，我们展示了Expire-Span可以扩展到数万大小的记忆，在字符级语言建模和逐帧移动物体任务等极长上下文任务中设定了新的技术状态。最后，我们分析了Expire-Span与现有方法相比的效率，并证明它训练更快且使用更少的内存。",
        "领域": "自然语言处理与视觉结合, 强化学习, 序列建模",
        "问题": "如何在序列建模任务中高效地管理和遗忘不重要的记忆信息",
        "动机": "减少保存和存储不重要记忆的计算成本，提高模型处理长序列的效率",
        "方法": "提出Expire-Span方法，通过学习机制决定保留重要信息和遗忘不重要信息",
        "关键词": [
            "注意力机制",
            "记忆管理",
            "长序列处理",
            "强化学习",
            "Transformers"
        ],
        "涉及的技术概念": {
            "Expire-Span": "一种通过学习决定保留或遗忘记忆的机制，用于提高模型处理长序列的效率",
            "注意力机制": "用于增强模型对序列中重要部分的关注，提高任务表现",
            "Transformers": "一种基于自注意力机制的深度学习模型，适用于处理序列数据"
        },
        "success": true
    },
    {
        "order": 728,
        "title": "Objective Bound Conditional Gaussian Process for Bayesian Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10235",
        "abstract": "A Gaussian process is a standard surrogate model for an unknown objective function in Bayesian optimization. In this paper, we propose a new surrogate model, called the objective bound conditional Gaussian process (OBCGP), to condition a Gaussian process on a bound on the optimal function value. The bound is obtained and updated as the best observed value during the sequential optimization procedure. Unlike the standard Gaussian process, the OBCGP explicitly incorporates the existence of a point that improves the best known bound. We treat the location of such a point as a model parameter and estimate it jointly with other parameters by maximizing the likelihood using variational inference. Within the standard Bayesian optimization framework, the OBCGP can be combined with various acquisition functions to select the next query point. In particular, we derive cumulative regret bounds for the OBCGP combined with the upper confidence bound acquisition algorithm. Furthermore, the OBCGP can inherently incorporate a new type of prior knowledge, i.e., the bounds on the optimum, if it is available. The incorporation of this type of prior knowledge into a surrogate model has not been studied previously. We demonstrate the effectiveness of the OBCGP through its application to Bayesian optimization tasks, such as the sequential design of experiments and hyperparameter optimization in neural networks.",
        "conference": "ICML",
        "中文标题": "目标边界条件高斯过程用于贝叶斯优化",
        "摘要翻译": "高斯过程是贝叶斯优化中未知目标函数的标准代理模型。本文提出了一种新的代理模型，称为目标边界条件高斯过程（OBCGP），用于在高斯过程上条件化最优函数值的边界。该边界是在顺序优化过程中获得并更新的最佳观测值。与标准高斯过程不同，OBCGP明确包含了一个点的存在，该点改进了已知的最佳边界。我们将这样一个点的位置视为模型参数，并通过使用变分推断最大化似然来与其他参数联合估计。在标准的贝叶斯优化框架内，OBCGP可以与各种采集函数结合来选择下一个查询点。特别是，我们为OBCGP结合上置信界采集算法推导了累积遗憾边界。此外，如果可用，OBCGP可以固有地纳入一种新型的先验知识，即最优值的边界。将这种类型的先验知识纳入代理模型以前尚未被研究过。我们通过将OBCGP应用于贝叶斯优化任务，如实验的顺序设计和神经网络中的超参数优化，来证明其有效性。",
        "领域": "贝叶斯优化, 高斯过程, 超参数优化",
        "问题": "如何在贝叶斯优化中更有效地利用最优函数值的边界信息",
        "动机": "探索如何将最优函数值的边界信息明确纳入高斯过程代理模型，以提高贝叶斯优化的效率和效果",
        "方法": "提出目标边界条件高斯过程（OBCGP），通过变分推断联合估计模型参数，并与各种采集函数结合使用",
        "关键词": [
            "目标边界条件高斯过程",
            "贝叶斯优化",
            "变分推断",
            "上置信界",
            "超参数优化"
        ],
        "涉及的技术概念": {
            "目标边界条件高斯过程": "一种新的代理模型，用于在高斯过程上条件化最优函数值的边界",
            "变分推断": "用于最大化似然，估计模型参数的方法",
            "上置信界采集算法": "一种采集函数，用于在贝叶斯优化中选择下一个查询点"
        },
        "success": true
    },
    {
        "order": 729,
        "title": "Object Segmentation Without Labels with Large-Scale Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/8667",
        "abstract": "The recent rise of unsupervised and self-supervised learning has dramatically reduced the dependency on labeled data, providing high-quality representations for transfer on downstream tasks. Furthermore, recent works also employed these representations in a fully unsupervised setup for image classification, reducing the need for human labels on the fine-tuning stage as well. \nThis work demonstrates that large-scale unsupervised models can also perform a more challenging object segmentation task, requiring neither pixel-level nor image-level labeling. Namely, we show that recent unsupervised GANs allow to differentiate between foreground/background pixels, providing high-quality saliency masks. By extensive comparison on common benchmarks, we outperform existing unsupervised alternatives for object segmentation, achieving new state-of-the-art.",
        "conference": "ICML",
        "中文标题": "无需标签的大规模生成模型对象分割",
        "摘要翻译": "最近，无监督和自监督学习的兴起极大地减少了对标记数据的依赖，为下游任务的迁移提供了高质量的表示。此外，最近的工作还在完全无监督的设置中利用这些表示进行图像分类，进一步减少了在微调阶段对人类标签的需求。这项工作表明，大规模无监督模型还可以执行更具挑战性的对象分割任务，既不需要像素级也不需要图像级标签。具体来说，我们展示了最近的无监督GANs能够区分前景/背景像素，提供高质量的显著性掩码。通过在常见基准上的广泛比较，我们在对象分割的无监督替代方案中超越了现有方法，达到了新的最先进水平。",
        "领域": "图像分割",
        "问题": "如何在无需任何形式标签（像素级或图像级）的情况下，利用大规模无监督模型进行高质量的对象分割。",
        "动机": "减少对象分割任务中对标记数据的依赖，探索无监督学习在复杂视觉任务中的应用潜力。",
        "方法": "利用最近的无监督生成对抗网络（GANs）技术，通过区分前景和背景像素来生成高质量的显著性掩码，实现无需标签的对象分割。",
        "关键词": [
            "无监督学习",
            "对象分割",
            "生成对抗网络",
            "显著性检测",
            "大规模模型"
        ],
        "涉及的技术概念": {
            "无监督学习": "在无需人工标注数据的情况下，通过算法自动学习数据的内在结构和模式。",
            "生成对抗网络（GANs）": "由生成器和判别器组成的框架，通过对抗过程生成高质量的数据样本。",
            "显著性检测": "识别图像中最吸引注意力的区域或对象，常用于对象分割和视觉注意力建模。"
        },
        "success": true
    },
    {
        "order": 730,
        "title": "Oblivious Sketching-based Central Path Method for Linear Programming",
        "html": "https://ICML.cc//virtual/2021/poster/10621",
        "abstract": "In this work, we propose a sketching-based central path method for solving linear programmings, whose running time matches the state of the art results [Cohen, Lee, Song STOC 19; Lee, Song, Zhang COLT 19]. Our method opens up the iterations of the central path method and deploys an 'iterate and sketch' approach towards the problem by introducing a new coordinate-wise embedding technique, which may be of independent interest. Compare to previous methods, the work [Cohen, Lee, Song STOC 19] enjoys feasibility while being non-oblivious, and [Lee, Song, Zhang COLT 19] is oblivious but infeasible, and relies on $\\mathit{dense}$ sketching matrices such as subsampled randomized Hadamard/Fourier transform matrices. Our method enjoys the benefits of being both oblivious and feasible, and can use $\\mathit{sparse}$ sketching matrix [Nelson, Nguyen FOCS 13] to speed up the online matrix-vector multiplication. Our framework for solving LP naturally generalizes to a broader class of convex optimization problems including empirical risk minimization.",
        "conference": "ICML",
        "中文标题": "基于无感知草图的线性规划中心路径方法",
        "摘要翻译": "在这项工作中，我们提出了一种基于草图的中心路径方法来解决线性规划问题，其运行时间与最新研究成果相匹配[Cohen, Lee, Song STOC 19; Lee, Song, Zhang COLT 19]。我们的方法打开了中心路径方法的迭代过程，并通过引入一种新的坐标嵌入技术，采用了一种‘迭代并草图’的方法来处理问题，这种方法可能具有独立的意义。与之前的方法相比，[Cohen, Lee, Song STOC 19]的工作在保持非无感知的同时实现了可行性，而[Lee, Song, Zhang COLT 19]的方法是无感知的但不可行，并且依赖于密集的草图矩阵，如子采样随机Hadamard/Fourier变换矩阵。我们的方法兼具无感知和可行性的优点，并且可以使用稀疏草图矩阵[Nelson, Nguyen FOCS 13]来加速在线矩阵向量乘法。我们解决线性规划的框架自然推广到了一类更广泛的凸优化问题，包括经验风险最小化。",
        "领域": "优化算法, 线性规划, 凸优化",
        "问题": "如何高效解决线性规划问题，同时保持方法的无感知性和可行性。",
        "动机": "为了克服现有方法在无感知性和可行性之间的权衡问题，提出一种新的方法，既能保持无感知性，又能保证可行性，同时提高计算效率。",
        "方法": "提出了一种基于草图的中心路径方法，通过引入新的坐标嵌入技术和‘迭代并草图’的策略，结合稀疏草图矩阵的使用，以提高计算效率。",
        "关键词": [
            "线性规划",
            "中心路径方法",
            "草图技术",
            "稀疏矩阵",
            "凸优化"
        ],
        "涉及的技术概念": {
            "中心路径方法": "一种用于解决线性规划和凸优化问题的迭代方法，通过中心路径逐步逼近最优解。",
            "草图技术": "通过降维技术减少问题的复杂度，同时保留关键信息，用于加速计算过程。",
            "稀疏矩阵": "具有大量零元素的矩阵，可以显著减少存储需求和计算复杂度，特别是在大规模问题中。"
        },
        "success": true
    },
    {
        "order": 731,
        "title": "Oblivious Sketching for Logistic Regression",
        "html": "https://ICML.cc//virtual/2021/poster/10263",
        "abstract": "What guarantees are possible for solving logistic regression in one pass over a data stream? To answer this question, we present the first data oblivious sketch for logistic regression. Our sketch can be computed in input sparsity time over a turnstile data stream and reduces the size of a $d$-dimensional data set from $n$ to only $\\operatorname{poly}(\\mu d\\log n)$ weighted points, where $\\mu$ is a useful parameter which captures the complexity of compressing the data. Solving (weighted) logistic regression on the sketch gives an $O(\\log n)$-approximation to the original problem on the full data set. We also show how to obtain an $O(1)$-approximation with slight modifications. Our sketches are fast, simple, easy to implement, and our experiments demonstrate their practicality.",
        "conference": "ICML",
        "success": true,
        "中文标题": "遗忘草图在逻辑回归中的应用",
        "摘要翻译": "在单次遍历数据流的情况下，逻辑回归的求解能达到什么样的保证？为了回答这个问题，我们提出了第一个用于逻辑回归的数据遗忘草图。我们的草图可以在周转数据流中以输入稀疏时间计算，并将一个 d 维数据集的大小从 n 减少到仅为 poly(μd log n) 个加权点，其中 μ 是一个有用的参数，它捕获了压缩数据的复杂性。在草图上求解（加权）逻辑回归，可以得到完整数据集上原始问题的 O(log n) 近似。我们还展示了如何通过稍微修改获得 O(1) 近似。我们的草图快速、简单、易于实现，并且我们的实验证明了它们的实用性。",
        "领域": "在线学习, 数据压缩, 算法设计",
        "问题": "如何在单次数据流遍历中高效地求解逻辑回归问题，同时保证一定的近似精度。",
        "动机": "传统的逻辑回归方法在处理大规模数据流时效率较低，需要设计一种能够在单次遍历中快速压缩数据并保持精度的算法。",
        "方法": "提出了一种数据遗忘草图算法，该算法能够在输入稀疏时间内计算，并显著降低数据集的维度，同时通过在草图上求解逻辑回归来获得原始问题的近似解。",
        "关键词": [
            "遗忘草图",
            "逻辑回归",
            "数据流",
            "数据压缩",
            "近似算法"
        ],
        "涉及的技术概念": {
            "遗忘草图": "一种数据压缩技术，能够在不了解数据内容的情况下，将高维数据压缩到低维空间，同时保留数据的重要信息。",
            "逻辑回归": "一种用于二分类或多分类的线性模型，通过sigmoid函数将线性组合转换为概率值。"
        }
    },
    {
        "order": 732,
        "title": "Off-Belief Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9801",
        "abstract": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to find a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents' actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy $\\pi_1$ that is optimized assuming past actions were taken by a given, fixed policy ($\\pi_0$), but assuming that future actions will be taken by $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents' behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a fictitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI & ZSC problem Hanabi.",
        "conference": "ICML",
        "中文标题": "非信念学习",
        "摘要翻译": "在Dec-POMDPs的标准问题设置中，自我对弈的目标是找到一组能够共同最优执行的策略。通过自我对弈学习的策略可能会采用任意的约定，并隐含地依赖于基于对其他智能体行为的脆弱假设的多步推理，因此在测试时与人类或独立训练的智能体配对时会失败。为了解决这个问题，我们提出了非信念学习（OBL）。在每一个时间步，OBL智能体遵循一个策略π1，该策略假设过去的行动是由一个给定的固定策略π0采取的，但假设未来的行动将由π1采取。当π0是均匀随机时，OBL收敛到一个不依赖于基于其他智能体行为的推理的最优策略（一个最优的基础策略）。OBL可以在一个层次结构中迭代，其中一个级别的最优策略成为下一个级别的输入，从而以可控的方式引入多层次认知推理。与现有方法不同，现有方法可能收敛到任何均衡策略，而OBL收敛到一个独特的策略，使其适用于零次协调（ZSC）。OBL可以通过虚构的转移机制扩展到高维设置，并在玩具设置和基准人-AI与ZSC问题Hanabi中表现出强大的性能。",
        "领域": "多智能体系统、零次协调、认知推理",
        "问题": "解决在Dec-POMDPs中通过自我对弈学习的策略在与其他智能体或人类配对时的失败问题",
        "动机": "为了开发一种能够在不需要对其他智能体行为做出脆弱假设的情况下，仍能实现最优协调的策略学习方法",
        "方法": "提出非信念学习（OBL），通过假设过去的行动由一个固定策略采取，而未来的行动由当前策略采取，来优化策略，从而不依赖于对其他智能体行为的推理",
        "关键词": [
            "非信念学习",
            "零次协调",
            "多智能体系统",
            "认知推理",
            "Dec-POMDPs"
        ],
        "涉及的技术概念": {
            "非信念学习（OBL）": "一种策略优化方法，假设过去的行动由一个固定策略采取，而未来的行动由当前策略采取，以减少对其他智能体行为的依赖",
            "零次协调（ZSC）": "指在没有预先协调的情况下，智能体能够有效地协作的能力",
            "Dec-POMDPs": "分散部分可观察马尔可夫决策过程，用于建模多智能体在部分可观察环境中的决策问题"
        },
        "success": true
    },
    {
        "order": 733,
        "title": "Offline Contextual Bandits with Overparameterized Models",
        "html": "https://ICML.cc//virtual/2021/poster/10477",
        "abstract": "Recent results in supervised learning suggest that while overparameterized models have the capacity to overfit, they in fact generalize quite well. We ask whether the same phenomenon occurs for offline contextual bandits. Our results are mixed. Value-based algorithms benefit from the same generalization behavior as overparameterized supervised learning, but policy-based algorithms do not. We show that this discrepancy is due to the \\emph{action-stability} of their objectives. An objective is action-stable if there exists a prediction (action-value vector or action distribution) which is optimal no matter which action is observed. While value-based objectives are action-stable, policy-based objectives are unstable. We formally prove upper bounds on the regret of overparameterized value-based learning and lower bounds on the regret for policy-based algorithms. In our experiments with large neural networks, this gap between action-stable value-based objectives and unstable policy-based objectives leads to significant performance differences.",
        "conference": "ICML",
        "中文标题": "离线上下文赌博机与过参数化模型",
        "摘要翻译": "监督学习的最新结果表明，尽管过参数化模型有能力过拟合，但它们实际上泛化得很好。我们询问同样的现象是否发生在离线上下文赌博机中。我们的结果好坏参半。基于价值的算法从过参数化监督学习的相同泛化行为中受益，但基于策略的算法则不然。我们表明，这种差异是由于它们目标的行动稳定性。如果一个目标存在一个预测（行动价值向量或行动分布），无论观察到哪个行动都是最优的，那么这个目标就是行动稳定的。基于价值的目标是行动稳定的，而基于策略的目标是不稳定的。我们正式证明了过参数化基于价值学习的遗憾上界和基于策略算法的遗憾下界。在我们使用大型神经网络的实验中，行动稳定的基于价值目标与不稳定的基于策略目标之间的这种差距导致了显著的性能差异。",
        "领域": "强化学习、机器学习理论、神经网络",
        "问题": "探讨过参数化模型在离线上下文赌博机中的泛化能力差异",
        "动机": "研究过参数化模型在监督学习中的良好泛化能力是否适用于离线上下文赌博机，以及基于价值和基于策略算法在此背景下的表现差异",
        "方法": "通过理论分析和实验验证，比较基于价值和基于策略的过参数化模型在离线上下文赌博机中的表现，并探讨行动稳定性对算法性能的影响",
        "关键词": [
            "离线上下文赌博机",
            "过参数化模型",
            "行动稳定性",
            "基于价值的算法",
            "基于策略的算法"
        ],
        "涉及的技术概念": {
            "过参数化模型": "指参数数量远大于训练样本数的模型，在监督学习中表现出良好的泛化能力",
            "行动稳定性": "指算法目标函数中存在一个无论观察到哪个行动都是最优的预测，基于价值的算法具有此特性",
            "离线上下文赌博机": "一种强化学习问题，算法需要在给定历史数据的情况下学习最优策略，而不与环境进行实时交互"
        },
        "success": true
    },
    {
        "order": 734,
        "title": "Offline Meta-Reinforcement Learning with Advantage Weighting",
        "html": "https://ICML.cc//virtual/2021/poster/8719",
        "abstract": "This paper introduces the offline meta-reinforcement learning (offline meta-RL) problem setting and proposes an algorithm that performs well in this setting. Offline meta-RL is analogous to the widely successful supervised learning strategy of pre-training a model on a large batch of fixed, pre-collected data (possibly from various tasks) and fine-tuning the model to a new task with relatively little data. That is, in offline meta-RL, we meta-train on fixed, pre-collected data from several tasks and adapt to a new task with a very small amount (less than 5 trajectories) of data from the new task. By nature of being offline, algorithms for offline meta-RL can utilize the largest possible pool of training data available and eliminate potentially unsafe or costly data collection during meta-training. This setting inherits the challenges of offline RL, but it differs significantly because offline RL does not generally consider a) transfer to new tasks or b) limited data from the test task, both of which we face in offline meta-RL. Targeting the offline meta-RL setting, we propose Meta-Actor Critic with Advantage Weighting (MACAW). MACAW is an optimization-based meta-learning algorithm that uses simple, supervised regression objectives for both the inner and outer loop of meta-training. On offline variants of common meta-RL benchmarks, we empirically find that this approach enables fully offline meta-reinforcement learning and achieves notable gains over prior methods.",
        "conference": "ICML",
        "中文标题": "离线元强化学习与优势加权",
        "摘要翻译": "本文介绍了离线元强化学习（离线元-RL）的问题设置，并提出了一种在此设置下表现良好的算法。离线元-RL类似于广泛成功的监督学习策略，即在大量固定的、预先收集的数据（可能来自各种任务）上预训练模型，并用相对较少的数据对新任务进行微调。也就是说，在离线元-RL中，我们在来自多个任务的固定、预先收集的数据上进行元训练，并用来自新任务的非常少量（少于5条轨迹）的数据适应新任务。由于离线的性质，离线元-RL的算法可以利用最大可能的训练数据池，并消除元训练期间可能不安全或成本高昂的数据收集。这一设置继承了离线RL的挑战，但它与离线RL有显著不同，因为离线RL通常不考虑a)转移到新任务或b)来自测试任务的有限数据，这两者在离线元-RL中我们都面临。针对离线元-RL设置，我们提出了带有优势加权的元演员评论家（MACAW）。MACAW是一种基于优化的元学习算法，它使用简单的监督回归目标进行元训练的内外循环。在常见的元-RL基准测试的离线变体上，我们经验性地发现，这种方法实现了完全离线的元强化学习，并取得了比先前方法显著的提升。",
        "领域": "元强化学习、离线学习、强化学习应用",
        "问题": "如何在离线设置下进行有效的元强化学习，特别是在面对新任务时仅有非常有限的数据可用。",
        "动机": "探索在完全离线的环境下，如何利用预先收集的大量数据来训练模型，使其能够快速适应新任务，同时避免在线数据收集的高成本和不安全性。",
        "方法": "提出了一种基于优化的元学习算法MACAW，该算法使用监督回归目标进行元训练的内外循环，以实现完全离线的元强化学习。",
        "关键词": [
            "离线元强化学习",
            "优势加权",
            "元学习",
            "监督回归",
            "数据效率"
        ],
        "涉及的技术概念": {
            "离线元强化学习": "在完全离线的环境下进行元强化学习，利用预先收集的数据进行训练和适应新任务。",
            "优势加权": "在算法中用于调整策略更新，以优化模型在新任务上的表现。",
            "监督回归目标": "用于元训练的内外循环，简化学习过程并提高数据效率。"
        },
        "success": true
    },
    {
        "order": 735,
        "title": "Offline Reinforcement Learning with Fisher Divergence Critic Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/8977",
        "abstract": "Many modern approaches to offline Reinforcement Learning (RL) utilize behavior regularization, typically augmenting a model-free actor critic algorithm with a penalty measuring divergence of the policy from the offline data. In this work, we propose an alternative approach to encouraging the learned policy to stay close to the data, namely parameterizing the critic as the log-behavior-policy, which generated the offline data, plus a state-action value offset term, which can be learned using a neural network. Behavior regularization then corresponds to an appropriate regularizer on the offset term. We propose using a gradient penalty regularizer for the offset term and demonstrate its equivalence to Fisher divergence regularization, suggesting connections to the score matching and generative energy-based model literature. We thus term our resulting algorithm Fisher-BRC (Behavior Regularized Critic). On standard offline RL benchmarks, Fisher-BRC achieves both improved performance and faster convergence over existing state-of-the-art methods.",
        "conference": "ICML",
        "中文标题": "基于费希尔散度批评者正则化的离线强化学习",
        "摘要翻译": "许多现代离线强化学习（RL）方法采用行为正则化，通常通过增加一个衡量策略与离线数据差异的惩罚项来增强无模型的行动者批评者算法。在这项工作中，我们提出了一种替代方法，鼓励学习到的策略保持接近数据，即将批评者参数化为生成离线数据的对数行为策略加上一个可以通过神经网络学习的状态-动作价值偏移项。行为正则化则对应于对偏移项的适当正则化。我们提出对偏移项使用梯度惩罚正则化，并证明其与费希尔散度正则化的等价性，暗示了与分数匹配和生成能量基模型文献的联系。因此，我们将我们的算法命名为Fisher-BRC（行为正则化批评者）。在标准的离线RL基准测试中，Fisher-BRC在性能和收敛速度上都优于现有的最先进方法。",
        "领域": "离线强化学习、行为正则化、深度强化学习",
        "问题": "如何在离线强化学习中有效地鼓励学习策略接近离线数据分布",
        "动机": "提出一种新的行为正则化方法，以替代现有的策略与数据差异惩罚方法，旨在提高离线强化学习的性能和收敛速度",
        "方法": "将批评者参数化为对数行为策略加状态-动作价值偏移项，并使用梯度惩罚正则化偏移项，等效于费希尔散度正则化",
        "关键词": [
            "离线强化学习",
            "费希尔散度",
            "行为正则化",
            "梯度惩罚",
            "批评者参数化"
        ],
        "涉及的技术概念": {
            "行为正则化": "用于鼓励学习策略接近离线数据分布的技术，通过增加惩罚项或正则化项实现",
            "费希尔散度": "一种衡量两个概率分布差异的方法，在本研究中用于正则化批评者",
            "梯度惩罚正则化": "一种正则化技术，通过惩罚模型参数的梯度来控制模型的复杂度，本研究中等效于费希尔散度正则化"
        },
        "success": true
    },
    {
        "order": 736,
        "title": "Offline Reinforcement Learning with Pseudometric Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10353",
        "abstract": "Offline Reinforcement Learning methods seek to learn a policy from logged transitions of an environment, without any interaction. In the presence of function approximation, and under the assumption of limited coverage of the state-action space of the environment, it is necessary to enforce the policy to visit state-action pairs close to the support of logged transitions. In this work, we propose an iterative procedure to learn a pseudometric (closely related to bisimulation metrics) from logged transitions, and use it to define this notion of closeness. We show its convergence and extend it to the function approximation setting. We then use this pseudometric to define a new lookup based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the actor to stay close, in terms of the defined pseudometric, to the support of logged transitions. Finally, we evaluate the method on hand manipulation and locomotion tasks.",
        "conference": "ICML",
        "中文标题": "基于伪度量学习的离线强化学习",
        "摘要翻译": "离线强化学习方法旨在无需任何交互的情况下，从环境记录的转移中学习策略。在函数逼近的存在下，并假设环境的状态-动作空间覆盖有限，有必要强制策略访问接近记录转移支持的状态-动作对。在这项工作中，我们提出了一种迭代过程，从记录的转移中学习一种伪度量（与双模拟度量密切相关），并用它来定义这种接近的概念。我们展示了其收敛性，并将其扩展到函数逼近设置中。然后，我们使用这种伪度量在演员-评论家算法中定义一个新的基于查找的奖励：PLOFF。这种奖励鼓励演员在定义的伪度量方面保持接近记录转移的支持。最后，我们在手部操作和运动任务上评估了该方法。",
        "领域": "强化学习、机器人控制、模仿学习",
        "问题": "在有限的状态-动作空间覆盖下，如何有效地从记录的转移中学习策略",
        "动机": "为了解决在函数逼近和有限环境覆盖条件下，离线强化学习策略的有效学习问题",
        "方法": "提出了一种迭代学习伪度量的方法，并基于此定义了一种新的奖励机制，用于演员-评论家算法",
        "关键词": [
            "离线强化学习",
            "伪度量学习",
            "演员-评论家算法",
            "手部操作",
            "运动任务"
        ],
        "涉及的技术概念": {
            "伪度量学习": "从记录的转移中学习一种与双模拟度量密切相关的伪度量，用于定义状态-动作对的接近程度",
            "演员-评论家算法": "一种结合了价值函数和策略函数的强化学习方法，用于策略优化",
            "双模拟度量": "一种用于衡量状态之间相似性的度量，与伪度量学习密切相关"
        },
        "success": true
    },
    {
        "order": 737,
        "title": "Off-Policy Confidence Sequences",
        "html": "https://ICML.cc//virtual/2021/poster/10471",
        "abstract": "We develop confidence bounds that hold uniformly over time for off-policy\nevaluation in the contextual bandit setting. These confidence sequences\nare based on recent ideas from martingale analysis and are\nnon-asymptotic, non-parametric, and valid at arbitrary stopping times.\nWe provide algorithms for computing these confidence sequences that\nstrike a good balance between computational and statistical efficiency.\nWe empirically demonstrate the tightness of our approach in terms of\nfailure probability and width and apply it to the ``gated deployment'' problem of safely upgrading a production contextual bandit system.",
        "conference": "ICML",
        "中文标题": "非策略置信序列",
        "摘要翻译": "我们开发了在上下文强盗设置中用于非策略评估的、随时间均匀保持的置信界限。这些置信序列基于来自鞅分析的最新思想，是非渐近的、非参数的，并且在任意停止时间都有效。我们提供了计算这些置信序列的算法，这些算法在计算效率和统计效率之间取得了良好的平衡。我们通过实验证明了我们的方法在失败概率和宽度方面的紧密度，并将其应用于安全升级生产上下文强盗系统的“门控部署”问题。",
        "领域": "强化学习、上下文强盗、非策略评估",
        "问题": "在上下文强盗设置中，开发随时间均匀保持的非策略评估置信界限。",
        "动机": "为了在非策略评估中提供非渐近、非参数且在任意停止时间都有效的置信序列，以支持安全升级生产系统。",
        "方法": "基于鞅分析的最新思想，开发非渐近、非参数的置信序列，并提供在计算和统计效率之间平衡的算法。",
        "关键词": [
            "非策略评估",
            "置信序列",
            "上下文强盗",
            "鞅分析",
            "门控部署"
        ],
        "涉及的技术概念": {
            "非策略评估": "在上下文强盗设置中评估不同于数据收集策略的另一策略的性能。",
            "置信序列": "基于鞅分析的非渐近、非参数置信界限，适用于任意停止时间。",
            "鞅分析": "用于开发随时间均匀保持的置信界限的数学工具，确保统计有效性。"
        },
        "success": true
    },
    {
        "order": 738,
        "title": "Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap",
        "html": "https://ICML.cc//virtual/2021/poster/9401",
        "abstract": "We provide a unifying view of a large family of previous imitation learning algorithms through the lens of moment matching. At its core, our classification scheme is based on whether the learner attempts to match (1) reward or (2) action-value moments of the expert's behavior, with each option leading to differing algorithmic approaches. By considering adversarially chosen divergences between learner and expert behavior, we are able to derive bounds on policy performance that apply for all algorithms in each of these classes, the first to our knowledge. We also introduce the notion of moment recoverability, implicit in many previous analyses of imitation learning, which allows us to cleanly delineate how well each algorithmic family is able to mitigate compounding errors. We derive three novel algorithm templates (AdVIL, AdRIL, and DAeQuIL) with strong guarantees, simple implementation, and competitive empirical performance.",
        "conference": "ICML",
        "中文标题": "关于矩与匹配：一个缩小模仿差距的博弈论框架",
        "摘要翻译": "我们通过矩匹配的视角，提供了一个对先前大量模仿学习算法的统一观点。我们的分类方案核心在于学习者是否尝试匹配（1）专家行为的奖励矩或（2）行动值矩，每种选择导致不同的算法方法。通过考虑学习者和专家行为之间对抗性选择的差异，我们能够为每一类算法中的所有算法推导出策略性能的界限，这是我们所知的首次。我们还引入了矩可恢复性的概念，这在先前许多模仿学习的分析中是隐含的，使我们能够清晰地描绘出每个算法家族如何能够减轻复合错误。我们推导出三个具有强大保证、简单实现和竞争性实证性能的新算法模板（AdVIL、AdRIL和DAeQuIL）。",
        "领域": "模仿学习、强化学习、博弈论与机器学习结合",
        "问题": "如何通过矩匹配的方法缩小模仿学习中的模仿差距",
        "动机": "为了提供一个统一的视角来理解和分类模仿学习算法，并通过矩匹配来优化策略性能",
        "方法": "通过矩匹配分类模仿学习算法，考虑对抗性选择的差异来推导策略性能界限，引入矩可恢复性概念，并开发新的算法模板",
        "关键词": [
            "矩匹配",
            "模仿学习",
            "博弈论",
            "策略性能",
            "对抗性学习"
        ],
        "涉及的技术概念": {
            "矩匹配": "用于分类和优化模仿学习算法，通过匹配专家行为的特定矩来缩小模仿差距",
            "对抗性选择的差异": "用于推导策略性能的界限，确保算法在所有情况下的有效性",
            "矩可恢复性": "用于评估算法减轻复合错误的能力，是理解和优化模仿学习算法的关键概念"
        },
        "success": true
    },
    {
        "order": 739,
        "title": "OmniNet: Omnidirectional Representations from Transformers",
        "html": "https://ICML.cc//virtual/2021/poster/9301",
        "abstract": "This  paper  proposes  Omnidirectional  Representations from Transformers (OMNINET).   In OmniNet, instead of maintaining a strictly horizon-tal receptive field, each token is allowed to attend to all tokens in the entire network.  This process can  also  be  interpreted  as  a  form  of  extreme or  intensive  attention  mechanism  that  has  the receptive field of the entire width and depth of the network. To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model.  In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based, low-rank attention and/or Big Bird as the meta-learner.  Extensive experiments are conducted on autoregressive language modeling(LM1B, C4), Machine Translation, Long Range Arena  (LRA),  and  Image  Recognition.The experiments show that OmniNet achieves considerable improvements across these tasks, including achieving state-of-the-art performance on LM1B,WMT’14 En-De/En-Fr, and Long Range Arena.Moreover, using omnidirectional representation in   Vision   Transformers   leads   to   significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups.",
        "conference": "ICML",
        "中文标题": "OmniNet：来自Transformer的全方位表示",
        "摘要翻译": "本文提出了来自Transformer的全方位表示（OMNINET）。在OmniNet中，每个标记不再局限于严格的水平感受野，而是可以关注整个网络中的所有标记。这一过程也可以被解释为一种极端或密集的注意力机制，其感受野覆盖网络的整个宽度和深度。为此，全方位注意力通过一个元学习器学习，该元学习器本质上是另一个基于自注意力的模型。为了减轻全感受野注意力计算成本高的问题，我们利用了高效的自注意力模型，如基于核的、低秩注意力和/或Big Bird作为元学习器。在自回归语言建模（LM1B、C4）、机器翻译、长距离竞技场（LRA）和图像识别等任务上进行了广泛的实验。实验表明，OmniNet在这些任务上均取得了显著的改进，包括在LM1B、WMT’14 En-De/En-Fr和长距离竞技场上达到了最先进的性能。此外，在视觉Transformer中使用全方位表示，在少样本学习和微调设置下的图像识别任务上均带来了显著的改进。",
        "领域": "自然语言处理与视觉结合, 自回归语言建模, 图像识别",
        "问题": "如何扩展Transformer模型的注意力机制，使其能够捕捉更广泛的上下文信息",
        "动机": "探索一种新型的注意力机制，能够突破传统Transformer模型的水平感受野限制，实现更全面的信息捕捉",
        "方法": "提出全方位注意力机制，通过元学习器学习，并利用高效的自注意力模型降低计算成本",
        "关键词": [
            "全方位注意力",
            "元学习器",
            "自回归语言建模",
            "视觉Transformer",
            "高效自注意力"
        ],
        "涉及的技术概念": {
            "全方位注意力": "一种允许每个标记关注网络中所有标记的注意力机制，扩展了传统Transformer的感受野",
            "元学习器": "用于学习全方位注意力的自注意力模型，旨在优化注意力机制的学习过程",
            "高效自注意力模型": "如基于核的、低秩注意力和Big Bird，用于降低全方位注意力机制的计算成本"
        },
        "success": true
    },
    {
        "order": 740,
        "title": "On a Combination of Alternating Minimization and Nesterov's Momentum",
        "html": "https://ICML.cc//virtual/2021/poster/9963",
        "abstract": "Alternating minimization (AM) procedures are practically efficient in many applications for solving convex and non-convex optimization problems. On the other hand, Nesterov's accelerated gradient is theoretically optimal first-order method for convex optimization. In this paper we combine AM and Nesterov's acceleration to propose an accelerated alternating minimization algorithm. We prove $1/k^2$ convergence rate in terms of the objective for convex problems and $1/k$ in terms of the squared gradient norm for non-convex problems, where $k$ is the iteration counter. Our method does not require any knowledge of neither convexity of the problem nor function parameters such as Lipschitz constant of the gradient, i.e. it is adaptive to convexity and smoothness and is uniformly optimal for smooth convex and non-convex problems. Further, we develop its primal-dual modification for strongly convex problems with linear constraints and prove the same $1/k^2$ for the primal objective residual and constraints feasibility.",
        "conference": "ICML",
        "中文标题": "关于交替最小化与Nesterov动量相结合的算法",
        "摘要翻译": "交替最小化（AM）方法在许多应用中对于解决凸和非凸优化问题具有实际效率。另一方面，Nesterov的加速梯度法是理论上最优的一阶凸优化方法。在本文中，我们结合AM和Nesterov的加速方法，提出了一种加速交替最小化算法。我们证明了对于凸问题，在目标函数方面具有1/k^2的收敛速度，对于非凸问题，在平方梯度范数方面具有1/k的收敛速度，其中k是迭代计数器。我们的方法不需要任何关于问题凸性或函数参数（如梯度的Lipschitz常数）的知识，即它对凸性和平滑性具有自适应性，并且对于平滑的凸和非凸问题都是统一最优的。此外，我们针对具有线性约束的强凸问题开发了其原始-对偶修改版本，并证明了原始目标残差和约束可行性同样具有1/k^2的收敛速度。",
        "领域": "优化算法, 凸优化, 非凸优化",
        "问题": "如何结合交替最小化和Nesterov加速方法以提高优化问题的求解效率",
        "动机": "结合交替最小化的实践效率和Nesterov加速的理论最优性，开发一种既高效又理论上有保障的优化算法",
        "方法": "提出了一种结合交替最小化和Nesterov加速的算法，并针对凸和非凸问题分别证明了其收敛速度，同时开发了针对强凸问题的原始-对偶修改版本",
        "关键词": [
            "交替最小化",
            "Nesterov加速",
            "凸优化",
            "非凸优化",
            "原始-对偶方法"
        ],
        "涉及的技术概念": {
            "交替最小化": "一种优化方法，通过交替固定一部分变量优化另一部分变量来求解问题",
            "Nesterov加速": "一种加速梯度下降的技术，通过引入动量项来加快收敛速度",
            "原始-对偶方法": "一种解决约束优化问题的技术，通过同时优化原始问题和对偶问题来寻找解"
        },
        "success": true
    },
    {
        "order": 741,
        "title": "On Characterizing GAN Convergence Through Proximal Duality Gap",
        "html": "https://ICML.cc//virtual/2021/poster/8559",
        "abstract": "Despite the accomplishments of Generative Adversarial Networks (GANs) in modeling data distributions, training them remains a challenging task. A contributing factor to this difficulty is the non-intuitive nature of the GAN loss curves, which necessitates a subjective evaluation of the generated output to infer training progress. Recently, motivated by game theory, Duality Gap has been proposed as a domain agnostic measure to monitor GAN training. However, it is restricted to the setting when the GAN converges to a Nash equilibrium. But GANs need not always converge to a Nash equilibrium to model the data distribution. In this work, we extend the notion of duality gap to proximal duality gap that is applicable to the general context of training GANs where Nash equilibria may not exist. We show theoretically that the proximal duality gap can monitor the convergence of GANs to a broader spectrum of equilibria that subsumes Nash equilibria. We also theoretically establish the relationship between the proximal duality gap and the divergence between the real and generated data distributions for different GAN formulations. Our results provide new insights into the nature of GAN convergence. Finally, we validate experimentally the usefulness of proximal duality gap for monitoring and influencing GAN training. ",
        "conference": "ICML",
        "中文标题": "通过近端对偶间隙表征GAN收敛性",
        "摘要翻译": "尽管生成对抗网络（GANs）在建模数据分布方面取得了成就，但训练它们仍然是一项具有挑战性的任务。这种困难的一个因素是GAN损失曲线的非直观性质，这需要通过主观评估生成的输出来推断训练进度。最近，受博弈论的启发，对偶间隙被提出作为一种领域无关的度量来监控GAN训练。然而，它仅限于GAN收敛到纳什均衡的情况。但GANs并不总是需要收敛到纳什均衡来建模数据分布。在这项工作中，我们将对偶间隙的概念扩展到近端对偶间隙，适用于GANs训练的更一般情况，其中纳什均衡可能不存在。我们从理论上证明，近端对偶间隙可以监控GANs收敛到包含纳什均衡的更广泛的均衡谱。我们还从理论上建立了近端对偶间隙与不同GAN公式中真实和生成数据分布之间差异的关系。我们的结果为GAN收敛的性质提供了新的见解。最后，我们通过实验验证了近端对偶间隙在监控和影响GAN训练中的有用性。",
        "领域": "生成对抗网络、深度学习优化、博弈论与机器学习",
        "问题": "如何有效监控和评估生成对抗网络（GANs）的训练过程和收敛性",
        "动机": "现有的对偶间隙方法仅适用于GANs收敛到纳什均衡的情况，而GANs在实际训练中可能收敛到更广泛的均衡状态，需要一种更通用的监控方法",
        "方法": "扩展对偶间隙概念至近端对偶间隙，理论上分析其在更广泛均衡状态下的适用性，并通过实验验证其有效性",
        "关键词": [
            "生成对抗网络",
            "近端对偶间隙",
            "收敛性分析",
            "博弈论",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "近端对偶间隙": "扩展自对偶间隙的概念，用于监控GANs在更广泛均衡状态下的训练过程",
            "纳什均衡": "博弈论中的一种均衡状态，GANs训练中传统收敛性分析的目标",
            "数据分布差异": "衡量真实数据分布与生成数据分布之间差异的指标，与近端对偶间隙有理论上的关联"
        },
        "success": true
    },
    {
        "order": 742,
        "title": "On Disentangled Representations Learned from Correlated Data",
        "html": "https://ICML.cc//virtual/2021/poster/9351",
        "abstract": "The focus of disentanglement approaches has been on identifying independent factors of variation in data. However, the causal variables underlying real-world observations are often not statistically independent. In this work, we bridge the gap to real-world scenarios by analyzing the behavior of the most prominent disentanglement approaches on correlated data in a large-scale empirical study (including 4260 models). We show and quantify that systematically induced correlations in the dataset are being learned and reflected in the latent representations, which has implications for downstream applications of disentanglement such as fairness. We also demonstrate how to resolve these latent correlations, either using weak supervision during training or by post-hoc correcting a pre-trained model with a small number of labels.",
        "conference": "ICML",
        "中文标题": "关于从相关数据中学习解耦表示的研究",
        "摘要翻译": "解耦方法的核心在于识别数据中的独立变化因素。然而，现实世界观察背后的因果变量往往在统计上并不独立。在这项工作中，我们通过在大规模实证研究（包括4260个模型）中分析最突出的解耦方法在相关数据上的行为，弥合了与现实世界场景的差距。我们展示并量化了数据集中系统性引入的相关性被学习并反映在潜在表示中，这对解耦的下游应用（如公平性）具有影响。我们还展示了如何解决这些潜在相关性，无论是在训练过程中使用弱监督，还是通过用少量标签对预训练模型进行事后纠正。",
        "领域": "解耦表示学习、因果推理、公平性机器学习",
        "问题": "解耦表示学习方法在处理统计上不独立的因果变量时的表现和影响",
        "动机": "研究解耦表示学习方法在现实世界相关数据上的行为，以弥合理论与实际应用之间的差距",
        "方法": "通过大规模实证研究分析解耦方法在相关数据上的表现，并提出使用弱监督或事后纠正来解决潜在相关性",
        "关键词": [
            "解耦表示",
            "相关数据",
            "弱监督",
            "公平性",
            "潜在表示"
        ],
        "涉及的技术概念": {
            "解耦表示": "识别数据中的独立变化因素，旨在分离数据生成过程中的不同因素",
            "弱监督": "在训练过程中使用不完全或噪声标签来指导模型学习",
            "潜在表示": "数据在低维空间中的表示，捕捉了数据的关键特征和结构"
        },
        "success": true
    },
    {
        "order": 743,
        "title": "One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9419",
        "abstract": "In recent years, federated learning has been embraced as an approach for bringing about collaboration across large populations of learning agents. However, little is known about  how collaboration protocols should take agents' incentives into account when allocating individual resources for communal learning in order to maintain such collaborations. Inspired by game theoretic notions, this paper introduces a framework for incentive-aware learning and data sharing in federated learning. Our stable and envy-free equilibria capture notions of collaboration in the presence of agents interested in meeting their learning objectives while keeping their own sample collection burden low.  For example, in an envy-free equilibrium, no agent would wish to swap their sampling burden with any other agent and in a stable equilibrium, no agent would wish to unilaterally reduce their sampling burden.\n\nIn addition to formalizing this framework, our contributions include characterizing the structural properties of such equilibria, proving when they exist, and showing how they can be computed. Furthermore, we compare the sample complexity of incentive-aware collaboration with that of optimal collaboration when one ignores agents' incentives.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "一对一或全对全：联邦学习中协作的均衡与最优性",
        "摘要翻译": "近年来，联邦学习作为一种促进大量学习代理之间协作的方法被广泛接受。然而，关于协作协议在分配个体资源以进行共同学习时应如何考虑代理的激励以维持此类协作，人们知之甚少。受博弈论概念的启发，本文引入了一个激励感知学习和数据共享的框架在联邦学习中。我们的稳定和无嫉妒均衡捕捉了在代理有兴趣实现其学习目标同时保持自身样本收集负担低的情况下的协作概念。例如，在一个无嫉妒均衡中，没有代理希望与其他任何代理交换其采样负担；在一个稳定均衡中，没有代理希望单方面减少其采样负担。除了形式化这一框架外，我们的贡献还包括描述这些均衡的结构特性，证明它们何时存在，并展示如何计算它们。此外，我们比较了激励感知协作与忽略代理激励时的最优协作的样本复杂性。",
        "领域": "联邦学习, 博弈论在机器学习中的应用, 激励机制设计",
        "问题": "如何在联邦学习中设计协作协议，以考虑代理的激励并合理分配资源，维持协作的稳定性和效率。",
        "动机": "研究联邦学习中代理协作的激励机制，确保代理在追求个人学习目标的同时，愿意参与共同学习并分担合理的样本收集负担。",
        "方法": "引入基于博弈论的激励感知学习和数据共享框架，定义并分析稳定和无嫉妒均衡，研究这些均衡的存在性、结构特性及计算方法，并与忽略激励的最优协作进行比较。",
        "关键词": [
            "联邦学习",
            "博弈论",
            "激励机制",
            "均衡分析",
            "样本复杂性"
        ],
        "涉及的技术概念": {
            "稳定均衡": "在联邦学习中，代理不愿意单方面改变其采样负担的协作状态，确保协作的稳定性。",
            "无嫉妒均衡": "在联邦学习中，没有代理希望与其他代理交换采样负担的协作状态，反映公平性。",
            "激励感知学习": "在联邦学习中考虑代理个人激励的学习框架，旨在平衡个体目标和共同学习效率。"
        }
    },
    {
        "order": 744,
        "title": "On Energy-Based Models with Overparametrized Shallow Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10311",
        "abstract": "Energy-based models (EBMs) are a simple yet powerful framework for generative modeling. They are based on a trainable energy function which defines an associated Gibbs measure, and they can be trained and sampled from via well-established statistical tools, such as MCMC. Neural networks may be used as energy function approximators, providing both a rich class of expressive models as well as a flexible device to incorporate data structure.\r\nIn this work we focus on shallow neural networks. Building from the incipient theory of overparametrized neural networks, we show that models trained in the so-called 'active' regime provide a statistical advantage over their associated 'lazy' or kernel regime, leading to improved adaptivity to hidden low-dimensional structure in the data distribution, as already observed in supervised learning. Our study covers both the maximum likelihood and Stein Discrepancy estimators, and we validate our theoretical results with numerical experiments on synthetic data.",
        "conference": "ICML",
        "中文标题": "基于超参数化浅层神经网络的能量模型研究",
        "摘要翻译": "能量基础模型（EBMs）是一种简单而强大的生成模型框架。它们基于一个可训练的能量函数，该函数定义了一个相关的吉布斯测度，并且可以通过成熟的统计工具（如MCMC）进行训练和采样。神经网络可以作为能量函数的近似器，不仅提供了一类丰富的表达模型，还提供了一个灵活的设备来融入数据结构。在这项工作中，我们专注于浅层神经网络。从超参数化神经网络的初步理论出发，我们展示了在所谓的'活跃'机制下训练的模型比其相关的'懒惰'或核机制提供了统计上的优势，从而提高了对数据分布中隐藏的低维结构的适应性，这已经在监督学习中观察到。我们的研究涵盖了最大似然和斯坦差异估计器，并且我们通过在合成数据上的数值实验验证了我们的理论结果。",
        "领域": "生成模型、神经网络理论、统计学习",
        "问题": "探讨超参数化浅层神经网络在能量基础模型中的统计优势及其对数据低维结构的适应性。",
        "动机": "研究在能量基础模型中，超参数化浅层神经网络在'活跃'机制下的表现优于'懒惰'或核机制的原因，以及这种优势如何提高模型对数据低维结构的适应性。",
        "方法": "通过理论分析和数值实验，比较了最大似然和斯坦差异估计器在超参数化浅层神经网络中的应用，验证了'活跃'机制下的统计优势。",
        "关键词": [
            "能量基础模型",
            "超参数化神经网络",
            "统计优势",
            "低维结构适应性",
            "斯坦差异估计器"
        ],
        "涉及的技术概念": {
            "能量基础模型": "一种生成模型框架，通过可训练的能量函数定义数据分布，支持通过统计方法进行训练和采样。",
            "超参数化神经网络": "指神经网络的参数数量远大于训练样本数，这种配置在特定条件下可以提高模型的表达能力和适应性。",
            "斯坦差异估计器": "一种用于评估和优化概率分布之间差异的方法，本文中用于验证模型在捕捉数据低维结构方面的有效性。"
        },
        "success": true
    },
    {
        "order": 745,
        "title": "One Pass Late Fusion Multi-view Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/10273",
        "abstract": "Existing late fusion multi-view clustering (LFMVC) optimally integrates a group of pre-specified base partition matrices to learn a consensus one. It is then taken as the input of the widely used k-means to generate the cluster labels. As observed, the learning of the consensus partition matrix and the generation of cluster labels are separately done. These two procedures lack necessary negotiation and can not best serve for each other, which may adversely affect the clustering performance. To address this issue, we propose to unify the aforementioned two learning procedures into a single optimization, in which the consensus partition matrix can better serve for the generation of cluster labels, and the latter is able to guide the learning of the former. To optimize the resultant optimization problem, we develop a four-step alternate algorithm with proved convergence. We theoretically analyze the clustering generalization error of the proposed algorithm on unseen data. Comprehensive experiments on multiple benchmark datasets demonstrate the superiority of our algorithm in terms of both clustering accuracy and computational efficiency. It is expected that the simplicity and effectiveness of our algorithm will make it a good option to be considered for practical multi-view clustering applications. ",
        "conference": "ICML",
        "中文标题": "一次性后期融合多视图聚类",
        "摘要翻译": "现有的后期融合多视图聚类（LFMVC）方法通过最优整合一组预先指定的基础分区矩阵来学习一个共识分区矩阵。随后，该共识矩阵被用作广泛使用的k-means算法的输入以生成聚类标签。观察到，共识分区矩阵的学习和聚类标签的生成是分开进行的。这两个过程缺乏必要的协商，无法最好地相互服务，这可能会对聚类性能产生不利影响。为了解决这个问题，我们提出将上述两个学习过程统一到一个单一的优化中，其中共识分区矩阵可以更好地服务于聚类标签的生成，而后者能够指导前者的学习。为了优化由此产生的优化问题，我们开发了一个四步交替算法，并证明了其收敛性。我们从理论上分析了所提出算法在未见数据上的聚类泛化误差。在多个基准数据集上的综合实验证明了我们的算法在聚类准确性和计算效率方面的优越性。预计我们算法的简单性和有效性将使其成为实际多视图聚类应用中的一个良好选择。",
        "领域": "多视图聚类",
        "问题": "现有的后期融合多视图聚类方法中，共识分区矩阵的学习和聚类标签的生成过程分离，缺乏相互优化，影响聚类性能。",
        "动机": "统一共识分区矩阵的学习和聚类标签的生成过程，以相互优化，提升聚类性能。",
        "方法": "提出一个将共识分区矩阵学习和聚类标签生成统一到一个单一优化的方法，并开发了一个四步交替算法进行优化。",
        "关键词": [
            "多视图聚类",
            "后期融合",
            "共识分区矩阵",
            "k-means",
            "优化算法"
        ],
        "涉及的技术概念": {
            "后期融合多视图聚类（LFMVC）": "一种通过整合多个视图的基础分区矩阵来学习共识分区矩阵的聚类方法。",
            "共识分区矩阵": "在多视图聚类中，通过整合不同视图的信息学习到的统一分区矩阵，用于生成最终的聚类标签。",
            "四步交替算法": "为了解决提出的优化问题而开发的算法，通过交替优化不同变量来求解，具有收敛性保证。"
        },
        "success": true
    },
    {
        "order": 746,
        "title": "Oneshot Differentially Private Top-k Selection",
        "html": "https://ICML.cc//virtual/2021/poster/10061",
        "abstract": "Being able to efficiently and accurately select the top-$k$ elements with differential privacy is an integral component of various private data analysis tasks. In this paper, we present the oneshot Laplace mechanism, which generalizes the well-known Report Noisy Max~\\cite{dwork2014algorithmic} mechanism to reporting noisy top-$k$ elements. We show that the oneshot Laplace mechanism with a noise level of $\\widetilde{O}(\\sqrt{k}/\\eps)$ is approximately differentially private. Compared to the previous peeling approach of running Report Noisy Max $k$ times, the oneshot Laplace mechanism only adds noises and computes the top $k$ elements once, hence much more efficient for large $k$. In addition, our proof of privacy relies on a novel coupling technique that bypasses the composition theorems so without the linear dependence on $k$ which is inherent to various composition theorems. Finally, we present a novel application of efficient top-$k$ selection in the classical problem of ranking from pairwise comparisons.",
        "conference": "ICML",
        "中文标题": "一次性差分隐私Top-k选择",
        "摘要翻译": "能够高效且准确地在差分隐私条件下选择前k个元素是各种隐私数据分析任务的重要组成部分。在本文中，我们提出了一次性拉普拉斯机制，该机制将著名的Report Noisy Max机制推广到报告噪声前k个元素。我们展示了噪声水平为√k/ε的一次性拉普拉斯机制近似满足差分隐私。与之前运行k次Report Noisy Max的剥离方法相比，一次性拉普拉斯机制仅需添加噪声并计算前k个元素一次，因此对于大k来说效率更高。此外，我们的隐私证明依赖于一种新颖的耦合技术，绕过了组合定理，因此避免了各种组合定理固有的对k的线性依赖。最后，我们在从成对比较中进行排名的经典问题中展示了高效top-k选择的新应用。",
        "领域": "差分隐私、数据挖掘、机器学习",
        "问题": "在差分隐私条件下高效且准确地选择前k个元素",
        "动机": "提高在差分隐私条件下选择前k个元素的效率和准确性，以支持更广泛的隐私数据分析任务",
        "方法": "提出一次性拉普拉斯机制，通过一次噪声添加和计算过程选择前k个元素，避免了多次运行Report Noisy Max机制的低效问题",
        "关键词": [
            "差分隐私",
            "Top-k选择",
            "拉普拉斯机制",
            "数据挖掘",
            "隐私保护"
        ],
        "涉及的技术概念": {
            "一次性拉普拉斯机制": "一种在差分隐私条件下选择前k个元素的新方法，通过一次噪声添加和计算过程提高效率",
            "差分隐私": "一种隐私保护技术，确保数据分析过程中个体数据的隐私不被泄露",
            "耦合技术": "用于隐私证明的技术，绕过组合定理，避免了对k的线性依赖"
        },
        "success": true
    },
    {
        "order": 747,
        "title": "One-sided Frank-Wolfe algorithms for saddle problems",
        "html": "https://ICML.cc//virtual/2021/poster/8533",
        "abstract": "  We study a class of convex-concave saddle-point problems of the form\n  $\\min_x\\max_y \\langle Kx,y\\rangle+f_{\\cal P}(x)-h^*(y)$ where $K$ is\n  a linear operator, $f_{\\cal P}$ is the sum of a convex function $f$\n  with a Lipschitz-continuous gradient and the indicator function of a\n  bounded convex polytope ${\\cal P}$, and $h^\\ast$ is a convex (possibly\n  nonsmooth) function. Such problem arises, for example, as a\n  Lagrangian relaxation of various discrete optimization problems. Our\n  main assumptions are the existence of an efficient {\\em linear\n    minimization oracle} ($lmo$) for $f_{\\cal P}$ and an efficient {\\em\n    proximal map} ($prox$) for $h^*$ which motivate the solution via\n  a blend of proximal primal-dual algorithms and Frank-Wolfe\n  algorithms. In case $h^*$ is the indicator function of a linear\n  constraint and function $f$ is quadratic, we show a $O(1/n^2)$\n  convergence rate on the dual objective, requiring $O(n \\log n)$\n  calls of $lmo$. If the problem comes from the constrained\n  optimization problem $\\min_{x\\in\\mathbb\n    R^d}\\{f_{\\cal P}(x)\\:|\\:Ax-b=0\\}$ then we additionally get bound\n  $O(1/n^2)$ both on the primal gap and on the infeasibility gap.  In\n  the most general case, we show a $O(1/n)$ convergence rate of the\n  primal-dual gap again requiring $O(n\\log n)$ calls of $lmo$. To the\n  best of our knowledge, this improves on the known convergence rates\n  for the considered class of saddle-point problems.  We show\n  applications to labeling problems frequently appearing in machine\n  learning and computer vision.\n",
        "conference": "ICML",
        "success": true,
        "中文标题": "单边Frank-Wolfe算法求解鞍点问题",
        "摘要翻译": "我们研究了一类凸凹鞍点问题，形式为min_x max_y ⟨Kx,y⟩+f_P(x)-h*(y)，其中K是一个线性算子，f_P是一个凸函数f与一个有Lipschitz连续梯度的函数的和，以及一个有界凸多面体P的指示函数，h*是一个凸（可能非光滑）函数。这类问题例如作为各种离散优化问题的拉格朗日松弛出现。我们的主要假设是存在一个高效的线性最小化预言（lmo）用于f_P和一个高效的近端映射（prox）用于h*，这促使我们通过结合近端原始对偶算法和Frank-Wolfe算法来求解。在h*是线性约束的指示函数且函数f是二次的情况下，我们展示了在双重目标上的O(1/n^2)收敛率，需要O(n log n)次lmo调用。如果问题来自于约束优化问题min_{x∈R^d}{f_P(x)|Ax-b=0}，那么我们还能在原始间隙和不可行间隙上得到O(1/n^2)的界限。在最一般的情况下，我们展示了原始对偶间隙的O(1/n)收敛率，同样需要O(n log n)次lmo调用。据我们所知，这改进了所考虑的鞍点问题类别的已知收敛率。我们展示了在机器学习和计算机视觉中频繁出现的标记问题上的应用。",
        "领域": "凸优化, 机器学习优化, 计算机视觉优化",
        "问题": "解决一类特定的凸凹鞍点问题，特别是在存在高效线性最小化预言和近端映射的情况下。",
        "动机": "改进现有对于特定鞍点问题的求解方法，特别是在机器学习和计算机视觉中的应用。",
        "方法": "结合近端原始对偶算法和Frank-Wolfe算法，利用高效的线性最小化预言和近端映射来求解问题。",
        "关键词": [
            "Frank-Wolfe算法",
            "鞍点问题",
            "凸优化",
            "原始对偶算法",
            "近端映射"
        ],
        "涉及的技术概念": {
            "线性最小化预言（lmo）": "用于高效地找到给定线性函数在凸集上的最小值，是Frank-Wolfe算法的核心组成部分。",
            "近端映射（prox）": "用于处理非光滑凸函数的优化问题，能够有效地计算给定点的近端算子。",
            "原始对偶间隙": "衡量原始问题和对偶问题解之间差异的指标，用于评估算法的收敛性能。"
        }
    },
    {
        "order": 748,
        "title": "On Estimation in Latent Variable Models",
        "html": "https://ICML.cc//virtual/2021/poster/10071",
        "abstract": "Latent variable models have been playing a central role in statistics, econometrics, machine learning with applications to repeated observation study, panel data inference, user behavior analysis, etc. In many modern applications, the inference based on latent variable models involves one or several of the following features: the presence of complex latent structure, the observed and latent variables being continuous or discrete, constraints on parameters, and data size being large. Therefore, solving an estimation problem for general latent variable models is highly non-trivial. In this paper, we consider a gradient based method via using variance reduction technique to accelerate estimation procedure.  Theoretically, we show the convergence results for the proposed method under general and mild model assumptions. The algorithm has better computational complexity compared with the classical gradient methods and maintains nice statistical properties. Various numerical results corroborate our theory.",
        "conference": "ICML",
        "中文标题": "关于潜变量模型中的估计问题",
        "摘要翻译": "潜变量模型在统计学、计量经济学、机器学习中扮演着核心角色，应用于重复观察研究、面板数据推断、用户行为分析等领域。在许多现代应用中，基于潜变量模型的推断涉及以下一个或多个特征：复杂的潜结构存在、观察变量和潜变量为连续或离散、参数约束以及数据规模庞大。因此，解决一般潜变量模型的估计问题非常不平凡。本文中，我们考虑了一种基于梯度的方法，通过使用方差缩减技术来加速估计过程。理论上，我们在一般和温和的模型假设下展示了所提出方法的收敛结果。与经典梯度方法相比，该算法具有更好的计算复杂度，并保持了良好的统计特性。各种数值结果证实了我们的理论。",
        "领域": "统计机器学习、计量经济学、用户行为分析",
        "问题": "解决一般潜变量模型在复杂条件下的估计问题",
        "动机": "针对现代应用中潜变量模型估计面临的复杂性和大规模数据挑战，提出更高效的估计方法",
        "方法": "采用基于梯度的方法结合方差缩减技术，加速估计过程并保证统计特性",
        "关键词": [
            "潜变量模型",
            "梯度方法",
            "方差缩减",
            "统计推断",
            "计算复杂度"
        ],
        "涉及的技术概念": {
            "潜变量模型": "用于描述观察数据与未观察到的潜变量之间关系的统计模型",
            "梯度方法": "一种优化技术，通过迭代地沿着目标函数的梯度方向更新参数，以寻找最优解",
            "方差缩减技术": "用于减少估计过程中的方差，提高估计效率和准确性的技术"
        },
        "success": true
    },
    {
        "order": 749,
        "title": "On Explainability of Graph Neural Networks via Subgraph Explorations",
        "html": "https://ICML.cc//virtual/2021/poster/8873",
        "abstract": "We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level. ",
        "conference": "ICML",
        "中文标题": "关于通过子图探索解释图神经网络的可解释性",
        "摘要翻译": "我们考虑解释图神经网络（GNNs）预测的问题，这些网络原本被视为黑箱。现有方法总是聚焦于解释图节点或边的重要性，而忽略了图的子结构，后者更为直观且易于人类理解。在这项工作中，我们提出了一种名为SubgraphX的新方法，通过识别重要子图来解释GNNs。给定一个训练好的GNN模型和一个输入图，我们的SubgraphX通过使用蒙特卡洛树搜索高效探索不同子图来解释其预测。为了使树搜索更有效，我们提出使用沙普利值作为子图重要性的度量，它也能捕捉不同子图间的相互作用。为了加速计算，我们提出了计算图数据沙普利值的高效近似方案。我们的工作代表了通过明确和直接识别子图来解释GNNs的首次尝试。实验结果表明，我们的SubgraphX在保持计算量合理的同时，显著提高了解释的质量。",
        "领域": "图神经网络解释性、子图识别、模型可解释性",
        "问题": "解释图神经网络的预测，特别是通过识别重要子图来提高模型的可解释性。",
        "动机": "现有方法忽视了图的子结构在解释GNN预测中的重要性，而子结构更直观且易于人类理解。",
        "方法": "提出SubgraphX方法，利用蒙特卡洛树搜索探索不同子图，并使用沙普利值度量子图重要性及其相互作用。",
        "关键词": [
            "图神经网络",
            "子图识别",
            "模型可解释性",
            "沙普利值",
            "蒙特卡洛树搜索"
        ],
        "涉及的技术概念": {
            "子图识别": "通过识别图中的重要子结构来解释GNN的预测，提高模型的可解释性。",
            "沙普利值": "用于度量子图重要性及其相互作用，是解释GNN预测的关键指标。",
            "蒙特卡洛树搜索": "高效探索不同子图的方法，用于在解释过程中识别重要子图。"
        },
        "success": true
    },
    {
        "order": 750,
        "title": "On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting",
        "html": "https://ICML.cc//virtual/2021/poster/9199",
        "abstract": "Deep learning empirically achieves high performance in many applications, but its training dynamics has not been fully understood theoretically. In this paper, we explore theoretical analysis on training two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs. We show that with a specific regularization and sufficient over-parameterization, the student network can identify the parameters of the teacher network with high probability via gradient descent with a norm dependent stepsize even though the objective function is highly non-convex. The key theoretical tool is the measure representation of the neural networks and a novel application of a dual certificate argument for sparse estimation on a measure space. We analyze the global minima and global convergence property in the measure space.",
        "conference": "ICML",
        "中文标题": "关于教师-学生设置中两层ReLU神经网络通过梯度方法的可学习性",
        "摘要翻译": "深度学习在许多应用中经验性地实现了高性能，但其训练动态尚未在理论上得到充分理解。在本文中，我们探讨了在教师-学生回归模型中对训练两层ReLU神经网络的理论分析，其中学生网络通过其输出来学习一个未知的教师网络。我们表明，通过特定的正则化和足够的过参数化，学生网络可以通过具有依赖于范数的步长的梯度下降，以高概率识别教师网络的参数，即使目标函数是高度非凸的。关键的理论工具是神经网络的测度表示和在测度空间上对稀疏估计的双重证书论证的新颖应用。我们分析了测度空间中的全局最小值和全局收敛性质。",
        "领域": "深度学习理论、神经网络优化、回归分析",
        "问题": "理论上理解两层ReLU神经网络在教师-学生设置中的训练动态和可学习性",
        "动机": "探索深度学习在理论上为何能在高度非凸的目标函数下成功训练，特别是在教师-学生模型中",
        "方法": "使用特定的正则化和过参数化，通过梯度下降方法在测度空间中分析神经网络的全局收敛性",
        "关键词": [
            "ReLU神经网络",
            "梯度下降",
            "教师-学生模型",
            "测度表示",
            "双重证书论证"
        ],
        "涉及的技术概念": {
            "测度表示": "用于表示神经网络的参数分布，帮助分析网络的全局性质",
            "双重证书论证": "在测度空间上应用的一种技术，用于证明稀疏估计问题的解的唯一性和收敛性",
            "过参数化": "通过增加网络的参数数量，使得网络能够更容易地学习目标函数，即使目标函数是非凸的"
        },
        "success": true
    },
    {
        "order": 751,
        "title": "On Limited-Memory Subsampling Strategies for Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9321",
        "abstract": "There has been a recent surge of interest in non-parametric bandit algorithms\nbased on subsampling. One drawback however of these approaches is the\nadditional complexity required by random subsampling and the storage\nof the full history of rewards. Our first contribution is to show \nthat a simple deterministic subsampling rule, proposed in the recent work of\n\\citet{baudry2020sub} under the name of “last-block subsampling”, is \nasymptotically optimal in one-parameter exponential families. In \naddition, we prove that these guarantees also hold when limiting \nthe algorithm memory to a polylogarithmic function of the \ntime horizon. These findings open up new perspectives, in \nparticular for non-stationary scenarios in which the arm \ndistributions evolve over time. We propose a variant of the \nalgorithm in which only the most recent observations are \nused for subsampling, achieving optimal regret guarantees \nunder the assumption of a known number of abrupt changes. Extensive \nnumerical simulations highlight the merits of this approach, particularly \nwhen the changes are not only affecting the means of the rewards.",
        "conference": "ICML",
        "中文标题": "关于老虎机问题中有限内存子采样策略的研究",
        "摘要翻译": "近期，基于子采样的非参数老虎机算法引起了广泛关注。然而，这些方法的一个缺点是随机子采样所需的额外复杂性以及存储全部奖励历史的负担。我们的第一个贡献是证明了一种简单的确定性子采样规则——在最近的研究中被命名为“最后块子采样”——在单参数指数族中是渐进最优的。此外，我们证明了当将算法内存限制为时间范围的多对数函数时，这些保证同样成立。这些发现开辟了新的视角，特别是在臂分布随时间演变的非平稳场景中。我们提出了一种算法变体，其中仅使用最近的观测进行子采样，在已知突变次数假设下实现了最优的遗憾保证。大量的数值模拟突出了这种方法的优点，特别是当变化不仅影响奖励的均值时。",
        "领域": "强化学习、在线学习、非参数统计",
        "问题": "解决非参数老虎机算法中随机子采样的复杂性和存储全部奖励历史的问题",
        "动机": "探索在有限内存条件下实现渐进最优性能的子采样策略，特别是在非平稳环境中",
        "方法": "提出并分析了一种确定性子采样规则（最后块子采样），并开发了一种仅使用最近观测的算法变体",
        "关键词": [
            "非参数老虎机",
            "子采样策略",
            "有限内存",
            "非平稳环境",
            "渐进最优"
        ],
        "涉及的技术概念": {
            "最后块子采样": "一种确定性子采样规则，用于在单参数指数族中实现渐进最优性能",
            "多对数内存限制": "将算法内存限制为时间范围的多对数函数，以保持性能的同时减少存储需求",
            "非平稳环境": "臂分布随时间变化的情境，算法需要适应这种变化以保持最优性能"
        },
        "success": true
    },
    {
        "order": 752,
        "title": "Online A-Optimal Design and Active Linear Regression",
        "html": "https://ICML.cc//virtual/2021/poster/9369",
        "abstract": "We consider in this paper the problem of optimal experiment design where a decision maker can choose which points to sample to obtain an estimate $\\hat{\\beta}$ of the hidden parameter $\\beta^{\\star}$ of an underlying linear model.\nThe key challenge of this work lies in the heteroscedasticity assumption that we make, meaning that each covariate has a different and unknown variance.\nThe goal of the decision maker is then to figure out on the fly the optimal way to allocate the total budget of $T$ samples between covariates, as sampling several times a specific one  will reduce the variance of the estimated model around it (but at the cost of a possible higher variance elsewhere).\nBy trying to minimize the $\\ell^2$-loss $\\mathbb{E} [\\lVert\\hat{\\beta}-\\beta^{\\star}\\rVert^2]$ the decision maker is actually minimizing the trace of the covariance matrix of the problem, which corresponds then to online A-optimal design.\nCombining techniques from  bandit and convex optimization we propose a new active sampling algorithm and we compare it with existing ones. We provide theoretical guarantees of this algorithm in different settings, including a $\\mathcal{O}(T^{-2})$ regret bound in the case where the covariates form a basis of the feature space, generalizing and improving existing results. Numerical experiments validate our theoretical findings.",
        "conference": "ICML",
        "中文标题": "在线A最优设计与主动线性回归",
        "摘要翻译": "本文考虑了最优实验设计的问题，其中决策者可以选择采样哪些点来获得底层线性模型隐藏参数β*的估计值β^。这项工作的关键挑战在于我们所做的异方差性假设，即每个协变量具有不同且未知的方差。决策者的目标是实时找出在协变量之间分配总样本预算T的最优方式，因为对特定协变量进行多次采样将减少估计模型在其周围的方差（但可能以其他地方的更高方差为代价）。通过尝试最小化ℓ2损失E[∥β^−β*∥^2]，决策者实际上是在最小化问题协方差矩阵的迹，这对应于在线A最优设计。结合来自赌博机和凸优化的技术，我们提出了一种新的主动采样算法，并将其与现有算法进行了比较。我们在不同设置下提供了该算法的理论保证，包括在协变量形成特征空间基的情况下，推广并改进了现有结果的O(T^-2)遗憾界。数值实验验证了我们的理论发现。",
        "领域": "实验设计优化, 线性回归分析, 主动学习",
        "问题": "在异方差性假设下，如何动态分配样本预算以最小化线性回归模型的估计误差",
        "动机": "解决在协变量具有不同且未知方差的情况下，如何有效分配样本资源以优化线性回归模型的估计精度",
        "方法": "结合赌博机和凸优化技术，提出一种新的主动采样算法，并在理论上分析其性能",
        "关键词": [
            "在线A最优设计",
            "主动线性回归",
            "异方差性",
            "样本分配",
            "遗憾界"
        ],
        "涉及的技术概念": {
            "在线A最优设计": "在动态样本分配过程中，通过最小化协方差矩阵的迹来优化实验设计",
            "异方差性": "协变量具有不同且未知的方差，影响样本分配策略的设计",
            "主动采样算法": "结合赌博机和凸优化技术，动态决定采样点以优化模型估计精度"
        },
        "success": true
    },
    {
        "order": 753,
        "title": "On Linear Identifiability of Learned Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10089",
        "abstract": "Identifiability is a desirable property of a statistical model:  it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are over-parameterized by design.  In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.",
        "conference": "ICML",
        "中文标题": "论学习表示的线性可识别性",
        "摘要翻译": "可识别性是统计模型的一个理想属性：它意味着在给定足够的计算资源和数据的情况下，可以以任何所需的精度估计真实的模型参数。我们在表示学习的背景下研究可识别性：发现对于某些下游任务最优的非线性数据表示。当参数化为深度神经网络时，这样的表示函数在参数空间中缺乏可识别性，因为它们被设计为过度参数化。在本文中，基于非线性独立成分分析的最新进展，我们旨在通过展示一大类判别模型在函数空间中实际上是可识别的，直到一个线性不确定性，来恢复可识别性。许多领域的表示学习模型在这种意义上都是可识别的，包括文本、图像和音频，发表时处于最先进水平。我们推导了线性可识别的充分条件，并在模拟和真实世界数据上为结果提供了实证支持。",
        "领域": "表示学习、独立成分分析、深度学习理论",
        "问题": "深度神经网络在参数空间中缺乏可识别性，导致模型参数无法准确估计。",
        "动机": "恢复深度神经网络在表示学习中的可识别性，以便更准确地估计模型参数。",
        "方法": "基于非线性独立成分分析的进展，证明一大类判别模型在函数空间中可识别，并推导线性可识别的充分条件。",
        "关键词": [
            "表示学习",
            "可识别性",
            "独立成分分析",
            "深度学习理论",
            "函数空间"
        ],
        "涉及的技术概念": {
            "可识别性": "统计模型的一个理想属性，意味着可以准确估计模型参数。",
            "表示学习": "发现对于下游任务最优的非线性数据表示的过程。",
            "独立成分分析": "一种统计技术，用于将多变量数据分解为统计上独立的非高斯成分。"
        },
        "success": true
    },
    {
        "order": 754,
        "title": "Online Graph Dictionary Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8549",
        "abstract": "Dictionary learning is a key tool for representation learning, that explains the\r\ndata as linear combination of few basic elements. Yet, this analysis is not\r\namenable in the context of graph learning, as graphs usually belong to different\r\nmetric spaces. We fill this gap by proposing a new online Graph Dictionary\r\nLearning approach, which uses the Gromov Wasserstein divergence for the data\r\nfitting term. In our work, graphs are encoded through their nodes' pairwise relations \r\nand modeled as convex combination of graph\r\natoms, i.e. dictionary elements, estimated thanks to  an online stochastic\r\nalgorithm, which operates on a dataset of unregistered graphs with potentially\r\ndifferent number of nodes. Our approach naturally extends to labeled graphs, and\r\nis completed by a novel upper bound  that can be used as a fast approximation of\r\nGromov Wasserstein in the embedding space. We provide numerical evidences\r\nshowing the interest of our approach for unsupervised embedding of graph\r\ndatasets and for online graph subspace estimation and tracking.",
        "conference": "ICML",
        "中文标题": "在线图字典学习",
        "摘要翻译": "字典学习是表示学习的关键工具，它将数据解释为少数基本元素的线性组合。然而，这种分析在图学习的背景下并不适用，因为图通常属于不同的度量空间。我们通过提出一种新的在线图字典学习方法填补了这一空白，该方法使用Gromov Wasserstein散度作为数据拟合项。在我们的工作中，图通过其节点的成对关系进行编码，并被建模为图原子的凸组合，即字典元素，这些元素通过一个在线随机算法估计，该算法操作于一组未注册的图上，这些图可能具有不同数量的节点。我们的方法自然地扩展到标记图，并通过一个新的上界完成，该上界可以用作嵌入空间中Gromov Wasserstein的快速近似。我们提供了数值证据，显示了我们的方法在图数据集的无监督嵌入以及在线图子空间估计和跟踪中的兴趣。",
        "领域": "图表示学习、无监督学习、图嵌入",
        "问题": "解决图数据在不同度量空间中的表示学习问题",
        "动机": "填补图学习领域中字典学习方法的空白，提供一种能够处理未注册图且节点数量可能不同的在线学习方法",
        "方法": "提出了一种新的在线图字典学习方法，使用Gromov Wasserstein散度作为数据拟合项，通过在线随机算法估计图原子，并扩展到标记图，引入新的上界作为快速近似",
        "关键词": [
            "图字典学习",
            "Gromov Wasserstein散度",
            "在线学习",
            "图嵌入",
            "无监督学习"
        ],
        "涉及的技术概念": {
            "Gromov Wasserstein散度": "用于衡量不同度量空间之间图的相似性，作为数据拟合项",
            "在线随机算法": "用于在未注册图数据集上估计图原子，适应不同节点数量的图",
            "图嵌入": "将图数据映射到低维空间，便于进行无监督学习和子空间估计"
        },
        "success": true
    },
    {
        "order": 755,
        "title": "Online Learning for Load Balancing of Unknown Monotone Resource Allocation Games",
        "html": "https://ICML.cc//virtual/2021/poster/9009",
        "abstract": "Consider N players that each uses a mixture of K resources. Each of the players' reward functions includes a linear pricing term for each resource that is controlled by the game manager. We assume that the game is strongly monotone, so if each player runs gradient descent, the dynamics converge to a unique Nash equilibrium (NE). Unfortunately, this NE can be inefficient since the total load on a given resource can be very high. In principle, we can control the total loads by tuning the coefficients of the pricing terms. However, finding pricing coefficients that balance the loads requires knowing the players' reward functions and their action sets. Obtaining this game structure information is infeasible in a large-scale network and violates the users' privacy. To overcome this, we propose a simple algorithm that learns to shift the NE of the game to meet the total load constraints by adjusting the pricing coefficients in an online manner. Our algorithm only requires the total load per resource as feedback and does not need to know the reward functions or the action sets. We prove that our algorithm guarantees convergence in L2 to a NE that meets target total load constraints. Simulations show the effectiveness of our approach when applied to smart grid demand-side management or power control in wireless networks. ",
        "conference": "ICML",
        "中文标题": "未知单调资源分配游戏的在线学习负载均衡",
        "摘要翻译": "考虑N个玩家，每个玩家使用K种资源的混合。每个玩家的奖励函数包括由游戏管理者控制的每种资源的线性定价项。我们假设游戏是强单调的，因此如果每个玩家运行梯度下降，动态将收敛到唯一的纳什均衡（NE）。不幸的是，这个NE可能是低效的，因为给定资源上的总负载可能非常高。原则上，我们可以通过调整定价项的系数来控制总负载。然而，找到平衡负载的定价系数需要知道玩家的奖励函数及其行动集。在大规模网络中获取这种游戏结构信息是不可行的，并且会侵犯用户的隐私。为了克服这一点，我们提出了一种简单的算法，该算法通过在线方式调整定价系数，学习将游戏的NE转移到满足总负载约束。我们的算法仅需要每种资源的总负载作为反馈，不需要知道奖励函数或行动集。我们证明了我们的算法保证在L2中收敛到满足目标总负载约束的NE。模拟显示了我们方法在智能电网需求侧管理或无线网络功率控制中的应用效果。",
        "领域": "博弈论与机器学习结合、资源分配优化、在线学习算法",
        "问题": "如何在不知道玩家奖励函数和行动集的情况下，通过调整资源定价系数来实现负载均衡。",
        "动机": "解决大规模网络中因缺乏游戏结构信息而难以实现资源负载均衡的问题，同时保护用户隐私。",
        "方法": "提出一种在线学习算法，通过调整定价系数，利用总负载反馈，无需知道奖励函数或行动集，实现纳什均衡的转移以满足负载约束。",
        "关键词": [
            "在线学习",
            "负载均衡",
            "纳什均衡",
            "资源分配",
            "定价策略"
        ],
        "涉及的技术概念": {
            "强单调游戏": "确保玩家通过梯度下降动态收敛到唯一的纳什均衡。",
            "纳什均衡（NE）": "游戏中玩家策略的稳定状态，无玩家能单方面改变策略以增加收益。",
            "L2收敛": "算法在L2范数下的收敛性，保证调整后的定价系数能够有效引导游戏状态满足目标负载约束。"
        },
        "success": true
    },
    {
        "order": 756,
        "title": "Online Learning in Unknown Markov Games",
        "html": "https://ICML.cc//virtual/2021/poster/9341",
        "abstract": "We study online learning in unknown Markov games, a problem that arises in episodic multi-agent reinforcement learning where the actions of the opponents are unobservable. We show that in this challenging setting, achieving sublinear regret against the best response in hindsight is statistically hard. We then consider a weaker notion of regret by competing with the \\emph{minimax value} of the game, and present an algorithm that achieves a sublinear $\\tilde{\\mathcal{O}}(K^{2/3})$ regret after $K$ episodes. This is the first sublinear regret bound (to our knowledge) for online learning in unknown Markov games. Importantly, our regret bound is independent of the size of the opponents' action spaces. As a result, even when the opponents' actions are fully observable, our regret bound improves upon existing analysis (e.g., (Xie et al., 2020)) by an exponential factor in the number of opponents.",
        "conference": "ICML",
        "中文标题": "未知马尔可夫博弈中的在线学习",
        "摘要翻译": "我们研究了未知马尔可夫博弈中的在线学习问题，这是在情节式多智能体强化学习中出现的，其中对手的行为是不可观察的。我们表明，在这一具有挑战性的设置中，实现相对于事后最佳反应的亚线性遗憾在统计上是困难的。然后，我们考虑了一种较弱的遗憾概念，即与博弈的极小极大值竞争，并提出了一种算法，该算法在K个情节后实现了亚线性遗憾界O~(K^{2/3})。据我们所知，这是未知马尔可夫博弈中在线学习的第一个亚线性遗憾界。重要的是，我们的遗憾界与对手行动空间的大小无关。因此，即使对手的行为完全可观察，我们的遗憾界也通过对手数量的指数因子改进了现有分析（例如，(Xie et al., 2020)）。",
        "领域": "多智能体强化学习、在线学习、博弈论",
        "问题": "在对手行为不可观察的未知马尔可夫博弈中实现亚线性遗憾",
        "动机": "解决在多智能体环境中，由于对手行为不可观察而导致的在线学习挑战",
        "方法": "提出一种与博弈的极小极大值竞争的算法，实现亚线性遗憾界",
        "关键词": [
            "马尔可夫博弈",
            "在线学习",
            "多智能体强化学习",
            "遗憾界",
            "极小极大值"
        ],
        "涉及的技术概念": {
            "马尔可夫博弈": "用于建模多智能体交互的框架，其中每个智能体的回报依赖于其他智能体的行为",
            "亚线性遗憾": "算法性能的度量，表示随着时间增长，算法的累积遗憾增长速度低于线性",
            "极小极大值": "在博弈论中，表示在最坏情况下能够保证的最小最大损失，用于评估算法的稳健性"
        },
        "success": true
    },
    {
        "order": 757,
        "title": "Online Learning with Optimism and Delay",
        "html": "https://ICML.cc//virtual/2021/poster/9561",
        "abstract": "Inspired by the demands of real-time climate and weather forecasting, we develop optimistic online learning algorithms that require no parameter tuning and have optimal regret guarantees under delayed feedback. Our algorithms---DORM, DORM+, and AdaHedgeD---arise from a novel reduction of delayed online learning to optimistic online learning that reveals how optimistic hints can mitigate the regret penalty caused by delay. We pair this delay-as-optimism perspective with a new analysis of optimistic learning that exposes its robustness to hinting errors and a new meta-algorithm for learning effective hinting strategies in the presence of delay. We conclude by benchmarking our algorithms on four subseasonal climate forecasting tasks, demonstrating low regret relative to state-of-the-art forecasting models.",
        "conference": "ICML",
        "中文标题": "基于乐观与延迟的在线学习",
        "摘要翻译": "受实时气候和天气预报需求的启发，我们开发了无需参数调优且在延迟反馈下具有最优遗憾保证的乐观在线学习算法。我们的算法——DORM、DORM+和AdaHedgeD——源于将延迟在线学习新颖地简化为乐观在线学习，揭示了乐观提示如何减轻由延迟引起的遗憾惩罚。我们将这种延迟即乐观的视角与对乐观学习的新分析相结合，揭示了其对提示错误的鲁棒性，并提出了一种在延迟存在时学习有效提示策略的新元算法。最后，我们在四个次季节气候预测任务上对我们的算法进行了基准测试，展示了相对于最先进预测模型的低遗憾。",
        "领域": "在线学习算法、气候预测模型、延迟反馈优化",
        "问题": "开发无需参数调优且在延迟反馈下具有最优遗憾保证的在线学习算法",
        "动机": "受实时气候和天气预报需求的启发，解决延迟反馈下的在线学习问题",
        "方法": "通过将延迟在线学习简化为乐观在线学习，开发了DORM、DORM+和AdaHedgeD算法，并结合新分析揭示乐观学习的鲁棒性和提出新元算法",
        "关键词": [
            "在线学习",
            "延迟反馈",
            "气候预测",
            "乐观算法",
            "遗憾保证"
        ],
        "涉及的技术概念": {
            "乐观在线学习": "一种利用未来信息的提示来指导当前决策的在线学习方法，旨在减少遗憾",
            "延迟反馈": "指决策与反馈之间存在时间延迟的情况，影响学习算法的性能",
            "遗憾保证": "衡量算法性能的指标，表示算法累积损失与最佳固定策略累积损失之间的差异"
        },
        "success": true
    },
    {
        "order": 758,
        "title": "Online Limited Memory Neural-Linear Bandits with Likelihood Matching",
        "html": "https://ICML.cc//virtual/2021/poster/9331",
        "abstract": "We study neural-linear bandits for solving problems where {\\em both} exploration and representation learning play an important role. Neural-linear bandits harnesses the representation power of Deep Neural Networks (DNNs) and combines it with efficient exploration mechanisms by leveraging uncertainty estimation of the model, designed for linear contextual bandits on top of the last hidden layer. In order to mitigate the problem of representation change during the process, new uncertainty estimations are computed using stored data from an unlimited buffer. Nevertheless, when the amount of stored data is limited, a phenomenon called catastrophic forgetting emerges. To alleviate this, we propose a likelihood matching algorithm that is resilient to catastrophic forgetting and is completely online. We applied our algorithm, Limited Memory Neural-Linear with Likelihood Matching (NeuralLinear-LiM2) on a variety of datasets and observed that our algorithm achieves comparable performance to the unlimited memory approach while exhibits resilience to catastrophic forgetting.",
        "conference": "ICML",
        "中文标题": "在线有限记忆神经线性老虎机与似然匹配",
        "摘要翻译": "我们研究了神经线性老虎机方法，用于解决探索和表示学习均起重要作用的问题。神经线性老虎机利用深度神经网络（DNNs）的表示能力，并通过利用模型的不确定性估计，将其与高效的探索机制相结合，该机制设计用于在最后一个隐藏层之上的线性上下文老虎机。为了缓解过程中表示变化的问题，使用来自无限缓冲区的存储数据计算新的不确定性估计。然而，当存储数据量有限时，会出现称为灾难性遗忘的现象。为了缓解这一问题，我们提出了一种对灾难性遗忘具有弹性且完全在线的似然匹配算法。我们将我们的算法——有限记忆神经线性与似然匹配（NeuralLinear-LiM2）应用于各种数据集，并观察到我们的算法在表现出对灾难性遗忘的弹性的同时，实现了与无限记忆方法相当的性能。",
        "领域": "强化学习、在线学习、表示学习",
        "问题": "在有限存储条件下，如何有效结合深度神经网络的表示能力和线性上下文老虎机的高效探索机制，同时避免灾难性遗忘。",
        "动机": "解决在有限存储条件下，神经线性老虎机方法中因表示变化和灾难性遗忘导致性能下降的问题。",
        "方法": "提出了一种完全在线的似然匹配算法，该算法对灾难性遗忘具有弹性，并在有限存储条件下有效结合了深度神经网络的表示能力和线性上下文老虎机的探索机制。",
        "关键词": [
            "神经线性老虎机",
            "灾难性遗忘",
            "似然匹配",
            "在线学习",
            "表示学习"
        ],
        "涉及的技术概念": {
            "神经线性老虎机": "结合深度神经网络的表示能力和线性上下文老虎机的高效探索机制的方法。",
            "灾难性遗忘": "在学习新任务时，模型忘记先前学习任务的现象。",
            "似然匹配": "一种算法，用于在有限存储条件下保持模型性能，避免灾难性遗忘。"
        },
        "success": true
    },
    {
        "order": 759,
        "title": "Online Optimization in Games via Control Theory: Connecting Regret, Passivity and Poincaré Recurrence",
        "html": "https://ICML.cc//virtual/2021/poster/9837",
        "abstract": "We present a novel control-theoretic understanding of online optimization and learning in games, via the notion of passivity. Passivity is a fundamental concept in control theory, which abstracts energy conservation and dissipation in physical systems. It has become a standard tool in analysis of general feedback systems, to which game dynamics belong. Our starting point is to show that all continuous-time Follow-the-Regularized-Leader (FTRL) dynamics, which include the well-known Replicator Dynamic, are lossless, i.e. it is passive with no energy dissipation. Interestingly, we prove that passivity implies bounded regret, connecting two fundamental primitives of control theory and online optimization.\n\nThe observation of energy conservation in FTRL inspires us to present a family of lossless learning dynamics, each of which has an underlying energy function with a simple gradient structure. This family is closed under convex combination; as an immediate corollary, any convex combination of FTRL dynamics is lossless and thus has bounded regret. This allows us to extend the framework of Fox & Shamma [Games 2013] to prove not just global asymptotic stability results for game dynamics, but Poincaré recurrence results as well. Intuitively, when a lossless game (e.g. graphical constant-sum game) is coupled with lossless learning dynamic, their interconnection is also lossless, which results in a pendulum-like energy-preserving recurrent behavior, generalizing Piliouras & Shamma [SODA 2014] and Mertikopoulos et al. [SODA 2018].",
        "conference": "ICML",
        "中文标题": "通过控制理论在游戏中进行在线优化：连接遗憾、被动性与庞加莱回归",
        "摘要翻译": "我们通过被动性这一概念，提出了一种新颖的控制理论视角来理解游戏中的在线优化和学习。被动性是控制理论中的一个基本概念，它抽象了物理系统中的能量守恒和耗散。它已成为分析一般反馈系统（游戏动态属于此类系统）的标准工具。我们的出发点是展示所有连续时间的跟随正则化领导者（FTRL）动态，包括著名的复制动态，都是无损的，即它是被动的且没有能量耗散。有趣的是，我们证明了被动性意味着有界的遗憾，从而连接了控制理论和在线优化的两个基本原语。FTRL中能量守恒的观察启发我们提出了一系列无损学习动态，每一种动态都有一个具有简单梯度结构的底层能量函数。这一系列在凸组合下是封闭的；作为一个直接的推论，任何FTRL动态的凸组合都是无损的，因此具有有界的遗憾。这使我们能够扩展Fox & Shamma [Games 2013]的框架，不仅证明了游戏动态的全局渐近稳定性结果，而且还证明了庞加莱回归结果。直观地说，当一个无损游戏（例如图形常数和游戏）与无损学习动态耦合时，它们的互连也是无损的，这导致了类似钟摆的能量保持回归行为，推广了Piliouras & Shamma [SODA 2014]和Mertikopoulos等人[SODA 2018]的工作。",
        "领域": "在线优化、游戏理论、控制理论",
        "问题": "如何在游戏中通过控制理论实现有效的在线优化和学习",
        "动机": "探索被动性概念在游戏动态和在线优化中的应用，以连接控制理论和在线优化的基本原语",
        "方法": "通过展示FTRL动态的无损性，提出一系列无损学习动态，并利用凸组合的性质扩展现有框架",
        "关键词": [
            "被动性",
            "FTRL动态",
            "庞加莱回归",
            "无损学习",
            "游戏动态"
        ],
        "涉及的技术概念": {
            "被动性": "控制理论中的基本概念，用于抽象物理系统中的能量守恒和耗散，在分析游戏动态中起到关键作用",
            "FTRL动态": "连续时间的跟随正则化领导者动态，包括复制动态，展示无损性质",
            "庞加莱回归": "在无损系统和动态中观察到的周期性行为，类似于钟摆的能量保持运动"
        },
        "success": true
    },
    {
        "order": 760,
        "title": "Online Policy Gradient for Model Free Learning of Linear Quadratic Regulators with √T Regret",
        "html": "https://ICML.cc//virtual/2021/poster/10103",
        "abstract": "We consider the task of learning to control a linear dynamical system under fixed quadratic costs, known as the Linear Quadratic Regulator (LQR) problem. While model-free approaches are often favorable in practice, thus far only model-based methods, which rely on costly system identification, have been shown to achieve regret that scales with the optimal dependence on the time horizon T. We present the first model-free algorithm that achieves similar regret guarantees. Our method relies on an efficient policy gradient scheme, and a novel and tighter analysis of the cost of exploration in policy space in this setting.",
        "conference": "ICML",
        "中文标题": "线性二次调节器无模型学习的在线策略梯度方法及其√T遗憾",
        "摘要翻译": "我们考虑在固定二次成本下学习控制线性动态系统的任务，即线性二次调节器（LQR）问题。虽然无模型方法在实践中往往更受青睐，但迄今为止，只有依赖于昂贵系统识别的基于模型的方法被证明能够实现与时间范围T最优依赖关系相匹配的遗憾。我们提出了第一个能够实现类似遗憾保证的无模型算法。我们的方法依赖于高效的策略梯度方案，以及在此设置中对策略空间中探索成本的新颖且更严格的分析。",
        "领域": "强化学习、控制系统优化、在线学习",
        "问题": "在无模型条件下学习控制线性动态系统，实现与时间范围T最优依赖关系相匹配的遗憾",
        "动机": "解决基于模型方法依赖昂贵系统识别的问题，提出更高效的无模型学习算法",
        "方法": "采用高效的策略梯度方案，结合对策略空间中探索成本的新颖且更严格的分析",
        "关键词": [
            "线性二次调节器",
            "无模型学习",
            "策略梯度",
            "在线学习",
            "遗憾最小化"
        ],
        "涉及的技术概念": {
            "线性二次调节器（LQR）": "用于在固定二次成本下控制线性动态系统的问题框架",
            "策略梯度": "一种直接优化策略参数的强化学习方法，用于在无模型条件下学习控制策略",
            "遗憾最小化": "衡量算法性能与最优策略之间差异的指标，目标是使这一差异最小化"
        },
        "success": true
    },
    {
        "order": 761,
        "title": "Online Selection Problems against Constrained Adversary",
        "html": "https://ICML.cc//virtual/2021/poster/9829",
        "abstract": "Inspired by a recent line of work in online algorithms with predictions, we study the constrained adversary model that utilizes predictions from a different perspective.  Prior works mostly focused on designing simultaneously robust and consistent algorithms, without making assumptions on the quality of the predictions. In contrary, our model assumes the adversarial instance is consistent with the predictions and aim to design algorithms that have best worst-case performance against all such instances. We revisit classical online selection problems under the constrained adversary model. For the single item selection problem, we design an optimal algorithm in the adversarial arrival model and an improved algorithm in the random arrival model (a.k.a., the secretary problem). For the online edge-weighted bipartite matching problem, we extend the classical Water-filling and Ranking algorithms and achieve improved competitive ratios.",
        "conference": "ICML",
        "中文标题": "针对受限对手的在线选择问题",
        "摘要翻译": "受到近期一系列关于带预测的在线算法研究的启发，我们从不同角度研究了利用预测的受限对手模型。先前的工作大多集中在设计同时具有鲁棒性和一致性的算法，而没有对预测质量做出假设。相反，我们的模型假设对抗实例与预测一致，并旨在设计针对所有此类实例具有最佳最坏情况性能的算法。我们在受限对手模型下重新审视了经典的在线选择问题。对于单项目选择问题，我们在对抗到达模型中设计了一个最优算法，并在随机到达模型（即秘书问题）中设计了一个改进的算法。对于在线边加权二分图匹配问题，我们扩展了经典的Water-filling和Ranking算法，并实现了改进的竞争比。",
        "领域": "在线算法、对抗学习、组合优化",
        "问题": "研究在受限对手模型下的在线选择问题，旨在设计具有最佳最坏情况性能的算法。",
        "动机": "从不同角度利用预测来改进在线算法的性能，特别是在对抗实例与预测一致的假设下。",
        "方法": "设计最优算法和改进算法，分别针对单项目选择问题和在线边加权二分图匹配问题，扩展经典算法以提高竞争比。",
        "关键词": [
            "在线选择问题",
            "受限对手模型",
            "竞争比",
            "Water-filling算法",
            "Ranking算法"
        ],
        "涉及的技术概念": {
            "受限对手模型": "假设对抗实例与预测一致，用于设计具有最佳最坏情况性能的算法。",
            "Water-filling算法": "用于在线边加权二分图匹配问题的经典算法，本文中进行了扩展以提高性能。",
            "Ranking算法": "另一种用于在线边加权二分图匹配问题的经典算法，本文中同样进行了扩展以提高竞争比。"
        },
        "success": true
    },
    {
        "order": 762,
        "title": "Online Submodular Resource Allocation with Applications to Rebalancing Shared Mobility Systems",
        "html": "https://ICML.cc//virtual/2021/poster/9237",
        "abstract": "Motivated by applications in shared mobility, we address the problem of allocating a group of agents to a set of resources to maximize a cumulative welfare objective. We model the welfare obtainable from each resource as a monotone DR-submodular function which is a-priori unknown and can only be learned by observing the welfare of selected allocations. Moreover, these functions can depend on time-varying contextual information. We propose a distributed scheme to maximize the cumulative welfare by designing a repeated game among the agents, who learn to act via regret minimization. We propose two design choices for the game rewards based on upper confidence bounds built around the unknown welfare functions. We analyze them theoretically, bounding the gap between the cumulative welfare of the game and the highest cumulative welfare obtainable in hindsight. Finally, we evaluate our approach in a realistic case study of rebalancing a shared mobility system (i.e., positioning vehicles in strategic areas). From observed trip data, our algorithm gradually learns the users' demand pattern and improves the overall system operation.",
        "conference": "ICML",
        "中文标题": "在线子模资源分配及其在共享移动系统再平衡中的应用",
        "摘要翻译": "受共享移动应用的启发，我们解决了将一组代理分配到一组资源以最大化累积福利目标的问题。我们将从每个资源可获得的福利建模为一个单调DR-子模函数，该函数是事先未知的，只能通过观察选定分配的福利来学习。此外，这些函数可以依赖于随时间变化的上下文信息。我们提出了一种分布式方案，通过设计代理之间的重复游戏来最大化累积福利，代理通过遗憾最小化学习行动。我们基于围绕未知福利函数构建的上置信界，提出了两种游戏奖励的设计选择。我们从理论上分析了它们，限制了游戏累积福利与事后可获得的最高累积福利之间的差距。最后，我们在共享移动系统再平衡（即在战略区域定位车辆）的现实案例研究中评估了我们的方法。从观察到的行程数据中，我们的算法逐渐学习用户的需求模式并改善整体系统运行。",
        "领域": "共享经济优化、在线学习算法、分布式资源分配",
        "问题": "如何在资源分配中最大化累积福利，特别是在共享移动系统中进行车辆再平衡。",
        "动机": "解决共享移动系统中资源分配效率低下的问题，通过学习用户需求模式优化系统运行。",
        "方法": "提出了一种基于重复游戏和遗憾最小化的分布式方案，利用上置信界设计游戏奖励，以学习并优化资源分配。",
        "关键词": [
            "子模优化",
            "在线学习",
            "共享移动系统",
            "资源分配",
            "遗憾最小化"
        ],
        "涉及的技术概念": {
            "DR-子模函数": "用于建模从资源中可获得的福利，具有单调性，能够有效描述资源分配的边际效益递减特性。",
            "上置信界": "用于设计游戏奖励，帮助代理在不确定性下做出决策，平衡探索与利用。",
            "遗憾最小化": "代理通过学习最小化长期遗憾，优化其行动策略，以提高累积福利。"
        },
        "success": true
    },
    {
        "order": 763,
        "title": "Online Unrelated Machine Load Balancing with Predictions Revisited",
        "html": "https://ICML.cc//virtual/2021/poster/10465",
        "abstract": "We study the online load balancing problem with machine learned predictions, and give results that improve upon and extend those in a recent paper by Lattanzi et al. (2020). First, we design deterministic and randomized online rounding algorithms for the problem in the unrelated machine setting, with $O(\\frac{\\log m}{\\log \\log m})$- and $O(\\frac{\\log \\log m}{\\log \\log \\log m})$-competitive ratios. They respectively improve upon the previous ratios of $O(\\log m)$ and $O(\\log^3\\log m)$, and match the lower bounds given by Lattanzi et al. Second, we extend their prediction scheme from the identical machine restricted assignment setting to the unrelated machine setting. With the knowledge of two vectors over machines, a dual vector and a weight vector, we can construct a good fractional assignment online, that can be passed to an online rounding algorithm. Finally, we consider the learning model introduced by Lavastida et al. (2020), and show that under the model, the two vectors can be learned efficiently with a few samples of instances.",
        "conference": "ICML",
        "中文标题": "在线无关机器负载均衡与预测再探",
        "摘要翻译": "我们研究了带有机器学习预测的在线负载均衡问题，并给出了改进和扩展Lattanzi等人（2020）近期论文中结果的研究。首先，我们为无关机器设置下的问题设计了确定性和随机性的在线舍入算法，分别具有O(log m/log log m)和O(log log m/log log log m)的竞争比。这些结果分别改进了之前的O(log m)和O(log^3 log m)竞争比，并且与Lattanzi等人给出的下界相匹配。其次，我们将他们的预测方案从相同的机器限制分配设置扩展到无关机器设置。通过了解两个关于机器的向量，一个对偶向量和一个权重向量，我们可以在线构建一个好的分数分配，该分配可以传递给在线舍入算法。最后，我们考虑了Lavastida等人（2020）引入的学习模型，并表明在该模型下，可以通过少量实例样本有效地学习这两个向量。",
        "领域": "负载均衡算法, 机器学习应用, 在线算法",
        "问题": "改进和扩展在线负载均衡问题中的机器学习预测应用，特别是在无关机器设置下的竞争比和预测方案的扩展。",
        "动机": "为了提升在线负载均衡问题的解决效率，特别是在无关机器设置下，通过机器学习预测来优化竞争比和扩展预测方案的应用范围。",
        "方法": "设计了确定性和随机性的在线舍入算法，扩展了预测方案到无关机器设置，并验证了在特定学习模型下向量学习的效率。",
        "关键词": [
            "在线负载均衡",
            "机器学习预测",
            "无关机器设置",
            "竞争比",
            "在线舍入算法"
        ],
        "涉及的技术概念": {
            "在线舍入算法": "用于在无关机器设置下在线构建分数分配，以优化负载均衡的竞争比。",
            "竞争比": "衡量在线算法性能的指标，本研究中的算法改进了现有结果的竞争比。",
            "机器学习预测": "利用机器学习技术预测负载分配，以优化在线负载均衡问题的解决方案。"
        },
        "success": true
    },
    {
        "order": 764,
        "title": "On Lower Bounds for Standard and Robust Gaussian Process Bandit Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10639",
        "abstract": "In this paper, we consider algorithm independent lower bounds for the problem of black-box optimization of functions having a bounded norm is some Reproducing Kernel Hilbert Space (RKHS), which can be viewed as a non-Bayesian Gaussian process bandit problem.  In the standard noisy setting, we provide a novel proof technique for deriving lower bounds on the regret, with benefits including simplicity, versatility, and an improved dependence on the error probability.  In a robust setting in which the final point is perturbed by an adversary, we strengthen an existing lower bound that only holds for target success probabilities very close to one, by allowing for arbitrary target success probabilities in (0, 1).  Furthermore, in a distinct robust setting in which every sampled point may be perturbed by a constrained adversary, we provide a novel lower bound for deterministic strategies, demonstrating an inevitable joint dependence of the cumulative regret on the corruption level and the time horizon, in contrast with existing lower bounds that only characterize the individual dependencies.",
        "conference": "ICML",
        "中文标题": "关于标准与鲁棒高斯过程赌博机优化下界的研究",
        "摘要翻译": "本文考虑了在某个再生核希尔伯特空间（RKHS）中具有有界范数的函数黑盒优化问题的算法独立下界，这可以被视为一种非贝叶斯高斯过程赌博机问题。在标准噪声设置下，我们提供了一种新的证明技术来推导遗憾下界，其优点包括简单性、多功能性以及对错误概率的改进依赖。在一个鲁棒设置中，最终点被对手扰动，我们通过允许(0, 1)内的任意目标成功概率，加强了一个仅适用于非常接近一的目标成功概率的现有下界。此外，在另一个鲁棒设置中，每个采样点都可能受到约束对手的扰动，我们为确定性策略提供了一个新的下界，展示了累积遗憾对腐败水平和时间范围的不可避免的联合依赖，与仅描述个体依赖的现有下界形成对比。",
        "领域": "强化学习、优化算法、鲁棒性分析",
        "问题": "研究在黑盒优化问题中，特别是在具有有界范数的再生核希尔伯特空间中的函数优化，算法独立的下界问题。",
        "动机": "为了在标准噪声和鲁棒设置下，提供更通用和强大的下界证明技术，以更好地理解和评估优化算法的性能极限。",
        "方法": "在标准噪声设置下，采用新的证明技术推导遗憾下界；在鲁棒设置中，通过允许任意目标成功概率加强现有下界，并为确定性策略提供新的下界。",
        "关键词": [
            "高斯过程赌博机",
            "黑盒优化",
            "再生核希尔伯特空间",
            "鲁棒性分析",
            "算法独立下界"
        ],
        "涉及的技术概念": {
            "再生核希尔伯特空间（RKHS）": "用于描述具有有界范数的函数空间，是研究黑盒优化问题的基础。",
            "算法独立下界": "指不依赖于特定算法的性能极限，用于评估优化问题的最优解。",
            "鲁棒性分析": "研究在存在扰动或不确定性的情况下，算法性能的稳定性和可靠性。"
        },
        "success": true
    },
    {
        "order": 765,
        "title": "On Monotonic Linear Interpolation of Neural Network Parameters",
        "html": "https://ICML.cc//virtual/2021/poster/10429",
        "abstract": "Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. 2014, persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network --- providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g., network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties.",
        "conference": "ICML",
        "中文标题": "神经网络参数的单调线性插值研究",
        "摘要翻译": "在初始神经网络参数与通过随机梯度下降（SGD）训练后的收敛参数之间进行线性插值，通常会导致训练目标的单调递减。这种单调线性插值（MLI）属性，最初由Goodfellow等人在2014年观察到，尽管神经网络的目标函数非凸且训练动态高度非线性，该属性仍然持续存在。扩展这项工作，我们评估了关于这一属性的几种假设，据我们所知，这些假设尚未被探索。利用微分几何的工具，我们建立了函数空间中插值路径与网络单调性之间的联系——为均方误差下的MLI属性提供了充分条件。虽然MLI属性在各种设置下（例如，网络架构和学习问题）成立，但我们实践中发现，通过鼓励权重远离初始化，可以系统地产生违反MLI属性的网络。MLI属性提出了关于神经网络损失景观几何的重要问题，并强调了进一步研究其全局属性的必要性。",
        "领域": "深度学习理论、优化算法、神经网络训练动态",
        "问题": "探索神经网络参数在初始和训练后状态之间线性插值时的单调性行为",
        "动机": "理解并解释神经网络在训练过程中参数插值表现出的单调性现象，以及这一现象背后的数学原理",
        "方法": "利用微分几何工具分析插值路径与网络单调性的关系，并通过实验验证在不同条件下MLI属性的成立与违反情况",
        "关键词": [
            "单调线性插值",
            "神经网络训练",
            "损失景观几何",
            "微分几何",
            "随机梯度下降"
        ],
        "涉及的技术概念": {
            "单调线性插值（MLI）": "描述神经网络参数在初始和训练后状态之间线性插值时，训练目标单调递减的现象",
            "微分几何": "用于分析插值路径与网络单调性之间关系的数学工具",
            "损失景观几何": "研究神经网络损失函数的高维几何形状，以理解优化过程和模型行为"
        },
        "success": true
    },
    {
        "order": 766,
        "title": "On-Off Center-Surround Receptive Fields for Accurate and Robust Image Classification",
        "html": "https://ICML.cc//virtual/2021/poster/10391",
        "abstract": "Robustness to variations in lighting conditions is a key objective for any deep vision system. To this end, our paper extends the receptive field of convolutional neural networks with two residual components, ubiquitous in the visual processing system of vertebrates: On-center and off-center pathways, with an excitatory center and inhibitory surround; OOCS for short. The On-center pathway is excited by the presence of a light stimulus in its center, but not in its surround, whereas the Off-center pathway is excited by the absence of a light stimulus in its center, but not in its surround. We design OOCS pathways via a difference of Gaussians, with their variance computed analytically from the size of the receptive fields. OOCS pathways complement each other in their response to light stimuli, ensuring this way a strong edge-detection capability, and as a result an accurate and robust inference under challenging lighting conditions. We provide extensive empirical evidence showing that networks supplied with OOCS pathways gain accuracy and illumination-robustness from the novel edge representation, compared to other baselines.",
        "conference": "ICML",
        "中文标题": "开-关中心-周边感受野用于精确且鲁棒的图像分类",
        "摘要翻译": "对光照条件变化的鲁棒性是任何深度视觉系统的关键目标。为此，我们的论文通过两种在脊椎动物视觉处理系统中普遍存在的残差组件扩展了卷积神经网络的感受野：开中心和关中心通路，简称OOCS，具有兴奋中心和抑制周边。开中心通路由其中心存在光刺激而非周边存在光刺激而兴奋，而关中心通路则由其中心不存在光刺激而非周边不存在光刺激而兴奋。我们通过高斯差分设计了OOCS通路，其方差从感受野的大小分析计算得出。OOCS通路在它们对光刺激的响应中相互补充，确保了强大的边缘检测能力，进而在具有挑战性的光照条件下实现精确且鲁棒的推理。我们提供了广泛的实证证据，表明与其它基线相比，配备OOCS通路的网络从新颖的边缘表示中获得了准确性和光照鲁棒性。",
        "领域": "图像分类",
        "问题": "提高深度视觉系统在不同光照条件下的鲁棒性和准确性",
        "动机": "通过模拟脊椎动物视觉系统中的开中心和关中心通路，增强卷积神经网络对光照变化的适应能力",
        "方法": "通过高斯差分设计开中心和关中心通路（OOCS），扩展卷积神经网络的感受野，以增强边缘检测能力和光照鲁棒性",
        "关键词": [
            "开-关中心-周边感受野",
            "图像分类",
            "光照鲁棒性",
            "边缘检测",
            "卷积神经网络"
        ],
        "涉及的技术概念": {
            "开-关中心-周边感受野（OOCS）": "模拟脊椎动物视觉系统中的两种通路，用于增强网络对光照变化的适应能力和边缘检测",
            "高斯差分": "用于设计OOCS通路的方法，通过分析感受野大小计算方差，实现精确的边缘检测",
            "光照鲁棒性": "指网络在不同光照条件下保持性能稳定的能力，OOCS通路通过增强边缘检测能力来提高这一性能"
        },
        "success": true
    },
    {
        "order": 767,
        "title": "On Perceptual Lossy Compression: The Cost of Perceptual Reconstruction and An Optimal Training Framework",
        "html": "https://ICML.cc//virtual/2021/poster/10279",
        "abstract": "Lossy compression algorithms are typically designed to achieve the lowest possible distortion at a given bit rate. However, recent studies show that pursuing high perceptual quality would lead to increase of the lowest achievable distortion (e.g., MSE). This paper provides nontrivial results theoretically revealing that, 1) the cost of achieving perfect perception quality is exactly a doubling of the lowest achievable MSE distortion, 2) an optimal encoder for the “classic” rate-distortion problem is also optimal for the perceptual compression problem, 3) distortion loss is unnecessary for training a perceptual decoder. Further, we propose a novel training framework to achieve the lowest MSE distortion under perfect perception constraint at a given bit rate. This framework uses a GAN with discriminator conditioned on an MSE-optimized encoder, which is superior over the traditional framework using distortion plus adversarial loss. Experiments are provided to verify the theoretical finding and demonstrate the superiority of the proposed training framework.\n",
        "conference": "ICML",
        "中文标题": "论感知有损压缩：感知重建的代价与最优训练框架",
        "摘要翻译": "有损压缩算法通常设计为在给定比特率下实现尽可能低的失真。然而，最近的研究表明，追求高感知质量会导致最低可达到的失真（例如，均方误差）增加。本文提供了非平凡的理论结果，揭示了：1）实现完美感知质量的代价恰好是最低可达到的均方误差失真的两倍，2）对于‘经典’率失真问题的最优编码器也是感知压缩问题的最优编码器，3）失真损失对于训练感知解码器是不必要的。此外，我们提出了一种新颖的训练框架，以在给定比特率下实现完美感知约束下的最低均方误差失真。该框架使用了一个以均方误差优化编码器为条件的生成对抗网络（GAN）鉴别器，其优于使用失真加对抗损失的传统框架。实验验证了理论发现并证明了所提出训练框架的优越性。",
        "领域": "图像压缩、生成对抗网络、率失真优化",
        "问题": "如何在保证高感知质量的同时，最小化有损压缩中的失真",
        "动机": "探索在图像压缩中实现高感知质量与低失真之间的权衡，并提出更优的训练方法",
        "方法": "提出了一种基于生成对抗网络（GAN）的新训练框架，该框架在编码器优化均方误差的条件下，实现完美感知约束下的最低失真",
        "关键词": [
            "感知压缩",
            "率失真优化",
            "生成对抗网络",
            "均方误差",
            "图像重建"
        ],
        "涉及的技术概念": {
            "感知质量": "指压缩图像在视觉上的质量，与人类视觉系统的感知特性相关",
            "率失真优化": "在给定比特率下最小化失真的过程，本文中扩展到包括感知质量的优化",
            "生成对抗网络（GAN）": "用于训练解码器，通过对抗训练提高生成图像的感知质量，而不需要显式的失真损失"
        },
        "success": true
    },
    {
        "order": 768,
        "title": "On-Policy Deep Reinforcement Learning for the Average-Reward Criterion",
        "html": "https://ICML.cc//virtual/2021/poster/10281",
        "abstract": "We develop theory and algorithms for average-reward on-policy Reinforcement Learning (RL). We first consider bounding the difference of the long-term average reward for two policies. We show that previous work based on the discounted return (Schulman et al. 2015, Achiam et al. 2017) results in a non-meaningful lower bound in the average reward setting.  By addressing the average-reward criterion directly, we then derive a novel bound which depends on the average divergence between the policies and on Kemeny's constant.  Based on this bound, we develop an iterative procedure which produces a sequence of monotonically improved policies for the average reward criterion. This iterative procedure can then be combined with classic Deep Reinforcement Learning (DRL) methods, resulting in practical DRL algorithms that target the long-run average reward criterion. In particular, we demonstrate that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.",
        "conference": "ICML",
        "中文标题": "基于策略的深度强化学习在平均奖励准则下的应用",
        "摘要翻译": "我们为基于策略的平均奖励强化学习（RL）开发了理论和算法。首先，我们考虑为两种策略的长期平均奖励差异设定界限。我们表明，先前基于折扣回报的工作（Schulman等人2015年，Achiam等人2017年）在平均奖励设定下导致了一个无意义的低界限。通过直接处理平均奖励准则，我们随后推导出一个新颖的界限，该界限依赖于策略之间的平均差异和Kemeny常数。基于这一界限，我们开发了一个迭代过程，该过程为平均奖励准则产生一系列单调改进的策略。然后，这一迭代过程可以与经典的深度强化学习（DRL）方法结合，产生针对长期平均奖励准则的实用DRL算法。特别是，我们证明了平均奖励TRPO（ATRPO），它将基于策略的TRPO算法适应于平均奖励准则，在最具有挑战性的MuJuCo环境中显著优于TRPO。",
        "领域": "深度强化学习、策略优化、机器人控制",
        "问题": "如何在平均奖励准则下优化基于策略的深度强化学习算法",
        "动机": "解决现有基于折扣回报的强化学习方法在平均奖励设定下界限无意义的问题，开发更有效的算法",
        "方法": "通过直接处理平均奖励准则，推导新的界限，并基于此开发迭代改进策略的过程，结合经典深度强化学习方法",
        "关键词": [
            "平均奖励准则",
            "策略优化",
            "深度强化学习",
            "Kemeny常数",
            "ATRPO"
        ],
        "涉及的技术概念": {
            "平均奖励准则": "用于评估强化学习策略长期表现的准则，替代传统的折扣回报准则",
            "Kemeny常数": "在马尔可夫链中，从任何状态到达其他状态的平均时间的度量，用于新界限的推导",
            "ATRPO": "适应于平均奖励准则的策略优化算法，通过结合经典深度强化学习方法，优化长期平均奖励"
        },
        "success": true
    },
    {
        "order": 769,
        "title": "On Proximal Policy Optimization's Heavy-tailed Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/9149",
        "abstract": "Modern policy gradient algorithms such as Proximal Policy Optimization (PPO) rely on an arsenal of heuristics, including loss clipping and gradient clipping, to ensure successful learning.  These heuristics are reminiscent of techniques from robust statistics, commonly used for estimation in outlier-rich ('heavy-tailed') regimes. In this paper, we present a detailed empirical study to characterize the heavy-tailed nature of the gradients of the PPO surrogate reward function.  We demonstrate that the gradients, especially for the actor network, exhibit pronounced heavy-tailedness and that it increases as the agent's policy diverges from the behavioral policy (i.e., as the agent goes further off policy). Further examination implicates the likelihood ratios and advantages in the surrogate reward as the main sources of the observed heavy-tailedness. We then highlight issues arising due to the heavy-tailed nature of the gradients. In this light, we study the effects of the standard PPO clipping heuristics, demonstrating that these tricks primarily serve to offset heavy-tailedness in gradients. Thus motivated, we propose incorporating GMOM, a high-dimensional robust estimator, into PPO as a substitute for three clipping tricks. Despite requiring less hyperparameter tuning, our method matches the performance of PPO (with all heuristics enabled) on a battery of MuJoCo continuous control tasks. \n",
        "conference": "ICML",
        "中文标题": "论近端策略优化算法的重尾梯度",
        "摘要翻译": "现代策略梯度算法，如近端策略优化（PPO），依赖于一系列启发式方法，包括损失裁剪和梯度裁剪，以确保学习的成功。这些启发式方法让人联想到鲁棒统计中的技术，常用于在异常值丰富（'重尾'）的体制中进行估计。在本文中，我们提出了一个详细的实证研究，以描述PPO代理奖励函数梯度的重尾性质。我们证明，梯度，尤其是行动者网络的梯度，表现出明显的重尾性，并且随着代理策略与行为策略的偏离（即代理进一步偏离策略）而增加。进一步的检查表明，代理奖励中的似然比和优势是观察到的重尾性的主要来源。然后，我们强调了由于梯度的重尾性质而产生的问题。有鉴于此，我们研究了标准PPO裁剪启发式方法的效果，证明这些技巧主要用于抵消梯度中的重尾性。因此，我们提出将GMOM（一种高维鲁棒估计器）纳入PPO，作为三种裁剪技巧的替代。尽管需要较少的超参数调整，我们的方法在一系列MuJoCo连续控制任务上的表现与PPO（所有启发式方法启用）相当。",
        "领域": "强化学习、连续控制、策略优化",
        "问题": "近端策略优化算法在训练过程中梯度呈现重尾性质，影响学习效率和稳定性",
        "动机": "探究PPO算法中梯度重尾性的原因及其对算法性能的影响，并提出改进方法",
        "方法": "通过实证研究分析PPO梯度的重尾性质，提出使用GMOM鲁棒估计器替代传统的裁剪技巧",
        "关键词": [
            "近端策略优化",
            "重尾梯度",
            "鲁棒估计",
            "连续控制",
            "策略优化"
        ],
        "涉及的技术概念": {
            "重尾梯度": "指梯度分布具有重尾性质，即存在较多极端值，影响算法的稳定性和收敛性",
            "GMOM估计器": "一种高维鲁棒估计器，用于替代传统的裁剪技巧，减少对超参数调整的依赖",
            "似然比": "在PPO算法中用于衡量当前策略与行为策略之间的差异，是梯度重尾性的主要来源之一"
        },
        "success": true
    },
    {
        "order": 770,
        "title": "On Recovering from Modeling Errors Using Testing Bayesian Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9395",
        "abstract": "We consider the problem of supervised learning with Bayesian Networks when the used dependency structure is incomplete due to missing edges or missing variable states. These modeling errors induce independence constraints on the learned model that may not hold in the true, data-generating distribution. We provide a unified treatment of these modeling errors as instances of state-space abstractions. We then identify a class of Bayesian Networks and queries which allow one to fully recover from such modeling errors if one can choose Conditional Probability Tables (CPTs) dynamically based on evidence. We show theoretically that the recently proposed Testing Bayesian Networks (TBNs), which can be trained by compiling them into Testing Arithmetic Circuits (TACs), provide a promising construct for emulating this CPT selection mechanism. Finally, we present empirical results that illustrate the promise of TBNs as a tool for recovering from certain modeling errors in the context of supervised learning.",
        "conference": "ICML",
        "中文标题": "关于使用测试贝叶斯网络从建模错误中恢复的研究",
        "摘要翻译": "我们考虑了在使用贝叶斯网络进行监督学习时，由于缺失边或缺失变量状态而导致依赖结构不完整的问题。这些建模错误在学习模型中引入了独立性约束，这些约束在真实的数据生成分布中可能不成立。我们将这些建模错误统一视为状态空间抽象的实例。然后，我们确定了一类贝叶斯网络和查询，如果能够根据证据动态选择条件概率表（CPTs），则可以完全从此类建模错误中恢复。我们从理论上证明了最近提出的测试贝叶斯网络（TBNs）——可以通过将它们编译成测试算术电路（TACs）来训练——为模拟这种CPT选择机制提供了一个有前景的构造。最后，我们展示了实证结果，说明了TBNs作为在监督学习背景下从某些建模错误中恢复的工具的潜力。",
        "领域": "贝叶斯网络学习、监督学习、概率图模型",
        "问题": "解决贝叶斯网络在监督学习中由于依赖结构不完整导致的建模错误问题",
        "动机": "研究如何从由于缺失边或缺失变量状态导致的建模错误中恢复，以提高贝叶斯网络在监督学习中的准确性和可靠性",
        "方法": "将建模错误视为状态空间抽象的实例，利用测试贝叶斯网络（TBNs）动态选择条件概率表（CPTs）以恢复错误",
        "关键词": [
            "贝叶斯网络",
            "监督学习",
            "建模错误恢复",
            "条件概率表",
            "测试算术电路"
        ],
        "涉及的技术概念": {
            "测试贝叶斯网络（TBNs）": "一种可以通过编译成测试算术电路（TACs）来训练的贝叶斯网络，用于模拟动态选择条件概率表（CPTs）的机制",
            "条件概率表（CPTs）": "贝叶斯网络中用于表示变量之间条件概率关系的表格，动态选择CPTs有助于从建模错误中恢复",
            "测试算术电路（TACs）": "用于训练测试贝叶斯网络（TBNs）的构造，支持高效的推理和学习"
        },
        "success": true
    },
    {
        "order": 771,
        "title": "On Reinforcement Learning with Adversarial Corruption and Its Application to Block MDP",
        "html": "https://ICML.cc//virtual/2021/poster/8617",
        "abstract": "We study reinforcement learning (RL) in episodic tabular MDPs with adversarial corruptions, where  some episodes can be adversarially corrupted. When the total number of corrupted episodes is known, we propose an algorithm, Corruption Robust Monotonic Value Propagation (\\textsf{CR-MVP}), which achieves a regret bound of $\\tilde{O}\\left(\\left(\\sqrt{SAK}+S^2A+CSA)\\right)\\polylog(H)\\right)$, where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, $K$ is the number of episodes, and $C$ is the corruption level. We also provide a corresponding lower bound, which indicates that our upper bound is tight. Finally, as an application, we study RL with rich observations in the block MDP model. We provide the first algorithm that achieves a $\\sqrt{K}$-type regret in this setting and is computationally efficient.",
        "conference": "ICML",
        "success": true,
        "中文标题": "关于对抗性腐败的强化学习及其在块MDP中的应用",
        "摘要翻译": "我们研究了在具有对抗性腐败的表格MDP中的强化学习（RL），其中某些情节可能被对抗性地腐败。当已知腐败情节的总数时，我们提出了一种算法，即腐败鲁棒单调值传播（CR-MVP），该算法实现了遗憾边界为Õ（（√SAK+S²A+CSA）polylog（H）），其中S是状态数，A是动作数，H是规划视野，K是情节数，C是腐败水平。我们还提供了一个相应的下界，表明我们的上界是紧的。最后，作为一个应用，我们研究了在块MDP模型中具有丰富观察的RL。我们提供了第一个在这种设置下实现√K型遗憾并且计算效率高的算法。",
        "领域": "强化学习、对抗性学习、块MDP",
        "问题": "研究在对抗性腐败环境下强化学习的性能及其在块MDP模型中的应用。",
        "动机": "探索在存在对抗性干扰的情况下，如何设计高效的强化学习算法以保持性能。",
        "方法": "提出了一种名为CR-MVP的算法，用于在对抗性腐败环境下进行强化学习，并在块MDP模型中应用以实现高效计算和性能保证。",
        "关键词": [
            "强化学习",
            "对抗性腐败",
            "块MDP",
            "CR-MVP算法",
            "遗憾边界"
        ],
        "涉及的技术概念": {
            "对抗性腐败": "在强化学习中，某些情节可能被对抗性地修改，影响学习过程。",
            "CR-MVP算法": "一种针对对抗性腐败环境的强化学习算法，旨在保持学习性能。",
            "块MDP": "一种MDP模型，其中状态空间被划分为块，用于研究具有丰富观察的强化学习问题。"
        }
    },
    {
        "order": 772,
        "title": "On Reward-Free RL with Kernel and Neural Function Approximations: Single-Agent MDP and Markov Game",
        "html": "https://ICML.cc//virtual/2021/poster/8781",
        "abstract": "To achieve sample efficiency in reinforcement learning (RL), it necessitates to efficiently explore the underlying environment. Under the offline setting, addressing the exploration challenge lies in collecting an offline dataset with sufficient coverage. Motivated by such a challenge, we study the reward-free RL problem, where an agent aims to thoroughly explore the environment without any pre-specified reward function. Then, given any extrinsic reward, the agent computes the optimal policy via offline RL with data collected in the exploration stage. Moreover, we tackle this problem under the context of function approximation, leveraging powerful function approximators. Specifically, we propose to explore via an optimistic variant of the value-iteration algorithm incorporating kernel and neural function approximations, where we adopt the associated exploration bonus as the exploration reward. Moreover, we design exploration and planning algorithms for both single-agent MDPs and zero-sum Markov games and prove that our methods can achieve $\\widetilde{\\mathcal{O}}(1 /\\varepsilon^2)$ sample complexity for generating a $\\varepsilon$-suboptimal policy or  $\\varepsilon$-approximate Nash equilibrium when given an arbitrary extrinsic reward. To the best of our knowledge, we establish the first provably efficient reward-free RL algorithm with kernel and neural function approximators.",
        "conference": "ICML",
        "中文标题": "关于使用核和神经函数近似的无奖励强化学习：单智能体MDP与马尔可夫博弈",
        "摘要翻译": "为了在强化学习（RL）中实现样本效率，必须有效地探索底层环境。在离线设置下，解决探索挑战的关键在于收集一个具有足够覆盖范围的离线数据集。受此挑战的启发，我们研究了无奖励RL问题，其中智能体的目标是在没有任何预先指定的奖励函数的情况下彻底探索环境。然后，给定任何外在奖励，智能体通过离线RL与探索阶段收集的数据计算最优策略。此外，我们在函数近似的背景下解决了这个问题，利用了强大的函数逼近器。具体来说，我们提出通过结合核和神经函数近似的价值迭代算法的乐观变体进行探索，其中我们采用相关的探索奖励作为探索奖励。此外，我们为单智能体MDP和零和马尔可夫博弈设计了探索和规划算法，并证明了当给定任意外在奖励时，我们的方法可以实现生成ε-次优策略或ε-近似纳什均衡的样本复杂度为O~(1/ε^2)。据我们所知，我们建立了第一个可证明高效的使用核和神经函数逼近器的无奖励RL算法。",
        "领域": "强化学习、函数逼近、马尔可夫决策过程",
        "问题": "在无预先指定奖励函数的情况下，如何有效地探索环境并计算最优策略。",
        "动机": "解决在离线强化学习中有效探索环境和计算最优策略的挑战。",
        "方法": "采用结合核和神经函数近似的价值迭代算法的乐观变体进行探索，并设计探索和规划算法。",
        "关键词": [
            "无奖励强化学习",
            "函数逼近",
            "马尔可夫决策过程",
            "样本复杂度",
            "纳什均衡"
        ],
        "涉及的技术概念": {
            "核函数近似": "用于在强化学习中逼近价值函数，提高探索效率。",
            "神经函数近似": "利用神经网络强大的表示能力来逼近复杂的价值函数。",
            "价值迭代算法": "一种动态规划方法，用于在强化学习中计算最优策略。"
        },
        "success": true
    },
    {
        "order": 773,
        "title": "On Robust Mean Estimation under Coordinate-level Corruption",
        "html": "https://ICML.cc//virtual/2021/poster/10339",
        "abstract": "We study the problem of robust mean estimation and introduce a novel Hamming distance-based measure of distribution shift for coordinate-level corruptions. We show that this measure yields adversary models that capture more realistic corruptions than those used in prior works, and present an information-theoretic analysis of robust mean estimation in these settings. We show that for structured distributions, methods that leverage the structure yield information theoretically more accurate mean estimation. We also focus on practical algorithms for robust mean estimation and study when data cleaning-inspired approaches that first fix corruptions in the input data and then perform robust mean estimation can match the information theoretic bounds of our analysis. We finally demonstrate experimentally that this two-step approach outperforms structure-agnostic robust estimation and provides accurate mean estimation even for high-magnitude corruption.",
        "conference": "ICML",
        "中文标题": "关于坐标级腐败下的鲁棒均值估计",
        "摘要翻译": "我们研究了鲁棒均值估计的问题，并引入了一种基于汉明距离的新颖分布偏移度量，用于坐标级腐败。我们表明，这种度量产生的对手模型比先前工作中使用的模型更能捕捉到现实的腐败情况，并在这些设置下对鲁棒均值估计进行了信息理论分析。我们证明，对于结构化分布，利用结构的方法在信息理论上能提供更准确的均值估计。我们还关注于鲁棒均值估计的实用算法，并研究了首先修复输入数据中的腐败然后进行鲁棒均值估计的数据清理启发式方法何时能匹配我们分析的信息理论界限。最后，我们通过实验证明，这种两步法优于不考虑结构的鲁棒估计，并且即使在高幅度腐败下也能提供准确的均值估计。",
        "领域": "鲁棒统计学习、数据清理、信息理论分析",
        "问题": "在坐标级腐败情况下进行鲁棒均值估计的问题",
        "动机": "为了开发一种能够更准确反映现实世界数据腐败情况的鲁棒均值估计方法",
        "方法": "引入基于汉明距离的分布偏移度量，进行信息理论分析，并提出一种两步法（数据清理后进行鲁棒均值估计）",
        "关键词": [
            "鲁棒均值估计",
            "汉明距离",
            "数据清理",
            "信息理论分析",
            "坐标级腐败"
        ],
        "涉及的技术概念": {
            "汉明距离": "用于度量分布偏移，帮助识别和量化数据中的腐败程度",
            "信息理论分析": "用于评估在不同腐败模型下鲁棒均值估计的理论性能界限",
            "两步法": "首先清理数据中的腐败，然后进行鲁棒均值估计，以提高估计的准确性和鲁棒性"
        },
        "success": true
    },
    {
        "order": 774,
        "title": "On Signal-to-Noise Ratio Issues in Variational Inference for Deep Gaussian Processes",
        "html": "https://ICML.cc//virtual/2021/poster/10363",
        "abstract": "We show that the gradient estimates used in training Deep Gaussian Processes (DGPs) with importance-weighted variational inference are susceptible to signal-to-noise ratio (SNR) issues. Specifically, we show both theoretically and via an extensive empirical evaluation that the SNR of the gradient estimates for the latent variable's variational parameters decreases as the number of importance samples increases. As a result, these gradient estimates degrade to pure noise if the number of importance samples is too large. To address this pathology, we show how doubly-reparameterized gradient estimators, originally proposed for training variational autoencoders, can be adapted to the DGP setting and that the resultant estimators completely remedy the SNR issue, thereby providing more reliable training. Finally, we demonstrate that our fix can lead to consistent improvements in the predictive performance of DGP models.",
        "conference": "ICML",
        "中文标题": "深度高斯过程中变分推理的信噪比问题研究",
        "摘要翻译": "我们展示了在使用重要性加权变分推理训练深度高斯过程（DGPs）时，梯度估计容易受到信噪比（SNR）问题的影响。具体来说，我们通过理论分析和广泛的实证评估表明，随着重要性样本数量的增加，潜在变量变分参数的梯度估计的SNR会降低。因此，如果重要性样本数量过大，这些梯度估计将退化为纯噪声。为了解决这一问题，我们展示了如何将最初为训练变分自编码器提出的双重重参数化梯度估计器适应于DGP设置，并且这些估计器能够完全解决SNR问题，从而提供更可靠的训练。最后，我们证明了我们的修复方法可以持续提高DGP模型的预测性能。",
        "领域": "变分推理、深度高斯过程、梯度估计优化",
        "问题": "深度高斯过程在训练过程中梯度估计的信噪比问题",
        "动机": "解决深度高斯过程在训练过程中由于重要性样本数量增加导致的梯度估计信噪比下降问题",
        "方法": "采用双重重参数化梯度估计器来适应深度高斯过程设置，以解决信噪比问题",
        "关键词": [
            "深度高斯过程",
            "变分推理",
            "信噪比",
            "梯度估计",
            "双重重参数化"
        ],
        "涉及的技术概念": {
            "重要性加权变分推理": "用于训练深度高斯过程的变分推理方法，通过重要性采样来估计梯度",
            "信噪比（SNR）": "衡量梯度估计中信号与噪声比例的指标，影响训练过程的稳定性和效率",
            "双重重参数化梯度估计器": "一种改进的梯度估计方法，通过双重重参数化技术减少梯度估计的方差，提高信噪比"
        },
        "success": true
    },
    {
        "order": 775,
        "title": "On the Convergence of Hamiltonian Monte Carlo with Stochastic Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/9521",
        "abstract": "Hamiltonian Monte Carlo (HMC),  built based on the Hamilton's equation, has been witnessed great success in sampling from high-dimensional posterior distributions. However, it also suffers from computational inefficiency, especially for large training datasets. One common idea to overcome this computational bottleneck is using stochastic gradients, which only queries a mini-batch of training data in each iteration. However, unlike the extensive studies on the convergence analysis of HMC using full gradients, few works focus on establishing the convergence guarantees of stochastic gradient HMC algorithms. In this paper, we propose a general framework for proving the convergence rate of HMC with stochastic gradient estimators, for sampling from strongly log-concave and log-smooth target distributions. We show that the convergence to the target distribution in $2$-Wasserstein distance can be guaranteed as long as the stochastic gradient estimator is unbiased and its variance is upper bounded along the algorithm trajectory. We further apply the proposed framework to analyze the convergence rates of HMC with four standard stochastic gradient estimators: mini-batch stochastic gradient (SG), stochastic variance reduced gradient (SVRG), stochastic average gradient (SAGA), and control variate gradient (CVG). Theoretical results explain the inefficiency of mini-batch SG, and suggest that SVRG and SAGA perform better in the tasks with high-precision requirements, while CVG performs better for large dataset. Experiment results \nverify our theoretical findings.",
        "conference": "ICML",
        "中文标题": "关于随机梯度哈密尔顿蒙特卡洛方法的收敛性",
        "摘要翻译": "基于哈密尔顿方程构建的哈密尔顿蒙特卡洛方法（HMC），在从高维后验分布中采样方面取得了巨大成功。然而，它也面临着计算效率低下的问题，特别是对于大型训练数据集。克服这一计算瓶颈的一个常见想法是使用随机梯度，它每次迭代仅查询一小批训练数据。然而，与使用全梯度对HMC收敛性分析的广泛研究不同，很少有工作专注于建立随机梯度HMC算法的收敛保证。在本文中，我们提出了一个通用框架，用于证明使用随机梯度估计器的HMC在采样强对数凹和对数平滑目标分布时的收敛速度。我们表明，只要随机梯度估计器是无偏的，并且其方差在算法轨迹上有上界，就可以保证在2-Wasserstein距离上收敛到目标分布。我们进一步应用所提出的框架分析了HMC与四种标准随机梯度估计器的收敛速度：小批量随机梯度（SG）、随机方差减少梯度（SVRG）、随机平均梯度（SAGA）和控制变量梯度（CVG）。理论结果解释了小批量SG的低效率，并表明SVRG和SAGA在高精度要求的任务中表现更好，而CVG在大型数据集上表现更好。实验结果验证了我们的理论发现。",
        "领域": "贝叶斯统计、蒙特卡洛方法、随机优化",
        "问题": "解决随机梯度哈密尔顿蒙特卡洛方法在高维后验分布采样中的收敛性问题",
        "动机": "提高哈密尔顿蒙特卡洛方法在大型数据集上的计算效率，同时保证其收敛性",
        "方法": "提出一个通用框架，分析使用不同随机梯度估计器的HMC方法的收敛速度，并通过实验验证理论发现",
        "关键词": [
            "哈密尔顿蒙特卡洛",
            "随机梯度",
            "收敛性分析",
            "高维采样",
            "贝叶斯统计"
        ],
        "涉及的技术概念": {
            "哈密尔顿蒙特卡洛（HMC）": "一种基于哈密尔顿方程的蒙特卡洛采样方法，用于高效地从高维后验分布中采样",
            "随机梯度估计器": "用于在每次迭代中仅使用一小批数据来估计梯度，以提高计算效率",
            "2-Wasserstein距离": "用于衡量两个概率分布之间差异的度量，本文中用于分析HMC方法收敛到目标分布的速度"
        },
        "success": true
    },
    {
        "order": 776,
        "title": "On the difficulty of unbiased alpha divergence minimization",
        "html": "https://ICML.cc//virtual/2021/poster/10337",
        "abstract": "Several approximate inference algorithms have been proposed to minimize an alpha-divergence between an approximating distribution and a target distribution. Many of these algorithms introduce bias, the magnitude of which becomes problematic in high dimensions. Other algorithms are unbiased. These often seem to suffer from high variance, but little is rigorously known. In this work we study unbiased methods for alpha-divergence minimization through the Signal-to-Noise Ratio (SNR) of the gradient estimator. We study several representative scenarios where strong analytical results are possible, such as fully-factorized or Gaussian distributions. We find that when alpha is not zero, the SNR worsens exponentially in the dimensionality of the problem. This casts doubt on the practicality of these methods. We empirically confirm these theoretical results.",
        "conference": "ICML",
        "中文标题": "论无偏alpha散度最小化的困难",
        "摘要翻译": "已有多种近似推理算法被提出，用于最小化近似分布与目标分布之间的alpha散度。这些算法中许多引入了偏差，其在高维度情况下变得尤为严重。其他算法则是无偏的。这些无偏算法往往看似受到高方差的困扰，但对此缺乏严格的了解。在本研究中，我们通过梯度估计器的信噪比（SNR）来研究无偏alpha散度最小化方法。我们研究了几个具有代表性的场景，在这些场景中可以获得强有力的分析结果，如完全因子化或高斯分布。我们发现，当alpha不为零时，SNR随问题维度的增加而指数级恶化。这让人对这些方法的实用性产生了怀疑。我们通过实验证实了这些理论结果。",
        "领域": "概率图模型、变分推断、统计机器学习",
        "问题": "无偏alpha散度最小化方法在高维情况下的信噪比恶化问题",
        "动机": "探究无偏alpha散度最小化方法在实际应用中的可行性，特别是在高维情况下信噪比恶化的现象",
        "方法": "通过分析梯度估计器的信噪比（SNR），研究无偏alpha散度最小化方法在几种代表性场景下的表现，包括完全因子化或高斯分布",
        "关键词": [
            "alpha散度",
            "无偏估计",
            "信噪比",
            "高维问题",
            "梯度估计"
        ],
        "涉及的技术概念": {
            "alpha散度": "用于衡量近似分布与目标分布之间差异的散度度量，本研究关注其最小化问题",
            "信噪比（SNR）": "梯度估计器的信噪比，用于评估无偏alpha散度最小化方法的性能，特别是在高维情况下的表现",
            "无偏估计": "指在统计估计中，估计量的期望等于被估计参数的真值，本研究探讨无偏alpha散度最小化方法的可行性"
        },
        "success": true
    },
    {
        "order": 777,
        "title": "On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10397",
        "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn,  minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.",
        "conference": "ICML",
        "中文标题": "论初始化对过参数化线性网络收敛性和隐式偏差的显式影响",
        "摘要翻译": "尽管神经网络在实践中高度过参数化，但通过随机初始化的梯度下降训练且没有任何正则化的情况下，它们仍能展现出良好的泛化性能。解释这一现象的一个有前景的方向是研究初始化和过参数化如何影响训练算法的收敛性和隐式偏差。在本文中，我们对梯度流下训练的单隐藏层线性网络进行了新颖的分析，该分析将初始化、优化和过参数化联系起来。首先，我们展示了平方损失以依赖于初始化不平衡水平的速率指数级收敛到其最优值。其次，我们证明了适当的初始化约束了网络参数的动态，使其位于一个不变集合内。进而，最小化该集合上的损失导致最小范数解。最后，我们展示了大的隐藏层宽度与（适当缩放的）随机初始化一起，确保了训练期间接近这样的不变集合，使我们能够推导出训练网络与最小范数解之间距离的新颖非渐近上界。",
        "领域": "深度学习理论、优化算法、神经网络初始化",
        "问题": "研究初始化和过参数化如何影响神经网络的收敛性和隐式偏差",
        "动机": "解释为何高度过参数化的神经网络在没有正则化的情况下仍能展现出良好的泛化性能",
        "方法": "分析单隐藏层线性网络在梯度流下的训练过程，研究初始化不平衡对收敛速率的影响，以及初始化如何约束参数动态到不变集合，进而导致最小范数解",
        "关键词": [
            "过参数化",
            "隐式偏差",
            "梯度流",
            "初始化影响",
            "最小范数解"
        ],
        "涉及的技术概念": {
            "过参数化": "指神经网络的参数数量远大于训练样本数，这在实践中常见且能带来良好的泛化性能",
            "隐式偏差": "指训练算法在没有显式正则化的情况下，倾向于选择特定类型的解（如最小范数解）的倾向",
            "梯度流": "指使用梯度下降的连续时间极限来研究优化动态，有助于理论分析"
        },
        "success": true
    },
    {
        "order": 778,
        "title": "On-the-fly Rectification for Robust Large-Vocabulary Topic Inference",
        "html": "https://ICML.cc//virtual/2021/poster/9807",
        "abstract": "Across many data domains, co-occurrence statistics about the joint appearance of objects are powerfully informative. By transforming unsupervised learning problems into decompositions of co-occurrence statistics, spectral algorithms provide transparent and efficient algorithms for posterior inference such as latent topic analysis and community detection. As object vocabularies grow, however, it becomes rapidly more expensive to store and run inference algorithms on co-occurrence statistics. Rectifying co-occurrence, the key process to uphold model assumptions, becomes increasingly more vital in the presence of rare terms, but current techniques cannot scale to large vocabularies. We propose novel methods that simultaneously compress and rectify co-occurrence statistics, scaling gracefully with the size of vocabulary and the dimension of latent space. We also present new algorithms learning latent variables from the compressed statistics, and verify that our methods perform comparably to previous approaches on both textual and non-textual data.",
        "conference": "ICML",
        "中文标题": "实时校正以实现鲁棒的大词汇量主题推断",
        "摘要翻译": "在许多数据领域，关于对象共同出现的共现统计信息极具信息量。通过将无监督学习问题转化为共现统计的分解，谱算法为后验推断（如潜在主题分析和社区检测）提供了透明且高效的算法。然而，随着对象词汇量的增长，存储和运行共现统计推断算法的成本迅速增加。在存在稀有术语的情况下，校正共现（维护模型假设的关键过程）变得越来越重要，但现有技术无法扩展到大型词汇量。我们提出了同时压缩和校正共现统计的新方法，能够优雅地随着词汇量和潜在空间维度的增长而扩展。我们还提出了从压缩统计中学习潜在变量的新算法，并验证了我们的方法在文本和非文本数据上的表现与之前的方法相当。",
        "领域": "潜在主题分析、社区检测、大规模数据处理",
        "问题": "在大词汇量情况下，如何高效存储和运行共现统计推断算法，并校正共现统计以维护模型假设。",
        "动机": "解决在大词汇量和潜在空间维度增长时，现有共现统计推断和校正技术无法有效扩展的问题。",
        "方法": "提出同时压缩和校正共现统计的新方法，以及从压缩统计中学习潜在变量的新算法。",
        "关键词": [
            "共现统计",
            "潜在主题分析",
            "社区检测",
            "大规模数据处理",
            "实时校正"
        ],
        "涉及的技术概念": {
            "共现统计": "用于分析对象共同出现频率的统计方法，是潜在主题分析和社区检测的基础。",
            "谱算法": "通过分解共现统计来解决无监督学习问题的算法，提供高效的后验推断。",
            "实时校正": "在共现统计推断过程中动态调整统计信息，以维护模型假设和提高推断的鲁棒性。"
        },
        "success": true
    },
    {
        "order": 779,
        "title": "On the Generalization Power of Overfitted Two-Layer Neural Tangent Kernel Models",
        "html": "https://ICML.cc//virtual/2021/poster/8703",
        "abstract": "In this paper, we study the generalization performance of min $\\ell_2$-norm overfitting solutions for the neural tangent kernel (NTK) model of a two-layer neural network with ReLU activation that has no bias term. We show that, depending on the ground-truth function, the test error of overfitted NTK models exhibits characteristics that are different from the 'double-descent' of other overparameterized linear models with simple Fourier or Gaussian features. Specifically, for a class of learnable functions, we provide a new upper bound of the generalization error that approaches a small limiting value, even when the number of neurons $p$ approaches infinity. This limiting value further decreases with the number of training samples $n$. For functions outside of this class, we provide a lower bound on the generalization error that does not diminish to zero even when $n$ and $p$ are both large.",
        "conference": "ICML",
        "success": true,
        "中文标题": "论过拟合双层神经正切核模型的泛化能力",
        "摘要翻译": "本文研究了无偏置项、采用ReLU激活函数的双层神经网络神经正切核（NTK）模型在最小$\\ell_2$范数过拟合解下的泛化性能。我们发现，根据真实函数的不同，过拟合NTK模型的测试误差展现出与其他具有简单傅里叶或高斯特征的过参数化线性模型的‘双下降’不同的特性。具体而言，对于一类可学习函数，我们提供了一个新的泛化误差上界，即使神经元数量$p$趋近于无穷大时，该上界也趋近于一个较小的极限值。这个极限值随着训练样本数量$n$的增加而进一步减小。对于不属于这一类的函数，我们提供了一个泛化误差的下界，即使$n$和$p$都很大时，该下界也不会减小到零。",
        "领域": "深度学习理论、神经网络、机器学习理论",
        "问题": "研究过拟合双层神经正切核模型在不同真实函数下的泛化性能差异",
        "动机": "探索过拟合NTK模型在泛化能力上的独特表现，区别于其他过参数化线性模型",
        "方法": "通过理论分析，为特定类别的函数提供泛化误差的上界和下界",
        "关键词": [
            "神经正切核",
            "泛化能力",
            "过拟合",
            "双层神经网络",
            "ReLU激活函数"
        ],
        "涉及的技术概念": {
            "神经正切核（NTK）": "用于描述无限宽度神经网络在梯度下降训练过程中的动态，本文中用于分析双层神经网络的泛化性能",
            "$\\ell_2$范数": "在本文中用于衡量过拟合解的复杂度，是分析泛化误差的关键"
        }
    },
    {
        "order": 780,
        "title": "On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent",
        "html": "https://ICML.cc//virtual/2021/poster/10387",
        "abstract": "Recent work has highlighted the role of initialization scale in determining the structure of the solutions that gradient methods converge to. In particular, it was shown that large initialization leads to the neural tangent kernel regime solution, whereas small initialization leads to so called ``rich regimes''. However, the initialization structure is richer than the overall scale alone and involves relative magnitudes of different weights and layers in the network. Here we show that these relative scales, which we refer to as initialization shape, play an important role in determining the learned model. We develop a novel technique for deriving the inductive bias of gradient-flow and use it to obtain closed-form implicit regularizers for multiple cases of interest.",
        "conference": "ICML",
        "中文标题": "论初始化形状的隐式偏差：超越无穷小镜像下降",
        "摘要翻译": "最近的研究强调了初始化规模在决定梯度方法收敛到的解结构中的作用。特别是，研究表明，大的初始化会导致神经切线核机制的解，而小的初始化会导致所谓的“丰富机制”。然而，初始化结构比单独的总体规模更为丰富，涉及网络中不同权重和层的相对大小。在这里，我们展示了这些相对规模，我们称之为初始化形状，在决定学习模型中起着重要作用。我们开发了一种新技术来推导梯度流的归纳偏差，并用它来为多个感兴趣的案例获得封闭形式的隐式正则化器。",
        "领域": "深度学习理论、神经网络优化、机器学习理论",
        "问题": "研究初始化形状（即网络中不同权重和层的相对大小）如何影响梯度方法收敛到的解的结构。",
        "动机": "探索初始化形状在决定学习模型中的作用，超越仅考虑初始化规模的影响。",
        "方法": "开发了一种新技术来推导梯度流的归纳偏差，并应用该技术为多个案例获得封闭形式的隐式正则化器。",
        "关键词": [
            "初始化形状",
            "隐式偏差",
            "梯度流",
            "神经网络优化",
            "隐式正则化"
        ],
        "涉及的技术概念": {
            "初始化形状": "指网络中不同权重和层的相对大小，研究其对学习模型的影响。",
            "梯度流": "用于描述梯度下降在连续时间极限下的行为，本研究通过分析梯度流来理解初始化形状的影响。",
            "隐式正则化": "指在优化过程中不直接指定但通过优化算法自然引入的正则化效应，本研究通过封闭形式的隐式正则化器来量化初始化形状的影响。"
        },
        "success": true
    },
    {
        "order": 781,
        "title": "On the Inherent Regularization Effects of Noise Injection During Training",
        "html": "https://ICML.cc//virtual/2021/poster/9413",
        "abstract": "Randomly perturbing networks during the training process is a commonly used approach to improving generalization performance. In this paper, we present a theoretical study of one particular way of random perturbation, which corresponds to injecting artificial noise to the training data. We provide a precise asymptotic characterization of the training and generalization errors of such randomly perturbed learning problems on a random feature model. Our analysis shows that Gaussian noise injection in the training process is equivalent to introducing a weighted ridge regularization, when the number of noise injections tends to infinity. The explicit form of the regularization is also given. Numerical results corroborate our asymptotic predictions, showing that they are accurate even in moderate problem dimensions. Our theoretical predictions are based on a new correlated Gaussian equivalence conjecture that generalizes recent results in the study of random feature models.",
        "conference": "ICML",
        "中文标题": "论训练过程中噪声注入的固有正则化效应",
        "摘要翻译": "在训练过程中随机扰动网络是提高泛化性能的常用方法。本文中，我们对一种特定的随机扰动方式进行了理论研究，这种方式对应于向训练数据中注入人工噪声。我们提供了一个随机特征模型上此类随机扰动学习问题的训练和泛化误差的精确渐近特征。我们的分析表明，当噪声注入次数趋于无穷大时，训练过程中的高斯噪声注入等同于引入加权岭正则化。同时，我们也给出了正则化的显式形式。数值结果证实了我们的渐近预测，显示即使在中等问题维度下，这些预测也是准确的。我们的理论预测基于一个新的相关高斯等价猜想，该猜想推广了随机特征模型研究中的最新结果。",
        "领域": "深度学习正则化、随机特征模型、高斯噪声注入",
        "问题": "研究噪声注入在训练过程中如何作为一种正则化方法影响模型的泛化性能。",
        "动机": "探索和理解噪声注入作为一种正则化技术的理论基础及其对模型性能的影响。",
        "方法": "通过理论分析和数值实验，研究高斯噪声注入在随机特征模型上的训练和泛化误差，提出相关高斯等价猜想。",
        "关键词": [
            "噪声注入",
            "正则化效应",
            "随机特征模型",
            "高斯噪声",
            "泛化误差"
        ],
        "涉及的技术概念": {
            "高斯噪声注入": "在训练数据中注入高斯噪声，作为一种正则化手段，以提高模型的泛化能力。",
            "加权岭正则化": "通过噪声注入等效引入的正则化形式，有助于防止模型过拟合。",
            "相关高斯等价猜想": "用于理论预测的新猜想，推广了随机特征模型的研究，为噪声注入的正则化效应提供了理论基础。"
        },
        "success": true
    },
    {
        "order": 782,
        "title": "On the Optimality of Batch Policy Optimization Algorithms",
        "html": "https://ICML.cc//virtual/2021/poster/9753",
        "abstract": "Batch policy optimization considers leveraging existing data for policy construction before interacting with an environment. Although interest in this problem has grown significantly in recent years, its theoretical foundations remain under-developed.\nTo advance the understanding of this problem, we provide three results that characterize the limits and possibilities of batch policy optimization in the finite-armed stochastic bandit setting. First, we introduce a class of confidence-adjusted index algorithms that unifies optimistic and pessimistic principles in a common framework, which enables a general analysis. For this family, we show that any confidence-adjusted index algorithm is minimax optimal, whether it be optimistic, pessimistic or neutral. Our analysis reveals that instance-dependent optimality, commonly used to establish optimality of on-line stochastic bandit algorithms, cannot be achieved by any algorithm in the batch setting. In particular, for any algorithm that performs optimally in some environment, there exists another environment where the same algorithm suffers arbitrarily larger regret. Therefore, to establish a framework for distinguishing algorithms, we introduce a new weighted-minimax criterion that considers the inherent difficulty of optimal value prediction. We demonstrate how this criterion can be used to justify commonly used pessimistic principles for batch policy optimization. ",
        "conference": "ICML",
        "中文标题": "论批量策略优化算法的最优性",
        "摘要翻译": "批量策略优化考虑在与环境交互之前利用现有数据进行策略构建。尽管近年来对这一问题的兴趣显著增长，但其理论基础仍然不够完善。为了推进对这一问题的理解，我们提供了三个结果，这些结果在有限臂随机老虎机设置中刻画了批量策略优化的限制和可能性。首先，我们介绍了一类置信度调整索引算法，这些算法在一个共同的框架内统一了乐观和悲观原则，从而实现了通用分析。对于这一家族，我们展示了任何置信度调整索引算法都是极小极大最优的，无论它是乐观的、悲观的还是中性的。我们的分析揭示，通常用于建立在线随机老虎机算法最优性的实例依赖性最优性，在批量设置中无法被任何算法实现。特别是，对于在某些环境中表现最优的任何算法，都存在另一个环境，其中同一算法遭受任意更大的遗憾。因此，为了建立一个区分算法的框架，我们引入了一个新的加权极小极大准则，该准则考虑了最优值预测的固有难度。我们展示了如何使用这一准则来证明批量策略优化中常用的悲观原则的合理性。",
        "领域": "强化学习",
        "问题": "批量策略优化在有限臂随机老虎机设置中的最优性问题",
        "动机": "推进对批量策略优化问题的理解，特别是在理论基础方面的不足",
        "方法": "引入置信度调整索引算法家族，进行通用分析，并引入加权极小极大准则",
        "关键词": [
            "批量策略优化",
            "随机老虎机",
            "置信度调整索引",
            "极小极大最优",
            "加权极小极大准则"
        ],
        "涉及的技术概念": {
            "置信度调整索引算法": "在共同框架内统一乐观和悲观原则，实现批量策略优化的通用分析",
            "极小极大最优": "展示任何置信度调整索引算法在批量策略优化中的最优性",
            "加权极小极大准则": "新引入的准则，用于考虑最优值预测的固有难度，并区分算法性能"
        },
        "success": true
    },
    {
        "order": 783,
        "title": "On the Power of Localized Perceptron for Label-Optimal Learning of Halfspaces with Adversarial Noise",
        "html": "https://ICML.cc//virtual/2021/poster/9039",
        "abstract": "We study {\\em online} active learning of homogeneous halfspaces in $\\mathbb{R}^d$ with adversarial noise where the overall probability of a noisy label is constrained to be at most $\\nu$. Our main contribution is a Perceptron-like online active learning algorithm that runs in polynomial time, and under the conditions that the marginal distribution is isotropic log-concave and $\\nu = \\Omega(\\epsilon)$, where $\\epsilon \\in (0, 1)$ is the target error rate, our algorithm PAC learns the underlying halfspace with near-optimal label complexity of $\\tilde{O}\\big(d \\cdot \\polylog(\\frac{1}{\\epsilon})\\big)$ and sample complexity of $\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$. Prior to this work, existing online algorithms designed for tolerating the adversarial noise are subject to either label complexity polynomial in $\\frac{1}{\\epsilon}$, or suboptimal noise tolerance, or restrictive marginal distributions. With the additional prior knowledge that the underlying halfspace is $s$-sparse, we obtain attribute-efficient label complexity of $\\tilde{O}\\big( s \\cdot \\polylog(d, \\frac{1}{\\epsilon}) \\big)$ and sample complexity of $\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot \\polylog(d) \\big)$. As an immediate corollary, we show that under the agnostic model where no assumption is made on the noise rate $\\nu$, our active learner achieves an error rate of $O(OPT) + \\epsilon$ with the same running time and label and sample complexity, where $OPT$ is the best possible error rate achievable by any homogeneous halfspace.",
        "conference": "ICML",
        "success": true,
        "中文标题": "关于局部感知器在对抗性噪声下半空间标签最优学习能力的研究",
        "摘要翻译": "我们研究了在对抗性噪声下，$\\mathbb{R}^d$空间中同质半空间的在线主动学习问题，其中噪声标签的总概率被限制在最多$\\nu$。我们的主要贡献是一个类似感知器的在线主动学习算法，该算法在多项式时间内运行，并且在边际分布为各向同性对数凹且$\\nu = \\Omega(\\epsilon)$的条件下，其中$\\epsilon \\in (0, 1)$是目标错误率，我们的算法以接近最优的标签复杂度$\\tilde{O}\\big(d \\cdot \\polylog(\\frac{1}{\\epsilon})\\big)$和样本复杂度$\\tilde{O}\\big(\\frac{d}{\\epsilon} \\big)$PAC学习底层半空间。在这项工作之前，现有的在线算法设计用于容忍对抗性噪声，要么标签复杂度在$\\frac{1}{\\epsilon}$上是多项式的，要么噪声容忍度次优，要么边际分布受限。在额外知道底层半空间是$s$-稀疏的先验知识下，我们获得了$\\tilde{O}\\big( s \\cdot \\polylog(d, \\frac{1}{\\epsilon}) \\big)$的属性高效标签复杂度和$\\tilde{O}\\big(\\frac{s}{\\epsilon} \\cdot \\polylog(d) \\big)$的样本复杂度。作为一个直接的推论，我们表明，在不可知模型下，没有对噪声率$\\nu$做出任何假设，我们的主动学习器以相同的运行时间和标签及样本复杂度实现了$O(OPT) + \\epsilon$的错误率，其中$OPT$是任何同质半空间可实现的最佳错误率。",
        "领域": "在线学习, 主动学习, 对抗性学习",
        "问题": "在对抗性噪声环境下，如何高效地进行同质半空间的在线主动学习。",
        "动机": "研究在对抗性噪声条件下，如何设计一个高效的在线主动学习算法，以接近最优的标签和样本复杂度学习同质半空间。",
        "方法": "提出了一种类似感知器的在线主动学习算法，该算法在多项式时间内运行，并在特定条件下实现了接近最优的标签和样本复杂度。",
        "关键词": [
            "在线主动学习",
            "对抗性噪声",
            "半空间学习",
            "标签复杂度",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "在线主动学习": "在数据流中动态选择最有信息量的样本进行标注，以提高学习效率。",
            "对抗性噪声": "在数据标签中故意引入的噪声，旨在干扰学习过程。"
        }
    },
    {
        "order": 784,
        "title": "On the Predictability of Pruning Across Scales ",
        "html": "https://ICML.cc//virtual/2021/poster/9937",
        "abstract": "We show that the error of iteratively magnitude-pruned networks empirically follows a scaling law with interpretable coefficients that depend on the architecture and task. We functionally approximate the error of the pruned networks, showing it is predictable in terms of an invariant tying width, depth, and pruning level, such that networks of vastly different pruned densities are interchangeable. We demonstrate the accuracy of this approximation over orders of magnitude in depth, width, dataset size, and density. We show that the functional form holds (generalizes) for large scale data (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks become ever larger and costlier to train, our findings suggest a framework for reasoning conceptually and analytically about a standard method for unstructured pruning.",
        "conference": "ICML",
        "中文标题": "关于跨尺度修剪的可预测性",
        "摘要翻译": "我们展示了迭代幅度修剪网络的误差在经验上遵循一个具有可解释系数的缩放定律，这些系数依赖于架构和任务。我们通过函数近似了修剪网络的误差，表明它在宽度、深度和修剪水平的绑定不变性方面是可预测的，因此，具有极大不同修剪密度的网络是可互换的。我们在深度、宽度、数据集大小和密度的数量级上证明了这种近似的准确性。我们展示了这种函数形式在大规模数据（例如，ImageNet）和架构（例如，ResNets）上的保持（泛化）。随着神经网络变得越来越大且训练成本越来越高，我们的研究结果提出了一个框架，用于概念性和分析性地推理一种标准的非结构化修剪方法。",
        "领域": "神经网络修剪、深度学习优化、模型压缩",
        "问题": "如何预测和优化不同规模和架构神经网络的修剪效果",
        "动机": "随着神经网络规模的增大和训练成本的上升，需要一种有效的方法来预测和优化修剪过程，以减少资源消耗同时保持模型性能。",
        "方法": "通过经验观察和函数近似，发现并验证了修剪网络误差的缩放定律，提出了一种基于不变性原理的框架来预测不同修剪密度下的网络性能。",
        "关键词": [
            "神经网络修剪",
            "缩放定律",
            "模型压缩",
            "深度学习优化",
            "非结构化修剪"
        ],
        "涉及的技术概念": {
            "迭代幅度修剪": "一种逐步移除网络中权重较小的连接的方法，以减少模型大小和计算需求。",
            "缩放定律": "描述了修剪网络误差如何随网络宽度、深度和修剪水平变化的数学关系。",
            "不变性原理": "指在特定条件下，不同修剪密度的网络在性能上可以互换的理论基础。"
        },
        "success": true
    },
    {
        "order": 785,
        "title": "On the price of  explainability for some clustering problems",
        "html": "https://ICML.cc//virtual/2021/poster/8897",
        "abstract": "The price of explainability for a clustering task can be defined as the unavoidable loss, in terms of the objective function,  if we force the final partition to be explainable. Here, we study this price for the following clustering problems: $k$-means, $k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds for a  natural model where explainability is achieved via decision trees.  For the $k$-means and $k$-medians problems our upper bounds improve those obtained by [Dasgupta et. al, ICML 20] for low dimensions. Another contribution is a simple and efficient algorithm for building explainable clusterings for the $k$-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering.\n",
        "conference": "ICML",
        "中文标题": "论某些聚类问题中可解释性的代价",
        "摘要翻译": "聚类任务的可解释性代价可以定义为，如果我们强制最终的分区是可解释的，那么在目标函数方面不可避免的损失。在此，我们针对以下聚类问题研究这一代价：k均值、k中位数、k中心和最大间距。我们为一个通过决策树实现可解释性的自然模型提供了上下界。对于k均值和k中位数问题，我们的上界改进了[Dasgupta等人，ICML 20]在低维度下获得的结果。另一个贡献是一个简单高效的算法，用于为k均值问题构建可解释的聚类。我们提供的实证证据表明，其性能优于当前基于决策树的可解释聚类的最新技术。",
        "领域": "聚类分析、机器学习可解释性、决策树",
        "问题": "研究在强制聚类结果可解释时，目标函数不可避免的损失问题",
        "动机": "探索在聚类任务中实现可解释性所必须付出的代价，并寻求优化这一代价的方法",
        "方法": "通过决策树模型实现可解释性，为k均值、k中位数、k中心和最大间距问题提供上下界，并提出一种新的高效算法",
        "关键词": [
            "可解释性代价",
            "决策树聚类",
            "k均值优化",
            "聚类分析",
            "机器学习可解释性"
        ],
        "涉及的技术概念": {
            "可解释性代价": "在聚类任务中，为了实现结果的可解释性，在目标函数上必须接受的损失",
            "决策树模型": "用于实现聚类结果可解释性的技术手段，通过树形结构解释聚类过程",
            "k均值优化": "针对k均值聚类问题，提出的算法旨在减少可解释性带来的性能损失"
        },
        "success": true
    },
    {
        "order": 786,
        "title": "On the Problem of Underranking in Group-Fair Ranking",
        "html": "https://ICML.cc//virtual/2021/poster/8785",
        "abstract": "Bias in ranking systems, especially among the top ranks, can worsen social and economic inequalities, polarize opinions, and reinforce stereotypes. On the other hand, a bias correction for minority groups can cause more harm if perceived as favoring group-fair outcomes over meritocracy. Most group-fair ranking algorithms post-process a given ranking and output a group-fair ranking. In this paper, we formulate the problem of underranking in group-fair rankings based on how close the group-fair rank of each item is to its original rank, and prove a lower bound on the trade-off achievable for simultaneous underranking and group fairness in ranking. We give a fair ranking algorithm that takes any given ranking and outputs another ranking with simultaneous underranking and group fairness guarantees comparable to the lower bound we prove. Our experimental results confirm the theoretical trade-off between underranking and group fairness, and also show that our algorithm achieves the best of both when compared to the state-of-the-art baselines.",
        "conference": "ICML",
        "中文标题": "论群体公平排名中的低排名问题",
        "摘要翻译": "排名系统中的偏见，尤其是在顶级排名中，可能会加剧社会和经济不平等，极化意见，并强化刻板印象。另一方面，对少数群体的偏见校正如果被视为将群体公平结果置于精英管理之上，可能会造成更大的伤害。大多数群体公平排名算法会对给定的排名进行后处理，并输出一个群体公平的排名。在本文中，我们基于每个项目的群体公平排名与其原始排名的接近程度，制定了群体公平排名中的低排名问题，并证明了在排名中同时实现低排名和群体公平的可达到的权衡下限。我们给出了一种公平排名算法，该算法接受任何给定的排名，并输出另一个具有同时低排名和群体公平保证的排名，与我们证明的下限相当。我们的实验结果证实了低排名与群体公平之间的理论权衡，并且还显示，与最先进的基线相比，我们的算法实现了两者的最佳平衡。",
        "领域": "公平性排名算法、社会计算、信息检索",
        "问题": "解决排名系统中群体公平与低排名之间的权衡问题",
        "动机": "排名系统中的偏见可能加剧不平等，而过度校正偏见可能被视为牺牲精英管理，因此需要一种平衡的方法",
        "方法": "提出了一种公平排名算法，该算法通过后处理给定排名，输出同时满足低排名和群体公平保证的排名，并证明了相关权衡下限",
        "关键词": [
            "群体公平排名",
            "低排名问题",
            "公平性算法",
            "排名系统偏见",
            "社会计算"
        ],
        "涉及的技术概念": {
            "群体公平排名": "一种旨在减少排名系统中对特定群体偏见的排名方法，确保不同群体在排名中获得公平代表",
            "低排名问题": "指在群体公平排名中，项目的新排名与其原始排名相比下降过多的问题，影响排名的质量和公平性",
            "权衡下限": "在同时优化低排名和群体公平时，理论上可达到的最佳平衡点，用于评估算法的性能"
        },
        "success": true
    },
    {
        "order": 787,
        "title": "On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths",
        "html": "https://ICML.cc//virtual/2021/poster/8855",
        "abstract": "We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $\\Omega(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.\n",
        "conference": "ICML",
        "中文标题": "关于深度ReLU网络线性宽度梯度下降全局收敛性的证明",
        "摘要翻译": "我们为使用标准平方损失训练的深度ReLU网络的梯度下降全局收敛性提供了一个简单的证明，并展示了其在某些方面对现有技术的改进。特别是，虽然先前的工作要求所有隐藏层宽度至少为Ω(N^8)（N为训练样本数），但我们仅需要一个线性、二次或三次宽度的单层宽层，具体取决于初始化的类型。与许多基于神经切线核（NTK）的最新证明不同，我们的证明不需要跟踪整个NTK矩阵的演变，或者更一般地说，不需要跟踪训练过程中激活模式变化的任何量。相反，我们只需要跟踪最后一层隐藏层输出的演变，这得益于ReLU的Lipschitz性质，可以更容易地完成。我们设置的一些亮点包括：（i）所有层都使用标准梯度下降进行训练，（ii）网络具有标准参数化，而不是NTK参数化，（iii）网络具有单个宽层，而不是像大多数NTK相关结果中那样拥有所有宽隐藏层。",
        "领域": "深度学习理论、优化算法、神经网络训练",
        "问题": "证明深度ReLU网络在梯度下降训练下的全局收敛性，并减少对网络宽度的要求。",
        "动机": "减少对深度ReLU网络训练过程中网络宽度的要求，简化全局收敛性的证明过程。",
        "方法": "通过仅跟踪最后一层隐藏层的输出演变，利用ReLU的Lipschitz性质，简化全局收敛性的证明。",
        "关键词": [
            "梯度下降",
            "全局收敛性",
            "ReLU网络",
            "线性宽度",
            "神经网络训练"
        ],
        "涉及的技术概念": {
            "梯度下降": "用于优化深度ReLU网络的训练过程，以实现损失函数的最小化。",
            "ReLU网络": "使用ReLU作为激活函数的深度神经网络，因其Lipschitz性质而便于分析。",
            "Lipschitz性质": "ReLU激活函数的一个关键性质，使得跟踪网络输出的演变变得更加容易。"
        },
        "success": true
    },
    {
        "order": 788,
        "title": "On the Random Conjugate Kernel and Neural Tangent Kernel",
        "html": "https://ICML.cc//virtual/2021/poster/8463",
        "abstract": "We investigate the distributions of Conjugate Kernel (CK) and Neural Tangent Kernel (NTK) for ReLU networks with random initialization. We derive the precise distributions and moments of the diagonal elements of these kernels. For a feedforward network, these values converge in law to a log-normal distribution when the network depth $d$ and width $n$ simultaneously tend to infinity and the variance of log diagonal elements is proportional to ${d}/{n}$. For the residual network, in the limit that number of branches $m$ increases to infinity and the width $n$ remains fixed, the diagonal elements of Conjugate Kernel converge in law to a log-normal distribution where the variance of log value is proportional to ${1}/{n}$, and the diagonal elements of NTK converge in law to a log-normal distributed variable times the conjugate kernel of one feedforward network. Our new theoretical analysis results suggest that residual network remains trainable in the limit of infinite branches and fixed network width. The numerical experiments are conducted and all results validate the soundness of our theoretical analysis.",
        "conference": "ICML",
        "中文标题": "论随机共轭核与神经正切核",
        "摘要翻译": "我们研究了具有随机初始化的ReLU网络的共轭核（CK）和神经正切核（NTK）的分布。我们推导了这些核对角元素的精确分布和矩。对于一个前馈网络，当网络深度d和宽度n同时趋向于无穷大且对数对角元素的方差与d/n成正比时，这些值在律上收敛于对数正态分布。对于残差网络，在分支数m增加到无穷大且宽度n保持固定的极限情况下，共轭核的对角元素在律上收敛于对数正态分布，其中对数值的方差与1/n成正比，而NTK的对角元素在律上收敛于一个对数正态分布的变量乘以一个前馈网络的共轭核。我们的新理论分析结果表明，在无限分支和固定网络宽度的极限情况下，残差网络仍然可训练。进行了数值实验，所有结果验证了我们理论分析的正确性。",
        "领域": "深度学习理论、神经网络分析、机器学习理论",
        "问题": "研究ReLU网络的共轭核和神经正切核在随机初始化下的分布特性及其在深度和宽度变化时的极限行为。",
        "动机": "理解深度神经网络在训练初期的核行为，特别是共轭核和神经正切核的分布特性，对于揭示神经网络训练动态和优化特性具有重要意义。",
        "方法": "通过理论分析推导共轭核和神经正切核对角元素的精确分布和矩，并在不同网络架构（前馈网络和残差网络）和参数设置（深度、宽度、分支数）下研究其极限行为，辅以数值实验验证。",
        "关键词": [
            "共轭核",
            "神经正切核",
            "ReLU网络",
            "对数正态分布",
            "残差网络"
        ],
        "涉及的技术概念": {
            "共轭核（CK）": "用于描述神经网络在随机初始化下输出之间的相关性，是理解网络初始行为的关键概念。",
            "神经正切核（NTK）": "描述了神经网络在训练过程中梯度下降动态的核，对于理解网络的训练过程和泛化能力至关重要。",
            "对数正态分布": "在本文中，共轭核和神经正切核的对角元素在特定条件下收敛于对数正态分布，这一发现对于理解网络行为的统计特性具有重要意义。"
        },
        "success": true
    },
    {
        "order": 789,
        "title": "On Variational Inference in Biclustering Models",
        "html": "https://ICML.cc//virtual/2021/poster/10077",
        "abstract": "Biclustering structures exist ubiquitously in data matrices and the biclustering problem was first formalized by John Hartigan (1972) to cluster rows and columns simultaneously. In this paper, we develop a theory for the estimation of general biclustering models, where the data is assumed to follow certain statistical distribution with underlying biclustering structure.  Due to the existence of latent variables, directly computing the maximal likelihood estimator is prohibitively difficult in practice and we instead consider the variational inference (VI) approach to solve the parameter estimation problem.  Although variational inference method generally has good empirical performance, there are very few theoretical results around VI. In this paper, we obtain the precise estimation bound of variational estimator and show that it matches the minimax rate in terms of estimation error under mild assumptions in biclustering setting. Furthermore, we study the convergence property of the coordinate ascent variational inference algorithm, where both local and global convergence results have been provided. Numerical results validate our new theories.",
        "conference": "ICML",
        "中文标题": "关于双聚类模型中的变分推断",
        "摘要翻译": "双聚类结构在数据矩阵中普遍存在，双聚类问题最初由John Hartigan（1972年）形式化，旨在同时聚类行和列。在本文中，我们发展了一个用于估计一般双聚类模型的理论，其中假设数据遵循具有潜在双聚类结构的特定统计分布。由于潜在变量的存在，直接计算最大似然估计量在实践中极为困难，因此我们转而考虑使用变分推断（VI）方法来解决参数估计问题。尽管变分推断方法通常具有良好的实证性能，但围绕VI的理论结果非常少。在本文中，我们获得了变分估计量的精确估计界限，并表明在双聚类设置的温和假设下，它在估计误差方面与极小极大率相匹配。此外，我们研究了坐标上升变分推断算法的收敛性质，其中提供了局部和全局收敛结果。数值结果验证了我们的新理论。",
        "领域": "统计机器学习、数据挖掘、模式识别",
        "问题": "解决在存在潜在变量的情况下，双聚类模型参数估计的困难问题。",
        "动机": "由于直接计算最大似然估计量在实践中极为困难，研究变分推断方法在双聚类模型中的应用及其理论性质。",
        "方法": "采用变分推断（VI）方法进行参数估计，并研究其估计界限和算法的收敛性质。",
        "关键词": [
            "双聚类",
            "变分推断",
            "参数估计",
            "统计分布",
            "收敛性质"
        ],
        "涉及的技术概念": {
            "变分推断": "用于近似计算难以直接求解的概率模型的后验分布，本文中用于解决双聚类模型的参数估计问题。",
            "双聚类": "同时聚类数据矩阵的行和列，以发现数据中的局部模式。",
            "坐标上升变分推断算法": "一种优化算法，用于在变分推断中迭代更新变分参数，本文中研究了其局部和全局收敛性质。"
        },
        "success": true
    },
    {
        "order": 790,
        "title": "Oops I Took A Gradient: Scalable Sampling for Discrete Distributions",
        "html": "https://ICML.cc//virtual/2021/poster/9335",
        "abstract": "We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables.  Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a Metropolis-Hastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate our improved sampler for training deep energy-based models on high dimensional discrete image data. This approach outperforms variational auto-encoders and existing energy-based models.  Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.",
        "conference": "ICML",
        "中文标题": "哎呀，我取了个梯度：离散分布的可扩展采样方法",
        "摘要翻译": "我们提出了一种通用且可扩展的近似采样策略，适用于具有离散变量的概率模型。我们的方法利用似然函数相对于其离散输入的梯度，在Metropolis-Hastings采样器中提出更新。我们通过实证表明，这种方法在包括Ising模型、Potts模型、受限玻尔兹曼机和因子隐马尔可夫模型在内的多种困难设置中优于通用采样器。我们还展示了我们改进的采样器在训练高维离散图像数据的深度能量基模型中的应用。这种方法优于变分自编码器和现有的能量基模型。最后，我们给出了界限，表明我们的方法在提出局部更新的采样器类别中是接近最优的。",
        "领域": "概率模型采样、深度能量基模型、离散变量优化",
        "问题": "解决在具有离散变量的概率模型中高效采样的问题",
        "动机": "开发一种能够超越现有通用采样器性能的采样策略，特别是在处理高维离散数据时",
        "方法": "利用似然函数相对于离散输入的梯度，在Metropolis-Hastings采样器中提出更新",
        "关键词": [
            "离散分布采样",
            "Metropolis-Hastings采样器",
            "深度能量基模型",
            "梯度方法",
            "高维离散数据"
        ],
        "涉及的技术概念": {
            "Metropolis-Hastings采样器": "一种马尔可夫链蒙特卡洛方法，用于从概率分布中获取样本序列",
            "深度能量基模型": "一种用于建模数据分布的深度生成模型，通过能量函数定义概率分布",
            "梯度方法": "利用目标函数的梯度信息来指导采样或优化过程的技术"
        },
        "success": true
    },
    {
        "order": 791,
        "title": "Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics",
        "html": "https://ICML.cc//virtual/2021/poster/10345",
        "abstract": "Democratization of machine learning requires architectures that automatically adapt to new problems. Neural Differential Equations (NDEs) have emerged as a popular modeling framework by removing the need for ML practitioners to choose the number of layers in a recurrent model. While we can control the computational cost by choosing the number of layers in standard architectures, in NDEs the number of neural network evaluations for a forward pass can depend on the number of steps of the adaptive ODE solver. But, can we force the NDE to learn the version with the least steps while not increasing the training cost? Current strategies to overcome slow prediction require high order automatic differentiation, leading to significantly higher training time. We describe a novel regularization method that uses the internal cost heuristics of adaptive differential equation solvers combined with discrete adjoint sensitivities to guide the training process towards learning NDEs that are easier to solve. This approach opens up the blackbox numerical analysis behind the differential equation solver's algorithm and directly uses its local error estimates and stiffness heuristics as cheap and accurate cost estimates. We incorporate our method without any change in the underlying NDE framework and show that our method extends beyond Ordinary Differential Equations to accommodate Neural Stochastic Differential Equations. We demonstrate how our approach can halve the prediction time and, unlike other methods which can increase the training time by an order of magnitude, we demonstrate similar reduction in training times. Together this showcases how the knowledge embedded within state-of-the-art equation solvers can be used to enhance machine learning.",
        "conference": "ICML",
        "中文标题": "揭开黑盒：通过正则化内部求解器启发式方法加速神经微分方程",
        "摘要翻译": "机器学习的民主化需要能够自动适应新问题的架构。神经微分方程（NDEs）通过消除机器学习从业者选择循环模型中层数的需求，已成为一种流行的建模框架。在标准架构中，我们可以通过选择层数来控制计算成本，而在NDEs中，前向传递的神经网络评估次数可能取决于自适应ODE求解器的步数。但是，我们能否在不增加训练成本的情况下，迫使NDE学习步数最少的版本？当前克服预测速度慢的策略需要高阶自动微分，导致训练时间显著增加。我们描述了一种新颖的正则化方法，该方法利用自适应微分方程求解器的内部成本启发式方法与离散伴随敏感性相结合，引导训练过程学习更易求解的NDEs。这种方法打开了微分方程求解器算法背后的黑盒数值分析，并直接使用其局部误差估计和刚度启发式方法作为廉价且准确的成本估计。我们在不改变底层NDE框架的情况下融入了我们的方法，并展示了我们的方法不仅适用于普通微分方程，还能适应神经随机微分方程。我们证明了我们的方法可以将预测时间减半，并且与其他可能将训练时间增加一个数量级的方法不同，我们展示了训练时间的类似减少。这共同展示了如何利用嵌入在最先进方程求解器中的知识来增强机器学习。",
        "领域": "神经微分方程、自适应求解器优化、机器学习效率提升",
        "问题": "如何在不增加训练成本的情况下，减少神经微分方程模型预测时的计算步骤",
        "动机": "解决神经微分方程模型在预测时因自适应ODE求解器步数过多导致的计算效率低下问题",
        "方法": "提出一种新颖的正则化方法，利用自适应微分方程求解器的内部成本启发式与离散伴随敏感性，引导训练过程学习更易求解的神经微分方程",
        "关键词": [
            "神经微分方程",
            "自适应求解器",
            "正则化方法",
            "离散伴随敏感性",
            "计算效率"
        ],
        "涉及的技术概念": {
            "神经微分方程": "一种通过微分方程建模的神经网络框架，消除了选择网络层数的需求",
            "自适应求解器": "用于求解微分方程的算法，能够根据局部误差估计和刚度启发式调整步长",
            "离散伴随敏感性": "一种用于计算梯度的方法，结合正则化方法优化神经微分方程的训练过程"
        },
        "success": true
    },
    {
        "order": 792,
        "title": "Operationalizing Complex Causes: A Pragmatic View of Mediation",
        "html": "https://ICML.cc//virtual/2021/poster/9915",
        "abstract": "We examine the problem of causal response estimation for complex objects (e.g., text, images, genomics). In this setting, classical \\emph{atomic} interventions are often not available (e.g., changes to characters, pixels, DNA base-pairs). Instead, we only have access to indirect or \\emph{crude} interventions (e.g., enrolling in a writing program, modifying a scene, applying a gene therapy). In this work, we formalize this problem and provide an initial solution. Given a collection of candidate mediators, we propose (a) a two-step method for predicting the causal responses of crude interventions; and (b) a testing procedure to identify mediators of crude interventions. We demonstrate, on a range of simulated and real-world-inspired examples, that our approach allows us to efficiently estimate the effect of crude interventions with limited data from new treatment regimes.",
        "conference": "ICML",
        "中文标题": "操作化复杂原因：中介的实用视角",
        "摘要翻译": "我们研究了复杂对象（如文本、图像、基因组学）的因果响应估计问题。在这种设置下，经典的原子干预通常不可用（如对字符、像素、DNA碱基对的更改）。相反，我们只能访问间接或粗略的干预（如参加写作项目、修改场景、应用基因治疗）。在这项工作中，我们形式化了这个问题并提供了一个初步的解决方案。给定一组候选中介，我们提出了（a）一个两步法来预测粗略干预的因果响应；以及（b）一个测试程序来识别粗略干预的中介。我们在一系列模拟和现实世界启发的例子中证明，我们的方法使我们能够有效地估计粗略干预的效果，且仅需来自新治疗方案的有限数据。",
        "领域": "因果推理、机器学习应用、基因组学数据分析",
        "问题": "在复杂对象（如文本、图像、基因组学）中，如何估计因果响应，尤其是在只能进行间接或粗略干预的情况下。",
        "动机": "解决在复杂对象上进行因果响应估计时，经典原子干预不可用的问题，提供一种能够处理间接或粗略干预的方法。",
        "方法": "提出了一种两步法来预测粗略干预的因果响应，并开发了一个测试程序来识别这些干预的中介。",
        "关键词": [
            "因果响应估计",
            "粗略干预",
            "中介识别",
            "复杂对象",
            "有限数据"
        ],
        "涉及的技术概念": {
            "因果响应估计": "在给定干预下，估计对象响应变化的技术，用于理解干预的效果。",
            "粗略干预": "指那些不能直接作用于对象基本单元（如像素、DNA碱基对）的干预，而是通过更高级别的操作（如修改场景、应用基因治疗）间接影响对象。",
            "中介识别": "识别在干预和结果之间起中介作用的变量或过程，有助于理解干预如何影响结果。"
        },
        "success": true
    },
    {
        "order": 793,
        "title": "OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/10187",
        "abstract": "We consider the offline reinforcement learning (RL) setting where the agent aims to optimize the policy solely from the data without further environment interactions. In offline RL, the distributional shift becomes the primary source of difficulty, which arises from the deviation of the target policy being optimized from the behavior policy used for data collection. This typically causes overestimation of action values, which poses severe problems for model-free algorithms that use bootstrapping. To mitigate the problem, prior offline RL algorithms often used sophisticated techniques that encourage underestimation of action values, which introduces an additional set of hyperparameters that need to be tuned properly. In this paper, we present an offline RL algorithm that prevents overestimation in a more principled way. Our algorithm, OptiDICE, directly estimates the stationary distribution corrections of the optimal policy and does not rely on policy-gradients, unlike previous offline RL algorithms. Using an extensive set of benchmark datasets for offline RL, we show that OptiDICE performs competitively with the state-of-the-art methods.\n",
        "conference": "ICML",
        "中文标题": "OptiDICE：通过稳态分布校正估计进行离线策略优化",
        "摘要翻译": "我们考虑离线强化学习（RL）设置，其中智能体的目标是从数据中优化策略，而无需进一步的环境交互。在离线RL中，分布偏移成为主要困难来源，这是由于被优化的目标策略与用于数据收集的行为策略之间的偏差引起的。这通常会导致对动作值的高估，这对使用自举的无模型算法构成了严重问题。为了缓解这个问题，先前的离线RL算法经常使用鼓励低估动作值的复杂技术，这引入了需要适当调整的额外超参数集。在本文中，我们提出了一种离线RL算法，它以更原则性的方式防止高估。我们的算法OptiDICE直接估计最优策略的稳态分布校正，并且不像以前的离线RL算法那样依赖于策略梯度。通过使用离线RL的广泛基准数据集集，我们表明OptiDICE与最先进的方法相比具有竞争力。",
        "领域": "离线强化学习、策略优化、分布校正估计",
        "问题": "解决离线强化学习中由于分布偏移导致的对动作值的高估问题",
        "动机": "为了在无需进一步环境交互的情况下优化策略，并减少由于分布偏移引起的高估问题",
        "方法": "提出OptiDICE算法，直接估计最优策略的稳态分布校正，避免使用策略梯度",
        "关键词": [
            "离线强化学习",
            "策略优化",
            "稳态分布校正",
            "动作值高估",
            "无模型算法"
        ],
        "涉及的技术概念": {
            "稳态分布校正": "用于直接估计最优策略的稳态分布，避免高估动作值",
            "离线强化学习": "在无需进一步环境交互的情况下，从数据中优化策略",
            "分布偏移": "由于目标策略与行为策略之间的偏差引起的主要困难来源"
        },
        "success": true
    },
    {
        "order": 794,
        "title": "Optimal Complexity in Decentralized Training",
        "html": "https://ICML.cc//virtual/2021/poster/8893",
        "abstract": "Decentralization is a promising method of scaling up parallel machine learning systems. In this paper, we provide a tight lower bound on the iteration complexity for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical gap in known convergence rates of many existing decentralized training algorithms, such as D-PSGD.  We prove by construction this lower bound is tight and achievable. Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized algorithm that achieves the lower bound with only a logarithm gap. Empirically, we compare DeTAG with other decentralized algorithms on image classification tasks, and we show DeTAG enjoys faster convergence compared to baselines, especially on unshuffled data and in sparse networks.",
        "conference": "ICML",
        "中文标题": "去中心化训练中的最优复杂度",
        "摘要翻译": "去中心化是一种有前景的扩展并行机器学习系统规模的方法。在本文中，我们为这种在随机非凸环境下的方法提供了一个紧密的迭代复杂度下界。我们的下界揭示了诸如D-PSGD等许多现有去中心化训练算法在已知收敛速率上的理论差距。我们通过构造证明这个下界是紧密且可达的。受到我们见解的启发，我们进一步提出了DeTAG，一种实用的gossip风格去中心化算法，该算法仅以对数差距达到下界。实证上，我们在图像分类任务上将DeTAG与其他去中心化算法进行了比较，结果显示DeTAG相比基线方法享有更快的收敛速度，尤其是在未洗牌的数据和稀疏网络上。",
        "领域": "并行机器学习系统、去中心化学习、随机优化",
        "问题": "解决去中心化训练方法在随机非凸环境下的迭代复杂度下界问题，以及现有算法收敛速率上的理论差距。",
        "动机": "揭示并填补现有去中心化训练算法在收敛速率上的理论差距，提出更高效的算法。",
        "方法": "通过构造证明迭代复杂度的下界是紧密且可达的，并提出DeTAG算法，一种gossip风格的去中心化算法，以对数差距达到下界。",
        "关键词": [
            "去中心化训练",
            "迭代复杂度",
            "随机非凸优化",
            "gossip算法",
            "收敛速率"
        ],
        "涉及的技术概念": {
            "迭代复杂度下界": "在随机非凸环境下，去中心化训练方法所需的最小迭代次数，用于衡量算法效率。",
            "gossip风格算法": "一种去中心化通信协议，节点仅与邻居交换信息，减少通信开销，提高系统扩展性。",
            "收敛速率": "算法接近最优解的速度，本文中通过构造下界和提出新算法来优化这一指标。"
        },
        "success": true
    },
    {
        "order": 795,
        "title": "Optimal Counterfactual Explanations in Tree Ensembles",
        "html": "https://ICML.cc//virtual/2021/poster/9761",
        "abstract": "Counterfactual explanations are usually generated through heuristics that are sensitive to the search's initial conditions. The absence of guarantees of performance and robustness hinders trustworthiness. In this paper, we take a disciplined approach towards counterfactual explanations for tree ensembles. We advocate for a model-based search aiming at 'optimal' explanations and propose efficient mixed-integer programming approaches. We show that isolation forests can be modeled within our framework to focus the search on plausible explanations with a low outlier score. We provide comprehensive coverage of additional constraints that model important objectives, heterogeneous data types, structural constraints on the feature space, along with resource and actionability restrictions. Our experimental analyses demonstrate that the proposed search approach requires a computational effort that is orders of magnitude smaller than previous mathematical programming algorithms. It scales up to large data sets and tree ensembles, where it provides, within seconds, systematic explanations grounded on well-defined models solved to optimality.",
        "conference": "ICML",
        "中文标题": "树集成中的最优反事实解释",
        "摘要翻译": "反事实解释通常通过启发式方法生成，这些方法对搜索的初始条件敏感。缺乏性能和鲁棒性的保证阻碍了可信度。在本文中，我们采取了一种有纪律的方法来为树集成生成反事实解释。我们提倡一种基于模型的搜索，旨在寻找‘最优’解释，并提出了高效的混合整数规划方法。我们展示了隔离森林可以在我们的框架内建模，以将搜索集中在具有低异常分数的合理解释上。我们全面覆盖了额外的约束，这些约束模拟了重要目标、异构数据类型、特征空间上的结构约束，以及资源和可操作性限制。我们的实验分析表明，所提出的搜索方法所需的计算努力比以前的数学规划算法小几个数量级。它可以扩展到大型数据集和树集成，在几秒钟内提供基于明确定义模型的最优解释。",
        "领域": "解释性人工智能、机器学习模型解释、树集成方法",
        "问题": "如何为树集成模型生成最优且可信的反事实解释",
        "动机": "提高反事实解释的性能和鲁棒性，增强其可信度",
        "方法": "采用基于模型的搜索和高效的混合整数规划方法，结合隔离森林模型来优化解释的生成",
        "关键词": [
            "反事实解释",
            "树集成",
            "混合整数规划",
            "隔离森林",
            "模型解释"
        ],
        "涉及的技术概念": {
            "反事实解释": "在机器学习中，反事实解释是指通过展示如果输入特征有所不同，模型输出会如何改变来解释模型决策的方法",
            "树集成": "一种机器学习方法，通过组合多个决策树的预测来提高模型的准确性和鲁棒性",
            "混合整数规划": "一种数学优化方法，用于在包含整数和连续变量的约束条件下寻找最优解"
        },
        "success": true
    },
    {
        "order": 796,
        "title": "Optimal Estimation of High Dimensional Smooth Additive Function Based on Noisy Observations",
        "html": "https://ICML.cc//virtual/2021/poster/9535",
        "abstract": " Given $\\bx_j = \\btheta + \\bepsilon_j$, $j=1,...,n$ where $\\btheta \\in \\RR^d$ is an unknown parameter and\n$\\bepsilon_j$ are i.i.d. Gaussian noise vectors,\nwe study the estimation of $f(\\btheta)$ for a given smooth function $f:\\RR^d \\rightarrow \\RR$ equipped with an additive structure.\nWe inherit the idea from a recent work which introduced an effective bias reduction technique through iterative bootstrap and derive\na bias-reducing estimator. \nBy establishing its normal approximation results, we show that the proposed estimator can achieve asymptotic normality with a looser constraint on smoothness compared with general smooth function due to the additive structure.\nSuch results further imply that the proposed estimator is asymptotically efficient.\nBoth upper and lower bounds on mean squared error are proved which shows the proposed estimator is minimax optimal for the smooth class considered. \nNumerical simulation results are presented to validate our analysis and show its superior performance of the proposed estimator\nover the plug-in approach in terms of bias reduction and building confidence~intervals.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于噪声观测的高维平滑可加函数的最优估计",
        "摘要翻译": "给定$\\\\bx_j = \\\\btheta + \\\\bepsilon_j$, $j=1,...,n$，其中$\\\\btheta \\\\in \\\\RR^d$是一个未知参数，$\\\\bepsilon_j$是独立同分布的高斯噪声向量，我们研究了对给定平滑函数$f:\\\\RR^d \\\\rightarrow \\\\RR$的估计，该函数具有可加结构。我们继承了一项近期工作中引入的通过迭代自举实现有效偏差减少技术的思路，并推导出一个减少偏差的估计器。通过建立其正态近似结果，我们表明，与一般平滑函数相比，由于可加结构，所提出的估计器可以在平滑度约束更宽松的情况下实现渐近正态性。这些结果进一步表明，所提出的估计器是渐近有效的。证明了均方误差的上界和下界，这表明所提出的估计器对于所考虑的平滑类是最小最大最优的。数值模拟结果验证了我们的分析，并展示了所提出的估计器在偏差减少和构建置信区间方面优于插件方法的性能。",
        "领域": "高维统计估计, 平滑函数建模, 偏差减少技术",
        "问题": "在高维噪声观测下，如何有效地估计具有可加结构的平滑函数",
        "动机": "研究旨在解决高维数据中平滑函数估计的偏差问题，特别是在存在噪声的情况下，通过减少偏差提高估计的准确性和效率",
        "方法": "采用迭代自举技术减少偏差，推导出偏差减少的估计器，并通过理论分析和数值模拟验证其性能",
        "关键词": [
            "高维估计",
            "平滑可加函数",
            "偏差减少",
            "迭代自举",
            "渐近正态性"
        ],
        "涉及的技术概念": {
            "迭代自举": "用于减少估计偏差的技术，通过重复采样和估计过程来改进估计器的性能",
            "渐近正态性": "估计器在样本量趋于无穷时的分布性质，表明估计器具有良好的统计性质"
        }
    },
    {
        "order": 797,
        "title": "Optimal Non-Convex Exact Recovery in Stochastic Block Model via Projected Power Method",
        "html": "https://ICML.cc//virtual/2021/poster/10669",
        "abstract": "In this paper, we study the problem of exact community recovery in the symmetric stochastic block model, where a graph of $n$ vertices is randomly generated by partitioning the vertices into $K \\ge 2$ equal-sized communities and then connecting each pair of vertices with probability that depends on their community memberships. Although the maximum-likelihood formulation of this problem is discrete and non-convex, we propose to tackle it directly using projected power iterations with an initialization that satisfies a partial recovery condition. Such an initialization can be obtained by a host of existing methods. We show that in the logarithmic degree regime of the considered problem, the proposed method can exactly recover the underlying communities at the information-theoretic limit. Moreover, with a qualified initialization, it runs in $\\mO(n\\log^2n/\\log\\log n)$ time, which is competitive with existing state-of-the-art methods. We also present numerical results of the proposed method to support and complement our theoretical development.",
        "conference": "ICML",
        "中文标题": "通过投影幂方法在随机块模型中实现最优非凸精确恢复",
        "摘要翻译": "本文研究了对称随机块模型中的精确社区恢复问题，其中通过将顶点划分为K≥2个大小相等的社区，并根据它们的社区成员资格以一定概率连接每对顶点，随机生成一个包含n个顶点的图。尽管该问题的最大似然公式是离散且非凸的，我们提出直接使用满足部分恢复条件的初始化的投影幂迭代来解决它。这样的初始化可以通过多种现有方法获得。我们表明，在所考虑问题的对数度制度下，所提出的方法可以在信息理论极限下精确恢复底层社区。此外，在合格的初始化下，它运行时间为O(nlog²n/loglogn)，与现有的最先进方法相比具有竞争力。我们还提供了所提出方法的数值结果，以支持和补充我们的理论发展。",
        "领域": "社区检测、图模型分析、统计机器学习",
        "问题": "在对称随机块模型中实现精确社区恢复",
        "动机": "解决在离散且非凸的最大似然问题中精确恢复社区结构的挑战",
        "方法": "使用满足部分恢复条件的初始化的投影幂迭代方法",
        "关键词": [
            "社区检测",
            "随机块模型",
            "投影幂方法",
            "非凸优化",
            "信息理论极限"
        ],
        "涉及的技术概念": {
            "投影幂迭代": "用于解决非凸优化问题的迭代方法，通过投影步骤确保解的可行性",
            "随机块模型": "一种生成具有社区结构的随机图的模型，用于社区检测问题的研究",
            "信息理论极限": "在给定模型下，理论上能够实现精确恢复的最低条件或边界"
        },
        "success": true
    },
    {
        "order": 798,
        "title": "Optimal Off-Policy Evaluation from Multiple Logging Policies",
        "html": "https://ICML.cc//virtual/2021/poster/10615",
        "abstract": "We study off-policy evaluation (OPE) from multiple logging policies, each generating a dataset of fixed size, i.e., stratified sampling. Previous work noted that in this setting the ordering of the variances of different importance sampling estimators is instance-dependent, which brings up a dilemma as to which importance sampling weights to use. In this paper, we resolve this dilemma by finding the OPE estimator for multiple loggers with minimum variance for any instance, i.e., the efficient one. In particular, we establish the efficiency bound under stratified sampling and propose an estimator achieving this bound when given consistent $q$-estimates. To guard against misspecification of $q$-functions, we also provide a way to choose the control variate in a hypothesis class to minimize variance. Extensive experiments demonstrate the benefits of our methods' efficiently leveraging of the stratified sampling of off-policy data from multiple loggers. ",
        "conference": "ICML",
        "中文标题": "从多个记录策略中进行最优离策略评估",
        "摘要翻译": "我们研究了从多个记录策略中进行离策略评估（OPE），每个策略生成一个固定大小的数据集，即分层抽样。先前的工作指出，在这种设置下，不同重要性抽样估计器的方差排序依赖于具体实例，这带来了关于使用哪些重要性抽样权重的困境。在本文中，我们通过找到对于任何实例都具有最小方差的多个记录器的OPE估计器，即高效的估计器，解决了这一困境。特别是，我们建立了分层抽样下的效率界限，并提出了一个在给定一致的q估计时达到这一界限的估计器。为了防止q函数的错误设定，我们还提供了一种方法，在假设类中选择控制变量以最小化方差。大量实验证明了我们的方法在高效利用来自多个记录器的离策略数据分层抽样方面的优势。",
        "领域": "强化学习、策略评估、数据抽样",
        "问题": "解决在多个记录策略下进行离策略评估时，如何选择重要性抽样权重以最小化方差的问题。",
        "动机": "在多个记录策略生成的数据集上进行离策略评估时，现有方法在选择重要性抽样权重上存在实例依赖的困境，本研究旨在解决这一问题。",
        "方法": "建立了分层抽样下的效率界限，并提出了一个在给定一致的q估计时达到这一界限的估计器，同时提供了一种方法来选择控制变量以最小化方差。",
        "关键词": [
            "离策略评估",
            "分层抽样",
            "重要性抽样",
            "效率界限",
            "控制变量"
        ],
        "涉及的技术概念": {
            "离策略评估（OPE）": "评估一个策略的性能，而数据是由另一个或多个不同的策略生成的。",
            "分层抽样": "一种抽样方法，其中总体被分成若干个互不重叠的子群（层），然后从每个子群中独立地进行抽样。",
            "效率界限": "在给定条件下，估计器可以达到的最小方差界限。"
        },
        "success": true
    },
    {
        "order": 799,
        "title": "Optimal regret algorithm for Pseudo-1d Bandit Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/10431",
        "abstract": "We study online learning with bandit feedback (i.e. learner has access to only zeroth-order oracle) where cost/reward functions $\\f_t$ admit a 'pseudo-1d' structure, i.e. $\\f_t(\\w) = \\loss_t(\\pred_t(\\w))$ where the output of $\\pred_t$ is one-dimensional. At each round, the learner observes context $\\x_t$, plays prediction $\\pred_t(\\w_t; \\x_t)$ (e.g. $\\pred_t(\\cdot)=\\langle \\x_t, \\cdot\\rangle$) for some $\\w_t \\in \\mathbb{R}^d$ and observes loss $\\loss_t(\\pred_t(\\w_t))$ where $\\loss_t$ is a convex Lipschitz-continuous function. The goal is to minimize the standard regret metric. This pseudo-1d bandit convex optimization problem  (\\SBCO) arises frequently in domains such as online decision-making or parameter-tuning in large systems. For this problem, we first show a regret lower bound of $\\min(\\sqrt{dT}, T^{3/4})$ for any algorithm, where $T$ is the number of rounds. We propose a new algorithm \\sbcalg that combines randomized online  gradient descent with a kernelized exponential weights method to exploit the pseudo-1d structure effectively, guaranteeing the {\\em optimal} regret bound mentioned above, up to additional logarithmic factors. In contrast, applying state-of-the-art online convex optimization methods leads to $\\tilde{O}\\left(\\min\\left(d^{9.5}\\sqrt{T},\\sqrt{d}T^{3/4}\\right)\\right)$ regret, that is significantly suboptimal in terms of $d$.",
        "conference": "ICML",
        "中文标题": "伪一维老虎机凸优化的最优遗憾算法",
        "摘要翻译": "我们研究了带有老虎机反馈的在线学习（即学习者只能访问零阶预言机），其中成本/奖励函数f_t具有‘伪一维’结构，即f_t(w) = loss_t(pred_t(w))，其中pred_t的输出是一维的。在每一轮中，学习者观察上下文x_t，对某些w_t ∈ R^d进行预测pred_t(w_t; x_t)（例如pred_t(·)=⟨x_t, ·⟩），并观察损失loss_t(pred_t(w_t))，其中loss_t是一个凸利普希茨连续函数。目标是最小化标准的遗憾度量。这种伪一维老虎机凸优化问题（SBCO）在诸如在线决策或大型系统中的参数调整等领域频繁出现。对于这个问题，我们首先展示了任何算法的遗憾下界为min(√(dT), T^(3/4))，其中T是轮数。我们提出了一种新算法sbcalg，它结合了随机在线梯度下降和核化指数权重方法，以有效利用伪一维结构，保证上述最优遗憾界限，最多附加对数因子。相比之下，应用最先进的在线凸优化方法会导致Õ(min(d^9.5√T,√d T^(3/4)))的遗憾，这在d方面显著次优。",
        "领域": "在线学习、凸优化、老虎机反馈",
        "问题": "在伪一维老虎机凸优化问题中最小化遗憾",
        "动机": "解决在只能访问零阶预言机的情况下，如何有效利用伪一维结构进行在线学习的问题",
        "方法": "结合随机在线梯度下降和核化指数权重方法的新算法",
        "关键词": [
            "伪一维结构",
            "老虎机反馈",
            "在线凸优化",
            "遗憾最小化",
            "随机梯度下降"
        ],
        "涉及的技术概念": {
            "伪一维结构": "指成本/奖励函数可以表示为一维函数的输出，简化了优化问题的复杂性",
            "老虎机反馈": "学习者只能观察到所选动作的成本或奖励，而不能获得关于其他可能动作的信息",
            "核化指数权重方法": "一种用于在线学习的技术，通过赋予不同预测以指数权重来优化决策过程"
        },
        "success": true
    },
    {
        "order": 800,
        "title": "Optimal Streaming Algorithms for Multi-Armed Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/8467",
        "abstract": "This paper studies two variants of the best arm identification (BAI) problem under the streaming model, where we have a stream of n arms with reward distributions supported on [0,1] with unknown means. The arms in the stream are arriving one by one, and the algorithm cannot access an arm unless it is stored in a limited size memory. \n\nWe first study the streaming \\epslion-topk-arms identification problem, which asks for k arms whose reward means are lower than that of the k-th best arm by at most \\epsilon with probability at least 1-\\delta. For general \\epsilon \\in (0,1), the existing solution for this problem assumes k = 1 and achieves the optimal sample complexity O(\\frac{n}{\\epsilon^2} \\log \\frac{1}{\\delta}) using O(\\log^*(n)) memory and a single pass of the stream. We propose an algorithm that works for any k and achieves the optimal sample complexity O(\\frac{n}{\\epsilon^2} \\log\\frac{k}{\\delta}) using a single-arm memory and a single pass of the stream. \n\nSecond, we study the streaming BAI problem, where the objective is to identify the arm with the maximum reward mean with at least 1-\\delta probability, using a single-arm memory and as few passes of the input stream as possible.  We present a single-arm-memory algorithm that achieves a near instance-dependent optimal sample complexity within O(\\log \\Delta_2^{-1}) passes, where \\Delta_2 is the gap between the mean of the best arm and that of the second best arm.",
        "conference": "ICML",
        "中文标题": "多臂老虎机问题的最优流式算法",
        "摘要翻译": "本文研究了在流式模型下的两种最佳臂识别（BAI）问题变体，其中我们有一个包含n个臂的流，这些臂的奖励分布支持在[0,1]区间内，均值未知。臂在流中一个接一个地到达，算法无法访问一个臂，除非它被存储在有限大小的内存中。\n\n我们首先研究了流式ε-topk臂识别问题，该问题要求识别出k个臂，这些臂的奖励均值与第k佳臂的均值相差不超过ε的概率至少为1-δ。对于一般的ε∈(0,1)，该问题的现有解决方案假设k=1，并使用O(log^*(n))内存和单次流式遍历实现了最优样本复杂度O(n/ε^2 log(1/δ))。我们提出了一种适用于任何k的算法，该算法使用单臂内存和单次流式遍历实现了最优样本复杂度O(n/ε^2 log(k/δ))。\n\n其次，我们研究了流式BAI问题，其目标是使用单臂内存和尽可能少的输入流遍历次数，以至少1-δ的概率识别出具有最大奖励均值的臂。我们提出了一种单臂内存算法，该算法在O(log Δ_2^-1)次遍历内实现了接近实例依赖最优的样本复杂度，其中Δ_2是最佳臂与第二佳臂均值之间的差距。",
        "领域": "强化学习、在线学习、算法优化",
        "问题": "在有限内存和流式数据条件下，高效识别多臂老虎机问题中的最佳臂或接近最佳的臂",
        "动机": "解决在流式数据处理环境下，如何利用有限的内存资源高效地识别最佳臂或接近最佳的臂的问题",
        "方法": "提出两种算法，分别针对流式ε-topk臂识别问题和流式最佳臂识别问题，使用单臂内存和优化的流式遍历策略",
        "关键词": [
            "多臂老虎机",
            "流式算法",
            "最佳臂识别",
            "样本复杂度",
            "有限内存"
        ],
        "涉及的技术概念": {
            "流式模型": "在数据连续到达且内存有限的环境下处理数据，适用于大规模或实时数据处理场景",
            "样本复杂度": "算法达到预定性能所需的最小样本数量，是衡量算法效率的重要指标",
            "ε-topk臂识别": "识别出奖励均值与第k佳臂相差不超过ε的k个臂的问题，是BAI问题的一种变体"
        },
        "success": true
    },
    {
        "order": 801,
        "title": "Optimal Thompson Sampling strategies  for support-aware CVaR bandits",
        "html": "https://ICML.cc//virtual/2021/poster/9315",
        "abstract": "In this paper we study a multi-arm bandit problem in which the quality of each arm is measured by the Conditional Value at Risk (CVaR) at some level alpha of the reward distribution. While existing works in this setting mainly focus on Upper Confidence Bound algorithms, we introduce a new Thompson Sampling approach for CVaR bandits on bounded rewards that is flexible enough to solve a variety of problems grounded on physical resources. \nBuilding on a recent work by Riou & Honda (2020), we introduce B-CVTS for continuous bounded rewards and M-CVTS for multinomial distributions. On the theoretical side, we provide a non-trivial extension of their analysis that enables to theoretically bound their CVaR regret minimization performance. Strikingly, our results show that these strategies are the first to provably achieve asymptotic optimality in CVaR bandits, matching the corresponding asymptotic lower bounds for this setting.\n Further, we illustrate empirically the benefit of Thompson Sampling approaches both in a realistic environment simulating a use-case in agriculture and on various synthetic examples.",
        "conference": "ICML",
        "中文标题": "支持感知CVaR老虎机的最优Thompson采样策略",
        "摘要翻译": "本文研究了一个多臂老虎机问题，其中每个臂的质量通过奖励分布在某个水平alpha下的条件风险价值（CVaR）来衡量。虽然这一设定下的现有工作主要集中在上置信界算法上，但我们引入了一种新的Thompson采样方法，用于有界奖励的CVaR老虎机，这种方法足够灵活，可以解决基于物理资源的各种问题。基于Riou & Honda (2020)最近的工作，我们为连续有界奖励引入了B-CVTS，为多项分布引入了M-CVTS。在理论方面，我们对他们的分析进行了非平凡的扩展，使得能够在理论上限制它们的CVaR遗憾最小化性能。引人注目的是，我们的结果表明，这些策略是首批在CVaR老虎机中可证明实现渐近最优性的策略，匹配了这一设定下的相应渐近下界。此外，我们通过模拟农业用例的现实环境以及各种合成例子，实证展示了Thompson采样方法的优势。",
        "领域": "强化学习、风险管理、农业应用",
        "问题": "如何在多臂老虎机问题中，通过条件风险价值（CVaR）来衡量每个臂的质量，并找到最优的Thompson采样策略。",
        "动机": "现有研究主要集中在上置信界算法上，缺乏针对CVaR老虎机的Thompson采样方法，特别是在有界奖励和多项分布的情况下。",
        "方法": "引入B-CVTS和M-CVTS两种Thompson采样方法，分别针对连续有界奖励和多项分布，扩展了现有理论分析以限制CVaR遗憾最小化性能。",
        "关键词": [
            "条件风险价值",
            "Thompson采样",
            "多臂老虎机",
            "渐近最优性",
            "风险管理"
        ],
        "涉及的技术概念": {
            "条件风险价值（CVaR）": "用于衡量在某个置信水平下，超出价值风险的预期损失，本文中用于评估老虎机臂的质量。",
            "Thompson采样": "一种基于贝叶斯思想的策略，用于在多臂老虎机问题中进行探索与利用的平衡。",
            "渐近最优性": "指随着试验次数的增加，策略的性能趋近于理论上的最优性能，本文中证明了所提策略在CVaR老虎机中的渐近最优性。"
        },
        "success": true
    },
    {
        "order": 802,
        "title": "Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search",
        "html": "https://ICML.cc//virtual/2021/poster/9843",
        "abstract": "Neural architecture search (NAS) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport (OT) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the OT is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein (TW), which is a negative definite variant of OT, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential NAS settings. Furthermore, we derive a novel parallel NAS, using quality k-determinantal point process on the GP posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our TW-based approaches outperform other baselines in both sequential and parallel NAS.",
        "conference": "ICML",
        "中文标题": "序列与并行神经架构搜索的最优传输核",
        "摘要翻译": "神经架构搜索（NAS）自动化了深度神经网络的设计。在搜索复杂且非连续架构时，主要挑战之一是比较网络的相似性，传统的欧几里得度量可能无法捕捉到这一点。最优传输（OT）通过考虑将一个网络传输到另一个网络的最小成本，对这种复杂结构具有弹性。然而，OT通常不是负定的，这可能会限制其在许多依赖核的框架中构建正定核的能力。基于树-瓦瑟斯坦（TW），这是OT的一个负定变体，我们开发了一种新的神经架构差异，并在序列NAS设置的高斯过程代理模型中展示了它。此外，我们利用GP后验上的质量k-行列式点过程，推导出一种新颖的并行NAS，从一组离散的候选中选择多样化和高性能的架构。经验上，我们证明了我们基于TW的方法在序列和并行NAS中都优于其他基线。",
        "领域": "神经架构搜索、最优传输理论、高斯过程",
        "问题": "如何在神经架构搜索中有效比较和选择复杂且非连续的神经网络架构。",
        "动机": "传统的欧几里得度量在比较复杂和非连续的神经网络架构时可能失效，需要一种更有效的相似性度量方法。",
        "方法": "基于树-瓦瑟斯坦（TW）的最优传输变体开发新的神经架构差异度量，并在高斯过程代理模型中应用；利用质量k-行列式点过程进行并行NAS。",
        "关键词": [
            "神经架构搜索",
            "最优传输",
            "树-瓦瑟斯坦",
            "高斯过程",
            "k-行列式点过程"
        ],
        "涉及的技术概念": {
            "最优传输（OT）": "用于比较神经网络架构相似性的方法，通过计算将一个网络转换为另一个网络的最小成本。",
            "树-瓦瑟斯坦（TW）": "最优传输的一个负定变体，用于构建正定核，以便在依赖核的框架中使用。",
            "高斯过程代理模型": "在序列神经架构搜索中用于预测和优化架构性能的模型。"
        },
        "success": true
    },
    {
        "order": 803,
        "title": "Optimization of Graph Neural Networks: Implicit Acceleration by Skip Connections and More Depth",
        "html": "https://ICML.cc//virtual/2021/poster/8619",
        "abstract": "Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization. However, their optimization properties are less well understood. We take the first step towards analyzing GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real-world graphs. Second, we study what may affect the GNNs' training speed. Our results show that the training  of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice.",
        "conference": "ICML",
        "中文标题": "图神经网络的优化：通过跳跃连接和增加深度实现隐式加速",
        "摘要翻译": "图神经网络（GNNs）的表达能力和泛化能力已被广泛研究。然而，它们的优化特性却较少被理解。我们通过研究GNNs的梯度动态，首次迈出了分析GNN训练的第一步。首先，我们分析了线性化的GNNs，并证明尽管训练是非凸的，但在我们验证过的现实世界图中的温和假设下，保证以线性速率收敛到全局最小值。其次，我们研究了可能影响GNNs训练速度的因素。我们的结果表明，GNNs的训练通过跳跃连接、增加深度和/或良好的标签分布隐式加速。实证结果证实，我们对线性化GNNs的理论结果与非线GNNs的训练行为一致。我们的结果为跳跃连接GNNs在优化方面的成功提供了首次理论支持，并表明具有跳跃连接的深度GNNs在实践中将大有可为。",
        "领域": "图神经网络优化、深度学习理论、图表示学习",
        "问题": "理解图神经网络的优化特性及其训练动态",
        "动机": "探索图神经网络在优化过程中的行为，特别是跳跃连接和网络深度对训练速度的影响",
        "方法": "通过分析线性化GNNs的梯度动态，研究训练收敛性和速度，并在真实世界图上验证假设",
        "关键词": [
            "图神经网络",
            "优化理论",
            "跳跃连接",
            "梯度动态",
            "深度网络"
        ],
        "涉及的技术概念": {
            "梯度动态": "研究GNN训练过程中梯度的变化行为，以理解优化过程",
            "跳跃连接": "在GNN中引入的跨层连接，用于加速训练和改善梯度流动",
            "线性化GNNs": "简化模型以便于理论分析，同时保持对非线性GNN训练行为的预测能力"
        },
        "success": true
    },
    {
        "order": 804,
        "title": "Optimization Planning for 3D ConvNets",
        "html": "https://ICML.cc//virtual/2021/poster/8521",
        "abstract": "It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D ConvNets) due to high complexity and various options of the training scheme. The most common hand-tuning process starts from learning 3D ConvNets using short video clips and then is followed by learning long-term temporal dependency using lengthy clips, while gradually decaying the learning rate from high to low as training progresses. The fact that such process comes along with several heuristic settings motivates the study to seek an optimal 'path' to automate the entire training. In this paper, we decompose the path into a series of training 'states' and specify the hyper-parameters, e.g., learning rate and the length of input clips, in each state. The estimation of the knee point on the performance-epoch curve triggers the transition from one state to another. We perform dynamic programming over all the candidate states to plan the optimal permutation of states, i.e., optimization path. Furthermore, we devise a new 3D ConvNets with a unique design of dual-head classifier to improve spatial and temporal discrimination. Extensive experiments on seven public video recognition benchmarks demonstrate the advantages of our proposal. With the optimization planning, our 3D ConvNets achieves superior results when comparing to the state-of-the-art recognition methods. More remarkably, we obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600 datasets, respectively.",
        "conference": "ICML",
        "中文标题": "3D卷积神经网络的优化规划",
        "摘要翻译": "由于训练方案的高复杂性和多样性，最优学习3D卷积神经网络（3D ConvNets）并非易事。最常见的手动调整过程始于使用短视频片段学习3D ConvNets，随后通过使用长片段学习长期时间依赖性，同时随着训练的进行逐渐从高到低衰减学习率。这一过程伴随着几种启发式设置的事实，激励了研究寻求自动化整个训练的最优‘路径’。在本文中，我们将路径分解为一系列训练‘状态’，并在每个状态中指定超参数，例如学习率和输入片段的长度。性能-epoch曲线上的拐点估计触发从一个状态到另一个状态的转换。我们对所有候选状态执行动态规划，以规划状态的最优排列，即优化路径。此外，我们设计了一种新的3D ConvNets，具有独特的双头分类器设计，以提高空间和时间辨别力。在七个公共视频识别基准上的大量实验证明了我们提案的优势。通过优化规划，我们的3D ConvNets在比较最先进的识别方法时取得了优异的结果。更值得注意的是，我们在Kinetics-400和Kinetics-600数据集上分别获得了80.5%和82.7%的top-1准确率。",
        "领域": "视频识别、深度学习优化、3D卷积神经网络",
        "问题": "如何自动化优化3D卷积神经网络的训练过程",
        "动机": "手动调整3D卷积神经网络的训练过程复杂且依赖启发式设置，需要寻找一种自动化方法来优化训练路径",
        "方法": "将训练过程分解为多个状态，通过动态规划寻找最优状态排列，并设计双头分类器以提高性能",
        "关键词": [
            "3D卷积神经网络",
            "优化规划",
            "动态编程",
            "双头分类器",
            "视频识别"
        ],
        "涉及的技术概念": {
            "3D卷积神经网络": "用于处理视频数据，捕捉时空特征的深度学习模型",
            "动态规划": "用于规划训练状态的最优排列，以实现训练过程的自动化优化",
            "双头分类器": "新设计的分类器结构，旨在提高模型对空间和时间特征的辨别能力"
        },
        "success": true
    },
    {
        "order": 805,
        "title": "Optimizing Black-box Metrics with Iterative Example Weighting",
        "html": "https://ICML.cc//virtual/2021/poster/9325",
        "abstract": "We consider learning to optimize a classification metric defined by a black-box function of the confusion matrix. Such black-box learning settings are ubiquitous, for example, when the learner only has query access to the metric of interest, or in noisy-label and domain adaptation applications where the learner must evaluate the metric via performance evaluation using a small validation sample. Our approach is to adaptively learn example weights on the training dataset such that the resulting weighted objective best approximates the metric on the validation sample. We show how to model and estimate the example weights and use them to iteratively post-shift a pre-trained class probability estimator to construct a classifier. We also analyze the resulting procedure's statistical properties. Experiments on various label noise, domain shift, and fair classification setups confirm that our proposal compares favorably to the state-of-the-art baselines for each application.",
        "conference": "ICML",
        "中文标题": "通过迭代样本加权优化黑盒指标",
        "摘要翻译": "我们考虑学习优化一个由混淆矩阵的黑盒函数定义的分类指标。这种黑盒学习设置无处不在，例如，当学习者只能通过查询访问感兴趣的指标时，或者在噪声标签和领域适应应用中，学习者必须通过使用小验证样本的性能评估来评估指标。我们的方法是自适应地学习训练数据集上的样本权重，使得由此产生的加权目标最佳地近似于验证样本上的指标。我们展示了如何建模和估计样本权重，并使用它们迭代地后移预训练的分类概率估计器以构建分类器。我们还分析了所得到程序的统计特性。在各种标签噪声、领域转移和公平分类设置上的实验证实，我们的提议在每个应用中都优于最先进的基线。",
        "领域": "机器学习优化, 噪声标签学习, 领域适应",
        "问题": "优化由黑盒函数定义的分类指标",
        "动机": "在只能通过查询访问指标或必须通过小验证样本评估指标的场景下，提高分类性能",
        "方法": "自适应学习训练数据集上的样本权重，迭代调整预训练的分类概率估计器",
        "关键词": [
            "黑盒优化",
            "样本加权",
            "分类指标",
            "噪声标签",
            "领域适应"
        ],
        "涉及的技术概念": {
            "黑盒函数": "用于定义分类指标的不可见内部工作机制的函数",
            "样本权重": "调整训练样本对模型训练影响的权重，以优化特定指标",
            "迭代后移": "通过迭代调整预训练模型的输出，以改善模型在特定指标上的表现"
        },
        "success": true
    },
    {
        "order": 806,
        "title": "Optimizing persistent homology based functions",
        "html": "https://ICML.cc//virtual/2021/poster/10265",
        "abstract": "Solving optimization tasks based on functions and losses with a topological flavor is a very active and growing field of research in data science and Topological Data Analysis, with applications in non-convex optimization, statistics and machine learning. However, the approaches  proposed in the literature are usually anchored to a specific application and/or topological construction, and do not come with theoretical guarantees. To address this issue, we study the differentiability of a general map associated with the most common topological construction, that is, the persistence map. Building on real analytic geometry arguments, we propose a general framework that allows us to define and compute gradients for persistence-based functions in a very simple way. We also provide a simple, explicit and sufficient condition for convergence of stochastic subgradient methods for such functions. This result encompasses all the constructions and applications of topological optimization in the literature. Finally, we provide associated code, that is easy to handle and to mix with other non-topological methods and constraints, as well as some experiments showcasing the versatility of our approach.",
        "conference": "ICML",
        "中文标题": "优化基于持久同调的函数",
        "摘要翻译": "解决基于具有拓扑特性的函数和损失的优化任务是数据科学和拓扑数据分析中一个非常活跃且不断增长的研究领域，应用于非凸优化、统计学和机器学习。然而，文献中提出的方法通常局限于特定的应用和/或拓扑构造，并且缺乏理论保证。为了解决这个问题，我们研究了与最常见的拓扑构造——持久性映射相关的一般映射的可微性。基于实解析几何的论点，我们提出了一个通用框架，使我们能够以一种非常简单的方式定义和计算基于持久性的函数的梯度。我们还为这类函数的随机次梯度方法的收敛提供了一个简单、明确且充分的条件。这一结果涵盖了文献中所有拓扑优化的构造和应用。最后，我们提供了易于处理且易于与其他非拓扑方法和约束混合的相关代码，以及一些展示我们方法多样性的实验。",
        "领域": "拓扑数据分析, 非凸优化, 机器学习",
        "问题": "解决基于拓扑特性的函数和损失的优化任务缺乏通用框架和理论保证的问题",
        "动机": "为了提供一个通用框架，使得基于持久性的函数的梯度可以简单定义和计算，并确保随机次梯度方法的收敛性",
        "方法": "基于实解析几何的论点，提出一个通用框架，定义和计算基于持久性的函数的梯度，并提供收敛条件",
        "关键词": [
            "持久同调",
            "拓扑优化",
            "随机次梯度方法",
            "实解析几何",
            "非凸优化"
        ],
        "涉及的技术概念": {
            "持久同调": "用于捕捉数据在不同尺度下的拓扑特征，是拓扑数据分析中的核心概念",
            "随机次梯度方法": "用于优化非光滑函数的算法，本文中用于优化基于持久性的函数",
            "实解析几何": "提供了理论支持，使得基于持久性的函数的梯度可以正确定义和计算"
        },
        "success": true
    },
    {
        "order": 807,
        "title": "Order-Agnostic Cross Entropy for Non-Autoregressive Machine Translation",
        "html": "https://ICML.cc//virtual/2021/poster/8931",
        "abstract": "We propose a new training objective named order-agnostic cross entropy (OaXE) for fully non-autoregressive translation (NAT) models. OaXE improves the standard cross-entropy loss to ameliorate the effect of word reordering, which is a common source of the critical multimodality problem in NAT.\nConcretely, OaXE removes the penalty for word order errors, and computes the cross entropy loss based on the best possible alignment between model predictions and target tokens.\nSince the log loss is very sensitive to invalid references, we leverage cross entropy initialization and loss truncation to ensure the model focuses on a good part of the search space.\nExtensive experiments on major WMT benchmarks demonstrate that OaXE substantially improves translation performance, setting new state of the art for fully NAT models. Further analyses show that OaXE indeed alleviates the multimodality problem by reducing token repetitions and increasing prediction confidence.\nOur code, data, and trained models are available at https://github.com/tencent-ailab/ICML21_OAXE.",
        "conference": "ICML",
        "中文标题": "无序感知交叉熵用于非自回归机器翻译",
        "摘要翻译": "我们提出了一种名为无序感知交叉熵（OaXE）的新训练目标，用于完全非自回归翻译（NAT）模型。OaXE改进了标准的交叉熵损失，以减轻单词重新排序的影响，这是NAT中关键多模态问题的常见来源。具体来说，OaXE消除了对词序错误的惩罚，并根据模型预测与目标标记之间的最佳可能对齐计算交叉熵损失。由于对数损失对无效参考非常敏感，我们利用交叉熵初始化和损失截断来确保模型专注于搜索空间的一个良好部分。在主要WMT基准上的大量实验表明，OaXE显著提高了翻译性能，为完全NAT模型设定了新的技术状态。进一步的分析表明，OaXE确实通过减少标记重复和增加预测置信度来缓解多模态问题。我们的代码、数据和训练模型可在https://github.com/tencent-ailab/ICML21_OAXE获取。",
        "领域": "机器翻译、非自回归模型、自然语言处理",
        "问题": "解决非自回归机器翻译模型中的多模态问题，特别是由单词重新排序引起的翻译质量下降。",
        "动机": "改进非自回归翻译模型的训练目标，以减轻单词重新排序对翻译质量的影响，从而提升翻译性能。",
        "方法": "提出无序感知交叉熵（OaXE）作为新的训练目标，通过消除词序错误的惩罚和基于最佳可能对齐计算损失，结合交叉熵初始化和损失截断技术，优化模型训练。",
        "关键词": [
            "非自回归机器翻译",
            "无序感知交叉熵",
            "多模态问题",
            "WMT基准",
            "翻译性能"
        ],
        "涉及的技术概念": {
            "无序感知交叉熵（OaXE）": "改进的交叉熵损失函数，用于非自回归翻译模型，通过消除词序错误的惩罚来优化模型训练。",
            "非自回归翻译（NAT）模型": "一种机器翻译模型，能够并行生成所有目标词，而不是按顺序生成，以提高翻译速度。",
            "多模态问题": "在非自回归翻译中，由于目标序列的多种有效排列导致的模型训练困难，影响翻译质量。"
        },
        "success": true
    },
    {
        "order": 808,
        "title": "Order Matters: Probabilistic Modeling of Node Sequence for Graph Generation",
        "html": "https://ICML.cc//virtual/2021/poster/8963",
        "abstract": "A graph generative model defines a distribution over graphs. Typically, the model consists of a sequential process that creates and adds nodes and edges. Such sequential process defines an ordering of the nodes in the graph. The computation of the model's likelihood requires to marginalize the node orderings; this makes maximum likelihood estimation (MLE) challenging due to the (factorial) number of possible permutations. In this work, we provide an expression for the likelihood of a graph generative model and show that its calculation is closely related to the problem of graph automorphism. In addition, we derive a variational inference (VI) algorithm for fitting a graph generative model that is based on the maximization of a variational bound of the log-likelihood. This allows the model to be trained with node orderings from the approximate posterior instead of ad-hoc orderings. Our experiments show that our log-likelihood bound is significantly tighter than the bound of previous schemes. The models fitted with the VI algorithm are able to generate high-quality graphs that match the structures of target graphs not seen during training.",
        "conference": "ICML",
        "中文标题": "顺序至关重要：图生成的节点序列概率建模",
        "摘要翻译": "图生成模型定义了图上的分布。通常，该模型由一个顺序过程组成，该过程创建并添加节点和边。这样的顺序过程定义了图中节点的顺序。模型似然的计算需要对节点顺序进行边缘化；这使得由于可能的排列数量（阶乘级）而导致的最大似然估计（MLE）具有挑战性。在这项工作中，我们提供了图生成模型似然的表达式，并表明其计算与图自同构问题密切相关。此外，我们推导了一个变分推断（VI）算法，用于拟合图生成模型，该算法基于对数似然的变分界的最大化。这使得模型可以用来自近似后验的节点顺序进行训练，而不是使用临时顺序。我们的实验表明，我们的对数似然界比以前方案的界要紧密得多。使用VI算法拟合的模型能够生成高质量图，这些图匹配训练期间未见的目标图的结构。",
        "领域": "图神经网络、图生成模型、变分推断",
        "问题": "解决图生成模型中由于节点顺序排列数量巨大导致的最大似然估计计算困难的问题",
        "动机": "为了更有效地训练图生成模型，并生成与目标图结构匹配的高质量图",
        "方法": "提出了一种基于变分推断的算法，通过最大化对数似然的变分界来拟合图生成模型",
        "关键词": [
            "图生成模型",
            "变分推断",
            "节点顺序",
            "最大似然估计",
            "图自同构"
        ],
        "涉及的技术概念": {
            "图生成模型": "定义了图上的分布，通过顺序过程创建和添加节点和边",
            "变分推断": "用于拟合图生成模型的算法，基于对数似然的变分界的最大化",
            "图自同构": "与图生成模型似然计算密切相关的问题，影响模型的训练和生成图的质量"
        },
        "success": true
    },
    {
        "order": 809,
        "title": "Outlier-Robust Optimal Transport",
        "html": "https://ICML.cc//virtual/2021/poster/9841",
        "abstract": "Optimal transport (OT) measures distances between distributions in a way that depends on the geometry of the sample space. In light of recent advances in computational OT, OT distances are widely used as loss functions in machine learning. Despite their prevalence and advantages, OT loss functions can be extremely sensitive to outliers. In fact, a single adversarially-picked outlier can increase the standard $W_2$-distance arbitrarily. To address this issue, we propose an outlier-robust formulation of OT. Our formulation is convex but challenging to scale at a first glance. Our main contribution is deriving an \\emph{equivalent} formulation based on cost truncation that is easy to incorporate into modern algorithms for computational OT. We demonstrate the benefits of our formulation in mean estimation problems under the Huber contamination model in simulations and outlier detection tasks on real data.",
        "conference": "ICML",
        "中文标题": "异常鲁棒的最优传输",
        "摘要翻译": "最优传输（OT）以一种依赖于样本空间几何的方式测量分布之间的距离。鉴于计算OT的最新进展，OT距离被广泛用作机器学习中的损失函数。尽管OT损失函数普遍且具有优势，但它们对异常值极为敏感。事实上，一个由对抗性选择的异常值可以任意增加标准的$W_2$-距离。为了解决这个问题，我们提出了一种异常鲁棒的OT公式。我们的公式是凸的，但乍一看难以扩展。我们的主要贡献是推导出一个基于成本截断的等效公式，该公式易于融入现代计算OT的算法中。我们在模拟中的Huber污染模型下的均值估计问题和真实数据的异常检测任务中展示了我们公式的优势。",
        "领域": "最优传输",
        "问题": "最优传输损失函数对异常值敏感的问题",
        "动机": "解决最优传输损失函数在存在异常值时性能下降的问题",
        "方法": "提出了一种基于成本截断的异常鲁棒最优传输公式",
        "关键词": [
            "最优传输",
            "异常鲁棒",
            "成本截断"
        ],
        "涉及的技术概念": {
            "最优传输": "用于测量分布之间的距离，依赖于样本空间的几何",
            "异常鲁棒": "提出的方法旨在减少异常值对最优传输距离的影响",
            "成本截断": "用于推导等效的最优传输公式，便于融入现代算法"
        },
        "success": true
    },
    {
        "order": 810,
        "title": "Out-of-Distribution Generalization via Risk Extrapolation (REx)",
        "html": "https://ICML.cc//virtual/2021/poster/9185",
        "abstract": "Distributional shift is one of the major obstacles when transferring machine learning prediction systems from the lab to the real world. To tackle this problem, we assume that variation across training domains is representative of the variation we might encounter at test time, but also that shifts at test time may be more extreme in magnitude. In particular, we show that reducing differences in risk across training domains can reduce a model’s sensitivity to a wide range of extreme distributional shifts, including the challenging setting where the input contains both causal and anti-causal elements. We motivate this approach, Risk Extrapolation (REx), as a form of robust optimization over a perturbation set of extrapolated domains (MM-REx), and propose a penalty on the variance of training risks (V-REx) as a simpler variant. We prove that variants of REx can recover the causal mechanisms of the targets, while also providing robustness to changes in the input distribution (``covariate shift''). By appropriately trading-off robustness to causally induced distributional shifts and covariate shift, REx is able to outperform alternative methods such as Invariant Risk Minimization in situations where these types of shift co-occur.",
        "conference": "ICML",
        "中文标题": "通过风险外推（REx）实现分布外泛化",
        "摘要翻译": "分布偏移是将机器学习预测系统从实验室转移到现实世界时面临的主要障碍之一。为了解决这个问题，我们假设训练领域之间的变化代表了我们在测试时可能遇到的变化，但测试时的偏移可能在幅度上更为极端。特别是，我们展示了减少训练领域间风险差异可以降低模型对包括输入同时包含因果和非因果元素的挑战性设置在内的广泛极端分布偏移的敏感性。我们将这种方法——风险外推（REx）——作为一种在推演领域扰动集（MM-REx）上的鲁棒优化形式提出动机，并提出了一种对训练风险方差（V-REx）的惩罚作为更简单的变体。我们证明了REx的变体可以恢复目标的因果机制，同时提供对输入分布变化（'协变量偏移'）的鲁棒性。通过适当权衡对因果诱导的分布偏移和协变量偏移的鲁棒性，REx能够在这些类型的偏移同时发生的情况下，优于如不变风险最小化等其他方法。",
        "领域": "机器学习泛化、鲁棒机器学习、因果推理",
        "问题": "解决机器学习模型在面对分布偏移时的泛化能力问题",
        "动机": "提高机器学习模型在现实世界中面对极端和未知分布变化时的鲁棒性和泛化能力",
        "方法": "提出风险外推（REx）方法，包括在推演领域扰动集上的鲁棒优化（MM-REx）和对训练风险方差的惩罚（V-REx）",
        "关键词": [
            "分布外泛化",
            "风险外推",
            "鲁棒优化",
            "因果推理",
            "协变量偏移"
        ],
        "涉及的技术概念": {
            "风险外推（REx）": "通过减少训练领域间风险差异来提高模型对极端分布偏移的鲁棒性",
            "推演领域扰动集（MM-REx）": "作为鲁棒优化的扰动集，用于模拟和应对测试时可能遇到的极端分布变化",
            "训练风险方差（V-REx）": "作为风险外推的简化变体，通过惩罚训练风险的方差来增强模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 811,
        "title": "Outside the Echo Chamber: Optimizing the Performative Risk",
        "html": "https://ICML.cc//virtual/2021/poster/9029",
        "abstract": "In performative prediction, predictions guide decision-making and hence can influence the distribution of future data. To date, work on performative prediction has focused on finding performatively stable models, which are the fixed points of repeated retraining. However, stable solutions can be far from optimal when evaluated in terms of the performative risk, the loss experienced by the decision maker when deploying a model. In this paper, we shift attention beyond performative stability and focus on optimizing the performative risk directly. We identify a natural set of properties of the loss function and model-induced distribution shift under which the performative risk is convex, a property which does not follow from convexity of the loss alone. Furthermore, we develop algorithms that leverage our structural assumptions to optimize the performative risk with better sample efficiency than generic methods for derivative-free convex optimization.",
        "conference": "ICML",
        "中文标题": "超越回声室效应：优化表现性风险",
        "摘要翻译": "在表现性预测中，预测指导决策制定，从而可以影响未来数据的分布。迄今为止，关于表现性预测的工作主要集中在寻找表现性稳定的模型，这些模型是重复训练的固定点。然而，当从表现性风险（即决策者在部署模型时所经历的损失）的角度进行评估时，稳定的解决方案可能远非最优。在本文中，我们将注意力从表现性稳定性转移，直接聚焦于优化表现性风险。我们确定了损失函数和模型引起的分布偏移的一组自然属性，在这些属性下，表现性风险是凸的，这一性质并不单独由损失的凸性决定。此外，我们开发了算法，这些算法利用我们的结构假设来优化表现性风险，其样本效率优于无导数凸优化的通用方法。",
        "领域": "机器学习优化、决策支持系统、预测模型",
        "问题": "如何在表现性预测中直接优化表现性风险，而不仅仅是寻找表现性稳定的模型。",
        "动机": "现有的表现性预测研究主要关注于寻找表现性稳定的模型，但这些模型在表现性风险方面可能不是最优的，因此需要直接优化表现性风险。",
        "方法": "通过识别损失函数和模型引起的分布偏移的特定属性，开发算法直接优化表现性风险，提高样本效率。",
        "关键词": [
            "表现性预测",
            "表现性风险",
            "凸优化",
            "样本效率",
            "决策支持"
        ],
        "涉及的技术概念": {
            "表现性预测": "预测指导决策制定，从而影响未来数据的分布。",
            "表现性风险": "决策者在部署模型时所经历的损失，是评估模型性能的关键指标。",
            "凸优化": "在特定条件下，表现性风险具有凸性，使得可以直接优化这一风险。"
        },
        "success": true
    },
    {
        "order": 812,
        "title": "Overcoming Catastrophic Forgetting by Bayesian Generative Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/9223",
        "abstract": "In this paper, we propose a new method to over-come catastrophic forgetting by adding generative regularization to Bayesian inference frame-work. Bayesian method provides a general frame-work for continual learning.   We could further construct a generative regularization term for all given classification models by leveraging energy-based models and Langevin dynamic sampling to enrich the features learned in each task.  By combining discriminative and generative loss together, we empirically show that the proposed method outperforms state-of-the-art methods on a variety of tasks, avoiding catastrophic forgetting in continual learning. In particular, the proposed method outperforms baseline methods over 15%on the Fashion-MNIST dataset and 10%on the CUB dataset.",
        "conference": "ICML",
        "中文标题": "通过贝叶斯生成正则化克服灾难性遗忘",
        "摘要翻译": "本文提出了一种新方法，通过在贝叶斯推理框架中添加生成正则化来克服灾难性遗忘。贝叶斯方法为持续学习提供了一个通用框架。我们可以进一步利用基于能量的模型和Langevin动态采样，为所有给定的分类模型构建一个生成正则化项，以丰富每个任务中学习到的特征。通过将判别性和生成性损失结合起来，我们实证表明，所提出的方法在各种任务上优于最先进的方法，避免了持续学习中的灾难性遗忘。特别是，所提出的方法在Fashion-MNIST数据集上比基线方法高出15%，在CUB数据集上高出10%。",
        "领域": "持续学习, 贝叶斯深度学习, 生成模型",
        "问题": "解决持续学习中的灾难性遗忘问题",
        "动机": "为了在持续学习过程中避免模型忘记之前学到的知识，提高模型的学习效率和性能",
        "方法": "在贝叶斯推理框架中引入生成正则化项，结合基于能量的模型和Langevin动态采样，以及判别性和生成性损失",
        "关键词": [
            "灾难性遗忘",
            "贝叶斯推理",
            "生成正则化",
            "持续学习",
            "Langevin动态采样"
        ],
        "涉及的技术概念": {
            "贝叶斯推理": "为持续学习提供了一个概率框架，允许模型在学习新任务时保留旧任务的知识",
            "生成正则化": "通过基于能量的模型和Langevin动态采样构建的正则化项，用于丰富模型在每个任务中学习到的特征",
            "Langevin动态采样": "一种用于从概率分布中采样的技术，用于生成正则化项中，以帮助模型更好地学习任务特征"
        },
        "success": true
    },
    {
        "order": 813,
        "title": "PAC-Learning for Strategic Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9137",
        "abstract": "The study of strategic or adversarial manipulation of testing data to fool a classifier has attracted much recent attention. Most previous works have focused on two extreme situations where any testing data point either is completely adversarial or always equally prefers the positive label. In this paper, we generalize both of these through a unified framework for strategic classification and introduce the notion of strategic VC-dimension (SVC) to capture the PAC-learnability in our general strategic setup. SVC provably generalizes the recent concept of adversarial VC-dimension (AVC) introduced by Cullina et al. (2018). We instantiate our framework for the fundamental strategic linear classification problem. We fully characterize: (1) the statistical learnability of linear classifiers by pinning down its SVC; (2) it's computational tractability by pinning down the complexity of the empirical risk minimization problem. Interestingly, the SVC  of linear classifiers is always upper bounded by its standard VC-dimension. This characterization also strictly generalizes the AVC bound for linear classifiers in (Cullina et al., 2018).",
        "conference": "ICML",
        "中文标题": "战略分类的PAC学习",
        "摘要翻译": "研究测试数据的战略性或对抗性操纵以欺骗分类器的问题近来引起了广泛关注。大多数先前的工作集中在两种极端情况：任何测试数据点要么是完全对抗性的，要么总是同样偏好正标签。在本文中，我们通过一个统一的战略分类框架概括了这两种情况，并引入了战略VC维（SVC）的概念，以捕捉我们一般战略设置中的PAC可学习性。SVC被证明可以推广Cullina等人（2018）最近提出的对抗性VC维（AVC）概念。我们为基本的战略线性分类问题实例化了我们的框架。我们完全描述了：（1）通过确定其SVC来线性分类器的统计可学习性；（2）通过确定经验风险最小化问题的复杂性来计算其可处理性。有趣的是，线性分类器的SVC总是被其标准VC维上界所限制。这一描述也严格推广了（Cullina等人，2018）中线性分类器的AVC界限。",
        "领域": "对抗性机器学习、统计学习理论、线性分类",
        "问题": "研究在测试数据可能被战略性或对抗性操纵的情况下，分类器的PAC可学习性问题。",
        "动机": "为了理解和解决在现实世界中，数据可能被对手操纵以欺骗机器学习模型的问题，从而提供更鲁棒的分类方法。",
        "方法": "引入战略VC维（SVC）的概念，构建统一的战略分类框架，并应用于线性分类问题，分析其统计可学习性和计算可处理性。",
        "关键词": [
            "战略分类",
            "PAC学习",
            "VC维",
            "对抗性机器学习",
            "线性分类"
        ],
        "涉及的技术概念": {
            "战略VC维（SVC）": "用于捕捉在战略性或对抗性数据操纵下的PAC可学习性，推广了对抗性VC维（AVC）的概念。",
            "PAC可学习性": "指在概率近似正确（PAC）框架下，学习算法能够从有限样本中学习到近似正确的假设的能力。",
            "经验风险最小化": "一种通过最小化训练数据上的损失函数来训练模型的方法，本文研究了在战略分类框架下该问题的复杂性。"
        },
        "success": true
    },
    {
        "order": 814,
        "title": "PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees",
        "html": "https://ICML.cc//virtual/2021/poster/10421",
        "abstract": "Meta-learning can successfully acquire useful inductive biases from data. Yet, its generalization properties to unseen learning tasks are poorly understood. Particularly if the number of meta-training tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our method results in a standard stochastic optimization problem which can be solved efficiently and scales well.When  instantiating our PAC-optimal hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as base learners, the resulting methods yield state-of-the-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates. Thanks to their principled treatment of uncertainty, our meta-learners can also be successfully employed for sequential decision problems.",
        "conference": "ICML",
        "中文标题": "PACOH：具有PAC保证的贝叶斯最优元学习",
        "摘要翻译": "元学习能够成功地从数据中获取有用的归纳偏置。然而，其对未见学习任务的泛化特性知之甚少。特别是如果元训练任务的数量较少，这会引发关于过拟合的担忧。我们使用PAC-贝叶斯框架提供了一个理论分析，并为元学习推导了新的泛化界限。利用这些界限，我们开发了一类具有性能保证和原则性元级正则化的PAC最优元学习算法。与之前的PAC-贝叶斯元学习器不同，我们的方法导致了一个标准的随机优化问题，可以高效解决并且具有良好的扩展性。当用高斯过程和贝叶斯神经网络作为基础学习器实例化我们的PAC最优超后验（PACOH）时，所得到的方法在预测准确性和不确定性估计质量方面都达到了最先进的性能。得益于其对不确定性的原则性处理，我们的元学习器也可以成功地用于序列决策问题。",
        "领域": "元学习、贝叶斯深度学习、不确定性估计",
        "问题": "元学习在少量元训练任务情况下的泛化能力不足和过拟合问题",
        "动机": "提高元学习算法在少量任务情况下的泛化能力，减少过拟合，同时提供理论保证",
        "方法": "利用PAC-贝叶斯框架推导泛化界限，开发具有性能保证和元级正则化的PAC最优元学习算法",
        "关键词": [
            "PAC-贝叶斯",
            "元学习",
            "高斯过程",
            "贝叶斯神经网络",
            "不确定性估计"
        ],
        "涉及的技术概念": {
            "PAC-贝叶斯框架": "用于分析学习算法的泛化性能，提供理论保证的数学框架",
            "元级正则化": "在元学习过程中引入的正则化技术，旨在提高模型的泛化能力",
            "高斯过程": "作为一种非参数贝叶斯模型，用于实例化PACOH，提供灵活的函数逼近和不确定性估计能力"
        },
        "success": true
    },
    {
        "order": 815,
        "title": "PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/8449",
        "abstract": "In this paper, we propose a novel stochastic gradient estimator---ProbAbilistic Gradient Estimator (PAGE)---for nonconvex optimization. PAGE is easy to implement as it is designed via a small adjustment to vanilla SGD: in each iteration, PAGE uses the vanilla minibatch SGD update with probability $p_t$ or reuses the previous gradient with a small adjustment, at a much lower computational cost, with probability $1-p_t$. We give a simple formula for the optimal choice of $p_t$. \nMoreover, we prove the first tight lower bound $\\Omega(n+\\frac{\\sqrt{n}}{\\epsilon^2})$ for nonconvex finite-sum problems, which also leads to a tight lower bound $\\Omega(b+\\frac{\\sqrt{b}}{\\epsilon^2})$ for nonconvex online problems, where $b:= \\min\\{\\frac{\\sigma^2}{\\epsilon^2}, n\\}$. \nThen, we show that PAGE obtains the optimal convergence results $O(n+\\frac{\\sqrt{n}}{\\epsilon^2})$ (finite-sum) and $O(b+\\frac{\\sqrt{b}}{\\epsilon^2})$ (online) matching our lower bounds for both nonconvex finite-sum and online problems. Besides, we also show that for nonconvex functions satisfying the Polyak-\\L ojasiewicz (PL) condition, PAGE can automatically switch to a faster linear convergence rate $O(\\cdot\\log \\frac{1}{\\epsilon})$. \nFinally, we conduct several deep learning experiments (e.g., LeNet, VGG, ResNet) on real datasets in PyTorch showing that PAGE not only converges much faster than SGD in training but also achieves the higher test accuracy, validating the optimal theoretical results and confirming the practical superiority of PAGE.",
        "conference": "ICML",
        "success": true,
        "中文标题": "PAGE：一种简单且最优的非凸优化概率梯度估计器",
        "摘要翻译": "本文提出了一种新颖的随机梯度估计器——概率梯度估计器（PAGE），用于非凸优化。PAGE易于实现，因为它通过对普通SGD进行小调整而设计：在每次迭代中，PAGE以概率$p_t$使用普通小批量SGD更新，或以概率$1-p_t$以更低的计算成本重用先前的梯度并进行小调整。我们给出了$p_t$最优选择的简单公式。此外，我们首次证明了非凸有限和问题的紧下界$\\Omega(n+\\frac{\\sqrt{n}}{\\epsilon^2})$，这也导致了非凸在线问题的紧下界$\\Omega(b+\\frac{\\sqrt{b}}{\\epsilon^2})$，其中$b:= \\min\\{\\frac{\\sigma^2}{\\epsilon^2}, n\\}$。然后，我们展示了PAGE获得了最优收敛结果$O(n+\\frac{\\sqrt{n}}{\\epsilon^2})$（有限和）和$O(b+\\frac{\\sqrt{b}}{\\epsilon^2})$（在线），与我们对非凸有限和和在线问题的下界相匹配。此外，我们还展示了对于满足Polyak-Łojasiewicz（PL）条件的非凸函数，PAGE可以自动切换到更快的线性收敛速率$O(\\cdot\\log \\frac{1}{\\epsilon})$。最后，我们在PyTorch中对真实数据集进行了几项深度学习实验（例如，LeNet、VGG、ResNet），表明PAGE不仅在训练中比SGD收敛得快得多，而且达到了更高的测试准确率，验证了最优的理论结果并确认了PAGE的实践优势。",
        "领域": "深度学习优化、非凸优化、随机梯度下降",
        "问题": "解决非凸优化问题中随机梯度估计器的效率和最优性问题",
        "动机": "提高非凸优化问题的计算效率和收敛速度，同时保持或提高模型的准确率",
        "方法": "提出概率梯度估计器（PAGE），通过动态调整梯度更新策略，结合理论分析和实验验证",
        "关键词": [
            "概率梯度估计器",
            "非凸优化",
            "随机梯度下降",
            "深度学习优化",
            "收敛速度"
        ],
        "涉及的技术概念": {
            "概率梯度估计器（PAGE）": "一种新颖的随机梯度估计器，通过动态调整梯度更新策略来提高非凸优化问题的计算效率和收敛速度",
            "非凸优化": "研究在目标函数非凸情况下的优化问题，PAGE针对此类问题提供了最优的梯度估计方法"
        }
    },
    {
        "order": 816,
        "title": "PAPRIKA: Private Online False Discovery Rate Control",
        "html": "https://ICML.cc//virtual/2021/poster/8425",
        "abstract": "In hypothesis testing, a \\emph{false discovery} occurs when a hypothesis is incorrectly rejected due to noise in the sample. When adaptively testing multiple hypotheses, the probability of a false discovery increases as more tests are performed. Thus the problem of \\emph{False Discovery Rate (FDR) control} is to find a procedure for testing multiple hypotheses that accounts for this effect in determining the set of hypotheses to reject. The goal is to minimize the number (or fraction) of false discoveries, while maintaining a high true positive rate (i.e., correct discoveries). In this work, we study False Discovery Rate (FDR) control in multiple hypothesis testing under the constraint of differential privacy for the sample. Unlike previous work in this direction, we focus on the \\emph{online setting}, meaning that a decision about each hypothesis must be made immediately after the test is performed, rather than waiting for the output of all tests as in the offline setting. We provide new private algorithms based on state-of-the-art results in non-private online FDR control. Our algorithms have strong provable guarantees for privacy and statistical performance as measured by FDR and power. We also provide experimental results to demonstrate the efficacy of our algorithms in a variety of data environments.",
        "conference": "ICML",
        "中文标题": "PAPRIKA：隐私在线错误发现率控制",
        "摘要翻译": "在假设检验中，当由于样本中的噪声而错误地拒绝假设时，就会发生错误发现。在自适应测试多个假设时，随着执行的测试增多，错误发现的概率也会增加。因此，错误发现率（FDR）控制的问题在于找到一个测试多个假设的程序，该程序在确定要拒绝的假设集时考虑到了这种效应。目标是在保持高真阳性率（即正确发现）的同时，最小化错误发现的数量（或比例）。在这项工作中，我们研究了在样本差分隐私约束下的多重假设检验中的错误发现率（FDR）控制。与这一方向上的先前工作不同，我们专注于在线设置，这意味着必须在执行测试后立即对每个假设做出决定，而不是像离线设置那样等待所有测试的输出。我们基于非私有在线FDR控制的最新成果提供了新的私有算法。我们的算法在隐私和统计性能（以FDR和功效衡量）方面具有强大的可证明保证。我们还提供了实验结果，以证明我们的算法在各种数据环境中的有效性。",
        "领域": "统计学习、隐私保护数据分析、在线算法",
        "问题": "在保证差分隐私的前提下，如何在在线设置下控制多重假设检验中的错误发现率（FDR）。",
        "动机": "解决在在线多重假设检验中，如何在保护数据隐私的同时有效控制错误发现率的问题。",
        "方法": "基于非私有在线FDR控制的最新成果，开发新的私有算法，并在多种数据环境中验证其有效性。",
        "关键词": [
            "错误发现率控制",
            "差分隐私",
            "在线算法",
            "多重假设检验",
            "统计性能"
        ],
        "涉及的技术概念": {
            "错误发现率（FDR）": "用于衡量在多重假设检验中错误拒绝假设的比例，是评估统计方法性能的重要指标。",
            "差分隐私": "一种隐私保护技术，确保算法的输出不会泄露个体数据的信息，用于保护样本数据的隐私。",
            "在线算法": "在数据逐步到达时即时处理数据的算法，适用于需要实时决策的场景。"
        },
        "success": true
    },
    {
        "order": 817,
        "title": "Parallel and Flexible Sampling from Autoregressive Models via Langevin Dynamics",
        "html": "https://ICML.cc//virtual/2021/poster/9745",
        "abstract": "This paper introduces an alternative approach to sampling from autoregressive models. Autoregressive models are typically sampled sequentially, according to the transition dynamics defined by the model. Instead, we propose a sampling procedure that initializes a sequence with white noise and follows a Markov chain defined by Langevin dynamics on the global log-likelihood of the sequence. This approach parallelizes the sampling process and generalizes to conditional sampling. Using an autoregressive model as a Bayesian prior, we can steer the output of a generative model using a conditional likelihood or constraints. We apply these techniques to autoregressive models in the visual and audio domains, with competitive results for audio source separation, super-resolution, and inpainting.",
        "conference": "ICML",
        "中文标题": "通过朗之万动力学实现自回归模型的并行与灵活采样",
        "摘要翻译": "本文介绍了一种从自回归模型中采样的替代方法。自回归模型通常根据模型定义的转移动态顺序采样。相反，我们提出了一种采样过程，该过程用白噪声初始化序列，并遵循由序列全局对数似然的朗之万动力学定义的马尔可夫链。这种方法并行化了采样过程，并推广到条件采样。使用自回归模型作为贝叶斯先验，我们可以通过条件似然或约束来引导生成模型的输出。我们将这些技术应用于视觉和音频领域的自回归模型，在音频源分离、超分辨率和修复方面取得了竞争性的结果。",
        "领域": "音频处理, 图像修复, 超分辨率",
        "问题": "如何从自回归模型中实现并行和灵活的采样",
        "动机": "传统的自回归模型采样方法顺序执行，效率低下，且难以适应条件采样需求。",
        "方法": "提出了一种基于朗之万动力学的并行采样方法，通过白噪声初始化和全局对数似然的马尔可夫链定义，实现高效和灵活的采样。",
        "关键词": [
            "自回归模型",
            "朗之万动力学",
            "并行采样",
            "条件采样",
            "音频处理"
        ],
        "涉及的技术概念": {
            "自回归模型": "用于序列数据的生成模型，通过当前数据点预测下一个数据点。",
            "朗之万动力学": "一种基于物理的采样方法，用于从概率分布中高效采样。",
            "条件采样": "在给定某些条件下从模型中采样的技术，用于引导生成过程以满足特定需求。"
        },
        "success": true
    },
    {
        "order": 818,
        "title": "Parallel Droplet Control in MEDA Biochips using Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10575",
        "abstract": "Microfluidic biochips are being utilized for clinical diagnostics, including COVID-19 testing, because of they provide sample-to-result turnaround at low cost. Recently, microelectrode-dot-array (MEDA) biochips have been proposed to advance microfluidics technology. A MEDA biochip manipulates droplets of nano/picoliter volumes to automatically execute biochemical protocols. During bioassay execution, droplets are transported in parallel to achieve high-throughput outcomes. However, a major concern associated with the use of MEDA biochips is microelectrode degradation over time. Recent work has shown that formulating droplet transportation as a reinforcement-learning (RL) problem enables the training of policies to capture the underlying health conditions of microelectrodes and ensure reliable fluidic operations. However, the above RL-based approach suffers from two key limitations: 1) it cannot be used for concurrent transportation of multiple droplets; 2) it requires the availability of CCD cameras for monitoring droplet movement. To overcome these problems, we present a multi-agent reinforcement learning (MARL) droplet-routing solution that can be used for various sizes of MEDA biochips with integrated sensors, and we demonstrate the reliable execution of a serial-dilution bioassay with the MARL droplet router on a fabricated MEDA biochip. To facilitate further research, we also present a simulation environment based on the PettingZoo Gym Interface for MARL-guided droplet-routing problems on MEDA biochips.",
        "conference": "ICML",
        "中文标题": "使用多智能体强化学习在MEDA生物芯片中实现并行液滴控制",
        "摘要翻译": "微流控生物芯片因其能够以低成本提供从样本到结果的快速转换，正被用于包括COVID-19检测在内的临床诊断。最近，微电极点阵列（MEDA）生物芯片被提出以推进微流控技术。MEDA生物芯片操纵纳升/皮升级别的液滴来自动执行生化协议。在生物测定执行过程中，液滴被并行传输以实现高通量结果。然而，使用MEDA生物芯片的一个主要问题是微电极随时间退化。最近的研究表明，将液滴传输问题表述为强化学习（RL）问题，可以训练策略以捕捉微电极的基本健康状况并确保可靠的流体操作。然而，上述基于RL的方法存在两个关键限制：1）它不能用于多个液滴的并发传输；2）它需要CCD相机来监控液滴运动。为了克服这些问题，我们提出了一种多智能体强化学习（MARL）液滴路由解决方案，可用于各种尺寸的带有集成传感器的MEDA生物芯片，并且我们在制造的MEDA生物芯片上使用MARL液滴路由器展示了连续稀释生物测定的可靠执行。为了促进进一步的研究，我们还提出了一个基于PettingZoo Gym接口的模拟环境，用于MEDA生物芯片上的MARL引导液滴路由问题。",
        "领域": "微流控技术、强化学习应用、生物芯片设计",
        "问题": "解决MEDA生物芯片中微电极随时间退化导致的液滴传输问题，以及现有RL方法无法处理多液滴并发传输和需要外部监控设备的问题。",
        "动机": "提高MEDA生物芯片的可靠性和效率，通过多智能体强化学习实现多液滴的并行控制，减少对外部监控设备的依赖。",
        "方法": "提出了一种多智能体强化学习（MARL）液滴路由解决方案，适用于各种尺寸的MEDA生物芯片，并开发了一个模拟环境来支持进一步研究。",
        "关键词": [
            "多智能体强化学习",
            "MEDA生物芯片",
            "液滴控制",
            "并行传输",
            "微流控技术"
        ],
        "涉及的技术概念": {
            "多智能体强化学习（MARL）": "用于训练多个智能体协同工作，实现MEDA生物芯片中多液滴的并行传输和控制。",
            "微电极点阵列（MEDA）生物芯片": "一种先进的微流控技术平台，能够操纵纳升/皮升级别的液滴执行生化协议。",
            "PettingZoo Gym接口": "提供了一个模拟环境，用于研究和开发MARL引导的液滴路由算法，促进MEDA生物芯片技术的进一步发展。"
        },
        "success": true
    },
    {
        "order": 819,
        "title": "Parallelizing Legendre Memory Unit Training",
        "html": "https://ICML.cc//virtual/2021/poster/9529",
        "abstract": "Recently, a new  recurrent neural network (RNN) named the Legendre Memory Unit (LMU) was proposed and shown to achieve state-of-the-art performance on several benchmark datasets. Here we leverage the linear time-invariant (LTI) memory component of the LMU to construct a simplified variant that can be parallelized during training (and yet executed as an RNN during inference), resulting in up to 200 times faster training. We note that our efficient parallelizing scheme is general and is applicable to any deep network whose recurrent components are linear dynamical systems. We demonstrate the improved accuracy of our new architecture compared to the original LMU and a variety of published LSTM and transformer networks across seven benchmarks. For instance, our LMU sets a new state-of-the-art result on psMNIST, and uses half the parameters while outperforming DistilBERT and LSTM models on IMDB sentiment analysis. ",
        "conference": "ICML",
        "中文标题": "并行化Legendre记忆单元训练",
        "摘要翻译": "最近，一种名为Legendre记忆单元（LMU）的新型循环神经网络（RNN）被提出，并在多个基准数据集上展示了最先进的性能。本文利用LMU的线性时不变（LTI）记忆组件构建了一个简化版本，该版本在训练期间可以并行化（但在推理期间仍可作为RNN执行），从而使训练速度提高了多达200倍。我们注意到，我们高效的并行化方案是通用的，适用于任何循环组件为线性动态系统的深度网络。我们展示了新架构在七个基准测试中相比原始LMU以及多种已发布的LSTM和transformer网络在准确性上的提升。例如，我们的LMU在psMNIST上设定了新的最先进结果，并在IMDB情感分析上使用了一半的参数，同时性能超过了DistilBERT和LSTM模型。",
        "领域": "循环神经网络优化, 深度学习加速, 自然语言处理",
        "问题": "如何提高Legendre记忆单元（LMU）的训练效率，同时保持或提升其性能",
        "动机": "为了解决LMU训练过程中的效率瓶颈，同时探索其在更广泛深度网络中的应用潜力",
        "方法": "利用LMU的线性时不变（LTI）记忆组件构建简化版本，实现训练期间的并行化",
        "关键词": [
            "Legendre记忆单元",
            "并行化训练",
            "线性时不变系统",
            "循环神经网络",
            "深度学习加速"
        ],
        "涉及的技术概念": {
            "Legendre记忆单元（LMU）": "一种新型循环神经网络，通过Legendre多项式近似延迟线，用于高效处理长序列数据",
            "线性时不变（LTI）系统": "在LMU中用于构建记忆组件的系统，其特性使得网络在训练期间可以并行化",
            "并行化训练": "通过利用LTI系统的特性，实现训练过程的并行化，显著提高训练速度"
        },
        "success": true
    },
    {
        "order": 820,
        "title": "Parallel tempering on optimized paths",
        "html": "https://ICML.cc//virtual/2021/poster/8923",
        "abstract": "Parallel tempering (PT) is a class of Markov chain\nMonte Carlo algorithms that constructs a path of\ndistributions annealing between a tractable reference\nand an intractable target, and then interchanges\nstates along the path to improve mixing\nin the target. The performance of PT depends on\nhow quickly a sample from the reference distribution\nmakes its way to the target, which in turn\ndepends on the particular path of annealing distributions.\nHowever, past work on PT has used\nonly simple paths constructed from convex combinations\nof the reference and target log-densities.\nThis paper begins by demonstrating that this path\nperforms poorly in the setting where the reference\nand target are nearly mutually singular. To address\nthis issue, we expand the framework of PT\nto general families of paths, formulate the choice\nof path as an optimization problem that admits\ntractable gradient estimates, and propose a flexible\nnew family of spline interpolation paths for\nuse in practice. Theoretical and empirical results\nboth demonstrate that our proposed methodology\nbreaks previously-established upper performance\nlimits for traditional paths.",
        "conference": "ICML",
        "中文标题": "优化路径上的并行回火",
        "摘要翻译": "并行回火（PT）是一类马尔可夫链蒙特卡洛算法，它构建了一条在易处理的参考分布和难处理的目标分布之间退火的分布路径，然后沿路径交换状态以改善目标分布中的混合。PT的性能取决于从参考分布到目标分布的样本传递速度，这又取决于退火分布的具体路径。然而，过去关于PT的工作仅使用了由参考和目标对数密度的凸组合构建的简单路径。本文首先证明了在参考和目标几乎相互奇异的情况下，这种路径表现不佳。为了解决这个问题，我们将PT的框架扩展到一般路径家族，将路径选择表述为一个允许易处理梯度估计的优化问题，并提出了一种新的灵活样条插值路径家族供实践使用。理论和实证结果都表明，我们提出的方法打破了传统路径先前建立的性能上限。",
        "领域": "马尔可夫链蒙特卡洛方法、优化算法、统计计算",
        "问题": "解决并行回火算法在参考和目标分布几乎相互奇异时性能不佳的问题",
        "动机": "提高并行回火算法在复杂分布间的混合效率",
        "方法": "扩展并行回火框架至一般路径家族，将路径选择问题公式化为优化问题，并提出新的样条插值路径家族",
        "关键词": [
            "并行回火",
            "马尔可夫链蒙特卡洛",
            "优化路径",
            "样条插值",
            "性能优化"
        ],
        "涉及的技术概念": {
            "并行回火": "一类马尔可夫链蒙特卡洛算法，通过在多个温度水平上并行运行马尔可夫链来改善采样效率",
            "马尔可夫链蒙特卡洛": "一种通过构建马尔可夫链来从概率分布中采样的计算方法",
            "样条插值": "一种通过分段多项式函数来近似复杂路径的数学方法，用于构建灵活的退火路径"
        },
        "success": true
    },
    {
        "order": 821,
        "title": "Parameter-free Locally Accelerated Conditional Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/9511",
        "abstract": "Projection-free conditional gradient (CG) methods are the algorithms of choice for constrained optimization setups in which projections are often computationally prohibitive but linear optimization over the constraint set remains computationally feasible. Unlike in projection-based methods, globally accelerated convergence rates are in general unattainable for CG. However, a very recent work on Locally accelerated CG (LaCG) has demonstrated that local acceleration for CG is possible for many settings of interest. The main downside of LaCG is that it requires knowledge of the smoothness and strong convexity parameters of the objective function. We remove this limitation by introducing a novel, Parameter-Free Locally accelerated CG (PF-LaCG) algorithm, for which we provide rigorous convergence guarantees. Our theoretical results are complemented by numerical experiments, which demonstrate local acceleration and showcase the practical improvements of PF-LaCG over non-accelerated algorithms, both in terms of iteration count and wall-clock time.",
        "conference": "ICML",
        "中文标题": "无参数局部加速条件梯度法",
        "摘要翻译": "投影自由条件梯度（CG）方法是针对那些投影计算通常非常昂贵但线性优化在约束集上仍然计算可行的约束优化设置的首选算法。与基于投影的方法不同，CG方法通常无法达到全局加速的收敛速率。然而，最近关于局部加速CG（LaCG）的工作表明，对于许多感兴趣的设置，CG的局部加速是可能的。LaCG的主要缺点是需要知道目标函数的平滑度和强凸性参数。我们通过引入一种新颖的无参数局部加速CG（PF-LaCG）算法来消除这一限制，并为其提供了严格的收敛保证。我们的理论结果得到了数值实验的补充，这些实验展示了局部加速，并展示了PF-LaCG在迭代次数和实际运行时间方面相对于非加速算法的实际改进。",
        "领域": "优化算法、机器学习、数值分析",
        "问题": "解决在约束优化设置中，条件梯度方法需要预先知道目标函数的平滑度和强凸性参数的限制问题。",
        "动机": "为了消除局部加速条件梯度方法中对目标函数参数知识的依赖，提出一种无需参数的方法，以实现更广泛的应用。",
        "方法": "引入一种新颖的无参数局部加速条件梯度（PF-LaCG）算法，通过理论分析和数值实验验证其性能。",
        "关键词": [
            "条件梯度",
            "局部加速",
            "无参数优化",
            "约束优化",
            "收敛保证"
        ],
        "涉及的技术概念": {
            "条件梯度（CG）": "一种在约束优化中避免昂贵投影计算的算法，通过线性优化在约束集上进行迭代。",
            "局部加速（LaCG）": "在条件梯度方法中实现局部加速的技术，提高收敛速度。",
            "无参数优化": "一种不依赖于目标函数参数（如平滑度和强凸性）的优化方法，提高了算法的适用性和灵活性。"
        },
        "success": true
    },
    {
        "order": 822,
        "title": "Parameterless Transductive Feature Re-representation for Few-Shot Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10079",
        "abstract": "Recent literature in few-shot learning (FSL) has shown that transductive methods often outperform their inductive counterparts. However, most transductive solutions, particularly the meta-learning based ones, require inserting trainable parameters on top of some inductive baselines to facilitate transduction. In this paper, we propose a parameterless transductive feature re-representation framework that differs from all existing solutions from the following perspectives. (1) It is widely compatible with existing FSL methods, including meta-learning and fine tuning based models. (2) The framework is simple and introduces no extra training parameters when applied to any architecture. We conduct experiments on three benchmark datasets by applying the framework to both representative meta-learning baselines and state-of-the-art FSL methods. Our framework consistently improves performances in all experiments and refreshes the state-of-the-art FSL results. ",
        "conference": "ICML",
        "中文标题": "无参数转导特征重表示用于小样本学习",
        "摘要翻译": "近期的小样本学习(FSL)文献表明，转导方法通常优于归纳方法。然而，大多数转导解决方案，特别是基于元学习的方法，需要在一些归纳基线之上插入可训练参数以促进转导。在本文中，我们提出了一个无参数的转导特征重表示框架，该框架在以下几个方面与所有现有解决方案不同。(1)它与现有的FSL方法广泛兼容，包括基于元学习和微调的模型。(2)该框架简单，应用于任何架构时不会引入额外的训练参数。我们通过在三个基准数据集上将该框架应用于代表性的元学习基线和最先进的FSL方法进行实验。我们的框架在所有实验中一致提高了性能，并刷新了最先进的FSL结果。",
        "领域": "小样本学习、元学习、特征表示",
        "问题": "解决小样本学习中转导方法需要额外训练参数的问题",
        "动机": "开发一个无需额外训练参数即可提升小样本学习性能的转导特征重表示框架",
        "方法": "提出一个无参数的转导特征重表示框架，兼容现有FSL方法，不引入额外训练参数",
        "关键词": [
            "小样本学习",
            "转导学习",
            "特征重表示",
            "元学习",
            "无参数"
        ],
        "涉及的技术概念": {
            "转导学习": "在本文中用于利用未标记数据提升模型性能的学习范式",
            "特征重表示": "通过重新表示特征来提升模型对小样本数据的泛化能力",
            "元学习": "作为兼容的基线方法之一，用于学习如何学习，以快速适应新任务"
        },
        "success": true
    },
    {
        "order": 823,
        "title": "Parametric Graph for Unimodal Ranking Bandit",
        "html": "https://ICML.cc//virtual/2021/poster/9637",
        "abstract": " We tackle the online ranking problem of assigning $L$ items to $K$ positions on a web page in order to maximize the number of user clicks. We propose an original algorithm, easy to implement and with strong theoretical guarantees to tackle this problem in the Position-Based Model (PBM) setting, well suited for applications where items are displayed on a grid. Besides learning to rank, our algorithm, GRAB (for parametric Graph for unimodal RAnking Bandit), also learns the parameter of a compact graph over permutations of $K$ items among $L$. The logarithmic regret bound of this algorithm is a direct consequence of the unimodality property of the bandit setting with respect to the learned graph. Experiments against state-of-the-art learning algorithms which also tackle the PBM setting, show that our method is more efficient while giving regret performance on par with the best known algorithms on simulated and real life datasets.",
        "conference": "ICML",
        "中文标题": "单峰排序老虎机的参数化图",
        "摘要翻译": "我们解决了在线排序问题，即在网页上将$L$个项目分配到$K$个位置上，以最大化用户点击次数。我们提出了一种原创算法，易于实现且具有强大的理论保证，以在位置基础模型（PBM）设置中解决这个问题，非常适合在网格上显示项目的应用。除了学习排序外，我们的算法GRAB（单峰排序老虎机的参数化图）还学习了$K$个项目在$L$中的排列的紧凑图的参数。该算法的对数遗憾界是与学习图相关的老虎机设置的单峰性质的直接结果。与同样处理PBM设置的最先进学习算法相比的实验表明，我们的方法更高效，同时在模拟和真实数据集上的遗憾性能与最知名的算法相当。",
        "领域": "在线学习排序、推荐系统、多臂老虎机",
        "问题": "解决在网页上如何分配项目以最大化用户点击次数的在线排序问题",
        "动机": "开发一种既易于实现又具有强大理论保证的算法，以优化网页上项目的展示顺序，从而提高用户点击率",
        "方法": "提出了一种名为GRAB的算法，该算法不仅学习排序，还学习项目排列的紧凑图的参数，利用单峰性质来优化排序",
        "关键词": [
            "在线排序",
            "位置基础模型",
            "单峰排序",
            "参数化图",
            "GRAB算法"
        ],
        "涉及的技术概念": {
            "位置基础模型（PBM）": "用于描述在网格上显示项目的应用场景，其中项目的排列顺序影响用户点击行为",
            "单峰性质": "在老虎机设置中，用于描述与学习图相关的遗憾界的性质，是GRAB算法高效性的关键",
            "GRAB算法": "一种原创的在线学习排序算法，通过学习项目排列的紧凑图参数来优化排序，具有对数遗憾界的理论保证"
        },
        "success": true
    },
    {
        "order": 824,
        "title": "Pareto GAN: Extending the Representational Power of GANs to Heavy-Tailed Distributions",
        "html": "https://ICML.cc//virtual/2021/poster/10707",
        "abstract": "Generative adversarial networks (GANs) are often billed as 'universal distribution learners', but precisely what distributions they can represent and learn is still an open question. Heavy-tailed distributions are prevalent in many different domains such as financial risk-assessment, physics, and epidemiology. We observe that existing GAN architectures do a poor job of matching the asymptotic behavior of heavy-tailed distributions, a problem that we show stems from their construction. Additionally, common loss functions produce unstable or near-zero gradients when faced with the infinite moments and large distances between outlier points characteristic of heavy-tailed distributions. We address these problems with the Pareto GAN. A Pareto GAN leverages extreme value theory and the functional properties of neural networks to learn a distribution that matches the asymptotic behavior of the marginal distributions of the features. We identify issues with standard loss functions and propose the use of alternative metric spaces that enable stable and efficient learning. Finally, we evaluate our proposed approach on a variety of heavy-tailed datasets.",
        "conference": "ICML",
        "中文标题": "Pareto GAN：将GAN的表示能力扩展到重尾分布",
        "摘要翻译": "生成对抗网络（GANs）常被誉为‘通用分布学习器’，但它们究竟能表示和学习哪些分布仍是一个开放性问题。重尾分布在金融风险评估、物理学和流行病学等多个领域普遍存在。我们观察到，现有的GAN架构在匹配重尾分布的渐近行为方面表现不佳，这一问题我们证明源于它们的构建方式。此外，面对重尾分布特有的无限矩和异常点间的大距离，常见的损失函数会产生不稳定或接近零的梯度。我们通过Pareto GAN解决了这些问题。Pareto GAN利用极值理论和神经网络的功能特性，学习一个与特征的边际分布的渐近行为相匹配的分布。我们指出了标准损失函数的问题，并提出了使用替代度量空间以实现稳定和高效学习的方法。最后，我们在多种重尾数据集上评估了我们提出的方法。",
        "领域": "生成对抗网络、极值理论、分布学习",
        "问题": "现有GAN架构在学习和表示重尾分布方面的不足",
        "动机": "解决GAN在处理重尾分布时的渐近行为匹配和梯度不稳定问题",
        "方法": "利用极值理论和神经网络的特性，提出Pareto GAN，并采用替代度量空间优化学习过程",
        "关键词": [
            "Pareto GAN",
            "重尾分布",
            "极值理论",
            "替代度量空间",
            "分布学习"
        ],
        "涉及的技术概念": {
            "极值理论": "用于分析和建模极端事件或异常值的统计理论，Pareto GAN中用于理解和匹配重尾分布的渐近行为",
            "替代度量空间": "不同于传统欧几里得空间的度量方式，用于在GAN训练中提供更稳定和有效的梯度",
            "重尾分布": "具有比指数分布更厚的尾部的概率分布，Pareto GAN专注于学习和表示这类分布"
        },
        "success": true
    },
    {
        "order": 825,
        "title": "Partially Observed Exchangeable Modeling",
        "html": "https://ICML.cc//virtual/2021/poster/10531",
        "abstract": "Modeling dependencies among features is fundamental for many machine learning tasks. Although there are often multiple related instances that may be leveraged to inform conditional dependencies, typical approaches only model conditional dependencies over individual instances. In this work, we propose a novel framework, partially observed exchangeable modeling (POEx) that takes in a set of related partially observed instances and infers the conditional distribution for the unobserved dimensions over multiple elements. Our approach jointly models the intra-instance (among features in a point) and inter-instance (among multiple points in a set) dependencies in data. POEx is a general framework that encompasses many existing tasks such as point cloud expansion and\nfew-shot generation, as well as new tasks like few-shot imputation. Despite its generality, extensive empirical evaluations show that our model achieves state-of-the-art performance across a range of applications.",
        "conference": "ICML",
        "中文标题": "部分可观测可交换建模",
        "摘要翻译": "建模特征之间的依赖关系是许多机器学习任务的基础。尽管经常存在多个相关实例可以用来告知条件依赖关系，但典型的方法仅对单个实例的条件依赖关系进行建模。在这项工作中，我们提出了一个新框架——部分可观测可交换建模（POEx），该框架接收一组相关的部分可观测实例，并推断出多个元素上未观测维度的条件分布。我们的方法联合建模了数据中的实例内（一个点内的特征之间）和实例间（一个集合中的多个点之间）的依赖关系。POEx是一个通用框架，涵盖了许多现有任务，如点云扩展和少样本生成，以及新任务如少样本插补。尽管具有通用性，广泛的实证评估显示，我们的模型在一系列应用中实现了最先进的性能。",
        "领域": "机器学习模型泛化、少样本学习、点云处理",
        "问题": "如何在部分观测数据中建模实例内和实例间的依赖关系，以推断未观测维度的条件分布。",
        "动机": "现有方法通常仅针对单个实例的条件依赖关系进行建模，无法充分利用多个相关实例的信息来推断未观测维度的条件分布。",
        "方法": "提出了部分可观测可交换建模（POEx）框架，联合建模实例内和实例间的依赖关系，以推断未观测维度的条件分布。",
        "关键词": [
            "部分可观测数据",
            "可交换建模",
            "少样本学习",
            "点云扩展",
            "条件分布推断"
        ],
        "涉及的技术概念": {
            "部分可观测可交换建模（POEx）": "一个通用框架，用于在部分观测数据中建模实例内和实例间的依赖关系，以推断未观测维度的条件分布。",
            "实例内依赖": "指一个实例内部特征之间的依赖关系，POEx框架中用于建模单个实例的条件分布。",
            "实例间依赖": "指不同实例之间的依赖关系，POEx框架中用于利用多个相关实例的信息来推断未观测维度的条件分布。"
        },
        "success": true
    },
    {
        "order": 826,
        "title": "Path Planning using Neural A* Search",
        "html": "https://ICML.cc//virtual/2021/poster/9055",
        "abstract": "We present Neural A*, a novel data-driven search method for path planning problems. Despite the recent increasing attention to data-driven path planning, machine learning approaches to search-based planning are still challenging due to the discrete nature of search algorithms. In this work, we reformulate a canonical A* search algorithm to be differentiable and couple it with a convolutional encoder to form an end-to-end trainable neural network planner. Neural A* solves a path planning problem by encoding a problem instance to a guidance map and then performing the differentiable A* search with the guidance map. By learning to match the search results with ground-truth paths provided by experts, Neural A* can produce a path consistent with the ground truth accurately and efficiently. Our extensive experiments confirmed that Neural A* outperformed state-of-the-art data-driven planners in terms of the search optimality and efficiency trade-off. Furthermore, Neural A* successfully predicted realistic human trajectories by directly performing search-based planning on natural image inputs.",
        "conference": "ICML",
        "中文标题": "使用神经A*搜索的路径规划",
        "摘要翻译": "我们提出了神经A*，一种新颖的数据驱动搜索方法，用于解决路径规划问题。尽管近年来数据驱动的路径规划受到了越来越多的关注，但由于搜索算法的离散性质，基于搜索的规划的机器学习方法仍然具有挑战性。在这项工作中，我们重新制定了经典的A*搜索算法，使其可微分，并将其与卷积编码器结合，形成一个端到端可训练的神经网络规划器。神经A*通过将问题实例编码为引导图，然后使用引导图执行可微分的A*搜索来解决路径规划问题。通过学习将搜索结果与专家提供的真实路径相匹配，神经A*能够准确高效地生成与真实路径一致的路径。我们的大量实验证实，神经A*在搜索最优性和效率的权衡方面优于最先进的数据驱动规划器。此外，神经A*通过在自然图像输入上直接执行基于搜索的规划，成功预测了现实的人类轨迹。",
        "领域": "路径规划、机器学习在机器人学中的应用、深度学习与计算机视觉结合",
        "问题": "解决在离散搜索算法中应用机器学习方法进行路径规划的挑战",
        "动机": "开发一种能够准确高效生成路径，并与真实路径一致的数据驱动路径规划方法",
        "方法": "重新制定经典的A*搜索算法为可微分形式，结合卷积编码器形成端到端可训练的神经网络规划器",
        "关键词": [
            "神经A*搜索",
            "路径规划",
            "数据驱动",
            "可微分搜索",
            "卷积编码器"
        ],
        "涉及的技术概念": {
            "神经A*搜索": "一种可微分的数据驱动搜索方法，用于路径规划",
            "可微分A*搜索": "重新制定的A*搜索算法，使其能够与神经网络结合进行端到端训练",
            "卷积编码器": "用于将路径规划问题实例编码为引导图的神经网络组件"
        },
        "success": true
    },
    {
        "order": 827,
        "title": "PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided Exploration",
        "html": "https://ICML.cc//virtual/2021/poster/9705",
        "abstract": "Model-based Reinforcement Learning (RL) is a popular learning paradigm due to its potential sample efficiency compared to model-free RL. However, existing empirical model-based RL approaches lack the ability to explore.  This work studies a computationally and statistically efficient model-based algorithm for both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes (MDPs). For both models, our algorithm guarantees polynomial sample complexity and only uses access to a planning oracle. Experimentally, we first demonstrate the flexibility and the efficacy of our algorithm on a set of exploration challenging control tasks where existing empirical model-based RL approaches completely fail. We then show that our approach retains excellent performance even in common dense reward control benchmarks that do not require heavy exploration. \n",
        "conference": "ICML",
        "中文标题": "PC-MLP：基于策略覆盖引导探索的模型强化学习",
        "摘要翻译": "基于模型的强化学习（RL）因其相比无模型RL潜在的样本效率而成为一种流行的学习范式。然而，现有的经验性基于模型的RL方法缺乏探索能力。本研究针对核化非线性调节器（KNR）和线性马尔可夫决策过程（MDPs）提出了一种计算和统计效率高的基于模型算法。对于这两种模型，我们的算法保证了多项式样本复杂度，并且仅需访问规划预言机。实验上，我们首先在一系列探索性挑战控制任务上展示了我们算法的灵活性和有效性，这些任务中现有的经验性基于模型RL方法完全失败。然后，我们展示了即使在不需要大量探索的常见密集奖励控制基准测试中，我们的方法也保持了优异的性能。",
        "领域": "强化学习、控制理论、机器学习",
        "问题": "解决基于模型的强化学习方法在探索能力上的不足",
        "动机": "提高基于模型的强化学习在探索性任务中的效率和性能",
        "方法": "提出了一种计算和统计效率高的基于模型算法，适用于核化非线性调节器和线性马尔可夫决策过程，保证多项式样本复杂度并仅需访问规划预言机",
        "关键词": [
            "模型强化学习",
            "策略覆盖",
            "探索算法",
            "核化非线性调节器",
            "马尔可夫决策过程"
        ],
        "涉及的技术概念": {
            "模型强化学习": "一种利用环境模型来提高学习效率和性能的强化学习方法",
            "策略覆盖": "引导探索的策略，确保在学习过程中充分探索状态空间",
            "规划预言机": "用于在给定模型下计算最优或近似最优策略的算法或工具"
        },
        "success": true
    },
    {
        "order": 828,
        "title": "PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training",
        "html": "https://ICML.cc//virtual/2021/poster/8487",
        "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",
        "conference": "ICML",
        "中文标题": "PEBBLE：通过经验重标定和无监督预训练实现反馈高效的交互式强化学习",
        "摘要翻译": "向强化学习（RL）智能体传达复杂目标往往很困难，需要精心设计既足够信息丰富又易于提供的奖励函数。人在环中的RL方法允许从业者通过定制反馈交互式地教导智能体；然而，这种方法难以扩展，因为人类反馈成本非常高。在这项工作中，我们旨在使这一过程更加样本和反馈高效。我们提出了一种离策略的交互式RL算法，该算法充分利用了反馈和离策略学习的优势。具体来说，我们通过主动查询教师在两段行为剪辑之间的偏好来学习奖励模型，并用它来训练智能体。为了实现离策略学习，我们在奖励模型变化时重标定智能体的所有过去经验。我们还表明，通过无监督探索预训练我们的智能体可以显著增加其查询的价值。我们证明了我们的方法能够学习比之前人在环方法考虑的更高复杂度的任务，包括各种运动和机器人操作技能。我们还展示了我们的方法能够利用实时人类反馈有效防止奖励利用，并学习难以用标准奖励函数指定的新行为。",
        "领域": "强化学习",
        "问题": "如何高效地向强化学习智能体传达复杂目标，减少对人类反馈的依赖",
        "动机": "人类反馈成本高，难以扩展，需要更高效的交互式强化学习方法",
        "方法": "提出一种离策略的交互式RL算法，结合奖励模型学习和经验重标定，利用无监督探索预训练提高效率",
        "关键词": [
            "交互式强化学习",
            "奖励模型",
            "经验重标定",
            "无监督预训练",
            "人在环中学习"
        ],
        "涉及的技术概念": {
            "离策略学习": "允许智能体从非当前策略生成的数据中学习，提高数据利用效率",
            "奖励模型": "通过人类反馈学习，用于指导智能体行为，替代传统奖励函数",
            "无监督探索": "在预训练阶段使用，帮助智能体在没有明确奖励信号的情况下学习环境的基本结构"
        },
        "success": true
    },
    {
        "order": 829,
        "title": "Perceiver: General Perception with Iterative Attention",
        "html": "https://ICML.cc//virtual/2021/poster/10291",
        "abstract": "Biological systems understand the world by simultaneously processing high-dimensional inputs from  modalities  as  diverse  as  vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities.  In this paper we introduce the Perceiver – a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio.  The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
        "conference": "ICML",
        "success": true,
        "中文标题": "感知器：通过迭代注意力实现通用感知",
        "摘要翻译": "生物系统通过同时处理来自视觉、听觉、触觉、本体感觉等多种模态的高维输入来理解世界。而深度学习中的感知模型则是为单一模态设计的，通常依赖于领域特定的假设，如几乎所有现有视觉模型所利用的局部网格结构。这些先验知识引入了有益的归纳偏置，但也将模型锁定在单一模态上。在本文中，我们介绍了感知器——一个基于Transformer构建的模型，因此对其输入之间的关系几乎没有架构上的假设，但也能像卷积网络那样扩展到数十万个输入。该模型利用非对称注意力机制迭代地将输入提炼成一个紧凑的潜在瓶颈，使其能够扩展到处理非常大的输入。我们展示了这种架构在各种模态的分类任务上与强大的专门模型竞争或超越它们：图像、点云、音频、视频和视频+音频。感知器在ImageNet上通过直接关注50,000个像素，无需2D卷积，就能获得与ResNet-50和ViT相当的性能。在AudioSet的所有模态中也具有竞争力。",
        "领域": "多模态学习, 注意力机制, 通用感知模型",
        "问题": "如何设计一个能够处理多种模态输入的通用感知模型，而不依赖于特定模态的架构假设",
        "动机": "现有的深度学习感知模型通常针对单一模态设计，依赖于特定模态的先验知识，限制了模型的通用性和灵活性",
        "方法": "基于Transformer构建的感知器模型，利用非对称注意力机制迭代地提炼输入到一个紧凑的潜在瓶颈，以处理大规模多模态输入",
        "关键词": [
            "多模态学习",
            "注意力机制",
            "Transformer",
            "通用感知",
            "非对称注意力"
        ],
        "涉及的技术概念": {
            "Transformer": "作为模型的基础架构，提供对输入之间关系的灵活建模能力",
            "非对称注意力机制": "用于迭代地提炼输入到一个紧凑的潜在瓶颈，使模型能够处理大规模输入",
            "潜在瓶颈": "模型通过迭代注意力机制将高维输入压缩到低维空间，以实现高效处理"
        }
    },
    {
        "order": 830,
        "title": "Permutation Weighting",
        "html": "https://ICML.cc//virtual/2021/poster/9133",
        "abstract": "A  commonly  applied  approach  for  estimating causal  effects  from  observational  data  is  to  apply  weights  which  render  treatments  independent of observed pre-treatment covariates. Recently emphasis has been placed on deriving balancing weights which explicitly target this independence condition. In  this  work  we  introduce permutation  weighting,  a  method  for  estimating  balancing weights using a standard binary classifier (regardless of cardinality of treatment).  A large class of probabilistic classifiers may be used in this  method;  the  choice  of  loss  for  the  classifier implies the particular definition of balance. We bound bias and variance in terms of the excess  risk  of  the  classifier,  show  that  these  disappear asymptotically, and demonstrate that our classification problem directly minimizes imbalance.  Additionally, hyper-parameter tuning and model selection can be performed with standard cross-validation methods.  Empirical evaluations indicate that permutation weighting provides favorable  performance  in  comparison  to  existing methods.\n",
        "conference": "ICML",
        "中文标题": "置换加权",
        "摘要翻译": "从观测数据中估计因果效应的一种常用方法是应用权重，使处理独立于观察到的处理前协变量。最近，重点放在了推导明确针对这种独立条件的平衡权重上。在这项工作中，我们介绍了置换加权，一种使用标准二元分类器（无论处理的基数如何）估计平衡权重的方法。这种方法可以使用一大类概率分类器；分类器的损失选择意味着特定的平衡定义。我们根据分类器的超额风险界定了偏差和方差，表明这些随着样本量的增加而消失，并证明我们的分类问题直接最小化了不平衡。此外，超参数调整和模型选择可以使用标准的交叉验证方法进行。实证评估表明，与现有方法相比，置换加权提供了更优的性能。",
        "领域": "因果推断、机器学习、统计学习",
        "问题": "如何从观测数据中更有效地估计因果效应",
        "动机": "为了解决现有方法在估计因果效应时可能存在的偏差和不平衡问题",
        "方法": "引入置换加权方法，利用标准二元分类器估计平衡权重，通过分类器的损失选择来定义平衡，并通过超额风险界定偏差和方差",
        "关键词": [
            "置换加权",
            "因果效应",
            "平衡权重",
            "二元分类器",
            "超额风险"
        ],
        "涉及的技术概念": {
            "置换加权": "一种使用标准二元分类器估计平衡权重的方法，旨在最小化处理与协变量之间的不平衡",
            "平衡权重": "旨在使处理独立于观察到的处理前协变量的权重，用于估计因果效应",
            "超额风险": "分类器的性能指标，用于界定置换加权方法中的偏差和方差"
        },
        "success": true
    },
    {
        "order": 831,
        "title": "Personalized Federated Learning using Hypernetworks",
        "html": "https://ICML.cc//virtual/2021/poster/10453",
        "abstract": "Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively while accounting for data disparities across clients and reducing communication costs.\n\nWe propose a novel approach to this problem using hypernetworks, termed pFedHN for personalized Federated HyperNetworks. In this approach, a central hypernetwork model is trained to generate a set of models, one model for each client. This architecture provides effective parameter sharing across clients while maintaining the capacity to generate unique and diverse personal models. Furthermore, since hypernetwork parameters are never transmitted, this approach decouples the communication cost from the trainable model size. We test pFedHN empirically in several personalized federated learning challenges and find that it outperforms previous methods. Finally, since hypernetworks share information across clients, we show that pFedHN can generalize better to new clients whose distributions differ from any client observed during training.",
        "conference": "ICML",
        "中文标题": "使用超网络的个性化联邦学习",
        "摘要翻译": "个性化联邦学习的任务是为多个客户端训练机器学习模型，每个客户端都有自己的数据分布。目标是在考虑客户端间数据差异和减少通信成本的同时，协作训练个性化模型。我们提出了一种使用超网络的新方法来解决这个问题，称为个性化联邦超网络（pFedHN）。在这种方法中，训练一个中央超网络模型来生成一组模型，每个客户端一个模型。这种架构在客户端之间提供了有效的参数共享，同时保持了生成独特和多样化个人模型的能力。此外，由于超网络参数从不传输，这种方法将通信成本与可训练模型的大小解耦。我们在几个个性化联邦学习挑战中实证测试了pFedHN，发现它优于以前的方法。最后，由于超网络在客户端之间共享信息，我们展示了pFedHN可以更好地泛化到那些在训练期间观察到的任何客户端分布不同的新客户端。",
        "领域": "联邦学习、个性化学习、超网络",
        "问题": "在考虑客户端间数据差异和减少通信成本的同时，协作训练个性化模型",
        "动机": "解决个性化联邦学习中的数据分布差异和通信成本问题",
        "方法": "使用中央超网络模型生成每个客户端的个性化模型，实现参数共享和通信成本解耦",
        "关键词": [
            "个性化联邦学习",
            "超网络",
            "参数共享",
            "通信成本",
            "模型泛化"
        ],
        "涉及的技术概念": {
            "超网络": "用于生成每个客户端的个性化模型，实现参数共享",
            "个性化联邦学习": "在多个客户端间协作训练个性化模型，考虑数据分布差异",
            "通信成本解耦": "通过不传输超网络参数，将通信成本与模型大小分离"
        },
        "success": true
    },
    {
        "order": 832,
        "title": "Phase Transitions, Distance Functions, and Implicit Neural Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10735",
        "abstract": "Representing surfaces as zero level sets of neural networks recently emerged as a powerful modeling paradigm, named Implicit Neural Representations (INRs), serving numerous downstream applications in geometric deep learning and 3D vision. Training INRs previously required choosing between occupancy and distance function representation and different losses with unknown limit behavior and/or bias. In this paper we draw inspiration from the theory of phase transitions of fluids and suggest a loss for training INRs that learns a density function that converges to a proper occupancy function, while its log transform converges to a distance function. Furthermore, we analyze the limit minimizer of this loss showing it satisfies the reconstruction constraints and has minimal surface perimeter, a desirable inductive bias for surface reconstruction. Training INRs with this new loss leads to state-of-the-art reconstructions on a standard benchmark.  ",
        "conference": "ICML",
        "中文标题": "相变、距离函数与隐式神经表示",
        "摘要翻译": "将表面表示为神经网络的零水平集最近成为一种强大的建模范式，称为隐式神经表示（INRs），服务于几何深度学习和3D视觉中的众多下游应用。以往训练INRs需要在占据和距离函数表示以及具有未知极限行为和/或偏差的不同损失函数之间做出选择。在本文中，我们从流体相变理论中汲取灵感，提出了一种用于训练INRs的损失函数，该函数学习一个收敛到适当占据函数的密度函数，同时其对数变换收敛到距离函数。此外，我们分析了这种损失的极限最小化器，表明它满足重建约束并具有最小的表面周长，这是表面重建的一个理想归纳偏差。使用这种新的损失函数训练INRs，在标准基准测试中实现了最先进的重建效果。",
        "领域": "几何深度学习, 3D视觉, 表面重建",
        "问题": "如何在训练隐式神经表示（INRs）时选择合适的损失函数，以同时实现占据函数和距离函数的有效表示。",
        "动机": "从流体相变理论中获得灵感，解决INRs训练中损失函数选择的问题，以实现更优的表面重建效果。",
        "方法": "提出一种新的损失函数，该函数能够学习一个收敛到占据函数的密度函数，同时其对数变换收敛到距离函数，并通过分析证明其极限最小化器满足重建约束并具有最小的表面周长。",
        "关键词": [
            "隐式神经表示",
            "表面重建",
            "损失函数",
            "相变理论",
            "几何深度学习"
        ],
        "涉及的技术概念": {
            "隐式神经表示（INRs）": "一种将表面表示为神经网络零水平集的建模范式，用于几何深度学习和3D视觉应用。",
            "损失函数": "本文提出的新型损失函数，用于训练INRs，能够同时学习占据函数和距离函数。",
            "相变理论": "从流体相变理论中汲取灵感，指导新型损失函数的设计，以实现更优的表面重建效果。"
        },
        "success": true
    },
    {
        "order": 833,
        "title": "Phasic Policy Gradient",
        "html": "https://ICML.cc//virtual/2021/poster/8475",
        "abstract": "We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark.",
        "conference": "ICML",
        "中文标题": "阶段性策略梯度",
        "摘要翻译": "我们介绍了阶段性策略梯度（PPG），这是一种强化学习框架，通过将策略和价值函数的训练分离到不同的阶段，修改了传统的同策略行动者-评论家方法。在先前的方法中，必须在共享网络或独立网络之间做出选择来表示策略和价值函数。使用独立网络可以避免目标之间的干扰，而使用共享网络则允许共享有用的特征。PPG通过将优化分为两个阶段，一个阶段推进训练，另一个阶段提炼特征，从而能够实现两全其美。PPG还使得价值函数能够通过更高水平的样本重用进行更积极的优化。与PPO相比，我们发现PPG在具有挑战性的Procgen基准测试中显著提高了样本效率。",
        "领域": "强化学习",
        "问题": "如何在强化学习中同时避免策略和价值函数训练之间的干扰并共享有用特征",
        "动机": "解决传统同策略行动者-评论家方法在策略和价值函数训练中必须选择共享或独立网络的问题",
        "方法": "通过将优化分为推进训练和提炼特征两个阶段，实现策略和价值函数训练的最优平衡",
        "关键词": [
            "阶段性策略梯度",
            "强化学习",
            "样本效率",
            "Procgen基准测试",
            "策略和价值函数训练"
        ],
        "涉及的技术概念": {
            "阶段性策略梯度（PPG）": "一种强化学习框架，通过分离策略和价值函数的训练阶段来优化性能",
            "同策略行动者-评论家方法": "一种结合策略梯度方法和价值函数方法的强化学习算法",
            "样本重用": "在训练过程中重复使用样本以提高学习效率和性能的技术"
        },
        "success": true
    },
    {
        "order": 834,
        "title": "PHEW : Constructing Sparse Networks that Learn Fast and Generalize Well without Training Data",
        "html": "https://ICML.cc//virtual/2021/poster/9917",
        "abstract": "Methods that sparsify a network at initialization are important in practice because they greatly improve the efficiency of both learning and inference. Our work is based on a recently proposed decomposition of the Neural Tangent Kernel (NTK) that has decoupled the dynamics of the training process into a data-dependent component and an architecture-dependent kernel – the latter referred to as Path Kernel. That work has shown how to design sparse neural networks for faster convergence, without any training data, using the Synflow-L2 algorithm. We first show that even though Synflow-L2 is optimal in terms of convergence, for a given network density, it results in sub-networks with ``bottleneck'' (narrow) layers – leading to poor performance as compared to other data-agnostic methods that use the same number of parameters. Then we propose a new method to construct sparse networks, without any training data, referred to as Paths with Higher-Edge Weights (PHEW). PHEW is a probabilistic network formation method based on biased random walks that only depends on the initial weights. It has similar path kernel properties as Synflow-L2 but it generates much wider layers, resulting in better generalization and performance. PHEW achieves significant improvements over the data-independent SynFlow and SynFlow-L2 methods at a wide range of network densities.",
        "conference": "ICML",
        "中文标题": "PHEW：无需训练数据构建学习快速且泛化良好的稀疏网络",
        "摘要翻译": "在初始化时稀疏化网络的方法在实践中非常重要，因为它们极大地提高了学习和推理的效率。我们的工作基于最近提出的神经切线核（NTK）分解，该分解将训练过程的动态解耦为数据依赖组件和架构依赖核——后者称为路径核。该工作展示了如何设计稀疏神经网络以实现更快收敛，而无需任何训练数据，使用Synflow-L2算法。我们首先表明，尽管Synflow-L2在收敛性方面是最优的，对于给定的网络密度，它会导致子网络具有“瓶颈”（狭窄）层——与使用相同数量参数的其他数据无关方法相比，性能较差。然后我们提出了一种新的方法来构建稀疏网络，无需任何训练数据，称为具有高边权重的路径（PHEW）。PHEW是一种基于偏置随机游走的概率网络形成方法，仅依赖于初始权重。它具有与Synflow-L2相似的路径核属性，但生成的层更宽，从而实现了更好的泛化和性能。PHEW在广泛的网络密度范围内，显著优于数据无关的SynFlow和SynFlow-L2方法。",
        "领域": "神经网络优化、深度学习理论、稀疏网络",
        "问题": "如何在无需训练数据的情况下，构建既学习快速又泛化良好的稀疏网络",
        "动机": "解决现有稀疏化方法在提高网络效率时可能导致性能下降的问题，特别是在网络层宽度不足时",
        "方法": "提出了一种基于偏置随机游走的概率网络形成方法PHEW，该方法仅依赖于初始权重，能够生成更宽的网络层，从而改善泛化和性能",
        "关键词": [
            "稀疏网络",
            "神经切线核",
            "路径核",
            "PHEW",
            "随机游走"
        ],
        "涉及的技术概念": {
            "神经切线核（NTK）": "用于分析神经网络训练动态的理论工具，将训练过程分解为数据依赖和架构依赖部分",
            "路径核": "架构依赖的核，影响网络的训练动态和性能",
            "偏置随机游走": "PHEW方法中用于生成稀疏网络的技术，通过偏置选择初始权重来优化网络结构"
        },
        "success": true
    },
    {
        "order": 835,
        "title": "PID Accelerated Value Iteration Algorithm",
        "html": "https://ICML.cc//virtual/2021/poster/8725",
        "abstract": "The convergence rate of Value Iteration (VI), a fundamental procedure in dynamic programming and reinforcement learning, for solving MDPs can be slow when the discount factor is close to one. We propose modifications to VI in order to potentially accelerate its convergence behaviour. The key insight is the realization that the evolution of the value function approximations $(V_k)_{k \\geq 0}$ in the VI procedure can be seen as a dynamical system. This opens up the possibility of using techniques from \\emph{control theory} to modify, and potentially accelerate, this dynamics. We present such modifications based on simple controllers, such as PD (Proportional-Derivative), PI (Proportional-Integral), and PID. We present the error dynamics of these variants of VI, and provably (for certain classes of MDPs) and empirically (for more general classes) show that the convergence rate can be significantly improved. We also propose a gain adaptation mechanism in order to automatically select the controller gains, and empirically show the effectiveness of this procedure.",
        "conference": "ICML",
        "success": true,
        "中文标题": "PID加速值迭代算法",
        "摘要翻译": "值迭代（VI）作为动态规划和强化学习中的一个基本过程，在解决马尔可夫决策过程（MDPs）时，当折扣因子接近一时，其收敛速度可能会很慢。我们提出了对VI的修改，以潜在地加速其收敛行为。关键见解是认识到VI过程中价值函数近似$(V_k)_{k \\geq 0}$的演变可以被视为一个动态系统。这开启了使用控制理论中的技术来修改并可能加速这一动态的可能性。我们提出了基于简单控制器（如PD（比例-微分）、PI（比例-积分）和PID）的修改方案。我们展示了这些VI变体的误差动态，并在理论上（对于某些类别的MDPs）和实证上（对于更一般的类别）证明了收敛速度可以显著提高。我们还提出了一种增益适应机制来自动选择控制器增益，并实证展示了这一过程的有效性。",
        "领域": "强化学习, 动态规划, 控制理论",
        "问题": "解决值迭代算法在折扣因子接近一时收敛速度慢的问题",
        "动机": "通过引入控制理论中的技术，加速值迭代算法的收敛速度",
        "方法": "基于PD、PI和PID控制器的修改方案，以及增益适应机制",
        "关键词": [
            "值迭代",
            "PID控制",
            "动态规划",
            "强化学习",
            "收敛加速"
        ],
        "涉及的技术概念": {
            "值迭代": "一种用于解决马尔可夫决策过程的动态规划和强化学习算法",
            "PID控制": "一种结合比例、积分和微分控制器的控制策略，用于调整系统的动态行为"
        }
    },
    {
        "order": 836,
        "title": "PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models",
        "html": "https://ICML.cc//virtual/2021/poster/9595",
        "abstract": "The size of Transformer models is growing at an unprecedented rate. It has taken less than one year to reach trillion-level parameters since the release of GPT-3 (175B). Training such models requires both substantial engineering efforts and enormous computing resources, which are luxuries most research teams cannot afford. In this paper, we propose PipeTransformer, which leverages automated elastic pipelining for efficient distributed training of Transformer models. In PipeTransformer, we design an adaptive on the fly freeze algorithm that can identify and freeze some layers gradually during training, and an elastic pipelining system that can dynamically allocate resources to train the remaining active layers.\nMore specifically, PipeTransformer  automatically excludes frozen layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on SQuAD and GLUE datasets. Our results show that compared to the state-of-the-art baseline, PipeTransformer attains up to 2.83-fold speedup without losing accuracy. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design. Finally, we have modularized our training system with flexible APIs and made the source code publicly available at https://DistML.ai.",
        "conference": "ICML",
        "中文标题": "PipeTransformer：大规模模型分布式训练的自动化弹性流水线技术",
        "摘要翻译": "Transformer模型的规模正以前所未有的速度增长。自GPT-3（1750亿参数）发布以来，不到一年时间就达到了万亿级参数。训练这样的模型既需要大量的工程努力，也需要巨大的计算资源，这是大多数研究团队无法承担的奢侈品。在本文中，我们提出了PipeTransformer，它利用自动化弹性流水线技术，高效地进行Transformer模型的分布式训练。在PipeTransformer中，我们设计了一种自适应即时冻结算法，可以在训练过程中逐步识别并冻结某些层，以及一个弹性流水线系统，可以动态分配资源来训练剩余的活跃层。更具体地说，PipeTransformer自动将冻结层排除在流水线之外，将活跃层打包到更少的GPU中，并分叉更多副本来增加数据并行宽度。我们使用ImageNet上的Vision Transformer（ViT）以及SQuAD和GLUE数据集上的BERT来评估PipeTransformer。我们的结果表明，与最先进的基线相比，PipeTransformer在不损失准确性的情况下实现了高达2.83倍的加速。我们还提供了各种性能分析，以便更全面地理解我们的算法和系统设计。最后，我们将训练系统模块化，提供了灵活的API，并将源代码公开在https://DistML.ai。",
        "领域": "自然语言处理与视觉结合、大规模模型训练、分布式计算",
        "问题": "如何高效地进行大规模Transformer模型的分布式训练",
        "动机": "解决训练大规模Transformer模型所需的高昂工程和计算资源问题",
        "方法": "采用自动化弹性流水线技术，包括自适应即时冻结算法和动态资源分配的弹性流水线系统",
        "关键词": [
            "自动化弹性流水线",
            "大规模模型训练",
            "分布式计算",
            "Transformer模型",
            "资源优化"
        ],
        "涉及的技术概念": {
            "自适应即时冻结算法": "在训练过程中逐步识别并冻结某些层，以减少计算资源消耗",
            "弹性流水线系统": "动态分配资源来训练剩余的活跃层，提高训练效率",
            "数据并行宽度": "通过分叉更多副本来增加数据并行处理的能力，加速训练过程"
        },
        "success": true
    },
    {
        "order": 837,
        "title": "PixelTransformer: Sample Conditioned Signal Generation",
        "html": "https://ICML.cc//virtual/2021/poster/8635",
        "abstract": "We propose a generative model that can infer a distribution for the underlying spatial signal conditioned on sparse samples e.g. plausible images given a few observed pixels. In contrast to sequential autoregressive generative models, our model allows conditioning on arbitrary samples and can answer distributional queries for any location. We empirically validate our approach across three image datasets and show that we learn to generate diverse and meaningful samples, with the distribution variance reducing given more observed pixels. We also show that our approach is applicable beyond images and can allow generating other types of spatial outputs e.g. polynomials, 3D shapes, and videos.",
        "conference": "ICML",
        "中文标题": "像素变换器：样本条件信号生成",
        "摘要翻译": "我们提出了一种生成模型，该模型能够基于稀疏样本推断出基础空间信号的分布，例如给定少量观察到的像素生成合理的图像。与顺序自回归生成模型不同，我们的模型允许基于任意样本进行条件化，并能够回答任何位置的分布查询。我们在三个图像数据集上实证验证了我们的方法，并展示了我们学会了生成多样且有意义的样本，随着观察到的像素增多，分布方差减小。我们还展示了我们的方法不仅适用于图像，还可以生成其他类型的空间输出，例如多项式、3D形状和视频。",
        "领域": "图像生成、条件生成模型、空间信号处理",
        "问题": "如何在给定稀疏样本的条件下，生成合理的空间信号分布",
        "动机": "解决传统顺序自回归生成模型无法灵活条件化于任意样本和回答分布查询的问题",
        "方法": "提出一种新的生成模型，能够基于稀疏样本推断空间信号的分布，支持任意样本条件化和分布查询",
        "关键词": [
            "生成模型",
            "条件生成",
            "空间信号",
            "像素变换器",
            "分布查询"
        ],
        "涉及的技术概念": {
            "生成模型": "用于基于稀疏样本生成合理的空间信号分布",
            "条件生成": "允许模型基于任意样本进行条件化，生成多样化的输出",
            "分布查询": "模型能够回答关于任何位置信号分布的查询，提供更灵活的应用场景"
        },
        "success": true
    },
    {
        "order": 838,
        "title": "PODS: Policy Optimization via  Differentiable Simulation",
        "html": "https://ICML.cc//virtual/2021/poster/8497",
        "abstract": " Current reinforcement learning (RL) methods use simulation models as simple black-box oracles. In this paper, with the goal of improving the performance exhibited by RL algorithms, we explore a systematic way of leveraging the additional information provided by an emerging class of differentiable simulators. Building on concepts established by Deterministic Policy Gradients (DPG) methods, the neural network policies learned with our approach represent deterministic actions. In a departure from standard methodologies, however, learning these policies does not hinge on approximations of the value function that must be learned concurrently in an actor-critic fashion. Instead, we exploit differentiable simulators to directly compute the analytic gradient of a policy's value function with respect to the actions it outputs. This, in turn, allows us to efficiently perform locally optimal policy improvement iterations. Compared against other state-of-the-art RL methods, we show that with minimal hyper-parameter tuning our approach consistently leads to better asymptotic behavior across a set of payload manipulation tasks that demand a high degree of accuracy and precision. ",
        "conference": "ICML",
        "中文标题": "PODS：通过可微分模拟实现策略优化",
        "摘要翻译": "当前的强化学习（RL）方法将仿真模型视为简单的黑盒预言机。在本文中，为了提高RL算法表现出的性能，我们探索了一种系统性的方法，以利用一类新兴的可微分模拟器提供的额外信息。基于确定性策略梯度（DPG）方法建立的概念，我们方法学习的神经网络策略代表确定性动作。然而，与标准方法不同，学习这些策略并不依赖于必须以一种演员-评论家方式同时学习的价值函数的近似。相反，我们利用可微分模拟器直接计算策略价值函数相对于其输出动作的解析梯度。这反过来使我们能够有效地执行局部最优策略改进迭代。与其他最先进的RL方法相比，我们展示了在需要高度准确性和精确性的一组有效载荷操作任务中，通过最小的超参数调整，我们的方法始终能够带来更好的渐进行为。",
        "领域": "强化学习、机器人操作、策略优化",
        "问题": "如何利用可微分模拟器提供的信息来提高强化学习算法的性能",
        "动机": "探索一种系统性方法，利用可微分模拟器的额外信息，以提高强化学习算法的性能，特别是在需要高精度和准确性的任务中。",
        "方法": "基于确定性策略梯度（DPG）方法，利用可微分模拟器直接计算策略价值函数相对于动作的解析梯度，实现局部最优策略改进。",
        "关键词": [
            "可微分模拟",
            "策略优化",
            "强化学习",
            "确定性策略梯度",
            "机器人操作"
        ],
        "涉及的技术概念": {
            "可微分模拟器": "一类新兴的模拟器，能够提供额外的信息，用于直接计算策略价值函数的解析梯度。",
            "确定性策略梯度（DPG）": "一种策略梯度方法，用于学习代表确定性动作的神经网络策略。",
            "局部最优策略改进": "通过直接计算解析梯度，有效地执行策略的局部最优改进。"
        },
        "success": true
    },
    {
        "order": 839,
        "title": "Pointwise Binary Classification with Pairwise Confidence Comparisons",
        "html": "https://ICML.cc//virtual/2021/poster/9839",
        "abstract": "To alleviate the data requirement for training effective binary classifiers in binary classification, many weakly supervised learning settings have been proposed. Among them, some consider using pairwise but not pointwise labels, when pointwise labels are not accessible due to privacy, confidentiality, or security reasons. However, as a pairwise label denotes whether or not two data points share a pointwise label, it cannot be easily collected if either point is equally likely to be positive or negative. Thus, in this paper, we propose a novel setting called pairwise comparison (Pcomp) classification, where we have only pairs of unlabeled data that we know one is more likely to be positive than the other. Firstly, we give a Pcomp data generation process, derive an unbiased risk estimator (URE) with theoretical guarantee, and further improve URE using correction functions. Secondly, we link Pcomp classification to noisy-label learning to develop a progressive URE and improve it by imposing consistency regularization. Finally, we demonstrate by experiments the effectiveness of our methods, which suggests Pcomp is a valuable and practically useful type of pairwise supervision besides the pairwise label.",
        "conference": "ICML",
        "中文标题": "基于成对置信度比较的逐点二分类",
        "摘要翻译": "为了减轻训练有效二分类器所需的数据要求，许多弱监督学习设置被提出。其中，一些考虑使用成对而非逐点标签，当由于隐私、保密或安全原因无法获取逐点标签时。然而，由于成对标签表示两个数据点是否共享一个逐点标签，如果任一点同样可能为正或负，则不易收集。因此，在本文中，我们提出了一种称为成对比较（Pcomp）分类的新设置，其中我们只有未标记数据的对，我们知道其中一个比另一个更可能为正。首先，我们给出了Pcomp数据生成过程，推导出具有理论保证的无偏风险估计器（URE），并通过校正函数进一步改进URE。其次，我们将Pcomp分类与噪声标签学习联系起来，开发了一个渐进式URE，并通过施加一致性正则化来改进它。最后，我们通过实验证明了我们方法的有效性，这表明Pcomp是一种有价值且实际有用的成对监督类型，除了成对标签之外。",
        "领域": "弱监督学习",
        "问题": "在无法获取逐点标签的情况下，如何利用成对比较信息进行有效的二分类",
        "动机": "解决在隐私、保密或安全限制下，无法直接获取逐点标签时的二分类问题",
        "方法": "提出成对比较（Pcomp）分类设置，开发无偏风险估计器（URE）及其改进方法，结合噪声标签学习和一致性正则化",
        "关键词": [
            "弱监督学习",
            "二分类",
            "成对比较",
            "无偏风险估计器",
            "一致性正则化"
        ],
        "涉及的技术概念": {
            "成对比较（Pcomp）分类": "一种新的弱监督学习设置，利用数据点之间的成对比较信息而非逐点标签进行学习",
            "无偏风险估计器（URE）": "用于从成对比较数据中估计分类器风险的统计方法，具有理论保证",
            "一致性正则化": "一种改进学习算法的方法，通过强制模型在不同扰动下的预测保持一致，以提高泛化能力"
        },
        "success": true
    },
    {
        "order": 840,
        "title": "Poisson-Randomised DirBN: Large Mutation is Needed in Dirichlet Belief Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9425",
        "abstract": "The Dirichlet Belief Network~(DirBN) was recently proposed as a promising deep generative model to learn interpretable deep latent distributions for objects. However, its current representation capability is limited since its latent distributions across different layers is prone to form similar patterns and can thus hardly use multi-layer structure to form flexible distributions. In this work, we propose Poisson-randomised Dirichlet Belief Networks (Pois-DirBN), which allows large mutations for the latent distributions across layers to enlarge the representation capability.  Based on our key idea of inserting Poisson random variables in the layer-wise connection, \nPois-DirBN first introduces a component-wise propagation mechanism to enable latent distributions to have large variations across different layers. Then, we develop a layer-wise Gibbs sampling algorithm to infer the latent distributions, leading to a larger number of effective layers compared to DirBN. In addition, we integrate out latent distributions and form a multi-stochastic deep integer network, which provides an alternative view on Pois-DirBN. We apply Pois-DirBN to relational modelling and validate its effectiveness through improved link prediction performance and more interpretable latent distribution visualisations. The code can be downloaded at https://github.com/xuhuifan/Pois_DirBN.",
        "conference": "ICML",
        "中文标题": "泊松随机化DirBN：Dirichlet信念网络需要大变异",
        "摘要翻译": "Dirichlet信念网络（DirBN）最近被提出作为一种有前景的深度生成模型，用于学习对象的可解释深度潜在分布。然而，其当前的表示能力有限，因为其不同层间的潜在分布易于形成相似的模式，因此难以利用多层结构形成灵活的分布。在这项工作中，我们提出了泊松随机化的Dirichlet信念网络（Pois-DirBN），它允许层间潜在分布的大变异以扩大表示能力。基于我们在层间连接中插入泊松随机变量的关键思想，Pois-DirBN首先引入了一种组件级传播机制，使潜在分布能够在不同层间具有大的变化。然后，我们开发了一种层级的Gibbs采样算法来推断潜在分布，与DirBN相比，导致了更多有效层的数量。此外，我们整合了潜在分布并形成了一个多随机深度整数网络，这为Pois-DirBN提供了另一种视角。我们将Pois-DirBN应用于关系建模，并通过改进的链接预测性能和更可解释的潜在分布可视化验证了其有效性。代码可以从https://github.com/xuhuifan/Pois_DirBN下载。",
        "领域": "深度生成模型、关系建模、潜在分布学习",
        "问题": "DirBN的潜在分布在多层间易于形成相似模式，限制了其表示能力和灵活性。",
        "动机": "通过引入大变异来扩大DirBN的表示能力，使其能够利用多层结构形成更灵活的潜在分布。",
        "方法": "在DirBN中插入泊松随机变量，引入组件级传播机制和层级Gibbs采样算法，整合潜在分布形成多随机深度整数网络。",
        "关键词": [
            "泊松随机化",
            "Dirichlet信念网络",
            "深度生成模型",
            "关系建模",
            "潜在分布"
        ],
        "涉及的技术概念": {
            "泊松随机变量": "在层间连接中插入，用于引入大变异，扩大潜在分布的表示能力。",
            "组件级传播机制": "使潜在分布能够在不同层间具有大的变化，增强模型的灵活性。",
            "层级Gibbs采样算法": "用于推断潜在分布，与DirBN相比，能够产生更多有效层。"
        },
        "success": true
    },
    {
        "order": 841,
        "title": "Policy Analysis using Synthetic Controls in Continuous-Time",
        "html": "https://ICML.cc//virtual/2021/poster/8675",
        "abstract": "Counterfactual estimation using synthetic controls is one of the most successful recent methodological developments in causal inference. Despite its popularity, the current description only considers time series aligned across units and synthetic controls expressed as linear combinations of observed control units. We propose a continuous-time alternative that models the latent counterfactual path explicitly using the formalism of controlled differential equations. This model is directly applicable to the general setting of irregularly-aligned multivariate time series and may be optimized in rich function spaces -- thereby improving on some limitations of existing approaches.",
        "conference": "ICML",
        "中文标题": "使用连续时间合成控制的政策分析",
        "摘要翻译": "使用合成控制进行反事实估计是因果推断领域最近方法论发展中最为成功的之一。尽管其流行，当前的描述仅考虑跨单位对齐的时间序列和表示为观察到的控制单位线性组合的合成控制。我们提出了一种连续时间的替代方案，该方案使用控制微分方程的形式明确地模拟潜在的反事实路径。该模型直接适用于不规则对齐的多元时间序列的一般设置，并且可以在丰富的函数空间中进行优化——从而改进了现有方法的一些限制。",
        "领域": "因果推断、时间序列分析、政策评估",
        "问题": "现有合成控制方法在处理不规则对齐的多元时间序列时的局限性",
        "动机": "改进现有合成控制方法，使其能够更灵活地处理不规则对齐的时间序列数据，提高反事实估计的准确性和适用性",
        "方法": "提出了一种基于控制微分方程的连续时间模型，用于明确模拟潜在的反事实路径，适用于不规则对齐的多元时间序列，并在丰富的函数空间中进行优化",
        "关键词": [
            "合成控制",
            "反事实估计",
            "连续时间模型",
            "控制微分方程",
            "不规则时间序列"
        ],
        "涉及的技术概念": {
            "合成控制": "一种通过观察到的控制单位的线性组合来估计反事实结果的方法",
            "控制微分方程": "用于在连续时间框架下明确模拟潜在的反事实路径的数学工具",
            "不规则时间序列": "指时间点不对齐或采样间隔不一致的时间序列数据，本方法专门设计用于处理此类数据"
        },
        "success": true
    },
    {
        "order": 842,
        "title": "Policy Caches with Successor Features",
        "html": "https://ICML.cc//virtual/2021/poster/9347",
        "abstract": "Transfer in reinforcement learning is based on the idea that it is possible to use what is learned in one task to improve the learning process in another task. For transfer between tasks which share transition dynamics but differ in reward function, successor features have been shown to be a useful representation which allows for efficient computation of action-value functions for previously-learned policies in new tasks. These functions induce policies in the new tasks, so an agent may not need to learn a new policy for each new task it encounters, especially if it is allowed some amount of suboptimality in those tasks. We present new bounds for the performance of optimal policies in a new task, as well as an approach to use these bounds to decide, when presented with a new task, whether to use cached policies or learn a new policy.",
        "conference": "ICML",
        "中文标题": "策略缓存与后继特征",
        "摘要翻译": "强化学习中的迁移基于这样一种理念：可以利用在一个任务中学到的知识来改善另一个任务的学习过程。对于共享转移动态但奖励函数不同的任务之间的迁移，后继特征已被证明是一种有用的表示方法，它允许在新任务中高效计算先前学习策略的动作价值函数。这些函数在新任务中诱导出策略，因此智能体可能不需要为遇到的每个新任务学习新策略，特别是在允许这些任务中存在一定程度的次优性时。我们提出了新任务中最优策略性能的新界限，以及一种利用这些界限来决定在面对新任务时是使用缓存策略还是学习新策略的方法。",
        "领域": "强化学习迁移",
        "问题": "如何在共享转移动态但奖励函数不同的任务之间进行有效的策略迁移",
        "动机": "利用已学习策略的后继特征来避免为每个新任务重新学习策略，提高学习效率",
        "方法": "提出新任务中最优策略性能的界限，并基于这些界限决定使用缓存策略或学习新策略",
        "关键词": [
            "强化学习迁移",
            "后继特征",
            "策略缓存",
            "动作价值函数",
            "次优性"
        ],
        "涉及的技术概念": {
            "后继特征": "用于表示和计算在不同奖励函数任务中先前学习策略的动作价值函数",
            "动作价值函数": "在新任务中评估和选择动作的函数，基于后继特征计算",
            "策略缓存": "存储先前学习策略的机制，用于在新任务中快速部署而不需要重新学习"
        },
        "success": true
    },
    {
        "order": 843,
        "title": "Policy Gradient Bayesian Robust Optimization for Imitation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9559",
        "abstract": "The difficulty in specifying rewards for many real-world problems has led to an increased focus on learning rewards from human feedback, such as demonstrations. However, there are often many different reward functions that explain the human feedback, leaving agents with uncertainty over what the true reward function is. While most policy optimization approaches handle this uncertainty by optimizing for expected performance, many applications demand risk-averse behavior. We derive a novel policy gradient-style robust optimization approach, PG-BROIL, that optimizes a soft-robust objective that balances expected performance and risk. To the best of our knowledge, PG-BROIL is the first policy optimization algorithm robust to a distribution of reward hypotheses which can scale to continuous MDPs. Results suggest that PG-BROIL can produce a family of behaviors ranging from risk-neutral to risk-averse and outperforms state-of-the-art imitation learning algorithms when learning from ambiguous demonstrations by hedging against uncertainty, rather than seeking to uniquely identify the demonstrator's reward function. ",
        "conference": "ICML",
        "中文标题": "模仿学习中的策略梯度贝叶斯鲁棒优化",
        "摘要翻译": "由于为许多现实世界问题指定奖励的困难，人们越来越关注从人类反馈（如演示）中学习奖励。然而，往往有许多不同的奖励函数可以解释人类反馈，使得代理对真实奖励函数存在不确定性。虽然大多数策略优化方法通过优化期望性能来处理这种不确定性，但许多应用需要风险规避行为。我们提出了一种新颖的策略梯度式鲁棒优化方法PG-BROIL，它优化了一个软鲁棒目标，平衡了期望性能和风险。据我们所知，PG-BROIL是第一个能够扩展到连续MDPs的、对奖励假设分布具有鲁棒性的策略优化算法。结果表明，PG-BROIL可以产生从风险中性到风险规避的一系列行为，并且通过学习模糊演示时通过对冲不确定性而不是试图唯一识别演示者的奖励函数，优于最先进的模仿学习算法。",
        "领域": "模仿学习",
        "问题": "在模仿学习中处理奖励函数的不确定性和实现风险规避行为",
        "动机": "解决在模仿学习中由于奖励函数不确定性导致的风险管理问题，提供一种能够平衡期望性能和风险的方法",
        "方法": "提出了一种新颖的策略梯度式鲁棒优化方法PG-BROIL，优化软鲁棒目标，平衡期望性能和风险",
        "关键词": [
            "模仿学习",
            "策略梯度",
            "鲁棒优化",
            "风险规避",
            "奖励学习"
        ],
        "涉及的技术概念": {
            "策略梯度": "用于优化策略的参数，使得策略能够根据奖励信号进行改进",
            "贝叶斯鲁棒优化": "结合贝叶斯方法处理不确定性，优化策略以在不确定的奖励函数下表现鲁棒",
            "软鲁棒目标": "一种平衡期望性能和风险的目标函数，允许在不确定环境中实现风险规避行为"
        },
        "success": true
    },
    {
        "order": 844,
        "title": "Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8571",
        "abstract": "Progress in deep reinforcement learning (RL) research is largely enabled by benchmark task environments. However, analyzing the nature of those environments is often overlooked. In particular, we still do not have agreeable ways to measure the difficulty or solvability of a task, given that each has fundamentally different actions, observations, dynamics, rewards, and can be tackled with diverse RL algorithms. In this work, we propose policy information capacity (PIC) -- the mutual information between policy parameters and episodic return -- and policy-optimal information capacity (POIC) -- between policy parameters and episodic optimality -- as two environment-agnostic, algorithm-agnostic quantitative metrics for task difficulty. Evaluating our metrics across toy environments as well as continuous control benchmark tasks from OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that these information-theoretic metrics have higher correlations with normalized task solvability scores than a variety of alternatives. Lastly, we show that these metrics can also be used for fast and compute-efficient optimizations of key design parameters such as reward shaping, policy architectures, and MDP properties for better solvability by RL algorithms without ever running full RL experiments.",
        "conference": "ICML",
        "中文标题": "策略信息容量：深度强化学习中任务复杂性的信息论度量",
        "摘要翻译": "深度强化学习（RL）研究的进展很大程度上得益于基准任务环境。然而，对这些环境本质的分析往往被忽视。特别是，我们仍然没有一致的方法来衡量任务的难度或可解性，因为每个任务在动作、观察、动态、奖励方面都有根本的不同，并且可以用多种RL算法来解决。在这项工作中，我们提出了策略信息容量（PIC）——策略参数与回合回报之间的互信息，以及策略最优信息容量（POIC）——策略参数与回合最优性之间的互信息，作为两种与环境无关、算法无关的任务难度定量度量。通过在玩具环境以及来自OpenAI Gym和DeepMind Control Suite的连续控制基准任务上评估我们的度量，我们经验性地证明了这些信息论度量与归一化任务可解性分数的相关性高于多种替代方案。最后，我们展示了这些度量还可以用于快速且计算效率高地优化关键设计参数，如奖励塑造、策略架构和MDP属性，以提高RL算法的可解性，而无需运行完整的RL实验。",
        "领域": "深度强化学习、任务复杂度评估、算法优化",
        "问题": "如何量化评估深度强化学习任务的复杂性和可解性",
        "动机": "当前缺乏一致的方法来衡量不同RL任务的难度或可解性，这限制了任务设计和算法优化的效率",
        "方法": "提出策略信息容量（PIC）和策略最优信息容量（POIC）作为任务难度的定量度量，并通过实验验证其有效性",
        "关键词": [
            "策略信息容量",
            "任务复杂度",
            "互信息",
            "深度强化学习",
            "算法优化"
        ],
        "涉及的技术概念": {
            "策略信息容量（PIC）": "衡量策略参数与回合回报之间互信息的指标，用于评估任务难度",
            "策略最优信息容量（POIC）": "衡量策略参数与回合最优性之间互信息的指标，进一步细化任务难度的评估",
            "互信息": "用于量化两个变量之间统计依赖性的信息论概念，在本研究中用于连接策略参数与任务表现"
        },
        "success": true
    },
    {
        "order": 845,
        "title": "Poolingformer: Long Document Modeling with Pooling Attention",
        "html": "https://ICML.cc//virtual/2021/poster/9603",
        "abstract": "In this paper, we introduce a two-level attention schema, Poolingformer, for long document modeling. Its first level uses a smaller sliding window pattern to aggregate information from neighbors. Its second level employs a larger window to increase receptive fields with pooling attention to reduce both computational cost and memory consumption. We first evaluate Poolingformer on two long sequence QA tasks: the monolingual NQ and the multilingual TyDi QA. Experimental results show that Poolingformer sits atop three official leaderboards measured by F1, outperforming previous state-of-the-art models by 1.9 points (79.8 vs. 77.9) on NQ long answer, 1.9 points (79.5 vs. 77.6) on TyDi QA passage answer, and 1.6 points (67.6 vs. 66.0) on TyDi QA minimal answer. We further evaluate Poolingformer on a long sequence summarization task. Experimental results on the arXiv benchmark continue to demonstrate its superior performance. ",
        "conference": "ICML",
        "中文标题": "Poolingformer：使用池化注意力进行长文档建模",
        "摘要翻译": "本文介绍了一种用于长文档建模的两级注意力机制——Poolingformer。其第一级使用较小的滑动窗口模式来聚合邻居信息。第二级采用较大的窗口，通过池化注意力来增加感受野，同时减少计算成本和内存消耗。我们首先在两个长序列问答任务上评估Poolingformer：单语NQ和多语言TyDi QA。实验结果显示，Poolingformer在F1分数上位居三个官方排行榜首位，NQ长答案上比之前的最优模型高出1.9分（79.8对77.9），TyDi QA段落答案上高出1.9分（79.5对77.6），TyDi QA最小答案上高出1.6分（67.6对66.0）。我们进一步在长序列摘要任务上评估Poolingformer。在arXiv基准上的实验结果继续展示了其卓越性能。",
        "领域": "自然语言处理与视觉结合、长文档理解、问答系统",
        "问题": "解决长文档建模中的计算效率和内存消耗问题",
        "动机": "提高长文档处理任务的性能，同时降低计算资源需求",
        "方法": "采用两级注意力机制，结合滑动窗口和池化注意力来优化长文档的处理",
        "关键词": [
            "Poolingformer",
            "长文档建模",
            "池化注意力",
            "问答系统",
            "摘要任务"
        ],
        "涉及的技术概念": {
            "滑动窗口模式": "用于第一级注意力，聚合邻居信息，提高局部特征提取效率",
            "池化注意力": "用于第二级注意力，扩大感受野，减少计算和内存消耗",
            "F1分数": "评估模型性能的指标，用于衡量问答任务的准确性和召回率"
        },
        "success": true
    },
    {
        "order": 846,
        "title": "PopSkipJump: Decision-Based Attack for Probabilistic Classifiers",
        "html": "https://ICML.cc//virtual/2021/poster/9475",
        "abstract": "Most current classifiers are vulnerable to adversarial examples, small input perturbations that change the classification output. Many existing attack algorithms cover various settings, from white-box to black-box classifiers, but usually assume that the answers are deterministic and often fail when they are not. We therefore propose a new adversarial decision-based attack specifically designed for classifiers with probabilistic outputs. It is based on the HopSkipJump attack by Chen et al. (2019), a strong and query efficient decision-based attack originally designed for deterministic classifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries to maintain HopSkipJump’s original output quality across various noise levels, while converging to its query efficiency as the noise level decreases. We test our attack on various noise models, including state-of-the-art off-the-shelf randomized defenses, and show that they offer almost no extra robustness to decision-based attacks. Code is available at https://github.com/cjsg/PopSkipJump.",
        "conference": "ICML",
        "中文标题": "PopSkipJump：针对概率分类器的基于决策的攻击",
        "摘要翻译": "当前大多数分类器都容易受到对抗性示例的影响，这些小的输入扰动会改变分类输出。许多现有的攻击算法涵盖了从白盒到黑盒分类器的各种设置，但通常假设答案是确定性的，当答案不是确定性时常常失败。因此，我们提出了一种新的对抗性基于决策的攻击，专门为具有概率输出的分类器设计。它基于Chen等人（2019年）的HopSkipJump攻击，这是一种强大且查询效率高的基于决策的攻击，最初为确定性分类器设计。我们的P(robabilisticH)opSkipJump攻击调整其查询量，以在各种噪声水平下保持HopSkipJump的原始输出质量，同时在噪声水平降低时收敛到其查询效率。我们在各种噪声模型上测试了我们的攻击，包括最先进的现成随机防御，并显示它们对基于决策的攻击几乎没有提供额外的鲁棒性。代码可在https://github.com/cjsg/PopSkipJump获取。",
        "领域": "对抗性攻击、机器学习安全、概率分类器",
        "问题": "解决概率分类器在对抗性攻击下的脆弱性问题",
        "动机": "现有攻击算法通常假设分类器输出是确定性的，这在面对概率输出分类器时效果不佳，因此需要专门针对概率分类器的攻击方法",
        "方法": "基于HopSkipJump攻击方法，调整查询量以适应概率分类器的输出特性，保持攻击效果和查询效率",
        "关键词": [
            "对抗性攻击",
            "概率分类器",
            "HopSkipJump",
            "机器学习安全",
            "查询效率"
        ],
        "涉及的技术概念": {
            "对抗性示例": "小的输入扰动，能够改变分类器的输出，用于测试和攻击分类器的鲁棒性",
            "HopSkipJump攻击": "一种强大且查询效率高的基于决策的攻击方法，最初为确定性分类器设计",
            "概率分类器": "输出为概率分布而非确定性结果的分类器，本文攻击方法专门针对此类分类器设计"
        },
        "success": true
    },
    {
        "order": 847,
        "title": "Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/18433",
        "abstract": "It is well-known that stochastic gradient noise (SGN) acts as implicit regularization for deep learning and is essentially important for both optimization and generalization of deep networks. Some works attempted to artificially simulate SGN by injecting random noise to improve deep learning. However, it turned out that the injected simple random noise cannot work as well as SGN, which is anisotropic and parameter-dependent. For simulating SGN at low computational costs and without changing the learning rate or batch size, we propose the Positive-Negative Momentum (PNM) approach that is a powerful alternative to conventional Momentum in classic optimizers. The introduced PNM method maintains two approximate independent momentum terms. Then, we can control the magnitude of SGN explicitly by adjusting the momentum difference. We theoretically prove the convergence guarantee and the generalization advantage of PNM over Stochastic Gradient Descent (SGD). By incorporating PNM into the two conventional optimizers, SGD with Momentum and Adam, our extensive experiments empirically verified the significant advantage of the PNM-based variants over the corresponding conventional Momentum-based optimizers. Code: \\url{https://github.com/zeke-xie/Positive-Negative-Momentum}.",
        "conference": "ICML",
        "中文标题": "正负动量：操纵随机梯度噪声以提升泛化能力",
        "摘要翻译": "众所周知，随机梯度噪声（SGN）在深度学习中充当隐式正则化角色，对深度网络的优化和泛化至关重要。一些研究尝试通过注入随机噪声来人工模拟SGN以改善深度学习效果。然而，事实证明，注入的简单随机噪声无法像SGN那样有效，因为SGN是各向异性且依赖于参数的。为了以低计算成本模拟SGN，并且不改变学习率或批量大小，我们提出了正负动量（PNM）方法，这是传统动量在经典优化器中的强大替代方案。引入的PNM方法保持两个近似独立的动量项。然后，我们可以通过调整动量差来显式控制SGN的幅度。我们从理论上证明了PNM相对于随机梯度下降（SGD）的收敛保证和泛化优势。通过将PNM整合到两种传统优化器——带动量的SGD和Adam中，我们的大量实验经验性地验证了基于PNM的变体相对于相应传统基于动量的优化器的显著优势。代码：https://github.com/zeke-xie/Positive-Negative-Momentum。",
        "领域": "深度学习优化、随机梯度下降、自适应优化算法",
        "问题": "如何有效模拟随机梯度噪声（SGN）以提升深度学习模型的泛化能力，而不增加计算成本或改变学习参数",
        "动机": "现有的简单随机噪声注入方法无法有效模拟SGN的各向异性和参数依赖性，限制了深度学习模型的优化和泛化效果",
        "方法": "提出正负动量（PNM）方法，通过维护两个近似独立的动量项并调整其差异来显式控制SGN的幅度，从而在不改变学习率或批量大小的条件下模拟SGN",
        "关键词": [
            "正负动量",
            "随机梯度噪声",
            "深度学习优化",
            "泛化能力",
            "动量调整"
        ],
        "涉及的技术概念": {
            "随机梯度噪声（SGN）": "在深度学习中充当隐式正则化角色，对优化和泛化至关重要",
            "正负动量（PNM）": "通过调整两个近似独立动量项的差异来显式控制SGN的幅度，是传统动量的替代方案",
            "泛化优势": "PNM方法相对于传统SGD在理论上有更好的泛化能力，实验也验证了这一点"
        },
        "success": true
    },
    {
        "order": 848,
        "title": "Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/9629",
        "abstract": "It is well-known that stochastic gradient noise (SGN) acts as implicit regularization for deep learning and is essentially important for both optimization and generalization of deep networks. Some works attempted to artificially simulate SGN by injecting random noise to improve deep learning. However, it turned out that the injected simple random noise cannot work as well as SGN, which is anisotropic and parameter-dependent. For simulating SGN at low computational costs and without changing the learning rate or batch size, we propose the Positive-Negative Momentum (PNM) approach that is a powerful alternative to conventional Momentum in classic optimizers. The introduced PNM method maintains two approximate independent momentum terms. Then, we can control the magnitude of SGN explicitly by adjusting the momentum difference. We theoretically prove the convergence guarantee and the generalization advantage of PNM over Stochastic Gradient Descent (SGD). By incorporating PNM into the two conventional optimizers, SGD with Momentum and Adam, our extensive experiments empirically verified the significant advantage of the PNM-based variants over the corresponding conventional Momentum-based optimizers. Code: \\url{https://github.com/zeke-xie/Positive-Negative-Momentum}.",
        "conference": "ICML",
        "中文标题": "正负动量：操纵随机梯度噪声以提升泛化能力",
        "摘要翻译": "众所周知，随机梯度噪声（SGN）在深度学习中充当隐式正则化，对深度网络的优化和泛化至关重要。一些研究尝试通过注入随机噪声来人工模拟SGN以改善深度学习。然而，事实证明，注入的简单随机噪声无法像SGN那样有效，因为SGN是各向异性且依赖于参数的。为了以低计算成本模拟SGN，并且不改变学习率或批量大小，我们提出了正负动量（PNM）方法，这是传统动量在经典优化器中的强大替代方案。引入的PNM方法保持两个近似独立的动量项。然后，我们可以通过调整动量差来显式控制SGN的大小。我们从理论上证明了PNM相对于随机梯度下降（SGD）的收敛保证和泛化优势。通过将PNM整合到两种传统优化器——带动量的SGD和Adam中，我们的大量实验经验性地验证了基于PNM的变体相对于相应传统基于动量的优化器的显著优势。代码：https://github.com/zeke-xie/Positive-Negative-Momentum。",
        "领域": "深度学习优化、随机梯度下降、自适应优化算法",
        "问题": "如何有效模拟随机梯度噪声（SGN）以提升深度学习模型的泛化能力，而不增加计算成本或改变学习参数",
        "动机": "现有的简单随机噪声注入方法无法有效模拟SGN的各向异性和参数依赖性，限制了深度学习模型的优化和泛化能力",
        "方法": "提出正负动量（PNM）方法，通过维护两个近似独立的动量项并调整其差异来显式控制SGN的大小，从而在不改变学习率或批量大小的条件下模拟SGN",
        "关键词": [
            "正负动量",
            "随机梯度噪声",
            "深度学习优化",
            "泛化能力",
            "动量调整"
        ],
        "涉及的技术概念": {
            "随机梯度噪声（SGN）": "在深度学习中充当隐式正则化，对优化和泛化至关重要",
            "正负动量（PNM）": "通过调整两个独立动量项的差异来显式控制SGN的大小，替代传统动量方法",
            "泛化能力": "指模型在未见数据上的表现能力，PNM通过模拟SGN提升模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 849,
        "title": "Posterior Value Functions: Hindsight Baselines for Policy Gradient Methods",
        "html": "https://ICML.cc//virtual/2021/poster/9783",
        "abstract": "Hindsight allows reinforcement learning agents to leverage new observations to make inferences about earlier states and transitions. In this paper, we exploit the idea of hindsight and introduce posterior value functions. Posterior value functions are computed by inferring the posterior distribution over hidden components of the state in previous timesteps and can be used to construct novel unbiased baselines for policy gradient methods. Importantly, we prove that these baselines reduce (and never increase) the variance of policy gradient estimators compared to traditional state value functions. While the posterior value function is motivated by partial observability, we extend these results to arbitrary stochastic MDPs by showing that hindsight-capable agents can model stochasticity in the environment as a special case of partial observability. Finally, we introduce a pair of methods for learning posterior value functions and prove their convergence.",
        "conference": "ICML",
        "中文标题": "后验价值函数：策略梯度方法的事后基线",
        "摘要翻译": "事后观察允许强化学习代理利用新的观察结果对早期状态和转换进行推断。在本文中，我们利用事后观察的思想，引入了后验价值函数。后验价值函数通过推断先前时间步中状态的隐藏组件的后验分布来计算，并可用于为策略梯度方法构建新颖的无偏基线。重要的是，我们证明了与传统状态价值函数相比，这些基线减少（且从不增加）策略梯度估计器的方差。虽然后验价值函数的动机来自于部分可观测性，但我们通过展示具有事后观察能力的代理可以将环境中的随机性建模为部分可观测性的特例，将这些结果扩展到任意随机MDP。最后，我们介绍了一对学习后验价值函数的方法，并证明了它们的收敛性。",
        "领域": "强化学习、策略优化、部分可观测马尔可夫决策过程",
        "问题": "如何在策略梯度方法中构建无偏基线以减少估计器的方差",
        "动机": "利用事后观察的思想，通过后验价值函数减少策略梯度估计器的方差，提高学习效率",
        "方法": "引入后验价值函数，通过推断先前时间步中状态的隐藏组件的后验分布来构建无偏基线，并证明其收敛性",
        "关键词": [
            "后验价值函数",
            "策略梯度方法",
            "无偏基线",
            "方差减少",
            "强化学习"
        ],
        "涉及的技术概念": {
            "后验价值函数": "通过推断先前时间步中状态的隐藏组件的后验分布来计算，用于构建无偏基线",
            "策略梯度方法": "一种通过直接优化策略来学习最优策略的强化学习方法",
            "部分可观测马尔可夫决策过程": "一种决策过程模型，其中代理无法完全观察到环境的状态，需要通过观察来推断"
        },
        "success": true
    },
    {
        "order": 850,
        "title": "Post-selection inference with HSIC-Lasso",
        "html": "https://ICML.cc//virtual/2021/poster/9689",
        "abstract": "Detecting influential features in non-linear and/or high-dimensional data is a challenging and increasingly important task in machine learning. Variable selection methods have thus been gaining much attention as well as post-selection inference. Indeed, the selected features can be significantly flawed when the selection procedure is not accounted for. We  propose a selective inference procedure using the so-called model-free 'HSIC-Lasso' based on the framework of truncated Gaussians combined with the polyhedral lemma. We then develop an algorithm, which allows for low computational costs and provides a selection of the regularisation parameter. The performance of our method is illustrated by both artificial and real-world data based experiments, which emphasise a tight control of the type-I error, even for small sample sizes.",
        "conference": "ICML",
        "中文标题": "后选择推断与HSIC-Lasso",
        "摘要翻译": "在非线性及/或高维数据中检测有影响的特征是一项具有挑战性且日益重要的机器学习任务。因此，变量选择方法以及后选择推断受到了广泛关注。实际上，当不考虑选择过程时，所选特征可能存在显著缺陷。我们提出了一种选择性推断程序，使用所谓的模型无关的'HSIC-Lasso'，基于截断高斯框架与多面体引理相结合。随后，我们开发了一种算法，该算法允许较低的计算成本，并提供了正则化参数的选择。通过基于人工和真实世界数据的实验，我们方法的性能得到了展示，这些实验强调了即使在样本量较小的情况下，也能严格控制I型错误。",
        "领域": "特征选择、非线性数据分析、高维统计",
        "问题": "在非线性及/或高维数据中准确检测有影响的特征",
        "动机": "解决在不考虑选择过程时，所选特征可能存在显著缺陷的问题",
        "方法": "提出了一种基于HSIC-Lasso的选择性推断程序，结合截断高斯框架与多面体引理，开发了一种低计算成本的算法",
        "关键词": [
            "HSIC-Lasso",
            "后选择推断",
            "特征选择",
            "非线性数据",
            "高维统计"
        ],
        "涉及的技术概念": {
            "HSIC-Lasso": "一种模型无关的特征选择方法，用于在非线性及高维数据中检测有影响的特征",
            "截断高斯框架": "用于构建选择性推断程序的统计框架，确保推断的准确性",
            "多面体引理": "在选择性推断中用于控制I型错误的数学工具"
        },
        "success": true
    },
    {
        "order": 851,
        "title": "Practical and Private (Deep) Learning Without Sampling or Shuffling",
        "html": "https://ICML.cc//virtual/2021/poster/9697",
        "abstract": "We consider training models with differential privacy (DP) using mini-batch gradients.\nThe existing state-of-the-art, Differentially Private Stochastic Gradient Descent (DP-SGD), requires \\emph{privacy amplification by sampling or shuffling} to obtain the best privacy/accuracy/computation trade-offs. Unfortunately, the precise requirements on exact sampling and shuffling can be hard to obtain in important practical scenarios, particularly federated learning (FL).\nWe design and analyze a DP variant of  Follow-The-Regularized-Leader (DP-FTRL) that compares favorably (both theoretically and empirically) to amplified DP-SGD, while allowing for much more flexible data access patterns. DP-FTRL does not use any form of privacy amplification.",
        "conference": "ICML",
        "中文标题": "无需采样或洗牌的实用且私密（深度）学习",
        "摘要翻译": "我们考虑使用小批量梯度训练具有差分隐私（DP）的模型。现有的最先进技术，差分隐私随机梯度下降（DP-SGD），需要通过采样或洗牌进行隐私放大，以获得最佳的隐私/准确性/计算权衡。不幸的是，在重要的实际场景中，特别是联邦学习（FL），对精确采样和洗牌的确切要求可能难以满足。我们设计并分析了跟随正则化领导者（DP-FTRL）的DP变体，它在理论和实证上都优于放大的DP-SGD，同时允许更灵活的数据访问模式。DP-FTRL不使用任何形式的隐私放大。",
        "领域": "联邦学习, 差分隐私, 深度学习优化",
        "问题": "在联邦学习等实际场景中，如何在不依赖精确采样或洗牌的情况下，实现差分隐私下的高效模型训练。",
        "动机": "解决DP-SGD在联邦学习等场景中因采样和洗牌要求难以满足而导致的隐私保护和模型性能之间的权衡问题。",
        "方法": "设计并分析了一种新的差分隐私跟随正则化领导者（DP-FTRL）方法，该方法不依赖隐私放大技术，提供了更灵活的数据访问模式。",
        "关键词": [
            "差分隐私",
            "联邦学习",
            "深度学习优化",
            "DP-FTRL",
            "隐私保护"
        ],
        "涉及的技术概念": {
            "差分隐私（DP）": "一种隐私保护框架，确保模型训练过程中个体数据的贡献不会被泄露。",
            "跟随正则化领导者（FTRL）": "一种在线学习算法，用于优化模型参数，DP-FTRL是其差分隐私变体。",
            "隐私放大": "通过采样或洗牌等技术增强差分隐私保护效果的方法，DP-FTRL不依赖此技术。"
        },
        "success": true
    },
    {
        "order": 852,
        "title": "Prediction-Centric Learning of Independent Cascade Dynamics from Partial Observations",
        "html": "https://ICML.cc//virtual/2021/poster/10525",
        "abstract": "Spreading processes play an increasingly important role in modeling for diffusion networks, information propagation, marketing and opinion setting. We address the problem of learning of a spreading model such that the predictions generated from this model are accurate and could be subsequently used for the optimization, and control of diffusion dynamics. We focus on a challenging setting where full observations of the dynamics are not available, and standard approaches such as maximum likelihood quickly become intractable for large network instances. We introduce a computationally efficient algorithm, based on a scalable dynamic message-passing approach, which is able to learn parameters of the effective spreading model given only limited information on the activation times of nodes in the network. The popular Independent Cascade model is used to illustrate our approach. We show that tractable inference from the learned model generates a better prediction of marginal probabilities compared to the original model. We develop a systematic procedure for learning a mixture of models which further improves the prediction quality.",
        "conference": "ICML",
        "中文标题": "基于部分观测的独立级联动态预测中心学习",
        "摘要翻译": "传播过程在扩散网络建模、信息传播、市场营销和意见设定中扮演着越来越重要的角色。我们解决了学习传播模型的问题，使得从该模型生成的预测准确，并可以随后用于扩散动态的优化和控制。我们专注于一个具有挑战性的环境，其中动态的完整观测不可用，且对于大型网络实例，最大似然等标准方法很快变得难以处理。我们引入了一种基于可扩展动态消息传递方法的计算高效算法，该算法能够在仅给定网络中节点激活时间的有限信息的情况下，学习有效传播模型的参数。流行的独立级联模型被用来说明我们的方法。我们展示了从学习模型中进行的可处理推理比原始模型生成了更好的边际概率预测。我们开发了一个系统程序来学习模型的混合，这进一步提高了预测质量。",
        "领域": "信息传播建模、网络动力学、扩散过程优化",
        "问题": "如何在部分观测条件下准确学习扩散网络的动态模型以优化和控制信息传播。",
        "动机": "解决在缺乏完整观测数据的情况下，传统方法难以处理大型网络扩散动态模型学习的问题。",
        "方法": "采用基于动态消息传递的可扩展算法学习有效传播模型参数，并通过模型混合提高预测质量。",
        "关键词": [
            "独立级联模型",
            "动态消息传递",
            "部分观测学习",
            "扩散动态优化",
            "模型混合"
        ],
        "涉及的技术概念": {
            "独立级联模型": "用于模拟信息或影响力在网络中传播的模型，本文中作为基础模型说明方法。",
            "动态消息传递": "一种可扩展的计算方法，用于在部分观测条件下高效学习模型参数。",
            "模型混合": "通过结合多个模型来提高预测准确性的系统方法。"
        },
        "success": true
    },
    {
        "order": 853,
        "title": "Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers",
        "html": "https://ICML.cc//virtual/2021/poster/9759",
        "abstract": "We propose Predict then Interpolate (PI), a simple algorithm for learning correlations that are stable across environments. The algorithm follows from the intuition that when using a classifier trained on one environment to make predictions on examples from another environment, its mistakes are informative as to which correlations are unstable. In this work, we prove that by interpolating the distributions of the correct predictions and the wrong predictions, we can uncover an oracle distribution where the unstable correlation vanishes. Since the oracle interpolation coefficients are not accessible, we use group distributionally robust optimization to minimize the worst-case risk across all such interpolations. We evaluate our method on both text classification and image classification. Empirical results demonstrate that our algorithm is able to learn robust classifiers (outperforms IRM by 23.85% on synthetic environments and 12.41% on natural environments). Our code and data are available at https://github.com/YujiaBao/ Predict-then-Interpolate.",
        "conference": "ICML",
        "中文标题": "预测后插值：一种学习稳定分类器的简单算法",
        "摘要翻译": "我们提出了预测后插值（PI），这是一种学习跨环境稳定相关性的简单算法。该算法基于以下直觉：当使用在一个环境中训练的分类器对另一个环境的示例进行预测时，其错误信息可以揭示哪些相关性是不稳定的。在这项工作中，我们证明通过插值正确预测和错误预测的分布，我们可以发现一个不稳定相关性消失的预言分布。由于无法访问预言插值系数，我们使用组分布鲁棒优化来最小化所有此类插值的最坏情况风险。我们在文本分类和图像分类上评估了我们的方法。实证结果表明，我们的算法能够学习到鲁棒的分类器（在合成环境中比IRM高出23.85%，在自然环境中高出12.41%）。我们的代码和数据可在https://github.com/YujiaBao/Predict-then-Interpolate获取。",
        "领域": "文本分类, 图像分类, 鲁棒学习",
        "问题": "学习跨环境稳定的相关性",
        "动机": "解决分类器在不同环境下预测时因不稳定相关性导致的性能下降问题",
        "方法": "通过预测后插值算法和组分布鲁棒优化，最小化最坏情况风险",
        "关键词": [
            "预测后插值",
            "组分布鲁棒优化",
            "稳定分类器",
            "跨环境学习",
            "鲁棒学习"
        ],
        "涉及的技术概念": {
            "预测后插值": "通过插值正确预测和错误预测的分布来发现不稳定相关性消失的预言分布",
            "组分布鲁棒优化": "用于最小化所有插值的最坏情况风险，以提高分类器的鲁棒性",
            "稳定分类器": "能够在不同环境下保持性能稳定的分类器"
        },
        "success": true
    },
    {
        "order": 854,
        "title": "Preferential Temporal Difference Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10313",
        "abstract": "Temporal-Difference (TD) learning is a general and very useful tool for estimating the value function of a given policy, which in turn is required to find good policies. Generally speaking, TD learning updates states whenever they are visited. When the agent lands in a state, its value can be used to compute the TD-error, which is then propagated to other states. However, it may be interesting, when computing updates, to take into account other information than whether a state is visited or not. For example, some states might be more important than others (such as states which are frequently seen in a successful trajectory). Or, some states might have unreliable value estimates (for example, due to partial observability or lack of data), making their values less desirable as targets. We propose an approach to re-weighting states used in TD updates, both when they are the input and when they provide the target for the update. We prove that our approach converges with linear function approximation and illustrate its desirable empirical behaviour compared to other TD-style methods.",
        "conference": "ICML",
        "中文标题": "优先时序差分学习",
        "摘要翻译": "时序差分（TD）学习是一种通用且非常有用的工具，用于估计给定策略的价值函数，而这是寻找好策略所必需的。一般来说，TD学习在访问状态时更新它们。当代理进入一个状态时，其价值可以用来计算TD误差，然后传播到其他状态。然而，在计算更新时，考虑除了状态是否被访问之外的其他信息可能是有趣的。例如，一些状态可能比其他状态更重要（如成功轨迹中频繁出现的状态）。或者，一些状态可能具有不可靠的价值估计（例如，由于部分可观察性或数据不足），使得它们的价值作为目标不太理想。我们提出了一种方法，用于在TD更新中重新加权状态，无论是在它们作为输入时还是在它们提供更新目标时。我们证明了我们的方法在线性函数逼近下收敛，并展示了与其他TD风格方法相比其理想的实证行为。",
        "领域": "强化学习、时序差分学习、策略优化",
        "问题": "如何在时序差分学习中更有效地利用状态信息以提高学习效率和策略质量",
        "动机": "考虑到不同状态在策略学习中的重要性不同，以及某些状态的价值估计可能不可靠，研究如何在这些情况下优化时序差分学习过程",
        "方法": "提出了一种在时序差分更新中重新加权状态的方法，包括作为输入和提供更新目标时的状态，以优化学习过程",
        "关键词": [
            "时序差分学习",
            "状态重加权",
            "强化学习",
            "策略优化",
            "价值函数估计"
        ],
        "涉及的技术概念": {
            "时序差分学习": "一种用于估计策略价值函数的方法，通过当前状态的估计值与下一个状态的估计值之间的差异来更新价值函数",
            "状态重加权": "在时序差分学习中，根据状态的重要性或可靠性调整其在更新中的权重，以提高学习效率和策略质量",
            "线性函数逼近": "一种用于近似价值函数的技术，通过线性组合特征来表示价值函数，使得在大状态空间中的学习成为可能"
        },
        "success": true
    },
    {
        "order": 855,
        "title": "Principal Bit Analysis: Autoencoding with Schur-Concave Loss",
        "html": "https://ICML.cc//virtual/2021/poster/9977",
        "abstract": "We consider a linear autoencoder in which the latent variables are quantized, or corrupted by noise, and the constraint is Schur-concave in the set of latent variances. Although finding the optimal encoder/decoder pair for this setup is a nonconvex optimization problem, we show that decomposing the source into its principal components is optimal. If the constraint is strictly Schur-concave and the empirical covariance matrix has only simple eigenvalues, then any optimal encoder/decoder must decompose the source in this way. As one application, we consider a strictly Schur-concave constraint that estimates the number of bits needed to represent the latent variables under fixed-rate encoding, a setup that we call \\emph{Principal Bit Analysis (PBA)}. This yields a practical, general-purpose, fixed-rate compressor that outperforms existing algorithms. As a second application, we show that a prototypical autoencoder-based variable-rate compressor is guaranteed to decompose the source into its principal components.",
        "conference": "ICML",
        "中文标题": "主位分析：使用Schur-凹损失的自编码",
        "摘要翻译": "我们考虑一种线性自编码器，其中潜在变量被量化或受到噪声污染，且约束在潜在方差集合上是Schur-凹的。尽管为这一设置找到最优的编码器/解码器对是一个非凸优化问题，但我们证明了将源分解为其主成分是最优的。如果约束是严格Schur-凹的，且经验协方差矩阵只有简单特征值，那么任何最优的编码器/解码器都必须以这种方式分解源。作为一个应用，我们考虑了一个严格Schur-凹约束，用于估计在固定速率编码下表示潜在变量所需的位数，这一设置我们称之为‘主位分析（PBA）’。这产生了一个实用的、通用目的的固定速率压缩器，其性能优于现有算法。作为第二个应用，我们展示了一个基于自编码器的原型可变速率压缩器保证将源分解为其主成分。",
        "领域": "数据压缩、自编码器、信号处理",
        "问题": "在潜在变量被量化或受噪声污染的情况下，如何最优地分解源数据",
        "动机": "探索在Schur-凹约束下，线性自编码器的最优编码器/解码器对的设计，以及其在数据压缩中的应用",
        "方法": "通过将源数据分解为其主成分，证明了在特定条件下这是最优的编码/解码方法，并应用于固定速率和可变速率压缩器的设计",
        "关键词": [
            "主位分析",
            "Schur-凹损失",
            "线性自编码器",
            "数据压缩",
            "主成分分解"
        ],
        "涉及的技术概念": {
            "Schur-凹约束": "在潜在方差集合上定义的约束，用于保证编码/解码过程的最优性",
            "主成分分解": "将数据分解为相互正交的主成分，用于在自编码器中实现最优的数据表示",
            "固定速率编码": "一种数据压缩方法，其中每个潜在变量使用固定数量的位表示，PBA方法在此设置下优于现有算法"
        },
        "success": true
    },
    {
        "order": 856,
        "title": "Principal Component Hierarchy for Sparse Quadratic Programs",
        "html": "https://ICML.cc//virtual/2021/poster/9113",
        "abstract": "We propose a novel approximation hierarchy for cardinality-constrained, convex quadratic programs that exploits the rank-dominating eigenvectors of the quadratic matrix. Each level of approximation admits a min-max characterization whose objective function can be optimized over the binary variables analytically, while preserving convexity in the continuous variables. Exploiting this property, we propose two scalable optimization algorithms, coined as the ``best response' and the ``dual program', that can efficiently screen the potential indices of the nonzero elements of the original program. We show that the proposed methods are competitive with the existing screening methods in the current sparse regression literature, and it is particularly fast on instances with high number of measurements in experiments with both synthetic and real datasets.",
        "conference": "ICML",
        "中文标题": "稀疏二次规划的主成分层次结构",
        "摘要翻译": "我们提出了一种新的基数约束凸二次规划的近似层次结构，该结构利用了二次矩阵的秩主导特征向量。每一层次的近似都允许一个最小-最大特征描述，其目标函数可以在二进制变量上解析优化，同时保持连续变量的凸性。利用这一特性，我们提出了两种可扩展的优化算法，分别称为“最佳响应”和“对偶程序”，它们可以有效地筛选原始程序中非零元素的潜在索引。我们表明，所提出的方法与当前稀疏回归文献中的现有筛选方法相比具有竞争力，并且在合成和真实数据集的实验中，对于测量数量较多的实例特别快速。",
        "领域": "稀疏回归、凸优化、特征向量分析",
        "问题": "解决基数约束的凸二次规划问题，特别是在高维数据中高效筛选非零元素索引的挑战。",
        "动机": "为了在保持连续变量凸性的同时，提高在稀疏回归问题中筛选非零元素索引的效率和可扩展性。",
        "方法": "提出了一种基于秩主导特征向量的近似层次结构，并开发了两种优化算法（最佳响应和对偶程序）来高效筛选非零元素索引。",
        "关键词": [
            "稀疏二次规划",
            "主成分层次",
            "基数约束",
            "凸优化",
            "特征向量"
        ],
        "涉及的技术概念": {
            "基数约束": "限制解中非零元素的数量，用于稀疏性建模。",
            "凸二次规划": "目标函数为二次且约束为凸的优化问题，本研究中保持连续变量的凸性。",
            "秩主导特征向量": "用于构建近似层次结构，帮助高效筛选非零元素索引。"
        },
        "success": true
    },
    {
        "order": 857,
        "title": "Principled Exploration via Optimistic Bootstrapping and Backward Induction",
        "html": "https://ICML.cc//virtual/2021/poster/8831",
        "abstract": "One principled approach for provably efficient exploration is incorporating the upper confidence bound (UCB) into the value function as a bonus. However, UCB is specified to deal with linear and tabular settings and is incompatible with Deep Reinforcement Learning (DRL). In this paper, we propose a principled exploration method for DRL through Optimistic Bootstrapping and Backward Induction (OB2I). OB2I constructs a general-purpose UCB-bonus through non-parametric bootstrap in DRL. The UCB-bonus estimates the epistemic uncertainty of state-action pairs for optimistic exploration. We build theoretical connections between the proposed UCB-bonus and the LSVI-UCB in linear setting. We propagate future uncertainty in a time-consistent manner through episodic backward update, which exploits the theoretical advantage and empirically improves the sample-efficiency. Our experiments in MNIST maze and Atari suit suggest that OB2I outperforms several state-of-the-art exploration approaches.",
        "conference": "ICML",
        "中文标题": "通过乐观自助法与逆向归纳法的原则性探索",
        "摘要翻译": "一种证明高效探索的原则性方法是将上置信界（UCB）作为奖励纳入价值函数中。然而，UCB专为处理线性和表格设置而设计，与深度强化学习（DRL）不兼容。在本文中，我们提出了一种通过乐观自助法与逆向归纳法（OB2I）进行DRL的原则性探索方法。OB2I通过DRL中的非参数自助法构建了一个通用的UCB奖励。该UCB奖励估计了状态-动作对的认知不确定性，以进行乐观探索。我们在提出的UCB奖励与线性设置中的LSVI-UCB之间建立了理论联系。我们通过情节逆向更新以时间一致的方式传播未来的不确定性，这利用了理论优势并在实证上提高了样本效率。我们在MNIST迷宫和Atari套件中的实验表明，OB2I优于几种最先进的探索方法。",
        "领域": "深度强化学习、探索策略、不确定性估计",
        "问题": "如何在深度强化学习中实现高效且原则性的探索",
        "动机": "现有的上置信界（UCB）方法不适用于深度强化学习，限制了探索的效率和原则性",
        "方法": "提出乐观自助法与逆向归纳法（OB2I），通过非参数自助法构建UCB奖励，估计状态-动作对的认知不确定性，并通过情节逆向更新传播未来不确定性",
        "关键词": [
            "乐观自助法",
            "逆向归纳法",
            "深度强化学习",
            "探索策略",
            "不确定性估计"
        ],
        "涉及的技术概念": {
            "上置信界（UCB）": "用于估计状态-动作对的认知不确定性，以指导乐观探索",
            "非参数自助法": "用于构建通用的UCB奖励，适用于深度强化学习环境",
            "情节逆向更新": "以时间一致的方式传播未来的不确定性，提高样本效率"
        },
        "success": true
    },
    {
        "order": 858,
        "title": "Principled Simplicial Neural Networks for Trajectory Prediction",
        "html": "https://ICML.cc//virtual/2021/poster/10021",
        "abstract": "We consider the construction of neural network architectures for data on simplicial complexes.  In studying maps on the chain complex of a simplicial complex, we define three desirable properties of a simplicial neural network architecture: namely, permutation equivariance, orientation equivariance, and simplicial awareness.  The first two properties respectively account for the fact that the node indexing and the simplex orientations in a simplicial complex are arbitrary. The last property encodes the desirable feature that the output of the neural network depends on the entire simplicial complex and not on a subset of its dimensions. Based on these properties, we propose a simple convolutional architecture, rooted in tools from algebraic topology, for the problem of trajectory prediction, and show that it obeys all three of these properties when an odd, nonlinear activation function is used.  We then demonstrate the effectiveness of this architecture in extrapolating trajectories on synthetic and real datasets, with particular emphasis on the gains in generalizability to unseen trajectories.",
        "conference": "ICML",
        "中文标题": "基于原则的单纯神经网络用于轨迹预测",
        "摘要翻译": "我们考虑了在单纯复形数据上构建神经网络架构的问题。在研究单纯复形的链复形上的映射时，我们定义了单纯神经网络架构的三个理想属性：即置换等变性、方向等变性和单纯意识。前两个属性分别解释了单纯复形中节点索引和单纯形方向的任意性。最后一个属性编码了神经网络输出依赖于整个单纯复形而非其维度子集的理想特性。基于这些属性，我们提出了一个简单的卷积架构，该架构植根于代数拓扑工具，用于轨迹预测问题，并展示了在使用奇数非线性激活函数时，该架构遵循所有这三个属性。随后，我们在合成和真实数据集上展示了该架构在轨迹外推方面的有效性，特别强调了其对未见轨迹的泛化能力的提升。",
        "领域": "轨迹预测",
        "问题": "构建适用于单纯复形数据的神经网络架构，并确保其具有置换等变性、方向等变性和单纯意识",
        "动机": "为了解决单纯复形数据中节点索引和单纯形方向的任意性问题，以及确保神经网络输出依赖于整个单纯复形",
        "方法": "提出了一种基于代数拓扑工具的简单卷积架构，使用奇数非线性激活函数确保满足三个理想属性",
        "关键词": [
            "单纯神经网络",
            "轨迹预测",
            "代数拓扑",
            "卷积架构",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "置换等变性": "确保神经网络对单纯复形中节点索引的任意排列具有不变性",
            "方向等变性": "确保神经网络对单纯形方向的任意选择具有不变性",
            "单纯意识": "确保神经网络的输出依赖于整个单纯复形，而不仅仅是其部分维度"
        },
        "success": true
    },
    {
        "order": 859,
        "title": "Prior Image-Constrained Reconstruction using Style-Based Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/9541",
        "abstract": "Obtaining a useful estimate of an object from highly incomplete imaging measurements remains a holy grail of imaging science. Deep learning methods have shown promise in learning object priors or constraints to improve the conditioning of an ill-posed imaging inverse problem. In this study, a framework for estimating an object of interest that is semantically related to a known prior image, is proposed. An optimization problem is formulated in the disentangled latent space of a style-based generative model, and semantically meaningful constraints are imposed using the disentangled latent representation of the prior image. Stable recovery from incomplete measurements with the help of a prior image is theoretically analyzed. Numerical experiments demonstrating the superior performance of our approach as compared to related methods are presented.",
        "conference": "ICML",
        "中文标题": "基于风格生成模型的先验图像约束重建",
        "摘要翻译": "从高度不完整的成像测量中获取物体的有用估计仍然是成像科学的圣杯。深度学习方法在学习对象先验或约束以改善不适定成像逆问题的条件方面显示出了潜力。在本研究中，提出了一个框架，用于估计与已知先验图像在语义上相关的感兴趣对象。在风格生成模型的解缠潜在空间中制定了一个优化问题，并使用先验图像的解缠潜在表示施加了语义上有意义的约束。理论上分析了在先验图像的帮助下从不完整测量中稳定恢复的可能性。数值实验展示了我们的方法相较于相关方法的优越性能。",
        "领域": "图像重建、生成模型、逆问题求解",
        "问题": "如何从高度不完整的成像测量中准确重建物体",
        "动机": "深度学习方法在改善不适定成像逆问题的条件方面显示出了潜力，本研究旨在利用风格生成模型和先验图像的信息，提高重建的准确性和稳定性。",
        "方法": "在风格生成模型的解缠潜在空间中制定优化问题，并利用先验图像的解缠潜在表示施加语义约束，以实现从不完整测量中的稳定恢复。",
        "关键词": [
            "图像重建",
            "风格生成模型",
            "解缠潜在空间",
            "先验图像",
            "逆问题"
        ],
        "涉及的技术概念": {
            "风格生成模型": "用于生成高质量图像的模型，本研究利用其解缠潜在空间进行优化。",
            "解缠潜在空间": "允许对图像的不同属性进行独立操作的空间，本研究在此空间中施加语义约束。",
            "逆问题求解": "从不完整的测量中恢复原始信息的问题，本研究通过结合先验图像和深度学习方法改善求解过程。"
        },
        "success": true
    },
    {
        "order": 860,
        "title": "Prioritized Level Replay",
        "html": "https://ICML.cc//virtual/2021/poster/9283",
        "abstract": "Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level's future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample-efficiency and generalization on Procgen Benchmark—matching the previous state-of-the-art in test return—and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over 76% improvement in test return relative to standard RL baselines.",
        "conference": "ICML",
        "中文标题": "优先级别重放",
        "摘要翻译": "程序化生成内容的环境在测试深度强化学习中的系统泛化能力方面发挥着重要作用。在这种设置下，每个级别都是一个算法创建的环境实例，具有其变化因素的独特配置。在预定义的级别子集上进行训练可以测试对未见级别的泛化能力。从一个级别可以学习到什么取决于当前策略，然而先前的工作默认独立于策略均匀采样训练级别。我们引入了优先级别重放（PLR），这是一个通过优先选择那些在未来重新访问时具有更高估计学习潜力的级别来选择性采样下一个训练级别的通用框架。我们展示了TD误差有效地估计了一个级别未来的学习潜力，并且当用于指导采样过程时，会诱导出一个难度逐渐增加的级别的涌现课程。通过调整训练级别的采样，PLR在Procgen Benchmark上显著提高了样本效率和泛化能力——与之前的测试回报状态最先进水平相匹配——并且易于与其他方法结合。与之前领先的方法结合，PLR将最先进水平提高到相对于标准RL基线测试回报超过76%的改进。",
        "领域": "深度强化学习、程序化内容生成、系统泛化测试",
        "问题": "如何在程序化生成的环境中通过改进训练级别的采样策略来提高深度强化学习的样本效率和泛化能力",
        "动机": "现有的方法在训练级别采样上独立于策略，均匀采样，未能充分利用每个级别的学习潜力",
        "方法": "引入优先级别重放（PLR）框架，通过TD误差估计级别的学习潜力，优先采样高潜力级别，形成难度递增的课程",
        "关键词": [
            "优先级别重放",
            "TD误差",
            "程序化生成环境",
            "样本效率",
            "系统泛化"
        ],
        "涉及的技术概念": {
            "优先级别重放（PLR）": "一种通过优先选择高学习潜力级别来选择性采样训练级别的框架",
            "TD误差": "用于估计级别未来学习潜力的技术，指导采样过程",
            "程序化生成环境": "算法创建的环境实例，用于测试深度强化学习的泛化能力"
        },
        "success": true
    },
    {
        "order": 861,
        "title": "Privacy-Preserving Feature Selection with Secure Multiparty Computation",
        "html": "https://ICML.cc//virtual/2021/poster/9777",
        "abstract": "Existing work on privacy-preserving machine learning with Secure Multiparty Computation (MPC) is almost exclusively focused on model training and on inference with trained models, thereby overlooking the important data pre-processing stage. In this work, we propose the first MPC based protocol for private feature selection based on the filter method, which is independent of model training, and can be used in combination with any MPC protocol to rank features. We propose an efficient feature scoring protocol based on Gini impurity to this end. To demonstrate the feasibility of our approach for practical data science, we perform experiments with the proposed MPC protocols for feature selection in a commonly used machine-learning-as-a-service configuration where computations are outsourced to multiple servers, with semi-honest and with malicious adversaries. Regarding effectiveness, we show that secure feature selection with the proposed protocols improves the accuracy of classifiers on a variety of real-world data sets, without leaking information about the feature values or even which features were selected. Regarding efficiency, we document runtimes ranging from several seconds to an hour for our protocols to finish, depending on the size of the data set and the security settings.",
        "conference": "ICML",
        "中文标题": "基于安全多方计算的隐私保护特征选择",
        "摘要翻译": "现有的关于使用安全多方计算（MPC）进行隐私保护机器学习的研究几乎完全集中在模型训练和训练模型的推理上，从而忽视了重要的数据预处理阶段。在这项工作中，我们提出了第一个基于MPC的协议，用于基于过滤方法的私有特征选择，该协议独立于模型训练，并且可以与任何MPC协议结合使用以对特征进行排序。为此，我们提出了一个基于基尼不纯度的有效特征评分协议。为了证明我们的方法在实际数据科学中的可行性，我们在常用的机器学习即服务配置中进行了实验，其中计算被外包给多个服务器，包括半诚实和恶意对手。关于有效性，我们展示了使用所提出的协议进行安全特征选择可以提高分类器在各种真实世界数据集上的准确性，而不会泄露关于特征值或甚至哪些特征被选择的信息。关于效率，我们记录了协议完成时间从几秒到一小时不等，具体取决于数据集的大小和安全设置。",
        "领域": "隐私保护机器学习、安全多方计算、特征选择",
        "问题": "解决在隐私保护机器学习中数据预处理阶段被忽视的问题，特别是特征选择过程中的隐私保护。",
        "动机": "现有的隐私保护机器学习研究主要集中在模型训练和推理阶段，忽视了数据预处理阶段，特别是特征选择过程中的隐私保护需求。",
        "方法": "提出了一个基于安全多方计算（MPC）的协议，用于私有特征选择，该协议基于过滤方法，独立于模型训练，并可以与任何MPC协议结合使用以对特征进行排序。提出了一个基于基尼不纯度的有效特征评分协议。",
        "关键词": [
            "隐私保护",
            "特征选择",
            "安全多方计算",
            "基尼不纯度",
            "机器学习即服务"
        ],
        "涉及的技术概念": {
            "安全多方计算（MPC）": "一种允许多方共同计算一个函数而无需泄露各自输入数据的密码学协议，用于保护特征选择过程中的数据隐私。",
            "基尼不纯度": "用于衡量数据集的不纯度或多样性的指标，在本研究中用于特征评分，以选择最有信息量的特征。",
            "机器学习即服务（MLaaS）": "一种云计算服务模式，允许用户通过互联网访问机器学习工具和算法，本研究在此配置下测试了提出的协议。"
        },
        "success": true
    },
    {
        "order": 862,
        "title": "Privacy-Preserving Video Classification with Convolutional Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10097",
        "abstract": "Many video classification applications require access to personal data, thereby posing an invasive security risk to the users' privacy. We propose a privacy-preserving implementation of single-frame method based video classification with convolutional neural networks that allows a party to infer a label from a video without necessitating the video owner to disclose their video to other entities in an unencrypted manner. Similarly, our approach removes the requirement of the classifier owner from revealing their model parameters to outside entities in plaintext. To this end, we combine existing Secure Multi-Party Computation (MPC) protocols for private image classification with our novel MPC protocols for oblivious single-frame selection and secure label aggregation across frames. The result is an end-to-end privacy-preserving video classification pipeline. We evaluate our proposed solution in an application for private human emotion recognition. Our results across a variety of security settings, spanning honest and dishonest majority configurations of the computing parties, and for both passive and active adversaries, demonstrate that videos can be classified with state-of-the-art accuracy, and without leaking sensitive user information. ",
        "conference": "ICML",
        "中文标题": "基于卷积神经网络的隐私保护视频分类",
        "摘要翻译": "许多视频分类应用需要访问个人数据，从而对用户的隐私构成侵入性安全风险。我们提出了一种基于卷积神经网络的单帧方法视频分类的隐私保护实现，允许一方从视频中推断标签，而无需视频所有者以未加密的方式向其他实体披露其视频。同样，我们的方法消除了分类器所有者向外部实体以明文形式揭示其模型参数的要求。为此，我们将现有的用于私有图像分类的安全多方计算（MPC）协议与我们新颖的用于无意识单帧选择和跨帧安全标签聚合的MPC协议结合起来。结果是一个端到端的隐私保护视频分类流程。我们在私人人类情感识别的应用中评估了我们提出的解决方案。我们在各种安全设置下的结果，包括计算方的诚实和不诚实多数配置，以及被动和主动对手，证明了视频可以以最先进的准确性进行分类，而不会泄露敏感用户信息。",
        "领域": "视频分类、隐私保护计算、情感识别",
        "问题": "解决视频分类过程中用户隐私泄露的问题",
        "动机": "保护用户在视频分类应用中的隐私，防止敏感信息泄露",
        "方法": "结合安全多方计算协议和新型MPC协议，实现隐私保护的视频分类",
        "关键词": [
            "隐私保护",
            "视频分类",
            "安全多方计算",
            "卷积神经网络",
            "情感识别"
        ],
        "涉及的技术概念": {
            "安全多方计算（MPC）": "用于在不泄露各自私有数据的情况下，多方共同计算一个函数的技术",
            "无意识单帧选择": "在不暴露视频内容的情况下，选择用于分类的视频帧的技术",
            "安全标签聚合": "在不泄露单个帧分类结果的情况下，聚合所有帧的分类结果以得出最终视频标签的技术"
        },
        "success": true
    },
    {
        "order": 863,
        "title": "Private Adaptive Gradient Methods for Convex Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9209",
        "abstract": "We study adaptive methods for differentially private convex optimization, proposing and analyzing differentially private variants of a Stochastic Gradient Descent (SGD) algorithm with adaptive stepsizes, as well as the AdaGrad algorithm. We provide upper bounds on the regret of both algorithms and show that the bounds are (worst-case) optimal. As a consequence of our development, we show that our private versions of AdaGrad outperform adaptive SGD, which in turn outperforms traditional SGD in scenarios with non-isotropic gradients where (non-private) Adagrad provably outperforms SGD. The major challenge is that the isotropic noise typically added for privacy dominates the signal in gradient geometry for high-dimensional problems; approaches to this that effectively optimize over lower-dimensional subspaces simply ignore the actual problems that varying gradient geometries introduce. In contrast, we study non-isotropic clipping and noise addition, developing a principled theoretical approach; the consequent procedures also enjoy significantly stronger empirical performance than prior approaches.",
        "conference": "ICML",
        "中文标题": "凸优化的私有自适应梯度方法",
        "摘要翻译": "我们研究了差分隐私凸优化的自适应方法，提出并分析了具有自适应步长的随机梯度下降（SGD）算法以及AdaGrad算法的差分隐私变体。我们为这两种算法提供了遗憾上界，并证明这些界限在（最坏情况下）是最优的。作为我们研究的一个结果，我们展示了我们的私有AdaGrad版本在非各向同性梯度场景下优于自适应SGD，而自适应SGD又优于传统SGD，其中（非私有）Adagrad被证明优于SGD。主要挑战在于，通常为隐私添加的各向同性噪声在高维问题中主导了梯度几何中的信号；有效优化低维子空间的方法简单地忽略了变化梯度几何引入的实际问题。相比之下，我们研究了非各向同性裁剪和噪声添加，发展了一种原则性的理论方法；随之而来的程序也比以前的方法享有显著更强的实证性能。",
        "领域": "差分隐私优化、自适应梯度方法、凸优化",
        "问题": "在差分隐私保护下，如何有效进行凸优化，特别是在非各向同性梯度场景下。",
        "动机": "研究差分隐私下的自适应梯度方法，以解决高维问题中各向同性噪声主导信号的问题，提升优化性能。",
        "方法": "提出并分析了差分隐私变体的自适应步长SGD和AdaGrad算法，采用非各向同性裁剪和噪声添加的方法。",
        "关键词": [
            "差分隐私",
            "自适应梯度方法",
            "凸优化",
            "非各向同性裁剪",
            "噪声添加"
        ],
        "涉及的技术概念": {
            "差分隐私": "在算法中引入噪声以保护数据隐私，同时尽量不影响优化性能。",
            "自适应梯度方法": "根据梯度信息动态调整步长，以提高优化效率和效果。",
            "非各向同性裁剪": "针对梯度几何变化的特点，采用不同方向的裁剪策略，以更有效地处理高维问题。"
        },
        "success": true
    },
    {
        "order": 864,
        "title": "Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates",
        "html": "https://ICML.cc//virtual/2021/poster/10123",
        "abstract": "We study the problem of differentially private (DP) matrix completion under user-level privacy. We design a joint differentially private variant of the popular Alternating-Least-Squares (ALS) method that achieves: i) (nearly) optimal sample complexity for matrix completion (in terms of number of items, users), and ii) the best known privacy/utility trade-off both theoretically, as well as on benchmark data sets. In particular, we provide the first global convergence analysis of ALS with noise introduced to ensure DP, and show that, in comparison to the best known alternative (the Private Frank-Wolfe algorithm by Jain et al. (2018)), our error bounds scale significantly better with respect to the number of items and users, which is critical in practical problems. Extensive validation on standard benchmarks demonstrate that the algorithm, in combination with carefully designed sampling procedures, is significantly more accurate than existing techniques, thus promising to be the first practical DP embedding model.",
        "conference": "ICML",
        "中文标题": "私有交替最小二乘法：具有更严格率的实用私有矩阵完成",
        "摘要翻译": "我们研究了在用户级隐私下的差分私有（DP）矩阵完成问题。我们设计了一种流行的交替最小二乘法（ALS）的联合差分私有变体，该变体实现了：i）矩阵完成的（近乎）最优样本复杂度（在项目数量、用户数量方面），以及ii）理论上以及在基准数据集上已知的最佳隐私/效用权衡。特别是，我们首次提供了引入噪声以确保DP的ALS的全局收敛性分析，并表明，与最知名的替代方案（Jain等人（2018）的私有Frank-Wolfe算法）相比，我们的误差界限在项目数量和用户数量方面显著更好，这在实际问题中至关重要。在标准基准上的广泛验证表明，该算法与精心设计的采样程序相结合，比现有技术显著更准确，因此有望成为第一个实用的DP嵌入模型。",
        "领域": "差分隐私、矩阵完成、推荐系统",
        "问题": "在保证用户级差分隐私的前提下，实现矩阵完成的高效和准确计算。",
        "动机": "解决现有差分隐私矩阵完成方法在样本复杂度和隐私/效用权衡上的不足，特别是在处理大量项目和用户时的效率问题。",
        "方法": "设计了一种联合差分私有的交替最小二乘法（ALS）变体，通过引入噪声确保差分隐私，并优化了样本复杂度和隐私/效用权衡。",
        "关键词": [
            "差分隐私",
            "矩阵完成",
            "交替最小二乘法",
            "用户级隐私",
            "隐私/效用权衡"
        ],
        "涉及的技术概念": {
            "差分隐私（DP）": "在矩阵完成过程中保护用户数据隐私的技术，确保算法的输出不会泄露个别用户的敏感信息。",
            "交替最小二乘法（ALS）": "一种用于矩阵分解和完成的流行算法，通过交替优化用户和项目因子矩阵来最小化误差。",
            "隐私/效用权衡": "在保护隐私的同时，保持或提高算法性能的平衡策略，是差分隐私研究中的核心问题。"
        },
        "success": true
    },
    {
        "order": 865,
        "title": "Private Stochastic Convex Optimization: Optimal Rates in L1 Geometry",
        "html": "https://ICML.cc//virtual/2021/poster/9989",
        "abstract": "Stochastic convex optimization over an $\\ell_1$-bounded domain is ubiquitous in machine learning applications  such  as  LASSO but  remains poorly understood when learning with differential privacy. We show that, up to logarithmic factors the optimal excess population loss of any $(\\epsilon,\\delta)$-differentially private optimizer is $\\sqrt{\\log(d)/n} + \\sqrt{d}/\\epsilon n.$ \nThe upper bound is based on a new algorithm that combines the iterative localization approach of Feldman et al. (2020) with a new analysis of private regularized mirror descent. It applies to $\\ell_p$ bounded domains for $p\\in [1,2]$ and queries at most $n^{3/2}$ gradients improving over the best previously known algorithm for the $\\ell_2$ case which needs $n^2$ gradients. Further, we show that when the loss functions satisfy additional smoothness assumptions, the excess loss is upper bounded (up to logarithmic factors) by $\\sqrt{\\log(d)/n} + (\\log(d)/\\epsilon n)^{2/3}.$ \nThis bound is achieved by a new variance-reduced version of the Frank-Wolfe algorithm that requires just a single pass over the data. We also show that the lower bound in this case is the minimum of the two rates mentioned above.",
        "conference": "ICML",
        "中文标题": "私有随机凸优化：L1几何中的最优速率",
        "摘要翻译": "在机器学习应用中，如LASSO，随机凸优化在ℓ1有界域上无处不在，但在差分隐私学习方面仍然知之甚少。我们表明，达到对数因子，任何(ε,δ)-差分隐私优化器的最优超额总体损失是√(log(d)/n) + √d/εn。上界基于一种新算法，该算法结合了Feldman等人(2020)的迭代定位方法和私有正则化镜像下降的新分析。它适用于p∈[1,2]的ℓp有界域，并且最多查询n^(3/2)次梯度，改进了之前已知的ℓ2情况下需要n^2次梯度的最佳算法。此外，我们表明，当损失函数满足额外的平滑性假设时，超额损失的上界（达到对数因子）为√(log(d)/n) + (log(d)/εn)^(2/3)。这一界限是通过Frank-Wolfe算法的新方差减少版本实现的，该版本仅需单次数据遍历。我们还表明，在这种情况下，下界是上述两个速率中的最小值。",
        "领域": "差分隐私优化、随机凸优化、机器学习理论",
        "问题": "在差分隐私约束下，随机凸优化在ℓ1有界域上的最优超额总体损失问题",
        "动机": "研究在差分隐私保护下，如何高效解决随机凸优化问题，特别是在ℓ1有界域上的应用，如LASSO",
        "方法": "结合迭代定位方法和私有正则化镜像下降的新分析，以及Frank-Wolfe算法的方差减少版本",
        "关键词": [
            "差分隐私",
            "随机凸优化",
            "ℓ1几何",
            "最优速率",
            "Frank-Wolfe算法"
        ],
        "涉及的技术概念": {
            "差分隐私优化器": "在保护数据隐私的同时进行优化，确保算法输出的差分隐私性",
            "迭代定位方法": "一种逐步缩小解空间范围的技术，用于提高优化效率和精度",
            "私有正则化镜像下降": "结合正则化和镜像下降的优化方法，适用于差分隐私保护的场景"
        },
        "success": true
    },
    {
        "order": 866,
        "title": "Probabilistic Generating Circuits",
        "html": "https://ICML.cc//virtual/2021/poster/10459",
        "abstract": "Generating functions, which are widely used in combinatorics and probability theory, encode function values into the coefficients of a polynomial. In this paper, we explore their use as a tractable probabilistic model, and propose probabilistic generating circuits (PGCs) for their efficient representation. PGCs are strictly more expressive efficient than many existing tractable probabilistic models, including determinantal point processes (DPPs), probabilistic circuits (PCs) such as sum-product networks, and tractable graphical models. We contend that PGCs are not just a theoretical framework that unifies vastly different existing models, but also show great potential in modeling realistic data. We exhibit a simple class of PGCs that are not trivially subsumed by simple combinations of PCs and DPPs, and obtain competitive performance on a suite of density estimation benchmarks. We also highlight PGCs' connection to the theory of strongly Rayleigh distributions.",
        "conference": "ICML",
        "中文标题": "概率生成电路",
        "摘要翻译": "生成函数在组合数学和概率论中被广泛使用，它将函数值编码为多项式的系数。在本文中，我们探索了其作为一种易处理的概率模型的用途，并提出了概率生成电路（PGCs）以实现其高效表示。PGCs在表达能力上严格优于许多现有的易处理概率模型，包括行列式点过程（DPPs）、概率电路（PCs）如和积网络，以及易处理的图模型。我们认为PGCs不仅是一个统一了多种不同现有模型的理论框架，而且在建模现实数据方面显示出巨大潜力。我们展示了一类简单的PGCs，它们不能简单地通过PCs和DPPs的简单组合来包含，并且在一套密度估计基准测试中获得了竞争性的性能。我们还强调了PGCs与强瑞利分布理论的联系。",
        "领域": "概率图模型、生成模型、密度估计",
        "问题": "探索生成函数作为易处理的概率模型的使用，并提出概率生成电路（PGCs）以实现高效表示。",
        "动机": "统一多种不同的现有概率模型，并在建模现实数据方面展示潜力。",
        "方法": "提出概率生成电路（PGCs），并展示其在密度估计基准测试中的性能。",
        "关键词": [
            "概率生成电路",
            "生成函数",
            "密度估计",
            "行列式点过程",
            "强瑞利分布"
        ],
        "涉及的技术概念": {
            "概率生成电路（PGCs）": "本文提出的用于高效表示生成函数的概率模型，具有比现有模型更强的表达能力。",
            "行列式点过程（DPPs）": "一种现有的概率模型，PGCs在表达能力上优于它。",
            "强瑞利分布": "与PGCs相关的概率理论，本文强调了它们之间的联系。"
        },
        "success": true
    },
    {
        "order": 867,
        "title": "Probabilistic Programs with Stochastic Conditioning",
        "html": "https://ICML.cc//virtual/2021/poster/8607",
        "abstract": "We tackle the problem of conditioning probabilistic programs on distributions of observable variables.  Probabilistic programs are usually conditioned on samples from the joint data distribution, which we refer to as deterministic conditioning.  However, in many real-life scenarios, the observations are given as marginal distributions, summary statistics, or samplers.  Conventional probabilistic programming systems lack adequate means for modeling and inference in such scenarios.  We propose a generalization of deterministic conditioning to stochastic conditioning, that is,  conditioning on the marginal distribution of a variable taking a particular form.  To this end, we first define the formal notion of stochastic conditioning and discuss its key properties.  We then show how to perform inference in the presence of stochastic conditioning.  We demonstrate potential usage of stochastic conditioning on several case studies which involve various kinds of stochastic conditioning and are difficult to solve otherwise.  Although we present stochastic conditioning in the context of probabilistic programming, our formalization is general and applicable to other settings. \n",
        "conference": "ICML",
        "中文标题": "具有随机条件的概率程序",
        "摘要翻译": "我们解决了在可观测变量的分布上条件化概率程序的问题。概率程序通常是在联合数据分布的样本上进行条件化的，我们称之为确定性条件化。然而，在许多现实生活场景中，观测值是以边缘分布、汇总统计或采样器的形式给出的。传统的概率编程系统在此类场景中缺乏足够的建模和推理手段。我们提出了确定性条件化到随机条件化的泛化，即在变量的边缘分布采取特定形式时进行条件化。为此，我们首先定义了随机条件化的形式概念，并讨论了其关键属性。然后，我们展示了如何在存在随机条件化的情况下进行推理。我们通过几个案例研究展示了随机条件化的潜在用途，这些案例涉及各种类型的随机条件化，并且难以用其他方法解决。尽管我们在概率编程的背景下介绍了随机条件化，但我们的形式化是通用的，适用于其他设置。",
        "领域": "概率编程、统计机器学习、贝叶斯推理",
        "问题": "解决在可观测变量的分布上条件化概率程序的问题",
        "动机": "传统概率编程系统在处理以边缘分布、汇总统计或采样器形式给出的观测值时缺乏足够的建模和推理手段",
        "方法": "提出了确定性条件化到随机条件化的泛化，定义了随机条件化的形式概念，并展示了如何在存在随机条件化的情况下进行推理",
        "关键词": [
            "概率编程",
            "随机条件化",
            "贝叶斯推理",
            "统计机器学习",
            "边缘分布"
        ],
        "涉及的技术概念": {
            "概率编程": "一种编程范式，用于构建和推理概率模型",
            "随机条件化": "在变量的边缘分布采取特定形式时进行条件化的方法",
            "贝叶斯推理": "利用贝叶斯定理进行统计推断的方法，用于更新概率模型的参数"
        },
        "success": true
    },
    {
        "order": 868,
        "title": "Probabilistic Sequential Shrinking: A Best Arm Identification Algorithm for Stochastic Bandits with Corruptions",
        "html": "https://ICML.cc//virtual/2021/poster/10685",
        "abstract": "We consider a best arm identification (BAI) problem for stochastic bandits with adversarial corruptions in the fixed-budget setting of T steps. We design a novel randomized algorithm, Probabilistic Sequential Shrinking(u) (PSS(u)), which is agnostic to the amount of corruptions. When the amount of corruptions per step (CPS) is below a threshold, PSS(u) identifies the best arm or item with probability tending to 1 as T→∞. Otherwise, the optimality gap of the identified item degrades gracefully with the CPS.We argue that such a bifurcation is necessary. In PSS(u), the parameter u serves to balance between the optimality gap and success probability. The injection of randomization is shown to be essential to mitigate the impact of corruptions. To demonstrate this, we design two attack strategies that are applicable to any algorithm. We apply one of them to a deterministic analogue of PSS(u) known as Successive Halving (SH) by Karnin et al. (2013). The attack strategy results in a high failure probability for SH, but PSS(u) remains robust. In the absence of corruptions, PSS(2)'s performance guarantee matches SH's. We show that when the CPS is sufficiently large, no algorithm can achieve a BAI probability tending to 1 as T→∞. Numerical experiments corroborate our theoretical findings.",
        "conference": "ICML",
        "中文标题": "概率顺序收缩：一种针对带有腐败的随机多臂老虎机的最佳臂识别算法",
        "摘要翻译": "我们考虑在固定预算T步设置下，针对带有对抗性腐败的随机多臂老虎机的最佳臂识别（BAI）问题。我们设计了一种新颖的随机算法——概率顺序收缩(u)（PSS(u)），该算法对腐败量不敏感。当每步的腐败量（CPS）低于某个阈值时，PSS(u)能够以概率趋近于1的方式识别出最佳臂或项目，随着T→∞。否则，识别出的项目的最优性差距会随着CPS的增加而优雅地降低。我们认为这种分叉是必要的。在PSS(u)中，参数u用于平衡最优性差距和成功概率。随机性的注入被证明是减轻腐败影响的关键。为了证明这一点，我们设计了两种适用于任何算法的攻击策略。我们将其中一种应用于PSS(u)的确定性类似物——Karnin等人（2013）提出的连续减半（SH）。攻击策略导致SH的失败概率很高，但PSS(u)仍然保持稳健。在没有腐败的情况下，PSS(2)的性能保证与SH相匹配。我们表明，当CPS足够大时，没有任何算法能够实现随着T→∞，BAI概率趋近于1。数值实验证实了我们的理论发现。",
        "领域": "强化学习、多臂老虎机问题、对抗性机器学习",
        "问题": "在存在对抗性腐败的情况下，如何在固定预算内有效地识别随机多臂老虎机中的最佳臂。",
        "动机": "研究动机是为了解决在对抗性环境下，传统最佳臂识别算法因腐败而性能下降的问题，提出一种能够抵抗腐败影响的算法。",
        "方法": "设计了一种名为概率顺序收缩(u)（PSS(u)）的随机算法，通过参数u平衡最优性差距和成功概率，并注入随机性以减轻腐败的影响。",
        "关键词": [
            "最佳臂识别",
            "随机多臂老虎机",
            "对抗性腐败",
            "概率顺序收缩",
            "连续减半"
        ],
        "涉及的技术概念": {
            "概率顺序收缩(u)（PSS(u)）": "一种新颖的随机算法，用于在存在对抗性腐败的情况下识别最佳臂，通过参数u平衡最优性差距和成功概率。",
            "连续减半（SH）": "一种确定性的最佳臂识别算法，PSS(u)的确定性类似物，但在对抗性环境下表现不如PSS(u)稳健。",
            "对抗性腐败": "指在多臂老虎机问题中，外部攻击者可能对奖励信号进行的恶意修改，影响算法的决策过程。"
        },
        "success": true
    },
    {
        "order": 869,
        "title": "Problem Dependent View on Structured Thresholding Bandit Problems",
        "html": "https://ICML.cc//virtual/2021/poster/10401",
        "abstract": "We investigate the \\textit{problem dependent regime} in the stochastic \\emph{Thresholding Bandit problem} (\\tbp) under several \\emph{shape constraints}. In the \\tbp the objective of the learner is to output, after interacting with the environment, the set of arms whose means are above a given threshold. The vanilla, unstructured, case is already well studied in the literature. Taking $K$ as the number of arms, we consider the case where (i) the sequence of arm's means $(\\mu_k){k=1}^K$ is monotonically increasing (\\textit{MTBP}) and (ii) the case where $(\\mu_k){k=1}^K$ is concave (\\textit{CTBP}). We consider both cases in the \\emph{problem dependent} regime and study the probability of error - i.e.~the probability to mis-classify at least one arm. In the fixed budget setting, we provide nearly matching upper and lower bounds for the probability of error in both the concave and monotone settings, as well as associated algorithms. Of interest, is that for both the monotone and concave cases, optimal bounds on probability of error are of the same order as those for the two armed bandit problem. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "结构化阈值赌博机问题的问题依赖视角",
        "摘要翻译": "我们在几种形状约束下研究了随机阈值赌博机问题（TBP）中的问题依赖机制。在TBP中，学习者的目标是在与环境交互后输出均值高于给定阈值的臂集合。文献中已经对普通的、非结构化的情况进行了充分研究。以K为臂的数量，我们考虑了以下两种情况：（i）臂的均值序列（μk）k=1^K单调递增（MTBP）和（ii）臂的均值序列（μk）k=1^K为凹（CTBP）。我们在问题依赖机制下考虑了这两种情况，并研究了错误概率——即至少错误分类一个臂的概率。在固定预算设置下，我们为凹和单调两种情况下的错误概率提供了几乎匹配的上界和下界，以及相关算法。有趣的是，对于单调和凹两种情况，错误概率的最优界限与两臂赌博机问题的界限相同。",
        "领域": "强化学习, 赌博机问题, 统计学习",
        "问题": "研究在形状约束下的随机阈值赌博机问题中，如何准确识别均值高于给定阈值的臂集合。",
        "动机": "探索在单调递增和凹形状约束下的阈值赌博机问题中，错误概率的界限及其算法实现，以优化学习者的决策过程。",
        "方法": "在固定预算设置下，通过提供错误概率的上界和下界，以及开发相关算法，研究单调和凹情况下的阈值赌博机问题。",
        "关键词": [
            "阈值赌博机问题",
            "问题依赖机制",
            "形状约束",
            "错误概率",
            "固定预算"
        ],
        "涉及的技术概念": {
            "问题依赖机制": "在赌博机问题中，根据问题的具体特性调整学习策略，以提高识别正确臂集合的效率。",
            "形状约束": "对臂的均值序列施加单调递增或凹的约束，以限制问题的复杂性并指导算法的设计。",
            "固定预算设置": "在有限的资源或时间预算内进行学习和决策，旨在最大化学习效率或最小化错误概率。"
        }
    },
    {
        "order": 870,
        "title": "ProGraML: A Graph-based Program Representation for Data Flow Analysis and Compiler Optimizations",
        "html": "https://ICML.cc//virtual/2021/poster/8885",
        "abstract": "Machine learning (ML) is increasingly seen as a viable approach for building compiler optimization heuristics, but many ML methods cannot replicate even the simplest of the data flow analyses that are critical to making good optimization decisions. We posit that if ML cannot do that, then it is insufficiently able to reason about programs. We formulate data flow analyses as supervised learning tasks and introduce a large open dataset of programs and their corresponding labels from several analyses. We use this dataset to benchmark ML methods and show that they struggle on these fundamental program reasoning tasks. We propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of program semantics. ProGraML overcomes the limitations of prior works and yields improved performance on downstream optimization tasks.",
        "conference": "ICML",
        "中文标题": "ProGraML：一种基于图的程序表示方法，用于数据流分析和编译器优化",
        "摘要翻译": "机器学习（ML）越来越被视为构建编译器优化启发式方法的可行途径，但许多ML方法甚至无法复制对做出良好优化决策至关重要的最简单的数据流分析。我们认为，如果ML无法做到这一点，那么它在程序推理方面的能力就不足。我们将数据流分析表述为监督学习任务，并引入了一个大型开放数据集，其中包含程序及其来自多个分析的相应标签。我们使用这个数据集来对ML方法进行基准测试，并显示它们在这些基本的程序推理任务上表现不佳。我们提出了ProGraML——用于机器学习的程序图——一种与语言无关、可移植的程序语义表示方法。ProGraML克服了先前工作的限制，并在下游优化任务上实现了性能提升。",
        "领域": "程序分析、编译器优化、机器学习与程序分析结合",
        "问题": "机器学习方法在复制数据流分析这一对编译器优化至关重要的任务上表现不佳",
        "动机": "提升机器学习在程序推理方面的能力，特别是在数据流分析和编译器优化中的应用",
        "方法": "提出了一种新的基于图的程序表示方法ProGraML，用于改善机器学习在程序分析和优化任务中的表现",
        "关键词": [
            "程序表示",
            "数据流分析",
            "编译器优化",
            "机器学习",
            "程序推理"
        ],
        "涉及的技术概念": {
            "程序图": "一种与语言无关、可移植的程序语义表示方法，用于改善机器学习在程序分析和优化任务中的表现",
            "数据流分析": "编译器优化中的关键步骤，用于分析程序中数据的流动和转换",
            "监督学习": "机器学习的一种方法，通过已标记的数据集训练模型，用于预测或分类任务"
        },
        "success": true
    },
    {
        "order": 871,
        "title": "Progressive-Scale Boundary Blackbox Attack via Projective Gradient Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/9235",
        "abstract": "Boundary based blackbox attack has been recognized as practical and effective, given that an attacker only needs to access the final model prediction. However, the query efficiency of it is in general high especially for high dimensional image data. In this paper, we show that such efficiency highly depends on the scale at which the attack is applied, and attacking at the optimal scale significantly improves the efficiency. In particular, we propose a theoretical framework to analyze and show three key characteristics to improve the query efficiency. We prove that there exists an optimal scale for projective gradient estimation. Our framework also explains the satisfactory performance achieved by existing boundary black-box attacks. Based on our theoretical framework, we propose Progressive-Scale enabled projective Boundary Attack (PSBA) to improve the query efficiency via progressive scaling techniques. In particular, we employ Progressive-GAN to optimize the scale of projections, which we call PSBA-PGAN. We evaluate our approach on both spatial and frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and ImageNet against different models including a real-world face recognition API show that PSBA-PGAN significantly outperforms existing baseline attacks in terms of query efficiency and attack success rate. We also observe relatively stable optimal scales for different models and datasets. The code is publicly available at https://github.com/AI-secure/PSBA.",
        "conference": "ICML",
        "中文标题": "通过投影梯度估计的渐进尺度边界黑盒攻击",
        "摘要翻译": "基于边界的黑盒攻击已被认为是实用且有效的，因为攻击者只需要访问最终的模型预测。然而，其查询效率通常较高，尤其是对于高维图像数据。在本文中，我们表明这种效率在很大程度上取决于攻击所应用的尺度，而在最优尺度上进行攻击显著提高了效率。特别是，我们提出了一个理论框架来分析并展示提高查询效率的三个关键特征。我们证明了存在一个用于投影梯度估计的最优尺度。我们的框架也解释了现有边界黑盒攻击所达到的满意性能。基于我们的理论框架，我们提出了渐进尺度启用的投影边界攻击（PSBA），通过渐进尺度技术来提高查询效率。特别是，我们使用渐进-GAN来优化投影的尺度，我们称之为PSBA-PGAN。我们在空间和频率尺度上评估了我们的方法。在MNIST、CIFAR-10、CelebA和ImageNet上对不同模型（包括一个真实世界的人脸识别API）进行的广泛实验表明，PSBA-PGAN在查询效率和攻击成功率方面显著优于现有的基线攻击。我们还观察到不同模型和数据集相对稳定的最优尺度。代码公开在https://github.com/AI-secure/PSBA。",
        "领域": "对抗性攻击、图像识别安全、生成对抗网络",
        "问题": "提高基于边界的黑盒攻击在对抗性攻击中的查询效率和攻击成功率",
        "动机": "现有的基于边界的黑盒攻击在高维图像数据上的查询效率不高，研究旨在通过优化攻击尺度来提高效率",
        "方法": "提出渐进尺度启用的投影边界攻击（PSBA），利用渐进-GAN优化投影尺度，提高查询效率和攻击成功率",
        "关键词": [
            "对抗性攻击",
            "查询效率",
            "渐进尺度",
            "投影梯度估计",
            "生成对抗网络"
        ],
        "涉及的技术概念": {
            "渐进尺度技术": "用于优化攻击尺度，提高查询效率和攻击成功率的技术",
            "投影梯度估计": "在对抗性攻击中用于估计最优攻击方向的技术",
            "生成对抗网络（GAN）": "用于生成逼真图像，优化攻击尺度，提高攻击效率"
        },
        "success": true
    },
    {
        "order": 872,
        "title": "Projection Robust Wasserstein Barycenters",
        "html": "https://ICML.cc//virtual/2021/poster/10607",
        "abstract": "Collecting and aggregating information from several probability measures or histograms is a fundamental task in machine learning. One of the popular solution methods for this task is to compute the barycenter of the probability measures under the Wasserstein metric. However, approximating the Wasserstein barycenter is numerically challenging because of the curse of dimensionality. This paper proposes the projection robust Wasserstein barycenter (PRWB) that has the potential to mitigate the curse of dimensionality, and a relaxed PRWB (RPRWB) model that is computationally more tractable. By combining the iterative Bregman projection algorithm and Riemannian optimization, we propose two algorithms for computing the RPRWB, which is a max-min problem over the Stiefel manifold. The complexity of arithmetic operations of the proposed algorithms for obtaining an $\\epsilon$-stationary solution is analyzed. We incorporate the RPRWB into a discrete distribution clustering algorithm, and the numerical results on real text datasets confirm that our RPRWB model helps improve the clustering performance significantly.\n",
        "conference": "ICML",
        "中文标题": "投影鲁棒Wasserstein重心",
        "摘要翻译": "从多个概率测度或直方图中收集和聚合信息是机器学习中的一项基本任务。解决这一任务的流行方法之一是在Wasserstein度量下计算概率测度的重心。然而，由于维数灾难，近似Wasserstein重心在数值上具有挑战性。本文提出了投影鲁棒Wasserstein重心（PRWB），它有可能缓解维数灾难，以及一个计算上更易处理的松弛PRWB（RPRWB）模型。通过结合迭代Bregman投影算法和黎曼优化，我们提出了两种计算RPRWB的算法，这是在Stiefel流形上的一个最大最小问题。分析了所提出算法获得ε-平稳解的算术运算复杂度。我们将RPRWB纳入到一个离散分布聚类算法中，真实文本数据集上的数值结果证实，我们的RPRWB模型有助于显著提高聚类性能。",
        "领域": "概率测度聚合、Wasserstein距离应用、离散分布聚类",
        "问题": "解决在高维空间中计算Wasserstein重心时遇到的维数灾难问题",
        "动机": "为了更有效地在机器学习中聚合信息，需要一种能够克服高维挑战的Wasserstein重心计算方法",
        "方法": "提出投影鲁棒Wasserstein重心（PRWB）及其松弛模型RPRWB，结合迭代Bregman投影算法和黎曼优化开发两种算法",
        "关键词": [
            "Wasserstein重心",
            "维数灾难",
            "离散分布聚类",
            "黎曼优化",
            "Bregman投影"
        ],
        "涉及的技术概念": {
            "投影鲁棒Wasserstein重心（PRWB）": "一种旨在缓解高维空间中计算Wasserstein重心时维数灾难问题的方法",
            "松弛PRWB（RPRWB）模型": "PRWB的一个计算上更易处理的变体，用于实际应用",
            "Stiefel流形": "在RPRWB模型中用于解决最大最小问题的数学结构"
        },
        "success": true
    },
    {
        "order": 873,
        "title": "Projection techniques to update the truncated SVD of evolving matrices with applications",
        "html": "https://ICML.cc//virtual/2021/poster/10159",
        "abstract": "This submission considers the problem of updating the rank-$k$ \ntruncated Singular Value Decomposition (SVD) of matrices subject to \nthe addition of new rows and/or columns over time. Such matrix \nproblems represent an important computational kernel in applications \nsuch as Latent Semantic Indexing and Recommender Systems. Nonetheless, \nthe proposed framework is purely algebraic and \ntargets general updating problems. The algorithm presented in this paper \nundertakes a projection viewpoint and focuses on building a pair of \nsubspaces which approximate the linear span of the sought singular vectors \nof the updated matrix. We discuss and analyze two different choices to \nform the projection subspaces. Results on matrices from real applications \nsuggest that the proposed algorithm can lead to higher accuracy, \nespecially for the singular triplets associated with the largest modulus \nsingular values. Several practical details and key differences with other \napproaches are also discussed.",
        "conference": "ICML",
        "中文标题": "投影技术更新演化矩阵的截断奇异值分解及其应用",
        "摘要翻译": "本文考虑了随着时间推移，因新增行和/或列而需要更新矩阵的秩-$k$截断奇异值分解（SVD）的问题。这类矩阵问题在潜在语义索引和推荐系统等应用中代表了一个重要的计算核心。尽管如此，提出的框架纯粹是代数的，并针对一般的更新问题。本文提出的算法采用投影视角，专注于构建一对子空间，这些子空间近似于更新后矩阵所求奇异向量的线性跨度。我们讨论并分析了形成投影子空间的两种不同选择。来自实际应用矩阵的结果表明，所提出的算法可以实现更高的准确性，特别是对于与最大模奇异值相关的奇异三元组。还讨论了一些实际细节和与其他方法的关键差异。",
        "领域": "推荐系统, 潜在语义索引, 矩阵分解",
        "问题": "更新因新增行和/或列而变化的矩阵的秩-$k$截断奇异值分解（SVD）",
        "动机": "解决在潜在语义索引和推荐系统等应用中，因矩阵动态变化而需要高效更新SVD的问题",
        "方法": "采用投影视角，构建近似更新后矩阵奇异向量线性跨度的子空间，并分析两种不同的子空间形成选择",
        "关键词": [
            "截断奇异值分解",
            "投影技术",
            "矩阵更新",
            "潜在语义索引",
            "推荐系统"
        ],
        "涉及的技术概念": {
            "截断奇异值分解（SVD）": "用于矩阵分解的技术，保留前$k$个最大的奇异值及其对应的奇异向量，以降低计算复杂度",
            "投影技术": "通过构建子空间来近似更新后矩阵的奇异向量线性跨度，以提高更新SVD的效率和准确性",
            "奇异三元组": "由奇异值及其对应的左右奇异向量组成的三元组，用于描述矩阵的奇异值分解"
        },
        "success": true
    },
    {
        "order": 874,
        "title": "Provable Generalization of SGD-trained Neural Networks of Any Width in the Presence of Adversarial Label Noise",
        "html": "https://ICML.cc//virtual/2021/poster/10399",
        "abstract": "We consider a one-hidden-layer leaky ReLU network of arbitrary width trained by stochastic gradient descent (SGD) following an arbitrary initialization.  We prove that SGD  produces neural networks that have classification accuracy competitive with that of the best halfspace over the distribution for a broad class of distributions that includes log-concave isotropic and hard margin distributions.  Equivalently, such networks can generalize when the data distribution is linearly separable but corrupted with adversarial label noise, despite the capacity to overfit.  To the best of our knowledge, this is the first work to show that overparameterized neural networks trained by SGD can generalize when the data is corrupted with adversarial label noise.  ",
        "conference": "ICML",
        "中文标题": "在存在对抗性标签噪声的情况下，任意宽度SGD训练的神经网络的可证明泛化性",
        "摘要翻译": "我们考虑一个任意宽度的单隐藏层泄漏ReLU网络，该网络通过随机梯度下降（SGD）在任意初始化后进行训练。我们证明，对于包括对数凹各向同性和硬边缘分布在内的广泛分布类别，SGD产生的神经网络具有与分布上最佳半空间相竞争的分类准确性。等效地，尽管有能力过拟合，这样的网络可以在数据分布是线性可分但被对抗性标签噪声破坏时泛化。据我们所知，这是首次展示通过SGD训练的过参数化神经网络在数据被对抗性标签噪声破坏时能够泛化的工作。",
        "领域": "深度学习理论、对抗性学习、神经网络泛化性",
        "问题": "研究在对抗性标签噪声存在的情况下，过参数化神经网络通过SGD训练后能否实现泛化。",
        "动机": "探索神经网络在对抗性环境下的泛化能力，特别是在数据标签被恶意篡改的情况下。",
        "方法": "使用随机梯度下降（SGD）训练任意宽度的单隐藏层泄漏ReLU网络，分析其在对抗性标签噪声下的泛化性能。",
        "关键词": [
            "对抗性标签噪声",
            "神经网络泛化",
            "随机梯度下降",
            "过参数化",
            "泄漏ReLU"
        ],
        "涉及的技术概念": {
            "随机梯度下降（SGD）": "用于训练神经网络，优化模型参数以适应数据分布。",
            "泄漏ReLU": "一种激活函数，允许小的负输入值通过，有助于缓解神经元死亡问题。",
            "对抗性标签噪声": "指在训练数据中故意引入的错误标签，旨在测试模型的鲁棒性和泛化能力。"
        },
        "success": true
    },
    {
        "order": 875,
        "title": "Provable Lipschitz Certification for Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/9171",
        "abstract": "We present a scalable technique for upper bounding the Lipschitz constant of generative models. We relate this quantity to the maximal norm over the set of attainable vector-Jacobian products of a given generative model. We approximate this set by layerwise convex approximations using zonotopes. Our approach generalizes and improves upon prior work using zonotope transformers and we extend to Lipschitz estimation of neural networks with large output dimension. This provides efficient and tight bounds on small networks and can scale to generative models on VAE and DCGAN architectures. ",
        "conference": "ICML",
        "中文标题": "可证明的生成模型Lipschitz认证",
        "摘要翻译": "我们提出了一种可扩展的技术，用于上界生成模型的Lipschitz常数。我们将这一量与给定生成模型可达到的向量-Jacobian乘积的最大范数联系起来。我们使用zonotopes通过逐层凸近似来近似这个集合。我们的方法推广并改进了之前使用zonotope变换器的工作，并且我们扩展到了具有大输出维度的神经网络的Lipschitz估计。这为小型网络提供了高效且紧密的界限，并且可以扩展到VAE和DCGAN架构上的生成模型。",
        "领域": "生成对抗网络、变分自编码器、深度学习理论",
        "问题": "如何高效且准确地估计生成模型的Lipschitz常数",
        "动机": "为了提供生成模型稳定性和鲁棒性的理论保证，需要准确估计其Lipschitz常数",
        "方法": "通过将Lipschitz常数与向量-Jacobian乘积的最大范数关联，并使用zonotopes进行逐层凸近似来估计",
        "关键词": [
            "Lipschitz常数",
            "生成模型",
            "zonotopes",
            "向量-Jacobian乘积",
            "深度学习理论"
        ],
        "涉及的技术概念": {
            "Lipschitz常数": "用于衡量生成模型输出对输入变化的敏感度，是模型稳定性和鲁棒性的重要指标",
            "zonotopes": "用于逐层近似生成模型的输出空间，以估计Lipschitz常数",
            "向量-Jacobian乘积": "在生成模型中，用于计算输出相对于输入的梯度，是估计Lipschitz常数的关键"
        },
        "success": true
    },
    {
        "order": 876,
        "title": "Provable Meta-Learning of Linear Representations",
        "html": "https://ICML.cc//virtual/2021/poster/9333",
        "abstract": "Meta-learning, or learning-to-learn, seeks to design algorithms that can utilize previous experience to rapidly learn new skills or adapt to new environments. Representation learning---a key tool for performing meta-learning---learns a data representation that can transfer knowledge across multiple tasks, which is essential in regimes where data is scarce. Despite a recent surge of interest in the practice of meta-learning, the theoretical underpinnings of meta-learning algorithms are lacking, especially in the context of learning transferable representations. In this paper, we focus on the problem of multi-task linear regression---in which multiple linear regression models share a common, low-dimensional linear representation. Here, we provide provably fast, sample-efficient algorithms to address the dual challenges of (1) learning a common set of features from multiple, related tasks, and (2) transferring this knowledge to new, unseen tasks. Both are central to the general problem of meta-learning. Finally, we complement these results by providing information-theoretic lower bounds on the sample complexity of learning these linear features.",
        "conference": "ICML",
        "中文标题": "可证明的线性表示元学习",
        "摘要翻译": "元学习，或称学会学习，旨在设计能够利用以往经验快速学习新技能或适应新环境的算法。表示学习——执行元学习的关键工具——学习一种能够跨多个任务转移知识的数据表示，这在数据稀缺的情况下尤为重要。尽管最近对元学习实践的兴趣激增，但元学习算法的理论基础仍然缺乏，特别是在学习可转移表示的背景下。在本文中，我们专注于多任务线性回归问题——其中多个线性回归模型共享一个共同的、低维线性表示。在这里，我们提供了可证明快速、样本高效的算法，以解决（1）从多个相关任务中学习一组共同特征，以及（2）将这些知识转移到新的、未见过的任务的双重挑战。这两者对于元学习的一般问题都是核心。最后，我们通过提供关于学习这些线性特征的样本复杂度的信息理论下界来补充这些结果。",
        "领域": "元学习、表示学习、多任务学习",
        "问题": "解决在多任务线性回归中学习共同低维线性表示以及将这种表示转移到新任务的问题。",
        "动机": "缺乏元学习算法，特别是在学习可转移表示方面的理论基础。",
        "方法": "提供可证明快速、样本高效的算法，用于学习共同特征集并转移知识到新任务。",
        "关键词": [
            "元学习",
            "表示学习",
            "多任务学习",
            "线性回归",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "元学习": "设计能够利用以往经验快速学习新技能或适应新环境的算法。",
            "表示学习": "学习一种能够跨多个任务转移知识的数据表示。",
            "多任务线性回归": "多个线性回归模型共享一个共同的、低维线性表示的问题。"
        },
        "success": true
    },
    {
        "order": 877,
        "title": " Provable Robustness of Adversarial Training for Learning Halfspaces with Noise",
        "html": "https://ICML.cc//virtual/2021/poster/8973",
        "abstract": "We analyze the properties of adversarial training for learning adversarially robust halfspaces in the presence of agnostic label noise.  Denoting $\\mathsf{OPT}_{p,r}$ as the best classification error achieved by a halfspace that is robust to perturbations of $\\ell^{p}$ balls of radius $r$, we show that adversarial training on the standard binary cross-entropy loss yields adversarially robust halfspaces up to classification error $\\tilde O(\\sqrt{\\mathsf{OPT}_{2,r}})$ for $p=2$, and $\\tilde O(d^{1/4} \\sqrt{\\mathsf{OPT}_{\\infty, r}})$ when $p=\\infty$.   Our results hold for distributions satisfying anti-concentration properties enjoyed by log-concave isotropic distributions among others.   We additionally show that if one instead uses a non-convex sigmoidal loss, adversarial training yields halfspaces with an improved robust classification error of $O(\\mathsf{OPT}_{2,r})$ for $p=2$, and $O(d^{1/4} \\mathsf{OPT}_{\\infty, r})$ when $p=\\infty$. To the best of our knowledge, this is the first work showing that adversarial training provably yields robust classifiers in the presence of noise.  ",
        "conference": "ICML",
        "success": true,
        "中文标题": "噪声环境下学习半空间的对抗训练的可证明鲁棒性",
        "摘要翻译": "我们分析了在存在不可知标签噪声的情况下，学习对抗性鲁棒半空间的对抗训练的性质。将$\\mathsf{OPT}_{p,r}$定义为在$\\ell^{p}$球半径$r$的扰动下，由半空间实现的最佳分类误差，我们展示了在标准二元交叉熵损失上进行对抗训练，可以得到分类误差为$\\tilde O(\\sqrt{\\mathsf{OPT}_{2,r}})$（当$p=2$时）和$\\tilde O(d^{1/4} \\sqrt{\\mathsf{OPT}_{\\infty, r}})$（当$p=\\infty$时）的对抗性鲁棒半空间。我们的结果适用于满足反集中性质的分布，如对数凹各向同性分布等。此外，我们还展示了如果使用非凸sigmoidal损失，对抗训练可以得到改进的鲁棒分类误差$O(\\mathsf{OPT}_{2,r})$（当$p=2$时）和$O(d^{1/4} \\mathsf{OPT}_{\\infty, r})$（当$p=\\infty$时）。据我们所知，这是首次展示对抗训练在噪声存在下可证明产生鲁棒分类器的工作。",
        "领域": "对抗学习, 鲁棒机器学习, 半空间学习",
        "问题": "在存在不可知标签噪声的情况下，如何通过对抗训练学习到对抗性鲁棒的半空间。",
        "动机": "研究在噪声环境下，对抗训练能否以及如何产生鲁棒的分类器，特别是在半空间学习的情境下。",
        "方法": "通过分析在标准二元交叉熵损失和非凸sigmoidal损失上进行对抗训练的效果，展示了对抗训练在不同$p$范数扰动下产生鲁棒半空间的能力。",
        "关键词": [
            "对抗训练",
            "半空间学习",
            "鲁棒分类",
            "标签噪声",
            "非凸优化"
        ],
        "涉及的技术概念": {
            "对抗训练": "通过在训练过程中引入对抗性扰动来提高模型的鲁棒性。",
            "半空间学习": "一种学习模型，用于将数据点分类为两个类别之一，基于数据点是否位于某个超平面的一侧。"
        }
    },
    {
        "order": 878,
        "title": "Provably Correct Optimization and Exploration with Non-linear Policies",
        "html": "https://ICML.cc//virtual/2021/poster/9303",
        "abstract": "Policy optimization methods remain a powerful workhorse in empirical Reinforcement Learning (RL), with a focus on neural policies that can easily reason over complex and continuous state and/or action spaces. Theoretical understanding of strategic exploration in policy-based methods with non-linear function approximation, however, is largely missing. In this paper, we address this question by designing ENIAC, an actor-critic method that allows non-linear function approximation in the critic. We show that under certain assumptions, e.g., a bounded eluder dimension $d$ for the critic class, the learner finds to a near-optimal policy in $\\widetilde{O}(\\mathrm{poly}(d))$ exploration rounds. The method is robust to model misspecification and strictly extends existing works on linear function approximation. We also develop some computational optimizations of our approach with slightly worse statistical guarantees, and an empirical adaptation building on existing deep RL tools. We empirically evaluate this adaptation, and show that it outperforms prior heuristics inspired by linear methods, establishing the value in correctly reasoning about the agent's uncertainty under non-linear function approximation.",
        "conference": "ICML",
        "success": true,
        "中文标题": "可证明正确的非线性策略优化与探索",
        "摘要翻译": "策略优化方法在实证强化学习（RL）中仍然是一个强大的工具，主要集中在能够轻松处理复杂和连续状态及/或动作空间的神经策略上。然而，对于基于策略的方法中非线性函数逼近的战略探索的理论理解，目前还 largely missing。在本文中，我们通过设计ENIAC来解决这个问题，ENIAC是一种允许在critic中使用非线性函数逼近的actor-critic方法。我们表明，在某些假设下，例如critic类的有界eluder维度d，学习者在O~(poly(d))探索轮次中找到接近最优的策略。该方法对模型错误设定具有鲁棒性，并严格扩展了现有关于线性函数逼近的工作。我们还开发了一些计算优化方法，这些方法具有稍差的统计保证，以及一个基于现有深度RL工具的经验性适应。我们实证评估了这一适应，并表明它优于受线性方法启发的先前启发式方法，确立了在非线性函数逼近下正确推理代理不确定性的价值。",
        "领域": "强化学习, 策略优化, 非线性函数逼近",
        "问题": "解决在非线性函数逼近下策略优化方法的战略探索问题",
        "动机": "填补基于策略的方法中非线性函数逼近的战略探索理论理解的空白",
        "方法": "设计ENIAC，一种允许在critic中使用非线性函数逼近的actor-critic方法，并在特定假设下证明其有效性",
        "关键词": [
            "强化学习",
            "策略优化",
            "非线性函数逼近",
            "actor-critic方法",
            "战略探索"
        ],
        "涉及的技术概念": {
            "非线性函数逼近": "在critic中使用非线性函数逼近，以处理更复杂的状态和动作空间",
            "eluder维度": "用于衡量函数类复杂度的概念，本文中假设critic类的eluder维度有界",
            "actor-critic方法": "一种结合策略梯度（actor）和价值函数（critic）的强化学习方法，本文中扩展了其在非线性函数逼近下的应用"
        }
    },
    {
        "order": 879,
        "title": "Provably Efficient Algorithms for Multi-Objective Competitive RL",
        "html": "https://ICML.cc//virtual/2021/poster/9339",
        "abstract": "We study multi-objective reinforcement learning (RL) where an agent's reward is represented as a vector. In settings where an  agent competes against opponents, its performance is measured by the distance of its average return vector to a target set. We develop statistically and computationally efficient algorithms to approach the associated target set. Our results extend Blackwell's approachability theorem~\\citep{blackwell1956analog} to tabular RL, where strategic exploration becomes essential. The algorithms presented are adaptive; their guarantees hold even without Blackwell's approachability condition. If the opponents use fixed policies, we give an improved rate of approaching the target set while also tackling the more ambitious goal of simultaneously minimizing a scalar cost function. We discuss our analysis for this special case by relating our results to previous works on constrained RL. To our knowledge, this work provides the first provably efficient algorithms for vector-valued Markov games and our theoretical guarantees are near-optimal.",
        "conference": "ICML",
        "中文标题": "可证明高效的多目标竞争强化学习算法",
        "摘要翻译": "我们研究了多目标强化学习（RL），其中智能体的奖励被表示为向量。在智能体与对手竞争的设置中，其性能通过其平均回报向量到目标集的距离来衡量。我们开发了统计和计算上高效的算法来接近相关的目标集。我们的结果将Blackwell的可接近性定理扩展到表格RL，其中策略性探索变得至关重要。提出的算法是自适应的；即使没有Blackwell的可接近性条件，它们的保证也成立。如果对手使用固定策略，我们给出了接近目标集的改进速率，同时解决了同时最小化标量成本函数的更雄心勃勃的目标。我们通过将我们的结果与之前关于约束RL的工作联系起来，讨论了我们对这一特殊案例的分析。据我们所知，这项工作为向量值马尔可夫游戏提供了第一个可证明高效的算法，我们的理论保证接近最优。",
        "领域": "多目标强化学习、竞争性强化学习、马尔可夫决策过程",
        "问题": "在多目标竞争性强化学习环境中，如何高效地接近目标集并优化性能。",
        "动机": "研究多目标强化学习环境中智能体与对手竞争时的性能优化问题，开发高效算法以接近目标集并可能同时最小化标量成本函数。",
        "方法": "扩展Blackwell的可接近性定理至表格RL，开发自适应算法，无需Blackwell条件即可保证性能，针对对手使用固定策略的情况提供改进的接近速率。",
        "关键词": [
            "多目标强化学习",
            "竞争性强化学习",
            "Blackwell可接近性",
            "自适应算法",
            "马尔可夫游戏"
        ],
        "涉及的技术概念": {
            "多目标强化学习": "研究智能体在奖励为向量形式的环境中的学习问题，旨在同时优化多个目标。",
            "Blackwell可接近性定理": "在博弈论中，用于判断玩家是否能够使其平均回报向量接近或进入特定目标集的定理。",
            "自适应算法": "能够根据环境或对手策略的变化自动调整其策略的算法，以提高性能或适应性的算法。"
        },
        "success": true
    },
    {
        "order": 880,
        "title": "Provably Efficient Fictitious Play Policy Optimization for Zero-Sum Markov Games with Structured Transitions",
        "html": "https://ICML.cc//virtual/2021/poster/8773",
        "abstract": "While single-agent policy optimization in a fixed environment has attracted a lot of research attention recently in the reinforcement learning community, much less is known theoretically when there are multiple agents playing in a potentially competitive environment. We take steps forward by proposing and analyzing new fictitious play policy optimization algorithms for two-player zero-sum Markov games with structured but unknown transitions. We consider two classes of transition structures: factored independent transition and single-controller transition. For both scenarios, we prove tight $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bounds after $T$ steps in a two-agent competitive game scenario. The regret of each player is measured against a potentially adversarial opponent who can choose a single best policy in hindsight after observing the full policy sequence. Our algorithms feature a combination of Upper Confidence Bound (UCB)-type optimism and fictitious play under the scope of simultaneous policy optimization in a non-stationary environment. When both players adopt the proposed algorithms, their overall optimality gap is $\\widetilde{\\mathcal{O}}(\\sqrt{T})$.",
        "conference": "ICML",
        "success": true,
        "中文标题": "可证明高效虚构玩法策略优化用于具有结构化转移的零和马尔可夫博弈",
        "摘要翻译": "尽管固定环境中的单智能体策略优化最近在强化学习社区中吸引了大量研究关注，但当多个智能体在潜在竞争环境中游戏时，理论上的了解却少得多。我们通过为具有结构化但未知转移的两人零和马尔可夫博弈提出并分析新的虚构玩法策略优化算法，向前迈出了几步。我们考虑了两类转移结构：分解独立转移和单控制器转移。对于这两种情景，我们证明了在两人竞争游戏情景中，经过T步后，紧致的$\\widetilde{\\mathcal{O}}(\\sqrt{T})$遗憾界。每个玩家的遗憾是针对一个潜在对抗性的对手来衡量的，该对手可以在观察到完整策略序列后选择事后最佳策略。我们的算法结合了上置信界（UCB）类型的乐观主义和虚构玩法，在非平稳环境中同时进行策略优化的范围内。当两个玩家都采用所提出的算法时，他们的整体最优性差距是$\\widetilde{\\mathcal{O}}(\\sqrt{T})$。",
        "领域": "多智能体强化学习, 博弈论, 策略优化",
        "问题": "在具有结构化但未知转移的两人零和马尔可夫博弈中，如何有效地进行策略优化以最小化遗憾。",
        "动机": "研究在竞争性多智能体环境中，如何通过算法优化策略以减少与最优策略的差距，特别是在转移结构未知的情况下。",
        "方法": "提出并分析了结合上置信界（UCB）类型乐观主义和虚构玩法的策略优化算法，用于处理具有分解独立转移和单控制器转移的两人零和马尔可夫博弈。",
        "关键词": [
            "虚构玩法",
            "策略优化",
            "零和博弈",
            "马尔可夫决策过程",
            "遗憾最小化"
        ],
        "涉及的技术概念": {
            "虚构玩法": "在非平稳环境中，通过模拟对手的策略来优化自身策略的方法。",
            "上置信界（UCB）": "一种用于平衡探索与利用的乐观主义算法，用于在不确定性下做出决策。"
        }
    },
    {
        "order": 881,
        "title": "Provably Efficient Learning of Transferable Rewards ",
        "html": "https://ICML.cc//virtual/2021/poster/10567",
        "abstract": "The reward function is widely accepted as a succinct, robust, and transferable representation of a task. Typical approaches, at the basis of Inverse Reinforcement Learning (IRL), leverage on expert demonstrations to recover a reward function. In this paper, we study the theoretical properties of the class of reward functions that are compatible with the expert’s behavior. We analyze how the limited knowledge of the expert’s policy and of the environment affects the reward reconstruction phase. Then, we examine how the error propagates to the learned policy’s performance when transferring the reward function to a different environment. We employ these findings to devise a provably efficient active sampling approach, aware of the need for transferring the reward function, that can be paired with a large variety of IRL algorithms. Finally, we provide numerical simulations on benchmark environments.",
        "conference": "ICML",
        "中文标题": "可证明高效学习可转移奖励",
        "摘要翻译": "奖励函数被广泛认为是任务的一种简洁、鲁棒且可转移的表示。基于逆强化学习（IRL）的典型方法利用专家演示来恢复奖励函数。在本文中，我们研究了与专家行为兼容的奖励函数类的理论性质。我们分析了专家策略和环境有限知识如何影响奖励重建阶段。然后，我们考察了当将奖励函数转移到不同环境时，错误如何传播到学习策略的性能上。我们利用这些发现设计了一种可证明高效的活动采样方法，该方法意识到转移奖励函数的需求，可以与多种IRL算法配对使用。最后，我们在基准环境上提供了数值模拟。",
        "领域": "逆强化学习、奖励函数学习、策略转移",
        "问题": "研究奖励函数在转移过程中的理论性质及其对学习策略性能的影响",
        "动机": "理解奖励函数在转移过程中的理论限制，并开发高效的学习方法",
        "方法": "分析奖励函数的理论性质，设计高效的活动采样方法，并与多种IRL算法结合使用",
        "关键词": [
            "奖励函数",
            "逆强化学习",
            "策略转移",
            "活动采样",
            "理论分析"
        ],
        "涉及的技术概念": {
            "奖励函数": "作为任务表示的核心，用于指导学习策略的优化",
            "逆强化学习": "通过专家演示恢复奖励函数的方法",
            "活动采样": "一种高效的学习方法，专注于转移奖励函数的需求"
        },
        "success": true
    },
    {
        "order": 882,
        "title": "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping",
        "html": "https://ICML.cc//virtual/2021/poster/9889",
        "abstract": " Modern tasks in reinforcement learning have large state and action spaces. To deal with them efficiently, one often uses predefined feature mapping to represent states and actions in a low dimensional space. In this paper, we study reinforcement learning for discounted Markov Decision Processes (MDPs), where the transition kernel can be parameterized as a linear function of certain feature mapping. We propose a novel algorithm which makes use of the feature mapping and obtains a $\\tilde O(d\\sqrt{T}/(1-\\gamma)^2)$ regret, where $d$ is the dimension of the feature space, $T$ is the time horizon and $\\gamma$ is the discount factor of the MDP. To the best of our knowledge, this is the first polynomial regret bound without accessing a generative model or making strong assumptions such as ergodicity of the MDP. By constructing a special class of MDPs, we also show that for any algorithms, the regret is lower bounded by  $\\Omega(d\\sqrt{T}/(1-\\gamma)^{1.5})$. Our upper and lower bound results together suggest that the proposed reinforcement learning algorithm is near-optimal up to a $(1-\\gamma)^{-0.5}$ factor.",
        "conference": "ICML",
        "success": true,
        "中文标题": "具有特征映射的折扣MDP的可证明高效强化学习",
        "摘要翻译": "现代强化学习任务具有庞大的状态和动作空间。为了高效处理这些任务，人们常常使用预定义的特征映射将状态和动作表示在低维空间中。本文研究了折扣马尔可夫决策过程（MDPs）的强化学习，其中转移核可以参数化为特定特征映射的线性函数。我们提出了一种新算法，该算法利用特征映射并获得了$\\\\tilde O(d\\\\sqrt{T}/(1-\\\\gamma)^2)$的遗憾，其中$d$是特征空间的维度，$T$是时间范围，$\\\\gamma$是MDP的折扣因子。据我们所知，这是第一个在不访问生成模型或做出诸如MDP的遍历性等强假设的情况下，获得多项式遗憾界的算法。通过构建一类特殊的MDPs，我们还表明，对于任何算法，遗憾的下限是$\\\\Omega(d\\\\sqrt{T}/(1-\\\\gamma)^{1.5})$。我们的上下界结果共同表明，所提出的强化学习算法在$(1-\\\\gamma)^{-0.5}$因子内是接近最优的。",
        "领域": "强化学习、马尔可夫决策过程、特征映射",
        "问题": "如何在具有大状态和动作空间的强化学习任务中，高效地利用特征映射来降低维度并优化学习过程。",
        "动机": "解决在大型状态和动作空间中，传统强化学习方法效率低下，难以直接应用的问题。",
        "方法": "提出了一种新算法，利用特征映射将状态和动作表示在低维空间，并获得了接近最优的遗憾界。",
        "关键词": [
            "强化学习",
            "马尔可夫决策过程",
            "特征映射",
            "遗憾界",
            "折扣因子"
        ],
        "涉及的技术概念": {
            "特征映射": "用于将高维的状态和动作空间映射到低维空间，以提高学习效率。",
            "遗憾界": "衡量算法性能的指标，表示算法与最优策略之间的性能差距。"
        }
    },
    {
        "order": 883,
        "title": "Provably End-to-end Label-noise Learning without Anchor Points",
        "html": "https://ICML.cc//virtual/2021/poster/10491",
        "abstract": "In label-noise learning, the transition matrix plays a key role in building statistically consistent classifiers. Existing consistent estimators for the transition matrix have been developed by exploiting anchor points. However, the anchor-point assumption is not always satisfied in real scenarios. In this paper, we propose an end-to-end framework for solving label-noise learning without anchor points, in which we simultaneously optimize two objectives: the cross entropy loss between the noisy label and the predicted probability by the neural network, and the volume of the simplex formed by the columns of the transition matrix. Our proposed framework can identify the transition matrix if the clean class-posterior probabilities are sufficiently scattered. This is by far the mildest assumption under which the transition matrix is provably identifiable and the learned classifier is statistically consistent. Experimental results on benchmark datasets demonstrate the effectiveness and robustness of the proposed method.",
        "conference": "ICML",
        "中文标题": "无需锚点的可证明端到端标签噪声学习",
        "摘要翻译": "在标签噪声学习中，转移矩阵在构建统计一致分类器中起着关键作用。现有的转移矩阵一致估计器通过利用锚点开发而来。然而，在实际场景中，锚点假设并不总是满足。本文提出了一种无需锚点的端到端框架来解决标签噪声学习问题，在该框架中，我们同时优化两个目标：噪声标签与神经网络预测概率之间的交叉熵损失，以及转移矩阵列形成的单纯形的体积。我们提出的框架可以识别转移矩阵，如果干净的类后验概率足够分散。这是迄今为止转移矩阵可证明可识别且学习到的分类器统计一致的最温和假设。基准数据集上的实验结果证明了所提出方法的有效性和鲁棒性。",
        "领域": "标签噪声学习、统计学习理论、深度学习",
        "问题": "解决在缺乏锚点的情况下，如何构建统计一致分类器的问题",
        "动机": "现实场景中锚点假设不总是成立，需要一种不依赖锚点的方法来学习标签噪声",
        "方法": "同时优化交叉熵损失和转移矩阵列形成的单纯形体积的端到端框架",
        "关键词": [
            "标签噪声学习",
            "转移矩阵",
            "端到端学习",
            "统计一致性",
            "无锚点"
        ],
        "涉及的技术概念": {
            "转移矩阵": "用于描述干净标签和噪声标签之间关系的矩阵，是构建统计一致分类器的关键",
            "端到端学习": "直接从输入到输出进行学习，无需中间步骤或锚点",
            "统计一致性": "确保学习到的分类器在数据量增加时收敛到最优分类器的性质"
        },
        "success": true
    },
    {
        "order": 884,
        "title": "Provably Strict Generalisation Benefit for Equivariant Models",
        "html": "https://ICML.cc//virtual/2021/poster/10385",
        "abstract": "It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.",
        "conference": "ICML",
        "中文标题": "可证明的等变模型严格泛化优势",
        "摘要翻译": "人们普遍认为，设计一个具有不变性/等变性的模型可以提高泛化能力。尽管这种方法越来越受欢迎，但对于泛化优势的精确描述仍然缺乏。通过考虑线性模型的最简单情况，本文首次证明了当目标分布相对于一个紧致群具有不变性/等变性时，不变性/等变性模型在泛化能力上的非零改进。此外，我们的工作揭示了泛化能力、训练样本数量与群作用性质之间有趣的关系。我们的结果基于对平均算子下函数空间结构的观察，这一观察及其对特征平均的影响可能具有独立的意义。",
        "领域": "深度学习理论、群等变网络、模型泛化",
        "问题": "精确描述不变性/等变性模型在泛化能力上的优势",
        "动机": "尽管不变性/等变性模型在提高泛化能力方面被广泛认可，但缺乏对其泛化优势的严格理论证明",
        "方法": "通过分析线性模型在紧致群作用下的泛化行为，提供理论证明",
        "关键词": [
            "等变模型",
            "泛化优势",
            "群作用",
            "线性模型",
            "特征平均"
        ],
        "涉及的技术概念": {
            "等变模型": "模型输出随输入在群作用下的变换而相应变换，保持结构一致性",
            "泛化优势": "模型在未见数据上的表现优于不具有特定结构性质的模型",
            "紧致群": "一种具有紧致性质的数学群结构，用于描述变换的对称性"
        },
        "success": true
    },
    {
        "order": 885,
        "title": "Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction",
        "html": "https://ICML.cc//virtual/2021/poster/9925",
        "abstract": "We address the problem of causal effect estima-tion in the presence of unobserved confounding,but where proxies for the latent confounder(s) areobserved.  We propose two kernel-based meth-ods for nonlinear causal effect estimation in thissetting: (a) a two-stage regression approach, and(b) a maximum moment restriction approach. Wefocus on the proximal causal learning setting, butour methods can be used to solve a wider classof inverse problems characterised by a Fredholmintegral equation. In particular, we provide a uni-fying view of two-stage and moment restrictionapproaches for solving this problem in a nonlin-ear setting.  We provide consistency guaranteesfor each algorithm, and demonstrate that these ap-proaches achieve competitive results on syntheticdata and data simulating a real-world task. In par-ticular, our approach outperforms earlier methodsthat are not suited to leveraging proxy variables.",
        "conference": "ICML",
        "中文标题": "基于核的近端因果学习：两阶段估计与矩限制",
        "摘要翻译": "我们解决了在存在未观测混杂但观测到潜在混杂因素的代理变量情况下的因果效应估计问题。我们提出了两种基于核的非线性因果效应估计方法：(a) 两阶段回归方法，和 (b) 最大矩限制方法。我们专注于近端因果学习设置，但我们的方法可用于解决由Fredholm积分方程表征的更广泛类别的逆问题。特别是，我们提供了在非线性设置中解决这一问题的两阶段和矩限制方法的统一视角。我们为每种算法提供了一致性保证，并证明这些方法在合成数据和模拟真实世界任务的数据上取得了竞争性的结果。特别是，我们的方法优于早期不适合利用代理变量的方法。",
        "领域": "因果推断、机器学习、统计学习",
        "问题": "在存在未观测混杂但观测到潜在混杂因素的代理变量情况下的因果效应估计",
        "动机": "解决在非线性设置中，利用代理变量进行因果效应估计的问题，特别是在存在未观测混杂的情况下",
        "方法": "提出了两种基于核的方法：两阶段回归方法和最大矩限制方法，用于非线性因果效应估计",
        "关键词": [
            "近端因果学习",
            "核方法",
            "两阶段回归",
            "最大矩限制",
            "Fredholm积分方程"
        ],
        "涉及的技术概念": {
            "近端因果学习": "在存在未观测混杂但观测到潜在混杂因素的代理变量情况下进行因果效应估计的框架",
            "核方法": "用于非线性因果效应估计的技术，通过核函数在高维空间中线性化问题",
            "Fredholm积分方程": "描述了一类逆问题的数学框架，本文方法可用于解决这类问题"
        },
        "success": true
    },
    {
        "order": 886,
        "title": "PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10119",
        "abstract": "We study reinforcement learning (RL) with no-reward demonstrations, a setting in which an RL agent has access to additional data from the interaction of other agents with the same environment. However, it has no access to the rewards or goals of these agents, and their objectives and levels of expertise may vary widely. These assumptions are common in multi-agent settings, such as autonomous driving. To effectively use this data, we turn to the framework of successor features. This allows us to disentangle shared features and dynamics of the environment from agent-specific rewards and policies. We propose a multi-task inverse reinforcement learning (IRL) algorithm, called \\emph{inverse temporal difference learning} (ITD), that learns shared state features, alongside per-agent successor features and preference vectors, purely from demonstrations without reward labels. We further show how to seamlessly integrate ITD with learning from online environment interactions, arriving at a novel algorithm for reinforcement learning with demonstrations, called $\\Psi \\Phi$-learning (pronounced `Sci-Fi'). We provide empirical evidence for the effectiveness of $\\Psi \\Phi$-learning as a method for improving RL, IRL, imitation, and few-shot transfer, and derive worst-case bounds for its performance in zero-shot transfer to new tasks.",
        "conference": "ICML",
        "中文标题": "PsiPhi学习：利用后继特征和逆时序差分学习的示范强化学习",
        "摘要翻译": "我们研究了无奖励示范的强化学习（RL），在这种设置下，RL代理可以访问其他代理与同一环境互动的额外数据。然而，它无法访问这些代理的奖励或目标，且它们的目标和专业知识水平可能差异很大。这些假设在多代理设置中很常见，例如自动驾驶。为了有效利用这些数据，我们转向后继特征的框架。这使我们能够将环境的共享特征和动态与代理特定的奖励和策略分离。我们提出了一种多任务逆强化学习（IRL）算法，称为逆时序差分学习（ITD），该算法仅从无奖励标签的示范中学习共享状态特征，以及每个代理的后继特征和偏好向量。我们进一步展示了如何将ITD与从在线环境互动中学习无缝集成，提出了一种名为ΨΦ学习（发音为'Sci-Fi'）的示范强化学习新算法。我们提供了ΨΦ学习作为改进RL、IRL、模仿和少样本转移方法的有效性的实证证据，并推导了其在零样本转移到新任务中性能的最坏情况界限。",
        "领域": "强化学习",
        "问题": "如何在无奖励示范的情况下有效利用多代理互动数据进行强化学习",
        "动机": "解决在多代理环境中，由于代理目标和专业知识水平差异大，无法直接利用示范数据进行强化学习的问题",
        "方法": "提出逆时序差分学习（ITD）算法，结合后继特征框架，从无奖励示范中学习共享状态特征和代理特定特征，并与在线学习集成",
        "关键词": [
            "强化学习",
            "逆强化学习",
            "后继特征",
            "多任务学习",
            "示范学习"
        ],
        "涉及的技术概念": {
            "后继特征": "用于分离环境的共享特征和动态与代理特定的奖励和策略",
            "逆时序差分学习": "一种多任务逆强化学习算法，从无奖励示范中学习共享状态特征和代理特定特征",
            "ΨΦ学习": "结合ITD和在线学习的示范强化学习新算法，用于改进RL、IRL、模仿和少样本转移"
        },
        "success": true
    },
    {
        "order": 887,
        "title": "Pure Exploration and Regret Minimization in Matching Bandits",
        "html": "https://ICML.cc//virtual/2021/poster/10347",
        "abstract": "Finding an optimal matching in a weighted graph is a standard combinatorial problem. We consider its  semi-bandit version where either a pair or a full matching is sampled sequentially. We prove that it is possible to leverage a rank-1 assumption on the adjacency matrix to reduce the sample complexity and the regret of off-the-shelf algorithms up to reaching a linear dependency in the number of vertices (up to to poly-log terms).",
        "conference": "ICML",
        "中文标题": "匹配赌博机中的纯探索与遗憾最小化",
        "摘要翻译": "在加权图中寻找最优匹配是一个标准的组合问题。我们考虑了其半赌博机版本，其中要么顺序采样一对，要么采样一个完整的匹配。我们证明，可以利用邻接矩阵的秩-1假设来降低现成算法的样本复杂度和遗憾，直至达到顶点数量的线性依赖（直至多对数项）。",
        "领域": "强化学习、组合优化、在线学习",
        "问题": "在加权图的半赌博机版本中寻找最优匹配，同时最小化样本复杂度和遗憾。",
        "动机": "研究如何利用邻接矩阵的秩-1假设来优化现有算法，以减少在寻找最优匹配过程中的样本复杂度和遗憾。",
        "方法": "通过利用邻接矩阵的秩-1假设，优化现成算法以减少样本复杂度和遗憾，实现顶点数量的线性依赖。",
        "关键词": [
            "匹配赌博机",
            "纯探索",
            "遗憾最小化",
            "半赌博机",
            "秩-1假设"
        ],
        "涉及的技术概念": {
            "半赌博机": "在顺序采样过程中，每次可以选择一个动作（如选择一对顶点或一个完整匹配）并观察其结果的模型。",
            "秩-1假设": "假设邻接矩阵的秩为1，这可以简化问题并减少算法的样本复杂度和遗憾。",
            "遗憾最小化": "在在线学习或强化学习中，通过算法减少与最优策略相比的累积损失。"
        },
        "success": true
    },
    {
        "order": 888,
        "title": "Putting the ``Learning' into Learning-Augmented Algorithms for Frequency Estimation",
        "html": "https://ICML.cc//virtual/2021/poster/10501",
        "abstract": "In learning-augmented algorithms, algorithms are enhanced using information from a machine learning algorithm. In turn, this suggests that we should tailor our machine-learning approach for the target algorithm.  We here consider this synergy in the context of the learned count-min sketch from (Hsu et al., 2019).  Learning here is used to predict heavy hitters from a data stream, which are counted explicitly outside the sketch.  We show that an approximately sufficient statistic for the performance of the underlying count-min sketch is given by the coverage of the predictor, or the normalized $L^1$ norm of keys that are filtered by the predictor to be explicitly counted. We show that machine learning models which are trained to optimize for coverage lead to large improvements in performance over prior approaches according to the average absolute frequency error. Our source code can be found at https://github.com/franklynwang/putting-the-learning-in-LAA.",
        "conference": "ICML",
        "中文标题": "将‘学习’融入频率估计的学习增强算法",
        "摘要翻译": "在学习增强算法中，算法通过机器学习算法提供的信息得到增强。这反过来表明，我们应该针对目标算法定制我们的机器学习方法。我们在此考虑了（Hsu等人，2019年）提出的学习计数最小草图背景下的这种协同作用。这里的学习用于预测数据流中的重击者，这些重击者在草图之外被明确计数。我们表明，底层计数最小草图性能的一个近似充分统计量由预测器的覆盖率给出，或者由预测器过滤以明确计数的键的归一化L1范数给出。我们展示了那些被训练以优化覆盖率的机器学习模型，在平均绝对频率误差方面，比之前的方法带来了性能上的大幅提升。我们的源代码可以在https://github.com/franklynwang/putting-the-learning-in-LAA找到。",
        "领域": "数据流分析、机器学习优化、频率估计",
        "问题": "如何通过机器学习优化频率估计算法的性能",
        "动机": "探索机器学习与特定算法（如计数最小草图）之间的协同作用，以提高频率估计的准确性和效率",
        "方法": "通过训练机器学习模型优化预测器的覆盖率，进而提升计数最小草图的性能",
        "关键词": [
            "学习增强算法",
            "频率估计",
            "计数最小草图",
            "机器学习优化",
            "数据流分析"
        ],
        "涉及的技术概念": {
            "学习增强算法": "算法通过机器学习提供的信息进行增强，以提高性能或效率",
            "计数最小草图": "一种用于频率估计的数据结构，通过哈希技术减少存储需求",
            "覆盖率": "预测器覆盖的数据流中键的比例，用于衡量预测器的有效性"
        },
        "success": true
    },
    {
        "order": 889,
        "title": "Quantifying and Reducing Bias in Maximum Likelihood Estimation of Structured Anomalies",
        "html": "https://ICML.cc//virtual/2021/poster/8655",
        "abstract": "Anomaly estimation, or the problem of finding a subset of a dataset that differs from the rest of the dataset, is a classic problem in machine learning and data mining. In both theoretical work and in applications, the anomaly is assumed to have a specific structure defined by membership in an anomaly family. For example, in temporal data the anomaly family may be time intervals, while in network data the anomaly family may be connected subgraphs. The most prominent approach for anomaly estimation is to compute the Maximum Likelihood Estimator (MLE) of the anomaly; however, it was recently observed that for normally distributed data, the MLE is a biased estimator for some anomaly families. \nIn this work, we demonstrate that in the normal means setting, the bias of the MLE depends on the size of the anomaly family. We prove that if the number of sets in the anomaly family that contain the anomaly is sub-exponential, then the MLE is asymptotically unbiased. We also provide empirical evidence that the converse is true: if the number of such sets is exponential, then the MLE is asymptotically biased. Our analysis unifies a number of earlier results on the bias of the MLE for specific anomaly families. Next, we derive a new anomaly estimator using a mixture model, and we prove that our anomaly estimator is asymptotically unbiased regardless of the size of the anomaly family. We illustrate the advantages of our estimator versus the MLE on disease outbreak data and highway traffic data.",
        "conference": "ICML",
        "中文标题": "量化并减少结构化异常最大似然估计中的偏差",
        "摘要翻译": "异常估计，即寻找数据集中与其余部分不同的子集的问题，是机器学习和数据挖掘中的一个经典问题。在理论工作和实际应用中，异常被假定为由异常家族成员资格定义的特定结构。例如，在时间数据中，异常家族可能是时间间隔，而在网络数据中，异常家族可能是连通的子图。异常估计的最主要方法是计算异常的最大似然估计量（MLE）；然而，最近观察到，对于正态分布的数据，MLE对于某些异常家族是有偏估计量。在这项工作中，我们证明了在正态均值设置中，MLE的偏差取决于异常家族的大小。我们证明，如果包含异常的异常家族中的集合数量是次指数的，那么MLE是渐近无偏的。我们还提供了经验证据表明反之亦然：如果这样的集合数量是指数的，那么MLE是渐近有偏的。我们的分析统一了早期关于特定异常家族MLE偏差的若干结果。接下来，我们使用混合模型推导出一个新的异常估计量，并证明我们的异常估计量无论异常家族的大小如何都是渐近无偏的。我们在疾病爆发数据和高速公路交通数据上展示了我们的估计量相对于MLE的优势。",
        "领域": "异常检测、统计机器学习、数据挖掘",
        "问题": "解决在结构化异常的最大似然估计中存在的偏差问题",
        "动机": "发现并解决MLE在特定异常家族中作为估计量时的偏差问题，提高异常估计的准确性",
        "方法": "通过理论证明和混合模型方法，提出一种新的渐近无偏异常估计量",
        "关键词": [
            "最大似然估计",
            "偏差量化",
            "结构化异常",
            "混合模型",
            "渐近无偏"
        ],
        "涉及的技术概念": {
            "最大似然估计（MLE）": "用于估计异常的传统方法，但在某些情况下存在偏差",
            "异常家族": "定义了异常的结构，如时间数据中的时间间隔或网络数据中的连通子图",
            "混合模型": "提出的新方法，用于推导出无论异常家族大小如何都是渐近无偏的异常估计量"
        },
        "success": true
    },
    {
        "order": 890,
        "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability",
        "html": "https://ICML.cc//virtual/2021/poster/8687",
        "abstract": "In this work, we consider how preference models in interactive recommendation systems determine the availability of content and users' opportunities for discovery. We propose an evaluation procedure based on stochastic reachability to quantify the maximum probability of recommending a target piece of content to an user for a set of allowable strategic modifications. This framework allows us to compute an upper bound on the likelihood of recommendation with minimal assumptions about user behavior. Stochastic reachability can be used to detect biases in the availability of content and diagnose limitations in the opportunities for discovery granted to users. We show that this metric can be computed efficiently as a convex program for a variety of  practical settings, and further argue that reachability is not inherently at odds with accuracy. We demonstrate evaluations of recommendation algorithms trained on large datasets of explicit and implicit ratings. Our results illustrate how preference models, selection rules, and user interventions impact reachability and how these effects can be distributed unevenly.",
        "conference": "ICML",
        "中文标题": "通过随机可达性量化推荐系统中的可用性与发现性",
        "摘要翻译": "在这项工作中，我们考虑了交互式推荐系统中的偏好模型如何决定内容的可用性以及用户的发现机会。我们提出了一种基于随机可达性的评估程序，用于量化在允许的策略修改集合内，向用户推荐目标内容的最大概率。这一框架使我们能够在最小化用户行为假设的情况下，计算推荐可能性的上限。随机可达性可用于检测内容可用性中的偏见，并诊断用户发现机会的限制。我们展示了这一指标可以在多种实际设置中作为凸程序高效计算，并进一步论证了可达性与准确性并非天生对立。我们展示了对基于显式和隐式评分的大规模数据集训练的推荐算法的评估结果。我们的结果说明了偏好模型、选择规则和用户干预如何影响可达性，以及这些影响如何可能不均勻分布。",
        "领域": "推荐系统、用户行为分析、内容发现",
        "问题": "量化推荐系统中内容的可用性和用户的发现机会，并检测其中的偏见和限制。",
        "动机": "理解偏好模型如何影响推荐系统中内容的可用性和用户的发现机会，以及如何评估和改善这些方面。",
        "方法": "提出基于随机可达性的评估框架，用于量化推荐概率上限，并通过凸程序高效计算这一指标。",
        "关键词": [
            "随机可达性",
            "推荐系统",
            "内容发现",
            "用户行为分析",
            "偏见检测"
        ],
        "涉及的技术概念": {
            "随机可达性": "用于量化在推荐系统中，向用户推荐特定内容的最大概率，允许对策略进行一定修改。",
            "凸程序": "用于高效计算随机可达性指标，适用于多种实际推荐场景。",
            "偏好模型": "在推荐系统中用于预测用户偏好的模型，影响内容的可用性和用户的发现机会。"
        },
        "success": true
    },
    {
        "order": 891,
        "title": "Quantifying Ignorance in Individual-Level Causal-Effect Estimates under Hidden Confounding",
        "html": "https://ICML.cc//virtual/2021/poster/10143",
        "abstract": "We study the problem of learning conditional average treatment effects (CATE) from high-dimensional, observational data with unobserved confounders. Unobserved confounders introduce ignorance---a level of unidentifiability---about an individual's response to treatment by inducing bias in CATE estimates. We present a new parametric interval estimator suited for high-dimensional data, that estimates a range of possible CATE values when given a predefined bound on the level of hidden confounding. Further, previous interval estimators do not account for ignorance about the CATE associated with samples that may be underrepresented in the original study, or samples that violate the overlap assumption. Our interval estimator also incorporates model uncertainty so that practitioners can be made aware of such out-of-distribution data. We prove that our estimator converges to tight bounds on CATE when there may be unobserved confounding and assess it using semi-synthetic, high-dimensional datasets.",
        "conference": "ICML",
        "中文标题": "量化隐藏混杂因素下个体层面因果效应估计的无知程度",
        "摘要翻译": "我们研究了从高维观测数据中学习条件平均处理效应（CATE）的问题，这些数据存在未观察到的混杂因素。未观察到的混杂因素通过引入CATE估计的偏差，导致了对个体治疗反应的无知——一种不可识别性水平。我们提出了一种新的参数化区间估计器，适用于高维数据，当给定隐藏混杂水平的预定义界限时，它能估计CATE值的可能范围。此外，以往的区间估计器没有考虑到与原始研究中可能代表性不足的样本或违反重叠假设的样本相关的CATE无知。我们的区间估计器还包含了模型不确定性，以便实践者能够意识到这种分布外的数据。我们证明了当可能存在未观察到的混杂因素时，我们的估计器收敛于CATE的严格界限，并使用半合成的高维数据集对其进行了评估。",
        "领域": "因果推断、高维数据分析、机器学习",
        "问题": "在高维观测数据中，存在未观察到的混杂因素时，如何准确估计条件平均处理效应（CATE）的问题。",
        "动机": "解决未观察到的混杂因素导致的CATE估计偏差，以及传统方法无法处理样本代表性不足或违反重叠假设的问题。",
        "方法": "提出了一种新的参数化区间估计器，适用于高维数据，能够估计CATE值的可能范围，并考虑了模型不确定性。",
        "关键词": [
            "条件平均处理效应",
            "高维数据",
            "隐藏混杂因素",
            "区间估计",
            "模型不确定性"
        ],
        "涉及的技术概念": {
            "条件平均处理效应（CATE）": "在给定协变量的条件下，处理组和对照组之间平均结果的差异，用于评估处理效应。",
            "隐藏混杂因素": "未观察到的变量，可能同时影响处理分配和结果，导致因果效应估计的偏差。",
            "区间估计器": "一种统计方法，用于估计参数的可能范围，特别是在存在不确定性的情况下。"
        },
        "success": true
    },
    {
        "order": 892,
        "title": "Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels",
        "html": "https://ICML.cc//virtual/2021/poster/8683",
        "abstract": "We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.",
        "conference": "ICML",
        "中文标题": "量化可微分学习相对于切线核的优势",
        "摘要翻译": "我们研究了在可微分模型（如神经网络）上使用梯度下降学习与使用相应的切线核的相对能力。我们表明，在某些条件下，梯度下降仅当相关的切线核方法相对于随机猜测（即弱学习）获得非平凡优势时才能实现小误差，尽管当梯度下降可以实现任意高精度时，这种优势可能非常小。作为补充，我们表明，在没有这些条件的情况下，梯度下降实际上可以在即使没有任何核方法（特别是使用切线核）能够相对于随机猜测获得非平凡优势的情况下，以小的误差学习。",
        "领域": "深度学习理论、核方法、优化算法",
        "问题": "比较梯度下降在可微分模型上的学习能力与切线核方法的学习能力",
        "动机": "探索梯度下降和切线核方法在不同条件下的学习效率和能力差异",
        "方法": "理论分析梯度下降和切线核方法在不同条件下的学习表现",
        "关键词": [
            "梯度下降",
            "切线核",
            "可微分学习",
            "弱学习",
            "理论分析"
        ],
        "涉及的技术概念": {
            "梯度下降": "用于优化可微分模型参数的迭代方法，通过沿着损失函数的梯度方向更新参数",
            "切线核": "与可微分模型相关联的核函数，用于分析模型在参数空间中的行为",
            "弱学习": "指学习算法能够以略高于随机猜测的准确率进行预测的能力"
        },
        "success": true
    },
    {
        "order": 893,
        "title": "Quantile Bandits for Best Arms Identification",
        "html": "https://ICML.cc//virtual/2021/poster/8699",
        "abstract": "We consider a variant of the best arm identification task in stochastic multi-armed bandits. Motivated by risk-averse decision-making problems, our goal is to identify a set of $m$ arms with the highest $\\tau$-quantile values within a fixed budget. We prove asymmetric two-sided concentration inequalities for order statistics and quantiles of random variables that have non-decreasing hazard rate, which may be of independent interest. With these inequalities, we analyse a quantile version of Successive Accepts and Rejects (Q-SAR). We derive an upper bound for the probability of arm misidentification, the first justification of a quantile based algorithm for fixed budget multiple best arms identification. We show illustrative experiments for best arm identification.",
        "conference": "ICML",
        "中文标题": "用于最佳臂识别的分位数老虎机",
        "摘要翻译": "我们考虑了随机多臂老虎机中最佳臂识别任务的一个变体。受风险规避决策问题的启发，我们的目标是在固定预算内识别出具有最高τ分位数的一组m个臂。我们证明了具有非递减危险率的随机变量的顺序统计量和分位数的不对称双侧集中不等式，这可能具有独立的意义。利用这些不等式，我们分析了连续接受和拒绝的分位数版本（Q-SAR）。我们推导出了臂识别错误的概率上界，这是首次对基于分位数的固定预算多最佳臂识别算法的合理性进行证明。我们展示了最佳臂识别的说明性实验。",
        "领域": "强化学习、决策理论、统计学习",
        "问题": "在固定预算内识别出具有最高τ分位数的一组m个臂",
        "动机": "受风险规避决策问题的启发，解决在不确定性下的最佳选择问题",
        "方法": "分析连续接受和拒绝的分位数版本（Q-SAR），并利用非递减危险率的随机变量的顺序统计量和分位数的不对称双侧集中不等式",
        "关键词": [
            "分位数老虎机",
            "最佳臂识别",
            "风险规避",
            "固定预算",
            "Q-SAR"
        ],
        "涉及的技术概念": {
            "分位数老虎机": "用于在不确定性下识别具有特定分位数性能的臂的技术",
            "非递减危险率": "随机变量的一个性质，用于证明顺序统计量和分位数的不对称双侧集中不等式",
            "Q-SAR": "连续接受和拒绝的分位数版本，用于在固定预算内识别最佳臂"
        },
        "success": true
    },
    {
        "order": 894,
        "title": "Quantitative Understanding of VAE as a Non-linearly Scaled Isometric Embedding",
        "html": "https://ICML.cc//virtual/2021/poster/8879",
        "abstract": "Variational autoencoder (VAE) estimates the posterior parameters (mean and variance) of latent variables corresponding to each input data. While it is used for many tasks, the transparency of the model is still an underlying issue. This paper provides a quantitative understanding of VAE property through the differential geometric and information-theoretic interpretations of VAE. According to the Rate-distortion theory, the optimal transform coding is achieved by using an orthonormal transform with PCA basis where the transform space is isometric to the input. Considering the analogy of transform coding to VAE,  we clarify theoretically and experimentally that VAE can be mapped to an implicit isometric embedding with a scale factor derived from the posterior parameter. As a result, we can estimate the data probabilities in the input space from the prior, loss metrics, and corresponding posterior parameters, and further, the quantitative importance of each latent variable can be evaluated like the eigenvalue of PCA.",
        "conference": "ICML",
        "中文标题": "变分自编码器作为非线性缩放等距嵌入的定量理解",
        "摘要翻译": "变分自编码器（VAE）估计与每个输入数据对应的潜在变量的后验参数（均值和方差）。尽管它被用于许多任务，模型的透明度仍然是一个潜在问题。本文通过VAE的微分几何和信息论解释，提供了对VAE性质的定量理解。根据率失真理论，最优变换编码是通过使用与输入等距的变换空间中的PCA基的正交变换实现的。考虑到变换编码与VAE的类比，我们从理论上和实验上阐明了VAE可以被映射到一个隐式的等距嵌入，其缩放因子源自后验参数。因此，我们可以从先验、损失度量和相应的后验参数估计输入空间中的数据概率，并且可以像PCA的特征值一样评估每个潜在变量的定量重要性。",
        "领域": "生成模型、深度学习理论、数据压缩",
        "问题": "理解变分自编码器（VAE）的性质及其在数据表示中的透明度问题",
        "动机": "提高对VAE模型内部工作机制的理解，特别是在数据表示和潜在变量重要性评估方面的透明度",
        "方法": "通过微分几何和信息论的视角分析VAE，理论结合实验验证VAE作为隐式等距嵌入的性质",
        "关键词": [
            "变分自编码器",
            "等距嵌入",
            "率失真理论",
            "潜在变量",
            "PCA"
        ],
        "涉及的技术概念": {
            "变分自编码器（VAE）": "一种生成模型，用于学习数据的潜在表示，通过估计潜在变量的后验分布参数",
            "等距嵌入": "在本文中指VAE能够将输入数据映射到一个保持距离关系的潜在空间",
            "率失真理论": "信息论中的一个理论，用于分析数据压缩的最优性能，本文中用于类比和理解VAE的工作原理"
        },
        "success": true
    },
    {
        "order": 895,
        "title": "Quantization Algorithms for Random Fourier Features",
        "html": "https://ICML.cc//virtual/2021/poster/8733",
        "abstract": "The method of random projection (RP) is the standard technique for dimensionality reduction, approximate near neighbor search, compressed sensing, etc., which provides a simple and effective scheme for approximating pairwise inner products and Euclidean distances in massive data.  Closely related to RP, the method of random Fourier features (RFF) has also become popular for approximating the (nonlinear) Gaussian kernel. RFF applies a specific nonlinear transformation on the projected data  from RP. In practice, using the Gaussian kernel often leads to better performance than the linear kernel (inner product).\n\nAfter random projections, quantization is an important step for efficient data storage, computation and transmission. Quantization for RP has  been extensively studied in the literature. In this paper, we focus on developing quantization algorithms for RFF. The task is in a sense challenging due to the tuning parameter $\\gamma$ in the Gaussian kernel. For example, the quantizer and the quantized data might be tied to each specific Gaussian kernel parameter $\\gamma$. Our contribution begins with the analysis on the probability distributions of RFF, and an interesting discovery that the marginal distribution of RFF is free of the parameter $\\gamma$. This significantly simplifies the design of the Lloyd-Max (LM) quantization scheme for RFF in that there would be only one LM quantizer (regardless of $\\gamma$). Detailed theoretical analysis is provided on the kernel estimators and approximation error, and experiments confirm the effectiveness and efficiency of the proposed method. ",
        "conference": "ICML",
        "中文标题": "随机傅里叶特征的量化算法",
        "摘要翻译": "随机投影（RP）方法是降维、近似最近邻搜索、压缩感知等的标准技术，它为大规模数据中的成对内积和欧几里得距离的近似提供了一种简单而有效的方案。与RP密切相关，随机傅里叶特征（RFF）方法也因其近似（非线性）高斯核而变得流行。RFF对RP投影的数据应用特定的非线性变换。在实践中，使用高斯核通常比线性核（内积）带来更好的性能。随机投影后，量化是高效数据存储、计算和传输的重要步骤。RP的量化已在文献中广泛研究。本文重点开发RFF的量化算法。由于高斯核中的调谐参数γ，这项任务在某种意义上具有挑战性。例如，量化器和量化数据可能与每个特定的高斯核参数γ绑定。我们的贡献始于对RFF概率分布的分析，以及一个有趣的发现，即RFF的边缘分布不受参数γ的影响。这大大简化了RFF的Lloyd-Max（LM）量化方案的设计，因为将只有一个LM量化器（不考虑γ）。提供了关于核估计器和近似误差的详细理论分析，实验证实了所提方法的有效性和效率。",
        "领域": "降维技术, 核方法, 数据压缩",
        "问题": "开发适用于随机傅里叶特征（RFF）的量化算法，解决因高斯核参数γ导致的量化设计复杂性。",
        "动机": "随机傅里叶特征（RFF）作为一种近似高斯核的方法，其量化处理对于高效数据存储和传输至关重要。然而，高斯核参数γ的存在使得量化设计变得复杂，本研究旨在简化这一过程。",
        "方法": "通过分析RFF的概率分布，发现其边缘分布不受参数γ影响，从而简化了Lloyd-Max量化方案的设计，仅需一个量化器即可适用于所有γ值。",
        "关键词": [
            "随机傅里叶特征",
            "量化算法",
            "高斯核",
            "Lloyd-Max量化",
            "降维"
        ],
        "涉及的技术概念": {
            "随机傅里叶特征（RFF）": "用于近似高斯核的方法，通过对随机投影数据应用非线性变换实现。",
            "Lloyd-Max量化": "一种量化技术，用于将连续信号转换为离散表示，优化量化误差。",
            "高斯核参数γ": "高斯核函数中的调谐参数，影响核函数的形状和宽度。"
        },
        "success": true
    },
    {
        "order": 896,
        "title": "Quantum algorithms for reinforcement learning with a generative model",
        "html": "https://ICML.cc//virtual/2021/poster/10207",
        "abstract": "Reinforcement learning studies how an agent should interact with an environment to maximize its cumulative reward. A standard way to study this question abstractly is to ask how many samples an agent needs from the environment to learn an optimal policy for a $\\gamma$-discounted Markov decision process (MDP). For such an MDP, we design quantum algorithms that approximate an optimal policy ($\\pi^*$), the optimal value function ($v^*$), and the optimal  $Q$-function ($q^*$), assuming the algorithms can access samples from the environment in quantum superposition. This assumption is justified whenever there exists a simulator for the environment; for example, if the environment is a video game or some other program. Our quantum algorithms, inspired by value iteration, achieve quadratic speedups over the best-possible classical sample complexities in the approximation accuracy ($\\epsilon$) and two main parameters of the MDP: the effective time horizon ($\\frac{1}{1-\\gamma}$) and the size of the action space ($A$). Moreover, we show that our quantum algorithm for computing $q^*$ is optimal by proving a matching quantum lower bound.",
        "conference": "ICML",
        "中文标题": "基于生成模型的强化学习量子算法",
        "摘要翻译": "强化学习研究的是智能体应如何与环境互动以最大化其累积奖励。研究这一问题的标准抽象方法是询问智能体需要从环境中获取多少样本才能学习到一个针对γ折扣马尔可夫决策过程（MDP）的最优策略。对于这样的MDP，我们设计了量子算法来近似最优策略（π*）、最优价值函数（v*）和最优Q函数（q*），假设算法可以访问量子叠加态中的环境样本。这一假设在环境模拟器存在时是合理的；例如，如果环境是一个视频游戏或其他程序。我们的量子算法受到价值迭代的启发，在近似精度（ε）和MDP的两个主要参数：有效时间范围（1/(1-γ)）和动作空间的大小（A）上，实现了相对于最佳可能经典样本复杂度的二次加速。此外，我们通过证明一个匹配的量子下界，展示了我们计算q*的量子算法是最优的。",
        "领域": "强化学习、量子计算、马尔可夫决策过程",
        "问题": "如何在量子计算框架下高效解决强化学习中的最优策略学习问题",
        "动机": "探索量子算法在强化学习中的应用潜力，特别是在减少样本复杂度和加速学习过程方面的优势",
        "方法": "设计基于价值迭代的量子算法，利用量子叠加态访问环境样本，实现样本复杂度的二次加速",
        "关键词": [
            "量子强化学习",
            "马尔可夫决策过程",
            "价值迭代",
            "量子算法",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "量子叠加态": "允许量子算法同时访问多个环境样本，从而在理论上减少样本复杂度",
            "价值迭代": "一种动态规划方法，用于近似求解最优价值函数和策略，本研究中被量子算法所采用",
            "量子下界": "证明了所提出的量子算法在计算最优Q函数时的效率是最优的，无法被其他量子算法超越"
        },
        "success": true
    },
    {
        "order": 897,
        "title": "Quasi-global Momentum: Accelerating Decentralized Deep Learning on Heterogeneous Data",
        "html": "https://ICML.cc//virtual/2021/poster/10319",
        "abstract": "Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks. In realistic learning scenarios, the presence of heterogeneity across different clients' local datasets poses an optimization challenge and may severely deteriorate the generalization performance.\nIn this paper, we investigate and identify the limitation of several decentralized optimization algorithms for different degrees of data heterogeneity. We propose a novel momentum-based method to mitigate this decentralized training difficulty. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10, ImageNet, and AG News) and several network topologies (Ring and Social Network) that our method is much more robust to the heterogeneity of clients' data than other existing methods, by a significant improvement in test performance (1%-20%).",
        "conference": "ICML",
        "中文标题": "准全局动量：在异构数据上加速去中心化深度学习",
        "摘要翻译": "深度学习模型的去中心化训练是实现数据隐私和网络设备上学习的关键要素。在实际学习场景中，不同客户端本地数据集之间的异构性存在带来了优化挑战，并可能严重恶化泛化性能。本文中，我们研究并识别了几种去中心化优化算法在不同数据异构程度下的局限性。我们提出了一种新颖的基于动量的方法来缓解这种去中心化训练的困难。通过在多种CV/NLP数据集（CIFAR-10、ImageNet和AG News）和几种网络拓扑（环形和社交网络）上进行的大量实证实验，我们展示了我们的方法比其他现有方法对客户端数据的异构性具有更强的鲁棒性，测试性能显著提高（1%-20%）。",
        "领域": "去中心化学习、异构数据处理、深度学习优化",
        "问题": "解决去中心化深度学习中因数据异构性导致的优化挑战和泛化性能下降问题",
        "动机": "提高去中心化深度学习在异构数据环境下的训练效率和模型性能",
        "方法": "提出一种基于动量的新方法，以增强去中心化训练对数据异构性的鲁棒性",
        "关键词": [
            "去中心化学习",
            "数据异构性",
            "动量方法",
            "深度学习优化",
            "泛化性能"
        ],
        "涉及的技术概念": {
            "去中心化训练": "在多个客户端上分散进行模型训练，保护数据隐私并实现设备上学习",
            "数据异构性": "不同客户端数据集之间的分布差异，影响模型训练和性能",
            "动量方法": "一种优化技术，通过积累过去梯度的方向来加速收敛并减少震荡"
        },
        "success": true
    },
    {
        "order": 898,
        "title": "Query Complexity of Adversarial Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9353",
        "abstract": "There are two main attack models considered in the adversarial robustness literature: black-box and white-box. We consider these threat models as two ends of a fine-grained spectrum, indexed by the number of queries the adversary can ask. Using this point of view we investigate how many queries the adversary needs to make to design an attack that is comparable to the best possible attack in the white-box model. We give a lower bound on that number of queries in terms of entropy of decision boundaries of the classifier. Using this result we analyze two classical learning algorithms on two synthetic tasks for which we prove meaningful security guarantees. The obtained bounds suggest that some learning algorithms are inherently more robust against query-bounded adversaries than others.",
        "conference": "ICML",
        "中文标题": "对抗性攻击的查询复杂度",
        "摘要翻译": "在对抗性鲁棒性文献中，主要考虑两种攻击模型：黑盒和白盒。我们将这些威胁模型视为一个细粒度谱的两端，通过对手可以提出的查询数量来索引。从这个角度出发，我们研究了对手需要提出多少查询才能设计出与白盒模型中最优攻击相当的攻击。我们给出了这个查询数量的下界，以分类器决策边界的熵来表示。利用这一结果，我们分析了两种经典学习算法在两个合成任务上的表现，并证明了有意义的安全保证。所得到的界限表明，某些学习算法本质上比其他算法更能抵抗查询受限的对手。",
        "领域": "对抗性机器学习、深度学习安全、模型鲁棒性",
        "问题": "研究对抗性攻击中查询复杂度的问题，即在不同的查询限制下，攻击者如何有效地设计攻击策略。",
        "动机": "探索对抗性攻击的查询复杂度，以理解不同学习算法在面对查询受限的对手时的固有鲁棒性差异。",
        "方法": "通过将黑盒和白盒攻击模型视为查询数量的连续谱，利用决策边界的熵来下界查询数量，分析两种经典学习算法在合成任务上的表现。",
        "关键词": [
            "对抗性攻击",
            "查询复杂度",
            "模型鲁棒性",
            "决策边界",
            "学习算法"
        ],
        "涉及的技术概念": {
            "对抗性攻击": "在机器学习中，对抗性攻击是指故意设计来欺骗模型的输入，使其产生错误输出。",
            "查询复杂度": "在对抗性攻击中，查询复杂度指的是攻击者为设计有效攻击所需向目标模型提出的查询数量。",
            "决策边界": "在分类问题中，决策边界是模型将不同类别分开的界限，其熵可以用来衡量分类器的复杂性和对抗性攻击的难度。"
        },
        "success": true
    },
    {
        "order": 899,
        "title": "Randomized Algorithms for Submodular Function Maximization with a $k$-System Constraint",
        "html": "https://ICML.cc//virtual/2021/poster/9387",
        "abstract": "Submodular optimization has numerous applications such as crowdsourcing and viral marketing. In this paper, we study the problem of non-negative submodular function maximization subject to a $k$-system constraint, which generalizes many other important constraints in submodular optimization such as cardinality constraint, matroid constraint, and $k$-extendible system constraint. The existing approaches for this problem are all based on deterministic algorithmic frameworks, and the best approximation ratio achieved by these algorithms (for a general submodular function) is $k+2\\sqrt{k+2}+3$. We propose a randomized algorithm with an improved approximation ratio of $(1+\\sqrt{k})^2$, while achieving nearly-linear time complexity significantly lower than that of the state-of-the-art algorithm. We also show that our algorithm can be further generalized to address a stochastic case where the elements can be adaptively selected, and propose an approximation ratio of $(1+\\sqrt{k+1})^2$ for the adaptive optimization case. The empirical performance of our algorithms is extensively evaluated in several applications related to data mining and social computing, and the experimental results demonstrate the superiorities of our algorithms in terms of both utility and efficiency.",
        "conference": "ICML",
        "中文标题": "带k-系统约束的子模函数最大化的随机算法",
        "摘要翻译": "子模优化在众包和病毒营销等领域有广泛应用。本文研究了在k-系统约束下的非负子模函数最大化问题，该问题概括了子模优化中的许多其他重要约束，如基数约束、拟阵约束和k-可扩展系统约束。现有的解决这一问题的方法均基于确定性算法框架，这些算法（对于一般子模函数）达到的最佳近似比为k+2√(k+2)+3。我们提出了一种随机算法，其改进的近似比为(1+√k)^2，同时实现了几乎线性的时间复杂度，显著低于现有最先进算法的时间复杂度。我们还展示了我们的算法可以进一步推广到元素可以自适应选择的随机情况，并为自适应优化情况提出了(1+√(k+1))^2的近似比。我们的算法在数据挖掘和社交计算相关的几个应用中的性能得到了广泛评估，实验结果证明了我们的算法在效用和效率方面的优越性。",
        "领域": "组合优化, 算法设计, 数据挖掘",
        "问题": "在k-系统约束下最大化非负子模函数的问题",
        "动机": "改进现有确定性算法在近似比和时间复杂度上的不足",
        "方法": "提出了一种随机算法，改进了近似比并降低了时间复杂度",
        "关键词": [
            "子模优化",
            "随机算法",
            "k-系统约束",
            "近似算法",
            "自适应优化"
        ],
        "涉及的技术概念": {
            "子模函数": "在组合优化中，子模函数是一种具有递减边际效应的集合函数，广泛应用于资源分配和优化问题",
            "k-系统约束": "一种广义的约束形式，包括基数约束、拟阵约束等，用于限制子模函数优化的解空间",
            "随机算法": "通过引入随机性来改进算法性能的算法设计方法，本文中用于提高近似比和降低时间复杂度"
        },
        "success": true
    },
    {
        "order": 900,
        "title": "Randomized Dimensionality Reduction for Facility Location and Single-Linkage Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/9975",
        "abstract": "Random dimensionality reduction is a versatile tool for speeding up algorithms for high-dimensional problems. We study its application to two clustering problems: the facility location problem, and the single-linkage hierarchical clustering problem, which is equivalent to computing the minimum spanning tree. We show that if we project the input pointset $X$ onto a random $d = O(d_X)$-dimensional subspace (where $d_X$ is the doubling dimension of $X$), then the optimum facility location cost in the projected space approximates the original cost up to a constant factor. We show an analogous statement for minimum spanning tree, but with the dimension $d$ having an extra $\\log \\log n$ term and the approximation factor being arbitrarily close to $1$. Furthermore, we extend these results to approximating {\\em solutions} instead of just their {\\em costs}. Lastly, we provide experimental results to validate the quality of solutions and the speedup due to the dimensionality reduction. Unlike several previous papers studying this approach in the context of $k$-means and $k$-medians, our dimension bound does not depend on the number of clusters but only on the intrinsic dimensionality of $X$. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "随机降维在设施定位和单链接聚类中的应用",
        "摘要翻译": "随机降维是加速高维问题算法的多功能工具。我们研究了其在两个聚类问题中的应用：设施定位问题和单链接层次聚类问题，后者等同于计算最小生成树。我们证明，如果将输入点集X投影到一个随机的d=O(d_X)维子空间（其中d_X是X的双倍维度），那么在投影空间中的最优设施定位成本可以在一个常数因子内近似原始成本。对于最小生成树，我们展示了一个类似的陈述，但维度d有一个额外的log log n项，并且近似因子可以任意接近1。此外，我们将这些结果扩展到近似解决方案而不仅仅是它们的成本。最后，我们提供了实验结果以验证解决方案的质量和由于降维带来的加速。与之前几篇在k均值和k中位数背景下研究这种方法的论文不同，我们的维度界限不依赖于聚类的数量，而仅依赖于X的内在维度。",
        "领域": "聚类分析, 降维技术, 计算几何",
        "问题": "如何在保持聚类问题解决方案质量的同时，通过随机降维技术减少计算复杂度。",
        "动机": "探索随机降维技术在设施定位和单链接聚类问题中的应用，以提高算法的效率和可扩展性。",
        "方法": "通过将高维数据投影到低维子空间，研究其对设施定位成本和最小生成树计算的影响，并通过实验验证方法的有效性。",
        "关键词": [
            "随机降维",
            "设施定位",
            "单链接聚类",
            "最小生成树",
            "双倍维度"
        ],
        "涉及的技术概念": {
            "随机降维": "通过随机投影将高维数据映射到低维空间，以减少计算复杂度同时保留数据的关键特性。",
            "双倍维度": "描述数据集内在维度的一个度量，用于确定降维后的维度大小。",
            "最小生成树": "在图论中，连接所有顶点的边的权重之和最小的树，用于单链接层次聚类问题的解决方案。"
        }
    },
    {
        "order": 901,
        "title": "Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10527",
        "abstract": "Multi-agent settings in the real world often involve tasks with varying types and quantities of agents and non-agent entities; however, common patterns of behavior often emerge among these agents/entities. Our method aims to leverage these commonalities by asking the question: ``What is the expected utility of each agent when only considering a randomly selected sub-group of its observed entities?'' By posing this counterfactual question, we can recognize state-action trajectories within sub-groups of entities that we may have encountered in another task and use what we learned in that task to inform our prediction in the current one. We then reconstruct a prediction of the full returns as a combination of factors considering these disjoint groups of entities and train this ``randomly factorized' value function as an auxiliary objective for value-based multi-agent reinforcement learning. By doing so, our model can recognize and leverage similarities across tasks to improve learning efficiency in a multi-task setting. Our approach, Randomized Entity-wise Factorization for Imagined Learning (REFIL), outperforms all strong baselines by a significant margin in challenging multi-task StarCraft micromanagement settings.",
        "conference": "ICML",
        "中文标题": "随机实体分解在多智能体强化学习中的应用",
        "摘要翻译": "现实世界中的多智能体环境经常涉及具有不同类型和数量的智能体及非智能体实体的任务；然而，这些智能体/实体之间常常会出现共同的行为模式。我们的方法旨在通过提出以下问题来利用这些共性：'当仅考虑随机选择的观察实体子群时，每个智能体的预期效用是什么？'通过提出这个反事实问题，我们能够识别在实体子群中的状态-动作轨迹，这些轨迹可能在另一个任务中遇到过，并利用在那个任务中学到的知识来为当前任务的预测提供信息。然后，我们将完整回报的预测重构为考虑这些不相交实体群的因子的组合，并将这种'随机分解'的价值函数作为基于价值的多智能体强化学习的辅助目标进行训练。通过这样做，我们的模型能够识别并利用跨任务的相似性，以提高在多任务设置中的学习效率。我们的方法，即用于想象学习的随机实体分解（REFIL），在具有挑战性的多任务星际争霸微操设置中显著优于所有强基线。",
        "领域": "多智能体强化学习、多任务学习、游戏AI",
        "问题": "如何在多智能体环境中有效利用跨任务的相似性以提高学习效率",
        "动机": "利用智能体和实体间的共同行为模式，通过反事实问题提高多任务学习中的效率",
        "方法": "提出随机实体分解方法，通过考虑随机选择的实体子群来重构完整回报的预测，并作为辅助目标训练",
        "关键词": [
            "多智能体强化学习",
            "随机实体分解",
            "多任务学习",
            "反事实学习",
            "星际争霸微操"
        ],
        "涉及的技术概念": {
            "随机实体分解": "通过随机选择实体子群来分解和重构回报预测，以提高学习效率",
            "反事实学习": "通过提出'如果仅考虑部分实体会怎样'的问题，来利用跨任务的知识",
            "多任务学习": "在多个相关任务间共享知识，以提高整体学习效率和性能"
        },
        "success": true
    },
    {
        "order": 902,
        "title": "Randomized Exploration in Reinforcement Learning with General Value Function Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/9803",
        "abstract": "We propose a model-free reinforcement learning algorithm  inspired by the popular randomized least squares value iteration (RLSVI) algorithm as well as the optimism principle. Unlike existing upper-confidence-bound (UCB) based approaches, which are often computationally intractable, our algorithm  drives exploration by simply perturbing the training data with judiciously chosen i.i.d. scalar noises. To attain optimistic value function estimation without resorting to a UCB-style bonus, we introduce an optimistic reward sampling procedure. When the value functions can be represented by a function class $\\mathcal{F}$, our algorithm achieves a worst-case regret bound of $\\tilde{O}(\\mathrm{poly}(d_EH)\\sqrt{T})$ where $T$ is the time elapsed, $H$ is the planning horizon and $d_E$  is the \\emph{eluder dimension} of $\\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a variant of RLSVI, that enjoys an $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret. We complement the theory with an empirical evaluation across known difficult exploration tasks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于通用价值函数近似的强化学习中的随机探索",
        "摘要翻译": "我们提出了一种无模型的强化学习算法，该算法受到流行的随机最小二乘价值迭代（RLSVI）算法以及乐观原则的启发。与现有的基于上置信界（UCB）的方法不同，这些方法通常在计算上难以处理，我们的算法通过简单地用精心选择的独立同分布标量噪声扰动训练数据来驱动探索。为了在不采用UCB风格奖励的情况下获得乐观的价值函数估计，我们引入了一种乐观的奖励采样过程。当价值函数可以由一个函数类$\\mathcal{F}$表示时，我们的算法实现了$\\tilde{O}(\\mathrm{poly}(d_EH)\\sqrt{T})$的最坏情况后悔界，其中$T$是经过的时间，$H$是规划视野，$d_E$是$\\mathcal{F}$的'eluder dimension'。在线性设置中，我们的算法简化为LSVI-PHE，这是RLSVI的一个变体，享有$\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$的后悔界。我们用对已知困难探索任务的实证评估来补充理论。",
        "领域": "强化学习, 价值函数近似, 探索策略",
        "问题": "解决在强化学习中如何有效进行探索的问题，特别是在通用价值函数近似的情况下。",
        "动机": "现有的基于上置信界（UCB）的探索方法在计算上往往难以处理，因此需要一种更高效且计算可行的探索策略。",
        "方法": "通过扰动训练数据并引入乐观的奖励采样过程，实现无需UCB风格奖励的乐观价值函数估计。",
        "关键词": [
            "随机探索",
            "价值函数近似",
            "乐观原则",
            "强化学习算法",
            "后悔界"
        ],
        "涉及的技术概念": {
            "随机最小二乘价值迭代（RLSVI）": "一种流行的强化学习算法，用于价值函数近似。",
            "eluder dimension": "用于衡量函数类的复杂度，影响算法的后悔界。"
        }
    },
    {
        "order": 903,
        "title": "Rate-Distortion Analysis of Minimum Excess Risk in Bayesian Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9217",
        "abstract": "In parametric Bayesian learning, a prior is assumed on the parameter $W$ which determines the distribution of samples. In this setting, Minimum Excess Risk (MER) is defined as the difference between the minimum expected loss achievable when learning from data and the minimum expected loss that could be achieved if $W$ was observed. In this paper, we build upon and extend the recent results of  (Xu & Raginsky, 2020) to analyze the MER in Bayesian learning and derive information-theoretic bounds on it. We formulate the problem as a (constrained) rate-distortion optimization and show how the solution can be bounded above and below by two other rate-distortion functions that are easier to study. The lower bound represents the minimum possible excess risk achievable by \\emph{any} process using $R$ bits of information from the parameter $W$. For the upper bound, the optimization is further constrained to use $R$ bits from the training set, a setting which relates MER to information-theoretic bounds on the generalization gap in frequentist learning. We derive information-theoretic bounds on the difference between these upper and lower bounds and show that they can provide order-wise tight rates for MER under certain conditions. This analysis gives more insight into the information-theoretic nature of Bayesian learning as well as providing novel bounds.",
        "conference": "ICML",
        "中文标题": "贝叶斯学习中最小超额风险的率失真分析",
        "摘要翻译": "在参数贝叶斯学习中，假设参数$W$上有一个先验分布，该参数决定了样本的分布。在此设置下，最小超额风险（MER）被定义为从数据学习中获得的最小期望损失与如果观察到$W$时可以实现的最小期望损失之间的差异。在本文中，我们基于并扩展了（Xu & Raginsky, 2020）的最新结果，以分析贝叶斯学习中的MER，并推导出其上的信息理论界限。我们将问题表述为一个（约束的）率失真优化问题，并展示了如何通过两个更容易研究的其他率失真函数来界定解决方案的上限和下限。下限代表了使用来自参数$W$的$R$位信息的任何过程可实现的最小可能超额风险。对于上限，优化进一步约束为使用来自训练集的$R$位信息，这一设置将MER与频率学习中泛化差距的信息理论界限联系起来。我们推导了这些上限和下限之间差异的信息理论界限，并表明在某些条件下，它们可以为MER提供顺序紧密的率。这一分析不仅提供了对贝叶斯学习信息理论性质的更深入理解，还提供了新颖的界限。",
        "领域": "贝叶斯学习",
        "问题": "分析贝叶斯学习中的最小超额风险（MER）并推导其信息理论界限",
        "动机": "深入理解贝叶斯学习中的信息理论性质，并提供新颖的界限",
        "方法": "将问题表述为率失真优化问题，并通过两个更容易研究的率失真函数来界定解决方案的上限和下限",
        "关键词": [
            "贝叶斯学习",
            "最小超额风险",
            "率失真优化",
            "信息理论界限",
            "泛化差距"
        ],
        "涉及的技术概念": {
            "最小超额风险（MER）": "在贝叶斯学习中，定义为从数据学习中获得的最小期望损失与如果观察到参数$W$时可以实现的最小期望损失之间的差异",
            "率失真优化": "用于分析MER的问题表述方法，通过优化率失真函数来界定MER的上限和下限",
            "信息理论界限": "用于量化MER的界限，提供对贝叶斯学习信息理论性质的深入理解"
        },
        "success": true
    },
    {
        "order": 904,
        "title": "RATT: Leveraging Unlabeled Data  to Guarantee Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/10203",
        "abstract": "To assess generalization, machine learning scientists typically either (i) bound the generalization gap and then (after training) plug in the empirical risk to obtain a bound on the true risk; or (ii) validate empirically on holdout data. However, (i) typically yields vacuous guarantees for overparameterized models; and (ii) shrinks the training set and its guarantee erodes with each re-use of the holdout set. In this paper, we leverage unlabeled data to produce generalization bounds. After augmenting our (labeled) training set with randomly labeled data, we train in the standard fashion. Whenever classifiers achieve low error on the clean data but high error on the random data, our bound ensures that the true risk is low. We prove that our bound is valid for 0-1 empirical risk minimization and with linear classifiers trained by gradient descent. Our approach is especially useful in conjunction with deep learning due to the early learning phenomenon whereby networks fit true labels before noisy labels but requires one intuitive assumption. Empirically, on canonical computer vision and NLP tasks, our bound provides non-vacuous generalization guarantees that track actual performance closely. This work enables practitioners to certify generalization even when (labeled) holdout data is unavailable and provides insights into the relationship between random label noise and generalization. ",
        "conference": "ICML",
        "中文标题": "RATT：利用未标记数据保证泛化能力",
        "摘要翻译": "为了评估泛化能力，机器学习科学家通常采取以下两种方法之一：(i) 限定泛化差距，然后在训练后插入经验风险以获得真实风险的界限；或 (ii) 在保留数据上进行实证验证。然而，(i) 通常对过参数化模型产生空洞的保证；(ii) 减少了训练集，并且每次重用保留集时其保证都会削弱。在本文中，我们利用未标记数据来产生泛化界限。在用随机标记的数据增强我们的（标记的）训练集后，我们以标准方式进行训练。每当分类器在干净数据上实现低错误但在随机数据上实现高错误时，我们的界限确保真实风险低。我们证明了我们的界限对于0-1经验风险最小化和通过梯度下降训练的线性分类器是有效的。由于网络在拟合噪声标签之前先拟合真实标签的早期学习现象，我们的方法特别适用于深度学习，但需要一个直观的假设。实证上，在经典的计算机视觉和NLP任务上，我们的界限提供了非空洞的泛化保证，这些保证紧密跟踪实际性能。这项工作使从业者即使在（标记的）保留数据不可用时也能证明泛化能力，并提供了关于随机标签噪声与泛化之间关系的见解。",
        "领域": "深度学习理论、计算机视觉、自然语言处理",
        "问题": "如何在缺乏标记保留数据的情况下，为过参数化模型提供非空洞的泛化保证。",
        "动机": "解决传统泛化评估方法在过参数化模型上产生空洞保证或减少训练集有效性的问题。",
        "方法": "通过引入随机标记的未标记数据增强训练集，利用分类器在干净数据和随机数据上的错误率差异来确保真实风险低。",
        "关键词": [
            "泛化保证",
            "未标记数据",
            "深度学习理论",
            "随机标签噪声",
            "早期学习现象"
        ],
        "涉及的技术概念": {
            "泛化界限": "用于确保机器学习模型在未见数据上表现良好的理论保证。",
            "早期学习现象": "指深度学习模型在训练过程中优先学习真实标签而非噪声标签的现象。",
            "随机标签噪声": "在训练数据中引入的随机错误标签，用于研究模型对噪声的鲁棒性和泛化能力。"
        },
        "success": true
    },
    {
        "order": 905,
        "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations",
        "html": "https://ICML.cc//virtual/2021/poster/9943",
        "abstract": "We present the Open Predicate Query Language (OPQL); a method for constructing a virtual KB (VKB) trained entirely from text. Large Knowledge Bases (KBs) are indispensable for a wide-range of industry applications such as question answering and recommendation. Typically, KBs encode world knowledge in a structured, readily accessible form derived from laborious human annotation efforts. Unfortunately, while they are extremely high precision, KBs are inevitably highly incomplete and automated methods for enriching them are far too inaccurate. Instead, OPQL constructs a VKB by encoding and indexing a set of relation mentions in a way that naturally enables reasoning and can be trained without any structured supervision. We demonstrate that OPQL outperforms prior VKB methods on two different KB reasoning tasks and, additionally, can be used as an external memory integrated into a language model (OPQL-LM) leading to improvements on two open-domain question answering tasks.",
        "conference": "ICML",
        "中文标题": "基于开放谓词关系的虚拟知识库推理",
        "摘要翻译": "我们提出了开放谓词查询语言（OPQL）；一种完全从文本训练构建虚拟知识库（VKB）的方法。大型知识库（KBs）对于问答和推荐等广泛的行业应用来说是不可或缺的。通常，知识库以结构化的、易于访问的形式编码世界知识，这些知识来源于繁琐的人工标注工作。不幸的是，尽管知识库的精确度极高，但它们不可避免地高度不完整，而自动丰富它们的方法又过于不准确。相反，OPQL通过编码和索引一组关系提及来构建VKB，这种方式自然支持推理，并且可以在没有任何结构化监督的情况下进行训练。我们证明，OPQL在两种不同的知识库推理任务上优于先前的VKB方法，并且，还可以作为外部记忆集成到语言模型（OPQL-LM）中，从而在两个开放领域问答任务上带来改进。",
        "领域": "知识库推理, 开放领域问答, 自然语言处理与知识库结合",
        "问题": "解决知识库高度不完整和自动丰富方法不准确的问题",
        "动机": "通过从文本训练构建虚拟知识库，支持无需结构化监督的推理，提高知识库推理和问答任务的性能",
        "方法": "提出开放谓词查询语言（OPQL），通过编码和索引关系提及构建虚拟知识库，支持推理并可作为外部记忆集成到语言模型中",
        "关键词": [
            "虚拟知识库",
            "开放谓词查询语言",
            "知识库推理",
            "开放领域问答",
            "语言模型集成"
        ],
        "涉及的技术概念": {
            "开放谓词查询语言（OPQL）": "一种从文本训练构建虚拟知识库的方法，支持无需结构化监督的推理",
            "虚拟知识库（VKB）": "通过编码和索引关系提及构建的知识库，支持自然推理",
            "外部记忆集成": "将OPQL作为外部记忆集成到语言模型中，以提高开放领域问答任务的性能"
        },
        "success": true
    },
    {
        "order": 906,
        "title": "Recomposing the Reinforcement Learning Building Blocks with Hypernetworks",
        "html": "https://ICML.cc//virtual/2021/poster/8489",
        "abstract": "The Reinforcement Learning (RL) building blocks, i.e. $Q$-functions and policy networks, usually take elements from the cartesian product of two domains as input. In particular, the input of the $Q$-function is both the state and the action, and in multi-task problems (Meta-RL) the policy can take a state and a context. Standard architectures tend to ignore these variables' underlying interpretations and simply concatenate their features into a single vector. In this work, we argue that this choice may lead to poor gradient estimation in actor-critic algorithms and high variance learning steps in Meta-RL algorithms. To consider the interaction between the input variables, we suggest using a Hypernetwork architecture where a primary network determines the weights of a conditional dynamic network. We show that this approach improves the gradient approximation and reduces the learning step variance, which both accelerates learning and improves the final performance. We demonstrate a consistent improvement across different locomotion tasks and different algorithms both in RL (TD3 and SAC) and in Meta-RL (MAML and PEARL).",
        "conference": "ICML",
        "中文标题": "利用超网络重构强化学习的构建模块",
        "摘要翻译": "强化学习（RL）的构建模块，即Q函数和策略网络，通常将来自两个领域笛卡尔积的元素作为输入。具体来说，Q函数的输入是状态和动作，而在多任务问题（Meta-RL）中，策略可以接受状态和上下文。标准架构往往忽略这些变量的潜在解释，简单地将它们的特征连接成一个单一向量。在这项工作中，我们认为这种选择可能导致演员-评论家算法中的梯度估计不佳和Meta-RL算法中的学习步骤方差高。为了考虑输入变量之间的相互作用，我们建议使用超网络架构，其中一个主网络决定条件动态网络的权重。我们展示了这种方法改善了梯度近似并减少了学习步骤的方差，这既加速了学习又提高了最终性能。我们在不同的运动任务和不同的算法中展示了持续的改进，无论是在RL（TD3和SAC）还是在Meta-RL（MAML和PEARL）中。",
        "领域": "强化学习、元强化学习、动态网络架构",
        "问题": "标准架构在处理强化学习中的多输入变量时，忽略了变量间的相互作用，导致梯度估计不佳和学习步骤方差高。",
        "动机": "改善强化学习算法中梯度估计的准确性和减少学习步骤的方差，以加速学习过程并提高最终性能。",
        "方法": "采用超网络架构，通过主网络动态生成条件网络的权重，以更好地建模输入变量间的相互作用。",
        "关键词": [
            "超网络",
            "强化学习",
            "元强化学习",
            "梯度估计",
            "动态网络"
        ],
        "涉及的技术概念": {
            "超网络": "一种网络架构，其中主网络生成另一网络的权重，用于动态调整模型参数以适应不同的输入条件。",
            "梯度估计": "在强化学习中用于优化策略和值函数的梯度计算，影响学习效率和稳定性。",
            "动态网络": "能够根据输入条件动态调整其结构和参数的神经网络，提高模型对不同任务的适应能力。"
        },
        "success": true
    },
    {
        "order": 907,
        "title": "Recovering AES Keys with a Deep Cold Boot Attack",
        "html": "https://ICML.cc//virtual/2021/poster/9771",
        "abstract": "Cold boot attacks inspect the corrupted random access memory soon after the power has been shut down. While most of the bits have been corrupted, many bits, at random locations, have not. Since the keys in many encryption schemes are being expanded in memory into longer keys with fixed redundancies, the keys can often be restored. In this work we combine a deep error correcting code technique together with a modified SAT solver scheme in order to apply the attack to AES keys. \nEven though AES consists Rijndael SBOX elements, that are specifically designed to be resistant to linear and differential cryptanalysis, our method provides a novel formalization of the AES key scheduling as a computational graph, which is implemented by neural message passing network. Our results show that our methods outperform the state of the art attack methods by a very large gap. ",
        "conference": "ICML",
        "中文标题": "利用深度冷启动攻击恢复AES密钥",
        "摘要翻译": "冷启动攻击在电源关闭后不久检查损坏的随机存取内存。虽然大部分位已经损坏，但许多位在随机位置尚未损坏。由于许多加密方案中的密钥在内存中被扩展为具有固定冗余的更长密钥，因此通常可以恢复密钥。在这项工作中，我们结合了深度纠错码技术和改进的SAT求解器方案，以对AES密钥应用攻击。尽管AES包含Rijndael SBOX元素，这些元素专门设计为对线性和差分密码分析具有抵抗力，但我们的方法提供了一种新颖的AES密钥调度作为计算图的形式化，该计算图由神经消息传递网络实现。我们的结果表明，我们的方法在性能上大大超过了现有的攻击方法。",
        "领域": "密码学攻击、深度学习应用、计算机安全",
        "问题": "如何在冷启动攻击中更有效地恢复AES密钥",
        "动机": "探索利用深度学习和改进的SAT求解器技术，提高在冷启动攻击中恢复AES密钥的效率和成功率",
        "方法": "结合深度纠错码技术和改进的SAT求解器方案，将AES密钥调度形式化为计算图，并通过神经消息传递网络实现",
        "关键词": [
            "冷启动攻击",
            "AES密钥恢复",
            "深度学习",
            "SAT求解器",
            "神经消息传递网络"
        ],
        "涉及的技术概念": {
            "深度纠错码技术": "用于在冷启动攻击中识别和纠正损坏的内存位，以提高密钥恢复的准确性",
            "改进的SAT求解器方案": "用于高效解决密钥恢复过程中的逻辑约束问题",
            "神经消息传递网络": "用于实现AES密钥调度的计算图形式化，提高密钥恢复的效率和成功率"
        },
        "success": true
    },
    {
        "order": 908,
        "title": "Regret and Cumulative Constraint Violation Analysis for Online Convex Optimization with Long Term Constraints",
        "html": "https://ICML.cc//virtual/2021/poster/9489",
        "abstract": "This paper considers online convex optimization with long term constraints, where constraints can be violated in intermediate rounds, but need to be satisfied in the long run. The cumulative constraint violation is used as the metric to measure constraint violations, which excludes the situation that strictly feasible constraints can compensate the effects of violated constraints. A novel algorithm is first proposed and it achieves an $\\mathcal{O}(T^{\\max\\{c,1-c\\}})$ bound for static regret and an $\\mathcal{O}(T^{(1-c)/2})$ bound for cumulative constraint violation, where $c\\in(0,1)$ is a user-defined trade-off parameter, and thus has improved performance compared with existing results. Both static regret and cumulative constraint violation bounds are reduced to $\\mathcal{O}(\\log(T))$ when the loss functions are strongly convex, which also improves existing results. %In order to bound the regret with respect to any comparator sequence,\nIn order to achieve the optimal regret with respect to any comparator sequence, another algorithm is then proposed and it achieves the optimal $\\mathcal{O}(\\sqrt{T(1+P_T)})$ regret and an $\\mathcal{O}(\\sqrt{T})$ cumulative constraint violation, where $P_T$ is the path-length of the comparator sequence. Finally, numerical simulations are provided to illustrate the effectiveness of the theoretical results.",
        "conference": "ICML",
        "success": true,
        "中文标题": "长期约束在线凸优化的遗憾与累积约束违反分析",
        "摘要翻译": "本文考虑了具有长期约束的在线凸优化问题，其中约束在中间轮次中可能被违反，但需要在长期内得到满足。累积约束违反被用作衡量约束违反的指标，这排除了严格可行约束可以补偿违反约束影响的情况。首先提出了一种新算法，该算法在静态遗憾上实现了$\\mathcal{O}(T^{\\max\\{c,1-c\\}})$界限，在累积约束违反上实现了$\\mathcal{O}(T^{(1-c)/2})$界限，其中$c\\in(0,1)$是用户定义的权衡参数，因此与现有结果相比具有改进的性能。当损失函数是强凸的时，静态遗憾和累积约束违反界限都减少到$\\mathcal{O}(\\log(T))$，这也改进了现有结果。为了实现对任何比较序列的最优遗憾，随后提出了另一种算法，该算法实现了最优的$\\mathcal{O}(\\sqrt{T(1+P_T)})$遗憾和$\\mathcal{O}(\\sqrt{T})$累积约束违反，其中$P_T$是比较序列的路径长度。最后，提供了数值模拟来说明理论结果的有效性。",
        "领域": "在线学习、凸优化、约束优化",
        "问题": "解决在长期约束下在线凸优化中的遗憾和累积约束违反问题",
        "动机": "研究如何在长期约束下进行在线凸优化，同时最小化遗憾和累积约束违反",
        "方法": "提出两种算法，第一种算法在静态遗憾和累积约束违反上实现改进的界限，第二种算法实现对任何比较序列的最优遗憾和累积约束违反",
        "关键词": [
            "在线凸优化",
            "长期约束",
            "累积约束违反",
            "静态遗憾",
            "强凸函数"
        ],
        "涉及的技术概念": {
            "累积约束违反": "用于衡量在长期约束下在线凸优化中约束违反的指标",
            "静态遗憾": "衡量在线凸优化算法性能的指标，与固定比较点相比的遗憾"
        }
    },
    {
        "order": 909,
        "title": "Regret Minimization in Stochastic Non-Convex Learning via a Proximal-Gradient Approach",
        "html": "https://ICML.cc//virtual/2021/poster/9651",
        "abstract": "This paper develops a methodology for regret minimization with stochastic first-order oracle feedback in online, constrained, non-smooth, non-convex problems.\nIn this setting, the minimization of external regret is beyond reach for first-order methods, and there are no gradient-based algorithmic frameworks capable of providing a solution.\nOn that account, we propose a conceptual approach that leverages non-convex optimality measures, leading to a suitable generalization of the learner's local regret. \nWe focus on a local regret measure defined via a proximal-gradient mapping, that also encompasses the original notion proposed by Hazan et al. (2017).\nTo achieve no local regret in this setting, we develop a proximal-gradient method based on stochastic first-order feedback, and a simpler method for when access to a perfect first-order oracle is possible.\nBoth methods are order-optimal (in the min-max sense), and we also establish a bound on the number of proximal-gradient queries these methods require.\nAs an important application of our results, we also obtain a link between online and offline non-convex stochastic optimization manifested as a new proximal-gradient scheme with complexity guarantees matching those obtained via variance reduction techniques.",
        "conference": "ICML",
        "中文标题": "通过近端梯度方法在随机非凸学习中的遗憾最小化",
        "摘要翻译": "本文开发了一种在在线、约束、非光滑、非凸问题中，通过随机一阶oracle反馈实现遗憾最小化的方法。在此设置下，一阶方法无法实现外部遗憾的最小化，且目前尚无基于梯度的算法框架能够提供解决方案。因此，我们提出了一种概念性方法，该方法利用非凸最优性度量，导致了对学习者局部遗憾的适当泛化。我们关注于通过近端梯度映射定义的局部遗憾度量，该度量也包含了Hazan等人（2017）提出的原始概念。为了在此设置下实现无局部遗憾，我们开发了一种基于随机一阶反馈的近端梯度方法，以及一种在能够访问完美一阶oracle时的更简单方法。这两种方法在极小极大意义下都是最优的，并且我们还建立了这些方法所需的近端梯度查询次数的界限。作为我们结果的一个重要应用，我们还获得了在线和离线非凸随机优化之间的联系，表现为一种新的近端梯度方案，其复杂度保证与通过方差减少技术获得的结果相匹配。",
        "领域": "随机优化、非凸优化、在线学习",
        "问题": "在在线、约束、非光滑、非凸问题中，如何通过随机一阶oracle反馈实现遗憾最小化",
        "动机": "当前一阶方法无法实现外部遗憾的最小化，且缺乏基于梯度的算法框架来解决这一问题",
        "方法": "提出了一种利用非凸最优性度量的概念性方法，开发了基于随机一阶反馈的近端梯度方法和一种更简单的方法",
        "关键词": [
            "遗憾最小化",
            "近端梯度方法",
            "非凸优化",
            "随机优化",
            "在线学习"
        ],
        "涉及的技术概念": {
            "非凸最优性度量": "用于泛化学习者的局部遗憾，是方法的核心概念",
            "近端梯度映射": "定义局部遗憾度量的技术，也包含了原始概念",
            "方差减少技术": "用于匹配新近端梯度方案的复杂度保证"
        },
        "success": true
    },
    {
        "order": 910,
        "title": "Regularized Online Allocation Problems: Fairness and Beyond",
        "html": "https://ICML.cc//virtual/2021/poster/8455",
        "abstract": "Online allocation problems with resource constraints have a rich history in computer science and operations research. In this paper, we introduce the regularized online allocation problem, a variant that includes a non-linear regularizer acting on the total resource consumption. In this problem, requests repeatedly arrive over time and, for each request, a decision maker needs to take an action that generates a reward and consumes resources. The objective is to simultaneously maximize total rewards and the value of the regularizer subject to the resource constraints. Our primary motivation is the online allocation of internet advertisements wherein firms seek to maximize additive objectives such as the revenue or efficiency of the allocation. By introducing a regularizer, firms can account for the fairness of the allocation or, alternatively, punish under-delivery of advertisements---two common desiderata in internet advertising markets. We design an algorithm when arrivals are drawn independently from a distribution that is unknown to the decision maker. Our algorithm is simple, fast, and attains the optimal order of sub-linear regret compared to the optimal allocation with the benefit of hindsight. Numerical experiments confirm the effectiveness of the proposed algorithm and of the regularizers in an internet advertising application.",
        "conference": "ICML",
        "success": true,
        "中文标题": "正则化在线分配问题：公平性及其他",
        "摘要翻译": "具有资源约束的在线分配问题在计算机科学和运筹学中有着丰富的历史。本文介绍了正则化在线分配问题，这是一种包含作用于总资源消耗的非线性正则化器的变体。在这个问题中，请求随时间反复到达，对于每个请求，决策者需要采取一个行动，该行动会产生奖励并消耗资源。目标是在资源约束下同时最大化总奖励和正则化器的值。我们的主要动机是互联网广告的在线分配，其中公司寻求最大化诸如分配的收入或效率等加性目标。通过引入正则化器，公司可以考虑分配的公平性，或者惩罚广告的交付不足——这是互联网广告市场中两个常见的需求。我们设计了一种算法，当到达是从决策者未知的分布中独立抽取时。我们的算法简单、快速，并且与具有事后优势的最优分配相比，达到了次线性遗憾的最优顺序。数值实验证实了所提出算法和正则化器在互联网广告应用中的有效性。",
        "领域": "在线优化, 资源分配, 互联网广告",
        "问题": "在资源约束下，如何在线分配资源以同时最大化奖励和正则化器的值。",
        "动机": "解决互联网广告分配中的公平性和广告交付不足问题。",
        "方法": "设计了一种简单快速的算法，适用于到达请求独立于未知分布的情况，达到次线性遗憾的最优顺序。",
        "关键词": [
            "正则化在线分配",
            "公平性",
            "互联网广告",
            "次线性遗憾",
            "资源约束"
        ],
        "涉及的技术概念": {
            "正则化器": "用于在资源分配中考虑公平性或惩罚交付不足的非线性函数。",
            "次线性遗憾": "算法性能的度量，表示与最优分配相比的遗憾增长速度低于线性。",
            "在线分配算法": "一种在请求到达时实时做出分配决策的算法，适用于资源约束下的优化问题。"
        }
    },
    {
        "order": 911,
        "title": "Regularized Submodular Maximization at Scale",
        "html": "https://ICML.cc//virtual/2021/poster/8669",
        "abstract": "In this paper, we propose scalable methods for maximizing a regularized submodular function $f \\triangleq g-\\ell$ expressed as the difference between a monotone submodular function $g$ and a modular function $\\ell$. Submodularity is inherently related to the notions of diversity, coverage, and representativeness. In particular, finding the mode (i.e., the most likely configuration) of many popular probabilistic models of diversity, such as determinantal point processes and strongly log-concave distributions, involves maximization of (regularized) submodular functions. Since a regularized function $f$ can potentially take on negative values, the classic theory of submodular maximization, which heavily relies on the non-negativity assumption of submodular functions, is not applicable. To circumvent this challenge, we develop the first one-pass streaming algorithm for maximizing a regularized submodular function subject to a $k$-cardinality constraint. Furthermore, we develop the first distributed algorithm that returns a solution $S$ in $O(1/ \\epsilon)$ rounds of MapReduce computation. We highlight that our result, even for the unregularized case where the modular term $\\ell$ is zero, improves the memory and communication complexity of the state-of-the-art by a factor of $O(1/ \\epsilon)$ while arguably provides a simpler distributed algorithm and a unifying analysis. We empirically study the performance of our scalable methods on a set of real-life applications, including finding the mode of negatively correlated distributions, vertex cover of social networks, and several data summarization tasks.",
        "conference": "ICML",
        "中文标题": "规模化正则化子模最大化",
        "摘要翻译": "本文中，我们提出了可扩展的方法，用于最大化表示为单调子模函数g与模函数ℓ之差的正则化子模函数f≜g−ℓ。子模性本质上与多样性、覆盖性和代表性等概念相关。特别是，寻找许多流行的多样性概率模型（如行列式点过程和强对数凹分布）的模式（即最可能的配置）涉及（正则化）子模函数的最大化。由于正则化函数f可能取负值，经典子模最大化理论（其严重依赖于子模函数的非负性假设）不适用。为了绕过这一挑战，我们开发了第一个一流通式算法，用于在k基数约束下最大化正则化子模函数。此外，我们开发了第一个分布式算法，该算法在O(1/ε)轮MapReduce计算中返回解S。我们强调，即使在模项ℓ为零的非正则化情况下，我们的结果也通过O(1/ε)因子改进了现有技术的内存和通信复杂度，同时可以说提供了一个更简单的分布式算法和统一的分析。我们实证研究了我们的可扩展方法在一系列实际应用中的性能，包括寻找负相关分布的模式、社交网络的顶点覆盖以及几个数据摘要任务。",
        "领域": "优化算法, 分布式计算, 数据摘要",
        "问题": "解决在正则化子模函数可能取负值的情况下，如何进行高效的最大化问题",
        "动机": "由于经典子模最大化理论依赖于子模函数的非负性假设，无法直接应用于正则化子模函数的最大化问题，因此需要开发新的算法来解决这一限制",
        "方法": "开发了一流通式算法和分布式算法，用于在k基数约束下最大化正则化子模函数，并在MapReduce框架中实现高效计算",
        "关键词": [
            "正则化子模最大化",
            "分布式算法",
            "数据摘要",
            "行列式点过程",
            "强对数凹分布"
        ],
        "涉及的技术概念": {
            "子模函数": "用于描述多样性、覆盖性和代表性等概念的数学函数，是本文研究的基础",
            "正则化子模函数": "由单调子模函数与模函数之差定义的函数，可能取负值，是本文研究的核心对象",
            "MapReduce": "一种编程模型，用于大规模数据集的并行处理，本文中用于实现分布式算法"
        },
        "success": true
    },
    {
        "order": 912,
        "title": "Regularizing towards Causal Invariance: Linear Models with Proxies",
        "html": "https://ICML.cc//virtual/2021/poster/9911",
        "abstract": "We propose a method for learning linear models whose predictive performance is robust to causal interventions on unobserved variables, when noisy proxies of those variables are available.  Our approach takes the form of a regularization term that trades off between in-distribution performance and robustness to interventions.  Under the assumption of a linear structural causal model, we show that a single proxy can be used to create estimators that are prediction optimal under interventions of bounded strength. This strength depends on the magnitude of the measurement noise in the proxy, which is, in general, not identifiable. In the case of two proxy variables, we propose a modified estimator that is prediction optimal under interventions up to a known strength.  We further show how to extend these estimators to scenarios where additional information about the 'test time' intervention is available during training. We evaluate our theoretical findings in synthetic experiments and using real data of hourly pollution levels across several cities in China.\n",
        "conference": "ICML",
        "中文标题": "向因果不变性正则化：带有代理的线性模型",
        "摘要翻译": "我们提出了一种学习线性模型的方法，该模型的预测性能对于未观测变量的因果干预具有鲁棒性，前提是这些变量的噪声代理是可用的。我们的方法采用了一种正则化项的形式，该正则化项在分布内性能和干预鲁棒性之间进行权衡。在线性结构因果模型的假设下，我们展示了可以使用单个代理来创建在有限强度干预下预测最优的估计器。这种强度取决于代理中测量噪声的大小，这在一般情况下是不可识别的。在两个代理变量的情况下，我们提出了一种改进的估计器，该估计器在已知强度以下的干预下预测最优。我们进一步展示了如何将这些估计器扩展到在训练期间可获得关于‘测试时间’干预的额外信息的场景。我们在合成实验中评估了我们的理论发现，并使用了中国几个城市每小时污染水平的真实数据。",
        "领域": "因果推断、机器学习、环境数据分析",
        "问题": "学习在未观测变量受到因果干预时预测性能依然鲁棒的线性模型",
        "动机": "提高模型在现实世界因果干预下的预测稳定性和准确性",
        "方法": "引入正则化项以平衡模型在分布内性能和干预鲁棒性之间的表现，基于线性结构因果模型假设开发预测最优估计器",
        "关键词": [
            "因果推断",
            "正则化",
            "线性模型",
            "预测鲁棒性",
            "代理变量"
        ],
        "涉及的技术概念": {
            "正则化项": "用于在模型训练中平衡分布内性能和干预鲁棒性的技术手段",
            "线性结构因果模型": "假设变量间关系为线性的因果模型框架，用于理论分析和估计器开发",
            "代理变量": "作为未观测变量噪声代理的变量，用于在无法直接观测的情况下进行因果推断"
        },
        "success": true
    },
    {
        "order": 913,
        "title": "Reinforcement Learning for Cost-Aware Markov Decision Processes",
        "html": "https://ICML.cc//virtual/2021/poster/10597",
        "abstract": "Ratio maximization has applications in areas as diverse as finance, reward shaping for reinforcement learning (RL), and the development of safe artificial intelligence, yet there has been very little exploration of RL algorithms for ratio maximization. This paper addresses this deficiency by introducing two new, model-free RL algorithms for solving cost-aware Markov decision processes, where the goal is to maximize the ratio of long-run average reward to long-run average cost. The first algorithm is a two-timescale scheme based on relative value iteration (RVI) Q-learning and the second is an actor-critic scheme. The paper proves almost sure convergence of the former to the globally optimal solution in the tabular case\nand almost sure convergence of the latter under linear function approximation for the critic. Unlike previous methods, the two algorithms provably converge for general reward and cost functions under suitable conditions. The paper also provides empirical results demonstrating promising performance and lending strong support to the theoretical results.",
        "conference": "ICML",
        "中文标题": "成本感知马尔可夫决策过程的强化学习",
        "摘要翻译": "比率最大化在金融、强化学习（RL）的奖励塑造以及安全人工智能的开发等多个领域有着广泛的应用，然而对于比率最大化的强化学习算法的探索却非常有限。本文通过引入两种新的、无模型的强化学习算法来解决成本感知马尔可夫决策过程的问题，其目标是最大化长期平均奖励与长期平均成本的比率。第一种算法是基于相对值迭代（RVI）Q学习的双时间尺度方案，第二种是演员-评论家方案。本文证明了在表格情况下，前者几乎必然收敛于全局最优解，并且在评论家使用线性函数逼近的情况下，后者也几乎必然收敛。与之前的方法不同，这两种算法在适当条件下对于一般的奖励和成本函数都能证明收敛。本文还提供了实证结果，展示了有希望的性能，并强有力地支持了理论结果。",
        "领域": "强化学习、马尔可夫决策过程、奖励塑造",
        "问题": "解决成本感知马尔可夫决策过程中的比率最大化问题",
        "动机": "探索比率最大化在强化学习中的应用，特别是在成本感知马尔可夫决策过程中的应用",
        "方法": "引入两种无模型的强化学习算法：基于相对值迭代（RVI）Q学习的双时间尺度方案和演员-评论家方案",
        "关键词": [
            "比率最大化",
            "成本感知马尔可夫决策过程",
            "强化学习",
            "相对值迭代",
            "演员-评论家"
        ],
        "涉及的技术概念": {
            "相对值迭代（RVI）Q学习": "用于在表格情况下几乎必然收敛于全局最优解的强化学习算法",
            "演员-评论家方案": "一种结合了策略梯度和值函数逼近的强化学习算法，适用于线性函数逼近的情况",
            "比率最大化": "在强化学习中，最大化长期平均奖励与长期平均成本的比率，应用于成本感知马尔可夫决策过程"
        },
        "success": true
    },
    {
        "order": 914,
        "title": "Reinforcement Learning of Implicit and Explicit Control Flow Instructions",
        "html": "https://ICML.cc//virtual/2021/poster/9567",
        "abstract": "Learning to flexibly follow task instructions in dynamic environments poses interesting challenges for reinforcement learning agents. We focus here on the problem of learning control flow that deviates from a strict step-by-step execution of instructions—that is, control flow that may skip forward over parts of the instructions or return backward to previously completed or skipped steps. Demand for such flexible control arises in two fundamental ways: explicitly when control is specified in the instructions themselves (such as conditional branching and looping) and implicitly when stochastic environment dynamics require re-completion of instructions whose effects have been perturbed, or opportunistic skipping of instructions whose effects are already present. We formulate an attention-based architecture that meets these challenges by learning, from task reward only, to flexibly attend to and condition behavior on an internal encoding of the instructions. We test the architecture's ability to learn both explicit and implicit control in two illustrative domains---one inspired by Minecraft and the other by StarCraft---and show that the architecture exhibits zero-shot generalization to novel instructions of length greater than those in a training set, at a performance level unmatched by three baseline recurrent architectures and one ablation architecture. ",
        "conference": "ICML",
        "中文标题": "隐式和显式控制流指令的强化学习",
        "摘要翻译": "在动态环境中灵活遵循任务指令对强化学习代理提出了有趣的挑战。我们在此关注于学习偏离严格逐步执行指令的控制流问题——即可能跳过指令部分或返回到先前完成或跳过的步骤的控制流。这种灵活控制的需求以两种基本方式产生：显式地当控制在指令本身中指定时（如条件分支和循环），以及隐式地当随机环境动态需要重新完成其效果已被扰动的指令，或机会性地跳过效果已经存在的指令时。我们提出了一种基于注意力的架构，通过仅从任务奖励中学习，灵活地关注并以指令的内部编码为条件来应对这些挑战。我们在两个说明性领域——一个受Minecraft启发，另一个受StarCraft启发——测试了该架构学习显式和隐式控制的能力，并展示了该架构在训练集中未见过的更长指令上的零样本泛化能力，其性能水平是三个基线循环架构和一个消融架构无法比拟的。",
        "领域": "强化学习、游戏AI、指令理解与执行",
        "问题": "如何在动态环境中学习灵活遵循偏离严格逐步执行指令的控制流",
        "动机": "解决强化学习代理在需要跳过或返回指令步骤的动态环境中灵活执行任务的问题",
        "方法": "提出了一种基于注意力的架构，通过从任务奖励中学习，灵活地关注并以指令的内部编码为条件来执行控制流",
        "关键词": [
            "强化学习",
            "控制流",
            "注意力机制",
            "零样本泛化",
            "游戏AI"
        ],
        "涉及的技术概念": {
            "注意力机制": "用于使代理能够灵活地关注指令的相关部分，并根据内部编码调整行为",
            "零样本泛化": "指模型能够处理训练集中未出现过的指令长度的能力",
            "控制流": "指代理在执行任务时跳过或返回指令步骤的能力，包括显式和隐式控制"
        },
        "success": true
    },
    {
        "order": 915,
        "title": "Reinforcement Learning Under Moral Uncertainty",
        "html": "https://ICML.cc//virtual/2021/poster/9589",
        "abstract": "An ambitious goal for machine learning is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed, e.g. fully autonomous vehicles will encounter charged moral decisions that complicate their deployment. While ethical agents could be trained by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement about the nature of morality. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. This paper translates such insights to the field of reinforcement learning, proposes two training methods that realize different points among competing desiderata, and trains agents in simple environments to act under moral uncertainty. The results illustrate (1) how such uncertainty can help curb extreme behavior from commitment to single theories and (2) several technical complications arising from attempting to ground moral philosophy in RL (e.g. how can a principled trade-off between two competing but incomparable reward functions be reached). The aim is to catalyze progress towards morally-competent agents and highlight the potential of RL to contribute towards the computational grounding of moral philosophy.",
        "conference": "ICML",
        "中文标题": "道德不确定性下的强化学习",
        "摘要翻译": "机器学习的一个雄心勃勃的目标是创造行为合乎道德的代理：遵守人类道德规范的能力将极大地扩展自主代理可以实际安全部署的上下文，例如完全自动驾驶汽车将遇到复杂的道德决策，这些决策会使其部署变得复杂。虽然可以通过在特定道德理论（如功利主义）下奖励正确行为来训练道德代理，但关于道德本质的广泛分歧仍然存在。承认这种分歧，道德哲学的最新研究提出，道德行为需要在道德不确定性下行动，即在行动时考虑到一个人的信念分散在几种看似合理的道德理论中。本文将这种见解转化为强化学习领域，提出了两种实现不同竞争需求点的训练方法，并在简单环境中训练代理在道德不确定性下行动。结果说明了（1）这种不确定性如何有助于遏制对单一理论的极端行为承诺，以及（2）尝试将道德哲学基础建立在强化学习中时出现的几个技术复杂性（例如，如何在两个竞争但不可比较的奖励函数之间达成原则性的权衡）。目标是催化向具有道德能力的代理的进展，并突出强化学习在道德哲学计算基础中的潜在贡献。",
        "领域": "强化学习伦理应用、自主代理道德决策、多道德理论整合",
        "问题": "如何在道德不确定性下训练强化学习代理，使其能够考虑多种道德理论并做出决策。",
        "动机": "为了解决自主代理在复杂道德决策中的行为问题，特别是在存在多种道德理论分歧的情况下，如何使代理能够在道德不确定性下行动。",
        "方法": "提出了两种训练方法，旨在在简单环境中训练代理在道德不确定性下行动，并探讨了在强化学习中实现道德哲学基础的技术挑战。",
        "关键词": [
            "道德不确定性",
            "强化学习",
            "自主代理",
            "道德决策",
            "多理论整合"
        ],
        "涉及的技术概念": {
            "道德不确定性": "指代理在行动时考虑到其信念分散在几种看似合理的道德理论中的状态，用于指导代理在复杂道德决策中的行为。",
            "强化学习": "一种机器学习方法，代理通过与环境互动并根据奖励信号学习最优行为策略，本文中用于训练代理在道德不确定性下行动。",
            "多理论整合": "指在决策过程中考虑并整合多种道德理论的观点，用于避免对单一理论的极端承诺，促进更全面和平衡的道德决策。"
        },
        "success": true
    },
    {
        "order": 916,
        "title": "Reinforcement Learning with Prototypical Representations",
        "html": "https://ICML.cc//virtual/2021/poster/10423",
        "abstract": "Learning effective representations in image-based environments is crucial for sample efficient Reinforcement Learning (RL). Unfortunately, in RL, representation learning is confounded with the exploratory experience of the agent -- learning a useful representation requires diverse data, while effective exploration is only possible with coherent representations. Furthermore, we would like to learn representations that not only generalize across tasks but also accelerate downstream exploration for efficient task-specific training. To address these challenges we propose Proto-RL, a self-supervised framework that ties representation learning with exploration through prototypical representations. These prototypes simultaneously serve as a summarization of the exploratory experience of an agent as well as a basis for representing observations. We pre-train these task-agnostic representations and prototypes on environments without downstream task information. This enables state-of-the-art downstream policy learning on a set of difficult continuous control tasks. ",
        "conference": "ICML",
        "中文标题": "基于原型表征的强化学习",
        "摘要翻译": "在基于图像的环境中学习有效的表征对于样本高效的强化学习（RL）至关重要。然而，在RL中，表征学习与智能体的探索经验相混淆——学习有用的表征需要多样化的数据，而有效的探索只有在有连贯的表征时才可能实现。此外，我们希望学习的表征不仅能够跨任务泛化，还能加速下游探索以实现高效的任务特定训练。为了解决这些挑战，我们提出了Proto-RL，一个通过原型表征将表征学习与探索结合的自监督框架。这些原型同时作为智能体探索经验的总结以及观察表征的基础。我们在没有下游任务信息的环境上预训练这些任务无关的表征和原型。这使得在一组困难的连续控制任务上实现了最先进的下游策略学习。",
        "领域": "强化学习、连续控制、自监督学习",
        "问题": "解决在强化学习中表征学习与探索经验之间的相互依赖问题，以及如何学习能够跨任务泛化并加速下游探索的表征。",
        "动机": "为了克服强化学习中表征学习与探索之间的相互依赖问题，并开发能够跨任务泛化且加速下游探索的表征学习方法。",
        "方法": "提出了Proto-RL框架，通过原型表征将表征学习与探索结合，预训练任务无关的表征和原型以支持下游策略学习。",
        "关键词": [
            "强化学习",
            "原型表征",
            "自监督学习",
            "连续控制",
            "表征学习"
        ],
        "涉及的技术概念": {
            "原型表征": "作为智能体探索经验的总结和观察表征的基础，用于连接表征学习与探索。",
            "自监督学习": "用于在没有下游任务信息的环境上预训练任务无关的表征和原型。",
            "连续控制任务": "Proto-RL框架在这些任务上实现了最先进的下游策略学习。"
        },
        "success": true
    },
    {
        "order": 917,
        "title": "Relative Deviation Margin Bounds",
        "html": "https://ICML.cc//virtual/2021/poster/10193",
        "abstract": "We present a series of new and more favorable margin-based learning guarantees that depend on the empirical margin loss of a predictor.  e give two types of learning bounds, in terms of either the Rademacher complexity or the empirical $\\ell_\\infty$-covering number of the hypothesis set used, both distribution-dependent and valid for general families.  Furthermore, using our relative deviation margin bounds, we derive distribution-dependent generalization bounds for unbounded loss functions under the assumption of a finite moment.  We also briefly highlight several applications of these bounds and discuss their connection with existing results.",
        "conference": "ICML",
        "success": true,
        "中文标题": "相对偏差边际界限",
        "摘要翻译": "我们提出了一系列新的、更有利的基于边际的学习保证，这些保证依赖于预测器的经验边际损失。我们给出了两种类型的学习界限，一种是基于假设集使用的Rademacher复杂性，另一种是基于经验$\\ell_\\infty$-覆盖数，这两种界限都是分布依赖的，并且适用于一般家族。此外，利用我们的相对偏差边际界限，我们在有限矩的假设下，为无界损失函数推导出了分布依赖的泛化界限。我们还简要强调了这些界限的几个应用，并讨论了它们与现有结果的联系。",
        "领域": "机器学习理论, 统计学习理论, 泛化界限",
        "问题": "提出新的基于边际的学习保证，以更准确地评估预测器的性能",
        "动机": "为了提供更准确和分布依赖的学习保证，特别是在处理无界损失函数时",
        "方法": "利用Rademacher复杂性和经验$\\ell_\\infty$-覆盖数来推导学习界限，并应用相对偏差边际界限来泛化无界损失函数",
        "关键词": [
            "边际界限",
            "Rademacher复杂性",
            "$\\ell_\\infty$-覆盖数",
            "泛化界限",
            "无界损失函数"
        ],
        "涉及的技术概念": {
            "Rademacher复杂性": "用于衡量假设集的复杂性，帮助推导学习界限",
            "$\\ell_\\infty$-覆盖数": "用于描述假设集在$\\ell_\\infty$范数下的覆盖能力，是推导学习界限的关键"
        }
    },
    {
        "order": 918,
        "title": "Relative Positional Encoding for Transformers with Linear Complexity",
        "html": "https://ICML.cc//virtual/2021/poster/9021",
        "abstract": "Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.",
        "conference": "ICML",
        "中文标题": "线性复杂度Transformer的相对位置编码",
        "摘要翻译": "近年来，Transformer模型的进步使得序列长度达到了前所未有的水平，这得益于线性的空间和时间复杂度。同时，相对位置编码（RPE）被提出作为对经典Transformer有益的技术，它利用滞后而非绝对位置进行推理。然而，RPE并不适用于最近的线性变体Transformer，因为它需要显式计算注意力矩阵，而这正是这些方法所避免的。在本文中，我们填补了这一空白，提出了随机位置编码作为生成PE的方法，可以替代经典的加法（正弦）PE，并且可证明其行为类似于RPE。主要的理论贡献是在位置编码和相关高斯过程的交叉协方差结构之间建立了联系。我们在长距离竞技场基准和音乐生成上展示了我们方法的性能。",
        "领域": "自然语言处理与视觉结合, 序列建模, 音乐生成",
        "问题": "解决线性变体Transformer无法使用相对位置编码的问题",
        "动机": "为了使线性变体Transformer能够利用相对位置编码的优势，提高模型处理长序列的能力",
        "方法": "提出随机位置编码方法，替代传统的加法位置编码，并在理论上证明其与相对位置编码的等效性",
        "关键词": [
            "相对位置编码",
            "线性复杂度",
            "Transformer",
            "随机位置编码",
            "高斯过程"
        ],
        "涉及的技术概念": {
            "相对位置编码（RPE）": "利用序列中元素之间的相对位置信息，而非绝对位置，以提高模型对序列结构的理解",
            "线性复杂度Transformer": "通过优化算法减少计算复杂度，使得Transformer模型能够处理更长的序列",
            "随机位置编码": "一种新的位置编码方法，通过随机生成的位置编码来模拟相对位置编码的效果，适用于线性复杂度Transformer"
        },
        "success": true
    },
    {
        "order": 919,
        "title": "REPAINT: Knowledge Transfer in Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8839",
        "abstract": "Accelerating learning processes for complex tasks by leveraging previously learned tasks has been one of the most challenging problems in reinforcement learning, especially when the similarity between source and target tasks is low. This work proposes REPresentation And INstance Transfer (REPAINT) algorithm for knowledge transfer in deep reinforcement learning. REPAINT not only transfers the representation of a pre-trained teacher policy in the on-policy learning, but also uses an advantage-based experience selection approach to transfer useful samples collected following the teacher policy in the off-policy learning. Our experimental results on several benchmark tasks show that REPAINT significantly reduces the total training time in generic cases of task similarity. In particular, when the source tasks are dissimilar to, or sub-tasks of, the target tasks, REPAINT outperforms other baselines in both training-time reduction and asymptotic performance of return scores.",
        "conference": "ICML",
        "中文标题": "REPAINT：深度强化学习中的知识迁移",
        "摘要翻译": "通过利用先前学习的任务来加速复杂任务的学习过程一直是强化学习中最具挑战性的问题之一，尤其是当源任务与目标任务之间的相似性较低时。这项工作提出了用于深度强化学习中知识迁移的表示与实例迁移（REPAINT）算法。REPAINT不仅在同策略学习中迁移预训练教师策略的表示，而且还使用基于优势的经验选择方法，在异策略学习中迁移遵循教师策略收集的有用样本。我们在几个基准任务上的实验结果表明，REPAINT在任务相似性的一般情况下显著减少了总训练时间。特别是，当源任务与目标任务不相似或是其子任务时，REPAINT在训练时间减少和回报分数的渐近性能方面均优于其他基线方法。",
        "领域": "深度强化学习、知识迁移、策略优化",
        "问题": "解决在源任务与目标任务相似性低的情况下，如何有效迁移知识以加速学习过程的问题。",
        "动机": "研究动机在于探索在深度强化学习中，如何通过知识迁移来加速复杂任务的学习过程，特别是在源任务与目标任务相似性低的情况下。",
        "方法": "提出了REPAINT算法，该算法在同策略学习中迁移预训练教师策略的表示，并在异策略学习中使用基于优势的经验选择方法来迁移有用的样本。",
        "关键词": [
            "知识迁移",
            "深度强化学习",
            "策略优化",
            "经验选择",
            "任务相似性"
        ],
        "涉及的技术概念": {
            "表示迁移": "在同策略学习中迁移预训练教师策略的表示，以加速目标任务的学习。",
            "实例迁移": "在异策略学习中使用基于优势的经验选择方法，迁移遵循教师策略收集的有用样本。",
            "优势函数": "用于评估和选择最有价值的经验样本，以优化知识迁移过程。"
        },
        "success": true
    },
    {
        "order": 920,
        "title": "Representational aspects of depth and conditioning in normalizing flows",
        "html": "https://ICML.cc//virtual/2021/poster/9495",
        "abstract": "Normalizing flows are among the most popular paradigms in generative modeling, especially for images, primarily because we can efficiently evaluate the likelihood of a data point. This is desirable both for evaluating the fit of a model, and for ease of training, as maximizing the likelihood can be done by gradient descent. However, training normalizing flows comes with difficulties as well: models which produce good samples typically need to be extremely deep -- which comes with accompanying vanishing/exploding gradient problems. A very related problem is that they are often poorly \\emph{conditioned}: since they are parametrized as invertible maps from $\\mathbb{R}^d \\to \\mathbb{R}^d$, and typical training data like images intuitively is lower-dimensional, the learned maps often have Jacobians that are close to being singular. \n\nIn our paper, we tackle representational aspects around depth and conditioning of normalizing flows: both for general invertible architectures, and for a particular common architecture, affine couplings. We prove that $\\Theta(1)$ affine coupling layers suffice to exactly represent a permutation or $1 \\times 1$ convolution, as used in GLOW, showing that representationally the choice of partition is not a bottleneck for depth. We also show that shallow affine coupling networks are universal approximators in Wasserstein distance if ill-conditioning is allowed, and experimentally investigate related phenomena involving padding. Finally, we show a depth lower bound for general flow architectures with few neurons per layer and bounded Lipschitz constant.",
        "conference": "ICML",
        "success": true,
        "中文标题": "规范化流中深度与条件性的表示方面",
        "摘要翻译": "规范化流是生成模型中最受欢迎的范式之一，尤其是在图像领域，主要因为我们可以高效地评估数据点的似然。这对于评估模型的拟合度以及简化训练过程都是非常理想的，因为通过梯度下降可以最大化似然。然而，训练规范化流也存在困难：产生良好样本的模型通常需要非常深——这会伴随着梯度消失/爆炸的问题。一个非常相关的问题是它们通常条件性差：因为它们被参数化为从ℝᵈ到ℝᵈ的可逆映射，而像图像这样的典型训练数据直观上是低维的，学习到的映射通常具有接近奇异的雅可比矩阵。在我们的论文中，我们解决了规范化流中关于深度和条件性的表示问题：既针对一般的可逆架构，也针对一种特定的常见架构——仿射耦合。我们证明了Θ(1)仿射耦合层足以精确表示GLOW中使用的排列或1×1卷积，表明在表示上，分区选择不是深度的瓶颈。我们还表明，如果允许条件性差，浅层仿射耦合网络在Wasserstein距离中是通用逼近器，并通过实验研究了涉及填充的相关现象。最后，我们展示了对于每层神经元较少且Lipschitz常数有界的一般流架构的深度下界。",
        "领域": "生成模型, 图像生成, 深度学习优化",
        "问题": "规范化流在训练过程中遇到的深度和条件性问题，以及如何优化这些流的表示能力。",
        "动机": "解决规范化流在生成模型中的深度和条件性问题，以提高模型的训练效率和生成质量。",
        "方法": "通过理论证明和实验研究，探讨了仿射耦合层的表示能力，以及在特定条件下浅层网络的通用逼近能力。",
        "关键词": [
            "规范化流",
            "仿射耦合",
            "Wasserstein距离",
            "深度下界",
            "条件性"
        ],
        "涉及的技术概念": {
            "规范化流": "一种生成模型，通过可逆变换将简单分布转换为复杂分布，用于高效评估数据点的似然。",
            "仿射耦合": "规范化流中的一种特定架构，通过仿射变换实现数据的可逆映射。",
            "Wasserstein距离": "用于衡量两个概率分布之间差异的度量，本文中用于分析浅层网络的逼近能力。"
        }
    },
    {
        "order": 921,
        "title": "Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data",
        "html": "https://ICML.cc//virtual/2021/poster/10069",
        "abstract": "Collecting more diverse and representative training data is often touted as a remedy for the disparate performance of machine learning predictors across subpopulations. However, a precise framework for understanding how dataset properties like diversity affect learning outcomes is largely lacking. By casting data collection as part of the learning process, we demonstrate that diverse representation in training data is key not only to increasing subgroup performances, but also to achieving population-level objectives. Our analysis and experiments describe how dataset compositions influence performance and provide constructive results for using trends in existing data, alongside domain knowledge, to help guide intentional, objective-aware dataset design",
        "conference": "ICML",
        "中文标题": "表征的重要性：评估训练数据中子群分配的重要性",
        "摘要翻译": "收集更多样化和具有代表性的训练数据常被视为解决机器学习预测器在不同子群体间性能差异的良方。然而，关于数据集属性（如多样性）如何影响学习结果的精确框架在很大程度上是缺失的。通过将数据收集视为学习过程的一部分，我们证明了训练数据中的多样性表征不仅对提高子群体性能至关重要，也对实现群体层面的目标至关重要。我们的分析和实验描述了数据集组成如何影响性能，并提供了利用现有数据趋势及领域知识来指导有目的、目标意识的数据集设计的建设性结果。",
        "领域": "机器学习公平性、数据多样性分析、子群体性能优化",
        "问题": "如何通过优化训练数据的子群分配来提高机器学习模型在不同子群体上的性能和公平性。",
        "动机": "解决机器学习模型在不同子群体间性能差异的问题，探索数据多样性对模型性能的影响。",
        "方法": "将数据收集视为学习过程的一部分，分析数据集组成对性能的影响，并提出基于现有数据趋势和领域知识的数据集设计方法。",
        "关键词": [
            "数据多样性",
            "子群性能",
            "机器学习公平性",
            "数据集设计",
            "表征学习"
        ],
        "涉及的技术概念": {
            "数据多样性": "指训练数据中包含广泛不同的样本，以确保模型能够泛化到不同的子群体。",
            "子群性能": "模型在特定子群体上的表现，研究旨在优化这些性能以减少差异。",
            "机器学习公平性": "确保机器学习模型的决策在不同子群体间是公平和无偏的。"
        },
        "success": true
    },
    {
        "order": 922,
        "title": "Representation Matters: Offline Pretraining for Sequential Decision Making",
        "html": "https://ICML.cc//virtual/2021/poster/10365",
        "abstract": "The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives – e.g., reward prediction, continuous or discrete representations, pretraining or finetuning – are most important and in which settings.",
        "conference": "ICML",
        "中文标题": "表征的重要性：序列决策制定的离线预训练",
        "摘要翻译": "最近，监督学习方法在越来越大的离线数据集上取得的成功激发了强化学习（RL）领域的兴趣，研究者们开始探讨是否可以将相同的范式转移到RL算法中。这一研究领域，被称为离线RL，主要集中于离线策略优化，旨在仅从离线数据中找到回报最大化的策略。在本文中，我们考虑了一种稍微不同的方法，将离线数据整合到序列决策制定中。我们旨在回答的问题是，应用于离线数据集的哪些无监督目标能够学习到提升下游任务性能的状态表征，无论这些下游任务是在线RL、从专家演示中模仿学习，还是基于相同离线数据集的离线策略优化？通过利用标准离线RL数据集进行的一系列实验，我们发现，使用无监督学习目标进行预训练可以显著提高策略学习算法的性能，这些算法在其他情况下表现平平。大量的消融实验进一步提供了关于这些无监督目标的哪些组成部分——例如，奖励预测、连续或离散表征、预训练或微调——在哪些设置中最为重要的见解。",
        "领域": "强化学习、模仿学习、策略优化",
        "问题": "如何通过无监督学习目标从离线数据中学习状态表征，以提升下游任务的性能",
        "动机": "探索无监督学习在离线数据上的应用，以提升序列决策制定任务的性能",
        "方法": "利用无监督学习目标进行预训练，通过实验评估不同组成部分对性能的影响",
        "关键词": [
            "离线强化学习",
            "无监督学习",
            "状态表征",
            "预训练",
            "策略优化"
        ],
        "涉及的技术概念": {
            "离线RL": "研究领域，专注于从离线数据中学习策略，而不需要与环境交互",
            "无监督学习目标": "用于预训练的目标，不需要标注数据，旨在学习有用的数据表征",
            "状态表征": "对环境的抽象表示，用于提升下游任务的性能和学习效率"
        },
        "success": true
    },
    {
        "order": 923,
        "title": "Representation Subspace Distance for Domain Adaptation Regression",
        "html": "https://ICML.cc//virtual/2021/poster/8637",
        "abstract": "Regression, as a counterpart to classification, is a major paradigm with a wide range of applications. Domain adaptation regression extends it by generalizing a regressor from a labeled source domain to an unlabeled target domain. Existing domain adaptation regression methods have achieved positive results limited only to the shallow regime. A question arises: Why learning invariant representations in the deep regime less pronounced? A key finding of this paper is that classification is robust to feature scaling but regression is not, and aligning the distributions of deep representations will alter feature scale and impede domain adaptation regression.\nBased on this finding, we propose to close the domain gap through orthogonal bases of the representation spaces, which are free from feature scaling. Inspired by Riemannian geometry of Grassmann manifold, we define a geometrical distance over representation subspaces and learn deep transferable representations by minimizing it. To avoid breaking the geometrical properties of deep representations, we further introduce the bases mismatch penalization to match the ordering of orthogonal bases across representation subspaces. Our method is evaluated on three domain adaptation regression benchmarks, two of which are introduced in this paper. Our method outperforms the state-of-the-art methods significantly, forming early positive results in the deep regime.",
        "conference": "ICML",
        "中文标题": "表示子空间距离用于域适应回归",
        "摘要翻译": "回归作为分类的对立面，是一种具有广泛应用的主要范式。域适应回归通过将回归器从有标记的源域推广到无标记的目标域来扩展它。现有的域适应回归方法仅在浅层机制中取得了有限的积极结果。一个问题随之而来：为什么在深层机制中学习不变表示不那么明显？本文的一个关键发现是，分类对特征缩放具有鲁棒性，而回归则不然，对齐深层表示的分布会改变特征尺度并阻碍域适应回归。基于这一发现，我们提出通过表示空间的正交基来缩小域间差距，这些正交基不受特征缩放的影响。受Grassmann流形的黎曼几何启发，我们定义了一个关于表示子空间的几何距离，并通过最小化它来学习深度可转移表示。为了避免破坏深层表示的几何特性，我们进一步引入了基不匹配惩罚来匹配表示子空间之间正交基的排序。我们的方法在三个域适应回归基准上进行了评估，其中两个是在本文中引入的。我们的方法显著优于最先进的方法，在深层机制中形成了早期的积极结果。",
        "领域": "域适应学习、回归分析、深度学习",
        "问题": "解决在深层机制中学习不变表示以促进域适应回归的问题",
        "动机": "发现分类对特征缩放具有鲁棒性而回归不具备，对齐深层表示的分布会改变特征尺度并阻碍域适应回归",
        "方法": "通过表示空间的正交基缩小域间差距，定义表示子空间的几何距离并最小化它来学习深度可转移表示，引入基不匹配惩罚来匹配正交基的排序",
        "关键词": [
            "域适应回归",
            "表示子空间",
            "Grassmann流形",
            "正交基",
            "几何距离"
        ],
        "涉及的技术概念": {
            "表示子空间": "用于描述不同域中数据的表示空间，通过其正交基来缩小域间差距",
            "Grassmann流形": "提供了一种几何框架，用于定义表示子空间之间的距离，促进深度可转移表示的学习",
            "基不匹配惩罚": "用于确保不同表示子空间的正交基排序一致，保持深层表示的几何特性"
        },
        "success": true
    },
    {
        "order": 924,
        "title": "Reserve Price Optimization for First Price Auctions in Display Advertising",
        "html": "https://ICML.cc//virtual/2021/poster/9295",
        "abstract": "The display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. In light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. In this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. Our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. We show that revenue in a first-price auction can be usefully decomposed into a \\emph{demand} component and a \\emph{bidding} component, and introduce techniques to reduce the variance of each component. We characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from a major ad exchange.",
        "conference": "ICML",
        "中文标题": "展示广告中第一价格拍卖的保留价优化",
        "摘要翻译": "展示广告行业最近从第二价格拍卖转变为第一价格拍卖，作为其广告分配和定价的主要机制。鉴于此，出版商需要重新评估并优化其拍卖参数，尤其是保留价。在本文中，我们提出了一种基于梯度的算法，根据投标人对保留价实验性变化的响应估计，自适应地更新和优化保留价。我们的关键创新是利用收入目标的固有结构，以减少梯度估计的方差，并在理论和实践中提高收敛速度。我们展示了第一价格拍卖中的收入可以有效地分解为需求部分和投标部分，并介绍了减少每部分方差的技术。我们描述了这些技术的偏差-方差权衡，并通过合成数据和来自主要广告交易平台的真实展示广告拍卖数据验证了我们提出算法的性能。",
        "领域": "在线广告拍卖优化、第一价格拍卖机制、保留价策略",
        "问题": "在第一价格拍卖机制下，如何优化保留价以最大化出版商收入",
        "动机": "随着展示广告行业从第二价格拍卖转向第一价格拍卖，出版商需要新的方法来优化拍卖参数，特别是保留价，以适应新的拍卖环境并最大化收入。",
        "方法": "提出了一种基于梯度的算法，通过估计投标人对保留价变化的响应来自适应地优化保留价，利用收入目标的结构减少梯度估计的方差，提高收敛速度。",
        "关键词": [
            "第一价格拍卖",
            "保留价优化",
            "梯度算法",
            "收入最大化",
            "广告拍卖"
        ],
        "涉及的技术概念": {
            "梯度算法": "用于自适应地更新和优化保留价，基于投标人对保留价变化的响应估计。",
            "收入分解": "将第一价格拍卖中的收入分解为需求部分和投标部分，以便更有效地优化。",
            "偏差-方差权衡": "在优化过程中，需要平衡估计的偏差和方差，以达到最优的保留价设置。"
        },
        "success": true
    },
    {
        "order": 925,
        "title": "Resource Allocation in Multi-armed Bandit Exploration: Overcoming Sublinear Scaling with Adaptive Parallelism",
        "html": "https://ICML.cc//virtual/2021/poster/10643",
        "abstract": "We study exploration in stochastic multi-armed bandits when we have access to a divisible resource that can be allocated in varying amounts to arm pulls. We focus in particular on the allocation of distributed computing resources, where we may obtain results faster by allocating more resources per pull, but might have reduced throughput due to nonlinear scaling. For example, in simulation-based scientific studies, an expensive simulation can be sped up by running it on multiple cores. This speed-up however, is partly offset by the communication among cores, which results in lower throughput than if fewer cores were allocated to run more trials in parallel. In this paper, we explore these trade-offs in two settings. First, in a fixed confidence setting, we need to find the best arm with a given target success probability as quickly as possible. We propose an algorithm which trades off between information accumulation and throughput and show that the time taken can be upper bounded by the solution of a dynamic program whose inputs are the gaps between the sub-optimal and optimal arms. We also prove a matching hardness result. Second, we present an algorithm for a fixed deadline setting, where we are given a time deadline and need to maximize the probability of finding the best arm. We corroborate our theoretical insights with simulation experiments that show that the algorithms consistently match or outperform baseline algorithms on a variety of problem instances.",
        "conference": "ICML",
        "中文标题": "多臂老虎机探索中的资源分配：通过自适应并行性克服次线性扩展",
        "摘要翻译": "我们研究了在可以获取可分资源并将其以不同量分配给臂拉动的情况下，随机多臂老虎机中的探索问题。我们特别关注分布式计算资源的分配，其中通过为每次拉动分配更多资源可以更快地获得结果，但由于非线性扩展可能会降低吞吐量。例如，在基于模拟的科学研究中，通过在多个核心上运行昂贵的模拟可以加速模拟过程。然而，这种加速部分被核心间的通信所抵消，导致吞吐量低于如果分配较少核心以并行运行更多试验的情况。在本文中，我们在两种设置下探索了这些权衡。首先，在固定置信度设置中，我们需要尽快找到具有给定目标成功概率的最佳臂。我们提出了一种在信息积累和吞吐量之间进行权衡的算法，并表明所花费的时间可以通过动态程序的解来上限，该动态程序的输入是次优臂和最优臂之间的差距。我们还证明了一个匹配的硬度结果。其次，我们提出了一种固定截止时间设置的算法，其中给定时间截止时间，需要最大化找到最佳臂的概率。我们通过模拟实验证实了我们的理论见解，这些实验表明算法在各种问题实例上始终匹配或优于基线算法。",
        "领域": "强化学习, 资源分配, 分布式计算",
        "问题": "在资源有限的情况下，如何高效分配资源以优化多臂老虎机探索的性能",
        "动机": "研究在分布式计算资源分配中，如何平衡资源分配以克服非线性扩展带来的吞吐量下降问题，从而更高效地进行多臂老虎机探索",
        "方法": "提出两种算法：一种在固定置信度设置下，通过动态程序权衡信息积累和吞吐量；另一种在固定截止时间设置下，最大化找到最佳臂的概率",
        "关键词": [
            "多臂老虎机",
            "资源分配",
            "自适应并行性",
            "分布式计算",
            "非线性扩展"
        ],
        "涉及的技术概念": {
            "多臂老虎机": "一种决策理论模型，用于在多个选择（臂）之间分配有限资源以最大化总奖励",
            "动态程序": "用于解决优化问题的数学方法，通过将问题分解为更简单的子问题来找到最优解",
            "非线性扩展": "指在增加资源时，系统性能提升的速度不与资源增加量成线性关系的现象"
        },
        "success": true
    },
    {
        "order": 926,
        "title": "Rethinking Neural vs. Matrix-Factorization Collaborative Filtering: the Theoretical Perspectives",
        "html": "https://ICML.cc//virtual/2021/poster/8747",
        "abstract": "The recent work by Rendle et al. (2020), based on empirical observations, argues that matrix-factorization collaborative filtering (MCF) compares favorably to neural collaborative filtering (NCF), and conjectures the dot product's superiority over the feed-forward neural network as similarity function. In this paper, we address the comparison rigorously by answering the following questions: 1. what is the limiting expressivity of each model; 2. under the practical gradient descent, to which solution does each optimization path converge; 3. how would the models generalize under the inductive and transductive learning setting. Our results highlight the similar expressivity for the overparameterized NCF and MCF as kernelized predictors, and reveal the relation between their optimization paths. We further show their different generalization behaviors, where MCF and NCF experience specific tradeoff and comparison in the transductive and inductive collaborative filtering setting. Lastly, by showing a novel generalization result, we reveal the critical role of correcting exposure bias for model evaluation in the inductive setting. Our results explain some of the previously observed conflicts, and we provide synthetic and real-data experiments to shed further insights to this topic.",
        "conference": "ICML",
        "中文标题": "重新思考神经协同过滤与矩阵分解协同过滤：理论视角",
        "摘要翻译": "Rendle等人（2020）的最新工作基于实证观察，认为矩阵分解协同过滤（MCF）优于神经协同过滤（NCF），并推测点积作为相似性函数优于前馈神经网络。在本文中，我们通过回答以下问题来严格比较这两种方法：1. 每种模型的极限表达能力是什么；2. 在实际梯度下降下，每种优化路径会收敛到哪个解；3. 在归纳和传导学习设置下，模型将如何泛化。我们的结果强调了过参数化的NCF和MCF作为核化预测器的相似表达能力，并揭示了它们优化路径之间的关系。我们进一步展示了它们不同的泛化行为，其中MCF和NCF在传导和归纳协同过滤设置中经历了特定的权衡和比较。最后，通过展示一种新颖的泛化结果，我们揭示了在归纳设置中纠正曝光偏差对模型评估的关键作用。我们的结果解释了之前观察到的一些冲突，并提供了合成和真实数据实验，以进一步深入探讨这一主题。",
        "领域": "推荐系统、协同过滤、深度学习",
        "问题": "比较神经协同过滤（NCF）和矩阵分解协同过滤（MCF）的理论性能和实际表现",
        "动机": "解决关于NCF和MCF哪种方法更优的争议，提供理论分析和实证支持",
        "方法": "通过理论分析模型表达能力、优化路径和泛化行为，结合合成和真实数据实验",
        "关键词": [
            "神经协同过滤",
            "矩阵分解",
            "推荐系统",
            "泛化能力",
            "优化路径"
        ],
        "涉及的技术概念": {
            "核化预测器": "用于分析NCF和MCF在过参数化情况下的相似表达能力",
            "梯度下降优化路径": "研究在实际梯度下降下，NCF和MCF优化路径的收敛行为",
            "曝光偏差纠正": "在归纳学习设置中，纠正曝光偏差对模型评估结果的影响"
        },
        "success": true
    },
    {
        "order": 927,
        "title": "Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss",
        "html": "https://ICML.cc//virtual/2021/poster/9045",
        "abstract": "Boundary discontinuity and its inconsistency to the final detection metric have been the bottleneck for rotating detection regression loss design. In this paper, we propose a novel regression loss based on Gaussian Wasserstein distance as a fundamental approach to solve the problem. Specifically, the rotated bounding box is converted to a 2-D Gaussian distribution, which enables to approximate the indifferentiable rotational IoU induced loss by the Gaussian Wasserstein distance (GWD) which can be learned efficiently by gradient back-propagation. GWD can still be informative for learning even there is no overlapping between two rotating bounding boxes which is often the case for small object detection. Thanks to its three unique properties, GWD can also elegantly solve the boundary discontinuity and square-like problem regardless how the bounding box is defined. Experiments on five datasets using different detectors show the effectiveness of our approach, and codes are available at https://github.com/yangxue0827/RotationDetection.",
        "conference": "ICML",
        "中文标题": "重新思考旋转目标检测：基于高斯Wasserstein距离损失的方法",
        "摘要翻译": "边界不连续性及其与最终检测指标的不一致性一直是旋转检测回归损失设计的瓶颈。本文中，我们提出了一种基于高斯Wasserstein距离的新型回归损失，作为解决这一问题的基本方法。具体而言，旋转边界框被转换为二维高斯分布，这使得通过高斯Wasserstein距离（GWD）可以近似不可微的旋转IoU诱导损失，该距离可以通过梯度反向传播高效学习。即使两个旋转边界框之间没有重叠（这在小型目标检测中常见），GWD仍然可以为学习提供信息。得益于其三个独特属性，GWD还可以优雅地解决边界不连续性和类似方形的问题，无论边界框如何定义。在五个数据集上使用不同检测器的实验证明了我们方法的有效性，代码可在https://github.com/yangxue0827/RotationDetection获取。",
        "领域": "旋转目标检测、小目标检测、深度学习",
        "问题": "解决旋转目标检测中边界不连续性和回归损失设计不一致的问题",
        "动机": "旋转目标检测中的边界不连续性和回归损失设计的不一致性限制了检测性能的提升",
        "方法": "提出基于高斯Wasserstein距离的回归损失，将旋转边界框转换为二维高斯分布，通过GWD近似不可微的旋转IoU损失",
        "关键词": [
            "旋转目标检测",
            "高斯Wasserstein距离",
            "回归损失",
            "边界不连续性",
            "小目标检测"
        ],
        "涉及的技术概念": {
            "高斯Wasserstein距离": "用于近似旋转IoU损失，解决边界不连续性和回归损失设计不一致的问题",
            "旋转边界框": "通过转换为二维高斯分布，使得损失函数能够更有效地学习",
            "梯度反向传播": "用于高效学习高斯Wasserstein距离，优化模型训练过程"
        },
        "success": true
    },
    {
        "order": 928,
        "title": "Re-understanding Finite-State Representations of Recurrent Policy Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8627",
        "abstract": "We introduce an approach for understanding control policies represented as recurrent neural networks. Recent work has approached this problem by transforming such recurrent policy networks into finite-state machines (FSM) and then analyzing the equivalent minimized FSM. While this led to interesting insights, the minimization process can obscure a deeper understanding of a machine's operation by merging states that are semantically distinct. To address this issue, we introduce an analysis approach that starts with an unminimized FSM and applies more-interpretable reductions that preserve the key decision points of the policy. We also contribute an attention tool to attain a deeper understanding of the role of observations in the decisions. Our case studies on 7 Atari games and 3 control benchmarks demonstrate that the approach can reveal insights that have not been previously noticed.",
        "conference": "ICML",
        "中文标题": "重新理解循环策略网络的有限状态表示",
        "摘要翻译": "我们提出了一种方法来理解以循环神经网络表示的控制策略。最近的研究通过将这类循环策略网络转换为有限状态机（FSM），然后分析等效的最小化FSM来探讨这一问题。虽然这带来了一些有趣的见解，但最小化过程可能会通过合并语义上不同的状态而掩盖对机器操作的更深入理解。为了解决这个问题，我们引入了一种分析方法，该方法从非最小化的FSM开始，应用更易解释的缩减，保留策略的关键决策点。我们还贡献了一个注意力工具，以更深入地理解观察在决策中的作用。我们在7个Atari游戏和3个控制基准上的案例研究表明，该方法可以揭示以前未被注意到的见解。",
        "领域": "强化学习、循环神经网络、有限状态机",
        "问题": "如何更深入地理解循环神经网络表示的控制策略，避免最小化有限状态机时合并语义上不同的状态",
        "动机": "最小化有限状态机过程可能掩盖对策略操作的深入理解，需要一种新方法来保留关键决策点并提高解释性",
        "方法": "从非最小化的有限状态机开始，应用更易解释的缩减方法，并开发注意力工具以分析观察在决策中的作用",
        "关键词": [
            "循环神经网络",
            "有限状态机",
            "控制策略",
            "强化学习",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "有限状态机（FSM）": "用于表示和简化循环策略网络的模型，通过状态和转换来描述策略行为",
            "循环神经网络": "用于表示控制策略的深度学习模型，能够处理序列数据并保持内部状态",
            "注意力机制": "用于分析和理解观察在策略决策中的作用，帮助揭示策略的关键决策点"
        },
        "success": true
    },
    {
        "order": 929,
        "title": "Revealing the Structure of Deep Neural Networks via Convex Duality",
        "html": "https://ICML.cc//virtual/2021/poster/10033",
        "abstract": "We study regularized deep neural networks (DNNs) and introduce a convex analytic framework to characterize the structure of the hidden layers. We show that a set of optimal hidden layer weights for a norm regularized DNN training problem can be explicitly found as the extreme points of a convex set. For the special case of deep linear networks, we prove that each optimal weight matrix aligns with the previous layers via duality. More importantly, we apply the same characterization to deep ReLU networks with whitened data and prove the same weight alignment holds. As a corollary, we also prove that norm regularized deep ReLU networks yield spline interpolation for one-dimensional datasets which was previously known only for two-layer networks. Furthermore, we provide closed-form solutions for the optimal layer weights when data is rank-one or whitened. The same analysis also applies to architectures with batch normalization even for arbitrary data. Therefore, we obtain a complete explanation for a recent empirical observation termed Neural Collapse where class means collapse to the vertices of a simplex equiangular tight frame.",
        "conference": "ICML",
        "中文标题": "通过凸对偶揭示深度神经网络的结构",
        "摘要翻译": "我们研究了正则化的深度神经网络（DNNs），并引入了一个凸分析框架来表征隐藏层的结构。我们表明，对于范数正则化的DNN训练问题，一组最优隐藏层权重可以明确地作为凸集的极值点找到。对于深度线性网络的特殊情况，我们证明了每个最优权重矩阵通过对偶性与前一层对齐。更重要的是，我们将相同的表征应用于具有白化数据的深度ReLU网络，并证明了相同的权重对齐成立。作为推论，我们还证明了范数正则化的深度ReLU网络为一维数据集产生样条插值，这之前仅对两层网络已知。此外，当数据是秩一或白化时，我们为最优层权重提供了闭式解。同样的分析也适用于具有批量归一化的架构，即使对于任意数据也是如此。因此，我们为最近一个被称为神经崩溃的经验观察提供了一个完整的解释，其中类均值崩溃到一个等角紧框架的单纯形的顶点。",
        "领域": "深度学习理论、神经网络优化、凸优化",
        "问题": "揭示深度神经网络隐藏层的结构及其优化过程",
        "动机": "为了更深入地理解深度神经网络的工作原理和优化过程，特别是在正则化和不同网络架构下的行为",
        "方法": "引入凸分析框架，分析正则化深度神经网络的最优权重结构，特别关注深度线性网络和ReLU网络，以及批量归一化架构",
        "关键词": [
            "深度神经网络",
            "凸对偶",
            "权重对齐",
            "正则化",
            "神经崩溃"
        ],
        "涉及的技术概念": {
            "凸对偶": "用于分析和揭示深度神经网络最优权重结构的数学工具",
            "权重对齐": "在特定条件下，网络中各层权重之间的优化关系",
            "神经崩溃": "描述在训练过程中，类均值趋向于等角紧框架单纯形顶点的现象"
        },
        "success": true
    },
    {
        "order": 930,
        "title": "Revenue-Incentive Tradeoffs in Dynamic Reserve Pricing",
        "html": "https://ICML.cc//virtual/2021/poster/10011",
        "abstract": "Online advertisements are primarily sold via repeated auctions with reserve prices. In this paper, we study how to set reserves to boost revenue based on the historical bids of strategic buyers, while controlling the impact of such a policy on the incentive compatibility of the repeated auctions. Adopting an incentive compatibility metric which quantifies the incentives to shade bids, we propose a novel class of reserve pricing policies and provide analytical tradeoffs between their revenue performance and bid-shading incentives. The policies are inspired by the exponential mechanism from the literature on differential privacy, but our study uncovers mechanisms with significantly better revenue-incentive tradeoffs than the exponential mechanism in practice. We further empirically evaluate the tradeoffs on synthetic data as well as real ad auction data from a major ad exchange to verify and support our theoretical findings.",
        "conference": "ICML",
        "中文标题": "动态保留价中的收入与激励权衡",
        "摘要翻译": "在线广告主要通过带有保留价的重复拍卖进行销售。在本文中，我们研究了如何根据战略性买家的历史出价设置保留价以提高收入，同时控制这种政策对重复拍卖激励兼容性的影响。采用一个量化降低出价激励的激励兼容性指标，我们提出了一类新颖的保留价定价策略，并提供了它们的收入表现与降低出价激励之间的分析权衡。这些策略受到差分隐私文献中指数机制的启发，但我们的研究揭示了在实践中比指数机制具有显著更好收入-激励权衡的机制。我们进一步在合成数据以及来自主要广告交易平台的真实广告拍卖数据上实证评估了这些权衡，以验证和支持我们的理论发现。",
        "领域": "在线广告拍卖、机制设计、收入管理",
        "问题": "如何在在线广告的重复拍卖中设置保留价以增加收入，同时最小化对拍卖激励兼容性的负面影响。",
        "动机": "研究旨在解决在线广告拍卖中保留价设置对收入与买家出价激励之间权衡的问题，以提高拍卖效率和收入。",
        "方法": "提出了一类基于历史出价的保留价定价策略，利用激励兼容性指标量化出价激励，并通过理论和实证分析评估收入与激励之间的权衡。",
        "关键词": [
            "保留价定价",
            "激励兼容性",
            "在线广告拍卖",
            "收入管理",
            "机制设计"
        ],
        "涉及的技术概念": {
            "激励兼容性": "用于量化买家在重复拍卖中降低出价的激励，是评估保留价策略对拍卖行为影响的关键指标。",
            "保留价定价策略": "一类新颖的策略，旨在通过分析历史出价数据来设置保留价，以优化拍卖收入。",
            "差分隐私的指数机制": "研究中的策略受到这一机制的启发，但提出了在实践中表现更优的机制，以更好地平衡收入与激励。"
        },
        "success": true
    },
    {
        "order": 931,
        "title": "Revisiting Peng's Q($\\lambda$) for Modern Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8453",
        "abstract": "Off-policy multi-step reinforcement learning algorithms consist of conservative and non-conservative algorithms: the former actively cut traces, whereas the latter do not. Recently, Munos et al. (2016) proved the convergence of conservative algorithms to an optimal Q-function. In contrast, non-conservative algorithms are thought to be unsafe and have a limited or no theoretical guarantee. Nonetheless, recent studies have shown that non-conservative algorithms empirically outperform conservative ones. Motivated by the empirical results and the lack of theory, we carry out theoretical analyses of Peng's Q($\\lambda$), a representative example of non-conservative algorithms. We prove that \\emph{it also converges to an optimal policy} provided that the behavior policy slowly tracks a greedy policy in a way similar to conservative policy iteration. Such a result has been conjectured to be true but has not been proven. We also experiment with Peng's Q($\\lambda$) in complex continuous control tasks, confirming that Peng's Q($\\lambda$) often outperforms conservative algorithms despite its simplicity. These results indicate that Peng's Q($\\lambda$), which was thought to be unsafe, is a theoretically-sound and practically effective algorithm.",
        "conference": "ICML",
        "中文标题": "重新审视彭的Q(λ)在现代强化学习中的应用",
        "摘要翻译": "离策略多步强化学习算法包括保守和非保守算法：前者主动切断轨迹，而后者不这样做。最近，Munos等人（2016年）证明了保守算法收敛于最优Q函数。相比之下，非保守算法被认为是不安全的，并且具有有限或没有理论保证。尽管如此，最近的研究表明，非保守算法在经验上优于保守算法。受到这些经验结果和理论缺乏的启发，我们对彭的Q(λ)进行了理论分析，这是非保守算法的一个代表性例子。我们证明，只要行为策略以类似于保守策略迭代的方式缓慢跟踪贪婪策略，它也收敛于最优策略。这样的结果曾被推测为真，但尚未得到证明。我们还在复杂的连续控制任务中实验了彭的Q(λ)，确认尽管其简单，彭的Q(λ)往往优于保守算法。这些结果表明，被认为不安全的彭的Q(λ)是一个理论上合理且实际有效的算法。",
        "领域": "强化学习、连续控制、算法收敛性分析",
        "问题": "非保守的离策略多步强化学习算法的理论保证不足及其在实际应用中的表现优于保守算法的问题",
        "动机": "探索非保守算法如彭的Q(λ)的理论基础，以解释其在实践中优于保守算法的现象",
        "方法": "对彭的Q(λ)进行理论分析，并在复杂的连续控制任务中进行实验验证",
        "关键词": [
            "强化学习",
            "离策略学习",
            "Q(λ)",
            "算法收敛性",
            "连续控制"
        ],
        "涉及的技术概念": {
            "离策略多步强化学习": "一种强化学习方法，允许学习策略与行为策略不同，并通过多步更新来加速学习",
            "保守与非保守算法": "保守算法通过主动切断轨迹来保证安全性，而非保守算法不这样做，可能在理论上缺乏保证但在实践中表现更好",
            "Q(λ)算法": "一种结合了多步更新和离策略学习的强化学习算法，彭的Q(λ)是其非保守实现的一个例子"
        },
        "success": true
    },
    {
        "order": 932,
        "title": "Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline",
        "html": "https://ICML.cc//virtual/2021/poster/9099",
        "abstract": "Processing point cloud data is an important component of many real-world systems. As such, a wide variety of point-based approaches have been proposed, reporting steady benchmark improvements over time. We study the key ingredients of this progress and uncover two critical results. First, we find that auxiliary factors like different evaluation schemes, data augmentation strategies, and loss functions, which are independent of the model architecture, make a large difference in performance. The differences are large enough that they obscure the effect of architecture. When these factors are controlled for, PointNet++, a relatively older network, performs competitively with recent methods. Second, a very simple projection-based method, which we refer to as SimpleView, performs surprisingly well. It achieves on par or better results than sophisticated state-of-the-art methods on ModelNet40 while being half the size of PointNet++. It also outperforms state-of-the-art methods on ScanObjectNN, a real-world point cloud benchmark, and demonstrates better cross-dataset generalization. Code is available at https://github.com/princeton-vl/SimpleView.",
        "conference": "ICML",
        "中文标题": "重新审视点云形状分类：一个简单而有效的基线方法",
        "摘要翻译": "处理点云数据是许多现实世界系统的重要组成部分。因此，已经提出了多种基于点的方法，报告了随着时间的推移基准测试的稳步改进。我们研究了这一进展的关键因素，并发现了两个关键结果。首先，我们发现辅助因素，如不同的评估方案、数据增强策略和损失函数，这些与模型架构无关的因素，在性能上产生了很大的差异。这些差异大到足以掩盖架构的影响。当这些因素被控制时，PointNet++，一个相对较旧的网络，与最近的方法相比表现具有竞争力。其次，一个非常简单的基于投影的方法，我们称之为SimpleView，表现出了令人惊讶的好。它在ModelNet40上达到了与复杂的最先进方法相当或更好的结果，同时体积只有PointNet++的一半。它还在ScanObjectNN（一个真实世界的点云基准测试）上优于最先进的方法，并展示了更好的跨数据集泛化能力。代码可在https://github.com/princeton-vl/SimpleView获取。",
        "领域": "点云分类、三维物体识别、深度学习模型优化",
        "问题": "点云形状分类中模型架构与辅助因素对性能的影响，以及寻找更简单有效的分类方法",
        "动机": "探索点云形状分类中性能提升的关键因素，并验证简单方法是否能达到或超越复杂方法的性能",
        "方法": "通过控制实验分析辅助因素对性能的影响，并提出一个简单的基于投影的点云分类方法SimpleView",
        "关键词": [
            "点云分类",
            "模型优化",
            "SimpleView",
            "性能评估",
            "跨数据集泛化"
        ],
        "涉及的技术概念": {
            "点云分类": "论文研究的核心任务，旨在对三维点云数据进行形状分类",
            "辅助因素": "包括评估方案、数据增强策略和损失函数等，研究发现这些因素对性能有显著影响",
            "SimpleView": "论文提出的简单投影方法，通过简化处理流程达到或超越复杂方法的性能"
        },
        "success": true
    },
    {
        "order": 933,
        "title": "Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research",
        "html": "https://ICML.cc//virtual/2021/poster/9069",
        "abstract": "Since the introduction of DQN, a vast majority of reinforcement learning research has focused on reinforcement learning with deep neural networks as function approximators. New methods are typically evaluated on a set of environments that have now become standard, such as Atari 2600 games. While these benchmarks help standardize evaluation, their computational cost has the unfortunate side effect of widening the gap between those with ample access to computational resources, and those without. In this work we argue that, despite the community’s emphasis on large-scale environments, the traditional small-scale environments can still yield valuable scientific insights and can help reduce the barriers to entry for underprivileged communities. To substantiate our claims, we empirically revisit the paper which introduced the Rainbow algorithm [Hessel et al., 2018] and present some new insights into the algorithms used by Rainbow.",
        "conference": "ICML",
        "中文标题": "重温彩虹算法：促进更深入和包容的深度强化学习研究",
        "摘要翻译": "自DQN引入以来，绝大多数强化学习研究都集中在使用深度神经网络作为函数逼近器的强化学习上。新方法通常在一组现已标准化的环境中进行评估，如Atari 2600游戏。虽然这些基准有助于标准化评估，但它们的计算成本不幸地扩大了那些拥有充足计算资源的人与那些没有的人之间的差距。在这项工作中，我们认为，尽管社区强调大规模环境，传统的较小规模环境仍然可以产生有价值的科学见解，并有助于减少弱势群体的入门障碍。为了证实我们的观点，我们实证地回顾了引入彩虹算法的论文[Hessel et al., 2018]，并对彩虹算法使用的一些算法提出了新的见解。",
        "领域": "深度强化学习、算法评估、计算资源优化",
        "问题": "解决深度强化学习研究中因计算资源不平等导致的科学见解获取和社区参与障碍问题",
        "动机": "通过重新评估传统小规模环境的价值，促进更公平和深入的科学研究参与",
        "方法": "实证回顾彩虹算法，并对其使用的算法提出新的见解",
        "关键词": [
            "深度强化学习",
            "彩虹算法",
            "计算资源",
            "科学见解",
            "包容性研究"
        ],
        "涉及的技术概念": {
            "深度强化学习": "结合深度学习和强化学习的技术，用于解决复杂的决策问题",
            "彩虹算法": "一种结合了多种深度Q网络改进技术的算法，用于提高强化学习的性能和稳定性",
            "计算资源优化": "通过算法或方法减少对高性能计算资源的依赖，使研究更加普及和可及"
        },
        "success": true
    },
    {
        "order": 934,
        "title": "Reward Identification in Inverse Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10341",
        "abstract": "We study the problem of reward identifiability in the context of Inverse Reinforcement Learning (IRL). The reward identifiability question is critical to answer when reasoning about the effectiveness of using Markov Decision Processes (MDPs) as computational models of real world decision makers in order to understand complex decision making behavior and perform counterfactual reasoning. While identifiability has been acknowledged as a fundamental theoretical question in IRL, little is known about the types of MDPs for which rewards are identifiable, or even if there exist such MDPs. In this work, we formalize the reward identification problem in IRL and study how identifiability relates to properties of the MDP model. For deterministic MDP models with the MaxEntRL objective, we prove necessary and sufficient conditions for identifiability. Building on these results, we present efficient algorithms for testing whether or not an MDP model is identifiable.",
        "conference": "ICML",
        "中文标题": "逆强化学习中的奖励识别",
        "摘要翻译": "我们研究了逆强化学习（IRL）背景下的奖励可识别性问题。奖励可识别性问题在考虑使用马尔可夫决策过程（MDPs）作为现实世界决策者的计算模型，以理解复杂决策行为并进行反事实推理时，是至关重要的。尽管可识别性已被认为是IRL中的一个基本理论问题，但对于哪些类型的MDPs其奖励是可识别的，或者是否存在这样的MDPs，知之甚少。在这项工作中，我们形式化了IRL中的奖励识别问题，并研究了可识别性与MDP模型属性之间的关系。对于具有MaxEntRL目标的确定性MDP模型，我们证明了可识别性的必要和充分条件。基于这些结果，我们提出了用于测试MDP模型是否可识别的高效算法。",
        "领域": "逆强化学习、马尔可夫决策过程、机器学习理论",
        "问题": "研究在逆强化学习中奖励的可识别性问题，特别是在马尔可夫决策过程模型中。",
        "动机": "理解复杂决策行为并进行反事实推理，需要明确在何种条件下奖励是可识别的。",
        "方法": "形式化奖励识别问题，研究可识别性与MDP模型属性的关系，提出测试MDP模型可识别性的算法。",
        "关键词": [
            "逆强化学习",
            "奖励识别",
            "马尔可夫决策过程",
            "可识别性",
            "MaxEntRL"
        ],
        "涉及的技术概念": {
            "逆强化学习": "一种从观察到的行为中推断出潜在奖励函数的方法。",
            "马尔可夫决策过程": "用于建模决策者在不确定环境中做出决策的数学框架。",
            "MaxEntRL": "最大熵强化学习，一种鼓励策略多样性的强化学习方法。"
        },
        "success": true
    },
    {
        "order": 935,
        "title": "Riemannian Convex Potential Maps",
        "html": "https://ICML.cc//virtual/2021/poster/9033",
        "abstract": "Modeling distributions on Riemannian manifolds is a crucial component in understanding non-Euclidean data that arises, e.g., in physics and geology. The budding approaches in this space are limited by representational and computational tradeoffs. We propose and study a class of flows that uses convex potentials from Riemannian optimal transport. These  are universal and can model distributions on any compact Riemannian manifold without requiring domain knowledge of the manifold to be integrated into the architecture. We demonstrate that these flows can model standard distributions on spheres, and tori, on synthetic and geological data.",
        "conference": "ICML",
        "中文标题": "黎曼凸势映射",
        "摘要翻译": "在黎曼流形上建模分布是理解非欧几里得数据（如物理学和地质学中出现的数据）的关键组成部分。这一领域的新兴方法受到表示和计算之间权衡的限制。我们提出并研究了一类利用黎曼最优传输中的凸势的流。这些流是通用的，可以在任何紧凑的黎曼流形上建模分布，而无需将流形的领域知识集成到架构中。我们证明这些流可以在球体和环面上的合成数据和地质数据上建模标准分布。",
        "领域": "非欧几里得数据分析、黎曼流形学习、最优传输",
        "问题": "如何在不需要将流形的领域知识集成到架构中的情况下，在紧凑的黎曼流形上建模分布。",
        "动机": "解决现有方法在表示和计算之间的权衡限制，提供一种通用的方法来建模非欧几里得数据分布。",
        "方法": "利用黎曼最优传输中的凸势，提出一类通用的流，能够在任何紧凑的黎曼流形上建模分布。",
        "关键词": [
            "黎曼流形",
            "最优传输",
            "凸势",
            "非欧几里得数据",
            "分布建模"
        ],
        "涉及的技术概念": {
            "黎曼最优传输": "用于在黎曼流形上定义和计算最优传输路径的技术，支持在非欧几里得空间中进行有效的数据分布建模。",
            "凸势": "在最优传输问题中使用的凸函数，用于定义和优化传输计划，使得在黎曼流形上的分布建模成为可能。",
            "紧凑黎曼流形": "一类具有有限体积和良好性质的黎曼流形，为在其上建模分布提供了数学基础。"
        },
        "success": true
    },
    {
        "order": 936,
        "title": "Risk Bounds and Rademacher Complexity in Batch Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10623",
        "abstract": "This paper considers batch Reinforcement Learning (RL) with general value function approximation. Our study investigates the minimal assumptions to reliably estimate/minimize Bellman error, and characterizes the generalization performance by (local) Rademacher complexities of general function classes, which makes initial steps in bridging the gap between statistical learning theory and batch RL. Concretely, we view the Bellman error as a surrogate loss for the optimality gap, and prove the followings: (1) In double sampling regime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the Rademacher complexity of the function class. (2) In the single sampling regime, sample-efficient risk minimization is not possible without further assumptions, regardless of algorithms. However, with completeness assumptions, the excess risk of FQI and a minimax style algorithm can be again bounded by the Rademacher complexity of the corresponding function classes.  (3) Fast statistical rates can be achieved by using tools of local Rademacher complexity. Our analysis covers a wide range of function classes, including finite classes, linear spaces, kernel spaces, sparse linear features, etc.",
        "conference": "ICML",
        "中文标题": "批量强化学习中的风险边界与Rademacher复杂性",
        "摘要翻译": "本文探讨了具有一般价值函数近似的批量强化学习（RL）。我们的研究调查了可靠估计/最小化Bellman误差的最小假设，并通过一般函数类的（局部）Rademacher复杂性来表征泛化性能，这初步搭建了统计学习理论与批量RL之间的桥梁。具体而言，我们将Bellman误差视为最优性差距的代理损失，并证明了以下几点：（1）在双采样机制下，经验风险最小化器（ERM）的过剩风险受限于函数类的Rademacher复杂性。（2）在单采样机制下，如果没有进一步的假设，无论采用何种算法，都无法实现样本高效的风险最小化。然而，在完整性假设下，FQI和一种极小极大风格算法的过剩风险可以再次受到相应函数类的Rademacher复杂性的限制。（3）通过使用局部Rademacher复杂性的工具，可以实现快速的统计速率。我们的分析涵盖了广泛的函数类，包括有限类、线性空间、核空间、稀疏线性特征等。",
        "领域": "强化学习理论、统计学习理论、函数近似",
        "问题": "如何在批量强化学习中可靠地估计和最小化Bellman误差，并理解其泛化性能。",
        "动机": "为了填补统计学习理论与批量强化学习之间的理论空白，提供对批量强化学习算法泛化性能的理论保证。",
        "方法": "通过将Bellman误差视为最优性差距的代理损失，利用Rademacher复杂性分析不同采样机制下的风险边界，并探讨在完整性假设下的算法性能。",
        "关键词": [
            "批量强化学习",
            "Bellman误差",
            "Rademacher复杂性",
            "统计学习理论",
            "函数近似"
        ],
        "涉及的技术概念": {
            "Bellman误差": "在强化学习中用于衡量当前价值函数与最优价值函数之间的差距，作为优化目标的代理损失。",
            "Rademacher复杂性": "用于衡量函数类的复杂性，进而分析学习算法的泛化性能。",
            "局部Rademacher复杂性": "Rademacher复杂性的一种变体，能够提供更精细的统计速率分析，特别是在处理具有特定结构的函数类时。"
        },
        "success": true
    },
    {
        "order": 937,
        "title": "Risk-Sensitive Reinforcement Learning with Function Approximation: A Debiasing Approach",
        "html": "https://ICML.cc//virtual/2021/poster/9329",
        "abstract": "We study function approximation for episodic reinforcement learning with entropic risk measure. We first propose an algorithm with linear function approximation. Compared to existing algorithms, which suffer from improper regularization and regression biases, this algorithm features debiasing transformations in backward induction and regression procedures. We further propose an algorithm with general function approximation, which features implicit debiasing transformations. We prove that both algorithms achieve a sublinear regret and demonstrate a trade-off between generality and efficiency. Our analysis provides a unified framework for function approximation in risk-sensitive reinforcement learning, which leads to the first sublinear regret bounds in the setting.",
        "conference": "ICML",
        "中文标题": "基于函数近似的风险敏感强化学习：一种去偏方法",
        "摘要翻译": "我们研究了带有熵风险度量的阶段性强化学习的函数近似问题。首先，我们提出了一种采用线性函数近似的算法。与现有算法相比，这些算法因不恰当的正则化和回归偏差而受到影响，而我们的算法在反向归纳和回归过程中采用了去偏变换。进一步，我们提出了一种采用一般函数近似的算法，该算法具有隐式去偏变换的特点。我们证明了这两种算法都能实现次线性遗憾，并展示了通用性与效率之间的权衡。我们的分析为风险敏感强化学习中的函数近似提供了一个统一的框架，从而在该设置下首次实现了次线性遗憾界限。",
        "领域": "强化学习、函数近似、风险敏感学习",
        "问题": "解决在风险敏感强化学习中，现有算法因不恰当的正则化和回归偏差而导致的性能问题",
        "动机": "为了在风险敏感强化学习中实现更高效的函数近似，减少偏差，提高算法的通用性和效率",
        "方法": "提出两种算法，一种采用线性函数近似并引入去偏变换，另一种采用一般函数近似并实现隐式去偏变换",
        "关键词": [
            "风险敏感强化学习",
            "函数近似",
            "去偏方法",
            "次线性遗憾",
            "熵风险度量"
        ],
        "涉及的技术概念": {
            "熵风险度量": "用于量化强化学习中的风险偏好，影响决策过程中的风险态度",
            "去偏变换": "在算法中应用的技术，旨在减少回归和反向归纳过程中的偏差，提高学习效率",
            "次线性遗憾": "算法性能的度量标准，表示随着时间推移，算法的累积遗憾增长速度低于线性"
        },
        "success": true
    },
    {
        "order": 938,
        "title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length",
        "html": "https://ICML.cc//virtual/2021/poster/10437",
        "abstract": "We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.",
        "conference": "ICML",
        "中文标题": "Rissanen数据分析：通过描述长度检验数据集特性",
        "摘要翻译": "我们引入了一种方法，用于确定某种能力是否有助于实现对给定数据的准确建模。我们将标签视为由具有不同能力的子程序组成的程序从输入生成，并提出一个子程序当且仅当调用它的最小程序比不调用它的程序短时才是有用的。由于最小程序长度是不可计算的，我们转而估计标签的最小描述长度（MDL）作为替代，为我们提供了一种理论基础的方法来分析数据集特性。我们称这种方法为Rissanen数据分析（RDA），以MDL之父命名，并在自然语言处理（NLP）的广泛设置中展示了其适用性，从评估在回答问题之前生成子问题的效用，到分析理由和解释的价值，再到调查不同词类的重要性，以及揭示数据集的性别偏见。",
        "领域": "自然语言处理与视觉结合",
        "问题": "如何确定特定能力对数据建模准确性的贡献",
        "动机": "开发一种理论基础的方法来分析数据集特性，评估不同能力对数据建模的影响",
        "方法": "通过估计标签的最小描述长度（MDL）作为最小程序长度的替代，分析子程序对数据建模的效用",
        "关键词": [
            "最小描述长度",
            "数据集分析",
            "自然语言处理",
            "数据建模",
            "性别偏见"
        ],
        "涉及的技术概念": {
            "最小描述长度（MDL）": "作为最小程序长度的替代，用于理论基础地分析数据集特性",
            "Rissanen数据分析（RDA）": "以MDL为基础的方法，用于评估不同能力对数据建模的影响",
            "自然语言处理（NLP）": "应用领域，展示了RDA方法在评估子问题生成、理由分析等方面的适用性"
        },
        "success": true
    },
    {
        "order": 939,
        "title": "RNNRepair: Automatic RNN Repair via Model-based Analysis",
        "html": "https://ICML.cc//virtual/2021/poster/8765",
        "abstract": "Deep neural networks are vulnerable to adversarial attacks. Due to their black-box nature, it is rather challenging to interpret and properly repair these incorrect behaviors. This paper focuses on interpreting and repairing the incorrect behaviors of Recurrent Neural Networks (RNNs). We propose a lightweight model-based approach (RNNRepair) to help understand and repair incorrect behaviors of an RNN. Specifically, we build an influence model to characterize the stateful and statistical behaviors of an RNN over all the training data and to perform the influence analysis for the errors. Compared with the existing techniques on influence function, our method can efficiently estimate the influence of existing or newly added training samples for a given prediction at both sample level and segmentation level. Our empirical evaluation shows that the proposed influence model is able to extract accurate and understandable features. Based on the influence model, our proposed technique could effectively infer the influential instances from not only an entire testing sequence but also a segment within that sequence. Moreover, with the sample-level and segment-level influence relations, RNNRepair could further remediate two types of incorrect predictions at the sample level and segment level. ",
        "conference": "ICML",
        "中文标题": "RNNRepair：基于模型分析的自动RNN修复",
        "摘要翻译": "深度神经网络容易受到对抗性攻击。由于其黑盒特性，解释和适当修复这些不正确行为相当具有挑战性。本文专注于解释和修复循环神经网络（RNNs）的不正确行为。我们提出了一种轻量级的基于模型的方法（RNNRepair），以帮助理解和修复RNN的不正确行为。具体来说，我们构建了一个影响模型来表征RNN在所有训练数据上的状态和统计行为，并对错误进行影响分析。与现有关于影响函数的技术相比，我们的方法可以高效地估计现有或新增训练样本在样本级别和分段级别上对给定预测的影响。我们的实证评估表明，所提出的影响模型能够提取准确且可理解的特征。基于影响模型，我们提出的技术可以有效地从整个测试序列以及该序列中的一个段推断出有影响的实例。此外，利用样本级别和段级别的影响关系，RNNRepair可以进一步修复样本级别和段级别上的两种类型的不正确预测。",
        "领域": "循环神经网络、对抗性攻击防御、模型修复",
        "问题": "修复循环神经网络（RNNs）在面对对抗性攻击时的不正确行为",
        "动机": "由于深度神经网络的黑盒特性，解释和修复其不正确行为具有挑战性，特别是在循环神经网络（RNNs）中",
        "方法": "提出了一种轻量级的基于模型的方法（RNNRepair），通过构建影响模型来表征RNN的行为并进行影响分析，以修复不正确行为",
        "关键词": [
            "循环神经网络",
            "对抗性攻击",
            "模型修复",
            "影响分析",
            "行为修复"
        ],
        "涉及的技术概念": {
            "影响模型": "用于表征RNN在所有训练数据上的状态和统计行为，并对错误进行影响分析",
            "样本级别影响": "估计单个训练样本对模型预测的影响",
            "段级别影响": "估计训练数据中的一段对模型预测的影响"
        },
        "success": true
    },
    {
        "order": 940,
        "title": "RNN with Particle Flow for Probabilistic Spatio-temporal Forecasting",
        "html": "https://ICML.cc//virtual/2021/poster/9195",
        "abstract": "Spatio-temporal forecasting has numerous applications in analyzing wireless, traffic, and financial networks. Many classical statistical models often fall short in handling the complexity and high non-linearity present in time-series data. Recent\nadvances in deep learning allow for better modelling of spatial and temporal dependencies. While most of these models focus on obtaining accurate point forecasts, they do not characterize the prediction uncertainty. In this work, we consider the time-series data as a random realization from a nonlinear state-space model and target Bayesian inference of the hidden states for probabilistic forecasting. We use particle flow as the tool for approximating the posterior distribution of the states, as it is shown to be highly effective in complex, high-dimensional settings. Thorough experimentation on several real world time-series datasets demonstrates that our approach provides better characterization of uncertainty while maintaining comparable accuracy to the state-of-the-art point forecasting methods.",
        "conference": "ICML",
        "中文标题": "基于粒子流的循环神经网络在概率时空预测中的应用",
        "摘要翻译": "时空预测在无线网络、交通网络和金融网络分析中有着广泛的应用。许多经典统计模型在处理时间序列数据中的复杂性和高度非线性时往往力不从心。深度学习的最新进展使得对空间和时间依赖性的建模更加精确。虽然这些模型大多专注于获得准确的点预测，但它们并未能表征预测的不确定性。在这项工作中，我们将时间序列数据视为非线性状态空间模型的随机实现，并以贝叶斯推理为目标，对隐藏状态进行概率预测。我们使用粒子流作为近似状态后验分布的工具，因为它在复杂、高维设置中显示出极高的效率。在多个真实世界时间序列数据集上的彻底实验表明，我们的方法在保持与最先进点预测方法相媲美的准确性的同时，提供了更好的不确定性表征。",
        "领域": "时间序列预测, 贝叶斯深度学习, 概率建模",
        "问题": "解决时间序列预测中不确定性表征不足的问题",
        "动机": "为了在复杂和高非线性的时间序列数据中更有效地进行概率预测和不确定性表征",
        "方法": "结合循环神经网络和粒子流技术，进行贝叶斯推理以近似状态的后验分布",
        "关键词": [
            "概率预测",
            "粒子流",
            "贝叶斯推理",
            "循环神经网络",
            "时空数据"
        ],
        "涉及的技术概念": {
            "粒子流": "用于在复杂、高维设置中高效近似状态后验分布的技术",
            "贝叶斯推理": "用于对隐藏状态进行概率预测的统计方法",
            "循环神经网络": "用于建模时间序列数据中的时间依赖性"
        },
        "success": true
    },
    {
        "order": 941,
        "title": "Robust Asymmetric Learning in POMDPs",
        "html": "https://ICML.cc//virtual/2021/poster/9327",
        "abstract": "Policies for partially observed Markov decision processes can be efficiently learned by imitating expert policies generated using asymmetric information.  Unfortunately, existing approaches for this kind of imitation learning have a serious flaw: the expert does not know what the trainee cannot see, and as a result may encourage actions that are sub-optimal or unsafe under partial information.  To address this issue, we derive an update which, when applied iteratively to an expert, maximizes the expected reward of the trainee's policy. Using this update, we construct a computationally efficient algorithm, adaptive asymmetric DAgger (A2D), that jointly trains the expert and trainee policies. We then show that A2D allows the trainee to safely imitate the modified expert, and outperforms policies learned either by imitating a fixed expert or through direct reinforcement learning.",
        "conference": "ICML",
        "中文标题": "POMDP中的鲁棒非对称学习",
        "摘要翻译": "通过模仿使用非对称信息生成的专家策略，可以有效地学习部分可观察马尔可夫决策过程的策略。不幸的是，现有的这种模仿学习方法存在一个严重缺陷：专家不知道学员无法看到的内容，因此可能会鼓励在部分信息下是次优或不安全的行动。为了解决这个问题，我们推导出一个更新规则，当迭代应用于专家时，可以最大化学员策略的预期奖励。利用这一更新规则，我们构建了一个计算效率高的算法——自适应非对称DAgger（A2D），该算法联合训练专家和学员策略。然后我们展示，A2D允许学员安全地模仿修改后的专家，并且优于通过模仿固定专家或直接强化学习学到的策略。",
        "领域": "强化学习、模仿学习、马尔可夫决策过程",
        "问题": "解决在部分可观察马尔可夫决策过程中，模仿学习因专家不了解学员的观察限制而导致的次优或不安全行动问题。",
        "动机": "改进现有的模仿学习方法，使其能够考虑到学员的观察限制，从而避免鼓励次优或不安全的行动。",
        "方法": "推导出一个迭代更新规则以优化学员策略的预期奖励，并开发自适应非对称DAgger（A2D）算法联合训练专家和学员策略。",
        "关键词": [
            "非对称学习",
            "模仿学习",
            "POMDP",
            "A2D算法",
            "强化学习"
        ],
        "涉及的技术概念": {
            "部分可观察马尔可夫决策过程（POMDP）": "用于建模决策过程，其中决策者只能部分观察到环境状态。",
            "模仿学习": "通过模仿专家行为来学习策略的方法。",
            "自适应非对称DAgger（A2D）": "一种联合训练专家和学员策略的算法，旨在优化学员在部分观察下的表现。"
        },
        "success": true
    },
    {
        "order": 942,
        "title": "Robust Density Estimation from Batches: The Best Things in Life are (Nearly) Free",
        "html": "https://ICML.cc//virtual/2021/poster/9439",
        "abstract": "In many applications data are collected in batches, some potentially biased, corrupt, or even adversarial. Learning algorithms for this setting have therefore garnered considerable recent attention. In particular, a sequence of works has shown that all approximately piecewise polynomial distributions---and in particular all Gaussian, Gaussian-mixture, log-concave, low-modal, and monotone-hazard distributions---can be learned robustly in polynomial time. However, these results left open the question, stated explicitly in~\\cite{chen2020learning}, about the best possible sample complexity of such algorithms. We answer this question, showing that, perhaps surprisingly, up to logarithmic factors, the optimal sample complexity is the same as for genuine, non-adversarial, data! To establish the result, we reduce robust learning of approximately piecewise polynomial distributions to robust learning of the probability of all subsets of size at most $k$ of a larger discrete domain, and learn these probabilities in optimal sample complexity linear in $k$ regardless of the domain size. In simulations, the algorithm runs very quickly and estimates distributions to essentially the accuracy achieved when all adversarial batches are removed. The results also imply the first polynomial-time sample-optimal algorithm for robust interval-based classification based on batched data.",
        "conference": "ICML",
        "success": true,
        "中文标题": "从批次中进行鲁棒密度估计：生活中最好的东西（几乎）是免费的",
        "摘要翻译": "在许多应用中，数据是以批次收集的，其中一些批次可能是有偏的、损坏的，甚至是敌对的。因此，针对这一设置的学习算法最近受到了相当大的关注。特别是，一系列工作已经表明，所有近似分段多项式分布——特别是所有高斯、高斯混合、对数凹、低模态和单调风险分布——都可以在多项式时间内被鲁棒地学习。然而，这些结果留下了一个问题，即在~\\\\cite{chen2020learning}中明确提出的，关于这类算法可能的最佳样本复杂度的问题。我们回答了这个问题，表明，或许令人惊讶的是，在对数因子范围内，最优样本复杂度与真实的、非敌对的数据相同！为了建立这一结果，我们将近似分段多项式分布的鲁棒学习问题简化为学习一个更大离散域中所有大小不超过$k$的子集的概率的鲁棒学习问题，并以与$k$线性相关的最优样本复杂度学习这些概率，而不考虑域的大小。在模拟中，该算法运行非常快，并且估计的分布基本上达到了当所有敌对批次被移除时所能达到的准确度。这些结果还意味着第一个基于批次数据的、基于区间的鲁棒分类的多项式时间样本最优算法。",
        "领域": "密度估计, 鲁棒学习, 对抗性学习",
        "问题": "如何在存在潜在偏置、损坏或敌对批次数据的情况下，进行鲁棒的密度估计。",
        "动机": "研究动机是为了解决在批次数据中，由于数据可能是有偏的、损坏的或敌对的，导致传统密度估计方法效果不佳的问题。",
        "方法": "通过将近似分段多项式分布的鲁棒学习问题简化为学习一个更大离散域中所有大小不超过k的子集的概率的鲁棒学习问题，并以与k线性相关的最优样本复杂度学习这些概率。",
        "关键词": [
            "鲁棒学习",
            "密度估计",
            "对抗性数据",
            "样本复杂度",
            "多项式时间算法"
        ],
        "涉及的技术概念": {
            "近似分段多项式分布": "论文中用于描述可以被鲁棒学习的一类分布，包括高斯、高斯混合、对数凹、低模态和单调风险分布。",
            "鲁棒学习": "论文中用于指代在存在偏置、损坏或敌对数据的情况下，仍能有效学习的技术。"
        }
    },
    {
        "order": 943,
        "title": "Robust Inference for High-Dimensional Linear Models via Residual Randomization",
        "html": "https://ICML.cc//virtual/2021/poster/9141",
        "abstract": "We propose a residual randomization procedure designed for robust inference using Lasso estimates in the high-dimensional setting. Compared to earlier work that focuses on sub-Gaussian errors, the proposed procedure is designed to work robustly in settings that also include heavy-tailed covariates and errors. Moreover, our procedure can be valid under clustered errors, which is important in practice, but has been largely overlooked by earlier work. Through extensive simulations, we illustrate our method's wider range of applicability as suggested by theory. In particular, we show that our method outperforms state-of-art methods in challenging, yet more realistic, settings where the distribution of covariates is heavy-tailed or the sample size is small, while it remains competitive in standard, ``well behaved' settings previously studied in the literature.",
        "conference": "ICML",
        "中文标题": "通过残差随机化实现高维线性模型的稳健推断",
        "摘要翻译": "我们提出了一种残差随机化程序，旨在高维设置下使用Lasso估计进行稳健推断。与早期专注于亚高斯误差的工作相比，所提出的程序设计用于在包括重尾协变量和误差的设置中也能稳健工作。此外，我们的程序在聚类误差下仍然有效，这在实际中非常重要，但早期工作大多忽视了这一点。通过广泛的模拟，我们展示了我们的方法在理论建议的更广泛范围内的适用性。特别是，我们展示了在协变量分布重尾或样本量小的挑战性但更现实的设置中，我们的方法优于最先进的方法，而在文献中先前研究的标准“表现良好”设置中，它仍然具有竞争力。",
        "领域": "高维统计推断",
        "问题": "在高维线性模型中实现稳健推断，特别是在存在重尾协变量和误差以及聚类误差的情况下。",
        "动机": "解决高维线性模型推断中对于非亚高斯误差和重尾分布数据的稳健性问题，以及聚类误差的实际应用场景中被忽视的问题。",
        "方法": "提出了一种基于残差随机化的方法，该方法适用于高维设置，能够处理重尾协变量和误差，并且在聚类误差下仍然有效。",
        "关键词": [
            "残差随机化",
            "高维线性模型",
            "稳健推断",
            "Lasso估计",
            "重尾分布"
        ],
        "涉及的技术概念": {
            "残差随机化": "一种在高维统计推断中用于增强模型稳健性的技术，通过随机化残差来模拟误差分布。",
            "Lasso估计": "一种用于高维数据变量选择和回归系数估计的技术，通过L1正则化实现稀疏性。",
            "重尾分布": "指那些尾部比正态分布更厚的概率分布，常见于实际数据中，对传统统计方法构成挑战。"
        },
        "success": true
    },
    {
        "order": 944,
        "title": "Robust Learning-Augmented Caching: An Experimental Study",
        "html": "https://ICML.cc//virtual/2021/poster/8581",
        "abstract": "Effective caching is crucial for performance of modern-day computing systems. A key optimization problem arising in caching -- which item to evict to make room for a new item -- cannot be optimally solved without knowing the future. There are many classical approximation algorithms for this problem, but more recently researchers started to successfully apply machine learning to decide what to evict by discovering implicit input patterns and predicting the future. While machine learning typically does not provide any worst-case guarantees, the new field of learning-augmented algorithms proposes solutions which leverage classical online caching algorithms to make the machine-learned predictors robust. We are the first to comprehensively evaluate these learning-augmented algorithms on real-world caching datasets and state-of-the-art machine-learned predictors. We show that a straightforward method -- blindly following either a predictor or a classical robust algorithm, and switching whenever one becomes worse than the other -- has only a low overhead over a well-performing predictor, while competing with classical methods when the coupled predictor fails, thus providing a cheap worst-case insurance.",
        "conference": "ICML",
        "中文标题": "鲁棒学习增强缓存：一项实验研究",
        "摘要翻译": "有效的缓存对于现代计算系统的性能至关重要。缓存中出现的一个关键优化问题——即为了给新项目腾出空间而驱逐哪个项目——在不知道未来的情况下无法最优解决。针对这一问题，已有许多经典的近似算法，但最近研究人员开始成功应用机器学习来通过发现隐含的输入模式和预测未来来决定驱逐内容。虽然机器学习通常不提供任何最坏情况下的保证，但学习增强算法这一新领域提出了利用经典在线缓存算法使机器学习预测器变得鲁棒的解决方案。我们是第一个在真实世界缓存数据集和最先进的机器学习预测器上全面评估这些学习增强算法的团队。我们展示了一种简单的方法——盲目跟随预测器或经典鲁棒算法，并在一个比另一个表现差时切换——在表现良好的预测器上只有较低的开销，同时在耦合预测器失败时与经典方法竞争，从而提供了一种廉价的最坏情况保险。",
        "领域": "机器学习应用、缓存优化、在线算法",
        "问题": "如何在不知道未来访问模式的情况下，优化缓存中项目的驱逐决策，以提高计算系统的性能。",
        "动机": "探索机器学习与经典缓存算法的结合，以在不牺牲最坏情况性能的前提下，利用机器学习预测未来访问模式来优化缓存性能。",
        "方法": "采用学习增强算法，结合机器学习预测器和经典在线缓存算法，通过简单切换策略在预测器表现不佳时回退到经典算法。",
        "关键词": [
            "学习增强算法",
            "缓存优化",
            "机器学习预测",
            "在线算法",
            "鲁棒性"
        ],
        "涉及的技术概念": {
            "学习增强算法": "结合机器学习预测和经典算法，旨在提供优于传统方法的性能同时保持最坏情况下的鲁棒性。",
            "缓存优化": "通过智能决策哪些数据应保留在缓存中，以提高数据访问速度和系统性能。",
            "在线算法": "在信息不完全的情况下实时做出决策的算法，适用于缓存管理等需要即时响应的场景。"
        },
        "success": true
    },
    {
        "order": 945,
        "title": "Robust Learning for Data Poisoning Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/10183",
        "abstract": "We investigate the robustness of stochastic approximation approaches against data poisoning attacks. We focus on two-layer neural networks with ReLU activation and show that under a specific notion of separability in the RKHS induced by the infinite-width network, training (finite-width) networks with stochastic gradient descent is robust against data poisoning attacks. Interestingly, we find that in addition to a lower bound on the width of the network, which is standard in the literature, we also require a distribution-dependent upper bound on the width for robust generalization. We provide extensive empirical evaluations that support and validate our theoretical results.",
        "conference": "ICML",
        "中文标题": "对抗数据投毒攻击的鲁棒学习研究",
        "摘要翻译": "我们研究了随机近似方法对抗数据投毒攻击的鲁棒性。我们专注于具有ReLU激活函数的两层神经网络，并表明在由无限宽度网络诱导的RKHS中的特定可分离性概念下，使用随机梯度下降训练（有限宽度）网络能够抵抗数据投毒攻击。有趣的是，我们发现除了文献中标准的网络宽度下限外，还需要一个依赖于分布的网络宽度上限以实现鲁棒的泛化。我们提供了广泛的经验评估，支持和验证了我们的理论结果。",
        "领域": "对抗性机器学习、神经网络鲁棒性、数据安全",
        "问题": "研究随机近似方法在数据投毒攻击下的鲁棒性",
        "动机": "探索在特定条件下，使用随机梯度下降训练的神经网络能够抵抗数据投毒攻击的可能性",
        "方法": "通过理论分析和实证评估，研究在无限宽度网络诱导的RKHS中的特定可分离性概念下，有限宽度网络的训练鲁棒性",
        "关键词": [
            "数据投毒攻击",
            "随机梯度下降",
            "神经网络鲁棒性",
            "RKHS",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "随机梯度下降": "用于训练神经网络，优化模型参数以适应数据",
            "ReLU激活函数": "在神经网络中引入非线性，帮助模型学习复杂模式",
            "RKHS（再生核希尔伯特空间）": "提供了一个理论框架，用于分析无限宽度网络的行为和性质"
        },
        "success": true
    },
    {
        "order": 946,
        "title": "Robust Policy Gradient against Strong Data Corruption",
        "html": "https://ICML.cc//virtual/2021/poster/10179",
        "abstract": "We study the problem of robust reinforcement learning under adversarial corruption on both rewards and transitions. Our attack model assumes an \\textit{adaptive} adversary who can arbitrarily corrupt the reward and transition at every step within an episode, for at most $\\epsilon$-fraction of the learning episodes.  Our attack model is strictly stronger than those considered in prior works. Our first result shows that no algorithm can find a better than $O(\\epsilon)$-optimal policy under our attack model. Next, we show that surprisingly the natural policy gradient (NPG) method retains a natural robustness property if the reward corruption is bounded, and can find an $O(\\sqrt{\\epsilon})$-optimal policy. Consequently, we develop a Filtered Policy Gradient (FPG) algorithm that can tolerate even unbounded reward corruption and can find an $O(\\epsilon^{1/4})$-optimal policy. We emphasize that FPG is the first that can achieve a meaningful learning guarantee when a constant fraction of episodes are corrupted. Complimentary to the theoretical results, we show that a neural implementation of FPG achieves strong robust learning performance on the MuJoCo continuous control benchmarks.",
        "conference": "ICML",
        "中文标题": "对抗强数据污染的鲁棒策略梯度",
        "摘要翻译": "我们研究了在奖励和状态转移均受到对抗性污染情况下的鲁棒强化学习问题。我们的攻击模型假设存在一个自适应对手，该对手能够在每个学习回合的每一步任意污染奖励和状态转移，最多污染学习回合的ε比例。我们的攻击模型严格强于先前工作中考虑的那些模型。我们的第一个结果表明，在我们的攻击模型下，没有任何算法能找到优于O(ε)的最优策略。接下来，我们展示了令人惊讶的结果，即如果奖励污染是有界的，自然策略梯度（NPG）方法保留了一种自然的鲁棒性属性，并且能够找到一个O(√ε)的最优策略。因此，我们开发了一种过滤策略梯度（FPG）算法，该算法甚至能够容忍无界的奖励污染，并且能够找到一个O(ε^1/4)的最优策略。我们强调，FPG是第一个在恒定比例回合被污染时能够实现有意义学习保证的算法。作为理论结果的补充，我们展示了FPG的神经实现在MuJoCo连续控制基准上实现了强大的鲁棒学习性能。",
        "领域": "强化学习、对抗性学习、连续控制",
        "问题": "在奖励和状态转移受到对抗性污染的情况下，如何实现鲁棒的强化学习",
        "动机": "研究在存在自适应对手污染奖励和状态转移的情况下，如何开发能够在这种强对抗环境下仍能有效学习的强化学习算法",
        "方法": "开发了过滤策略梯度（FPG）算法，该算法能够容忍无界的奖励污染，并在恒定比例回合被污染时实现有意义的学习保证",
        "关键词": [
            "鲁棒强化学习",
            "对抗性污染",
            "策略梯度",
            "连续控制",
            "自适应对手"
        ],
        "涉及的技术概念": {
            "自然策略梯度（NPG）": "一种策略梯度方法，用于在奖励污染有界的情况下保持鲁棒性",
            "过滤策略梯度（FPG）": "一种新开发的算法，能够容忍无界的奖励污染，并在对抗性环境下找到最优策略",
            "对抗性学习": "研究在存在对手干扰的情况下如何保持学习算法的性能和鲁棒性"
        },
        "success": true
    },
    {
        "order": 947,
        "title": "Robust Pure Exploration in Linear Bandits with Limited Budget",
        "html": "https://ICML.cc//virtual/2021/poster/9799",
        "abstract": "We consider the pure exploration problem in the fixed-budget linear bandit setting. We provide a new algorithm that identifies the best arm with high probability while being robust to unknown levels of observation noise as well as to moderate levels of misspecification in the linear model. Our technique combines prior approaches to pure exploration in the multi-armed bandit problem with optimal experimental design algorithms to obtain both problem dependent and problem independent bounds. Our success probability is never worse than that of an algorithm that ignores the linear structure, but seamlessly takes advantage of such structure when possible. Furthermore, we only need the number of samples to scale with the dimension of the problem rather than the number of arms. We complement our theoretical results with empirical validation.",
        "conference": "ICML",
        "中文标题": "有限预算下线性赌博机的鲁棒纯探索",
        "摘要翻译": "我们考虑了固定预算线性赌博机设置中的纯探索问题。我们提出了一种新算法，该算法能够以高概率识别最佳臂，同时对未知水平的观测噪声以及线性模型中的适度错误设定具有鲁棒性。我们的技术将多臂赌博机问题中纯探索的先前方法与最优实验设计算法相结合，以获得问题依赖和问题独立的界限。我们的成功概率从不低于忽略线性结构的算法，但在可能时无缝利用这种结构。此外，我们只需要样本数量随问题的维度而非臂的数量缩放。我们用实证验证补充了我们的理论结果。",
        "领域": "强化学习、多臂赌博机问题、最优实验设计",
        "问题": "在固定预算和存在观测噪声及模型错误设定的情况下，如何高效地识别线性赌博机中的最佳臂。",
        "动机": "解决在有限资源和存在不确定性的条件下，如何有效地进行探索以识别最优选择的问题。",
        "方法": "结合多臂赌博机问题中的纯探索方法和最优实验设计算法，提出一种新算法，该算法能够适应观测噪声和模型错误设定，同时利用线性结构提高效率。",
        "关键词": [
            "线性赌博机",
            "纯探索",
            "最优实验设计",
            "鲁棒性",
            "固定预算"
        ],
        "涉及的技术概念": {
            "纯探索": "在多臂赌博机问题中，专注于识别最佳臂而非最大化即时奖励的策略。",
            "最优实验设计": "一种统计方法，用于在给定资源限制下，设计实验以最大化信息增益或减少不确定性。",
            "线性模型错误设定": "指实际数据生成过程与假设的线性模型之间存在偏差，可能影响算法的性能和鲁棒性。"
        },
        "success": true
    },
    {
        "order": 948,
        "title": "Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees",
        "html": "https://ICML.cc//virtual/2021/poster/8693",
        "abstract": "This paper addresses the problem of model-free reinforcement learning for Robust Markov Decision Process (RMDP) with large state spaces. The goal of the RMDPs framework is to find a policy that is robust against the parameter uncertainties due to the  mismatch between the simulator model and  real-world settings.  We first propose the Robust Least Squares Policy Evaluation algorithm, which is a multi-step online model-free learning algorithm for policy evaluation. We prove the convergence of this algorithm using stochastic approximation techniques.  We then propose Robust Least Squares Policy Iteration (RLSPI) algorithm for learning the optimal robust policy. We also give a general weighted Euclidean norm  bound on the error (closeness to optimality) of the resulting policy. Finally, we demonstrate the performance of our RLSPI algorithm on some benchmark problems from OpenAI Gym.",
        "conference": "ICML",
        "中文标题": "使用具有可证明性能保证的最小二乘策略迭代的鲁棒强化学习",
        "摘要翻译": "本文解决了具有大状态空间的鲁棒马尔可夫决策过程（RMDP）的无模型强化学习问题。RMDPs框架的目标是找到一个策略，该策略能够对由于模拟器模型与真实世界设置之间的不匹配导致的参数不确定性具有鲁棒性。我们首先提出了鲁棒最小二乘策略评估算法，这是一种用于策略评估的多步在线无模型学习算法。我们使用随机逼近技术证明了该算法的收敛性。然后，我们提出了用于学习最优鲁棒策略的鲁棒最小二乘策略迭代（RLSPI）算法。我们还给出了关于结果策略误差（接近最优性）的一般加权欧几里得范数界限。最后，我们在OpenAI Gym的一些基准问题上展示了我们的RLSPI算法的性能。",
        "领域": "强化学习、鲁棒控制、策略优化",
        "问题": "解决在大状态空间下鲁棒马尔可夫决策过程的无模型强化学习问题，寻找对参数不确定性具有鲁棒性的策略。",
        "动机": "由于模拟器模型与真实世界设置之间的不匹配导致的参数不确定性，需要开发能够在这种不确定性下仍能保持性能的策略。",
        "方法": "提出鲁棒最小二乘策略评估算法进行策略评估，并证明其收敛性；进一步提出鲁棒最小二乘策略迭代算法学习最优鲁棒策略，并给出策略误差的界限。",
        "关键词": [
            "鲁棒强化学习",
            "最小二乘策略迭代",
            "马尔可夫决策过程",
            "策略评估",
            "OpenAI Gym"
        ],
        "涉及的技术概念": {
            "鲁棒马尔可夫决策过程（RMDP）": "用于在参数不确定性下寻找鲁棒策略的框架。",
            "最小二乘策略评估": "一种多步在线无模型学习算法，用于评估给定策略的性能。",
            "鲁棒最小二乘策略迭代（RLSPI）": "结合策略评估和策略改进的算法，用于学习最优鲁棒策略。"
        },
        "success": true
    },
    {
        "order": 949,
        "title": "Robust Representation Learning via Perceptual Similarity Metrics",
        "html": "https://ICML.cc//virtual/2021/poster/10357",
        "abstract": "A fundamental challenge in artificial intelligence is learning useful representations of data that yield good performance on a downstream classification task, without overfitting to spurious input features. Extracting such task-relevant predictive information becomes particularly difficult for noisy and high-dimensional real-world data. In this work, we propose Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance. Our method leverages a perceptual similarity metric via a triplet loss to ensure that the transformation preserves task-relevant information. Empirically, we demonstrate the efficacy of our approach on various tasks which typically suffer from the presence of spurious correlations: classification with nuisance information, out-of-distribution generalization, and preservation of subgroup accuracies. We additionally show that CIM is complementary to other mutual information-based representation learning techniques, and demonstrate that it improves the performance of variational information bottleneck (VIB) when used in conjunction. ",
        "conference": "ICML",
        "中文标题": "通过感知相似性度量实现鲁棒表示学习",
        "摘要翻译": "人工智能中的一个基本挑战是学习数据的有效表示，这些表示在下游分类任务上表现良好，同时不会过度拟合到虚假的输入特征。对于噪声多且高维的真实世界数据，提取这种与任务相关的预测信息变得尤为困难。在这项工作中，我们提出了对比输入变形（CIM），一个表示学习框架，它学习数据的输入空间变换，以减轻不相关输入特征对下游性能的影响。我们的方法通过三元组损失利用感知相似性度量，确保变换保留了任务相关信息。实证上，我们在通常受到虚假相关性存在的各种任务上证明了我们方法的有效性：带有干扰信息的分类、分布外泛化以及子群准确性的保持。我们还展示了CIM与其他基于互信息的表示学习技术是互补的，并且证明了它与变分信息瓶颈（VIB）结合使用时能提高性能。",
        "领域": "表示学习、深度学习鲁棒性、计算机视觉",
        "问题": "学习能够在下游分类任务中表现良好且不过度拟合虚假输入特征的数据表示",
        "动机": "解决在噪声多且高维的真实世界数据中提取与任务相关的预测信息的困难",
        "方法": "提出对比输入变形（CIM）框架，通过输入空间变换减轻不相关输入特征的影响，利用感知相似性度量和三元组损失确保变换保留任务相关信息",
        "关键词": [
            "表示学习",
            "感知相似性",
            "对比输入变形",
            "三元组损失",
            "变分信息瓶颈"
        ],
        "涉及的技术概念": {
            "对比输入变形（CIM）": "一个表示学习框架，通过学习输入空间的变换来减轻不相关输入特征对下游任务性能的影响",
            "感知相似性度量": "用于确保数据变换过程中保留与任务相关的信息，通过三元组损失实现",
            "三元组损失": "一种损失函数，用于在表示学习中确保相似样本在嵌入空间中接近，而不相似样本远离"
        },
        "success": true
    },
    {
        "order": 950,
        "title": "Robust Testing and Estimation under Manipulation Attacks",
        "html": "https://ICML.cc//virtual/2021/poster/9361",
        "abstract": "We study robust testing and estimation of discrete distributions in the strong contamination model. Our results cover both centralized setting and distributed setting with general local information constraints including communication and LDP constraints. Our technique relates the strength of manipulation attacks to the earth-mover distance using Hamming distance as the metric between messages (samples) from the users. In the centralized setting, we provide optimal error bounds for both learning and testing. Our lower bounds under local information constraints build on the recent lower bound methods in distributed inference. In the communication constrained setting, we develop novel algorithms based on random hashing and an L1-L1 isometry.",
        "conference": "ICML",
        "中文标题": "在操纵攻击下的鲁棒测试与估计",
        "摘要翻译": "我们研究了在强污染模型下离散分布的鲁棒测试与估计。我们的结果涵盖了集中式设置和具有一般本地信息约束（包括通信和LDP约束）的分布式设置。我们的技术通过使用汉明距离作为用户消息（样本）之间的度量，将操纵攻击的强度与地球移动距离联系起来。在集中式设置中，我们为学习和测试提供了最优误差界限。我们在本地信息约束下的下界建立在分布式推理中最近的下界方法之上。在通信约束设置中，我们开发了基于随机哈希和L1-L1等距的新算法。",
        "领域": "统计学习理论、分布式计算、隐私保护数据分析",
        "问题": "在存在操纵攻击的情况下，如何进行离散分布的鲁棒测试与估计",
        "动机": "研究在强污染模型和本地信息约束下，如何有效地进行离散分布的鲁棒测试与估计，以应对操纵攻击",
        "方法": "通过将操纵攻击的强度与地球移动距离联系起来，在集中式和分布式设置中提供最优误差界限，并开发基于随机哈希和L1-L1等距的新算法",
        "关键词": [
            "鲁棒测试",
            "离散分布估计",
            "操纵攻击",
            "分布式推理",
            "L1-L1等距"
        ],
        "涉及的技术概念": {
            "强污染模型": "用于描述数据被操纵攻击污染的程度，是研究鲁棒性的基础",
            "地球移动距离": "用于量化操纵攻击的强度，通过汉明距离作为样本间的度量",
            "L1-L1等距": "在通信约束设置中开发的新算法基础，用于保持数据结构的完整性"
        },
        "success": true
    },
    {
        "order": 951,
        "title": "Robust Unsupervised Learning via L-statistic Minimization",
        "html": "https://ICML.cc//virtual/2021/poster/9291",
        "abstract": "Designing learning algorithms that are resistant to perturbations of the underlying data distribution is a problem of wide practical and theoretical importance. We present a general approach to this problem focusing on unsupervised learning. The key assumption is that the perturbing distribution is characterized by larger losses relative to a given class of admissible models. This is exploited by a general descent algorithm which minimizes an $L$-statistic criterion over the model class, weighting small losses more. Our analysis characterizes the robustness of the method in terms of bounds on the reconstruction error relative to the underlying unperturbed distribution. As a byproduct, we prove uniform convergence bounds with respect to the proposed criterion for several popular models in unsupervised learning, a result which may be of independent interest. Numerical experiments  with \\textsc{kmeans} clustering and principal subspace analysis demonstrate the effectiveness of our approach.",
        "conference": "ICML",
        "中文标题": "通过L统计量最小化实现鲁棒无监督学习",
        "摘要翻译": "设计能够抵抗基础数据分布扰动的学习算法，是一个具有广泛实践和理论重要性的问题。我们提出了一种针对无监督学习的通用方法来解决这一问题。关键假设是扰动分布相对于给定可接受模型类具有较大的损失。我们利用这一假设，通过一种通用的下降算法，在模型类上最小化一个L统计量准则，给予较小损失更多的权重。我们的分析从相对于基础未扰动分布的重构误差界限方面，描述了该方法的鲁棒性。作为副产品，我们证明了针对无监督学习中几种流行模型的所提出准则的均匀收敛界限，这一结果可能具有独立的意义。通过k均值聚类和主子空间分析的数值实验，证明了我们方法的有效性。",
        "领域": "无监督学习、聚类分析、降维",
        "问题": "设计能够抵抗数据分布扰动的无监督学习算法",
        "动机": "提高无监督学习算法在面对数据分布扰动时的鲁棒性",
        "方法": "通过最小化L统计量准则，给予较小损失更多权重，以抵抗数据分布扰动",
        "关键词": [
            "鲁棒学习",
            "L统计量",
            "无监督学习",
            "聚类分析",
            "降维"
        ],
        "涉及的技术概念": {
            "L统计量": "用于衡量模型损失分布的一种统计量，本研究中通过最小化该统计量来提高算法的鲁棒性",
            "无监督学习": "研究重点在于不依赖标注数据的学习方法，特别是如何提高其在数据扰动下的稳定性",
            "均匀收敛界限": "证明了所提出方法在多种无监督学习模型上的理论保证，确保算法性能的稳定性"
        },
        "success": true
    },
    {
        "order": 952,
        "title": "RRL: Resnet as representation for Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10161",
        "abstract": "The ability to autonomously learn behaviors via direct interactions in uninstrumented environments can lead to generalist robots capable of enhancing productivity or providing care in unstructured settings like homes. Such uninstrumented settings warrant operations only using the robot’s proprioceptive sensor such as onboard cameras, joint encoders, etc which can be challenging for policy learning owing to the high dimensionality and partial observability issues. We propose RRL: Resnet as representation for Reinforcement Learning – a straightforward yet effective approach that can learn complex behaviors directly from proprioceptive inputs. RRL fuses features extracted from pre-trained Resnet into the standard reinforcement learning pipeline and delivers results comparable to learning directly from the state. In a simulated dexterous manipulation benchmark, where the state of the art methods fails to make significant progress, RRL delivers contact rich behaviors. The appeal of RRL lies in its simplicity in bringing together progress from the fields of Representation Learning, Imitation Learning, and Reinforcement Learning. Its effectiveness in learning behaviors directly from visual inputs with performance and sample efficiency matching learning directly from the state, even in complex high dimensional domains, is far from obvious.",
        "conference": "ICML",
        "中文标题": "RRL：将Resnet作为强化学习的表示",
        "摘要翻译": "在无仪器环境中通过直接交互自主学习行为的能力可以导致能够在不结构化设置（如家庭）中提高生产力或提供护理的通用机器人。这种无仪器设置仅使用机器人的本体感受传感器（如机载摄像头、关节编码器等）进行操作，由于高维度和部分可观测性问题，这对策略学习可能具有挑战性。我们提出了RRL：将Resnet作为强化学习的表示——一种简单而有效的方法，可以直接从本体感受输入中学习复杂行为。RRL将从预训练的Resnet中提取的特征融合到标准的强化学习流程中，并提供了与直接从状态学习相媲美的结果。在一个模拟的灵巧操作基准测试中，现有技术方法未能取得显著进展，而RRL则提供了丰富的接触行为。RRL的吸引力在于其简单地将表示学习、模仿学习和强化学习领域的进展结合在一起。它直接从视觉输入中学习行为的有效性，以及性能和样本效率与直接从状态学习相匹配，即使在复杂的高维领域中也远非显而易见。",
        "领域": "机器人学习、强化学习、视觉表示学习",
        "问题": "在无仪器环境中，仅使用机器人的本体感受传感器进行策略学习时面临的高维度和部分可观测性问题。",
        "动机": "开发一种能够直接从本体感受输入中学习复杂行为的通用机器人，以提高在不结构化设置中的生产力或提供护理。",
        "方法": "将预训练的Resnet提取的特征融合到标准的强化学习流程中，直接从本体感受输入中学习行为。",
        "关键词": [
            "强化学习",
            "Resnet",
            "机器人学习",
            "视觉表示",
            "本体感受输入"
        ],
        "涉及的技术概念": {
            "Resnet": "预训练的深度残差网络，用于从视觉输入中提取特征，作为强化学习的表示。",
            "强化学习": "一种通过与环境交互学习最优策略的机器学习方法，RRL中用于直接从本体感受输入中学习行为。",
            "表示学习": "学习数据的表示形式，以便于从原始输入中提取有用的信息，RRL中通过Resnet实现。"
        },
        "success": true
    },
    {
        "order": 953,
        "title": "Run-Sort-ReRun: Escaping Batch Size Limitations in Sliced Wasserstein Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/10569",
        "abstract": "When training an implicit generative model, ideally one would like the generator to reproduce all the different modes and subtleties of the target distribution. Naturally, when comparing two empirical distributions, the larger the sample population, the more these statistical nuances can be captured. However, existing objective functions are computationally constrained in the amount of samples they can consider by the memory required to process a batch of samples. In this paper, we build upon  recent progress in sliced Wasserstein distances, a family of  differentiable metrics for distribution discrepancy based on the Optimal Transport paradigm. We introduce a procedure to train these distances with virtually any batch size, allowing the discrepancy measure to capture richer statistics and better approximating the distance between the underlying continuous distributions. As an example, we demonstrate the matching of the distribution of Inception features with batches of tens of thousands of samples, achieving FID scores that outperform state-of-the-art implicit generative models.",
        "conference": "ICML",
        "中文标题": "运行-排序-重新运行：在切片Wasserstein生成模型中突破批量大小限制",
        "摘要翻译": "在训练隐式生成模型时，理想情况下，我们希望生成器能够重现目标分布的所有不同模式和细微差别。自然，当比较两个经验分布时，样本量越大，越能捕捉到这些统计上的细微差别。然而，现有的目标函数在计算上受到处理一批样本所需内存的限制，限制了它们可以考虑的样本量。在本文中，我们基于切片Wasserstein距离的最新进展，这是一种基于最优传输范式的分布差异可微分度量家族。我们引入了一种训练这些距离的过程，几乎可以使用任何批量大小，使得差异度量能够捕捉更丰富的统计信息，并更好地逼近底层连续分布之间的距离。作为一个例子，我们展示了与数万个样本批次匹配Inception特征的分布，实现了超越最先进隐式生成模型的FID分数。",
        "领域": "生成对抗网络、最优传输、隐式生成模型",
        "问题": "如何在隐式生成模型的训练中突破批量大小的限制，以更好地捕捉目标分布的统计特性",
        "动机": "现有的目标函数由于内存限制无法处理大批量样本，限制了模型捕捉目标分布细微差异的能力",
        "方法": "基于切片Wasserstein距离，提出了一种新的训练过程，允许使用任意大小的批量样本",
        "关键词": [
            "切片Wasserstein距离",
            "隐式生成模型",
            "最优传输",
            "批量大小",
            "FID分数"
        ],
        "涉及的技术概念": {
            "切片Wasserstein距离": "一种基于最优传输的可微分度量，用于衡量两个分布之间的差异",
            "隐式生成模型": "通过隐式方式学习数据分布的生成模型，如GANs",
            "最优传输": "一种数学框架，用于在两个概率分布之间找到最优的传输方案"
        },
        "success": true
    },
    {
        "order": 954,
        "title": "Safe Reinforcement Learning Using Advantage-Based Intervention",
        "html": "https://ICML.cc//virtual/2021/poster/10129",
        "abstract": "Many sequential decision problems involve finding a policy that maximizes total reward while obeying safety constraints. Although much recent research has focused on the development of safe reinforcement learning (RL) algorithms that produce a safe policy after training, ensuring safety during training as well remains an open problem. A fundamental challenge is performing exploration while still satisfying constraints in an unknown Markov decision process (MDP). In this work, we address this problem for the chance-constrained setting.We propose a new algorithm, SAILR, that uses an intervention mechanism based on advantage functions to keep the agent safe throughout training and optimizes the agent's policy using off-the-shelf RL algorithms designed for unconstrained MDPs. Our method comes with strong guarantees on safety during 'both' training and deployment (i.e., after training and without the intervention mechanism) and policy performance compared to the optimal safety-constrained policy. In our experiments, we show that SAILR violates constraints far less during training than standard safe RL and constrained MDP approaches and converges to a well-performing policy that can be deployed safely without intervention. Our code is available at https://github.com/nolanwagener/safe_rl.",
        "conference": "ICML",
        "中文标题": "基于优势干预的安全强化学习",
        "摘要翻译": "许多顺序决策问题涉及在遵守安全约束的同时找到最大化总奖励的策略。尽管最近的研究主要集中在开发在训练后产生安全策略的安全强化学习（RL）算法上，但在训练期间确保安全仍然是一个未解决的问题。一个基本挑战是在未知的马尔可夫决策过程（MDP）中探索的同时仍然满足约束。在这项工作中，我们针对机会约束设置解决了这个问题。我们提出了一种新算法SAILR，该算法使用基于优势函数的干预机制，在整个训练过程中保持代理的安全，并使用为无约束MDP设计的现成RL算法优化代理的策略。我们的方法在训练和部署（即训练后且没有干预机制）期间的安全性以及与最优安全约束策略相比的策略性能方面提供了强有力的保证。在我们的实验中，我们表明SAILR在训练期间违反约束的情况远少于标准安全RL和约束MDP方法，并且收敛到一个可以在没有干预的情况下安全部署的性能良好的策略。我们的代码可在https://github.com/nolanwagener/safe_rl获取。",
        "领域": "强化学习安全机制、马尔可夫决策过程、机会约束优化",
        "问题": "在未知的马尔可夫决策过程中进行探索时如何满足安全约束",
        "动机": "解决在强化学习训练期间确保安全性的开放性问题",
        "方法": "提出SAILR算法，利用基于优势函数的干预机制保持训练安全，并使用现成RL算法优化策略",
        "关键词": [
            "安全强化学习",
            "优势函数",
            "干预机制",
            "马尔可夫决策过程",
            "机会约束"
        ],
        "涉及的技术概念": {
            "优势函数": "用于评估行动相对于平均表现的优劣，指导干预机制以确保安全",
            "干预机制": "在训练过程中动态调整代理行为以避免违反安全约束",
            "机会约束优化": "在满足一定概率约束的条件下优化策略，确保决策过程的安全性"
        },
        "success": true
    },
    {
        "order": 955,
        "title": "Safe Reinforcement Learning with Linear Function Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/9847",
        "abstract": "Safety in reinforcement learning has become increasingly important in recent years. Yet, existing solutions either fail to strictly avoid choosing unsafe actions, which may lead to catastrophic results in safety-critical systems, or fail to provide regret guarantees for settings where safety constraints need to be learned. In this paper, we address both problems by first modeling safety as an unknown linear cost function of states and actions, which must always fall below a certain threshold. We then present algorithms, termed SLUCB-QVI and RSLUCB-QVI, for episodic Markov decision processes (MDPs) with linear function approximation. We show that SLUCB-QVI and RSLUCB-QVI, while with \\emph{no safety violation}, achieve a $\\tilde{\\mathcal{O}}\\left(\\kappa\\sqrt{d^3H^3T}\\right)$ regret, nearly matching that of state-of-the-art unsafe algorithms, where $H$ is the duration of each episode, $d$ is the dimension of the feature mapping, $\\kappa$ is a constant characterizing the safety constraints, and $T$ is the total number of action plays. We further present numerical simulations that corroborate our theoretical findings.",
        "conference": "ICML",
        "success": true,
        "中文标题": "基于线性函数逼近的安全强化学习",
        "摘要翻译": "近年来，强化学习的安全性变得越来越重要。然而，现有的解决方案要么无法严格避免选择不安全的行为，这可能在安全关键系统中导致灾难性后果，要么无法为需要学习安全约束的环境提供遗憾保证。在本文中，我们通过首先将安全建模为状态和行为的未知线性成本函数来解决这两个问题，该函数必须始终低于某个阈值。然后，我们提出了两种算法，称为SLUCB-QVI和RSLUCB-QVI，用于具有线性函数逼近的阶段性马尔可夫决策过程（MDPs）。我们展示了SLUCB-QVI和RSLUCB-QVI在没有任何安全违规的情况下，实现了接近最先进不安全算法的遗憾度，其中H是每个阶段的持续时间，d是特征映射的维度，κ是表征安全约束的常数，T是行为播放的总次数。我们进一步提供了数值模拟，证实了我们的理论发现。",
        "领域": "强化学习, 安全关键系统, 马尔可夫决策过程",
        "问题": "解决强化学习中的安全问题，包括避免不安全行为和提供遗憾保证。",
        "动机": "在安全关键系统中，避免不安全行为至关重要，同时需要保证学习算法的性能。",
        "方法": "将安全建模为状态和行为的未知线性成本函数，并提出两种算法SLUCB-QVI和RSLUCB-QVI，用于具有线性函数逼近的阶段性马尔可夫决策过程。",
        "关键词": [
            "安全强化学习",
            "线性函数逼近",
            "马尔可夫决策过程",
            "遗憾保证",
            "安全约束"
        ],
        "涉及的技术概念": {
            "线性函数逼近": "用于建模安全作为状态和行为的未知线性成本函数，确保行为选择的安全性。",
            "马尔可夫决策过程": "提供了一种框架，用于在具有不确定性的环境中做出序列决策。",
            "遗憾保证": "衡量算法性能的指标，确保算法在长期运行中的表现接近最优。"
        }
    },
    {
        "order": 956,
        "title": "SagaNet: A Small Sample Gated Network for Pediatric Cancer Diagnosis",
        "html": "https://ICML.cc//virtual/2021/poster/8479",
        "abstract": "The scarcity of available samples and the high annotation cost of medical data cause a bottleneck in many digital diagnosis tasks based on deep learning. This problem is especially severe in pediatric tumor tasks, due to the small population base of children and high sample diversity caused by the high metastasis rate of related tumors. Targeted research on pediatric tumors is urgently needed but lacks sufficient attention. In this work, we propose a novel model to solve the diagnosis task of small round blue cell tumors (SRBCTs). To solve the problem of high noise and high diversity in the small sample scenario, the model is constrained to pay attention to the valid areas in the pathological image with a masking mechanism, and a length-aware loss is proposed to improve the tolerance to feature diversity. We evaluate this framework on a challenging small sample SRBCTs dataset, whose classification is difficult even for professional pathologists. The proposed model shows the best performance compared with state-of-the-art deep models and generalization on another pathological dataset, which illustrates the potentiality of deep learning applications in difficult small sample medical tasks.",
        "conference": "ICML",
        "中文标题": "SagaNet：一种用于儿科癌症诊断的小样本门控网络",
        "摘要翻译": "可用样本的稀缺性和医疗数据的高标注成本导致了许多基于深度学习的数字诊断任务遇到瓶颈。这一问题在儿科肿瘤任务中尤为严重，原因是儿童人口基数小以及相关肿瘤高转移率导致的高样本多样性。针对儿科肿瘤的针对性研究迫切需要但缺乏足够的关注。在这项工作中，我们提出了一种新模型来解决小圆蓝细胞肿瘤（SRBCTs）的诊断任务。为了解决小样本场景下的高噪声和高多样性问题，该模型通过掩蔽机制约束关注病理图像中的有效区域，并提出了一种长度感知损失以提高对特征多样性的容忍度。我们在一个具有挑战性的小样本SRBCTs数据集上评估了这一框架，其分类即使对于专业病理学家来说也很困难。与最先进的深度模型相比，所提出的模型显示出最佳性能，并在另一个病理数据集上展示了泛化能力，这说明了深度学习在困难的小样本医疗任务中的应用潜力。",
        "领域": "医学图像分析、儿科肿瘤诊断、小样本学习",
        "问题": "解决小样本场景下儿科肿瘤诊断中的高噪声和高多样性问题",
        "动机": "针对儿科肿瘤诊断中样本稀缺和多样性高的问题，开发一种有效的深度学习方法",
        "方法": "通过掩蔽机制关注病理图像中的有效区域，并提出长度感知损失以提高对特征多样性的容忍度",
        "关键词": [
            "小样本学习",
            "儿科肿瘤诊断",
            "掩蔽机制",
            "长度感知损失",
            "医学图像分析"
        ],
        "涉及的技术概念": {
            "掩蔽机制": "用于约束模型关注病理图像中的有效区域，减少噪声干扰",
            "长度感知损失": "一种新型损失函数，旨在提高模型对特征多样性的容忍度，优化小样本学习效果",
            "小样本学习": "在样本数量有限的情况下进行有效学习的技术，特别适用于医疗图像分析等数据稀缺领域"
        },
        "success": true
    },
    {
        "order": 957,
        "title": "SAINT-ACC: Safety-Aware Intelligent Adaptive Cruise Control for Autonomous Vehicles Using Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8685",
        "abstract": "We present a novel adaptive cruise control (ACC) system namely SAINT-ACC: {S}afety-{A}ware {Int}elligent {ACC} system (SAINT-ACC) that is designed to achieve simultaneous optimization of traffic efficiency, driving safety, and driving comfort through dynamic adaptation of the inter-vehicle gap based on deep reinforcement learning (RL). A novel dual RL agent-based approach is developed to seek and adapt the optimal balance between traffic efficiency and driving safety/comfort by effectively controlling the driving safety model parameters and inter-vehicle gap based on macroscopic and microscopic traffic information collected from dynamically changing and complex traffic environments. Results obtained through over 12,000 simulation runs with varying traffic scenarios and penetration rates demonstrate that SAINT-ACC significantly enhances traffic flow, driving safety and comfort compared with a state-of-the-art approach.",
        "conference": "ICML",
        "中文标题": "SAINT-ACC：基于深度强化学习的自动驾驶汽车安全感知智能自适应巡航控制",
        "摘要翻译": "我们提出了一种新型自适应巡航控制（ACC）系统，即SAINT-ACC：安全感知智能ACC系统（SAINT-ACC），该系统旨在通过基于深度强化学习（RL）的动态调整车辆间距离，同时优化交通效率、驾驶安全和驾驶舒适性。开发了一种基于双RL代理的新方法，通过有效控制驾驶安全模型参数和基于从动态变化和复杂交通环境中收集的宏观和微观交通信息的车辆间距离，寻求并适应交通效率与驾驶安全/舒适性之间的最佳平衡。通过超过12,000次不同交通场景和渗透率的模拟运行获得的结果表明，与最先进的方法相比，SAINT-ACC显著提高了交通流量、驾驶安全和舒适性。",
        "领域": "自动驾驶、智能交通系统、深度强化学习",
        "问题": "如何在自动驾驶汽车中实现交通效率、驾驶安全和驾驶舒适性的同时优化",
        "动机": "开发一种能够动态适应复杂交通环境，同时优化交通效率、驾驶安全和舒适性的自适应巡航控制系统",
        "方法": "采用基于双深度强化学习代理的方法，动态调整车辆间距离和驾驶安全模型参数",
        "关键词": [
            "自适应巡航控制",
            "深度强化学习",
            "自动驾驶",
            "交通效率",
            "驾驶安全"
        ],
        "涉及的技术概念": {
            "深度强化学习": "用于动态调整车辆间距离和驾驶安全模型参数，以实现交通效率、安全和舒适性的优化",
            "双RL代理": "用于寻求和适应交通效率与驾驶安全/舒适性之间的最佳平衡",
            "车辆间距离动态调整": "基于宏观和微观交通信息，动态调整以优化驾驶安全和舒适性"
        },
        "success": true
    },
    {
        "order": 958,
        "title": "Sample Complexity of Robust Linear Classification on Separated Data",
        "html": "https://ICML.cc//virtual/2021/poster/9381",
        "abstract": "We consider the sample complexity of learning with adversarial robustness. Most prior theoretical results for this problem have considered a setting where different classes in the data are close together or overlapping. We consider, in contrast, the well-separated case where there exists a classifier with perfect accuracy and robustness, and show that the sample complexity narrates an entirely different story. Specifically, for linear classifiers, we show a large class of well-separated distributions where the expected robust loss of any algorithm is at least $\\Omega(\\frac{d}{n})$, whereas the max margin algorithm has expected standard loss $O(\\frac{1}{n})$. This shows a gap in the standard and robust losses that cannot be obtained via prior techniques. Additionally, we present an algorithm that, given an instance where the robustness radius is much smaller than the gap between the classes, gives a solution with expected robust loss is $O(\\frac{1}{n})$. This shows that for very well-separated data, convergence rates of $O(\\frac{1}{n})$ are achievable, which is not the case otherwise. Our results apply to robustness measured in any $\\ell_p$ norm with $p > 1$ (including $p = \\infty$).",
        "conference": "ICML",
        "中文标题": "分离数据上鲁棒线性分类的样本复杂度",
        "摘要翻译": "我们考虑了具有对抗鲁棒性的学习样本复杂度。此前关于这一问题的理论结果大多考虑了数据中不同类别接近或重叠的情况。与之相反，我们考虑了存在一个具有完美准确性和鲁棒性的分类器的良好分离情况，并展示了样本复杂度讲述了一个完全不同的故事。具体来说，对于线性分类器，我们展示了一大类良好分离的分布，其中任何算法的期望鲁棒损失至少为Ω(d/n)，而最大间隔算法的期望标准损失为O(1/n)。这显示了标准和鲁棒损失之间的差距，这是通过之前的技术无法获得的。此外，我们提出了一种算法，在鲁棒性半径远小于类别间差距的实例中，给出了期望鲁棒损失为O(1/n)的解决方案。这表明，对于非常良好分离的数据，可以实现O(1/n)的收敛率，否则则不行。我们的结果适用于以任何p>1（包括p=∞）的ℓp范数测量的鲁棒性。",
        "领域": "对抗性机器学习、线性分类、鲁棒性分析",
        "问题": "研究在数据类别良好分离的情况下，鲁棒线性分类的样本复杂度问题。",
        "动机": "探索在数据类别良好分离的情况下，鲁棒性学习与标准学习在样本复杂度上的差异，以及如何在这种条件下实现更高效的鲁棒学习。",
        "方法": "通过理论分析展示在良好分离数据上鲁棒学习与标准学习的样本复杂度差异，并提出一种算法在特定条件下实现高效鲁棒学习。",
        "关键词": [
            "对抗性鲁棒性",
            "样本复杂度",
            "线性分类",
            "最大间隔算法",
            "ℓp范数"
        ],
        "涉及的技术概念": {
            "对抗性鲁棒性": "研究模型在面对精心设计的输入扰动时的性能稳定性。",
            "样本复杂度": "学习算法达到特定性能水平所需的最小样本数量。",
            "最大间隔算法": "一种线性分类算法，旨在找到能够最大化不同类别数据点之间间隔的分类边界。"
        },
        "success": true
    },
    {
        "order": 959,
        "title": "Sample Efficient Reinforcement Learning In Continuous State Spaces: A Perspective Beyond Linearity",
        "html": "https://ICML.cc//virtual/2021/poster/9163",
        "abstract": "Reinforcement learning (RL) is empirically successful in complex nonlinear Markov decision processes (MDPs) with continuous state spaces. By contrast, the majority of theoretical RL literature requires the MDP to satisfy some form of linear structure, in order to guarantee sample efficient RL. Such efforts typically assume the transition dynamics or value function of the MDP are described by linear functions of the state features. To resolve this discrepancy between theory and practice, we introduce the Effective Planning Window (EPW) condition, a structural condition on MDPs that makes no linearity assumptions. We demonstrate that the EPW condition permits sample efficient RL, by providing an algorithm which provably solves MDPs satisfying this condition. Our algorithm requires minimal assumptions on the policy class, which can include multi-layer neural networks with nonlinear activation functions. Notably, the EPW condition is directly motivated by popular gaming benchmarks, and we show that many classic Atari games satisfy this condition. We additionally show the necessity of conditions like EPW, by demonstrating that simple MDPs with slight nonlinearities cannot be solved sample efficiently.",
        "conference": "ICML",
        "中文标题": "连续状态空间中样本高效的强化学习：超越线性的视角",
        "摘要翻译": "强化学习（RL）在具有连续状态空间的复杂非线性马尔可夫决策过程（MDPs）中取得了实证上的成功。相比之下，大多数理论上的RL文献要求MDP满足某种形式的线性结构，以保证样本高效的RL。这些努力通常假设MDP的转移动态或价值函数由状态特征的线性函数描述。为了解决理论与实践之间的这种差异，我们引入了有效规划窗口（EPW）条件，这是一种对MDP的结构性条件，不做线性假设。我们证明，EPW条件允许样本高效的RL，通过提供一个算法，该算法可证明地解决了满足此条件的MDPs。我们的算法对策略类的要求极低，可以包括具有非线性激活函数的多层神经网络。值得注意的是，EPW条件直接受到流行的游戏基准的启发，我们展示了许多经典的Atari游戏满足这一条件。我们还通过展示具有轻微非线性的简单MDPs不能被样本高效地解决，来证明像EPW这样的条件的必要性。",
        "领域": "强化学习、连续控制、游戏AI",
        "问题": "解决在连续状态空间中，非线性MDPs样本高效强化学习的理论与实践的差异问题",
        "动机": "为了弥合强化学习在复杂非线性MDPs中实证成功与理论要求线性结构之间的差距",
        "方法": "引入有效规划窗口（EPW）条件，开发一种算法，该算法在满足EPW条件的MDPs上可证明实现样本高效的RL",
        "关键词": [
            "强化学习",
            "连续状态空间",
            "非线性MDPs",
            "样本效率",
            "有效规划窗口"
        ],
        "涉及的技术概念": {
            "有效规划窗口（EPW）条件": "一种对MDP的结构性条件，不做线性假设，允许样本高效的RL",
            "马尔可夫决策过程（MDPs）": "用于建模决策问题的数学框架，其中结果部分随机且部分受决策者控制",
            "非线性激活函数": "在神经网络中引入非线性，使得网络能够学习复杂的模式和关系"
        },
        "success": true
    },
    {
        "order": 960,
        "title": "Sample-Optimal PAC Learning of Halfspaces with Malicious Noise",
        "html": "https://ICML.cc//virtual/2021/poster/9095",
        "abstract": "We study efficient PAC learning of homogeneous halfspaces in $\\mathbb{R}^d$ in the presence of malicious noise of Valiant (1985). This is a challenging noise model and only until recently has near-optimal noise tolerance bound been established under the mild condition that the unlabeled data distribution is isotropic log-concave. However, it remains unsettled how to obtain the optimal sample complexity simultaneously. In this work, we present a new analysis for the algorithm of Awasthi et al. (2017) and show that it essentially achieves the near-optimal sample complexity bound of $\\tilde{O}(d)$, improving the best known result of $\\tilde{O}(d^2)$. Our main ingredient is a novel incorporation of a matrix Chernoff-type inequality to bound the spectrum of an empirical covariance matrix for well-behaved distributions, in conjunction with a careful exploration of the localization schemes of Awasthi et al. (2017). We further extend the algorithm and analysis to the more general and stronger nasty noise model of Bshouty et al. (2002), showing that it is still possible to achieve near-optimal noise tolerance and sample complexity in polynomial time.",
        "conference": "ICML",
        "success": true,
        "中文标题": "恶意噪声下半空间样本最优PAC学习",
        "摘要翻译": "我们研究了在Valiant（1985）提出的恶意噪声存在下，$\\mathbb{R}^d$中齐次半空间的有效PAC学习。这是一个具有挑战性的噪声模型，直到最近在未标记数据分布为各向同性对数凹的温和条件下，才建立了接近最优的噪声容忍界限。然而，如何同时获得最优样本复杂度仍然是一个未解决的问题。在这项工作中，我们对Awasthi等人（2017）的算法进行了新的分析，并表明它基本上实现了$\\tilde{O}(d)$的接近最优样本复杂度界限，改进了已知的最佳结果$\\tilde{O}(d^2)$。我们的主要贡献是新颖地结合了矩阵Chernoff型不等式来限制行为良好分布的实证协方差矩阵的谱，同时仔细探索了Awasthi等人（2017）的定位方案。我们进一步将算法和分析扩展到Bshouty等人（2002）提出的更一般和更强的恶意噪声模型，表明在多项式时间内仍然可以实现接近最优的噪声容忍和样本复杂度。",
        "领域": "机器学习理论, 噪声鲁棒学习, 半空间学习",
        "问题": "在恶意噪声存在下，如何实现齐次半空间的样本最优PAC学习。",
        "动机": "解决在恶意噪声环境下，齐次半空间学习的样本复杂度优化问题，以提高学习效率和准确性。",
        "方法": "通过结合矩阵Chernoff型不等式和Awasthi等人的定位方案，改进算法以实现接近最优的样本复杂度。",
        "关键词": [
            "PAC学习",
            "恶意噪声",
            "半空间学习",
            "样本复杂度",
            "噪声鲁棒性"
        ],
        "涉及的技术概念": {
            "矩阵Chernoff型不等式": "用于限制行为良好分布的实证协方差矩阵的谱，是分析样本复杂度的关键工具。",
            "定位方案": "Awasthi等人提出的技术，用于在噪声环境下精确定位学习目标，提高学习效率。"
        }
    },
    {
        "order": 961,
        "title": "Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network",
        "html": "https://ICML.cc//virtual/2021/poster/8403",
        "abstract": "Hierarchical topic models such as the gamma belief network (GBN) have delivered promising results in mining multi-layer document representations and discovering interpretable topic taxonomies. However, they often assume in the prior that the topics at each layer are independently drawn from the Dirichlet distribution, ignoring the dependencies between the topics both at the same layer and across different layers. To relax this assumption, we propose sawtooth factorial topic embedding guided GBN, a deep generative model of documents that captures the dependencies and semantic similarities between the topics in the embedding space. Specifically, both the words and topics are represented as embedding vectors of the same dimension. The topic matrix at a layer is factorized into the product of a factor loading matrix and a topic embedding matrix, the transpose of which is set as the factor loading matrix of the layer above. Repeating this particular type of factorization, which shares components between adjacent layers, leads to a structure referred to as sawtooth factorization. An auto-encoding variational inference network is constructed to optimize the model parameter via stochastic gradient descent. Experiments on big corpora show that our models outperform other neural topic models on extracting deeper interpretable topics and deriving better document representations.",
        "conference": "ICML",
        "中文标题": "锯齿因子主题嵌入引导的伽马信念网络",
        "摘要翻译": "诸如伽马信念网络（GBN）之类的层次主题模型在挖掘多层文档表示和发现可解释的主题分类方面取得了有希望的结果。然而，它们通常在先验中假设每一层的主题是从Dirichlet分布中独立抽取的，忽略了同一层内和不同层间主题之间的依赖关系。为了放宽这一假设，我们提出了锯齿因子主题嵌入引导的GBN，这是一种捕获嵌入空间中主题间依赖关系和语义相似性的文档深度生成模型。具体来说，单词和主题都被表示为相同维度的嵌入向量。一层的主题矩阵被分解为一个因子载荷矩阵和一个主题嵌入矩阵的乘积，后者的转置被设置为上一层的因子载荷矩阵。重复这种特定类型的分解，即在相邻层之间共享组件，导致了一种被称为锯齿分解的结构。构建了一个自动编码变分推理网络，通过随机梯度下降优化模型参数。在大规模语料库上的实验表明，我们的模型在提取更深层次的可解释主题和推导更好的文档表示方面优于其他神经主题模型。",
        "领域": "自然语言处理与视觉结合、文本挖掘、深度学习模型",
        "问题": "解决层次主题模型中忽略主题间依赖关系的问题",
        "动机": "捕捉主题间的依赖关系和语义相似性，以提高主题模型的表现",
        "方法": "提出锯齿因子主题嵌入引导的GBN，利用嵌入空间表示主题和单词，通过锯齿分解结构共享相邻层组件，并使用自动编码变分推理网络优化模型参数",
        "关键词": [
            "伽马信念网络",
            "主题嵌入",
            "锯齿分解",
            "变分推理",
            "文档表示"
        ],
        "涉及的技术概念": {
            "伽马信念网络": "一种层次主题模型，用于挖掘多层文档表示和发现可解释的主题分类",
            "锯齿分解": "一种在相邻层之间共享组件的特定类型分解，用于构建层次主题模型",
            "自动编码变分推理网络": "用于通过随机梯度下降优化模型参数的推理网络"
        },
        "success": true
    },
    {
        "order": 962,
        "title": "Scalable Certified Segmentation via Randomized Smoothing",
        "html": "https://ICML.cc//virtual/2021/poster/8809",
        "abstract": "We present a new certification method for image and point cloud segmentation based on randomized smoothing. The method leverages a novel scalable algorithm for prediction and certification that correctly accounts for multiple testing, necessary for ensuring statistical guarantees. The key to our approach is reliance on established multiple-testing correction mechanisms as well as the ability to abstain from classifying single pixels or points while still robustly segmenting the overall input. Our experimental evaluation on synthetic data and challenging datasets, such as Pascal Context, Cityscapes, and ShapeNet, shows that our algorithm can achieve, for the first time, competitive accuracy and certification guarantees on real-world segmentation tasks.\nWe provide an implementation at https://github.com/eth-sri/segmentation-smoothing.",
        "conference": "ICML",
        "中文标题": "基于随机平滑的可扩展认证分割方法",
        "摘要翻译": "我们提出了一种基于随机平滑的图像和点云分割新认证方法。该方法利用了一种新颖的可扩展预测和认证算法，该算法正确考虑了多重测试，这对于确保统计保证是必要的。我们方法的关键在于依赖已建立的多重测试校正机制，以及能够在不对单个像素或点进行分类的情况下，仍然对整体输入进行鲁棒分割的能力。我们在合成数据和具有挑战性的数据集（如Pascal Context、Cityscapes和ShapeNet）上的实验评估表明，我们的算法首次在现实世界分割任务上实现了竞争性的准确性和认证保证。我们在https://github.com/eth-sri/segmentation-smoothing提供了实现。",
        "领域": "图像分割, 点云分割, 认证机器学习",
        "问题": "如何在图像和点云分割任务中实现可扩展的认证方法，确保统计保证。",
        "动机": "为了解决现有分割方法在认证保证和可扩展性方面的不足，特别是在处理复杂数据集时。",
        "方法": "提出了一种基于随机平滑的新认证方法，结合多重测试校正机制和选择性分类策略，以实现鲁棒且可扩展的分割。",
        "关键词": [
            "随机平滑",
            "认证分割",
            "多重测试校正",
            "点云分割",
            "图像分割"
        ],
        "涉及的技术概念": {
            "随机平滑": "用于提高模型预测的鲁棒性和认证保证的技术，通过在输入数据上添加随机噪声并聚合预测结果。",
            "多重测试校正": "在统计测试中调整显著性水平以控制错误发现率的方法，确保统计结果的可靠性。",
            "选择性分类": "允许模型在某些情况下拒绝做出预测的策略，以提高整体预测的准确性和鲁棒性。"
        },
        "success": true
    },
    {
        "order": 963,
        "title": "Scalable Computations of Wasserstein Barycenter via Input Convex Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8951",
        "abstract": "Wasserstein Barycenter is a principled approach to represent the weighted mean of a given set of probability distributions, utilizing the geometry induced by optimal transport. In this work, we present a novel scalable algorithm to approximate the Wasserstein Barycenters aiming at high-dimensional applications in machine learning. Our proposed algorithm is based on the Kantorovich dual formulation of the Wasserstein-2 distance as well as a recent neural network architecture, input convex neural network, that is known to parametrize convex functions. The distinguishing features of our method are: i) it only requires samples from the marginal distributions; ii) unlike the existing approaches, it represents the Barycenter with a generative model and can thus generate infinite samples from the barycenter without querying the marginal distributions; iii) it works similar to Generative Adversarial Model in one marginal case. We demonstratethe efficacy of our algorithm by comparing it with the state-of-art methods in multiple experiments.",
        "conference": "ICML",
        "中文标题": "通过输入凸神经网络实现Wasserstein质心的可扩展计算",
        "摘要翻译": "Wasserstein质心是一种利用最优输运诱导的几何原理来表示给定一组概率分布的加权平均的方法。在这项工作中，我们提出了一种新颖的可扩展算法，旨在机器学习中的高维应用近似Wasserstein质心。我们提出的算法基于Wasserstein-2距离的Kantorovich对偶公式以及一种已知能够参数化凸函数的最近神经网络架构——输入凸神经网络。我们方法的显著特点是：i)它仅需要来自边缘分布的样本；ii)与现有方法不同，它通过生成模型表示质心，因此可以在不查询边缘分布的情况下从质心生成无限样本；iii)在单一边缘情况下，它的工作方式类似于生成对抗模型。我们通过在多组实验中与最先进的方法进行比较，证明了我们算法的有效性。",
        "领域": "最优输运理论、生成模型、高维概率分布",
        "问题": "如何高效且可扩展地计算高维空间中的Wasserstein质心",
        "动机": "为了解决现有方法在处理高维数据时计算Wasserstein质心的效率和可扩展性问题",
        "方法": "基于Kantorovich对偶公式和输入凸神经网络，开发了一种新的算法来近似计算Wasserstein质心",
        "关键词": [
            "Wasserstein质心",
            "输入凸神经网络",
            "生成模型",
            "最优输运",
            "高维数据"
        ],
        "涉及的技术概念": {
            "Wasserstein质心": "用于表示一组概率分布的加权平均，基于最优输运理论",
            "输入凸神经网络": "一种能够参数化凸函数的神经网络架构，用于保证生成函数的凸性",
            "Kantorovich对偶公式": "用于计算Wasserstein距离的数学工具，为算法提供了理论基础"
        },
        "success": true
    },
    {
        "order": 964,
        "title": "Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot",
        "html": "https://ICML.cc//virtual/2021/poster/9947",
        "abstract": "Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.",
        "conference": "ICML",
        "中文标题": "可扩展的多智能体强化学习评估框架：Melting Pot",
        "摘要翻译": "现有的多智能体强化学习（MARL）评估套件并未将评估对新情境的泛化能力作为其主要目标（与监督学习基准不同）。我们的贡献Melting Pot是一个填补这一空白的MARL评估套件，它利用强化学习来减少创建新测试场景所需的人力。这种方法之所以有效，是因为一个智能体的行为构成了另一个智能体环境的一部分。为了展示可扩展性，我们创建了超过80个独特的测试场景，涵盖了广泛的研究主题，如社会困境、互惠、资源共享和任务分区。我们将这些测试场景应用于标准的MARL训练算法，并展示了Melting Pot如何揭示仅从训练性能中无法看出的弱点。",
        "领域": "多智能体系统、强化学习、社会行为模拟",
        "问题": "评估多智能体强化学习在新情境下的泛化能力",
        "动机": "填补现有MARL评估套件在评估智能体对新情境泛化能力方面的空白，减少创建多样化测试场景的人力成本",
        "方法": "利用强化学习自动生成多样化的测试场景，通过一个智能体的行为作为另一个智能体环境的一部分，评估MARL算法在新情境下的表现",
        "关键词": [
            "多智能体强化学习",
            "评估框架",
            "泛化能力",
            "社会行为模拟",
            "自动化测试场景生成"
        ],
        "涉及的技术概念": {
            "多智能体强化学习": "论文中用于评估智能体在新情境下表现的核心技术",
            "泛化能力评估": "论文中通过多样化测试场景评估智能体适应新情境的能力",
            "自动化测试场景生成": "论文中利用强化学习减少创建测试场景的人力成本，提高评估的多样性和可扩展性"
        },
        "success": true
    },
    {
        "order": 965,
        "title": "Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9469",
        "abstract": "Marginal-likelihood based model-selection, even though promising, is rarely used in deep learning due to estimation difficulties. Instead, most approaches rely on validation data, which may not be readily available. In this work, we present a scalable marginal-likelihood estimation method to select both hyperparameters and network architectures, based on the training data alone. Some hyperparameters can be estimated online during training, simplifying the procedure. Our marginal-likelihood estimate is based on Laplace’s method and Gauss-Newton approximations to the Hessian, and it outperforms cross-validation and manual tuning on standard regression and image classification datasets, especially in terms of calibration and out-of-distribution detection. Our work shows that marginal likelihoods can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings).",
        "conference": "ICML",
        "中文标题": "深度学习模型选择中可扩展的边缘似然估计",
        "摘要翻译": "基于边缘似然的模型选择虽然前景广阔，但由于估计困难，在深度学习中很少使用。相反，大多数方法依赖于验证数据，而这些数据可能不易获得。在这项工作中，我们提出了一种可扩展的边缘似然估计方法，仅基于训练数据来选择超参数和网络架构。一些超参数可以在训练过程中在线估计，从而简化了流程。我们的边缘似然估计基于拉普拉斯方法和高斯-牛顿对Hessian矩阵的近似，在标准回归和图像分类数据集上，尤其是在校准和分布外检测方面，优于交叉验证和手动调优。我们的工作表明，边缘似然可以改善泛化能力，并在验证数据不可用时（例如，在非平稳设置中）非常有用。",
        "领域": "深度学习模型选择、超参数优化、神经网络架构搜索",
        "问题": "解决在深度学习中基于边缘似然进行模型选择的估计困难问题",
        "动机": "减少对验证数据的依赖，提供一种仅基于训练数据即可进行模型选择的方法",
        "方法": "采用拉普拉斯方法和高斯-牛顿近似Hessian矩阵来估计边缘似然，实现在线超参数估计",
        "关键词": [
            "边缘似然估计",
            "模型选择",
            "超参数优化",
            "神经网络架构",
            "拉普拉斯方法"
        ],
        "涉及的技术概念": {
            "边缘似然估计": "用于模型选择的一种方法，通过估计模型对数据的边缘似然来选择最优模型",
            "拉普拉斯方法": "一种近似积分的技术，用于估计边缘似然，特别是在高维空间中",
            "高斯-牛顿近似": "用于近似Hessian矩阵的技术，以简化边缘似然的计算过程"
        },
        "success": true
    },
    {
        "order": 966,
        "title": "Scalable Normalizing Flows for Permutation Invariant Densities",
        "html": "https://ICML.cc//virtual/2021/poster/10325",
        "abstract": "Modeling sets is an important problem in machine learning since this type of data can be found in many domains. A promising approach defines a family of permutation invariant densities with continuous normalizing flows. This allows us to maximize the likelihood directly and sample new realizations with ease. In this work, we demonstrate how calculating the trace, a crucial step in this method, raises issues that occur both during training and inference, limiting its practicality. We propose an alternative way of defining permutation equivariant transformations that give closed form trace. This leads not only to improvements while training, but also to better final performance. We demonstrate the benefits of our approach on point processes and general set modeling.",
        "conference": "ICML",
        "中文标题": "可扩展的排列不变密度归一化流",
        "摘要翻译": "在机器学习中，集合建模是一个重要问题，因为这类数据可以在许多领域中找到。一种有前景的方法通过连续归一化流定义了一族排列不变的密度。这使我们能够直接最大化似然并轻松采样新的实现。在这项工作中，我们展示了计算迹（这一方法中的关键步骤）如何在训练和推理过程中引发问题，限制了其实用性。我们提出了一种定义排列等变变换的替代方法，该方法给出了闭合形式的迹。这不仅带来了训练过程中的改进，也提高了最终性能。我们在点过程和一般集合建模上展示了我们方法的优势。",
        "领域": "概率图模型, 生成模型, 集合学习",
        "问题": "解决在排列不变密度建模中计算迹时出现的训练和推理问题",
        "动机": "提高排列不变密度建模的实用性和性能",
        "方法": "提出了一种定义排列等变变换的替代方法，该方法给出了闭合形式的迹",
        "关键词": [
            "归一化流",
            "排列不变性",
            "集合建模",
            "点过程",
            "概率密度估计"
        ],
        "涉及的技术概念": {
            "归一化流": "用于定义排列不变密度的连续变换，允许直接最大化似然和采样",
            "排列不变性": "确保模型输出不依赖于输入集合的顺序，适用于集合数据建模",
            "闭合形式迹": "提出的方法中关键步骤，解决了原方法在训练和推理中的问题，提高了效率和性能"
        },
        "success": true
    },
    {
        "order": 967,
        "title": "Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More",
        "html": "https://ICML.cc//virtual/2021/poster/8777",
        "abstract": "The current best practice for computing optimal transport (OT) is via entropy regularization and Sinkhorn iterations. This algorithm runs in quadratic time as it requires the full pairwise cost matrix, which is prohibitively expensive for large sets of objects. In this work we propose two effective log-linear time approximations of the cost matrix: First, a sparse approximation based on locality sensitive hashing (LSH) and, second, a Nyström approximation with LSH-based sparse corrections, which we call locally corrected Nyström (LCN). These approximations enable general log-linear time algorithms for entropy-regularized OT that perform well even for the complex, high-dimensional spaces common in deep learning. We analyse these approximations theoretically and evaluate them experimentally both directly and end-to-end as a component for real-world applications. Using our approximations for unsupervised word embedding alignment enables us to speed up a state-of-the-art method by a factor of 3 while also improving the accuracy by 3.1 percentage points without any additional model changes. For graph distance regression we propose the graph transport network (GTN), which combines graph neural networks (GNNs) with enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales log-linearly in the number of nodes.",
        "conference": "ICML",
        "中文标题": "高维可扩展最优传输在图距离、嵌入对齐等方面的应用",
        "摘要翻译": "当前计算最优传输（OT）的最佳实践是通过熵正则化和Sinkhorn迭代。该算法以二次时间运行，因为它需要完整的成对成本矩阵，这对于大型对象集来说是极其昂贵的。在这项工作中，我们提出了两种有效的对数线性时间成本矩阵近似方法：首先，基于局部敏感哈希（LSH）的稀疏近似；其次，我们称之为局部校正Nyström（LCN）的带有LSH稀疏校正的Nyström近似。这些近似使得熵正则化OT的通用对数线性时间算法成为可能，即使在深度学习中常见的高维复杂空间中也表现良好。我们从理论上分析了这些近似方法，并通过实验直接和端到端地评估了它们作为现实世界应用组件的性能。使用我们的近似方法进行无监督词嵌入对齐，使我们能够将最先进方法的速度提高3倍，同时在不进行任何额外模型更改的情况下，准确率提高了3.1个百分点。对于图距离回归，我们提出了图传输网络（GTN），它将图神经网络（GNNs）与增强的Sinkhorn相结合。GTN比之前的模型性能高出48%，并且在节点数量上仍然保持对数线性扩展。",
        "领域": "最优传输、图神经网络、词嵌入对齐",
        "问题": "解决在大规模数据集上计算最优传输的高成本问题",
        "动机": "为了在大规模和高维数据上高效计算最优传输，减少计算成本和时间",
        "方法": "提出了基于局部敏感哈希的稀疏近似和局部校正Nyström近似两种方法，用于高效计算最优传输",
        "关键词": [
            "最优传输",
            "局部敏感哈希",
            "Nyström近似",
            "图神经网络",
            "词嵌入对齐"
        ],
        "涉及的技术概念": {
            "熵正则化": "用于优化最优传输问题的计算，通过引入熵项来平滑传输计划",
            "局部敏感哈希（LSH）": "用于高效近似计算高维数据之间的距离，减少计算复杂度",
            "图传输网络（GTN）": "结合图神经网络和增强的Sinkhorn算法，用于图距离回归任务，提高性能和效率"
        },
        "success": true
    },
    {
        "order": 968,
        "title": "Scalable Variational Gaussian Processes via Harmonic Kernel Decomposition",
        "html": "https://ICML.cc//virtual/2021/poster/9537",
        "abstract": "We introduce a new scalable variational Gaussian process approximation which provides a high fidelity approximation while retaining general applicability. We propose the harmonic kernel decomposition (HKD), which uses Fourier series to decompose a kernel as a sum of orthogonal kernels. Our variational approximation exploits this orthogonality to enable a large number of inducing points at a low computational cost. We demonstrate that, on a range of regression and classification problems, our approach can exploit input space symmetries such as translations and reflections, and it significantly outperforms standard variational methods in scalability and accuracy. Notably, our approach achieves state-of-the-art results on CIFAR-10 among pure GP models.",
        "conference": "ICML",
        "中文标题": "可扩展的变分高斯过程通过谐波核分解",
        "摘要翻译": "我们引入了一种新的可扩展变分高斯过程近似方法，该方法在保持广泛适用性的同时提供了高保真度的近似。我们提出了谐波核分解（HKD），它使用傅里叶级数将核分解为正交核的和。我们的变分近似利用这种正交性，以较低的计算成本实现了大量诱导点。我们证明，在一系列回归和分类问题上，我们的方法可以利用输入空间的对称性，如平移和反射，并且在可扩展性和准确性上显著优于标准变分方法。值得注意的是，我们的方法在纯GP模型中在CIFAR-10上达到了最先进的结果。",
        "领域": "高斯过程、机器学习、模式识别",
        "问题": "如何在保持高斯过程模型广泛适用性的同时，提高其在大规模数据集上的可扩展性和准确性。",
        "动机": "为了解决高斯过程模型在大规模数据集上计算成本高和可扩展性差的问题，同时保持其高保真度和广泛适用性。",
        "方法": "提出谐波核分解（HKD）方法，利用傅里叶级数将核分解为正交核的和，通过变分近似利用这种正交性降低计算成本，提高模型的可扩展性和准确性。",
        "关键词": [
            "谐波核分解",
            "变分高斯过程",
            "傅里叶级数",
            "正交核",
            "可扩展性"
        ],
        "涉及的技术概念": {
            "谐波核分解（HKD）": "使用傅里叶级数将核分解为正交核的和，以降低计算成本并提高模型的可扩展性。",
            "变分近似": "利用谐波核分解中的正交性，实现大量诱导点的低计算成本处理，提高模型效率。",
            "输入空间对称性": "通过谐波核分解，模型能够利用输入空间的对称性（如平移和反射），提高模型的准确性和泛化能力。"
        },
        "success": true
    },
    {
        "order": 969,
        "title": "Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing",
        "html": "https://ICML.cc//virtual/2021/poster/8583",
        "abstract": "Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.",
        "conference": "ICML",
        "中文标题": "通过选择性参数共享扩展多智能体强化学习",
        "摘要翻译": "在多智能体深度强化学习中，参数共享在使算法能够扩展到大量智能体方面发挥了至关重要的作用。智能体之间的参数共享显著减少了可训练参数的数量，将训练时间缩短到可处理水平，并与更高效的学习相关联。然而，让所有智能体共享相同的参数也可能对学习产生不利影响。我们展示了参数共享方法对训练速度和收敛回报的影响，证明当不加区别地应用时，它们的有效性高度依赖于环境。我们提出了一种新方法，通过根据智能体的能力和目标对其进行分区，自动识别可能从参数共享中受益的智能体。我们的方法结合了参数共享增加的样本效率和多个独立网络的表示能力，以减少训练时间并提高最终回报。",
        "领域": "多智能体系统、深度强化学习、参数优化",
        "问题": "如何在多智能体强化学习中有效平衡参数共享与独立学习的需求，以提高训练效率和最终性能。",
        "动机": "探索参数共享在多智能体强化学习中的影响，提出一种方法来自动识别哪些智能体可以从参数共享中受益，以优化训练过程和结果。",
        "方法": "提出了一种基于智能体能力和目标的分区方法，自动识别适合参数共享的智能体，结合参数共享的样本效率和独立网络的表示能力。",
        "关键词": [
            "多智能体强化学习",
            "参数共享",
            "样本效率",
            "训练优化",
            "智能体分区"
        ],
        "涉及的技术概念": {
            "参数共享": "在多智能体强化学习中，多个智能体共享相同的模型参数，以减少训练参数数量和提高训练效率。",
            "样本效率": "指算法在训练过程中利用样本数据的能力，高效的样本利用可以减少训练所需的数据量和时间。",
            "智能体分区": "根据智能体的能力和目标将其分组，以便更有效地应用参数共享策略，优化学习过程。"
        },
        "success": true
    },
    {
        "order": 970,
        "title": "Scaling Properties of Deep Residual Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10287",
        "abstract": "Residual networks (ResNets) have displayed impressive results in pattern recognition and, recently, have garnered considerable theoretical interest due to a perceived link with neural ordinary differential equations (neural ODEs). This link relies on the convergence of network weights to a smooth function as the number of layers increases. We investigate the properties of weights trained by stochastic gradient descent and their scaling with network depth through detailed numerical experiments. We observe the existence of scaling regimes markedly different from those assumed in neural ODE literature. Depending on certain features of the network architecture, such as the smoothness of the activation function, one may obtain an alternative ODE limit, a stochastic differential equation or neither of these. These findings cast doubts on the validity of the neural ODE model as an adequate asymptotic description of deep ResNets and point to an alternative class of differential equations as a better description of the deep network limit.",
        "conference": "ICML",
        "中文标题": "深度残差网络的缩放特性",
        "摘要翻译": "残差网络（ResNets）在模式识别中展示了令人印象深刻的结果，并且最近由于与神经普通微分方程（神经ODE）的感知联系而引起了相当大的理论兴趣。这种联系依赖于随着层数增加，网络权重收敛到一个平滑函数。我们通过详细的数值实验研究了通过随机梯度下降训练的权重特性及其与网络深度的缩放关系。我们观察到了与神经ODE文献中假设的缩放机制明显不同的缩放机制的存在。根据网络架构的某些特征，如激活函数的平滑性，可能会得到一个替代的ODE极限、一个随机微分方程或者两者都不是。这些发现对神经ODE模型作为深度ResNets的充分渐近描述的有效性提出了质疑，并指出了一类替代的微分方程作为深度网络极限的更好描述。",
        "领域": "深度学习理论、神经网络架构、微分方程在深度学习中的应用",
        "问题": "探讨深度残差网络的权重缩放特性及其与神经ODE理论假设的差异",
        "动机": "理解深度残差网络在实际训练中的行为与理论模型（如神经ODE）之间的差异，以提供更准确的网络行为描述",
        "方法": "通过详细的数值实验分析随机梯度下降训练的权重特性及其与网络深度的缩放关系",
        "关键词": [
            "残差网络",
            "神经ODE",
            "权重缩放",
            "随机梯度下降",
            "微分方程"
        ],
        "涉及的技术概念": {
            "残差网络（ResNets）": "一种深度学习架构，通过引入残差连接解决了深层网络训练中的梯度消失问题",
            "神经普通微分方程（神经ODE）": "将神经网络的前向传播过程建模为微分方程的求解过程，为理解网络行为提供了新的理论框架",
            "随机梯度下降（SGD）": "一种常用的优化算法，用于训练深度学习模型，通过迭代更新模型参数以最小化损失函数"
        },
        "success": true
    },
    {
        "order": 971,
        "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision",
        "html": "https://ICML.cc//virtual/2021/poster/10657",
        "abstract": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text  without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages.  For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",
        "conference": "ICML",
        "中文标题": "利用噪声文本监督扩展视觉与视觉-语言表示学习",
        "摘要翻译": "预训练表示对于许多自然语言处理（NLP）和感知任务变得至关重要。尽管NLP中的表示学习已经转向无需人工标注的原始文本训练，视觉和视觉-语言表示仍然严重依赖于昂贵的或需要专家知识的精心策划的训练数据集。对于视觉应用，表示大多使用具有明确类别标签的数据集（如ImageNet或OpenImages）进行学习。对于视觉-语言，流行的数据集如Conceptual Captions、MSCOCO或CLIP都涉及非平凡的数据收集（和清理）过程。这种昂贵的策划过程限制了数据集的规模，从而阻碍了训练模型的扩展。在本文中，我们利用了一个超过十亿图像替代文本对的噪声数据集，这些数据在Conceptual Captions数据集中无需昂贵的过滤或后处理步骤即可获得。一个简单的双编码器架构学习使用对比损失对齐图像和文本对的视觉和语言表示。我们展示了我们语料库的规模可以弥补其噪声，并导致即使使用如此简单的学习方案也能达到最先进的表示。我们的视觉表示在转移到分类任务（如ImageNet和VTAB）时表现出强大的性能。对齐的视觉和语言表示实现了零样本图像分类，并在Flickr30K和MSCOCO图像-文本检索基准上设置了新的最先进结果，即使与更复杂的交叉注意力模型相比也是如此。这些表示还支持使用复杂文本和文本+图像查询的跨模态搜索。",
        "领域": "视觉-语言表示学习、零样本学习、图像-文本检索",
        "问题": "如何利用噪声文本监督扩展视觉与视觉-语言表示学习，以降低数据收集和清理的成本",
        "动机": "减少对昂贵或需要专家知识的精心策划训练数据集的依赖，通过利用大规模噪声数据集来扩展视觉和视觉-语言表示学习",
        "方法": "采用简单的双编码器架构和对比损失，利用超过十亿图像替代文本对的噪声数据集进行训练",
        "关键词": [
            "视觉-语言表示学习",
            "噪声文本监督",
            "零样本学习",
            "图像-文本检索",
            "对比损失"
        ],
        "涉及的技术概念": {
            "双编码器架构": "用于对齐图像和文本对的视觉和语言表示的简单架构",
            "对比损失": "用于训练模型以区分正负样本对的损失函数，有助于学习有意义的表示",
            "零样本图像分类": "利用对齐的视觉和语言表示，无需特定类别训练数据即可进行分类的能力"
        },
        "success": true
    },
    {
        "order": 972,
        "title": "SCC: an efficient deep reinforcement learning agent mastering the game of StarCraft II",
        "html": "https://ICML.cc//virtual/2021/poster/9579",
        "abstract": "AlphaStar, the AI that reaches GrandMaster level in StarCraft II, is a remarkable milestone demonstrating what deep reinforcement learning can achieve in complex Real-Time Strategy (RTS) games. However, the complexities of the game, algorithms and systems, and especially the tremendous amount of computation needed are big obstacles for the community to conduct further research in this direction. We propose a deep reinforcement learning agent, StarCraft Commander (SCC). With order of magnitude less computation, it demonstrates top human performance defeating GrandMaster players in test matches and top professional players in a live event. Moreover, it shows strong robustness to various human strategies and discovers novel strategies unseen from human plays. In this paper, we’ll share the key insights and optimizations on efficient imitation learning and reinforcement learning for StarCraft II full game.",
        "conference": "ICML",
        "中文标题": "SCC：一种高效掌握《星际争霸II》游戏的深度强化学习智能体",
        "摘要翻译": "AlphaStar是在《星际争霸II》中达到宗师级别的AI，它是深度强化学习在复杂实时策略（RTS）游戏中能取得成就的显著里程碑。然而，游戏的复杂性、算法和系统，尤其是所需的大量计算，是社区在这一方向进行进一步研究的大障碍。我们提出了一种深度强化学习智能体，星际争霸指挥官（SCC）。它以数量级更少的计算，展示了击败测试赛中宗师级玩家和现场活动中顶级职业玩家的顶级人类表现。此外，它对各种人类策略表现出强大的鲁棒性，并发现了人类游戏中未见的新策略。在本文中，我们将分享关于《星际争霸II》完整游戏高效模仿学习和强化学习的关键见解和优化。",
        "领域": "深度强化学习、实时策略游戏AI、模仿学习",
        "问题": "如何在减少计算资源消耗的同时，开发出能够在《星际争霸II》中达到顶级人类玩家水平的AI。",
        "动机": "克服现有AI在《星际争霸II》中因高计算需求而难以进一步研究和应用的障碍。",
        "方法": "提出了一种名为星际争霸指挥官（SCC）的深度强化学习智能体，通过高效的模仿学习和强化学习优化，以较少的计算资源实现顶级人类表现。",
        "关键词": [
            "深度强化学习",
            "星际争霸II",
            "模仿学习",
            "实时策略游戏",
            "AI"
        ],
        "涉及的技术概念": {
            "深度强化学习": "论文中用于训练AI智能体在复杂环境中做出决策的核心技术。",
            "模仿学习": "论文中用于从人类玩家数据中学习策略，以加速和优化AI训练过程的技术。",
            "实时策略游戏AI": "论文中研究的特定应用领域，专注于开发能够在《星际争霸II》等实时策略游戏中达到人类顶级水平的AI。"
        },
        "success": true
    },
    {
        "order": 973,
        "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies",
        "html": "https://ICML.cc//virtual/2021/poster/9317",
        "abstract": "Generalization has been a long-standing challenge for reinforcement learning (RL). Visual RL, in particular, can be easily distracted by irrelevant factors in high-dimensional observation space. In this work, we consider robust policy learning which targets zero-shot generalization to unseen visual environments with large distributional shift. We propose SECANT, a novel self-expert cloning technique that leverages image augmentation in two stages to *decouple* robust representation learning from policy optimization. Specifically, an expert policy is first trained by RL from scratch with weak augmentations. A student network then learns to mimic the expert policy by supervised learning with strong augmentations, making its representation more robust against visual variations compared to the expert.\nExtensive experiments demonstrate that SECANT significantly advances the state of the art in zero-shot generalization across 4 challenging domains. Our average reward improvements over prior SOTAs are:  DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based autonomous driving (+47.7%), and indoor object navigation (+15.8%). Code release and video are available at https://linxifan.github.io/secant-site/.",
        "conference": "ICML",
        "中文标题": "SECANT：视觉策略零样本泛化的自我专家克隆技术",
        "摘要翻译": "泛化一直是强化学习（RL）中长期存在的挑战。特别是视觉RL，在高维观察空间中容易被无关因素分散注意力。在这项工作中，我们考虑了针对具有大分布偏移的未见视觉环境的零样本泛化的鲁棒策略学习。我们提出了SECANT，一种新颖的自我专家克隆技术，该技术利用图像增强在两个阶段*解耦*鲁棒表示学习与策略优化。具体来说，首先通过RL从头开始训练一个专家策略，使用弱增强。然后，一个学生网络通过监督学习使用强增强来模仿专家策略，使其表示相比于专家对视觉变化更加鲁棒。大量实验表明，SECANT在4个挑战性领域的零样本泛化方面显著推进了现有技术水平。我们相对于先前SOTAs的平均奖励改进为：DeepMind控制（+26.5%）、机器人操作（+337.8%）、基于视觉的自动驾驶（+47.7%）和室内物体导航（+15.8%）。代码发布和视频可在https://linxifan.github.io/secant-site/获取。",
        "领域": "强化学习、机器人视觉控制、自动驾驶",
        "问题": "解决视觉强化学习在零样本泛化方面的挑战，特别是在面对大分布偏移的未见视觉环境时。",
        "动机": "提高视觉强化学习模型在未见环境中的泛化能力，减少对无关视觉因素的敏感性。",
        "方法": "提出SECANT技术，通过两阶段的图像增强（弱增强训练专家策略，强增强训练学生网络模仿专家策略）来解耦鲁棒表示学习与策略优化。",
        "关键词": [
            "零样本泛化",
            "自我专家克隆",
            "视觉强化学习",
            "图像增强",
            "策略优化"
        ],
        "涉及的技术概念": {
            "自我专家克隆": "通过训练学生网络模仿专家策略来提高模型对视觉变化的鲁棒性。",
            "图像增强": "用于在训练过程中引入视觉变化，增强模型的泛化能力。",
            "零样本泛化": "指模型在训练阶段未接触过的环境或任务上的表现能力。"
        },
        "success": true
    },
    {
        "order": 974,
        "title": "Segmenting Hybrid Trajectories using Latent ODEs",
        "html": "https://ICML.cc//virtual/2021/poster/9545",
        "abstract": "Smooth dynamics interrupted by discontinuities are known as hybrid systems and arise commonly in nature. Latent ODEs allow for powerful representation of irregularly sampled time series but are not designed to capture trajectories arising from hybrid systems. Here, we propose the Latent Segmented ODE (LatSegODE), which uses Latent ODEs to perform reconstruction and changepoint detection within hybrid trajectories featuring jump discontinuities and switching dynamical modes. Where it is possible to train a Latent ODE on the smooth dynamical flows between discontinuities, we apply the pruned exact linear time (PELT) algorithm to detect changepoints where latent dynamics restart, thereby maximizing the joint probability of a piece-wise continuous latent dynamical representation. We propose usage of the marginal likelihood as a score function for PELT, circumventing the need for model-complexity-based penalization. The LatSegODE outperforms baselines in reconstructive and segmentation tasks including synthetic data sets of sine waves, Lotka Volterra dynamics, and UCI Character Trajectories.",
        "conference": "ICML",
        "success": true,
        "中文标题": "使用潜在常微分方程分割混合轨迹",
        "摘要翻译": "由间断打断的平滑动态被称为混合系统，在自然界中普遍存在。潜在常微分方程（Latent ODEs）能够强大地表示不规则采样的时间序列，但并不设计用于捕捉来自混合系统的轨迹。在此，我们提出了潜在分段常微分方程（LatSegODE），它使用潜在常微分方程在具有跳跃间断和切换动态模式的混合轨迹内进行重建和变化点检测。在可以在间断之间的平滑动态流上训练潜在常微分方程的情况下，我们应用修剪精确线性时间（PELT）算法来检测潜在动态重新开始的变化点，从而最大化分段连续潜在动态表示的联合概率。我们提出使用边际似然作为PELT的评分函数，避免了基于模型复杂性的惩罚需求。LatSegODE在包括正弦波、Lotka Volterra动态和UCI字符轨迹的合成数据集的重建和分割任务中优于基线。",
        "领域": "时间序列分析, 动态系统建模, 变化点检测",
        "问题": "如何有效地分割和重建由间断打断的混合动态系统的轨迹",
        "动机": "现有的潜在常微分方程方法无法有效处理混合系统中的轨迹分割和重建问题",
        "方法": "提出潜在分段常微分方程（LatSegODE），结合潜在常微分方程和PELT算法进行轨迹重建和变化点检测",
        "关键词": [
            "潜在常微分方程",
            "变化点检测",
            "混合系统",
            "轨迹分割",
            "动态建模"
        ],
        "涉及的技术概念": {
            "潜在常微分方程": "用于表示不规则采样时间序列的强大工具，能够捕捉动态系统的连续演变",
            "PELT算法": "一种高效的变化点检测算法，用于识别时间序列中的结构变化点",
            "边际似然": "作为评分函数，用于评估模型拟合度，避免了对模型复杂性的直接惩罚"
        }
    },
    {
        "order": 975,
        "title": "Selecting Data Augmentation for Simulating Interventions",
        "html": "https://ICML.cc//virtual/2021/poster/8887",
        "abstract": "Machine learning models trained with purely observational data and the principle of empirical risk minimization (Vapnik 1992) can fail to generalize to unseen domains. In this paper, we focus on the case where the problem arises through spurious correlation between the observed domains and the actual task labels. We find that many domain generalization methods do not explicitly take this spurious correlation into account. Instead, especially in more application-oriented research areas like medical imaging or robotics, data augmentation techniques that are based on heuristics are used to learn domain invariant features. To bridge the gap between theory and practice, we develop a causal perspective on the problem of domain generalization. We argue that causal concepts can be used to explain the success of data augmentation by describing how they can weaken the spurious correlation between the observed domains and the task labels. We demonstrate that data augmentation can serve as a tool for simulating interventional data. We use these theoretical insights to derive a simple algorithm that is able to select data augmentation techniques that will lead to better domain generalization.",
        "conference": "ICML",
        "中文标题": "选择数据增强以模拟干预",
        "摘要翻译": "仅使用观测数据和经验风险最小化原则（Vapnik 1992）训练的机器学习模型可能无法泛化到未见过的领域。在本文中，我们关注的问题是由于观察到的领域与实际任务标签之间的虚假相关性导致的。我们发现许多领域泛化方法没有明确考虑这种虚假相关性。相反，特别是在医学成像或机器人等更偏向应用的研究领域，基于启发式的数据增强技术被用来学习领域不变特征。为了弥合理论与实践之间的差距，我们开发了一个关于领域泛化问题的因果视角。我们认为因果概念可以用来解释数据增强的成功，通过描述它们如何能够削弱观察到的领域与任务标签之间的虚假相关性。我们证明数据增强可以作为模拟干预数据的工具。我们利用这些理论见解推导出一个简单的算法，该算法能够选择将导致更好领域泛化的数据增强技术。",
        "领域": "领域泛化、医学成像、机器人技术",
        "问题": "解决机器学习模型因观测数据与任务标签间的虚假相关性而无法泛化到新领域的问题",
        "动机": "探索数据增强技术如何通过削弱虚假相关性来提升模型的领域泛化能力",
        "方法": "采用因果视角分析领域泛化问题，并提出一个基于理论的选择数据增强技术的算法",
        "关键词": [
            "领域泛化",
            "数据增强",
            "因果视角",
            "虚假相关性",
            "干预数据"
        ],
        "涉及的技术概念": {
            "经验风险最小化": "用于训练机器学习模型的原则，可能导致模型无法泛化到新领域",
            "虚假相关性": "观察到的领域与任务标签之间的非因果关联，影响模型的泛化能力",
            "因果视角": "提供了一种理解数据增强如何通过干预削弱虚假相关性的框架"
        },
        "success": true
    },
    {
        "order": 976,
        "title": "Self-Damaging Contrastive Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10081",
        "abstract": "The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imbalanced and shows a long-tail distribution, and it is unclear how robustly the latest contrastive learning methods could perform in the practical scenario. This paper proposes to explicitly tackle this challenge, via a principled framework called Self-Damaging Contrastive Learning (SDCLR), to automatically balance the representation learning without knowing the classes. Our main inspiration is drawn from the recent finding that deep models have difficult-to-memorize samples, and those may be exposed through network pruning. It is further natural to hypothesize that long-tail samples are also tougher for the model to learn well due to insufficient examples. Hence, the key innovation in SDCLR is to create a dynamic self-competitor model to contrast with the target model, which is a pruned version of the latter. During training, contrasting the two models will lead to adaptive online mining of the most easily forgotten samples for the current target model, and implicitly emphasize them more in the contrastive loss. Extensive experiments across multiple datasets and imbalance settings show that SDCLR significantly improves not only overall accuracies but also balancedness, in terms of linear evaluation on the full-shot and few-shot settings. Our code is available at https://github.com/VITA-Group/SDCLR.",
        "conference": "ICML",
        "中文标题": "自损对比学习",
        "摘要翻译": "对比学习最近取得的突破加速了在现实世界数据应用上部署无监督训练的进程。然而，现实中的未标记数据通常是不平衡的，呈现出长尾分布，目前尚不清楚最新的对比学习方法在实际场景中能有多稳健的表现。本文提出通过一个称为自损对比学习（SDCLR）的原则性框架来明确解决这一挑战，以在不知道类别的情况下自动平衡表示学习。我们的主要灵感来源于最近的发现，即深度模型有难以记忆的样本，这些样本可能通过网络剪枝暴露出来。进一步自然地假设，由于样本不足，长尾样本对模型来说也更难学习。因此，SDCLR的关键创新是创建一个动态的自我竞争模型与目标模型对比，后者是前者的剪枝版本。在训练过程中，对比这两个模型将导致对当前目标模型最容易被遗忘的样本进行自适应在线挖掘，并在对比损失中隐式地更加强调它们。在多个数据集和不平衡设置下的广泛实验表明，SDCLR不仅在整体准确率上显著提高，而且在全面射击和少射击设置的线性评估方面也提高了平衡性。我们的代码可在https://github.com/VITA-Group/SDCLR获取。",
        "领域": "无监督学习、长尾学习、对比学习",
        "问题": "解决在数据不平衡和长尾分布的实际场景中，对比学习方法稳健性不足的问题。",
        "动机": "探索深度模型在长尾分布数据上的学习能力，特别是在样本不足的情况下如何提高模型的泛化能力和平衡性。",
        "方法": "提出自损对比学习（SDCLR）框架，通过创建动态自我竞争模型与目标模型对比，自适应在线挖掘易遗忘样本，并在对比损失中隐式强调这些样本。",
        "关键词": [
            "自损对比学习",
            "长尾学习",
            "无监督学习",
            "网络剪枝",
            "动态自我竞争模型"
        ],
        "涉及的技术概念": {
            "对比学习": "一种无监督学习方法，通过对比正负样本来学习数据的表示。",
            "网络剪枝": "一种减少神经网络复杂度的方法，用于识别和移除网络中不重要的部分，提高模型效率。",
            "动态自我竞争模型": "SDCLR中提出的创新概念，通过剪枝目标模型创建一个动态变化的竞争模型，用于在线挖掘易遗忘样本。"
        },
        "success": true
    },
    {
        "order": 977,
        "title": "Self-Improved Retrosynthetic Planning",
        "html": "https://ICML.cc//virtual/2021/poster/10749",
        "abstract": "Retrosynthetic planning is a fundamental problem in chemistry for finding a pathway of reactions to synthesize a target molecule. Recently, search algorithms have shown promising results for solving this problem by using deep neural networks (DNNs) to expand their candidate solutions, i.e., adding new reactions to reaction pathways. However, the existing works on this line are suboptimal; the retrosynthetic planning problem requires the reaction pathways to be (a) represented by real-world reactions and (b) executable using “building block” molecules, yet the DNNs expand reaction pathways without fully incorporating such requirements. Motivated by this, we propose an end-to-end framework for directly training the DNNs towards generating reaction pathways with the desirable properties. Our main idea is based on a self-improving procedure that trains the model to imitate successful trajectories found by itself. We also propose a novel reaction augmentation scheme based on a forward reaction model. Our experiments demonstrate that our scheme significantly improves the success rate of solving the retrosynthetic problem from 86.84% to 96.32% while maintaining the performance of DNN for predicting valid reactions.",
        "conference": "ICML",
        "中文标题": "自我改进的逆合成规划",
        "摘要翻译": "逆合成规划是化学中的一个基本问题，旨在找到合成目标分子的反应路径。最近，搜索算法通过使用深度神经网络（DNNs）扩展候选解决方案（即向反应路径中添加新反应）来解决这一问题，显示出有希望的结果。然而，这一领域的现有工作并不理想；逆合成规划问题要求反应路径（a）由真实世界的反应表示，并且（b）使用“构建块”分子可执行，但DNNs在扩展反应路径时并未完全纳入这些要求。受此启发，我们提出了一个端到端框架，用于直接训练DNNs以生成具有理想属性的反应路径。我们的主要想法基于一个自我改进的过程，该过程训练模型模仿其自身找到的成功轨迹。我们还提出了一种基于正向反应模型的新型反应增强方案。我们的实验表明，我们的方案将解决逆合成问题的成功率从86.84%显著提高到96.32%，同时保持了DNN预测有效反应的性能。",
        "领域": "化学信息学、逆合成分析、深度学习应用",
        "问题": "如何提高逆合成规划中反应路径的实用性和执行效率",
        "动机": "现有方法在生成反应路径时未能充分考虑真实世界反应的可执行性和构建块分子的使用，导致结果不理想",
        "方法": "提出一个端到端框架，通过自我改进过程和新型反应增强方案直接训练DNNs生成具有理想属性的反应路径",
        "关键词": [
            "逆合成规划",
            "深度神经网络",
            "反应增强",
            "自我改进",
            "化学信息学"
        ],
        "涉及的技术概念": {
            "深度神经网络（DNNs）": "用于扩展候选解决方案，即向反应路径中添加新反应",
            "自我改进过程": "训练模型模仿其自身找到的成功轨迹，以提高生成反应路径的质量",
            "正向反应模型": "用于新型反应增强方案，以提高反应路径的实用性和执行效率"
        },
        "success": true
    },
    {
        "order": 978,
        "title": "Selfish Sparse RNN Training",
        "html": "https://ICML.cc//virtual/2021/poster/9375",
        "abstract": "Sparse neural networks have been widely applied to reduce the computational demands of training and deploying over-parameterized deep neural networks. For inference acceleration, methods that discover a sparse network from a pre-trained dense network (dense-to-sparse training) work effectively. Recently, dynamic sparse training (DST) has been proposed to train sparse neural networks without pre-training a dense model (sparse-to-sparse training), so that the training process can also be accelerated. However, previous sparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs) and Convolutional Neural Networks (CNNs), failing to match the performance of dense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In this paper, we propose an approach to train intrinsically sparse RNNs with a fixed parameter count in one single run, without compromising performance. During training, we allow RNN layers to have a non-uniform redistribution across cell gates for better regularization. Further, we propose SNT-ASGD, a novel variant of the averaged stochastic gradient optimizer, which significantly improves the performance of all sparse training methods for RNNs. Using these strategies, we achieve state-of-the-art sparse training results, better than the dense-to-sparse methods, with various types of RNNs on Penn TreeBank and Wikitext-2 datasets. Our codes are available at https://github.com/Shiweiliuiiiiiii/Selfish-RNN.",
        "conference": "ICML",
        "中文标题": "自私稀疏RNN训练",
        "摘要翻译": "稀疏神经网络已被广泛应用于减少训练和部署过参数化深度神经网络的计算需求。为了加速推理，从预训练的密集网络中发现稀疏网络（密集到稀疏训练）的方法效果显著。最近，动态稀疏训练（DST）被提出，无需预训练密集模型即可训练稀疏神经网络（稀疏到稀疏训练），从而也能加速训练过程。然而，之前的稀疏到稀疏方法主要集中于多层感知器网络（MLPs）和卷积神经网络（CNNs），在循环神经网络（RNNs）设置中未能匹配密集到稀疏方法的性能。在本文中，我们提出了一种方法，以固定参数数量在单次运行中训练本质稀疏的RNNs，而不影响性能。在训练过程中，我们允许RNN层在细胞门之间进行非均匀重新分配，以实现更好的正则化。此外，我们提出了SNT-ASGD，一种平均随机梯度优化器的新变体，显著提高了所有RNNs稀疏训练方法的性能。使用这些策略，我们在Penn TreeBank和Wikitext-2数据集上实现了最先进的稀疏训练结果，优于密集到稀疏方法，适用于各种类型的RNNs。我们的代码可在https://github.com/Shiweiliuiiiiiii/Selfish-RNN获取。",
        "领域": "循环神经网络优化、稀疏训练、深度学习效率提升",
        "问题": "如何在循环神经网络（RNNs）中实现高效的稀疏训练，而不需要预训练密集模型，同时保持或超越密集到稀疏方法的性能。",
        "动机": "现有的稀疏到稀疏训练方法在RNNs中性能不佳，无法与密集到稀疏方法竞争，限制了RNNs在资源受限环境下的应用潜力。",
        "方法": "提出了一种固定参数数量的稀疏RNN训练方法，允许RNN层在细胞门之间进行非均匀参数重新分配，并开发了一种新的优化器变体SNT-ASGD，以提高训练性能。",
        "关键词": [
            "稀疏训练",
            "循环神经网络",
            "动态稀疏训练",
            "SNT-ASGD",
            "非均匀参数分配"
        ],
        "涉及的技术概念": {
            "动态稀疏训练（DST）": "一种无需预训练密集模型即可直接训练稀疏神经网络的方法，旨在加速训练过程。",
            "非均匀参数重新分配": "在训练过程中，允许RNN层的参数在不同细胞门之间非均匀分配，以提高模型的表达能力和正则化效果。",
            "SNT-ASGD": "一种新型的平均随机梯度优化器变体，专门设计用于提高稀疏RNN训练的性能和稳定性。"
        },
        "success": true
    },
    {
        "order": 979,
        "title": "Self Normalizing Flows",
        "html": "https://ICML.cc//virtual/2021/poster/10717",
        "abstract": "Efficient gradient computation of the Jacobian determinant term is a core problem in many machine learning settings, and especially so in the normalizing flow framework. Most proposed flow models therefore either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. In this work, we propose \\emph{Self Normalizing Flows}, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer's exact update from $\\mathcal{O}(D^3)$ to $\\mathcal{O}(D^2)$, allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling. We show experimentally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while training more quickly and surpassing the performance of functionally constrained counterparts. ",
        "conference": "ICML",
        "中文标题": "自归一化流",
        "摘要翻译": "在许多机器学习设置中，尤其是归一化流框架中，雅可比行列式项的高效梯度计算是一个核心问题。因此，大多数提出的流模型要么限制于一个易于评估雅可比行列式的函数类，要么限制于其高效估计器。然而，这些限制限制了此类密度模型的性能，常常需要显著的深度才能达到期望的性能水平。在这项工作中，我们提出了自归一化流，这是一个灵活的框架，用于通过在每个层中用学习到的近似逆替换梯度中的昂贵项来训练归一化流。这将每个层的精确更新的计算复杂度从O(D^3)降低到O(D^2)，允许训练那些在计算上原本不可行的流架构，同时也提供了高效的采样。我们通过实验表明，这样的模型非常稳定，并且优化到与其精确梯度对应物相似的数据似然值，同时训练更快，并超越了功能受限对应物的性能。",
        "领域": "归一化流、深度学习优化、概率密度估计",
        "问题": "解决归一化流中雅可比行列式项梯度计算的高效性问题",
        "动机": "提高归一化流模型的训练效率和性能，减少计算复杂度",
        "方法": "提出自归一化流框架，通过在每个层中用学习到的近似逆替换梯度中的昂贵项，降低计算复杂度",
        "关键词": [
            "自归一化流",
            "雅可比行列式",
            "计算复杂度",
            "深度学习优化",
            "概率密度估计"
        ],
        "涉及的技术概念": {
            "归一化流": "一种用于概率密度估计的生成模型，通过一系列可逆变换将简单分布转换为复杂分布",
            "雅可比行列式": "在归一化流中用于计算变换后概率密度的关键项，其计算复杂度直接影响模型的训练效率",
            "近似逆": "在自归一化流中用于替换梯度中昂贵项的技术，显著降低了计算复杂度，提高了训练效率"
        },
        "success": true
    },
    {
        "order": 980,
        "title": "Self-Paced Context Evaluation for Contextual Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9685",
        "abstract": "Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging.\nTo improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE).\nBased on self-paced learning, SPaCE automatically generates instance curricula online with little computational overhead. \nTo this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new \\tasks from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. \nWe demonstrate SPaCE's ability to speed up learning of different value-based RL agents on two environments, showing \nbetter generalization capabilities and\nup to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.",
        "conference": "ICML",
        "中文标题": "上下文强化学习中的自定进度上下文评估",
        "摘要翻译": "强化学习（RL）在解决给定环境中的单一问题方面取得了许多进展；但学习能够推广到问题未见变体的策略仍然具有挑战性。为了提高在此类问题域实例上学习的样本效率，我们提出了自定进度上下文评估（SPaCE）。基于自定进度学习，SPaCE以极少的计算开销在线自动生成实例课程。为此，SPaCE利用训练期间状态值中包含的信息，以加速和提高训练性能，以及对新任务（来自同一问题域）的泛化能力。尽管如此，SPaCE与当前问题域无关，可以应用于任何具有状态值函数近似的RL代理之上。我们在两个环境中展示了SPaCE加速不同基于值的RL代理学习的能力，显示出更好的泛化能力，并且与最接近的现有技术方法如轮询或SPDRL相比，学习速度提高了10倍。",
        "领域": "强化学习、泛化能力提升、样本效率优化",
        "问题": "如何提高强化学习策略在未见问题变体上的泛化能力和样本效率",
        "动机": "解决强化学习在泛化到新任务时样本效率低下的问题",
        "方法": "提出自定进度上下文评估（SPaCE），基于自定进度学习自动生成实例课程，利用状态值信息加速训练并提高泛化能力",
        "关键词": [
            "自定进度学习",
            "上下文评估",
            "强化学习",
            "泛化能力",
            "样本效率"
        ],
        "涉及的技术概念": {
            "自定进度学习": "一种学习策略，通过自动调整学习难度来优化学习过程",
            "状态值函数近似": "用于估计给定状态下预期回报的技术，是SPaCE加速训练和提高泛化能力的基础",
            "实例课程": "一系列按难度排序的学习实例，SPaCE自动生成这些课程以优化学习效率"
        },
        "success": true
    },
    {
        "order": 981,
        "title": "Self-supervised and Supervised Joint Training for Resource-rich Machine Translation",
        "html": "https://ICML.cc//virtual/2021/poster/9673",
        "abstract": "Self-supervised pre-training of text representations has been successfully applied to low-resource Neural Machine Translation (NMT). However, it usually fails to achieve notable gains on resource-rich NMT. In this paper, we propose a joint training approach, F2-XEnDec, to combine self-supervised and supervised learning to optimize NMT models. To exploit complementary self-supervised signals for supervised learning, NMT models are trained on examples that are interbred from monolingual and parallel sentences through a new process called crossover encoder-decoder. Experiments on two resource-rich translation benchmarks, WMT'14 English-German and WMT'14 English-French, demonstrate that our approach achieves substantial improvements over several strong baseline methods and obtains a new state of the art of 46.19 BLEU on English-French when incorporating back translation. Results also show that our approach is capable of improving model robustness to input perturbations such as code-switching noise which frequently appears on the social media.",
        "conference": "ICML",
        "中文标题": "自监督与监督联合训练用于资源丰富的机器翻译",
        "摘要翻译": "文本表示的自监督预训练已成功应用于低资源神经机器翻译（NMT）。然而，在资源丰富的NMT上，它通常无法取得显著成效。本文提出了一种联合训练方法F2-XEnDec，将自监督学习与监督学习相结合以优化NMT模型。为了利用自监督信号对监督学习的补充作用，NMT模型通过一种称为交叉编码器-解码器的新过程，在由单语和并行句子杂交而来的示例上进行训练。在WMT'14英语-德语和WMT'14英语-法语这两个资源丰富的翻译基准上的实验表明，我们的方法在结合反向翻译时，比几种强基线方法有显著改进，并在英语-法语翻译上达到了46.19 BLEU的新最高水平。结果还显示，我们的方法能够提高模型对输入扰动（如社交媒体上频繁出现的代码转换噪声）的鲁棒性。",
        "领域": "神经机器翻译、自监督学习、多语言处理",
        "问题": "在资源丰富的机器翻译任务中，自监督预训练方法通常无法取得显著成效。",
        "动机": "探索如何将自监督学习与监督学习相结合，以优化资源丰富的神经机器翻译模型。",
        "方法": "提出了一种联合训练方法F2-XEnDec，通过交叉编码器-解码器过程在由单语和并行句子杂交而来的示例上训练NMT模型。",
        "关键词": [
            "自监督学习",
            "神经机器翻译",
            "联合训练",
            "交叉编码器-解码器",
            "模型鲁棒性"
        ],
        "涉及的技术概念": {
            "自监督预训练": "用于从大量未标注数据中学习文本表示，为低资源NMT提供初始模型参数。",
            "交叉编码器-解码器": "一种新过程，通过将单语和并行句子杂交生成训练示例，以结合自监督和监督信号。",
            "反向翻译": "一种数据增强技术，用于提高翻译模型的质量和鲁棒性。"
        },
        "success": true
    },
    {
        "order": 982,
        "title": "Self-supervised Graph-level Representation Learning with Local and Global Structure",
        "html": "https://ICML.cc//virtual/2021/poster/10691",
        "abstract": "This paper studies unsupervised/self-supervised whole-graph representation learning, which is critical in many tasks such as molecule properties prediction in drug and material discovery. Existing methods mainly focus on preserving the local similarity structure between different graph instances but fail to discover the global semantic structure of the entire data set. In this paper, we propose a unified framework called Local-instance and Global-semantic Learning (GraphLoG) for self-supervised whole-graph representation learning. Specifically, besides preserving the local similarities, GraphLoG introduces the hierarchical prototypes to capture the global semantic clusters. An efficient online expectation-maximization (EM) algorithm is further developed for learning the model. We evaluate GraphLoG by pre-training it on massive unlabeled graphs followed by fine-tuning on downstream tasks. Extensive experiments on both chemical and biological benchmark data sets demonstrate the effectiveness of the proposed approach. ",
        "conference": "ICML",
        "中文标题": "基于局部与全局结构的自监督图级表示学习",
        "摘要翻译": "本文研究了无监督/自监督的整图表示学习，这在药物和材料发现中的分子性质预测等许多任务中至关重要。现有方法主要侧重于保留不同图实例之间的局部相似性结构，但未能发现整个数据集的全局语义结构。在本文中，我们提出了一个名为局部实例与全局语义学习（GraphLoG）的统一框架，用于自监督的整图表示学习。具体来说，除了保留局部相似性外，GraphLoG引入了分层原型来捕捉全局语义簇。进一步开发了一种高效的在线期望最大化（EM）算法来学习模型。我们通过在大量未标记图上预训练GraphLoG，然后在下游任务上进行微调来评估其性能。在化学和生物基准数据集上的大量实验证明了所提方法的有效性。",
        "领域": "图表示学习、分子性质预测、自监督学习",
        "问题": "如何在无监督/自监督的整图表示学习中同时保留局部相似性结构和发现全局语义结构。",
        "动机": "现有方法在整图表示学习中主要关注局部相似性结构，忽略了全局语义结构的发现，限制了表示学习的全面性和应用效果。",
        "方法": "提出了一个统一框架GraphLoG，通过引入分层原型捕捉全局语义簇，并结合在线EM算法进行模型学习，实现了局部相似性和全局语义结构的共同保留。",
        "关键词": [
            "图表示学习",
            "自监督学习",
            "全局语义结构",
            "分层原型",
            "在线EM算法"
        ],
        "涉及的技术概念": {
            "分层原型": "用于捕捉和表示数据中的全局语义簇，增强模型对数据全局结构的理解能力。",
            "在线期望最大化（EM）算法": "用于高效学习模型参数，特别是在处理大规模未标记数据时，提高模型的训练效率和适应性。",
            "局部相似性结构": "指不同图实例之间的相似性关系，是图表示学习中保留图实例间局部一致性的关键。"
        },
        "success": true
    },
    {
        "order": 983,
        "title": "Self-Tuning for Data-Efficient Deep Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8615",
        "abstract": "Deep learning has made revolutionary advances to diverse applications in the presence of large-scale labeled datasets. However, it is prohibitively time-costly and labor-expensive to collect sufficient labeled data in most realistic scenarios. To mitigate the requirement for labeled data, semi-supervised learning (SSL) focuses on simultaneously exploring both labeled and unlabeled data, while transfer learning (TL) popularizes a favorable practice of fine-tuning a pre-trained model to the target data. A dilemma is thus encountered: Without a decent pre-trained model to provide an implicit regularization, SSL through self-training from scratch will be easily misled by inaccurate pseudo-labels, especially in large-sized label space; Without exploring the intrinsic structure of unlabeled data, TL through fine-tuning from limited labeled data is at risk of under-transfer caused by model shift. To escape from this dilemma, we present Self-Tuning to enable data-efficient deep learning by unifying the exploration of labeled and unlabeled data and the transfer of a pre-trained model, as well as a Pseudo Group Contrast (PGC) mechanism to mitigate the reliance on pseudo-labels and boost the tolerance to false labels. Self-Tuning outperforms its SSL and TL counterparts on five tasks by sharp margins, e.g. it doubles the accuracy of fine-tuning on Cars with $15\\%$ labels. ",
        "conference": "ICML",
        "中文标题": "自调整实现数据高效的深度学习",
        "摘要翻译": "深度学习在大规模标记数据集的支持下，对多样化的应用带来了革命性的进步。然而，在大多数现实场景中，收集足够的标记数据既耗时又昂贵。为了减轻对标记数据的需求，半监督学习（SSL）专注于同时探索标记和未标记数据，而迁移学习（TL）则推广了一种将预训练模型微调到目标数据的有利做法。因此，我们面临一个困境：没有一个良好的预训练模型来提供隐式正则化，通过自训练从头开始的SSL很容易被不准确的伪标签误导，尤其是在大尺寸标签空间中；如果不探索未标记数据的内在结构，通过有限标记数据微调的TL就有因模型偏移而导致迁移不足的风险。为了摆脱这一困境，我们提出了自调整（Self-Tuning），通过统一探索标记和未标记数据以及迁移预训练模型，实现数据高效的深度学习，并提出了一种伪组对比（PGC）机制来减轻对伪标签的依赖并提高对错误标签的容忍度。自调整在五个任务上大幅优于其SSL和TL的对应方法，例如，它在Cars数据集上使用15%的标签时，将微调的准确率提高了一倍。",
        "领域": "半监督学习, 迁移学习, 深度学习优化",
        "问题": "如何在有限标记数据的情况下，有效结合半监督学习和迁移学习，以提高深度学习模型的性能",
        "动机": "解决在有限标记数据情况下，半监督学习和迁移学习各自面临的挑战，实现数据高效的深度学习",
        "方法": "提出自调整（Self-Tuning）方法，统一探索标记和未标记数据及迁移预训练模型，并引入伪组对比（PGC）机制以减少对伪标签的依赖",
        "关键词": [
            "自调整",
            "伪组对比",
            "半监督学习",
            "迁移学习",
            "数据高效"
        ],
        "涉及的技术概念": {
            "自调整（Self-Tuning）": "一种统一探索标记和未标记数据及迁移预训练模型的方法，旨在实现数据高效的深度学习",
            "伪组对比（PGC）": "一种机制，用于减轻对伪标签的依赖并提高模型对错误标签的容忍度",
            "半监督学习（SSL）": "一种同时利用标记和未标记数据进行学习的方法，旨在减轻对大量标记数据的需求"
        },
        "success": true
    },
    {
        "order": 984,
        "title": "Sequential Domain Adaptation by Synthesizing Distributionally Robust Experts",
        "html": "https://ICML.cc//virtual/2021/poster/10743",
        "abstract": "Least squares estimators, when trained on few target domain samples, may predict poorly. Supervised domain adaptation aims to improve the predictive accuracy by exploiting additional labeled training samples from a source distribution that is close to the target distribution. Given available data, we investigate novel strategies to synthesize a family of least squares estimator experts that are robust with regard to moment conditions. When these moment conditions are specified using Kullback-Leibler or Wasserstein-type divergences, we can find the robust estimators efficiently using convex optimization. We use the Bernstein online aggregation algorithm on the proposed family of robust experts to generate predictions for the sequential stream of target test samples. Numerical experiments on real data show that the robust strategies systematically outperform non-robust interpolations of the empirical least squares estimators.",
        "conference": "ICML",
        "中文标题": "通过合成分布鲁棒专家的序列域适应",
        "摘要翻译": "当在少量目标域样本上训练时，最小二乘估计器的预测可能表现不佳。监督域适应的目标是通过利用来自与目标分布接近的源分布的额外标记训练样本来提高预测准确性。基于可用数据，我们研究了新的策略来合成一系列对矩条件具有鲁棒性的最小二乘估计器专家。当这些矩条件使用Kullback-Leibler或Wasserstein型散度指定时，我们可以通过凸优化高效地找到鲁棒估计器。我们在提出的鲁棒专家家族上使用Bernstein在线聚合算法，为目标测试样本的序列流生成预测。真实数据的数值实验表明，鲁棒策略系统地优于经验最小二乘估计器的非鲁棒插值。",
        "领域": "监督学习、域适应、在线学习",
        "问题": "提高在少量目标域样本上训练的预测准确性",
        "动机": "利用源域的额外标记数据来提高目标域的预测性能",
        "方法": "合成对矩条件具有鲁棒性的最小二乘估计器专家，并使用Bernstein在线聚合算法进行预测",
        "关键词": [
            "监督域适应",
            "分布鲁棒优化",
            "在线聚合",
            "最小二乘估计",
            "凸优化"
        ],
        "涉及的技术概念": {
            "Kullback-Leibler散度": "用于衡量两个概率分布之间的差异，本研究中用于指定矩条件",
            "Wasserstein型散度": "另一种衡量概率分布之间差异的方法，本研究中也用于指定矩条件",
            "Bernstein在线聚合算法": "用于在序列流的目标测试样本上生成预测的算法"
        },
        "success": true
    },
    {
        "order": 985,
        "title": "SGA: A Robust Algorithm for  Partial Recovery of Tree-Structured  Graphical  Models with Noisy Samples",
        "html": "https://ICML.cc//virtual/2021/poster/8737",
        "abstract": "We consider learning Ising tree models when the observations from the nodes are corrupted by independent but non-identically distributed noise with unknown statistics. Katiyar et al. (2020) showed that although the exact tree structure cannot be recovered, one can recover a partial tree structure; that is, a structure belonging to the equivalence class containing the true tree. This paper presents a systematic improvement of Katiyar et al. (2020). First, we present a novel impossibility result by deriving a bound on the necessary number of samples for partial recovery. Second, we derive a significantly improved sample complexity result in which the dependence on the minimum correlation $\\rho_{\\min}$ is $\\rho_{\\min}^{-8}$ instead of $\\rho_{\\min}^{-24}$. Finally, we propose Symmetrized Geometric Averaging (SGA),  a more statistically robust algorithm for partial tree recovery. We provide error exponent analyses and extensive numerical results on a variety of trees  to show that the sample complexity of SGA is significantly better than the algorithm of Katiyar et al. (2020).  SGA can be  readily extended to Gaussian models and is shown via numerical experiments to be similarly superior.",
        "conference": "ICML",
        "success": true,
        "中文标题": "SGA：一种用于噪声样本下树结构图模型部分恢复的鲁棒算法",
        "摘要翻译": "我们考虑在节点观测被具有未知统计特性的独立但非同分布噪声污染时，学习Ising树模型。Katiyar等人（2020年）表明，虽然无法恢复确切的树结构，但可以恢复部分树结构；即，属于包含真实树的等价类的结构。本文对Katiyar等人（2020年）的工作进行了系统性改进。首先，我们通过推导部分恢复所需样本数量的界限，提出了一个新的不可能性结果。其次，我们推导了一个显著改进的样本复杂度结果，其中对最小相关性ρ_min的依赖是ρ_min^-8，而不是ρ_min^-24。最后，我们提出了对称几何平均（SGA），一种用于部分树恢复的更统计鲁棒的算法。我们提供了错误指数分析和在各种树上的广泛数值结果，以显示SGA的样本复杂度显著优于Katiyar等人（2020年）的算法。SGA可以轻松扩展到高斯模型，并通过数值实验显示出类似的优越性。",
        "领域": "统计机器学习、图模型学习、噪声数据处理",
        "问题": "在节点观测被独立但非同分布噪声污染的情况下，如何有效地学习Ising树模型并恢复部分树结构。",
        "动机": "改进现有方法在噪声环境下学习树结构图模型的效率和鲁棒性，特别是在样本复杂度和算法性能方面。",
        "方法": "提出了一种新的算法——对称几何平均（SGA），通过改进样本复杂度和算法鲁棒性，实现了在噪声环境下更有效的部分树结构恢复。",
        "关键词": [
            "Ising树模型",
            "部分恢复",
            "样本复杂度",
            "对称几何平均",
            "噪声数据"
        ],
        "涉及的技术概念": {
            "Ising树模型": "用于描述变量间依赖关系的图模型，特别适用于二元变量。",
            "部分恢复": "在无法完全恢复原始结构的情况下，恢复属于同一等价类的结构。",
            "对称几何平均（SGA）": "一种新的算法，用于在噪声环境下更有效地恢复部分树结构，具有更好的样本复杂度和鲁棒性。"
        }
    },
    {
        "order": 986,
        "title": "SGLB: Stochastic Gradient Langevin Boosting",
        "html": "https://ICML.cc//virtual/2021/poster/8665",
        "abstract": "This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a powerful and efficient machine learning framework that may deal with a wide range of loss functions and has provable generalization guarantees. The method is based on a special form of the Langevin diffusion equation specifically designed for gradient boosting. This allows us to theoretically guarantee the global convergence even for multimodal loss functions, while standard gradient boosting algorithms can guarantee only local optimum. We also empirically show that SGLB outperforms classic gradient boosting when applied to classification tasks with 0-1 loss function, which is known to be multimodal.",
        "conference": "ICML",
        "中文标题": "SGLB: 随机梯度朗之万提升",
        "摘要翻译": "本文介绍了随机梯度朗之万提升（SGLB）——一个强大且高效的机器学习框架，能够处理广泛的损失函数，并具有可证明的泛化保证。该方法基于一种特殊形式的朗之万扩散方程，专门为梯度提升设计。这使得我们即使在多模态损失函数的情况下，也能理论上保证全局收敛，而标准梯度提升算法只能保证局部最优。我们还通过实验证明，当应用于具有0-1损失函数的分类任务时，SGLB优于经典梯度提升，而0-1损失函数已知是多模态的。",
        "领域": "梯度提升算法、机器学习优化、分类任务",
        "问题": "解决梯度提升算法在多模态损失函数下只能达到局部最优的问题",
        "动机": "为了在机器学习中更有效地处理多模态损失函数，并保证算法的全局收敛性",
        "方法": "基于朗之万扩散方程的特殊形式，设计了一种新的梯度提升方法",
        "关键词": [
            "随机梯度朗之万提升",
            "多模态损失函数",
            "全局收敛",
            "梯度提升",
            "分类任务"
        ],
        "涉及的技术概念": {
            "朗之万扩散方程": "用于设计新的梯度提升方法，以实现在多模态损失函数下的全局收敛",
            "梯度提升": "一种集成学习技术，通过迭代地添加弱学习器来优化任意可微的损失函数",
            "多模态损失函数": "具有多个局部最优点的损失函数，如0-1损失函数，使得优化更加复杂"
        },
        "success": true
    },
    {
        "order": 987,
        "title": "SG-PALM: a Fast Physically Interpretable Tensor Graphical Model",
        "html": "https://ICML.cc//virtual/2021/poster/9797",
        "abstract": "We propose a new graphical model inference procedure, called SG-PALM, for learning conditional dependency structure of high-dimensional tensor-variate data. Unlike most other tensor graphical models the proposed model is interpretable and computationally scalable to high dimension. Physical interpretability follows from the Sylvester generative (SG) model on which SG-PALM is based: the model is exact for any observation process that is a solution of a partial differential equation of Poisson type. Scalability follows from the fast proximal alternating linearized minimization (PALM) procedure that SG-PALM uses during training. We establish that SG-PALM converges linearly (i.e., geometric convergence rate) to a global optimum of its objective function. We demonstrate scalability and accuracy of SG-PALM for an important but challenging climate prediction problem: spatio-temporal forecasting of solar flares from multimodal imaging data.",
        "conference": "ICML",
        "中文标题": "SG-PALM：一种快速物理可解释的张量图模型",
        "摘要翻译": "我们提出了一种新的图模型推理程序，称为SG-PALM，用于学习高维张量数据的条件依赖结构。与大多数其他张量图模型不同，所提出的模型是可解释的，并且在计算上可扩展到高维度。物理可解释性源自SG-PALM所基于的Sylvester生成（SG）模型：该模型对于任何泊松型偏微分方程解的观测过程都是精确的。可扩展性源自SG-PALM在训练期间使用的快速近端交替线性化最小化（PALM）程序。我们确立了SG-PALM线性收敛（即几何收敛率）到其目标函数的全局最优。我们展示了SG-PALM在一个重要但具有挑战性的气候预测问题中的可扩展性和准确性：从多模态成像数据中进行太阳耀斑的时空预测。",
        "领域": "张量图模型、高维数据分析、气候预测",
        "问题": "学习高维张量数据的条件依赖结构，并解决传统张量图模型在可解释性和计算可扩展性方面的限制。",
        "动机": "开发一种既物理可解释又计算高效的方法，以处理高维张量数据的条件依赖结构学习问题，特别是在气候预测等复杂应用中。",
        "方法": "基于Sylvester生成（SG）模型和快速近端交替线性化最小化（PALM）程序，提出SG-PALM方法，用于高效学习高维张量数据的条件依赖结构。",
        "关键词": [
            "张量图模型",
            "高维数据分析",
            "气候预测",
            "SG-PALM",
            "PALM"
        ],
        "涉及的技术概念": {
            "Sylvester生成（SG）模型": "SG-PALM所基于的模型，提供了物理可解释性，适用于泊松型偏微分方程解的观测过程。",
            "快速近端交替线性化最小化（PALM）": "SG-PALM在训练期间使用的优化程序，确保了模型的高效计算和可扩展性。",
            "几何收敛率": "SG-PALM在优化过程中展示的线性收敛特性，保证了算法快速收敛到全局最优解。"
        },
        "success": true
    },
    {
        "order": 988,
        "title": "Sharf: Shape-conditioned Radiance Fields from a Single View",
        "html": "https://ICML.cc//virtual/2021/poster/8913",
        "abstract": "We present  a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object.\nThe explicit disentanglement of shape and appearance\nallows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs).\nFinally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape.\nWe demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.",
        "conference": "ICML",
        "中文标题": "Sharf: 基于单视图的形状条件辐射场",
        "摘要翻译": "我们提出了一种仅凭单张图像估计物体神经场景表示的方法。我们方法的核心在于估计物体的几何支架，并将其用作重建底层辐射场的指导。我们的公式基于一个生成过程，该过程首先将潜在代码映射到体素化形状，然后将其渲染为图像，物体外观由第二个潜在代码控制。在推理过程中，我们优化潜在代码和网络以适应新物体的测试图像。形状和外观的显式解耦允许我们的模型在给定单张图像的情况下进行微调。然后，我们可以以几何一致的方式渲染新视图，并且它们忠实地代表输入物体。此外，我们的方法能够泛化到训练域之外的图像（更真实的渲染甚至真实照片）。最后，推断出的几何支架本身就是物体3D形状的准确估计。我们在多个实验中证明了我们的方法在合成和真实图像中的有效性。",
        "领域": "三维重建、神经渲染、计算机视觉",
        "问题": "如何从单张图像中准确估计物体的3D形状和外观，并生成几何一致的新视图。",
        "动机": "解决从单张图像进行三维重建和视图合成的挑战，特别是在形状和外观解耦的情况下，提高重建的准确性和泛化能力。",
        "方法": "通过估计物体的几何支架作为指导，使用生成过程将潜在代码映射到体素化形状并渲染图像，优化潜在代码和网络以适应新物体的测试图像。",
        "关键词": [
            "三维重建",
            "神经渲染",
            "单视图学习",
            "几何支架",
            "辐射场"
        ],
        "涉及的技术概念": {
            "几何支架": "作为物体3D形状的估计，用于指导辐射场的重建。",
            "潜在代码": "用于控制物体形状和外观的生成过程，通过优化以适应新物体。",
            "辐射场": "用于表示物体的外观和光照属性，通过神经渲染技术生成新视图。"
        },
        "success": true
    },
    {
        "order": 989,
        "title": "Sharing Less is More: Lifelong Learning in Deep Networks with Selective Layer Transfer",
        "html": "https://ICML.cc//virtual/2021/poster/10559",
        "abstract": "Effective lifelong learning across diverse tasks requires the transfer of diverse knowledge, yet transferring irrelevant knowledge may lead to interference and catastrophic forgetting. In deep networks, transferring the appropriate granularity of knowledge is as important as the transfer mechanism, and must be driven by the relationships among tasks. We first show that the lifelong learning performance of several current deep learning architectures can be significantly improved by transfer at the appropriate layers. We then develop an expectation-maximization (EM) method to automatically select the appropriate transfer configuration and optimize the task network weights. This EM-based selective transfer is highly effective, balancing transfer performance on all tasks with avoiding catastrophic forgetting, as demonstrated on three algorithms in several lifelong object classification scenarios.",
        "conference": "ICML",
        "中文标题": "少即是多：通过选择性层转移实现深度网络的终身学习",
        "摘要翻译": "跨多样任务的有效终身学习需要转移多样化的知识，然而转移不相关的知识可能导致干扰和灾难性遗忘。在深度网络中，转移适当粒度的知识与转移机制同样重要，且必须由任务间的关系驱动。我们首先展示，通过适当层的转移，可以显著提高几种当前深度学习架构的终身学习性能。接着，我们开发了一种期望最大化（EM）方法，自动选择适当的转移配置并优化任务网络权重。这种基于EM的选择性转移非常有效，在所有任务上平衡转移性能与避免灾难性遗忘，如在几种终身对象分类场景中的三种算法所证明的那样。",
        "领域": "终身学习、深度学习、对象分类",
        "问题": "如何在终身学习中有效转移知识以避免干扰和灾难性遗忘",
        "动机": "研究旨在解决终身学习中知识转移的粒度选择和机制优化问题，以提高学习效率和避免灾难性遗忘",
        "方法": "通过期望最大化（EM）方法自动选择适当的转移配置并优化任务网络权重，实现选择性知识转移",
        "关键词": [
            "终身学习",
            "选择性层转移",
            "期望最大化",
            "灾难性遗忘",
            "对象分类"
        ],
        "涉及的技术概念": {
            "选择性层转移": "在深度网络中，根据任务间关系选择性地转移知识，以提高终身学习性能",
            "期望最大化（EM）": "用于自动选择最佳知识转移配置和优化网络权重的统计方法",
            "灾难性遗忘": "在学习新任务时，模型忘记之前学习任务的现象，本文通过选择性转移来避免"
        },
        "success": true
    },
    {
        "order": 990,
        "title": "Sharper Generalization Bounds for Clustering",
        "html": "https://ICML.cc//virtual/2021/poster/8953",
        "abstract": "Existing generalization analysis of clustering mainly focuses on specific instantiations, such as (kernel) $k$-means, and a unified framework for studying clustering performance is still lacking. Besides, the existing excess clustering risk bounds are mostly of order $\\mathcal{O}(K/\\sqrt{n})$ provided that the underlying distribution has bounded support, where $n$ is the sample size and $K$ is the cluster numbers, or of order $\\mathcal{O}(K^2/n)$ under strong assumptions on the underlying distribution, where these assumptions are hard to be verified in general. In this paper, we propose a unified clustering learning framework and investigate its excess risk bounds, obtaining state-of-the-art upper bounds under mild assumptions. Specifically, we derive sharper bounds of order $\\mathcal{O}(K^2/n)$ under mild assumptions on the covering number of the hypothesis spaces, where these assumptions are easy to be verified. Moreover, for the hard clustering scheme, such as (kernel) $k$-means, if just assume the hypothesis functions to be bounded, we improve the upper bounds from the order $\\mathcal{O}(K/\\sqrt{n})$ to $\\mathcal{O}(\\sqrt{K}/\\sqrt{n})$. Furthermore, state-of-the-art bounds of faster order $\\mathcal{O}(K/n)$ are obtained with the covering number assumptions.",
        "conference": "ICML",
        "success": true,
        "中文标题": "聚类更尖锐的泛化界限",
        "摘要翻译": "现有的聚类泛化分析主要集中于特定的实例，如（核）k-均值，而研究聚类性能的统一框架仍然缺乏。此外，现有的超额聚类风险界限大多为阶$\\mathcal{O}(K/\\sqrt{n})$，前提是基础分布有界支持，其中$n$是样本大小，$K$是聚类数目，或者在基础分布的强假设下为阶$\\mathcal{O}(K^2/n)$，这些假设在一般情况下难以验证。在本文中，我们提出了一个统一的聚类学习框架，并研究了其超额风险界限，在温和假设下获得了最先进的上界。具体来说，我们在假设空间的覆盖数的温和假设下，推导出了阶$\\mathcal{O}(K^2/n)$的更尖锐界限，这些假设易于验证。此外，对于硬聚类方案，如（核）k-均值，如果仅假设假设函数有界，我们将上界从阶$\\mathcal{O}(K/\\sqrt{n})$改进为$\\mathcal{O}(\\sqrt{K}/\\sqrt{n})$。此外，在覆盖数假设下，获得了更快阶$\\mathcal{O}(K/n)$的最先进界限。",
        "领域": "聚类分析、机器学习理论、统计学习理论",
        "问题": "缺乏统一的聚类性能研究框架，以及现有超额聚类风险界限在一般条件下难以验证或不够尖锐。",
        "动机": "建立一个统一的聚类学习框架，以在更温和且易于验证的假设下，获得更尖锐的泛化界限。",
        "方法": "提出一个统一的聚类学习框架，并在不同的假设条件下，推导出超额风险的最先进上界。",
        "关键词": [
            "聚类分析",
            "泛化界限",
            "超额风险",
            "统一框架",
            "覆盖数假设"
        ],
        "涉及的技术概念": {
            "超额聚类风险界限": "衡量聚类算法性能与最优聚类性能之间差异的界限。",
            "覆盖数假设": "用于描述假设空间复杂度的概念，有助于推导更尖锐的泛化界限。"
        }
    },
    {
        "order": 991,
        "title": "Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks",
        "html": "https://ICML.cc//virtual/2021/poster/9117",
        "abstract": "We propose the k-Shortest-Path (k-SP) constraint: a novel constraint on the agent’s trajectory that improves the sample efficiency in sparse-reward MDPs. We show that any optimal policy necessarily satisfies the k-SP constraint. Notably, the k-SP constraint prevents the policy from exploring state-action pairs along the non-k-SP trajectories (e.g., going back and forth). However, in practice, excluding state-action pairs may hinder the convergence of RL algorithms. To overcome this, we propose a novel cost function that penalizes the policy violating SP constraint, instead of completely excluding it. Our numerical experiment in a tabular RL setting demonstrates that the SP-constraint can significantly reduce the trajectory space of policy. As a result, our constraint enables more sample efficient learning by suppressing redundant exploration and exploitation. Our experiments on MiniGrid, DeepMind Lab, Atari, and Fetch show that the proposed method significantly improves proximal policy optimization (PPO) and outperforms existing novelty-seeking exploration methods including count-based exploration even in continuous control tasks, indicating that it improves the sample efficiency by preventing the agent from taking redundant actions.",
        "conference": "ICML",
        "中文标题": "最短路径约束下的稀疏奖励任务强化学习",
        "摘要翻译": "我们提出了k-最短路径（k-SP）约束：一种新颖的约束条件，用于改善稀疏奖励马尔可夫决策过程（MDPs）中的样本效率。我们证明任何最优策略必然满足k-SP约束。值得注意的是，k-SP约束阻止策略探索非k-SP轨迹上的状态-动作对（例如，来回走动）。然而，在实践中，排除状态-动作对可能会阻碍强化学习算法的收敛。为了克服这一点，我们提出了一种新的成本函数，该函数惩罚违反SP约束的策略，而不是完全排除它。我们在表格RL设置中的数值实验表明，SP约束可以显著减少策略的轨迹空间。因此，我们的约束通过抑制冗余探索和利用，实现了更高效的样本学习。我们在MiniGrid、DeepMind Lab、Atari和Fetch上的实验表明，所提出的方法显著改进了近端策略优化（PPO），并且在连续控制任务中优于包括基于计数的探索在内的现有新颖性探索方法，这表明它通过防止代理采取冗余动作提高了样本效率。",
        "领域": "强化学习、路径规划、样本效率优化",
        "问题": "在稀疏奖励环境中提高强化学习的样本效率",
        "动机": "解决稀疏奖励环境下强化学习样本效率低下的问题，通过约束策略探索的轨迹空间来减少冗余探索",
        "方法": "提出k-最短路径约束和相应的成本函数，以惩罚违反约束的策略，而非完全排除，从而优化样本效率",
        "关键词": [
            "k-最短路径约束",
            "稀疏奖励",
            "样本效率",
            "强化学习",
            "近端策略优化"
        ],
        "涉及的技术概念": {
            "k-最短路径约束": "一种约束条件，限制策略探索的轨迹必须是最短路径之一，以提高样本效率",
            "稀疏奖励马尔可夫决策过程": "指奖励信号稀疏的环境，强化学习在这种环境中面临样本效率低下的挑战",
            "近端策略优化": "一种强化学习算法，通过限制策略更新的幅度来稳定训练过程，本文提出的方法显著改进了其性能"
        },
        "success": true
    },
    {
        "order": 992,
        "title": "SiameseXML: Siamese Networks meet Extreme Classifiers with 100M Labels",
        "html": "https://ICML.cc//virtual/2021/poster/8847",
        "abstract": "Deep extreme multi-label learning (XML) requires training deep architectures that can tag a data point with its most relevant subset of labels from an extremely large label set. XML applications such as ad and product recommendation involve labels rarely seen during training but which nevertheless hold the key to recommendations that delight users. Effective utilization of label metadata and high quality predictions for rare labels at the scale of millions of labels are thus key challenges in contemporary XML research. To address these, this paper develops the SiameseXML framework based on a novel probabilistic model that naturally motivates a modular approach melding Siamese architectures with high-capacity extreme classifiers, and a training pipeline that effortlessly scales to tasks with 100 million labels. SiameseXML offers predictions 2--13% more accurate than leading XML methods on public benchmark datasets, as well as in live A/B tests on the Bing search engine, it offers significant gains in click-through-rates, coverage, revenue and other online metrics over state-of-the-art techniques currently in production. Code for SiameseXML is available at https://github.com/Extreme-classification/siamesexml",
        "conference": "ICML",
        "中文标题": "SiameseXML：结合Siamese网络与极端分类器的千万级标签分类框架",
        "摘要翻译": "深度极端多标签学习（XML）需要训练能够从极其庞大的标签集合中为数据点标注最相关标签子集的深度架构。XML应用，如广告和产品推荐，涉及在训练中很少见但对用户推荐至关重要的标签。因此，有效利用标签元数据和在百万级标签规模上对稀有标签进行高质量预测是当代XML研究的关键挑战。为解决这些问题，本文基于一种新颖的概率模型开发了SiameseXML框架，该模型自然地激发了一种将Siamese架构与高容量极端分类器融合的模块化方法，以及一个能够轻松扩展到具有1亿个标签任务的训练流程。SiameseXML在公共基准数据集上的预测准确率比领先的XML方法高2-13%，在Bing搜索引擎的实时A/B测试中，与当前生产中的最先进技术相比，它在点击率、覆盖率、收入和其他在线指标上提供了显著的增益。SiameseXML的代码可在https://github.com/Extreme-classification/siamesexml获取。",
        "领域": "极端多标签学习、推荐系统、深度学习应用",
        "问题": "在极端多标签学习（XML）中，如何有效利用标签元数据并提高对稀有标签的预测质量，特别是在百万级标签规模下。",
        "动机": "解决XML应用中，如广告和产品推荐，对训练中罕见但对用户推荐至关重要的标签进行高质量预测的挑战。",
        "方法": "开发了基于新颖概率模型的SiameseXML框架，结合Siamese架构与高容量极端分类器，采用模块化方法和可扩展的训练流程。",
        "关键词": [
            "极端多标签学习",
            "Siamese网络",
            "推荐系统",
            "深度学习",
            "标签预测"
        ],
        "涉及的技术概念": {
            "极端多标签学习（XML）": "一种从极其庞大的标签集合中为数据点标注最相关标签子集的学习任务。",
            "Siamese网络": "一种用于比较两个输入的相似性的神经网络架构，本文中用于提升标签预测的准确性。",
            "概率模型": "本文提出的新颖模型，用于自然地融合Siamese架构与极端分类器，优化标签预测过程。"
        },
        "success": true
    },
    {
        "order": 993,
        "title": "SigGPDE: Scaling Sparse Gaussian Processes on Sequential Data",
        "html": "https://ICML.cc//virtual/2021/poster/10563",
        "abstract": "Making predictions and quantifying their uncertainty when the input data is sequential is a fundamental learning challenge, recently attracting increasing attention. We develop SigGPDE, a new scalable sparse variational inference framework for Gaussian Processes (GPs) on sequential data. Our contribution is twofold. First, we construct inducing variables underpinning the sparse approximation so that the resulting evidence lower bound (ELBO) does not require any matrix inversion. Second, we show that the gradients of the GP signature kernel are solutions of a hyperbolic partial differential equation (PDE). This theoretical insight allows us to build an efficient back-propagation algorithm to optimize the ELBO. We showcase the significant computational gains of SigGPDE compared to existing methods, while achieving state-of-the-art performance for classification tasks on large datasets of up to 1 million multivariate time series.",
        "conference": "ICML",
        "中文标题": "SigGPDE: 在序列数据上扩展稀疏高斯过程",
        "摘要翻译": "当输入数据为序列时，进行预测并量化其不确定性是一个基本的学习挑战，最近引起了越来越多的关注。我们开发了SigGPDE，这是一个新的可扩展的稀疏变分推理框架，用于序列数据上的高斯过程（GPs）。我们的贡献有两个方面。首先，我们构建了支撑稀疏近似的诱导变量，使得所得的证据下界（ELBO）不需要任何矩阵求逆。其次，我们证明了GP签名核的梯度是双曲偏微分方程（PDE）的解。这一理论见解使我们能够构建一个高效的反向传播算法来优化ELBO。我们展示了SigGPDE与现有方法相比的显著计算优势，同时在多达100万多元时间序列的大型数据集上实现了最先进的分类任务性能。",
        "领域": "序列数据分析、高斯过程、变分推理",
        "问题": "在序列数据上进行预测并量化预测的不确定性",
        "动机": "解决序列数据上高斯过程模型的可扩展性问题，提高计算效率同时保持预测性能",
        "方法": "开发了一个新的稀疏变分推理框架SigGPDE，通过构建不需要矩阵求逆的诱导变量和利用GP签名核梯度的理论见解，实现了高效的反向传播算法",
        "关键词": [
            "稀疏高斯过程",
            "变分推理",
            "序列数据",
            "签名核",
            "偏微分方程"
        ],
        "涉及的技术概念": {
            "稀疏变分推理": "用于在高斯过程模型中实现可扩展性的技术，通过稀疏近似减少计算复杂度",
            "证据下界（ELBO）": "变分推理中用于近似后验分布的优化目标，SigGPDE通过特定构造的诱导变量避免了矩阵求逆",
            "GP签名核": "用于序列数据的高斯过程核函数，其梯度作为双曲偏微分方程的解，为高效反向传播算法提供了理论基础"
        },
        "success": true
    },
    {
        "order": 994,
        "title": "Signatured Deep Fictitious Play for Mean Field Games with Common Noise",
        "html": "https://ICML.cc//virtual/2021/poster/9157",
        "abstract": "Existing deep learning methods for solving mean-field games (MFGs) with common noise fix the sampling common noise paths and then solve the corresponding MFGs. This leads to a nested loop structure with millions of simulations of common noise paths in order to produce accurate solutions, which results in prohibitive computational cost and limits the applications to a large extent. In this paper, based on the rough path theory, we propose a novel single-loop algorithm, named signatured deep fictitious play (Sig-DFP), by which we can work with the unfixed common noise setup to avoid the nested loop structure and reduce the computational complexity significantly. The proposed algorithm can accurately capture the effect of common uncertainty changes on mean-field equilibria without further training of neural networks, as previously needed in the existing machine learning algorithms. The efficiency is supported by three applications, including linear-quadratic MFGs, mean-field portfolio game, and mean-field game of optimal consumption and investment. Overall, we provide a new point of view from the rough path theory to solve MFGs with common noise with significantly improved efficiency and an extensive range of applications. In addition, we report the first deep learning work to deal with extended MFGs (a mean-field interaction via both the states and controls) with common noise. ",
        "conference": "ICML",
        "中文标题": "签名深度虚构博弈用于具有共同噪声的均值场博弈",
        "摘要翻译": "现有的用于解决具有共同噪声的均值场博弈（MFGs）的深度学习方法固定了采样共同噪声路径，然后解决相应的MFGs。这导致了一个嵌套循环结构，需要数百万次共同噪声路径的模拟以产生精确解，这导致了极高的计算成本并在很大程度上限制了应用。在本文中，基于粗糙路径理论，我们提出了一种新颖的单循环算法，名为签名深度虚构博弈（Sig-DFP），通过该算法，我们可以处理未固定的共同噪声设置，以避免嵌套循环结构并显著降低计算复杂度。所提出的算法能够准确捕捉共同不确定性变化对均值场均衡的影响，而无需像现有机器学习算法那样进一步训练神经网络。效率通过三个应用得到支持，包括线性二次MFGs、均值场投资组合博弈和最优消费与投资的均值场博弈。总体而言，我们从粗糙路径理论提供了一个新的视角来解决具有共同噪声的MFGs，显著提高了效率并扩展了应用范围。此外，我们报告了第一个处理具有共同噪声的扩展MFGs（通过状态和控制进行均值场交互）的深度学习工作。",
        "领域": "均值场博弈、深度学习在博弈论中的应用、粗糙路径理论",
        "问题": "解决具有共同噪声的均值场博弈的高计算成本问题",
        "动机": "减少解决具有共同噪声的均值场博弈时的计算复杂度，扩展其应用范围",
        "方法": "基于粗糙路径理论，提出签名深度虚构博弈（Sig-DFP）算法，避免嵌套循环结构",
        "关键词": [
            "均值场博弈",
            "共同噪声",
            "粗糙路径理论",
            "深度学习",
            "计算效率"
        ],
        "涉及的技术概念": {
            "签名深度虚构博弈（Sig-DFP）": "一种基于粗糙路径理论的单循环算法，用于有效解决具有共同噪声的均值场博弈",
            "粗糙路径理论": "提供了一种处理未固定共同噪声设置的理论框架，避免了传统方法中的嵌套循环结构",
            "均值场均衡": "在具有共同噪声的均值场博弈中，算法能够准确捕捉共同不确定性变化对均衡的影响"
        },
        "success": true
    },
    {
        "order": 995,
        "title": "SimAM: A Simple, Parameter-Free Attention Module for Convolutional Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8921",
        "abstract": "In this paper, we propose a conceptually simple but very effective attention module for Convolutional Neural Networks (ConvNets). In contrast to existing channel-wise and spatial-wise attention modules, our module instead infers 3-D attention weights for the feature map in a layer without adding parameters to the original networks. Specifically, we base on some well-known neuroscience theories and propose to optimize an energy function to find the importance of each neuron. We further derive a fast closed-form solution for the energy function, and show that the solution can be implemented in less than ten lines of code. Another advantage of the module is that most of the operators are selected based on the solution to the defined energy function, avoiding too many efforts for structure tuning. Quantitative evaluations on various visual tasks demonstrate that the proposed module is flexible and effective to improve the representation ability of many ConvNets. Our code is available at Pytorch-SimAM.",
        "conference": "ICML",
        "中文标题": "SimAM：一种用于卷积神经网络的简单无参数注意力模块",
        "摘要翻译": "在本文中，我们提出了一种概念上简单但非常有效的卷积神经网络（ConvNets）注意力模块。与现有的通道注意力和空间注意力模块不同，我们的模块无需向原始网络添加参数，即可推断出特征图的三维注意力权重。具体来说，我们基于一些著名的神经科学理论，提出通过优化能量函数来找到每个神经元的重要性。我们进一步推导了能量函数的快速闭式解，并展示了解法可以用不到十行代码实现。该模块的另一个优点是大多数算子都是基于定义的能量函数的解来选择的，避免了结构调优的过多努力。在各种视觉任务上的定量评估表明，所提出的模块灵活且有效地提高了许多卷积神经网络的表示能力。我们的代码可在Pytorch-SimAM获取。",
        "领域": "注意力机制、卷积神经网络优化、视觉任务增强",
        "问题": "如何在卷积神经网络中实现无需额外参数的三维注意力权重推断",
        "动机": "现有注意力模块通常需要增加网络参数，本研究旨在开发一种简单且无需额外参数的注意力机制，以提升网络表示能力",
        "方法": "基于神经科学理论优化能量函数，推导闭式解实现三维注意力权重推断，代码实现简洁高效",
        "关键词": [
            "注意力机制",
            "无参数优化",
            "卷积神经网络",
            "能量函数",
            "视觉任务"
        ],
        "涉及的技术概念": {
            "三维注意力权重": "通过优化能量函数推断特征图中每个神经元的重要性，实现无需额外参数的三维注意力分配",
            "能量函数": "基于神经科学理论设计，用于量化神经元重要性，其闭式解简化了注意力权重的计算过程",
            "闭式解": "能量函数的解析解，使得注意力模块的实现简洁高效，代码量少"
        },
        "success": true
    },
    {
        "order": 996,
        "title": "Simple and Effective VAE Training with Calibrated Decoders",
        "html": "https://ICML.cc//virtual/2021/poster/9721",
        "abstract": "Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method. ",
        "conference": "ICML",
        "中文标题": "简单有效的VAE训练与校准解码器",
        "摘要翻译": "变分自编码器（VAEs）为建模复杂分布提供了一种有效且简单的方法。然而，训练VAE通常需要大量的超参数调优以确定潜在变量保留的最佳信息量。我们研究了校准解码器对VAE性能的影响，这些解码器学习解码分布的不确定性，并能自动确定这一信息量。尽管已经提出了许多学习校准解码器的方法，但许多最近使用VAE的论文仍然依赖于启发式超参数和临时修改。我们首次对校准解码器进行了全面的比较分析，并为简单有效的VAE训练提供了建议。我们的分析涵盖了一系列数据集和几种单图像及序列VAE模型。我们进一步提出了对常用高斯解码器的一个简单但新颖的修改，该修改可以解析地计算预测方差。我们通过实证观察发现，使用我们的方法时不需要启发式修改。",
        "领域": "生成模型、变分自编码器、深度学习优化",
        "问题": "确定潜在变量保留的最佳信息量，减少VAE训练中的超参数调优需求",
        "动机": "研究校准解码器如何自动确定信息量，简化VAE训练过程",
        "方法": "比较分析校准解码器，提出高斯解码器的解析预测方差计算方法",
        "关键词": [
            "变分自编码器",
            "校准解码器",
            "高斯解码器",
            "预测方差",
            "深度学习优化"
        ],
        "涉及的技术概念": {
            "变分自编码器": "一种生成模型，用于建模复杂数据分布",
            "校准解码器": "学习解码分布的不确定性，自动确定潜在变量保留的信息量",
            "高斯解码器": "一种常用的解码器类型，本文提出其预测方差的解析计算方法"
        },
        "success": true
    },
    {
        "order": 997,
        "title": "Simultaneous Similarity-based Self-Distillation for Deep Metric Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9287",
        "abstract": "Deep Metric Learning (DML) provides a crucial tool for visual similarity and zero-shot retrieval applications by learning generalizing embedding spaces, although recent work in DML has shown strong performance saturation across training objectives. However, generalization capacity is known to scale with the embedding space dimensionality.  Unfortunately, high dimensional embeddings also create higher retrieval cost for downstream applications.  To remedy this, we propose S2SD - Simultaneous Similarity-based Self-distillation. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offering highly significant improvements of up to 7% in Recall@1, while also setting a new state-of-the-art.",
        "conference": "ICML",
        "中文标题": "基于同步相似性的自蒸馏深度度量学习",
        "摘要翻译": "深度度量学习（DML）通过学习泛化的嵌入空间，为视觉相似性和零样本检索应用提供了关键工具，尽管最近的DML工作显示训练目标之间存在强烈的性能饱和现象。然而，已知泛化能力随着嵌入空间维度的增加而扩展。不幸的是，高维嵌入也为下游应用创造了更高的检索成本。为了解决这个问题，我们提出了S2SD——基于同步相似性的自蒸馏。S2SD通过从辅助的高维嵌入和特征空间进行知识蒸馏来扩展DML，以在训练期间利用互补的上下文，同时保留测试时的成本，并且对训练时间的改变可以忽略不计。跨不同目标和标准基准的实验和消融研究表明，S2SD在Recall@1上提供了高达7%的显著改进，同时也设定了新的最先进水平。",
        "领域": "深度度量学习、视觉相似性、零样本检索",
        "问题": "解决深度度量学习在高维嵌入空间中泛化能力与检索成本之间的矛盾问题",
        "动机": "通过知识蒸馏技术，利用高维嵌入和特征空间的互补信息，提高模型的泛化能力而不增加测试时的检索成本",
        "方法": "提出S2SD方法，通过同步相似性自蒸馏从高维辅助空间向主空间传递知识，优化训练过程",
        "关键词": [
            "深度度量学习",
            "知识蒸馏",
            "自蒸馏",
            "高维嵌入",
            "视觉相似性"
        ],
        "涉及的技术概念": {
            "深度度量学习（DML）": "通过学习泛化的嵌入空间来支持视觉相似性和零样本检索应用的技术",
            "知识蒸馏": "从高维辅助空间向主空间传递知识，以提高模型性能而不增加测试成本的技术",
            "自蒸馏": "一种特殊的知识蒸馏形式，模型自身生成的知识用于指导同一模型的训练过程"
        },
        "success": true
    },
    {
        "order": 998,
        "title": "Single Pass Entrywise-Transformed Low Rank Approximation",
        "html": "https://ICML.cc//virtual/2021/poster/8761",
        "abstract": "In applications such as natural language processing or computer vision, one is given a large $n \\times n$ matrix $A = (a_{i,j})$ and would like to compute a matrix decomposition, e.g., a low rank approximation, of a function $f(A) = (f(a_{i,j}))$ applied entrywise to $A$. A very important special case is the likelihood function $f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right| +1\\right)}$. A natural way to do this would be to simply apply $f$ to each entry of $A$, and then compute the matrix decomposition, but this requires storing all of $A$ as well as multiple passes over its entries. Recent work of Liang et al. shows how to find a rank-$k$ factorization to $f(A)$ using only $n \\cdot \\poly(\\eps^{-1}k\\log n)$ words of memory, with overall error $10\\|f(A)-[f(A)]_k\\|_F^2 + \\poly(\\epsilon/k) \\|f(A)\\|_{1,2}^2$, where $[f(A)]_k$ is the best rank-$k$ approximation to $f(A)$ and $\\|f(A)\\|_{1,2}^2$ is the square of the sum of Euclidean lengths of rows of $f(A)$. Their algorithm uses $3$ passes over the entries of $A$. The authors pose the open question of obtaining an algorithm with $n \\cdot \\poly(\\eps^{-1}k\\log n)$ words of memory using only a single pass over the entries of $A$.\n\nIn this paper we resolve this open question, obtaining the first single-pass algorithm for this problem and for the same class of functions $f$ studied by Liang et al. Moreover, our error is $\\|f(A)-[f(A)]_k\\|_F^2 + \\poly(\\epsilon/k) \\|f(A)\\|_F^2$, where $\\|f(A)\\|_F^2$ is the sum of squares of Euclidean lengths of rows of $f(A)$. Thus our error is significantly smaller, as it removes the factor of $10$ and also $\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "单次遍历逐项变换的低秩近似",
        "摘要翻译": "在自然语言处理或计算机视觉等应用中，给定一个大型的$n \\times n$矩阵$A = (a_{i,j})$，人们希望计算一个矩阵分解，例如，对$A$逐项应用函数$f(A) = (f(a_{i,j}))$的低秩近似。一个非常重要的特例是似然函数$f\\left( A \\right ) = \\log{\\left( \\left| a_{ij}\\right| +1\\right)}$。一种自然的方法是简单地对$A$的每个条目应用$f$，然后计算矩阵分解，但这需要存储整个$A$以及多次遍历其条目。Liang等人的最新工作展示了如何仅使用$n \\cdot \\poly(\\eps^{-1}k\\log n)$字内存找到$f(A)$的秩-$k$分解，总体误差为$10\\|f(A)-[f(A)]_k\\|_F^2 + \\poly(\\epsilon/k) \\|f(A)\\|_{1,2}^2$，其中$[f(A)]_k$是$f(A)$的最佳秩-$k$近似，$\\|f(A)\\|_{1,2}^2$是$f(A)$行欧几里得长度之和的平方。他们的算法使用了$3$次遍历$A$的条目。作者提出了一个开放性问题，即如何获得一个仅使用$n \\cdot \\poly(\\eps^{-1}k\\log n)$字内存且仅需单次遍历$A$条目的算法。在本文中，我们解决了这个开放性问题，首次为这个问题以及Liang等人研究的同一类函数$f$提供了单次遍历算法。此外，我们的误差是$\\|f(A)-[f(A)]_k\\|_F^2 + \\poly(\\epsilon/k) \\|f(A)\\|_F^2$，其中$\\|f(A)\\|_F^2$是$f(A)$行欧几里得长度平方之和。因此，我们的误差显著减小，因为它去除了$10$的因子，并且$\\|f(A)\\|_F^2 \\leq \\|f(A)\\|_{1,2}^2$。",
        "领域": "自然语言处理与视觉结合, 矩阵分解, 低秩近似",
        "问题": "如何在仅单次遍历矩阵条目的情况下，高效计算逐项变换后的矩阵的低秩近似。",
        "动机": "解决在有限内存和计算资源下，高效处理大规模矩阵分解的问题，特别是在需要逐项变换矩阵的应用场景中。",
        "方法": "提出了一种单次遍历算法，用于计算逐项变换后的矩阵的低秩近似，显著降低了内存需求和计算误差。",
        "关键词": [
            "单次遍历算法",
            "低秩近似",
            "矩阵分解",
            "逐项变换",
            "内存效率"
        ],
        "涉及的技术概念": {
            "低秩近似": "用于近似表示矩阵，减少存储和计算复杂度，同时保留矩阵的主要特征。",
            "逐项变换": "对矩阵的每个元素应用特定函数，如对数变换，以适应不同的应用需求。"
        }
    },
    {
        "order": 999,
        "title": "SinIR: Efficient General Image Manipulation with Single Image Reconstruction",
        "html": "https://ICML.cc//virtual/2021/poster/10223",
        "abstract": "We propose SinIR, an efficient reconstruction-based framework trained on a single natural image for general image manipulation, including super-resolution, editing, harmonization, paint-to-image, photo-realistic style transfer, and artistic style transfer. We train our model on a single image with cascaded multi-scale learning, where each network at each scale is responsible for image reconstruction. This reconstruction objective greatly reduces the complexity and running time of training, compared to the GAN objective. However, the reconstruction objective also exacerbates the output quality. Therefore, to solve this problem, we further utilize simple random pixel shuffling, which also gives control over manipulation, inspired by the Denoising Autoencoder. With quantitative evaluation, we show that SinIR has competitive performance on various image manipulation tasks. Moreover, with a much simpler training objective (i.e., reconstruction), SinIR is trained 33.5 times faster than SinGAN (for 500x500 images) that solves similar tasks. Our code is publicly available at github.com/YooJiHyeong/SinIR.",
        "conference": "ICML",
        "中文标题": "SinIR：基于单图像重建的高效通用图像处理",
        "摘要翻译": "我们提出了SinIR，这是一个基于重建的高效框架，通过在单张自然图像上进行训练，实现包括超分辨率、编辑、协调、绘画到图像、照片级真实感风格转换和艺术风格转换在内的通用图像处理。我们的模型通过级联多尺度学习在单张图像上进行训练，其中每个尺度的网络负责图像重建。与GAN目标相比，这种重建目标大大降低了训练的复杂度和运行时间。然而，重建目标也会影响输出质量。因此，为了解决这个问题，我们进一步利用了简单的随机像素洗牌技术，这也提供了对处理的控制，灵感来自于去噪自编码器。通过定量评估，我们展示了SinIR在各种图像处理任务上的竞争性能。此外，由于采用了更简单的训练目标（即重建），SinIR的训练速度比解决类似任务的SinGAN（对于500x500图像）快了33.5倍。我们的代码已在github.com/YooJiHyeong/SinIR公开。",
        "领域": "图像超分辨率, 图像风格转换, 图像编辑",
        "问题": "如何在单张图像上高效训练模型以实现多种图像处理任务",
        "动机": "减少训练复杂度和时间，同时保持或提升图像处理任务的质量",
        "方法": "采用级联多尺度学习和随机像素洗牌技术，基于单图像重建目标训练模型",
        "关键词": [
            "单图像重建",
            "级联多尺度学习",
            "随机像素洗牌",
            "图像处理",
            "训练效率"
        ],
        "涉及的技术概念": {
            "级联多尺度学习": "在不同尺度上训练多个网络，每个网络负责相应尺度的图像重建，以提高模型对不同尺度特征的捕捉能力",
            "随机像素洗牌": "通过随机重新排列像素来增加图像的多样性，同时提供对图像处理的控制，以提高输出质量",
            "重建目标": "通过最小化输入图像与重建图像之间的差异来训练模型，相比GAN目标，简化了训练过程并减少了计算资源的需求"
        },
        "success": true
    },
    {
        "order": 1000,
        "title": "Sinkhorn Label Allocation: Semi-Supervised Classification via Annealed Self-Training",
        "html": "https://ICML.cc//virtual/2021/poster/9221",
        "abstract": "Self-training is a standard approach to semi-supervised learning where the learner's own predictions on unlabeled data are used as supervision during training. In this paper, we reinterpret this label assignment process as an optimal transportation problem between examples and classes, wherein the cost of assigning an example to a class is mediated by the current predictions of the classifier. This formulation facilitates a practical annealing strategy for label assignment and allows for the inclusion of prior knowledge on class proportions via flexible upper bound constraints. The solutions to these assignment problems can be efficiently approximated using Sinkhorn iteration, thus enabling their use in the inner loop of standard stochastic optimization algorithms. We demonstrate the effectiveness of our algorithm on the CIFAR-10, CIFAR-100, and SVHN datasets in comparison with FixMatch, a state-of-the-art self-training algorithm.",
        "conference": "ICML",
        "中文标题": "Sinkhorn标签分配：通过退火自训练的半监督分类",
        "摘要翻译": "自训练是半监督学习的一种标准方法，其中学习器对未标记数据的预测被用作训练期间的监督。在本文中，我们将这种标签分配过程重新解释为示例和类别之间的最优传输问题，其中将示例分配给类别的成本由分类器的当前预测调节。这种表述为标签分配提供了一种实用的退火策略，并允许通过灵活的上界约束纳入关于类别比例的先前知识。这些分配问题的解决方案可以使用Sinkhorn迭代有效地近似，从而使其能够在标准随机优化算法的内循环中使用。我们在CIFAR-10、CIFAR-100和SVHN数据集上展示了我们算法的有效性，与最先进的自训练算法FixMatch进行了比较。",
        "领域": "半监督学习, 图像分类, 最优传输理论",
        "问题": "如何在半监督学习中更有效地分配未标记数据的标签以提高分类性能",
        "动机": "通过重新解释标签分配过程为最优传输问题，利用分类器的当前预测来调节分配成本，以提高半监督学习的效率和准确性",
        "方法": "将标签分配过程建模为最优传输问题，采用Sinkhorn迭代近似解决方案，并结合退火策略和类比例先验知识的上界约束",
        "关键词": [
            "Sinkhorn迭代",
            "最优传输",
            "半监督学习",
            "退火策略",
            "自训练"
        ],
        "涉及的技术概念": {
            "最优传输": "用于重新解释标签分配过程，将示例分配给类别的成本由分类器的当前预测调节",
            "Sinkhorn迭代": "用于高效近似解决最优传输问题，使其适用于标准随机优化算法的内循环",
            "退火策略": "在标签分配过程中采用的一种实用策略，有助于逐步优化分配结果"
        },
        "success": true
    },
    {
        "order": 1001,
        "title": "Size-Invariant Graph Representations for Graph Classification Extrapolations",
        "html": "https://ICML.cc//virtual/2021/poster/9239",
        "abstract": "In general, graph representation learning methods assume that the train and test data come from the same distribution. In this work we consider an underexplored area of an otherwise rapidly developing field of graph representation learning: The task of out-of-distribution (OOD) graph classification, where train and test data have different distributions, with test data unavailable during training. Our work shows it is possible to use a causal model to learn approximately invariant representations that better extrapolate between train and test data. Finally, we conclude with synthetic and real-world dataset experiments showcasing the benefits of representations that are invariant to train/test distribution shifts.\n",
        "conference": "ICML",
        "中文标题": "图分类外推中的尺寸不变图表示",
        "摘要翻译": "通常，图表示学习方法假设训练和测试数据来自同一分布。在这项工作中，我们考虑了图表示学习这一快速发展领域中一个未被充分探索的方面：分布外（OOD）图分类任务，其中训练和测试数据具有不同的分布，且测试数据在训练期间不可用。我们的工作表明，可以使用因果模型来学习近似不变的表示，从而更好地在训练和测试数据之间进行外推。最后，我们通过合成和真实世界数据集的实验得出结论，展示了对于训练/测试分布变化不变的表示的优势。",
        "领域": "图表示学习、分布外泛化、因果推理",
        "问题": "解决在图分类任务中，训练和测试数据分布不同时的泛化问题。",
        "动机": "探索图表示学习在分布外（OOD）条件下的应用，提高模型在未见数据上的表现。",
        "方法": "采用因果模型学习近似不变的图表示，以增强模型在训练和测试数据分布不同时的外推能力。",
        "关键词": [
            "图表示学习",
            "分布外泛化",
            "因果模型",
            "图分类",
            "不变表示"
        ],
        "涉及的技术概念": {
            "图表示学习": "用于将图结构数据转换为低维向量空间的技术，以便于机器学习模型处理。",
            "分布外泛化": "指模型在训练数据分布与测试数据分布不同的情况下，仍能保持良好的性能。",
            "因果模型": "一种模型，旨在通过识别和利用数据中的因果关系来学习更稳定的表示，以提高模型的泛化能力。"
        },
        "success": true
    },
    {
        "order": 1002,
        "title": "SketchEmbedNet: Learning Novel Concepts by Imitating Drawings",
        "html": "https://ICML.cc//virtual/2021/poster/9851",
        "abstract": "Sketch drawings capture the salient information of visual concepts. Previous \nwork has shown that neural networks are capable of producing sketches of \nnatural objects drawn from a small number of classes. While earlier \napproaches focus on generation quality or retrieval, we explore properties \nof image representations learned by training a model to produce sketches of \nimages. We show that this generative, class-agnostic model produces \ninformative embeddings of images from novel examples, classes, and even \nnovel datasets in a few-shot setting. Additionally, we find that these learned representations exhibit interesting structure and compositionality.",
        "conference": "ICML",
        "中文标题": "SketchEmbedNet：通过模仿绘画学习新概念",
        "摘要翻译": "草图绘画捕捉了视觉概念的显著信息。先前的工作已经表明，神经网络能够绘制出从少量类别中提取的自然物体的草图。虽然早期的方法侧重于生成质量或检索，但我们探索了通过训练模型生成图像草图所学习的图像表示的特性。我们展示了这种生成性的、类别无关的模型能够在少量样本设置下，对来自新示例、新类别甚至新数据集的图像产生信息丰富的嵌入。此外，我们发现这些学习到的表示展现出有趣的结构和组合性。",
        "领域": "图像生成、视觉表示学习、少样本学习",
        "问题": "探索通过生成草图学习的图像表示特性，以及这些表示在新示例、新类别和新数据集上的应用。",
        "动机": "研究如何通过模仿绘画过程，使神经网络能够学习并生成新概念的视觉表示，进而探索这些表示的性质和应用。",
        "方法": "训练一个生成性的、类别无关的模型来生成图像的草图，并分析由此学习的图像表示。",
        "关键词": [
            "草图生成",
            "视觉表示学习",
            "少样本学习",
            "图像嵌入",
            "组合性"
        ],
        "涉及的技术概念": {
            "生成性模型": "用于生成图像的草图，探索图像表示的学习过程。",
            "类别无关学习": "模型不依赖于特定类别，能够处理新类别和新数据集。",
            "少样本学习": "在仅有少量样本的情况下，模型能够学习并生成新概念的表示。"
        },
        "success": true
    },
    {
        "order": 1003,
        "title": "Skew Orthogonal Convolutions",
        "html": "https://ICML.cc//virtual/2021/poster/9675",
        "abstract": "Training convolutional neural networks with a Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients, stable training, etc. While 1-Lipschitz networks can be designed by imposing a 1-Lipschitz constraint on each layer, training such networks requires each layer to be gradient norm preserving (GNP) to prevent gradients from vanishing. However, existing GNP convolutions suffer from slow training, lead to significant reduction in accuracy and provide no guarantees on their approximations. In this work, we propose a GNP convolution layer called \\textbf{S}kew \\textbf{O}rthogonal \\textbf{C}onvolution (SOC) that uses the following mathematical property: when a matrix is {\\it Skew-Symmetric}, its exponential function is an {\\it orthogonal} matrix. To use this property, we first construct a convolution filter whose Jacobian is Skew-Symmetric. Then, we use the Taylor series expansion of the Jacobian exponential to construct the SOC layer that is orthogonal. To efficiently implement SOC, we keep a finite number of terms from the Taylor series and provide a provable guarantee on the approximation error. Our experiments on CIFAR-10 and CIFAR-100 show that SOC allows us to train provably Lipschitz, large convolutional neural networks significantly faster than prior works while achieving significant improvements for both standard and certified robust accuracies. ",
        "conference": "ICML",
        "中文标题": "斜交正交卷积",
        "摘要翻译": "在$l_{2}$范数下训练具有Lipschitz约束的卷积神经网络对于可证明的对抗鲁棒性、可解释的梯度、稳定的训练等是有用的。虽然可以通过在每一层上施加1-Lipschitz约束来设计1-Lipschitz网络，但训练这样的网络需要每一层都是梯度范数保持（GNP）的，以防止梯度消失。然而，现有的GNP卷积存在训练速度慢、导致准确率显著下降且无法保证其近似质量的问题。在这项工作中，我们提出了一种称为斜交正交卷积（SOC）的GNP卷积层，它利用了以下数学性质：当矩阵是斜对称时，其指数函数是一个正交矩阵。为了利用这一性质，我们首先构造一个Jacobian为斜对称的卷积滤波器。然后，我们使用Jacobian指数的泰勒级数展开来构造正交的SOC层。为了高效实现SOC，我们保留了泰勒级数中的有限项，并提供了近似误差的可证明保证。我们在CIFAR-10和CIFAR-100上的实验表明，SOC使我们能够比先前的工作显著更快地训练可证明的Lipschitz大型卷积神经网络，同时在标准和认证的鲁棒准确率上都实现了显著提升。",
        "领域": "对抗性防御、深度学习优化、卷积神经网络",
        "问题": "如何在保持梯度范数（GNP）的同时，提高卷积神经网络的训练效率和准确率",
        "动机": "现有的GNP卷积方法训练速度慢、准确率低且无法保证近似质量，需要一种更高效、更准确的方法",
        "方法": "提出斜交正交卷积（SOC）层，利用斜对称矩阵的指数函数为正交矩阵的性质，通过构造Jacobian为斜对称的卷积滤波器，并使用泰勒级数展开实现正交SOC层",
        "关键词": [
            "斜交正交卷积",
            "Lipschitz约束",
            "梯度范数保持",
            "对抗鲁棒性",
            "泰勒级数展开"
        ],
        "涉及的技术概念": {
            "斜对称矩阵": "用于构造Jacobian为斜对称的卷积滤波器，其指数函数为正交矩阵",
            "泰勒级数展开": "用于近似计算斜对称矩阵的指数函数，实现正交SOC层",
            "梯度范数保持（GNP）": "确保网络训练过程中梯度不消失，保持稳定的训练过程"
        },
        "success": true
    },
    {
        "order": 1004,
        "title": "SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice for Scalable Gaussian Processes",
        "html": "https://ICML.cc//virtual/2021/poster/9749",
        "abstract": "State-of-the-art methods for scalable Gaussian processes use iterative algorithms, requiring fast matrix vector multiplies (MVMs) with the co-variance kernel. The Structured Kernel Interpolation (SKI) framework accelerates these MVMs by performing efficient MVMs on a grid and interpolating back to the original space. In this work, we develop a connection between SKI and the permutohedral lattice used for high-dimensional fast bilateral filtering. Using a sparse simplicial grid instead of a dense rectangular one, we can perform GP inference exponentially faster in the dimension than SKI. Our approach, Simplex-GP, enables scaling SKI to high dimensions, while maintaining strong predictive performance. We additionally provide a CUDA implementation of Simplex-GP, which enables significant GPU acceleration of MVM based inference.",
        "conference": "ICML",
        "中文标题": "在单纯形上滑行：利用排列面体格进行核插值以实现可扩展高斯过程",
        "摘要翻译": "当前可扩展高斯过程的最先进方法使用迭代算法，这需要与协方差核进行快速的矩阵向量乘法（MVMs）。结构化核插值（SKI）框架通过在网格上执行高效的MVMs并插值回原始空间来加速这些MVMs。在这项工作中，我们开发了SKI与用于高维快速双边滤波的排列面体格之间的联系。使用稀疏的单纯形网格而非密集的矩形网格，我们可以在维度上比SKI指数级更快地执行GP推理。我们的方法，Simplex-GP，使得SKI能够扩展到高维度，同时保持强大的预测性能。此外，我们还提供了Simplex-GP的CUDA实现，这使得基于MVM的推理能够显著加速GPU。",
        "领域": "高斯过程、核方法、高维数据处理",
        "问题": "如何在高维空间中高效地执行高斯过程的推理",
        "动机": "为了解决现有方法在高维数据处理中的效率问题，特别是在执行高斯过程推理时的计算瓶颈",
        "方法": "通过开发SKI与排列面体格之间的联系，使用稀疏的单纯形网格替代密集的矩形网格，以实现更高效的GP推理",
        "关键词": [
            "高斯过程",
            "核插值",
            "排列面体格",
            "高维数据处理",
            "GPU加速"
        ],
        "涉及的技术概念": {
            "结构化核插值（SKI）": "一种通过网格上的高效矩阵向量乘法和插值来加速高斯过程推理的框架",
            "排列面体格": "用于高维快速双边滤波的一种结构，本研究将其应用于高斯过程的核插值中",
            "单纯形网格": "一种稀疏的网格结构，用于替代传统的密集矩形网格，以指数级提高高维空间中的计算效率"
        },
        "success": true
    },
    {
        "order": 1005,
        "title": "Skill Discovery for Exploration and Planning using Deep Skill Graphs",
        "html": "https://ICML.cc//virtual/2021/poster/9497",
        "abstract": "We introduce a new skill-discovery algorithm that builds a discrete graph representation of large continuous MDPs, where nodes correspond to skill subgoals and the edges  to skill policies. The agent constructs this graph during an unsupervised training phase where it interleaves discovering skills and planning using them to gain coverage over ever-increasing portions of the state-space. Given a novel goal at test time, the agent plans with the acquired skill graph to reach a nearby state, then switches to learning to reach the goal. We show that the resulting algorithm, Deep Skill Graphs, outperforms both flat and existing hierarchical reinforcement learning methods on four difficult continuous control tasks.",
        "conference": "ICML",
        "中文标题": "利用深度技能图进行探索与规划的技能发现",
        "摘要翻译": "我们引入了一种新的技能发现算法，该算法构建了大型连续马尔可夫决策过程（MDPs）的离散图表示，其中节点对应于技能子目标，边对应于技能策略。代理在无监督训练阶段构建此图，期间它交替发现技能和利用这些技能进行规划，以覆盖状态空间中不断增长的部分。在测试时给定一个新目标，代理利用获得的技能图规划到达附近状态，然后切换到学习以达到目标。我们展示了由此产生的算法——深度技能图，在四个困难的连续控制任务上优于平面和现有的分层强化学习方法。",
        "领域": "强化学习、连续控制、技能发现",
        "问题": "如何在大型连续MDPs中有效地发现和利用技能进行探索与规划",
        "动机": "提高在复杂连续控制任务中的探索效率和规划能力",
        "方法": "构建离散技能图表示，通过无监督训练交替发现技能和规划，利用技能图进行目标导向的学习",
        "关键词": [
            "技能发现",
            "深度技能图",
            "连续控制",
            "强化学习",
            "规划"
        ],
        "涉及的技术概念": {
            "技能子目标": "在技能图中表示的具体技能目标点，用于指导代理的探索和规划",
            "技能策略": "连接技能子目标的边，代表代理从一个子目标到达另一个子目标的策略",
            "无监督训练": "代理在没有外部奖励信号的情况下，通过自我探索构建技能图的过程"
        },
        "success": true
    },
    {
        "order": 1006,
        "title": "Sliced Iterative Normalizing Flows",
        "html": "https://ICML.cc//virtual/2021/poster/9119",
        "abstract": "We develop an iterative (greedy) deep learning (DL) algorithm which is able to transform an arbitrary probability distribution function (PDF) into the target PDF. The model is based on iterative Optimal Transport of a series of 1D slices, matching on each slice the marginal PDF to the target. The axes of the orthogonal slices are chosen to maximize the PDF difference using Wasserstein distance at each iteration, which enables the algorithm to scale well to high dimensions. As special cases of this algorithm, we introduce two sliced iterative Normalizing Flow (SINF) models, which map from the data to the latent space (GIS) and vice versa (SIG). We show that SIG is able to generate high quality samples of image datasets, which match the GAN benchmarks, while GIS obtains competitive results on density estimation tasks compared to the density trained NFs, and is more stable, faster, and achieves higher p(x) when trained on small training sets. SINF approach deviates significantly from the current DL paradigm, as it is greedy and does not use concepts such as mini-batching, stochastic gradient descent and gradient back-propagation through deep layers.",
        "conference": "ICML",
        "success": true,
        "中文标题": "切片迭代归一化流",
        "摘要翻译": "我们开发了一种迭代（贪婪）深度学习（DL）算法，该算法能够将任意概率分布函数（PDF）转换为目标PDF。该模型基于一系列一维切片的迭代最优传输，在每个切片上将边际PDF与目标匹配。正交切片的轴选择在每次迭代中最大化使用Wasserstein距离的PDF差异，这使得算法能够很好地扩展到高维度。作为该算法的特例，我们引入了两种切片迭代归一化流（SINF）模型，它们分别从数据映射到潜在空间（GIS）和反之（SIG）。我们展示了SIG能够生成高质量的图像数据集样本，这些样本与GAN基准相匹配，而GIS在密度估计任务上获得了与密度训练的NFs相比具有竞争力的结果，并且在小训练集上训练时更稳定、更快、达到更高的p(x)。SINF方法与当前的DL范式有显著不同，因为它是贪婪的，并且不使用诸如小批量、随机梯度下降和通过深层梯度反向传播等概念。",
        "领域": "概率密度估计, 生成对抗网络, 归一化流",
        "问题": "如何高效地将任意概率分布函数转换为目标概率分布函数",
        "动机": "开发一种能够高效转换概率分布函数且在高维度中表现良好的算法",
        "方法": "基于迭代最优传输和一维切片的方法，引入两种切片迭代归一化流模型",
        "关键词": [
            "归一化流",
            "最优传输",
            "Wasserstein距离",
            "概率密度估计",
            "生成对抗网络"
        ],
        "涉及的技术概念": {
            "最优传输": "用于在每个切片上将边际PDF与目标匹配的技术",
            "Wasserstein距离": "用于最大化PDF差异的度量，使算法能够扩展到高维度",
            "归一化流": "一种用于密度估计和生成模型的深度学习方法，通过一系列可逆变换将简单分布转换为复杂分布"
        }
    },
    {
        "order": 1007,
        "title": "Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/9155",
        "abstract": "In contrast to traditional weight optimization in a continuous space, we demonstrate the existence of effective random networks whose weights are never updated. By selecting a weight among a fixed set of random values for each individual connection, our method uncovers combinations of random weights that match the performance of traditionally-trained networks of the same capacity. We refer to our networks as 'slot machines' where each reel (connection) contains a fixed set of symbols (random values). Our backpropagation algorithm 'spins' the reels to seek 'winning'  combinations, i.e., selections of random weight values that minimize the given loss. Quite surprisingly, we find that allocating just a few random values to each connection (e.g., 8 values per connection) yields highly competitive combinations despite being dramatically more constrained compared to traditionally learned weights. Moreover, finetuning these combinations often improves performance over the trained baselines.  A randomly initialized VGG-19 with 8 values per connection contains a combination that achieves 91% test accuracy on CIFAR-10. Our method also achieves an impressive performance of 98.2% on MNIST for neural networks containing only random weights. ",
        "conference": "ICML",
        "中文标题": "老虎机：在神经网络中发现随机权重的获胜组合",
        "摘要翻译": "与传统在连续空间中进行权重优化不同，我们证明了存在有效的随机网络，其权重从未被更新。通过为每个连接从一组固定的随机值中选择一个权重，我们的方法发现了与传统训练网络相同容量的随机权重组合，其性能相当。我们将我们的网络称为‘老虎机’，其中每个卷轴（连接）包含一组固定的符号（随机值）。我们的反向传播算法‘旋转’卷轴以寻找‘获胜’组合，即选择能够最小化给定损失的随机权重值。令人惊讶的是，我们发现即使每个连接只分配几个随机值（例如，每个连接8个值），也能产生极具竞争力的组合，尽管与传统学习的权重相比受到更大的限制。此外，对这些组合进行微调通常可以提高性能，超过训练基线。一个随机初始化的VGG-19，每个连接有8个值，包含一个在CIFAR-10上达到91%测试准确率的组合。我们的方法在仅包含随机权重的神经网络上，在MNIST上也达到了98.2%的令人印象深刻的性能。",
        "领域": "神经网络优化, 深度学习, 随机权重初始化",
        "问题": "如何在神经网络中发现有效的随机权重组合，而无需传统的权重优化过程",
        "动机": "探索和证明存在有效的随机网络，其权重无需更新即可达到与传统训练网络相当的性能",
        "方法": "通过为每个神经网络连接从一组固定的随机值中选择权重，并使用反向传播算法寻找最小化损失的随机权重组合",
        "关键词": [
            "随机权重",
            "神经网络优化",
            "反向传播",
            "老虎机模型",
            "权重选择"
        ],
        "涉及的技术概念": {
            "随机权重": "在神经网络中使用随机初始化的权重，而非通过传统训练方法优化的权重",
            "反向传播算法": "用于寻找能够最小化损失的随机权重组合的算法",
            "老虎机模型": "比喻为每个神经网络连接从一组固定的随机值中选择权重，类似于老虎机旋转卷轴寻找获胜组合的过程"
        },
        "success": true
    },
    {
        "order": 1008,
        "title": "SMG: A Shuffling Gradient-Based Method with Momentum",
        "html": "https://ICML.cc//virtual/2021/poster/8437",
        "abstract": "We combine two advanced ideas widely used in optimization for machine learning: \\textit{shuffling} strategy and \\textit{momentum} technique to develop a novel shuffling gradient-based method with momentum, coined \\textbf{S}huffling \\textbf{M}omentum \\textbf{G}radient (SMG),  for non-convex finite-sum optimization problems. \nWhile our method is inspired by momentum techniques, its update is fundamentally different from existing momentum-based methods.\nWe establish state-of-the-art convergence rates of SMG for any shuffling strategy using either constant or diminishing learning rate under standard assumptions (i.e. \\textit{$L$-smoothness} and \\textit{bounded variance}).\nWhen the shuffling strategy is fixed, we develop another new algorithm that is similar to existing momentum methods,\nand prove the same convergence rates for this algorithm under the $L$-smoothness and bounded gradient assumptions. \nWe demonstrate our algorithms via numerical simulations on standard datasets and compare them with existing shuffling methods.\nOur tests have shown encouraging performance of the new algorithms.",
        "conference": "ICML",
        "中文标题": "SMG：一种基于动量与梯度洗牌的方法",
        "摘要翻译": "我们结合了机器学习优化中广泛使用的两种先进思想：洗牌策略和动量技术，开发了一种新颖的基于梯度洗牌的方法，命名为洗牌动量梯度（SMG），用于非凸有限和优化问题。虽然我们的方法受到动量技术的启发，但其更新方式与现有的基于动量的方法有根本的不同。我们在标准假设（即L-平滑性和有界方差）下，使用恒定或递减学习率，为任何洗牌策略建立了SMG的最先进收敛率。当洗牌策略固定时，我们开发了另一种类似于现有动量方法的新算法，并在L-平滑性和有界梯度假设下证明了该算法的相同收敛率。我们通过在标准数据集上的数值模拟展示了我们的算法，并与现有的洗牌方法进行了比较。我们的测试显示了新算法的令人鼓舞的性能。",
        "领域": "优化算法、机器学习、非凸优化",
        "问题": "解决非凸有限和优化问题中的梯度洗牌和动量技术结合问题",
        "动机": "结合洗牌策略和动量技术，开发一种新的优化方法以提高收敛性能",
        "方法": "开发了一种基于梯度洗牌和动量技术的新方法SMG，并在不同假设下证明了其收敛率",
        "关键词": [
            "洗牌动量梯度",
            "非凸优化",
            "机器学习优化",
            "收敛率",
            "数值模拟"
        ],
        "涉及的技术概念": {
            "洗牌策略": "在优化过程中随机或按特定顺序选择数据子集，以提高模型的泛化能力和收敛速度",
            "动量技术": "通过引入历史梯度信息来加速优化过程，帮助克服局部最优和加速收敛",
            "L-平滑性": "描述函数梯度变化率的假设，用于分析优化算法的收敛性能"
        },
        "success": true
    },
    {
        "order": 1009,
        "title": "Smooth $p$-Wasserstein Distance: Structure, Empirical Approximation, and Statistical Applications",
        "html": "https://ICML.cc//virtual/2021/poster/9437",
        "abstract": "Discrepancy measures between probability distributions, often termed statistical distances, are ubiquitous in probability theory, statistics and machine learning. To combat the curse of dimensionality when estimating these distances from data, recent work has proposed smoothing out local irregularities in the measured distributions via convolution with a Gaussian kernel. Motivated by the scalability of this framework to high dimensions, we investigate the structural and statistical behavior of the Gaussian-smoothed $p$-Wasserstein distance $\\mathsf{W}_p^{(\\sigma)}$, for arbitrary $p\\geq 1$. After establishing basic metric and topological properties of $\\mathsf{W}_p^{(\\sigma)}$, we explore the asymptotic statistical properties of $\\mathsf{W}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$, where $\\hat{\\mu}_n$ is the empirical distribution of $n$ independent observations from $\\mu$. We prove that $\\mathsf{W}_p^{(\\sigma)}$ enjoys a parametric empirical convergence rate of $n^{-1/2}$, which contrasts the $n^{-1/d}$ rate for unsmoothed $\\Wp$ when $d \\geq 3$. Our proof relies on controlling $\\mathsf{W}_p^{(\\sigma)}$ by a $p$th-order smooth Sobolev distance $\\mathsf{d}_p^{(\\sigma)}$ and deriving the limit distribution of $\\sqrt{n}\\,\\mathsf{d}_p^{(\\sigma)}(\\hat{\\mu}_n,\\mu)$ for all dimensions $d$. As applications, we provide asymptotic guarantees for two-sample testing and minimum distance estimation using $\\mathsf{W}_p^{(\\sigma)}$, with experiments for $p=2$ using a maximum mean discrepancy formulation~of~$\\mathsf{d}_2^{(\\sigma)}$.",
        "conference": "ICML",
        "success": true,
        "中文标题": "平滑p-Wasserstein距离：结构、经验近似与统计应用",
        "摘要翻译": "概率分布之间的差异度量，通常称为统计距离，在概率论、统计学和机器学习中无处不在。为了在从数据估计这些距离时对抗维数灾难，最近的工作提出了通过高斯核卷积来平滑测量分布中的局部不规则性。受这一框架在高维情况下的可扩展性启发，我们研究了高斯平滑的p-Wasserstein距离Wp(σ)的结构和统计行为，对于任意的p≥1。在确立了Wp(σ)的基本度量和拓扑性质后，我们探讨了Wp(σ)(μ̂n,μ)的渐近统计性质，其中μ̂n是来自μ的n个独立观测的经验分布。我们证明了Wp(σ)享有n−1/2的参数经验收敛率，这与当d≥3时未平滑的Wp的n−1/d率形成对比。我们的证明依赖于通过p阶平滑Sobolev距离dp(σ)控制Wp(σ)，并为所有维度d推导出√n dp(σ)(μ̂n,μ)的极限分布。作为应用，我们为使用Wp(σ)的两样本测试和最小距离估计提供了渐近保证，并通过使用d2(σ)的最大均值差异公式对p=2进行了实验。",
        "领域": "概率度量学习, 高维统计, 非参数统计",
        "问题": "研究高斯平滑的p-Wasserstein距离的结构和统计行为，以对抗高维数据中的维数灾难问题。",
        "动机": "探索高斯平滑p-Wasserstein距离在高维数据中的可扩展性和统计性质，以提供更有效的统计距离度量。",
        "方法": "通过高斯核卷积平滑概率分布，研究平滑后的p-Wasserstein距离的度量和拓扑性质，以及其经验收敛率和极限分布。",
        "关键词": [
            "高斯平滑",
            "Wasserstein距离",
            "维数灾难",
            "经验收敛率",
            "两样本测试"
        ],
        "涉及的技术概念": {
            "高斯平滑": "通过高斯核卷积平滑概率分布，以减少高维数据中的局部不规则性。",
            "Wasserstein距离": "用于度量两个概率分布之间差异的距离，平滑后在高维数据中具有更好的统计性质。",
            "Sobolev距离": "一种平滑的距离度量，用于控制平滑后的Wasserstein距离，并推导其极限分布。"
        }
    },
    {
        "order": 1010,
        "title": "Soft then Hard: Rethinking the Quantization in Neural Image Compression",
        "html": "https://ICML.cc//virtual/2021/poster/8433",
        "abstract": "Quantization is one of the core components in lossy image compression. For neural image compression, end-to-end optimization requires differentiable approximations of quantization, which can generally be grouped into three categories: additive uniform noise, straight-through estimator and soft-to-hard annealing. Training with additive uniform noise approximates the quantization error variationally but suffers from the train-test mismatch. The other two methods do not encounter this mismatch but, as shown in this paper, hurt the rate-distortion performance since the latent representation ability is weakened. We thus propose a novel soft-then-hard quantization strategy for neural image compression that first learns an expressive latent space softly, then closes the train-test mismatch with hard quantization. In addition, beyond the fixed integer-quantization, we apply scaled additive uniform noise to adaptively control the quantization granularity by deriving a new variational upper bound on actual rate. Experiments demonstrate that our proposed methods are easy to adopt, stable to train, and highly effective especially on complex compression models.",
        "conference": "ICML",
        "中文标题": "软后硬：重新思考神经图像压缩中的量化",
        "摘要翻译": "量化是有损图像压缩的核心组成部分之一。对于神经图像压缩，端到端优化需要量化的可微分近似，这通常可以分为三类：加性均匀噪声、直通估计器和软到硬退火。使用加性均匀噪声训练可以变分地近似量化误差，但会受到训练-测试不匹配的影响。其他两种方法不会遇到这种不匹配，但如本文所示，由于潜在表示能力被削弱，会损害率失真性能。因此，我们提出了一种新颖的软后硬量化策略，用于神经图像压缩，首先软性地学习一个表达性强的潜在空间，然后通过硬量化关闭训练-测试不匹配。此外，超越固定的整数量化，我们应用了缩放加性均匀噪声，通过推导实际速率的新变分上界来自适应控制量化粒度。实验证明，我们提出的方法易于采用，训练稳定，并且特别在复杂压缩模型上非常有效。",
        "领域": "神经图像压缩、量化技术、率失真优化",
        "问题": "解决神经图像压缩中量化方法导致的训练-测试不匹配和潜在表示能力削弱的问题",
        "动机": "提高神经图像压缩的率失真性能，通过改进量化策略减少训练与测试之间的不匹配",
        "方法": "提出软后硬量化策略，先软性学习潜在空间，后硬量化关闭不匹配；应用缩放加性均匀噪声自适应控制量化粒度",
        "关键词": [
            "神经图像压缩",
            "量化策略",
            "率失真优化",
            "潜在空间学习",
            "变分上界"
        ],
        "涉及的技术概念": {
            "软后硬量化策略": "先软性学习潜在空间，后硬量化关闭训练-测试不匹配的策略",
            "缩放加性均匀噪声": "用于自适应控制量化粒度，通过推导新的变分上界来优化实际速率",
            "率失真性能": "衡量压缩算法性能的指标，平衡压缩率与图像失真"
        },
        "success": true
    },
    {
        "order": 1011,
        "title": "Solving Challenging Dexterous Manipulation Tasks With Trajectory Optimisation and Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8517",
        "abstract": "Training agents to autonomously control anthropomorphic robotic hands has the potential to lead to systems capable of performing a multitude of complex manipulation tasks in unstructured and uncertain environments. In this work, we first introduce a suite of challenging simulated manipulation tasks where current reinforcement learning and trajectory optimisation techniques perform poorly. These include environments where two simulated hands have to pass or throw objects between each other, as well as an environment where the agent must learn to spin a long pen between its fingers. We then introduce a simple trajectory optimisation algorithm that performs significantly better than existing methods on these environments. Finally, on the most challenging ``PenSpin' task, we combine sub-optimal demonstrations generated through trajectory optimisation with off-policy reinforcement learning, obtaining performance that far exceeds either of these approaches individually. Videos of all of our results are available at: https://dexterous-manipulation.github.io",
        "conference": "ICML",
        "中文标题": "利用轨迹优化和强化学习解决具有挑战性的灵巧操作任务",
        "摘要翻译": "训练智能体自主控制仿人机器人手有潜力开发出能够在非结构化和不确定环境中执行多种复杂操作任务的系统。在这项工作中，我们首先介绍了一套具有挑战性的模拟操作任务，这些任务中当前的强化学习和轨迹优化技术表现不佳。包括两个模拟手之间需要传递或投掷物体的环境，以及一个智能体必须学会在手指间旋转长笔的环境。然后，我们介绍了一种简单的轨迹优化算法，在这些环境中表现显著优于现有方法。最后，在最具挑战性的“PenSpin”任务上，我们将通过轨迹优化生成的次优演示与离策略强化学习相结合，获得的性能远超这两种方法单独使用时的表现。我们所有结果的视频可在以下网址查看：https://dexterous-manipulation.github.io",
        "领域": "机器人操作、强化学习应用、轨迹优化",
        "问题": "解决在非结构化和不确定环境中，仿人机器人手执行复杂操作任务时，现有强化学习和轨迹优化技术表现不佳的问题",
        "动机": "开发能够自主控制仿人机器人手执行复杂操作任务的智能体，以应对非结构化和不确定环境中的挑战",
        "方法": "结合轨迹优化算法和离策略强化学习，特别是在最具挑战性的任务中利用轨迹优化生成的次优演示与强化学习相结合",
        "关键词": [
            "仿人机器人手",
            "轨迹优化",
            "强化学习",
            "灵巧操作",
            "离策略学习"
        ],
        "涉及的技术概念": {
            "轨迹优化": "用于生成操作任务的次优演示，作为强化学习训练的起点",
            "强化学习": "用于训练智能体自主控制仿人机器人手执行复杂操作任务",
            "离策略学习": "允许智能体从非当前策略生成的数据中学习，提高学习效率和性能"
        },
        "success": true
    },
    {
        "order": 1012,
        "title": "Solving high-dimensional parabolic PDEs using the tensor train format",
        "html": "https://ICML.cc//virtual/2021/poster/9927",
        "abstract": "High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid-based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.",
        "conference": "ICML",
        "中文标题": "使用张量列车格式解决高维抛物型偏微分方程",
        "摘要翻译": "高维偏微分方程（PDEs）在经济学、科学和工程学中无处不在。然而，它们的数值处理提出了巨大的挑战，因为传统的基于网格的方法往往会受到维度诅咒的困扰。在本文中，我们认为张量列车为抛物型偏微分方程提供了一个吸引人的近似框架：结合后向随机微分方程的重新表述和张量格式中的回归型方法，有望利用潜在的低秩结构实现压缩和高效计算。遵循这一范式，我们开发了新颖的迭代方案，包括快速显式更新和精确隐式更新。通过一系列例子，我们证明了与最先进的基于神经网络的方法相比，我们的方法在准确性和计算效率之间实现了有利的权衡。",
        "领域": "数值分析、偏微分方程求解、张量分解",
        "问题": "解决高维抛物型偏微分方程的数值处理难题",
        "动机": "克服传统网格方法在处理高维偏微分方程时的维度诅咒问题",
        "方法": "结合后向随机微分方程的重新表述和张量格式中的回归型方法，开发新颖的迭代方案",
        "关键词": [
            "高维偏微分方程",
            "张量列车",
            "数值解法",
            "低秩结构",
            "计算效率"
        ],
        "涉及的技术概念": {
            "张量列车": "用于高维数据的压缩表示和高效计算，通过低秩近似减少计算复杂度",
            "后向随机微分方程": "提供了一种重新表述偏微分方程的方法，便于数值求解",
            "低秩结构": "被利用来实现数据的压缩和计算的高效性，是方法能够克服维度诅咒的关键"
        },
        "success": true
    },
    {
        "order": 1013,
        "title": "Solving Inverse Problems with a Flow-based Noise Model",
        "html": "https://ICML.cc//virtual/2021/poster/9451",
        "abstract": "We study image inverse problems with a normalizing flow prior. Our formulation views the solution as the maximum a posteriori estimate of the image conditioned on the measurements. This formulation allows us to use noise models with arbitrary dependencies as well as non-linear forward operators. We empirically validate the efficacy of our method on various inverse problems, including compressed sensing with quantized measurements and denoising with highly structured noise patterns. We also present initial theoretical recovery guarantees for solving inverse problems with a flow prior.",
        "conference": "ICML",
        "中文标题": "使用基于流的噪声模型解决逆问题",
        "摘要翻译": "我们研究了使用归一化流先验的图像逆问题。我们的公式将解视为基于测量条件的图像的最大后验估计。这一公式允许我们使用具有任意依赖性的噪声模型以及非线性前向算子。我们通过实验验证了我们的方法在各种逆问题上的有效性，包括量化测量的压缩感知和具有高度结构化噪声模式的去噪。我们还提出了使用流先验解决逆问题的初步理论恢复保证。",
        "领域": "图像恢复、压缩感知、去噪",
        "问题": "解决图像逆问题，特别是在存在复杂噪声和非线性前向算子的情况下。",
        "动机": "探索使用归一化流先验和最大后验估计来解决更广泛的逆问题，包括那些传统方法难以处理的情况。",
        "方法": "采用归一化流先验和最大后验估计方法，结合任意依赖性的噪声模型和非线性前向算子，解决图像逆问题。",
        "关键词": [
            "归一化流",
            "逆问题",
            "最大后验估计",
            "压缩感知",
            "去噪"
        ],
        "涉及的技术概念": {
            "归一化流": "用于建立图像先验分布，帮助模型更好地理解和处理图像数据。",
            "最大后验估计": "用于基于测量条件估计图像，是解决逆问题的核心方法。",
            "非线性前向算子": "在逆问题中模拟复杂的图像退化过程，允许处理更广泛的实际情况。"
        },
        "success": true
    },
    {
        "order": 1014,
        "title": "SoundDet: Polyphonic Moving Sound Event Detection and Localization from Raw Waveform",
        "html": "https://ICML.cc//virtual/2021/poster/8603",
        "abstract": "We present a new framework SoundDet, which is an end-to-end trainable and light-weight framework, for polyphonic moving sound event detection and localization. Prior methods typically approach this problem by preprocessing raw waveform into time-frequency representations, which is more amenable to process with well-established image processing pipelines. Prior methods also detect in segment-wise manner, leading to incomplete and partial detections. SoundDet takes a novel approach and directly consumes the raw, multichannel waveform and treats the spatio-temporal sound event as a complete ``sound-object' to be detected. Specifically, SoundDet consists of a backbone neural network and two parallel heads for temporal detection and spatial localization, respectively. Given the large sampling rate of raw waveform, the backbone network first learns a set of phase-sensitive and frequency-selective bank of filters to explicitly retain direction-of-arrival information, whilst being highly computationally and parametrically efficient than standard 1D/2D convolution. A dense sound event proposal map is then constructed to handle the challenges of predicting events with large varying temporal duration. Accompanying the dense proposal map are a temporal overlapness map and a motion smoothness map that measure a proposal's confidence to be an event from temporal detection accuracy and movement consistency perspective. Involving the two maps guarantees SoundDet to be trained in a spatio-temporally unified manner. Experimental results on the public DCASE dataset show the advantage of SoundDet on both segment-based evaluation and our newly proposed event-based evaluation system.",
        "conference": "ICML",
        "中文标题": "SoundDet：基于原始波形的多音移动声音事件检测与定位",
        "摘要翻译": "我们提出了一个新的框架SoundDet，这是一个端到端可训练的轻量级框架，用于多音移动声音事件的检测与定位。以往的方法通常通过将原始波形预处理为时频表示来解决这一问题，这更适合于使用成熟的图像处理流程进行处理。以往的方法还以分段方式进行检测，导致检测不完整和部分检测。SoundDet采用了一种新颖的方法，直接处理原始的多通道波形，并将时空声音事件视为一个完整的‘声音对象’进行检测。具体来说，SoundDet由一个骨干神经网络和两个分别用于时间检测和空间定位的并行头部组成。鉴于原始波形的大采样率，骨干网络首先学习一组相位敏感和频率选择的滤波器组，以明确保留到达方向信息，同时比标准的1D/2D卷积在计算和参数上更为高效。然后构建一个密集的声音事件提议图，以应对预测具有大变化时间持续时间事件的挑战。伴随密集提议图的是一个时间重叠图和一个运动平滑图，它们从时间检测准确性和运动一致性的角度衡量提议作为事件的置信度。涉及这两个图保证了SoundDet能够在时空统一的方式下进行训练。在公开的DCASE数据集上的实验结果表明，SoundDet在基于片段的评估和我们新提出的事件基于评估系统上都具有优势。",
        "领域": "声音事件检测, 声音定位, 多音处理",
        "问题": "解决多音移动声音事件的检测与定位问题，特别是在处理原始波形时保持高效性和准确性。",
        "动机": "以往的方法在处理原始波形时需要进行预处理，且以分段方式进行检测，导致效率低下和检测不完整。SoundDet旨在直接处理原始波形，提高检测和定位的效率和准确性。",
        "方法": "采用端到端可训练的轻量级框架，直接处理原始多通道波形，通过骨干网络学习相位敏感和频率选择的滤波器组，构建密集声音事件提议图，并结合时间重叠图和运动平滑图进行时空统一的训练。",
        "关键词": [
            "多音处理",
            "声音事件检测",
            "声音定位",
            "原始波形处理",
            "时空统一训练"
        ],
        "涉及的技术概念": {
            "相位敏感和频率选择的滤波器组": "用于在骨干网络中明确保留到达方向信息，提高处理效率。",
            "密集声音事件提议图": "用于应对预测具有大变化时间持续时间事件的挑战。",
            "时间重叠图和运动平滑图": "用于从时间检测准确性和运动一致性的角度衡量提议作为事件的置信度，保证时空统一的训练方式。"
        },
        "success": true
    },
    {
        "order": 1015,
        "title": "SPADE: A Spectral Method for Black-Box Adversarial Robustness Evaluation",
        "html": "https://ICML.cc//virtual/2021/poster/8987",
        "abstract": " A black-box spectral method is introduced for evaluating the adversarial robustness of a given machine learning (ML) model. Our approach, named SPADE, exploits bijective distance mapping between the input/output graphs constructed for approximating the manifolds corresponding to the input/output data. By leveraging the generalized Courant-Fischer theorem, we propose a SPADE score for evaluating the adversarial robustness of a given model, which is proved to be an upper bound of the best Lipschitz constant under the manifold setting. To reveal the most non-robust data samples highly vulnerable to adversarial attacks, we develop a spectral graph embedding procedure leveraging dominant generalized eigenvectors. This embedding step allows assigning each data point a robustness score that can be further harnessed for more effective adversarial training of ML models. Our experiments show promising empirical results for neural networks trained with the MNIST and CIFAR-10 data sets.",
        "conference": "ICML",
        "中文标题": "SPADE：一种用于黑盒对抗鲁棒性评估的谱方法",
        "摘要翻译": "本文介绍了一种用于评估给定机器学习（ML）模型对抗鲁棒性的黑盒谱方法，命名为SPADE。我们的方法利用输入/输出图之间的双射距离映射来近似对应于输入/输出数据的流形。通过利用广义Courant-Fischer定理，我们提出了一个SPADE评分来评估给定模型的对抗鲁棒性，该评分被证明是流形设置下最佳Lipschitz常数的上界。为了揭示最易受对抗攻击影响的非鲁棒数据样本，我们开发了一种利用主导广义特征向量的谱图嵌入过程。这一嵌入步骤允许为每个数据点分配一个鲁棒性评分，该评分可以进一步用于更有效的ML模型对抗训练。我们的实验在MNIST和CIFAR-10数据集上训练的神经网络中显示出了有希望的实证结果。",
        "领域": "对抗机器学习、谱方法、模型鲁棒性评估",
        "问题": "如何有效评估机器学习模型在黑盒设置下的对抗鲁棒性",
        "动机": "开发一种无需了解模型内部结构即可评估其对抗鲁棒性的方法，以识别模型弱点并指导更有效的对抗训练",
        "方法": "利用谱方法和广义Courant-Fischer定理，通过构建输入/输出图的双射距离映射来评估模型鲁棒性，并开发谱图嵌入过程识别易受攻击样本",
        "关键词": [
            "对抗鲁棒性",
            "谱方法",
            "黑盒评估",
            "SPADE评分",
            "对抗训练"
        ],
        "涉及的技术概念": {
            "双射距离映射": "用于近似输入/输出数据流形的技术，通过构建输入/输出图之间的映射关系",
            "广义Courant-Fischer定理": "用于推导SPADE评分，证明其为流形设置下最佳Lipschitz常数的上界",
            "谱图嵌入": "利用主导广义特征向量为数据点分配鲁棒性评分，识别易受对抗攻击的样本"
        },
        "success": true
    },
    {
        "order": 1016,
        "title": "Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm",
        "html": "https://ICML.cc//virtual/2021/poster/10035",
        "abstract": "Sparse adversarial attacks can fool deep neural networks (DNNs) by only perturbing a few pixels (regularized by $\\ell_0$ norm). Recent efforts combine it with another $\\ell_\\infty$ imperceptible on the perturbation magnitudes. The resultant sparse and imperceptible attacks are practically relevant, and indicate an even higher vulnerability of DNNs that we usually imagined. However, such attacks are more challenging to generate due to the optimization difficulty by coupling the $\\ell_0$ regularizer and box constraints with a non-convex objective. In this paper, we address this challenge by proposing a homotopy algorithm, to jointly tackle the sparsity and the perturbation bound in one unified framework. Each iteration, the main step of our algorithm is to optimize an $\\ell_0$-regularized adversarial loss, by leveraging the nonmonotone Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is followed by an $\\ell_0$ change control step, and an optional post-attack step designed to escape bad local minima. We also extend the algorithm to handling the structural sparsity regularizer. We extensively examine the effectiveness of our proposed \\textbf{homotopy attack} for both targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art methods, our homotopy attack leads to significantly fewer  perturbations, e.g., reducing  42.91\\% on CIFAR-10 and 75.03\\% on ImageNet (average case, targeted attack), at similar maximal perturbation magnitudes, when still achieving 100\\% attack success rates. Our codes are available at: {\\small\\url{https://github.com/VITA-Group/SparseADV_Homotopy}}.",
        "conference": "ICML",
        "中文标题": "通过同伦算法实现稀疏且不易察觉的对抗攻击",
        "摘要翻译": "稀疏对抗攻击仅通过扰动少量像素（由ℓ0范数正则化）即可欺骗深度神经网络（DNNs）。最近的研究将其与另一种在扰动幅度上不易察觉的ℓ∞范数相结合。这种稀疏且不易察觉的攻击在实际中具有重要意义，表明DNNs的脆弱性比我们通常想象的要高。然而，由于将ℓ0正则化器和箱约束与非凸目标耦合在一起的优化困难，这种攻击更难生成。在本文中，我们通过提出一种同伦算法来解决这一挑战，以在一个统一的框架中共同解决稀疏性和扰动边界问题。每次迭代中，我们算法的主要步骤是通过利用非单调加速近端梯度法（nmAPG）进行非凸编程，优化一个ℓ0正则化的对抗损失；随后是一个ℓ0变化控制步骤，以及一个设计用于逃离不良局部极小值的可选后攻击步骤。我们还将算法扩展到处理结构稀疏正则化器。我们广泛检验了我们提出的同伦攻击在CIFAR-10和ImageNet数据集上针对目标和非目标攻击场景的有效性。与最先进的方法相比，我们的同伦攻击在相似的最大扰动幅度下，仍能实现100%的攻击成功率，但扰动显著减少，例如在CIFAR-10上减少42.91%，在ImageNet上减少75.03%（平均情况，目标攻击）。我们的代码可在https://github.com/VITA-Group/SparseADV_Homotopy获取。",
        "领域": "对抗样本生成、深度神经网络安全、图像分类",
        "问题": "如何在保持攻击不易察觉的同时，减少对抗样本所需的扰动像素数量",
        "动机": "揭示深度神经网络在稀疏且不易察觉的对抗攻击下的更高脆弱性，并提出有效的攻击生成方法",
        "方法": "提出一种同伦算法，结合非单调加速近端梯度法（nmAPG）优化ℓ0正则化的对抗损失，通过ℓ0变化控制步骤和可选的后攻击步骤，共同解决稀疏性和扰动边界问题",
        "关键词": [
            "稀疏对抗攻击",
            "同伦算法",
            "非单调加速近端梯度法",
            "ℓ0正则化",
            "深度神经网络安全"
        ],
        "涉及的技术概念": {
            "同伦算法": "用于在一个统一的框架中共同解决稀疏性和扰动边界问题，通过连续变换优化问题来逼近解",
            "非单调加速近端梯度法（nmAPG）": "用于优化非凸的ℓ0正则化对抗损失，加速收敛并处理非凸性",
            "ℓ0正则化": "用于控制对抗攻击中的稀疏性，即限制扰动的像素数量"
        },
        "success": true
    },
    {
        "order": 1017,
        "title": "Sparse Bayesian Learning via Stepwise Regression",
        "html": "https://ICML.cc//virtual/2021/poster/9953",
        "abstract": "Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity in probabilistic models. Herein, we propose a coordinate ascent algorithm for SBL termed Relevance Matching Pursuit (RMP) and show that, as its noise variance parameter goes to zero, RMP exhibits a surprising connection to Stepwise Regression. Further, we derive novel guarantees for Stepwise Regression algorithms, which also shed light on RMP. Our guarantees for Forward Regression improve on deterministic and probabilistic results for Orthogonal Matching Pursuit with noise. Our analysis of Backward Regression culminates in a bound on the residual of the optimal solution to the subset selection problem that, if satisfied, guarantees the optimality of the result. To our knowledge, this bound is the first that can be computed in polynomial time and depends chiefly on the smallest singular value of the matrix. We report numerical experiments using a variety of feature selection algorithms. Notably, RMP and its limiting variant are both efficient and maintain strong performance with correlated features.",
        "conference": "ICML",
        "中文标题": "通过逐步回归的稀疏贝叶斯学习",
        "摘要翻译": "稀疏贝叶斯学习（SBL）是实现概率模型稀疏性的强大框架。在此，我们提出了一种称为相关性匹配追踪（RMP）的SBL坐标上升算法，并表明，当其噪声方差参数趋近于零时，RMP与逐步回归展现出惊人的联系。此外，我们为逐步回归算法推导了新的保证，这些保证也为RMP提供了启示。我们对前向回归的保证改进了带有噪声的正交匹配追踪的确定性和概率性结果。我们对后向回归的分析最终得出了一个关于子集选择问题最优解残差的界限，如果满足这一界限，就能保证结果的优性。据我们所知，这是第一个可以在多项式时间内计算且主要取决于矩阵最小奇异值的界限。我们报告了使用多种特征选择算法的数值实验结果。值得注意的是，RMP及其极限变体都高效且在特征相关时保持了强劲的性能。",
        "领域": "稀疏学习、特征选择、贝叶斯方法",
        "问题": "如何在概率模型中实现稀疏性，并提高特征选择算法的性能和效率",
        "动机": "探索稀疏贝叶斯学习与逐步回归之间的联系，为特征选择提供更高效和强健的算法",
        "方法": "提出了一种称为相关性匹配追踪（RMP）的坐标上升算法，并分析了其与逐步回归的联系，同时为逐步回归算法提供了新的理论保证",
        "关键词": [
            "稀疏贝叶斯学习",
            "逐步回归",
            "特征选择",
            "相关性匹配追踪",
            "子集选择"
        ],
        "涉及的技术概念": {
            "稀疏贝叶斯学习（SBL）": "一种实现概率模型稀疏性的框架，通过引入稀疏先验来促进模型参数的稀疏性",
            "相关性匹配追踪（RMP）": "本文提出的坐标上升算法，用于稀疏贝叶斯学习，与逐步回归有密切联系",
            "逐步回归": "一种特征选择方法，通过逐步添加或移除特征来优化模型，本文为其提供了新的理论保证"
        },
        "success": true
    },
    {
        "order": 1018,
        "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention",
        "html": "https://ICML.cc//virtual/2021/poster/9813",
        "abstract": "Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",
        "conference": "ICML",
        "中文标题": "SparseBERT：重新思考自注意力中的重要性分析",
        "摘要翻译": "基于Transformer的模型在自然语言处理（NLP）中广受欢迎。其核心组件——自注意力机制，引起了广泛的兴趣。为了理解自注意力机制，一种直接的方法是可视化预训练模型的注意力图。基于观察到的模式，一系列具有不同稀疏注意力掩码的高效Transformer被提出。从理论角度来看，基于Transformer的模型的通用逼近能力最近也得到了证明。然而，上述对自注意力的理解和分析是基于预训练模型的。为了重新思考自注意力中的重要性分析，我们研究了预训练过程中注意力矩阵中不同位置的重要性。一个令人惊讶的结果是，与注意力图中的其他位置相比，对角线元素是最不重要的。我们提供了一个证明，表明这些对角线元素确实可以被移除而不会降低模型性能。此外，我们提出了一种可微分注意力掩码（DAM）算法，进一步指导了SparseBERT的设计。大量实验验证了我们有趣的发现，并说明了所提出算法的效果。",
        "领域": "自然语言处理与视觉结合、自注意力机制研究、高效Transformer设计",
        "问题": "重新评估自注意力机制中不同位置的重要性，特别是对角线元素的作用",
        "动机": "基于预训练模型的自注意力分析可能不完全准确，需要更深入地理解自注意力机制中各部分的作用",
        "方法": "研究预训练过程中注意力矩阵的位置重要性，提出可微分注意力掩码算法指导稀疏BERT设计",
        "关键词": [
            "自注意力机制",
            "稀疏注意力",
            "可微分注意力掩码",
            "Transformer",
            "预训练模型"
        ],
        "涉及的技术概念": {
            "自注意力机制": "Transformer模型的核心组件，用于捕捉序列中不同位置间的依赖关系",
            "稀疏注意力": "通过减少注意力计算中的非必要连接，提高模型效率的技术",
            "可微分注意力掩码（DAM）": "一种算法，用于动态学习和优化注意力模式，指导稀疏BERT的设计"
        },
        "success": true
    },
    {
        "order": 1019,
        "title": "Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient",
        "html": "https://ICML.cc//virtual/2021/poster/8513",
        "abstract": "This paper provides a statistical analysis of high-dimensional batch reinforcement learning (RL) using sparse linear function approximation. When there is a large number of candidate features, our result sheds light on the fact that sparsity-aware methods can make batch RL more sample efficient. We first consider the off-policy policy evaluation problem. To evaluate a new target policy, we analyze a Lasso fitted Q-evaluation method and establish a finite-sample error bound that has no polynomial dependence on the ambient dimension. To reduce the Lasso bias, we further propose a post model-selection estimator that applies fitted Q-evaluation to the features selected via group Lasso. Under an additional signal strength assumption, we derive a sharper instance-dependent error bound that depends on a divergence function measuring the distribution mismatch  between the data distribution and occupancy measure of the target policy. Further, we study the Lasso fitted Q-iteration for batch policy optimization and establish a finite-sample error bound depending on the ratio between the number of relevant features and restricted minimal eigenvalue of the data's covariance. In the end, we complement the results with minimax lower bounds for batch-data policy evaluation/optimization that nearly match our upper bounds. The results suggest that having well-conditioned data is crucial for sparse batch policy learning.",
        "conference": "ICML",
        "中文标题": "稀疏特征选择提升批量强化学习的样本效率",
        "摘要翻译": "本文对使用稀疏线性函数近似的高维批量强化学习（RL）进行了统计分析。当存在大量候选特征时，我们的结果揭示了稀疏感知方法可以提高批量RL的样本效率。我们首先考虑离策略策略评估问题。为了评估一个新的目标策略，我们分析了Lasso拟合Q评估方法，并建立了一个有限样本误差界，该误差界对环境维度没有多项式依赖。为了减少Lasso偏差，我们进一步提出了一个后模型选择估计器，该估计器将拟合Q评估应用于通过组Lasso选择的特征。在额外的信号强度假设下，我们推导出一个更尖锐的实例依赖误差界，该误差界依赖于一个衡量数据分布与目标策略占用测度之间分布不匹配的散度函数。此外，我们研究了用于批量策略优化的Lasso拟合Q迭代，并建立了一个有限样本误差界，该误差界取决于相关特征数量与数据协方差的受限最小特征值之间的比率。最后，我们用与我们的上界几乎匹配的批量数据策略评估/优化的极小极大下界补充了结果。结果表明，拥有良好条件的数据对于稀疏批量策略学习至关重要。",
        "领域": "强化学习",
        "问题": "高维批量强化学习中的样本效率问题",
        "动机": "提高批量强化学习在存在大量候选特征时的样本效率",
        "方法": "使用稀疏线性函数近似和Lasso拟合Q评估方法，结合组Lasso进行特征选择，以减少偏差并提高样本效率",
        "关键词": [
            "稀疏特征选择",
            "批量强化学习",
            "Lasso拟合Q评估",
            "样本效率",
            "高维数据分析"
        ],
        "涉及的技术概念": {
            "稀疏线性函数近似": "用于在高维批量强化学习中减少特征空间的维度，提高模型效率",
            "Lasso拟合Q评估": "一种评估目标策略的方法，通过Lasso回归减少特征选择中的偏差",
            "组Lasso": "用于特征选择的技术，通过考虑特征组的结构信息，进一步减少偏差并提高选择的相关性"
        },
        "success": true
    },
    {
        "order": 1020,
        "title": "Sparse within Sparse Gaussian Processes using Neighbor Information",
        "html": "https://ICML.cc//virtual/2021/poster/9153",
        "abstract": "Approximations to Gaussian processes (GPs) based on inducing variables, combined with variational inference techniques, enable state-of-the-art sparse approaches to infer GPs at scale through mini-batch based learning. In this work, we further push the limits of scalability of sparse GPs by allowing large number of inducing variables without imposing a special structure on the inducing inputs. In particular, we introduce a novel hierarchical prior, which imposes sparsity on the set of inducing variables. We treat our model variationally, and we experimentally show considerable computational gains compared to standard sparse GPs when sparsity on the inducing variables is realized considering the nearest inducing inputs of a random mini-batch of the data. We perform an extensive experimental validation that demonstrates the effectiveness of our approach compared to the state-of-the-art. Our approach enables the possibility to use sparse GPs using a large number of inducing points without incurring a prohibitive computational cost.",
        "conference": "ICML",
        "中文标题": "利用邻居信息的稀疏中的稀疏高斯过程",
        "摘要翻译": "基于诱导变量的高斯过程（GPs）近似，结合变分推理技术，通过基于小批次的学习，实现了最先进的稀疏方法以大规模推断GPs。在这项工作中，我们通过允许大量诱导变量而不对诱导输入施加特殊结构，进一步推动了稀疏GPs的可扩展性极限。特别是，我们引入了一种新颖的层次先验，它对诱导变量集施加了稀疏性。我们变分地处理我们的模型，并且实验显示，当考虑到数据的随机小批量的最近诱导输入实现诱导变量上的稀疏性时，与标准稀疏GPs相比，我们获得了相当大的计算收益。我们进行了广泛的实验验证，证明了我们的方法相比于最先进技术的有效性。我们的方法使得使用大量诱导点而不产生过高的计算成本的稀疏GPs成为可能。",
        "领域": "高斯过程近似、变分推理、大规模机器学习",
        "问题": "如何在不显著增加计算成本的情况下，使用大量诱导变量来提高稀疏高斯过程的可扩展性。",
        "动机": "为了突破稀疏高斯过程在处理大规模数据时的可扩展性限制，研究提出了一种新的方法，允许使用大量诱导变量而不需要特殊的结构安排。",
        "方法": "引入了一种新颖的层次先验来对诱导变量集施加稀疏性，并采用变分推理技术处理模型，通过考虑数据的随机小批量的最近诱导输入来实现计算效率。",
        "关键词": [
            "稀疏高斯过程",
            "变分推理",
            "层次先验",
            "诱导变量",
            "大规模学习"
        ],
        "涉及的技术概念": {
            "诱导变量": "用于近似高斯过程的关键变量，通过减少计算复杂度使得大规模学习成为可能。",
            "变分推理": "一种近似推断方法，用于高效地处理复杂的概率模型，特别是在大规模数据集上。",
            "层次先验": "一种新颖的先验设置，用于在诱导变量上施加稀疏性，从而在不显著增加计算成本的情况下提高模型的可扩展性。"
        },
        "success": true
    },
    {
        "order": 1021,
        "title": "Sparsifying Networks via Subdifferential Inclusion",
        "html": "https://ICML.cc//virtual/2021/poster/10671",
        "abstract": "Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on low-memory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting.",
        "conference": "ICML",
        "中文标题": "通过次微分包含实现网络稀疏化",
        "摘要翻译": "深度神经网络的稀疏化在许多领域至关重要，尤其是当这些网络需要在低内存设备上实现时。在本文中，我们提出了一种新的方法来为预训练的神经网络生成稀疏权重。通过利用标准非线性激活函数的特性，我们证明了该问题等同于一个近似次微分包含问题。近似的准确性控制了稀疏度。我们展示了所提出的方法适用于广泛的激活函数（ReLU、sigmoid、softmax）。我们提出了一种迭代优化算法来诱导稀疏性，其收敛性得到了保证。由于算法的灵活性，稀疏性可以通过部分训练数据以minibatch方式确保。为了证明我们方法的有效性，我们在不同的应用背景下对多种网络进行了实验：图像分类、语音识别、自然语言处理和时间序列预测。",
        "领域": "模型压缩, 神经网络优化, 深度学习应用",
        "问题": "如何在预训练的神经网络中高效生成稀疏权重以适应低内存设备的实现需求",
        "动机": "解决深度神经网络在资源受限设备上部署时的内存占用问题",
        "方法": "提出一种基于近似次微分包含问题的迭代优化算法，通过控制近似准确性来诱导权重稀疏性",
        "关键词": [
            "网络稀疏化",
            "次微分包含",
            "模型压缩",
            "迭代优化",
            "深度学习应用"
        ],
        "涉及的技术概念": {
            "次微分包含": "用于描述和解决稀疏权重生成问题的数学框架，确保稀疏性的理论基础",
            "迭代优化算法": "提出的算法，通过迭代过程逐步诱导网络权重的稀疏性，保证收敛性",
            "minibatch训练": "允许算法仅使用部分训练数据进行稀疏化处理，提高算法的灵活性和效率"
        },
        "success": true
    },
    {
        "order": 1022,
        "title": "Sparsity-Agnostic Lasso Bandit",
        "html": "https://ICML.cc//virtual/2021/poster/9623",
        "abstract": "We consider a stochastic contextual bandit problem where the dimension $d$ of the feature vectors is potentially large, however, only a sparse subset of features of cardinality $s_0 \\ll d$  affect the reward function. Essentially all existing algorithms for sparse bandits require a priori knowledge  of the value of the sparsity index $s_0$. This knowledge is almost never available in practice, and misspecification of this parameter can lead to severe deterioration in the performance of existing methods. The main contribution of this paper is to propose an algorithm that does not require  prior knowledge of the sparsity index $s_0$ and establish tight regret bounds on its performance under mild conditions. We also comprehensively evaluate our proposed algorithm numerically and show that it consistently outperforms  existing methods, even when the correct sparsity index is revealed to them but is kept hidden from our algorithm.",
        "conference": "ICML",
        "success": true,
        "中文标题": "稀疏无关的Lasso赌博机",
        "摘要翻译": "我们考虑一个随机上下文赌博机问题，其中特征向量的维度$d$可能很大，但实际上只有基数$s_0 \\ll d$的稀疏特征子集影响奖励函数。几乎所有现有的稀疏赌博机算法都需要预先知道稀疏指数$s_0$的值。这种知识在实践中几乎不可得，而对这个参数的误判可能导致现有方法性能的严重下降。本文的主要贡献是提出了一种不需要预先知道稀疏指数$s_0$的算法，并在温和条件下建立了其性能的严格遗憾界限。我们还全面数值评估了我们提出的算法，并表明它始终优于现有方法，即使正确的稀疏指数被透露给它们但对我们的算法保持隐藏。",
        "领域": "强化学习, 稀疏学习, 上下文赌博机",
        "问题": "在稀疏特征影响奖励函数的随机上下文赌博机问题中，现有算法需要预先知道稀疏指数$s_0$，而这一知识在实践中难以获取，导致算法性能下降。",
        "动机": "为了解决现有稀疏赌博机算法需要预先知道稀疏指数$s_0$的问题，提出一种不需要这一先验知识的算法，以提高在实际应用中的性能和适应性。",
        "方法": "提出了一种不需要预先知道稀疏指数$s_0$的算法，并在温和条件下建立了其性能的严格遗憾界限，通过数值评估验证了其优于现有方法的性能。",
        "关键词": [
            "稀疏学习",
            "上下文赌博机",
            "Lasso",
            "强化学习",
            "遗憾界限"
        ],
        "涉及的技术概念": {
            "稀疏学习": "在特征维度高但只有少量特征影响结果的情况下，学习这些关键特征的技术。",
            "上下文赌博机": "一种在给定上下文信息的情况下，通过选择动作来最大化累积奖励的强化学习问题。"
        }
    },
    {
        "order": 1023,
        "title": "Spectral Normalisation for Deep Reinforcement Learning: An Optimisation Perspective",
        "html": "https://ICML.cc//virtual/2021/poster/9739",
        "abstract": "Most of the recent deep reinforcement learning advances take an RL-centric perspective and focus on refinements of the training objective. We diverge from this view and show we can recover the performance of these developments not by changing the objective, but by regularising the value-function estimator. Constraining the Lipschitz constant of a single layer using spectral normalisation is sufficient to elevate the performance of a Categorical-DQN agent to that of a more elaborated agent on the challenging Atari domain. We conduct ablation studies to disentangle the various effects normalisation has on the learning dynamics and show that is sufficient to  modulate the parameter updates to  recover most of the performance of spectral normalisation. These findings hint towards the need to also focus on the neural component and its learning dynamics to tackle the peculiarities of Deep Reinforcement Learning.",
        "conference": "ICML",
        "中文标题": "深度强化学习中的谱归一化：优化视角",
        "摘要翻译": "近年来，大多数深度强化学习的进展都采取了以强化学习为中心的视角，并专注于训练目标的改进。我们偏离了这一观点，并表明我们可以通过正则化价值函数估计器，而不是改变目标，来恢复这些进展的性能。在具有挑战性的Atari领域，使用谱归一化约束单层的Lipschitz常数，足以将Categorical-DQN智能体的性能提升至更复杂智能体的水平。我们进行了消融研究，以分离归一化对学习动态的各种影响，并表明调节参数更新足以恢复谱归一化的大部分性能。这些发现暗示了也需要关注神经组件及其学习动态，以应对深度强化学习的特殊性。",
        "领域": "深度强化学习、价值函数估计、Atari游戏AI",
        "问题": "如何在不改变训练目标的情况下，通过正则化价值函数估计器来提升深度强化学习模型的性能。",
        "动机": "探索通过正则化技术，特别是谱归一化，来改进深度强化学习模型的学习动态和性能，而非仅仅依赖于训练目标的调整。",
        "方法": "使用谱归一化技术约束价值函数估计器中单层的Lipschitz常数，进行消融研究以分析归一化对学习动态的影响。",
        "关键词": [
            "谱归一化",
            "深度强化学习",
            "价值函数估计",
            "Lipschitz常数",
            "Atari游戏"
        ],
        "涉及的技术概念": {
            "谱归一化": "用于约束神经网络层的Lipschitz常数，以稳定训练过程和提高模型性能。",
            "Lipschitz常数": "衡量函数变化率的界限，谱归一化通过限制这一常数来正则化模型。",
            "Categorical-DQN": "一种深度Q学习算法，用于处理离散动作空间的强化学习问题。"
        },
        "success": true
    },
    {
        "order": 1024,
        "title": "Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders",
        "html": "https://ICML.cc//virtual/2021/poster/9363",
        "abstract": "Variational autoencoders with deep hierarchies of stochastic layers have been known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input. We suggest that the hierarchical VAE objective explicitly includes the variance of the function parameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function. Building on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions and show that this can help to solve the problem of posterior collapse. We further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter that is similar for the same model across datasets. We validate the phenomenon across model configurations and datasets.",
        "conference": "ICML",
        "中文标题": "谱平滑揭示层次变分自编码器中的相变",
        "摘要翻译": "具有深层随机层级的变分自编码器已知会遭受后验崩溃的问题，即顶层回归到先验并与输入无关。我们提出，层次变分自编码器的目标函数明确包括了参数化潜在高斯分布均值和方差的函数的方差，这本身通常是一个高方差函数。基于此，我们通过引入一个受高斯分析启发的平滑参数来泛化变分自编码器神经网络，以减少高频成分从而降低参数化函数中的方差，并表明这有助于解决后验崩溃问题。我们进一步表明，在这种平滑下，变分自编码器的损失表现出相变，即在平滑参数的临界值处，顶层KL散度急剧下降至零，这一临界值对于同一模型在不同数据集中是相似的。我们在不同模型配置和数据集上验证了这一现象。",
        "领域": "变分自编码器、深度学习优化、生成模型",
        "问题": "解决层次变分自编码器中后验崩溃的问题",
        "动机": "探索如何通过减少参数化函数中的高方差来防止后验崩溃，从而提高模型的性能和稳定性",
        "方法": "引入基于高斯分析的平滑参数，减少参数化函数中的高频成分和方差，以解决后验崩溃问题",
        "关键词": [
            "层次变分自编码器",
            "后验崩溃",
            "谱平滑",
            "相变",
            "高斯分析"
        ],
        "涉及的技术概念": {
            "后验崩溃": "在层次变分自编码器中，顶层回归到先验并与输入无关的现象",
            "谱平滑": "通过减少参数化函数中的高频成分来降低方差的技术",
            "相变": "在平滑参数的临界值处，模型性能发生急剧变化的现象"
        },
        "success": true
    },
    {
        "order": 1025,
        "title": "Spectral vertex sparsifiers and pair-wise spanners over distributed graphs",
        "html": "https://ICML.cc//virtual/2021/poster/8983",
        "abstract": "Graph sparsification is a powerful tool to approximate an arbitrary graph and has been used in machine learning over graphs.  As real-world networks are becoming very large and naturally distributed, distributed graph sparsification has drawn considerable attention.  In this work, we design communication-efficient distributed algorithms for constructing spectral vertex sparsifiers, which closely preserve effective resistance distances on a subset of vertices of interest in the original graphs, under the well-established message passing communication model. We prove that the communication cost approximates the lower bound with only a small gap. We further provide algorithms for constructing pair-wise spanners which approximate the shortest distances between each pair of vertices in a target set, instead of all pairs, and incur communication costs that are much smaller than those of existing algorithms in the message passing model. Experiments are performed to validate the communication efficiency of the proposed algorithms under the guarantee that the constructed sparsifiers have a good approximation quality.",
        "conference": "ICML",
        "中文标题": "分布式图上的谱顶点稀疏化与成对跨度器",
        "摘要翻译": "图稀疏化是一种近似任意图的强大工具，并已在图机器学习中使用。随着现实世界网络变得非常庞大且自然分布，分布式图稀疏化引起了相当大的关注。在这项工作中，我们设计了通信效率高的分布式算法，用于构建谱顶点稀疏化器，这些稀疏化器在原始图中密切保留感兴趣顶点子集上的有效电阻距离，基于已建立的消息传递通信模型。我们证明通信成本近似于下界，仅有一个小的差距。我们进一步提供了构建成对跨度器的算法，这些算法近似目标集合中每对顶点之间的最短距离，而不是所有对，并且产生的通信成本比消息传递模型中现有算法的成本小得多。进行了实验以验证所提出算法在保证构建的稀疏化器具有良好的近似质量下的通信效率。",
        "领域": "图机器学习、分布式计算、图稀疏化",
        "问题": "如何在分布式环境下高效构建能够近似原始图特性的稀疏化图",
        "动机": "现实世界网络的规模日益庞大且自然分布，需要高效的分布式图稀疏化方法来处理这些大规模图数据",
        "方法": "设计通信效率高的分布式算法构建谱顶点稀疏化器和成对跨度器，优化通信成本并保持图的近似特性",
        "关键词": [
            "谱顶点稀疏化",
            "成对跨度器",
            "分布式算法",
            "通信效率",
            "图稀疏化"
        ],
        "涉及的技术概念": {
            "谱顶点稀疏化": "一种图稀疏化技术，旨在保留图中特定顶点子集的有效电阻距离",
            "成对跨度器": "一种稀疏子图，近似目标顶点集合中每对顶点之间的最短距离",
            "消息传递通信模型": "分布式计算中节点间通过消息传递进行通信的模型，用于评估算法的通信效率"
        },
        "success": true
    },
    {
        "order": 1026,
        "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context",
        "html": "https://ICML.cc//virtual/2021/poster/8713",
        "abstract": "Spreadsheet formula prediction has been an important program synthesis problem with many real-world applications. Previous works typically utilize input-output examples as the specification for spreadsheet formula synthesis, where each input-output pair simulates a separate row in the spreadsheet. However, this formulation does not fully capture the rich context in real-world spreadsheets. First, spreadsheet data entries are organized as tables, thus rows and columns are not necessarily independent from each other. In addition, many spreadsheet tables include headers, which provide high-level descriptions of the cell data. However, previous synthesis approaches do not consider headers as part of the specification. In this work, we present the first approach for synthesizing spreadsheet formulas from tabular context, which includes both headers and semi-structured tabular data. In particular, we propose SpreadsheetCoder, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats. We train our model on a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ rich tabular context. Compared to the rule-based system, SpreadsheetCoder assists 82% more users in composing formulas on Google Sheets.",
        "conference": "ICML",
        "中文标题": "SpreadsheetCoder：从半结构化上下文中预测公式",
        "摘要翻译": "电子表格公式预测一直是一个重要的程序综合问题，具有许多实际应用。以往的工作通常利用输入-输出示例作为电子表格公式综合的规范，其中每个输入-输出对模拟电子表格中的一个单独行。然而，这种表述并未完全捕捉到现实世界电子表格中的丰富上下文。首先，电子表格数据条目以表格形式组织，因此行和列之间不一定相互独立。此外，许多电子表格表格包含标题，这些标题提供了单元格数据的高级描述。然而，先前的综合方法并未将标题视为规范的一部分。在这项工作中，我们提出了第一种从表格上下文中综合电子表格公式的方法，该上下文包括标题和半结构化表格数据。特别是，我们提出了SpreadsheetCoder，一种基于BERT的模型架构，以行和列为基础的格式表示表格上下文。我们在一个大型电子表格数据集上训练我们的模型，并证明SpreadsheetCoder实现了42.51%的top-1预测准确率，这是对不使用丰富表格上下文的基线方法的显著改进。与基于规则的系统相比，SpreadsheetCoder在Google Sheets上帮助用户编写公式的效率提高了82%。",
        "领域": "程序综合、电子表格处理、自然语言处理与视觉结合",
        "问题": "如何从包含标题和半结构化数据的电子表格上下文中预测公式",
        "动机": "现有的电子表格公式预测方法未能充分利用表格中的丰富上下文信息，如行与列之间的依赖关系和标题信息",
        "方法": "提出了一种基于BERT的模型架构SpreadsheetCoder，以行和列为基础的格式表示表格上下文，并在大型电子表格数据集上进行训练",
        "关键词": [
            "电子表格公式预测",
            "BERT模型",
            "程序综合",
            "半结构化数据",
            "表格上下文"
        ],
        "涉及的技术概念": {
            "BERT模型": "用于表示电子表格的表格上下文，支持行和列两种格式的输入",
            "程序综合": "指从高级规范自动生成程序代码的过程，此处特指电子表格公式的生成",
            "半结构化数据": "指那些不完全符合传统数据库表格结构的数据，如电子表格中的数据和标题"
        },
        "success": true
    },
    {
        "order": 1027,
        "title": "Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness",
        "html": "https://ICML.cc//virtual/2021/poster/10373",
        "abstract": "Stochastic gradient algorithms are often unstable when applied to functions that do not have Lipschitz-continuous and/or bounded gradients. Gradient clipping is a simple and effective technique to stabilize the training process for problems that are prone to the exploding gradient problem. Despite its widespread popularity, the convergence properties of the gradient clipping heuristic are poorly understood, especially for stochastic problems. This paper establishes both qualitative and quantitative convergence results of the clipped stochastic (sub)gradient method (SGD) for non-smooth convex functions with rapidly growing subgradients. Our analyses show that clipping enhances the stability of SGD and that the clipped SGD algorithm enjoys finite convergence rates in many cases. We also study the convergence of a clipped method with momentum, which includes clipped SGD as a special case, for weakly convex problems under standard assumptions. With a novel Lyapunov analysis, we show that the proposed method achieves the best-known rate for the considered class of problems, demonstrating the effectiveness of clipped methods also in this regime. Numerical results confirm our theoretical developments.",
        "conference": "ICML",
        "中文标题": "随机梯度裁剪的稳定性和收敛性：超越Lipschitz连续性和平滑性",
        "摘要翻译": "当应用于不具有Lipschitz连续和/或有界梯度的函数时，随机梯度算法往往不稳定。梯度裁剪是一种简单而有效的技术，用于稳定那些容易出现梯度爆炸问题的训练过程。尽管梯度裁剪启发式方法广泛流行，但其收敛性质，特别是对于随机问题的收敛性质，却鲜为人知。本文为非光滑凸函数建立了具有快速增长次梯度的裁剪随机（次）梯度方法（SGD）的定性和定量收敛结果。我们的分析表明，裁剪增强了SGD的稳定性，并且在许多情况下，裁剪后的SGD算法享有有限的收敛速率。我们还研究了带有动量的裁剪方法的收敛性，该方法在标准假设下包括裁剪SGD作为一个特例，用于弱凸问题。通过一种新颖的Lyapunov分析，我们表明所提出的方法对于所考虑的问题类别达到了已知的最佳速率，证明了裁剪方法在这一领域的有效性。数值结果证实了我们的理论发展。",
        "领域": "优化算法、机器学习理论、深度学习优化",
        "问题": "解决随机梯度算法在应用于非Lipschitz连续和/或有界梯度的函数时的不稳定性问题",
        "动机": "探索梯度裁剪技术在稳定随机梯度下降算法中的应用及其收敛性质",
        "方法": "使用梯度裁剪技术稳定SGD，并通过Lyapunov分析证明其收敛性",
        "关键词": [
            "随机梯度裁剪",
            "非光滑优化",
            "收敛性分析",
            "Lyapunov分析",
            "动量方法"
        ],
        "涉及的技术概念": {
            "梯度裁剪": "一种技术，用于限制梯度的大小，防止训练过程中的梯度爆炸，从而稳定训练过程",
            "Lyapunov分析": "用于证明算法收敛性的数学工具，通过构造Lyapunov函数来展示算法的稳定性",
            "动量方法": "一种加速梯度下降算法收敛的技术，通过引入动量项来积累之前的梯度信息，从而在优化过程中减少震荡"
        },
        "success": true
    },
    {
        "order": 1028,
        "title": "Stability and Generalization of Stochastic Gradient Methods for Minimax Problems",
        "html": "https://ICML.cc//virtual/2021/poster/10015",
        "abstract": "Many machine learning problems can be formulated as minimax problems such as Generative Adversarial Networks (GANs), AUC maximization and robust estimation, to mention but a few. A substantial amount of studies are devoted to studying the convergence behavior of their stochastic gradient-type algorithms. In contrast, there is relatively little work on understanding their generalization, i.e., how the learning models built from training examples would behave on test examples. In this paper, we provide a comprehensive generalization analysis of stochastic gradient methods for minimax problems under both convex-concave and nonconvex-nonconcave cases through the lens of algorithmic stability. We establish a quantitative connection between stability and several generalization measures both in expectation and with high probability. For the convex-concave setting, our stability analysis shows that stochastic gradient descent ascent attains optimal generalization bounds for both smooth and nonsmooth minimax problems. We also establish  generalization bounds for both weakly-convex-weakly-concave and gradient-dominated problems. We report preliminary experimental results to verify our theory.",
        "conference": "ICML",
        "中文标题": "随机梯度方法在极小极大问题中的稳定性与泛化性",
        "摘要翻译": "许多机器学习问题可以被表述为极小极大问题，例如生成对抗网络（GANs）、AUC最大化和鲁棒估计等。大量研究致力于研究其随机梯度类型算法的收敛行为。相比之下，关于理解它们的泛化性，即从训练样本构建的学习模型在测试样本上的表现如何，的研究相对较少。在本文中，我们通过算法稳定性的视角，对凸凹和非凸非凹情况下的极小极大问题的随机梯度方法进行了全面的泛化分析。我们建立了稳定性与几种泛化度量之间在期望和高概率下的定量联系。对于凸凹设置，我们的稳定性分析表明，随机梯度下降上升法在平滑和非平滑极小极大问题上都达到了最优的泛化界限。我们还为弱凸弱凹和梯度主导问题建立了泛化界限。我们报告了初步的实验结果以验证我们的理论。",
        "领域": "生成对抗网络",
        "问题": "研究随机梯度方法在极小极大问题中的泛化性能",
        "动机": "理解从训练样本构建的学习模型在测试样本上的表现",
        "方法": "通过算法稳定性的视角，对随机梯度方法进行泛化分析",
        "关键词": [
            "随机梯度方法",
            "极小极大问题",
            "泛化性",
            "算法稳定性",
            "机器学习"
        ],
        "涉及的技术概念": {
            "随机梯度下降上升法": "用于解决极小极大问题的优化算法，结合了梯度下降和上升的特点",
            "算法稳定性": "分析学习算法泛化性能的理论框架，通过算法对输入数据微小变化的敏感性来衡量",
            "泛化界限": "量化学习模型在未见数据上表现的数学界限，用于评估模型的泛化能力"
        },
        "success": true
    },
    {
        "order": 1029,
        "title": "Stabilizing Equilibrium Models by Jacobian Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/10635",
        "abstract": "Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single non-linear layer. These models have been shown to achieve performance competitive with the state-of-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available https://github.com/locuslab/deq.",
        "conference": "ICML",
        "中文标题": "通过雅可比正则化稳定平衡模型",
        "摘要翻译": "深度平衡网络（DEQs）是一类新型模型，它摒弃了传统的深度，转而寻找单一非线性层的固定点。这些模型已被证明在使用显著较少内存的同时，能够达到与最先进深度网络相竞争的性能。然而，它们也较慢，对架构选择敏感，并可能引入模型的不稳定性。在本文中，我们提出了一种针对DEQ模型的正则化方案，该方案明确地对固定点更新方程的雅可比矩阵进行正则化，以稳定平衡模型的学习。我们表明，这种正则化仅增加最小的计算成本，显著稳定了前向和后向传递中的固定点收敛，并且能够很好地扩展到高维、实际的领域（例如，WikiText-103语言建模和ImageNet分类）。使用这种方法，我们首次展示了一个隐式深度模型，其运行速度和性能水平与流行的传统深度网络（如ResNet-101）大致相同，同时仍然保持DEQs的恒定内存占用和架构简单性。代码可在https://github.com/locuslab/deq获取。",
        "领域": "深度学习优化、语言建模、图像分类",
        "问题": "深度平衡网络（DEQs）在训练和应用过程中存在的不稳定性和效率问题",
        "动机": "为了解决DEQs模型在训练和应用中的不稳定性和效率问题，提高其在实际应用中的可用性和性能",
        "方法": "提出了一种针对DEQ模型的雅可比矩阵正则化方案，以稳定模型的学习过程，同时保持计算效率和模型性能",
        "关键词": [
            "深度平衡网络",
            "雅可比正则化",
            "模型稳定性",
            "语言建模",
            "图像分类"
        ],
        "涉及的技术概念": {
            "深度平衡网络（DEQs）": "一类新型模型，通过寻找单一非线性层的固定点来替代传统的深度网络结构，以减少内存使用",
            "雅可比矩阵正则化": "对固定点更新方程的雅可比矩阵进行正则化，以稳定模型的学习过程，提高模型的稳定性和效率",
            "固定点收敛": "在DEQs模型中，前向和后向传递过程中固定点的稳定收敛是模型性能的关键"
        },
        "success": true
    },
    {
        "order": 1030,
        "title": "State Entropy Maximization with Random Encoders for Efficient Exploration",
        "html": "https://ICML.cc//virtual/2021/poster/9857",
        "abstract": "Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks.",
        "conference": "ICML",
        "中文标题": "利用随机编码器实现状态熵最大化以进行高效探索",
        "摘要翻译": "近期的探索方法已被证明是提高深度强化学习（RL）样本效率的有效途径。然而，在高维观察空间中进行高效探索仍然是一个挑战。本文提出了随机编码器高效探索（RE3）方法，该方法利用状态熵作为内在奖励。为了在高维观察环境中估计状态熵，我们在卷积编码器的低维表示空间中使用了k最近邻熵估计器。特别是，我们发现通过使用随机初始化的编码器（在训练过程中固定不变），可以稳定且计算高效地估计状态熵。我们的实验表明，RE3显著提高了无模型和基于模型的RL方法在DeepMind Control Suite和MiniGrid基准测试中的运动与导航任务的样本效率。我们还展示了RE3允许在没有外在奖励的情况下学习多样化的行为，有效提高了下游任务的样本效率。",
        "领域": "深度强化学习、机器人控制、导航任务",
        "问题": "在高维观察空间中进行高效探索的挑战",
        "动机": "提高深度强化学习在高维观察空间中的样本效率",
        "方法": "利用随机初始化的固定编码器和k最近邻熵估计器估计状态熵作为内在奖励",
        "关键词": [
            "状态熵",
            "随机编码器",
            "高效探索",
            "深度强化学习",
            "样本效率"
        ],
        "涉及的技术概念": {
            "状态熵": "作为内在奖励，用于量化探索过程中的不确定性或多样性",
            "随机编码器": "固定不变的编码器，用于将高维观察映射到低维表示空间，以便高效估计状态熵",
            "k最近邻熵估计器": "用于在低维表示空间中估计状态熵的方法，提高了估计的稳定性和计算效率"
        },
        "success": true
    },
    {
        "order": 1031,
        "title": "State Relevance for Off-Policy Evaluation",
        "html": "https://ICML.cc//virtual/2021/poster/9393",
        "abstract": "Importance sampling-based estimators for off-policy evaluation (OPE) are valued for their simplicity, unbiasedness, and reliance on relatively few assumptions. However, the variance of these estimators is often high, especially when trajectories are of different lengths.  In this work, we introduce Omitting-States-Irrelevant-to-Return Importance Sampling (OSIRIS), an estimator which reduces variance by strategically omitting likelihood ratios associated with certain states.  We formalize the conditions under which OSIRIS is unbiased and has lower variance than ordinary importance sampling, and we demonstrate these properties empirically.",
        "conference": "ICML",
        "中文标题": "状态相关性在离策略评估中的重要性",
        "摘要翻译": "基于重要性采样的离策略评估（OPE）估计器因其简单性、无偏性以及对相对较少假设的依赖而受到重视。然而，这些估计器的方差通常很高，尤其是当轨迹长度不同时。在这项工作中，我们介绍了‘省略与回报无关状态的重要性采样’（OSIRIS），这是一种通过策略性地省略与某些状态相关的似然比来降低方差的估计器。我们形式化了OSIRIS无偏且比普通重要性采样具有更低方差的条件，并通过实验证明了这些特性。",
        "领域": "强化学习、离策略评估、重要性采样",
        "问题": "解决离策略评估中重要性采样估计器方差高的问题",
        "动机": "重要性采样估计器在离策略评估中方差高，尤其是在轨迹长度不同的情况下，影响了评估的准确性和效率。",
        "方法": "提出了一种新的估计器OSIRIS，通过策略性地省略与回报无关的状态的似然比来降低方差。",
        "关键词": [
            "离策略评估",
            "重要性采样",
            "方差减少",
            "OSIRIS",
            "强化学习"
        ],
        "涉及的技术概念": {
            "重要性采样": "用于估计在不同于数据收集策略的策略下的期望回报，是离策略评估中的核心技术。",
            "离策略评估": "评估一个策略的性能，而不需要遵循该策略收集新数据的过程。",
            "方差减少": "通过技术手段降低估计器的方差，提高估计的准确性和稳定性。"
        },
        "success": true
    },
    {
        "order": 1032,
        "title": "Statistical Estimation from Dependent Data",
        "html": "https://ICML.cc//virtual/2021/poster/10333",
        "abstract": "We consider a general statistical estimation problem wherein binary labels across different observations are not independent conditioning on their feature vectors, but dependent, capturing  settings where e.g. these observations are collected on a spatial domain, a temporal domain, or a social network, which induce dependencies. We model these dependencies in the language of Markov Random Fields and, importantly, allow these dependencies to be substantial, i.e. do not assume that the Markov Random Field capturing these dependencies is in high temperature. As our main contribution we provide algorithms and statistically efficient estimation rates for this model,  giving several instantiations of our bounds in logistic regression, sparse logistic regression, and neural network regression settings with dependent data. Our estimation guarantees follow from novel results for estimating the parameters (i.e. external fields and interaction strengths) of Ising models from a single sample.",
        "conference": "ICML",
        "中文标题": "从依赖数据中进行统计估计",
        "摘要翻译": "我们考虑一个一般的统计估计问题，其中不同观测值的二元标签在给定其特征向量的条件下不是独立的，而是依赖的，这捕捉了例如这些观测值是在空间域、时间域或社交网络上收集的设置，这些设置引入了依赖性。我们用马尔可夫随机场的语言对这些依赖性进行建模，并且重要的是，允许这些依赖性很大，即不假设捕捉这些依赖性的马尔可夫随机场处于高温状态。作为我们的主要贡献，我们为这个模型提供了算法和统计上有效的估计率，给出了我们的界限在逻辑回归、稀疏逻辑回归和具有依赖数据的神经网络回归设置中的几个实例。我们的估计保证来自于从单个样本估计伊辛模型参数（即外部场和相互作用强度）的新结果。",
        "领域": "统计学习理论, 依赖数据分析, 马尔可夫随机场",
        "问题": "在观测数据存在依赖性的情况下进行统计估计",
        "动机": "研究在观测数据间存在依赖性的情况下，如何进行有效的统计估计，特别是在空间域、时间域或社交网络等场景下。",
        "方法": "使用马尔可夫随机场对数据依赖性进行建模，并提出算法和统计上有效的估计率，应用于逻辑回归、稀疏逻辑回归和神经网络回归等场景。",
        "关键词": [
            "统计估计",
            "依赖数据",
            "马尔可夫随机场",
            "伊辛模型",
            "逻辑回归"
        ],
        "涉及的技术概念": {
            "马尔可夫随机场": "用于对观测数据间的依赖性进行建模，允许依赖性很大，不限于高温状态。",
            "伊辛模型": "用于从单个样本估计外部场和相互作用强度的参数，为统计估计提供理论基础。",
            "逻辑回归": "在存在依赖数据的情况下，应用统计估计方法的一个实例场景。"
        },
        "success": true
    },
    {
        "order": 1033,
        "title": "Stochastic Iterative Graph Matching",
        "html": "https://ICML.cc//virtual/2021/poster/9913",
        "abstract": "Recent works apply Graph Neural Networks (GNNs) to graph matching tasks and show promising results. Considering that model outputs are complex matchings, we devise several techniques to improve the learning of GNNs and obtain a new model, Stochastic Iterative Graph MAtching (SIGMA). Our model predicts a distribution of matchings, instead of a single matching, for a graph pair so the model can explore several probable matchings. We further introduce a novel multi-step matching procedure, which learns how to refine a graph pair's matching results incrementally. The model also includes dummy nodes so that the model does not have to find matchings for nodes without correspondence. We fit this model to data via scalable stochastic optimization. We conduct extensive experiments across synthetic graph datasets as well as biochemistry and computer vision applications. Across all tasks, our results show that SIGMA can produce significantly improved graph matching results compared to state-of-the-art models. Ablation studies verify that each of our components (stochastic training, iterative matching, and dummy nodes) offers noticeable improvement.",
        "conference": "ICML",
        "中文标题": "随机迭代图匹配",
        "摘要翻译": "最近的研究将图神经网络（GNNs）应用于图匹配任务，并显示出有希望的结果。考虑到模型输出是复杂的匹配，我们设计了几种技术来改进GNNs的学习，并获得了一个新模型——随机迭代图匹配（SIGMA）。我们的模型预测的是匹配的分布，而不是单一的匹配，因此模型可以探索几种可能的匹配。我们进一步引入了一种新颖的多步匹配过程，该过程学习如何逐步细化图对的匹配结果。模型还包括虚拟节点，这样模型就不必为没有对应关系的节点寻找匹配。我们通过可扩展的随机优化将模型拟合到数据中。我们在合成图数据集以及生物化学和计算机视觉应用中进行了广泛的实验。在所有任务中，我们的结果表明，与最先进的模型相比，SIGMA可以产生显著改进的图匹配结果。消融研究验证了我们的每个组件（随机训练、迭代匹配和虚拟节点）都提供了明显的改进。",
        "领域": "图神经网络、图匹配、计算机视觉与生物化学应用",
        "问题": "提高图匹配任务的准确性和效率",
        "动机": "现有的图神经网络在图匹配任务中表现良好，但输出结果单一，无法探索多种可能的匹配方案，且在处理无对应节点时存在不足。",
        "方法": "开发了随机迭代图匹配（SIGMA）模型，通过预测匹配分布、引入多步匹配过程和虚拟节点，以及使用随机优化方法，提高了图匹配的性能。",
        "关键词": [
            "图神经网络",
            "图匹配",
            "随机优化",
            "虚拟节点",
            "多步匹配"
        ],
        "涉及的技术概念": {
            "随机训练": "在模型训练过程中引入随机性，以探索多种可能的匹配分布，提高模型的泛化能力。",
            "迭代匹配": "通过多步匹配过程逐步细化匹配结果，提高匹配的准确性和稳定性。",
            "虚拟节点": "引入虚拟节点处理无对应关系的节点，避免模型在这些节点上产生错误的匹配。"
        },
        "success": true
    },
    {
        "order": 1034,
        "title": "Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions",
        "html": "https://ICML.cc//virtual/2021/poster/10697",
        "abstract": " We study the stochastic Multi-Armed Bandit~(MAB) problem with random delays in the feedback received by the algorithm. We consider two settings: the {\\it reward dependent} delay setting, where realized delays may depend on the stochastic rewards, and the {\\it reward-independent} delay setting. Our main contribution is algorithms that achieve near-optimal regret in each of the settings, with an additional additive dependence on the quantiles of the delay distribution. Our results do not make any assumptions on the delay distributions: in particular, we do not assume they come from any parametric family of distributions and allow for unbounded support and expectation; we further allow for the case of infinite delays where the algorithm might occasionally not observe any feedback.",
        "conference": "ICML",
        "中文标题": "具有无限制延迟分布的随机多臂老虎机问题",
        "摘要翻译": "我们研究了算法接收反馈中存在随机延迟的随机多臂老虎机（MAB）问题。我们考虑了两种设置：奖励依赖延迟设置，其中实现的延迟可能依赖于随机奖励；以及奖励独立延迟设置。我们的主要贡献是在每种设置中都能实现接近最优遗憾的算法，并额外依赖于延迟分布的分位数。我们的结果对延迟分布不做任何假设：特别是，我们不假设它们来自任何参数分布族，并允许无界支持和期望；我们进一步允许无限延迟的情况，即算法偶尔可能不观察任何反馈。",
        "领域": "强化学习、在线学习、延迟反馈优化",
        "问题": "在随机多臂老虎机问题中处理无限制延迟分布的反馈",
        "动机": "研究在反馈延迟可能无限且分布未知的情况下，如何设计算法以最小化遗憾",
        "方法": "开发了在奖励依赖和奖励独立延迟设置下都能实现接近最优遗憾的算法，并考虑了延迟分布的分位数",
        "关键词": [
            "随机多臂老虎机",
            "延迟反馈",
            "遗憾最小化",
            "无限制延迟分布",
            "在线学习"
        ],
        "涉及的技术概念": {
            "随机多臂老虎机": "一种在线决策框架，用于在不确定环境中通过尝试和错误学习最优行动策略",
            "延迟反馈": "算法执行动作后，观察到反馈的时间延迟，可能影响学习效率和性能",
            "遗憾最小化": "衡量算法性能的指标，表示算法与知道最优动作的基准之间的累积奖励差距"
        },
        "success": true
    },
    {
        "order": 1035,
        "title": "Stochastic Sign Descent Methods: New Algorithms and Better Theory",
        "html": "https://ICML.cc//virtual/2021/poster/9485",
        "abstract": "Various gradient compression schemes have been proposed to mitigate the communication cost in distributed training of large scale machine learning models. Sign-based methods, such as signSGD (Bernstein et al., 2018), have recently been gaining popularity because of their simple compression rule and connection to adaptive gradient methods, like ADAM.\nIn this paper, we analyze sign-based methods for non-convex optimization in three key settings: (i) standard single node, (ii) parallel with shared data and (iii) distributed with partitioned data. For single machine case, we generalize the previous analysis of signSGD relying on intuitive bounds on success probabilities and allowing even biased estimators. Furthermore, we extend the analysis to parallel setting within a parameter server framework, where exponentially fast noise reduction is guaranteed with respect to number of nodes, maintaining $1$-bit compression in both directions and using small mini-batch sizes. Next, we identify a fundamental issue with signSGD to converge in distributed environment. To resolve this issue, we propose a new sign-based method, {\\em Stochastic Sign Descent with Momentum (SSDM)}, which converges under standard bounded variance assumption with the optimal asymptotic rate. We validate several aspects of our theoretical findings with numerical experiments.",
        "conference": "ICML",
        "success": true,
        "中文标题": "随机符号下降方法：新算法与更优理论",
        "摘要翻译": "为了降低大规模机器学习模型分布式训练中的通信成本，已经提出了各种梯度压缩方案。符号基方法，如signSGD（Bernstein等人，2018年），由于它们简单的压缩规则和与自适应梯度方法（如ADAM）的联系，最近变得越来越受欢迎。在本文中，我们在三个关键设置下分析了符号基方法用于非凸优化：（i）标准单节点，（ii）共享数据的并行和（iii）分区数据的分布式。对于单机情况，我们基于直观的成功概率界限并允许甚至偏置估计器，推广了之前对signSGD的分析。此外，我们将分析扩展到参数服务器框架内的并行设置，其中相对于节点数量保证了指数级快速的噪声减少，保持双向的1位压缩并使用小批量大小。接下来，我们确定了signSGD在分布式环境中收敛的一个基本问题。为了解决这个问题，我们提出了一种新的符号基方法，即带有动量的随机符号下降（SSDM），它在标准有界方差假设下以最优渐近速率收敛。我们通过数值实验验证了我们理论发现的几个方面。",
        "领域": "分布式机器学习优化, 非凸优化, 梯度压缩",
        "问题": "解决大规模机器学习模型分布式训练中的高通信成本问题",
        "动机": "探索符号基方法在非凸优化中的有效性，特别是在分布式环境中的收敛性问题",
        "方法": "提出了一种新的符号基方法SSDM，通过理论分析和数值实验验证其在分布式环境中的收敛性和效率",
        "关键词": [
            "符号基方法",
            "分布式训练",
            "非凸优化",
            "梯度压缩",
            "SSDM"
        ],
        "涉及的技术概念": {
            "符号基方法": "通过符号操作压缩梯度信息，减少通信成本",
            "非凸优化": "在目标函数非凸的情况下进行优化，适用于复杂的机器学习模型",
            "梯度压缩": "减少梯度信息的大小以降低分布式训练中的通信开销"
        }
    },
    {
        "order": 1036,
        "title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
        "html": "https://ICML.cc//virtual/2021/poster/8965",
        "abstract": "Advanced large-scale neural language models have led to significant success in many language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a  modification straight to the gradient of the loss function, to  remedy the degeneration issue of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation tasks. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks.",
        "conference": "ICML",
        "中文标题": "直指梯度：学习使用新标记进行神经文本生成",
        "摘要翻译": "先进的大规模神经语言模型在许多语言生成任务中取得了显著成功。然而，最常用的训练目标——最大似然估计（MLE）已被证明存在问题，训练出的模型倾向于使用单调和重复的短语。在这项工作中，我们引入了ScaleGrad，一种直接修改损失函数梯度的方法，以解决标准MLE目标的退化问题。通过直接操纵梯度信息，ScaleGrad使模型学会使用新标记。实证结果显示，我们的方法不仅在开放式生成中有效，也在定向生成任务中表现良好。由于其架构的简洁性，我们的方法可以作为一种通用的训练目标，适用于大多数神经文本生成任务。",
        "领域": "自然语言处理与视觉结合",
        "问题": "解决神经语言模型在文本生成任务中使用单调和重复短语的问题",
        "动机": "改进最大似然估计（MLE）训练目标，以促进模型生成更多样化和新颖的文本",
        "方法": "引入ScaleGrad方法，直接修改损失函数的梯度，鼓励模型使用新标记",
        "关键词": [
            "神经文本生成",
            "梯度修改",
            "ScaleGrad",
            "最大似然估计",
            "文本多样性"
        ],
        "涉及的技术概念": {
            "ScaleGrad": "一种直接修改损失函数梯度的方法，用于解决标准MLE目标的退化问题",
            "最大似然估计（MLE）": "一种常用的训练目标，但在神经文本生成中可能导致模型生成单调和重复的文本",
            "梯度信息": "在训练过程中用于调整模型参数的信息，ScaleGrad通过直接操纵这些信息来改进模型性能"
        },
        "success": true
    },
    {
        "order": 1037,
        "title": "Strategic Classification in the Dark",
        "html": "https://ICML.cc//virtual/2021/poster/9097",
        "abstract": "Strategic classification studies the interaction between a classification rule and the strategic agents it governs. Agents respond by manipulating their features, under the assumption that the classifier is known. However, in many real-life scenarios of high-stake classification (e.g., credit scoring), the classifier is not revealed to the agents, which leads agents to attempt to learn the classifier and game it too. In this paper we generalize the strategic classification model to such scenarios and analyze the effect of an unknown classifier. We define the ''price of opacity'' as the difference between the prediction error under the opaque and transparent policies, characterize it, and give a sufficient condition for it to be strictly positive, in which case transparency is the recommended policy. Our experiments show how Hardt et al.’s robust classifier is affected by keeping agents in the dark.",
        "conference": "ICML",
        "中文标题": "黑暗中的战略分类",
        "摘要翻译": "战略分类研究分类规则与其治理的战略代理之间的互动。代理在假设分类器已知的情况下，通过操纵其特征来响应。然而，在许多高风险分类的现实场景中（例如信用评分），分类器并未向代理公开，这导致代理尝试学习并利用分类器。在本文中，我们将战略分类模型推广到此类场景，并分析未知分类器的影响。我们将'不透明的代价'定义为不透明和透明策略下预测误差的差异，对其进行表征，并给出一个充分条件使其严格为正，在这种情况下，透明是推荐的政策。我们的实验展示了Hardt等人的鲁棒分类器如何通过让代理处于黑暗中而受到影响。",
        "领域": "信用评分、对抗性机器学习、战略行为建模",
        "问题": "研究在分类器不公开的情况下，战略代理如何尝试学习并利用分类器，以及这种不透明性对分类性能的影响。",
        "动机": "探索在高风险分类场景中，分类器透明度对代理行为及分类性能的影响，以指导更有效的分类策略设计。",
        "方法": "通过定义'不透明的代价'并分析其在透明与不透明策略下的差异，以及实验验证Hardt等人的鲁棒分类器在不透明条件下的表现。",
        "关键词": [
            "战略分类",
            "不透明代价",
            "信用评分",
            "对抗性学习",
            "鲁棒分类器"
        ],
        "涉及的技术概念": {
            "战略分类": "研究分类规则与战略代理之间的互动，代理通过操纵特征响应分类规则。",
            "不透明的代价": "衡量分类器不透明性对预测误差的影响，用于评估透明与不透明策略的效果差异。",
            "鲁棒分类器": "在对抗性环境中设计的分类器，旨在抵抗代理的操纵行为，保持分类性能。"
        },
        "success": true
    },
    {
        "order": 1038,
        "title": "Strategic Classification Made Practical",
        "html": "https://ICML.cc//virtual/2021/poster/8861",
        "abstract": "Strategic classification regards the problem of learning in settings where users can strategically modify their features to improve outcomes. This setting applies broadly, and has received much recent attention. But despite its practical significance, work in this space has so far been predominantly theoretical. In this paper we present a learning framework for strategic classification that is practical. Our approach directly minimizes the ``strategic'' empirical risk, which we achieve by differentiating through the strategic response of users. This provides flexibility that allows us to extend beyond the original problem formulation and towards more realistic learning scenarios. A series of experiments demonstrates the effectiveness of our approach on various learning settings.",
        "conference": "ICML",
        "中文标题": "实用化战略分类",
        "摘要翻译": "战略分类关注的是在用户可以通过策略性修改其特征以改善结果的环境中进行学习的问题。这一设定广泛适用，并受到了最近的广泛关注。尽管其实际意义重大，但迄今为止，这一领域的工作主要是理论性的。在本文中，我们提出了一个实用的战略分类学习框架。我们的方法直接最小化‘战略’经验风险，这是通过区分用户的战略响应来实现的。这提供了灵活性，使我们能够超越原始问题表述，向更现实的学习场景扩展。一系列实验证明了我们的方法在各种学习环境中的有效性。",
        "领域": "机器学习、对抗性学习、决策系统",
        "问题": "在用户能够策略性修改其特征以影响分类结果的环境中，如何有效进行学习。",
        "动机": "解决战略分类领域理论与实践之间的差距，提出一个更实用的学习框架。",
        "方法": "通过直接最小化‘战略’经验风险，并区分用户的战略响应，扩展了战略分类的实用性。",
        "关键词": [
            "战略分类",
            "对抗性学习",
            "经验风险最小化",
            "用户策略响应",
            "实用学习框架"
        ],
        "涉及的技术概念": {
            "战略分类": "在用户能够策略性修改其特征的环境中进行的分类学习。",
            "经验风险最小化": "通过最小化经验风险来优化学习模型的方法。",
            "用户策略响应": "用户根据分类器的行为策略性调整自己的特征，以期望获得更有利的分类结果。"
        },
        "success": true
    },
    {
        "order": 1039,
        "title": "Streaming and Distributed Algorithms for Robust Column Subset Selection",
        "html": "https://ICML.cc//virtual/2021/poster/9919",
        "abstract": "We give the first single-pass streaming algorithm for Column Subset Selection with respect to the entrywise $\\ell_p$-norm with $1 \\leq p < 2$. We study the $\\ell_p$ norm loss since it is often considered more robust to noise than the standard Frobenius norm. Given an input matrix $A \\in \\mathbb{R}^{d \\times n}$ ($n \\gg d$), our algorithm achieves a multiplicative $k^{\\frac{1}{p} - \\frac{1}{2}}\\poly(\\log nd)$-approximation to the error with respect to the \\textit{best possible column subset} of size $k$. Furthermore, the space complexity of the streaming algorithm is optimal up to a logarithmic factor. Our streaming algorithm also extends naturally to a 1-round distributed protocol with nearly optimal communication cost. A key ingredient in our algorithms is a reduction to column subset selection in the $\\ell_{p,2}$-norm, which corresponds to the $p$-norm of the vector of Euclidean norms of each of the columns of $A$. This enables us to leverage strong coreset constructions for the Euclidean norm, which previously had not been applied in this context. We also give the first provable guarantees for greedy column subset selection in the $\\ell_{1, 2}$ norm, which can be used as an alternative, practical subroutine in our algorithms. Finally, we show that our algorithms give significant practical advantages on real-world data analysis tasks.",
        "conference": "ICML",
        "中文标题": "流式与分布式算法在鲁棒列子集选择中的应用",
        "摘要翻译": "我们首次提出了针对1 ≤ p < 2的entrywise ℓp-范数的列子集选择的单遍流式算法。我们研究ℓp范数损失，因为它通常被认为比标准的Frobenius范数对噪声更鲁棒。给定一个输入矩阵A ∈ ℝ^(d×n)（n ≫ d），我们的算法在大小为k的最佳可能列子集方面，实现了k^(1/p - 1/2)poly(log nd)的乘法近似误差。此外，流式算法的空间复杂度在达到对数因子之前是最优的。我们的流式算法也自然地扩展到具有近乎最优通信成本的1轮分布式协议。我们算法中的一个关键要素是减少到ℓp,2-范数中的列子集选择，这对应于A的每列的欧几里得范数的p-范数。这使我们能够利用对欧几里得范数的强核心集构造，这在以前没有被应用在这个上下文中。我们还首次为ℓ1,2范数中的贪婪列子集选择提供了可证明的保证，这可以作为我们算法中的一个替代的、实用的子程序。最后，我们展示了我们的算法在现实世界的数据分析任务中提供了显著的实际优势。",
        "领域": "机器学习算法优化, 大数据处理, 分布式计算",
        "问题": "在流式和分布式环境下，高效且鲁棒地选择矩阵列子集的问题",
        "动机": "研究动机是为了解决在大规模数据处理中，如何高效且鲁棒地选择矩阵列子集，以提高数据分析和处理的效率和准确性",
        "方法": "采用基于ℓp范数的流式算法和分布式协议，结合ℓp,2-范数的列子集选择和欧几里得范数的强核心集构造",
        "关键词": [
            "列子集选择",
            "流式算法",
            "分布式计算",
            "ℓp范数",
            "鲁棒性"
        ],
        "涉及的技术概念": {
            "ℓp范数": "用于衡量列子集选择的误差，提供对噪声的鲁棒性",
            "流式算法": "允许单遍处理大规模数据，空间复杂度接近最优",
            "分布式协议": "扩展流式算法到分布式环境，实现高效的通信成本"
        },
        "success": true
    },
    {
        "order": 1040,
        "title": "Streaming Bayesian Deep Tensor Factorization",
        "html": "https://ICML.cc//virtual/2021/poster/10483",
        "abstract": "Despite the success of existing tensor factorization methods, most of them conduct a multilinear decomposition, and rarely  exploit powerful modeling frameworks, like deep neural networks, to capture a variety of complicated interactions in data. More important, for highly expressive, deep factorization, we lack an effective approach to handle streaming data, which are ubiquitous in real-world applications. To address these issues, we propose SBTD, a Streaming Bayesian  Deep Tensor factorization method. We first use Bayesian neural networks (NNs) to build a deep tensor factorization model. We assign a spike-and-slab prior over each NN weight to encourage sparsity and to prevent overfitting. We then use multivariate Delta's method and moment matching to approximate the posterior of the NN output and calculate the running model evidence, based on which we develop an efficient streaming posterior inference algorithm in the assumed-density-filtering and expectation propagation framework. Our algorithm provides responsive incremental updates for the posterior of the latent factors and NN weights upon receiving newly observed tensor entries, and meanwhile identify and inhibit redundant/useless weights. We show the advantages of our approach in four real-world applications. ",
        "conference": "ICML",
        "中文标题": "流式贝叶斯深度张量分解",
        "摘要翻译": "尽管现有的张量分解方法取得了成功，但大多数方法进行的是多线性分解，很少利用强大的建模框架，如深度神经网络，来捕捉数据中的各种复杂交互。更重要的是，对于高度表达性的深度分解，我们缺乏一种有效的方法来处理流式数据，这些数据在现实世界的应用中无处不在。为了解决这些问题，我们提出了SBTD，一种流式贝叶斯深度张量分解方法。我们首先使用贝叶斯神经网络（NNs）构建一个深度张量分解模型。我们对每个神经网络权重分配一个尖峰和平板先验，以鼓励稀疏性并防止过拟合。然后，我们使用多元Delta方法和矩匹配来近似神经网络输出的后验，并计算运行模型证据，基于此，我们在假设密度过滤和期望传播框架下开发了一种高效的流式后验推理算法。我们的算法在接收到新观察到的张量条目时，为潜在因子和神经网络权重的后验提供响应式的增量更新，同时识别并抑制冗余/无用的权重。我们在四个实际应用中展示了我们方法的优势。",
        "领域": "张量分解、深度学习、流式数据处理",
        "问题": "现有张量分解方法多进行多线性分解，缺乏利用深度神经网络捕捉数据复杂交互的能力，且无法有效处理流式数据。",
        "动机": "开发一种能够利用深度神经网络捕捉数据复杂交互，并能有效处理流式数据的张量分解方法。",
        "方法": "使用贝叶斯神经网络构建深度张量分解模型，采用尖峰和平板先验鼓励稀疏性，利用多元Delta方法和矩匹配近似后验，开发高效的流式后验推理算法。",
        "关键词": [
            "流式贝叶斯",
            "深度张量分解",
            "贝叶斯神经网络",
            "稀疏性",
            "后验推理"
        ],
        "涉及的技术概念": {
            "贝叶斯神经网络": "用于构建深度张量分解模型，通过引入不确定性来提高模型的表达能力。",
            "尖峰和平板先验": "分配给神经网络权重的先验，旨在鼓励权重稀疏性，防止模型过拟合。",
            "多元Delta方法和矩匹配": "用于近似神经网络输出的后验分布，为流式后验推理算法提供理论基础。"
        },
        "success": true
    },
    {
        "order": 1041,
        "title": "STRODE: Stochastic Boundary Ordinary Differential Equation",
        "html": "https://ICML.cc//virtual/2021/poster/8689",
        "abstract": "Perception of time from sequentially acquired sensory inputs is rooted in everyday behaviors of individual organisms. Yet, most algorithms for time-series modeling fail to learn dynamics of random event timings directly from visual or audio inputs, requiring timing annotations during training that are usually unavailable for real-world applications. For instance, neuroscience perspectives on postdiction imply that there exist variable temporal ranges within which the incoming sensory inputs can affect the earlier perception, but such temporal ranges are mostly unannotated for real applications such as automatic speech recognition (ASR). In this paper, we present a probabilistic ordinary differential equation (ODE), called STochastic boundaRy ODE (STRODE), that learns both the timings and the dynamics of time series data without requiring any timing annotations during training. STRODE allows the usage of differential equations to sample from the posterior point processes, efficiently and analytically. We further provide theoretical guarantees on the learning of STRODE. Our empirical results show that our approach successfully infers event timings of time series data. Our method achieves competitive or superior performances compared to existing state-of-the-art methods for both synthetic and real-world datasets. ",
        "conference": "ICML",
        "中文标题": "STRODE：随机边界常微分方程",
        "摘要翻译": "从连续获取的感官输入中感知时间是生物个体日常行为的基础。然而，大多数时间序列建模算法无法直接从视觉或音频输入中学习随机事件时间动态，训练时需要时间注释，而这些注释在现实应用中通常不可用。例如，神经科学对后效的观点暗示，存在可变的时间范围，在这些范围内，传入的感官输入可以影响先前的感知，但对于自动语音识别（ASR）等实际应用，这些时间范围大多未被注释。在本文中，我们提出了一种概率常微分方程（ODE），称为随机边界ODE（STRODE），它可以在训练过程中不需要任何时间注释的情况下学习时间序列数据的时间和动态。STRODE允许使用微分方程高效且分析地从后点点过程中采样。我们进一步提供了关于STRODE学习的理论保证。我们的实证结果表明，我们的方法成功地推断了时间序列数据的事件时间。与现有的最先进方法相比，我们的方法在合成和真实世界数据集上都达到了竞争或更优的性能。",
        "领域": "时间序列分析, 自动语音识别, 神经科学计算模型",
        "问题": "如何直接从视觉或音频输入中学习随机事件时间动态，而不需要时间注释。",
        "动机": "现实应用中时间注释通常不可用，需要一种能够学习时间序列数据的时间和动态的方法。",
        "方法": "提出了一种概率常微分方程（STRODE），允许使用微分方程高效且分析地从后点点过程中采样，无需时间注释。",
        "关键词": [
            "随机边界ODE",
            "时间序列分析",
            "无监督学习",
            "后效",
            "自动语音识别"
        ],
        "涉及的技术概念": {
            "概率常微分方程": "用于学习时间序列数据的时间和动态，无需时间注释。",
            "后点点过程": "STRODE允许从这些过程中高效且分析地采样。",
            "无监督学习": "STRODE在训练过程中不需要任何时间注释，实现了对时间序列数据的无监督学习。"
        },
        "success": true
    },
    {
        "order": 1042,
        "title": "Structured Convolutional Kernel Networks for Airline Crew Scheduling",
        "html": "https://ICML.cc//virtual/2021/poster/9175",
        "abstract": "Motivated by the needs from an airline crew scheduling application, we introduce structured convolutional kernel networks (Struct-CKN), which combine CKNs from Mairal et al. (2014) in a structured prediction framework that supports constraints on the outputs. CKNs are a particular kind of convolutional neural networks that approximate a kernel feature map on training data, thus combining properties of deep learning with the non-parametric flexibility of kernel methods. Extending CKNs to structured outputs allows us to obtain useful initial solutions on a flight-connection dataset that can be further refined by an airline crew scheduling solver. More specifically, we use a flight-based network modeled as a general conditional random field capable of incorporating local constraints in the learning process. Our experiments demonstrate that this approach yields significant improvements for the large-scale crew pairing problem (50,000 flights per month) over standard approaches, reducing the solution cost by 17% (a gain of millions of dollars) and the cost of global constraints by 97%.",
        "conference": "ICML",
        "中文标题": "结构化卷积核网络在航空公司机组排班中的应用",
        "摘要翻译": "受到航空公司机组排班应用需求的启发，我们引入了结构化卷积核网络（Struct-CKN），该网络将Mairal等人（2014）提出的CKN结合到一个支持输出约束的结构化预测框架中。CKN是一种特殊类型的卷积神经网络，它在训练数据上近似核特征映射，从而结合了深度学习的特性与核方法的非参数灵活性。将CKN扩展到结构化输出使我们能够在航班连接数据集上获得有用的初始解决方案，这些解决方案可以通过航空公司机组排班求解器进一步优化。更具体地说，我们使用一个基于航班的网络，该网络被建模为一个能够将局部约束纳入学习过程的通用条件随机场。我们的实验表明，与标准方法相比，这种方法在大规模机组配对问题（每月50,000个航班）上带来了显著的改进，解决方案成本降低了17%（节省了数百万美元），全局约束成本降低了97%。",
        "领域": "深度学习与优化结合、结构化预测、航空运营优化",
        "问题": "如何在航空公司机组排班中有效结合深度学习与结构化预测，以优化排班解决方案。",
        "动机": "解决航空公司机组排班中的大规模优化问题，通过结合深度学习的特性与核方法的非参数灵活性，提高排班效率和降低成本。",
        "方法": "引入结构化卷积核网络（Struct-CKN），结合卷积核网络（CKN）和结构化预测框架，使用基于航班的网络建模为通用条件随机场，以纳入局部约束。",
        "关键词": [
            "结构化卷积核网络",
            "机组排班",
            "条件随机场",
            "深度学习优化",
            "航空运营"
        ],
        "涉及的技术概念": {
            "结构化卷积核网络（Struct-CKN）": "结合卷积核网络和结构化预测框架，支持输出约束，用于优化航空公司机组排班。",
            "卷积核网络（CKN）": "一种特殊类型的卷积神经网络，近似核特征映射，结合深度学习的特性与核方法的非参数灵活性。",
            "条件随机场": "用于建模基于航班的网络，能够将局部约束纳入学习过程，优化机组排班解决方案。"
        },
        "success": true
    },
    {
        "order": 1043,
        "title": "Structured World Belief for Reinforcement Learning in POMDP",
        "html": "https://ICML.cc//virtual/2021/poster/9539",
        "abstract": "Object-centric world models provide structured representation of the scene and can be an important backbone in reinforcement learning and planning. However, existing approaches suffer in partially-observable environments due to the lack of belief states. In this paper, we propose Structured World Belief, a model for learning and inference of object-centric belief states. Inferred by Sequential Monte Carlo (SMC), our belief states provide multiple object-centric scene hypotheses. To synergize the benefits of SMC particles with object representations, we also propose a new object-centric dynamics model that considers the inductive bias of object permanence. This enables tracking of object states even when they are invisible for a long time. To further facilitate object tracking in this regime, we allow our model to attend flexibly to any spatial location in the image which was restricted in previous models. In experiments, we show that object-centric belief provides a more accurate and robust performance for filtering and generation. Furthermore, we show the efficacy of structured world belief in improving the performance of reinforcement learning, planning and supervised reasoning.",
        "conference": "ICML",
        "中文标题": "结构化世界信念在部分可观察马尔可夫决策过程中的强化学习应用",
        "摘要翻译": "以对象为中心的世界模型提供了场景的结构化表示，并可以成为强化学习和规划中的重要支柱。然而，现有方法在部分可观察环境中由于缺乏信念状态而表现不佳。在本文中，我们提出了结构化世界信念，一个用于学习和推断以对象为中心的信念状态的模型。通过序列蒙特卡洛（SMC）推断，我们的信念状态提供了多个以对象为中心的场景假设。为了将SMC粒子的好处与对象表示协同起来，我们还提出了一种新的以对象为中心的动态模型，该模型考虑了对象持久性的归纳偏差。这使得即使在对象长时间不可见的情况下也能跟踪对象状态。为了进一步促进在这种情况下的对象跟踪，我们允许我们的模型灵活地关注图像中的任何空间位置，这在以前的模型中是被限制的。在实验中，我们展示了以对象为中心的信念为过滤和生成提供了更准确和鲁棒的性能。此外，我们还展示了结构化世界信念在提高强化学习、规划和监督推理性能方面的有效性。",
        "领域": "强化学习、部分可观察马尔可夫决策过程、对象中心表示",
        "问题": "在部分可观察环境中，现有方法由于缺乏信念状态而表现不佳。",
        "动机": "提高在部分可观察环境中强化学习和规划的准确性和鲁棒性。",
        "方法": "提出结构化世界信念模型，结合序列蒙特卡洛推断和新的对象中心动态模型，以跟踪对象状态并灵活关注图像空间位置。",
        "关键词": [
            "结构化世界信念",
            "序列蒙特卡洛",
            "对象中心表示"
        ],
        "涉及的技术概念": {
            "结构化世界信念": "用于学习和推断以对象为中心的信念状态的模型，提高在部分可观察环境中的性能。",
            "序列蒙特卡洛（SMC）": "用于推断多个以对象为中心的场景假设，增强模型的鲁棒性和准确性。",
            "对象中心动态模型": "考虑了对象持久性的归纳偏差，使得即使在对象长时间不可见的情况下也能跟踪对象状态。"
        },
        "success": true
    },
    {
        "order": 1044,
        "title": "Submodular Maximization subject to a Knapsack Constraint: Combinatorial Algorithms with Near-optimal Adaptive Complexity",
        "html": "https://ICML.cc//virtual/2021/poster/10581",
        "abstract": "The growing need to deal with massive instances motivates the design of algorithms balancing the quality of the solution with applicability. For the latter, an important measure is the \\emph{adaptive complexity}, capturing the number of sequential rounds of parallel computation needed. In this work we obtain the first \\emph{constant factor} approximation algorithm for non-monotone submodular maximization subject to a knapsack constraint with \\emph{near-optimal} $O(\\log n)$ adaptive complexity. Low adaptivity by itself, however, is not enough: one needs to account for the total number of function evaluations (or value queries) as well. Our algorithm asks $\\tilde{O}(n^2)$ value queries, but can be modified to run with only $\\tilde{O}(n)$ instead, while retaining a low adaptive complexity of $O(\\log^2n)$. Besides the above improvement in adaptivity, this is also the first \\emph{combinatorial} approach with sublinear adaptive complexity for the problem and yields algorithms comparable to the state-of-the-art even for the special cases of cardinality constraints or monotone objectives. Finally, we showcase our algorithms' applicability on real-world datasets.",
        "conference": "ICML",
        "success": true,
        "中文标题": "背包约束下的子模最大化：具有近最优自适应复杂度的组合算法",
        "摘要翻译": "处理大规模实例的需求日益增长，促使设计算法时需平衡解决方案的质量与适用性。对于后者，一个重要的衡量标准是自适应复杂度，它捕捉了所需的并行计算的顺序轮数。在这项工作中，我们首次获得了针对背包约束下的非单调子模最大化问题的常数因子近似算法，其具有近最优的O(log n)自适应复杂度。然而，低自适应性本身并不足够：还需要考虑函数评估（或值查询）的总次数。我们的算法进行了约O(n^2)次值查询，但可以修改为仅进行约O(n)次，同时保持O(log^2n)的低自适应复杂度。除了上述自适应性的改进外，这也是针对该问题的首个具有次线性自适应复杂度的组合方法，并且即使在基数约束或单调目标的特殊情况下，也能产生与最先进算法相媲美的算法。最后，我们在真实世界的数据集上展示了我们算法的适用性。",
        "领域": "组合优化, 近似算法, 并行计算",
        "问题": "在背包约束下实现非单调子模最大化问题的高效近似解",
        "动机": "设计能够在处理大规模实例时平衡解决方案质量与计算效率的算法",
        "方法": "开发了一种具有近最优自适应复杂度的组合算法，通过减少值查询次数和保持低自适应复杂度来优化性能",
        "关键词": [
            "子模最大化",
            "背包约束",
            "自适应复杂度",
            "组合算法",
            "近似算法"
        ],
        "涉及的技术概念": {
            "自适应复杂度": "衡量算法所需的并行计算的顺序轮数，本研究旨在降低这一指标以提高算法的适用性",
            "子模最大化": "在给定约束下寻找使子模函数值最大化的元素子集，是组合优化中的核心问题",
            "组合算法": "通过组合数学的方法解决问题，本研究首次为背包约束下的子模最大化问题提供了具有次线性自适应复杂度的组合算法"
        }
    },
    {
        "order": 1045,
        "title": "SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10641",
        "abstract": "Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments.",
        "conference": "ICML",
        "中文标题": "SUNRISE：深度强化学习中集成学习的简单统一框架",
        "摘要翻译": "离策略深度强化学习（RL）在一系列具有挑战性的领域中取得了成功。然而，标准的离策略RL算法可能会遇到几个问题，例如Q学习的不稳定性以及探索与利用的平衡。为了缓解这些问题，我们提出了SUNRISE，一个简单统一的集成方法，它与各种离策略RL算法兼容。SUNRISE整合了两个关键组成部分：（a）基于集成的加权Bellman备份，它根据Q集成的不确定性估计重新加权目标Q值，以及（b）一种推理方法，使用最高上置信界限选择动作以实现高效探索。通过使用随机初始化的Bootstrap强制代理之间的多样性，我们表明这些不同的想法在很大程度上是正交的，并且可以富有成效地整合在一起，进一步提高了现有离策略RL算法（如Soft Actor-Critic和Rainbow DQN）在低维和高维环境中的连续和离散控制任务的性能。",
        "领域": "深度强化学习、集成学习、控制任务",
        "问题": "解决标准离策略RL算法中的不稳定性和探索与利用平衡问题",
        "动机": "提高离策略RL算法在各种环境下的性能和稳定性",
        "方法": "提出SUNRISE框架，整合基于集成的加权Bellman备份和高效探索的推理方法",
        "关键词": [
            "集成学习",
            "深度强化学习",
            "离策略学习",
            "Bellman备份",
            "高效探索"
        ],
        "涉及的技术概念": {
            "集成学习": "通过整合多个学习算法来提高模型的性能和稳定性",
            "加权Bellman备份": "根据Q集成的不确定性估计重新加权目标Q值，以提高学习稳定性",
            "高效探索": "使用最高上置信界限选择动作，以优化探索与利用的平衡"
        },
        "success": true
    },
    {
        "order": 1046,
        "title": "Supervised Tree-Wasserstein Distance",
        "html": "https://ICML.cc//virtual/2021/poster/10645",
        "abstract": "To measure the similarity of documents, the Wasserstein distance is a powerful tool, but it requires a high computational cost. Recently, for fast computation of the Wasserstein distance, methods for approximating the Wasserstein distance using a tree metric have been proposed. These tree-based methods allow fast comparisons of a large number of documents; however, they are unsupervised and do not learn task-specific distances. In this work, we propose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised metric learning method based on the tree metric. Specifically, we rewrite the Wasserstein distance on the tree metric by the parent-child relationships of a tree, and formulate it as a continuous optimization problem using a contrastive loss. Experimentally, we show that the STW distance can be computed fast, and improves the accuracy of document classification tasks. Furthermore, the STW distance is formulated by matrix multiplications, runs on a GPU, and is suitable for batch processing. Therefore, we show that the STW distance is extremely efficient when comparing a large number of documents.",
        "conference": "ICML",
        "中文标题": "监督树-瓦瑟斯坦距离",
        "摘要翻译": "为了衡量文档之间的相似性，瓦瑟斯坦距离是一个强大的工具，但它需要较高的计算成本。最近，为了快速计算瓦瑟斯坦距离，提出了使用树度量近似瓦瑟斯坦距离的方法。这些基于树的方法允许快速比较大量文档；然而，它们是无监督的，不学习任务特定的距离。在这项工作中，我们提出了监督树-瓦瑟斯坦（STW）距离，这是一种基于树度量的快速、监督的度量学习方法。具体来说，我们通过树的父子关系重写了树度量上的瓦瑟斯坦距离，并使用对比损失将其表述为一个连续优化问题。实验上，我们展示了STW距离可以快速计算，并提高了文档分类任务的准确性。此外，STW距离通过矩阵乘法表述，可在GPU上运行，适合批量处理。因此，我们展示了STW距离在比较大量文档时极为高效。",
        "领域": "自然语言处理与视觉结合、文档分类、度量学习",
        "问题": "如何高效计算并学习任务特定的文档相似性度量",
        "动机": "现有的基于树的瓦瑟斯坦距离近似方法虽然计算快速，但缺乏监督学习能力，无法适应特定任务的需求",
        "方法": "提出监督树-瓦瑟斯坦距离（STW），通过树的父子关系重写瓦瑟斯坦距离，并利用对比损失进行连续优化",
        "关键词": [
            "监督学习",
            "树度量",
            "瓦瑟斯坦距离",
            "文档分类",
            "GPU加速"
        ],
        "涉及的技术概念": {
            "瓦瑟斯坦距离": "用于衡量文档之间相似性的强大工具，但计算成本高",
            "树度量": "用于近似瓦瑟斯坦距离，实现快速文档比较",
            "对比损失": "用于在监督学习框架下优化STW距离，使其适应特定任务需求"
        },
        "success": true
    },
    {
        "order": 1047,
        "title": "Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach",
        "html": "https://ICML.cc//virtual/2021/poster/9683",
        "abstract": "Learning faithful graph representations as sets of vertex embeddings has become a fundamental intermediary step in a wide range of machine learning applications. We propose the systematic use of symmetric spaces in representation learning, a class encompassing many of the previously used embedding targets. This enables us to introduce a new method, the use of Finsler metrics integrated in a Riemannian optimization scheme, that better adapts to dissimilar structures in the graph. We develop a tool to analyze the embeddings and infer structural properties of the data sets. For implementation, we choose Siegel spaces, a versatile family of symmetric spaces. Our approach outperforms competitive baselines for graph reconstruction tasks on various synthetic and real-world datasets. We further demonstrate its applicability on two downstream tasks, recommender systems and node classification.",
        "conference": "ICML",
        "中文标题": "图嵌入的对称空间：一种Finsler-Riemannian方法",
        "摘要翻译": "学习忠实的图表示作为顶点嵌入的集合已成为广泛机器学习应用中的基本中间步骤。我们提出在表示学习中系统使用对称空间，这一类包含了许多先前使用的嵌入目标。这使我们能够引入一种新方法，即在Riemannian优化方案中集成Finsler度量，以更好地适应图中的不同结构。我们开发了一个工具来分析嵌入并推断数据集的结构属性。在实现上，我们选择了Siegel空间，这是一个多功能的对称空间家族。我们的方法在各种合成和真实世界的数据集上的图重建任务中优于竞争基线。我们进一步证明了其在两个下游任务——推荐系统和节点分类上的适用性。",
        "领域": "图表示学习、推荐系统、节点分类",
        "问题": "如何更有效地学习图的表示以适应图中的不同结构",
        "动机": "为了提升图表示学习的效率和适应性，特别是在处理图中不同结构时的表现",
        "方法": "提出在Riemannian优化方案中集成Finsler度量，并使用Siegel空间作为对称空间的实现",
        "关键词": [
            "图嵌入",
            "对称空间",
            "Finsler度量",
            "Riemannian优化",
            "Siegel空间"
        ],
        "涉及的技术概念": {
            "对称空间": "用于图表示学习的一类空间，包含多种先前使用的嵌入目标，提供更灵活的表示能力",
            "Finsler度量": "在Riemannian优化方案中引入，以更好地适应图中的不同结构，提升嵌入的适应性",
            "Siegel空间": "作为对称空间的一个具体实现，用于图嵌入的实际应用，因其多功能性被选用"
        },
        "success": true
    },
    {
        "order": 1048,
        "title": "Synthesizer: Rethinking Self-Attention for Transformer Models",
        "html": "https://ICML.cc//virtual/2021/poster/9307",
        "abstract": "The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. In our experiments, we first show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we find that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only $60\\%$ faster but also improves perplexity by a relative $3.5\\%$. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks. ",
        "conference": "ICML",
        "中文标题": "合成器：重新思考Transformer模型中的自注意力机制",
        "摘要翻译": "点积自注意力机制被认为是当前最先进的Transformer模型的核心且不可或缺的部分。但它真的必需吗？本文研究了基于点积的自注意力机制对Transformer模型性能的真正重要性和贡献。通过大量实验，我们发现（1）随机对齐矩阵的表现出人意料地具有竞争力；（2）从令牌-令牌（查询-键）交互中学习注意力权重虽然有用，但终究不是那么重要。为此，我们提出了\textsc{Synthesizer}，一种无需令牌-令牌交互即可学习合成注意力权重的模型。在我们的实验中，我们首先展示了与原始Transformer模型相比，简单的合成器在一系列任务中（包括机器翻译、语言建模、文本生成和GLUE/SuperGLUE基准测试）实现了高度竞争性的性能。当与点积注意力结合使用时，我们发现合成器始终优于Transformer。此外，我们还对合成器与动态卷积进行了额外比较，结果显示简单的随机合成器不仅速度快60%，而且相对改进了困惑度3.5%。最后，我们展示了简单的因子化合成器在仅编码任务上可以优于Linformer。",
        "领域": "自然语言处理与视觉结合, 语言模型优化, 注意力机制研究",
        "问题": "探讨点积自注意力机制在Transformer模型中的真正必要性和作用",
        "动机": "质疑并验证点积自注意力机制是否是Transformer模型性能提升的关键因素",
        "方法": "提出Synthesizer模型，通过实验比较随机对齐矩阵、学习注意力权重的方法及合成器模型的性能",
        "关键词": [
            "自注意力机制",
            "Transformer模型",
            "合成器",
            "语言模型",
            "性能优化"
        ],
        "涉及的技术概念": {
            "点积自注意力机制": "传统Transformer模型中用于计算令牌间关系的核心机制",
            "合成器": "一种无需令牌间直接交互即可生成注意力权重的新模型",
            "随机对齐矩阵": "实验中用作对比的一种简单方法，用于验证自注意力机制的必要性"
        },
        "success": true
    },
    {
        "order": 1049,
        "title": "Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures",
        "html": "https://ICML.cc//virtual/2021/poster/8429",
        "abstract": "Many cluster similarity indices are used to evaluate clustering algorithms, and choosing the best one for a particular task remains an open problem. We demonstrate that this problem is crucial: there are many disagreements among the indices, these disagreements do affect which algorithms are preferred in applications, and this can lead to degraded performance in real-world systems. We propose a theoretical framework to tackle this problem: we develop a list of desirable properties and conduct an extensive theoretical analysis to verify which indices satisfy them. This allows for making an informed choice: given a particular application, one can first select properties that are desirable for the task and then identify indices satisfying these. Our work unifies and considerably extends existing attempts at analyzing cluster similarity indices: we introduce new properties, formalize existing ones, and mathematically prove or disprove each property for an extensive list of validation indices. This broader and more rigorous approach leads to recommendations that considerably differ from how validation indices are currently being chosen by practitioners. Some of the most popular indices are even shown to be dominated by previously overlooked ones.",
        "conference": "ICML",
        "中文标题": "聚类相似性指标的系统性分析：如何验证验证措施",
        "摘要翻译": "许多聚类相似性指标被用于评估聚类算法，然而为特定任务选择最佳指标仍是一个未解决的问题。我们证明这一问题至关重要：指标之间存在许多分歧，这些分歧确实影响了应用中算法的选择，并可能导致实际系统性能下降。我们提出了一个理论框架来解决这一问题：我们制定了一系列理想属性，并进行了广泛的理论分析以验证哪些指标满足这些属性。这使得能够做出明智的选择：给定特定应用，可以首先选择对任务有利的属性，然后识别满足这些属性的指标。我们的工作统一并显著扩展了现有对聚类相似性指标分析的尝试：我们引入了新属性，形式化了现有属性，并对一系列验证指标的每个属性进行了数学证明或反驳。这种更广泛且更严格的方法得出的推荐与当前实践者选择验证指标的方式大相径庭。一些最受欢迎的指标甚至被证明被之前忽视的指标所主导。",
        "领域": "聚类分析",
        "问题": "如何为特定聚类任务选择最佳的相似性指标",
        "动机": "解决聚类相似性指标选择中的分歧问题，以提升聚类算法在实际应用中的性能",
        "方法": "提出理论框架，制定理想属性列表，并进行广泛的理论分析以验证指标满足的属性",
        "关键词": [
            "聚类相似性指标",
            "验证措施",
            "理论框架",
            "聚类算法评估"
        ],
        "涉及的技术概念": {
            "聚类相似性指标": "用于评估聚类算法性能的指标，衡量聚类结果之间的相似性",
            "理论框架": "用于系统分析和验证聚类相似性指标满足的理想属性的结构化方法",
            "验证措施": "用于确定聚类算法性能的评估标准，确保所选指标适合特定应用场景"
        },
        "success": true
    },
    {
        "order": 1050,
        "title": "Targeted Data Acquisition for Evolving Negotiation Agents",
        "html": "https://ICML.cc//virtual/2021/poster/9211",
        "abstract": "Successful negotiators must learn how to balance optimizing for self-interest and cooperation. Yet current artificial negotiation agents often heavily depend on the quality of the static datasets they were trained on, limiting their capacity to fashion an adaptive response balancing self-interest and cooperation. For this reason, we find that these agents can achieve either high utility or cooperation, but not both. To address this, we introduce a targeted data acquisition framework where we guide the exploration of a reinforcement learning agent using annotations from an expert oracle. The guided exploration incentivizes the learning agent to go beyond its static dataset and develop new negotiation strategies. We show that this enables our agents to obtain higher-reward and more Pareto-optimal solutions when negotiating with both simulated and human partners compared to standard supervised learning and reinforcement learning methods. This trend additionally holds when comparing agents using our targeted data acquisition framework to variants of agents trained with a mix of supervised learning and reinforcement learning, or to agents using tailored reward functions that explicitly optimize for utility and Pareto-optimality.",
        "conference": "ICML",
        "中文标题": "为进化谈判代理定向获取数据",
        "摘要翻译": "成功的谈判者必须学会如何在自我利益优化与合作之间找到平衡。然而，当前的人工谈判代理往往严重依赖于它们训练所用的静态数据集的质量，这限制了它们制定出既能平衡自我利益又能合作的适应性反应的能力。因此，我们发现这些代理可以取得高效用或高合作性，但无法同时实现两者。为了解决这个问题，我们引入了一个定向数据获取框架，在该框架中，我们利用专家预言机的注释来指导强化学习代理的探索。这种引导探索激励学习代理超越其静态数据集，开发新的谈判策略。我们展示，与标准的监督学习和强化学习方法相比，这种方法使我们的代理在与模拟和人类伙伴谈判时能够获得更高奖励和更多帕累托最优解。此外，当比较使用我们的定向数据获取框架的代理与使用监督学习和强化学习混合训练的代理变体，或使用专门优化效用和帕累托最优性的定制奖励函数的代理时，这一趋势仍然成立。",
        "领域": "强化学习、人机交互、自动谈判系统",
        "问题": "当前人工谈判代理无法同时实现高效用和高合作性，因为它们依赖于静态数据集，缺乏适应性。",
        "动机": "提高谈判代理在自我利益与合作之间的平衡能力，使其能够开发出更有效的谈判策略。",
        "方法": "引入定向数据获取框架，通过专家预言机的注释指导强化学习代理的探索，超越静态数据集，开发新策略。",
        "关键词": [
            "定向数据获取",
            "强化学习",
            "谈判代理",
            "帕累托最优",
            "专家预言机"
        ],
        "涉及的技术概念": {
            "定向数据获取": "通过专家预言机的注释指导强化学习代理的探索，以获取更有价值的数据。",
            "强化学习": "一种让代理通过与环境互动学习最优策略的机器学习方法。",
            "帕累托最优": "在谈判中，指没有一方可以在不损害另一方利益的情况下进一步改善自己的利益的状态。"
        },
        "success": true
    },
    {
        "order": 1051,
        "title": "Task-Optimal Exploration in Linear Dynamical Systems",
        "html": "https://ICML.cc//virtual/2021/poster/8917",
        "abstract": "Exploration in unknown environments is a fundamental problem in reinforcement learning and control. In this work, we study task-guided exploration and determine what precisely an agent must learn about their environment in order to complete a particular task. Formally, we study a broad class of decision-making problems in the setting of linear dynamical systems, a class that includes the linear quadratic regulator problem. We provide instance- and task-dependent lower bounds which explicitly quantify the difficulty of completing a task of interest. Motivated by our lower bound, we propose a computationally efficient experiment-design based exploration algorithm. We show that it optimally explores the environment, collecting precisely the information needed to complete the task, and provide finite-time bounds guaranteeing that it achieves the instance- and task-optimal sample complexity, up to constant factors. Through several examples of the linear quadratic regulator problem, we show that performing task-guided exploration provably improves on exploration schemes which do not take into account the task of interest. Along the way, we establish that certainty equivalence decision making is instance- and task-optimal, and obtain the first algorithm for the linear quadratic regulator problem which is instance-optimal. We conclude with several experiments illustrating the effectiveness of our approach in practice.",
        "conference": "ICML",
        "中文标题": "线性动态系统中的任务最优探索",
        "摘要翻译": "在未知环境中的探索是强化学习和控制中的一个基本问题。在这项工作中，我们研究了任务引导的探索，并确定了一个代理必须了解其环境的哪些具体信息才能完成特定任务。形式上，我们研究了一类广泛的决策问题，在线性动态系统的设置下，这类问题包括线性二次调节器问题。我们提供了实例和任务依赖的下界，这些下界明确量化了完成感兴趣任务的难度。受到我们下界的启发，我们提出了一种计算效率高的基于实验设计的探索算法。我们证明它能够最优地探索环境，精确收集完成任务所需的信息，并提供有限时间界限，保证它达到实例和任务最优的样本复杂度，直至常数因子。通过线性二次调节器问题的几个例子，我们展示了执行任务引导的探索在理论上优于不考虑感兴趣任务的探索方案。在此过程中，我们确立了确定性等价决策是实例和任务最优的，并获得了第一个实例最优的线性二次调节器问题算法。最后，我们通过几个实验说明了我们方法在实际中的有效性。",
        "领域": "强化学习、控制理论、线性动态系统",
        "问题": "如何在未知环境中进行有效的任务引导探索，以完成特定任务",
        "动机": "研究任务引导探索的必要性，以及如何精确地收集完成任务所需的环境信息",
        "方法": "提出了一种基于实验设计的计算效率高的探索算法，该算法能够最优地探索环境并收集必要信息",
        "关键词": [
            "任务引导探索",
            "线性动态系统",
            "线性二次调节器",
            "实例最优",
            "任务最优"
        ],
        "涉及的技术概念": {
            "任务引导探索": "根据特定任务需求，精确探索环境中完成任务所需的信息",
            "线性动态系统": "研究决策问题的数学模型，包括线性二次调节器问题",
            "实例最优": "算法在特定实例下达到最优性能，特别是在样本复杂度方面"
        },
        "success": true
    },
    {
        "order": 1052,
        "title": "Taylor Expansion of Discount Factors",
        "html": "https://ICML.cc//virtual/2021/poster/10085",
        "abstract": "In practical reinforcement learning (RL), the discount factor used for estimating value functions often differs from that used for defining the evaluation objective. In this work, we study the effect that this discrepancy of discount factors has during learning, and discover a family of objectives that interpolate value functions of two distinct discount factors. Our analysis suggests new ways for estimating value functions and performing policy optimization updates, which demonstrate empirical performance gains. This framework also leads to new insights on commonly-used deep RL heuristic modifications to policy optimization algorithms.",
        "conference": "ICML",
        "中文标题": "折扣因子的泰勒展开",
        "摘要翻译": "在实际的强化学习（RL）中，用于估计价值函数的折扣因子通常与用于定义评估目标的折扣因子不同。在这项工作中，我们研究了这种折扣因子差异在学习过程中的影响，并发现了一系列目标，这些目标在两个不同的折扣因子之间插值价值函数。我们的分析提出了估计价值函数和执行策略优化更新的新方法，这些方法展示了经验性能的提升。这一框架还带来了对常用深度RL启发式修改策略优化算法的新见解。",
        "领域": "强化学习、策略优化、价值函数估计",
        "问题": "研究折扣因子差异对强化学习过程中价值函数估计和策略优化的影响",
        "动机": "探索折扣因子差异如何影响强化学习的性能，并提出新的方法来优化这一过程",
        "方法": "通过分析折扣因子差异的影响，提出了一系列在两个不同折扣因子之间插值的目标，并开发了新的价值函数估计和策略优化方法",
        "关键词": [
            "强化学习",
            "折扣因子",
            "价值函数",
            "策略优化",
            "泰勒展开"
        ],
        "涉及的技术概念": {
            "折扣因子": "用于调整未来奖励在当前决策中的重要性，影响价值函数的估计",
            "价值函数": "评估在特定状态下遵循特定策略的预期回报，是强化学习中的核心概念",
            "策略优化": "通过调整策略参数来最大化预期回报的过程，本研究提出了基于折扣因子差异的新优化方法"
        },
        "success": true
    },
    {
        "order": 1053,
        "title": "TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL",
        "html": "https://ICML.cc//virtual/2021/poster/10321",
        "abstract": "Training autonomous agents able to generalize to multiple tasks is a key target of Deep Reinforcement Learning (DRL) research. In parallel to improving DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how teacher algorithms can train DRL agents more efficiently by adapting task selection to their evolving abilities. While multiple standard benchmarks exist to compare DRL agents, there is currently no such thing for ACL algorithms. Thus, comparing existing approaches is difficult, as too many experimental parameters differ from paper to paper. In this work, we identify several key challenges faced by ACL algorithms. Based on these, we present TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment. We then use TeachMyAgent to conduct a comparative study of representative existing approaches, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge. We also show that the Parkour environment remains an open problem. We open-source our environments, all studied ACL algorithms (collected from open-source code or re-implemented), and DRL students in a Python package available at https://github.com/flowersteam/TeachMyAgent.",
        "conference": "ICML",
        "中文标题": "TeachMyAgent：深度强化学习中自动课程学习的基准",
        "摘要翻译": "训练能够泛化到多个任务的自主代理是深度强化学习（DRL）研究的一个关键目标。在改进DRL算法本身的同时，自动课程学习（ACL）研究教师算法如何通过根据代理不断发展的能力调整任务选择来更有效地训练DRL代理。虽然存在多个标准基准来比较DRL代理，但目前还没有用于ACL算法的此类基准。因此，比较现有方法很困难，因为太多实验参数在论文之间有所不同。在这项工作中，我们确定了ACL算法面临的几个关键挑战。基于这些，我们提出了TeachMyAgent（TA），一个利用程序任务生成的当前ACL算法的基准。它包括1）使用程序Box2D双足步行者环境变体的挑战特定单元测试，和2）一个新的程序Parkour环境，结合了大多数ACL挑战，使其成为全球性能评估的理想选择。然后，我们使用TeachMyAgent对代表性现有方法进行比较研究，展示了一些不使用专家知识的ACL算法的竞争力。我们还表明，Parkour环境仍然是一个未解决的问题。我们在一个Python包中开源了我们的环境、所有研究的ACL算法（从开源代码收集或重新实现）和DRL学生，可在https://github.com/flowersteam/TeachMyAgent获取。",
        "领域": "深度强化学习、自动课程学习、程序任务生成",
        "问题": "缺乏用于比较自动课程学习（ACL）算法的标准基准",
        "动机": "为了更有效地训练深度强化学习（DRL）代理，通过自动调整任务选择以适应其发展能力",
        "方法": "提出了TeachMyAgent基准，包括挑战特定单元测试和新的程序Parkour环境，用于比较现有ACL方法",
        "关键词": [
            "自动课程学习",
            "深度强化学习",
            "程序任务生成",
            "基准测试",
            "Parkour环境"
        ],
        "涉及的技术概念": {
            "自动课程学习（ACL）": "研究如何通过调整任务选择来更有效地训练DRL代理",
            "程序任务生成": "用于创建多样化任务环境，以测试和评估ACL算法的能力",
            "深度强化学习（DRL）": "训练能够泛化到多个任务的自主代理的核心技术"
        },
        "success": true
    },
    {
        "order": 1054,
        "title": "Temporal Difference Learning as Gradient Splitting",
        "html": "https://ICML.cc//virtual/2021/poster/10519",
        "abstract": "Temporal difference learning with linear function approximation is a popular method to obtain a low-dimensional approximation of the value function of a  policy in a Markov Decision Process. We provide an interpretation of this method in terms of a splitting of the gradient of an appropriately chosen function. As a consequence of this interpretation, convergence proofs for gradient descent can be applied almost verbatim to temporal difference learning. Beyond giving a fuller explanation of why temporal difference works, this interpretation also yields improved convergence times. We consider the setting with $1/\\sqrt{T}$ step-size, where previous comparable finite-time convergence time bounds for temporal difference learning had the multiplicative factor $1/(1-\\gamma)$ in front of the bound, with $\\gamma$ being the discount factor. We show that a minor variation on TD learning which estimates the mean of the value function separately has a convergence time where  $1/(1-\\gamma)$ only multiplies an asymptotically negligible term.  ",
        "conference": "ICML",
        "success": true,
        "中文标题": "时间差分学习作为梯度分割",
        "摘要翻译": "在马尔可夫决策过程中，使用线性函数近似的时间差分学习是一种流行的方法，用于获得策略价值函数的低维近似。我们提供了一种解释，将这种方法视为对适当选择函数的梯度分割。作为这种解释的结果，梯度下降的收敛证明几乎可以直接应用于时间差分学习。除了更全面地解释时间差分学习为何有效外，这种解释还带来了收敛时间的改进。我们考虑了步长为$1/\\\\sqrt{T}$的设置，其中先前时间差分学习的有限时间收敛时间界限在界限前有乘性因子$1/(1-\\\\gamma)$，$\\\\gamma$为折扣因子。我们展示了一个对TD学习的微小变化，即单独估计价值函数的均值，其收敛时间中$1/(1-\\\\gamma)$仅乘以一个渐近可忽略的项。",
        "领域": "强化学习, 机器学习优化, 近似动态规划",
        "问题": "如何在马尔可夫决策过程中更有效地近似价值函数并改进时间差分学习的收敛时间。",
        "动机": "为了更全面地理解时间差分学习的工作原理，并在此基础上改进其收敛时间。",
        "方法": "通过将时间差分学习解释为梯度分割，并应用梯度下降的收敛证明，同时提出一种单独估计价值函数均值的TD学习变体。",
        "关键词": [
            "时间差分学习",
            "梯度分割",
            "马尔可夫决策过程",
            "线性函数近似",
            "收敛时间"
        ],
        "涉及的技术概念": {
            "时间差分学习": "一种在强化学习中用于近似价值函数的方法，通过当前估计和未来估计之间的差异来更新价值函数。",
            "梯度分割": "将梯度下降方法应用于时间差分学习，提供了一种新的解释框架，有助于理解其收敛性。"
        }
    },
    {
        "order": 1055,
        "title": "Temporally Correlated Task Scheduling for Sequence Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9449",
        "abstract": "Sequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. For example, (i) in simultaneous machine translation, one can conduct translation under different latency (i.e., how many input words to read/wait before translation); (ii) in stock trend forecasting, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). While it is clear that those temporally correlated tasks can help each other, there is a very limited exploration on how to better leverage multiple auxiliary tasks to boost the performance of the main task. In this work, we introduce a learnable scheduler to sequence learning, which can adaptively select auxiliary tasks for training depending on the model status and the current training data. The scheduler and the model for the main task are jointly trained through bi-level optimization. Experiments show that our method significantly improves the performance of simultaneous machine translation and stock trend forecasting.",
        "conference": "ICML",
        "中文标题": "时序相关任务调度用于序列学习",
        "摘要翻译": "近年来，序列学习吸引了机器学习社区的广泛研究关注。在许多应用中，一个序列学习任务通常与多个时序相关的辅助任务相关联，这些辅助任务在使用多少输入信息或预测未来哪一步方面有所不同。例如，（i）在同步机器翻译中，可以在不同的延迟下进行翻译（即，在翻译前读取/等待多少个输入单词）；（ii）在股票趋势预测中，可以预测股票在不同未来天数（例如，明天、后天）的价格。虽然这些时序相关的任务显然可以互相帮助，但关于如何更好地利用多个辅助任务来提升主要任务性能的探索非常有限。在这项工作中，我们为序列学习引入了一个可学习的调度器，它可以根据模型状态和当前训练数据自适应地选择辅助任务进行训练。调度器和主要任务的模型通过双层优化联合训练。实验表明，我们的方法显著提高了同步机器翻译和股票趋势预测的性能。",
        "领域": "自然语言处理与视觉结合, 时间序列预测, 机器学习优化",
        "问题": "如何有效利用多个时序相关的辅助任务来提升主要序列学习任务的性能",
        "动机": "探索在序列学习中如何通过自适应选择辅助任务来优化主要任务的性能",
        "方法": "引入一个可学习的调度器，通过双层优化与主要任务模型联合训练，自适应选择辅助任务",
        "关键词": [
            "序列学习",
            "任务调度",
            "双层优化",
            "同步机器翻译",
            "股票趋势预测"
        ],
        "涉及的技术概念": {
            "时序相关任务": "与主要任务在时间上相关联的辅助任务，用于提升主要任务的性能",
            "可学习调度器": "根据模型状态和训练数据自适应选择辅助任务的机制",
            "双层优化": "同时优化调度器和主要任务模型的训练策略"
        },
        "success": true
    },
    {
        "order": 1056,
        "title": "Temporal Predictive Coding For Model-Based Planning In Latent Space",
        "html": "https://ICML.cc//virtual/2021/poster/9447",
        "abstract": "High-dimensional observations are a major challenge in the application of model-based reinforcement learning (MBRL) to real-world environments. To handle high-dimensional sensory inputs, existing approaches use representation learning to map high-dimensional observations into a lower-dimensional latent space that is more amenable to dynamics estimation and planning. In this work, we present an information-theoretic approach that employs temporal predictive coding to encode elements in the environment that can be predicted across time. Since this approach focuses on encoding temporally-predictable information, we implicitly prioritize the encoding of task-relevant components over nuisance information within the environment that are provably task-irrelevant. By learning this representation in conjunction with a recurrent state space model, we can then perform planning in latent space. We evaluate our model on a challenging modification of standard DMControl tasks where the background is replaced with natural videos that contain complex but irrelevant information to the planning task. Our experiments show that our model is superior to existing methods in the challenging complex-background setting while remaining competitive with current state-of-the-art models in the standard setting.",
        "conference": "ICML",
        "中文标题": "基于潜在空间模型规划的时间预测编码",
        "摘要翻译": "高维观测数据是将基于模型的强化学习（MBRL）应用于现实世界环境时面临的主要挑战。为了处理高维感官输入，现有方法使用表示学习将高维观测映射到更适合动态估计和规划的低维潜在空间。在这项工作中，我们提出了一种信息论方法，该方法采用时间预测编码来编码环境中可以跨时间预测的元素。由于这种方法专注于编码时间上可预测的信息，我们隐式地优先编码任务相关组件，而非环境中被证明与任务无关的干扰信息。通过在循环状态空间模型中学习这种表示，我们可以在潜在空间中进行规划。我们在标准DMControl任务的挑战性修改版本上评估了我们的模型，其中背景被替换为包含复杂但与规划任务无关信息的自然视频。我们的实验表明，在具有挑战性的复杂背景设置下，我们的模型优于现有方法，同时在标准设置下与当前最先进的模型保持竞争力。",
        "领域": "模型基强化学习、表示学习、潜在空间规划",
        "问题": "处理高维观测数据以在复杂环境中进行有效的模型基强化学习规划",
        "动机": "解决高维感官输入在模型基强化学习应用中的挑战，通过优先编码任务相关信息来提高规划效率",
        "方法": "采用时间预测编码和循环状态空间模型，在潜在空间中学习和规划，优先处理时间上可预测的任务相关信息",
        "关键词": [
            "时间预测编码",
            "潜在空间规划",
            "模型基强化学习",
            "表示学习",
            "信息论方法"
        ],
        "涉及的技术概念": {
            "时间预测编码": "用于编码环境中可以跨时间预测的元素，优先处理任务相关信息",
            "潜在空间规划": "在低维潜在空间中进行动态估计和规划，以提高处理高维观测数据的效率",
            "循环状态空间模型": "用于学习和表示环境动态，支持在潜在空间中进行有效的规划"
        },
        "success": true
    },
    {
        "order": 1057,
        "title": "TempoRL: Learning When to Act",
        "html": "https://ICML.cc//virtual/2021/poster/9879",
        "abstract": "Reinforcement learning is a powerful approach to learn behaviour through interactions with an environment.\nHowever, behaviours are usually learned in a purely reactive fashion, where an appropriate action is selected based on an observation.\nIn this form, it is challenging to learn when it is necessary to execute new decisions.\nThis makes learning inefficient especially in environments that need various degrees of fine and coarse control.\nTo address this, we propose a proactive setting in which the agent not only selects an action in a state but also for how long to commit to that action.\nOur TempoRL approach introduces skip connections between states and learns a skip-policy for repeating the same action along these skips.\nWe demonstrate the effectiveness of TempoRL on a variety of traditional and deep RL environments, showing that our approach is capable of learning successful policies up to an order of magnitude faster than vanilla Q-learning.",
        "conference": "ICML",
        "中文标题": "TempoRL：学习何时行动",
        "摘要翻译": "强化学习是一种通过与环境的互动来学习行为的强大方法。然而，行为通常是以纯粹反应式的方式学习的，即根据观察选择适当的行动。在这种形式下，学习何时需要执行新的决策是具有挑战性的。这使得学习效率低下，尤其是在需要不同程度精细和粗略控制的环境中。为了解决这个问题，我们提出了一种主动设置，在这种设置中，代理不仅在一个状态下选择一个动作，而且还决定对该动作的承诺时间。我们的TempoRL方法引入了状态之间的跳跃连接，并学习了一个跳跃策略，用于沿着这些跳跃重复相同的动作。我们在各种传统和深度强化学习环境中证明了TempoRL的有效性，表明我们的方法能够比普通的Q学习快一个数量级的速度学习成功的策略。",
        "领域": "强化学习、深度强化学习、智能决策",
        "问题": "如何在强化学习中高效地学习何时执行新的决策",
        "动机": "提高在需要不同程度控制的环境中强化学习的效率",
        "方法": "提出TempoRL方法，引入状态间的跳跃连接和学习跳跃策略，以决定动作的承诺时间",
        "关键词": [
            "强化学习",
            "TempoRL",
            "跳跃策略",
            "决策效率",
            "Q学习"
        ],
        "涉及的技术概念": {
            "强化学习": "通过与环境的互动学习行为的方法",
            "跳跃连接": "在状态之间引入的连接，允许代理跳过某些状态",
            "跳跃策略": "决定在跳跃连接上重复相同动作的策略"
        },
        "success": true
    },
    {
        "order": 1058,
        "title": "Tensor Programs IIb: Architectural Universality Of Neural Tangent Kernel Training Dynamics",
        "html": "https://ICML.cc//virtual/2021/poster/10765",
        "abstract": "Yang (2020) recently showed that the Neural Tangent Kernel (NTK) at initialization has an infinite-width limit for a large class of architectures including modern staples such as ResNet and Transformers. However, their analysis does not apply to training. Here, we show the same neural networks (in the so-called NTK parametrization) during training follow a kernel gradient descent dynamics in function space, where the kernel is the infinite-width NTK. This completes the proof of the architectural universality of NTK behavior. To achieve this result, we apply the Tensor Programs technique: Write the entire SGD dynamics inside a Tensor Program and analyze it via the Master Theorem. To facilitate this proof, we develop a graphical notation for Tensor Programs, which we believe is also an important contribution toward the pedagogy and exposition of the Tensor Programs technique.",
        "conference": "ICML",
        "中文标题": "张量程序IIb：神经正切核训练动态的架构普适性",
        "摘要翻译": "Yang（2020）最近表明，对于包括ResNet和Transformer在内的现代主流架构，初始化时的神经正切核（NTK）具有无限宽度极限。然而，他们的分析不适用于训练过程。在这里，我们展示了相同的神经网络（在所谓的NTK参数化下）在训练过程中遵循函数空间中的核梯度下降动态，其中核是无限宽度的NTK。这完成了NTK行为架构普适性的证明。为了实现这一结果，我们应用了张量程序技术：将整个SGD动态写入张量程序中，并通过主定理进行分析。为了促进这一证明，我们开发了张量程序的图形表示法，我们认为这也是对张量程序技术的教学和展示的重要贡献。",
        "领域": "深度学习理论、神经网络架构分析、梯度下降优化",
        "问题": "证明神经正切核（NTK）在训练过程中的架构普适性",
        "动机": "为了扩展Yang（2020）关于NTK在初始化时无限宽度极限的研究，使其包含训练过程的分析",
        "方法": "应用张量程序技术分析SGD动态，并开发图形表示法以促进理解和证明",
        "关键词": [
            "神经正切核",
            "张量程序",
            "架构普适性",
            "梯度下降",
            "无限宽度极限"
        ],
        "涉及的技术概念": {
            "神经正切核（NTK）": "用于描述神经网络在无限宽度极限下的训练动态的核函数",
            "张量程序": "一种技术，用于将优化算法的动态编码为张量操作，便于理论分析",
            "主定理": "用于分析张量程序中SGD动态的理论工具，帮助证明NTK的架构普适性"
        },
        "success": true
    },
    {
        "order": 1059,
        "title": "Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10689",
        "abstract": "As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization).\nHowever, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT.\nWe propose simple modifications to the standard parametrization to allow for feature learning in the limit. \nUsing the *Tensor Programs* technique, we derive explicit formulas for such limits.\nOn Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly.\nWe find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.",
        "conference": "ICML",
        "中文标题": "张量程序IV：无限宽度神经网络中的特征学习",
        "摘要翻译": "当深度神经网络的宽度趋向于无穷大时，如果参数化适当（例如NTK参数化），其在梯度下降下的行为可以变得简化和可预测（例如由神经切线核（NTK）给出）。然而，我们表明，神经网络的标准和NTK参数化不允许可以*学习*特征的无限宽度限制，这对于预训练和迁移学习（如BERT）至关重要。我们提出了对标准参数化的简单修改，以允许在极限中进行特征学习。使用*张量程序*技术，我们导出了这些极限的显式公式。在Word2Vec和通过MAML在Omniglot上进行的小样本学习这两个关键依赖于特征学习的典型任务上，我们精确计算了这些极限。我们发现它们优于NTK基线和有限宽度网络，后者随着宽度的增加接近无限宽度特征学习的性能。",
        "领域": "深度学习理论、迁移学习、小样本学习",
        "问题": "解决无限宽度神经网络在标准参数化和NTK参数化下无法进行特征学习的问题",
        "动机": "为了在无限宽度神经网络中实现特征学习，以支持预训练和迁移学习等应用",
        "方法": "提出对标准参数化的修改，使用张量程序技术推导无限宽度极限的显式公式，并在Word2Vec和Omniglot小样本学习任务上验证",
        "关键词": [
            "无限宽度神经网络",
            "特征学习",
            "张量程序",
            "NTK参数化",
            "迁移学习"
        ],
        "涉及的技术概念": {
            "神经切线核（NTK）": "用于描述无限宽度神经网络在梯度下降下行为的理论工具",
            "张量程序": "用于推导神经网络在无限宽度极限下的显式公式的技术",
            "特征学习": "指神经网络在学习过程中自动提取和优化输入数据的表示能力"
        },
        "success": true
    },
    {
        "order": 1060,
        "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models",
        "html": "https://ICML.cc//virtual/2021/poster/9181",
        "abstract": "Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe",
        "conference": "ICML",
        "中文标题": "TeraPipe：用于训练大规模语言模型的令牌级流水线并行技术",
        "摘要翻译": "模型并行已成为训练现代大规模深度语言模型的必要条件。在这项工作中，我们发现了一个与现有模型并行方法正交的新维度：由于基于Transformer的语言模型的自回归特性，可以在单个训练序列内进行流水线并行。这使得与之前的工作相比，可以实现更细粒度的流水线。基于这一关键思想，我们设计了TeraPipe，一种高性能的令牌级流水线并行算法，用于基于Transformer的语言模型的同步模型并行训练。我们开发了一种基于动态规划的新算法，以计算给定特定模型和集群配置下的最优流水线执行方案。我们展示，与最先进的模型并行方法相比，TeraPipe可以在48个p3.16xlarge实例的AWS集群上，为拥有1750亿参数的最大GPT-3模型加速训练5.0倍。复现代码可在https://github.com/zhuohan123/terapipe找到。",
        "领域": "大规模语言模型训练、模型并行技术、Transformer模型优化",
        "问题": "如何在大规模语言模型训练中实现更高效的模型并行技术",
        "动机": "现有模型并行方法在处理大规模语言模型训练时效率不足，需要更细粒度的并行技术以提高训练速度",
        "方法": "提出TeraPipe，一种令牌级流水线并行算法，利用动态规划计算最优流水线执行方案",
        "关键词": [
            "令牌级流水线并行",
            "模型并行训练",
            "Transformer模型",
            "动态规划",
            "大规模语言模型"
        ],
        "涉及的技术概念": {
            "令牌级流水线并行": "在单个训练序列内实现细粒度的流水线并行，提高训练效率",
            "动态规划": "用于计算给定模型和集群配置下的最优流水线执行方案",
            "自回归特性": "基于Transformer的语言模型的特性，使得在单个序列内进行流水线并行成为可能"
        },
        "success": true
    },
    {
        "order": 1061,
        "title": "Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8723",
        "abstract": "Reinforcement Learning in large action spaces is a challenging problem. This is especially true for cooperative multi-agent reinforcement learning (MARL), which often requires tractable learning while respecting various constraints like communication budget and information about other agents.\nIn this work, we focus on the fundamental hurdle affecting both value-based and policy-gradient approaches: an exponential blowup of the action space with the number of agents. For value-based methods, it poses challenges in accurately representing the optimal value function for value-based methods, thus inducing suboptimality. For policy gradient methods, it renders the critic ineffective and exacerbates the problem of the lagging critic. We show that from a learning theory perspective, both problems can be addressed by accurately representing the associated action-value function with a low-complexity  hypothesis class. This requires accurately modelling the agent interactions in a sample efficient way. To this end, we propose a novel tensorised formulation of the Bellman equation. This gives rise to our method Tesseract, which utilises the view of Q-function seen as a tensor where the modes correspond to action spaces of different agents. Algorithms derived from Tesseract decompose the Q-tensor across the agents and utilise low-rank tensor approximations to model the agent interactions relevant to the task.  We provide PAC analysis for Tesseract based algorithms and highlight their relevance to the class of rich observation MDPs. Empirical results in different domains confirm the gains in sample efficiency using Tesseract as supported by the theory.",
        "conference": "ICML",
        "中文标题": "Tesseract：用于多智能体强化学习的张量化执行者",
        "摘要翻译": "在大动作空间中进行强化学习是一个具有挑战性的问题。对于合作式多智能体强化学习（MARL）尤其如此，它通常需要在尊重通信预算和其他智能体信息等各种约束的同时进行可处理的学习。在这项工作中，我们关注影响基于价值和策略梯度方法的基本障碍：随着智能体数量的增加，动作空间呈指数级膨胀。对于基于价值的方法，这给准确表示最优价值函数带来了挑战，从而导致了次优性。对于策略梯度方法，它使得评论者无效，并加剧了评论者滞后的问题。我们从学习理论的角度展示了，这两个问题都可以通过用低复杂度假设类准确表示相关的动作价值函数来解决。这需要以样本高效的方式准确建模智能体之间的交互。为此，我们提出了一种新颖的贝尔曼方程的张量化公式。这产生了我们的方法Tesseract，它将Q函数视为一个张量，其中模式对应于不同智能体的动作空间。从Tesseract派生的算法将Q张量分解到各个智能体，并利用低秩张量近似来建模与任务相关的智能体交互。我们为基于Tesseract的算法提供了PAC分析，并强调了它们与丰富观察MDP类别的相关性。不同领域的实证结果证实了Tesseract在样本效率上的提升，这与理论支持相符。",
        "领域": "多智能体强化学习、张量分解、价值函数近似",
        "问题": "解决多智能体强化学习中动作空间随智能体数量指数级膨胀的问题",
        "动机": "提高在大动作空间中多智能体强化学习的样本效率和算法性能",
        "方法": "提出一种新颖的贝尔曼方程的张量化公式，利用低秩张量近似建模智能体交互",
        "关键词": [
            "多智能体强化学习",
            "张量分解",
            "贝尔曼方程",
            "低秩近似",
            "样本效率"
        ],
        "涉及的技术概念": {
            "张量化贝尔曼方程": "将传统的贝尔曼方程重新表述为张量形式，以便更有效地处理多智能体系统中的高维动作空间",
            "低秩张量近似": "通过低秩近似技术减少模型复杂度，同时保持对智能体交互的准确建模",
            "PAC分析": "为Tesseract算法提供概率近似正确（PAC）的理论保证，确保其在实际应用中的有效性和可靠性"
        },
        "success": true
    },
    {
        "order": 1062,
        "title": "Testing DNN-based Autonomous Driving Systems under Critical Environmental Conditions",
        "html": "https://ICML.cc//virtual/2021/poster/9405",
        "abstract": "Due to the increasing usage of Deep Neural Network (DNN) based autonomous driving systems (ADS) where erroneous or unexpected behaviours can lead to catastrophic accidents, testing such systems is of growing importance. Existing approaches often just focus on finding erroneous behaviours and have not thoroughly studied the impact of environmental conditions. In this paper, we propose to test DNN-based ADS under different environmental conditions to identify the critical ones, that is, the environmental conditions under which the ADS are more prone to errors. To tackle the problem of the space of environmental conditions being extremely large, we present a novel approach named TACTIC that employs the search-based method to identify critical environmental conditions generated by an image-to-image translation model. Large-scale experiments show that TACTIC can effectively identify critical environmental conditions and produce realistic testing images, and meanwhile, reveal more erroneous behaviours compared to existing approaches.",
        "conference": "ICML",
        "中文标题": "在关键环境条件下测试基于深度神经网络的自动驾驶系统",
        "摘要翻译": "随着基于深度神经网络（DNN）的自动驾驶系统（ADS）使用日益增多，其中错误或意外的行为可能导致灾难性事故，测试此类系统变得越来越重要。现有方法往往仅关注于发现错误行为，并未深入研究环境条件的影响。本文提出在不同环境条件下测试基于DNN的ADS，以识别关键环境条件，即那些ADS更容易出错的环境条件。针对环境条件空间极大的问题，我们提出了一种名为TACTIC的新方法，该方法采用基于搜索的方法来识别由图像到图像翻译模型生成的关键环境条件。大规模实验表明，TACTIC能有效识别关键环境条件并生成真实的测试图像，同时与现有方法相比，揭示了更多的错误行为。",
        "领域": "自动驾驶系统测试、深度神经网络应用、图像到图像翻译",
        "问题": "识别在哪些关键环境条件下基于DNN的自动驾驶系统更容易出错",
        "动机": "提高自动驾驶系统在各种环境条件下的安全性和可靠性",
        "方法": "提出TACTIC方法，结合基于搜索的方法和图像到图像翻译模型，识别关键环境条件并生成测试图像",
        "关键词": [
            "自动驾驶系统测试",
            "深度神经网络",
            "关键环境条件",
            "图像到图像翻译",
            "搜索方法"
        ],
        "涉及的技术概念": {
            "深度神经网络（DNN）": "作为自动驾驶系统的核心技术，用于处理和理解复杂的驾驶环境",
            "图像到图像翻译模型": "用于生成不同环境条件下的测试图像，模拟真实世界中的各种驾驶场景",
            "基于搜索的方法": "用于高效地探索和识别导致自动驾驶系统出错的关键环境条件"
        },
        "success": true
    },
    {
        "order": 1063,
        "title": "Testing Group Fairness via Optimal Transport Projections",
        "html": "https://ICML.cc//virtual/2021/poster/10251",
        "abstract": "We have developed a statistical testing framework to detect if a given machine learning classifier fails to satisfy a wide range of group fairness notions. Our test is a flexible, interpretable, and statistically rigorous tool for auditing whether exhibited biases are intrinsic to the algorithm or simply due to the randomness in the data. The statistical challenges, which may arise from multiple impact criteria that define group fairness and which are discontinuous on model parameters, are conveniently tackled by projecting the empirical measure to the set of group-fair probability models using optimal transport. This statistic is efficiently computed using linear programming, and its asymptotic distribution is explicitly obtained. The proposed framework can also be used to test for composite fairness hypotheses and fairness with multiple sensitive attributes. The optimal transport testing formulation improves interpretability by characterizing the minimal covariate perturbations that eliminate the bias observed in the audit.",
        "conference": "ICML",
        "中文标题": "通过最优传输投影测试群体公平性",
        "摘要翻译": "我们开发了一个统计测试框架，用于检测给定的机器学习分类器是否未能满足广泛的群体公平性概念。我们的测试是一个灵活、可解释且统计严谨的工具，用于审计所展现的偏见是算法固有的还是仅仅由于数据中的随机性。通过使用最优传输将经验测度投影到群体公平概率模型集合上，可以方便地解决可能由定义群体公平性的多个影响标准以及这些标准在模型参数上的不连续性所引起的统计挑战。该统计量通过线性规划高效计算，并明确获得了其渐近分布。所提出的框架还可用于测试复合公平性假设和具有多个敏感属性的公平性。最优传输测试公式通过表征消除审计中观察到的偏见所需的最小协变量扰动，提高了可解释性。",
        "领域": "公平机器学习、统计测试、算法审计",
        "问题": "检测机器学习分类器是否满足群体公平性概念",
        "动机": "开发一个灵活、可解释且统计严谨的工具，用于审计机器学习算法中的偏见来源",
        "方法": "使用最优传输将经验测度投影到群体公平概率模型集合上，通过线性规划高效计算统计量",
        "关键词": [
            "群体公平性",
            "最优传输",
            "统计测试",
            "算法审计",
            "线性规划"
        ],
        "涉及的技术概念": {
            "最优传输": "用于将经验测度投影到群体公平概率模型集合上，解决统计挑战",
            "线性规划": "用于高效计算最优传输统计量",
            "群体公平性": "定义和测试机器学习分类器是否满足广泛的公平性概念"
        },
        "success": true
    },
    {
        "order": 1064,
        "title": "TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer",
        "html": "https://ICML.cc//virtual/2021/poster/8789",
        "abstract": "The problem of fixing errors in programs has attracted substantial interest over the years. The key challenge for building an effective code fixing tool is to capture a wide range of errors and meanwhile maintain high accuracy. In this paper, we address this challenge and present a new learning-based system, called TFix. TFix works directly on program text and phrases the problem of code fixing as a text-to-text task. In turn, this enables it to leverage a powerful Transformer based model pre-trained on natural language and fine-tuned to generate code fixes (via a large, high-quality dataset obtained from GitHub commits). TFix is not specific to a particular programming language or class of defects and, in fact, improved its precision by simultaneously fine-tuning on 52 different error types reported by a popular static analyzer. Our evaluation on a massive dataset of JavaScript programs shows that TFix is practically effective: it is able to synthesize code that fixes the error in ~67 percent of cases and significantly outperforms existing learning-based approaches.",
        "conference": "ICML",
        "中文标题": "TFix：使用文本到文本转换器学习修复编码错误",
        "摘要翻译": "多年来，修复程序中的错误问题引起了广泛关注。构建一个有效的代码修复工具的关键挑战在于捕捉广泛的错误同时保持高准确性。在本文中，我们应对这一挑战，提出了一个新的基于学习的系统，称为TFix。TFix直接在程序文本上工作，并将代码修复问题表述为一个文本到文本的任务。这使其能够利用一个基于Transformer的强大模型，该模型在自然语言上进行了预训练，并通过从GitHub提交中获得的大型高质量数据集进行了微调以生成代码修复。TFix不特定于任何编程语言或缺陷类别，事实上，它通过在52种不同的错误类型上同时进行微调（这些错误类型由一个流行的静态分析器报告），提高了其精确度。我们对JavaScript程序的大规模数据集进行的评估显示，TFix在实践中非常有效：它能够合成修复错误的代码，成功率约为67%，显著优于现有的基于学习的方法。",
        "领域": "程序错误修复、自然语言处理与代码结合、静态代码分析",
        "问题": "如何有效地捕捉广泛的编程错误并保持高准确性的代码修复",
        "动机": "解决构建高效代码修复工具的关键挑战，即同时处理广泛的错误类型并保持高修复准确性",
        "方法": "采用基于Transformer的文本到文本模型，预训练于自然语言并微调于代码修复任务，支持多错误类型同时处理",
        "关键词": [
            "代码修复",
            "Transformer模型",
            "静态分析",
            "多错误处理",
            "JavaScript"
        ],
        "涉及的技术概念": {
            "Transformer模型": "用于将代码修复任务建模为文本到文本的转换问题，基于自然语言预训练模型进行微调",
            "静态分析": "用于识别和分类程序中的错误类型，为模型提供多样化的训练数据",
            "微调": "通过在特定任务（如代码修复）上进一步训练预训练模型，以提高模型在该任务上的性能"
        },
        "success": true
    },
    {
        "order": 1065,
        "title": "The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation",
        "html": "https://ICML.cc//virtual/2021/poster/9805",
        "abstract": "We consider training models on private data that are distributed across user devices. To ensure privacy, we add on-device noise and use secure aggregation so that only the noisy sum is revealed to the server. We present a comprehensive end-to-end system, which appropriately discretizes the data and adds discrete Gaussian noise before performing secure aggregation. We provide a novel privacy analysis for sums of discrete Gaussians and carefully analyze the effects of data quantization and modular summation arithmetic. Our theoretical guarantees highlight the complex tension between communication, privacy, and accuracy. Our extensive experimental results demonstrate that our solution is essentially able to match the accuracy to central differential privacy with less than 16 bits of precision per value.",
        "conference": "ICML",
        "中文标题": "联邦学习中用于安全聚合的分布式离散高斯机制",
        "摘要翻译": "我们考虑在分布在用户设备上的私有数据上训练模型。为了确保隐私，我们在设备端添加噪声并使用安全聚合，以便仅向服务器揭示带噪声的总和。我们提出了一个全面的端到端系统，该系统在执行安全聚合之前适当地离散化数据并添加离散高斯噪声。我们为离散高斯和提供了一种新颖的隐私分析，并仔细分析了数据量化和模求和算术的影响。我们的理论保证突出了通信、隐私和准确性之间复杂的紧张关系。我们的大量实验结果表明，我们的解决方案基本上能够以每个值少于16位的精度匹配中心差分隐私的准确性。",
        "领域": "联邦学习, 隐私保护, 差分隐私",
        "问题": "在联邦学习环境中，如何在保证数据隐私的同时，有效地训练模型。",
        "动机": "研究如何在分布式私有数据上训练模型，同时通过添加噪声和安全聚合技术保护用户隐私。",
        "方法": "提出一个端到端系统，包括数据离散化、添加离散高斯噪声和执行安全聚合，以及对这些操作对隐私、通信和准确性影响的理论分析。",
        "关键词": [
            "联邦学习",
            "安全聚合",
            "离散高斯噪声",
            "差分隐私",
            "数据量化"
        ],
        "涉及的技术概念": {
            "离散高斯噪声": "在设备端添加的噪声，用于保护数据隐私，同时允许在安全聚合后仍能进行有效的模型训练。",
            "安全聚合": "一种技术，确保在联邦学习中只有带噪声的总和被服务器看到，从而保护单个用户的数据隐私。",
            "差分隐私": "一种隐私保护框架，通过添加噪声来确保数据处理的隐私性，同时尽可能保留数据的实用性。"
        },
        "success": true
    },
    {
        "order": 1066,
        "title": "The Earth Mover's Pinball Loss: Quantiles for Histogram-Valued Regression",
        "html": "https://ICML.cc//virtual/2021/poster/9859",
        "abstract": "Although ubiquitous in the sciences, histogram data have not received much attention by the Deep Learning community. Whilst regression and classification tasks for scalar and vector data are routinely solved by neural networks, a principled approach for estimating histogram labels as a function of an input vector or image is lacking in the literature. We present a dedicated method for Deep Learning-based histogram regression, which incorporates cross-bin information and yields distributions over possible histograms, expressed by $\\tau$-quantiles of the cumulative histogram in each bin. The crux of our approach is a new loss function obtained by applying the pinball loss to the cumulative histogram, which for 1D histograms reduces to the Earth Mover's distance (EMD) in the special case of the median ($\\tau = 0.5$), and generalizes it to arbitrary quantiles. We validate our method with an illustrative toy example, a football-related task, and an astrophysical computer vision problem. We show that with our loss function, the accuracy of the predicted median histograms is very similar to the standard EMD case (and higher than for per-bin loss functions such as cross-entropy), while the predictions become much more informative at almost no additional computational cost.",
        "conference": "ICML",
        "中文标题": "地球移动者的弹球损失：直方图值回归的分位数",
        "摘要翻译": "尽管在科学领域无处不在，直方图数据尚未受到深度学习社区的太多关注。虽然标量和向量数据的回归和分类任务通常由神经网络解决，但文献中缺乏一种原则性的方法来估计作为输入向量或图像函数的直方图标签。我们提出了一种基于深度学习的直方图回归的专用方法，该方法结合了跨箱信息，并通过每个箱中累积直方图的τ分位数表达了可能直方图的分布。我们方法的关键是通过将弹球损失应用于累积直方图获得的新损失函数，对于一维直方图，在中位数（τ=0.5）的特殊情况下简化为地球移动者距离（EMD），并将其推广到任意分位数。我们通过一个说明性的玩具例子、一个足球相关任务和一个天体物理计算机视觉问题验证了我们的方法。我们表明，使用我们的损失函数，预测的中位数直方图的准确性与标准EMD情况非常相似（并且高于每箱损失函数，如交叉熵），而预测在几乎没有额外计算成本的情况下变得更加信息丰富。",
        "领域": "直方图回归、深度学习应用、计算机视觉",
        "问题": "深度学习在直方图数据回归任务中的应用不足，缺乏有效的损失函数来预测直方图标签。",
        "动机": "解决深度学习在处理直方图数据回归任务中的不足，提供一种能够预测直方图标签并表达分布信息的方法。",
        "方法": "提出了一种新的损失函数，将弹球损失应用于累积直方图，推广了地球移动者距离（EMD）到任意分位数，用于直方图回归。",
        "关键词": [
            "直方图回归",
            "弹球损失",
            "地球移动者距离",
            "深度学习",
            "分位数回归"
        ],
        "涉及的技术概念": {
            "弹球损失": "用于累积直方图的损失函数，推广了地球移动者距离（EMD）到任意分位数。",
            "地球移动者距离（EMD）": "在中位数（τ=0.5）的特殊情况下，弹球损失简化为EMD，用于衡量两个直方图之间的距离。",
            "分位数回归": "通过τ分位数表达直方图的分布，提供比单一中位数预测更丰富的信息。"
        },
        "success": true
    },
    {
        "order": 1067,
        "title": "The Emergence of Individuality",
        "html": "https://ICML.cc//virtual/2021/poster/8933",
        "abstract": "Individuality is essential in human society. It induces the division of labor and thus improves the efficiency and productivity. Similarly, it should also be a key to multi-agent cooperation. Inspired by that individuality is of being an individual separate from others, we propose a simple yet efficient method for the emergence of individuality (EOI) in multi-agent reinforcement learning (MARL). EOI learns a probabilistic classifier that predicts a probability distribution over agents given their observation and gives each agent an intrinsic reward of being correctly predicted by the classifier. The intrinsic reward encourages the agents to visit their own familiar observations, and learning the classifier by such observations makes the intrinsic reward signals stronger and in turn makes the agents more identifiable. To further enhance the intrinsic reward and promote the emergence of individuality, two regularizers are proposed to increase the discriminability of the classifier. We implement EOI on top of popular MARL algorithms. Empirically, we show that EOI outperforms existing methods in a variety of multi-agent cooperative scenarios. ",
        "conference": "ICML",
        "中文标题": "个体性的涌现",
        "摘要翻译": "个体性在人类社会中至关重要。它促进了劳动分工，从而提高了效率和生产力。同样，它也应该是多智能体合作的关键。受到个体性意味着与其他人分离的启发，我们提出了一种简单而有效的方法，用于多智能体强化学习（MARL）中个体性的涌现（EOI）。EOI学习一个概率分类器，该分类器根据智能体的观察预测智能体的概率分布，并为每个智能体提供一个被分类器正确预测的内在奖励。这种内在奖励鼓励智能体访问它们自己熟悉的观察，而通过学习这些观察来学习分类器使得内在奖励信号更强，进而使得智能体更易识别。为了进一步增强内在奖励并促进个体性的涌现，提出了两个正则化器来增加分类器的可区分性。我们在流行的MARL算法上实现了EOI。经验上，我们展示了EOI在各种多智能体合作场景中优于现有方法。",
        "领域": "多智能体强化学习",
        "问题": "如何在多智能体强化学习中促进个体性的涌现以提高合作效率",
        "动机": "受到人类社会个体性促进劳动分工和提高效率的启发，研究旨在通过增强智能体的个体性来优化多智能体合作",
        "方法": "提出了一种名为EOI的方法，通过学习一个概率分类器来预测智能体的概率分布，并为每个智能体提供内在奖励，以及引入两个正则化器来增强分类器的可区分性",
        "关键词": [
            "多智能体强化学习",
            "个体性涌现",
            "内在奖励",
            "概率分类器",
            "正则化器"
        ],
        "涉及的技术概念": {
            "概率分类器": "用于根据智能体的观察预测智能体的概率分布，为每个智能体提供内在奖励",
            "内在奖励": "鼓励智能体访问自己熟悉的观察，增强智能体的个体性",
            "正则化器": "用于增加分类器的可区分性，进一步促进个体性的涌现"
        },
        "success": true
    },
    {
        "order": 1068,
        "title": "The Heavy-Tail Phenomenon in SGD",
        "html": "https://ICML.cc//virtual/2021/poster/9017",
        "abstract": "In recent years, various notions of capacity and complexity have been proposed for characterizing the generalization properties of stochastic gradient descent (SGD) in deep learning. Some of the popular notions that correlate well with the performance on unseen data are (i) the `flatness' of the local minimum found by SGD, which is related to the eigenvalues of the Hessian, (ii) the ratio of the stepsize $\\eta$ to the batch-size $b$, which essentially controls the magnitude of the stochastic gradient noise, and (iii) the `tail-index', which measures the heaviness of the tails of the network weights at convergence. In this paper, we argue that these three seemingly unrelated perspectives for generalization are deeply linked to each other. We claim that depending on the structure of the Hessian of the loss at the minimum, and the choices of the algorithm parameters $\\eta$ and $b$, the SGD iterates will converge to a \\emph{heavy-tailed} stationary distribution. We rigorously prove this claim in the setting of quadratic optimization: we show that even in a simple linear regression problem with independent and identically distributed data whose distribution has finite moments of all order, the iterates can be heavy-tailed with infinite variance. We further characterize the behavior of the tails with respect to algorithm parameters, the dimension, and the curvature. We then translate our results into insights about the behavior of SGD in deep learning. We support our theory with experiments conducted on synthetic data, fully connected, and convolutional neural networks.",
        "conference": "ICML",
        "success": true,
        "中文标题": "随机梯度下降中的重尾现象",
        "摘要翻译": "近年来，为了描述深度学习随机梯度下降（SGD）的泛化特性，提出了各种容量和复杂度的概念。一些与未见数据性能良好相关的流行概念包括：（i）SGD找到的局部最小值的‘平坦度’，这与Hessian矩阵的特征值有关；（ii）步长$\\eta$与批量大小$b$的比率，这基本上控制了随机梯度噪声的幅度；以及（iii）‘尾指数’，它衡量了网络权重在收敛时的尾部厚重程度。在本文中，我们认为这三种看似无关的泛化视角是深刻相互关联的。我们声称，根据损失在最小值处的Hessian矩阵结构，以及算法参数$\\eta$和$b$的选择，SGD迭代将收敛到一个重尾的平稳分布。我们在二次优化的设置中严格证明了这一主张：我们表明，即使在一个简单的线性回归问题中，其数据分布具有所有阶的有限矩，迭代也可能是具有无限方差的重尾。我们进一步描述了尾部行为与算法参数、维度和曲率的关系。然后，我们将我们的结果转化为关于SGD在深度学习中行为的见解。我们通过在合成数据、全连接和卷积神经网络上进行的实验支持我们的理论。",
        "领域": "深度学习优化, 随机梯度下降, 神经网络训练",
        "问题": "理解随机梯度下降（SGD）在深度学习中的泛化特性及其与算法参数的关系",
        "动机": "探索SGD迭代收敛到重尾分布的条件及其对深度学习模型泛化能力的影响",
        "方法": "通过理论分析和实验验证，研究SGD在二次优化和深度学习中的行为，特别是其收敛到重尾分布的条件和影响",
        "关键词": [
            "随机梯度下降",
            "重尾分布",
            "深度学习优化",
            "泛化能力",
            "Hessian矩阵"
        ],
        "涉及的技术概念": {
            "平坦度": "描述SGD找到的局部最小值的平坦程度，与Hessian矩阵的特征值相关，影响模型的泛化能力",
            "步长与批量大小的比率": "控制随机梯度噪声的幅度，影响SGD的收敛行为和泛化性能"
        }
    },
    {
        "order": 1069,
        "title": "The Hintons in your Neural Network: a Quantum Field Theory View of Deep Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10761",
        "abstract": "In this work we develop a quantum field theory formalism for deep learning, where input signals are encoded in Gaussian states, a generalization of Gaussian processes which encode the agent's uncertainty about the input signal. We show how to represent linear and non-linear layers as unitary quantum gates, and interpret the fundamental excitations of the quantum model as particles, dubbed ``Hintons''.\nOn top of opening a new perspective and techniques for studying neural networks, the quantum formulation is well suited for optical quantum computing, and provides quantum deformations of neural networks that can be run efficiently on those devices.\nFinally, we discuss a semi-classical limit of the quantum deformed models which is amenable to classical simulation.",
        "conference": "ICML",
        "中文标题": "神经网络中的Hintons：深度学习的量子场论视角",
        "摘要翻译": "在这项工作中，我们为深度学习开发了一种量子场论形式，其中输入信号被编码在高斯态中，这是高斯过程的一种推广，编码了代理对输入信号的不确定性。我们展示了如何将线性和非线性层表示为单一量子门，并将量子模型的基本激发解释为粒子，称为‘Hintons’。除了为研究神经网络开辟了新的视角和技术外，量子形式非常适合光学量子计算，并提供了可以在这些设备上高效运行的神经网络的量子变形。最后，我们讨论了量子变形模型的半经典极限，这适合于经典模拟。",
        "领域": "量子机器学习、深度学习理论、光学量子计算",
        "问题": "如何将深度学习模型与量子场论相结合，以及如何利用量子计算的优势来改进神经网络。",
        "动机": "探索深度学习与量子场论之间的联系，开发新的量子计算方法以提高神经网络的计算效率和性能。",
        "方法": "开发了一种量子场论形式，将输入信号编码在高斯态中，线性和非线性层表示为单一量子门，并引入‘Hintons’粒子概念。",
        "关键词": [
            "量子场论",
            "深度学习",
            "高斯态",
            "量子计算",
            "Hintons"
        ],
        "涉及的技术概念": {
            "高斯态": "用于编码输入信号及其不确定性，是高斯过程的推广。",
            "单一量子门": "用于表示神经网络中的线性和非线性层，实现量子计算中的信息处理。",
            "Hintons": "量子模型中的基本激发粒子，为神经网络研究提供了新的视角。"
        },
        "success": true
    },
    {
        "order": 1070,
        "title": "The Impact of Record Linkage on Learning from Feature Partitioned Data",
        "html": "https://ICML.cc//virtual/2021/poster/9167",
        "abstract": "There has been recently a significant boost to machine\n  learning with distributed data, in particular with the success of\n  federated learning. A common and very challenging setting is that of\n  vertical or feature partitioned data, when multiple data\n  providers hold \n  different features about common entities. In general, training needs\n  to be preceded by record linkage (RL), a step that finds the correspondence between\n   the observations of the datasets. RL is\n  prone to mistakes in the real world. Despite the importance of\n  the problem, there has been so far no formal assessment\nof the way in which RL errors impact learning models. Work in the area\n  either use heuristics or assume that the optimal RL is known in\n  advance.\nIn this paper, we provide the first assessment of the problem for\nsupervised learning. For wide sets of losses, we provide technical conditions\nunder which the\nclassifier learned after noisy RL converges (with the data size) to the best classifier that would be learned from\n  mistake-free RL. This yields new insights on the way the\n  pipeline RL + ML operates, from the role of large margin\n  classification on dampening the impact of RL mistakes to clues on how to further\n  optimize RL as a preprocessing step to ML. Experiments on a large\n  UCI benchmark validate\n  those formal observations. \n",
        "conference": "ICML",
        "中文标题": "记录链接对特征分区数据学习的影响",
        "摘要翻译": "近年来，分布式数据在机器学习领域，特别是联邦学习的成功，带来了显著的推动。一个常见且极具挑战性的场景是垂直或特征分区数据，即多个数据提供者持有关于共同实体的不同特征。通常，训练之前需要进行记录链接（RL），这是一个寻找数据集观测之间对应关系的步骤。在现实世界中，RL容易出错。尽管问题的重要性，迄今为止还没有对RL错误如何影响学习模型的方式进行正式评估。该领域的工作要么使用启发式方法，要么假设最优RL是预先已知的。在本文中，我们首次对监督学习中的这一问题进行了评估。对于广泛的损失集，我们提供了技术条件，在这些条件下，经过噪声RL学习后的分类器（随着数据量的增加）会收敛于无错误RL学习到的最佳分类器。这为RL + ML流水线的运作方式提供了新的见解，从大边距分类在减轻RL错误影响中的作用，到如何进一步优化RL作为ML预处理步骤的线索。在一个大型UCI基准上的实验验证了这些正式观察。",
        "领域": "联邦学习, 监督学习, 记录链接",
        "问题": "评估记录链接错误如何影响学习模型的性能",
        "动机": "理解记录链接错误对机器学习模型性能的影响，以优化记录链接作为机器学习预处理步骤",
        "方法": "提供技术条件，分析在噪声记录链接后学习的分类器如何收敛于无错误记录链接学习到的最佳分类器",
        "关键词": [
            "记录链接",
            "监督学习",
            "联邦学习",
            "特征分区数据",
            "机器学习"
        ],
        "涉及的技术概念": {
            "记录链接（RL）": "寻找数据集观测之间对应关系的步骤，作为机器学习预处理的一部分",
            "监督学习": "在给定输入输出对的情况下训练模型以预测输出的机器学习方法",
            "联邦学习": "一种分布式机器学习方法，允许多个数据提供者共同训练模型而不共享原始数据"
        },
        "success": true
    },
    {
        "order": 1071,
        "title": "The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10219",
        "abstract": "Despite their overwhelming capacity to overfit, deep neural networks trained by specific optimization algorithms tend to generalize relatively well to unseen data.  Recently, researchers explained it by investigating the implicit bias of optimization algorithms. A remarkable progress is the work (Lyu & Li, 2019), which proves gradient descent (GD) maximizes the margin of homogeneous deep neural networks. Except the first-order optimization algorithms like GD, adaptive algorithms such as AdaGrad, RMSProp and Adam are popular owing to their rapid training process. Mean-while, numerous works have provided empirical evidence that adaptive methods may suffer from poor generalization performance. However, theoretical explanation for the generalization of adaptive optimization algorithms is still lacking.  In this paper, we study the implicit bias of adaptive optimization algorithms on homogeneous neural networks. In particular, we study the convergent direction of parameters when they are optimizing the logistic loss.  We prove that the convergent direction of Adam and RMSProp is the same as GD, while for AdaGrad, the convergent direction depends on the adaptive conditioner. Technically, we provide a unified framework to analyze convergent direction of adaptive optimization algorithms by constructing novel and nontrivial adaptive gradient flow and surrogate margin. The theoretical findings explain the superiority on generalization of exponential moving average strategy that is adopted by RMSProp and Adam.   To the best of  knowledge,  it  is  the  first  work  to  study  the convergent direction of adaptive optimizations on non-linear deep neural networks",
        "conference": "ICML",
        "中文标题": "同质神经网络上自适应优化算法的隐式偏差",
        "摘要翻译": "尽管深度神经网络具有极强的过拟合能力，但通过特定优化算法训练的深度神经网络往往能够较好地泛化到未见过的数据。最近，研究人员通过研究优化算法的隐式偏差来解释这一现象。一个显著的进展是Lyu和Li在2019年的工作，他们证明了梯度下降（GD）能够最大化同质深度神经网络的边际。除了像GD这样的一阶优化算法外，AdaGrad、RMSProp和Adam等自适应算法因其快速的训练过程而广受欢迎。同时，大量工作提供了经验证据，表明自适应方法可能泛化性能较差。然而，对于自适应优化算法泛化的理论解释仍然缺乏。在本文中，我们研究了同质神经网络上自适应优化算法的隐式偏差。特别是，我们研究了在优化逻辑损失时参数的收敛方向。我们证明了Adam和RMSProp的收敛方向与GD相同，而对于AdaGrad，收敛方向取决于自适应调节器。在技术上，我们通过构建新颖且非平凡的自适应梯度流和代理边际，提供了一个统一框架来分析自适应优化算法的收敛方向。理论发现解释了RMSProp和Adam采用的指数移动平均策略在泛化上的优越性。据我们所知，这是第一个研究非线性深度神经网络上自适应优化收敛方向的工作。",
        "领域": "深度学习优化算法、神经网络理论、自适应优化",
        "问题": "理解自适应优化算法在同质神经网络上的隐式偏差及其对泛化性能的影响",
        "动机": "尽管自适应优化算法在训练速度上有优势，但其泛化性能的理论解释仍然缺乏，本研究旨在填补这一空白",
        "方法": "通过构建自适应梯度流和代理边际的统一框架，分析自适应优化算法的收敛方向",
        "关键词": [
            "隐式偏差",
            "自适应优化",
            "同质神经网络",
            "泛化性能",
            "收敛方向"
        ],
        "涉及的技术概念": {
            "隐式偏差": "指优化算法在训练过程中无意中引入的偏好，影响模型的泛化性能",
            "自适应梯度流": "用于分析自适应优化算法收敛方向的技术工具，通过模拟梯度下降过程来研究参数更新",
            "代理边际": "在理论分析中构建的概念，用于衡量模型在训练数据上的表现与泛化能力之间的关系"
        },
        "success": true
    },
    {
        "order": 1072,
        "title": "The Limits of Min-Max Optimization Algorithms: Convergence to Spurious Non-Critical Sets",
        "html": "https://ICML.cc//virtual/2021/poster/8633",
        "abstract": "Compared to minimization, the min-max optimization in machine learning applications is considerably more convoluted because of the existence of cycles and similar phenomena. Such oscillatory behaviors are well-understood in the convex-concave regime, and many algorithms are known to overcome them. In this paper, we go beyond this basic setting and characterize the convergence properties of many popular methods in solving non-convex/non-concave  problems. In particular, we show that a wide class of state-of-the-art schemes and heuristics may converge with arbitrarily high probability to attractors that are in no way min-max optimal or even stationary. Our work thus points out a potential pitfall among many existing theoretical frameworks, and we corroborate our theoretical claims by explicitly showcasing spurious attractors in simple two-dimensional problems.",
        "conference": "ICML",
        "中文标题": "最小-最大优化算法的极限：向虚假非临界集的收敛",
        "摘要翻译": "与最小化相比，机器学习应用中的最小-最大优化由于循环和类似现象的存在而显得更加复杂。这些振荡行为在凸-凹体系中已得到充分理解，并且已知许多算法可以克服它们。在本文中，我们超越了这一基本设置，并描述了在解决非凸/非凹问题时许多流行方法的收敛特性。特别是，我们展示了一类广泛的最先进方案和启发式方法可能会以任意高的概率收敛到吸引子，这些吸引子在某种程度上既不是最小-最大最优的，甚至也不是静止的。因此，我们的工作指出了许多现有理论框架中潜在的陷阱，并通过在简单的二维问题中明确展示虚假吸引子来证实我们的理论主张。",
        "领域": "优化算法、机器学习理论、非凸优化",
        "问题": "最小-最大优化算法在非凸/非凹问题中的收敛性问题",
        "动机": "揭示和解决最小-最大优化算法在非凸/非凹问题中可能收敛到非最优或非静止吸引子的问题",
        "方法": "分析多种流行最小-最大优化算法的收敛特性，并通过理论分析和二维问题示例展示虚假吸引子的存在",
        "关键词": [
            "最小-最大优化",
            "非凸优化",
            "收敛性分析",
            "虚假吸引子",
            "优化算法"
        ],
        "涉及的技术概念": {
            "最小-最大优化": "一种在机器学习中广泛使用的优化方法，旨在最小化最大损失",
            "非凸/非凹问题": "指目标函数既不凸也不凹的优化问题，这类问题的优化更为复杂",
            "虚假吸引子": "优化算法可能错误地收敛到的非最优或非静止点，这在非凸/非凹优化中尤为常见"
        },
        "success": true
    },
    {
        "order": 1073,
        "title": "The Lipschitz Constant of Self-Attention",
        "html": "https://ICML.cc//virtual/2021/poster/8677",
        "abstract": "Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected\nor convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.",
        "conference": "ICML",
        "中文标题": "自注意力机制的Lipschitz常数",
        "摘要翻译": "在深度学习的多个领域中，如可证明的对抗鲁棒性、估计Wasserstein距离、稳定生成对抗网络（GANs）的训练以及构建可逆神经网络等，神经网络Lipschitz常数的研究已被广泛探讨。这些研究主要集中在由线性映射和点态非线性组成的全连接或卷积网络的Lipschitz常数界定上。本文中，我们研究了自注意力机制这一在序列建模中广泛使用的非线性神经网络模块的Lipschitz常数。我们证明了标准点积自注意力对于无界输入域不是Lipschitz的，并提出了一种替代的L2自注意力机制，该机制是Lipschitz的。我们推导了L2自注意力Lipschitz常数的上界，并提供了其渐近紧性的经验证据。为了展示我们理论工作的实际相关性，我们构建了可逆自注意力，并将其用于基于Transformer的架构中，完成了一个字符级语言建模任务。",
        "领域": "序列建模、可逆神经网络、语言模型",
        "问题": "研究自注意力机制的Lipschitz常数，解决其在无界输入域下不满足Lipschitz条件的问题。",
        "动机": "为了在序列建模等应用中更稳定地使用自注意力机制，需要理解并改进其Lipschitz性质。",
        "方法": "提出L2自注意力机制替代标准点积自注意力，证明其Lipschitz性质，并推导其上界。",
        "关键词": [
            "Lipschitz常数",
            "自注意力机制",
            "序列建模",
            "可逆神经网络",
            "Transformer"
        ],
        "涉及的技术概念": {
            "Lipschitz常数": "用于衡量神经网络对输入变化的敏感度，影响模型的稳定性和鲁棒性。",
            "自注意力机制": "一种允许模型在处理序列数据时动态地关注不同位置信息的机制。",
            "可逆神经网络": "一种可以精确反转输入输出映射的神经网络，有助于模型的可解释性和稳定性。"
        },
        "success": true
    },
    {
        "order": 1074,
        "title": "The Logical Options Framework",
        "html": "https://ICML.cc//virtual/2021/poster/9379",
        "abstract": "Learning composable policies for environments with complex rules and tasks is a challenging problem. We introduce a hierarchical reinforcement learning framework called the Logical Options Framework (LOF) that learns policies that are satisfying, optimal, and composable. LOF efficiently learns policies that satisfy tasks by representing the task as an automaton and integrating it into learning and planning. We provide and prove conditions under which LOF will learn satisfying, optimal policies. And lastly, we show how LOF's learned policies can be composed to satisfy unseen tasks with only 10-50 retraining steps on our benchmarks. We evaluate LOF on four tasks in discrete and continuous domains, including a 3D pick-and-place environment.",
        "conference": "ICML",
        "中文标题": "逻辑选项框架",
        "摘要翻译": "学习适用于具有复杂规则和任务的环境的可组合策略是一个具有挑战性的问题。我们引入了一个名为逻辑选项框架（LOF）的分层强化学习框架，该框架学习满足、最优且可组合的策略。LOF通过将任务表示为自动机并将其集成到学习和规划中，有效地学习满足任务的策略。我们提供并证明了LOF将学习满足、最优策略的条件。最后，我们展示了LOF学习的策略如何组合以满足未见过的任务，在我们的基准测试中仅需10-50次重新训练步骤。我们在离散和连续领域的四个任务上评估了LOF，包括一个3D拾取和放置环境。",
        "领域": "分层强化学习、自动机理论、策略组合",
        "问题": "如何在具有复杂规则和任务的环境中学习可组合的策略",
        "动机": "为了解决在复杂环境中学习满足、最优且可组合策略的挑战",
        "方法": "引入逻辑选项框架（LOF），通过将任务表示为自动机并集成到学习和规划中，学习满足任务的策略",
        "关键词": [
            "分层强化学习",
            "逻辑选项框架",
            "策略组合",
            "自动机理论",
            "3D拾取和放置"
        ],
        "涉及的技术概念": {
            "分层强化学习": "用于在复杂环境中学习可组合策略的框架",
            "自动机理论": "用于表示任务并集成到学习和规划中",
            "策略组合": "允许将学习到的策略组合以满足新的未见任务"
        },
        "success": true
    },
    {
        "order": 1075,
        "title": "Theory of Spectral Method for Union of Subspaces-Based Random Geometry Graph",
        "html": "https://ICML.cc//virtual/2021/poster/8625",
        "abstract": "Spectral method is a commonly used scheme to cluster data points lying close to Union of Subspaces, a task known as Subspace Clustering. The typical usage is to construct a Random Geometry Graph first and then apply spectral method to the graph to obtain clustering result. The latter step has been coined the name Spectral Clustering. As far as we know, in spite of the significance of both steps in spectral-method-based Subspace Clustering, all existing theoretical results focus on the first step of constructing the graph, but ignore the final step to correct false connections through spectral clustering. This paper establishes a theory to show the power of this method for the first time, in which we demonstrate the mechanism of spectral clustering by analyzing a simplified algorithm under the widely used semi-random model. Based on this theory, we prove the efficiency of Subspace Clustering in fairly broad conditions. The insights and analysis techniques developed in this paper might also have implications for other random graph problems.",
        "conference": "ICML",
        "中文标题": "基于子空间联合的随机几何图谱方法理论",
        "摘要翻译": "谱方法是聚类靠近子空间联合的数据点的常用方案，这一任务被称为子空间聚类。典型的用法是首先构建一个随机几何图，然后对该图应用谱方法以获得聚类结果。后一步骤被称为谱聚类。据我们所知，尽管在基于谱方法的子空间聚类中这两个步骤都非常重要，但所有现有的理论结果都集中在构建图的第一步，而忽略了通过谱聚类纠正错误连接的最终步骤。本文首次建立了一个理论来展示这种方法的力量，其中我们通过分析广泛使用的半随机模型下的简化算法来展示谱聚类的机制。基于这一理论，我们证明了在相当广泛的条件下子空间聚类的效率。本文开发的见解和分析技术也可能对其他随机图问题产生影响。",
        "领域": "子空间聚类、谱聚类、随机几何图",
        "问题": "现有子空间聚类理论忽略了谱聚类步骤在纠正错误连接中的作用",
        "动机": "填补子空间聚类理论中关于谱聚类步骤作用的空白，证明其在广泛条件下的有效性",
        "方法": "通过分析半随机模型下的简化算法，建立理论展示谱聚类的机制，并证明子空间聚类的效率",
        "关键词": [
            "子空间聚类",
            "谱聚类",
            "随机几何图",
            "半随机模型",
            "谱方法"
        ],
        "涉及的技术概念": {
            "谱方法": "用于聚类靠近子空间联合的数据点的技术，通过构建和应用随机几何图来实现",
            "随机几何图": "在子空间聚类中构建的图结构，用于表示数据点之间的关系",
            "半随机模型": "广泛使用的模型，用于分析谱聚类的机制和效率"
        },
        "success": true
    },
    {
        "order": 1076,
        "title": "The Power of Adaptivity for Stochastic Submodular Cover",
        "html": "https://ICML.cc//virtual/2021/poster/9121",
        "abstract": "In the stochastic submodular cover problem, the goal is to select a subset of stochastic items of minimum expected cost to cover a submodular function. Solutions in this setting correspond to a sequential decision process that selects items one by one ``adaptively'' (depending on prior observations). While such adaptive solutions achieve the best objective, the inherently sequential nature makes them undesirable in many applications. We ask: \\emph{how well can solutions with only a few adaptive rounds approximate fully-adaptive solutions?} We consider both cases where the stochastic items are independent, and where they are correlated. For both  situations, we obtain nearly tight answers, establishing smooth tradeoffs between the number of adaptive rounds and the solution quality, relative to fully adaptive solutions. Experiments on synthetic and real datasets validate the practical performance of our algorithms, showing qualitative improvements in the solutions as we allow more rounds of adaptivity; in practice, solutions using just a few rounds of adaptivity are nearly as good as fully adaptive solutions.",
        "conference": "ICML",
        "success": true,
        "中文标题": "自适应能力在随机子模覆盖问题中的力量",
        "摘要翻译": "在随机子模覆盖问题中，目标是以最小的期望成本选择一组随机项来覆盖一个子模函数。此设置下的解决方案对应于一个顺序决策过程，该过程‘自适应地’（取决于先前的观察）一个接一个地选择项。虽然这种自适应解决方案实现了最佳目标，但其固有的顺序性质使得它们在许多应用中不受欢迎。我们提出：仅具有少量自适应轮次的解决方案能在多大程度上近似完全自适应的解决方案？我们考虑了随机项独立和相关两种情况。对于这两种情况，我们都获得了几乎紧密的答案，建立了自适应轮次数量与解决方案质量之间的平滑权衡，相对于完全自适应的解决方案。在合成和真实数据集上的实验验证了我们算法的实际性能，显示随着我们允许更多的自适应轮次，解决方案在质量上的定性改进；在实践中，仅使用几轮自适应性的解决方案几乎与完全自适应的解决方案一样好。",
        "领域": "组合优化, 随机算法, 机器学习",
        "问题": "如何在随机子模覆盖问题中，通过减少自适应轮次来近似完全自适应解决方案的性能。",
        "动机": "研究动机是为了解决自适应解决方案因其顺序性质而在实际应用中受限的问题，探索减少自适应轮次对解决方案性能的影响。",
        "方法": "通过理论分析和实验验证，研究了在随机项独立和相关两种情况下，自适应轮次数量与解决方案质量之间的关系。",
        "关键词": [
            "随机子模覆盖",
            "自适应算法",
            "组合优化",
            "随机项",
            "解决方案质量"
        ],
        "涉及的技术概念": {
            "随机子模覆盖": "在随机环境下，选择一组项以覆盖子模函数的问题，目标是成本最小化。",
            "自适应算法": "根据先前的观察结果动态调整决策过程的算法，以提高解决方案的质量。",
            "组合优化": "研究在有限集合上寻找最优子集的问题，广泛应用于算法设计和机器学习中。"
        }
    },
    {
        "order": 1077,
        "title": "The Power of Log-Sum-Exp: Sequential Density Ratio Matrix Estimation for Speed-Accuracy Optimization",
        "html": "https://ICML.cc//virtual/2021/poster/9873",
        "abstract": "We propose a model for multiclass classification of time series to make a prediction as early and as accurate as possible. The matrix sequential probability ratio test (MSPRT) is known to be asymptotically optimal for this setting, but contains a critical assumption that hinders broad real-world applications; the MSPRT requires the underlying probability density. To address this problem, we propose to solve density ratio matrix estimation (DRME), a novel type of density ratio estimation that consists of estimating matrices of multiple density ratios with constraints and thus is more challenging than the conventional density ratio estimation. We propose a log-sum-exp-type loss function (LSEL) for solving DRME and prove the following: (i) the LSEL provides the true density ratio matrix as the sample size of the training set increases (consistency); (ii) it assigns larger gradients to harder classes (hard class weighting effect); and (iii) it provides discriminative scores even on class-imbalanced datasets (guess-aversion). Our overall architecture for early classification, MSPRT-TANDEM, statistically significantly outperforms baseline models on four datasets including action recognition, especially in the early stage of sequential observations. Our code and datasets are publicly available.",
        "conference": "ICML",
        "中文标题": "对数-求和-指数的力量：用于速度-精度优化的序列密度比矩阵估计",
        "摘要翻译": "我们提出了一种时间序列多类分类模型，旨在尽可能早且准确地做出预测。矩阵序列概率比检验（MSPRT）已知在此设置下是渐进最优的，但包含一个阻碍广泛实际应用的关键假设；MSPRT需要基础概率密度。为了解决这个问题，我们提出解决密度比矩阵估计（DRME），这是一种新型的密度比估计，包括估计具有约束的多个密度比的矩阵，因此比传统的密度比估计更具挑战性。我们提出了一种对数-求和-指数型损失函数（LSEL）来解决DRME，并证明了以下内容：（i）随着训练集样本量的增加，LSEL提供了真实的密度比矩阵（一致性）；（ii）它为更难的类别分配更大的梯度（难类别加权效应）；（iii）即使在类别不平衡的数据集上，它也提供了区分性分数（猜测避免）。我们用于早期分类的总体架构MSPRT-TANDEM，在包括动作识别在内的四个数据集上统计显著优于基线模型，尤其是在序列观察的早期阶段。我们的代码和数据集是公开可用的。",
        "领域": "时间序列分类, 动作识别, 密度比估计",
        "问题": "解决在时间序列多类分类中，如何在保证预测准确性的同时尽可能早地进行预测的问题。",
        "动机": "克服矩阵序列概率比检验（MSPRT）在实际应用中需要基础概率密度的限制，提出一种更通用的密度比矩阵估计方法。",
        "方法": "提出密度比矩阵估计（DRME）和对数-求和-指数型损失函数（LSEL），用于解决多类分类中的早期和准确预测问题。",
        "关键词": [
            "密度比矩阵估计",
            "对数-求和-指数型损失函数",
            "时间序列分类",
            "动作识别",
            "早期预测"
        ],
        "涉及的技术概念": {
            "密度比矩阵估计（DRME）": "一种新型的密度比估计方法，涉及估计具有约束的多个密度比的矩阵，用于多类分类问题。",
            "对数-求和-指数型损失函数（LSEL）": "用于解决DRME的损失函数，具有一致性、难类别加权效应和猜测避免的特性。",
            "矩阵序列概率比检验（MSPRT）": "一种在时间序列分类中渐进最优的方法，但需要基础概率密度，限制了其实际应用。"
        },
        "success": true
    },
    {
        "order": 1078,
        "title": "The Symmetry between Arms and Knapsacks: A Primal-Dual Approach for Bandits with Knapsacks",
        "html": "https://ICML.cc//virtual/2021/poster/9151",
        "abstract": "In this paper, we study the bandits with knapsacks (BwK) problem and develop a primal-dual based algorithm that achieves a problem-dependent logarithmic regret bound. The BwK problem extends the multi-arm bandit (MAB) problem to model the resource consumption, and the existing BwK literature has been mainly focused on deriving asymptotically optimal distribution-free regret bounds. We first study the primal and dual linear programs underlying the BwK problem. From this primal-dual perspective, we discover symmetry between arms and knapsacks, and then propose a new notion of suboptimality measure for the BwK problem. The suboptimality measure highlights the important role of knapsacks in determining algorithm regret and inspires the design of our two-phase algorithm. In the first phase, the algorithm identifies the optimal arms and the binding knapsacks, and in the second phase, it exhausts the binding knapsacks via playing the optimal arms through an adaptive procedure. Our regret upper bound involves the proposed suboptimality measure and it has a logarithmic dependence on length of horizon $T$ and a polynomial dependence on $m$ (the numbers of arms) and $d$ (the number of knapsacks). To the best of our knowledge, this is the first problem-dependent logarithmic regret bound for solving the general BwK problem.",
        "conference": "ICML",
        "中文标题": "臂与背包之间的对称性：一种针对带背包的多臂老虎机问题的原始-对偶方法",
        "摘要翻译": "本文研究了带背包的多臂老虎机（BwK）问题，并提出了一种基于原始-对偶的算法，该算法实现了问题依赖的对数遗憾界。BwK问题将多臂老虎机（MAB）问题扩展到资源消耗的建模，现有的BwK文献主要集中在推导渐近最优的无分布遗憾界。我们首先研究了BwK问题背后的原始和对偶线性规划。从这种原始-对偶的视角，我们发现了臂与背包之间的对称性，并提出了一个新的BwK问题的次优性度量。这个次优性度量强调了背包在决定算法遗憾中的重要作用，并激发了我们两阶段算法的设计。在第一阶段，算法识别出最优臂和绑定背包；在第二阶段，通过一个自适应过程，算法通过播放最优臂来耗尽绑定背包。我们的遗憾上界涉及提出的次优性度量，并且它对时间范围T的长度有对数依赖，对臂的数量m和背包的数量d有多项式依赖。据我们所知，这是解决一般BwK问题的第一个问题依赖的对数遗憾界。",
        "领域": "多臂老虎机问题, 资源分配优化, 在线学习",
        "问题": "如何在资源有限的情况下，通过多臂老虎机模型有效地分配资源以最小化遗憾。",
        "动机": "现有的带背包的多臂老虎机（BwK）研究主要集中在无分布的遗憾界上，缺乏问题依赖的对数遗憾界的研究。",
        "方法": "提出了一种基于原始-对偶视角的两阶段算法，首先识别最优臂和绑定背包，然后通过自适应过程耗尽绑定背包。",
        "关键词": [
            "带背包的多臂老虎机",
            "原始-对偶方法",
            "对数遗憾界",
            "资源分配",
            "在线学习"
        ],
        "涉及的技术概念": {
            "原始-对偶线性规划": "用于分析BwK问题，发现臂与背包之间的对称性。",
            "次优性度量": "新提出的度量标准，用于评估算法的性能，强调背包在决定遗憾中的重要性。",
            "两阶段算法": "第一阶段识别最优臂和绑定背包，第二阶段通过自适应过程耗尽绑定背包，以实现有效的资源分配。"
        },
        "success": true
    },
    {
        "order": 1079,
        "title": "Think Global and Act Local: Bayesian Optimisation over High-Dimensional Categorical and Mixed Search Spaces",
        "html": "https://ICML.cc//virtual/2021/poster/8551",
        "abstract": "High-dimensional black-box optimisation remains an important yet notoriously challenging problem. Despite the success of Bayesian optimisation methods on continuous domains, domains that are categorical, or that mix continuous and categorical variables, remain challenging. We propose a novel solution---we combine local optimisation with a tailored kernel design, effectively handling high-dimensional categorical and mixed search spaces, whilst retaining sample efficiency. We further derive convergence guarantee for the proposed approach. Finally, we demonstrate empirically that our method outperforms the current baselines on a variety of synthetic and real-world tasks in terms of performance, computational costs, or both.",
        "conference": "ICML",
        "中文标题": "全局思考与局部行动：高维分类及混合搜索空间的贝叶斯优化",
        "摘要翻译": "高维黑盒优化仍然是一个重要但极具挑战性的问题。尽管贝叶斯优化方法在连续域上取得了成功，但在分类域或混合连续与分类变量的域上仍然面临挑战。我们提出了一种新颖的解决方案——我们将局部优化与定制的核设计相结合，有效处理高维分类及混合搜索空间，同时保持样本效率。我们进一步推导了所提出方法的收敛保证。最后，我们通过实验证明，我们的方法在性能、计算成本或两者上均优于当前基线，适用于各种合成和真实世界的任务。",
        "领域": "贝叶斯优化、高维优化、混合变量优化",
        "问题": "解决在高维分类及混合搜索空间中进行有效优化的问题",
        "动机": "针对高维黑盒优化中分类及混合变量域优化效率低下的挑战，提出更高效的解决方案",
        "方法": "结合局部优化与定制核设计的方法，有效处理高维分类及混合搜索空间",
        "关键词": [
            "贝叶斯优化",
            "高维优化",
            "混合搜索空间",
            "核设计",
            "局部优化"
        ],
        "涉及的技术概念": {
            "贝叶斯优化": "一种基于贝叶斯理论的优化方法，用于在有限的样本下高效寻找全局最优解",
            "核设计": "定制化的核函数设计，用于在高维空间中有效衡量样本间的相似性",
            "局部优化": "在全局优化框架内引入局部搜索策略，以提高在高维空间中的优化效率"
        },
        "success": true
    },
    {
        "order": 1080,
        "title": "Thinking Like Transformers",
        "html": "https://ICML.cc//virtual/2021/poster/9995",
        "abstract": "What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder---attention and feed-forward computation---into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.",
        "conference": "ICML",
        "success": true,
        "中文标题": "像Transformer一样思考",
        "摘要翻译": "Transformer背后的计算模型是什么？虽然循环神经网络与有限状态机有直接的相似之处，允许对架构变体或训练模型进行清晰的讨论和思考，但Transformer却没有这样熟悉的相似物。在本文中，我们旨在改变这一现状，提出了一种以编程语言形式存在的transformer-encoder计算模型。我们将transformer-encoder的基本组件——注意力和前馈计算——映射为简单的原语，围绕这些原语我们形成了一种编程语言：受限访问序列处理语言（RASP）。我们展示了如何使用RASP来编程解决可能由Transformer学习的任务，以及如何训练Transformer来模仿RASP解决方案。特别是，我们为直方图、排序和Dyck语言提供了RASP程序。我们进一步使用我们的模型来关联它们在所需层数和注意力头数方面的难度：分析RASP程序意味着编码任务到transformer中所需的最大头数和层数。最后，我们看到了从我们的抽象中获得的见解如何用于解释最近作品中观察到的现象。",
        "领域": "自然语言处理与视觉结合, 序列建模, 编程语言理论",
        "问题": "理解并形式化Transformer的计算模型",
        "动机": "为Transformer提供一个清晰的计算模型，以便更好地理解和设计其架构",
        "方法": "提出一种名为RASP的编程语言，将Transformer的基本操作映射为编程原语，并通过编程解决特定任务来展示其有效性",
        "关键词": [
            "Transformer",
            "RASP",
            "计算模型",
            "注意力机制",
            "序列处理"
        ],
        "涉及的技术概念": {
            "注意力机制": "用于在序列中动态地聚焦于相关信息部分的技术",
            "前馈计算": "在神经网络中，通过一系列线性变换和非线性激活函数处理输入数据的过程",
            "受限访问序列处理语言（RASP）": "一种专门设计的编程语言，用于模拟Transformer的计算过程，提供了一种形式化理解Transformer操作的方式"
        }
    },
    {
        "order": 1081,
        "title": "Three Operator Splitting with a Nonconvex Loss Function",
        "html": "https://ICML.cc//virtual/2021/poster/9695",
        "abstract": "We consider the problem of minimizing the sum of three functions, one of which is nonconvex but differentiable, and the other two are convex but possibly nondifferentiable. We investigate the Three Operator Splitting method (TOS) of Davis & Yin (2017) with an aim to extend its theoretical guarantees for this nonconvex problem template. In particular, we prove convergence of TOS with nonasymptotic bounds on its nonstationarity and infeasibility errors. In contrast with the existing work on nonconvex TOS, our guarantees do not require additional smoothness assumptions on the terms comprising the objective; hence they cover instances of particular interest where the nondifferentiable terms are indicator functions. We also extend our results to a stochastic setting where we have access only to an unbiased estimator of the gradient. Finally, we illustrate the effectiveness of the proposed method through numerical experiments on quadratic assignment problems. ",
        "conference": "ICML",
        "中文标题": "非凸损失函数下的三算子分裂方法",
        "摘要翻译": "我们考虑最小化三个函数之和的问题，其中一个是非凸但可微的，另外两个是凸但可能不可微的。我们研究了Davis & Yin（2017）提出的三算子分裂方法（TOS），旨在扩展其对于这一非凸问题模板的理论保证。特别是，我们证明了TOS的收敛性，并给出了其非平稳性和不可行性误差的非渐近界。与现有关于非凸TOS的研究相比，我们的保证不需要对构成目标的项附加光滑性假设；因此，它们涵盖了特别感兴趣的实例，其中不可微项是指示函数。我们还将结果扩展到了随机设置，其中我们只能访问梯度的无偏估计。最后，我们通过二次分配问题的数值实验说明了所提出方法的有效性。",
        "领域": "优化算法、非凸优化、随机优化",
        "问题": "在包含一个非凸但可微函数和两个凸但可能不可微函数的和的最小化问题中，扩展三算子分裂方法（TOS）的理论保证。",
        "动机": "扩展TOS方法以处理更广泛的非凸优化问题，特别是那些不可微项为指示函数的实例，同时不增加额外的光滑性假设。",
        "方法": "研究并证明了TOS方法在非凸设置下的收敛性，提供了非平稳性和不可行性误差的非渐近界，并将方法扩展到随机梯度设置。",
        "关键词": [
            "三算子分裂",
            "非凸优化",
            "随机优化",
            "收敛性分析",
            "数值实验"
        ],
        "涉及的技术概念": {
            "三算子分裂方法（TOS）": "一种用于解决包含多个函数的优化问题的算法，特别适用于目标函数中同时包含可微和不可微项的情况。",
            "非凸优化": "处理目标函数不是凸函数的优化问题，这类问题通常更复杂，因为可能存在多个局部最小值。",
            "随机优化": "在优化问题中，使用随机梯度或其他随机方法来近似计算梯度，适用于大规模数据集或无法精确计算梯度的情况。"
        },
        "success": true
    },
    {
        "order": 1082,
        "title": "Tight Bounds on the Smallest Eigenvalue of the Neural Tangent Kernel for Deep ReLU Networks",
        "html": "https://ICML.cc//virtual/2021/poster/8857",
        "abstract": "A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of $N$ neurons, $N$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.\n",
        "conference": "ICML",
        "中文标题": "深度ReLU网络神经切线核最小特征值的紧致界",
        "摘要翻译": "最近的一系列工作通过神经切线核（NTK）分析了深度神经网络的理论特性。特别是，NTK的最小特征值与记忆能力、梯度下降算法的全局收敛性以及深度网络的泛化能力相关联。然而，现有的结果要么提供了两层网络设置下的界，要么假设多层网络的NTK矩阵谱远离0。在本文中，我们为深度ReLU网络的NTK矩阵的最小特征值提供了紧致界，包括无限宽度和有限宽度的情况。在有限宽度设置下，我们考虑的网络架构相当一般：我们要求存在一个宽度大约为$N$个神经元的宽层，$N$为数据样本的数量；其余层宽度的缩放是任意的（最多对数因子）。为了获得我们的结果，我们分析了多个独立有趣的量：我们给出了隐藏特征矩阵最小奇异值的下界，以及输入-输出特征映射的Lipschitz常数的上界。",
        "领域": "深度学习理论、神经网络优化、机器学习理论",
        "问题": "为深度ReLU网络的神经切线核（NTK）矩阵的最小特征值提供紧致界",
        "动机": "现有研究在多层网络中对NTK矩阵谱的假设过于严格，或仅限于两层网络，缺乏对深度ReLU网络NTK最小特征值的紧致界分析",
        "方法": "分析深度ReLU网络在无限宽度和有限宽度情况下的NTK矩阵最小特征值，通过给出隐藏特征矩阵最小奇异值的下界和输入-输出特征映射的Lipschitz常数的上界来获得紧致界",
        "关键词": [
            "神经切线核",
            "深度ReLU网络",
            "最小特征值",
            "紧致界",
            "Lipschitz常数"
        ],
        "涉及的技术概念": {
            "神经切线核（NTK）": "用于分析深度神经网络训练动态的理论工具，本文中用于研究网络的记忆能力和泛化能力",
            "最小奇异值": "衡量矩阵可逆性的指标，本文中用于分析隐藏特征矩阵的性质",
            "Lipschitz常数": "描述函数输出变化对输入变化的敏感度，本文中用于限制输入-输出特征映射的变化范围"
        },
        "success": true
    },
    {
        "order": 1083,
        "title": "Tightening the Dependence on Horizon in the Sample Complexity of Q-Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8929",
        "abstract": "Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. Focusing on the synchronous setting (such that independent samples for all state-action pairs are queried via a generative model in each iteration), substantial progress has been made recently towards understanding the sample efficiency of Q-learning. To yield an entrywise $\\varepsilon$-accurate estimate of the optimal Q-function, state-of-the-art theory requires at least an order of $\\frac{|S||A|}{(1-\\gamma)^5\\varepsilon^{2}}$ samples in the infinite-horizon $\\gamma$-discounted setting. In this work, we sharpen the sample complexity of synchronous Q-learning to the order of $\\frac{|S||A|}{(1-\\gamma)^4\\varepsilon^2}$ (up to some logarithmic factor) for any $0<\\varepsilon <1$, leading to an order-wise improvement in $\\frac{1}{1-\\gamma}$. Analogous results are derived for finite-horizon MDPs as well. Notably, our sample complexity analysis unveils the effectiveness of vanilla Q-learning, which matches that of speedy Q-learning without requiring extra computation and storage. Our result is obtained by identifying novel error decompositions and recursion relations, which might shed light on how to  study other variants of Q-learning. ",
        "conference": "ICML",
        "success": true,
        "中文标题": "收紧Q学习样本复杂度对时间跨度的依赖",
        "摘要翻译": "Q学习以无模型的方式寻求学习马尔可夫决策过程（MDP）的最优Q函数，是强化学习的核心。聚焦于同步设置（例如，在每次迭代中通过生成模型查询所有状态-动作对的独立样本），近期在理解Q学习的样本效率方面取得了实质性进展。为了获得最优Q函数的逐项ε-准确估计，最先进的理论要求在无限时间跨度γ-折扣设置中至少需要$\\frac{|S||A|}{(1-\\gamma)^5\\varepsilon^{2}}$样本量级。在这项工作中，我们将同步Q学习的样本复杂度锐化到$\\frac{|S||A|}{(1-\\gamma)^4\\varepsilon^2}$量级（除了一些对数因子外），对于任何$0<\\varepsilon <1$，从而在$\\frac{1}{1-\\gamma}$上实现了量级上的改进。类似的结果也适用于有限时间跨度的MDP。值得注意的是，我们的样本复杂度分析揭示了普通Q学习的有效性，它与快速Q学习相匹配，而不需要额外的计算和存储。我们的结果是通过识别新的误差分解和递归关系获得的，这可能为如何研究Q学习的其他变体提供启示。",
        "领域": "强化学习, 马尔可夫决策过程, 样本复杂度分析",
        "问题": "减少Q学习在无限时间跨度γ-折扣设置中样本复杂度的量级依赖",
        "动机": "提高Q学习的样本效率，减少对时间跨度的依赖，以更少的样本量达到相同的学习效果",
        "方法": "通过识别新的误差分解和递归关系，锐化同步Q学习的样本复杂度",
        "关键词": [
            "Q学习",
            "样本复杂度",
            "马尔可夫决策过程",
            "强化学习",
            "误差分解"
        ],
        "涉及的技术概念": {
            "同步Q学习": "在每次迭代中通过生成模型查询所有状态-动作对的独立样本的Q学习方法",
            "样本复杂度": "衡量学习算法达到预定精度所需样本量的指标"
        }
    },
    {
        "order": 1084,
        "title": "Tighter Bounds on the Log Marginal Likelihood of Gaussian Process Regression Using Conjugate Gradients",
        "html": "https://ICML.cc//virtual/2021/poster/10067",
        "abstract": "We propose a lower bound on the log marginal likelihood of Gaussian process regression models that can be computed without matrix factorisation of the full kernel matrix. We show that approximate maximum likelihood learning of model parameters by maximising our lower bound retains many benefits of the sparse variational approach while reducing the bias introduced into hyperparameter learning. The basis of our bound is a more careful analysis of the log-determinant term appearing in the log marginal likelihood, as well as using the method of conjugate gradients to derive tight lower bounds on the term involving a quadratic form. Our approach is a step forward in unifying methods relying on lower bound maximisation (e.g. variational methods) and iterative approaches based on conjugate gradients for training Gaussian processes. In experiments, we show improved predictive performance with our model for a comparable amount of training time compared to other conjugate gradient based approaches.",
        "conference": "ICML",
        "中文标题": "使用共轭梯度法对高斯过程回归的对数边际似然进行更紧的界估计",
        "摘要翻译": "我们提出了一种无需对完整核矩阵进行矩阵分解即可计算的高斯过程回归模型对数边际似然的下界。我们表明，通过最大化我们的下界进行模型参数的近似最大似然学习，保留了稀疏变分方法的许多优点，同时减少了超参数学习中引入的偏差。我们界的基础是对出现在对数边际似然中的对数行列式项进行更仔细的分析，以及使用共轭梯度法来推导涉及二次形式的项的紧下界。我们的方法是在统一依赖于下界最大化的方法（例如变分方法）和基于共轭梯度的迭代方法用于训练高斯过程方面迈出的一步。在实验中，我们展示了与其他基于共轭梯度的方法相比，在相当的训练时间内，我们的模型具有改进的预测性能。",
        "领域": "高斯过程回归、机器学习优化、概率模型学习",
        "问题": "如何在不进行完整核矩阵分解的情况下，更准确地估计高斯过程回归的对数边际似然，并减少超参数学习中的偏差。",
        "动机": "为了结合稀疏变分方法和共轭梯度法的优点，减少超参数学习中的偏差，提高高斯过程回归模型的预测性能。",
        "方法": "通过更仔细地分析对数行列式项，并使用共轭梯度法推导涉及二次形式的项的紧下界，提出了一种新的下界估计方法。",
        "关键词": [
            "高斯过程回归",
            "对数边际似然",
            "共轭梯度法",
            "下界估计",
            "超参数学习"
        ],
        "涉及的技术概念": {
            "对数边际似然": "用于评估高斯过程回归模型拟合度的统计量，本文提出了一种新的下界估计方法。",
            "共轭梯度法": "一种迭代方法，用于优化二次形式项的估计，本文中用于推导紧下界。",
            "稀疏变分方法": "一种近似推断技术，本文的方法在保留其优点的同时减少了超参数学习中的偏差。"
        },
        "success": true
    },
    {
        "order": 1085,
        "title": "Tilting the playing field: Dynamical loss functions for machine learning",
        "html": "https://ICML.cc//virtual/2021/poster/9505",
        "abstract": "We show that learning can be improved by using loss functions that evolve cyclically during training to emphasize one class at a time. In underparameterized networks, such dynamical loss functions can lead to successful training for networks that fail to find deep minima of the standard cross-entropy loss. In overparameterized networks, dynamical loss functions can lead to better generalization. Improvement arises from the interplay of the changing loss landscape with the dynamics of the system as it evolves to minimize the loss. In particular, as the loss function oscillates, instabilities develop in the form of bifurcation cascades, which we study using the Hessian and Neural Tangent Kernel. Valleys in the landscape widen and deepen, and then narrow and rise as the loss landscape changes during a cycle. As the landscape narrows, the learning rate becomes too large and the network becomes unstable and bounces around the valley. This process ultimately pushes the system into deeper and wider regions of the loss landscape and is characterized by decreasing eigenvalues of the Hessian. This results in better regularized models with improved generalization performance.",
        "conference": "ICML",
        "中文标题": "倾斜竞技场：机器学习中的动态损失函数",
        "摘要翻译": "我们展示了通过使用在训练过程中周期性演变的损失函数，每次强调一个类别，可以改善学习效果。在参数不足的网络中，这样的动态损失函数可以使得那些无法找到标准交叉熵损失深度最小值的网络成功训练。在参数过多的网络中，动态损失函数可以带来更好的泛化能力。这种改进源于变化中的损失景观与系统动态之间的相互作用，系统动态旨在最小化损失。特别是，随着损失函数的振荡，不稳定性以分岔级联的形式发展，我们使用Hessian矩阵和神经切线核对此进行了研究。景观中的谷地在循环过程中随着损失景观的变化而变宽加深，然后变窄上升。当景观变窄时，学习率变得过大，网络变得不稳定并在谷地中弹跳。这一过程最终将系统推入损失景观中更深更广的区域，并以Hessian矩阵特征值的减小为特征。这导致了具有更好泛化性能的更好正则化模型。",
        "领域": "深度学习优化、神经网络训练、机器学习正则化",
        "问题": "如何通过动态调整损失函数来改善神经网络的训练效果和泛化能力",
        "动机": "探索动态损失函数在改善网络训练深度和泛化性能方面的潜力",
        "方法": "使用周期性变化的动态损失函数，研究其在参数不足和过多网络中的效果，并通过Hessian矩阵和神经切线核分析系统的动态行为",
        "关键词": [
            "动态损失函数",
            "神经网络训练",
            "泛化能力",
            "Hessian矩阵",
            "神经切线核"
        ],
        "涉及的技术概念": {
            "动态损失函数": "在训练过程中周期性变化的损失函数，用于改善学习效果和泛化能力",
            "Hessian矩阵": "用于分析损失景观的曲率和系统的稳定性",
            "神经切线核": "用于研究无限宽度神经网络的训练动态和泛化行为"
        },
        "success": true
    },
    {
        "order": 1086,
        "title": "To be Robust or to be Fair: Towards Fairness in Adversarial Training",
        "html": "https://ICML.cc//virtual/2021/poster/9585",
        "abstract": "Adversarial training algorithms have been proved to be reliable to improve machine learning models' robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ''automobile'' but only 65% and 17% on class ''cat''. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models' robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.",
        "conference": "ICML",
        "中文标题": "追求鲁棒还是公平：对抗训练中的公平性探讨",
        "摘要翻译": "对抗训练算法已被证明能够可靠地提高机器学习模型对抗对抗样本的鲁棒性。然而，我们发现对抗训练算法倾向于在不同数据组之间引入严重的准确性和鲁棒性差异。例如，在CIFAR-10上使用PGD对抗训练的ResNet18模型，在‘汽车’类别上具有93%的清洁准确率和67%的PGD l_infty-8对抗准确率，而在‘猫’类别上仅分别有65%和17%。这种现象在平衡数据集中发生，并且在仅使用清洁样本的自然训练模型中不存在。在这项工作中，我们通过实证和理论表明，这种现象在最小化DNN模型鲁棒误差的对抗训练算法下普遍存在。基于这些发现，我们提出了一个公平鲁棒学习（FRL）框架，以在对抗防御时缓解这种不公平问题，实验结果验证了FRL的有效性。",
        "领域": "对抗学习、公平机器学习、深度学习安全",
        "问题": "对抗训练算法在不同数据组间引入的准确性和鲁棒性差异问题",
        "动机": "解决对抗训练中不同类别数据间准确性和鲁棒性不公平的问题",
        "方法": "提出公平鲁棒学习（FRL）框架，以在对抗防御时缓解不公平问题",
        "关键词": [
            "对抗训练",
            "公平性",
            "鲁棒性",
            "深度学习安全",
            "机器学习公平"
        ],
        "涉及的技术概念": {
            "对抗训练": "一种提高模型对抗对抗样本鲁棒性的训练方法",
            "公平鲁棒学习（FRL）": "提出的框架，旨在解决对抗训练中的公平性问题",
            "鲁棒误差": "模型在对抗样本下的误差，对抗训练算法旨在最小化此误差"
        },
        "success": true
    },
    {
        "order": 1087,
        "title": "Top-k eXtreme Contextual Bandits with Arm Hierarchy",
        "html": "https://ICML.cc//virtual/2021/poster/10217",
        "abstract": "Motivated by modern applications, such as online advertisement and recommender systems, we study the top-$k$ extreme contextual bandits problem, where the total number of arms can be enormous, and the learner is allowed to select $k$ arms and observe all or some of the rewards for the chosen arms. We first propose an algorithm for the non-extreme realizable setting, utilizing the Inverse Gap Weighting strategy for selecting multiple arms. We show that our algorithm has a regret guarantee of $O(k\\sqrt{(A-k+1)T \\log (|F|T)})$, where $A$ is the total number of arms and $F$ is the class containing the regression function, while only requiring $\\tilde{O}(A)$ computation per time step. In the extreme setting, where the total number of arms can be in the millions, we propose a practically-motivated arm hierarchy model that induces a certain structure in mean rewards to ensure statistical and computational efficiency. The hierarchical structure allows for an exponential reduction in the number of relevant arms for each context, thus resulting in a regret guarantee of $O(k\\sqrt{(\\log A-k+1)T \\log (|F|T)})$. Finally, we implement our algorithm using a hierarchical linear function class and show superior performance with respect to well-known benchmarks on simulated bandit feedback experiments using extreme multi-label classification datasets. On a dataset with three million arms, our reduction scheme has an average inference time of only 7.9 milliseconds, which is a 100x improvement.",
        "conference": "ICML",
        "中文标题": "基于臂层次结构的Top-k极端上下文赌博机",
        "摘要翻译": "受现代应用（如在线广告和推荐系统）的启发，我们研究了top-k极端上下文赌博机问题，其中臂的总数可能非常庞大，且学习者被允许选择k个臂并观察所选臂的部分或全部奖励。我们首先为非极端可实现设置提出了一种算法，利用逆差距加权策略选择多个臂。我们展示了我们的算法具有O(k√((A-k+1)T log (|F|T)))的遗憾保证，其中A是臂的总数，F是包含回归函数的类，同时每个时间步仅需要O(A)的计算。在极端设置中，臂的总数可能达到数百万，我们提出了一种实用驱动的臂层次结构模型，该模型在平均奖励中诱导出某种结构以确保统计和计算效率。层次结构允许对每个上下文的相关臂数量进行指数级减少，从而得到O(k√((log A-k+1)T log (|F|T)))的遗憾保证。最后，我们使用层次线性函数类实现了我们的算法，并在使用极端多标签分类数据集模拟的赌博机反馈实验中展示了相对于知名基准的优越性能。在一个拥有三百万臂的数据集上，我们的减少方案平均推理时间仅为7.9毫秒，实现了100倍的改进。",
        "领域": "在线推荐系统、多臂赌博机问题、极端多标签分类",
        "问题": "在臂的总数极大的情况下，如何高效地选择top-k臂以最大化奖励。",
        "动机": "解决在线广告和推荐系统中面临的极端多臂选择问题，以提高推荐效率和用户满意度。",
        "方法": "提出了一种基于逆差距加权策略的算法和臂层次结构模型，以减少计算复杂度并保持统计效率。",
        "关键词": [
            "上下文赌博机",
            "臂层次结构",
            "逆差距加权",
            "极端多标签分类",
            "在线推荐系统"
        ],
        "涉及的技术概念": {
            "逆差距加权策略": "用于选择多个臂的策略，以减少遗憾并提高选择效率。",
            "臂层次结构模型": "通过构建臂的层次结构，减少每个上下文需要考虑的臂的数量，从而提高计算效率。",
            "极端多标签分类": "处理具有极多类别（臂）的分类问题，应用于大规模推荐和广告系统。"
        },
        "success": true
    },
    {
        "order": 1088,
        "title": "Toward Better Generalization Bounds with Locally Elastic Stability",
        "html": "https://ICML.cc//virtual/2021/poster/9565",
        "abstract": "Algorithmic stability is a key characteristic to ensure the generalization ability of a learning algorithm. Among different notions of stability, \\emph{uniform stability} is arguably the most popular one, which yields exponential generalization bounds. However, uniform stability only considers the worst-case loss change (or so-called sensitivity) by removing a single data point, which is distribution-independent and therefore undesirable. There are many cases that the worst-case sensitivity of the loss is much larger than the average sensitivity taken over the single data point that is removed, especially in some advanced models such as random feature models or neural networks. Many previous works try to mitigate the distribution independent issue by proposing weaker notions of stability, however, they either only yield polynomial bounds or the bounds derived do not vanish as sample size goes to infinity. Given that, we propose \\emph{locally elastic stability} as a weaker and distribution-dependent stability notion, which still yields exponential generalization bounds. We further demonstrate that locally elastic stability implies tighter generalization bounds than those derived based on uniform stability in many situations by revisiting the examples of bounded support vector machines, regularized least square regressions, and stochastic gradient descent.",
        "conference": "ICML",
        "success": true,
        "中文标题": "迈向更好的泛化界限与局部弹性稳定性",
        "摘要翻译": "算法稳定性是确保学习算法泛化能力的关键特性。在不同的稳定性概念中，均匀稳定性可以说是最受欢迎的一个，它提供了指数级的泛化界限。然而，均匀稳定性仅考虑通过移除单个数据点带来的最坏情况损失变化（或称为敏感性），这是与分布无关的，因此并不理想。在许多情况下，损失的最坏情况敏感性远大于移除单个数据点所取的平均敏感性，特别是在一些高级模型中，如随机特征模型或神经网络。许多先前的工作试图通过提出较弱的稳定性概念来缓解分布无关的问题，然而，它们要么只产生多项式界限，要么导出的界限在样本量趋于无穷大时不消失。鉴于此，我们提出了局部弹性稳定性作为一种较弱且依赖于分布的稳定性概念，它仍然能够产生指数级的泛化界限。我们进一步证明，在许多情况下，通过重新审视有界支持向量机、正则化最小二乘回归和随机梯度下降的例子，局部弹性稳定性意味着比基于均匀稳定性导出的更紧的泛化界限。",
        "领域": "机器学习理论, 算法稳定性分析, 泛化能力研究",
        "问题": "如何提出一种新的稳定性概念，以在保持指数级泛化界限的同时，克服均匀稳定性的分布无关性和敏感性过高的问题。",
        "动机": "均匀稳定性虽然提供了指数级的泛化界限，但其分布无关性和对最坏情况敏感性的依赖限制了其在高级模型中的应用效果。",
        "方法": "提出局部弹性稳定性作为一种新的稳定性概念，通过依赖于分布的敏感性分析，实现在保持指数级泛化界限的同时，提供更紧的泛化界限。",
        "关键词": [
            "算法稳定性",
            "泛化界限",
            "局部弹性稳定性",
            "机器学习理论",
            "敏感性分析"
        ],
        "涉及的技术概念": {
            "均匀稳定性": "一种稳定性概念，考虑移除单个数据点带来的最坏情况损失变化，提供指数级泛化界限，但与分布无关。",
            "局部弹性稳定性": "新提出的稳定性概念，依赖于分布的敏感性分析，旨在提供更紧的泛化界限，同时保持指数级界限。",
            "泛化能力": "学习算法在未见数据上的表现能力，是评估算法性能的关键指标。"
        }
    },
    {
        "order": 1089,
        "title": "Towards Better Laplacian Representation in Reinforcement Learning with Generalized Graph Drawing",
        "html": "https://ICML.cc//virtual/2021/poster/8967",
        "abstract": "The Laplacian representation recently gains increasing attention for reinforcement learning as it provides succinct and informative representation for states, by taking the eigenvectors of the Laplacian matrix of the state-transition graph as state embeddings. Such representation captures the geometry of the underlying state space and is beneficial to RL tasks such as option discovery and reward shaping. To approximate the Laplacian representation in large (or even continuous) state spaces, recent works propose to minimize a spectral graph drawing objective, which however has infinitely many global minimizers other than the eigenvectors. As a result, their learned Laplacian representation may differ from the ground truth. To solve this problem, we reformulate the graph drawing objective into a generalized form and derive a new learning objective, which is proved to have eigenvectors as its unique global minimizer. It enables learning high-quality Laplacian representations that faithfully approximate the ground truth. We validate this via comprehensive experiments on a set of gridworld and continuous control environments. Moreover, we show that our learned Laplacian representations lead to more exploratory options and better reward shaping.",
        "conference": "ICML",
        "中文标题": "迈向强化学习中更优的拉普拉斯表示：广义图绘制方法",
        "摘要翻译": "拉普拉斯表示最近在强化学习中获得了越来越多的关注，因为它通过将状态转移图的拉普拉斯矩阵的特征向量作为状态嵌入，为状态提供了简洁且信息丰富的表示。这种表示捕捉了底层状态空间的几何结构，对强化学习任务如选项发现和奖励塑造有益。为了在大（甚至连续）状态空间中近似拉普拉斯表示，最近的工作提出最小化一个谱图绘制目标，然而这个目标除了特征向量外还有无限多个全局最小化器。因此，它们学习的拉普拉斯表示可能与真实情况不同。为了解决这个问题，我们将图绘制目标重新表述为一个广义形式，并推导出一个新的学习目标，该目标被证明以特征向量为其唯一的全局最小化器。这使得能够学习高质量、忠实近似真实情况的拉普拉斯表示。我们通过在一系列网格世界和连续控制环境中的全面实验验证了这一点。此外，我们展示了我们学习的拉普拉斯表示导致了更具探索性的选项和更好的奖励塑造。",
        "领域": "强化学习、图表示学习、自动奖励塑造",
        "问题": "解决在强化学习中近似拉普拉斯表示时存在的无限多个全局最小化器问题，确保学习的表示能够忠实近似真实情况。",
        "动机": "为了提高强化学习任务（如选项发现和奖励塑造）中拉普拉斯表示的质量和准确性。",
        "方法": "通过将图绘制目标重新表述为广义形式，并推导出一个新的学习目标，确保特征向量为唯一的全局最小化器，从而学习高质量的拉普拉斯表示。",
        "关键词": [
            "拉普拉斯表示",
            "强化学习",
            "广义图绘制",
            "状态嵌入",
            "奖励塑造"
        ],
        "涉及的技术概念": {
            "拉普拉斯矩阵": "用于生成状态嵌入的矩阵，其特征向量能够捕捉状态空间的几何结构。",
            "谱图绘制目标": "用于近似拉普拉斯表示的目标函数，但存在无限多个全局最小化器的问题。",
            "广义图绘制": "重新表述的图绘制目标，确保特征向量为唯一的全局最小化器，从而提高拉普拉斯表示的质量。"
        },
        "success": true
    },
    {
        "order": 1090,
        "title": "Towards Better Robust Generalization with Shift Consistency Regularization",
        "html": "https://ICML.cc//virtual/2021/poster/10733",
        "abstract": "While adversarial training becomes one of the most promising defending approaches against adversarial attacks for deep neural networks, the conventional wisdom through robust optimization may usually not guarantee good generalization for robustness. Concerning with robust generalization over unseen adversarial data, this paper investigates adversarial training from a novel perspective of shift consistency in latent space. We argue that the poor robust generalization of adversarial training is owing to the significantly dispersed latent representations generated by training and test adversarial data, as the adversarial perturbations push the latent features of natural examples in the same class towards diverse directions. This is underpinned by the theoretical analysis of the robust generalization gap, which is upper-bounded by the standard one over the natural data and a term of feature inconsistent shift caused by adversarial perturbation – a measure of latent dispersion. Towards better robust generalization, we propose a new regularization method – shift consistency regularization (SCR) – to steer the same-class latent features of both natural and adversarial data into a common direction during adversarial training. The effectiveness of SCR in adversarial training is evaluated through extensive experiments over different datasets, such as CIFAR-10, CIFAR-100, and SVHN, against several competitive methods.",
        "conference": "ICML",
        "中文标题": "迈向更好的鲁棒泛化：通过移位一致性正则化",
        "摘要翻译": "尽管对抗训练已成为深度神经网络抵御对抗攻击最有希望的防御方法之一，但通过鲁棒优化的传统智慧通常不能保证良好的鲁棒泛化。针对未见对抗数据的鲁棒泛化问题，本文从潜在空间移位一致性的新视角研究了对抗训练。我们认为，对抗训练的鲁棒泛化能力差是由于训练和测试对抗数据生成的潜在表示显著分散，因为对抗扰动将同一类别自然样本的潜在特征推向不同方向。这一观点得到了鲁棒泛化差距理论分析的支持，该差距的上界是自然数据的标准差距和由对抗扰动引起的特征不一致移位项——一种潜在分散的度量。为了获得更好的鲁棒泛化，我们提出了一种新的正则化方法——移位一致性正则化（SCR）——在对抗训练过程中引导自然和对抗数据的同一类别潜在特征朝向共同方向。SCR在对抗训练中的有效性通过对不同数据集（如CIFAR-10、CIFAR-100和SVHN）对抗几种竞争方法的广泛实验进行了评估。",
        "领域": "对抗训练、鲁棒机器学习、深度学习安全",
        "问题": "提高深度神经网络对抗训练的鲁棒泛化能力",
        "动机": "对抗训练在对抗攻击防御中表现出色，但其鲁棒泛化能力不足，尤其是在面对未见过的对抗数据时。",
        "方法": "提出移位一致性正则化（SCR）方法，通过在对抗训练中引导自然和对抗数据的潜在特征朝向共同方向，减少潜在表示的分散。",
        "关键词": [
            "对抗训练",
            "鲁棒泛化",
            "移位一致性正则化",
            "潜在空间",
            "深度学习安全"
        ],
        "涉及的技术概念": {
            "对抗训练": "一种通过引入对抗样本来增强模型鲁棒性的训练方法，用于提高模型对对抗攻击的防御能力。",
            "鲁棒泛化": "指模型在面对未见过的对抗数据时，仍能保持良好的性能表现的能力。",
            "移位一致性正则化（SCR）": "一种新的正则化方法，旨在通过减少潜在特征的分散，提高对抗训练的鲁棒泛化能力。"
        },
        "success": true
    },
    {
        "order": 1091,
        "title": "Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons",
        "html": "https://ICML.cc//virtual/2021/poster/9827",
        "abstract": "It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small $\\ell_\\infty$-norm bounded adversarial perturbations. Although many attempts have been made, most previous works either can only provide empirical verification of the defense to a particular attack method, or can only develop a certified guarantee of the model robustness in limited scenarios. In this paper, we seek for a new approach to develop a theoretically principled neural network that inherently resists $\\ell_\\infty$ perturbations. In particular, we design a novel neuron that uses $\\ell_\\infty$-distance as its basic operation (which we call $\\ell_\\infty$-dist neuron), and show that any neural network constructed with $\\ell_\\infty$-dist neurons (called $\\ell_{\\infty}$-dist net) is naturally a 1-Lipschitz function with respect to $\\ell_\\infty$-norm. This directly provides a rigorous guarantee of the certified robustness based on the margin of prediction outputs. We then prove that such networks have enough expressive power to approximate any 1-Lipschitz function with robust generalization guarantee. We further provide a holistic training strategy that can greatly alleviate optimization difficulties. Experimental results show that using $\\ell_{\\infty}$-dist nets as basic building blocks, we consistently achieve state-of-the-art performance on commonly used datasets: 93.09\\% certified accuracy on MNIST ($\\epsilon=0.3$), 35.42\\% on CIFAR-10 ($\\epsilon=8/255$) and 16.31\\% on TinyImageNet ($\\epsilon=1/255$).",
        "conference": "ICML",
        "中文标题": "迈向使用L-inf距离神经元的神经网络认证L-inf鲁棒性",
        "摘要翻译": "众所周知，标准的神经网络即使具有很高的分类准确率，也容易受到小的ℓ∞范数有界对抗性扰动的影响。尽管已经进行了许多尝试，但大多数先前的工作要么只能提供对特定攻击方法的防御经验验证，要么只能在有限场景下开发模型鲁棒性的认证保证。在本文中，我们寻求一种新方法来开发一种理论上具有原则性的神经网络，该网络本质上抵抗ℓ∞扰动。特别是，我们设计了一种新颖的神经元，它使用ℓ∞距离作为其基本操作（我们称之为ℓ∞-距离神经元），并表明任何由ℓ∞-距离神经元构建的神经网络（称为ℓ∞-距离网络）自然地是关于ℓ∞范数的1-Lipschitz函数。这直接基于预测输出的边际提供了认证鲁棒性的严格保证。然后我们证明这样的网络具有足够的表达能力来近似任何具有鲁棒泛化保证的1-Lipschitz函数。我们进一步提供了一种全面的训练策略，可以大大缓解优化困难。实验结果表明，使用ℓ∞-距离网络作为基本构建块，我们在常用数据集上始终达到最先进的性能：在MNIST上达到93.09%的认证准确率（ε=0.3），在CIFAR-10上达到35.42%（ε=8/255），在TinyImageNet上达到16.31%（ε=1/255）。",
        "领域": "对抗性防御、鲁棒性认证、深度学习安全",
        "问题": "解决标准神经网络对ℓ∞范数有界对抗性扰动的脆弱性问题",
        "动机": "开发一种理论上具有原则性的神经网络，能够本质上抵抗ℓ∞扰动，并提供认证鲁棒性的严格保证",
        "方法": "设计使用ℓ∞-距离作为基本操作的神经元，构建ℓ∞-距离网络，证明其自然为1-Lipschitz函数，并提供全面的训练策略",
        "关键词": [
            "ℓ∞-距离神经元",
            "认证鲁棒性",
            "1-Lipschitz函数",
            "对抗性防御",
            "深度学习安全"
        ],
        "涉及的技术概念": {
            "ℓ∞-距离神经元": "使用ℓ∞距离作为基本操作的神经元，用于构建抵抗ℓ∞扰动的神经网络",
            "1-Lipschitz函数": "保证网络输出的变化不超过输入变化的函数，提供对抗性扰动的理论保证",
            "认证鲁棒性": "通过理论证明而非经验验证，确保模型对特定范围内所有对抗性攻击的抵抗能力"
        },
        "success": true
    },
    {
        "order": 1092,
        "title": "Towards Defending against Adversarial Examples via Attack-Invariant Features",
        "html": "https://ICML.cc//virtual/2021/poster/8629",
        "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. Their adversarial robustness can be improved by exploiting adversarial examples. However, given the continuously evolving attacks, models trained on seen types of adversarial examples generally cannot generalize well to unseen types of adversarial examples. To solve this problem, in this paper, we propose to remove adversarial noise by learning generalizable invariant features across attacks which maintain semantic classification information. Specifically, we introduce an adversarial feature learning mechanism to disentangle invariant features from adversarial noise. A normalization term has been proposed in the encoded space of the attack-invariant features to address the bias issue between the seen and unseen types of attacks. Empirical evaluations demonstrate that our method could provide better protection in comparison to previous state-of-the-art approaches, especially against unseen types of attacks and adaptive attacks.",
        "conference": "ICML",
        "中文标题": "通过攻击不变特征防御对抗样本",
        "摘要翻译": "深度神经网络（DNNs）容易受到对抗性噪声的影响。通过利用对抗样本可以提高其对抗鲁棒性。然而，考虑到攻击手段的不断进化，基于已知类型对抗样本训练的模型通常无法很好地泛化到未知类型的对抗样本。为了解决这个问题，本文提出通过学习跨攻击的泛化不变特征来去除对抗性噪声，这些特征保持了语义分类信息。具体来说，我们引入了一种对抗性特征学习机制，将不变特征与对抗性噪声分离。在攻击不变特征的编码空间中提出了一个归一化项，以解决已知和未知类型攻击之间的偏差问题。实证评估表明，与之前的最先进方法相比，我们的方法能够提供更好的保护，特别是针对未知类型的攻击和自适应攻击。",
        "领域": "对抗性防御、深度学习安全、图像分类",
        "问题": "深度神经网络在面对不断进化的对抗攻击时，如何提高其对未知类型攻击的泛化能力。",
        "动机": "解决深度神经网络在面对未知类型对抗攻击时泛化能力不足的问题，提高模型的对抗鲁棒性。",
        "方法": "提出了一种对抗性特征学习机制，通过学习跨攻击的泛化不变特征来去除对抗性噪声，并在编码空间中引入归一化项以减少偏差。",
        "关键词": [
            "对抗性防御",
            "不变特征学习",
            "深度神经网络安全",
            "对抗样本",
            "自适应攻击"
        ],
        "涉及的技术概念": {
            "对抗性特征学习机制": "用于从对抗性噪声中分离出不变特征，保持语义分类信息的技术。",
            "归一化项": "在攻击不变特征的编码空间中引入，以减少已知和未知类型攻击之间的偏差。",
            "对抗鲁棒性": "指深度神经网络抵抗对抗性攻击的能力，本文旨在通过不变特征学习提高这一能力。"
        },
        "success": true
    },
    {
        "order": 1093,
        "title": "Towards Distraction-Robust Active Visual Tracking",
        "html": "https://ICML.cc//virtual/2021/poster/8903",
        "abstract": "In active visual tracking, it is notoriously difficult when distracting objects appear, as distractors often mislead the tracker by occluding the target or bringing a confusing appearance. To address this issue, we propose a mixed cooperative-competitive multi-agent game, where a target and multiple distractors form a collaborative team to play against a tracker and make it fail to follow. Through learning in our game, diverse distracting behaviors of the distractors naturally emerge, thereby exposing the tracker's weakness, which helps enhance the distraction-robustness of the tracker. For effective learning, we then present a bunch of practical methods, including a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention mechanism for the tracker. The experimental results show that our tracker performs desired distraction-robust active visual tracking and can be well generalized to unseen environments. We also show that the multi-agent game can be used to adversarially test the robustness of trackers.",
        "conference": "ICML",
        "中文标题": "迈向抗干扰的主动视觉追踪",
        "摘要翻译": "在主动视觉追踪中，当干扰物体出现时，追踪变得异常困难，因为干扰物常常通过遮挡目标或带来混淆的外观误导追踪器。为了解决这个问题，我们提出了一种混合合作-竞争的多智能体游戏，其中目标和多个干扰物形成一个协作团队对抗追踪器，使其无法成功追踪。通过在我们的游戏中学习，干扰物的多样化干扰行为自然涌现，从而暴露出追踪器的弱点，这有助于增强追踪器的抗干扰能力。为了有效学习，我们随后提出了一系列实用方法，包括为干扰物设计的奖励函数、跨模态师生学习策略以及追踪器的循环注意力机制。实验结果表明，我们的追踪器实现了理想的抗干扰主动视觉追踪，并且能够很好地泛化到未见过的环境中。我们还展示了多智能体游戏可以用于对抗性测试追踪器的鲁棒性。",
        "领域": "主动视觉追踪、多智能体系统、抗干扰学习",
        "问题": "解决主动视觉追踪中干扰物体导致的追踪失败问题",
        "动机": "增强视觉追踪器在干扰物出现时的鲁棒性和追踪准确性",
        "方法": "提出混合合作-竞争的多智能体游戏框架，结合奖励函数、跨模态师生学习策略和循环注意力机制",
        "关键词": [
            "主动视觉追踪",
            "多智能体游戏",
            "抗干扰学习",
            "循环注意力机制",
            "跨模态学习"
        ],
        "涉及的技术概念": {
            "混合合作-竞争多智能体游戏": "通过目标和干扰物协作对抗追踪器，模拟真实干扰场景，增强追踪器的抗干扰能力",
            "跨模态师生学习策略": "利用不同模态间的知识传递，提升追踪器对干扰物的识别和适应能力",
            "循环注意力机制": "帮助追踪器在连续帧中保持对目标的关注，减少干扰物的影响"
        },
        "success": true
    },
    {
        "order": 1094,
        "title": "Towards Domain-Agnostic Contrastive Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10693",
        "abstract": "Despite recent successes, most contrastive self-supervised learning methods  are domain-specific, relying heavily on data augmentation techniques that require knowledge about a particular domain, such as image cropping and rotation.\nTo overcome such limitation, we propose a domain-agnostic approach to contrastive learning, named DACL, that is applicable to problems where  domain-specific data augmentations are not readily available. Key to our approach is the use of Mixup noise to create similar and dissimilar examples by mixing data samples differently either at the input or hidden-state levels. We theoretically analyze our method and show advantages over the Gaussian-noise based contrastive learning approach. To demonstrate the effectiveness of DACL, we conduct experiments across various domains such as tabular data, images, and graphs. Our results show that DACL not only outperforms other domain-agnostic noising methods, such as Gaussian-noise, but also combines well with domain-specific methods, such as SimCLR, to improve self-supervised visual representation learning.",
        "conference": "ICML",
        "中文标题": "迈向领域无关的对比学习",
        "摘要翻译": "尽管最近取得了一些成功，但大多数对比自监督学习方法都是领域特定的，严重依赖于需要特定领域知识的数据增强技术，如图像裁剪和旋转。为了克服这一限制，我们提出了一种领域无关的对比学习方法，名为DACL，适用于那些领域特定数据增强不易获得的问题。我们方法的关键在于使用Mixup噪声，通过在输入或隐藏状态层面以不同方式混合数据样本来创建相似和不相似的例子。我们从理论上分析了我们的方法，并展示了其优于基于高斯噪声的对比学习方法的优势。为了证明DACL的有效性，我们在表格数据、图像和图等多种领域进行了实验。我们的结果表明，DACL不仅优于其他领域无关的噪声方法，如高斯噪声，而且还能与领域特定方法（如SimCLR）结合，以改进自监督视觉表示学习。",
        "领域": "自监督学习、对比学习、多领域学习",
        "问题": "解决领域特定数据增强技术依赖性强，难以应用于领域特定数据增强不易获得的问题",
        "动机": "开发一种不依赖于特定领域知识的数据增强技术，能够广泛应用于多种领域的对比学习方法",
        "方法": "提出了一种名为DACL的领域无关对比学习方法，利用Mixup噪声在输入或隐藏状态层面混合数据样本，创建相似和不相似的例子",
        "关键词": [
            "领域无关学习",
            "对比学习",
            "Mixup噪声",
            "自监督学习",
            "多领域应用"
        ],
        "涉及的技术概念": {
            "对比学习": "一种自监督学习方法，通过比较数据样本间的相似性和差异性来学习表示",
            "Mixup噪声": "通过在输入或隐藏状态层面混合数据样本来创建新的训练样本，用于增强模型的泛化能力",
            "自监督学习": "一种无需人工标注数据的学习范式，通过数据本身的结构或特性来指导学习过程"
        },
        "success": true
    },
    {
        "order": 1095,
        "title": "Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10713",
        "abstract": "Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with teammates without prior coordination mechanisms, including joint training. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents with different fixed policies to enter and leave the environment without prior notification. Our solution builds on graph neural networks to learn agent models and joint-action value models under varying team compositions. We contribute a novel action-value computation that integrates the agent model and joint-action value model to produce action-value estimates. We empirically demonstrate that our approach successfully models the effects other agents have on the learner, leading to policies that robustly adapt to dynamic team compositions and significantly outperform several alternative methods.",
        "conference": "ICML",
        "中文标题": "基于图策略学习的开放临时团队协作研究",
        "摘要翻译": "临时团队协作是一个具有挑战性的问题，旨在设计一个能够快速适应与未经预先协调机制（包括联合训练）的队友协作的自主代理。该领域的先前工作主要集中在固定数量代理的封闭团队上。在这项工作中，我们通过允许具有不同固定策略的代理在未经事先通知的情况下进入和离开环境来考虑开放团队。我们的解决方案建立在图神经网络上，以学习不同团队组成下的代理模型和联合行动价值模型。我们贡献了一种新颖的行动价值计算，它整合了代理模型和联合行动价值模型以产生行动价值估计。我们通过实验证明，我们的方法成功地模拟了其他代理对学习者的影响，从而产生了能够稳健适应动态团队组成并显著优于几种替代方法的策略。",
        "领域": "多智能体系统、图神经网络、强化学习",
        "问题": "设计一个能够在开放环境中快速适应并与未经预先协调的队友协作的自主代理。",
        "动机": "解决开放团队中代理的动态加入和退出带来的协作挑战，提高团队协作的灵活性和效率。",
        "方法": "利用图神经网络学习不同团队组成下的代理模型和联合行动价值模型，提出一种整合这两种模型的新颖行动价值计算方法。",
        "关键词": [
            "开放临时团队协作",
            "图神经网络",
            "联合行动价值模型",
            "多智能体系统",
            "强化学习"
        ],
        "涉及的技术概念": {
            "图神经网络": "用于学习代理模型和联合行动价值模型，以处理动态变化的团队组成。",
            "联合行动价值模型": "评估在特定团队组成下采取联合行动的价值，指导代理的决策过程。",
            "行动价值计算": "整合代理模型和联合行动价值模型，为代理提供适应动态团队组成的行动建议。"
        },
        "success": true
    },
    {
        "order": 1096,
        "title": "Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach",
        "html": "https://ICML.cc//virtual/2021/poster/8799",
        "abstract": "Recommendation models can effectively estimate underlying user interests and predict one's future behaviors by factorizing an observed user-item rating matrix into products of two sets of latent factors. However, the user-specific embedding factors can only be learned in a transductive way, making it difficult to handle new users on-the-fly. In this paper, we propose an inductive collaborative filtering framework that contains two representation models. The first model follows conventional matrix factorization which factorizes a group of key users' rating matrix to obtain meta latents. The second model resorts to attention-based structure learning that estimates hidden relations from query to key users and learns to leverage meta latents to inductively compute embeddings for query users via neural message passing. Our model enables inductive representation learning for users and meanwhile guarantees equivalent representation capacity as matrix factorization. Experiments demonstrate that our model achieves promising results for recommendation on few-shot users with limited training ratings and new unseen users which are commonly encountered in open-world recommender systems.",
        "conference": "ICML",
        "中文标题": "迈向开放世界推荐：一种基于归纳模型的协同过滤方法",
        "摘要翻译": "推荐模型通过将观察到的用户-物品评分矩阵分解为两组潜在因子的乘积，能够有效估计潜在用户兴趣并预测其未来行为。然而，用户特定的嵌入因子只能以传导方式学习，难以即时处理新用户。本文提出了一种包含两种表示模型的归纳协同过滤框架。第一个模型遵循传统的矩阵分解方法，分解一组关键用户的评分矩阵以获得元潜在因子。第二个模型采用基于注意力的结构学习，估计从查询用户到关键用户的隐藏关系，并通过神经消息传递学习利用元潜在因子归纳计算查询用户的嵌入。我们的模型实现了用户的归纳表示学习，同时保证了与矩阵分解等效的表示能力。实验表明，我们的模型在训练评分有限的少样本用户和开放世界推荐系统中常见的新未见用户推荐上取得了有希望的结果。",
        "领域": "推荐系统、协同过滤、开放世界学习",
        "问题": "解决推荐系统中新用户即时处理和少样本用户推荐的问题",
        "动机": "传统的推荐系统难以即时处理新用户，且对少样本用户的推荐效果有限，需要一种能够即时适应新用户并有效处理少样本情况的推荐方法",
        "方法": "提出一种包含矩阵分解和基于注意力的结构学习的归纳协同过滤框架，通过神经消息传递归纳计算新用户的嵌入",
        "关键词": [
            "开放世界推荐",
            "归纳协同过滤",
            "神经消息传递",
            "少样本学习",
            "注意力机制"
        ],
        "涉及的技术概念": {
            "矩阵分解": "用于分解关键用户的评分矩阵以获得元潜在因子，作为归纳学习的基础",
            "注意力机制": "用于估计从查询用户到关键用户的隐藏关系，帮助模型聚焦于最相关的信息",
            "神经消息传递": "用于在归纳学习过程中传递和聚合信息，实现新用户嵌入的即时计算"
        },
        "success": true
    },
    {
        "order": 1097,
        "title": "Towards Practical Mean Bounds for Small Samples",
        "html": "https://ICML.cc//virtual/2021/poster/8979",
        "abstract": "Historically, to bound the mean for small sample sizes, practitioners have had to choose between using methods with unrealistic assumptions about the unknown distribution (e.g., Gaussianity) and methods like Hoeffding's inequality that use weaker assumptions but produce much looser (wider) intervals. In 1969, \\citet{Anderson1969}  proposed a mean confidence interval strictly better than or equal to Hoeffding's whose only assumption is that the distribution's support is contained in an interval $[a,b]$. For the first time since then, we present a new family of bounds that compares favorably to Anderson's. We prove that each bound in the family has {\\em guaranteed coverage}, i.e., it holds with probability at least $1-\\alpha$ for all distributions on an interval $[a,b]$. Furthermore, one of the bounds is tighter than or equal to Anderson's for all samples. In simulations, we show that for many distributions, the gain over Anderson's bound is substantial.",
        "conference": "ICML",
        "中文标题": "面向小样本实用的均值界限研究",
        "摘要翻译": "历史上，为了对小样本量的均值进行界限估计，实践者不得不在使用对未知分布做出不现实假设（如高斯性）的方法与使用较弱假设但产生更宽松（更宽）区间的方法（如Hoeffding不等式）之间做出选择。1969年，Anderson等人提出了一种均值置信区间，该区间严格优于或等于Hoeffding的，其唯一假设是分布的支撑包含在区间[a,b]内。自那时以来，我们首次提出了一个新的界限家族，与Anderson的相比表现更优。我们证明了该家族中的每个界限都具有保证的覆盖率，即对于区间[a,b]上的所有分布，它至少以1-α的概率成立。此外，对于所有样本，其中一个界限比Anderson的更紧或相等。在模拟中，我们显示对于许多分布，相对于Anderson界限的增益是显著的。",
        "领域": "统计学习理论、小样本统计、置信区间估计",
        "问题": "如何在小样本情况下，更紧且具有保证覆盖率地估计均值界限",
        "动机": "解决传统方法在小样本均值估计中要么假设过强、要么界限过松的问题",
        "方法": "提出一个新的界限家族，证明其具有保证覆盖率，并在模拟中展示其相对于现有方法的优势",
        "关键词": [
            "小样本统计",
            "均值界限",
            "置信区间",
            "统计学习理论",
            "覆盖率保证"
        ],
        "涉及的技术概念": {
            "Hoeffding不等式": "一种基于较弱假设的均值界限估计方法，但产生的区间较宽",
            "保证覆盖率": "新提出的界限家族确保对于所有符合条件的分布，界限至少以预定概率成立",
            "置信区间": "用于估计统计参数（如均值）可能范围的区间，新方法在此框架下提供了更紧的估计"
        },
        "success": true
    },
    {
        "order": 1098,
        "title": "Towards Rigorous Interpretations: a Formalisation of Feature Attribution",
        "html": "https://ICML.cc//virtual/2021/poster/10687",
        "abstract": "Feature attribution is often loosely presented as the process of selecting a subset of relevant features as a rationale of a prediction. Task-dependent by nature, precise definitions of 'relevance' encountered in the literature are however not always consistent. This lack of clarity stems from the fact that we usually do not have access to any notion of ground-truth attribution and from a more general debate on what good interpretations are. In this paper we propose to formalise feature selection/attribution based on the concept of relaxed functional dependence. In particular, we extend our notions to the instance-wise setting and derive necessary properties for candidate selection solutions, while leaving room for task-dependence. By computing ground-truth attributions on synthetic datasets, we evaluate many state-of-the-art attribution methods and show that, even when optimised, some fail to verify the proposed  properties and provide wrong solutions.",
        "conference": "ICML",
        "中文标题": "迈向严谨的解释：特征归因的形式化",
        "摘要翻译": "特征归因常常被宽松地描述为选择相关特征子集作为预测理由的过程。由于任务依赖性，文献中遇到的‘相关性’的精确定义并不总是一致的。这种不清晰源于我们通常无法接触到任何真实归因的概念，以及关于什么是好的解释的更广泛辩论。在本文中，我们提出基于松弛功能依赖概念的形式化特征选择/归因。特别是，我们将我们的概念扩展到实例级设置，并为候选选择解决方案推导出必要的属性，同时为任务依赖性留出空间。通过在合成数据集上计算真实归因，我们评估了许多最先进的归因方法，并表明，即使经过优化，一些方法也无法验证所提出的属性并提供错误的解决方案。",
        "领域": "可解释人工智能、特征选择、模型解释",
        "问题": "如何形式化特征归因以提供一致且可靠的解释",
        "动机": "解决特征归因定义不一致和缺乏清晰度的问题，以促进更可靠和可解释的机器学习模型",
        "方法": "基于松弛功能依赖概念的形式化特征选择/归因方法，扩展到实例级设置，并评估现有方法的性能",
        "关键词": [
            "特征归因",
            "形式化",
            "松弛功能依赖",
            "模型解释",
            "可解释人工智能"
        ],
        "涉及的技术概念": {
            "松弛功能依赖": "用于形式化特征选择/归因的核心概念，允许在保持任务依赖性的同时提供一致的定义",
            "实例级设置": "将特征归因的概念扩展到单个实例，允许更细致和具体的解释",
            "真实归因": "在合成数据集上计算，用于评估和验证归因方法的准确性和可靠性"
        },
        "success": true
    },
    {
        "order": 1099,
        "title": "Towards the Unification and Robustness of Perturbation and Gradient Based Explanations",
        "html": "https://ICML.cc//virtual/2021/poster/9991",
        "abstract": "As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a post hoc manner. In this work, we analyze two popular post hoc interpretation techniques: SmoothGrad which is a gradient based method, and a variant of LIME which is a perturbation based method. More specifically, we derive explicit closed form expressions for the explanations output by these two methods and show that they both converge to the same explanation in expectation, i.e., when the number of perturbed samples used by these methods is large. We then leverage this connection to establish other desirable properties, such as robustness, for these techniques. We also derive finite sample complexity bounds for the number of perturbations required for these methods to converge to their expected explanation. Finally, we empirically validate our theory using extensive experimentation on both synthetic and real-world datasets.",
        "conference": "ICML",
        "中文标题": "朝向扰动与梯度解释的统一与鲁棒性",
        "摘要翻译": "随着机器学习黑箱在医疗保健和刑事司法等关键领域的部署日益增多，人们越来越重视开发事后解释这些黑箱的技术。在这项工作中，我们分析了两种流行的事后解释技术：基于梯度的SmoothGrad方法和基于扰动的LIME方法的变体。更具体地说，我们为这两种方法输出的解释推导出了明确的闭式表达式，并表明它们在期望中收敛到相同的解释，即当这些方法使用的扰动样本数量很大时。然后，我们利用这种联系为这些技术建立了其他理想的属性，如鲁棒性。我们还推导了这些方法收敛到其预期解释所需的扰动数量的有限样本复杂度界限。最后，我们通过在合成和真实世界数据集上的广泛实验，对我们的理论进行了实证验证。",
        "领域": "可解释人工智能、机器学习模型解释、深度学习解释方法",
        "问题": "解决机器学习黑箱模型在关键领域应用中的解释性问题，特别是基于梯度和基于扰动的解释方法的统一性和鲁棒性问题。",
        "动机": "随着机器学习模型在关键决策领域的广泛应用，提高模型的可解释性和鲁棒性成为迫切需求，以增强用户对模型决策的信任和理解。",
        "方法": "通过理论分析推导基于梯度的SmoothGrad和基于扰动的LIME变体解释方法的闭式表达式，证明它们在大量扰动样本下收敛于同一解释，并利用这一联系建立方法的鲁棒性等属性，同时推导收敛所需的扰动样本数量的复杂度界限。",
        "关键词": [
            "可解释性",
            "SmoothGrad",
            "LIME",
            "鲁棒性",
            "样本复杂度"
        ],
        "涉及的技术概念": {
            "SmoothGrad": "一种基于梯度的解释方法，通过向输入添加噪声并计算梯度的平均值来平滑解释，减少噪声影响。",
            "LIME": "一种基于扰动的解释方法，通过在输入数据的局部邻域内训练可解释模型来近似黑箱模型的行为。",
            "样本复杂度": "指算法或方法达到预期性能所需的最小样本数量，本文中用于量化解释方法收敛到期望解释所需的扰动样本数量。"
        },
        "success": true
    },
    {
        "order": 1100,
        "title": "Towards Tight Bounds on the Sample Complexity of Average-reward MDPs",
        "html": "https://ICML.cc//virtual/2021/poster/9781",
        "abstract": "We prove new upper and lower bounds for sample complexity of finding an $\\epsilon$-optimal policy of an infinite-horizon average-reward Markov decision process (MDP) given access to a generative model. When the mixing time of the probability transition matrix of all policies is at most $t_\\mathrm{mix}$, we provide an algorithm that solves the problem using $\\widetilde{O}(t_\\mathrm{mix} \\epsilon^{-3})$ (oblivious) samples per state-action pair. Further, we provide a lower bound showing that a linear dependence on $t_\\mathrm{mix}$ is necessary in the worst case for any algorithm which computes oblivious samples. We obtain our results by establishing connections between infinite-horizon average-reward MDPs and discounted MDPs of possible further utility.",
        "conference": "ICML",
        "success": true,
        "中文标题": "关于平均奖励MDPs样本复杂度紧密界限的研究",
        "摘要翻译": "我们证明了在给定生成模型访问权限的情况下，寻找无限时间范围平均奖励马尔可夫决策过程（MDP）的ε-最优策略的样本复杂度的新上限和下限。当所有策略的概率转移矩阵的混合时间最多为t_mix时，我们提供了一个算法，该算法使用每个状态-动作对的O~(t_mix ε^-3)（无意识）样本来解决问题。此外，我们提供了一个下限，表明对于任何计算无意识样本的算法，在最坏情况下对t_mix的线性依赖是必要的。我们通过建立无限时间范围平均奖励MDPs与可能进一步使用的折扣MDPs之间的联系来获得我们的结果。",
        "领域": "强化学习、马尔可夫决策过程、样本复杂度分析",
        "问题": "寻找无限时间范围平均奖励马尔可夫决策过程的ε-最优策略的样本复杂度界限",
        "动机": "为了更有效地理解和优化在无限时间范围内平均奖励马尔可夫决策过程中的策略寻找过程，需要明确样本复杂度的上下界限。",
        "方法": "通过建立无限时间范围平均奖励MDPs与折扣MDPs之间的联系，提出算法并证明样本复杂度的上下界限。",
        "关键词": [
            "平均奖励MDPs",
            "样本复杂度",
            "生成模型",
            "混合时间",
            "ε-最优策略"
        ],
        "涉及的技术概念": {
            "马尔可夫决策过程（MDP）": "用于建模决策问题的数学框架，其中结果部分由决策者控制，部分随机。",
            "样本复杂度": "指算法为了达到一定的性能水平所需的样本数量。",
            "混合时间": "指马尔可夫链接近其稳态分布所需的时间，影响算法的效率和性能。"
        }
    },
    {
        "order": 1101,
        "title": "Towards Understanding and Mitigating Social Biases in Language Models",
        "html": "https://ICML.cc//virtual/2021/poster/9611",
        "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
        "conference": "ICML",
        "中文标题": "理解和缓解语言模型中的社会偏见",
        "摘要翻译": "随着机器学习方法在医疗保健、法律系统和社会科学等现实世界环境中的部署，认识到它们如何在这些敏感的决策过程中塑造社会偏见和刻板印象至关重要。在这些现实世界的部署中，大规模预训练语言模型（LMs）可能在表现不良的代表性偏见方面具有潜在的危险性——这些有害的偏见源于刻板印象，传播涉及性别、种族、宗教和其他社会结构的负面概括。作为提高LMs公平性的一步，我们在提出新的基准和度量标准来衡量它们之前，仔细定义了几种代表性偏见的来源。利用这些工具，我们提出了在文本生成过程中缓解社会偏见的步骤。我们的实证结果和人类评估表明，在保留高保真文本生成的关键上下文信息的同时，有效地缓解了偏见，从而推动了性能-公平性帕累托前沿。",
        "领域": "自然语言处理与公平性、社会计算、机器学习伦理",
        "问题": "识别和缓解大规模预训练语言模型中的社会偏见和刻板印象",
        "动机": "提高语言模型在敏感决策过程中的公平性，防止传播负面社会刻板印象",
        "方法": "定义代表性偏见的来源，提出新的基准和度量标准，实施文本生成过程中的偏见缓解步骤",
        "关键词": [
            "社会偏见",
            "语言模型",
            "公平性",
            "文本生成",
            "机器学习伦理"
        ],
        "涉及的技术概念": {
            "代表性偏见": "指语言模型在文本生成过程中反映出的社会刻板印象和偏见",
            "帕累托前沿": "在优化语言模型性能与公平性之间找到最佳平衡点的概念",
            "预训练语言模型": "指在大规模文本数据上预先训练的模型，用于理解和生成自然语言"
        },
        "success": true
    },
    {
        "order": 1102,
        "title": "Towards Understanding Learning in Neural Networks with Linear Teachers",
        "html": "https://ICML.cc//virtual/2021/poster/9639",
        "abstract": "Can a neural network minimizing cross-entropy learn linearly separable data? Despite progress in the theory of deep learning, this question remains unsolved. Here we prove that SGD globally optimizes this learning problem for a two-layer network with Leaky ReLU activations. The learned network can in principle be very complex. However, empirical evidence suggests that it often turns out to be approximately linear. We provide theoretical support for this phenomenon by proving that if network weights converge to two weight clusters, this will imply an approximately linear decision boundary. Finally, we show a condition on the optimization that leads to weight clustering. We provide empirical results that validate our theoretical analysis.",
        "conference": "ICML",
        "中文标题": "迈向理解线性教师神经网络中的学习",
        "摘要翻译": "一个最小化交叉熵的神经网络能否学习线性可分的数据？尽管深度学习理论取得了进展，这个问题仍未解决。在这里，我们证明了对于具有Leaky ReLU激活的两层网络，SGD全局优化了这一学习问题。学习到的网络在原则上可以非常复杂。然而，实证证据表明，它往往近似线性。我们通过证明如果网络权重收敛到两个权重簇，这将意味着一个近似线性的决策边界，为这一现象提供了理论支持。最后，我们展示了一个导致权重聚类的优化条件。我们提供的实证结果验证了我们的理论分析。",
        "领域": "深度学习理论、神经网络优化、机器学习理论",
        "问题": "探讨神经网络在最小化交叉熵的情况下是否能够学习线性可分数据的问题。",
        "动机": "解决深度学习理论中关于神经网络学习线性可分数据能力的未解问题。",
        "方法": "使用具有Leaky ReLU激活的两层网络，通过SGD进行全局优化，并分析网络权重收敛到两个权重簇的条件。",
        "关键词": [
            "线性可分数据",
            "交叉熵最小化",
            "Leaky ReLU",
            "权重聚类",
            "SGD优化"
        ],
        "涉及的技术概念": {
            "交叉熵最小化": "用于衡量神经网络输出与真实标签之间差异的损失函数，优化目标是使其最小化。",
            "Leaky ReLU激活": "一种改进的ReLU激活函数，允许小的负输入值通过，有助于缓解神经元死亡问题。",
            "权重聚类": "指网络权重在训练过程中收敛到几个集中的值，这种现象可能导致网络的决策边界近似线性。"
        },
        "success": true
    },
    {
        "order": 1103,
        "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8579",
        "abstract": "We formally study how contrastive learning learns the feature representations for neural networks by investigating its feature learning process. We consider the case where our data are comprised of two types of features: the sparse features which we want to learn from, and the dense features we want to get rid of. Theoretically, we prove that contrastive learning using ReLU networks provably learns the desired features if proper augmentations are adopted. We present an underlying principle called feature decoupling to explain the effects of augmentations, where we theoretically characterize how augmentations can reduce the correlations of dense features between positive samples while keeping the correlations of sparse features intact, thereby forcing the neural networks to learn from the self-supervision of sparse features. Empirically, we verified that the feature decoupling principle matches the underlying mechanism of contrastive learning in practice.",
        "conference": "ICML",
        "中文标题": "理解自监督对比学习的特征学习过程",
        "摘要翻译": "我们通过研究对比学习的特征学习过程，正式探讨了对比学习如何为神经网络学习特征表示。我们考虑的数据由两种类型的特征组成：我们希望学习的稀疏特征和我们希望去除的密集特征。理论上，我们证明了使用ReLU网络的对比学习在采用适当增强的情况下能够可靠地学习到所需的特征。我们提出了一个称为特征解耦的基本原理来解释增强的效果，其中我们从理论上描述了增强如何能够减少正样本之间密集特征的相关性，同时保持稀疏特征的相关性不变，从而迫使神经网络从稀疏特征的自我监督中学习。实证上，我们验证了特征解耦原理与实践中的对比学习机制相匹配。",
        "领域": "自监督学习、特征学习、神经网络",
        "问题": "理解对比学习如何有效地学习特征表示，特别是如何区分和学习稀疏特征而非密集特征。",
        "动机": "探索对比学习在特征学习过程中的内在机制，特别是如何通过增强技术促进稀疏特征的学习。",
        "方法": "通过理论分析和实证验证，研究对比学习中的特征解耦原理及其对特征学习的影响。",
        "关键词": [
            "对比学习",
            "特征解耦",
            "自监督学习",
            "神经网络",
            "特征学习"
        ],
        "涉及的技术概念": {
            "对比学习": "一种自监督学习方法，通过比较正负样本学习特征表示。",
            "特征解耦": "解释增强技术如何减少密集特征的相关性，同时保留稀疏特征的相关性。",
            "ReLU网络": "使用ReLU激活函数的神经网络，用于实现对比学习中的特征学习。"
        },
        "success": true
    },
    {
        "order": 1104,
        "title": "Tractable structured natural-gradient descent using local parameterizations",
        "html": "https://ICML.cc//virtual/2021/poster/9733",
        "abstract": "Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank covariances) is computationally challenging due to difficult Fisher-matrix computations.\nWe address this issue by using \\emph{local-parameter coordinates} to obtain a flexible and efficient NGD method that works well for a wide-variety of structured parameterizations. \nWe show four applications where our method (1) generalizes the exponential natural evolutionary strategy, (2) recovers existing Newton-like algorithms, (3) yields new structured second-order algorithms, and (4) gives new algorithms to learn covariances of Gaussian and Wishart-based distributions. \nWe show results on a range of problems from deep learning, variational inference, and evolution strategies. Our work opens a new direction for scalable structured geometric methods.",
        "conference": "ICML",
        "中文标题": "使用局部参数化的可处理结构化自然梯度下降",
        "摘要翻译": "在结构化参数空间（例如，低秩协方差）上的自然梯度下降（NGD）由于Fisher矩阵计算的困难而具有计算上的挑战性。我们通过使用局部参数坐标来解决这个问题，以获得一种灵活且高效的NGD方法，该方法适用于多种结构化参数化。我们展示了四个应用，其中我们的方法（1）推广了指数自然进化策略，（2）恢复了现有的类牛顿算法，（3）产生了新的结构化二阶算法，以及（4）提供了学习高斯和基于Wishart分布的协方差的新算法。我们在深度学习、变分推理和进化策略等一系列问题上展示了结果。我们的工作为可扩展的结构化几何方法开辟了新的方向。",
        "领域": "优化算法, 变分推理, 进化策略",
        "问题": "解决在结构化参数空间上自然梯度下降计算困难的问题",
        "动机": "为了开发一种灵活且高效的自然梯度下降方法，适用于多种结构化参数化，以克服Fisher矩阵计算的困难",
        "方法": "使用局部参数坐标来获得一种灵活且高效的NGD方法，适用于多种结构化参数化，并在多个应用中展示其效果",
        "关键词": [
            "自然梯度下降",
            "结构化参数空间",
            "局部参数化",
            "Fisher矩阵",
            "进化策略"
        ],
        "涉及的技术概念": {
            "自然梯度下降（NGD）": "一种在参数空间中考虑信息几何结构的优化方法，用于更有效的参数更新",
            "局部参数坐标": "用于简化Fisher矩阵计算的局部参数化方法，提高NGD的效率和灵活性",
            "Fisher矩阵": "在自然梯度下降中用于衡量参数空间曲率的矩阵，其计算在结构化参数空间中具有挑战性"
        },
        "success": true
    },
    {
        "order": 1105,
        "title": "Training Adversarially Robust Sparse Networks via Bayesian Connectivity Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/8563",
        "abstract": "Deep neural networks have been shown to be susceptible to adversarial attacks. This lack of adversarial robustness is even more pronounced when models are compressed in order to meet hardware limitations. Hence, if adversarial robustness is an issue, training of sparsely connected networks necessitates considering adversarially robust sparse learning. Motivated by the efficient and stable computational function of the brain in the presence of a highly dynamic synaptic connectivity structure, we propose an intrinsically sparse rewiring approach to train neural networks with state-of-the-art robust learning objectives under high sparsity. Importantly, in contrast to previously proposed pruning techniques, our approach satisfies global connectivity constraints throughout robust optimization, i.e., it does not require dense pre-training followed by pruning. Based on a Bayesian posterior sampling principle, a network rewiring process simultaneously learns the sparse connectivity structure and the robustness-accuracy trade-off based on the adversarial learning objective. Although our networks are sparsely connected throughout the whole training process, our experimental benchmark evaluations show that their performance is superior to recently proposed robustness-aware network pruning methods which start from densely connected networks.",
        "conference": "ICML",
        "中文标题": "通过贝叶斯连接采样训练对抗性鲁棒的稀疏网络",
        "摘要翻译": "深度神经网络已被证明容易受到对抗性攻击。当模型被压缩以满足硬件限制时，这种对抗性鲁棒性的缺乏更加明显。因此，如果对抗性鲁棒性是一个问题，训练稀疏连接网络就需要考虑对抗性鲁棒的稀疏学习。受到大脑在高度动态的突触连接结构存在下高效稳定计算功能的启发，我们提出了一种本质稀疏的重新布线方法，在高稀疏性下训练具有最先进鲁棒学习目标的神经网络。重要的是，与之前提出的剪枝技术相比，我们的方法在鲁棒优化过程中满足全局连接约束，即不需要先进行密集预训练再进行剪枝。基于贝叶斯后验采样原理，网络重新布线过程同时学习稀疏连接结构和基于对抗性学习目标的鲁棒性-准确性权衡。尽管我们的网络在整个训练过程中都是稀疏连接的，但我们的实验基准评估显示，它们的性能优于最近提出的从密集连接网络开始的鲁棒性感知网络剪枝方法。",
        "领域": "对抗性防御、神经网络压缩、稀疏学习",
        "问题": "在神经网络压缩过程中保持对抗性鲁棒性",
        "动机": "受到大脑在动态突触连接下保持计算功能的启发，探索在高度稀疏条件下训练对抗性鲁棒网络的方法",
        "方法": "提出一种基于贝叶斯后验采样的稀疏网络重新布线方法，无需密集预训练即可同时学习稀疏结构和鲁棒性-准确性权衡",
        "关键词": [
            "对抗性鲁棒性",
            "稀疏网络",
            "贝叶斯采样",
            "网络剪枝",
            "鲁棒学习"
        ],
        "涉及的技术概念": {
            "贝叶斯后验采样": "用于在网络训练过程中动态调整稀疏连接结构，同时优化对抗性鲁棒性和准确性",
            "对抗性鲁棒性": "指网络在面对精心设计的对抗性攻击时保持性能的能力，是本研究的核心目标之一",
            "稀疏学习": "在保持网络性能的同时减少网络中的连接数量，以提高计算效率和对抗性鲁棒性"
        },
        "success": true
    },
    {
        "order": 1106,
        "title": "Training data-efficient image transformers & distillation through attention",
        "html": "https://ICML.cc//virtual/2021/poster/8671",
        "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.\n\nIn this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data.\n\nWe also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.",
        "conference": "ICML",
        "中文标题": "通过注意力机制训练数据高效的图像变换器及蒸馏",
        "摘要翻译": "最近，完全基于注意力的神经网络被证明能够处理图像理解任务，如图像分类。这些高性能的视觉变换器需要使用数百万张图像在大规模基础设施上进行预训练，这限制了它们的普及。在这项工作中，我们仅在ImageNet上使用单一计算机在不到3天的时间内训练出了具有竞争力的无卷积变换器。我们的参考视觉变换器（8600万参数）在没有外部数据的情况下，在ImageNet上达到了83.1%的top-1准确率（单裁剪）。我们还引入了一种专门针对变换器的师生策略。它依赖于一个蒸馏令牌，确保学生通过注意力从教师那里学习，通常是从卷积网络教师那里。学习到的变换器在ImageNet上与现有技术相比具有竞争力（85.2% top-1准确率），并且在迁移到其他任务时同样表现良好。我们将分享我们的代码和模型。",
        "领域": "图像分类, 视觉变换器, 知识蒸馏",
        "问题": "如何在不依赖大规模预训练数据和基础设施的情况下，训练出高性能的视觉变换器。",
        "动机": "减少视觉变换器训练对大规模数据和计算资源的依赖，使其更易于普及和应用。",
        "方法": "开发了一种数据高效的视觉变换器训练方法，结合了特定的师生蒸馏策略，通过注意力机制从卷积网络教师那里学习。",
        "关键词": [
            "视觉变换器",
            "知识蒸馏",
            "注意力机制",
            "图像分类",
            "数据高效训练"
        ],
        "涉及的技术概念": {
            "视觉变换器": "一种完全基于注意力机制的神经网络架构，用于图像理解任务。",
            "知识蒸馏": "一种师生学习策略，通过蒸馏令牌使学生模型从教师模型中学习知识。",
            "注意力机制": "使模型能够专注于输入数据中最相关的部分，提高学习效率和性能。"
        },
        "success": true
    },
    {
        "order": 1107,
        "title": "Training Data Subset Selection for Regression with Controlled Generalization Error",
        "html": "https://ICML.cc//virtual/2021/poster/8757",
        "abstract": "Data subset selection from a large number of training instances has been a successful approach toward efficient and cost-effective machine learning. However, models trained on a smaller subset may show poor generalization ability. In this paper, our goal is to design an algorithm for selecting a subset of the training data, so that the model can be trained quickly, without significantly sacrificing on accuracy. More specifically, we focus on data subset selection for $L_2$ regularized regression problems and provide a novel problem formulation which seeks to minimize the training loss with respect to both the trainable parameters and  the subset of training data, subject to error bounds on the validation set. We tackle this problem using several  technical innovations. First, we represent this problem with simplified constraints using the dual of the original training problem and show that the objective of this new representation is a monotone and $\\alpha$-submodular function, for a wide variety of modeling choices. Such properties lead us to develop SELCON, an efficient majorization-minimization algorithm for data subset selection, that admits an approximation guarantee even when the training provides an imperfect estimate of the trained model. Finally, our experiments on several datasets show that SELCON\ntrades off accuracy and efficiency more effectively than the current state-of-the-art.",
        "conference": "ICML",
        "success": true,
        "中文标题": "控制泛化误差的回归训练数据子集选择",
        "摘要翻译": "从大量训练实例中选择数据子集已成为实现高效且成本效益显著的机器学习的一种成功方法。然而，在较小子集上训练的模型可能表现出较差的泛化能力。本文的目标是设计一种算法，用于选择训练数据的子集，以便能够快速训练模型，而不会显著牺牲准确性。更具体地说，我们专注于$L_2$正则化回归问题的数据子集选择，并提供了一个新颖的问题表述，该表述寻求在验证集上的误差界限约束下，最小化关于可训练参数和训练数据子集的训练损失。我们通过几项技术创新来解决这个问题。首先，我们使用原始训练问题的对偶来表示这个问题，并展示了对于多种建模选择，这个新表示的目标是一个单调且α-子模的函数。这些性质引导我们开发了SELCON，一种用于数据子集选择的高效主化-最小化算法，即使在训练提供了训练模型的不完美估计时，也允许近似保证。最后，我们在多个数据集上的实验表明，SELCON在准确性和效率之间的权衡比当前最先进的方法更有效。",
        "领域": "机器学习优化, 回归分析, 数据子集选择",
        "问题": "如何在保证模型泛化能力的前提下，高效选择训练数据的子集以加速模型训练。",
        "动机": "解决在大规模数据集上训练模型时，如何在保持模型准确性的同时，通过选择数据子集来提高训练效率的问题。",
        "方法": "提出了一种新颖的问题表述，结合对偶理论和α-子模性质，开发了SELCON算法，用于高效选择数据子集。",
        "关键词": [
            "数据子集选择",
            "回归分析",
            "泛化误差",
            "SELCON算法",
            "α-子模"
        ],
        "涉及的技术概念": {
            "对偶理论": "用于简化原始训练问题的约束，使得问题更易于处理和分析。",
            "α-子模函数": "用于描述目标函数的性质，为开发高效算法提供理论基础。"
        }
    },
    {
        "order": 1108,
        "title": "Training Graph Neural Networks with 1000 Layers",
        "html": "https://ICML.cc//virtual/2021/poster/10455",
        "abstract": "Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of 87.74 ± 0.13 and 88.14 ± 0.15 on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude.",
        "conference": "ICML",
        "中文标题": "训练具有1000层的图神经网络",
        "摘要翻译": "深度图神经网络（GNNs）在包含数百万节点和边的大型图数据集上的各种任务中取得了优异的成绩。然而，由于节点、边和中间激活的数量巨大，内存复杂性已成为训练深度GNNs用于实际应用的主要障碍。为了提高GNNs的可扩展性，先前的工作提出了智能图采样或分区策略，以使用较小的节点集或子图训练GNNs。在这项工作中，我们研究了可逆连接、组卷积、权重绑定和平衡模型，以提高GNNs的内存和参数效率。我们发现，结合深度网络架构的可逆连接能够训练过参数化的GNNs，这些GNNs在多个数据集上显著优于现有方法。我们的模型RevGNN-Deep（每层80通道的1001层）和RevGNN-Wide（每层224通道的448层）均在单个商用GPU上训练，并在ogbn-proteins数据集上实现了87.74 ± 0.13和88.14 ± 0.15的ROC-AUC。据我们所知，RevGNN-Deep是文献中最深的GNN，深度达到了一个数量级。",
        "领域": "图神经网络、深度学习优化、大规模图数据处理",
        "问题": "解决训练深度图神经网络时的内存和参数效率问题",
        "动机": "由于大型图数据集中的节点和边数量巨大，训练深度GNNs面临内存复杂性的挑战，需要提高其可扩展性和效率",
        "方法": "采用可逆连接、组卷积、权重绑定和平衡模型等技术，结合深度网络架构，训练过参数化的GNNs",
        "关键词": [
            "可逆连接",
            "深度图神经网络",
            "参数效率",
            "大规模图数据",
            "ROC-AUC"
        ],
        "涉及的技术概念": {
            "可逆连接": "用于减少训练深度网络时的内存需求，允许在反向传播时重新计算激活而非存储",
            "组卷积": "通过分组处理输入特征来减少模型的参数数量和计算复杂度",
            "权重绑定": "在不同层或部分网络中共享权重，以减少模型的参数数量和提高训练效率"
        },
        "success": true
    },
    {
        "order": 1109,
        "title": "Training Quantized Neural Networks to Global Optimality via Semidefinite Programming",
        "html": "https://ICML.cc//virtual/2021/poster/9259",
        "abstract": "Neural networks (NNs) have been extremely successful across many tasks in machine learning. Quantization of NN weights has become an important topic due to its impact on their energy efficiency, inference time and deployment on hardware. Although post-training quantization is well-studied, training optimal quantized NNs involves combinatorial non-convex optimization problems which appear intractable. In this work, we introduce a convex optimization strategy to train quantized NNs with polynomial activations. Our method leverages hidden convexity in two-layer neural networks from the recent literature, semidefinite lifting, and Grothendieck's identity. Surprisingly, we show that certain quantized NN problems can be solved to global optimality provably in polynomial time in all relevant parameters via tight semidefinite relaxations. We present numerical examples to illustrate the effectiveness of our method.",
        "conference": "ICML",
        "中文标题": "通过半定规划训练量化神经网络至全局最优",
        "摘要翻译": "神经网络（NNs）在机器学习的许多任务中取得了极大的成功。由于量化对神经网络的能效、推理时间及硬件部署的影响，神经网络权重的量化已成为一个重要课题。尽管训练后量化已被深入研究，但训练最优量化神经网络涉及组合非凸优化问题，这些问题似乎难以解决。在这项工作中，我们引入了一种凸优化策略来训练具有多项式激活函数的量化神经网络。我们的方法利用了最近文献中两层神经网络的隐藏凸性、半定提升和Grothendieck恒等式。令人惊讶的是，我们表明，通过紧密的半定松弛，某些量化神经网络问题可以在所有相关参数中以多项式时间可证明地解决至全局最优。我们提供了数值例子来说明我们方法的有效性。",
        "领域": "神经网络优化、量化神经网络、凸优化",
        "问题": "解决量化神经网络训练中的组合非凸优化问题",
        "动机": "提高量化神经网络的能效、推理时间及硬件部署效率",
        "方法": "利用两层神经网络的隐藏凸性、半定提升和Grothendieck恒等式，通过凸优化策略训练量化神经网络",
        "关键词": [
            "量化神经网络",
            "全局最优",
            "半定规划",
            "凸优化",
            "多项式激活函数"
        ],
        "涉及的技术概念": {
            "半定规划": "用于将非凸优化问题转化为可解决的凸优化问题",
            "Grothendieck恒等式": "在优化过程中用于处理特定类型的约束",
            "隐藏凸性": "揭示神经网络优化问题中的凸结构，使得全局最优解可求"
        },
        "success": true
    },
    {
        "order": 1110,
        "title": "Training Recurrent Neural Networks via Forward Propagation Through Time",
        "html": "https://ICML.cc//virtual/2021/poster/10383",
        "abstract": "Back-propagation through time (BPTT) has been widely used for training Recurrent Neural Networks (RNNs).  BPTT updates RNN parameters on an instance by back-propagating the error in time over the entire sequence length, and as a result, leads to poor trainability due to the well-known gradient explosion/decay phenomena. While a number of prior works have proposed to mitigate vanishing/explosion effect through careful RNN architecture design, these RNN variants still train with BPTT. We propose a novel forward-propagation algorithm, FPTT, where at each time, for an instance, we update RNN parameters by optimizing an instantaneous risk function. Our proposed risk is a regularization penalty at time $t$ that evolves dynamically based on previously observed losses, and allows for RNN parameter updates to converge to a stationary solution of the empirical RNN objective. We consider both sequence-to-sequence as well as terminal loss problems. Empirically FPTT outperforms BPTT on a number of well-known benchmark tasks, thus enabling architectures like LSTMs to solve long range dependencies problems.\n\n",
        "conference": "ICML",
        "中文标题": "通过时间前向传播训练循环神经网络",
        "摘要翻译": "时间反向传播（BPTT）已被广泛用于训练循环神经网络（RNNs）。BPTT通过在时间上对整个序列长度的误差进行反向传播来更新RNN参数，因此，由于众所周知的梯度爆炸/消失现象，导致训练能力较差。虽然许多先前的工作提出通过精心设计的RNN架构来缓解消失/爆炸效应，但这些RNN变体仍然使用BPTT进行训练。我们提出了一种新颖的前向传播算法FPTT，在每个时间点，对于一个实例，我们通过优化瞬时风险函数来更新RNN参数。我们提出的风险是在时间$t$的动态正则化惩罚，它基于先前观察到的损失动态演变，并允许RNN参数更新收敛到经验RNN目标的平稳解。我们考虑了序列到序列以及终端损失问题。实证上，FPTT在许多知名基准任务上优于BPTT，从而使像LSTM这样的架构能够解决长距离依赖问题。",
        "领域": "循环神经网络训练、序列建模、长距离依赖问题",
        "问题": "解决循环神经网络在训练过程中由于梯度爆炸或消失导致的训练能力差的问题",
        "动机": "通过提出一种新的前向传播算法FPTT，替代传统的时间反向传播（BPTT），以提高循环神经网络的训练效率和性能",
        "方法": "提出了一种基于瞬时风险函数优化的前向传播算法FPTT，通过动态调整的正则化惩罚来更新RNN参数，使其收敛到经验RNN目标的平稳解",
        "关键词": [
            "循环神经网络",
            "前向传播",
            "梯度问题",
            "LSTM",
            "长距离依赖"
        ],
        "涉及的技术概念": {
            "时间反向传播（BPTT）": "传统的RNN训练方法，通过时间上反向传播误差来更新参数，但容易导致梯度爆炸或消失",
            "前向传播算法（FPTT）": "提出的新算法，通过优化瞬时风险函数来更新参数，避免了梯度问题",
            "动态正则化惩罚": "FPTT中使用的技术，基于先前损失动态调整，帮助参数更新收敛到平稳解"
        },
        "success": true
    },
    {
        "order": 1111,
        "title": "Train simultaneously, generalize better: Stability of gradient-based minimax learners",
        "html": "https://ICML.cc//virtual/2021/poster/10111",
        "abstract": "The success of minimax learning problems of generative adversarial networks (GANs) has been observed to depend on the minimax optimization algorithm used for their training. This dependence is commonly attributed to the convergence speed and robustness properties of the underlying optimization algorithm. In this paper, we show that the optimization algorithm also plays a key role in the generalization performance of the trained minimax model. To this end, we analyze the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms through the lens of algorithmic stability as defined by Bousquet & Elisseeff, 2002 under both convex-concave and nonconvex-nonconcave minimax settings. While the GDA algorithm is not guaranteed to have a vanishing excess risk in convex-concave problems, we show the PPM algorithm enjoys a bounded excess risk in the same setup. For nonconvex-nonconcave problems, we compare the generalization performance of stochastic GDA and GDmax algorithms where the latter fully solves the maximization subproblem at every iteration. Our generalization analysis suggests the superiority of GDA provided that the minimization and maximization subproblems are solved simultaneously with similar learning rates. We discuss several numerical results indicating the role of optimization algorithms in the generalization of learned minimax models.",
        "conference": "ICML",
        "中文标题": "同时训练，更好泛化：基于梯度的极小极大学习器的稳定性",
        "摘要翻译": "生成对抗网络（GANs）的极小极大学习问题的成功被观察到依赖于用于其训练的极小极大优化算法。这种依赖性通常归因于底层优化算法的收敛速度和鲁棒性属性。在本文中，我们展示了优化算法在训练后的极小极大模型的泛化性能中也扮演着关键角色。为此，我们通过Bousquet & Elisseeff, 2002定义的算法稳定性视角，分析了标准梯度下降上升（GDA）和近端点方法（PPM）算法在凸凹和非凸非凹极小极大设置下的泛化性质。虽然GDA算法在凸凹问题中不保证有过剩风险消失，但我们展示了PPM算法在同一设置下享有有界的过剩风险。对于非凸非凹问题，我们比较了随机GDA和GDmax算法的泛化性能，后者在每次迭代中完全解决了最大化子问题。我们的泛化分析表明，只要最小化和最大化子问题以相似的学习率同时解决，GDA的优越性就显现出来。我们讨论了几项数值结果，表明了优化算法在学习极小极大模型泛化中的作用。",
        "领域": "生成对抗网络、优化算法、机器学习理论",
        "问题": "极小极大学习问题中优化算法对模型泛化性能的影响",
        "动机": "探索不同优化算法在极小极大学习问题中对模型泛化性能的具体影响，以指导更有效的模型训练策略。",
        "方法": "通过算法稳定性的视角，分析标准梯度下降上升（GDA）和近端点方法（PPM）算法在不同极小极大设置下的泛化性能，并比较随机GDA和GDmax算法在非凸非凹问题中的表现。",
        "关键词": [
            "极小极大学习",
            "泛化性能",
            "优化算法",
            "梯度下降上升",
            "近端点方法"
        ],
        "涉及的技术概念": {
            "算法稳定性": "用于评估优化算法在训练数据微小变化下输出模型变化程度的指标，影响模型的泛化能力。",
            "梯度下降上升（GDA）": "一种常用的极小极大优化算法，交替执行梯度下降和梯度上升步骤，用于训练生成对抗网络等模型。",
            "近端点方法（PPM）": "一种优化算法，通过在每次迭代中添加一个近端项来稳定优化过程，适用于凸凹问题。"
        },
        "success": true
    },
    {
        "order": 1112,
        "title": "Trajectory Diversity for Zero-Shot Coordination",
        "html": "https://ICML.cc//virtual/2021/poster/9433",
        "abstract": "We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are compatible with novel partners not seen during training. Our first contribution is to consider the need for diversity in generating such agents. Because self-play (SP) agents control their own trajectory distribution during training, each policy typically only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a common best response (BR) to a population of agents, which we regulate to be diverse. To this end, we introduce \\textit{Trajectory Diversity} (TrajeDi) -- a differentiable objective for generating diverse reinforcement learning policies. We derive TrajeDi as a generalization of the Jensen-Shannon divergence between policies and motivate it experimentally in two simple settings. We then focus on the collaborative card game Hanabi, demonstrating the scalability of our method and improving upon the cross-play scores of both independently trained SP agents and BRs to unregularized populations.",
        "conference": "ICML",
        "success": true,
        "中文标题": "零样本协作中的轨迹多样性",
        "摘要翻译": "我们研究了零样本协作（ZSC）问题，其中智能体必须独立为协作游戏生成策略，这些策略需要与训练期间未见的新伙伴兼容。我们的第一个贡献是考虑在生成此类智能体时对多样性的需求。因为自玩（SP）智能体在训练期间控制自己的轨迹分布，每个策略通常仅在此确切分布上表现良好。因此，它们在ZSC中得分较低，因为与另一个智能体玩耍很可能会使它们处于训练期间未遇到的情况。为了解决这个问题，我们训练了一个对多样化的智能体群体的共同最佳响应（BR）。为此，我们引入了轨迹多样性（TrajeDi）——一种用于生成多样化强化学习策略的可微分目标。我们将TrajeDi推导为策略间Jensen-Shannon散度的泛化，并在两个简单设置中通过实验激发其动机。然后，我们专注于协作卡牌游戏Hanabi，展示了我们方法的可扩展性，并改进了独立训练的SP智能体和对未规范化群体的BR的交叉游戏分数。",
        "领域": "多智能体系统、强化学习、游戏AI",
        "问题": "解决零样本协作中智能体策略与未见伙伴的兼容性问题",
        "动机": "提高智能体在零样本协作环境中的适应性和协作效率",
        "方法": "引入轨迹多样性（TrajeDi）作为生成多样化强化学习策略的可微分目标，并训练对多样化智能体群体的共同最佳响应",
        "关键词": [
            "零样本协作",
            "轨迹多样性",
            "强化学习",
            "多智能体系统",
            "Hanabi游戏"
        ],
        "涉及的技术概念": {
            "零样本协作（ZSC）": "智能体需要与训练期间未见的新伙伴协作的情境",
            "轨迹多样性（TrajeDi）": "一种用于生成多样化强化学习策略的可微分目标，旨在提高策略的泛化能力",
            "共同最佳响应（BR）": "对多样化智能体群体的最佳策略响应，用于提高协作效率"
        }
    },
    {
        "order": 1113,
        "title": "Transfer-Based Semantic Anomaly Detection",
        "html": "https://ICML.cc//virtual/2021/poster/9479",
        "abstract": "Detecting semantic anomalies is challenging due to the countless ways in which they may appear in real-world data. While enhancing the robustness of networks may be sufficient for modeling simplistic anomalies, there is no good known way of preparing models for all potential and unseen anomalies that can potentially occur, such as the appearance of new object classes. In this paper, we show that a previously overlooked strategy for anomaly detection (AD) is to introduce an explicit inductive bias toward representations transferred over from some large and varied semantic task. We rigorously verify our hypothesis in controlled trials that utilize intervention, and show that it gives rise to surprisingly effective auxiliary objectives that outperform previous AD paradigms.",
        "conference": "ICML",
        "中文标题": "基于迁移的语义异常检测",
        "摘要翻译": "检测语义异常具有挑战性，因为它们在现实世界数据中可能以无数种方式出现。虽然增强网络的鲁棒性可能足以建模简单的异常，但目前还没有已知的好方法来为所有潜在和未见过的异常（如新对象类别的出现）准备模型。在本文中，我们展示了一种之前被忽视的异常检测（AD）策略，即引入一种明确的归纳偏向，偏向于从某些大型且多样化的语义任务中迁移过来的表示。我们在利用干预的对照试验中严格验证了我们的假设，并表明它产生了出奇有效的辅助目标，这些目标优于以前的AD范式。",
        "领域": "异常检测、语义分割、迁移学习",
        "问题": "如何有效检测和准备模型应对现实世界中可能出现的各种未见过的语义异常。",
        "动机": "探索并验证通过迁移学习引入明确的归纳偏向，以提高模型对未见过的语义异常的检测能力。",
        "方法": "通过从大型且多样化的语义任务中迁移表示，引入明确的归纳偏向，并在对照试验中验证这一策略的有效性。",
        "关键词": [
            "语义异常检测",
            "迁移学习",
            "归纳偏向",
            "对照试验",
            "辅助目标"
        ],
        "涉及的技术概念": {
            "语义异常检测": "检测与已知语义模式不符的数据实例，特别是在面对未见过的异常时。",
            "迁移学习": "利用从一个任务学到的知识来帮助解决另一个相关任务，这里用于引入对语义异常的检测能力。",
            "归纳偏向": "在学习过程中引入的偏好或假设，这里指偏向于从大型语义任务中迁移过来的表示，以提高异常检测的效果。"
        },
        "success": true
    },
    {
        "order": 1114,
        "title": "Trees with Attention for Set Prediction Tasks",
        "html": "https://ICML.cc//virtual/2021/poster/10127",
        "abstract": "In many machine learning applications, each record represents a set of items. For example, when making predictions from medical records, the medications prescribed to a patient are a set whose size is not fixed and whose order is arbitrary. However, most machine learning algorithms are not designed to handle set structures and are limited to processing records of fixed size. Set-Tree, presented in this work, extends the support for sets to tree-based models, such as Random-Forest and Gradient-Boosting, by introducing an attention mechanism and set-compatible split criteria. We evaluate the new method empirically on a wide range of problems ranging from making predictions on sub-atomic particle jets to estimating the redshift of galaxies. The new method outperforms existing tree-based methods consistently and significantly. Moreover, it is competitive and often outperforms Deep Learning. We also discuss the theoretical properties of Set-Trees and explain how they enable item-level explainability.",
        "conference": "ICML",
        "中文标题": "用于集合预测任务的注意力树",
        "摘要翻译": "在许多机器学习应用中，每条记录代表一组项目。例如，在根据医疗记录进行预测时，患者服用的药物是一个大小不固定且顺序任意的集合。然而，大多数机器学习算法并非设计用于处理集合结构，仅限于处理固定大小的记录。本工作中提出的Set-Tree通过引入注意力机制和集合兼容的分裂标准，将集合支持扩展到基于树的模型，如随机森林和梯度提升。我们在从亚原子粒子喷流预测到星系红移估计的广泛问题上对新方法进行了实证评估。新方法一致且显著地优于现有的基于树的方法。此外，它与深度学习竞争，并且经常超越深度学习。我们还讨论了Set-Trees的理论特性，并解释了它们如何实现项目级别的可解释性。",
        "领域": "集合学习、注意力机制、树模型",
        "问题": "如何使基于树的模型能够处理集合结构的数据",
        "动机": "大多数机器学习算法无法有效处理集合结构的数据，限制了其在需要处理可变大小和无序集合的应用中的使用",
        "方法": "通过引入注意力机制和集合兼容的分裂标准，扩展基于树的模型以支持集合结构的数据",
        "关键词": [
            "集合学习",
            "注意力机制",
            "树模型",
            "随机森林",
            "梯度提升"
        ],
        "涉及的技术概念": {
            "注意力机制": "用于增强模型对集合中不同项目的关注能力，使模型能够根据项目的重要性动态调整其关注点",
            "集合兼容的分裂标准": "允许基于树的模型在处理集合数据时进行有效的分裂决策，从而支持集合结构的数据处理",
            "项目级别的可解释性": "通过Set-Trees提供的机制，可以解释模型对集合中每个项目的决策过程，增强模型的可解释性"
        },
        "success": true
    },
    {
        "order": 1115,
        "title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed Coverage for Cox-MLP",
        "html": "https://ICML.cc//virtual/2021/poster/10303",
        "abstract": "It is challenging to deal with censored data, where we only have access to the incomplete information of survival time instead of its exact value. Fortunately, under linear predictor assumption, people can obtain guaranteed coverage for the confidence interval of survival time using methods like Cox Regression. However, when relaxing the linear assumption with neural networks (e.g., Cox-MLP \\citep{katzman2018deepsurv,kvamme2019time}), we lose the guaranteed coverage. To recover the guaranteed coverage without linear assumption, we propose two algorithms based on conformal inference. In the first algorithm \\emph{WCCI}, we revisit weighted conformal inference and introduce a new non-conformity score based on partial likelihood. We then propose a two-stage algorithm \\emph{T-SCI}, where we run WCCI in the first stage and apply quantile conformal inference to calibrate the results in the second stage. Theoretical analysis shows that T-SCI returns guaranteed coverage under milder assumptions than WCCI. We conduct extensive experiments on synthetic data and real data using different methods, which validate our analysis.",
        "conference": "ICML",
        "中文标题": "T-SCI：一种具有保证覆盖率的Cox-MLP两阶段保形推理算法",
        "摘要翻译": "处理删失数据具有挑战性，因为我们只能获取生存时间的不完整信息而非其确切值。幸运的是，在线性预测假设下，人们可以通过诸如Cox回归等方法获得生存时间置信区间的保证覆盖率。然而，当用神经网络（例如Cox-MLP）放宽线性假设时，我们失去了保证覆盖率。为了在不依赖线性假设的情况下恢复保证覆盖率，我们提出了两种基于保形推理的算法。在第一个算法WCCI中，我们重新审视了加权保形推理，并引入了一种基于部分似然的新非一致性评分。接着，我们提出了一种两阶段算法T-SCI，其中在第一阶段运行WCCI，在第二阶段应用分位数保形推理来校准结果。理论分析表明，T-SCI在比WCCI更温和的假设下返回保证覆盖率。我们在合成数据和真实数据上使用不同方法进行了广泛的实验，验证了我们的分析。",
        "领域": "生存分析、神经网络应用、统计学习",
        "问题": "在神经网络放宽线性假设的情况下，如何恢复生存时间置信区间的保证覆盖率",
        "动机": "解决在神经网络模型中处理删失数据时保证覆盖率丢失的问题",
        "方法": "提出基于保形推理的两阶段算法T-SCI，包括加权保形推理和分位数保形推理的应用",
        "关键词": [
            "保形推理",
            "Cox-MLP",
            "生存分析",
            "神经网络",
            "保证覆盖率"
        ],
        "涉及的技术概念": {
            "保形推理": "一种统计方法，用于在不完全依赖模型假设的情况下提供预测区间",
            "Cox-MLP": "结合Cox比例风险模型和多层感知机的神经网络模型，用于生存分析",
            "部分似然": "在生存分析中用于估计模型参数的方法，基于观察到的事件时间顺序"
        },
        "success": true
    },
    {
        "order": 1116,
        "title": "Two Heads are Better Than One: Hypergraph-Enhanced Graph Reasoning for Visual Event Ratiocination",
        "html": "https://ICML.cc//virtual/2021/poster/9031",
        "abstract": "Even with a still image, humans can ratiocinate various visual cause-and-effect descriptions before, at present, and after, as well as beyond the given image. However, it is challenging for models to achieve such task--the visual event ratiocination, owing to the limitations of time and space. To this end, we propose a novel multi-modal model, Hypergraph-Enhanced Graph Reasoning. First it represents the contents from the same modality as a semantic graph and mines the intra-modality relationship, therefore breaking the limitations in the spatial domain. Then, we introduce the Graph Self-Attention Enhancement. On the one hand, this enables semantic graph representations from different modalities to enhance each other and captures the inter-modality relationship along the line. On the other hand, it utilizes our built multi-modal hypergraphs in different moments to boost individual semantic graph representations, and breaks the limitations in the temporal domain. Our method illustrates the case of 'two heads are better than one' in the sense that semantic graph representations with the help of the proposed enhancement mechanism are more robust than those without. Finally, we re-project these representations and leverage their outcomes to generate textual cause-and-effect descriptions. Experimental results show that our model achieves significantly higher performance in comparison with other state-of-the-arts.",
        "conference": "ICML",
        "中文标题": "双头优于单头：超图增强的图推理用于视觉事件推理",
        "摘要翻译": "即使是一张静态图像，人类也能推理出图像之前、当前、之后以及超越图像本身的各种视觉因果关系描述。然而，由于时间和空间的限制，模型实现这样的任务——视觉事件推理，是具有挑战性的。为此，我们提出了一种新颖的多模态模型，超图增强的图推理。首先，它将同一模态的内容表示为语义图，并挖掘模态内关系，从而打破空间领域的限制。然后，我们引入了图自注意力增强机制。一方面，这使得来自不同模态的语义图表示能够相互增强，并沿线捕捉模态间关系。另一方面，它利用我们在不同时刻构建的多模态超图来增强个体语义图表示，并打破时间领域的限制。我们的方法展示了‘双头优于单头’的情况，即在提出的增强机制的帮助下，语义图表示比没有帮助时更加稳健。最后，我们重新投影这些表示，并利用它们的结果生成文本因果关系描述。实验结果表明，与其他最先进的技术相比，我们的模型实现了显著更高的性能。",
        "领域": "视觉事件推理、多模态学习、图神经网络",
        "问题": "解决视觉事件推理中时间和空间限制的问题",
        "动机": "为了克服视觉事件推理中由于时间和空间限制带来的挑战",
        "方法": "提出了一种超图增强的图推理模型，通过构建语义图和超图来挖掘模态内和模态间关系，并引入图自注意力增强机制",
        "关键词": [
            "视觉事件推理",
            "超图增强",
            "图自注意力",
            "多模态学习",
            "语义图"
        ],
        "涉及的技术概念": {
            "超图增强的图推理": "通过构建超图来增强语义图表示，以捕捉复杂的模态间关系",
            "图自注意力增强机制": "用于增强不同模态间语义图表示的相互影响，提升模型的推理能力",
            "多模态超图": "在不同时间点构建的超图，用于增强个体语义图表示，打破时间限制"
        },
        "success": true
    },
    {
        "order": 1117,
        "title": "Two-way kernel matrix puncturing: towards resource-efficient PCA and spectral clustering",
        "html": "https://ICML.cc//virtual/2021/poster/10275",
        "abstract": "The article introduces an elementary cost and storage reduction method for spectral clustering and principal component analysis. The method consists in randomly ``puncturing'' both the data matrix $X\\in\\mathbb{C}^{p\\times n}$ (or $\\mathbb{R}^{p\\times n}$) and its corresponding kernel (Gram) matrix $K$ through Bernoulli masks: $S\\in\\{0,1\\}^{p\\times n}$  for $X$ and $B\\in\\{0,1\\}^{n\\times n}$ for $K$. The resulting ``two-way punctured'' kernel is thus given by $K=\\frac1p[(X\\odot S)^\\H (X\\odot S)]\\odot B$.\nWe demonstrate that, for $X$ composed of independent columns drawn from a Gaussian mixture model, as $n,p\\to\\infty$ with $p/n\\to c_0\\in(0,\\infty)$, the spectral behavior of $K$ -- its limiting eigenvalue distribution, as well as its isolated eigenvalues and eigenvectors -- is fully tractable and exhibits a series of counter-intuitive phenomena. We notably prove, and empirically confirm on various image databases, that it is possible to drastically puncture the data, thereby providing possibly huge computational and storage gains, for a virtually constant (clustering or PCA) performance. This preliminary study opens as such the path towards rethinking, from a large dimensional standpoint, computational and storage costs in elementary machine learning models.",
        "conference": "ICML",
        "success": true,
        "中文标题": "双向核矩阵穿孔：面向资源高效的主成分分析与谱聚类",
        "摘要翻译": "本文介绍了一种用于谱聚类和主成分分析的基本成本和存储减少方法。该方法包括通过伯努利掩码随机‘穿孔’数据矩阵$X\\\\in\\\\mathbb{C}^{p\\\\times n}$（或$\\\\mathbb{R}^{p\\\\times n}$）及其对应的核（Gram）矩阵$K$：对于$X$使用$S\\\\in\\\\{0,1\\\\}^{p\\\\times n}$，对于$K$使用$B\\\\in\\\\{0,1\\\\}^{n\\\\times n}$。由此产生的‘双向穿孔’核由$K=\\\\frac1p[(X\\\\odot S)^\\\\H (X\\\\odot S)]\\\\odot B$给出。我们证明，对于由高斯混合模型独立抽取的列组成的$X$，当$n,p\\\\to\\\\infty$且$p/n\\\\to c_0\\\\in(0,\\\\infty)$时，$K$的谱行为——其极限特征值分布，以及其孤立特征值和特征向量——是完全可追踪的，并展示了一系列反直觉的现象。我们特别证明，并在各种图像数据库上实证确认，可以大幅穿孔数据，从而可能提供巨大的计算和存储增益，而（聚类或PCA）性能几乎不变。这项初步研究因此开辟了从大维度角度重新思考基本机器学习模型中计算和存储成本的道路。",
        "领域": "谱聚类, 主成分分析, 高斯混合模型",
        "问题": "如何在谱聚类和主成分分析中减少计算和存储成本",
        "动机": "探索在大维度数据下，通过数据矩阵和核矩阵的随机穿孔，实现计算和存储效率的提升，同时保持性能不变",
        "方法": "通过伯努利掩码随机穿孔数据矩阵和核矩阵，研究其在大维度极限下的谱行为及其对性能的影响",
        "关键词": [
            "谱聚类",
            "主成分分析",
            "核矩阵穿孔",
            "高斯混合模型",
            "大维度数据"
        ],
        "涉及的技术概念": {
            "伯努利掩码": "用于随机选择数据矩阵和核矩阵中的元素进行穿孔，以减少计算和存储需求",
            "极限特征值分布": "研究在大维度极限下，穿孔核矩阵的特征值分布，以理解其性能表现"
        }
    },
    {
        "order": 1118,
        "title": "UCB Momentum Q-learning: Correcting the bias without forgetting",
        "html": "https://ICML.cc//virtual/2021/poster/8561",
        "abstract": "We propose UCBMQ, Upper Confidence Bound Momentum Q-learning, a new algorithm for reinforcement learning in tabular and possibly stage-dependent, episodic Markov decision process. UCBMQ is based on Q-learning where we add a momentum term and rely on the principle of optimism in face of uncertainty to deal with exploration.\nOur new technical ingredient of UCBMQ is the use of momentum to correct the bias that Q-learning suffers while, \\emph{at the same time}, limiting the impact it has on the second-order term of the regret.\nFor UCBMQ, we are able to guarantee a regret of at most $\\tilde{O}(\\sqrt{H^3SAT}+ H^4 S A)$ where $H$ is the length of an episode, $S$ the number of states, $A$ the number of actions, $T$ the number of episodes and ignoring terms in poly$\\log(SAHT)$.\nNotably, UCBMQ is the first algorithm that simultaneously matches the lower bound of $\\Omega(\\sqrt{H^3SAT})$ for large enough $T$ and has a second-order term (with respect to $T$) that scales \\emph{only linearly} with the number of states $S$.",
        "conference": "ICML",
        "success": true,
        "中文标题": "UCB动量Q学习：纠正偏差而不遗忘",
        "摘要翻译": "我们提出了UCBMQ，即上置信界动量Q学习，这是一种用于表格型和可能阶段依赖的、分幕马尔可夫决策过程中的强化学习新算法。UCBMQ基于Q学习，我们添加了一个动量项，并依靠面对不确定性时的乐观原则来处理探索问题。UCBMQ的新技术成分是使用动量来纠正Q学习所遭受的偏差，同时限制其对遗憾二阶项的影响。对于UCBMQ，我们能够保证最多为$\\tilde{O}(\\sqrt{H^3SAT}+ H^4 S A)$的遗憾，其中$H$是一幕的长度，$S$是状态的数量，$A$是动作的数量，$T$是幕的数量，忽略poly$\\log(SAHT)$中的项。值得注意的是，UCBMQ是第一个在足够大的$T$下同时匹配$\\Omega(\\sqrt{H^3SAT})$下界并且其二阶项（相对于$T$）仅与状态数量$S$成线性比例的算法。",
        "领域": "强化学习, 马尔可夫决策过程, 算法优化",
        "问题": "解决Q学习在强化学习中存在的偏差问题，同时控制对遗憾二阶项的影响。",
        "动机": "为了在强化学习中更有效地探索和利用，同时减少偏差对学习过程的影响。",
        "方法": "在Q学习算法中引入动量项，并应用面对不确定性时的乐观原则。",
        "关键词": [
            "UCBMQ",
            "动量Q学习",
            "强化学习",
            "马尔可夫决策过程",
            "算法优化"
        ],
        "涉及的技术概念": {
            "动量项": "用于纠正Q学习中的偏差，同时限制对遗憾二阶项的影响。",
            "上置信界": "一种面对不确定性时的乐观原则，用于指导探索策略。"
        }
    },
    {
        "order": 1119,
        "title": "Unbalanced minibatch Optimal Transport; applications to Domain Adaptation",
        "html": "https://ICML.cc//virtual/2021/poster/8555",
        "abstract": "Optimal transport distances have found many applications in machine learning for their capacity to compare non-parametric probability distributions. Yet their algorithmic complexity generally prevents their direct use on large scale datasets. Among the possible strategies to alleviate this issue, practitioners can rely on computing estimates of these distances over subsets of data, i.e. minibatches. While computationally appealing, we highlight in this paper some limits of this strategy, arguing it can lead to undesirable smoothing effects. As an alternative, we suggest that the same minibatch strategy coupled with unbalanced optimal transport can yield more robust behaviors. We discuss the associated theoretical properties, such as unbiased estimators, existence of gradients and concentration bounds. Our experimental study shows that in  challenging problems associated to domain adaptation, the use of unbalanced optimal transport leads to significantly better results, competing with or surpassing recent baselines.",
        "conference": "ICML",
        "中文标题": "不平衡小批量最优传输；在领域适应中的应用",
        "摘要翻译": "最优传输距离因其能够比较非参数概率分布而在机器学习中找到了许多应用。然而，它们的算法复杂性通常阻止了它们在大规模数据集上的直接使用。在缓解这一问题的可能策略中，从业者可以依赖于计算数据子集（即小批量）上这些距离的估计。虽然计算上吸引人，但我们在本文中强调了这一策略的一些限制，认为它可能导致不希望的平滑效果。作为替代方案，我们建议将相同的小批量策略与不平衡最优传输结合使用，可以产生更稳健的行为。我们讨论了相关的理论性质，如无偏估计、梯度的存在性和集中界限。我们的实验研究表明，在与领域适应相关的挑战性问题中，使用不平衡最优传输可以带来显著更好的结果，与最近的基线竞争或超越。",
        "领域": "领域适应、最优传输、机器学习",
        "问题": "最优传输距离在大规模数据集上的直接使用受到算法复杂性的限制，以及小批量策略可能导致的平滑效果问题。",
        "动机": "探索不平衡最优传输与小批量策略结合使用，以克服最优传输在大规模应用中的计算限制和平滑效果问题。",
        "方法": "提出将不平衡最优传输与小批量策略结合使用，讨论其理论性质，并通过实验验证其在领域适应问题中的有效性。",
        "关键词": [
            "不平衡最优传输",
            "小批量策略",
            "领域适应",
            "机器学习",
            "算法复杂性"
        ],
        "涉及的技术概念": {
            "最优传输距离": "用于比较非参数概率分布的距离度量，在机器学习中有广泛应用。",
            "小批量策略": "通过计算数据子集上的估计来缓解最优传输在大规模数据集上的计算复杂性。",
            "不平衡最优传输": "一种最优传输的变体，允许在传输过程中质量不守恒，用于提高模型的稳健性和适应性。"
        },
        "success": true
    },
    {
        "order": 1120,
        "title": "Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies",
        "html": "https://ICML.cc//virtual/2021/poster/10175",
        "abstract": "Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.",
        "conference": "ICML",
        "中文标题": "在展开计算图中使用持久进化策略进行无偏梯度估计",
        "摘要翻译": "展开的计算图出现在许多场景中，包括训练RNNs、通过展开优化调整超参数以及训练学习到的优化器。当前优化此类计算图中参数的方法存在梯度方差高、偏差、更新慢或内存使用大的问题。我们引入了一种称为持久进化策略（PES）的方法，它将计算图划分为一系列截断的展开，并在每次展开后执行基于进化策略的更新步骤。PES通过在整个展开序列中累积校正项，消除了这些截断带来的偏差。PES允许快速参数更新，内存使用低，无偏，并具有合理的方差特性。我们通过实验证明了PES在合成任务上与其他几种梯度估计方法相比的优势，并展示了其在训练学习到的优化器和调整超参数方面的适用性。",
        "领域": "优化算法、机器学习、深度学习",
        "问题": "解决在展开计算图中优化参数时遇到的高方差梯度、偏差、更新慢或内存使用大的问题",
        "动机": "为了提供一种更高效、更稳定的方法来优化展开计算图中的参数，减少梯度估计的偏差和方差，同时降低内存消耗",
        "方法": "引入持久进化策略（PES），将计算图划分为截断的展开序列，并在每次展开后执行基于进化策略的更新步骤，通过累积校正项消除截断偏差",
        "关键词": [
            "持久进化策略",
            "梯度估计",
            "展开计算图",
            "优化算法",
            "机器学习"
        ],
        "涉及的技术概念": {
            "持久进化策略（PES）": "一种将计算图划分为截断展开序列并在每次展开后执行基于进化策略的更新步骤的方法，旨在减少梯度估计的偏差和方差",
            "展开计算图": "指在训练RNNs、调整超参数或训练学习到的优化器等场景中出现的计算图结构，其特点是参数优化过程中需要处理的时间或步骤展开",
            "梯度估计": "在机器学习中，用于估计参数更新方向的技术，PES通过特定的策略减少估计过程中的偏差和方差，提高优化效率"
        },
        "success": true
    },
    {
        "order": 1121,
        "title": "Uncertainty Principles of Encoding GANs",
        "html": "https://ICML.cc//virtual/2021/poster/9053",
        "abstract": "The compelling synthesis results of Generative Adversarial Networks (GANs) demonstrate rich semantic knowledge in their latent codes. To obtain this knowledge for downstream applications,  encoding GANs has been proposed to learn encoders, such that real world data can be encoded to latent codes, which can be fed to generators to reconstruct those data. \nHowever, despite the theoretical guarantees of precise reconstruction in previous works, current algorithms generally reconstruct inputs with non-negligible deviations from inputs. In this paper we study this predicament of encoding GANs, which is indispensable research for the GAN community. We prove three uncertainty principles of encoding GANs in practice: a) the `perfect' encoder and generator cannot be continuous at the same time, which implies that current framework of encoding GANs is ill-posed and needs rethinking; b) neural networks cannot approximate the underlying encoder and generator precisely at the same time, which explains why we cannot get `perfect' encoders and generators as promised in previous theories; c) neural networks cannot be stable and accurate at the same time, which demonstrates the difficulty of training and trade-off between fidelity and disentanglement encountered in previous works. Our work may eliminate gaps between previous theories and empirical results, promote the understanding of GANs, and guide network designs for follow-up works.",
        "conference": "ICML",
        "中文标题": "编码生成对抗网络的不确定性原理",
        "摘要翻译": "生成对抗网络（GANs）引人注目的合成结果展示了其潜在代码中丰富的语义知识。为了将这些知识应用于下游任务，编码生成对抗网络被提出以学习编码器，使得现实世界的数据可以被编码为潜在代码，这些代码可以被输入到生成器以重建那些数据。然而，尽管先前的工作在理论上保证了精确重建，当前的算法通常重建的输入与原始输入存在不可忽视的偏差。在本文中，我们研究了编码生成对抗网络的这一困境，这对GAN社区来说是必不可少的研究。我们证明了编码生成对抗网络实践中的三个不确定性原理：a）'完美'的编码器和生成器不能同时连续，这意味着当前编码生成对抗网络的框架是不适定的，需要重新思考；b）神经网络不能同时精确逼近底层的编码器和生成器，这解释了为什么我们无法如先前理论所承诺的那样获得'完美'的编码器和生成器；c）神经网络不能同时稳定和准确，这展示了在先前工作中遇到的训练难度以及在保真度和解缠之间的权衡。我们的工作可能消除先前理论与实证结果之间的差距，促进对GANs的理解，并指导后续工作的网络设计。",
        "领域": "生成对抗网络、图像重建、深度学习理论",
        "问题": "编码生成对抗网络在实践中无法实现精确重建的问题",
        "动机": "探索编码生成对抗网络在实践中遇到的精确重建难题，理解理论与实证结果之间的差距",
        "方法": "通过理论分析，证明了编码生成对抗网络实践中的三个不确定性原理",
        "关键词": [
            "生成对抗网络",
            "编码器",
            "不确定性原理",
            "图像重建",
            "深度学习理论"
        ],
        "涉及的技术概念": {
            "编码生成对抗网络": "一种通过学习编码器将现实世界数据编码为潜在代码，进而通过生成器重建数据的技术",
            "不确定性原理": "在编码生成对抗网络实践中发现的三个基本原理，揭示了编码器和生成器在连续性、精确逼近和稳定性方面的限制",
            "神经网络逼近": "指神经网络在理论上能够逼近任何函数，但在实践中由于编码生成对抗网络的不确定性原理，无法同时精确逼近编码器和生成器"
        },
        "success": true
    },
    {
        "order": 1122,
        "title": "Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8869",
        "abstract": "Offline Reinforcement Learning promises to learn effective policies from previously-collected, static datasets without the need for exploration. However, existing Q-learning and actor-critic based off-policy RL algorithms fail when bootstrapping from out-of-distribution (OOD) actions or states. We hypothesize that a key missing ingredient from the existing methods is a proper treatment of uncertainty in the offline setting. We propose Uncertainty Weighted Actor-Critic (UWAC), an algorithm that detects OOD state-action pairs and down-weights their contribution in the training objectives accordingly. Implementation-wise, we adopt a practical and effective dropout-based uncertainty estimation method that introduces very little overhead over existing RL algorithms. Empirically, we observe that UWAC substantially improves model stability during training. In addition, UWAC out-performs existing offline RL methods on a variety of competitive tasks, and achieves significant performance gains over the state-of-the-art baseline on datasets with sparse demonstrations collected from human experts.",
        "conference": "ICML",
        "中文标题": "不确定性加权行动者-评论家离线强化学习算法",
        "摘要翻译": "离线强化学习承诺能够从先前收集的静态数据集中学习有效策略，而无需进行探索。然而，现有的基于Q学习和行动者-评论家的离线策略强化学习算法在从分布外（OOD）动作或状态进行自举时失败。我们假设现有方法中缺失的一个关键成分是对离线设置中不确定性的适当处理。我们提出了不确定性加权行动者-评论家（UWAC），这是一种检测OOD状态-动作对并在训练目标中相应降低其贡献的算法。在实现方面，我们采用了一种实用且有效的基于dropout的不确定性估计方法，该方法对现有RL算法的开销非常小。实证上，我们观察到UWAC显著提高了训练过程中的模型稳定性。此外，UWAC在各种竞争任务上优于现有的离线RL方法，并在由人类专家收集的稀疏演示数据集上实现了对最先进基线的显著性能提升。",
        "领域": "离线强化学习、深度强化学习、策略优化",
        "问题": "解决离线强化学习在分布外动作或状态自举时的失败问题",
        "动机": "现有离线强化学习方法在处理不确定性方面的不足，特别是在分布外状态-动作对的情况下",
        "方法": "提出不确定性加权行动者-评论家算法（UWAC），通过检测并降低分布外状态-动作对在训练目标中的贡献，采用基于dropout的不确定性估计方法",
        "关键词": [
            "离线强化学习",
            "不确定性加权",
            "行动者-评论家",
            "分布外动作",
            "dropout"
        ],
        "涉及的技术概念": {
            "不确定性加权": "在训练目标中根据不确定性调整分布外状态-动作对的权重，以减少其对学习过程的影响",
            "行动者-评论家算法": "一种结合了策略梯度方法和价值函数方法的强化学习算法，用于策略优化",
            "dropout-based不确定性估计": "通过随机丢弃网络中的部分连接来估计模型预测的不确定性，是一种实用且高效的方法"
        },
        "success": true
    },
    {
        "order": 1123,
        "title": "Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability",
        "html": "https://ICML.cc//virtual/2021/poster/8745",
        "abstract": "Knowledge transferability, or transfer learning, has been widely adopted to allow a pre-trained model in the source domain to be effectively adapted to downstream tasks in the target domain. It is thus important to explore and understand the factors affecting knowledge transferability. In this paper, as the first work, we analyze and demonstrate the connections between knowledge transferability and another important phenomenon--adversarial transferability, \\emph{i.e.}, adversarial examples generated against one model can be transferred to attack other models. Our theoretical studies show that adversarial transferability indicates knowledge transferability, and vice versa. Moreover, based on the theoretical insights, we propose two practical adversarial transferability metrics to characterize this process,  serving as bidirectional indicators between adversarial and knowledge transferability. We conduct extensive experiments for different scenarios on diverse datasets, showing a positive correlation between adversarial transferability and knowledge transferability. Our findings will shed light on future research about effective knowledge transfer learning and adversarial transferability analyses.",
        "conference": "ICML",
        "中文标题": "揭示对抗可转移性与知识可转移性之间的联系",
        "摘要翻译": "知识可转移性，或称迁移学习，已被广泛采用，使得在源领域预训练的模型能够有效地适应目标领域的下游任务。因此，探索和理解影响知识可转移性的因素变得尤为重要。在本文中，作为首次工作，我们分析并展示了知识可转移性与另一个重要现象——对抗可转移性之间的联系，即针对一个模型生成的对抗性示例可以被转移以攻击其他模型。我们的理论研究表明，对抗可转移性指示了知识可转移性，反之亦然。此外，基于理论见解，我们提出了两个实用的对抗可转移性度量来表征这一过程，作为对抗性和知识可转移性之间的双向指标。我们在不同数据集上的多种场景下进行了广泛的实验，展示了对抗可转移性与知识可转移性之间的正相关关系。我们的发现将为未来关于有效知识迁移学习和对抗可转移性分析的研究提供启示。",
        "领域": "迁移学习、对抗性攻击、深度学习安全",
        "问题": "探索和验证对抗可转移性与知识可转移性之间的联系",
        "动机": "理解影响知识可转移性的因素，以及对抗性攻击在模型间的转移机制",
        "方法": "理论分析对抗可转移性与知识可转移性的关系，并提出两个对抗可转移性度量作为双向指标",
        "关键词": [
            "对抗可转移性",
            "知识可转移性",
            "迁移学习",
            "深度学习安全",
            "对抗性攻击"
        ],
        "涉及的技术概念": {
            "对抗可转移性": "指对抗性示例能够从一个模型转移到另一个模型的现象，用于研究模型间的安全性和鲁棒性",
            "知识可转移性": "指预训练模型的知识能够有效迁移到不同任务或领域的能力，是迁移学习的核心",
            "迁移学习": "一种机器学习方法，通过利用源领域的知识来提高目标领域任务的性能"
        },
        "success": true
    },
    {
        "order": 1124,
        "title": "Understanding and Mitigating Accuracy Disparity in Regression",
        "html": "https://ICML.cc//virtual/2021/poster/10023",
        "abstract": "With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity on prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.",
        "conference": "ICML",
        "中文标题": "理解与缓解回归中的准确率差异",
        "摘要翻译": "随着大规模预测系统在高风险领域的广泛部署，例如人脸识别、刑事司法等，不同人口亚群之间预测准确率的差异引发了对这种差异来源的根本理解以及通过算法干预来缓解这种差异的需求。在本文中，我们研究了回归中的准确率差异问题。首先，我们提出了一个误差分解定理，该定理将准确率差异分解为边际标签分布之间的距离和条件表示之间的距离，以帮助解释为什么在实践中会出现这种准确率差异。受这种误差分解和利用统计距离进行分布对齐的一般思路的启发，我们随后提出了一种算法来减少这种差异，并分析了所提出目标函数的博弈论最优解。为了证实我们的理论发现，我们还在五个基准数据集上进行了实验。实验结果表明，我们提出的算法能够有效缓解准确率差异，同时保持回归模型的预测能力。",
        "领域": "公平机器学习、回归分析、算法公平性",
        "问题": "解决回归模型中不同人口亚群之间预测准确率的差异问题",
        "动机": "理解和减少高风险领域中大规模预测系统对不同人口亚群预测准确率的差异",
        "方法": "提出误差分解定理解释准确率差异的来源，并开发基于统计距离分布对齐的算法以减少差异",
        "关键词": [
            "准确率差异",
            "回归分析",
            "算法公平性",
            "误差分解",
            "分布对齐"
        ],
        "涉及的技术概念": {
            "误差分解定理": "将准确率差异分解为边际标签分布和条件表示之间的距离，解释差异来源",
            "统计距离": "用于衡量和减少不同亚群之间分布差异的技术",
            "博弈论最优解": "分析提出的目标函数在减少准确率差异时的最优解"
        },
        "success": true
    },
    {
        "order": 1125,
        "title": "Understanding Failures in Out-of-Distribution Detection with Deep Generative Models",
        "html": "https://ICML.cc//virtual/2021/poster/9421",
        "abstract": "Deep generative models (DGMs) seem a natural fit for detecting out-of-distribution (OOD) inputs, but such models have been shown to assign higher probabilities or densities to OOD images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that OOD detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for OOD detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based OOD detection and out-distributions of interest, and we illustrate how even minimal estimation error can lead to OOD detection failures, yielding implications for future work in\ndeep generative modeling and OOD detection.",
        "conference": "ICML",
        "中文标题": "理解深度生成模型在分布外检测中的失败",
        "摘要翻译": "深度生成模型（DGMs）看似是检测分布外（OOD）输入的自然选择，但此类模型已被证明会为OOD图像分配比训练分布中的图像更高的概率或密度。在这项工作中，我们解释了为什么这种行为应归因于模型的错误估计。我们首先证明，在没有对相关分布外做出假设的情况下，任何方法都无法保证性能超越随机机会。然后，我们探讨了典型集假设，即相关分布外可以位于数据分布的高似然区域，并且OOD检测应基于数据分布的典型集来定义。我们强调了假设内分布和分布外之间存在支持重叠的后果，以及典型集对于OOD检测的任意性。我们的结果表明，估计误差是一个比基于似然的OOD检测与感兴趣的分布外之间的不对齐更合理的解释，并且我们说明了即使是最小的估计误差也可能导致OOD检测失败，这对深度生成建模和OOD检测的未来工作产生了影响。",
        "领域": "深度生成模型、异常检测、概率模型",
        "问题": "解释深度生成模型在分布外检测中失败的原因",
        "动机": "探究为何深度生成模型会错误地为分布外输入分配高概率，并寻找更合理的解释",
        "方法": "通过理论证明和实验分析，探讨模型错误估计和典型集假设的影响",
        "关键词": [
            "深度生成模型",
            "分布外检测",
            "典型集假设",
            "模型错误估计",
            "概率模型"
        ],
        "涉及的技术概念": {
            "深度生成模型": "用于生成数据和检测分布外输入的概率模型",
            "分布外检测": "识别不属于训练数据分布的输入的技术",
            "典型集假设": "认为相关分布外输入位于数据分布的高似然区域的假设"
        },
        "success": true
    },
    {
        "order": 1126,
        "title": "Understanding Instance-Level Label Noise: Disparate Impacts and Treatments",
        "html": "https://ICML.cc//virtual/2021/poster/10229",
        "abstract": "This paper aims to provide understandings for the effect of an over-parameterized model, e.g. a deep neural network, memorizing instance-dependent noisy labels. We first quantify the harms caused by memorizing noisy instances, and show the disparate impacts of noisy labels for sample instances with different representation frequencies.  We then analyze how several popular solutions for learning with noisy labels mitigate this harm at the instance level. Our analysis reveals that existing approaches lead to disparate treatments when handling noisy instances. While higher-frequency instances often enjoy a high probability of an improvement by applying these solutions, lower-frequency instances do not. Our analysis reveals new understandings for when these approaches work, and provides theoretical justifications for previously reported empirical observations. This observation requires us to rethink the distribution of label noise across instances and calls for different treatments for instances in different regimes.",
        "conference": "ICML",
        "中文标题": "理解实例级标签噪声：不同影响与处理方式",
        "摘要翻译": "本文旨在提供对过参数化模型（如深度神经网络）记忆实例依赖噪声标签效应的理解。我们首先量化了记忆噪声实例造成的危害，并展示了噪声标签对不同表示频率样本实例的不同影响。然后，我们分析了学习带有噪声标签的几种流行解决方案如何在实例级别减轻这种危害。我们的分析揭示了现有方法在处理噪声实例时导致的不同处理方式。虽然较高频率的实例通常通过应用这些解决方案享有较高的改进概率，但较低频率的实例则不然。我们的分析揭示了这些方法何时有效的新理解，并为先前报告的实证观察提供了理论依据。这一观察要求我们重新思考标签噪声在实例间的分布，并呼吁对不同制度下的实例采取不同的处理方式。",
        "领域": "深度学习噪声鲁棒性、标签噪声处理、实例级学习",
        "问题": "研究过参数化模型记忆实例依赖噪声标签的影响，以及现有噪声标签学习方法对不同频率实例的不同处理效果。",
        "动机": "理解噪声标签对模型性能的影响，特别是对不同表示频率实例的差异化影响，以及现有解决方案在处理这些影响时的局限性。",
        "方法": "量化噪声实例记忆的危害，分析不同噪声标签学习解决方案在实例级别的效果，揭示现有方法的差异化处理现象。",
        "关键词": [
            "实例级标签噪声",
            "过参数化模型",
            "噪声标签学习",
            "表示频率",
            "差异化处理"
        ],
        "涉及的技术概念": {
            "过参数化模型": "指参数数量远大于训练样本数量的模型，如深度神经网络，能够记忆训练数据中的噪声。",
            "实例依赖噪声标签": "指噪声标签的分布依赖于实例的特征，不同实例可能有不同的噪声模式。",
            "表示频率": "指实例在特征空间中的出现频率，高频实例和低频实例在噪声标签学习过程中可能受到不同的影响。"
        },
        "success": true
    },
    {
        "order": 1127,
        "title": "Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers",
        "html": "https://ICML.cc//virtual/2021/poster/10083",
        "abstract": "A discriminatively trained neural net classifier can fit the training data perfectly if all information about its  input other than class membership has been discarded prior to the output layer. Surprisingly,  past research has discovered that some extraneous visual detail remains in the  unnormalized logits. This finding is based on inversion techniques that map deep embeddings back to images.  We explore this phenomenon further using a novel synthesis of methods, yielding a feedforward inversion model that produces remarkably high fidelity reconstructions, qualitatively superior to those of past efforts. When applied to an adversarially robust classifier model, the reconstructions contain sufficient local detail and  global structure that they might be confused with the original image in a quick glance, and the object  category can clearly be gleaned from the reconstruction. Our approach is based on BigGAN (Brock, 2019), with  conditioning on logits instead of one-hot class labels. We use our reconstruction model as a tool for exploring the nature of representations, including: the influence of model architecture and training objectives (specifically robust losses), the forms of invariance that networks achieve, representational differences between correctly and incorrectly classified images, and the effects of manipulating logits and images. We believe that our method can inspire future investigations into the nature of information flow in a neural net and can provide diagnostics for improving discriminative models. We provide pre-trained models and visualizations at \\url{https://sites.google.com/view/understanding-invariance/home}.",
        "conference": "ICML",
        "中文标题": "通过前馈反转判别训练分类器理解不变性",
        "摘要翻译": "如果判别训练的神经网络分类器在输出层之前丢弃了除类别成员之外的所有输入信息，那么它可以完美地拟合训练数据。令人惊讶的是，过去的研究发现，在未归一化的logits中仍然保留了一些无关的视觉细节。这一发现基于将深度嵌入映射回图像的反转技术。我们使用一种新颖的方法综合进一步探索了这一现象，产生了一个前馈反转模型，该模型产生了显著高保真度的重建，质量上优于过去的努力。当应用于对抗性鲁棒分类器模型时，重建包含足够的局部细节和全局结构，以至于在快速浏览时可能会与原始图像混淆，并且可以从重建中清晰地识别出对象类别。我们的方法基于BigGAN（Brock，2019），以logits而非one-hot类别标签为条件。我们使用我们的重建模型作为探索表示性质的工具，包括：模型架构和训练目标（特别是鲁棒损失）的影响、网络实现的不变性形式、正确和错误分类图像之间的表示差异，以及操作logits和图像的效果。我们相信，我们的方法可以激发未来对神经网络中信息流性质的调查，并可以为改进判别模型提供诊断。我们在https://sites.google.com/view/understanding-invariance/home提供了预训练模型和可视化。",
        "领域": "图像重建、对抗性鲁棒性、深度表示学习",
        "问题": "探索判别训练分类器中保留的视觉细节信息及其对图像重建的影响",
        "动机": "研究神经网络分类器在训练过程中保留的视觉信息，以及这些信息如何影响图像的重建和理解",
        "方法": "基于BigGAN构建前馈反转模型，以logits为条件进行图像重建，探索不同条件下的重建效果",
        "关键词": [
            "图像重建",
            "对抗性鲁棒性",
            "深度表示学习",
            "前馈反转",
            "BigGAN"
        ],
        "涉及的技术概念": {
            "前馈反转模型": "用于从分类器的logits重建图像，探索分类器中保留的视觉信息",
            "BigGAN": "一种生成对抗网络，用于高质量图像生成，在本研究中用于图像重建",
            "对抗性鲁棒性": "分类器对对抗性攻击的抵抗能力，影响重建图像的质量和细节"
        },
        "success": true
    },
    {
        "order": 1128,
        "title": "Understanding Noise Injection in GANs",
        "html": "https://ICML.cc//virtual/2021/poster/8863",
        "abstract": " Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising effectiveness when\n  generating high-fidelity images in Generative Adversarial Networks (GANs) (e.g. StyleGAN). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. First, we point out the existence of the adversarial dimension trap inherent in GANs, which leads to the difficulty of learning a proper generator. Second, we successfully model the noise injection framework with exponential maps based on Riemannian geometry. Guided by our theories, we propose a general geometric realization for noise injection. Under our novel framework, the simple noise injection used in StyleGAN reduces to the Euclidean case. The goal of our work is to make theoretical steps towards understanding  the underlying mechanism of state-of-the-art GAN algorithms.  Experiments on image generation and GAN inversion validate our theory in practice.",
        "conference": "ICML",
        "中文标题": "理解生成对抗网络中的噪声注入",
        "摘要翻译": "噪声注入是避免过拟合和增强机器学习泛化能力的有效方法，其原理在深度学习中也得到了验证。最近，噪声注入在生成对抗网络（GANs）（如StyleGAN）中生成高保真图像时显示出惊人的效果。尽管在GANs中应用成功，但其有效性机制仍不明确。本文提出了一个几何框架，从理论上分析噪声注入在GANs中的作用。首先，我们指出了GANs中固有的对抗维度陷阱的存在，这导致学习一个合适的生成器变得困难。其次，我们成功地用基于黎曼几何的指数映射对噪声注入框架进行了建模。在我们的理论指导下，我们提出了噪声注入的一般几何实现。在我们的新框架下，StyleGAN中使用的简单噪声注入简化为欧几里得情况。我们工作的目标是在理解最先进的GAN算法底层机制方面迈出理论步骤。图像生成和GAN反转的实验验证了我们的理论在实践中是有效的。",
        "领域": "生成对抗网络、图像生成、深度学习理论",
        "问题": "噪声注入在生成对抗网络中的有效性机制不明确",
        "动机": "理解噪声注入在生成对抗网络中生成高保真图像的有效性机制",
        "方法": "提出一个几何框架，从理论上分析噪声注入在GANs中的作用，并通过实验验证",
        "关键词": [
            "噪声注入",
            "生成对抗网络",
            "几何框架",
            "图像生成",
            "GAN反转"
        ],
        "涉及的技术概念": {
            "对抗维度陷阱": "GANs中固有的问题，导致学习一个合适的生成器变得困难",
            "指数映射": "基于黎曼几何，用于建模噪声注入框架",
            "欧几里得情况": "在提出的新框架下，StyleGAN中使用的简单噪声注入简化为欧几里得情况"
        },
        "success": true
    },
    {
        "order": 1129,
        "title": "Understanding self-supervised learning dynamics without contrastive pairs",
        "html": "https://ICML.cc//virtual/2021/poster/10403",
        "abstract": "While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent \\emph{non-contrastive} SSL (e.g., BYOL and SimSiam) show remarkable performance {\\it without} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question rises: why they do not collapse into trivial representation? In this paper, we answer this question via a simple theoretical study and propose a novel approach, \\ourmethod{}, that \\emph{directly} sets the linear predictor based on the statistics of its inputs, rather than trained with gradient update. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms linear predictor by $2.5\\%$ in 300-epoch training (and $5\\%$ in 60-epoch). \\ourmethod{} is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released\\footnote{\\url{https://github.com/facebookresearch/luckmatters/tree/master/ssl}}.",
        "conference": "ICML",
        "success": true,
        "中文标题": "理解无需对比对的自监督学习动态",
        "摘要翻译": "尽管自监督学习（SSL）的对比方法通过最小化同一数据点的两个增强视图（正对）之间的距离和最大化来自不同数据点的视图（负对）之间的距离来学习表示，但最近的非对比SSL（例如BYOL和SimSiam）在没有负对的情况下显示出显著的性能，这得益于一个额外的可学习预测器和停止梯度操作。一个基本问题随之而来：为什么它们不会崩溃为平凡的表示？在本文中，我们通过一个简单的理论研究回答了这个问题，并提出了一种新方法，我们的方法，它直接基于其输入的统计数据设置线性预测器，而不是通过梯度更新进行训练。在ImageNet上，它的表现与使用BatchNorm的更复杂的双层非线性预测器相当，并且在300轮训练中比线性预测器高出2.5%（在60轮训练中高出5%）。我们的方法受到我们对简单线性网络中非对比SSL的非线性学习动态的理论研究的启发。我们的研究提供了关于非对比SSL方法如何学习、如何避免表示崩溃以及预测器网络、停止梯度、指数移动平均和权重衰减等多个因素如何发挥作用的概念性见解。我们的简单理论重现了STL-10和ImageNet中真实世界消融研究的结果。代码已发布。",
        "领域": "自监督学习、深度学习、计算机视觉",
        "问题": "非对比自监督学习方法为何不会崩溃为平凡的表示",
        "动机": "探索非对比自监督学习方法的学习动态及其避免表示崩溃的机制",
        "方法": "提出一种基于输入统计数据直接设置线性预测器的新方法，而非通过梯度更新训练",
        "关键词": [
            "自监督学习",
            "非对比学习",
            "表示学习",
            "线性预测器",
            "学习动态"
        ],
        "涉及的技术概念": {
            "非对比SSL": "一种不依赖于负对的自监督学习方法，通过其他机制避免表示崩溃",
            "停止梯度操作": "在训练过程中阻止梯度流向某些部分，以避免表示崩溃"
        }
    },
    {
        "order": 1130,
        "title": "Understanding the Dynamics of Gradient Flow in Overparameterized Linear models",
        "html": "https://ICML.cc//virtual/2021/poster/9573",
        "abstract": "We provide a detailed analysis of the dynamics ofthe gradient flow in overparameterized two-layerlinear models. A particularly interesting featureof this model is that its nonlinear dynamics can beexactly solved as a consequence of a large num-ber of conservation laws that constrain the systemto follow particular trajectories. More precisely,the gradient flow preserves the difference of theGramian matrices of the input and output weights,and its convergence to equilibrium depends onboth the magnitude of that difference (which isfixed at initialization) and the spectrum of the data.In addition, and generalizing prior work, we proveour results without assuming small, balanced orspectral initialization for the weights. Moreover,we establish interesting mathematical connectionsbetween matrix factorization problems and differ-ential equations of the Riccati type.",
        "conference": "ICML",
        "中文标题": "理解过参数化线性模型中梯度流的动态",
        "摘要翻译": "我们提供了对过参数化两层线性模型中梯度流动力的详细分析。该模型的一个特别有趣的特征是，由于大量守恒定律约束系统遵循特定轨迹，其非线性动力学可以被精确求解。更准确地说，梯度流保持了输入和输出权重的Gramian矩阵的差异，并且其向平衡的收敛取决于该差异的大小（在初始化时固定）和数据的谱。此外，并且推广了先前的工作，我们在不假设权重的小、平衡或谱初始化的情况下证明了我们的结果。此外，我们建立了矩阵分解问题与Riccati型微分方程之间有趣的数学联系。",
        "领域": "深度学习理论、优化算法、线性模型",
        "问题": "分析过参数化两层线性模型中梯度流的动态特性及其收敛行为",
        "动机": "探索过参数化线性模型中梯度流的非线性动力学特性，以及这些特性如何影响模型的训练动态和收敛行为",
        "方法": "通过分析梯度流在过参数化两层线性模型中的动态，利用守恒定律精确求解非线性动力学，并研究其对模型收敛的影响",
        "关键词": [
            "梯度流",
            "过参数化模型",
            "线性模型",
            "守恒定律",
            "Riccati方程"
        ],
        "涉及的技术概念": {
            "梯度流": "在优化过程中，参数沿着损失函数梯度的负方向更新的连续时间动态",
            "过参数化模型": "指参数数量远大于训练样本数量的模型，这类模型在深度学习中常见，能够拟合训练数据到零误差",
            "Riccati方程": "一类非线性微分方程，在控制理论和矩阵分解问题中有广泛应用，本文中用于描述梯度流的动态"
        },
        "success": true
    },
    {
        "order": 1131,
        "title": "UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10297",
        "abstract": "VDN and QMIX are two popular value-based algorithms for cooperative MARL that learn a centralized action value function as a monotonic mixing of per-agent utilities. While this enables easy decentralization of the learned policy, the restricted joint action value function can prevent them from solving tasks that require significant coordination between agents at a given timestep. We show that this problem can be overcome by improving the joint exploration of all agents during training.  Specifically, we propose a novel MARL approach called Universal Value Exploration (UneVEn) that learns a set of related tasks simultaneously with a linear decomposition of universal successor features. With the policies of already solved related tasks, the joint exploration process of all agents can be improved to help them achieve better coordination. Empirical results on a set of exploration games, challenging cooperative predator-prey tasks requiring significant coordination among agents, and StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where other state-of-the-art MARL methods fail. ",
        "conference": "ICML",
        "中文标题": "UneVEn：多智能体强化学习的通用价值探索",
        "摘要翻译": "VDN和QMIX是两种流行的基于价值的合作多智能体强化学习（MARL）算法，它们学习一个集中式动作价值函数作为每个智能体效用的单调混合。虽然这使得学习到的策略易于分散化，但受限的联合动作价值函数可能阻止它们解决需要在给定时间步上智能体之间进行显著协调的任务。我们表明，通过改进训练期间所有智能体的联合探索可以克服这个问题。具体来说，我们提出了一种名为通用价值探索（UneVEn）的新型MARL方法，该方法通过通用后继特征的线性分解同时学习一组相关任务。利用已经解决的相关任务的策略，可以改进所有智能体的联合探索过程，以帮助它们实现更好的协调。在一组探索游戏、需要智能体之间显著协调的具有挑战性的合作捕食者-猎物任务以及StarCraft II微管理基准测试上的实证结果表明，UneVEn可以解决其他最先进的MARL方法无法解决的任务。",
        "领域": "多智能体强化学习、合作任务解决、智能体协调",
        "问题": "解决在需要智能体之间显著协调的任务中，现有基于价值的MARL算法因受限的联合动作价值函数而无法有效工作的问题。",
        "动机": "通过改进所有智能体在训练期间的联合探索，克服现有算法在需要高度协调的任务中的局限性。",
        "方法": "提出通用价值探索（UneVEn）方法，通过线性分解通用后继特征同时学习一组相关任务，利用已解决任务的策略改进智能体的联合探索。",
        "关键词": [
            "多智能体强化学习",
            "通用价值探索",
            "智能体协调",
            "后继特征",
            "合作任务"
        ],
        "涉及的技术概念": {
            "通用后继特征": "用于同时学习一组相关任务的技术，通过线性分解实现，有助于改进智能体的联合探索。",
            "联合探索": "指所有智能体在训练期间共同探索环境的过程，UneVEn通过利用已解决任务的策略来优化这一过程。",
            "单调混合": "VDN和QMIX算法中用于组合单个智能体效用以形成集中式动作价值函数的方法，限制了在需要高度协调任务中的应用。"
        },
        "success": true
    },
    {
        "order": 1132,
        "title": "UnICORNN: A recurrent model for learning very long time dependencies",
        "html": "https://ICML.cc//virtual/2021/poster/10541",
        "abstract": "The design of recurrent neural networks (RNNs) to accurately process sequential inputs with long-time dependencies is very challenging on account of the exploding and vanishing gradient problem. To overcome this, we propose a novel RNN architecture which is based on a structure preserving discretization of a Hamiltonian system of second-order ordinary differential equations that models networks of oscillators. The resulting RNN is fast, invertible (in time), memory efficient and we derive rigorous bounds on the hidden state gradients to prove the mitigation of the exploding and vanishing gradient problem. A suite of experiments are presented to demonstrate that the proposed RNN provides state of the art performance on a variety of learning tasks with (very) long-time dependencies.",
        "conference": "ICML",
        "中文标题": "UnICORNN：一种用于学习极长时间依赖性的循环模型",
        "摘要翻译": "设计能够准确处理具有长时间依赖性序列输入的循环神经网络（RNN）非常具有挑战性，这主要是由于梯度爆炸和消失问题。为了克服这一问题，我们提出了一种新颖的RNN架构，该架构基于对模拟振荡器网络的二阶常微分方程哈密顿系统的结构保持离散化。所得到的RNN快速、可逆（在时间上）、内存高效，并且我们推导了隐藏状态梯度的严格界限，以证明梯度爆炸和消失问题的缓解。通过一系列实验展示，所提出的RNN在多种具有（极）长时间依赖性的学习任务上提供了最先进的性能。",
        "领域": "循环神经网络、时间序列分析、深度学习优化",
        "问题": "解决循环神经网络在处理长时间依赖性序列时的梯度爆炸和消失问题",
        "动机": "为了克服传统RNN在处理长时间依赖性序列时遇到的梯度爆炸和消失问题，提出一种新型RNN架构以提高性能",
        "方法": "基于二阶常微分方程哈密顿系统的结构保持离散化，设计了一种新型RNN架构，该架构具有快速、可逆和内存高效的特点，并通过实验验证了其有效性",
        "关键词": [
            "循环神经网络",
            "长时间依赖性",
            "哈密顿系统",
            "梯度问题",
            "结构保持离散化"
        ],
        "涉及的技术概念": {
            "循环神经网络（RNN）": "用于处理序列数据的神经网络架构，能够捕捉时间序列中的依赖性",
            "哈密顿系统": "一种物理系统的数学模型，用于描述系统的总能量守恒，本文中用于设计新型RNN架构",
            "梯度爆炸和消失问题": "在训练深度神经网络时，梯度可能会指数级增长或减小，导致训练困难，本文提出的方法旨在缓解这一问题"
        },
        "success": true
    },
    {
        "order": 1133,
        "title": "Unified Robust Semi-Supervised Variational Autoencoder",
        "html": "https://ICML.cc//virtual/2021/poster/9043",
        "abstract": "In this paper, we propose a novel noise-robust\nsemi-supervised deep generative model by jointly tackling noisy\nlabels and outliers simultaneously in a unified robust semi-supervised variational\nautoencoder (URSVAE).\nTypically, the uncertainty of\nof input data is characterized by placing uncertainty prior on the\nparameters of the probability density distributions in order to ensure the robustness of the\nvariational encoder towards outliers. Subsequently, a\nnoise transition model is integrated naturally into our model to alleviate the detrimental effects of noisy labels. Moreover, \na robust divergence measure is employed to further enhance the robustness, where a novel\nvariational lower bound is derived and optimized to infer the network parameters. By \nproving the influence function on the proposed evidence lower bound is bounded, the enormous potential of the proposed \nmodel in the classification in the presence of the compound noise is demonstrated.\nThe experimental results highlight the superiority of the proposed\nframework by the evaluating on image classification tasks and comparing with the state-of-the-art\napproaches.",
        "conference": "ICML",
        "中文标题": "统一鲁棒半监督变分自编码器",
        "摘要翻译": "本文提出了一种新颖的噪声鲁棒半监督深度生成模型，通过在一个统一的鲁棒半监督变分自编码器（URSVAE）中同时处理噪声标签和异常值。通常，输入数据的不确定性通过在概率密度分布的参数上放置不确定性先验来表征，以确保变分编码器对异常值的鲁棒性。随后，一个噪声转换模型被自然地集成到我们的模型中，以减轻噪声标签的有害影响。此外，采用了一种鲁棒的散度度量来进一步增强鲁棒性，其中推导并优化了一个新颖的变分下界来推断网络参数。通过证明所提出的证据下界的影响函数是有界的，展示了所提出模型在存在复合噪声的分类中的巨大潜力。实验结果通过在图像分类任务上的评估和与最先进方法的比较，突出了所提出框架的优越性。",
        "领域": "半监督学习、变分自编码器、噪声鲁棒学习",
        "问题": "解决在半监督学习环境下，模型对噪声标签和异常值的鲁棒性问题。",
        "动机": "提高深度生成模型在噪声环境下的分类性能和鲁棒性。",
        "方法": "提出了一种统一的鲁棒半监督变分自编码器（URSVAE），通过不确定性先验、噪声转换模型和鲁棒散度度量来增强模型的鲁棒性。",
        "关键词": [
            "半监督学习",
            "变分自编码器",
            "噪声鲁棒",
            "深度生成模型",
            "图像分类"
        ],
        "涉及的技术概念": {
            "不确定性先验": "用于表征输入数据的不确定性，确保变分编码器对异常值的鲁棒性。",
            "噪声转换模型": "集成到模型中，以减轻噪声标签的有害影响。",
            "鲁棒散度度量": "用于进一步增强模型的鲁棒性，通过优化变分下界来推断网络参数。"
        },
        "success": true
    },
    {
        "order": 1134,
        "title": "Uniform Convergence, Adversarial Spheres and a Simple Remedy",
        "html": "https://ICML.cc//virtual/2021/poster/8795",
        "abstract": "Previous work has cast doubt on the general framework of uniform convergence and its ability to explain generalization in neural networks. By considering a specific dataset, it was observed that a neural network completely misclassifies a projection of the training data (adversarial set), rendering any existing generalization bound based on uniform convergence vacuous. We provide an extensive theoretical investigation of the\npreviously studied data setting through the lens of infinitely-wide models. We prove that the Neural Tangent Kernel (NTK) also suffers from the same phenomenon and we uncover its origin. We highlight the important role of the output bias and show theoretically as well as empirically how a sensible choice completely mitigates the problem. We identify sharp phase transitions in the accuracy on the adversarial set and study its dependency on the training sample size. As a result, we are able to characterize critical sample sizes beyond which the effect disappears. Moreover, we study decompositions of a neural network into a clean and noisy part by considering its canonical decomposition into its different eigenfunctions and show empirically that for too small bias the adversarial phenomenon still persists.",
        "conference": "ICML",
        "中文标题": "一致收敛、对抗球面及简单补救措施",
        "摘要翻译": "先前的工作对一致收敛的通用框架及其解释神经网络泛化能力的能力提出了质疑。通过考虑一个特定的数据集，观察到神经网络完全错误分类了训练数据的一个投影（对抗集），使得任何基于一致收敛的现有泛化界限变得空洞。我们通过无限宽模型的视角，对先前研究的数据设置进行了广泛的理论研究。我们证明了神经切线核（NTK）也遭受了同样的现象，并揭示了其根源。我们强调了输出偏差的重要作用，并从理论上和实证上展示了如何通过明智的选择完全缓解这一问题。我们在对抗集的准确度上识别了尖锐的相变，并研究了其对训练样本大小的依赖性。结果，我们能够描述临界样本大小，超过这个大小，效应就会消失。此外，我们通过考虑神经网络到其不同特征函数的规范分解，研究了神经网络到干净和噪声部分的分解，并实证表明，对于太小的偏差，对抗现象仍然存在。",
        "领域": "深度学习理论、对抗机器学习、神经网络泛化",
        "问题": "研究神经网络在特定数据集上的泛化能力问题，特别是对抗集上的错误分类问题。",
        "动机": "揭示神经网络在对抗集上错误分类的根源，并提出有效的缓解措施。",
        "方法": "通过无限宽模型的理论研究，分析神经切线核的行为，探讨输出偏差的作用，并通过实证研究验证理论发现。",
        "关键词": [
            "一致收敛",
            "对抗机器学习",
            "神经切线核",
            "输出偏差",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "一致收敛": "研究神经网络泛化能力的理论框架，用于解释模型在训练集和测试集上性能的一致性。",
            "神经切线核（NTK）": "在无限宽神经网络中，描述网络训练动态的核函数，用于分析网络的收敛性和泛化行为。",
            "输出偏差": "神经网络输出层的一个参数，研究显示其选择对缓解对抗集上的错误分类问题具有关键作用。"
        },
        "success": true
    },
    {
        "order": 1135,
        "title": "Unifying Vision-and-Language Tasks via Text Generation",
        "html": "https://ICML.cc//virtual/2021/poster/9961",
        "abstract": "Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5",
        "conference": "ICML",
        "中文标题": "通过文本生成统一视觉与语言任务",
        "摘要翻译": "现有的视觉与语言学习方法通常需要为每个任务设计特定的架构和目标。例如，视觉问答的多标签答案分类器、指代表达理解的区域评分器以及图像描述的语言解码器等。为了减轻这些麻烦，在这项工作中，我们提出了一个统一的框架，该框架在单一架构中以相同的语言建模目标学习不同的任务，即多模态条件文本生成，其中我们的模型学习基于视觉和文本输入生成文本标签。在7个流行的视觉与语言基准测试上，包括视觉问答、指代表达理解、视觉常识推理等，其中大多数以前被建模为判别任务，我们的生成方法（使用单一统一架构）达到了与最近特定任务的最先进视觉与语言模型相当的性能。此外，我们的生成方法在答案罕见的问题上显示出更好的泛化能力。同时，我们展示了我们的框架允许在单一架构中使用单一参数集进行多任务学习，达到与单独优化的单任务模型相似的性能。我们的代码公开在：https://github.com/j-min/VL-T5",
        "领域": "视觉问答、指代表达理解、视觉常识推理",
        "问题": "如何统一处理多种视觉与语言任务，避免为每个任务设计特定的架构和目标",
        "动机": "减轻为每个视觉与语言任务设计特定架构和目标的麻烦，提高模型的泛化能力和多任务学习效率",
        "方法": "提出一个统一的框架，通过多模态条件文本生成在单一架构中以相同的语言建模目标学习不同的任务",
        "关键词": [
            "统一框架",
            "多模态条件文本生成",
            "视觉与语言任务",
            "多任务学习",
            "泛化能力"
        ],
        "涉及的技术概念": {
            "多模态条件文本生成": "基于视觉和文本输入生成文本标签的技术，用于统一处理多种视觉与语言任务",
            "语言建模目标": "在统一框架中用于学习不同任务的共同目标，通过生成文本来实现",
            "单一架构多任务学习": "在单一架构中使用单一参数集同时学习多个任务，提高学习效率和模型性能"
        },
        "success": true
    },
    {
        "order": 1136,
        "title": "UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data",
        "html": "https://ICML.cc//virtual/2021/poster/10507",
        "abstract": "In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both labeled and unlabeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture information more correlated with phonetic structures and improve the generalization across languages and domains.\nWe evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4\\% and 26.9\\% relative phone error rate reductions respectively  (averaged over all testing languages). The transferability of UniSpeech is also verified on a domain-shift speech recognition task, i.e., a relative word error rate reduction of 6\\% against the previous approach.",
        "conference": "ICML",
        "中文标题": "UniSpeech：利用标记与未标记数据统一学习语音表示",
        "摘要翻译": "本文提出了一种名为UniSpeech的统一预训练方法，旨在通过标记和未标记数据学习语音表示。该方法以多任务学习的方式，结合了监督的音素CTC学习和音素感知的对比自监督学习。由此产生的表示能够捕捉与音素结构更相关的信息，并提高跨语言和跨领域的泛化能力。我们在公开的CommonVoice语料库上评估了UniSpeech在跨语言表示学习中的有效性。结果表明，UniSpeech在语音识别任务中，相对于自监督预训练和监督迁移学习，分别最大减少了13.4%和26.9%的相对音素错误率（在所有测试语言上平均）。UniSpeech的可迁移性也在一个领域转移的语音识别任务上得到了验证，即相对于之前的方法，实现了6%的相对词错误率减少。",
        "领域": "语音识别、自监督学习、跨语言学习",
        "问题": "如何有效利用标记和未标记数据进行语音表示学习，以提高语音识别的跨语言和跨领域泛化能力",
        "动机": "探索一种统一的学习框架，结合监督学习和自监督学习的优势，以提升语音表示的质量和泛化能力",
        "方法": "提出UniSpeech方法，通过多任务学习结合监督的音素CTC学习和音素感知的对比自监督学习，学习统一的语音表示",
        "关键词": [
            "语音表示学习",
            "多任务学习",
            "跨语言语音识别",
            "自监督学习",
            "音素感知"
        ],
        "涉及的技术概念": {
            "音素CTC学习": "用于监督学习部分，通过连接时序分类损失优化模型对音素序列的预测",
            "音素感知的对比自监督学习": "在自监督学习部分引入，通过对比学习增强模型对音素结构的感知能力",
            "多任务学习": "将监督学习和自监督学习结合在一个统一的框架中，共同优化语音表示"
        },
        "success": true
    },
    {
        "order": 1137,
        "title": "Unitary Branching Programs: Learnability and Lower Bounds",
        "html": "https://ICML.cc//virtual/2021/poster/10679",
        "abstract": "Bounded width branching programs are a formalism that can be used to capture the notion of non-uniform constant-space computation. In this work, we study a generalized version of bounded width branching programs where instructions are defined by unitary matrices of bounded dimension. We introduce a new learning framework for these branching programs that leverages on a combination of local search techniques with gradient descent over Riemannian manifolds. We also show that gapped, read-once branching programs of bounded dimension can be learned with a polynomial number of queries in the presence of a teacher. Finally, we provide explicit near-quadratic size lower-bounds for bounded-dimension unitary branching programs, and exponential size lower-bounds for bounded-dimension read-once gapped unitary branching programs. The first lower bound is proven using a combination of Neciporuk’s lower bound technique with classic results from algebraic geometry. The second lower bound is proven within the framework of communication complexity theory. ",
        "conference": "ICML",
        "中文标题": "酉分支程序：可学习性与下界",
        "摘要翻译": "有界宽度分支程序是一种可以捕捉非均匀常数空间计算概念的形式化方法。在这项工作中，我们研究了一种广义版本的有界宽度分支程序，其中指令由有界维度的酉矩阵定义。我们为这些分支程序引入了一个新的学习框架，该框架利用了局部搜索技术与黎曼流形上的梯度下降相结合的方法。我们还展示了在有教师的情况下，有界维度的间隙、一次性读取分支程序可以通过多项式数量的查询来学习。最后，我们为有界维度酉分支程序提供了明确的近二次大小下界，并为有界维度一次性读取间隙酉分支程序提供了指数大小下界。第一个下界是通过将Neciporuk的下界技术与代数几何的经典结果相结合来证明的。第二个下界是在通信复杂性理论的框架内证明的。",
        "领域": "计算复杂性理论, 量子计算, 机器学习",
        "问题": "研究有界宽度分支程序的可学习性及其计算下界",
        "动机": "探索广义有界宽度分支程序的学习框架和计算能力限制",
        "方法": "结合局部搜索技术和黎曼流形上的梯度下降，以及利用Neciporuk的下界技术和代数几何结果",
        "关键词": [
            "酉分支程序",
            "可学习性",
            "计算下界",
            "黎曼流形",
            "通信复杂性"
        ],
        "涉及的技术概念": {
            "酉矩阵": "用于定义分支程序指令的有界维度矩阵，保持向量的长度不变",
            "黎曼流形": "提供了一个梯度下降优化的几何框架，用于学习分支程序",
            "Neciporuk的下界技术": "用于证明有界维度酉分支程序的大小下界的技术"
        },
        "success": true
    },
    {
        "order": 1138,
        "title": "Unsupervised Co-part Segmentation through Assembly",
        "html": "https://ICML.cc//virtual/2021/poster/9615",
        "abstract": "Co-part segmentation is an important problem in computer vision for its rich applications. We propose an unsupervised learning approach for co-part segmentation from images. For the training stage, we leverage motion information embedded in videos and explicitly extract latent representations to segment meaningful object parts. More importantly, we introduce a dual procedure of part-assembly to form a closed loop with part-segmentation, enabling an effective self-supervision. We demonstrate the effectiveness of our approach with a host of extensive experiments, ranging from human bodies, hands, quadruped, and robot arms. We show that our approach can achieve meaningful and compact part segmentation, outperforming state-of-the-art approaches on diverse benchmarks.",
        "conference": "ICML",
        "中文标题": "无监督共部件分割的组装方法",
        "摘要翻译": "共部件分割是计算机视觉中的一个重要问题，因其丰富的应用价值。我们提出了一种从图像中进行无监督学习的共部件分割方法。在训练阶段，我们利用视频中嵌入的运动信息，并显式提取潜在表示以分割有意义的物体部件。更重要的是，我们引入了部件组装的双重程序，与部件分割形成闭环，从而实现有效的自我监督。我们通过一系列广泛的实验，包括人体、手、四足动物和机器人手臂，证明了我们方法的有效性。我们展示了我们的方法能够实现有意义且紧凑的部件分割，在多样化的基准测试中超越了最先进的方法。",
        "领域": "图像分割、无监督学习、计算机视觉",
        "问题": "如何在无监督的情况下，从图像中有效地分割出共部件",
        "动机": "解决传统共部件分割方法需要大量标注数据的问题，探索利用视频中的运动信息进行无监督学习",
        "方法": "利用视频中的运动信息提取潜在表示进行部件分割，并通过部件组装与分割形成闭环自我监督",
        "关键词": [
            "无监督学习",
            "共部件分割",
            "运动信息",
            "自我监督",
            "部件组装"
        ],
        "涉及的技术概念": {
            "无监督学习": "在没有标注数据的情况下，利用视频中的运动信息进行学习",
            "共部件分割": "从图像中分割出共享相似结构或功能的部件",
            "自我监督": "通过部件组装与分割的闭环过程，自动生成监督信号以优化模型"
        },
        "success": true
    },
    {
        "order": 1139,
        "title": "Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction for Few-Shot Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9617",
        "abstract": "We propose unsupervised embedding adaptation for the downstream few-shot classification task.\nBased on findings that deep neural networks learn to generalize before memorizing, we develop Early-Stage Feature Reconstruction (ESFR) --- a novel adaptation scheme with feature reconstruction and dimensionality-driven early stopping that finds generalizable features.\nIncorporating ESFR consistently improves the performance of baseline methods on all standard settings, including the recently proposed transductive method.\nESFR used in conjunction with the transductive method further achieves state-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB; especially with 1.2%~2.0% improvements in accuracy over the previous best performing method on 1-shot setting.",
        "conference": "ICML",
        "中文标题": "通过早期特征重构的无监督嵌入适应用于少样本分类",
        "摘要翻译": "我们提出了一种用于下游少样本分类任务的无监督嵌入适应方法。基于深度神经网络在记忆之前学习泛化的发现，我们开发了早期特征重构（ESFR）——一种结合特征重构和维度驱动早期停止的新型适应方案，以寻找可泛化的特征。将ESFR纳入基线方法后，在所有标准设置下，包括最近提出的转导方法，性能均得到一致提升。ESFR与转导方法结合使用，在mini-ImageNet、tiered-ImageNet和CUB上进一步实现了最先进的性能；特别是在1-shot设置下，准确率比之前表现最佳的方法提高了1.2%~2.0%。",
        "领域": "少样本学习, 图像分类, 特征学习",
        "问题": "解决少样本分类任务中无监督嵌入适应的问题",
        "动机": "基于深度神经网络在记忆之前学习泛化的发现，开发一种能够寻找可泛化特征的适应方案",
        "方法": "提出早期特征重构（ESFR）方法，结合特征重构和维度驱动早期停止，以提升少样本分类的性能",
        "关键词": [
            "无监督嵌入适应",
            "少样本分类",
            "早期特征重构",
            "维度驱动早期停止",
            "转导方法"
        ],
        "涉及的技术概念": {
            "无监督嵌入适应": "在少样本分类任务中，无需监督信号即可调整嵌入空间以适应下游任务",
            "早期特征重构（ESFR）": "一种结合特征重构和维度驱动早期停止的适应方案，旨在寻找可泛化的特征",
            "转导方法": "一种利用测试数据信息来提升模型性能的方法，与ESFR结合使用可达到最先进性能"
        },
        "success": true
    },
    {
        "order": 1140,
        "title": "Unsupervised Learning of Visual 3D Keypoints for Control",
        "html": "https://ICML.cc//virtual/2021/poster/9719",
        "abstract": "Learning sensorimotor control policies from high-dimensional images crucially relies on the quality of the underlying visual representations. Prior works show that structured latent space such as visual keypoints often outperforms unstructured representations for robotic control. However, most of these representations, whether structured or unstructured are learned in a 2D space even though the control tasks are usually performed in a 3D environment. In this work, we propose a framework to learn such a 3D geometric structure directly from images in an end-to-end unsupervised manner. The input images are embedded into latent 3D keypoints via a differentiable encoder which is trained to optimize both a multi-view consistency loss and downstream task objective. These discovered 3D keypoints tend to meaningfully capture robot joints as well as object movements in a consistent manner across both time and 3D space. The proposed approach outperforms prior state-of-art methods across a variety of reinforcement learning benchmarks. Code and videos at https://buoyancy99.github.io/unsup-3d-keypoints/.",
        "conference": "ICML",
        "中文标题": "无监督学习视觉3D关键点以用于控制",
        "摘要翻译": "从高维图像中学习感觉运动控制策略，关键在于底层视觉表示的质量。先前的研究表明，对于机器人控制而言，如视觉关键点这样的结构化潜在空间通常优于非结构化表示。然而，尽管控制任务通常在3D环境中执行，这些表示大多数，无论是结构化还是非结构化，都是在2D空间中学习的。在这项工作中，我们提出了一个框架，以端到端的无监督方式直接从图像中学习这样的3D几何结构。输入图像通过一个可微分的编码器嵌入到潜在的3D关键点中，该编码器被训练以优化多视角一致性损失和下游任务目标。这些发现的3D关键点倾向于以一种在时间和3D空间上一致的方式有意义地捕捉机器人关节以及物体运动。所提出的方法在各种强化学习基准测试中优于先前的最先进方法。代码和视频可在https://buoyancy99.github.io/unsup-3d-keypoints/获取。",
        "领域": "机器人视觉控制、3D视觉表示学习、强化学习",
        "问题": "如何从高维图像中无监督学习3D几何结构以提升机器人控制策略的效果",
        "动机": "现有的视觉表示学习方法大多局限于2D空间，而机器人控制任务通常在3D环境中进行，这限制了控制策略的性能。",
        "方法": "提出一个端到端的无监督学习框架，通过可微分编码器将图像嵌入到3D关键点，优化多视角一致性损失和下游任务目标。",
        "关键词": [
            "3D关键点",
            "无监督学习",
            "机器人控制",
            "强化学习",
            "视觉表示学习"
        ],
        "涉及的技术概念": {
            "3D关键点": "通过无监督学习从图像中提取的3D几何结构，用于捕捉机器人关节和物体运动",
            "多视角一致性损失": "用于训练编码器的损失函数，确保从不同视角观察到的同一场景的3D关键点表示一致",
            "可微分编码器": "将输入图像转换为3D关键点的神经网络，其可微分性允许通过反向传播进行端到端训练"
        },
        "success": true
    },
    {
        "order": 1141,
        "title": "Unsupervised Part Representation by Flow Capsules",
        "html": "https://ICML.cc//virtual/2021/poster/10591",
        "abstract": "Capsule networks aim to parse images into a hierarchy of objects, parts and relations.\nWhile promising, they remain limited by an inability to learn effective low level part descriptions.\nTo address this issue we propose a way to learn primary capsule encoders that \ndetect atomic parts from a single image.\nDuring training we exploit motion as a powerful perceptual cue for part definition, \nwith an expressive decoder for part generation within a layered image model with occlusion.\nExperiments demonstrate robust part discovery in the presence of multiple objects, cluttered \nbackgrounds, and occlusion. The learned part decoder is shown to infer the underlying shape \nmasks, effectively filling in occluded regions of the detected shapes.\nWe evaluate FlowCapsules on unsupervised part segmentation and unsupervised image classification.",
        "conference": "ICML",
        "中文标题": "通过流胶囊的无监督部件表示",
        "摘要翻译": "胶囊网络旨在将图像解析为对象、部件和关系的层次结构。尽管前景广阔，但由于无法学习有效的低级部件描述，它们仍然受限。为了解决这个问题，我们提出了一种学习初级胶囊编码器的方法，该编码器可以从单个图像中检测原子部件。在训练过程中，我们利用运动作为部件定义的强大感知线索，使用一个表达力强的解码器在具有遮挡的分层图像模型中进行部件生成。实验证明，在存在多个对象、杂乱背景和遮挡的情况下，能够稳健地发现部件。学习到的部件解码器能够推断出基础的形状掩码，有效地填充检测到的形状的遮挡区域。我们在无监督部件分割和无监督图像分类上评估了流胶囊。",
        "领域": "无监督学习、图像分割、胶囊网络",
        "问题": "胶囊网络在低级部件描述学习上的不足",
        "动机": "提高胶囊网络在解析图像为对象、部件和关系层次结构时的效率和准确性",
        "方法": "利用运动作为感知线索，通过表达力强的解码器在分层图像模型中生成部件",
        "关键词": [
            "流胶囊",
            "无监督学习",
            "部件分割",
            "图像分类",
            "胶囊网络"
        ],
        "涉及的技术概念": {
            "胶囊网络": "用于将图像解析为对象、部件和关系的层次结构",
            "无监督学习": "在不使用标注数据的情况下学习部件表示",
            "运动感知": "作为部件定义的感知线索，帮助模型理解部件"
        },
        "success": true
    },
    {
        "order": 1142,
        "title": "Unsupervised Representation Learning via Neural Activation Coding",
        "html": "https://ICML.cc//virtual/2021/poster/10239",
        "abstract": "We present neural activation coding (NAC) as a novel approach for learning deep representations from unlabeled data for downstream applications. We argue that the deep encoder should maximize its nonlinear expressivity on the data for downstream predictors to take full advantage of its representation power. To this end, NAC maximizes the mutual information between activation patterns of the encoder and the data over a noisy communication channel. We show that learning for a noise-robust activation code increases the number of distinct linear regions of ReLU encoders, hence the maximum nonlinear expressivity. More interestingly, NAC learns both continuous and discrete representations of data, which we respectively evaluate on two downstream tasks: (i) linear classification on CIFAR-10 and ImageNet-1K and (ii) nearest neighbor retrieval on CIFAR-10 and FLICKR-25K. Empirical results show that NAC attains better or comparable performance on both tasks over recent baselines including SimCLR and DistillHash. In addition, NAC pretraining provides significant benefits to the training of deep generative models. Our code is available at https://github.com/yookoon/nac.",
        "conference": "ICML",
        "中文标题": "通过神经激活编码的无监督表示学习",
        "摘要翻译": "我们提出了神经激活编码（NAC）作为一种新颖的方法，用于从未标记数据中学习深度表示以供下游应用使用。我们认为，深度编码器应最大化其在数据上的非线性表达能力，以便下游预测器能够充分利用其表示能力。为此，NAC在噪声通信信道上最大化编码器的激活模式与数据之间的互信息。我们表明，学习噪声鲁棒的激活编码增加了ReLU编码器的不同线性区域的数量，从而提高了最大非线性表达能力。更有趣的是，NAC学习了数据的连续和离散表示，我们分别在两个下游任务上进行了评估：（i）在CIFAR-10和ImageNet-1K上的线性分类，以及（ii）在CIFAR-10和FLICKR-25K上的最近邻检索。实证结果表明，NAC在这两项任务上的表现优于或与包括SimCLR和DistillHash在内的最新基线相当。此外，NAC预训练为深度生成模型的训练提供了显著的好处。我们的代码可在https://github.com/yookoon/nac获取。",
        "领域": "无监督学习, 表示学习, 深度生成模型",
        "问题": "如何从未标记数据中学习有效的深度表示以供下游任务使用",
        "动机": "提高深度编码器在数据上的非线性表达能力，以便下游预测器能够充分利用其表示能力",
        "方法": "通过最大化编码器的激活模式与数据之间的互信息来学习噪声鲁棒的激活编码",
        "关键词": [
            "神经激活编码",
            "无监督学习",
            "表示学习",
            "深度生成模型",
            "互信息"
        ],
        "涉及的技术概念": {
            "神经激活编码": "一种新颖的方法，用于从未标记数据中学习深度表示，通过最大化编码器的激活模式与数据之间的互信息",
            "互信息": "用于衡量编码器的激活模式与数据之间关系的指标，NAC通过最大化这一指标来提高表示能力",
            "ReLU编码器": "使用ReLU激活函数的编码器，NAC通过增加其不同线性区域的数量来提高非线性表达能力"
        },
        "success": true
    },
    {
        "order": 1143,
        "title": "Unsupervised Skill Discovery with Bottleneck Option Learning",
        "html": "https://ICML.cc//virtual/2021/poster/8531",
        "abstract": "Having the ability to acquire inherent skills from environments without any external rewards or supervision like humans is an important problem. We propose a novel unsupervised skill discovery method named Information Bottleneck Option Learning (IBOL). On top of the linearization of environments that promotes more various and distant state transitions, IBOL enables the discovery of diverse skills. It provides the abstraction of the skills learned with the information bottleneck framework for the options with improved stability and encouraged disentanglement. We empirically demonstrate that IBOL outperforms multiple state-of-the-art unsupervised skill discovery methods on the information-theoretic evaluations and downstream tasks in MuJoCo environments, including Ant, HalfCheetah, Hopper and D'Kitty. Our code is available at https://vision.snu.ac.kr/projects/ibol.",
        "conference": "ICML",
        "中文标题": "无监督技能发现与瓶颈选项学习",
        "摘要翻译": "具备像人类一样无需任何外部奖励或监督就能从环境中获取内在技能的能力是一个重要问题。我们提出了一种名为信息瓶颈选项学习（IBOL）的新型无监督技能发现方法。在促进更多样化和更远距离状态转换的环境线性化基础上，IBOL能够发现多样化的技能。它通过信息瓶颈框架为选项提供了学习技能的抽象，提高了稳定性并鼓励了解缠。我们通过实验证明，在MuJoCo环境（包括Ant、HalfCheetah、Hopper和D'Kitty）的信息论评估和下游任务中，IBOL优于多种最先进的无监督技能发现方法。我们的代码可在https://vision.snu.ac.kr/projects/ibol获取。",
        "领域": "强化学习、无监督学习、机器人控制",
        "问题": "如何在无外部奖励或监督的情况下从环境中发现多样化的技能",
        "动机": "模仿人类无需外部指导就能学习多样技能的能力",
        "方法": "提出信息瓶颈选项学习（IBOL）方法，通过环境线性化和信息瓶颈框架促进多样化技能的发现与抽象",
        "关键词": [
            "无监督技能发现",
            "信息瓶颈",
            "选项学习",
            "强化学习",
            "机器人控制"
        ],
        "涉及的技术概念": {
            "信息瓶颈选项学习（IBOL）": "一种无监督技能发现方法，通过信息瓶颈框架促进技能的多样化和解缠",
            "环境线性化": "促进更多样化和更远距离状态转换的技术，为技能发现提供基础",
            "选项": "在强化学习中表示可重用子策略的概念，IBOL通过信息瓶颈框架对其进行抽象以提高稳定性和解缠"
        },
        "success": true
    },
    {
        "order": 1144,
        "title": "Valid Causal Inference with (Some) Invalid Instruments",
        "html": "https://ICML.cc//virtual/2021/poster/10139",
        "abstract": "Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable 'exclusion' assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is 'black-box' in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem.",
        "conference": "ICML",
        "中文标题": "有效因果推断与（部分）无效工具变量",
        "摘要翻译": "工具变量方法为在存在未观测混杂因素的情况下估计因果效应提供了强有力的途径。然而，应用这些方法时的一个关键挑战是依赖于不可测试的‘排除’假设，这些假设排除了工具变量与响应之间不通过治疗介导的任何关系。在本文中，我们展示了如何在违反排除假设的情况下进行一致的IV估计。特别是，我们表明，当有多个候选工具变量时，只需要这些候选变量中的多数——或者更一般地说，模态候选-响应关系——是有效的，就可以估计因果效应。我们的方法使用了来自工具变量估计器集合的模态预测估计。这项技术易于应用，并且是‘黑箱’的，因为它可以与任何工具变量估计器一起使用，只要对每个有效工具变量独立识别治疗效果。因此，它与最近基于机器学习的估计器兼容，这些估计器允许在复杂的高维数据上估计条件平均治疗效果（CATE）。在实验中，我们使用基于深度网络的估计器集合，包括在一个具有挑战性的模拟孟德尔随机化问题上，实现了对条件平均治疗效果的准确估计。",
        "领域": "因果推断、机器学习应用、统计方法",
        "问题": "在工具变量方法中，如何在不满足排除假设的情况下进行一致的因果效应估计。",
        "动机": "解决工具变量方法中因排除假设不可测试而导致的估计偏差问题。",
        "方法": "利用多个候选工具变量中的多数有效关系，通过工具变量估计器集合的模态预测估计因果效应。",
        "关键词": [
            "工具变量",
            "因果推断",
            "条件平均治疗效果",
            "机器学习",
            "孟德尔随机化"
        ],
        "涉及的技术概念": {
            "工具变量": "用于在存在未观测混杂因素的情况下估计因果效应的统计方法。",
            "条件平均治疗效果（CATE）": "在给定条件下，治疗对结果的平均影响。",
            "孟德尔随机化": "利用遗传变异作为工具变量来估计因果效应的统计方法。"
        },
        "success": true
    },
    {
        "order": 1145,
        "title": "Value Alignment Verification",
        "html": "https://ICML.cc//virtual/2021/poster/9547",
        "abstract": "As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values? The goal is to construct a kind of 'driver's test' that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents, propose and test heuristics for value alignment verification in gridworlds and a continuous autonomous driving domain, and prove that there exist sufficient conditions such that we can verify epsilon-alignment in any environment via a constant-query-complexity alignment test.\n",
        "conference": "ICML",
        "中文标题": "价值对齐验证",
        "摘要翻译": "随着人类与自主代理的互动以执行日益复杂、潜在高风险的任务，能够有效评估代理的性能和正确性变得尤为重要。在本文中，我们形式化并理论分析了高效价值对齐验证的问题：如何高效测试另一个代理的行为是否与人类的价值对齐？目标是构建一种‘驾驶测试’，人类可以将其给予任何代理，通过最少数量的查询来验证价值对齐。我们研究了与具有明确奖励函数的理想化人类以及具有隐含价值的问题的对齐验证问题。我们分析了理性代理的精确价值对齐验证，在网格世界和连续自动驾驶领域提出并测试了价值对齐验证的启发式方法，并证明了存在足够条件，使得我们可以在任何环境中通过恒定查询复杂度的对齐测试来验证ε-对齐。",
        "领域": "自主代理、价值对齐、人机交互",
        "问题": "如何高效测试自主代理的行为是否与人类的价值对齐",
        "动机": "为了确保自主代理在执行高风险任务时能够与人类的价值保持一致，需要一种有效的方法来验证这种对齐性",
        "方法": "形式化和理论分析高效价值对齐验证问题，构建‘驾驶测试’，研究理想化人类和隐含价值的对齐验证，提出并测试启发式方法，证明ε-对齐验证的充分条件",
        "关键词": [
            "价值对齐",
            "自主代理",
            "人机交互",
            "验证测试",
            "ε-对齐"
        ],
        "涉及的技术概念": {
            "价值对齐验证": "验证自主代理的行为是否与人类的价值对齐的过程",
            "驾驶测试": "一种由人类给予代理的测试，用于通过最少数量的查询验证价值对齐",
            "ε-对齐": "在足够条件下，可以在任何环境中通过恒定查询复杂度的对齐测试验证的对齐标准"
        },
        "success": true
    },
    {
        "order": 1146,
        "title": "Value-at-Risk Optimization with Gaussian Processes",
        "html": "https://ICML.cc//virtual/2021/poster/8793",
        "abstract": "Value-at-risk (VaR) is an established measure to assess risks in critical real-world applications with random environmental factors. This paper presents a novel VaR upper confidence bound (V-UCB) algorithm for maximizing the VaR of a black-box objective function with the first no-regret guarantee. To realize this, we first derive a confidence bound of VaR and then prove the existence of values of the environmental random variable (to be selected to achieve no regret) such that the confidence bound of VaR lies within that of the objective function evaluated at such values. Our V-UCB algorithm empirically demonstrates state-of-the-art performance in optimizing synthetic benchmark functions, a portfolio optimization problem, and a simulated robot task.",
        "conference": "ICML",
        "中文标题": "基于高斯过程的风险价值优化",
        "摘要翻译": "风险价值（VaR）是评估具有随机环境因素的关键现实应用中风险的既定指标。本文提出了一种新颖的VaR上置信界（V-UCB）算法，用于最大化黑盒目标函数的VaR，并首次提供了无遗憾保证。为实现这一点，我们首先推导了VaR的置信界，然后证明了环境随机变量的值（被选择以实现无遗憾）的存在性，使得VaR的置信界位于在这些值处评估的目标函数的置信界内。我们的V-UCB算法在优化合成基准函数、投资组合优化问题和模拟机器人任务中实证展示了最先进的性能。",
        "领域": "金融风险管理, 机器人控制, 投资组合优化",
        "问题": "如何在具有随机环境因素的黑盒目标函数中最大化风险价值（VaR）",
        "动机": "为了在关键的现实世界应用中更有效地评估和管理风险，特别是在存在不确定性和随机性的环境中",
        "方法": "提出了一种基于高斯过程的VaR上置信界（V-UCB）算法，通过推导VaR的置信界并选择特定的环境随机变量值来实现无遗憾的VaR最大化",
        "关键词": [
            "风险价值",
            "高斯过程",
            "无遗憾学习",
            "投资组合优化",
            "机器人控制"
        ],
        "涉及的技术概念": {
            "风险价值（VaR）": "用于评估在特定时间周期内，投资组合可能遭受的最大损失，是金融风险管理中的核心指标",
            "高斯过程": "用于建模目标函数的不确定性，为VaR优化提供概率保证",
            "无遗憾保证": "算法在长期运行中与最佳可能策略相比，累积损失的上界趋近于零，保证了算法的有效性"
        },
        "success": true
    },
    {
        "order": 1147,
        "title": "Value Iteration in Continuous Actions, States and Time",
        "html": "https://ICML.cc//virtual/2021/poster/9345",
        "abstract": "Classical value iteration approaches are not applicable to environments with continuous states and actions. For such environments the states and actions must be discretized, which leads to an exponential increase in computational complexity. In this paper, we propose continuous fitted value iteration (cFVI). This algorithm enables dynamic programming for continuous states and actions with a known dynamics model. Exploiting the continuous time formulation, the optimal policy can be derived for non-linear control-affine dynamics. This closed-form solution enables the efficient extension of value iteration to continuous environments. We show in non-linear control experiments that the dynamic programming solution obtains the same quantitative performance as deep reinforcement learning methods in simulation but excels when transferred to the physical system.The policy obtained by cFVI is more robust to changes in the dynamics despite using only a deterministic model and without explicitly incorporating robustness in the optimization",
        "conference": "ICML",
        "中文标题": "连续动作、状态与时间中的价值迭代",
        "摘要翻译": "经典的价值迭代方法不适用于具有连续状态和动作的环境。对于这样的环境，状态和动作必须被离散化，这导致计算复杂度呈指数级增加。在本文中，我们提出了连续拟合价值迭代（cFVI）。该算法使得动态规划能够应用于具有已知动态模型的连续状态和动作环境。利用连续时间公式，可以为非线性控制仿射动态推导出最优策略。这种封闭形式的解决方案使得价值迭代能够高效地扩展到连续环境。我们在非线性控制实验中展示，动态规划解决方案在模拟中获得了与深度强化学习方法相同的定量性能，但在转移到物理系统时表现更优。尽管仅使用确定性模型且未在优化中明确纳入鲁棒性，cFVI获得的策略对动态变化更加鲁棒。",
        "领域": "强化学习、动态规划、非线性控制",
        "问题": "解决在连续状态和动作空间中应用价值迭代方法的高计算复杂度问题",
        "动机": "为了在连续状态和动作环境中高效应用动态规划方法，避免离散化带来的计算复杂度指数级增加",
        "方法": "提出连续拟合价值迭代（cFVI）算法，利用连续时间公式推导最优策略，实现动态规划在连续环境中的高效扩展",
        "关键词": [
            "连续拟合价值迭代",
            "动态规划",
            "非线性控制",
            "强化学习",
            "鲁棒性"
        ],
        "涉及的技术概念": {
            "连续拟合价值迭代（cFVI）": "一种允许在连续状态和动作空间中应用动态规划的算法，通过连续时间公式推导最优策略",
            "动态规划": "用于在已知动态模型的环境中求解最优策略的数学方法",
            "非线性控制仿射动态": "一类特定的动态系统模型，cFVI算法能够为此类系统推导出封闭形式的最优策略解决方案"
        },
        "success": true
    },
    {
        "order": 1148,
        "title": "Variance Reduced Training with Stratified Sampling for Forecasting Models",
        "html": "https://ICML.cc//virtual/2021/poster/8895",
        "abstract": "In large-scale time series forecasting, one often encounters the situation where the temporal patterns of time series, while drifting over time, differ from one another in the same dataset. In this paper, we provably show under such heterogeneity, training a forecasting model with commonly used stochastic optimizers (e.g. SGD) potentially suffers large variance on gradient estimation, and thus incurs long-time training. We show that this issue can be efficiently alleviated via stratification, which allows the optimizer to sample from pre-grouped time series strata.  For better trading-off gradient variance and computation complexity, we further propose SCott (Stochastic Stratified Control Variate Gradient Descent), a variance reduced SGD-style optimizer that utilizes stratified sampling via control variate.  In theory, we provide the convergence guarantee of SCott on smooth non-convex objectives. Empirically, we evaluate SCott and other baseline optimizers on both synthetic and real-world time series forecasting problems, and demonstrate SCott converges faster with respect to both iterations and wall clock time.",
        "conference": "ICML",
        "中文标题": "采用分层抽样的方差缩减训练用于预测模型",
        "摘要翻译": "在大规模时间序列预测中，经常会遇到时间序列的时间模式随时间漂移，但在同一数据集中彼此不同的情况。本文在这种异质性下证明，使用常用的随机优化器（如SGD）训练预测模型可能会在梯度估计上遭受较大的方差，从而导致训练时间较长。我们表明，通过分层可以有效地缓解这一问题，分层允许优化器从预先分组的时间序列层中抽样。为了更好地权衡梯度方差和计算复杂度，我们进一步提出了SCott（随机分层控制变量梯度下降），这是一种利用控制变量通过分层抽样的方差缩减SGD风格优化器。理论上，我们提供了SCott在平滑非凸目标上的收敛保证。实证上，我们在合成和真实世界的时间序列预测问题上评估了SCott和其他基线优化器，并证明SCott在迭代次数和实际时间上都收敛得更快。",
        "领域": "时间序列预测、优化算法、梯度下降",
        "问题": "在大规模时间序列预测中，由于时间序列模式的异质性和漂移，使用传统随机优化器训练模型时梯度估计方差大，导致训练时间长。",
        "动机": "为了解决在大规模时间序列预测中，由于时间序列模式的异质性和漂移导致的梯度估计方差大和训练时间长的问题。",
        "方法": "提出了一种名为SCott的方差缩减SGD风格优化器，它通过分层抽样和控制变量技术来减少梯度估计的方差，并在理论上提供了收敛保证。",
        "关键词": [
            "时间序列预测",
            "分层抽样",
            "方差缩减",
            "梯度下降",
            "优化算法"
        ],
        "涉及的技术概念": {
            "分层抽样": "在优化过程中，将时间序列数据分成若干层，从每层中抽样以减少梯度估计的方差。",
            "控制变量": "一种用于减少方差的技术，通过引入与原始估计量相关的辅助变量来调整估计量，从而降低方差。",
            "平滑非凸目标": "指优化问题的目标函数是非凸的，但在局部区域内具有良好的平滑性质，SCott优化器针对这类目标函数提供了收敛保证。"
        },
        "success": true
    },
    {
        "order": 1149,
        "title": "Variance Reduction via Primal-Dual Accelerated Dual Averaging for Nonsmooth Convex Finite-Sums",
        "html": "https://ICML.cc//virtual/2021/poster/8865",
        "abstract": "Structured nonsmooth convex finite-sum optimization appears in many machine learning applications, including support vector machines and least absolute deviation. For the primal-dual formulation of this problem, we propose a novel algorithm called \\emph{Variance Reduction via Primal-Dual Accelerated Dual Averaging (\\vrpda)}. In the nonsmooth and general convex setting, \\vrpda~has the overall complexity $O(nd\\log\\min \\{1/\\epsilon, n\\} + d/\\epsilon )$ in terms of the primal-dual gap, where $n$ denotes the number of samples, $d$ the dimension of the primal variables, and $\\epsilon$ the desired accuracy. In the nonsmooth and strongly convex setting, the overall complexity of \\vrpda~becomes $O(nd\\log\\min\\{1/\\epsilon, n\\} + d/\\sqrt{\\epsilon})$ in terms of both the primal-dual gap and the distance between iterate and optimal solution. Both these results for \\vrpda~improve significantly on state-of-the-art complexity estimates---which are $O(nd\\log \\min\\{1/\\epsilon, n\\} + \\sqrt{n}d/\\epsilon)$ for the  nonsmooth and general convex  setting and $O(nd\\log \\min\\{1/\\epsilon, n\\} + \\sqrt{n}d/\\sqrt{\\epsilon})$ for the nonsmooth and strongly convex setting---with a simpler and more straightforward algorithm and analysis. Moreover, both complexities are better than \\emph{lower} bounds for general convex finite-sum optimization, because our approach makes use of additional, commonly occurring structure. Numerical experiments reveal competitive performance of \\vrpda~compared to state-of-the-art approaches.",
        "conference": "ICML",
        "中文标题": "通过原始-对偶加速对偶平均法实现非光滑凸有限和问题的方差减少",
        "摘要翻译": "结构化非光滑凸有限和优化出现在许多机器学习应用中，包括支持向量机和最小绝对偏差。针对这一问题的原始-对偶形式，我们提出了一种名为《通过原始-对偶加速对偶平均法实现方差减少（VRPDA）》的新算法。在非光滑和一般凸设置下，VRPDA在原始-对偶间隙方面的总体复杂度为O(ndlogmin{1/ε, n} + d/ε)，其中n表示样本数量，d表示原始变量的维度，ε表示期望的准确度。在非光滑和强凸设置下，VRPDA在原始-对偶间隙和迭代与最优解之间的距离方面的总体复杂度变为O(ndlogmin{1/ε, n} + d/√ε)。VRPDA的这些结果显著改进了最先进的复杂度估计——在非光滑和一般凸设置下为O(ndlogmin{1/ε, n} + √nd/ε)，在非光滑和强凸设置下为O(ndlogmin{1/ε, n} + √nd/√ε)——通过更简单和更直接的算法和分析。此外，这两种复杂度都优于一般凸有限和优化的下限，因为我们的方法利用了额外的、常见的结构。数值实验显示，与最先进的方法相比，VRPDA具有竞争性的性能。",
        "领域": "凸优化、机器学习优化算法、支持向量机",
        "问题": "解决非光滑凸有限和优化问题的效率和复杂度问题",
        "动机": "提高非光滑凸有限和优化问题的解决效率，减少计算复杂度",
        "方法": "提出了一种名为VRPDA的新算法，通过原始-对偶加速对偶平均法实现方差减少",
        "关键词": [
            "非光滑凸优化",
            "原始-对偶方法",
            "方差减少",
            "加速对偶平均法",
            "复杂度优化"
        ],
        "涉及的技术概念": {
            "原始-对偶方法": "用于解决优化问题的一种方法，通过同时考虑原始问题和对偶问题来寻找最优解",
            "方差减少": "一种技术，用于减少算法中的方差，提高算法的稳定性和效率",
            "加速对偶平均法": "一种优化算法，通过加速技术提高对偶平均法的收敛速度"
        },
        "success": true
    },
    {
        "order": 1150,
        "title": "Variational Auto-Regressive Gaussian Processes for Continual Learning",
        "html": "https://ICML.cc//virtual/2021/poster/9871",
        "abstract": "Through sequential construction of posteriors on observing data online, Bayes’ theorem provides a natural framework for continual learning. We develop Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled posterior updating mechanism to solve sequential tasks in continual learning. By relying on sparse inducing point approximations for scalable posteriors, we propose a novel auto-regressive variational distribution which reveals two fruitful connections to existing results in Bayesian inference, expectation propagation and orthogonal inducing points. Mean predictive entropy estimates show VAR-GPs prevent catastrophic forgetting, which is empirically supported by strong performance on modern continual learning benchmarks against competitive baselines. A thorough ablation study demonstrates the efficacy of our modeling choices.",
        "conference": "ICML",
        "中文标题": "变分自回归高斯过程在持续学习中的应用",
        "摘要翻译": "通过在线观察数据时后验的序贯构建，贝叶斯定理为持续学习提供了一个自然的框架。我们开发了变分自回归高斯过程（VAR-GPs），一种原则性的后验更新机制，以解决持续学习中的序贯任务。通过依赖稀疏诱导点近似以获得可扩展的后验，我们提出了一种新颖的自回归变分分布，该分布揭示了与贝叶斯推断、期望传播和正交诱导点现有结果的两个富有成果的联系。平均预测熵估计显示VAR-GPs防止了灾难性遗忘，这一点在现代持续学习基准测试中对抗竞争基线时的强劲表现得到了实证支持。一项彻底的消融研究证明了我们建模选择的有效性。",
        "领域": "持续学习、贝叶斯深度学习、高斯过程",
        "问题": "解决持续学习中的序贯任务，防止灾难性遗忘",
        "动机": "开发一种原则性的后验更新机制，以在持续学习中有效地序贯构建后验，防止模型在学习新任务时忘记旧任务",
        "方法": "提出变分自回归高斯过程（VAR-GPs），利用稀疏诱导点近似和自回归变分分布，结合贝叶斯推断、期望传播和正交诱导点的现有结果",
        "关键词": [
            "持续学习",
            "变分自回归高斯过程",
            "灾难性遗忘",
            "贝叶斯推断",
            "稀疏诱导点"
        ],
        "涉及的技术概念": {
            "变分自回归高斯过程（VAR-GPs）": "一种用于持续学习的后验更新机制，通过自回归变分分布和稀疏诱导点近似实现",
            "稀疏诱导点近似": "用于实现可扩展的后验计算，减少计算复杂度",
            "灾难性遗忘": "指模型在学习新任务时忘记旧任务的现象，VAR-GPs通过特定的机制防止这一现象"
        },
        "success": true
    },
    {
        "order": 1151,
        "title": "Variational Data Assimilation with a Learned Inverse Observation Operator",
        "html": "https://ICML.cc//virtual/2021/poster/9289",
        "abstract": "Variational data assimilation optimizes for an initial state of a dynamical system such that its evolution fits observational data. The physical model can subsequently be evolved into the future to make predictions. This principle is a cornerstone of large scale forecasting applications such as numerical weather prediction. As such, it is implemented in current operational systems of weather forecasting agencies across the globe. However, finding a good initial state poses a difficult optimization problem in part due to the non-invertible relationship between physical states and their corresponding observations. We learn a mapping from observational data to physical states and show how it can be used to improve optimizability. We employ this mapping in two ways: to better initialize the non-convex optimization problem, and to reformulate the objective function in better behaved physics space instead of observation space. Our experimental results for the Lorenz96 model and a two-dimensional turbulent fluid flow demonstrate that this procedure significantly improves forecast quality for chaotic systems.",
        "conference": "ICML",
        "中文标题": "使用学习到的逆观测算子进行变分数据同化",
        "摘要翻译": "变分数据同化优化动态系统的初始状态，使其演化过程与观测数据相匹配。随后，物理模型可以被演化到未来以进行预测。这一原理是大规模预测应用（如数值天气预报）的基石。因此，它被全球各地的天气预报机构的当前操作系统中实现。然而，找到一个好的初始状态提出了一个困难的优化问题，部分原因是物理状态与其相应观测之间的不可逆关系。我们学习了一个从观测数据到物理状态的映射，并展示了如何用它来提高优化的可能性。我们以两种方式使用这个映射：更好地初始化非凸优化问题，以及在更好的物理空间而非观测空间中重新表述目标函数。我们对Lorenz96模型和二维湍流流体流动的实验结果表明，这一过程显著提高了混沌系统的预测质量。",
        "领域": "数值天气预报、混沌系统预测、动态系统优化",
        "问题": "解决在变分数据同化中由于物理状态与观测数据之间的不可逆关系导致的初始状态优化困难问题。",
        "动机": "提高混沌系统预测的准确性和可靠性，通过优化初始状态的选择和改进目标函数的表述方式。",
        "方法": "学习从观测数据到物理状态的映射，并利用该映射改进初始状态的初始化和目标函数的表述。",
        "关键词": [
            "变分数据同化",
            "逆观测算子",
            "混沌系统预测",
            "动态系统优化",
            "数值天气预报"
        ],
        "涉及的技术概念": {
            "变分数据同化": "一种优化动态系统初始状态的技术，使其演化与观测数据匹配，用于提高预测准确性。",
            "逆观测算子": "学习到的从观测数据到物理状态的映射，用于改进初始状态的优化过程。",
            "混沌系统预测": "指对初始条件极为敏感的系统进行预测，如气象系统，需要高精度的初始状态估计。"
        },
        "success": true
    },
    {
        "order": 1152,
        "title": "Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10367",
        "abstract": "Learning to reach goal states and learning diverse skills through mutual information maximization have been proposed as principled frameworks for unsupervised reinforcement  learning,  allowing agents to acquire broadly applicable multi-task policies with minimal reward engineering. In this paper, we discuss how these two approaches — goal-conditioned RL (GCRL) and MI-based RL — can be generalized into a single family of methods, interpreting mutual information maximization and variational empowerment as representation learning methods that acquire function-ally aware state representations for goal reaching.Starting from a simple observation that the standard GCRL is encapsulated by the optimization objective of variational empowerment,  we can derive novel variants of GCRL and variational empowerment under a single, unified optimization objective, such as adaptive-variance GCRL and linear-mapping GCRL, and study the characteristics of representation learning each variant provides. Furthermore, through the lens of GCRL, we show that adapting powerful techniques fromGCRL such as goal relabeling into the variationalMI context as well as proper regularization on the variational posterior provides substantial gains in algorithm performance, and propose a novel evaluation metric named latent goal reaching (LGR)as an objective measure for evaluating empowerment algorithms akin to goal-based RL. Through principled mathematical derivations and careful experimental validations, our work lays a novel foundation from which representation learning can be evaluated and analyzed in goal-based RL",
        "conference": "ICML",
        "中文标题": "变分赋权作为目标条件强化学习的表示学习",
        "摘要翻译": "学习达到目标状态和通过互信息最大化学习多样技能已被提出作为无监督强化学习的原理性框架，允许代理以最小的奖励工程获得广泛适用的多任务策略。在本文中，我们讨论了这两种方法——目标条件强化学习（GCRL）和基于互信息的强化学习（MI-based RL）——如何被概括为一个单一的方法家族，将互信息最大化和变分赋权解释为获取功能感知状态表示以达成目标的表示学习方法。从标准GCRL被变分赋权的优化目标所封装这一简单观察出发，我们可以在一个统一的优化目标下推导出GCRL和变分赋权的新变体，如自适应方差GCRL和线性映射GCRL，并研究每种变体提供的表示学习特性。此外，通过GCRL的视角，我们展示了将GCRL中的强大技术（如目标重标记）适应到变分MI上下文中，以及对变分后验进行适当正则化，可以显著提高算法性能，并提出了一种名为潜在目标达成（LGR）的新评估指标，作为类似于基于目标的RL的赋权算法的客观评估标准。通过原理性的数学推导和细致的实验验证，我们的工作为在基于目标的RL中评估和分析表示学习奠定了新的基础。",
        "领域": "强化学习、表示学习、多任务学习",
        "问题": "如何在无监督强化学习中有效地学习多任务策略和多样技能",
        "动机": "探索目标条件强化学习和基于互信息的强化学习的统一框架，以提高算法性能和学习效率",
        "方法": "将变分赋权和互信息最大化作为表示学习方法，提出统一的优化目标，并引入新变体和评估指标",
        "关键词": [
            "变分赋权",
            "目标条件强化学习",
            "表示学习",
            "互信息最大化",
            "潜在目标达成"
        ],
        "涉及的技术概念": {
            "变分赋权": "用于在强化学习中学习多样技能和状态表示的技术",
            "互信息最大化": "通过最大化状态和行动之间的互信息来学习多样技能的方法",
            "潜在目标达成": "提出的新评估指标，用于客观评估赋权算法的性能"
        },
        "success": true
    },
    {
        "order": 1153,
        "title": "Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models",
        "html": "https://ICML.cc//virtual/2021/poster/8663",
        "abstract": "This paper presents new estimates of the score function and its gradient with respect to the model parameters in a general energy-based latent variable model (EBLVM). The score function and its gradient can be expressed as combinations of expectation and covariance terms over the (generally intractable) posterior of the latent variables. New estimates are obtained by introducing a variational posterior  to approximate the true posterior in these terms. The variational posterior is trained to minimize a certain divergence (e.g., the KL divergence) between itself and the true posterior. Theoretically, the divergence characterizes upper bounds of the bias of the estimates. In principle, our estimates can be applied to a wide range of objectives, including kernelized Stein discrepancy (KSD), score matching (SM)-based methods and exact Fisher divergence with a minimal model assumption. In particular, these estimates applied to SM-based methods outperform existing methods in learning EBLVMs on several image datasets.",
        "conference": "ICML",
        "中文标题": "基于能量的潜变量模型中评分函数及其梯度的变分估计",
        "摘要翻译": "本文提出了一种在一般基于能量的潜变量模型（EBLVM）中，对评分函数及其关于模型参数的梯度进行估计的新方法。评分函数及其梯度可以表示为潜变量（通常难以处理）后验的期望和协方差项的组合。通过引入变分后验来近似这些项中的真实后验，我们获得了新的估计。变分后验通过最小化其与真实后验之间的某种散度（例如KL散度）来进行训练。理论上，这种散度表征了估计偏差的上界。原则上，我们的估计可以应用于广泛的目标，包括核化Stein差异（KSD）、基于分数匹配（SM）的方法和具有最小模型假设的精确Fisher散度。特别是，这些估计应用于基于SM的方法时，在多个图像数据集上学习EBLVM的性能优于现有方法。",
        "领域": "变分推断、生成模型、无监督学习",
        "问题": "在基于能量的潜变量模型中准确估计评分函数及其梯度的问题",
        "动机": "为了解决在难以处理的潜变量后验下准确估计评分函数及其梯度的挑战，提高模型学习的效率和准确性",
        "方法": "引入变分后验近似真实后验，通过最小化变分后验与真实后验之间的散度来训练，从而获得评分函数及其梯度的新估计",
        "关键词": [
            "变分推断",
            "评分函数",
            "能量基模型",
            "潜变量模型",
            "分数匹配"
        ],
        "涉及的技术概念": {
            "变分后验": "用于近似难以处理的真实后验分布，以便于估计评分函数及其梯度",
            "KL散度": "用于衡量变分后验与真实后验之间的差异，指导变分后验的训练",
            "评分函数": "模型对数概率密度关于数据的梯度，用于模型训练和评估"
        },
        "success": true
    },
    {
        "order": 1154,
        "title": "Vector Quantized Models for Planning",
        "html": "https://ICML.cc//virtual/2021/poster/9543",
        "abstract": "Recent developments in the field of model-based RL have proven successful in a range of environments, especially ones where planning is essential. However, such successes have been limited to deterministic fully-observed environments. We present a new approach that handles stochastic and partially-observable environments. Our key insight is to use discrete autoencoders to capture the multiple possible effects of an action in a stochastic environment. We use a stochastic variant of Monte Carlo tree search to plan over both the agent's actions and the discrete latent variables representing the environment's response. Our approach significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment. We also show that our approach scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.",
        "conference": "ICML",
        "中文标题": "用于规划的向量量化模型",
        "摘要翻译": "基于模型的强化学习领域的最新发展已在一系列环境中证明是成功的，尤其是在规划至关重要的环境中。然而，这些成功仅限于确定性完全可观察的环境。我们提出了一种新方法，能够处理随机性和部分可观察的环境。我们的关键见解是使用离散自编码器来捕捉随机环境中动作的多种可能效果。我们使用蒙特卡洛树搜索的随机变体来规划代理的动作和代表环境响应的离散潜在变量。我们的方法在一个将对手视为环境一部分的国际象棋随机解释上显著优于MuZero的离线版本。我们还展示了我们的方法能够扩展到DeepMind Lab，这是一个具有大型视觉观察和部分可观察性的第一人称3D环境。",
        "领域": "强化学习、蒙特卡洛方法、自编码器",
        "问题": "处理随机性和部分可观察环境中的规划问题",
        "动机": "扩展基于模型的强化学习在随机性和部分可观察环境中的应用",
        "方法": "使用离散自编码器捕捉动作的多种可能效果，结合蒙特卡洛树搜索的随机变体进行规划",
        "关键词": [
            "向量量化",
            "蒙特卡洛树搜索",
            "离散自编码器",
            "强化学习",
            "部分可观察性"
        ],
        "涉及的技术概念": {
            "离散自编码器": "用于捕捉随机环境中动作的多种可能效果",
            "蒙特卡洛树搜索": "用于规划代理的动作和环境的响应",
            "向量量化": "用于模型中对状态和动作的量化表示"
        },
        "success": true
    },
    {
        "order": 1155,
        "title": "Versatile Verification of Tree Ensembles",
        "html": "https://ICML.cc//virtual/2021/poster/9897",
        "abstract": "Machine learned models often must abide by certain requirements (e.g., fairness or legal). This has spurred interested in developing approaches that can provably verify whether a model satisfies certain properties. This paper introduces a generic algorithm called Veritas that enables tackling multiple different verification tasks for tree ensemble models like random forests (RFs) and gradient boosted decision trees (GBDTs). This generality contrasts with previous work, which has focused exclusively on either adversarial example generation or robustness checking. Veritas formulates the verification task as a generic optimization problem and introduces a novel search space representation. Veritas offers two key advantages. First, it provides anytime lower and upper bounds when the optimization problem cannot be solved exactly. In contrast, many existing methods have focused on exact solutions and are thus limited by the verification problem being NP-complete. Second, Veritas produces full (bounded suboptimal) solutions that can be used to generate concrete examples. We experimentally show that our method produces state-of-the-art robustness estimates, especially when executed with strict time constraints. This is exceedingly important when checking the robustness of large datasets. Additionally, we show that Veritas enables tackling more real-world verification scenarios.",
        "conference": "ICML",
        "中文标题": "树集成模型的多功能验证",
        "摘要翻译": "机器学习模型通常需要遵守某些要求（例如，公平性或合法性）。这激发了开发能够可证明验证模型是否满足特定属性的方法的兴趣。本文介绍了一种名为Veritas的通用算法，该算法能够处理树集成模型（如随机森林（RFs）和梯度提升决策树（GBDTs））的多种不同验证任务。这种通用性与之前的工作形成对比，之前的工作仅专注于对抗性示例生成或鲁棒性检查。Veritas将验证任务表述为一个通用的优化问题，并引入了一种新颖的搜索空间表示。Veritas提供了两个关键优势。首先，当优化问题无法精确解决时，它提供了随时可用的下限和上限。相比之下，许多现有方法专注于精确解，因此受到验证问题是NP完全的限制。其次，Veritas产生完整的（有界次优）解决方案，可用于生成具体示例。我们通过实验表明，我们的方法在严格的时间约束下执行时，能够产生最先进的鲁棒性估计，这在检查大型数据集的鲁棒性时极为重要。此外，我们还展示了Veritas能够处理更多现实世界的验证场景。",
        "领域": "机器学习模型验证、树集成模型、鲁棒性检查",
        "问题": "如何高效且通用地验证树集成模型是否满足特定属性",
        "动机": "开发一种能够可证明验证机器学习模型是否满足特定属性（如公平性或合法性）的方法",
        "方法": "引入一种名为Veritas的通用算法，将验证任务表述为通用优化问题，并采用新颖的搜索空间表示",
        "关键词": [
            "树集成模型",
            "模型验证",
            "鲁棒性检查",
            "优化问题",
            "Veritas算法"
        ],
        "涉及的技术概念": {
            "树集成模型": "包括随机森林（RFs）和梯度提升决策树（GBDTs）等，是Veritas算法验证的对象",
            "优化问题": "Veritas将验证任务表述为通用优化问题，以寻找满足特定属性的模型验证方法",
            "搜索空间表示": "Veritas引入的新颖表示方法，用于高效探索可能的验证解决方案"
        },
        "success": true
    },
    {
        "order": 1156,
        "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
        "html": "https://ICML.cc//virtual/2021/poster/9491",
        "abstract": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.",
        "conference": "ICML",
        "中文标题": "ViLT：无需卷积或区域监督的视觉与语言Transformer",
        "摘要翻译": "视觉与语言预训练（VLP）已在多种联合视觉与语言下游任务上提升了性能。当前的VLP方法严重依赖于图像特征提取过程，其中大多数涉及区域监督（例如，目标检测）和卷积架构（例如，ResNet）。尽管在文献中被忽视，我们发现这在（1）效率/速度方面存在问题，即仅提取输入特征就需要比多模态交互步骤多得多的计算；（2）表达能力方面，因为它受限于视觉嵌入器及其预定义视觉词汇的表达能力。在本文中，我们提出了一个极简的VLP模型，视觉与语言Transformer（ViLT），其单一性在于视觉输入的处理被极大地简化为与我们处理文本输入相同的无卷积方式。我们表明，ViLT比以前的VLP模型快数十倍，但在下游任务性能上具有竞争力或更好。我们的代码和预训练权重可在https://github.com/dandelin/vilt获取。",
        "领域": "视觉与语言预训练、多模态学习、Transformer模型",
        "问题": "当前视觉与语言预训练模型依赖复杂的图像特征提取过程，导致计算效率低下和表达能力受限。",
        "动机": "提高视觉与语言预训练模型的效率和表达能力，简化视觉输入处理过程。",
        "方法": "提出ViLT模型，采用无卷积方式处理视觉输入，与处理文本输入的方式相同，简化了视觉输入的处理过程。",
        "关键词": [
            "视觉与语言预训练",
            "Transformer",
            "多模态学习",
            "无卷积处理",
            "效率优化"
        ],
        "涉及的技术概念": {
            "视觉与语言预训练": "通过预训练模型联合处理视觉和语言信息，提升下游任务性能。",
            "Transformer": "采用自注意力机制的模型架构，用于处理序列数据，本文中用于同时处理视觉和语言输入。",
            "无卷积处理": "简化视觉输入处理过程，避免使用传统的卷积神经网络，提高处理效率。"
        },
        "success": true
    },
    {
        "order": 1157,
        "title": "Voice2Series: Reprogramming Acoustic Models for Time Series Classification",
        "html": "https://ICML.cc//virtual/2021/poster/9059",
        "abstract": "Learning to classify time series with limited data is a practical yet challenging  problem.\nCurrent methods are primarily based on hand-designed feature extraction rules or domain-specific data augmentation. Motivated by the advances in deep speech processing models and the fact that voice data are univariate temporal signals,\nin this paper we propose Voice2Serie (V2S), a novel end-to-end approach that reprograms acoustic models for time series classification, through input transformation learning and output label mapping.\nLeveraging the representation learning power of a large-scale pre-trained speech processing model, on 31 different time series tasks we show that V2S outperforms or is on part with state-of-the-art methods on 22 tasks, and improves their average accuracy by 1.72%. \nWe further provide theoretical justification of V2S by proving its population risk is upper bounded by the source risk and a Wasserstein distance accounting for feature alignment via reprogramming. Our results offer new and effective means to time series classification.",
        "conference": "ICML",
        "中文标题": "Voice2Series：重新编程声学模型用于时间序列分类",
        "摘要翻译": "学习在有限数据下分类时间序列是一个实际但具有挑战性的问题。当前的方法主要基于手工设计的特征提取规则或特定领域的数据增强。受到深度语音处理模型进步的启发，以及语音数据是单变量时间信号的事实，本文提出了Voice2Serie（V2S），一种新颖的端到端方法，通过输入转换学习和输出标签映射，重新编程声学模型用于时间序列分类。利用大规模预训练语音处理模型的表示学习能力，在31个不同的时间序列任务上，我们展示了V2S在22个任务上优于或与最先进方法持平，并将它们的平均准确率提高了1.72%。我们进一步通过证明其总体风险受源风险和通过重新编程实现特征对齐的Wasserstein距离上界，为V2S提供了理论依据。我们的结果为时间序列分类提供了新的有效手段。",
        "领域": "时间序列分类、语音处理、深度学习",
        "问题": "在有限数据下有效分类时间序列",
        "动机": "利用深度语音处理模型的表示学习能力，解决时间序列分类中的数据限制问题",
        "方法": "通过输入转换学习和输出标签映射，重新编程预训练的声学模型用于时间序列分类",
        "关键词": [
            "时间序列分类",
            "声学模型",
            "表示学习",
            "输入转换",
            "输出标签映射"
        ],
        "涉及的技术概念": {
            "输入转换学习": "通过学习将时间序列数据转换为适合预训练声学模型处理的格式",
            "输出标签映射": "将声学模型的输出映射到时间序列分类任务的标签空间",
            "Wasserstein距离": "用于衡量通过重新编程实现的特征对齐效果，为方法提供理论支持"
        },
        "success": true
    },
    {
        "order": 1158,
        "title": "Wasserstein Distributional Normalization For Robust Distributional Certification of Noisy Labeled Data",
        "html": "https://ICML.cc//virtual/2021/poster/10301",
        "abstract": "We propose a novel Wasserstein distributional normalization method that can classify noisy labeled data accurately. Recently, noisy labels have been successfully handled based on small-loss criteria, but have not been clearly understood from the theoretical point of view. In this paper, we address this problem by adopting distributionally robust optimization (DRO). In particular, we present a theoretical investigation of the distributional relationship between uncertain and certain samples based on the small-loss criteria. Our method takes advantage of this relationship to exploit useful information from uncertain samples. To this end, we normalize uncertain samples into the robustly certified region by introducing the non-parametric Ornstein-Ulenbeck type of Wasserstein gradient flows called Wasserstein distributional normalization, which is cheap and fast to implement. We verify that network confidence and distributional certification are fundamentally correlated and show the concentration inequality when the network escapes from over-parameterization. Experimental results demonstrate that our non-parametric classification method outperforms other parametric baselines on the Clothing1M and CIFAR-10/100 datasets when the data have diverse noisy labels.",
        "conference": "ICML",
        "中文标题": "Wasserstein分布归一化用于噪声标签数据的鲁棒分布认证",
        "摘要翻译": "我们提出了一种新颖的Wasserstein分布归一化方法，能够准确分类带有噪声标签的数据。最近，基于小损失准则的噪声标签处理已取得成功，但从理论角度尚未得到清晰理解。本文通过采用分布鲁棒优化（DRO）来解决这一问题。特别是，我们基于小损失准则对不确定样本和确定样本之间的分布关系进行了理论探讨。我们的方法利用这一关系从未确定样本中提取有用信息。为此，我们通过引入称为Wasserstein分布归一化的非参数Ornstein-Ulenbeck型Wasserstein梯度流，将不确定样本归一化到鲁棒认证区域，这种方法实现起来既便宜又快速。我们验证了网络置信度和分布认证之间存在根本相关性，并展示了网络逃离过参数化时的集中不等式。实验结果表明，当数据具有多样化的噪声标签时，我们的非参数分类方法在Clothing1M和CIFAR-10/100数据集上优于其他参数基线。",
        "领域": "噪声标签学习、分布鲁棒优化、深度学习理论",
        "问题": "如何准确分类带有噪声标签的数据，并从理论角度理解噪声标签处理的有效性。",
        "动机": "探索噪声标签数据分类的理论基础，并提出一种既高效又经济的解决方案。",
        "方法": "采用分布鲁棒优化（DRO）和Wasserstein分布归一化技术，从未确定样本中提取有用信息，并将其归一化到鲁棒认证区域。",
        "关键词": [
            "Wasserstein分布归一化",
            "噪声标签学习",
            "分布鲁棒优化",
            "非参数分类",
            "深度学习理论"
        ],
        "涉及的技术概念": {
            "Wasserstein分布归一化": "一种非参数的Ornstein-Ulenbeck型Wasserstein梯度流，用于将不确定样本归一化到鲁棒认证区域，实现既便宜又快速。",
            "分布鲁棒优化（DRO）": "一种优化方法，用于处理数据分布的不确定性，提高模型对噪声标签的鲁棒性。",
            "小损失准则": "基于损失大小筛选样本的策略，用于识别和处理噪声标签数据。"
        },
        "success": true
    },
    {
        "order": 1159,
        "title": "Watermarking Deep Neural Networks with Greedy Residuals",
        "html": "https://ICML.cc//virtual/2021/poster/10595",
        "abstract": "Deep neural networks (DNNs) are considered as intellectual property of their corresponding owners and thus are in urgent need of ownership protection, due to the massive amount of time and resources invested in designing, tuning and training them. In this paper, we propose a novel watermark-based ownership protection method by using the residuals of important parameters. Different from other watermark-based ownership protection methods that rely on some specific neural network architectures and during verification require external data source, namely ownership indicators, our method does not explicitly use ownership indicators for verification to defeat various attacks against DNN watermarks. Specifically, we greedily select a few and important model parameters for embedding so that the impairment caused by the changed parameters can be reduced and the robustness against different attacks can be improved as the selected parameters can well preserve the model information. Also, without the external data sources for verification, the adversary can hardly cast doubts on ownership verification by forging counterfeit watermarks. The extensive experiments show that our method outperforms previous state-of-the-art methods in five tasks.",
        "conference": "ICML",
        "中文标题": "使用贪婪残差为深度神经网络添加水印",
        "摘要翻译": "深度神经网络（DNNs）被视为其相应所有者的知识产权，因此由于在设计、调优和训练它们上投入的大量时间和资源，迫切需要所有权保护。在本文中，我们提出了一种新颖的基于水印的所有权保护方法，该方法利用重要参数的残差。与其他基于水印的所有权保护方法不同，那些方法依赖于一些特定的神经网络架构，并且在验证时需要外部数据源，即所有权指示器，我们的方法在验证时不明确使用所有权指示器，以抵御针对DNN水印的各种攻击。具体来说，我们贪婪地选择少量重要的模型参数进行嵌入，这样可以通过减少由改变的参数引起的损害，并且由于所选参数能够很好地保留模型信息，从而提高对不同攻击的鲁棒性。此外，由于没有用于验证的外部数据源，对手很难通过伪造水印来对所有权验证提出质疑。大量实验表明，我们的方法在五个任务中优于之前的最先进方法。",
        "领域": "深度学习安全、模型保护、数字水印",
        "问题": "如何在不依赖特定神经网络架构和外部数据源的情况下，为深度神经网络提供有效的所有权保护。",
        "动机": "深度神经网络作为重要的知识产权，需要有效的方法来保护其所有权，防止未经授权的使用和复制。",
        "方法": "提出了一种基于贪婪选择重要参数残差的水印嵌入方法，该方法不依赖外部数据源进行验证，提高了对不同攻击的鲁棒性。",
        "关键词": [
            "深度神经网络",
            "所有权保护",
            "数字水印",
            "贪婪选择",
            "残差嵌入"
        ],
        "涉及的技术概念": {
            "贪婪选择": "在模型中贪婪地选择少量重要的参数进行水印嵌入，以减少对模型性能的影响。",
            "残差嵌入": "利用模型参数的残差进行水印嵌入，以提高水印的隐蔽性和鲁棒性。",
            "所有权验证": "在不依赖外部数据源的情况下进行所有权验证，防止伪造水印的攻击。"
        },
        "success": true
    },
    {
        "order": 1160,
        "title": "Weight-covariance alignment for adversarially robust neural networks",
        "html": "https://ICML.cc//virtual/2021/poster/9309",
        "abstract": "Stochastic Neural Networks (SNNs) that inject noise into their hidden layers have recently been shown to achieve strong robustness against adversarial attacks. However, existing SNNs are usually heuristically motivated, and often rely on adversarial training, which is computationally costly. We propose a new SNN that achieves state-of-the-art performance without relying on adversarial training, and enjoys solid theoretical justification. Specifically, while existing SNNs inject learned or hand-tuned isotropic noise, our SNN learns an anisotropic noise distribution to optimize a learning-theoretic bound on adversarial robustness. We evaluate our method on a number of popular benchmarks, show that it can be applied to different architectures, and that it provides robustness to a variety of white-box and black-box attacks, while being simple and fast to train compared to existing alternatives.",
        "conference": "ICML",
        "中文标题": "对抗性鲁棒神经网络的权重-协方差对齐",
        "摘要翻译": "随机神经网络（SNNs）通过在其隐藏层中注入噪声，最近被证明能够实现对对抗攻击的强鲁棒性。然而，现有的SNNs通常是启发式地设计的，并且经常依赖于对抗训练，这在计算上是昂贵的。我们提出了一种新的SNN，它不依赖于对抗训练就能达到最先进的性能，并且有坚实的理论依据。具体来说，现有的SNNs注入的是学习到的或手动调整的各向同性噪声，而我们的SNN学习一个各向异性的噪声分布，以优化对抗鲁棒性的学习理论界限。我们在多个流行基准上评估了我们的方法，表明它可以应用于不同的架构，并且对多种白盒和黑盒攻击提供鲁棒性，同时与现有替代方案相比，训练简单且快速。",
        "领域": "对抗性防御、随机神经网络、深度学习安全",
        "问题": "如何在不依赖计算成本高昂的对抗训练的情况下，提高神经网络对抗攻击的鲁棒性。",
        "动机": "现有的随机神经网络（SNNs）通常依赖于启发式方法和对抗训练，这限制了它们的效率和理论支持。本研究旨在开发一种新的SNN，通过优化各向异性噪声分布来提高对抗鲁棒性，同时避免对抗训练的高计算成本。",
        "方法": "提出了一种新的随机神经网络（SNN），通过学习各向异性噪声分布来优化对抗鲁棒性的学习理论界限，而不是依赖于对抗训练或各向同性噪声注入。",
        "关键词": [
            "对抗性鲁棒性",
            "随机神经网络",
            "各向异性噪声",
            "学习理论界限",
            "白盒和黑盒攻击"
        ],
        "涉及的技术概念": {
            "随机神经网络（SNNs）": "通过在隐藏层中注入噪声来提高模型对抗攻击的鲁棒性。",
            "各向异性噪声分布": "与各向同性噪声不同，各向异性噪声在不同方向上具有不同的噪声强度，能够更有效地优化对抗鲁棒性。",
            "学习理论界限": "用于理论上保证模型对抗鲁棒性的界限，指导噪声分布的学习过程。"
        },
        "success": true
    },
    {
        "order": 1161,
        "title": "Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks",
        "html": "https://ICML.cc//virtual/2021/poster/10197",
        "abstract": "The pairwise interaction paradigm of graph machine learning has predominantly governed the modelling of relational systems. However, graphs alone cannot capture the multi-level interactions present in many complex systems and the expressive power of such schemes was proven to be limited. To overcome these limitations, we propose Message Passing Simplicial Networks (MPSNs), a class of models that perform message passing on simplicial complexes (SCs). To theoretically analyse the expressivity of our model we introduce a Simplicial Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic SCs. We relate the power of SWL to the problem of distinguishing non-isomorphic graphs and show that SWL and MPSNs are strictly more powerful than the WL test and not less powerful than the 3-WL test. We deepen the analysis by comparing our model with traditional graph neural networks (GNNs) with ReLU activations in terms of the number of linear regions of the functions they can represent. We empirically support our theoretical claims by showing that MPSNs can distinguish challenging strongly regular graphs for which GNNs fail and, when equipped with orientation equivariant layers, they can improve classification accuracy in oriented SCs compared to a GNN baseline.",
        "conference": "ICML",
        "中文标题": "Weisfeiler与Lehman走向拓扑：消息传递单纯网络",
        "摘要翻译": "图机器学习的成对交互范式主要支配了关系系统的建模。然而，仅凭图无法捕捉许多复杂系统中存在的多层次交互，并且这种方案的表达能力被证明是有限的。为了克服这些限制，我们提出了消息传递单纯网络（MPSNs），这是一类在单纯复形（SCs）上执行消息传递的模型。为了从理论上分析我们模型的表达能力，我们引入了一种单纯Weisfeiler-Lehman（SWL）着色程序，用于区分非同构的SCs。我们将SWL的能力与区分非同构图的问题联系起来，并表明SWL和MPSNs严格比WL测试更强大，且不比3-WL测试弱。我们通过比较我们的模型与具有ReLU激活的传统图神经网络（GNNs）在它们可以表示的函数的线性区域数量方面的表现，深化了这一分析。我们通过展示MPSNs可以区分GNNs失败的挑战性强正则图，并且当配备方向等变层时，它们可以在定向SCs中与GNN基线相比提高分类准确性，来实证支持我们的理论主张。",
        "领域": "图神经网络、拓扑数据分析、复杂系统建模",
        "问题": "如何捕捉和建模复杂系统中的多层次交互",
        "动机": "克服图机器学习在表达多层次交互方面的限制",
        "方法": "提出消息传递单纯网络（MPSNs），在单纯复形上执行消息传递，并引入单纯Weisfeiler-Lehman（SWL）着色程序以区分非同构的单纯复形",
        "关键词": [
            "消息传递单纯网络",
            "单纯复形",
            "Weisfeiler-Lehman测试",
            "图神经网络",
            "方向等变层"
        ],
        "涉及的技术概念": {
            "单纯复形": "用于捕捉和建模复杂系统中的多层次交互的数学结构",
            "Weisfeiler-Lehman测试": "用于区分非同构的图或单纯复形的着色程序",
            "方向等变层": "增强模型在定向单纯复形上的分类准确性的技术"
        },
        "success": true
    },
    {
        "order": 1162,
        "title": "WGAN with an Infinitely Wide Generator Has No Spurious Stationary Points",
        "html": "https://ICML.cc//virtual/2021/poster/10233",
        "abstract": "Generative adversarial networks (GAN) are a widely used class of deep generative models, but their minimax training dynamics are not understood very well. In this work, we show that GANs with a 2-layer infinite-width generator and a 2-layer finite-width discriminator trained with stochastic gradient ascent-descent have no spurious stationary points. We then show that when the width of the generator is finite but wide, there are no spurious stationary points within a ball whose radius becomes arbitrarily large (to cover the entire parameter space) as the width goes to infinity.",
        "conference": "ICML",
        "中文标题": "无限宽生成器的WGAN无虚假平稳点",
        "摘要翻译": "生成对抗网络（GAN）是一类广泛使用的深度生成模型，但其极小极大训练动态尚未被很好理解。在这项工作中，我们展示了使用随机梯度上升-下降训练的具有两层无限宽度生成器和两层有限宽度判别器的GAN没有虚假平稳点。然后我们表明，当生成器的宽度有限但很宽时，在一个半径随着宽度趋近于无限而变得任意大（以覆盖整个参数空间）的球内没有虚假平稳点。",
        "领域": "生成对抗网络、深度学习理论、优化方法",
        "问题": "理解并证明在特定条件下生成对抗网络训练过程中不存在虚假平稳点",
        "动机": "由于GAN的极小极大训练动态复杂且不完全被理解，研究旨在揭示在无限宽度生成器条件下GAN训练的优化特性，以减少训练中的不稳定性和模式崩溃问题。",
        "方法": "通过理论分析，研究了两层无限宽度生成器和有限宽度判别器的GAN在随机梯度上升-下降训练下的优化特性，证明了在无限宽度条件下无虚假平稳点的存在，并扩展到有限但宽生成器的情况。",
        "关键词": [
            "生成对抗网络",
            "无限宽度",
            "虚假平稳点",
            "优化理论",
            "深度学习"
        ],
        "涉及的技术概念": {
            "无限宽度生成器": "在理论分析中，假设生成器的宽度无限，以简化模型并专注于优化动态的研究。",
            "虚假平稳点": "指在优化过程中遇到的非全局最优的平稳点，可能导致训练停滞或性能不佳。",
            "随机梯度上升-下降": "用于训练GAN的优化方法，交替更新生成器和判别器以寻求纳什均衡。"
        },
        "success": true
    },
    {
        "order": 1163,
        "title": "What Are Bayesian Neural Network Posteriors Really Like?",
        "html": "https://ICML.cc//virtual/2021/poster/10335",
        "abstract": "The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC).  To investigate foundational questions in Bayesian deep learning, we instead use full batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a ``cold posterior'' effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.",
        "conference": "ICML",
        "中文标题": "贝叶斯神经网络后验究竟是什么样子的？",
        "摘要翻译": "贝叶斯神经网络（BNN）参数的后验分布是极高维且非凸的。出于计算效率的考虑，研究者们通常使用小批量方法如均值场变分推断或随机梯度马尔可夫链蒙特卡洛（SGMCMC）来近似这一后验分布。为了探索贝叶斯深度学习中的基础性问题，我们转而采用全批量哈密尔顿蒙特卡洛（HMC）方法应用于现代架构。我们展示了：（1）BNNs能够实现比标准训练和深度集成更显著的性能提升；（2）单一长链HMC能够提供与多个短链相当的后验表示；（3）与最近的研究相反，我们发现后验调温对于接近最优性能并非必需，几乎没有证据支持‘冷后验’效应，我们证明这主要是数据增强的产物；（4）BMA性能对先验规模的选择具有鲁棒性，对于对角高斯、高斯混合和逻辑先验相对类似；（5）贝叶斯神经网络在领域转移下表现出令人惊讶的泛化能力差；（6）虽然更便宜的替代方法如深度集成和SGMCMC可以提供良好的泛化能力，它们的预测分布与HMC不同。值得注意的是，深度集成的预测分布与标准SGLD接近HMC的程度相似，且比标准变分推断更接近。",
        "领域": "贝叶斯深度学习、神经网络优化、不确定性估计",
        "问题": "探索贝叶斯神经网络后验分布的真实性质及其在深度学习中的应用效果。",
        "动机": "理解贝叶斯神经网络后验分布的特性，以及如何有效近似这一分布以提升模型性能和泛化能力。",
        "方法": "采用全批量哈密尔顿蒙特卡洛（HMC）方法对现代神经网络架构进行后验分布的高效近似，并与小批量方法进行比较。",
        "关键词": [
            "贝叶斯神经网络",
            "哈密尔顿蒙特卡洛",
            "后验分布",
            "深度学习",
            "不确定性估计"
        ],
        "涉及的技术概念": {
            "贝叶斯神经网络（BNN）": "一种将贝叶斯推断应用于神经网络的模型，旨在通过后验分布量化模型参数的不确定性。",
            "哈密尔顿蒙特卡洛（HMC）": "一种高效的马尔可夫链蒙特卡洛方法，用于从高维概率分布中采样，特别适用于贝叶斯神经网络的后验近似。",
            "后验调温": "一种调整后验分布温度参数的技术，旨在优化模型的泛化性能，但研究表明其对性能的影响有限。"
        },
        "success": true
    },
    {
        "order": 1164,
        "title": "What does LIME really see in images?",
        "html": "https://ICML.cc//virtual/2021/poster/9699",
        "abstract": "The performance of modern algorithms on certain computer vision tasks such as object recognition is now close to that of humans. This success was achieved at the price of complicated architectures depending on millions of parameters and it has become quite challenging to understand how particular predictions are made. Interpretability methods propose to give us this understanding. In this paper, we study LIME, perhaps one of the most popular. On the theoretical side, we show that when the number of generated examples is large, LIME explanations are concentrated around a limit explanation for which we give an explicit expression. We further this study for elementary shape detectors and linear models. As a consequence of this analysis, we uncover a connection between LIME and integrated gradients, another explanation method. More precisely, the LIME explanations are similar to the sum of integrated gradients over the superpixels used in the preprocessing step of LIME.",
        "conference": "ICML",
        "中文标题": "LIME究竟在图像中看到了什么？",
        "摘要翻译": "现代算法在诸如对象识别等某些计算机视觉任务上的表现现已接近人类水平。这一成功是以依赖数百万参数的复杂架构为代价的，理解特定预测是如何做出的变得相当具有挑战性。可解释性方法旨在提供这种理解。在本文中，我们研究了可能是最受欢迎的LIME。在理论方面，我们表明，当生成的示例数量很大时，LIME解释集中在极限解释周围，我们为此给出了明确的表达式。我们进一步对基本形状检测器和线性模型进行了这项研究。作为这一分析的后果，我们揭示了LIME与另一种解释方法——积分梯度之间的联系。更准确地说，LIME的解释类似于在LIME预处理步骤中使用的超像素上的积分梯度的总和。",
        "领域": "可解释性人工智能、计算机视觉、深度学习模型解释",
        "问题": "理解LIME（局部可解释模型无关解释）方法在图像识别任务中如何生成解释，并揭示其与积分梯度方法之间的联系。",
        "动机": "随着深度学习模型在计算机视觉任务中的成功应用，模型的复杂性和参数数量急剧增加，使得理解模型如何做出特定预测变得困难。研究LIME方法的解释机制，旨在提高模型的可解释性。",
        "方法": "通过理论分析，研究LIME在大量生成示例情况下的解释行为，并探讨其与积分梯度方法的关系。",
        "关键词": [
            "LIME",
            "可解释性",
            "积分梯度",
            "深度学习解释",
            "计算机视觉"
        ],
        "涉及的技术概念": {
            "LIME": "局部可解释模型无关解释方法，用于解释任何机器学习模型的预测，通过局部近似模型的行为来提供解释。",
            "积分梯度": "一种解释方法，通过计算输入特征对模型输出的梯度积分来解释模型的预测。",
            "超像素": "在图像预处理步骤中使用的图像区域，将像素聚合成更大的、有意义的区域，以减少计算复杂度并提高解释的可理解性。"
        },
        "success": true
    },
    {
        "order": 1165,
        "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?",
        "html": "https://ICML.cc//virtual/2021/poster/8587",
        "abstract": "Understanding classifier decision under novel environments is central to the community, and a common practice is evaluating it on labeled test sets. However, in real-world testing, image annotations are difficult and expensive to obtain, especially when the test environment is changing. A natural question then arises: given a trained classifier, can we evaluate its accuracy on varying unlabeled test sets? In this work, we train semantic classification and rotation prediction in a multi-task way. On a series of datasets, we report an interesting finding, i.e., the semantic classification accuracy exhibits a strong linear relationship with the accuracy of the rotation prediction task (Pearson's Correlation  r > 0.88). This finding allows us to utilize linear regression to estimate classifier performance from the accuracy of rotation prediction which can be obtained on the test set through the freely generated rotation labels. ",
        "conference": "ICML",
        "中文标题": "旋转预测能告诉我们关于分类器在不同测试环境下准确性的什么信息？",
        "摘要翻译": "理解分类器在新环境下的决策是社区关注的核心问题，常见的做法是在有标签的测试集上进行评估。然而，在现实世界的测试中，图像标注既困难又昂贵，尤其是在测试环境不断变化的情况下。这就引出了一个自然的问题：给定一个训练好的分类器，我们能否评估其在各种未标记测试集上的准确性？在这项工作中，我们以多任务的方式训练了语义分类和旋转预测。在一系列数据集上，我们报告了一个有趣的发现，即语义分类的准确性与旋转预测任务的准确性呈现出强烈的线性关系（皮尔逊相关系数r > 0.88）。这一发现使我们能够利用线性回归从旋转预测的准确性中估计分类器的性能，而旋转预测的准确性可以通过自由生成的旋转标签在测试集上获得。",
        "领域": "图像分类、多任务学习、模型评估",
        "问题": "如何在不依赖标注数据的情况下评估分类器在不同测试环境下的准确性。",
        "动机": "现实世界中获取标注数据困难且成本高昂，尤其是在测试环境变化的情况下，需要一种方法来评估分类器在未标记测试集上的性能。",
        "方法": "通过多任务学习同时训练语义分类和旋转预测任务，利用两者准确性之间的线性关系，通过旋转预测的准确性来估计分类器的性能。",
        "关键词": [
            "旋转预测",
            "多任务学习",
            "模型评估",
            "线性回归",
            "未标记数据"
        ],
        "涉及的技术概念": {
            "多任务学习": "同时训练分类器和旋转预测任务，以提高模型的泛化能力和效率。",
            "旋转预测": "通过预测图像的旋转角度来作为辅助任务，帮助模型学习更鲁棒的特征表示。",
            "线性回归": "用于建立旋转预测准确性和分类器准确性之间的线性关系模型，从而估计分类器的性能。"
        },
        "success": true
    },
    {
        "order": 1166,
        "title": "What Makes for End-to-End Object Detection?",
        "html": "https://ICML.cc//virtual/2021/poster/8867",
        "abstract": "Object detection has recently achieved a breakthrough for removing the last one non-differentiable component in the pipeline, Non-Maximum Suppression (NMS), and building up an end-to-end system. However, what makes for its one-to-one prediction has not been well understood. In this paper, we first point out that one-to-one positive sample assignment is the key factor, while, one-to-many assignment in previous detectors causes redundant predictions in inference. Second, we surprisingly find that even training with one-to-one assignment, previous detectors still produce redundant predictions. We identify that classification cost in matching cost is the main ingredient: (1) previous detectors only consider location cost, (2) by additionally introducing classification cost, previous detectors immediately produce one-to-one prediction during inference. We introduce the concept of score gap to explore the effect of matching cost. Classification cost enlarges the score gap by choosing positive samples as those of highest score in the training iteration and reducing noisy positive samples brought by only location cost. Finally, we demonstrate the advantages of end-to-end object detection on crowded scenes. ",
        "conference": "ICML",
        "中文标题": "什么造就了端到端的目标检测？",
        "摘要翻译": "目标检测最近取得了一项突破，即移除了流程中最后一个不可微分的组件——非极大值抑制（NMS），并建立了一个端到端的系统。然而，什么使其能够进行一对一预测尚未得到充分理解。在本文中，我们首先指出，一对一的正面样本分配是关键因素，而先前检测器中的一对多分配在推理过程中导致了冗余预测。其次，我们惊讶地发现，即使使用一对一分配进行训练，先前的检测器仍然会产生冗余预测。我们确定匹配成本中的分类成本是主要原因：（1）先前的检测器仅考虑位置成本，（2）通过额外引入分类成本，先前的检测器在推理过程中立即产生一对一预测。我们引入了分数差距的概念来探索匹配成本的影响。分类成本通过选择训练迭代中得分最高的样本作为正面样本，并减少仅由位置成本带来的噪声正面样本，从而扩大了分数差距。最后，我们展示了端到端目标检测在拥挤场景中的优势。",
        "领域": "目标检测",
        "问题": "理解并实现端到端目标检测中的一对一预测机制",
        "动机": "探索端到端目标检测系统中一对一预测的关键因素，解决冗余预测问题",
        "方法": "分析一对一正面样本分配的重要性，引入分类成本以优化匹配成本，利用分数差距概念提升模型性能",
        "关键词": [
            "端到端学习",
            "目标检测",
            "非极大值抑制",
            "一对一预测",
            "匹配成本"
        ],
        "涉及的技术概念": {
            "非极大值抑制（NMS）": "传统目标检测流程中用于去除冗余边界框的后处理步骤，本文中通过端到端系统移除",
            "一对一正面样本分配": "本文提出的关键因素，用于减少推理过程中的冗余预测",
            "匹配成本": "本文中通过引入分类成本优化，以实现更有效的一对一预测"
        },
        "success": true
    },
    {
        "order": 1167,
        "title": "What's in the Box? Exploring the Inner Life of Neural Networks with Robust Rules",
        "html": "https://ICML.cc//virtual/2021/poster/8539",
        "abstract": "We propose a novel method for exploring how neurons within neural networks interact. In particular, we consider activation values of a network for given data, and propose to mine noise-robust rules of the form X → Y , where X and Y are sets of neurons in different layers. We identify the best set of rules by the Minimum Description Length Principle as the rules that together are most descriptive of the activation data. To learn good rule sets in practice, we propose the unsupervised ExplaiNN algorithm. Extensive evaluation shows that the patterns it discovers give clear insight in how networks perceive the world: they identify shared, respectively class-specific traits, compositionality within the network, as well as locality in convolutional layers. Moreover, these patterns are not only easily interpretable, but also supercharge prototyping as they identify which groups of neurons to consider in unison.",
        "conference": "ICML",
        "中文标题": "盒子里有什么？用稳健规则探索神经网络的内部生活",
        "摘要翻译": "我们提出了一种新颖的方法来探索神经网络中神经元之间如何相互作用。具体来说，我们考虑了网络对于给定数据的激活值，并提出挖掘形式为X→Y的噪声稳健规则，其中X和Y是不同层中的神经元集合。我们通过最小描述长度原则识别最佳规则集，这些规则共同最能描述激活数据。为了在实践中学习好的规则集，我们提出了无监督的ExplaiNN算法。广泛的评估表明，它发现的模式清晰地展示了网络如何感知世界：它们识别了共享的、类别特定的特征、网络内的组合性，以及卷积层中的局部性。此外，这些模式不仅易于解释，而且还能加速原型设计，因为它们指出了哪些神经元组需要统一考虑。",
        "领域": "神经网络解释性、深度学习模型分析、卷积神经网络",
        "问题": "探索神经网络内部神经元之间的相互作用及其对数据激活的响应模式。",
        "动机": "为了更深入地理解神经网络如何工作，以及它们如何感知和处理信息，从而提高模型的可解释性和设计效率。",
        "方法": "提出了一种基于最小描述长度原则的无监督算法ExplaiNN，用于挖掘神经网络中神经元之间的噪声稳健规则。",
        "关键词": [
            "神经网络解释性",
            "最小描述长度",
            "无监督学习",
            "神经元交互",
            "ExplaiNN算法"
        ],
        "涉及的技术概念": {
            "最小描述长度原则": "用于识别最能描述激活数据的最佳规则集，作为规则选择的准则。",
            "无监督学习": "ExplaiNN算法不依赖于标记数据，自主挖掘神经元间的交互规则。",
            "卷积层局部性": "指在卷积神经网络中，神经元对输入数据的局部区域有响应，这一特性在ExplaiNN发现的模式中被识别和分析。"
        },
        "success": true
    },
    {
        "order": 1168,
        "title": "When All We Need is a Piece of the Pie: A Generic Framework for Optimizing Two-way Partial AUC",
        "html": "https://ICML.cc//virtual/2021/poster/8431",
        "abstract": "  The Area Under the ROC Curve (AUC) is a crucial metric for machine learning, which evaluates the average performance over all possible True Positive Rates (TPRs) and False Positive Rates (FPRs). Based on the knowledge that a skillful classifier should simultaneously embrace a high TPR and a low FPR, we turn to study a more general variant called Two-way Partial AUC (TPAUC), where only the region with $\\mathsf{TPR} \\ge \\alpha, \\mathsf{FPR} \\le \\beta$ is included in the area. Moreover, a recent work shows that the TPAUC is essentially inconsistent with the existing Partial AUC metrics where only the FPR range is restricted, opening a new problem to seek solutions to leverage high TPAUC. Motivated by this, we present the first trial in this paper to optimize this new metric.  The critical challenge along this course lies in the difficulty of performing gradient-based optimization with end-to-end stochastic training, even with a proper choice of surrogate loss. To address this issue, we propose a generic framework to construct surrogate optimization problems, which supports efficient end-to-end training with deep-learning. Moreover, our theoretical analyses show that: 1) the objective function of the surrogate problems will achieve an upper bound of the original problem under mild conditions, and 2) optimizing the surrogate problems leads to good generalization performance in terms of TPAUC with a high probability. Finally, empirical studies over several benchmark datasets speak to the efficacy of our framework.\n  \n",
        "conference": "ICML",
        "中文标题": "当我们只需要一片派时：优化双向部分AUC的通用框架",
        "摘要翻译": "ROC曲线下面积（AUC）是机器学习中的一个关键指标，它评估了在所有可能的真正率（TPR）和假正率（FPR）下的平均性能。基于一个熟练的分类器应同时具备高TPR和低FPR的认识，我们转而研究一个更通用的变体，称为双向部分AUC（TPAUC），其中仅包括TPR≥α、FPR≤β的区域。此外，最近的一项工作表明，TPAUC与现有的仅限制FPR范围的部分AUC指标本质上不一致，这为寻求利用高TPAUC的解决方案开辟了新问题。受此启发，我们在本文中首次尝试优化这一新指标。这一过程中的关键挑战在于，即使选择了合适的替代损失函数，进行基于梯度的优化与端到端的随机训练也存在困难。为了解决这个问题，我们提出了一个构建替代优化问题的通用框架，该框架支持与深度学习的高效端到端训练。此外，我们的理论分析表明：1）在温和条件下，替代问题的目标函数将达到原始问题的上界；2）优化替代问题在TPAUC方面以高概率带来良好的泛化性能。最后，对几个基准数据集的实证研究证明了我们框架的有效性。",
        "领域": "机器学习优化、ROC曲线分析、深度学习训练策略",
        "问题": "如何优化双向部分AUC（TPAUC）这一新指标",
        "动机": "现有的部分AUC指标与TPAUC不一致，需要新的解决方案来优化TPAUC",
        "方法": "提出一个构建替代优化问题的通用框架，支持高效的端到端深度学习训练",
        "关键词": [
            "双向部分AUC",
            "替代优化",
            "深度学习训练",
            "ROC曲线分析",
            "机器学习优化"
        ],
        "涉及的技术概念": {
            "双向部分AUC（TPAUC）": "一种更通用的ROC曲线下面积变体，仅包括特定TPR和FPR范围的区域",
            "替代优化问题": "为克服直接优化TPAUC的困难，构建的易于优化的替代问题",
            "端到端训练": "支持从输入到输出的完整模型训练流程，提高训练效率和模型性能"
        },
        "success": true
    },
    {
        "order": 1169,
        "title": "When Does Data Augmentation Help With Membership Inference Attacks?",
        "html": "https://ICML.cc//virtual/2021/poster/9601",
        "abstract": "Deep learning models often raise privacy concerns as they leak information about their training data. This leakage enables membership inference attacks (MIA) that can identify whether a data point was in a model's training set. Research shows that some 'data augmentation' mechanisms may reduce the risk by combatting a key factor increasing the leakage, overfitting. While many mechanisms exist, their effectiveness against MIAs and privacy properties have not been studied systematically. Employing two recent MIAs, we explore the lower bound on the risk in the absence of formal upper bounds. First, we evaluate 7 mechanisms and differential privacy, on three image classification tasks. We find that applying augmentation to increase the model's utility does not mitigate the risk and protection comes with a utility penalty. Further, we also investigate why popular label smoothing mechanism consistently amplifies the risk. Finally, we propose 'loss-rank-correlation' (LRC) metric to assess how similar the effects of different mechanisms are. This, for example, reveals the similarity of applying high-intensity augmentation against MIAs to simply reducing the training time. Our findings emphasize the utility-privacy trade-off and provide practical guidelines on using augmentation to manage the trade-off.  ",
        "conference": "ICML",
        "中文标题": "数据增强何时有助于成员推理攻击？",
        "摘要翻译": "深度学习模型常常引发隐私担忧，因为它们会泄露关于其训练数据的信息。这种泄露使得成员推理攻击（MIA）能够识别一个数据点是否在模型的训练集中。研究表明，某些'数据增强'机制可能通过对抗增加泄露的关键因素——过拟合，来降低风险。虽然存在许多机制，但它们对MIAs的有效性和隐私属性尚未被系统地研究。采用两种最近的MIAs，我们探索了在没有形式上界的情况下的风险下限。首先，我们在三个图像分类任务上评估了7种机制和差分隐私。我们发现，应用增强来提高模型的效用并不能减轻风险，保护伴随着效用的损失。此外，我们还研究了为什么流行的标签平滑机制会持续放大风险。最后，我们提出了'损失-排名-相关性'（LRC）度量来评估不同机制效果的相似性。例如，这揭示了应用高强度增强对抗MIAs与简单地减少训练时间的相似性。我们的发现强调了效用与隐私之间的权衡，并提供了关于使用增强来管理这种权衡的实用指南。",
        "领域": "隐私保护深度学习、成员推理攻击、数据增强",
        "问题": "研究数据增强机制在减少深度学习模型隐私泄露风险方面的有效性。",
        "动机": "深度学习模型在提高性能的同时，可能泄露训练数据信息，引发隐私担忧。研究旨在探索数据增强如何影响成员推理攻击的风险。",
        "方法": "采用两种最新的成员推理攻击方法，评估7种数据增强机制和差分隐私在三个图像分类任务上的效果，并提出'损失-排名-相关性'（LRC）度量来比较不同机制的效果。",
        "关键词": [
            "成员推理攻击",
            "数据增强",
            "隐私保护",
            "效用-隐私权衡",
            "损失-排名-相关性"
        ],
        "涉及的技术概念": {
            "成员推理攻击（MIA）": "一种攻击方法，用于判断特定数据是否属于模型的训练集，从而泄露隐私信息。",
            "数据增强": "通过变换训练数据来增加数据多样性，旨在提高模型的泛化能力和减少过拟合。",
            "损失-排名-相关性（LRC）": "论文提出的新度量标准，用于评估不同数据增强机制在对抗成员推理攻击时的效果相似性。"
        },
        "success": true
    },
    {
        "order": 1170,
        "title": "Which transformer architecture fits my data?  A vocabulary bottleneck in self-attention",
        "html": "https://ICML.cc//virtual/2021/poster/8763",
        "abstract": "After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (i.e., 10x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. \nAs an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of 25%-50% in leading NLP models such as ALBERT and T5.",
        "conference": "ICML",
        "中文标题": "哪种Transformer架构适合我的数据？自注意力中的词汇瓶颈",
        "摘要翻译": "Transformer架构在自然语言处理领域成功亮相后，现已成为许多领域的事实标准。它们在新模态上部署的一个障碍是架构配置：深度与宽度的最佳比例已被证明在不同数据类型间存在巨大差异（例如，图像上的比例比语言上的大10倍）。我们从理论上预测了存在一个嵌入秩瓶颈，该瓶颈限制了自注意力宽度对Transformer表达能力的贡献。因此，我们直接将输入词汇量和秩与最佳深度与宽度比例联系起来，因为小的词汇量或秩决定了深度相对于宽度的额外优势。我们通过实证证明了这一瓶颈的存在及其对Transformer架构深度与宽度相互作用的影响，将跨领域的架构可变性与不同领域中使用不同词汇量或嵌入秩的常被忽视的做法联系起来。作为一个额外的好处，我们的秩瓶颈框架使我们能够识别出领先的NLP模型（如ALBERT和T5）中25%-50%的大小冗余。",
        "领域": "自然语言处理与视觉结合",
        "问题": "确定Transformer架构在不同数据类型上的最佳深度与宽度比例",
        "动机": "解决Transformer架构在新模态上部署时因架构配置（特别是深度与宽度比例）差异而遇到的障碍",
        "方法": "理论预测嵌入秩瓶颈的存在，并通过实证研究验证其对Transformer架构表达能力的影响",
        "关键词": [
            "Transformer架构",
            "自注意力",
            "词汇瓶颈",
            "深度与宽度比例",
            "嵌入秩"
        ],
        "涉及的技术概念": {
            "嵌入秩瓶颈": "限制自注意力宽度对Transformer表达能力贡献的理论预测现象",
            "自注意力": "Transformer架构中的核心机制，用于捕捉输入数据内部的依赖关系",
            "深度与宽度比例": "描述Transformer架构中网络深度与宽度之间关系的参数，影响模型的表达能力和效率"
        },
        "success": true
    },
    {
        "order": 1171,
        "title": "Whitening and Second Order Optimization Both Make Information in the Dataset Unusable During Training, and Can Reduce or Prevent Generalization",
        "html": "https://ICML.cc//virtual/2021/poster/8717",
        "abstract": "Machine learning is predicated on the concept of generalization: a model achieving low error on a sufficiently large training set should also perform well on novel samples from the same distribution. We show that both data whitening and second order optimization can harm or entirely prevent generalization. In general, model training harnesses information contained in the sample-sample second moment matrix of a dataset. For a general class of models, namely models with a fully connected first layer, we prove that the information contained in this matrix is the only information which can be used to generalize. Models trained using whitened data, or with certain second order optimization schemes, have less access to this information, resulting in reduced or nonexistent generalization ability. We experimentally verify these predictions for several architectures, and further demonstrate that generalization continues to be harmed even when theoretical requirements are relaxed. However, we also show experimentally that regularized second order optimization can provide a practical tradeoff, where training is accelerated but less information is lost, and generalization can in some circumstances even improve.",
        "conference": "ICML",
        "success": true,
        "中文标题": "数据白化和二阶优化在训练过程中使数据集中的信息无法使用，并可能减少或阻止泛化",
        "摘要翻译": "机器学习的基础是泛化概念：在足够大的训练集上达到低误差的模型，在来自同一分布的新样本上也应表现良好。我们表明，数据白化和二阶优化都可能损害或完全阻止泛化。通常，模型训练利用了数据集中样本-样本二阶矩矩阵包含的信息。对于一类广泛的模型，即具有全连接第一层的模型，我们证明该矩阵中包含的信息是可用于泛化的唯一信息。使用白化数据或某些二阶优化方案训练的模型，对这些信息的访问减少，导致泛化能力降低或不存在。我们通过实验验证了这些预测适用于几种架构，并进一步证明即使放宽理论要求，泛化仍会受到损害。然而，我们还通过实验表明，正则化的二阶优化可以提供一种实用的权衡，即训练加速但信息丢失减少，在某些情况下泛化甚至可以得到改善。",
        "领域": "深度学习理论、优化算法、泛化能力研究",
        "问题": "数据白化和二阶优化如何影响模型的泛化能力",
        "动机": "探索数据预处理和优化方法对机器学习模型泛化能力的影响，以指导更有效的模型训练策略",
        "方法": "理论分析和实验验证，包括对全连接第一层模型的理论证明，以及对不同架构的实验测试",
        "关键词": [
            "数据白化",
            "二阶优化",
            "泛化能力",
            "机器学习理论",
            "优化算法"
        ],
        "涉及的技术概念": {
            "数据白化": "一种数据预处理技术，旨在去除数据中的冗余信息，使特征之间不相关且具有相同的方差，但在本研究中发现可能损害模型的泛化能力",
            "二阶优化": "利用二阶导数信息（如Hessian矩阵）来指导优化过程的算法，可以加速训练但可能减少模型对数据集中信息的利用",
            "泛化能力": "模型在未见过的数据上表现良好的能力，是机器学习模型成功的关键指标"
        }
    },
    {
        "order": 1172,
        "title": "Whitening for Self-Supervised Representation Learning",
        "html": "https://ICML.cc//virtual/2021/poster/10241",
        "abstract": "Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance  ('positives') are contrasted with instances extracted from other images ('negatives'). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a 'scattering' effect on the batch samples,  avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution  does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.",
        "conference": "ICML",
        "中文标题": "自监督表示学习中的白化技术",
        "摘要翻译": "当前大多数自监督表示学习（SSL）方法基于对比损失和实例区分任务，其中同一图像实例的增强版本（'正样本'）与其他图像中提取的实例（'负样本'）进行对比。为了使学习有效，需要将许多负样本与一个正样本对进行比较，这在计算上是昂贵的。在本文中，我们提出了一个不同的方向和一个新的SSL损失函数，该函数基于潜在空间特征的白化。白化操作对批次样本具有'散射'效应，避免了所有样本表示崩溃到单个点的退化解决方案。我们的解决方案不需要非对称网络，概念上简单。此外，由于不需要负样本，我们可以从同一图像实例中提取多个正样本对。该方法的源代码及所有实验的代码可在以下网址获取：https://github.com/htdt/self-supervised。",
        "领域": "自监督学习、特征表示学习、计算机视觉",
        "问题": "解决自监督表示学习中需要大量负样本对比导致计算成本高的问题",
        "动机": "减少自监督学习中的计算需求，同时避免表示崩溃的问题",
        "方法": "提出基于潜在空间特征白化的新损失函数，避免使用负样本，允许从同一图像实例中提取多个正样本对",
        "关键词": [
            "自监督学习",
            "白化技术",
            "表示学习",
            "对比损失",
            "特征散射"
        ],
        "涉及的技术概念": {
            "白化操作": "用于对潜在空间特征进行散射，避免样本表示崩溃到单个点",
            "对比损失": "传统SSL方法中用于区分正负样本的损失函数",
            "实例区分任务": "SSL中的一种任务，旨在区分不同图像实例的表示"
        },
        "success": true
    },
    {
        "order": 1173,
        "title": "Whittle Networks: A Deep Likelihood Model for Time Series",
        "html": "https://ICML.cc//virtual/2021/poster/10295",
        "abstract": "hile probabilistic circuits have been extensively explored for tabular data, less attention has been paid to time series. Here, the goal is to estimate joint densities among the entire time series and, in turn, determining, for instance, conditional independence relations between them. To this end, we propose the first probabilistic circuits (PCs) approach for modeling the joint distribution of multivariate time series, called Whittle sum-product networks (WSPNs). WSPNs leverage the Whittle approximation, casting the likelihood in the frequency domain, and place a complex-valued sum-product network, the most prominent PC, over the frequencies. The conditional independence relations among the time series can then be determined efficiently in the spectral domain. Moreover, WSPNs can naturally be placed into the deep neural learning stack for time series, resulting in Whittle Networks, opening the likelihood toolbox for training deep neural models and inspecting their behaviour. Our experiments show that Whittle Networks can indeed capture complex dependencies between time series and provide a useful measure of uncertainty for neural networks. ",
        "conference": "ICML",
        "中文标题": "惠特尔网络：一种用于时间序列的深度似然模型",
        "摘要翻译": "虽然概率电路已广泛用于表格数据的研究，但对时间序列的关注较少。这里的目的是估计整个时间序列的联合密度，进而确定它们之间的条件独立关系。为此，我们提出了第一种用于建模多元时间序列联合分布的概率电路（PCs）方法，称为惠特尔和积网络（WSPNs）。WSPNs利用惠特尔近似，在频域中转换似然，并在频率上放置一个复数值的和积网络，这是最突出的概率电路。然后，可以在谱域中高效地确定时间序列之间的条件独立关系。此外，WSPNs可以自然地融入时间序列的深度神经网络堆栈中，形成惠特尔网络，为训练深度神经模型和检查其行为打开了似然工具箱。我们的实验表明，惠特尔网络确实能够捕捉时间序列之间的复杂依赖关系，并为神经网络提供有用的不确定性度量。",
        "领域": "时间序列分析、深度学习、概率图模型",
        "问题": "估计多元时间序列的联合密度并确定条件独立关系",
        "动机": "扩展概率电路在时间序列数据上的应用，提供一种新的方法来建模时间序列的联合分布和条件独立关系",
        "方法": "提出惠特尔和积网络（WSPNs），利用惠特尔近似在频域中转换似然，使用复数值的和积网络建模，并集成到深度神经网络中",
        "关键词": [
            "惠特尔网络",
            "概率电路",
            "时间序列分析",
            "联合密度估计",
            "条件独立关系"
        ],
        "涉及的技术概念": {
            "概率电路（PCs）": "用于建模多元时间序列联合分布的概率图模型方法",
            "惠特尔近似": "在频域中转换似然的技术，用于简化时间序列分析",
            "和积网络（SPNs）": "一种概率电路，用于在频域中高效建模和推断"
        },
        "success": true
    },
    {
        "order": 1174,
        "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
        "html": "https://ICML.cc//virtual/2021/poster/10117",
        "abstract": "Distribution shifts---where the training distribution differs from the test distribution---can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. The full paper, code, and leaderboards are available at https://wilds.stanford.edu.\n",
        "conference": "ICML",
        "中文标题": "WILDS：野外分布偏移的基准测试",
        "摘要翻译": "分布偏移——训练分布与测试分布不同——可能会显著降低部署在野外的机器学习（ML）系统的准确性。尽管这些分布偏移在现实世界的部署中无处不在，但在今天ML社区广泛使用的数据集中，这些分布偏移的代表性不足。为了填补这一空白，我们提出了WILDS，一个精心策划的10个数据集的基准测试，反映了现实世界应用中自然出现的各种分布偏移，如肿瘤识别中医院间的偏移；野生动物监测中相机陷阱间的偏移；以及卫星成像和贫困地图绘制中时间和地点的偏移。在每个数据集上，我们展示了标准训练导致的分布外性能显著低于分布内性能。即使使用现有方法训练的模型来解决分布偏移，这一差距仍然存在，这强调了需要新的方法来训练对实践中出现的分布偏移类型更加鲁棒的模型。为了促进方法开发，我们提供了一个开源包，自动化数据集加载，包含默认模型架构和超参数，并标准化评估。完整的论文、代码和排行榜可在https://wilds.stanford.edu获取。",
        "领域": "机器学习鲁棒性、分布偏移适应、野外部署",
        "问题": "解决机器学习模型在分布偏移情况下的性能下降问题",
        "动机": "现实世界中的分布偏移普遍存在，但现有数据集和模型训练方法未能充分代表和应对这些偏移，导致模型在野外部署时性能下降",
        "方法": "提出WILDS基准测试，包含10个反映现实世界分布偏移的数据集，并提供开源工具包支持模型训练和评估",
        "关键词": [
            "分布偏移",
            "机器学习鲁棒性",
            "野外部署",
            "基准测试",
            "开源工具包"
        ],
        "涉及的技术概念": {
            "分布偏移": "训练和测试数据分布不一致，导致模型性能下降的现象",
            "机器学习鲁棒性": "模型在面对数据分布变化时保持性能的能力",
            "野外部署": "将机器学习模型应用于现实世界，面对不可控和变化的环境"
        },
        "success": true
    },
    {
        "order": 1175,
        "title": "Winograd Algorithm for AdderNet",
        "html": "https://ICML.cc//virtual/2021/poster/8909",
        "abstract": "Adder neural network (AdderNet) is a new kind of deep model that replaces the original massive multiplications in convolutions by additions while preserving the high performance. Since the hardware complexity of additions is much lower than that of multiplications, the overall energy consumption is thus reduced significantly. To further optimize the hardware overhead of using AdderNet, this paper studies the winograd algorithm, which is a widely used fast algorithm for accelerating convolution and saving the computational costs. Unfortunately, the conventional Winograd algorithm cannot be directly applied to AdderNets since the distributive law in multiplication is not valid for the l1-norm. Therefore, we replace the element-wise multiplication in the Winograd equation by additions and then develop a new set of transform matrixes that can enhance the representation ability of output features to maintain the performance. Moreover, we propose the l2-to-l1 training strategy to mitigate the negative impacts caused by formal inconsistency. Experimental results on both FPGA and benchmarks show that the new method can further reduce the energy consumption without affecting the accuracy of the original AdderNet.",
        "conference": "ICML",
        "中文标题": "用于加法神经网络的Winograd算法",
        "摘要翻译": "加法神经网络（AdderNet）是一种新型的深度模型，它通过加法替换卷积中原有的大量乘法运算，同时保持高性能。由于加法的硬件复杂度远低于乘法，因此整体能耗显著降低。为了进一步优化使用加法神经网络的硬件开销，本文研究了Winograd算法，这是一种广泛用于加速卷积并节省计算成本的快速算法。遗憾的是，传统的Winograd算法不能直接应用于加法神经网络，因为乘法中的分配律对于l1范数不成立。因此，我们将Winograd方程中的元素乘法替换为加法，然后开发了一组新的变换矩阵，可以增强输出特征的表示能力以保持性能。此外，我们提出了l2到l1的训练策略，以减轻形式不一致带来的负面影响。在FPGA和基准测试上的实验结果表明，新方法可以在不影响原始加法神经网络准确性的情况下进一步降低能耗。",
        "领域": "神经网络优化、硬件加速、节能计算",
        "问题": "如何在不影响加法神经网络性能的前提下，进一步降低其硬件实现的能耗",
        "动机": "加法神经网络通过加法替换乘法降低了能耗，但传统的Winograd算法无法直接应用于加法神经网络，限制了其硬件加速潜力",
        "方法": "开发新的变换矩阵替换Winograd方程中的乘法为加法，并提出l2到l1的训练策略以保持性能",
        "关键词": [
            "加法神经网络",
            "Winograd算法",
            "硬件加速",
            "节能计算",
            "l1范数"
        ],
        "涉及的技术概念": {
            "加法神经网络（AdderNet）": "一种通过加法替换卷积中乘法运算的深度模型，旨在降低能耗",
            "Winograd算法": "一种快速算法，用于加速卷积运算并节省计算成本，本文中针对加法神经网络进行了适配",
            "l1范数": "在加法神经网络中用于衡量特征差异的范数，替代了传统卷积中的乘法运算"
        },
        "success": true
    },
    {
        "order": 1176,
        "title": "World Model as a Graph: Learning Latent Landmarks for Planning",
        "html": "https://ICML.cc//virtual/2021/poster/10063",
        "abstract": "Planning, the ability to analyze the structure of a problem in the large and decompose it into interrelated subproblems, is a hallmark of human intelligence. While deep reinforcement learning (RL) has shown great promise for solving relatively straightforward control tasks, it remains an open problem how to best incorporate planning into existing deep RL paradigms to handle increasingly complex environments. One prominent framework, Model-Based RL, learns a world model and plans using step-by-step virtual rollouts. This type of world model quickly diverges from reality when the planning horizon increases, thus struggling at long-horizon planning. How can we learn world models that endow agents with the ability to do temporally extended reasoning? In this work, we propose to learn graph-structured world models composed of sparse, multi-step transitions. We devise a novel algorithm to learn latent landmarks that are scattered (in terms of reachability) across the goal space as the nodes on the graph. In this same graph, the edges are the reachability estimates distilled from Q-functions. On a variety of high-dimensional continuous control tasks ranging from robotic manipulation to navigation, we demonstrate that our method, named L3P, significantly outperforms prior work, and is oftentimes the only method capable of leveraging both the robustness of model-free RL and generalization of graph-search algorithms. We believe our work is an important step towards scalable planning in reinforcement learning.",
        "conference": "ICML",
        "中文标题": "世界模型作为图：学习潜在地标以进行规划",
        "摘要翻译": "规划，即分析问题的整体结构并将其分解为相互关联的子问题的能力，是人类智能的标志。尽管深度强化学习（RL）在解决相对直接的控制任务方面显示出了巨大的潜力，但如何最好地将规划融入现有的深度RL范式以处理日益复杂的环境仍然是一个未解决的问题。一个突出的框架，基于模型的RL，学习一个世界模型并使用逐步虚拟滚动进行规划。这种类型的世界模型在规划视野增加时迅速与现实脱节，因此在长期视野规划中挣扎。我们如何学习能够赋予代理进行时间扩展推理能力的世界模型？在这项工作中，我们提出学习由稀疏、多步转换组成的图结构世界模型。我们设计了一种新颖的算法来学习潜在地标，这些地标在目标空间中（就可达性而言）分散作为图上的节点。在同一图中，边是从Q函数中提取的可达性估计。在从机器人操作到导航的各种高维连续控制任务上，我们证明了我们的方法（名为L3P）显著优于先前的工作，并且通常是唯一能够利用无模型RL的鲁棒性和图搜索算法的泛化能力的方法。我们相信我们的工作是朝着强化学习中可扩展规划迈出的重要一步。",
        "领域": "深度强化学习、机器人控制、导航规划",
        "问题": "如何在深度强化学习中有效地融入规划以处理复杂环境",
        "动机": "解决现有基于模型的强化学习在长期视野规划中因世界模型与现实脱节而表现不佳的问题",
        "方法": "提出学习图结构的世界模型，其中节点为潜在地标，边为从Q函数中提取的可达性估计，以支持时间扩展推理",
        "关键词": [
            "图结构世界模型",
            "潜在地标",
            "深度强化学习",
            "长期规划",
            "可达性估计"
        ],
        "涉及的技术概念": {
            "图结构世界模型": "将世界模型表示为图，其中节点代表潜在地标，边代表状态间的可达性，以支持复杂的规划任务",
            "潜在地标": "在目标空间中分散的关键点，作为图中的节点，帮助代理理解和规划长期目标",
            "Q函数": "用于估计状态-动作对的长期回报，本文中用于提取状态间的可达性信息，构建图的边"
        },
        "success": true
    },
    {
        "order": 1177,
        "title": "XOR-CD: Linearly Convergent Constrained Structure Generation",
        "html": "https://ICML.cc//virtual/2021/poster/9143",
        "abstract": "We propose XOR-Contrastive Divergence learning (XOR-CD), a provable approach for constrained structure generation, which remains difficult for state-of-the-art neural network and constraint reasoning approaches. XOR-CD harnesses XOR-Sampling to generate samples from the model distribution in CD learning and is guaranteed to generate valid structures. In addition, XOR-CD has a linear convergence rate towards the global maximum of the likelihood function within a vanishing constant in learning exponential family models. Constraint satisfaction enabled by XOR-CD also boosts its learning performance. Our real-world experiments on data-driven experimental design, dispatching route generation, and sequence-based protein homology detection demonstrate the superior performance of XOR-CD compared to baseline approaches in generating valid structures as well as capturing the inductive bias in the training set. ",
        "conference": "ICML",
        "中文标题": "XOR-CD：线性收敛的约束结构生成方法",
        "摘要翻译": "我们提出了XOR-对比散度学习（XOR-CD），一种可证明的约束结构生成方法，这对于当前最先进的神经网络和约束推理方法来说仍然是一个难题。XOR-CD利用XOR采样在CD学习中从模型分布生成样本，并保证生成有效的结构。此外，XOR-CD在学习指数族模型时，对似然函数的全局最大值具有线性收敛速度，且收敛常数趋近于零。通过XOR-CD实现的约束满足也提升了其学习性能。我们在数据驱动的实验设计、调度路线生成和基于序列的蛋白质同源性检测等实际应用中的实验表明，与基线方法相比，XOR-CD在生成有效结构以及捕捉训练集中的归纳偏差方面表现出卓越的性能。",
        "领域": "约束结构生成、数据驱动的实验设计、蛋白质同源性检测",
        "问题": "解决在约束条件下生成有效结构的问题，以及如何在学习过程中保证结构的有效性。",
        "动机": "当前最先进的神经网络和约束推理方法在生成满足特定约束条件的结构方面存在困难，XOR-CD旨在提供一种可证明有效的方法来解决这一问题。",
        "方法": "采用XOR-对比散度学习（XOR-CD）方法，结合XOR采样技术，从模型分布中生成满足约束条件的样本，并保证生成的结构有效性。",
        "关键词": [
            "XOR-CD",
            "约束结构生成",
            "对比散度学习",
            "XOR采样",
            "线性收敛"
        ],
        "涉及的技术概念": {
            "XOR-对比散度学习（XOR-CD）": "一种结合了XOR采样技术的对比散度学习方法，用于在约束条件下生成有效的结构。",
            "XOR采样": "一种采样技术，用于在对比散度学习中从模型分布生成样本，保证生成的结构满足特定的约束条件。",
            "线性收敛": "指XOR-CD方法在学习指数族模型时，对似然函数的全局最大值具有线性收敛速度，且收敛常数趋近于零。"
        },
        "success": true
    },
    {
        "order": 1178,
        "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling",
        "html": "https://ICML.cc//virtual/2021/poster/9755",
        "abstract": "Transformer-based models are widely used in natural language processing (NLP). Central to the transformer model is the self-attention mechanism, which captures the interactions of token pairs in the input sequences and depends quadratically on the sequence length. Training such models on longer sequences is expensive. In this paper, we show that a Bernoulli sampling attention mechanism based on Locality Sensitive Hashing (LSH), decreases the quadratic complexity of such models to linear. We bypass the quadratic cost by considering self-attention as a sum of individual tokens associated with Bernoulli random variables that can, in principle, be sampled at once by a single hash (although in practice, this number may be a small constant). This leads to an efficient sampling scheme to estimate self-attention which relies on specific modifications of LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at https://github.com/mlpen/YOSO.",
        "conference": "ICML",
        "中文标题": "你几乎只需采样一次：通过伯努利采样实现线性成本的自注意力机制",
        "摘要翻译": "基于Transformer的模型在自然语言处理（NLP）中广泛应用。Transformer模型的核心是自注意力机制，它捕获输入序列中令牌对的相互作用，并且依赖于序列长度的二次方。在较长序列上训练此类模型成本高昂。在本文中，我们展示了一种基于局部敏感哈希（LSH）的伯努利采样注意力机制，将此类模型的二次复杂度降低到线性。我们通过将自注意力视为与伯努利随机变量相关联的单个令牌的总和来绕过二次成本，这些随机变量原则上可以通过单个哈希一次采样（尽管在实践中，这个数字可能是一个小的常数）。这导致了一个有效的采样方案来估计自注意力，该方案依赖于LSH的特定修改（以便在GPU架构上部署）。我们在标准512序列长度的GLUE基准上评估了我们的算法，在那里我们看到了相对于标准预训练Transformer的有利性能。在用于评估长序列性能的Long Range Arena（LRA）基准上，我们的方法实现了与softmax自注意力一致的结果，但具有显著的速度提升和内存节省，并且经常优于其他高效的自注意力方法。我们的代码可在https://github.com/mlpen/YOSO获取。",
        "领域": "自然语言处理与视觉结合",
        "问题": "降低Transformer模型在长序列处理中的计算成本",
        "动机": "解决Transformer模型在处理长序列时因自注意力机制的二次复杂度导致的高昂计算成本问题",
        "方法": "提出一种基于局部敏感哈希（LSH）的伯努利采样注意力机制，将自注意力的计算复杂度从二次降低到线性",
        "关键词": [
            "自注意力机制",
            "伯努利采样",
            "局部敏感哈希",
            "线性复杂度",
            "GPU加速"
        ],
        "涉及的技术概念": {
            "自注意力机制": "Transformer模型的核心机制，用于捕获输入序列中令牌对的相互作用",
            "伯努利采样": "一种采样方法，用于估计自注意力，通过将自注意力视为与伯努利随机变量相关联的单个令牌的总和",
            "局部敏感哈希（LSH）": "一种哈希技术，用于高效地近似最近邻搜索，本文中用于降低自注意力计算的复杂度"
        },
        "success": true
    },
    {
        "order": 1179,
        "title": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model",
        "html": "https://ICML.cc//virtual/2021/poster/10257",
        "abstract": "Knowledge distillation (KD) is a successful approach for deep neural network acceleration, with which a compact network (student) is trained by mimicking the softmax output of a pre-trained high-capacity network (teacher). In tradition, KD usually relies on access to the training samples and the parameters of the white-box teacher to acquire the transferred knowledge. However, these prerequisites are not always realistic due to storage costs or privacy issues in real-world applications. Here we propose the concept of decision-based black-box (DB3) knowledge distillation, with which the student is trained by distilling the knowledge from a black-box teacher (parameters are not accessible) that only returns classes rather than softmax outputs. We start with the scenario when the training set is accessible. We represent a sample's robustness against other classes by computing its distances to the teacher's decision boundaries and use it to construct the soft label for each training sample. After that, the student can be trained via standard KD. We then extend this approach to a more challenging scenario in which even accessing the training data is not feasible. We propose to generate pseudo samples that are distinguished by the decision boundaries of the DB3 teacher to the largest extent and construct soft labels for these samples, which are used as the transfer set. We evaluate our approaches on various benchmark networks and datasets and experiment results demonstrate their effectiveness.",
        "conference": "ICML",
        "中文标题": "基于决策的黑盒模型零样本知识蒸馏",
        "摘要翻译": "知识蒸馏（KD）是一种成功的深度神经网络加速方法，通过这种方法，一个紧凑的网络（学生）通过模仿预训练的高容量网络（教师）的softmax输出来进行训练。传统上，KD通常依赖于访问训练样本和白盒教师的参数来获取转移的知识。然而，由于存储成本或隐私问题，在实际应用中这些前提条件并不总是现实的。在这里，我们提出了基于决策的黑盒（DB3）知识蒸馏的概念，通过这种方法，学生通过从一个黑盒教师（参数不可访问）那里蒸馏知识来训练，该教师只返回类别而不是softmax输出。我们从训练集可访问的场景开始。我们通过计算样本到教师决策边界的距离来表示一个样本对其他类别的鲁棒性，并用它来为每个训练样本构建软标签。之后，学生可以通过标准的KD进行训练。然后，我们将这种方法扩展到一个更具挑战性的场景，即甚至访问训练数据也不可行。我们提出生成在最大程度上被DB3教师的决策边界区分的伪样本，并为这些样本构建软标签，这些标签被用作转移集。我们在各种基准网络和数据集上评估了我们的方法，实验结果证明了它们的有效性。",
        "领域": "模型压缩",
        "问题": "在无法访问教师模型参数或训练数据的情况下进行有效的知识蒸馏",
        "动机": "解决由于存储成本或隐私问题导致的传统知识蒸馏方法在实际应用中的限制",
        "方法": "通过计算样本到教师决策边界的距离来表示样本的鲁棒性，构建软标签进行知识蒸馏；在无法访问训练数据时，生成被教师决策边界最大程度区分的伪样本作为转移集",
        "关键词": [
            "知识蒸馏",
            "黑盒模型",
            "决策边界",
            "模型压缩",
            "零样本学习"
        ],
        "涉及的技术概念": {
            "知识蒸馏": "一种模型压缩技术，通过让小型模型模仿大型模型的输出来实现知识转移",
            "决策边界": "在分类问题中，决策边界是模型区分不同类别的界限，用于评估样本的鲁棒性",
            "零样本学习": "在训练阶段没有直接接触到某些类别的样本的情况下，模型能够识别这些类别的能力"
        },
        "success": true
    },
    {
        "order": 1180,
        "title": "Zero-Shot Text-to-Image Generation",
        "html": "https://ICML.cc//virtual/2021/poster/9429",
        "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
        "conference": "ICML",
        "中文标题": "零样本文本到图像生成",
        "摘要翻译": "文本到图像的生成传统上侧重于在固定数据集上寻找更好的建模假设进行训练。这些假设可能涉及复杂的架构、辅助损失或在训练期间提供的侧面信息，如对象部分标签或分割掩码。我们描述了一种基于转换器的简单方法，该方法自回归地将文本和图像令牌建模为单一数据流。在足够的数据和规模下，我们的方法在以零样本方式评估时与以前的领域特定模型具有竞争力。",
        "领域": "文本到图像生成、深度学习模型、计算机视觉",
        "问题": "如何在不需要特定领域训练数据的情况下，实现高质量的文本到图像生成。",
        "动机": "探索一种不依赖于复杂建模假设或额外训练信息，能够通过大规模数据训练实现高效文本到图像转换的方法。",
        "方法": "采用基于转换器的自回归模型，将文本和图像令牌统一建模为单一数据流，利用大规模数据进行训练。",
        "关键词": [
            "零样本学习",
            "文本到图像生成",
            "转换器模型",
            "自回归模型",
            "大规模训练"
        ],
        "涉及的技术概念": {
            "转换器": "用于自回归地建模文本和图像令牌的统一框架，是实现高效文本到图像转换的核心架构。",
            "自回归模型": "通过逐步预测下一个令牌来生成图像，使得模型能够处理文本和图像的联合分布。",
            "零样本学习": "模型在没有特定领域训练数据的情况下，能够理解和生成未见过的文本描述对应的图像。"
        },
        "success": true
    },
    {
        "order": 1181,
        "title": "Zeroth-Order Non-Convex Learning via Hierarchical Dual Averaging",
        "html": "https://ICML.cc//virtual/2021/poster/10737",
        "abstract": "We propose a hierarchical version of dual averaging for zeroth-order online non-convex optimization – i.e., learning processes where, at each stage, the optimizer is facing an unknown non-convex loss function and only receives the incurred loss as feedback. The proposed class of policies relies on the construction of an online model that aggregates loss information as it arrives, and it consists of two principal components: (a) a regularizer adapted to the Fisher information metric (as opposed to the metric norm of the ambient space); and (b) a principled exploration of the problem’s state space based on an adapted hierarchical schedule. This construction enables sharper control of the model’s bias and variance, and allows us to derive tight bounds for both the learner’s static and dynamic regret – i.e., the regret incurred against the best dynamic policy in hindsight over the horizon of play.",
        "conference": "ICML",
        "中文标题": "基于层次双平均的零阶非凸学习",
        "摘要翻译": "我们提出了一种层次版本的双平均方法，用于零阶在线非凸优化——即在每个阶段，优化器面对一个未知的非凸损失函数，并且仅接收所遭受的损失作为反馈的学习过程。所提出的策略类别依赖于构建一个在线模型，该模型在损失信息到达时进行聚合，并且主要由两个部分组成：(a) 一个适应于Fisher信息度量（与周围空间的度量范数相对）的正则化器；以及(b) 基于适应的层次调度对问题状态空间的原则性探索。这种构建能够更精确地控制模型的偏差和方差，并允许我们为学习者的静态和动态遗憾——即与游戏视野内最佳动态策略相比所遭受的遗憾——导出紧密的界限。",
        "领域": "非凸优化、在线学习、优化算法",
        "问题": "解决在仅接收损失反馈的情况下，进行零阶在线非凸优化的问题",
        "动机": "为了在非凸优化和在线学习的环境中，提供一种能够有效聚合损失信息并控制模型偏差和方差的方法",
        "方法": "提出了一种层次双平均方法，结合了适应于Fisher信息度量的正则化器和基于层次调度的状态空间探索",
        "关键词": [
            "零阶优化",
            "非凸学习",
            "双平均",
            "在线优化",
            "Fisher信息度量"
        ],
        "涉及的技术概念": {
            "双平均": "一种优化策略，用于在线学习环境中聚合损失信息",
            "Fisher信息度量": "用于正则化器的构建，适应于问题的信息结构而非周围空间的几何",
            "层次调度": "用于系统地探索问题的状态空间，以控制模型的偏差和方差"
        },
        "success": true
    },
    {
        "order": 1182,
        "title": "Z-GCNETs: Time Zigzags at Graph Convolutional Networks for Time Series Forecasting",
        "html": "https://ICML.cc//virtual/2021/poster/9391",
        "abstract": "There recently has been a surge of interest in developing a new class of deep learning (DL) architectures that integrate an explicit time dimension as a fundamental building block of learning and representation mechanisms. In turn, many recent results show that topological descriptors of the observed data, encoding information on the shape of the dataset in a topological space at different scales, that is, persistent homology of the data, may contain important complementary information, improving both performance and robustness of DL. As convergence of these two emerging ideas, we propose to enhance DL architectures with the most salient time-conditioned topological information of the data and introduce the concept of zigzag persistence into time-aware graph convolutional networks (GCNs). Zigzag persistence provides a systematic and mathematically rigorous framework to track the most important topological features of the observed data that tend to manifest themselves over time. To integrate the extracted time-conditioned topological descriptors into DL, we develop a new topological summary, zigzag persistence image, and derive its theoretical stability guarantees.  We validate the new GCNs with a time-aware zigzag topological layer (Z-GCNETs), in application to traffic forecasting and Ethereum blockchain price prediction. Our results indicate that Z-GCNET outperforms 13 state-of-the-art methods on 4 time series datasets.",
        "conference": "ICML",
        "中文标题": "Z-GCNETs：时间之字形图卷积网络用于时间序列预测",
        "摘要翻译": "最近，开发一类新的深度学习（DL）架构的兴趣激增，这些架构将明确的时间维度作为学习和表示机制的基本构建块。反过来，许多最近的结果表明，观察数据的拓扑描述符，即在不同的尺度上编码数据集在拓扑空间中的形状信息，即数据的持久同源性，可能包含重要的补充信息，提高DL的性能和鲁棒性。作为这两个新兴思想的融合，我们提出用数据的最显著时间条件拓扑信息来增强DL架构，并将之字形持久性的概念引入时间感知图卷积网络（GCNs）中。之字形持久性提供了一个系统且数学上严格的框架，以跟踪观察数据中随时间显现的最重要拓扑特征。为了将提取的时间条件拓扑描述符集成到DL中，我们开发了一种新的拓扑摘要，之字形持久性图像，并推导出其理论稳定性保证。我们在交通预测和以太坊区块链价格预测的应用中验证了带有时间感知之字形拓扑层的新GCNs（Z-GCNETs）。我们的结果表明，Z-GCNET在4个时间序列数据集上优于13种最先进的方法。",
        "领域": "图神经网络、时间序列预测、拓扑数据分析",
        "问题": "如何将时间维度和拓扑信息有效整合到深度学习架构中，以提高时间序列预测的性能和鲁棒性",
        "动机": "探索时间条件拓扑信息与深度学习架构的结合，以捕捉数据随时间变化的重要拓扑特征，提升预测性能",
        "方法": "提出Z-GCNETs，通过引入之字形持久性概念和时间感知拓扑层，将时间条件拓扑信息整合到图卷积网络中",
        "关键词": [
            "时间序列预测",
            "图卷积网络",
            "之字形持久性",
            "拓扑数据分析",
            "深度学习"
        ],
        "涉及的技术概念": {
            "之字形持久性": "提供了一种系统的方法来跟踪数据随时间变化的重要拓扑特征",
            "时间感知图卷积网络": "整合时间维度和拓扑信息，用于时间序列数据的深度学习",
            "拓扑摘要": "开发了之字形持久性图像作为新的拓扑摘要，用于将拓扑信息集成到深度学习中"
        },
        "success": true
    },
    {
        "order": 1183,
        "title": "Zoo-Tuning: Adaptive Transfer from A Zoo of Models",
        "html": "https://ICML.cc//virtual/2021/poster/9887",
        "abstract": "With the development of deep networks on various large-scale datasets, a large zoo of pretrained models are available. When transferring from a model zoo, applying classic single-model-based transfer learning methods to each source model suffers from high computational cost and cannot fully utilize the rich knowledge in the zoo. We propose \\emph{Zoo-Tuning} to address these challenges, which learns to adaptively transfer the parameters of pretrained models to the target task. With the learnable channel alignment layer and adaptive aggregation layer, Zoo-Tuning \\emph{adaptively aggregates channel aligned pretrained parameters to derive the target model}, which simultaneously promotes knowledge transfer and adapts source models to downstream tasks. The adaptive aggregation substantially reduces the computation cost at both training and inference. We further propose lite Zoo-Tuning with the temporal ensemble of batch average gating values to reduce the storage cost at the inference time. We evaluate our approach on a variety of tasks, including reinforcement learning, image classification, and facial landmark detection. Experiment results demonstrate that the proposed adaptive transfer learning approach can more effectively and efficiently transfer knowledge from a zoo of models.",
        "conference": "ICML",
        "中文标题": "动物园调谐：从模型动物园中自适应迁移",
        "摘要翻译": "随着在各种大规模数据集上深度网络的发展，大量的预训练模型变得可用。当从模型动物园迁移时，将经典的基于单一模型的迁移学习方法应用于每个源模型会遭受高计算成本的困扰，并且无法充分利用动物园中的丰富知识。我们提出了Zoo-Tuning来解决这些挑战，它学习自适应地将预训练模型的参数迁移到目标任务。通过可学习的通道对齐层和自适应聚合层，Zoo-Tuning自适应地聚合通道对齐的预训练参数以导出目标模型，这同时促进了知识迁移并使源模型适应下游任务。自适应聚合显著降低了训练和推理时的计算成本。我们进一步提出了轻量级Zoo-Tuning，通过批量平均门控值的时间集成来减少推理时的存储成本。我们在多种任务上评估了我们的方法，包括强化学习、图像分类和面部标志点检测。实验结果表明，所提出的自适应迁移学习方法可以更有效和高效地从模型动物园中迁移知识。",
        "领域": "迁移学习、模型压缩、多任务学习",
        "问题": "如何高效地从大量预训练模型中迁移知识到特定任务，同时降低计算和存储成本",
        "动机": "解决传统单模型迁移学习方法在处理模型动物园时的高计算成本和知识利用不足的问题",
        "方法": "提出Zoo-Tuning方法，通过可学习的通道对齐层和自适应聚合层，自适应地迁移和聚合预训练模型的参数",
        "关键词": [
            "自适应迁移学习",
            "模型动物园",
            "通道对齐",
            "参数聚合",
            "计算效率"
        ],
        "涉及的技术概念": {
            "可学习的通道对齐层": "用于调整不同预训练模型的输出通道，使其能够有效聚合",
            "自适应聚合层": "动态地选择和组合来自不同源模型的参数，以适应目标任务",
            "批量平均门控值的时间集成": "减少模型推理时的存储需求，通过集成批量处理的门控值来优化资源使用"
        },
        "success": true
    }
]